
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/82/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/cs.LG_2023_07_25/" class="article-date">
  <time datetime="2023-07-25T10:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/cs.LG_2023_07_25/">cs.LG - 2023-07-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes"><a href="#Multi-GPU-Approach-for-Training-of-Graph-ML-Models-on-large-CFD-Meshes" class="headerlink" title="Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes"></a>Multi-GPU Approach for Training of Graph ML Models on large CFD Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13592">http://arxiv.org/abs/2307.13592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Strönisch, Maximilian Sander, Andreas Knüpfer, Marcus Meyer</li>
<li>For: 这 paper 的目的是开发一种基于机器学习的拟合模型，用于加速计算流体力学 simulate 的过程。* Methods: 这 paper 使用了图 neural network (GNN) 作为拟合模型，并在多个 GPU 上分区和分配流体流体Domain。* Results:  Comparing 该 paper 的拟合模型与传统的分布式模型，后者 производи了更好的预测结果，并且超越了该拟合模型。<details>
<summary>Abstract</summary>
Mesh-based numerical solvers are an important part in many design tool chains. However, accurate simulations like computational fluid dynamics are time and resource consuming which is why surrogate models are employed to speed-up the solution process. Machine Learning based surrogate models on the other hand are fast in predicting approximate solutions but often lack accuracy. Thus, the development of the predictor in a predictor-corrector approach is the focus here, where the surrogate model predicts a flow field and the numerical solver corrects it. This paper scales a state-of-the-art surrogate model from the domain of graph-based machine learning to industry-relevant mesh sizes of a numerical flow simulation. The approach partitions and distributes the flow domain to multiple GPUs and provides halo exchange between these partitions during training. The utilized graph neural network operates directly on the numerical mesh and is able to preserve complex geometries as well as all other properties of the mesh. The proposed surrogate model is evaluated with an application on a three dimensional turbomachinery setup and compared to a traditionally trained distributed model. The results show that the traditional approach produces superior predictions and outperforms the proposed surrogate model. Possible explanations, improvements and future directions are outlined.
</details>
<details>
<summary>摘要</summary>
mesh-based numerical solvers 是设计工具链中的一个重要部分。然而，精确的 simulate like computational fluid dynamics 需要时间和资源，这是 why 使用 surrogate models 来快速解决方案。机器学习基于 surrogate models 则很快速预测 approximate solutions，但frequently lack accuracy。因此，在这里的发展问题是predictor-corrector方法中的开发预测器，这里的 surrogate model 预测了流场，而numerical solver 则更正。这篇文章 scales 一个 state-of-the-art surrogate model 从 domain of graph-based machine learning 到 industry-relevant mesh sizes 的 numerical flow simulation。该方法将流体Domain  partitioned 和分配到多个 GPUs，并在训练中提供了 halos exchange  между这些分区。使用的 graph neural network 直接操作 numerical mesh，能够保留复杂的几何和所有其他 mesh 的属性。提案的 surrogate model 与一个 three-dimensional turbomachinery 应用中进行比较，与传统的分布式训练模型相比，传统方法产生了更好的预测，超越了提案的 surrogate model。 possible explanations, improvements 和 future directions 也被详细描述。
</details></li>
</ul>
<hr>
<h2 id="Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning"><a href="#Settling-the-Sample-Complexity-of-Online-Reinforcement-Learning" class="headerlink" title="Settling the Sample Complexity of Online Reinforcement Learning"></a>Settling the Sample Complexity of Online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13586">http://arxiv.org/abs/2307.13586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du</li>
<li>for: The paper is written to address the issue of data efficiency in online reinforcement learning, specifically the problem of achieving minimax-optimal regret without incurring any burn-in cost.</li>
<li>methods: The paper proposes a modified version of Monotonic Value Propagation (MVP), a model-based algorithm, and develops a new regret decomposition strategy and analysis paradigm to decouple complicated statistical dependency.</li>
<li>results: The paper achieves a regret on the order of $(SAH^3K)&#x2F;\sqrt{\log K}$, which matches the minimax lower bound for the entire range of sample size $K\geq 1$, and translates to a PAC sample complexity of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range.<details>
<summary>Abstract</summary>
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.   We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}   \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax lower bound for the entire range of sample size $K\geq 1$, essentially eliminating any burn-in requirement. It also translates to a PAC sample complexity (i.e., the number of episodes needed to yield $\varepsilon$-accuracy) of $\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for the full $\varepsilon$-range.   Further, we extend our theory to unveil the influences of problem-dependent quantities like the optimal value/cost and certain variances. The key technical innovation lies in the development of a new regret decomposition strategy and a novel analysis paradigm to decouple complicated statistical dependency -- a long-standing challenge facing the analysis of online RL in the sample-hungry regime.
</details>
<details>
<summary>摘要</summary>
在在线强化学习中，数据效率是中心问题。虽然一些最近的研究已达到了几何上的最小误差，但这些结果的可行性只在大样本 regime 中保证，这意味着在使用这些算法时需要支付巨大的烧毁成本。如何在不支付任何烧毁成本的情况下实现最优误差响应是在RL理论中的开放问题。我们在具有finite-horizon不规则 Markov决策过程的上下文中解决了这个问题。我们证明了一种修改后的升权宣传（MVP）算法可以在不支付任何烧毁成本的情况下实现误差的最小化。具体来说，我们证明了MVP算法在SAH^3K sample size中的误差是(modulo log factor)最多为\begin{equation*}   \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*}  where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes.这个误差与整个样本大小$K\geq 1$的最小误差响应相同，实际上消除了任何烧毁要求。它还翻译到一个PAC样本复杂度（即要求episode数来实现 $\varepsilon$-精度）为$\frac{SAH^3}{\varepsilon^2}$，这是最优的PAC样本复杂度。此外，我们还扩展了我们的理论，探讨了问题依赖于问题特定的量，如优值/成本和某些方差。我们的关键技术创新在于开发了一种新的误差分解策略和一种新的分析方法，用于解耦在线RL在样本匮乏 regime 中的复杂的统计依赖关系。
</details></li>
</ul>
<hr>
<h2 id="Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks"><a href="#Piecewise-Linear-Functions-Representable-with-Infinite-Width-Shallow-ReLU-Neural-Networks" class="headerlink" title="Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks"></a>Piecewise Linear Functions Representable with Infinite Width Shallow ReLU Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14373">http://arxiv.org/abs/2307.14373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah McCarty</li>
<li>for: 这个论文研究了连续piecewise线性函数的无限宽深度学习模型，使用Rectified Linear Unit（ReLU）作为活化函数。</li>
<li>methods: 通过积分表示，这种深度学习模型可以被视为一种finite cost shallow neural network，并且可以被相应的signed,finite measure表示在适当的参数空间中。</li>
<li>results: 论文证明了 ONgie et al.的 conjecture，即任何连续piecewise线性函数都可以通过这种无限宽深度学习模型表示，而且这种表示可以被finite width shallow ReLU neural network来实现。<details>
<summary>Abstract</summary>
This paper analyzes representations of continuous piecewise linear functions with infinite width, finite cost shallow neural networks using the rectified linear unit (ReLU) as an activation function. Through its integral representation, a shallow neural network can be identified by the corresponding signed, finite measure on an appropriate parameter space. We map these measures on the parameter space to measures on the projective $n$-sphere cross $\mathbb{R}$, allowing points in the parameter space to be bijectively mapped to hyperplanes in the domain of the function. We prove a conjecture of Ongie et al. that every continuous piecewise linear function expressible with this kind of infinite width neural network is expressible as a finite width shallow ReLU neural network.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这篇论文研究了无限宽 continuous piecewise linear function的表示，使用 finite cost shallow neural network 和 rectified linear unit (ReLU)  activation function。通过积分表示，我们可以将 shallow neural network 与signed, finite measure on 相应的参数空间进行对应。然后，我们将这些度量映射到 projective $n$-sphere cross $\mathbb{R}$ 上，使得参数空间中的点可以一一映射到函数的域中的hyperplane。我们证明了 Ongie et al. 的 conjecture，即任何可表示为 infinite width neural network 的 continuous piecewise linear function都可以表示为 finite width shallow ReLU neural network。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys"><a href="#Comparing-Forward-and-Inverse-Design-Paradigms-A-Case-Study-on-Refractory-High-Entropy-Alloys" class="headerlink" title="Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys"></a>Comparing Forward and Inverse Design Paradigms: A Case Study on Refractory High-Entropy Alloys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13581">http://arxiv.org/abs/2307.13581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arindam Debnath, Lavanya Raman, Wenjie Li, Adam M. Krajewski, Marcia Ahn, Shuang Lin, Shunli Shang, Allison M. Beese, Zi-Kui Liu, Wesley F. Reinhart</li>
<li>for: 本研究的目的是比较前向和反向设计模型范文在实际应用中的性能。</li>
<li>methods: 本研究使用了反向设计方法，并对其进行了对比，以评估其在不同的目标和约束下的性能。</li>
<li>results: 研究发现，反向设计方法在refractory高 entropy合金设计中表现出色，能够更好地满足不同的目标和约束。<details>
<summary>Abstract</summary>
The rapid design of advanced materials is a topic of great scientific interest. The conventional, ``forward'' paradigm of materials design involves evaluating multiple candidates to determine the best candidate that matches the target properties. However, recent advances in the field of deep learning have given rise to the possibility of an ``inverse'' design paradigm for advanced materials, wherein a model provided with the target properties is able to find the best candidate. Being a relatively new concept, there remains a need to systematically evaluate how these two paradigms perform in practical applications. Therefore, the objective of this study is to directly, quantitatively compare the forward and inverse design modeling paradigms. We do so by considering two case studies of refractory high-entropy alloy design with different objectives and constraints and comparing the inverse design method to other forward schemes like localized forward search, high throughput screening, and multi objective optimization.
</details>
<details>
<summary>摘要</summary>
rapid design of advanced materials 是科学领域中很受欢迎的话题。传统的，“前进”的材料设计方法是评估多个候选者，以确定最佳符合目标性能的候选者。然而，近年，深度学习的发展使得“反向”的材料设计方法变得可能，其中一个模型提供目标性能后，能够找到最佳候选者。由于是一个新的概念，因此还需要系统地评估这两种方法在实际应用中的性能。因此，本研究的目标是直接、量化地比较前进和反向设计模型方法。我们通过考虑高熔环境高级合金设计的两个案例研究，并与其他前进方案如本地前进搜索、高通过率检测和多目标优化进行比较。
</details></li>
</ul>
<hr>
<h2 id="Reinterpreting-survival-analysis-in-the-universal-approximator-age"><a href="#Reinterpreting-survival-analysis-in-the-universal-approximator-age" class="headerlink" title="Reinterpreting survival analysis in the universal approximator age"></a>Reinterpreting survival analysis in the universal approximator age</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13579">http://arxiv.org/abs/2307.13579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdittmer/survival_analysis_sumo_plus_plus">https://github.com/sdittmer/survival_analysis_sumo_plus_plus</a></li>
<li>paper_authors: Sören Dittmer, Michael Roberts, Jacobus Preller, AIX COVNET, James H. F. Rudd, John A. D. Aston, Carola-Bibiane Schönlieb</li>
<li>for: 本文旨在提供用于深度学习中survival分析的工具，以便全面利用survival分析的潜在力量。</li>
<li>methods: 本文提出了一种新的损失函数、评价指标和首个可提供survival曲线的universalapproximation网络。</li>
<li>results: 对比其他方法，该损失函数和模型在大规模数字实验中表现出色。<details>
<summary>Abstract</summary>
Survival analysis is an integral part of the statistical toolbox. However, while most domains of classical statistics have embraced deep learning, survival analysis only recently gained some minor attention from the deep learning community. This recent development is likely in part motivated by the COVID-19 pandemic. We aim to provide the tools needed to fully harness the potential of survival analysis in deep learning. On the one hand, we discuss how survival analysis connects to classification and regression. On the other hand, we provide technical tools. We provide a new loss function, evaluation metrics, and the first universal approximating network that provably produces survival curves without numeric integration. We show that the loss function and model outperform other approaches using a large numerical study.
</details>
<details>
<summary>摘要</summary>
生存分析是统计工具箱中的一个重要组成部分。然而，在классиical统计领域中，大多数领域都已经欢迎了深度学习，而生存分析则只是最近才从深度学习社区得到了一些微的关注。这种最近的发展可能受到了COVID-19大流行的影响。我们的目标是为生存分析在深度学习中充分发挥作用的工具。一方面，我们讨论了生存分析与分类和回归之间的连接。另一方面，我们提供技术工具。我们提出了一个新的损失函数、评估指标和首个可提供生存曲线的全面拟合网络，无需数值积分。我们通过大量的数据分析表明，我们的损失函数和模型在其他方法上出现较好的表现。
</details></li>
</ul>
<hr>
<h2 id="PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances"><a href="#PT-mathrm-L-p-Partial-Transport-mathrm-L-p-Distances" class="headerlink" title="PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances"></a>PT$\mathrm{L}^{p}$: Partial Transport $\mathrm{L}^{p}$ Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13571">http://arxiv.org/abs/2307.13571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinran Liu, Yikun Bai, Huy Tran, Zhanqi Zhu, Matthew Thorpe, Soheil Kolouri</li>
<li>for: 本文提出了一种新的策略来比较普通的信号，即基于优化运输的partial transport $\mathrm{L}^{p}$距离。</li>
<li>methods: 本文使用了优化运输框架，并提出了partial transport $\mathrm{L}^{p}$距离作为一种新的策略来比较普通的信号。</li>
<li>results: 本文提供了partial transport $\mathrm{L}^{p}$距离的理论背景，包括优化计划的存在和距离在不同的限制下的行为。此外，本文还介绍了对这种距离的剖分变化，以及它在信号分类和最近邻近分类中的应用。<details>
<summary>Abstract</summary>
Optimal transport and its related problems, including optimal partial transport, have proven to be valuable tools in machine learning for computing meaningful distances between probability or positive measures. This success has led to a growing interest in defining transport-based distances that allow for comparing signed measures and, more generally, multi-channeled signals. Transport $\mathrm{L}^{p}$ distances are notable extensions of the optimal transport framework to signed and possibly multi-channeled signals. In this paper, we introduce partial transport $\mathrm{L}^{p}$ distances as a new family of metrics for comparing generic signals, benefiting from the robustness of partial transport distances. We provide theoretical background such as the existence of optimal plans and the behavior of the distance in various limits. Furthermore, we introduce the sliced variation of these distances, which allows for rapid comparison of generic signals. Finally, we demonstrate the application of the proposed distances in signal class separability and nearest neighbor classification.
</details>
<details>
<summary>摘要</summary>
优化交通和其相关问题，包括优化部分交通，在机器学习中证明是有用的工具来计算概率或正式推论中的有意义距离。这种成功引起了比较Transport基于距离的定义，以便比较签名的推论和更一般的多通道信号。TransportLP distances是优化交通框架中的可扩展，用于比较签名或多通道信号。在这篇论文中，我们介绍partial transportLP distances作为比较通用信号的新家族度量，受到部分交通距离的稳定性的启发。我们还提供了有关最佳计划的存在和距离的不同限制下的行为。此外，我们还介绍了这些距离的割裂变种，可以快速比较通用信号。最后，我们示出了提案的距离在信号分类和最近邻居分类中的应用。
</details></li>
</ul>
<hr>
<h2 id="Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization"><a href="#Introducing-Hybrid-Modeling-with-Time-series-Transformers-A-Comparative-Study-of-Series-and-Parallel-Approach-in-Batch-Crystallization" class="headerlink" title="Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization"></a>Introducing Hybrid Modeling with Time-series-Transformers: A Comparative Study of Series and Parallel Approach in Batch Crystallization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05749">http://arxiv.org/abs/2308.05749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Joseph S Kwon</li>
<li>For: The paper is written for the development of a first-of-a-kind, attention-based time-series transformer (TST) hybrid framework for batch crystallization, which aims to improve the accuracy and interpretability of digital twins in chemical manufacturing.* Methods: The paper uses a hybrid approach that combines first-principles physics-based dynamics with machine learning (ML) models, specifically attention-based TSTs, to capture long-term and short-term changes in process states. The authors compare two different configurations of TST-based hybrid models and evaluate their performance using normalized-mean-square-error (NMSE) and $R^2$ values.* Results: The paper reports improved accuracy and interpretability of the TST-based hybrid models compared to traditional black-box models, with NMSE values in the range of $[10, 50]\times10^{-4}$ and $R^2$ values over 0.99. The authors also demonstrate the effectiveness of the hybrid models in predicting batch crystallization processes.<details>
<summary>Abstract</summary>
Most existing digital twins rely on data-driven black-box models, predominantly using deep neural recurrent, and convolutional neural networks (DNNs, RNNs, and CNNs) to capture the dynamics of chemical systems. However, these models have not seen the light of day, given the hesitance of directly deploying a black-box tool in practice due to safety and operational issues. To tackle this conundrum, hybrid models combining first-principles physics-based dynamics with machine learning (ML) models have increased in popularity as they are considered a 'best of both worlds' approach. That said, existing simple DNN models are not adept at long-term time-series predictions and utilizing contextual information on the trajectory of the process dynamics. Recently, attention-based time-series transformers (TSTs) that leverage multi-headed attention mechanism and positional encoding to capture long-term and short-term changes in process states have shown high predictive performance. Thus, a first-of-a-kind, TST-based hybrid framework has been developed for batch crystallization, demonstrating improved accuracy and interpretability compared to traditional black-box models. Specifically, two different configurations (i.e., series and parallel) of TST-based hybrid models are constructed and compared, which show a normalized-mean-square-error (NMSE) in the range of $[10, 50]\times10^{-4}$ and an $R^2$ value over 0.99. Given the growing adoption of digital twins, next-generation attention-based hybrid models are expected to play a crucial role in shaping the future of chemical manufacturing.
</details>
<details>
<summary>摘要</summary>
现有的数字双胞虫大多采用数据驱动黑盒模型，主要使用深度循环神经网络（RNN）和卷积神经网络（CNN）来捕捉化学系统的动态。然而，这些模型尚未得到实际应用，因为直接部署黑盒工具会带来安全和运营问题。为解决这个悖论，Hybrid模型，结合物理基础知识和机器学习（ML）模型，在化学制造中得到了广泛的应用。然而，现有的简单的DNN模型不具备长期时间序列预测和利用过程动态轨迹上的 контекст信息。最近，听力基于时间序列变换器（TST），利用多头听力机制和位置编码，能够捕捉长期和短期变化的过程状态，已经显示出高预测性能。因此，一种首次实现的TST基于混合框架，在批晶凝聚过程中得到了改进的准确性和可解释性，相比传统黑盒模型。具体来说，我们构建了两种不同的配置（即串行和平行）的TST基于混合模型，其NMSE在 $[10, 50]\times10^{-4}$ 之间，$R^2$ 值超过 0.99。随着数字双胞虫的普及，未来的听力基于混合模型将在化学制造中扮演关键的角色。
</details></li>
</ul>
<hr>
<h2 id="Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities"><a href="#Decision-Focused-Learning-Foundations-State-of-the-Art-Benchmark-and-Future-Opportunities" class="headerlink" title="Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities"></a>Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13565">http://arxiv.org/abs/2307.13565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/predopt/predopt-benchmarks">https://github.com/predopt/predopt-benchmarks</a></li>
<li>paper_authors: Jayanta Mandi, James Kotary, Senne Berden, Maxime Mulamba, Victor Bucarey, Tias Guns, Ferdinando Fioretto</li>
<li>for: 本研究は、机器学习中的决策专注式学习（DFL）的实现方法についての综观的评估を提供。</li>
<li>methods: 本研究使用了多种integraging machine learning和优化模型的技术，包括内置式学习、迭代式学习、迭代式优化、等。</li>
<li>results: 本研究提出了一个综合性的DFL方法分类系统，并进行了广泛的实验评估。结果显示，DFL方法可以在许多不确定性下的决策任务中实现更好的性能。<details>
<summary>Abstract</summary>
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models, introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
</details>
<details>
<summary>摘要</summary>
决策驱动学习（DFL）是一种emerging paradigm在机器学习领域，它将机器学习模型训练为优化决策，并将预测和优化 integrate into an end-to-end system。这种 paradigm 承诺可以 revolutionize 决策making 在uncertainty 环境中的应用，因为在这些决策模型中 estimate unknown parameters 时常常成为一个substantial roadblock。这篇文章提供了 DFL 的全面 Review，包括了不同技术的 integrate machine learning 和优化模型的分析，并提出了 DFL 方法的分类，以及适用于 DFL 的 Benchmark 数据集和任务。最后，文章还提供了有价值的 Insight 到当前和未来 DFL 研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Node-Injection-Link-Stealing-Attack"><a href="#Node-Injection-Link-Stealing-Attack" class="headerlink" title="Node Injection Link Stealing Attack"></a>Node Injection Link Stealing Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13548">http://arxiv.org/abs/2307.13548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oualid Zari, Javier Parra-Arnau, Ayşe Ünsal, Melek Önen</li>
<li>for: 攻击Graph Neural Networks（GNNs）中的隐私漏洞，泄露图形数据中的私有链接信息。</li>
<li>methods: 利用新节点加入图和API查询预测来研究隐私链接信息泄露的可能性，并提出防止隐私泄露的方法以保持模型实用性。</li>
<li>results: 对比现有方法，我们的攻击方法在推断链接信息方面表现出色，同时我们还分析了应用 differential privacy（DP）机制来mitigate攻击的影响，并研究了隐私保护和模型实用性之间的质量衡量。<details>
<summary>Abstract</summary>
In this paper, we present a stealthy and effective attack that exposes privacy vulnerabilities in Graph Neural Networks (GNNs) by inferring private links within graph-structured data. Focusing on the inductive setting where new nodes join the graph and an API is used to query predictions, we investigate the potential leakage of private edge information. We also propose methods to preserve privacy while maintaining model utility. Our attack demonstrates superior performance in inferring the links compared to the state of the art. Furthermore, we examine the application of differential privacy (DP) mechanisms to mitigate the impact of our proposed attack, we analyze the trade-off between privacy preservation and model utility. Our work highlights the privacy vulnerabilities inherent in GNNs, underscoring the importance of developing robust privacy-preserving mechanisms for their application.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种隐蔽和有效的攻击，暴露了图神经网络（GNN）中的隐私漏洞，通过推断图结构数据中的私人链接。我们在新节点加入图时的招呼设定下进行研究，并使用 API 来查询预测结果。我们发现了私人边信息泄露的可能性，并提出了保持隐私的方法，以保持模型的有用性。我们的攻击性能superior于现有的状态。此外，我们还研究了在应用 differential privacy（DP）机制时，如何减轻我们所提出的攻击的影响。我们分析了隐私保护和模型有用性之间的负担，我们的工作抛光了 GNN 中的隐私漏洞，强调了在其应用中发展Robust隐私保护机制的重要性。
</details></li>
</ul>
<hr>
<h2 id="Transfer-Learning-for-Portfolio-Optimization"><a href="#Transfer-Learning-for-Portfolio-Optimization" class="headerlink" title="Transfer Learning for Portfolio Optimization"></a>Transfer Learning for Portfolio Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13546">http://arxiv.org/abs/2307.13546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Cao, Haotian Gu, Xin Guo, Mathieu Rosenbaum</li>
<li>for: 本文探讨了通过传输学习技术解决金融 portefolio优化问题的可能性。</li>
<li>methods: 本文引入了一种新的概念 called “传输风险”，用于优化传输学习方法的优化框架。</li>
<li>results: numerical experiments 表明，传输风险与传输学习方法的总性表现之间存在强相关关系，表明传输风险为”可传输性”的可靠指标。<details>
<summary>Abstract</summary>
In this work, we explore the possibility of utilizing transfer learning techniques to address the financial portfolio optimization problem. We introduce a novel concept called "transfer risk", within the optimization framework of transfer learning. A series of numerical experiments are conducted from three categories: cross-continent transfer, cross-sector transfer, and cross-frequency transfer. In particular, 1. a strong correlation between the transfer risk and the overall performance of transfer learning methods is established, underscoring the significance of transfer risk as a viable indicator of "transferability"; 2. transfer risk is shown to provide a computationally efficient way to identify appropriate source tasks in transfer learning, enhancing the efficiency and effectiveness of the transfer learning approach; 3. additionally, the numerical experiments offer valuable new insights for portfolio management across these different settings.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们探讨了使用传输学习技术来解决金融股票优化问题的可能性。我们介绍了一种新的概念 called "传输风险"，这个概念在传输学习优化框架中被引入。我们从三类实验中进行了数据分析：跨大陆传输、跨业传输和跨频传输。具体来说，我们发现：1. 传输风险和传输学习方法的总性性能之间存在强正相关关系，这确立了传输风险作为"传输可用性"的可靠指标的重要性。2. 传输风险可以提供一种计算效率高的方法来确定合适的源任务，从而提高传输学习方法的效率和效果。3. 数据分析还提供了有价值的新视角 для股票管理在不同设置下。Here's the translation in Traditional Chinese:在这个工作中，我们探讨了使用传递学习技术来解决金融股票优化问题的可能性。我们介绍了一个新的概念 called "传递风险"，这个概念在传递学习优化框架中被引入。我们从三种类型的实验中进行了数据分析：跨大陆传递、跨业传递和跨频传递。具体来说，我们发现：1. 传递风险和传递学习方法的总性性能之间存在强正相关关系，这确立了传递风险作为"传递可用性"的可靠指标的重要性。2. 传递风险可以提供一种计算效率高的方法来决定合适的源任务，从而提高传递学习方法的效率和效果。3. 数据分析还提供了有价值的新视角 для股票管理在不同设置下。
</details></li>
</ul>
<hr>
<h2 id="A-model-for-efficient-dynamical-ranking-in-networks"><a href="#A-model-for-efficient-dynamical-ranking-in-networks" class="headerlink" title="A model for efficient dynamical ranking in networks"></a>A model for efficient dynamical ranking in networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13544">http://arxiv.org/abs/2307.13544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Della Vecchia, Kibidi Neocosmos, Daniel B. Larremore, Cristopher Moore, Caterina De Bacco</li>
<li>for: 这篇论文旨在提出一种物理启发的方法，用于在指定的时间网络中INFER动态排名。</li>
<li>methods: 该方法是解一个线性方程系统，只需要一个参数调整。</li>
<li>results: 经测试，该方法可以更好地预测交互（边的存在）和其结果（边的方向），在许多情况下表现比较出色。<details>
<summary>Abstract</summary>
We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dynamic rankings and interaction outcomes.
</details>
<details>
<summary>摘要</summary>
我们提出一种物理启发的方法估算直接时间网络中的动态排名 - 直接时间网络中每个Directed和时间戳的边都表示对话的结果和时间，例如赢利或失败。我们的方法会解决一个线性方程系统，只需要一个参数调整，因此算法可扩展和高效。我们对多种应用进行测试，包括 sintetic 数据和实际数据，并发现我们的方法在许多情况下的性能比现有方法更高。
</details></li>
</ul>
<hr>
<h2 id="Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation"><a href="#Model-Calibration-in-Dense-Classification-with-Adaptive-Label-Perturbation" class="headerlink" title="Model Calibration in Dense Classification with Adaptive Label Perturbation"></a>Model Calibration in Dense Classification with Adaptive Label Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13539">http://arxiv.org/abs/2307.13539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carlisle-liu/aslp">https://github.com/carlisle-liu/aslp</a></li>
<li>paper_authors: Jiawei Liu, Changkun Ye, Shan Wang, Ruikai Cui, Jing Zhang, Kaihao Zhang, Nick Barnes</li>
<li>for: 本研究旨在提高深度神经网络的准确率和信任度，以便在安全应用中使用。</li>
<li>methods: 本文提出了一种名为 Adaptive Stochastic Label Perturbation（ASLP）的方法，它可以学习每个训练图像的唯一标签杂化水平。ASLP使用的是我们提出的 Self-Calibrating Binary Cross Entropy（SC-BCE）损失函数，它将杂化过程、标签杂化和标签平滑等进程集成到一起，以达到更好的准确率和信任度。</li>
<li>results: 对比于传统的 dense binary classification 模型，ASLP 可以显著提高模型的准确率和信任度。在 known data 上保持 классификация精度作为保守解决方案，或者特定地改进模型的准确率和信任度。<details>
<summary>Abstract</summary>
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label. Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on https://github.com/Carlisle-Liu/ASLP.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>保持知道数据上的分类精度作为保守解决方案，或2. 特别是提高模型准确率和预期确信度之间的差异，以iminimize the gap between the prediction accuracy and expected confidence of the target training label.Extensive results demonstrate that ASLP can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on <a target="_blank" rel="noopener" href="https://github.com/Carlisle-Liu/ASLP">https://github.com/Carlisle-Liu/ASLP</a>.</details></li>
</ol>
<hr>
<h2 id="INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations"><a href="#INFINITY-Neural-Field-Modeling-for-Reynolds-Averaged-Navier-Stokes-Equations" class="headerlink" title="INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations"></a>INFINITY: Neural Field Modeling for Reynolds-Averaged Navier-Stokes Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13538">http://arxiv.org/abs/2307.13538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Louis Serrano, Leon Migus, Yuan Yin, Jocelyn Ahmed Mazari, Patrick Gallinari</li>
<li>for: 这篇论文的目的是提出一种基于深度学习的精准仿真模型，用于简化物理现象的计算。</li>
<li>methods: 该方法使用含义 Neil 表示（INRs）来地址这个挑战，将物理场景的几何信息和物理场景编码成紧凑的表示，然后学习这些表示之间的映射，以便从物理场景中推断物理场景。</li>
<li>results: 在一个空气foil设计优化问题中，该方法达到了当前最佳性能，准确地预测了物理场景中的场景和表面上的物理场景。此外，该方法还可以在设计探索和形状优化等应用中使用，并能够正确预测拖拽和升力系数。<details>
<summary>Abstract</summary>
For numerical design, the development of efficient and accurate surrogate models is paramount. They allow us to approximate complex physical phenomena, thereby reducing the computational burden of direct numerical simulations. We propose INFINITY, a deep learning model that utilizes implicit neural representations (INRs) to address this challenge. Our framework encodes geometric information and physical fields into compact representations and learns a mapping between them to infer the physical fields. We use an airfoil design optimization problem as an example task and we evaluate our approach on the challenging AirfRANS dataset, which closely resembles real-world industrial use-cases. The experimental results demonstrate that our framework achieves state-of-the-art performance by accurately inferring physical fields throughout the volume and surface. Additionally we demonstrate its applicability in contexts such as design exploration and shape optimization: our model can correctly predict drag and lift coefficients while adhering to the equations.
</details>
<details>
<summary>摘要</summary>
für numerische Entwurfsdesign ist die Entwicklung effizienter und genauer surrogatmodelle von entscheidender Bedeutung. Sie ermöglichen uns, komplexe physikalische Phänomene zu approximiieren, somit die computermonierte Last von direkten numerischen Simulationen zu reduzieren. Wir schlagen INFINITY vor, ein tiefes lernendes Modell, das impliciten neuronalen Darstellungen (INRs) nutzt, um diese Herausforderung zu meistern. Unser Framework kodiert geometrische Informationen und physikalische Felder in compacten Darstellungen und lernt eine Abbildung zwischen ihnen, um die physikalischen Felder zu infolgen. Wir verwenden ein airfoil-Design-Optimierungstask als Beispielaufgabe und bewerten unsere Methode am schwierigen AirfRANS-Datensatz, der eng mit realen industriellen Anwendungen verwandt ist. Die experimentellen Ergebnisse zeigen, dass unsere Methode einen neuen Standardsatz erreicht, indem sie physikalische Felder in Volumen und Oberfläche genau approximiiert. Wir demonstrieren ferner ihre Anwendbarkeit in Kontexten wie Design-Exploration und Form-Optimierung: unser Modell kann richtig druck- und liftkoeffizienten vorhersagen, ohne die Gleichungen zu verletzen.
</details></li>
</ul>
<hr>
<h2 id="Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings"><a href="#Do-algorithms-and-barriers-for-sparse-principal-component-analysis-extend-to-other-structured-settings" class="headerlink" title="Do algorithms and barriers for sparse principal component analysis extend to other structured settings?"></a>Do algorithms and barriers for sparse principal component analysis extend to other structured settings?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13535">http://arxiv.org/abs/2307.13535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guanyi Wang, Mengqi Lou, Ashwin Pananjady</li>
<li>for: 本研究探讨了带有杂质模型的主成分分析问题，使用 union-of-subspace 模型捕捉信号结构。</li>
<li>methods: 本研究使用了统计和计算方法，并设立了基本的限制，证明了一种自然的投影力方法在特定情况下展现了局部 converges 性。</li>
<li>results: 研究发现，一些对于普通稀缺 PCA 的现象也适用于其结构化版本。<details>
<summary>Abstract</summary>
We study a principal component analysis problem under the spiked Wishart model in which the structure in the signal is captured by a class of union-of-subspace models. This general class includes vanilla sparse PCA as well as its variants with graph sparsity. With the goal of studying these problems under a unified statistical and computational lens, we establish fundamental limits that depend on the geometry of the problem instance, and show that a natural projected power method exhibits local convergence to the statistically near-optimal neighborhood of the solution. We complement these results with end-to-end analyses of two important special cases given by path and tree sparsity in a general basis, showing initialization methods and matching evidence of computational hardness. Overall, our results indicate that several of the phenomena observed for vanilla sparse PCA extend in a natural fashion to its structured counterparts.
</details>
<details>
<summary>摘要</summary>
我们研究一个主成分分析问题，其中信号结构是由一类联合子空间模型捕捉的。这个总类包括普通稀畴PCA以及其变体具有图稀畴。为了在统一的统计和计算镜头下研究这些问题，我们确立了基本的限制，并证明自然的投影力方法在统计上准确的邻居解决方案附近进行本地准确。我们补充了一些重要的特殊情况分析，包括路径和树稀畴在一般基础上，并提供了初始化方法和匹配证明的计算困难。总之，我们的结果表明，许多vanilla sparse PCA的现象在其结构化对应中也有自然的扩展。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Turbulence-II"><a href="#Differentiable-Turbulence-II" class="headerlink" title="Differentiable Turbulence II"></a>Differentiable Turbulence II</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13533">http://arxiv.org/abs/2307.13533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Varun Shankar, Romit Maulik, Venkatasubramanian Viswanathan</li>
<li>for:  This paper is written for developing data-driven models in computational fluid dynamics (CFD) using differentiable fluid simulators and machine learning (ML) methods.</li>
<li>methods: The paper proposes a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, and applies the technique to learn a sub-grid scale closure using a multi-scale graph neural network.</li>
<li>results: The learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid, resulting in an equivalent speedup of 10x. The method has been demonstrated on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry.<details>
<summary>Abstract</summary>
Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x. As the desire and need for cheaper CFD simulations grows, we see hybrid physics-ML methods as a path forward to be exploited in the near future.
</details>
<details>
<summary>摘要</summary>
diferenciable 流体 simulator 在 Computational Fluid Dynamics (CFD) 中展示了越来越多的价值，作为数据驱动模型的有用工具。 diferenciable turbulence，也就是在 CFD 解决方案算法中嵌入机器学习 (ML) 模型的终端训练，捕捉了物理基础的通用力和初始成本的限制，以及深度学习方法的自动训练和灵活性。我们开发了一个抽象 Finite Element 数学模型的框架，将深度学习模型集成到 Navier-Stokes 方程中，并使用多尺度图 neural network 来学习子网格抑制。我们在不同的 Reynolds 数和新geometry 上进行了多个实现，并示出了学习 closure 可以与传统大 Eddy simulation 相当的精度相比，在一个等效的加速10倍的粗网上。随着 CFD  simulation 的成本下降的需求和需求，我们认为 hybrid physics-ML 方法将在未来被利用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators"><a href="#Towards-Long-Term-predictions-of-Turbulence-using-Neural-Operators" class="headerlink" title="Towards Long-Term predictions of Turbulence using Neural Operators"></a>Towards Long-Term predictions of Turbulence using Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13517">http://arxiv.org/abs/2307.13517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fernando Gonzalez, François-Xavier Demoulin, Simon Bernard</li>
<li>for: 这 paper 探讨了使用神经操作符来预测湍流，主要关注于欧姆 neural 操作符（FNO）模型。它的目的是通过机器学习来开发减少的order&#x2F;代理模型，以便为湍流计算 simulate 提供更好的估算。</li>
<li>methods: 这 paper 使用了不同的模型配置，包括 U-NET 结构（UNO 和 U-FNET），并发现这些结构在准确性和稳定性方面表现更好于标准 FNO。U-FNET 在更高的 Reynolds 数下预测湍流的能力更高。使用梯度和稳定性损失来保证模型的稳定和准确预测。</li>
<li>results: 这 paper 发现，使用不同的模型配置和梯度损失可以获得更好的预测结果。特别是，U-FNET 在更高的 Reynolds 数下预测湍流的能力更高。然而，为了更好地评估深度学习模型在液流预测中的性能，还需要开发更好的评价指标。<details>
<summary>Abstract</summary>
This paper explores Neural Operators to predict turbulent flows, focusing on the Fourier Neural Operator (FNO) model. It aims to develop reduced-order/surrogate models for turbulent flow simulations using Machine Learning. Different model configurations are analyzed, with U-NET structures (UNO and U-FNET) performing better than the standard FNO in accuracy and stability. U-FNET excels in predicting turbulence at higher Reynolds numbers. Regularization terms, like gradient and stability losses, are essential for stable and accurate predictions. The study emphasizes the need for improved metrics for deep learning models in fluid flow prediction. Further research should focus on models handling complex flows and practical benchmarking metrics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes"><a href="#An-Empirical-Study-on-Fairness-Improvement-with-Multiple-Protected-Attributes" class="headerlink" title="An Empirical Study on Fairness Improvement with Multiple Protected Attributes"></a>An Empirical Study on Fairness Improvement with Multiple Protected Attributes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01923">http://arxiv.org/abs/2308.01923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenpeng Chen, Jie M. Zhang, Federica Sarro, Mark Harman</li>
<li>for: 本研究旨在探讨多个保护特征的公平改进策略的效果，以帮助更好地理解多特征公平改进策略的性能。</li>
<li>methods: 本研究使用了11种最新的公平改进方法，包括对多个保护特征进行公平改进。我们使用不同的数据集、度量和机器学习模型来分析这些方法的效果。</li>
<li>results: 研究发现，为一个保护特征进行公平改进可能会导致其他保护特征的公平性下降，这种下降的比例可达88.3%（平均为57.5%）。同时，我们发现在考虑多个保护特征时，精度和准确率的影响相对较小，但是 recall 的影响相对较大。这些结论有重要意义，因为现有的研究通常只报告精度作为机器学习性能指标，这并不充分。<details>
<summary>Abstract</summary>
Existing research mostly improves the fairness of Machine Learning (ML) software regarding a single protected attribute at a time, but this is unrealistic given that many users have multiple protected attributes. This paper conducts an extensive study of fairness improvement regarding multiple protected attributes, covering 11 state-of-the-art fairness improvement methods. We analyze the effectiveness of these methods with different datasets, metrics, and ML models when considering multiple protected attributes. The results reveal that improving fairness for a single protected attribute can largely decrease fairness regarding unconsidered protected attributes. This decrease is observed in up to 88.3% of scenarios (57.5% on average). More surprisingly, we find little difference in accuracy loss when considering single and multiple protected attributes, indicating that accuracy can be maintained in the multiple-attribute paradigm. However, the effect on precision and recall when handling multiple protected attributes is about 5 times and 8 times that of a single attribute. This has important implications for future fairness research: reporting only accuracy as the ML performance metric, which is currently common in the literature, is inadequate.
</details>
<details>
<summary>摘要</summary>
现有研究主要是在单个保护属性上提高机器学习软件的公平性，但这是不切实际的，因为用户通常有多个保护属性。这篇论文进行了对多个保护属性的公平性提高方法的广泛研究，涵盖了11种现状最佳实践。我们对不同的数据集、 метри和机器学习模型进行了这些方法的分析，并评估了它们在考虑多个保护属性时的效果。结果表明，只考虑一个保护属性进行公平性提高可能会导致其他保护属性的不公平性减少，这种减少率在88.3%的场景中（57.5%的平均值）被观察到。更有趣的是，考虑单个和多个保护属性时，准确性损失的差异很小，这表示在多属性情况下，准确性可以维持。然而，处理多个保护属性时的精度和回归的影响相对较大，大约是单个属性的5倍和8倍。这有重要的意义，未来公平性研究应该不再仅仅是通过准确性来评估机器学习软件的性能。
</details></li>
</ul>
<hr>
<h2 id="Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series"><a href="#Continuous-Time-Evidential-Distributions-for-Irregular-Time-Series" class="headerlink" title="Continuous Time Evidential Distributions for Irregular Time Series"></a>Continuous Time Evidential Distributions for Irregular Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13503">http://arxiv.org/abs/2307.13503</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/twkillian/edict">https://github.com/twkillian/edict</a></li>
<li>paper_authors: Taylor W. Killian, Haoran Zhang, Thomas Hartvigsen, Ava P. Amini</li>
<li>for: 这篇论文是用于描述一种为健康照顾等实际应用场景中的不规则时间序列进行预测的方法。</li>
<li>methods: 这篇论文使用了一种名为EDICT的策略，它 learns一个证据分布来描述不规则时间序列。这个分布可以在不同的时间点进行不同的推论，并且可以在缺乏观察的情况下提供更好的uncertainty estimation。</li>
<li>results: 这篇论文的结果显示，EDICT可以在具有挑战性的时间序列分类任务中实现竞争性的表现，并且可以在缺乏观察的情况下提供更好的uncertainty-guided推论。<details>
<summary>Abstract</summary>
Prevalent in many real-world settings such as healthcare, irregular time series are challenging to formulate predictions from. It is difficult to infer the value of a feature at any given time when observations are sporadic, as it could take on a range of values depending on when it was last observed. To characterize this uncertainty we present EDICT, a strategy that learns an evidential distribution over irregular time series in continuous time. This distribution enables well-calibrated and flexible inference of partially observed features at any time of interest, while expanding uncertainty temporally for sparse, irregular observations. We demonstrate that EDICT attains competitive performance on challenging time series classification tasks and enabling uncertainty-guided inference when encountering noisy data.
</details>
<details>
<summary>摘要</summary>
广泛存在在现实世界中的应用场景，如医疗、财经等，不规则时间序列是预测的挑战。因为观察是间歇的，特征值的推测是具有uncertainty的，可能在不同的时间点 prendre on a range of values。为了捕捉这种uncertainty，我们提出了EDICT策略，它在连续时间中学习不规则时间序列的证据分布。这种分布允许在任何时间点进行高度抽象和灵活的特征值推测，同时在笼统观察中扩展uncertainty。我们示例了EDICT在具有噪声数据的时间序列分类任务中的竞争性表现和uncertainty导航能力。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management"><a href="#Deep-Reinforcement-Learning-for-Robust-Goal-Based-Wealth-Management" class="headerlink" title="Deep Reinforcement Learning for Robust Goal-Based Wealth Management"></a>Deep Reinforcement Learning for Robust Goal-Based Wealth Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13501">http://arxiv.org/abs/2307.13501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Bauman, Bruno Gašperov, Stjepan Begušić, Zvonko Kostanjčar</li>
<li>for: 这个研究旨在提出一种基于深度强化学习的 Robust Goal-Based Wealth Management 方法，以便实现特定金融目标。</li>
<li>methods: 本研究使用了深度强化学习技术来估算投资选择，并通过训练一个对应投资策略的神经网络来实现目标。</li>
<li>results: 实验结果显示，该方法比较多的目标评估和投资策略选择方法来得到更好的效果，并在实际市场数据上显示出优越性。<details>
<summary>Abstract</summary>
Goal-based investing is an approach to wealth management that prioritizes achieving specific financial goals. It is naturally formulated as a sequential decision-making problem as it requires choosing the appropriate investment until a goal is achieved. Consequently, reinforcement learning, a machine learning technique appropriate for sequential decision-making, offers a promising path for optimizing these investment strategies. In this paper, a novel approach for robust goal-based wealth management based on deep reinforcement learning is proposed. The experimental results indicate its superiority over several goal-based wealth management benchmarks on both simulated and historical market data.
</details>
<details>
<summary>摘要</summary>
目的基本投资是一种财务管理方法，强调达到特定的金融目标。这是一个顺序决策问题，因为需要选择适当的投资直到达到目标。因此，深度回归学习，一种适合顺序决策的机器学习技术，对于优化这些投资策略表现出了承诺。在这篇论文中，一种基于深度回归学习的新方法 дляrobust目的基本财务管理被提议。实验结果表明，这种方法在模拟和历史市场数据上都超过了多个目的基本财务管理参考标准。
</details></li>
</ul>
<hr>
<h2 id="Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks"><a href="#Finding-Money-Launderers-Using-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Finding Money Launderers Using Heterogeneous Graph Neural Networks"></a>Finding Money Launderers Using Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13499">http://arxiv.org/abs/2307.13499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredjo89/heterogeneous-mpnn">https://github.com/fredjo89/heterogeneous-mpnn</a></li>
<li>paper_authors: Fredrik Johannessen, Martin Jullum</li>
<li>for: 本研究旨在提高银行电子监测系统的检测犯罪吸收能力，使用机器学习方法对大规模不同类型数据图进行分析。</li>
<li>methods: 本研究使用图神经网络（GNN）方法，对实际世界银行交易和企业角色数据构建的大型不同类型数据图进行分析。特别是，我们对现有的同质GNN方法（Message Passing Neural Network，MPNN）进行扩展，使其在不同类型数据图上有效运行。我们还提出了一种新的消息汇聚方法，以便在不同边的消息之间进行效果地汇聚。</li>
<li>results: 我们的模型实现了在大规模不同类型数据图上对犯罪吸收进行有效检测，提高了银行电子监测系统的检测精度。这是首次将GNN应用于实际世界大规模不同类型数据图进行反洗钱检测，我们的研究成果具有广泛的应用前景。<details>
<summary>Abstract</summary>
Current anti-money laundering (AML) systems, predominantly rule-based, exhibit notable shortcomings in efficiently and precisely detecting instances of money laundering. As a result, there has been a recent surge toward exploring alternative approaches, particularly those utilizing machine learning. Since criminals often collaborate in their money laundering endeavors, accounting for diverse types of customer relations and links becomes crucial. In line with this, the present paper introduces a graph neural network (GNN) approach to identify money laundering activities within a large heterogeneous network constructed from real-world bank transactions and business role data belonging to DNB, Norway's largest bank. Specifically, we extend the homogeneous GNN method known as the Message Passing Neural Network (MPNN) to operate effectively on a heterogeneous graph. As part of this procedure, we propose a novel method for aggregating messages across different edges of the graph. Our findings highlight the importance of using an appropriate GNN architecture when combining information in heterogeneous graphs. The performance results of our model demonstrate great potential in enhancing the quality of electronic surveillance systems employed by banks to detect instances of money laundering. To the best of our knowledge, this is the first published work applying GNN on a large real-world heterogeneous network for anti-money laundering purposes.
</details>
<details>
<summary>摘要</summary>
现有的反贩卖财 (AML) 系统，主要基于规则，显示出明显的缺陷，无法有效地和精准地检测贩卖财活动。因此，有一些最新的研究尝试使用机器学习方法。由于别派犯罪分子通常会合作，在检测贩卖财活动时，考虑到不同类型的客户关系和链接变得非常重要。针对这一点，本文提出了一种基于图神经网络 (GNN) 的方法，用于在大规模不同类型图中检测贩卖财活动。具体来说，我们将已知的同类GNN方法——消息传递神经网络 (MPNN)——修改以适应不同类型图。在这个过程中，我们提出了一种新的消息汇聚方法，用于在不同的图边缘上进行消息汇聚。我们的发现表明，在组合不同类型图的情况下，使用适当的 GNN 建筑可以提高电子监测系统的检测贩卖财活动质量。我们知道，这是首次在实际世界上大规模不同类型图上应用 GNN 的反贩卖财研究。
</details></li>
</ul>
<hr>
<h2 id="Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction"><a href="#Zshot-An-Open-source-Framework-for-Zero-Shot-Named-Entity-Recognition-and-Relation-Extraction" class="headerlink" title="Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction"></a>Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13497">http://arxiv.org/abs/2307.13497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Picco, Marcos Martínez Galindo, Alberto Purpura, Leopold Fuchs, Vanessa López, Hoang Thanh Lam</li>
<li>For: The paper is written for researchers and industry professionals who are interested in zero-shot learning (ZSL) and its applications in natural language processing (NLP).* Methods: The paper proposes a novel ZSL framework called Zshot, which aims to address the challenges of ZSL by providing a platform for comparing different state-of-the-art ZSL methods with standard benchmark datasets. The framework also includes readily available APIs for production under the standard SpaCy NLP pipeline, and it is designed to be extendible and evaluable.* Results: The paper does not provide specific results, but it aims to provide a platform for comparing different ZSL methods and evaluating their performance on standard benchmark datasets. The authors also include numerous enhancements such as pipeline ensembling and visualization utilities available as a SpaCy extension.<details>
<summary>Abstract</summary>
The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.In this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.
</details>
<details>
<summary>摘要</summary>
zero-shot learning (ZSL) 任务是指在训练过程中未看过的文本中预测实体或关系。 ZSL 已成为一个重要的研究领域，因为特定领域的标注数据稀缺，而其应用也在过去几年内有所增长。随着大型预训语言模型的出现，许多新的方法被提出，导致 ZSL 性能得到了显著提高。在研究 сообществе和industry 中，有一个增长的需求，即开发一个通用的 ZSL 框架，以便开发和访问最新的方法和预训模型。在这项研究中，我们提出了一个新的 ZSL 框架，称为 Zshot。我们的主要目标是提供一个平台，允许研究人员比较不同的状态艺术 ZSL 方法，并使用标准的 benchmark 数据集进行比较。此外，我们设计了我们的框架，以支持产业，并提供了可靠的 SpaCy NLP 管道中的 API。我们的 API 可扩展和评估，并且包括了多种改进，例如将管道 ensemble 提高准确性，以及可用于 SpaCy 扩展的可视化工具。
</details></li>
</ul>
<hr>
<h2 id="Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding"><a href="#Duet-efficient-and-scalable-hybriD-neUral-rElation-undersTanding" class="headerlink" title="Duet: efficient and scalable hybriD neUral rElation undersTanding"></a>Duet: efficient and scalable hybriD neUral rElation undersTanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13494">http://arxiv.org/abs/2307.13494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GIS-PuppetMaster/Duet">https://github.com/GIS-PuppetMaster/Duet</a></li>
<li>paper_authors: Kaixin Zhang, Hongzhi Wang, Yabin Lu, Ziqi Li, Chang Shu, Yu Yan, Donghua Yang</li>
<li>for: 本研究旨在解决learned cardinality estimation方法中的数据和工作负荷飘移问题，以及高纬度和高维度表中Cardinality estimator的应用问题。</li>
<li>methods: 本文提出了一种基于预测模型的hybrid方法，named Duet，它可以直接估计 cardinality  без采样或任何非�ifferentiable过程，并且可以提高高纬度和高维度表中Cardinality estimator的准确性。</li>
<li>results: 实验结果表明，Duet可以实现所有设计目标，并且在CPU上比较多学方法更加实用，甚至在GPU上也具有较低的推理成本。<details>
<summary>Abstract</summary>
Learned cardinality estimation methods have achieved high precision compared to traditional methods. Among learned methods, query-driven approaches face the data and workload drift problem for a long time. Although both query-driven and hybrid methods are proposed to avoid this problem, even the state-of-the-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high-dimensional tables, which seriously affects the practical application of learned cardinality estimators. In this paper, we prove that most of these problems are directly caused by the widely used progressive sampling. We solve this problem by introducing predicates information into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduces the inference complexity from O(n) to O(1) compared to Naru and UAE but also achieve higher accuracy on high cardinality and high-dimensional tables. Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.
</details>
<details>
<summary>摘要</summary>
现有学习Cardinality estimation方法已经实现了高精度比 tradicional方法。 Among these learned methods, query-driven approaches have faced the data and workload drift problem for a long time. Although both query-driven and hybrid methods have been proposed to avoid this problem, even the state-of-the-art of them suffer from high training and estimation costs, limited scalability, instability, and long-tailed distribution problem on high cardinality and high-dimensional tables, which seriously affects the practical application of learned cardinality estimators.在这篇论文中，我们证明了大多数这些问题是由广泛使用进度 sampling 所导致的。 We solve this problem by introducing predicates information into the autoregressive model and propose Duet, a stable, efficient, and scalable hybrid method to estimate cardinality directly without sampling or any non-differentiable process, which can not only reduce the inference complexity from O(n) to O(1) compared to Naru and UAE but also achieve higher accuracy on high cardinality and high-dimensional tables. Experimental results show that Duet can achieve all the design goals above and be much more practical and even has a lower inference cost on CPU than that of most learned methods on GPU.
</details></li>
</ul>
<hr>
<h2 id="ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field"><a href="#ECG-classification-using-Deep-CNN-and-Gramian-Angular-Field" class="headerlink" title="ECG classification using Deep CNN and Gramian Angular Field"></a>ECG classification using Deep CNN and Gramian Angular Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02395">http://arxiv.org/abs/2308.02395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Elmir, Yassine Himeur, Abbes Amira</li>
<li>for: 这个研究提供了一种新的ECG信号分析方法，用于心血管疾病诊断和异常检测。</li>
<li>methods: 该方法基于将时域1D вектор转换为2D图像使用 Gramian Angular Field transform，并使用卷积神经网络（CNN）进行分类。</li>
<li>results: 实验结果显示，提出的方法可以达到97.47%和98.65%的分类精度，并且可以辨别和可视化ECG信号中的时间特征，如心率、心音和信号形态变化，这些变化可能不可见于原始信号中。<details>
<summary>Abstract</summary>
This paper study provides a novel contribution to the field of signal processing and DL for ECG signal analysis by introducing a new feature representation method for ECG signals. The proposed method is based on transforming time frequency 1D vectors into 2D images using Gramian Angular Field transform. Moving on, the classification of the transformed ECG signals is performed using Convolutional Neural Networks (CNN). The obtained results show a classification accuracy of 97.47% and 98.65% for anomaly detection. Accordingly, in addition to improving the classification performance compared to the state-of-the-art, the feature representation helps identify and visualize temporal patterns in the ECG signal, such as changes in heart rate, rhythm, and morphology, which may not be apparent in the original signal. This has significant implications in the diagnosis and treatment of cardiovascular diseases and detection of anomalies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions"><a href="#Rational-kernel-based-interpolation-for-complex-valued-frequency-response-functions" class="headerlink" title="Rational kernel-based interpolation for complex-valued frequency response functions"></a>Rational kernel-based interpolation for complex-valued frequency response functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13484">http://arxiv.org/abs/2307.13484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stk-kriging/complex-rational-interpolation">https://github.com/stk-kriging/complex-rational-interpolation</a></li>
<li>paper_authors: Julien Bect, Niklas Georg, Ulrich Römer, Sebastian Schöps</li>
<li>for: 这个论文关注了使用kernel方法估计复杂数值函数的问题，特别是在频域中的频谱响应函数。</li>
<li>methods: 这篇论文使用了kernel方法，但标准kernel并不perform well。作者引入了新的复杂数值函数抽象空间，并将问题转化为最小二乘问题在这些空间中。此外，作者还结合了一个低阶racional函数，其阶数由一个新的模型选择 criterion 来动态选择。</li>
<li>results: 作者的方法在不同领域的实例中进行了数值实验，包括电磁学和声学实例。比较 Result 与可用的racionalapproximation方法，这种方法的性能很高。<details>
<summary>Abstract</summary>
This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the method, also in comparison to available rational approximation methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets"><a href="#Combinatorial-Auctions-and-Graph-Neural-Networks-for-Local-Energy-Flexibility-Markets" class="headerlink" title="Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets"></a>Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13470">http://arxiv.org/abs/2307.13470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Awadelrahman M. A. Ahmed, Frank Eliassen, Yan Zhang</li>
<li>for: 本研究提出了一个新的 combinatorial 拍卖框架，用于地方能源灵活性市场，以解决潜在参与者无法组合多个灵活时间间隔的问题。</li>
<li>methods: 本研究使用了简单 yet powerful 三元图表示法和图生物学网络模型来解决背景NP-完备的胜出决定问题。</li>
<li>results: 模型实现了与商业解决方案相对的优化值差不 біль于5%，并且显示了线性推论时间复杂性，与商业解决方案的指数复杂性相比。<details>
<summary>Abstract</summary>
This paper proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals. To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models. Our models achieve an average optimal value deviation of less than 5\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver. Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese as follows:这篇论文提出了一种新的 combinatorial 拍卖框架，用于本地能源灵活性市场，解决了潜在用户无法组合多个灵活时间间隔的问题。为解决这个下面NP完备的赢家决定问题，我们提出了一种简单强大的三元 Graph 表示和图 neural network 模型。我们的模型在与 commercial 优化工具的比较中，实现了 Less than 5% 的最佳值偏差，并且显示了与商业解决方案的线性推理时间复杂度。研讨和结果表明，使用机器学习可以有效地分配本地能源灵活性资源，并在总体上解决优化问题。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation"><a href="#Gaussian-Graph-with-Prototypical-Contrastive-Learning-in-E-Commerce-Bundle-Recommendation" class="headerlink" title="Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation"></a>Gaussian Graph with Prototypical Contrastive Learning in E-Commerce Bundle Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13468">http://arxiv.org/abs/2307.13468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao-Yang Liu, Liucheng Sun, Chenwei Weng, Qijin Chen, Chengfu Huo</li>
<li>for: 提高电商平台上的bundle推荐的精度和效果，解决实际推荐场景中的uncertainty问题。</li>
<li>methods: 提出了一种新的 Gaussian Graph with Prototypical Contrastive Learning (GPCL)框架，使用Gaussian分布代替固定向量，并设计了一种prototypical contrastive learning模块来捕捉 Contextual信息，缓解采样偏见问题。</li>
<li>results: 经验表明，GPCL在多个公共数据集上达到了新的州OF-the-art性能水平，并在真实的电商平台上实现了显著的提升。<details>
<summary>Abstract</summary>
Bundle recommendation aims to provide a bundle of items to satisfy the user preference on e-commerce platform. Existing successful solutions are based on the contrastive graph learning paradigm where graph neural networks (GNNs) are employed to learn representations from user-level and bundle-level graph views with a contrastive learning module to enhance the cooperative association between different views. Nevertheless, they ignore the uncertainty issue which has a significant impact in real bundle recommendation scenarios due to the lack of discriminative information caused by highly sparsity or diversity. We further suggest that their instancewise contrastive learning fails to distinguish the semantically similar negatives (i.e., sampling bias issue), resulting in performance degradation. In this paper, we propose a novel Gaussian Graph with Prototypical Contrastive Learning (GPCL) framework to overcome these challenges. In particular, GPCL embeds each user/bundle/item as a Gaussian distribution rather than a fixed vector. We further design a prototypical contrastive learning module to capture the contextual information and mitigate the sampling bias issue. Extensive experiments demonstrate that benefiting from the proposed components, we achieve new state-of-the-art performance compared to previous methods on several public datasets. Moreover, GPCL has been deployed on real-world e-commerce platform and achieved substantial improvements.
</details>
<details>
<summary>摘要</summary>
<izin>电商平台上的Bundle推荐 aimsto提供一个Bundle的item来满足用户的首选。现有的成功解决方案基于对冲raph学习 paradigm，使用граф neural networks (GNNs)来学习用户和Bundle的表示，并通过对冲学习模块来增强不同视图之间的合作关系。然而，它们忽略了uncertainty问题，这有着很大的影响在实际的Bundle推荐场景中，因为缺乏特征信息引起的高稀疏或多样性。我们还指出，它们的Instancewise对冲学习无法分辨semantic Similarity的负样本（即采样偏见问题），导致性能下降。在这篇论文中，我们提出了一种novel Gaussian Graph with Prototypical Contrastive Learning (GPCL)框架，以解决这些挑战。具体来说，GPCL将每个用户/Bundle/itemembed为 Gaussian Distribution而不是固定的 вектор。我们还设计了一个prototypical对冲学习模块，以捕捉上下文信息并缓解采样偏见问题。我们的实验证明，由于我们提出的组件，我们在多个公共数据集上达到了新的状态态performanced比之前的方法。此外，GPCL已经部署在实际的电商平台上，并实现了显著的改善。</izin>Note: "izin" is a marker in Simplified Chinese to indicate that the following text is a translation of a foreign language text.
</details></li>
</ul>
<hr>
<h2 id="Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction"><a href="#Integrating-processed-based-models-and-machine-learning-for-crop-yield-prediction" class="headerlink" title="Integrating processed-based models and machine learning for crop yield prediction"></a>Integrating processed-based models and machine learning for crop yield prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13466">http://arxiv.org/abs/2307.13466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michiel G. J. Kallenberg, Bernardo Maestrini, Ron van Bree, Paul Ravensbergen, Christos Pylianidis, Frits van Evert, Ioannis N. Athanasiadis</li>
<li>for: 预测哈比甜菜的产量</li>
<li>methods: 使用гибрид元模型方法，结合理论驱动的植物生长模型和数据驱动的神经网络</li>
<li>results: 在silico和实际场景中，元模型方法比基eline方法更好地预测哈比甜菜的产量，但需要进一步的 validate和优化以确定实际效果。<details>
<summary>Abstract</summary>
Crop yield prediction typically involves the utilization of either theory-driven process-based crop growth models, which have proven to be difficult to calibrate for local conditions, or data-driven machine learning methods, which are known to require large datasets. In this work we investigate potato yield prediction using a hybrid meta-modeling approach. A crop growth model is employed to generate synthetic data for (pre)training a convolutional neural net, which is then fine-tuned with observational data. When applied in silico, our meta-modeling approach yields better predictions than a baseline comprising a purely data-driven approach. When tested on real-world data from field trials (n=303) and commercial fields (n=77), the meta-modeling approach yields competitive results with respect to the crop growth model. In the latter set, however, both models perform worse than a simple linear regression with a hand-picked feature set and dedicated preprocessing designed by domain experts. Our findings indicate the potential of meta-modeling for accurate crop yield prediction; however, further advancements and validation using extensive real-world datasets is recommended to solidify its practical effectiveness.
</details>
<details>
<summary>摘要</summary>
通常，耐作预测通过使用理论驱动的过程基于植物生长模型或数据驱动的机器学习方法进行实现。这两种方法都有其缺点，其中一种是难以适应当地区条件，另一种是需要大量数据。在这种情况下，我们研究了使用半结构化模型的hybrid meta-modeling方法来预测芋头收获。我们使用植物生长模型生成了合成数据，然后使用卷积神经网络进行预测，并且通过观察数据进行精度调整。在silico中应用的meta-modeling方法比基准情况下的纯数据驱动方法更好。在实际场景中，我们对303个试验场和77个商业场的数据进行测试，并发现meta-modeling方法和植物生长模型在这些场景中具有竞争力。然而，在这些场景中，一个简单的直线回归模型和专业人员设计的特定预处理和特征集合得到了更好的表现。我们的发现表明meta-modeling方法在准确预测耐作收获方面存在潜力，但是进一步的发展和验证使用广泛的实际数据是必要的，以固化其实际效果。
</details></li>
</ul>
<hr>
<h2 id="Fundamental-causal-bounds-of-quantum-random-access-memories"><a href="#Fundamental-causal-bounds-of-quantum-random-access-memories" class="headerlink" title="Fundamental causal bounds of quantum random access memories"></a>Fundamental causal bounds of quantum random access memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13460">http://arxiv.org/abs/2307.13460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfei Wang, Yuri Alexeev, Liang Jiang, Frederic T. Chong, Junyu Liu</li>
<li>for: This paper explores the fundamental limits of rapid quantum memories in quantum computing applications, particularly in the context of hybrid quantum acoustic systems.</li>
<li>methods: The paper employs relativistic quantum field theory and Lieb-Robinson bounds to critically examine the causality constraints of quantum memories and their impact on quantum computing performance.</li>
<li>results: The paper shows that the number of logical qubits that can be accommodated in a QRAM design can be scaled up to $\mathcal{O}(10^7)$ in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions, subject to the causality bound. These findings have important implications for the long-term performance of quantum computing applications in data science.<details>
<summary>Abstract</summary>
Quantum devices should operate in adherence to quantum physics principles. Quantum random access memory (QRAM), a fundamental component of many essential quantum algorithms for tasks such as linear algebra, data search, and machine learning, is often proposed to offer $\mathcal{O}(\log N)$ circuit depth for $\mathcal{O}(N)$ data size, given $N$ qubits. However, this claim appears to breach the principle of relativity when dealing with a large number of qubits in quantum materials interacting locally. In our study we critically explore the intrinsic bounds of rapid quantum memories based on causality, employing the relativistic quantum field theory and Lieb-Robinson bounds in quantum many-body systems. In this paper, we consider a hardware-efficient QRAM design in hybrid quantum acoustic systems. Assuming clock cycle times of approximately $10^{-3}$ seconds and a lattice spacing of about 1 micrometer, we show that QRAM can accommodate up to $\mathcal{O}(10^7)$ logical qubits in 1 dimension, $\mathcal{O}(10^{15})$ to $\mathcal{O}(10^{20})$ in various 2D architectures, and $\mathcal{O}(10^{24})$ in 3 dimensions. We contend that this causality bound broadly applies to other quantum hardware systems. Our findings highlight the impact of fundamental quantum physics constraints on the long-term performance of quantum computing applications in data science and suggest potential quantum memory designs for performance enhancement.
</details>
<details>
<summary>摘要</summary>
量子设备应遵循量子物理原理运行。量子随机访问存储器（QRAM），许多关键量子算法中的基本组件，通常被提议可以提供 $\mathcal{O}(\log N)$ 圈深度，对于 $\mathcal{O}(N)$ 数据大小， givent $N$  qubits。然而，这个宣称似乎违反了 relativity 原理，当处理大量的 qubits 在量子材料中互动时。在我们的研究中，我们 kritisch 探讨了快速量子存储器的内在 bound，基于 causality，使用量子场论和 Lieb-Robinson bound 在量子多体系统中。在这篇文章中，我们考虑了硬件高效的 QRAM 设计，在半导体量子声学系统中。假设clock cycle 时间约为 $10^{-3}$ 秒，格子间距约为 1 微米，我们显示了 QRAM 可以容纳 $\mathcal{O}(10^7)$ 逻辑 qubits 在1维度，$\mathcal{O}(10^{15})$ 到 $\mathcal{O}(10^{20})$ 在不同的 2D 架构中，以及 $\mathcal{O}(10^{24})$ 在 3 维度。我们认为这种 causality bound 广泛适用于其他量子硬件系统。我们的发现指出了量子计算应用中长期表现的基本量子物理约束，并提出了可能的量子存储器设计来提高性能。
</details></li>
</ul>
<hr>
<h2 id="A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human"><a href="#A-behavioural-transformer-for-effective-collaboration-between-a-robot-and-a-non-stationary-human" class="headerlink" title="A behavioural transformer for effective collaboration between a robot and a non-stationary human"></a>A behavioural transformer for effective collaboration between a robot and a non-stationary human</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13447">http://arxiv.org/abs/2307.13447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruaridh Mon-Williams, Theodoros Stouraitis, Sethu Vijayakumar</li>
<li>for: 本研究旨在解决人机合作中人类行为变化带来的非站立性问题，提高机器人的预测能力以适应新的人类代理人。</li>
<li>methods: 本研究提出了一种原则式的meta学框架，并基于这个框架开发了Behavior-Transform（BeTrans）。BeTrans是一种可适应新人类代理人的 conditional transformer，可以快速适应新的人类行为变化。</li>
<li>results: 通过在 simulated human agents 上进行训练，我们发现BeTrans在合作设置下与不同系统偏见的人类代理人协作得非常好，并且比SOTA技术更快地适应新的人类行为变化。<details>
<summary>Abstract</summary>
A key challenge in human-robot collaboration is the non-stationarity created by humans due to changes in their behaviour. This alters environmental transitions and hinders human-robot collaboration. We propose a principled meta-learning framework to explore how robots could better predict human behaviour, and thereby deal with issues of non-stationarity. On the basis of this framework, we developed Behaviour-Transform (BeTrans). BeTrans is a conditional transformer that enables a robot agent to adapt quickly to new human agents with non-stationary behaviours, due to its notable performance with sequential data. We trained BeTrans on simulated human agents with different systematic biases in collaborative settings. We used an original customisable environment to show that BeTrans effectively collaborates with simulated human agents and adapts faster to non-stationary simulated human agents than SOTA techniques.
</details>
<details>
<summary>摘要</summary>
人机合作中的一大挑战是由人类行为引起的非站点性，这会导致环境变化和人机合作困难。我们提出了一种原则正的meta学框架，以便机器人更好地预测人类行为，从而更好地处理非站点性问题。基于这个框架，我们开发了Behaviour-Transform（BeTrans）。BeTrans是一种 Conditional Transformer，它允许机器人代理人类快速适应新的人类行为，并且在序列数据上表现出色。我们在 simulate human agents with different systematic biases in collaborative settings 中训练了 BeTrans，并用自定义环境示出了它在与 simulate human agents 合作中的有效性，并且更快地适应非站点性 simulate human agents than SOTA技术。
</details></li>
</ul>
<hr>
<h2 id="Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis"><a href="#Network-Traffic-Classification-based-on-Single-Flow-Time-Series-Analysis" class="headerlink" title="Network Traffic Classification based on Single Flow Time Series Analysis"></a>Network Traffic Classification based on Single Flow Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13434">http://arxiv.org/abs/2307.13434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koumajos/classificationbasedonsfts">https://github.com/koumajos/classificationbasedonsfts</a></li>
<li>paper_authors: Josef Koumar, Karel Hynek, Tomáš Čejka</li>
<li>for: 用于分析加密网络通信的现场挑战</li>
<li>methods: 基于时间序列分析单流时间序列（包括每个 packet 的字节数和时间戳），提出69种统一特征</li>
<li>results: 在15种公共可用的数据集上进行了多种网络流量分类任务的评估，表明提议的特征向量可以达到与相关工作相当或更好的分类性能，在超过一半的评估任务中，分类性能提高了5%以上。<details>
<summary>Abstract</summary>
Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication. Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5\%.
</details>
<details>
<summary>摘要</summary>
网络流量监测使用流量记录来处理现在的挑战，即分析加密网络通信。然而，流量聚合到记录中自然导致信息损失，因此这篇论文提议一种新的流量扩展 для交通特征基于单流时间序列分析，即每个包的字节数和时间戳创建的时间序列。我们提出69个统一特征，包括数据点统计分析、时间域分析、流时间范围内包分布、时间序列行为和频域分析。我们通过使用15个公共可用的数据集来证明提议的特征向量的可用性和通用性，并在 binary 和多类分类任务中达到类似或更好的性能。在评估中，在超过半个评估任务中，分类性能提高5%以上。
</details></li>
</ul>
<hr>
<h2 id="Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization"><a href="#Achieving-Linear-Speedup-in-Decentralized-Stochastic-Compositional-Minimax-Optimization" class="headerlink" title="Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization"></a>Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13430">http://arxiv.org/abs/2307.13430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongchang Gao</li>
<li>for: 本研究的目的是提出一种基于分布式数据的各自练习作业的分布式compositional minimax问题的解决方案，以便在分布式设置下优化这类问题。</li>
<li>methods: 我们提出了一种基于均匀采样和动量的分布式Stochastic Compositional Gradient Descent Ascent算法，用于降低内层函数的共识错误。</li>
<li>results: 我们的 teoría results 表明，该算法可以实现线性增速，即与工作者数量 linearly 相关。  Additionally, we applied our method to the imbalanced classification problem and obtained extensive experimental results, which demonstrate the effectiveness of our algorithm.<details>
<summary>Abstract</summary>
The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algorithmic design could benefit the development of decentralized compositional optimization. Finally, we applied our methods to the imbalanced classification problem. The extensive experimental results provide evidence for the effectiveness of our algorithm.
</details>
<details>
<summary>摘要</summary>
“ Stochastic compositional minimax problem 在 recent years 已经吸引了许多关注，因为它涵盖了许多emerging machine learning models。然而，由于分布式数据的出现，对这种问题的分布式优化成为了非常需要。然而，compositional structure 在损失函数中带来了独特的挑战，对于设计高效的分布式优化算法。具体来说，我们的研究显示，标准的gossip Communication Strategy 不能实现linear speedup  для分布式compositional minimax problem，因为内部函数的大调和错误。为了解决这个问题，我们开发了一个新的分布式随机compositional gradient descent ascent with momentum algorithm，以减少内部函数的调和错误。因此，我们的理论结果显示，它能够实现linear speedup 与 respect to the number of workers。我们认为这个新的算法设计可以帮助分布式compositional optimization的发展。 finally，我们将我们的方法应用到不对称类别问题。广泛的实验结果为我们的算法的有效性提供了证据。”
</details></li>
</ul>
<hr>
<h2 id="A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks"><a href="#A-signal-processing-interpretation-of-noise-reduction-convolutional-neural-networks" class="headerlink" title="A signal processing interpretation of noise-reduction convolutional neural networks"></a>A signal processing interpretation of noise-reduction convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13425">http://arxiv.org/abs/2307.13425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis A. Zavala-Mondragón, Peter H. N. de With, Fons van der Sommen</li>
<li>for: 本文旨在为数据驱动降噪和深度学习算法中的Encoding-decoding CNNs提供理论基础，以便更好地理解这些架构的内部工作机制。</li>
<li>methods: 本文使用了深度卷积架构，并提出了一种基于深度学习和信号处理的理论框架，用于解释Encoding-decoding CNNs的内部工作机制。</li>
<li>results: 本文通过 connecting basic principles from signal processing to the field of deep learning, 提供了一种可以用于设计robust和高效的新型Encoding-decoding CNNs架构的有效指导。<details>
<summary>Abstract</summary>
Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.
</details>
<details>
<summary>摘要</summary>
Encoding-decoding CNNs 在数据驱动的噪声缓解中扮演中心角色，可以在多种深度学习算法中找到。然而，这些 CNN 架构的开发通常是靠悄悄话的，lacking 理论基础。到目前为止，有很多相关的工作努力来解释这些 CNN 的内部运作。然而，这些想法是分散的，或者需要一定的专业知识才能访问。为了开放这个激动人心的领域，这篇文章建立了深度卷积框架的理论基础，并将多种 ED CNN 架构集成到一个统一的理论框架中。通过将信号处理的基本原理与深度学习相连接，这篇自包含的材料提供了设计robust和高效的新的 CNN 架构的重要指南。
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations"><a href="#Non-Intrusive-Intelligibility-Predictor-for-Hearing-Impaired-Individuals-using-Self-Supervised-Speech-Representations" class="headerlink" title="Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations"></a>Non Intrusive Intelligibility Predictor for Hearing Impaired Individuals using Self Supervised Speech Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13423">http://arxiv.org/abs/2307.13423</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Close, Thomas Hain, Stefan Goetze</li>
<li>for: 这个论文旨在扩展自我监督抽象语音表示法（SSSR），以便非侵入式方式预测听力障碍用户的语音质量评分。</li>
<li>methods: 本文使用了SSSR作为输入特征，通过非侵入式预测模型来预测听力障碍用户的语音理解度。</li>
<li>results: 研究发现，SSSR可以作为输入特征，实现与更复杂的系统相当的竞争性性能。分析表明，更多的数据可能需要用于将预测模型 generalized to unknown systems and individuals。<details>
<summary>Abstract</summary>
Self-supervised speech representations (SSSRs) have been successfully applied to a number of speech-processing tasks, e.g. as feature extractor for speech quality (SQ) prediction, which is, in turn, relevant for assessment and training speech enhancement systems for users with normal or impaired hearing. However, exact knowledge of why and how quality-related information is encoded well in such representations remains poorly understood. In this work, techniques for non-intrusive prediction of SQ ratings are extended to the prediction of intelligibility for hearing-impaired users. It is found that self-supervised representations are useful as input features to non-intrusive prediction models, achieving competitive performance to more complex systems. A detailed analysis of the performance depending on Clarity Prediction Challenge 1 listeners and enhancement systems indicates that more data might be needed to allow generalisation to unknown systems and (hearing-impaired) individuals
</details>
<details>
<summary>摘要</summary>
自我监督的语音表示 (SSSR) 已成功应用于多个语音处理任务中，例如作为语音质量预测的特征提取器，这同时对于评估和培训语音增强系统的用户进行评估和培训具有重要性。然而，关于为什么和如何在这些表示中编码质量相关信息的准确知识仍然不够了解。在这种情况下，非侵入式预测模型的扩展被应用于预测听力障碍用户的语音明白度。结果表明，自我监督表示可以作为输入特征进行非侵入式预测模型，实现与更复杂的系统相当的性能。一个细化的性能分析，具体分析了根据Clearity Prediction Challenge 1的听众和增强系统，表明更多的数据可能需要以Allow generalization to unknown systems and (hearing-impaired) individuals。
</details></li>
</ul>
<hr>
<h2 id="On-the-Learning-Dynamics-of-Attention-Networks"><a href="#On-the-Learning-Dynamics-of-Attention-Networks" class="headerlink" title="On the Learning Dynamics of Attention Networks"></a>On the Learning Dynamics of Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13421">http://arxiv.org/abs/2307.13421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks">https://github.com/vashisht-rahul/on-the-learning-dynamics-of-attention-networks</a></li>
<li>paper_authors: Rahul Vashisht, Harish G. Ramaswamy</li>
<li>for: 本文探讨了三种常见的注意力模型优化方法，即软注意力、硬注意力和隐变量 marginal likelihood（LVML）注意力。这三种方法都是为了找到一个 <code>ocus&#39; 模型，可以选择输入中的正确段落，以及一个 </code>classification’ 模型，可以处理选择的段落并生成目标标签。但是它们在选取段落的方式不同，导致了不同的动态和最终结果。</li>
<li>methods: 本文使用了不同的注意力优化方法，包括软注意力损失、硬注意力损失和隐变量 marginal likelihood（LVML）注意力损失。这些方法的选择对于模型的性能有很大的影响。</li>
<li>results: 本文通过对一系列半 sintetic和实际世界数据集进行实验，发现了不同的注意力优化方法在模型性能上的不同表现。软注意力损失在初始化时能够快速改进 focus 模型，但是后续会降低。与此相反，硬注意力损失在初始化时会降低 focus 模型，但是后续会快速改进。基于这些观察，文章提出了一种简单的混合方法，可以结合不同的注意力优化方法的优点，并在实验中得到了良好的性能。<details>
<summary>Abstract</summary>
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization and splutters later on. On the other hand, hard attention loss behaves in the opposite fashion. Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions and demonstrates it on a collection of semi-synthetic and real-world datasets
</details>
<details>
<summary>摘要</summary>
听力模型通常通过优化三种标准损失函数来学习：软注意力、硬注意力和隐变量极值 probabilistic（LVML）注意力。这三种方法均由同一目标而导向：找到一个`焦点'模型，可以选择输入中正确的`段'，以及一个`分类'模型，可以处理选择的段来生成目标标签。然而，它们在选择段的方式不同，从而导致了不同的动态和最终结果。我们观察到这些模型学习使用这些方法的独特签名，并解释这为梯度下降在 fixes 焦点模型下的演化。我们还分析了这些方法在简单的设置下，并 deriv 出closed-form 表达式用于参数轨迹的梯度流。与软注意力损失函数相比，硬注意力损失函数在初始化时快速改进，然后后来停滞不前进。相反，硬注意力损失函数在另一方面表现出opposite的特点。基于我们的观察，我们提出了一种简单的混合方法，将不同损失函数的优点结合起来，并在一些半Synthetic 和实际数据集上进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems"><a href="#Co-Design-of-Out-of-Distribution-Detectors-for-Autonomous-Emergency-Braking-Systems" class="headerlink" title="Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems"></a>Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13419">http://arxiv.org/abs/2307.13419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Yuhas, Arvind Easwaran</li>
<li>For: The paper aims to improve the safety of autonomous vehicles (AVs) by co-designing an out-of-distribution (OOD) detector and a learning-enabled component (LEC) to detect and mitigate potential failures in the LEC.* Methods: The paper uses a risk model to analyze the impact of design parameters on both the OOD detector and the LEC, and co-designs the two components to minimize the risk of failure.* Results: The paper demonstrates a 42.3% risk reduction in the system while maintaining equivalent resource utilization, indicating the effectiveness of the co-design methodology in improving the safety of AVs.<details>
<summary>Abstract</summary>
Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions. Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs. For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety. For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC. We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We formulate a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that decrease risk below that of the baseline system and demonstrate it on a vision based AEBS. Using our methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization.
</details>
<details>
<summary>摘要</summary>
We use risk, which is the combination of the severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We develop a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that minimize risk. We demonstrate the effectiveness of our methodology on a vision-based autonomous emergency braking system (AEBS).By using our co-design methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization. This demonstrates the potential of our approach to improve the safety of AVs by optimizing the design parameters of both OOD detectors and LECs.
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning"><a href="#Communication-Efficient-Orchestrations-for-URLLC-Service-via-Hierarchical-Reinforcement-Learning" class="headerlink" title="Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning"></a>Communication-Efficient Orchestrations for URLLC Service via Hierarchical Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13415">http://arxiv.org/abs/2307.13415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Shi, Milad Ganjalizadeh, Hossein Shokri Ghadikolaei, Marina Petrova</li>
<li>for: 此研究旨在提高5G中的可靠低延迟通信服务（URLLC）的可靠性和响应速度。</li>
<li>methods: 本研究使用多代理 Hierarchical Reinforcement Learning（HRL）框架，实现多级政策的实现，并且通过不同控制循环时间的调整，提高控制循环的响应速度和灵活性。</li>
<li>results: 在一个先前的实验中，使用HRL框架优化工业设备的最大重传数和传输功率，并获得了较好的性能，比基eline单代理RL方法更好，同时具有较少的信号传输 overhead和延迟。<details>
<summary>Abstract</summary>
Ultra-reliable low latency communications (URLLC) service is envisioned to enable use cases with strict reliability and latency requirements in 5G. One approach for enabling URLLC services is to leverage Reinforcement Learning (RL) to efficiently allocate wireless resources. However, with conventional RL methods, the decision variables (though being deployed at various network layers) are typically optimized in the same control loop, leading to significant practical limitations on the control loop's delay as well as excessive signaling and energy consumption. In this paper, we propose a multi-agent Hierarchical RL (HRL) framework that enables the implementation of multi-level policies with different control loop timescales. Agents with faster control loops are deployed closer to the base station, while the ones with slower control loops are at the edge or closer to the core network providing high-level guidelines for low-level actions. On a use case from the prior art, with our HRL framework, we optimized the maximum number of retransmissions and transmission power of industrial devices. Our extensive simulation results on the factory automation scenario show that the HRL framework achieves better performance as the baseline single-agent RL method, with significantly less overhead of signal transmissions and delay compared to the one-agent RL methods.
</details>
<details>
<summary>摘要</summary>
超可靠低延迟通信服务（URLLC）在5G中被视为实现严格可靠性和延迟要求的应用场景。一种实现URLLC服务的方法是通过强化学习（RL）有效地分配无线资源。然而，传统RL方法中的决策变量（即在不同网络层部署）通常在同一控制循环中优化，这会导致控制循环延迟的限制以及过分的信号传输和能耗。在这篇论文中，我们提出了一种多代理层RL（HRL）框架，该框架允许实现多级策略，并且在不同层次上有不同的控制循环时间尺度。靠近基站的代理在更快的控制循环中部署，而Edge或更接近核心网络的代理则提供高级指导 для低级动作。在一个优化 industrial device 的Factory automation scenario中，我们使用HRL框架进行优化，并取得了较好的性能，与基准单代理RL方法相比，减少了信号传输的 overhead和延迟。Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and widely used in informal writing. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation"><a href="#Mitigating-Memory-Wall-Effects-in-CNN-Engines-with-On-the-Fly-Weights-Generation" class="headerlink" title="Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation"></a>Mitigating Memory Wall Effects in CNN Engines with On-the-Fly Weights Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13412">http://arxiv.org/abs/2307.13412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane</li>
<li>for: 本研究旨在提高FPGA基于Convolutional Neural Networks（CNN）加速器的性能和能效性。</li>
<li>methods: 本文提出了一种新的CNN推理系统，称为unzipFPGA，它使用了一种新的硬件架构，包括一个 weights生成模块，可以在运行时生成权重，以解决受限制的带宽的问题。此外，文章还提出了一种自动硬件快照方法，可以根据目标CNN设备对硬件进行优化，从而提高精度和性能的平衡。</li>
<li>results: 根据结果表明，unzipFPGA可以实现2.57倍的性能效率提升相比高优化的GPU设计，并且可以达到3.94倍的性能密度，超过了一系列state-of-the-art FPGA基于CNN加速器。<details>
<summary>Abstract</summary>
The unprecedented accuracy of convolutional neural networks (CNNs) across a broad range of AI tasks has led to their widespread deployment in mobile and embedded settings. In a pursuit for high-performance and energy-efficient inference, significant research effort has been invested in the design of FPGA-based CNN accelerators. In this context, single computation engines constitute a popular approach to support diverse CNN modes without the overhead of fabric reconfiguration. Nevertheless, this flexibility often comes with significantly degraded performance on memory-bound layers and resource underutilisation due to the suboptimal mapping of certain layers on the engine's fixed configuration. In this work, we investigate the implications in terms of CNN engine design for a class of models that introduce a pre-convolution stage to decompress the weights at run time. We refer to these approaches as on-the-fly. This paper presents unzipFPGA, a novel CNN inference system that counteracts the limitations of existing CNN engines. The proposed framework comprises a novel CNN hardware architecture that introduces a weights generator module that enables the on-chip on-the-fly generation of weights, alleviating the negative impact of limited bandwidth on memory-bound layers. We further enhance unzipFPGA with an automated hardware-aware methodology that tailors the weights generation mechanism to the target CNN-device pair, leading to an improved accuracy-performance balance. Finally, we introduce an input selective processing element (PE) design that balances the load between PEs in suboptimally mapped layers. The proposed framework yields hardware designs that achieve an average of 2.57x performance efficiency gain over highly optimised GPU designs for the same power constraints and up to 3.94x higher performance density over a diverse range of state-of-the-art FPGA-based CNN accelerators.
</details>
<details>
<summary>摘要</summary>
“ convolutional neural networks (CNNs) 在许多人工智能任务中表现无 precedent 的准确率，导致它们在移动和嵌入设备上广泛应用。为了实现高性能且能效的推理，大量的研究精力被投入到基于 FPGA 的 CNN 加速器的设计中。在这个上下文中，单 computation 引擎是一种广泛使用的方法，以支持多种 CNN 模式，而无需 fabric 重新配置的开销。然而，这种灵活性通常会导致在占用内存层的执行中表现下降和资源利用率下降，因为在固定配置的引擎上对某些层的映射是不优化的。在这种情况下，我们 investigate 了在 CNN 引擎设计方面的影响，特别是那些引入预处理阶段来压缩 веса的模型。我们称这些方法为“在 fly”。本文提出了 unzipFPGA，一种新的 CNN 推理系统，该系统通过引入 weights 生成器模块来解决限制现有 CNN 引擎的缺点。我们还提供了一种自适应硬件方法，该方法根据目标 CNN-设备对可以自动调整 weights 生成机制，从而提高精度-性能平衡。 finally，我们引入了一种输入选择处理元件（PE）的设计，以平衡在不优化的层中的负载。提出的框架可以在同等功耗和能源约束下实现2.57倍的性能效率提升，相比高优化的 GPU 设计，以及3.94倍的性能密度提升，超过了多种现有的 FPGA 基于 CNN 加速器。”
</details></li>
</ul>
<hr>
<h2 id="The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking"><a href="#The-Double-Edged-Sword-of-Big-Data-and-Information-Technology-for-the-Disadvantaged-A-Cautionary-Tale-from-Open-Banking" class="headerlink" title="The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking"></a>The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13408">http://arxiv.org/abs/2307.13408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savina Dine Kim, Galina Andreeva, Michael Rovatsos</li>
<li>for: 本研究探讨了开放银行技术的隐含不平等风险，通过使用 machine learning（ML）技术和 UK FinTech 银行数据集来示例。</li>
<li>methods: 本研究使用了三种 ML 分类器来预测 финан参与者的可能性，并通过集成特征分析groups exhibiting不同的大小和形式的 Financial Vulnerability（FV）。</li>
<li>results: 研究发现，工程化的金融行为特征可以预测排除个人信息的 omitted 个人特征，特别是敏感或保护特征，这解释了开放银行数据的隐藏危险。<details>
<summary>Abstract</summary>
This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its safe usage. Three ML classifiers are compared in predicting the likelihood of FV, and groups exhibiting different magnitudes and forms of FV are identified via clustering to highlight the effects of feature combination. Our results indicate that engineered features of financial behavior can be predictive of omitted personal information, particularly sensitive or protected characteristics, shedding light on the hidden dangers of Open Banking data. We discuss the implications and conclude fairness via unawareness is ineffective in this new technological environment.
</details>
<details>
<summary>摘要</summary>
Using a unique dataset from a UK FinTech lender, the article demonstrates the power of fine-grained transaction data while cautioning its safe usage. Three machine learning (ML) classifiers are compared in predicting the likelihood of FV, and groups exhibiting different magnitudes and forms of FV are identified through clustering. The results show that engineered features of financial behavior can be predictive of omitted personal information, particularly sensitive or protected characteristics, highlighting the hidden dangers of Open Banking data.The article concludes that fairness via unawareness is ineffective in this new technological environment and discusses the implications for ensuring fairness in the use of Open Banking data. The findings have important implications for the financial industry, policymakers, and consumers, highlighting the need for careful consideration of the potential risks and benefits of Open Banking data and the importance of ensuring fairness in its use.
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space"><a href="#Counterfactual-Explanation-via-Search-in-Gaussian-Mixture-Distributed-Latent-Space" class="headerlink" title="Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space"></a>Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13390">http://arxiv.org/abs/2307.13390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Zhao, Klaus Broelemann, Gjergji Kasneci</li>
<li>for: 本研究旨在提供一种新的方法来生成Counterfactual Explanations（CE），以帮助用户更好地理解AI系统的决策过程和改进其结果。</li>
<li>methods: 本研究使用了一种基于自适应卷积神经网络的方法，首先将矩阵空间转换成一个 mixture of Gaussian distributions 的形式，然后通过线性 interpolate 生成 CE。</li>
<li>results: 对于各种图像和表格数据集，我们的方法能够具有比例尺度和数据抽象的优势，并且能够高效地返回更加真实的结果，相比三种现有的方法。<details>
<summary>Abstract</summary>
Counterfactual Explanations (CEs) are an important tool in Algorithmic Recourse for addressing two questions: 1. What are the crucial factors that led to an automated prediction/decision? 2. How can these factors be changed to achieve a more favorable outcome from a user's perspective? Thus, guiding the user's interaction with AI systems by proposing easy-to-understand explanations and easy-to-attain feasible changes is essential for the trustworthy adoption and long-term acceptance of AI systems. In the literature, various methods have been proposed to generate CEs, and different quality measures have been suggested to evaluate these methods. However, the generation of CEs is usually computationally expensive, and the resulting suggestions are unrealistic and thus non-actionable. In this paper, we introduce a new method to generate CEs for a pre-trained binary classifier by first shaping the latent space of an autoencoder to be a mixture of Gaussian distributions. CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class. We show that our method maintains the characteristics of the input sample during the counterfactual search. In various experiments, we show that the proposed method is competitive based on different quality measures on image and tabular datasets -- efficiently returns results that are closer to the original data manifold compared to three state-of-the-art methods, which are essential for realistic high-dimensional machine learning applications.
</details>
<details>
<summary>摘要</summary>
“ counterfactual 解释 (CEs) 是 Algorithmic Recourse 中一种重要的工具，用于回答以下两个问题：1. 自动预测/决策中的关键因素为何？2. 如何变化这些因素以获得更有利的结果从用户的角度？因此，为AI系统的使用者提供易于理解的解释和可行的改变建议是Algorithmic Recourse 的重要 Component。在文献中，多种方法已经被提出供Counterfactual 解释，并且不同的质量指标已经被建议来评估这些方法。然而，Counterfactual 解释的生成通常是 computationally expensive 的，并且生成的建议通常是不现实的，因此无法使用。在本文中，我们创新了一种用于预训binary classifier的Counterfactual 解释方法，通过首先将 autoencoder 的latent space 变成一个 mixture of Gaussian distributions。Counterfactual 解释在latent space中generated by linear interpolation between the query sample and the centroid of the target class。我们显示了我们的方法可以维持输入样本的特性。在多个实验中，我们显示了我们的方法与三种state-of-the-art方法相比，能够实现更高的质量指标。”
</details></li>
</ul>
<hr>
<h2 id="BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects"><a href="#BotHawk-An-Approach-for-Bots-Detection-in-Open-Source-Software-Projects" class="headerlink" title="BotHawk: An Approach for Bots Detection in Open Source Software Projects"></a>BotHawk: An Approach for Bots Detection in Open Source Software Projects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13386">http://arxiv.org/abs/2307.13386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bifenglin/bothawk">https://github.com/bifenglin/bothawk</a></li>
<li>paper_authors: Fenglin Bi, Zhiwei Zhu, Wei Wang, Xiaoya Xia, Hassan Ali Khan, Peng Pu</li>
<li>for: 这个研究旨在调查开源软件项目中的机器人账户，并尝试准确地识别机器人账户。</li>
<li>methods: 该研究使用了一种严格的数据采集工作流程，以确保收集到的数据准确、可重复、可扩展和有效。研究人员还提出了一种名为BotHawk的机器人检测模型，可以高效地检测开源软件项目中的机器人账户。</li>
<li>results: 研究人员通过分析17个特征在5个维度中，确定了开源软件项目中机器人账户的四种类型。此外，研究人员发现，跟踪者数、仓库数和标签含义最有用于识别账户类型。BotHawk模型在检测开源软件项目中的机器人账户方面表现出色，其AUC为0.947，F1分数为0.89。<details>
<summary>Abstract</summary>
Social coding platforms have revolutionized collaboration in software development, leading to using software bots for streamlining operations. However, The presence of open-source software (OSS) bots gives rise to problems including impersonation, spamming, bias, and security risks. Identifying bot accounts and behavior is a challenging task in the OSS project. This research aims to investigate bots' behavior in open-source software projects and identify bot accounts with maximum possible accuracy. Our team gathered a dataset of 19,779 accounts that meet standardized criteria to enable future research on bots in open-source projects. We follow a rigorous workflow to ensure that the data we collect is accurate, generalizable, scalable, and up-to-date. We've identified four types of bot accounts in open-source software projects by analyzing their behavior across 17 features in 5 dimensions. Our team created BotHawk, a highly effective model for detecting bots in open-source software projects. It outperforms other models, achieving an AUC of 0.947 and an F1-score of 0.89. BotHawk can detect a wider variety of bots, including CI/CD and scanning bots. Furthermore, we find that the number of followers, number of repositories, and tags contain the most relevant features to identify the account type.
</details>
<details>
<summary>摘要</summary>
社交代码平台已经革命化软件开发合作，使用软件机器人来简化操作。然而，开源软件（OSS）机器人的存在导致了多种问题，包括伪造、诈骗、偏见和安全风险。标识机器人帐户和行为是开源项目中的挑战。本研究目的是调查开源软件项目中的机器人行为，并尽可能准确地识别机器人帐户。我们的团队收集了19,779个符合标准化riteria的帐户，以便未来对开源项目中的机器人进行研究。我们采用了严格的工作流程，以确保收集的数据准确、可重复、可扩展和时尚。我们通过分析17个特征在5个维度来Identify four types of bot accounts in open-source software projects。我们创建了BotHawk模型，可以高效地检测开源软件项目中的机器人。它比其他模型高效，AUC为0.947，F1分数为0.89。BotHawk可以检测更多的机器人，包括CI/CD和扫描机器人。此外，我们发现帐户类型的最有用特征是粉丝数、仓库数和标签。
</details></li>
</ul>
<hr>
<h2 id="Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning"><a href="#Scaff-PD-Communication-Efficient-Fair-and-Robust-Federated-Learning" class="headerlink" title="Scaff-PD: Communication Efficient Fair and Robust Federated Learning"></a>Scaff-PD: Communication Efficient Fair and Robust Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13381">http://arxiv.org/abs/2307.13381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaodong Yu, Sai Praneeth Karimireddy, Yi Ma, Michael I. Jordan</li>
<li>for: 提高 Federated Learning 中的公平性和鲁棒性，适用于资源受限和多样化环境。</li>
<li>methods: 使用 acceleration primal dual (APD) 算法，利用偏好 corrected local steps (as in Scaffold) 实现更高效的通信协调和更快的收敛速度。</li>
<li>results: 在多个 benchmark 数据集上测试，Scaff-PD 能够提高公平性和鲁棒性，同时保持竞争性的准确率。<details>
<summary>Abstract</summary>
We present Scaff-PD, a fast and communication-efficient algorithm for distributionally robust federated learning. Our approach improves fairness by optimizing a family of distributionally robust objectives tailored to heterogeneous clients. We leverage the special structure of these objectives, and design an accelerated primal dual (APD) algorithm which uses bias corrected local steps (as in Scaffold) to achieve significant gains in communication efficiency and convergence speed. We evaluate Scaff-PD on several benchmark datasets and demonstrate its effectiveness in improving fairness and robustness while maintaining competitive accuracy. Our results suggest that Scaff-PD is a promising approach for federated learning in resource-constrained and heterogeneous settings.
</details>
<details>
<summary>摘要</summary>
我团队现请Scaff-PD，一种快速并通信效率高的分布robust federated learning算法。我们的方法通过优化适应于异构客户端的分布robust目标函数来提高公平性。我们利用这些目标函数的特殊结构，并设计了一种加速的 principales dual（APD）算法，使用偏好修正的本地步骤（如Scaffold）来实现重要的通信效率和速度增加。我们在多个 Referenced datasets上评估Scaff-PD，并证明其在保持竞争性准确性的情况下提高公平性和鲁棒性。我们的结果表明Scaff-PD是一种有前途的approach federated learning中的资源受限和异构环境中。
</details></li>
</ul>
<hr>
<h2 id="Submodular-Reinforcement-Learning"><a href="#Submodular-Reinforcement-Learning" class="headerlink" title="Submodular Reinforcement Learning"></a>Submodular Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13372">http://arxiv.org/abs/2307.13372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manish-pra/non-additive-rl">https://github.com/manish-pra/non-additive-rl</a></li>
<li>paper_authors: Manish Prajapat, Mojmír Mutný, Melanie N. Zeilinger, Andreas Krause</li>
<li>for: 这个论文是为了解决强迫学习（RL）中的奖励问题，奖励通常是加法的，但在许多重要应用中，奖励具有减少返回的特点，例如覆盖控制、实验设计和信息 PATH 规划。</li>
<li>methods: 作者提出了一种新的概念——submodular RL（SubRL），它寻找更一般、非加法（历史相互作用）的奖励模型，使用 submodular 集合函数来捕捉减少返回的特点。然而，在总的来说，即使在表格设定中，这种优化问题是Difficult to approximate。</li>
<li>results: 作者提出了一种简单的policy gradient算法——SubPO，它可以处理非加法奖励。SubPO 可以在一些假设下 recuperate 优化的常数因子应用，并且在大 state-和 action- 空间下可以进行本地优化。作者通过应用 SubPO 到不同的应用中，如生物多样性监测、抽象实验设计、信息 PATH 规划和覆盖最大化，来展示其效果。结果表明 SubPO 具有高效的样本使用和可扩展性。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SubPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SubRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying SubPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.
</details>
<details>
<summary>摘要</summary>
在增强学习（RL）中，状态奖励通常是加法的，并且根据马可夫假设，它们是独立的。在许多重要应用中，如覆盖控制、实验设计和有益路径规划，奖励自然地具有减少的返回，即在相似的状态前后访问的情况下，奖励的价值逐渐减少。为解决这个问题，我们提议了“增强RL”（SubRL），一种潜在优化更一般、非加法（历史相依）奖励的 paradigma。然而，在总的来说，即使在表格设置中，我们显示出的优化问题是难以估算的。在这种情况下，我们提出了一种简单的政策梯度方法，即SubPO，用于解决SubRL问题。SubPO通过积极地最大化权重的增加来处理非加法奖励。在某些假设下，SubPO可以在MDP中获得优化的常量因子approximation。此外，我们还 deriv了一种自然的政策梯度方法，用于本地优化SubRL实例，即使在大的状态和动作空间中。我们在各种应用中应用SubPO，如生物多样性监测、推论实验设计、有益路径规划和覆盖最大化。我们的结果显示了样本效率，以及可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation"><a href="#Learning-Regions-of-Interest-for-Bayesian-Optimization-with-Adaptive-Level-Set-Estimation" class="headerlink" title="Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation"></a>Learning Regions of Interest for Bayesian Optimization with Adaptive Level-Set Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13371">http://arxiv.org/abs/2307.13371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengxue Zhang, Jialin Song, James Bowden, Alexander Ladd, Yisong Yue, Thomas A. Desautels, Yuxin Chen</li>
<li>for: 这篇论文是关于 Bayesian 优化 (BO) 在高维和非站ARY enario 中的研究。</li>
<li>methods: 论文提出了一个框架，called BALLET，它可以在高维和非站ARY enario 中实现 Bayesian 优化。BALLET 使用了两个 probabilistic 模型：一个粗糙的 Gaussian 过程 (GP) 来识别高信度区域 (ROI)，以及一个本地化的 GP 来优化在 ROI 中。</li>
<li>results: 论文证明了 BALLET 可以有效缩小搜索空间，并且可以比标准 BO 方法更紧的对应 regret bound。论文还进行了实验证明，证明了 BALLET 在实际应用中的效果。<details>
<summary>Abstract</summary>
We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios. Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness. We propose a framework, called BALLET, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP). Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods. The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that BALLET can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering. We demonstrate empirically the effectiveness of BALLET on both synthetic and real-world optimization tasks.
</details>
<details>
<summary>摘要</summary>
我们研究 bayesian 优化（BO）在高维和非站点场景下。现有的算法通常需要广泛的 гипер参数调整，这限制了它们的实际效果。我们提出了一个框架，叫做 BALLET，它可以动态筛选出高信息域的兴趣点（ROI），作为非参数型 probabilistic 模型，如 Gaussian process（GP）的超级集。我们的方法容易调整，可以将关注点放在可以由现有的 BO 方法解决的本地优化空间上。关键思想是使用两种 probabilistic 模型：一个粗细的 GP 来识别 ROI，并一个局部化的 GP 进行优化在 ROI 中。我们证明了 BALLET 可以有效缩小搜索空间，并可以比标准 BO 无 ROI 筛选更紧的 regret  bound。我们在 sintetic 和实际优化任务上证明了 BALLET 的实际效果。
</details></li>
</ul>
<hr>
<h2 id="Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations"><a href="#Computational-Guarantees-for-Doubly-Entropic-Wasserstein-Barycenters-via-Damped-Sinkhorn-Iterations" class="headerlink" title="Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations"></a>Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13370">http://arxiv.org/abs/2307.13370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lénaïc Chizat, Tomas Vaškevičius</li>
<li>for:  Computation of doubly regularized Wasserstein barycenters</li>
<li>methods:  Damped Sinkhorn iterations followed by exact maximization&#x2F;minimization steps</li>
<li>results:  Convergence guarantees for any choice of regularization parameters, and non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support&#x2F;grid-free setting.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.
</details>
<details>
<summary>摘要</summary>
我们研究双正则化 Wasserstein 质心的计算，这是最近引入的一种内外正则化强度控制的泛化积分质心。先前的研究表明，不同的正则化参数选择可以统一各种束缚积分质心，同时还可以揭示新的质心，包括特殊情况下的减偏积分质心。在这篇文章中，我们提出了一种计算双正则化 Wasserstein 质心的算法，该算法基于抑制式谱散数据进行融合，然后使用精确的最大化/最小化步骤，可以保证任何正则化参数选择的收敛。在实际应用中，我们还提出了一种使用伪随机抽样来实现准确的质心计算的不精准变体，这是自由支持/网格自由设置下的第一个不对称收敛保证的方法。
</details></li>
</ul>
<hr>
<h2 id="Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers"><a href="#Prot2Text-Multimodal-Protein’s-Function-Generation-with-GNNs-and-Transformers" class="headerlink" title="Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers"></a>Prot2Text: Multimodal Protein’s Function Generation with GNNs and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14367">http://arxiv.org/abs/2307.14367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, Michalis Vazirgiannis</li>
<li>for: 这篇论文的目的是提出一个新的方法来预测蛋白质的功能，这个方法使用 Graph Neural Networks(GNNs) 和 Large Language Models(LLMs) 在encoder-decoder架构中结合，以生成蛋白质功能的详细描述。</li>
<li>methods: 这篇论文使用的方法是一种 multimodal 方法，结合蛋白质的序列、结构和文本描述，使用 GNNs 和 LLMs 进行融合，实现蛋白质功能的全面表示。</li>
<li>results: 这篇论文的实验结果显示，这个新的方法可以实现更高的预测精度，并且可以生成蛋白质功能的详细描述。<details>
<summary>Abstract</summary>
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate prediction of proteins' functions. The code, the models and a demo will be publicly released.
</details>
<details>
<summary>摘要</summary>
大生物系统的复杂性让一些科学家将其理解列入不可思议任务之列。不同的等级挑战困扰了这个任务，其中之一是蛋白质功能预测。在过去几年，我们在这一领域进行了重要的进步，通过开发多种机器学习方法。然而，大多数现有方法将任务定义为多类别问题，即将蛋白质分配预先定义的标签。在这种情况下，我们提出了一种新的方法——Prot2Text，它预测蛋白质功能在自由文本格式下，超越传统的二分或分类化预测。我们通过将图 neural network（GNN）和大型自然语言模型（LLM）组合在encoder-decoder框架中，能够集成多种蛋白质数据类型，包括序列、结构和文本注释。这种多模式方法允许我们对蛋白质功能进行整体表示，使得生成详细和准确的描述。为评估我们的模型，我们从SwissProt中提取了多模式蛋白质数据集，并通过实验证明Prot2Text的效果。这些结果显示了多模式模型的融合，特别是GNN和LLM的融合，为研究人员提供了更加准确的蛋白质功能预测工具。代码、模型和demo将公共发布。
</details></li>
</ul>
<hr>
<h2 id="High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers"><a href="#High-Dimensional-Distributed-Gradient-Descent-with-Arbitrary-Number-of-Byzantine-Attackers" class="headerlink" title="High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers"></a>High Dimensional Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13352">http://arxiv.org/abs/2307.13352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puning Zhao, Zhiguo Wan</li>
<li>for: 这篇论文是关于robust分布式学习，拥有Byzantine失败的情况下的研究。</li>
<li>methods: 该方法采用了直方正方向的半验证方法，可以在高维问题上进行解决，并且可以适应arbitrary数量的Byzantine攻击者。</li>
<li>results: 我们的研究表明，该方法可以在高维问题上实现最佳的统计效果，并且与前一些研究相比，它在维度上具有更好的性能。<details>
<summary>Abstract</summary>
Robust distributed learning with Byzantine failures has attracted extensive research interests in recent years. However, most of existing methods suffer from curse of dimensionality, which is increasingly serious with the growing complexity of modern machine learning models. In this paper, we design a new method that is suitable for high dimensional problems, under arbitrary number of Byzantine attackers. The core of our design is a direct high dimensional semi-verified mean estimation method. Our idea is to identify a subspace first. The components of mean value perpendicular to this subspace can be estimated via gradient vectors uploaded from worker machines, while the components within this subspace are estimated using auxiliary dataset. We then use our new method as the aggregator of distributed learning problems. Our theoretical analysis shows that the new method has minimax optimal statistical rates. In particular, the dependence on dimensionality is significantly improved compared with previous works.
</details>
<details>
<summary>摘要</summary>
robust 分布式学习受到了最近几年的广泛研究兴趣。然而，大多数现有方法受到了维度灾难的拥挤，这在现代机器学习模型的复杂度逐渐增加时变得越来越严重。在这篇论文中，我们设计了适用于高维问题的新方法，可以抗 resist 任意数量的拜占庭攻击者。我们的设计核心在于直接使用高维半验证平均值计算方法。我们的想法是先Identify一个子空间，然后通过上传 worker 机器的梯度向量来计算沿着这个子空间的方向的部分，而在这个子空间内部使用 auxillary 数据来计算剩余的部分。我们然后使用我们的新方法来汇集分布式学习问题。我们的理论分析表明，我们的新方法具有最优的最小最大统计率。具体来说，与前一些工作相比，我们的方法在维度方面具有显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking"><a href="#Explainable-Disparity-Compensation-for-Efficient-Fair-Ranking" class="headerlink" title="Explainable Disparity Compensation for Efficient Fair Ranking"></a>Explainable Disparity Compensation for Efficient Fair Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14366">http://arxiv.org/abs/2307.14366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abraham Gale, Amélie Marian</li>
<li>for: This paper aims to address the issue of disparate outcomes in decision systems, specifically in ranking functions, and proposes data-driven compensatory measures to improve fairness.</li>
<li>methods: The proposed measures rely on generating bonus points for members of underrepresented groups to address disparity in the ranking function. Efficient sampling-based algorithms are used to calculate the number of bonus points to minimize disparity.</li>
<li>results: The authors validate their algorithms using real-world school admissions and recidivism datasets, and compare their results with those of existing fair ranking algorithms. The results show that their proposed measures can effectively improve fairness in the ranking function.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文目标是解决决策系统中的不平等结果问题，具体来说是对排名函数中的不平等进行补偿。</li>
<li>methods: 提议的补偿措施基于为受排除群体成员分配加分点，以解决排名函数中的不平等。 authors使用高效的采样算法来计算加分点的数量，以最小化不平等。</li>
<li>results: authors使用实际的学校招生和重犯罪数据集来验证他们的算法，并与现有的公平排名算法进行比较。结果表明，提议的补偿措施可以有效地提高排名函数的公平性。<details>
<summary>Abstract</summary>
Ranking functions that are used in decision systems often produce disparate results for different populations because of bias in the underlying data. Addressing, and compensating for, these disparate outcomes is a critical problem for fair decision-making. Recent compensatory measures have mostly focused on opaque transformations of the ranking functions to satisfy fairness guarantees or on the use of quotas or set-asides to guarantee a minimum number of positive outcomes to members of underrepresented groups. In this paper we propose easily explainable data-driven compensatory measures for ranking functions. Our measures rely on the generation of bonus points given to members of underrepresented groups to address disparity in the ranking function. The bonus points can be set in advance, and can be combined, allowing for considering the intersections of representations and giving better transparency to stakeholders. We propose efficient sampling-based algorithms to calculate the number of bonus points to minimize disparity. We validate our algorithms using real-world school admissions and recidivism datasets, and compare our results with that of existing fair ranking algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Feature-Importance-Measurement-based-on-Decision-Tree-Sampling"><a href="#Feature-Importance-Measurement-based-on-Decision-Tree-Sampling" class="headerlink" title="Feature Importance Measurement based on Decision Tree Sampling"></a>Feature Importance Measurement based on Decision Tree Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13333">http://arxiv.org/abs/2307.13333</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tsudalab/dt-sampler">https://github.com/tsudalab/dt-sampler</a></li>
<li>paper_authors: Chao Huang, Diptesh Das, Koji Tsuda</li>
<li>for: 用于提高树基型模型中FeatureImportance的可 interpretability和稳定性。</li>
<li>methods: 使用SAT理论来测试FeatureImportance，具有 fewer parameters 和更高的可 interpretability，适用于实际问题。</li>
<li>results: 在实际问题中，DT-Sampler可以提供更高的可 interpretability和稳定性，并且比Random Forest具有更少的参数。Translation:</li>
<li>for: Used to improve the interpretability and stability of feature importance in tree-based models.</li>
<li>methods: Uses SAT theory to test feature importance, with fewer parameters and higher interpretability, applicable to real-world problems.</li>
<li>results: In practical problems, DT-Sampler can provide higher interpretability and stability, and has fewer parameters than Random Forest.<details>
<summary>Abstract</summary>
Random forest is effective for prediction tasks but the randomness of tree generation hinders interpretability in feature importance analysis. To address this, we proposed DT-Sampler, a SAT-based method for measuring feature importance in tree-based model. Our method has fewer parameters than random forest and provides higher interpretability and stability for the analysis in real-world problems. An implementation of DT-Sampler is available at https://github.com/tsudalab/DT-sampler.
</details>
<details>
<summary>摘要</summary>
随机森林可以很有效地进行预测任务，但随机生成树的randomness会降低特征重要性的解释性。为解决这个问题，我们提出了DT-Sampler，一种基于SAT的方法来测量树型模型中特征的重要性。我们的方法有 fewer parameters than random forest，并且在实际问题中提供了更高的解释性和稳定性。DT-Sampler的实现可以在https://github.com/tsudalab/DT-sampler中找到。
</details></li>
</ul>
<hr>
<h2 id="The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation"><a href="#The-Optimal-Approximation-Factors-in-Misspecified-Off-Policy-Value-Function-Estimation" class="headerlink" title="The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation"></a>The Optimal Approximation Factors in Misspecified Off-Policy Value Function Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13332">http://arxiv.org/abs/2307.13332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philip Amortila, Nan Jiang, Csaba Szepesvári</li>
<li>for: 这篇论文主要针对 linear off-policy value function estimation 问题进行研究，具体来说是研究函数approximation factor在不同设置下的优化形式。</li>
<li>methods: 论文使用了各种方法来研究函数approximation factor，包括使用权重$L_2$ norm、$L_\infty$ norm、状态别名和完整&#x2F;半 coverage的state space。</li>
<li>results: 论文的结果显示，在不同的设置下，函数approximation factor的优化形式是不同的，并且可以确定具体的常数因素。特别是，$L_2(\mu)$ norm 下的两个实例特定因素和 $L_\infty$ norm 下的一个常数因素被证明为决定了偏离策略评估的困难程度。<details>
<summary>Abstract</summary>
Theoretical guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. Yet, the nature of such \emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. In this paper we study this question in linear off-policy value function estimation, where many open questions remain. We study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. We establish the optimal asymptotic approximation factors (up to constants) for all of these settings. In particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.
</details>
<details>
<summary>摘要</summary>
theoretically guarantees in reinforcement learning (RL) are known to suffer multiplicative blow-up factors with respect to the misspecification error of function approximation. yet, the nature of such \emph{approximation factors} -- especially their optimal form in a given learning problem -- is poorly understood. in this paper, we study this question in linear off-policy value function estimation, where many open questions remain. we study the approximation factor in a broad spectrum of settings, such as with the weighted $L_2$-norm (where the weighting is the offline state distribution), the $L_\infty$ norm, the presence vs. absence of state aliasing, and full vs. partial coverage of the state space. we establish the optimal asymptotic approximation factors (up to constants) for all of these settings. in particular, our bounds identify two instance-dependent factors for the $L_2(\mu)$ norm and only one for the $L_\infty$ norm, which are shown to dictate the hardness of off-policy evaluation under misspecification.Note that Simplified Chinese is a written language, and the translation is based on the standardized grammar and vocabulary of Simplified Chinese. However, the actual translation may vary depending on the specific context and register used in the original text.
</details></li>
</ul>
<hr>
<h2 id="Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models"><a href="#Unleash-the-Power-of-Context-Enhancing-Large-Scale-Recommender-Systems-with-Context-Based-Prediction-Models" class="headerlink" title="Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models"></a>Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01231">http://arxiv.org/abs/2308.01231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Hartman, Assaf Klein, Davorin Kopič, Natalia Silberstein</li>
<li>for: 提高大规模商业推荐系统的性能，具有广泛的个性化推荐应用场景。</li>
<li>methods: 基于用户和上下文特征的预测模型，不考虑物品特征，可以减少服务成本。</li>
<li>results: 实验表明，这种方法可以在线上和离线上的商业指标中带来显著改善，而且对服务成本的影响很小。<details>
<summary>Abstract</summary>
In this work, we introduce the notion of Context-Based Prediction Models. A Context-Based Prediction Model determines the probability of a user's action (such as a click or a conversion) solely by relying on user and contextual features, without considering any specific features of the item itself. We have identified numerous valuable applications for this modeling approach, including training an auxiliary context-based model to estimate click probability and incorporating its prediction as a feature in CTR prediction models. Our experiments indicate that this enhancement brings significant improvements in offline and online business metrics while having minimal impact on the cost of serving. Overall, our work offers a simple and scalable, yet powerful approach for enhancing the performance of large-scale commercial recommender systems, with broad implications for the field of personalized recommendations.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了基于上下文的预测模型。这种预测模型根据用户和上下文特征决定用户行为（如点击或购买）的概率，不考虑物品本身的特征。我们已经认为这种模型途径具有很多有价值的应用，包括培养一个辅助上下文基于模型来估计点击概率，并将其预测作为ctr预测模型中的一个特征。我们的实验表明，这种增强可以在线上和Offline商业指标方面带来显著改善，而无需增加服务成本。总之，我们的工作提供了一种简单、可扩展、 yet 具有强大能力的方法来提高大规模的商业推荐系统的性能，对个人化推荐领域产生广泛的影响。
</details></li>
</ul>
<hr>
<h2 id="QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees"><a href="#QuIP-2-Bit-Quantization-of-Large-Language-Models-With-Guarantees" class="headerlink" title="QuIP: 2-Bit Quantization of Large Language Models With Guarantees"></a>QuIP: 2-Bit Quantization of Large Language Models With Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13304">http://arxiv.org/abs/2307.13304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/quip">https://github.com/jerry-chee/quip</a></li>
<li>paper_authors: Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa</li>
<li>for: 本研究探讨大语言模型（LLM）后期参数归一化。</li>
<li>methods: 我们提出了一种基于干扰量和矩阵方向的归一化方法（QuIP），包括两个步骤：(1) 适应归一化过程中的二次proxy目标函数; (2) 高效的预处理和后处理，通过随机正交矩阵来保证参数和偏差矩阵的不一致。</li>
<li>results: 我们的实验表明，QuIP可以提高多种现有的归一化算法的性能，并且在只使用两个位数据时实现了首个可行的LLM归一化方法。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/jerry-chee/QuIP上找到。</a><details>
<summary>Abstract</summary>
This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/jerry-chee/QuIP .
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>An adaptive rounding procedure that minimizes a quadratic proxy objective.2. Efficient pre- and post-processing that ensures weight and Hessian incoherence through multiplication by random orthogonal matrices.We also provide the first theoretical analysis for an LLM-scale quantization algorithm and show that our theory applies to an existing method, OPTQ. Our experiments demonstrate that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/jerry-chee/QuIP">https://github.com/jerry-chee/QuIP</a>.</details></li>
</ol>
<hr>
<h2 id="Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts"><a href="#Word-Sense-Disambiguation-as-a-Game-of-Neurosymbolic-Darts" class="headerlink" title="Word Sense Disambiguation as a Game of Neurosymbolic Darts"></a>Word Sense Disambiguation as a Game of Neurosymbolic Darts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16663">http://arxiv.org/abs/2307.16663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiansi Dong, Rafet Sifa</li>
<li>for: 提高Word Sense Disambiguation（WSD）任务的性能，突破深度学习方法的“玻璃天花”</li>
<li>methods: 提出了一种新的神经符号方法，利用嵌入式的方法，通过嵌入式的方法来实现简单的逻辑推理，并通过游戏“dart”来训练Transformer模型</li>
<li>results: 在多个测试数据集上达到了F1分数的90%以上，并且在不同的n-ball嵌入中得到了70%-75%的覆盖率，表明该方法可以超越深度学习方法的性能 bound.<details>
<summary>Abstract</summary>
Word Sense Disambiguation (WSD) is one of the hardest tasks in natural language understanding and knowledge engineering. The glass ceiling of 80% F1 score is recently achieved through supervised deep-learning, enriched by a variety of knowledge graphs. Here, we propose a novel neurosymbolic methodology that is able to push the F1 score above 90%. The core of our methodology is a neurosymbolic sense embedding, in terms of a configuration of nested balls in n-dimensional space. The centre point of a ball well-preserves word embedding, which partially fix the locations of balls. Inclusion relations among balls precisely encode symbolic hypernym relations among senses, and enable simple logic deduction among sense embeddings, which cannot be realised before. We trained a Transformer to learn the mapping from a contextualized word embedding to its sense ball embedding, just like playing the game of darts (a game of shooting darts into a dartboard). A series of experiments are conducted by utilizing pre-training n-ball embeddings, which have the coverage of around 70% training data and 75% testing data in the benchmark WSD corpus. The F1 scores in experiments range from 90.1% to 100.0% in all six groups of test data-sets (each group has 4 testing data with different sizes of n-ball embeddings). Our novel neurosymbolic methodology has the potential to break the ceiling of deep-learning approaches for WSD. Limitations and extensions of our current works are listed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error"><a href="#Modify-Training-Directions-in-Function-Space-to-Reduce-Generalization-Error" class="headerlink" title="Modify Training Directions in Function Space to Reduce Generalization Error"></a>Modify Training Directions in Function Space to Reduce Generalization Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13290">http://arxiv.org/abs/2307.13290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Yu, Wenlian Lu, Boyu Chen</li>
<li>for: 提高神经网络模型的泛化性能</li>
<li>methods: 使用修改后的自然向导方法在神经网络函数空间进行 theoretically 分析，并利用 eigendecompositions 和统计学理论来derive 神经网络函数的泛化误差</li>
<li>results: 提出一个基于 eigendecompositions 和统计学理论的泛化误差减少方法，并通过数学示例证明该方法可以改善神经网络模型的泛化性能。此外，这种 theoretically 方法还可以解释许多现有的泛化提高方法的效果。<details>
<summary>Abstract</summary>
We propose theoretical analyses of a modified natural gradient descent method in the neural network function space based on the eigendecompositions of neural tangent kernel and Fisher information matrix. We firstly present analytical expression for the function learned by this modified natural gradient under the assumptions of Gaussian distribution and infinite width limit. Thus, we explicitly derive the generalization error of the learned neural network function using theoretical methods from eigendecomposition and statistics theory. By decomposing of the total generalization error attributed to different eigenspace of the kernel in function space, we propose a criterion for balancing the errors stemming from training set and the distribution discrepancy between the training set and the true data. Through this approach, we establish that modifying the training direction of the neural network in function space leads to a reduction in the total generalization error. Furthermore, We demonstrate that this theoretical framework is capable to explain many existing results of generalization enhancing methods. These theoretical results are also illustrated by numerical examples on synthetic data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于神经网络函数空间的修改后自然算法的理论分析。我们首先提出了假设 Gaussian 分布和无限宽限的情况下，修改后自然算法所学习的函数的analytical表达。这使我们可以明确地 Compute 修改后神经网络函数的通用错误。我们还将Total generalization error decomposed into different eigenspace of the kernel in function space，并提出一个均衡错误的对象，以减少Total generalization error。此外，我们还证明了这个理论框架可以解释许多现有的通用提升方法的结果。这些理论结果也通过了实验示例 validate 在Synthetic data 上。Here's the translation in Traditional Chinese:我们提出了一种基于神经网络函数空间的修改后自然算法的理论分析。我们首先提出了假设 Gaussian 分布和无限宽限的情况下，修改后自然算法所学习的函数的analytical表达。这使我们可以明确地 Compute 修改后神经网络函数的通用错误。我们还将Total generalization error decomposed into different eigenspace of the kernel in function space，并提出一个均衡错误的对象，以减少Total generalization error。此外，我们还证明了这个理论框架可以解释许多现有的通用提升方法的结果。这些理论结果也通过了实验示例 validate 在Synthetic data 上。
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Transformer-for-Molecular-Property-Prediction"><a href="#Curvature-based-Transformer-for-Molecular-Property-Prediction" class="headerlink" title="Curvature-based Transformer for Molecular Property Prediction"></a>Curvature-based Transformer for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13275">http://arxiv.org/abs/2307.13275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yili Chen, Zhengyu Li, Zheng Wan, Hui Yu, Xian Wei</li>
<li>for: 提高基于人工智能的药物设计中分子属性预测的能力</li>
<li>methods: 引入Discretization of Ricci Curvature来提高图граaph神经网络模型对分子图数据的结构信息抽取能力</li>
<li>results: 在PCQM4M-LST、MoleculeNet等化学分子数据集上进行实验，与Uni-Mol、Graphormer等模型进行比较，结果表明该方法可以达到当前最佳结果。另外，Discretized Ricci curvature还能够捕捉分子结构和功能关系的信息，描述分子图数据的地方几何特征。<details>
<summary>Abstract</summary>
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments on chemical molecular datasets including PCQM4M-LST, MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the results show that this method can achieve the state-of-the-art results. It is proved that the discretized Ricci curvature also reflects the structural and functional relationship while describing the local geometry of the graph molecular data.
</details>
<details>
<summary>摘要</summary>
“分子质量预测是人工智能基于药物设计的一个最重要和挑战性任务。现今主流方法中，最常用的特征表示方法是基于SMILES和分子图，尽管这些方法简洁有效，但它们也限制了捕捉空间信息的能力。在这种工作中，我们提出了几何基于 transformer 的 Curvature-based Transformer，以提高对分子图数据的结构信息抽取能力。在计算注意力分布时，我们添加了图的 Ricci 曲率信息作为 pozitional Encoding，以将曲率信息 embed 到节点特征中。这种方法可以在不改变原始网络结构的情况下，将曲率信息从图数据中引入到模型中，并且具有扩展性。我们在 PCQM4M-LST、MoleculeNet 等化学分子数据集上进行了实验，与 Uni-Mol、Graphormer 等模型进行比较，结果显示，这种方法可以达到领先的成绩。这也证明了对分子图数据的几何基本特征的描述，同时还能够反映分子的结构和功能关系。”
</details></li>
</ul>
<hr>
<h2 id="Unbiased-Weight-Maximization"><a href="#Unbiased-Weight-Maximization" class="headerlink" title="Unbiased Weight Maximization"></a>Unbiased Weight Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13270">http://arxiv.org/abs/2307.13270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung</li>
<li>for: 本研究旨在提出一种生物学上有效的人工神经网络（ANN）训练方法，即对每个单元视为杂种学习（RL）代理，以视网膜为一个团队。</li>
<li>methods: 本方法使用REINFORCE算法，但是由于单元之间的信息不准确传递，会导致学习过程缓慢，难以扩展到大规模网络。为解决这个问题，提出了Weight Maximization方法，即每个隐藏单元可以通过自己的出口权重的 нор 来最大化自己的学习效果。</li>
<li>results: 研究人员分析了Weight Maximization方法的理论性质，并提出了一种变体Unbiased Weight Maximization。这种新方法可以提供一种不偏学习规则，使学习速度加快，并在网络规模增加时保持良好的性能。具体来说，这是目前所知道的第一种可以快速学习、适用于大规模网络的不偏学习规则。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. Nevertheless, this learning method is often slow and scales poorly with network size due to inefficient structural credit assignment, since a single reward signal is broadcast to all units without considering individual contributions. Weight Maximization, a proposed solution, replaces a unit's reward signal with the norm of its outgoing weight, thereby allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal. In this research report, we analyze the theoretical properties of Weight Maximization and propose a variant, Unbiased Weight Maximization. This new approach provides an unbiased learning rule that increases learning speed and improves asymptotic performance. Notably, to our knowledge, this is the first learning rule for a network of Bernoulli-logistic units that is unbiased and scales well with the number of network's units in terms of learning speed.
</details>
<details>
<summary>摘要</summary>
一种生物学可能性的方法 для训练人工神经网络（ANN）是将每个单元视为一个随机反弹学习（RL）代理，从而考虑神经网络为一支代理队伍。因此，所有单元都可以通过REINFORCE本地学习规则，该规则由全局奖励信号修饰，与生物观察到的 synaptic plasticity更加相似。然而，这种学习方法通常慢和网络大小不好扩展，因为不效的结构归因分配，单个奖励信号 Broadcast 到所有单元而无法考虑单元之间的贡献。Weight Maximization，一种提议的解决方案，将单元的奖励信号替换为出口重量的norm， allowing each hidden unit to maximize the norm of the outgoing weight instead of the global reward signal。在这份研究报告中，我们分析Weight Maximization的理论性质和提出一种变体，即不偏学习Weight Maximization。这种新的学习规则提供了一个不偏的学习规则，提高学习速度和长期性表现。值得注意的是，我们知道，这是一种可以扩展到神经网络中的 Bernoulli-logistic 单元数量的学习规则，并且与网络大小成直线关系。
</details></li>
</ul>
<hr>
<h2 id="Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization"><a href="#Federated-K-Means-Clustering-via-Dual-Decomposition-based-Distributed-Optimization" class="headerlink" title="Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization"></a>Federated K-Means Clustering via Dual Decomposition-based Distributed Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13267">http://arxiv.org/abs/2307.13267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vassilios Yfantis, Achim Wagner, Martin Ruskowski</li>
<li>for: This paper is written for researchers and practitioners interested in distributed optimization for machine learning, particularly in the context of $ K $-means clustering.</li>
<li>methods: The paper uses dual decomposition to solve the distributed training of $ K $-means clustering problems. The authors propose a mixed-integer quadratically constrained programming-based formulation of the clustering training problem and evaluate the performance of three optimization algorithms (subgradient method, bundle trust method, and quasi-Newton dual ascent algorithm) on a set of benchmark problems.</li>
<li>results: The paper demonstrates the potential of using dual decomposition for distributed training of $ K $-means clustering problems, but notes that the mixed-integer programming-based formulation of the clustering problems suffers from weak integer relaxations. The authors evaluate the performance of three optimization algorithms and show that the proposed approach can potentially enable an efficient solution in the future, both in a central and distributed setting.<details>
<summary>Abstract</summary>
The use of distributed optimization in machine learning can be motivated either by the resulting preservation of privacy or the increase in computational efficiency. On the one hand, training data might be stored across multiple devices. Training a global model within a network where each node only has access to its confidential data requires the use of distributed algorithms. Even if the data is not confidential, sharing it might be prohibitive due to bandwidth limitations. On the other hand, the ever-increasing amount of available data leads to large-scale machine learning problems. By splitting the training process across multiple nodes its efficiency can be significantly increased. This paper aims to demonstrate how dual decomposition can be applied for distributed training of $ K $-means clustering problems. After an overview of distributed and federated machine learning, the mixed-integer quadratically constrained programming-based formulation of the $ K $-means clustering training problem is presented. The training can be performed in a distributed manner by splitting the data across different nodes and linking these nodes through consensus constraints. Finally, the performance of the subgradient method, the bundle trust method, and the quasi-Newton dual ascent algorithm are evaluated on a set of benchmark problems. While the mixed-integer programming-based formulation of the clustering problems suffers from weak integer relaxations, the presented approach can potentially be used to enable an efficient solution in the future, both in a central and distributed setting.
</details>
<details>
<summary>摘要</summary>
使用分布式优化在机器学习中可以受到保持隐私和提高计算效率的两种动机。一个是训练数据可能会被存储在多个设备上，而每个节点只有访问自己的敏感数据时，需要使用分布式算法来训练全球模型。另一个是由于数据量的增加，导致大规模机器学习问题的出现。通过将训练过程分布到多个节点来加速其效率。这篇论文旨在演示如何使用分布式优化解决分布式训练 $ K $-means clustering问题。文章首先介绍分布式和联邦机器学习，然后提出基于杂Integer编程的$ K $-means clustering训练问题的混合形式。通过在不同节点上分布数据并通过共识约束相连接这些节点来进行分布式训练。最后，文章评估了在一组 benchmark 问题上的贪婪法、杂Integer法和 quasi-Newton  dual ascent 算法的性能。虽然杂Integer编程基本形式的 clustering 问题受到弱的整数放宽，但是该方法可能可以在未来的中心和分布式设置中启用高效的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment"><a href="#Federated-Split-Learning-with-Only-Positive-Labels-for-resource-constrained-IoT-environment" class="headerlink" title="Federated Split Learning with Only Positive Labels for resource-constrained IoT environment"></a>Federated Split Learning with Only Positive Labels for resource-constrained IoT environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13266">http://arxiv.org/abs/2307.13266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Praveen Joshi, Chandra Thapa, Mohammed Hasanuzzaman, Ted Scully, Haithem Afli</li>
<li>for: 提高 IoT 设备数据隐私和提高模型训练效率</li>
<li>methods: 使用分布式合作机器学习（DCML）和分布式分裂学习（SFL）技术，并在客户端模型部分应用本地批处理和批处理随机混淆</li>
<li>results: SFPL 比 SFL 提高了模型训练效率和预测精度，具体达到了以下因素：（i）CIFAR-100 数据集上，SFPL 比 SFL 提高了 ResNet-56 和 ResNet-32 模型的训练效率，分别提高了51.54和32.57倍；（ii）CIFAR-10 数据集上，SFPL 比 SFL 提高了 ResNet-32 和 ResNet-8 模型的训练效率，分别提高了9.23和8.52倍。<details>
<summary>Abstract</summary>
Distributed collaborative machine learning (DCML) is a promising method in the Internet of Things (IoT) domain for training deep learning models, as data is distributed across multiple devices. A key advantage of this approach is that it improves data privacy by removing the necessity for the centralized aggregation of raw data but also empowers IoT devices with low computational power. Among various techniques in a DCML framework, federated split learning, known as splitfed learning (SFL), is the most suitable for efficient training and testing when devices have limited computational capabilities. Nevertheless, when resource-constrained IoT devices have only positive labeled data, multiclass classification deep learning models in SFL fail to converge or provide suboptimal results. To overcome these challenges, we propose splitfed learning with positive labels (SFPL). SFPL applies a random shuffling function to the smashed data received from clients before supplying it to the server for model training. Additionally, SFPL incorporates the local batch normalization for the client-side model portion during the inference phase. Our results demonstrate that SFPL outperforms SFL: (i) by factors of 51.54 and 32.57 for ResNet-56 and ResNet-32, respectively, with the CIFAR-100 dataset, and (ii) by factors of 9.23 and 8.52 for ResNet-32 and ResNet-8, respectively, with CIFAR-10 dataset. Overall, this investigation underscores the efficacy of the proposed SFPL framework in DCML.
</details>
<details>
<summary>摘要</summary>
分布式协同机器学习（DCML）是互联网物联网（IoT）领域的一种有前途的方法，用于训练深度学习模型，因为数据分布在多个设备上。DCML的一个优点是改善数据隐私，不需要将原始数据集中化，同时也使 IoT 设备具备较低的计算能力。在 DCML 框架中，联邦分割学习（SFL）是最适合高效地训练和测试，当设备有限的计算能力时。然而，当资源有限的 IoT 设备只有正例数据时，SFL 中的多类分类深度学习模型会无法 converge 或提供低优的结果。为了解决这些挑战，我们提议使用分布式学习Positive Label（SFPL）。SFPL 使用客户端接收到的数据进行随机混淆函数处理，然后将其提供给服务器进行模型训练。此外，SFPL 还在推理阶段在客户端上实现本地批处理标准化。我们的结果表明，SFPL 在 CIFAR-100 和 CIFAR-10 数据集上分别比 SFL 提高了51.54 和 32.57 倍，并且在 CIFAR-10 数据集上比 SFL 提高了9.23 和 8.52 倍。总的来说，这种研究证明了我们提议的 SFPL 框架在 DCML 中的效果。
</details></li>
</ul>
<hr>
<h2 id="Structural-Credit-Assignment-with-Coordinated-Exploration"><a href="#Structural-Credit-Assignment-with-Coordinated-Exploration" class="headerlink" title="Structural Credit Assignment with Coordinated Exploration"></a>Structural Credit Assignment with Coordinated Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13256">http://arxiv.org/abs/2307.13256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Chung<br>for: 这个论文旨在提出一种生物学上可能性的人工神经网络（ANN）训练方法，该方法是将每个单元视为随机奖励学习（RL）代理，从而将网络视为一群代理。methods: 该方法使用REINFORCE本地学习规则，该规则由全局奖励信号修饰，与生物观察到的 synaptic plasticity更加一致。然而，这种学习方法的启用缓慢，并且与网络大小不相关。这种缓慢的原因是：（i）所有单元独立探索网络，（ii）所有单元都使用同一个奖励来评估其行动。因此，可以分为两类方法来改进结构准确评估。results: 我们提出使用博尔ツ曼机或回卷网络进行协调探索。我们发现，在训练博尔ツ曼机时，可以消除负阶段，其学习规则与奖励调整 Hebbian 学习规则相似。实验结果表明，协调探索在多个随机和离散单元基于 REINFORCE 训练速度上明显超过独立探索，甚至超过 straight-through estimator（STE）反propagation。<details>
<summary>Abstract</summary>
A biologically plausible method for training an Artificial Neural Network (ANN) involves treating each unit as a stochastic Reinforcement Learning (RL) agent, thereby considering the network as a team of agents. Consequently, all units can learn via REINFORCE, a local learning rule modulated by a global reward signal, which aligns more closely with biologically observed forms of synaptic plasticity. However, this learning method tends to be slow and does not scale well with the size of the network. This inefficiency arises from two factors impeding effective structural credit assignment: (i) all units independently explore the network, and (ii) a single reward is used to evaluate the actions of all units. Accordingly, methods aimed at improving structural credit assignment can generally be classified into two categories. The first category includes algorithms that enable coordinated exploration among units, such as MAP propagation. The second category encompasses algorithms that compute a more specific reward signal for each unit within the network, like Weight Maximization and its variants. In this research report, our focus is on the first category. We propose the use of Boltzmann machines or a recurrent network for coordinated exploration. We show that the negative phase, which is typically necessary to train Boltzmann machines, can be removed. The resulting learning rules are similar to the reward-modulated Hebbian learning rule. Experimental results demonstrate that coordinated exploration significantly exceeds independent exploration in training speed for multiple stochastic and discrete units based on REINFORCE, even surpassing straight-through estimator (STE) backpropagation.
</details>
<details>
<summary>摘要</summary>
生物学上有效的人工神经网络（ANN）训练方法是将每个单元视为随机奖励学习（RL）代理，从而考虑整个网络为一支代理队伍。这样做的优点是每个单元都可以通过REINFORCE本地学习规则和全局奖励信号来学习，这更加符合生物观察到的神经元强化突变。然而，这种学习方法具有两个缺点，导致效率低下：（1）所有单元独立探索网络，（2）网络中的所有单元都接受同一个奖励。这两点导致了结构准确评价的障碍。为了改进结构准确评价，一般可以分为两类方法：第一类是使单元之间协同探索的算法，如MAP协同传播；第二类是计算网络中每个单元的更加准确的奖励信号，如质量最大化和其变种。本研究报告的焦点是第一类方法。我们提议使用博尔ツ曼机或回归网络进行协同探索。我们发现，通常需要训练博尔ツ曼机的负阶段可以除去。结果的学习规则类似于奖励调节的希质bean学习规则。实验结果表明，协同探索在多个随机离散单元基于REINFORCE训练速度上明显高于独立探索，甚至超过STE归整梯度归整。
</details></li>
</ul>
<hr>
<h2 id="RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision"><a href="#RoSAS-Deep-Semi-Supervised-Anomaly-Detection-with-Contamination-Resilient-Continuous-Supervision" class="headerlink" title="RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision"></a>RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13239">http://arxiv.org/abs/2307.13239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuhongzuo/rosas">https://github.com/xuhongzuo/rosas</a></li>
<li>paper_authors: Hongzuo Xu, Yijie Wang, Guansong Pang, Songlei Jian, Ning Liu, Yongjun Wang</li>
<li>for: 这篇论文的目的是提出一种新的半指导型异常检测方法，以提高异常检测的性能。</li>
<li>methods: 这篇论文使用了一种新的混合梯度法，将涉猎到异常的标签资料与正常标签资料混合在一起，从而创建了新的标签资料集。同时，这篇论文还使用了一个特别的目标函数来规范网络，以提高网络的Robustness。</li>
<li>results: 这篇论文的实验结果显示，该方法可以在11个真实世界数据集上取得20%-30%的提升，并且在不同的异常污染水平和不同数量的标签异常下展现出更加稳定和更好的性能。<details>
<summary>Abstract</summary>
Semi-supervised anomaly detection methods leverage a few anomaly examples to yield drastically improved performance compared to unsupervised models. However, they still suffer from two limitations: 1) unlabeled anomalies (i.e., anomaly contamination) may mislead the learning process when all the unlabeled data are employed as inliers for model training; 2) only discrete supervision information (such as binary or ordinal data labels) is exploited, which leads to suboptimal learning of anomaly scores that essentially take on a continuous distribution. Therefore, this paper proposes a novel semi-supervised anomaly detection method, which devises \textit{contamination-resilient continuous supervisory signals}. Specifically, we propose a mass interpolation method to diffuse the abnormality of labeled anomalies, thereby creating new data samples labeled with continuous abnormal degrees. Meanwhile, the contaminated area can be covered by new data samples generated via combinations of data with correct labels. A feature learning-based objective is added to serve as an optimization constraint to regularize the network and further enhance the robustness w.r.t. anomaly contamination. Extensive experiments on 11 real-world datasets show that our approach significantly outperforms state-of-the-art competitors by 20%-30% in AUC-PR and obtains more robust and superior performance in settings with different anomaly contamination levels and varying numbers of labeled anomalies. The source code is available at https://github.com/xuhongzuo/rosas/.
</details>
<details>
<summary>摘要</summary>
semi-supervised异常检测方法可以借鉴一些异常示例，以实现与不supervised模型相比的显著改善。然而，它们仍然受到两个限制：1）无标签异常（即异常杂化）可能会导致学习过程中的混乱，当所有无标签数据作为模型训练中的内liers使用时；2）只是利用简单的数据标签（如二分或ORDinal数据标签），导致异常分数的学习变得不优化。因此，本文提出了一种新的 semi-supervised异常检测方法，即使用“杂化防御”的continuous supervisory signals。具体来说，我们提出了一种杂化 interpolating方法，以帮助异常标注样本的异常程度进行灵活的销毁，并创建了新的数据样本，其中每个样本都有连续的异常度标签。此外，杂化区域可以通过组合正确标注的数据来覆盖。我们还添加了一个特征学习基于的目标函数，以便为杂化异常进行更好的规范化和强化。我们在11个真实世界数据集上进行了广泛的实验，结果显示，我们的方法在AUC-PR方面与当前竞争对手相比，提高了20%-30%，并且在不同的异常杂化水平和异常标注数量的情况下具有更加稳定和优秀的性能。代码可以在https://github.com/xuhongzuo/rosas/ obtain。
</details></li>
</ul>
<hr>
<h2 id="Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation"><a href="#Audio-aware-Query-enhanced-Transformer-for-Audio-Visual-Segmentation" class="headerlink" title="Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation"></a>Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13236">http://arxiv.org/abs/2307.13236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinxiang Liu, Chen Ju, Chaofan Ma, Yanfeng Wang, Yu Wang, Ya Zhang</li>
<li>for:  Audio-visual segmentation (AVS) task, 用于将声音cue integrate into视频帧中的物体 segmentation</li>
<li>methods: 提出了一种新的AUDIO-aware query-enhanced TRANSFORMER（AuTR）方法，通过多模态变换架构和声音注意力机制来深度融合和聚合声音-视频特征</li>
<li>results: 比前方法更高的性能和更好的泛化能力在多声音和开放集成enario中<details>
<summary>Abstract</summary>
The goal of the audio-visual segmentation (AVS) task is to segment the sounding objects in the video frames using audio cues. However, current fusion-based methods have the performance limitations due to the small receptive field of convolution and inadequate fusion of audio-visual features. To overcome these issues, we propose a novel \textbf{Au}dio-aware query-enhanced \textbf{TR}ansformer (AuTR) to tackle the task. Unlike existing methods, our approach introduces a multimodal transformer architecture that enables deep fusion and aggregation of audio-visual features. Furthermore, we devise an audio-aware query-enhanced transformer decoder that explicitly helps the model focus on the segmentation of the pinpointed sounding objects based on audio signals, while disregarding silent yet salient objects. Experimental results show that our method outperforms previous methods and demonstrates better generalization ability in multi-sound and open-set scenarios.
</details>
<details>
<summary>摘要</summary>
文本：目标是使用音频cue将视频帧中的响应物分 segment。然而，现有的混合方法受限于小感知范围和不足的音频视频特征混合。为解决这些问题，我们提出一种新的听音感知Query加强的 transformer（AuTR）来解决这个任务。与现有方法不同，我们的方法 introduce一种多模态 transformer架构，该架构允许深度融合和音频视频特征的总结。此外，我们设计了一种听音感知Query加强的 transformer解码器，该解码器会在音频信号上显式地帮助模型将注意力集中在音频cue上，而忽略沉默却重要的物体。实验结果表明，我们的方法在多音和开放集成enario中表现出色，并且比前方法更好地普适化。Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering"><a href="#Spectral-DP-Differentially-Private-Deep-Learning-through-Spectral-Perturbation-and-Filtering" class="headerlink" title="Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering"></a>Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13231">http://arxiv.org/abs/2307.13231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ce Feng, Nuo Xu, Wujie Wen, Parv Venkitasubramaniam, Caiwen Ding</li>
<li>For: This paper proposes a new approach to differentially private deep learning called Spectral-DP, which improves upon existing methods by achieving a desired privacy guarantee with a lower noise scale and thus better utility.* Methods: The paper uses a combination of gradient perturbation in the spectral domain and spectral filtering to achieve differential privacy, and develops methods for both convolutional and fully connected layers.* Results: The paper shows through comprehensive experiments that Spectral-DP has uniformly better utility performance compared to state-of-the-art DP-SGD based approaches, both in training from scratch and transfer learning settings.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文提出了一种新的扩展privacy深度学习方法，叫做Spectral-DP，它可以在保持隐私性的同时提高实用性。</li>
<li>methods: 该论文使用了spectral domain中的梯度偏移和spectral filtering来实现扩展privacy，并为 convolutional和fully connected层都开发了方法。</li>
<li>results: 经过了广泛的实验，论文显示Spectral-DP在训练从头开始和传输学习设置下都有uniformly更好的实用性表现，比采用现有的DP-SGD基于方法更好。<details>
<summary>Abstract</summary>
Differential privacy is a widely accepted measure of privacy in the context of deep learning algorithms, and achieving it relies on a noisy training approach known as differentially private stochastic gradient descent (DP-SGD). DP-SGD requires direct noise addition to every gradient in a dense neural network, the privacy is achieved at a significant utility cost. In this work, we present Spectral-DP, a new differentially private learning approach which combines gradient perturbation in the spectral domain with spectral filtering to achieve a desired privacy guarantee with a lower noise scale and thus better utility. We develop differentially private deep learning methods based on Spectral-DP for architectures that contain both convolution and fully connected layers. In particular, for fully connected layers, we combine a block-circulant based spatial restructuring with Spectral-DP to achieve better utility. Through comprehensive experiments, we study and provide guidelines to implement Spectral-DP deep learning on benchmark datasets. In comparison with state-of-the-art DP-SGD based approaches, Spectral-DP is shown to have uniformly better utility performance in both training from scratch and transfer learning settings.
</details>
<details>
<summary>摘要</summary>
diffeential privacy 是深度学习算法中广泛接受的隐私标准，实现 diffeential privacy 需要使用带有直接噪声的 dense neural network 的启发式梯度下降（DP-SGD）。DP-SGD 需要在每个梯度上添加直接噪声，以实现隐私，但是这会导致较高的额外成本。在这项工作中，我们介绍 Spectral-DP，一种新的启发式隐私学习方法，它在spectral domain中添加梯度扰动，并通过spectral filtering来实现隐私保证，并且具有较低的噪声级别和更好的实用性。我们开发了基于 Spectral-DP 的启发式深度学习方法，包括具有 convolution 和 fully connected 层的架构。尤其是在 fully connected 层上，我们将 block-circulant 基于的空间重构与 Spectral-DP 结合使用，以实现更好的实用性。通过广泛的实验，我们研究了 Spectral-DP 深度学习的实现方法，并提供了实现指南。与state-of-the-art DP-SGD 基于方法相比，Spectral-DP 在训练从头开始和转移学习设置下具有更好的实用性表现。
</details></li>
</ul>
<hr>
<h2 id="A-Primer-on-the-Data-Cleaning-Pipeline"><a href="#A-Primer-on-the-Data-Cleaning-Pipeline" class="headerlink" title="A Primer on the Data Cleaning Pipeline"></a>A Primer on the Data Cleaning Pipeline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13219">http://arxiv.org/abs/2307.13219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca C. Steorts</li>
<li>for: 这篇论文主要是为了介绍数据整理管道（data cleaning pipeline）的科学，以便在下游任务、预测分析或统计分析中使用“净化数据”。</li>
<li>methods: 论文介绍了数据整理管道的四个阶段，包括数据预处理、数据整理、数据检查和数据净化。</li>
<li>results: 论文介绍了一些常用的数据整理方法和技术，以及在实际应用中的效果。<details>
<summary>Abstract</summary>
The availability of both structured and unstructured databases, such as electronic health data, social media data, patent data, and surveys that are often updated in real time, among others, has grown rapidly over the past decade. With this expansion, the statistical and methodological questions around data integration, or rather merging multiple data sources, has also grown. Specifically, the science of the ``data cleaning pipeline'' contains four stages that allow an analyst to perform downstream tasks, predictive analyses, or statistical analyses on ``cleaned data.'' This article provides a review of this emerging field, introducing technical terminology and commonly used methods.
</details>
<details>
<summary>摘要</summary>
“过去一代，数据库的可用性，包括电子健康数据、社交媒体数据、专利数据和调查等，在快速增长。这种增长也导致了数据集成问题的统计和方法问题的增长。特别是“数据清洁管道”科学中的四个阶段，允许分析员在“净化数据”后进行下游任务、预测分析或统计分析。本文将介绍这个新兴领域，并介绍技术术语和常用方法。”Note: "数据清洁管道" (data cleaning pipeline) is a term used to describe the process of preparing data for analysis, including cleaning, transforming, and integrating data from multiple sources.
</details></li>
</ul>
<hr>
<h2 id="FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning"><a href="#FedMEKT-Distillation-based-Embedding-Knowledge-Transfer-for-Multimodal-Federated-Learning" class="headerlink" title="FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning"></a>FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13214">http://arxiv.org/abs/2307.13214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Q. Le, Minh N. H. Nguyen, Chu Myaet Thwal, Yu Qiao, Chaoning Zhang, Choong Seon Hong<br>for:这个研究旨在提出一个基于联邦学习的多 modal 机器学习架构，以便在多个客户端上实现共同训练一个通用的全球模型，而无需分享私人数据。methods:这个架构使用了一种 semi-supervised 学习方法，以利用不同modalities的表现。它还包括一个发散基于多 modal 嵌入知识转移机制，名为 FedMEKT，可以将服务器和客户端的学习模型中的共同知识转移到参与的客户端上。results:这个研究透过三个多 modal 人类活动识别数据集进行广泛的实验，展示了 FedMEKT 可以实现更好的全球嵌入器性能，并且保护使用者的隐私和模型参数，并且需要较少的通信成本。<details>
<summary>Abstract</summary>
Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset. Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. Thereby, to address the modality discrepancy and labeled data constraint in existing FL systems, our proposed FedMEKT comprises local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）可以实现分布式机器学习的分布式机器学习模型，让多个客户端共同训练一个通用全球模型，而不需要分享私人数据。现有大多数工作都只是提议典型的FL系统，因此限制了其在多模态数据上的潜在应用。另外，大多数FL方法仍然依赖客户端上的标注数据，这在实际应用中是有限的，由于用户无法进行自我标注。为了解决这些限制，我们提出了一种新的多模态FL框架，该框架使用半指导学习方法，以利用不同模式的表示。将这个概念引入系统，我们开发了一种基于储革的多模态嵌入知识传递机制，即FedMEKT，该机制让服务器和客户端可以交换归一化的知识。我们的FedMEKT通过迭代更新通用全球编码器，以获取参与客户端的共同归一化知识。因此，我们的提议的FedMEKT包括本地多模态自动编码学习、通用多模态自动编码器建构和通用分类学习。通过对三个多模态人动识别数据集进行广泛的实验，我们证明了FedMEKT可以在线评估中 достичь更高的全球编码器性能，保护用户隐私和个人数据，同时减少通信成本。
</details></li>
</ul>
<hr>
<h2 id="Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories"><a href="#Transferability-of-Graph-Neural-Networks-using-Graphon-and-Sampling-Theories" class="headerlink" title="Transferability of Graph Neural Networks using Graphon and Sampling Theories"></a>Transferability of Graph Neural Networks using Graphon and Sampling Theories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13206">http://arxiv.org/abs/2307.13206</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Martina Neuman, Jason J. Bramburger</li>
<li>for: 本研究旨在应用graphon来提高graph neural network（GNN）的可转移性。</li>
<li>methods: 本研究使用了two-layer graphon neural network（WNN）架构，并证明了其能够高效地近似带限信号。</li>
<li>results: 研究表明，使用WNN架构可以在不同图形式的数据上保持高度的表现，而且可以在不同图大小之间进行可转移学习。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs and overcomes issues related to the curse of dimensionality that arise in other GNN results. The proposed WNN and GNN architectures offer practical solutions for handling graph data of varying sizes while maintaining performance guarantees without extensive retraining.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已成为处理图形信息的有力工具。一个愿望的特性是转移性，其中训练好的网络可以将图形信息交换到另一个图形上，而不需要重新训练，并保持准确性。一种最近提出的捕捉GNNs的转移性的方法是通过图拟函数（graphons），它们是对大量紧凑图的极限函数。在这个工作中，我们对GNNs中的图拟函数应用了Explicit two-layer graphon neural network（WNN）架构。我们证明了它可以在给定的误差范围内近似离散信号，使用最小的网络重量。然后，我们利用这个结论，以证明Explicit two-layer GNN在所有足够大的图上的转移性。我们的工作解决了对权重图和简单随机图之间的转移性问题，并超越了其他GNN结果中的尺度繁殖问题。提出的WNN和GNN架构为处理图数据的不同大小而提供了实用的解决方案，无需进行广泛的重新训练。
</details></li>
</ul>
<hr>
<h2 id="Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis"><a href="#Federated-Distributionally-Robust-Optimization-with-Non-Convex-Objectives-Algorithm-and-Analysis" class="headerlink" title="Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis"></a>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14364">http://arxiv.org/abs/2307.14364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Jiao, Kai Yang, Dongjin Song</li>
<li>For:	+ The paper aims to solve the federated distributionally robust optimization (FDRO) problem, which is to find an optimal decision that minimizes the worst-case cost over the ambiguity set of probability distributions in a distributed environment.* Methods:	+ The proposed algorithm is called Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE).	+ The algorithm leverages the prior distribution using a new uncertainty set called constrained D-norm uncertainty set.* Results:	+ The proposed algorithm is guaranteed to converge and the iteration complexity is analyzed.	+ Extensive empirical studies on real-world datasets demonstrate that the proposed method can achieve fast convergence, remain robust against data heterogeneity and malicious attacks, and trade off robustness with performance.<details>
<summary>Abstract</summary>
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness. Finally, our theoretical analysis elucidates that the proposed algorithm is guaranteed to converge and the iteration complexity is also analyzed. Extensive empirical studies on real-world datasets demonstrate that the proposed method can not only achieve fast convergence, and remain robust against data heterogeneity as well as malicious attacks, but also tradeoff robustness with performance.
</details>
<details>
<summary>摘要</summary>
Distributionally Robust Optimization (DRO) 目标是找到最佳决策，以最小化不确定性集中的最差成本。这种技术在多个应用中广泛使用，如网络行为分析、风险管理等。然而，现有的 DRO 技术面临三大挑战：1）如何在分布式环境中 asynchronous 更新; 2）如何有效地利用先前分布; 3）如何适当调整不确定性中的度量。为此，我们提出了一个异步分布式算法，名为异步单脚拟合gradient projection（ASPIRE）算法，并与itErative Active SEt方法（EASE）结合以解决联邦分布式不确定优化（FDRO）问题。此外，我们还开发了一个新的不确定集，即受限的 D-norm 不确定集，以有效地利用先前分布并flexibly控制不确定性度量。最后，我们的理论分析表明，提案的算法可以保证收敛，并且迭代复杂性也进行了分析。实际实验表明，提案的方法可以不仅快速收敛，同时也能够对数据不一致和恶意攻击具有抗性。
</details></li>
</ul>
<hr>
<h2 id="An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO"><a href="#An-Investigation-into-Glomeruli-Detection-in-Kidney-H-E-and-PAS-Images-using-YOLO" class="headerlink" title="An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO"></a>An Investigation into Glomeruli Detection in Kidney H&amp;E and PAS Images using YOLO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13199">http://arxiv.org/abs/2307.13199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">https://github.com/AlexeyAB/darknet</a></li>
<li>paper_authors: Kimia Hemmatirad, Morteza Babaie, Jeffrey Hodgin, Liron Pantanowitz, H. R. Tizhoosh<br>for: This paper aims to assist pathologists in detecting glomeruli in human kidney images using computerized solutions, specifically by proposing an automated tissue structure detection and segmentation method using the YOLO-v4 object detector.methods: The YOLO-v4 model was trained on whole slide images and fine-tuned on a private dataset from the University of Michigan for glomeruli detection. Multiple experiments were conducted using different training data and stains.results: The model achieved average specificity and sensitivity for all experiments and outperformed existing segmentation methods on the same datasets. However, the design and validation for different stains still depend on the variability of public multi-stain datasets.<details>
<summary>Abstract</summary>
Context: Analyzing digital pathology images is necessary to draw diagnostic conclusions by investigating tissue patterns and cellular morphology. However, manual evaluation can be time-consuming, expensive, and prone to inter- and intra-observer variability. Objective: To assist pathologists using computerized solutions, automated tissue structure detection and segmentation must be proposed. Furthermore, generating pixel-level object annotations for histopathology images is expensive and time-consuming. As a result, detection models with bounding box labels may be a feasible solution. Design: This paper studies. YOLO-v4 (You-Only-Look-Once), a real-time object detector for microscopic images. YOLO uses a single neural network to predict several bounding boxes and class probabilities for objects of interest. YOLO can enhance detection performance by training on whole slide images. YOLO-v4 has been used in this paper. for glomeruli detection in human kidney images. Multiple experiments have been designed and conducted based on different training data of two public datasets and a private dataset from the University of Michigan for fine-tuning the model. The model was tested on the private dataset from the University of Michigan, serving as an external validation of two different stains, namely hematoxylin and eosin (H&E) and periodic acid-Schiff (PAS). Results: Average specificity and sensitivity for all experiments, and comparison of existing segmentation methods on the same datasets are discussed. Conclusions: Automated glomeruli detection in human kidney images is possible using modern AI models. The design and validation for different stains still depends on variability of public multi-stain datasets.
</details>
<details>
<summary>摘要</summary>
Context: 分析数字 PATHOLOGY 图像是必要的，以便从investigating 组织模式和细胞形态中提取诊断结论。然而，手动评估可能会 consume 时间和成本，并且存在 между观察者和内部观察者的差异。目标：使用计算机化解决方案，自动检测和分割组织结构。此外，为 Histopathology 图像生成像素级对象标注是昂贵和时间consuming。因此，使用 bounding box 标签的检测模型可能是一个可行的解决方案。Design：本文研究了 YOLO-v4（You-Only-Look-Once），一种实时物体检测器，用于微scopic 图像。YOLO 使用单个神经网络预测多个 bounding box 和对象类概率。YOLO 可以通过训练整个扫描图像来提高检测性能。本文使用 YOLO-v4 进行了 glomeruli 检测在人脏肾图像中。基于两个公共数据集和一个由美国密歇根大学提供的私人数据集进行了多个实验，并对模型进行了微调。模型在密歇根大学私人数据集上进行了测试，作为对两种染色物（HE和PAS）的EXternal 验证。Results：本文的结果显示，使用 YOLO-v4 可以自动检测人脏肾中的 glomeruli。average 特异性和敏感性的讨论，以及现有的分 segmentation 方法在同一个数据集上的比较。Conclusions：使用现代 AI 模型，自动检测人脏肾中的 glomeruli 是可能的。然而，设计和验证不同染色物的方法仍然виси于多个公共多染色物数据集的变化。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy"><a href="#Knowledge-enhanced-Neuro-Symbolic-AI-for-Cybersecurity-and-Privacy" class="headerlink" title="Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy"></a>Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02031">http://arxiv.org/abs/2308.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritran Piplai, Anantaa Kotal, Seyedreza Mohseni, Manas Gaur, Sudip Mittal, Anupam Joshi</li>
<li>for: This paper is written to explore the potential of Neuro-Symbolic Artificial Intelligence (AI) in addressing the challenges of explainability and safety in AI systems, particularly in the domains of cybersecurity and privacy.</li>
<li>methods: The paper uses a combination of neural networks and symbolic knowledge graphs to integrate the strengths of both approaches, enabling AI systems to reason, learn, and generalize in a manner understandable to experts.</li>
<li>results: The paper demonstrates the potential of Neuro-Symbolic AI to improve the explainability and safety of AI systems in complex environments, specifically in the domains of cybersecurity and privacy.<details>
<summary>Abstract</summary>
Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly advancing field that combines the subsymbolic strengths of (deep) neural networks and explicit, symbolic knowledge contained in knowledge graphs to enhance explainability and safety in AI systems. This approach addresses a key criticism of current generation systems, namely their inability to generate human-understandable explanations for their outcomes and ensure safe behaviors, especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity, privacy). The integration of neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts. This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI.
</details>
<details>
<summary>摘要</summary>
neuromorphic artificial intelligence (AI) 是一个emerging 和rapidly advancing field，它将SUBSYMBOLIC  strengths of (deep) neural networks 和Explicit, symbolic knowledge contained in knowledge graphs 搭配，以提高AI系统的可解释性和安全性。这种方法解决了当前一代系统的一个批评，即它们无法生成人理解的解释，特别是在“Unknown unknowns”（例如Cybersecurity, privacy）的场景中。 combining neural networks, which excel at exploring complex data spaces, and symbolic knowledge graphs, which represent domain knowledge, allows AI systems to reason, learn, and generalize in a manner understandable to experts。This article describes how applications in cybersecurity and privacy, two most demanding domains in terms of the need for AI to be explainable while being highly accurate in complex environments, can benefit from Neuro-Symbolic AI。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Explanation-Policies-in-RL"><a href="#Counterfactual-Explanation-Policies-in-RL" class="headerlink" title="Counterfactual Explanation Policies in RL"></a>Counterfactual Explanation Policies in RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13192">http://arxiv.org/abs/2307.13192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shripad V. Deshmukh, Srivatsan R, Supriti Vijay, Jayakumar Subramanian, Chirag Agarwal</li>
<li>for: 这个论文旨在探讨如何使用对假设的解释来分析RL策略，以便更好地理解RL策略的含义。</li>
<li>methods: 该论文提出了一种新的Counterpol方法，通过在RL中 incorporating counterfactuals in supervised learning，以生成对策略的解释。</li>
<li>results: 实验结果表明，Counterpol可以帮助分析RL策略，并且可以在不同的状态和动作空间中工作。<details>
<summary>Abstract</summary>
As Reinforcement Learning (RL) agents are increasingly employed in diverse decision-making problems using reward preferences, it becomes important to ensure that policies learned by these frameworks in mapping observations to a probability distribution of the possible actions are explainable. However, there is little to no work in the systematic understanding of these complex policies in a contrastive manner, i.e., what minimal changes to the policy would improve/worsen its performance to a desired level. In this work, we present COUNTERPOL, the first framework to analyze RL policies using counterfactual explanations in the form of minimal changes to the policy that lead to the desired outcome. We do so by incorporating counterfactuals in supervised learning in RL with the target outcome regulated using desired return. We establish a theoretical connection between Counterpol and widely used trust region-based policy optimization methods in RL. Extensive empirical analysis shows the efficacy of COUNTERPOL in generating explanations for (un)learning skills while keeping close to the original policy. Our results on five different RL environments with diverse state and action spaces demonstrate the utility of counterfactual explanations, paving the way for new frontiers in designing and developing counterfactual policies.
</details>
<details>
<summary>摘要</summary>
“在增强学习（Reinforcement Learning，RL）Agent increasingly 应用于多元决策问题中，使得Policy learned by these frameworks in mapping observations to a probability distribution of possible actions的可读性变得越来越重要。然而，有很少关于这些复杂的策略的系统性理解，即对策略的改进或恶化的最小变化。在这种情况下，我们提出了COUNTERPOL，第一个使用对假设的批评性解释来分析RL策略的框架。我们通过在RL中 incorporating counterfactuals in supervised learning with the target outcome regulated using desired return来实现这一点。我们还证明了COUNTERPOL和常用的信任区基于RL策略优化方法之间的理论联系。我们的实验结果表明，COUNTERPOL能够生成高效的解释，并保持与原始策略几乎相同。我们在五个不同的RL环境中进行了Extensive empirical analysis，这些环境包括多种状态和动作空间。我们的结果表明，对假设的批评性解释可以帮助RL Agent 学习和改进策略，开辟出新的前iers in designing and developing counterfactual policies。”Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning"><a href="#Neural-Memory-Decoding-with-EEG-Data-and-Representation-Learning" class="headerlink" title="Neural Memory Decoding with EEG Data and Representation Learning"></a>Neural Memory Decoding with EEG Data and Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13181">http://arxiv.org/abs/2307.13181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Glenn Bruns, Michael Haidar, Federico Rubino</li>
<li>for: 本研究描述了一种使用神经网络快速解oding记忆的方法，用于从EEG数据中提取记忆。</li>
<li>methods: 该方法使用深度表示学习，并使用supervised contrastive损失来将EEG记录转换到低维度空间中。</li>
<li>results: 该方法可以在EEG数据中准确地识别概念，其准确率为About 78.4%（ chance 4%）。此外，该方法还应用于信息检索问题，可以生成基于EEG数据的预测文档列表。<details>
<summary>Abstract</summary>
We describe a method for the neural decoding of memory from EEG data. Using this method, a concept being recalled can be identified from an EEG trace with an average top-1 accuracy of about 78.4% (chance 4%). The method employs deep representation learning with supervised contrastive loss to map an EEG recording of brain activity to a low-dimensional space. Because representation learning is used, concepts can be identified even if they do not appear in the training data set. However, reference EEG data must exist for each such concept. We also show an application of the method to the problem of information retrieval. In neural information retrieval, EEG data is captured while a user recalls the contents of a document, and a list of links to predicted documents is produced.
</details>
<details>
<summary>摘要</summary>
我们描述了一种使用神经网络对电enzephalogram（EEG）数据进行记忆解oding的方法。使用这种方法，在EEG追踪记录中检测到的概念可以与高达78.4%的准确率（Random 4%）进行匹配。该方法利用深度表示学习和监督对比损失来将EEG记录的脑动activty映射到低维度空间中。由于使用表示学习，即使概念没有出现在训练数据集中，也可以正确地识别出它们。然而，每个概念都需要相应的参照EEG数据。我们还展示了该方法的应用于信息检索问题。在神经信息检索中，EEG数据被记录在用户回忆文档内容时，并生成一个预测文档列表。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates"><a href="#Evaluating-the-reliability-of-automatically-generated-pedestrian-and-bicycle-crash-surrogates" class="headerlink" title="Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates"></a>Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13178">http://arxiv.org/abs/2307.13178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnimitra Sengupta, S. Ilgin Guler, Vikash V. Gayah, Shannon Warchol</li>
<li>For: This study aims to assess the reliability of automatically generated surrogates in predicting confirmed conflicts involving vulnerable road users (VRUs) at signalized intersections.* Methods: The study uses a video-based event monitoring system to collect data on VRU and motor vehicle interactions at 15 signalized intersections in Pennsylvania. Advanced data-driven models are used to analyze the surrogate data, including automatically collectable variables such as speeds, movements, and post-encroachment time, as well as manually collected variables like signal states, lighting, and weather conditions.* Results: The findings highlight the varying importance of specific surrogates in predicting true conflicts, with some being more informative than others. The results can assist transportation agencies in prioritizing infrastructure investments, such as bike lanes and crosswalks, and evaluating their effectiveness.<details>
<summary>Abstract</summary>
Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU and motor vehicle interactions at fifteen signalized intersections across Pennsylvania to improve VRU safety performance. This research builds on that study to assess the reliability of automatically generated surrogates in predicting confirmed conflicts using advanced data-driven models. The surrogate data used for analysis include automatically collectable variables such as vehicular and VRU speeds, movements, post-encroachment time, in addition to manually collected variables like signal states, lighting, and weather conditions. The findings highlight the varying importance of specific surrogates in predicting true conflicts, some being more informative than others. The findings can assist transportation agencies to collect the right types of data to help prioritize infrastructure investments, such as bike lanes and crosswalks, and evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
易受损的路用者（VRU），如行人和自行车客，与机动车在交通冲突中更容易发生事故，事故也更可能导致严重的伤害或死亡。信号控制的交叉口是VRU安全的主要问题，因为它们的复杂性和动态性使得需要更好地了解路用者与机动车之间的互动，并采取基于证据的减少措施以提高安全性。与机动车相撞的事故 rarely occurs，这使得Difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Using a video-based system to automatically detect these conflicts is a crucial step in developing smart infrastructure to enhance VRU safety.在美国宾夕法尼亚州交通部的一项研究中，使用视频基本监测系统评估了全州十五个信号控制交叉口中VRU和机动车之间的互动，以提高VRU安全性。本研究基于这项研究，以自动生成的代理数据来评估确定的冲突。代理数据包括自动收集的变量，如机动车和VRU的速度、移动、后续时间，以及手动收集的变量，如信号状态、灯光和天气情况。研究发现代理数据中的特定变量在预测真正的冲突中扮演着不同的重要性。这些发现可以帮助交通部门收集适当的数据，以便优先投入基础设施投资，如自行车道和横渡道，并评估其效果。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields"><a href="#Unsupervised-reconstruction-of-accelerated-cardiac-cine-MRI-using-Neural-Fields" class="headerlink" title="Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields"></a>Unsupervised reconstruction of accelerated cardiac cine MRI using Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14363">http://arxiv.org/abs/2307.14363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tabita Catalán, Matías Courdurier, Axel Osses, René Botnar, Francisco Sahli Costabal, Claudia Prieto</li>
<li>for: 这项研究的目的是提出一种无监督的深度学习方法来加速卡ди亚维度MRI的重建。</li>
<li>methods: 该方法基于启发型神经场表示，并在实验中使用金字塔型多晶维度收集器进行受损样本收集。</li>
<li>results: 实验结果表明，该方法可以在实验室中实现高质量的卡ди亚维度MRI重建，并且比传统方法更具有空间和时间表示能力。<details>
<summary>Abstract</summary>
Cardiac cine MRI is the gold standard for cardiac functional assessment, but the inherently slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions. Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.
</details>
<details>
<summary>摘要</summary>
卡ди亚金枪MRI是心脏功能评估的标准，但它的自然slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions.  Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.Here's the translation in Traditional Chinese:卡迪亚金枪MRI是心脏功能评估的标准，但它的自然slow acquisition process creates the necessity of reconstruction approaches for accelerated undersampled acquisitions.  Several regularization approaches that exploit spatial-temporal redundancy have been proposed to reconstruct undersampled cardiac cine MRI. More recently, methods based on supervised deep learning have been also proposed to further accelerate acquisition and reconstruction. However, these techniques rely on usually large dataset for training, which are not always available. In this work, we propose an unsupervised approach based on implicit neural field representations for cardiac cine MRI (so called NF-cMRI). The proposed method was evaluated in in-vivo undersampled golden-angle radial multi-coil acquisitions for undersampling factors of 26x and 52x, achieving good image quality, and comparable spatial and improved temporal depiction than a state-of-the-art reconstruction technique.
</details></li>
</ul>
<hr>
<h2 id="Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching"><a href="#Multi-UAV-Speed-Control-with-Collision-Avoidance-and-Handover-aware-Cell-Association-DRL-with-Action-Branching" class="headerlink" title="Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching"></a>Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell Association: DRL with Action Branching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13158">http://arxiv.org/abs/2307.13158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijiang Yan, Wael Jaafar, Bassant Selim, Hina Tabassum</li>
<li>for: 提高运输和通信性能，包括碰撞避免、连接稳定和交换。</li>
<li>methods: 使用深度强化学习解决多架空交通干道上UAV协调决策和移动速度优化问题，形式化为Markov决策过程（MDP），UAV状态定义为速度和通信数据率。提议一种神经网络架构，具有共享决策模块和多个网络分支，每个分支专门处理特定的行动维度在2D交通通信空间。</li>
<li>results: 通过 simulation 结果表明，与现有 refer  bench 比较，该方法可以提高18.32%。<details>
<summary>Abstract</summary>
This paper presents a deep reinforcement learning solution for optimizing multi-UAV cell-association decisions and their moving velocity on a 3D aerial highway. The objective is to enhance transportation and communication performance, including collision avoidance, connectivity, and handovers. The problem is formulated as a Markov decision process (MDP) with UAVs' states defined by velocities and communication data rates. We propose a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design efficiently handles the multi-dimensional action space, allowing independence for individual action dimensions. We introduce two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.
</details>
<details>
<summary>摘要</summary>
To address the multi-dimensional action space, the proposed solution features a neural architecture with a shared decision module and multiple network branches, each dedicated to a specific action dimension in a 2D transportation-communication space. This design enables independence for individual action dimensions.Two models, Branching Dueling Q-Network (BDQ) and Branching Dueling Double Deep Q-Network (Dueling DDQN), are introduced to demonstrate the approach. Simulation results show a significant improvement of 18.32% compared to existing benchmarks.translate to 简化中文 as follows:这篇论文提出了一种深度强化学习解决方案，用于优化多架航空器（UAV）的维度协调决策和移动速度在3D空中高速公路上。目标是提高交通和通信性能，包括避免冲突、连接和交换。问题被形式化为一个Markov决策过程（MDP），UAV的状态被定义为速度和通信数据速率。为了处理多维动作空间，提议的解决方案具有一个共享决策模块和多个网络分支，每个分支专门处理一个特定的动作维度在2D交通通信空间中。这种设计允许每个动作维度独立进行决策。提议的解决方案还 introduce了两种模型：分支决策网络（BDQ）和分支决策双深度网络（DDQN），以示解决方案。实验结果显示，与现有 referential 比较，提议的方案具有18.32%的显著提高。
</details></li>
</ul>
<hr>
<h2 id="Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions"><a href="#Discovering-interpretable-elastoplasticity-models-via-the-neural-polynomial-method-enabled-symbolic-regressions" class="headerlink" title="Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions"></a>Discovering interpretable elastoplasticity models via the neural polynomial method enabled symbolic regressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13149">http://arxiv.org/abs/2307.13149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahador Bahmani, Hyoung Suk Suh, WaiChing Sun</li>
<li>for: 这篇论文旨在提出一种两步机器学习方法，以帮助解释神经网络模型的含义。</li>
<li>methods: 这种方法首先使用超级vised学习获得单变量特征映射，然后使用符号回归将这些映射转化为数学模型。</li>
<li>results: 这种方法可以解决神经网络模型的解释性问题，同时提供了一些优点，如缩放问题的解决和代码的可重用性。<details>
<summary>Abstract</summary>
Conventional neural network elastoplasticity models are often perceived as lacking interpretability. This paper introduces a two-step machine-learning approach that returns mathematical models interpretable by human experts. In particular, we introduce a surrogate model where yield surfaces are expressed in terms of a set of single-variable feature mappings obtained from supervised learning. A postprocessing step is then used to re-interpret the set of single-variable neural network mapping functions into mathematical form through symbolic regression. This divide-and-conquer approach provides several important advantages. First, it enables us to overcome the scaling issue of symbolic regression algorithms. From a practical perspective, it enhances the portability of learned models for partial differential equation solvers written in different programming languages. Finally, it enables us to have a concrete understanding of the attributes of the materials, such as convexity and symmetries of models, through automated derivations and reasoning. Numerical examples have been provided, along with an open-source code to enable third-party validation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Scalability: The symbolic regression algorithms are able to handle large datasets.2. Portability: The learned models can be easily integrated into partial differential equation solvers written in different programming languages.3. Interpretability: The approach provides a concrete understanding of the attributes of the materials, such as convexity and symmetries of models, through automated derivations and reasoning.Numerical examples are provided, along with open-source code for third-party validation.</details></li>
</ol>
<hr>
<h2 id="Learnable-wavelet-neural-networks-for-cosmological-inference"><a href="#Learnable-wavelet-neural-networks-for-cosmological-inference" class="headerlink" title="Learnable wavelet neural networks for cosmological inference"></a>Learnable wavelet neural networks for cosmological inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14362">http://arxiv.org/abs/2307.14362</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chris-pedersen/learnablewavelets">https://github.com/chris-pedersen/learnablewavelets</a></li>
<li>paper_authors: Christian Pedersen, Michael Eickenberg, Shirley Ho</li>
<li>for:  cosmological inference and marginalisation over astrophysical effects</li>
<li>methods: 使用可学习散射转换，一种基于 convolutional neural network 的trainable wavelets滤波器</li>
<li>results: 比 CNN 更高效，特别是对小训练数据样本; 提供可读的干扰网络Here’s the breakdown of each point:1. for: The paper is written for the purpose of cosmological inference and marginalisation over astrophysical effects using the learnable scattering transform.2. methods: The paper uses the learnable scattering transform, which is a type of convolutional neural network that utilizes trainable wavelets as filters, to perform cosmological inference and marginalisation over astrophysical effects.3. results: The paper shows that scattering architectures can outperform a convolutional neural network (CNN) in terms of performance, especially when dealing with small training data samples. Additionally, the paper presents a lightweight scattering network that is highly interpretable.<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have been shown to both extract more information than the traditional two-point statistics from cosmological fields, and marginalise over astrophysical effects extremely well. However, CNNs require large amounts of training data, which is potentially problematic in the domain of expensive cosmological simulations, and it is difficult to interpret the network. In this work we apply the learnable scattering transform, a kind of convolutional neural network that uses trainable wavelets as filters, to the problem of cosmological inference and marginalisation over astrophysical effects. We present two models based on the scattering transform, one constructed for performance, and one constructed for interpretability, and perform a comparison with a CNN. We find that scattering architectures are able to outperform a CNN, significantly in the case of small training data samples. Additionally we present a lightweight scattering network that is highly interpretable.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）能够提取更多的信息于 cosmological 场的传统两点统计，并且很好地把astrophysical效应卷积。但是，CNN需要大量的训练数据，这可能是costly cosmological simulations中的问题，而且很难 interpret the network。在这个工作中，我们使用可学习的散射变换，一种基于trainable wavelets的卷积神经网络，来解决 cosmological inference和astrophysical effects的卷积。我们提出了两种基于散射变换的模型，一种是 для性能，另一种是 для可读性，并与CNN进行比较。我们发现，散射架构能够超过CNN，特别是在小训练样本情况下。此外，我们还提出了一个轻量级的散射网络，具有很好的可读性。
</details></li>
</ul>
<hr>
<h2 id="Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework"><a href="#Extending-Path-Dependent-NJ-ODEs-to-Noisy-Observations-and-a-Dependent-Observation-Framework" class="headerlink" title="Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework"></a>Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent Observation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13147">http://arxiv.org/abs/2307.13147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/floriankrach/pd-njode">https://github.com/floriankrach/pd-njode</a></li>
<li>paper_authors: William Andersson, Jakob Heiss, Florian Krach, Josef Teichmann</li>
<li>for: 预测连续时间随机过程，具有不规则和部分观测。</li>
<li>methods: 使用Path-Dependent Neural Jump ODE（PD-NJ-ODE）模型，学习最佳预测给 irregularly sampled时间序列的受限观测。</li>
<li>results: 提供了两种扩展，使模型能够满足不独立的时间序列和均匀观测，并且提供了理论保证和实际示例。<details>
<summary>Abstract</summary>
The Path-Dependent Neural Jump ODE (PD-NJ-ODE) is a model for predicting continuous-time stochastic processes with irregular and incomplete observations. In particular, the method learns optimal forecasts given irregularly sampled time series of incomplete past observations. So far the process itself and the coordinate-wise observation times were assumed to be independent and observations were assumed to be noiseless. In this work we discuss two extensions to lift these restrictions and provide theoretical guarantees as well as empirical examples for them.
</details>
<details>
<summary>摘要</summary>
“path-dependent neural jump ODE（PD-NJ-ODE）是一种预测连续时间随机过程的模型，特别是在含有异常和缺失观测的时间序列上。在现有的方法中，这种方法学习最佳预测， givens irregularly sampled time series of incomplete past observations。在这篇文章中，我们讨论了两种扩展，以增强这种模型的应用范围和提供理论保证。”Here's a breakdown of the translation:* "Path-Dependent Neural Jump ODE" (PD-NJ-ODE) is translated as "path-dependent neural jump ODE" (PD-NJ-ODE), which is a direct translation of the English name.* "in particular" is translated as "特别是" (tèbié shì), which is a common way to emphasize a specific aspect of the previous statement.* "the method learns optimal forecasts given irregularly sampled time series of incomplete past observations" is translated as "这种方法学习最佳预测， givens irregularly sampled time series of incomplete past observations" (zhè zhǒng fāng yào xué xí zhì yì, gěi yìn zhèng yè qián zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "givens" to clarify the context.* "So far the process itself and the coordinate-wise observation times were assumed to be independent" is translated as "在现有的方法中，这种过程自身和坐标点wise的观测时间被认为是独立的" (zhè zhǒng fāng yào xiàng yì, zhè zhǒng fāng yào xiàng yì zhèng yè qián zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word " coordinate-wise" to clarify the context.* "and observations were assumed to be noiseless" is translated as "并且观测被认为是噪声的" (bèi qǐ zhèng yì, gòu yì zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "噪声" (noise) to clarify the context.* "In this work we discuss two extensions to lift these restrictions" is translated as "在这篇文章中，我们讨论了两种扩展，以增强这种模型的应用范围" (zhè zhǒng fāng yào xiàng yì, wǒmen tiǎo yì zhèng zhèng yīn zhèng yè qián zhèng yǐ jīn zhèng yè qián zhèng). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "扩展" (extension) to clarify the context.* "and provide theoretical guarantees as well as empirical examples for them" is translated as "并提供理论保证，以及实证示例" (bèi tīng yì zhèng, yǐng qǐ zhèng yì). This translation maintains the structure of the original sentence, but uses more formal language and adds the word "理论保证" (theoretical guarantees) to clarify the context.
</details></li>
</ul>
<hr>
<h2 id="Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization"><a href="#Does-Progress-On-Object-Recognition-Benchmarks-Improve-Real-World-Generalization" class="headerlink" title="Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?"></a>Does Progress On Object Recognition Benchmarks Improve Real-World Generalization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13136">http://arxiv.org/abs/2307.13136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Megan Richards, Polina Kirichenko, Diane Bouchacourt, Mark Ibrahim<br>for:* 这种研究旨在测试对象识别模型在不同地区的普遍性。methods:* 使用了两个 datasets of objects from households across the globe，并进行了大量的实验研究。results:* 发现标准 benchmarks 不能准确度量实际世界中的普遍性，而实际的地区差异导致模型的性能差异较大。* 发现规模alone 不能 garantate 实际世界中的一致性，而在早期实验中，简单地 retrained 最后一层的数据可以减少地区差异。<details>
<summary>Abstract</summary>
For more than a decade, researchers have measured progress in object recognition on ImageNet-based generalization benchmarks such as ImageNet-A, -C, and -R. Recent advances in foundation models, trained on orders of magnitude more data, have begun to saturate these standard benchmarks, but remain brittle in practice. This suggests standard benchmarks, which tend to focus on predefined or synthetic changes, may not be sufficient for measuring real world generalization. Consequently, we propose studying generalization across geography as a more realistic measure of progress using two datasets of objects from households across the globe. We conduct an extensive empirical evaluation of progress across nearly 100 vision models up to most recent foundation models. We first identify a progress gap between standard benchmarks and real-world, geographical shifts: progress on ImageNet results in up to 2.5x more progress on standard generalization benchmarks than real-world distribution shifts. Second, we study model generalization across geographies by measuring the disparities in performance across regions, a more fine-grained measure of real world generalization. We observe all models have large geographic disparities, even foundation CLIP models, with differences of 7-20% in accuracy between regions. Counter to modern intuition, we discover progress on standard benchmarks fails to improve geographic disparities and often exacerbates them: geographic disparities between the least performant models and today's best models have more than tripled. Our results suggest scaling alone is insufficient for consistent robustness to real-world distribution shifts. Finally, we highlight in early experiments how simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work, reducing geographic disparity on both benchmarks by over two-thirds.
</details>
<details>
<summary>摘要</summary>
We conducted an extensive empirical evaluation of progress across nearly 100 vision models, including the most recent foundation models. Our results reveal a significant gap between progress on ImageNet and real-world, geographical shifts. Specifically, we found that progress on ImageNet results in up to 2.5 times more progress on standard generalization benchmarks than real-world distribution shifts.Furthermore, we studied model generalization across geographies by measuring the disparities in performance across regions, providing a more fine-grained measure of real-world generalization. Our findings show that all models, including foundation CLIP models, exhibit large geographic disparities, with differences of 7-20% in accuracy between regions. Surprisingly, we found that progress on standard benchmarks does not improve geographic disparities and often exacerbates them. In fact, the geographic disparities between the least performant models and today's best models have more than tripled.Our results suggest that scaling alone is insufficient for achieving consistent robustness to real-world distribution shifts. However, we discovered that simple last layer retraining on more representative, curated data can complement scaling as a promising direction of future work. By doing so, we were able to reduce geographic disparity on both benchmarks by over two-thirds. Our findings highlight the importance of considering real-world distribution shifts when evaluating progress in object recognition and provide a new direction for future research.
</details></li>
</ul>
<hr>
<h2 id="simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects"><a href="#simPLE-a-visuotactile-method-learned-in-simulation-to-precisely-pick-localize-regrasp-and-place-objects" class="headerlink" title="simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects"></a>simPLE: a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13133">http://arxiv.org/abs/2307.13133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Bauza, Antonia Bronars, Yifan Hou, Ian Taylor, Nikhil Chavan-Dafle, Alberto Rodriguez</li>
<li>for: 这篇论文旨在解决 robotic manipulation 中的一般性和精度之间的矛盾。</li>
<li>methods: 该论文提出了一种叫做 simPLE 的解决方案，用于精度的 pick-and-place 任务。 simPLE 包括三个主要组成部分：任务意识 grasping，视听感知和重新抓取规划。</li>
<li>results: 在一个 dual-arm 机器人上，使用 simPLE 可以成功地完成 15 种不同的物体的 pick-and-place 任务，其中 6 种物体的成功率高于 90%，而剩下 11 种物体的成功率高于 80%。视频可以在 <a target="_blank" rel="noopener" href="http://mcube.mit.edu/research/simPLE.html">http://mcube.mit.edu/research/simPLE.html</a> 上查看。<details>
<summary>Abstract</summary>
Existing robotic systems have a clear tension between generality and precision. Deployed solutions for robotic manipulation tend to fall into the paradigm of one robot solving a single task, lacking precise generalization, i.e., the ability to solve many tasks without compromising on precision. This paper explores solutions for precise and general pick-and-place. In precise pick-and-place, i.e. kitting, the robot transforms an unstructured arrangement of objects into an organized arrangement, which can facilitate further manipulation. We propose simPLE (simulation to Pick Localize and PLacE) as a solution to precise pick-and-place. simPLE learns to pick, regrasp and place objects precisely, given only the object CAD model and no prior experience. We develop three main components: task-aware grasping, visuotactile perception, and regrasp planning. Task-aware grasping computes affordances of grasps that are stable, observable, and favorable to placing. The visuotactile perception model relies on matching real observations against a set of simulated ones through supervised learning. Finally, we compute the desired robot motion by solving a shortest path problem on a graph of hand-to-hand regrasps. On a dual-arm robot equipped with visuotactile sensing, we demonstrate pick-and-place of 15 diverse objects with simPLE. The objects span a wide range of shapes and simPLE achieves successful placements into structured arrangements with 1mm clearance over 90% of the time for 6 objects, and over 80% of the time for 11 objects. Videos are available at http://mcube.mit.edu/research/simPLE.html .
</details>
<details>
<summary>摘要</summary>
现有的 роботиче系统存在一种明显的矛盾：一个机器人可以解决一个特定任务，但缺乏高级别的总体化能力，即能够解决多个任务而不失去精度。这篇论文探讨精度和通用性的pick-and-place解决方案。在精度的pick-and-place任务中，机器人将无结构的物品排序成结构化的排序，以便进一步的操作。我们提出了simPLE（从模拟到选择和置入）解决方案，该方案可以帮助机器人准确地找到、重新抓取和置入物品，只需要物品的CAD模型，无需互联网经验。我们开发了三个主要组成部分：任务意识 grasping、视听感知和重新抓取规划。任务意识 grasping计算可行的抓取方式，以确保稳定、可见和置入的情况。视听感知模型通过对实际观察与 simulate 的对比，通过超过学习来学习实际观察。最后，我们解决了一个短路问题，以计算手中重新抓取的最佳动作。在配备视听感知的双臂机器人上，我们用simPLE实现了15种多样化的物品的pick-and-place任务，物品的形状覆盖广泛，simPLE在90%的时间内成功地将物品置入结构化的排序中，距离1毫米。视频可以在http://mcube.mit.edu/research/simPLE.html 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning"><a href="#A-Differentially-Private-Weighted-Empirical-Risk-Minimization-Procedure-and-its-Application-to-Outcome-Weighted-Learning" class="headerlink" title="A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning"></a>A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13127">http://arxiv.org/abs/2307.13127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Giddens, Yiwang Zhou, Kevin R. Krull, Tara M. Brinkman, Peter X. K. Song, Fang Liu</li>
<li>For: 这个论文的目的是提出一种在使用敏感数据时实现隐私保护的可靠学习方法，以解决数据隐私issue。* Methods: 该论文使用了 differential privacy（DP）框架，并提出了一种基于 weights ERM 的首个具有隐私保证的算法，以及一种对现有 DP-ERM 方法的推广。* Results: 实验研究表明，通过在 wERM 中应用 DP 保证可以train OWL 模型，而不会导致模型性能下降。这种隐私保护的 OWL 方法在实验中得到了证明，并在实际临床试验中得到了应用。<details>
<summary>Abstract</summary>
It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to deriving privacy-preserving learning methods for individualized treatment rules, including the popular outcome weighted learning (OWL). We evaluate the performance of the DP-wERM application to OWL in a simulation study and in a real clinical trial of melatonin for sleep health. All empirical results demonstrate the viability of training OWL models via wERM with DP guarantees while maintaining sufficiently useful model performance. Therefore, we recommend practitioners consider implementing the proposed privacy-preserving OWL procedure in real-world scenarios involving sensitive data.
</details>
<details>
<summary>摘要</summary>
通常使用包含个人信息的数据来构建预测模型在预测风险最小化（ERM）框架中。虽然这些模型可以在预测上具有非常高的准确性，但使用敏感数据时可能会遭受隐私攻击。不等式隐私（DP）是一个吸引人的框架，可以为隐私泄露提供数学上可证明的约束。前一个研究主要集中在应用DP于不加权的ERM。我们考虑了一个重要的扩展，即加权ERM（wERM）。在wERM中，每个个体的对象函数中的贡献可以被分配不同的权重。在这种情况下，我们提出了首个具有DP保证的wERM算法，并提供了在轻度Regularity Conditions下的理论证明。通过扩展现有的DP-ERM过程，我们开辟了一条privacy-preserving学习方法的道路，包括流行的结果权重学习（OWL）。我们在一个Simulation Study和一个实际的临床试验中评估了DP-wERM应用于OWL的性能。所有实验结果表明，可以通过wERM WITH DP保证来训练OWL模型，而不会丧失有用的模型性能。因此，我们建议实践者在涉及敏感数据的实际场景中考虑实施我们提议的隐私保护OWL过程。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe"><a href="#A-Hybrid-Machine-Learning-Model-for-Classifying-Gene-Mutations-in-Cancer-using-LSTM-BiLSTM-CNN-GRU-and-GloVe" class="headerlink" title="A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe"></a>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14361">http://arxiv.org/abs/2307.14361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanad Aburass, Osama Dorgham, Jamil Al Shaqsi</li>
<li>for: 这个研究旨在使用Kaggle的个性化医学：重新定义癌症治疗数据集来类型基因变化。</li>
<li>methods: 该模型 combinig LSTM, BiLSTM, CNN, GRU, 以及GloVe来进行基因变化分类。</li>
<li>results: 该模型在准确率、精度、回归率、F1分数和平均方差方面与其他模型进行比较，并取得了最高的表现。此外，它还需要较少的训练时间，从而实现了性能和效率的完美协同。<details>
<summary>Abstract</summary>
This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and GloVe to classify gene mutations using Kaggle's Personalized Medicine: Redefining Cancer Treatment dataset. The results were compared against well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and their LSTM ensembles. Our model outperformed all other models in terms of accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it also needed less training time, resulting in a perfect combination of performance and efficiency. This study demonstrates the utility of ensemble models for difficult tasks such as gene mutation classification.
</details>
<details>
<summary>摘要</summary>
Note:* LSTM: Long Short-Term Memory* BiLSTM: Bidirectional Long Short-Term Memory* CNN: Convolutional Neural Network* GRU: Gated Recurrent Unit* GloVe: Global Vectors for Word Representation* BERT: Bidirectional Encoder Representations from Transformers* Electra: Efficient Lifelong End-to-End Text Recognition* Roberta: Robustly Optimized BERT Pretraining Approach* XLNet: Extreme Language Modeling* Distilbert: Distilled BERTPlease note that the translation is in Simplified Chinese, and the words and phrases in bold are the names of the models and techniques used in the study.
</details></li>
</ul>
<hr>
<h2 id="Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons"><a href="#Deep-Bradley-Terry-Rating-Quantifying-Properties-from-Comparisons" class="headerlink" title="Deep Bradley-Terry Rating: Quantifying Properties from Comparisons"></a>Deep Bradley-Terry Rating: Quantifying Properties from Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13709">http://arxiv.org/abs/2307.13709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satoru Fujii</li>
<li>for: 这篇论文的目的是为了量化和评估未知对象的属性。</li>
<li>methods: 这篇论文使用了深度学习框架，并将布莱德利-泰勒模型 integrate 到 neural network 结构中。此外，它还推广到不平等环境下，以便更好地应用于实际场景。</li>
<li>results: 经过实验分析，这篇论文成功地量化和估算了欲要的属性。<details>
<summary>Abstract</summary>
Many properties in the real world can't be directly observed, making them difficult to learn. To deal with this challenging problem, prior works have primarily focused on estimating those properties by using graded human scores as the target label in the training. Meanwhile, rating algorithms based on the Bradley-Terry model are extensively studied to evaluate the competitiveness of players based on their match history. In this paper, we introduce the Deep Bradley-Terry Rating (DBTR), a novel machine learning framework designed to quantify and evaluate properties of unknown items. Our method seamlessly integrates the Bradley-Terry model into the neural network structure. Moreover, we generalize this architecture further to asymmetric environments with unfairness, a condition more commonly encountered in real-world settings. Through experimental analysis, we demonstrate that DBTR successfully learns to quantify and estimate desired properties.
</details>
<details>
<summary>摘要</summary>
多种Properties在现实世界中难以直接观察，这使得它们学习变得困难。以前的工作主要通过使用排名作为目标标签进行估算这些Properties。而BRADLEY-TERRY模型的评分算法在评估玩家的竞争力方面得到了广泛的研究。在这篇论文中，我们引入了深度BRADLEY-TERRY评分（DBTR），一种新的机器学习框架，用于评估和评价未知项目的属性。我们将BRADLEY-TERRY模型与神经网络结构结合，并将其扩展到偏袋环境中，以适应实际世界中更常见的不平等条件。通过实验分析，我们证明了DBTR成功地学习和估算所需的属性。
</details></li>
</ul>
<hr>
<h2 id="Conformal-prediction-for-frequency-severity-modeling"><a href="#Conformal-prediction-for-frequency-severity-modeling" class="headerlink" title="Conformal prediction for frequency-severity modeling"></a>Conformal prediction for frequency-severity modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13124">http://arxiv.org/abs/2307.13124</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/heltongraziadei/conformal-fs">https://github.com/heltongraziadei/conformal-fs</a></li>
<li>paper_authors: Helton Graziadei, Paulo C. Marques F., Eduardo F. L. de Melo, Rodrigo S. Targino</li>
<li>for: 预测保险索赔数量</li>
<li>methods: 非参数模型无关框架、分割兼容预测、基于出袋机制的适应宽度调整</li>
<li>results: 适用于实际数据集和仿真数据集，可以生成具有finite sample statistcial guarantees的预测间隔<details>
<summary>Abstract</summary>
We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.
</details>
<details>
<summary>摘要</summary>
我们提出了一种非参数化、模型无关的框架，用于构建保险索赔预测范围，具有有限样本统计保证，基于分割哲学预测技术扩展到两个阶段频率严重模型预测领域。我们通过使用 simulate 和实际数据示例，证明了该框架的效iveness。当下面际严重模型是随机森林时，我们对两个阶段分割哲学预测过程进行扩展，并示出了如何通过尝试机制来消除需要Calibration集和生成适应宽度的预测范围。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment"><a href="#An-Explainable-Geometric-Weighted-Graph-Attention-Network-for-Identifying-Functional-Networks-Associated-with-Gait-Impairment" class="headerlink" title="An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment"></a>An Explainable Geometric-Weighted Graph Attention Network for Identifying Functional Networks Associated with Gait Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13108">http://arxiv.org/abs/2307.13108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/favour-nerrise/xgw-gat">https://github.com/favour-nerrise/xgw-gat</a></li>
<li>paper_authors: Favour Nerrise, Qingyu Zhao, Kathleen L. Poston, Kilian M. Pohl, Ehsan Adeli<br>for:这个研究旨在更好地理解帕金森病（PD）的运动进程，特别是步行障碍和平衡问题的发展。通过识别大脑功能失调的特征，可以更好地理解PD的运动进程，从而开发更有效和个性化的治疗方法。methods:这个研究使用了一种可解释的、几何学的、质量权重 graphs attention neural network（xGW-GAT），用于预测帕金森病患者的步行障碍级别。xGW-GAT使用了函数连接矩阵来表示整个连接网络，并使用个性化的注意力mask来提取个体和群体水平的解释。results:这个研究发现，xGW-GAT在resting-state功能磁共振成像（rs-fMRI）数据集中的帕金森病患者中出色地预测了步行障碍级别，并提供了解释性的功能子网络结构。与现有的方法相比，xGW-GAT模型成功地超越了其他方法，同时揭示了临床有关的连接模式。<details>
<summary>Abstract</summary>
One of the hallmark symptoms of Parkinson's Disease (PD) is the progressive loss of postural reflexes, which eventually leads to gait difficulties and balance problems. Identifying disruptions in brain function associated with gait impairment could be crucial in better understanding PD motor progression, thus advancing the development of more effective and personalized therapeutics. In this work, we present an explainable, geometric, weighted-graph attention neural network (xGW-GAT) to identify functional networks predictive of the progression of gait difficulties in individuals with PD. xGW-GAT predicts the multi-class gait impairment on the MDS Unified PD Rating Scale (MDS-UPDRS). Our computational- and data-efficient model represents functional connectomes as symmetric positive definite (SPD) matrices on a Riemannian manifold to explicitly encode pairwise interactions of entire connectomes, based on which we learn an attention mask yielding individual- and group-level explainability. Applied to our resting-state functional MRI (rs-fMRI) dataset of individuals with PD, xGW-GAT identifies functional connectivity patterns associated with gait impairment in PD and offers interpretable explanations of functional subnetworks associated with motor impairment. Our model successfully outperforms several existing methods while simultaneously revealing clinically-relevant connectivity patterns. The source code is available at https://github.com/favour-nerrise/xGW-GAT .
</details>
<details>
<summary>摘要</summary>
一个典型的parkinson病（PD）的表现之一是慢慢地失去姿态反射，这会导致步态困难和平衡问题。确定潜在的脑功能干预在步态困难方面可能是理解PD motor进程的关键，从而开发更有效和个性化的治疗方法。在这项工作中，我们提出了一种可解释的、几何的、质量权重的神经网络（xGW-GAT），用于预测PD患者的步态困难级别。xGW-GAT预测了MDS联合PD评估rating scale（MDS-UPDRS）中的多个步态困难类型。我们的计算和数据有效的模型将功能连接矩阵（connectome）表示为对称正定的矩阵（SPD），并在RIemannian manifold上进行Explicitly编码对整个connectome的对称对应关系，基于这些对应关系我们学习一个注意力mask，以获得个体和组级别的解释。应用于我们的resting-state功能MRI（rs-fMRI）数据集中的PD患者，xGW-GAT确定了与步态困难相关的功能连接模式，并提供了可解释的功能子网络相关于运动障碍。我们的模型成功击败了一些现有的方法，同时揭示了临床有用的连接模式。模型代码可以在https://github.com/favour-nerrise/xGW-GAT 上获取。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Example-Based-Control"><a href="#Contrastive-Example-Based-Control" class="headerlink" title="Contrastive Example-Based Control"></a>Contrastive Example-Based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13101">http://arxiv.org/abs/2307.13101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khatch31/laeo">https://github.com/khatch31/laeo</a></li>
<li>paper_authors: Kyle Hatch, Benjamin Eysenbach, Rafael Rafailov, Tianhe Yu, Ruslan Salakhutdinov, Sergey Levine, Chelsea Finn</li>
<li>for: 本研究旨在提出一种基于例子的离线控制方法，可以学习多步转移的隐藏模型，而不需要 specify 奖励函数。</li>
<li>methods: 本方法使用了数据驱动的方法，通过从转移动力学中采样得到的例子来学习隐藏模型，并使用这些模型来预测转移的Q值。</li>
<li>results: 相比基elines，本方法在多个状态基于和图像基于的离线控制任务中表现出色，并且在数据集大小增加时显示了更好的稳定性和扩展性。<details>
<summary>Abstract</summary>
While many real-world problems that might benefit from reinforcement learning, these problems rarely fit into the MDP mold: interacting with the environment is often expensive and specifying reward functions is challenging. Motivated by these challenges, prior work has developed data-driven approaches that learn entirely from samples from the transition dynamics and examples of high-return states. These methods typically learn a reward function from high-return states, use that reward function to label the transitions, and then apply an offline RL algorithm to these transitions. While these methods can achieve good results on many tasks, they can be complex, often requiring regularization and temporal difference updates. In this paper, we propose a method for offline, example-based control that learns an implicit model of multi-step transitions, rather than a reward function. We show that this implicit model can represent the Q-values for the example-based control problem. Across a range of state-based and image-based offline control tasks, our method outperforms baselines that use learned reward functions; additional experiments demonstrate improved robustness and scaling with dataset size.
</details>
<details>
<summary>摘要</summary>
虽然许多实际问题可以受惠于强化学习，但这些问题很少遵循MDP模型：与环境交互通常是昂贵的，并且指定奖励函数是困难的。为了解决这些挑战，先前的工作已经开发了基于数据的方法，这些方法通过从转移动态和高返回状态中提取样本来学习。这些方法通常会学习一个奖励函数从高返回状态，使用这个奖励函数来标注转移，然后应用在这些转移上的离线RL算法。虽然这些方法可以在许多任务上达到良好的结果，但它们可能是复杂的，经常需要规则化和时间差更新。在这篇论文中，我们提出了一种离线、示例基于的控制方法，这种方法学习了多步转移的隐藏模型，而不是奖励函数。我们示示了这个隐藏模型可以表示示例基于的控制问题中的Q值。在一系列的状态基本和图像基本的离线控制任务上，我们的方法超过了基于学习的奖励函数的基eline，其他实验还证明了我们的方法具有更好的 Robustness 和数据集大小增长。
</details></li>
</ul>
<hr>
<h2 id="Label-Noise-Correcting-a-Correction"><a href="#Label-Noise-Correcting-a-Correction" class="headerlink" title="Label Noise: Correcting a Correction"></a>Label Noise: Correcting a Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13100">http://arxiv.org/abs/2307.13100</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Toner, Amos Storkey</li>
<li>for: addressing the issue of overfitting in training neural network classifiers on datasets with label noise</li>
<li>methods: proposing a more direct approach to mitigate overfitting by imposing a lower bound on the empirical risk during training</li>
<li>results: providing theoretical results with explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions, and demonstrating significant enhancement in robustness with virtually no additional computational cost.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究旨在解决对于训练神经网络分类器的标签杂读问题</li>
<li>methods: 我们提出了一种更直接的方法，通过在训练过程中对实验风险下设置下限，以遏止过拟合</li>
<li>results: 我们提供了具体、容易计算的下限 bounds，以及实验结果，显示这种方法可以帮助提高神经网络分类器的Robustness，而且无需额外的计算成本。<details>
<summary>Abstract</summary>
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将神经网络分类器训练数据集中的标签噪声可能会导致模型过度适应。为解决这个问题，研究人员已经探索了一些alternative的损失函数，以增强模型的Robustness。然而，许多这些alternative都是HEURISTIC的 Natur，可能会导致过度适应或者下降。在这项工作中，我们提出了一种更直接的方法来处理标签噪声导致的过度适应。我们发现，标签噪声存在一个下界，这个下界对于不同的损失函数来说都是可能的。基于这个发现，我们提议在训练过程中尝试在Empirical risk下设置下界，以避免过度适应。我们的主要贡献是提供了理论结果，可以给出不同损失函数的明确、容易计算的下界，以降低过度适应的风险。我们的实验结果表明，使用这些下界可以在不同的设置下提高模型的Robustness，而且几乎没有额外的计算成本。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example"><a href="#Interpretable-Ensemble-Learning-for-Materials-Property-Prediction-with-Classical-Interatomic-Potentials-Carbon-as-an-Example" class="headerlink" title="Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example"></a>Interpretable Ensemble Learning for Materials Property Prediction with Classical Interatomic Potentials: Carbon as an Example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10818">http://arxiv.org/abs/2308.10818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Jiang, Haofan Sun, Kamal Choudhary, Houlong Zhuang, Qiong Nian</li>
<li>for: 这篇论文目的是用机器学习（ML）技术预测晶体材料的性能。</li>
<li>methods: 该方法使用了回归树 ensemble learning，不需要任何描述符，直接使用分子动力学计算的物理性质作为输入。</li>
<li>results: 结果显示，对于碳杂合物的小样本数据，ensemble learning的预测结果比使用传统的分子动力学potential更准确，并且能够捕捉9种不同的分子动力学potential中的相对准确性。<details>
<summary>Abstract</summary>
Machine learning (ML) is widely used to explore crystal materials and predict their properties. However, the training is time-consuming for deep-learning models, and the regression process is a black box that is hard to interpret. Also, the preprocess to transfer a crystal structure into the input of ML, called descriptor, needs to be designed carefully. To efficiently predict important properties of materials, we propose an approach based on ensemble learning consisting of regression trees to predict formation energy and elastic constants based on small-size datasets of carbon allotropes as an example. Without using any descriptor, the inputs are the properties calculated by molecular dynamics with 9 different classical interatomic potentials. Overall, the results from ensemble learning are more accurate than those from classical interatomic potentials, and ensemble learning can capture the relatively accurate properties from the 9 classical potentials as criteria for predicting the final properties.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）广泛应用于探索晶体材料和预测其性能。然而，训练深度学习模型需时间consuming， regression 过程是一个难以解释的黑盒子。此外，将晶体结构转换为 ML 的输入，即描述符，需要仔细设计。为了有效预测材料的重要性能，我们提出了基于集成学习的方法，包括回归树来预测基于小型 datasets of carbon allotropes 的形成能gy和弹性常数。无需使用任何描述符，输入是通过分子动力学计算的物理性质。总的来说， ensemble 学习的结果比 классиical interatomic potentials 更加准确，并且 ensemble 学习可以捕捉来自 nine classical potentials 的相对准确性作为预测最终性能的标准。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Under-Demographic-Scarce-Regime"><a href="#Fairness-Under-Demographic-Scarce-Regime" class="headerlink" title="Fairness Under Demographic Scarce Regime"></a>Fairness Under Demographic Scarce Regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13081">http://arxiv.org/abs/2307.13081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrik Joslin Kenfack, Samira Ebrahimi Kahou, Ulrich Aïvodji</li>
<li>for: 本研究旨在 Addressing the limitation of prior works on fairness, which assume full access to demographic information, but in reality, demographic information may be partially available or unavailable due to privacy concerns.</li>
<li>methods: 本研究提出了一个 attribute classifier 建构框架，通过将不确定性写入模型中，并在有demographic信息的样本上强制施行公平性约束，以提高公平精度贡献。</li>
<li>results: 经过实验显示，提出的框架可以实现更好的公平精度贡献，并且超越了使用真实敏感特征的模型。此外，模型还能够在没有demographic信息的情况下提供更好的公平精度贡献。<details>
<summary>Abstract</summary>
Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.
</details>
<details>
<summary>摘要</summary>
现有大多数工作假设模型拥有完整的人口信息。然而，有些场景下人口信息部分可用，例如记录不完整或因隐私原因无法获取。这种情况被称为人口缺乏 regime。先前的研究表明，使用代理敏感特征来代替缺失的敏感特征可以改善公平。然而，使用代理敏感特征会对公平精度负面影响比使用真实的敏感特征更大。为解决这些限制，我们提出了一个框架，用于建立具有更好的公平精度负面影响的特征分类器。我们的方法在特征分类器中引入了不确定性意识，并在具有最低不确定性的人口信息上遵循公平约束。我们的实验表明，对不确定的敏感特征进行公平约束是对公平和准确性的负面影响。我们的方法在两个数据集上进行了实验，并显示了与经典特征分类器相比，我们的框架可以获得显著更好的公平精度负面影响。另外，我们的方法还超过使用约束的真实敏感特征模型。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs"><a href="#Adaptive-Certified-Training-Towards-Better-Accuracy-Robustness-Tradeoffs" class="headerlink" title="Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs"></a>Adaptive Certified Training: Towards Better Accuracy-Robustness Tradeoffs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13078">http://arxiv.org/abs/2307.13078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhakshylyk Nurlanov, Frank R. Schmidt, Florian Bernard</li>
<li>for: 提高模型的强健性和标准准确率之间的质量衡量。</li>
<li>methods: 基于适应证明的半径的训练方法，通过提高模型的准确率和强健性来实现更好的准确率-强健性质量衡量。</li>
<li>results: 在MNIST、CIFAR-10和TinyImageNet dataset上，提出的方法可以实现更高的强健性和标准准确率之间的质量衡量，特别是在CIFAR-10和TinyImageNet dataset上，模型的强健性可以提高至两倍的水平，而且保持同等水平的标准准确率。<details>
<summary>Abstract</summary>
As deep learning models continue to advance and are increasingly utilized in real-world systems, the issue of robustness remains a major challenge. Existing certified training methods produce models that achieve high provable robustness guarantees at certain perturbation levels. However, the main problem of such models is a dramatically low standard accuracy, i.e. accuracy on clean unperturbed data, that makes them impractical. In this work, we consider a more realistic perspective of maximizing the robustness of a model at certain levels of (high) standard accuracy. To this end, we propose a novel certified training method based on a key insight that training with adaptive certified radii helps to improve both the accuracy and robustness of the model, advancing state-of-the-art accuracy-robustness tradeoffs. We demonstrate the effectiveness of the proposed method on MNIST, CIFAR-10, and TinyImageNet datasets. Particularly, on CIFAR-10 and TinyImageNet, our method yields models with up to two times higher robustness, measured as an average certified radius of a test set, at the same levels of standard accuracy compared to baseline approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="General-Purpose-Multi-Modal-OOD-Detection-Framework"><a href="#General-Purpose-Multi-Modal-OOD-Detection-Framework" class="headerlink" title="General-Purpose Multi-Modal OOD Detection Framework"></a>General-Purpose Multi-Modal OOD Detection Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13069">http://arxiv.org/abs/2307.13069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viet Duong, Qiong Wu, Zhengyi Zhou, Eric Zavesky, Jiahe Chen, Xiangzhou Liu, Wen-Ling Hsu, Huajie Shao</li>
<li>for: 这个研究的目的是为了实现多种不同的假值检测方法，以确保机器学习系统的安全性和可靠性。</li>
<li>methods: 这个研究使用了一个通用的弱监督的假值检测框架，叫做WOOD，它结合了一个二分类器和一个对照学习部分，以获得两者的好处。</li>
<li>results: 实验结果显示，WOOD模型在多个真实世界的数据集上表现出色，能够同时在三个不同的假值enario中实现高准确性。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection identifies test samples that differ from the training data, which is critical to ensuring the safety and reliability of machine learning (ML) systems. While a plethora of methods have been developed to detect uni-modal OOD samples, only a few have focused on multi-modal OOD detection. Current contrastive learning-based methods primarily study multi-modal OOD detection in a scenario where both a given image and its corresponding textual description come from a new domain. However, real-world deployments of ML systems may face more anomaly scenarios caused by multiple factors like sensor faults, bad weather, and environmental changes. Hence, the goal of this work is to simultaneously detect from multiple different OOD scenarios in a fine-grained manner. To reach this goal, we propose a general-purpose weakly-supervised OOD detection framework, called WOOD, that combines a binary classifier and a contrastive learning component to reap the benefits of both. In order to better distinguish the latent representations of in-distribution (ID) and OOD samples, we adopt the Hinge loss to constrain their similarity. Furthermore, we develop a new scoring metric to integrate the prediction results from both the binary classifier and contrastive learning for identifying OOD samples. We evaluate the proposed WOOD model on multiple real-world datasets, and the experimental results demonstrate that the WOOD model outperforms the state-of-the-art methods for multi-modal OOD detection. Importantly, our approach is able to achieve high accuracy in OOD detection in three different OOD scenarios simultaneously. The source code will be made publicly available upon publication.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）检测可以识别测试样本与训练数据之间的差异，这是机器学习（ML）系统的安全性和可靠性的关键。虽然大量方法已经开发出来检测uni-modal OOD样本，但只有一些关注多模态 OOD 检测。当前的对比学习基于方法主要在一个给定的图像和其相应的文本描述来自新领域的情况下进行多模态 OOD 检测。然而，实际世界中 ML 系统的部署可能会遇到更多的异常情况，如感知器故障、坏天气和环境变化。因此，本研究的目标是同时从多个不同的 OOD 场景中进行细致的检测。为达到这个目标，我们提出了一种通用的弱监督 OOD 检测框架，称为 WOOD，该框架结合了一个二分类器和一个对比学习组件，以便充分利用它们的优势。为了更好地分别 ID 和 OOD 样本的幂本表示，我们采用了缺角损失来约束它们的相似性。此外，我们开发了一个新的评分指标，以集成 binary 分类器和对比学习的预测结果，以便更好地识别 OOD 样本。我们对多个实际世界数据集进行了实验，结果显示，提出的 WOOD 模型在多modal OOD 检测中高度超越了现状的方法。特别是，我们的方法能够同时高精度地识别 OOD 样本在三个不同的 OOD 场景中。代码将在发表后公开。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations"><a href="#Personalized-Category-Frequency-prediction-for-Buy-It-Again-recommendations" class="headerlink" title="Personalized Category Frequency prediction for Buy It Again recommendations"></a>Personalized Category Frequency prediction for Buy It Again recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01195">http://arxiv.org/abs/2308.01195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Pande, Kunal Ghosh, Rankyung Park</li>
<li>for: 提高用户体验和站点参与度，预测用户可能会购买的商品</li>
<li>methods: 提出一种层次PCIC模型，包括个性化类别模型（PC模型）和个性化类别内项模型（IC模型），模型使用存活模型和时间序列模型生成特征，然后使用类别划分的神经网络进行训练</li>
<li>results: 与12个基线比较，PCIC在四个标准开放数据集上提高了NDCG达16%，同时提高了回归率约2%，并在大规模数据集上进行了扩展和训练（超过8小时），并在一家大商户的官方网站上进行了AB测试，导致了用户参与度的显著提高<details>
<summary>Abstract</summary>
Buy It Again (BIA) recommendations are crucial to retailers to help improve user experience and site engagement by suggesting items that customers are likely to buy again based on their own repeat purchasing patterns. Most existing BIA studies analyze guests personalized behavior at item granularity. A category-based model may be more appropriate in such scenarios. We propose a recommendation system called a hierarchical PCIC model that consists of a personalized category model (PC model) and a personalized item model within categories (IC model). PC model generates a personalized list of categories that customers are likely to purchase again. IC model ranks items within categories that guests are likely to consume within a category. The hierarchical PCIC model captures the general consumption rate of products using survival models. Trends in consumption are captured using time series models. Features derived from these models are used in training a category-grained neural network. We compare PCIC to twelve existing baselines on four standard open datasets. PCIC improves NDCG up to 16 percent while improving recall by around 2 percent. We were able to scale and train (over 8 hours) PCIC on a large dataset of 100M guests and 3M items where repeat categories of a guest out number repeat items. PCIC was deployed and AB tested on the site of a major retailer, leading to significant gains in guest engagement.
</details>
<details>
<summary>摘要</summary>
Buy It Again (BIA) 建议是重要的 для零售商，帮助改善用户体验和网站参与度，通过建议客户可能会再次购买的商品，基于他们自己的重复购买模式。大多数现有的BIA研究分析客人个性化行为的项目粒度。我们提出一种推荐系统，即层次PCIC模型，包括个性化类别模型（PC模型）和个性化类别内容模型（IC模型）。PC模型生成个性化的类别列表，客户可能会购买的类别。IC模型将类别内容排名，客户可能会在类别内消耗的商品。层次PCIC模型捕捉产品的总消耗率，使用存生模型捕捉消耗趋势。这些特征被用于训练分类器。我们与十二个基准值进行比较，PCIC提高了NDCG达16%，同时提高了回归率约2%。我们可以在8小时内扩展和训练PCIC（超过100万客户和300万个商品），其中客户重复购买的类别大于客户重复购买的商品。PCIC在一家大型零售商的官方网站上进行了部署和AB测试，导致用户参与度显著提高。
</details></li>
</ul>
<hr>
<h2 id="Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction"><a href="#Feature-Gradient-Flow-for-Interpreting-Deep-Neural-Networks-in-Head-and-Neck-Cancer-Prediction" class="headerlink" title="Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction"></a>Feature Gradient Flow for Interpreting Deep Neural Networks in Head and Neck Cancer Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13061">http://arxiv.org/abs/2307.13061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinzhu Jin, Jonathan C. Garneau, P. Thomas Fletcher</li>
<li>for: 这篇论文旨在介绍一种新的深度学习模型解释技术，即通过计算模型的梯度流来解释模型做出决策时使用的特征。</li>
<li>methods: 该技术使用了计算模型的梯度流来定义输入数据空间中的非线性坐标，从而解释模型做出决策时使用的信息。然后，通过比较特征的梯度流度量和基线噪声特征的梯度流度量来评估特征的重要性。</li>
<li>results: 在使用了该技术进行训练后，模型的解释性得到了提高。研究人员通过计算模型的梯度流来评估特征的重要性，并发现了一些有用的特征，例如肿瘤大小和形态等。<details>
<summary>Abstract</summary>
This paper introduces feature gradient flow, a new technique for interpreting deep learning models in terms of features that are understandable to humans. The gradient flow of a model locally defines nonlinear coordinates in the input data space representing the information the model is using to make its decisions. Our idea is to measure the agreement of interpretable features with the gradient flow of a model. To then evaluate the importance of a particular feature to the model, we compare that feature's gradient flow measure versus that of a baseline noise feature. We then develop a technique for training neural networks to be more interpretable by adding a regularization term to the loss function that encourages the model gradients to align with those of chosen interpretable features. We test our method in a convolutional neural network prediction of distant metastasis of head and neck cancer from a computed tomography dataset from the Cancer Imaging Archive.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文介绍了一种新的技术，即特征涟流，可以使深度学习模型变得更加解释性。特征涟流定义了模型在输入数据空间中的非线性坐标，表示模型做出决策的信息。我们的方法是测量特定的可解释特征与模型涟流的一致程度，并评估特定特征对模型的重要性。我们还开发了一种方法，通过添加一个减少项到损失函数中，使模型的梯度与选择的可解释特征的梯度进行对齐。我们在计算tomography数据集中预测头颈癌 distant metastasis的 convolutional neural network中测试了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning"><a href="#MARIO-Model-Agnostic-Recipe-for-Improving-OOD-Generalization-of-Graph-Contrastive-Learning" class="headerlink" title="MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning"></a>MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13055">http://arxiv.org/abs/2307.13055</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhuyun97/mario">https://github.com/zhuyun97/mario</a></li>
<li>paper_authors: Yun Zhu, Haizhou Shi, Zhenshuo Zhang, Siliang Tang</li>
<li>for: 本文研究的问题是非标注图数据上的非标注泛化（Out-of-distribution, OOD）泛化问题，特别是图神经网络（Graph Neural Network, GNN）在分布转移时的敏感性问题。</li>
<li>methods: 我们提出了一种名为MARIO的模型无关的热革命方法，用于改进非标注图谱离散学习方法的OOD泛化性能。 MARIO包括两个原则：信息瓶颈（Information Bottleneck, IB）原则以实现泛化表示，以及不变原则，通过对敏感数据进行对抗数据增强来获得不变表示。</li>
<li>results: 我们通过广泛的实验表明，我们的方法可以在OOD测试集上实现状态之最的性能，而与现有方法相比，在标注测试集上保持相似的性能。代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/ZhuYun97/MARIO%E3%80%82">https://github.com/ZhuYun97/MARIO。</a><details>
<summary>Abstract</summary>
In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem of graph contrastive learning, with a specific focus on node-level tasks. Through extensive experiments, we demonstrate that our method achieves state-of-the-art performance on the OOD test set, while maintaining comparable performance on the in-distribution test set when compared to existing approaches. The source code for our method can be found at: https://github.com/ZhuYun97/MARIO
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了无监督学习方法在图数据上的 OUT-OF-DISTRIBUTION（OOD）泛化问题。这种情况特别是挑战性的，因为图神经网络（GNNs）已经被证明是 Distributional Shifts 敏感的，即使标签可用。为了解决这个挑战，我们提出了一种名为 MARIO 的模型无关的照片，用于提高无监督图对比学习方法的 OOD 泛化性。MARIO 包括两个原则：(i) 信息瓶颈（IB）原则，以实现泛化表示，以及(ii) 不变原则，通过对数据进行对抗式数据增强来获得不变表示。根据我们所知，这是首次研究 OOD 泛化问题的图对比学习方法，具体注重节点级任务。通过广泛的实验，我们证明了我们的方法在 OOD 测试集上具有最佳性能，而且与现有方法相比，在同一个测试集上保持了相似的性能。MARIO 的源代码可以在 GitHub 上找到：https://github.com/ZhuYun97/MARIO。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation"><a href="#Parallel-Q-Learning-Scaling-Off-policy-Reinforcement-Learning-under-Massively-Parallel-Simulation" class="headerlink" title="Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation"></a>Parallel $Q$-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12983">http://arxiv.org/abs/2307.12983</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Improbable-AI/pql">https://github.com/Improbable-AI/pql</a></li>
<li>paper_authors: Zechu Li, Tao Chen, Zhang-Wei Hong, Anurag Ajay, Pulkit Agrawal</li>
<li>For: 这篇论文是关于加速复杂任务的强化学习的研究，通过使用大量的训练数据来提高模型的性能。* Methods: 这篇论文使用了Isaac Gym提供的GPU基于的 simulate系统，通过并行收集数据、策略学习和价值学习来提高强化学习的效率。* Results: 这篇论文提出了一种并行$Q$-学习（PQL）方案，可以在几个工作站上并行进行数据收集、策略学习和价值学习，从而提高强化学习的效率。在实验中，PQL方案可以扩展到数以千计的并行环境，并调查了学习速度的重要因素。<details>
<summary>Abstract</summary>
Reinforcement learning is time-consuming for complex tasks due to the need for large amounts of training data. Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to \textit{tens of thousands of parallel environments} and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.
</details>
<details>
<summary>摘要</summary>
强化学习因为复杂任务而需要大量训练数据，Recent advances in GPU-based simulation, such as Isaac Gym, have sped up data collection thousands of times on a commodity GPU. Most prior works used on-policy methods like PPO due to their simplicity and ease of scaling. Off-policy methods are more data efficient but challenging to scale, resulting in a longer wall-clock training time. This paper presents a Parallel $Q$-Learning (PQL) scheme that outperforms PPO in wall-clock time while maintaining superior sample efficiency of off-policy learning. PQL achieves this by parallelizing data collection, policy learning, and value learning. Different from prior works on distributed off-policy learning, such as Apex, our scheme is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. In experiments, we demonstrate that $Q$-learning can be scaled to 万�� nombreux parallel environments and investigate important factors affecting learning speed. The code is available at https://github.com/Improbable-AI/pql.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="3D-LLM-Injecting-the-3D-World-into-Large-Language-Models"><a href="#3D-LLM-Injecting-the-3D-World-into-Large-Language-Models" class="headerlink" title="3D-LLM: Injecting the 3D World into Large Language Models"></a>3D-LLM: Injecting the 3D World into Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12981">http://arxiv.org/abs/2307.12981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UMass-Foundation-Model/3D-LLM">https://github.com/UMass-Foundation-Model/3D-LLM</a></li>
<li>paper_authors: Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan</li>
<li>for: 这个研究想要尝试在大语言模型（LLM）和视力语言模型（VLM）上插入3D世界，以提高这些模型在多个任务上的表现，包括常识推理。</li>
<li>methods: 这个研究使用了三种提示机制，并利用3D特征提取器从渲染的多视图图像中提取3D特征来训练3D语言模型（3D-LLM）。</li>
<li>results: 研究表明，在ScanQA任务上，该模型的表现比状态正常基eline高出9%的BLEU-1分数。此外，在3D描述、任务组合和3D辅助对话等任务上，该模型也表现出优于2D VLM。 Qualitative例子也显示，该模型可以完成跨度外的任务。<details>
<summary>Abstract</summary>
Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi- view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin (e.g., the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https://vis-www.cs.umass.edu/3dllm/.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）和视觉语言模型（VLM）已经被证明可以在多个任务上表现出色，如常识 reasoning。强大的这些模型可以是，它们没有与3D物理世界相关的概念，包括空间关系、可用性、物理学、布局等。在这项工作中，我们提议将3D世界注入到大型语言模型中，并引入一个全新的3D-LLM家族。specifically，3D-LLM可以从3D点云和其特征输入，并完成一系列3D相关任务，包括captioning、dense captioning、3D问答、任务分解、3D定位、3D-assisted dialog、导航等。通过我们设计的三种提示机制，我们能够收集超过300k个3D语言数据覆盖这些任务。为了效率地训练3D-LLM，我们首先利用3D特征提取器从渲染多视图图像中获取3D特征。然后，我们使用2D VLM作为我们的背部来训练我们的3D-LLM。通过引入3D本地化机制，3D-LLM可以更好地捕捉3D空间信息。在ScanQA上进行实验，我们的模型比州态艺术基eline的基eline高出大幅度（例如，BLEU-1分数超过州态艺术基eline的分数 by 9%）。此外，在我们保留的数据集上进行3D captioning、任务组合和3D-assisted dialogue的实验中，我们的模型超过2D VLM。Qualitative例子也表明我们的模型可以完成现有LLM和VLM的任务之外的更多任务。项目页面：https://vis-www.cs.umass.edu/3dllm/.
</details></li>
</ul>
<hr>
<h2 id="An-Isometric-Stochastic-Optimizer"><a href="#An-Isometric-Stochastic-Optimizer" class="headerlink" title="An Isometric Stochastic Optimizer"></a>An Isometric Stochastic Optimizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12979">http://arxiv.org/abs/2307.12979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Jackson</li>
<li>for: 这 paper 的目的是解释 Adam 优化器的成功，并基于这个原理提出一种新的优化器。</li>
<li>methods: 这 paper 使用了一种新的优化器 called Iso，它使每个参数的步长独立于其他参数的 нор。 Additionally, the paper proposes a variant of Iso called IsoAdam, which allows for the transfer of optimal hyperparameters from Adam.</li>
<li>results: 实验结果表明，IsoAdam 在训练一个小型 Transformer 时比 Adam 快速。<details>
<summary>Abstract</summary>
The Adam optimizer is the standard choice in deep learning applications. I propose a simple explanation of Adam's success: it makes each parameter's step size independent of the norms of the other parameters. Based on this principle I derive Iso, a new optimizer which makes the norm of a parameter's update invariant to the application of any linear transformation to its inputs and outputs. I develop a variant of Iso called IsoAdam that allows optimal hyperparameters to be transferred from Adam, and demonstrate that IsoAdam obtains a speedup over Adam when training a small Transformer.
</details>
<details>
<summary>摘要</summary>
《Adam优化器在深度学习应用中是标准选择。我提出了对Adam成功的简单解释：它使每个参数的步长独立于其他参数的norm。基于这个原理，我 derivated一种新的优化器叫做Iso，它使参数更新的 нор免受输入和输出的任何线性变换的影响。我还开发了一种名为IsoAdam的变体，它允许从Adam中传输优化参数，并证明IsoAdam在训练小型Transformer时比Adam快。》Note that Simplified Chinese is used here, which is one of the two standard forms of Chinese writing. Traditional Chinese is the other form, and it may be used in different regions or contexts.
</details></li>
</ul>
<hr>
<h2 id="Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems"><a href="#Provable-Benefits-of-Policy-Learning-from-Human-Preferences-in-Contextual-Bandit-Problems" class="headerlink" title="Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems"></a>Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12975">http://arxiv.org/abs/2307.12975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Ji, Huazheng Wang, Minshuo Chen, Tuo Zhao, Mengdi Wang</li>
<li>for: 这个论文是关于决策问题中的奖励工程。在实际应用中，常有无明显的奖励函数选择的情况，因此引入人工反馈以帮助学习奖励函数的方法得到了广泛应用。</li>
<li>methods: 这个论文使用了人工反馈来学习奖励函数，并且提出了一种基于偏好的方法，该方法在recent empirical applications such as InstructGPT中表现出色。</li>
<li>results: 论文提供了一种理论分析，证明了基于偏好的方法在offline上下文ual bandits中的优势。具体来说，论文提高了运行policy learning方法的人工分类样本的模型和下界分析，并与基于偏好的方法的下界比较，证明了基于偏好的方法具有较低的下界。<details>
<summary>Abstract</summary>
A crucial task in decision-making problems is reward engineering. It is common in practice that no obvious choice of reward function exists. Thus, a popular approach is to introduce human feedback during training and leverage such feedback to learn a reward function. Among all policy learning methods that use human feedback, preference-based methods have demonstrated substantial success in recent empirical applications such as InstructGPT. In this work, we develop a theory that provably shows the benefits of preference-based methods in offline contextual bandits. In particular, we improve the modeling and suboptimality analysis for running policy learning methods on human-scored samples directly. Then, we compare it with the suboptimality guarantees of preference-based methods and show that preference-based methods enjoy lower suboptimality.
</details>
<details>
<summary>摘要</summary>
决策问题中一项非常重要的任务是奖励工程。在实践中，不存在明显的奖励函数选择。因此，一种受欢迎的方法是在训练过程中引入人类反馈，并使用这些反馈学习一个奖励函数。在所有基于策略学习方法中使用人类反馈的方法中，偏好基于方法在最近的实际应用中，如InstructGPT，已经实现了显著的成功。在这项工作中，我们发展了一种理论，证明了偏好基于方法在线上上下文带动机中的优点。具体来说，我们改进了运行策略学习方法直接使用人类评分样本的模型和下optimality分析。然后，我们与偏好基于方法的下optimality保证进行比较，并证明偏好基于方法在下optimality方面具有更低的下optimality。
</details></li>
</ul>
<hr>
<h2 id="Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques"><a href="#Big-Data-Supply-Chain-Management-Framework-for-Forecasting-Data-Preprocessing-and-Machine-Learning-Techniques" class="headerlink" title="Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques"></a>Big Data - Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12971">http://arxiv.org/abs/2307.12971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Jungpil Shin, Istiyaque Ahmed Ridoy, Yoichi Tomioka, M. F. Mridha</li>
<li>For: This paper aims to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies, and to propose a novel framework incorporating Big Data Analytics in SC Management.* Methods: The proposed framework includes problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization. The paper discusses the need for different types of forecasting according to the period or SC objective, and recommends SC KPIs and error-measurement systems to optimize the top-performing model.* Results: The paper illustrates the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning).Here are the three points in Simplified Chinese text:* For: 这篇论文目标是系统地检查和比较现有的供应链（SC）预测策略和技术，并提出一种新的框架，将大数据分析integrated into SC Management。* Methods: 该提案的框架包括问题识别、数据来源、探索数据分析、机器学习模型训练、 гипер参数调整、性能评估和优化。 paper discusses the need for different types of forecasting according to the period or SC objective, and recommends SC KPIs and error-measurement systems to optimize the top-performing model.* Results: paper illustrates the adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning).<details>
<summary>Abstract</summary>
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency have been illustrated. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning). The contribution of this research lies in the standard SC process framework proposal, recommended forecasting data analysis, forecasting effects on SC performance, machine learning algorithms optimization followed, and in shedding light on future research.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇文章的目的是系统地找出当前最佳实践的供应链（SC）预测策略和技术，并提出一种新的框架，该框架包括在SC管理中使用大数据分析。文章讨论了SC预测的数据收集方式和不同类型的预测，以及适用于不同的时间间隔或SC目标。文章还建议了SC指标和错误度量系统，以便优化最佳模型。文章还描述了预测对人工资源、存储和整体SC的影响，以及管理决策的依赖于SC指标以确定模型性能参数和改善运营管理、透明度和规划效率。文章还提出了一种循环连接的框架，该框架包括根据后处理指标进行预处理优化，以及供应链管理、人力决策、成本、生产和容量规划。本文的贡献在于提出了标准SC过程框架、预测数据分析、预测对SC性能的影响、机器学习算法优化和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning"><a href="#A-Connection-between-One-Step-Regularization-and-Critic-Regularization-in-Reinforcement-Learning" class="headerlink" title="A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning"></a>A Connection between One-Step Regularization and Critic Regularization in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12968">http://arxiv.org/abs/2307.12968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ben-eysenbach/ac-connection">https://github.com/ben-eysenbach/ac-connection</a></li>
<li>paper_authors: Benjamin Eysenbach, Matthieu Geist, Sergey Levine, Ruslan Salakhutdinov</li>
<li>for: 这 paper 的目的是解释一些关于 Offline Reinforcement Learning 的问题，包括如何使用一步或多步的策略改进来避免过拟合。</li>
<li>methods: 这 paper 使用了一些不同的方法来进行策略改进，包括 advantage-weighted regression 和 conditional behavioral cloning。它们的主要区别在于，一步方法会在一步的策略改进后停止，而多步方法会通过多个步骤来进行策略改进。</li>
<li>results: 这 paper 的实验结果表明，一步 RL 可以与多步 RL 相比肩，但是它们在不同的问题上的表现可能不同。具体来说，一步 RL 在需要强regularization的问题上可能更 Competitive。<details>
<summary>Abstract</summary>
As with any machine learning problem with limited data, effective offline RL algorithms require careful regularization to avoid overfitting. One-step methods perform regularization by doing just a single step of policy improvement, while critic regularization methods do many steps of policy improvement with a regularized objective. These methods appear distinct. One-step methods, such as advantage-weighted regression and conditional behavioral cloning, truncate policy iteration after just one step. This ``early stopping'' makes one-step RL simple and stable, but can limit its asymptotic performance. Critic regularization typically requires more compute but has appealing lower-bound guarantees. In this paper, we draw a close connection between these methods: applying a multi-step critic regularization method with a regularization coefficient of 1 yields the same policy as one-step RL. While practical implementations violate our assumptions and critic regularization is typically applied with smaller regularization coefficients, our experiments nevertheless show that our analysis makes accurate, testable predictions about practical offline RL methods (CQL and one-step RL) with commonly-used hyperparameters. Our results that every problem can be solved with a single step of policy improvement, but rather that one-step RL might be competitive with critic regularization on RL problems that demand strong regularization.
</details>
<details>
<summary>摘要</summary>
如果机器学习问题受限于数据，有效的离线RL算法需要谨慎的补偿来避免过拟合。一步方法通过做一步策略改进来实现补偿，而批评补偿方法则通过多个步骤的策略改进来实现补偿。这些方法看起来很不同。一步方法，如优点权重回归和Conditional Behavioral Cloning，在策略迭代后 truncate 策略迭代。这个``早期停止''使一步RL简单和稳定，但可能限制其极限性表现。批评补偿通常需要更多的计算，但它具有吸引人的下界保证。在这篇论文中，我们 Draw a close connection between these methods：在应用多步批评补偿方法时，使用补偿系数为1就可以获得与一步RL相同的策略。虽然实际实现可能违反我们的假设，但我们的实验表明，我们的分析可以准确预测实际的离线RL方法（CQL和一步RL）在通用的 гиперпараметры下的表现。我们的结果表明，每个问题都可以通过一步策略改进来解决，但是一步RL可能与批评补偿在RL问题中具有强补偿需求的问题竞争。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dense-Correspondences-between-Photos-and-Sketches"><a href="#Learning-Dense-Correspondences-between-Photos-and-Sketches" class="headerlink" title="Learning Dense Correspondences between Photos and Sketches"></a>Learning Dense Correspondences between Photos and Sketches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12967">http://arxiv.org/abs/2307.12967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogtoolslab/photo-sketch-correspondence">https://github.com/cogtoolslab/photo-sketch-correspondence</a></li>
<li>paper_authors: Xuanchen Lu, Xiaolong Wang, Judith E Fan</li>
<li>for: 这个论文旨在研究计算机系统是如何模拟人类的简图理解能力？</li>
<li>methods: 作者提出了一种基于自我超vised学习的方法，使用尺寸变换网络来估计简图和照片之间的匹配关系。</li>
<li>results: 研究发现，该方法可以超过多个强化基elines，并且与其他抽象级别的方法相比，具有更高的准确率。然而，研究还发现了人类和机器系统之间的差异，提示了更多的研究空间。<details>
<summary>Abstract</summary>
Humans effortlessly grasp the connection between sketches and real-world objects, even when these sketches are far from realistic. Moreover, human sketch understanding goes beyond categorization -- critically, it also entails understanding how individual elements within a sketch correspond to parts of the physical world it represents. What are the computational ingredients needed to support this ability? Towards answering this question, we make two contributions: first, we introduce a new sketch-photo correspondence benchmark, $\textit{PSC6k}$, containing 150K annotations of 6250 sketch-photo pairs across 125 object categories, augmenting the existing Sketchy dataset with fine-grained correspondence metadata. Second, we propose a self-supervised method for learning dense correspondences between sketch-photo pairs, building upon recent advances in correspondence learning for pairs of photos. Our model uses a spatial transformer network to estimate the warp flow between latent representations of a sketch and photo extracted by a contrastive learning-based ConvNet backbone. We found that this approach outperformed several strong baselines and produced predictions that were quantitatively consistent with other warp-based methods. However, our benchmark also revealed systematic differences between predictions of the suite of models we tested and those of humans. Taken together, our work suggests a promising path towards developing artificial systems that achieve more human-like understanding of visual images at different levels of abstraction. Project page: https://photo-sketch-correspondence.github.io
</details>
<details>
<summary>摘要</summary>
人类很自然地理解绘图和现实世界之间的连接，即使绘图非常不真实。此外，人类绘图理解不仅是分类，更重要的是理解绘图中的各个元素与物理世界中的部分之间的对应关系。为解答这个问题，我们提出了两个贡献：首先，我们引入了一个新的绘图-照片对应 bencmark，称为 $\textit{PSC6k}$，包含150万个绘图-照片对应的注释，其中有6250个绘图和125种物品类别。其次，我们提出了一种自动学习的方法，用于学习绘图-照片对应的密集对应关系，基于近期对照片对应学习的进步。我们的模型使用一个空间变换网络来估算绘图和照片的卷积流，从而学习绘图中的各个元素与物理世界中的部分之间的对应关系。我们发现，这种方法比许多强大的基elines表现出色，并且生成的预测值与其他旋转基elines的预测值是量化一致的。然而，我们的benchmark还发现了模型预测值与人类预测值之间的系统性差异。总之，我们的工作建议了一种可能的方法，可以开发出更加人类化的视觉图像理解系统，以达到不同层次的抽象水平。项目页面：https://photo-sketch-correspondence.github.io
</details></li>
</ul>
<hr>
<h2 id="Synthetic-pre-training-for-neural-network-interatomic-potentials"><a href="#Synthetic-pre-training-for-neural-network-interatomic-potentials" class="headerlink" title="Synthetic pre-training for neural-network interatomic potentials"></a>Synthetic pre-training for neural-network interatomic potentials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.15714">http://arxiv.org/abs/2307.15714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jla-gardner/nnp-pre-training">https://github.com/jla-gardner/nnp-pre-training</a></li>
<li>paper_authors: John L. A. Gardner, Kathryn T. Baker, Volker L. Deringer</li>
<li>for: 该研究旨在提高atomistic materials模型中ml基于potential的精度和稳定性，通过使用“synthetic”数据进行预训练。</li>
<li>methods: 研究使用了一种基于图 neural network的equivariant graph-neural-network potential，并通过大量生成的synthetic数据进行预训练，然后 fine-tune 到一个较小的量子力学参考数据集。</li>
<li>results: 研究发现，通过使用synthetic数据进行预训练，可以提高模型的精度和稳定性，并且可以避免一些由量子力学参考数据集的限制。<details>
<summary>Abstract</summary>
Machine learning (ML) based interatomic potentials have transformed the field of atomistic materials modelling. However, ML potentials depend critically on the quality and quantity of quantum-mechanical reference data with which they are trained, and therefore developing datasets and training pipelines is becoming an increasingly central challenge. Leveraging the idea of "synthetic" (artificial) data that is common in other areas of ML research, we here show that synthetic atomistic data, themselves obtained at scale with an existing ML potential, constitute a useful pre-training task for neural-network interatomic potential models. Once pre-trained with a large synthetic dataset, these models can be fine-tuned on a much smaller, quantum-mechanical one, improving numerical accuracy and stability in computational practice. We demonstrate feasibility for a series of equivariant graph-neural-network potentials for carbon, and we carry out initial experiments to test the limits of the approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk"><a href="#Efficiently-Sampling-the-PSD-Cone-with-the-Metric-Dikin-Walk" class="headerlink" title="Efficiently Sampling the PSD Cone with the Metric Dikin Walk"></a>Efficiently Sampling the PSD Cone with the Metric Dikin Walk</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12943">http://arxiv.org/abs/2307.12943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunbum Kook, Santosh S. Vempala</li>
<li>for: 这篇论文关注于半定义程序的效率计算 frontier。</li>
<li>methods: 论文使用了Dikin walk和相关的metric的概念，并提出了一种新的metric的选择方法，以提高混合时间和每步复杂度。</li>
<li>results: 论文的结果表明，使用这种新的metric和混合方法可以大幅降低混合时间和每步复杂度，并且可以将依赖于约束的数量限制为多少。<details>
<summary>Abstract</summary>
Semi-definite programs represent a frontier of efficient computation. While there has been much progress on semi-definite optimization, with moderate-sized instances currently solvable in practice by the interior-point method, the basic problem of sampling semi-definite solutions remains a formidable challenge. The direct application of known polynomial-time algorithms for sampling general convex bodies to semi-definite sampling leads to a prohibitively high running time. In addition, known general methods require an expensive rounding phase as pre-processing. Here we analyze the Dikin walk, by first adapting it to general metrics, then devising suitable metrics for the PSD cone with affine constraints. The resulting mixing time and per-step complexity are considerably smaller, and by an appropriate choice of the metric, the dependence on the number of constraints can be made polylogarithmic. We introduce a refined notion of self-concordant matrix functions and give rules for combining different metrics. Along the way, we further develop the theory of interior-point methods for sampling.
</details>
<details>
<summary>摘要</summary>
semi-definite 计划表示一种高效计算的前沿。虽然有很多进步在半定义优化方面，但基本的半定义解析问题仍然是一项挑战。直接将通用 convex 体的算法应用到半定义 sampling 中会导致非常高的运行时间。此外，已知的通用方法需要费时的舒缩阶段作为先决条件。我们分析了 Dikin 步行，首先适应到通用度量，然后适应 PSD  cone 中的 affine 约束。得到的混合时间和每步复杂度较小，并且通过适当的度量选择，对数量约束的依赖可以被polylogarithmic。我们引入了自适应矩阵函数的更加细化的定义，并给出了不同度量的结合规则。在过程中，我们进一步发展了内点方法的 sampling 理论。
</details></li>
</ul>
<hr>
<h2 id="On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations"><a href="#On-Privileged-and-Convergent-Bases-in-Neural-Network-Representations" class="headerlink" title="On Privileged and Convergent Bases in Neural Network Representations"></a>On Privileged and Convergent Bases in Neural Network Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12941">http://arxiv.org/abs/2307.12941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Brown, Nikhil Vyas, Yamini Bansal</li>
<li>for: 本研究探讨了神经网络学习的表示学习是否具有特权和共同基准。Specifically, we examine the significance of feature directions represented by individual neurons.</li>
<li>methods: 我们使用arbitrary rotations of neural representations来检验神经网络的特权性。我们还比较了由不同随机初始化生成的神经网络的基准。</li>
<li>results: 我们的研究发现，神经网络不 converge to a unique basis，而且基准相关性在各层神经网络中增加了 significannotly。Linear Mode Connectivity也与网络宽度相关，但这种相关性不是由基准相关性增加的。<details>
<summary>Abstract</summary>
In this study, we investigate whether the representations learned by neural networks possess a privileged and convergent basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically.   Furthermore, we analyze Linear Mode Connectivity, which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们研究神经网络学习的表示方式是否具有特权和归一化的基准。我们专门研究神经元个体表达的特征方向的重要性。首先，我们证明神经网络中的表示不能被逆转（不同于线性网络），这表明它们不具有完全的旋转不变性。接着，我们探索是否存在多个基准可以达到同样的性能。为此，我们比较具有相同参数但具有不同随机初始化的网络的基准。我们的研究发现了两个结论：（1）甚至在宽度较大的网络如WideResNets中，神经网络并不会 converges to a unique basis;（2）基准相关性在冻结某些早期层时明显增加。此外，我们分析了Linear Mode Connectivity，这是一种基准相关性的度量。我们的发现表明，Linear Mode Connectivity在网络宽度增加时会提高，但这并不是基准相关性的提高。
</details></li>
</ul>
<hr>
<h2 id="HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar"><a href="#HOOD-Real-Time-Robust-Human-Presence-and-Out-of-Distribution-Detection-with-Low-Cost-FMCW-Radar" class="headerlink" title="HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar"></a>HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02396">http://arxiv.org/abs/2308.02396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach</li>
<li>For: 这种研究旨在实现indoor环境中的人员存在检测，使用60GHz短距离FMCW雷达，并提供一个实时Robust人员存在和非典型检测方法（HOOD）。* Methods: 该方法基于重构建立 architecture，使用60GHz短距离FMCW雷达生成 macro和微距离Doppler图像（RDIs），并通过对RDIs的重构来实现人员存在和非典型检测。* Results: 在基于60GHz短距离FMCW雷达的数据集上，HOOD方法实现了94.36%的平均AUROC水平，并在不同的人类场景下表现良好。此外，HOOD方法也在常见的OOD检测指标上表现出excel。实际实验结果可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://muskahya.github.io/HOOD">https://muskahya.github.io/HOOD</a><details>
<summary>Abstract</summary>
Human presence detection in indoor environments using millimeter-wave frequency-modulated continuous-wave (FMCW) radar is challenging due to the presence of moving and stationary clutters in indoor places. This work proposes "HOOD" as a real-time robust human presence and out-of-distribution (OOD) detection method by exploiting 60 GHz short-range FMCW radar. We approach the presence detection application as an OOD detection problem and solve the two problems simultaneously using a single pipeline. Our solution relies on a reconstruction-based architecture and works with radar macro and micro range-Doppler images (RDIs). HOOD aims to accurately detect the "presence" of humans in the presence or absence of moving and stationary disturbers. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD detection methods in terms of common OOD detection metrics. Our real-time experiments are available at: https://muskahya.github.io/HOOD
</details>
<details>
<summary>摘要</summary>
人体存在检测在室内环境中使用毫米波频率调制连续波（FMCW）雷达是具有挑战性，原因在于室内的移动和静止干扰物。这项工作提出了“HOOD”实时可靠人体存在和非典型检测方法，通过利用60GHz短距离FMCW雷达。我们将存在检测应用作为非典型检测问题，并同时解决两个问题使用单一管道。我们的解决方案基于重建建筑，并与雷达macro和微范围Doppler图像（RDI）结合使用。HOOD hoped to accurately detect human presence in the presence or absence of moving and stationary disturbances. Since it is also an OOD detector, it aims to detect moving or stationary clutters as OOD in humans' absence and predicts the current scene's output as "no presence." HOOD is an activity-free approach that performs well in different human scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD detection methods in terms of common OOD detection metrics. Our real-time experiments are available at: https://muskahya.github.io/HOOD.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries"><a href="#Contextual-Bandits-and-Imitation-Learning-via-Preference-Based-Active-Queries" class="headerlink" title="Contextual Bandits and Imitation Learning via Preference-Based Active Queries"></a>Contextual Bandits and Imitation Learning via Preference-Based Active Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12926">http://arxiv.org/abs/2307.12926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Sekhari, Karthik Sridharan, Wen Sun, Runzhe Wu</li>
<li>for: 本文研究了 Contextual Bandits 和模仿学习问题，learner 缺乏直接行动的奖励信息，而是可以在每个回合 queries 专家，获得偏好反馈。learner 的目标是 minimize 执行动的尴尬吗，同时 minimize  queries 的数量。</li>
<li>methods: 本文提出了一种 Algorithm ，利用在线回归 oracle 来选择行动和决定是否 queries。该 Algorithm 基于函数类型，可以在适当的链接函数下表示专家的偏好模型。</li>
<li>results: 本文证明了该 Algorithm 在 Contextual Bandits 设置下可以获得 $O(\min{\sqrt{T}, d&#x2F;\Delta})$ 的尴尬 regret bound，其中 $T$ 是互动次数，$d$ 是函数类型的 eluder 维度，$\Delta$ 是最佳动作对所有上下文的最小偏好。此外，该 Algorithm 只需要 $O(\min{T, d^2&#x2F;\Delta^2})$ 个 queries。在 imitation learning 设置下，本文也提出了一种 Algorithm，并证明了其在 regret 和 queries 上有类似的 guarantees。 interessingly，该 Algorithm 可以在专家不优秀时，even learn to outperform the underlying expert，这表明了 preference-based feedback 在 imitation learning 中的实际优势。<details>
<summary>Abstract</summary>
We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\{\sqrt{T}, d/\Delta\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\{T, d^2/\Delta^2\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length $H$ each, and provide similar guarantees for regret and query complexity. Interestingly, our algorithm for imitation learning can even learn to outperform the underlying expert, when it is suboptimal, highlighting a practical benefit of preference-based feedback in imitation learning.
</details>
<details>
<summary>摘要</summary>
我们考虑了上下文强化策略和模仿学习问题，learner缺乏直接行动的奖励信息。相反，learner可以在每个回合中活动地询问专家， compare two actions 并获得受损的偏好反馈。learner的目标是二元的：一方面，避免 Executed 动作的后悔，另一方面，避免向专家提问。在这篇文章中，我们假设learner有Function class的存在，可以表示专家的偏好模型，并提供了一个给予online regression oracle 的算法，用于选择动作和决定当 query。 For 上下文强化策略设定，我们的算法可以获得 $O(\min\{\sqrt{T}, d/\Delta\})$ 的后悔 bound，where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts。我们的算法不需要知道 $\Delta$，而且的后悔 bound 与标准上下文强化策略设定，where the learner observes reward signals at each round,相同。此外，我们的算法仅需要 $O(\min\{T, d^2/\Delta^2\})$ 问题给专家。然后，我们延伸我们的算法到模仿学习设定，learner在每个回合中与未知环境进行交互，并提供了相似的后悔和问题复杂性 guarantee。有趣的是，我们的算法可以在模仿学习设定中learn to outperform the underlying expert，当专家是不良的时候，显示了偏好反馈在模仿学习中的实际优点。
</details></li>
</ul>
<hr>
<h2 id="A-new-derivative-free-optimization-method-Gaussian-Crunching-Search"><a href="#A-new-derivative-free-optimization-method-Gaussian-Crunching-Search" class="headerlink" title="A new derivative-free optimization method: Gaussian Crunching Search"></a>A new derivative-free optimization method: Gaussian Crunching Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14359">http://arxiv.org/abs/2307.14359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benny Wong</li>
<li>for: 这个研究论文是为了探讨一种新的优化方法—— Gaussian Crunching Search（GCS），以及其在不同领域中的应用。</li>
<li>methods: 本研究使用 Gaussian Crunching Search（GCS）方法，启发自狄拉克分布中的粒子行为，旨在高效地探索解决空间并趋向于全局最优点。</li>
<li>results: 通过实验评估和与现有优化方法比较，本研究展示了 GCS 方法的优势和特点。这篇研究论文对于优化方法的研究和实践者都是一个有价值的资源。<details>
<summary>Abstract</summary>
Optimization methods are essential in solving complex problems across various domains. In this research paper, we introduce a novel optimization method called Gaussian Crunching Search (GCS). Inspired by the behaviour of particles in a Gaussian distribution, GCS aims to efficiently explore the solution space and converge towards the global optimum. We present a comprehensive analysis of GCS, including its working mechanism, and potential applications. Through experimental evaluations and comparisons with existing optimization methods, we highlight the advantages and strengths of GCS. This research paper serves as a valuable resource for researchers, practitioners, and students interested in optimization, providing insights into the development and potential of Gaussian Crunching Search as a new and promising approach.
</details>
<details>
<summary>摘要</summary>
优化方法是解决复杂问题的关键，在不同领域都有广泛应用。本研究论文介绍一种新的优化方法——高斯压缩搜索（GCS）。该方法 draws inspiration from高斯分布中粒子的行为，旨在效率地探索解决空间并趋向于全球最优点。我们对GCS进行了全面的分析，包括它的工作机制和潜在应用。通过实验评估和现有优化方法的比较，我们提出了GCS的优势和特点。这篇研究论文对优化领域的研究人员、实践者和学生都是一种有价值的资源，为他们提供了GCS的开发和潜力的深入了解。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version"><a href="#Graph-Neural-Networks-For-Mapping-Variables-Between-Programs-–-Extended-Version" class="headerlink" title="Graph Neural Networks For Mapping Variables Between Programs – Extended Version"></a>Graph Neural Networks For Mapping Variables Between Programs – Extended Version</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13014">http://arxiv.org/abs/2307.13014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs">https://github.com/pmorvalho/ecai23-gnns-for-mapping-variables-between-programs</a></li>
<li>paper_authors: Pedro Orvalho, Jelle Piepenbrock, Mikoláš Janota, Vasco Manquinho</li>
<li>for: 本研究旨在提出一种基于图神经网络（GNN）的变量映射方法，用于比较两个程序的变量集。</li>
<li>methods: 本研究使用GNN来映射两个程序的抽象 sintaxis树（AST）中的变量集。</li>
<li>results: 实验结果表明，我们的方法可以正确地映射83%的评估数据集中的变量。此外，当前领先的程序修复方法仅能修复约72%的错误程序，而我们的方法则可以修复约88.5%的错误程序。<details>
<summary>Abstract</summary>
Automated program analysis is a pivotal research domain in many areas of Computer Science -- Formal Methods and Artificial Intelligence, in particular. Due to the undecidability of the problem of program equivalence, comparing two programs is highly challenging. Typically, in order to compare two programs, a relation between both programs' sets of variables is required. Thus, mapping variables between two programs is useful for a panoply of tasks such as program equivalence, program analysis, program repair, and clone detection. In this work, we propose using graph neural networks (GNNs) to map the set of variables between two programs based on both programs' abstract syntax trees (ASTs). To demonstrate the strength of variable mappings, we present three use-cases of these mappings on the task of program repair to fix well-studied and recurrent bugs among novice programmers in introductory programming assignments (IPAs). Experimental results on a dataset of 4166 pairs of incorrect/correct programs show that our approach correctly maps 83% of the evaluation dataset. Moreover, our experiments show that the current state-of-the-art on program repair, greatly dependent on the programs' structure, can only repair about 72% of the incorrect programs. In contrast, our approach, which is solely based on variable mappings, can repair around 88.5%.
</details>
<details>
<summary>摘要</summary>
自动化程序分析是计算机科学多个领域的关键研究领域之一，尤其是形式方法和人工智能。由于程序等价问题是不可解决的，因此比较两个程序很困难。通常，为了比较两个程序，需要在两个程序中变量集的关系。因此，将变量 между两个程序映射到相同的空间是有用的，可以用于许多任务，如程序等价、程序分析、程序修复和冲击检测。在这种工作中，我们提出使用图 neural network（GNN）将两个程序的抽象语法树（AST）中的变量集映射到相同的空间。为了证明变量映射的强大性，我们提出了三种用例，用于修复 novice 程序员在入门编程作业（IPA）中常见的错误。我们的实验结果表明，我们的方法可以正确地映射83%的评估数据集。此外，我们的实验还表明，现有的程序修复方法，强调程序结构，只能修复约72%的错误程序。与此相比，我们的方法， solely 基于变量映射，可以修复约88.5%的错误程序。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/cs.LG_2023_07_25/" data-id="clpxp6c3500p0ee88e66w2shp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/25/eess.IV_2023_07_25/" class="article-date">
  <time datetime="2023-07-25T09:00:00.000Z" itemprop="datePublished">2023-07-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/25/eess.IV_2023_07_25/">eess.IV - 2023-07-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines"><a href="#Towards-Unifying-Anatomy-Segmentation-Automated-Generation-of-a-Full-body-CT-Dataset-via-Knowledge-Aggregation-and-Anatomical-Guidelines" class="headerlink" title="Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines"></a>Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13375">http://arxiv.org/abs/2307.13375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexanderjaus/atlasdataset">https://github.com/alexanderjaus/atlasdataset</a></li>
<li>paper_authors: Alexander Jaus, Constantin Seibold, Kelsey Hermann, Alexandra Walter, Kristina Giske, Johannes Haubold, Jens Kleesiek, Rainer Stiefelhagen<br>for: 这种方法用于生成自动生成的解剖学 segmentation 数据集，使用紧跟式的 nnU-Net 基于 pseudo-labeling 和 anatomy-guided pseudo-label 精度调整。methods: 这种方法通过结合多种分立的知识库，生成了一个整体 CT 扫描图像的 $142$ 块级标签，提供了全面的解剖学覆盖。results: 我们的方法不需要手动标注 durante 标签聚合阶段，并在 BTCV 数据集上实现了 85% dice 分数。此外，我们还进行了医学有效性检查和可扩展自动检查。<details>
<summary>Abstract</summary>
In this study, we present a method for generating automated anatomy segmentation datasets using a sequential process that involves nnU-Net-based pseudo-labeling and anatomy-guided pseudo-label refinement. By combining various fragmented knowledge bases, we generate a dataset of whole-body CT scans with $142$ voxel-level labels for 533 volumes providing comprehensive anatomical coverage which experts have approved. Our proposed procedure does not rely on manual annotation during the label aggregation stage. We examine its plausibility and usefulness using three complementary checks: Human expert evaluation which approved the dataset, a Deep Learning usefulness benchmark on the BTCV dataset in which we achieve 85% dice score without using its training dataset, and medical validity checks. This evaluation procedure combines scalable automated checks with labor-intensive high-quality expert checks. Besides the dataset, we release our trained unified anatomical segmentation model capable of predicting $142$ anatomical structures on CT data.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种方法，用于自动生成骨科影像分割数据集，通过nnU-Net基于pseudo标签和骨科指导pseudo标签纠正的顺序 proces。通过结合多个分割知识库，我们生成了整体 CT 扫描图像的 $142$ 块级标签，对 533 幅提供了全面的解剖学覆盖，经过专家审核。我们的提议过程不依赖于手动标注 during the label aggregation stage。我们使用三种 complementary 检查来评估我们的方法：人工专家评估，btcv 数据集上的深度学习有用性测试，以及医学有效性检查。这种评估过程结合了扩展自动检查和劳动密集高质量专家检查。除了数据集之外，我们发布了我们的训练过的一体解剖学分割模型，可以在 CT 数据上预测 $142$ 种解剖学结构。
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks"><a href="#Overcoming-Distribution-Mismatch-in-Quantizing-Image-Super-Resolution-Networks" class="headerlink" title="Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks"></a>Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13337">http://arxiv.org/abs/2307.13337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheeun Hong, Kyoung Mu Lee</li>
<li>for: 这篇论文的目的是提出一个新的几何量化框架，以解决图像超解析网络中的分布差异问题，以提高量化后的精准度。</li>
<li>methods: 这篇论文使用了一个新的几何量化框架，named ODM，它通过在训练过程中直接调整特征分布的方式来降低分布差异问题。此外，ODM还引入了分布偏移来更好地调整各个通道的特征分布。</li>
<li>results: 实验结果显示，ODM可以对图像超解析网络进行有效的量化，并且与现有的量化方法相比，ODM可以更好地维持精准度。此外，ODM还可以降低分布差异问题的影响，使量化后的精准度得到更大的提升。<details>
<summary>Abstract</summary>
Quantization is a promising approach to reduce the high computational complexity of image super-resolution (SR) networks. However, compared to high-level tasks like image classification, low-bit quantization leads to severe accuracy loss in SR networks. This is because feature distributions of SR networks are significantly divergent for each channel or input image, and is thus difficult to determine a quantization range. Existing SR quantization works approach this distribution mismatch problem by dynamically adapting quantization ranges to the variant distributions during test time. However, such dynamic adaptation incurs additional computational costs that limit the benefits of quantization. Instead, we propose a new quantization-aware training framework that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, the mismatch can be reduced by directly regularizing the variance in features during training. However, we observe that variance regularization can collide with the reconstruction loss during training and adversely impact SR accuracy. Thus, we avoid the conflict between two losses by regularizing the variance only when the gradients of variance regularization are cooperative with that of reconstruction. Additionally, to further reduce the distribution mismatch, we introduce distribution offsets to layers with a significant mismatch, which either scales or shifts channel-wise features. Our proposed algorithm, called ODM, effectively reduces the mismatch in distributions with minimal computational overhead. Experimental results show that ODM effectively outperforms existing SR quantization approaches with similar or fewer computations, demonstrating the importance of reducing the distribution mismatch problem. Our code is available at https://github.com/Cheeun/ODM.
</details>
<details>
<summary>摘要</summary>
“量化是一种可能的方法来降低图像超解像网络的高度计算复杂性。然而，相比高水平任务如图像分类，低位数量化对SR网络导致严重的准确损失。这是因为SR网络的特征分布在每个通道或输入图像之间存在严重的分布不对称性。现有的SR量化工作通过在试用时适应性的方式来解决这个分布不对称问题。然而，这种动态适应带来更多的计算成本，限制了量化的利弊。相反，我们提出了一个新的量化意识训练框架，可以有效地解决SR网络中的分布不对称问题，无需动态适应。”“我们观察到，SR网络的特征分布存在严重的分布不对称性，这可以通过对特征的方差调控来缓和。然而，我们发现，在训练时对方差进行调控可能会与重建loss发生冲突，导致SR准确下降。因此，我们避免了这两个损失之间的冲突，通过对方差调控时只有在重建loss的Gradient与方差调控的Gradient之间有着合作的情况下进行调控。”“此外，为了进一步缓和分布不对称问题，我们引入了分布偏移，将通道对频率偏移或扭转。我们称之为ODM。实验结果显示，ODM可以对SR量化进行有效的缓和，并且与相同或 fewer 的计算成本下，实现SR准确的提高。”“我们的代码可以在https://github.com/Cheeun/ODM上找到。”
</details></li>
</ul>
<hr>
<h2 id="A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document"><a href="#A-Visual-Quality-Assessment-Method-for-Raster-Images-in-Scanned-Document" class="headerlink" title="A Visual Quality Assessment Method for Raster Images in Scanned Document"></a>A Visual Quality Assessment Method for Raster Images in Scanned Document</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13241">http://arxiv.org/abs/2307.13241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Yang, Peter Bauer, Todd Harris, Changhyung Lee, Hyeon Seok Seo, Jan P Allebach, Fengqing Zhu</li>
<li>for: 本研究探讨了扫描文档中的图像质量，特别是针对灰度图像区域。</li>
<li>methods: 我们提出了一种基于机器学习的分类方法，以确定扫描灰度图像的视觉质量是否符合标准。</li>
<li>results: 我们通过进行心理学实验，确定了不同分辨率设定下图像质量的可接受程度，并使用这些人工标准来训练机器学习模型。 However, this dataset is unbalanced as most images were rated as visually acceptable. To address the data imbalance problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.<details>
<summary>Abstract</summary>
Image quality assessment (IQA) is an active research area in the field of image processing. Most prior works focus on visual quality of natural images captured by cameras. In this paper, we explore visual quality of scanned documents, focusing on raster image areas. Different from many existing works which aim to estimate a visual quality score, we propose a machine learning based classification method to determine whether the visual quality of a scanned raster image at a given resolution setting is acceptable. We conduct a psychophysical study to determine the acceptability at different image resolutions based on human subject ratings and use them as the ground truth to train our machine learning model. However, this dataset is unbalanced as most images were rated as visually acceptable. To address the data imbalance problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.
</details>
<details>
<summary>摘要</summary>
We conduct a psychophysical study to determine the acceptability of images at different resolutions based on human subject ratings and use them as the ground truth to train our machine learning model. However, the dataset is unbalanced as most images were rated as visually acceptable. To address this problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.
</details></li>
</ul>
<hr>
<h2 id="One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction"><a href="#One-for-Multiple-Physics-informed-Synthetic-Data-Boosts-Generalizable-Deep-Learning-for-Fast-MRI-Reconstruction" class="headerlink" title="One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction"></a>One for Multiple: Physics-informed Synthetic Data Boosts Generalizable Deep Learning for Fast MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13220">http://arxiv.org/abs/2307.13220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangziblake/pisf">https://github.com/wangziblake/pisf</a></li>
<li>paper_authors: Zi Wang, Xiaotong Yu, Chengyan Wang, Weibo Chen, Jiazheng Wang, Ying-Hua Chu, Hongwei Sun, Rushuai Li, Peiyong Li, Fan Yang, Haiwei Han, Taishan Kang, Jianzhong Lin, Chen Yang, Shufu Chang, Zhang Shi, Sha Hua, Yan Li, Juan Hu, Liuhong Zhu, Jianjun Zhou, Meijing Lin, Jiefeng Guo, Congbo Cai, Zhong Chen, Di Guo, Xiaobo Qu</li>
<li>for: 这个论文旨在提高快速磁共振成像（MRI）的扫描时间，使用深度学习（DL）技术进行图像重建，但是现有的DL方法在不同的成像场景下的应用尚未得到广泛开发。</li>
<li>methods: 这个研究使用了物理学习Synthetic数据框架（PISF），该框架可以通过具有一个训练后可通用的模型来实现多种成像场景下的MRI重建。在2D图像重建中，扫描是分解成多个1D基本问题，并从1D数据生成开始，以便普适化。</li>
<li>results: 研究发现，使用PISF学习Synthetic数据，并与提高学习技术相结合，可以在实验室中比较或者更好地重建实验室中的MRI图像，比对使用匹配的真实数据训练的模型。此外，PISF还能够在多种供应商多个中心的成像中表现出优异的适应性。10名经验丰富的医生也证明了PISF在实际应用中的优秀适应性。<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is a principal radiological modality that provides radiation-free, abundant, and diverse information about the whole human body for medical diagnosis, but suffers from prolonged scan time. The scan time can be significantly reduced through k-space undersampling but the introduced artifacts need to be removed in image reconstruction. Although deep learning (DL) has emerged as a powerful tool for image reconstruction in fast MRI, its potential in multiple imaging scenarios remains largely untapped. This is because not only collecting large-scale and diverse realistic training data is generally costly and privacy-restricted, but also existing DL methods are hard to handle the practically inevitable mismatch between training and target data. Here, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF, which is the first to enable generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems and starts with the 1D data synthesis, to facilitate generalization. We demonstrate that training DL models on synthetic data, integrated with enhanced learning techniques, can achieve comparable or even better in vivo MRI reconstruction compared to models trained on a matched realistic dataset, reducing the demand for real-world MRI data by up to 96%. Moreover, our PISF shows impressive generalizability in multi-vendor multi-center imaging. Its excellent adaptability to patients has been verified through 10 experienced doctors' evaluations. PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we present a Physics-Informed Synthetic data learning framework for Fast MRI, called PISF. PISF enables generalizable DL for multi-scenario MRI reconstruction using solely one trained model. For a 2D image, the reconstruction is separated into many 1D basic problems, starting with the synthesis of 1D data. This approach facilitates generalization and reduces the demand for real-world MRI data by up to 96%. Additionally, PISF demonstrates impressive generalizability in multi-vendor multi-center imaging and has been evaluated by 10 experienced doctors, who have verified its excellent adaptability to patients.PISF provides a feasible and cost-effective way to markedly boost the widespread usage of DL in various fast MRI applications, while freeing from the intractable ethical and practical considerations of in vivo human data acquisitions.
</details></li>
</ul>
<hr>
<h2 id="Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement"><a href="#Magnetic-Resonance-Parameter-Mapping-using-Self-supervised-Deep-Learning-with-Model-Reinforcement" class="headerlink" title="Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement"></a>Magnetic Resonance Parameter Mapping using Self-supervised Deep Learning with Model Reinforcement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13211">http://arxiv.org/abs/2307.13211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Bian, Albert Jang, Fang Liu</li>
<li>for: 本研究提出了一种新的自动学习方法，RELAX-MORE，用于生物医学影像重建（qMRI）。</li>
<li>methods: 该方法使用优化算法将模型基于qMRI重建拓展到深度学习框架中，以生成高精度和可靠的MR参数图像。</li>
<li>results: 在不同的脑、膝和phantom实验中，提出的方法能够高效地重建MR参数图像，正确地纠正影像损害、除除噪音和恢复图像特征。与其他状态前方法相比，RELAX-MORE显著提高了效率、准确性、可靠性和通用性。这种方法有很大的应用前途，可能为qMRI的临床翻译提供很大的助力。<details>
<summary>Abstract</summary>
This paper proposes a novel self-supervised learning method, RELAX-MORE, for quantitative MRI (qMRI) reconstruction. The proposed method uses an optimization algorithm to unroll a model-based qMRI reconstruction into a deep learning framework, enabling the generation of highly accurate and robust MR parameter maps at imaging acceleration. Unlike conventional deep learning methods requiring a large amount of training data, RELAX-MORE is a subject-specific method that can be trained on single-subject data through self-supervised learning, making it accessible and practically applicable to many qMRI studies. Using the quantitative $T_1$ mapping as an example at different brain, knee and phantom experiments, the proposed method demonstrates excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noises, and recovering image features at imperfect imaging conditions. Compared with other state-of-the-art conventional and deep learning methods, RELAX-MORE significantly improves efficiency, accuracy, robustness, and generalizability for rapid MR parameter mapping. This work demonstrates the feasibility of a new self-supervised learning method for rapid MR parameter mapping, with great potential to enhance the clinical translation of qMRI.
</details>
<details>
<summary>摘要</summary>
The proposed method was tested using the quantitative $T_1$ mapping as an example at different brain, knee, and phantom experiments. The results showed that RELAX-MORE demonstrated excellent performance in reconstructing MR parameters, correcting imaging artifacts, removing noise, and recovering image features at imperfect imaging conditions. Compared with other state-of-the-art conventional and deep learning methods, RELAX-MORE significantly improved efficiency, accuracy, robustness, and generalizability for rapid MR parameter mapping.This work demonstrates the feasibility of self-supervised learning for rapid MR parameter mapping, with great potential to enhance the clinical translation of qMRI.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review"><a href="#Deep-Learning-Approaches-for-Data-Augmentation-in-Medical-Imaging-A-Review" class="headerlink" title="Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review"></a>Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13125">http://arxiv.org/abs/2307.13125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Aghiles Kebaili, Jérôme Lapuyade-Lahorgue, Su Ruan</li>
<li>for: 这篇论文主要针对医疗影像分析领域中深度学习模型的训练数据有限制的问题，即使用深度生成模型来生成更真实和多样化的数据，以提高深度学习模型在医疗影像分析中的表现。</li>
<li>methods: 这篇论文主要介绍了三种深度生成模型，即变量自动编码器、对抗网络和扩散模型，以及它们在医疗影像分析中的应用。</li>
<li>results: 论文提供了现有深度生成模型在医疗影像分析中的最新状况，以及它们在不同下游任务中的潜在应用，包括分类、分割和cross-modal翻译。同时，论文也评估了每种模型的优缺点，并提出了未来研究的方向。<details>
<summary>Abstract</summary>
Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.
</details>
<details>
<summary>摘要</summary>
深度学习已成为医疗影像分析的流行工具，但培训数据的有限性仍然是主要的挑战，尤其在医疗领域，数据获取可能昂贵且受隐私法规限制。数据扩充技术可以人工增加培训样本数量，但这些技术通常生成有限和不置人心的结果。为解决这个问题，一些研究提出使用深度生成模型生成更真实和多样的数据，以符合实际数据的分布。在这篇评论中，我们关注了医疗影像增强中三种深度生成模型：变量自适应网络、对抗网络和扩散模型。我们提供了每种模型的当前状态之讲，并讨论它们在不同下游任务中的潜在应用，包括分类、分割和cross-modal翻译。我们还评估了每种模型的优缺点，并建议未来在这一领域的发展方向。我们的目标是提供深度生成模型在医疗影像增强中的全面评论，并高亮这些模型在医疗影像分析中的潜在优势，以及未来研究的发展方向。
</details></li>
</ul>
<hr>
<h2 id="In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance"><a href="#In-Situ-Thickness-Measurement-of-Die-Silicon-Using-Voltage-Imaging-for-Hardware-Assurance" class="headerlink" title="In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance"></a>In-Situ Thickness Measurement of Die Silicon Using Voltage Imaging for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13118">http://arxiv.org/abs/2307.13118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Nitin Varshney, M Tanjidur Rahman, Michael Strizich, Haoting Shen, Navid Asadizanjani</li>
<li>for: 这篇论文的目的是提出一种基于电子束电压成像、图像处理和蒙特卡洛模拟的快速减厚方法，以便在减厚过程中保证层的厚度均匀。</li>
<li>methods: 该方法使用电子束电压成像技术、图像处理和蒙特卡洛模拟来测量剩下的硅层的厚度，以便在减厚过程中实时监测和调整层的厚度。</li>
<li>results: 该方法可以快速、准确地测量硅层的厚度，并且可以在减厚过程中实时监测和调整层的厚度，以保证减厚过程的准确性和效率。<details>
<summary>Abstract</summary>
Hardware assurance of electronics is a challenging task and is of great interest to the government and the electronics industry. Physical inspection-based methods such as reverse engineering (RE) and Trojan scanning (TS) play an important role in hardware assurance. Therefore, there is a growing demand for automation in RE and TS. Many state-of-the-art physical inspection methods incorporate an iterative imaging and delayering workflow. In practice, uniform delayering can be challenging if the thickness of the initial layer of material is non-uniform. Moreover, this non-uniformity can reoccur at any stage during delayering and must be corrected. Therefore, it is critical to evaluate the thickness of the layers to be removed in a real-time fashion. Our proposed method uses electron beam voltage imaging, image processing, and Monte Carlo simulation to measure the thickness of remaining silicon to guide a uniform delayering process
</details>
<details>
<summary>摘要</summary>
硬件保证是电子设备领域的一项挑战，政府和电子行业对其具有很大的兴趣。物理检查方法如反工程（RE）和 Trojan 扫描（TS）在硬件保证中扮演着重要的角色。因此，自动化在 RE 和 TS 中的需求在增长。许多当今的物理检查方法具有迭代性的图像和层去除工艺。在实践中，均匀的层去除可以是一个挑战，特别是当初始材料厚度不均匀时。此外，这种不均匀性可能会在任何阶段重新出现，需要实时纠正。因此，我们的提议的方法使用电子束电幕摄影、图像处理和 Монте卡洛 simulate 来测量剩下的硬件厚度，以便实现均匀的层去除过程。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark"><a href="#Automatic-Infant-Respiration-Estimation-from-Video-A-Deep-Flow-based-Algorithm-and-a-Novel-Public-Benchmark" class="headerlink" title="Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark"></a>Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13110">http://arxiv.org/abs/2307.13110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/infant-respiration-estimation">https://github.com/ostadabbas/infant-respiration-estimation</a></li>
<li>paper_authors: Sai Kumar Reddy Manne, Shaotong Zhu, Sarah Ostadabbas, Michael Wan</li>
<li>for: 这篇论文目标是为新生儿提供自动、无接触的呼吸监测。</li>
<li>methods: 该论文使用深度学习方法，使用普通的视频捕捉来估计新生儿的呼吸速率和呼吸波形。</li>
<li>results: 该论文在使用AIRFlowNet模型和AIR-125 infant数据集上进行训练后，与其他state-of-the-art方法相比，在呼吸速率估计中显著提高了精度， сред平均误差为$\sim$2.9 breaths per minute。<details>
<summary>Abstract</summary>
Respiration is a critical vital sign for infants, and continuous respiratory monitoring is particularly important for newborns. However, neonates are sensitive and contact-based sensors present challenges in comfort, hygiene, and skin health, especially for preterm babies. As a step toward fully automatic, continuous, and contactless respiratory monitoring, we develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings. Our automated infant respiration flow-based network (AIRFlowNet) combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. We support our model with the first public annotated infant respiration dataset with 125 videos (AIR-125), drawn from eight infant subjects, set varied pose, lighting, and camera conditions. We include manual respiration annotations and optimize AIRFlowNet training on them using a novel spectral bandpass loss function. When trained and tested on the AIR-125 infant data, our method significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute, compared to $\sim$4.7--6.2 for other public models designed for adult subjects and more uniform environments.
</details>
<details>
<summary>摘要</summary>
呼吸是新生儿的重要生命 Parameter，连续呼吸监测特别重要。然而，新生儿脆弱， contact-based 感测器会带来舒适、卫生和皮肤健康问题，特别是 Premature 新生儿。为了实现完全自动、不间断、无接触的呼吸监测，我们开发了一种深度学习方法，可以从平面视频 Footage 中获取呼吸速率和波形。我们的自动 infant 呼吸流基本网络（AIRFlowNet）将视频提取的光学流输入和空间时间卷积处理相结合，并在婴儿领域进行了调整。我们为模型提供了首个公共标注 infant 呼吸数据集（AIR-125），包含 125 个视频，来自八个婴儿素材，有不同的姿势、照明和摄像头条件。我们还包括手动呼吸标注和使用一种新的spectral bandpass损失函数来优化 AIRFlowNet 的训练。当我们在 AIR-125 婴儿数据集上训练和测试 AIRFlowNet 时，它在呼吸速率估计方面表现出色，与其他公共模型在 adult 主题和更uniform 环境中的表现相比，表现出较低的mean absolute error（约为 2.9 呼吸/分钟）。
</details></li>
</ul>
<hr>
<h2 id="Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance"><a href="#Framework-for-Automatic-PCB-Marking-Detection-and-Recognition-for-Hardware-Assurance" class="headerlink" title="Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance"></a>Framework for Automatic PCB Marking Detection and Recognition for Hardware Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13105">http://arxiv.org/abs/2307.13105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia P. Dizon-Paradis, Daniel E. Capecci, Nathan T. Jessurun, Damon L. Woodard, Mark M. Tehranipoor, Navid Asadizanjani</li>
<li>for: 这个研究的目的是提出一种自动电路板标注EXTRACTION方法，以便为政府和电子行业提供高精度的自动硬件保证。</li>
<li>methods: 该研究提出了一种收集PCB标注数据的计划，以及一种将该数据integrated到自动硬件保证过程中的框架。</li>
<li>results: 该研究提出了一种收集PCB标注数据的计划和一种将该数据integrated到自动硬件保证过程中的框架，可以提高自动硬件保证的精度。<details>
<summary>Abstract</summary>
A Bill of Materials (BoM) is a list of all components on a printed circuit board (PCB). Since BoMs are useful for hardware assurance, automatic BoM extraction (AutoBoM) is of great interest to the government and electronics industry. To achieve a high-accuracy AutoBoM process, domain knowledge of PCB text and logos must be utilized. In this study, we discuss the challenges associated with automatic PCB marking extraction and propose 1) a plan for collecting salient PCB marking data, and 2) a framework for incorporating this data for automatic PCB assurance. Given the proposed dataset plan and framework, subsequent future work, implications, and open research possibilities are detailed.
</details>
<details>
<summary>摘要</summary>
一份成本物品列表（Bill of Materials，BoM）是印刷电路板（Printed Circuit Board，PCB）上所有组件的列表。由于BoM对硬件保证有益，因此自动BoM提取（AutoBoM）对政府和电子业界来说非常有利。为实现高精度的AutoBoM过程，需要利用PCB文本和标识符的领域知识。在这篇研究中，我们介绍了自动PCB标识提取的挑战，并提出了1）PCB标识数据收集计划，2）基于这些数据的自动PCB保证框架。根据提出的数据计划和框架，我们释放了未来工作、后续研究和开放的研究可能性。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework"><a href="#Enhancing-image-captioning-with-depth-information-using-a-Transformer-based-framework" class="headerlink" title="Enhancing image captioning with depth information using a Transformer-based framework"></a>Enhancing image captioning with depth information using a Transformer-based framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03767">http://arxiv.org/abs/2308.03767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Mahmoud Ahmed, Mohamed Yousef, Khaled F. Hussain, Yousef Bassyouni Mahdy</li>
<li>for: 该论文旨在提高图像captioning任务中的场景理解，通过将RGB图像和其相应的深度图 integrate into一个Transformer-based encoder-decoder框架中，生成多句文本描述3D场景。</li>
<li>methods: 该论文提出了一种将RGB图像和深度图进行拼接的方法，并使用Transformer架构来生成多句文本描述。不同的拼接方法也被研究以实现最佳的结果。</li>
<li>results: 实验结果表明，使用RGB图像和深度图进行拼接可以提高图像captioning任务的效果，无论depth图是否为真实的或估算值。此外，该论文还提出了一个更正版的NYU-v2数据集，以解决存在问题的标注问题。<details>
<summary>Abstract</summary>
Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image paragraph captioning dataset. During our work with the NYU-v2 dataset, we found inconsistent labeling that prevents the benefit of using depth information to enhance the captioning task. The results were even worse than using RGB images only. As a result, we propose a cleaned version of the NYU-v2 dataset that is more consistent and informative. Our results on both datasets demonstrate that the proposed framework effectively benefits from depth information, whether it is ground truth or estimated, and generates better captions. Code, pre-trained models, and the cleaned version of the NYU-v2 dataset will be made publically available.
</details>
<details>
<summary>摘要</summary>
标题： integrate depth information to enhance image captioning task摘要：在图像描述任务中，将RGB图像和深度图像融合在一起，可以提高描述任务的质量。我们提出了一种基于Transformer的RGB图像和深度图像融合框架，用于生成多句话描述3D场景。我们的框架可以将RGB图像和其对应的深度图像作为输入，并将它们融合在一起，以更好地理解输入场景。我们实现了不同的融合方法，并对NYU-v2数据集和Stanford图像描述数据集进行了实验。在我们的工作中，我们发现了NYU-v2数据集中的不一致标注问题，这使得使用深度信息来提高描述任务的 beneficial effects become less effective。最终，我们提出了一个更加一致的NYU-v2数据集，并对这两个数据集进行了实验。我们的结果表明，我们的提议的框架可以借助深度信息，无论是真实的深度图像还是估计的深度图像，生成更好的描述。我们将代码、预训练模型和清洁版NYU-v2数据集公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/25/eess.IV_2023_07_25/" data-id="clpxp6ca80187ee889uhseozj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.SD_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T15:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.SD_2023_07_24/">cs.SD - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes"><a href="#An-objective-evaluation-of-Hearing-Aids-and-DNN-based-speech-enhancement-in-complex-acoustic-scenes" class="headerlink" title="An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes"></a>An objective evaluation of Hearing Aids and DNN-based speech enhancement in complex acoustic scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12888">http://arxiv.org/abs/2307.12888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enricguso/guso_waspaa23">https://github.com/enricguso/guso_waspaa23</a></li>
<li>paper_authors: Enric Gusó, Joanna Luberadzka, Martí Baig, Umut Sayin Saraç, Xavier Serra</li>
<li>for: 评估五种高级商业听力器（HA）device的目标性能，并与基于深度神经网络（DNN）的抽象环境中的语音提高算法进行比较。</li>
<li>methods: 测量一个HAdevice的Head-Related Transfer Functions（HRTFs），用于Synthesize一个双抽象 dataset для训练两种 state-of-the-art causal和非 causal DNN增强模型。然后，通过Ambisonics loudspeaker设置生成一个评估集，并通过KU100 dummy head记录每个HA device上的语音，包括和不包括传统HA算法。</li>
<li>results: 发现DNN增强比传统HA算法在噪声抑制和对话情况中的对话智能度指标方面表现更好。<details>
<summary>Abstract</summary>
We investigate the objective performance of five high-end commercially available Hearing Aid (HA) devices compared to DNN-based speech enhancement algorithms in complex acoustic environments. To this end, we measure the HRTFs of a single HA device to synthesize a binaural dataset for training two state-of-the-art causal and non-causal DNN enhancement models. We then generate an evaluation set of realistic speech-in-noise situations using an Ambisonics loudspeaker setup and record with a KU100 dummy head wearing each of the HA devices, both with and without the conventional HA algorithms, applying the DNN enhancers to the latter. We find that the DNN-based enhancement outperforms the HA algorithms in terms of noise suppression and objective intelligibility metrics.
</details>
<details>
<summary>摘要</summary>
我们研究了五种高级商业可用的听力器（HA）device的目标性能，与基于深度学习（DNN）的语音提升算法在复杂的噪声环境中进行比较。为此，我们测量了一个单个HAdevice的Head-Related Transfer Functions（HRTFs），以生成一个双核心state-of-the-art causal和非 causal DNN增强模型的训练数据集。然后，我们使用一个Ambisonics喇叭设置生成一个评估集，记录了每个HA设备上的KU100假头和不同的传统HA算法，并应用DNN增强器。我们发现，DNN基于的增强方法在噪声抑制和对象智能度量标中超过HA算法。
</details></li>
</ul>
<hr>
<h2 id="Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains"><a href="#Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains" class="headerlink" title="Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains"></a>Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13012">http://arxiv.org/abs/2307.13012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Lebourdais, Théo Mariotte, Marie Tahon, Anthony Larcher, Antoine Laurent, Silvio Montresor, Sylvain Meignier, Jean-Hugh Thomas</li>
<li>for: 本研究旨在提供一个完整的精度评估材料，用于评估不同的语音分类器在单&#x2F;多通道和多种语音频道上的性能。</li>
<li>methods: 本研究使用了一种新的多类别分类模型，将语音分类和 overlap speech detection 融合到一起进行训练。</li>
<li>results: 研究结果显示，该模型在多种语音频道和单&#x2F;多通道上具有优秀的性能，与单独的语音分类和 overlap speech detection 系统相比，具有更高的一致性和更低的训练成本。<details>
<summary>Abstract</summary>
Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details>
<details>
<summary>摘要</summary>
声音活动和重叠说话检测（简称VAD和OSD）是speaker分类的关键前置处理任务。最终 segmentation 性能强度取决于这两个子任务的稳定性。近年来研究表明，VAD和OSD可以通过多类型分类模型进行合作训练。然而，这些研究通常受到特定的speech域的限制，缺乏对系统总体化能力的信息。本文提出了一个完整的VAD和OSD模型 benchmark，在多个音频设置（单/多通道）和语音域（例如媒体、会议等）上进行测试。我们的2/3类系统，即将时间卷积网络与适应设置的语音表示相结合，在F1分数上超越了现有的state-of-the-artResult。我们显示，将这两个任务合作训练可以与专门的VAD和OSD系统相比，提高了training cost，同时保持了相似的F1分数性能。此特有的架构还可以用于单通道和多通道speech处理。
</details></li>
</ul>
<hr>
<h2 id="Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition"><a href="#Integration-of-Frame-and-Label-synchronous-Beam-Search-for-Streaming-Encoder-decoder-Speech-Recognition" class="headerlink" title="Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition"></a>Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12767">http://arxiv.org/abs/2307.12767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe</li>
<li>for: 这个论文是为了提高自动语音识别的精度和Robustness，尤其是在遇到过去知识不足的情况下。</li>
<li>methods: 本论文使用了几种方法，包括frame-based模型和label-based注意力Encoder-Decoder，并通过在单一搜寻算法中交替进行F-Sync和L-Sync搜寻。</li>
<li>results: 实验结果显示，提案的搜寻算法比其他搜寻方法来得更低的误差率，并且在过去知识不足的情况下保持稳定性。<details>
<summary>Abstract</summary>
Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder-decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations.
</details>
<details>
<summary>摘要</summary>
尽管框架基模型，如 CTC 和传播器，与流动自动语音识别有着很好的相互作用，但它们的解码没有使用未来知识，这可能会导致错误的剪辑。相反，标签基于注意力Encoder-Decoder可以通过软注意力来输入，但它往往对它的训练领域偏好，不同于 CTC。我们利用这些 complementary 特点，并提议将帧和标签同步（F-/L-Sync）的解码 alternately 在同一个搜索方案中进行。F-Sync 解码在块级处理中领先，而 L-Sync 解码在预测未来帧内一个块中提供了优先的 гипотезы。我们保留了两个解码方法的假设，以实现有效的剪辑。实验表明，我们提议的搜索算法可以与其他搜索方法相比，在低误差情况下实现更好的性能，而且对于不同领域的情况也具有更高的稳定性。
</details></li>
</ul>
<hr>
<h2 id="Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN"><a href="#Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN" class="headerlink" title="Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN"></a>Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12759">http://arxiv.org/abs/2307.12759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sage-khan/code-switched-noisy-urdu-asr">https://github.com/sage-khan/code-switched-noisy-urdu-asr</a></li>
<li>paper_authors: Muhammad Danyal Khan, Raheem Ali, Arshad Aziz</li>
<li>For: The paper aims to develop a resource-efficient Automatic Speech Recognition (ASR) system for code-switched Urdu language in a noisy call-center environment.* Methods: The proposed system uses a Chain Hybrid HMM and CNN-TDNN approach, which combines the advantages of HMM and DNN models with less labelled data. The system also utilizes a noisy environment-aware CNN to improve accuracy.* Results: The proposed system achieves a Word Error Rate (WER) of 5.2% in both noisy and clean environments, outperforming other ASR systems for code-switched Urdu language. The system also shows improved performance in recognizing isolated words, numbers, and continuous spontaneous speech.<details>
<summary>Abstract</summary>
Call Centers have huge amount of audio data which can be used for achieving valuable business insights and transcription of phone calls is manually tedious task. An effective Automated Speech Recognition system can accurately transcribe these calls for easy search through call history for specific context and content allowing automatic call monitoring, improving QoS through keyword search and sentiment analysis. ASR for Call Center requires more robustness as telephonic environment are generally noisy. Moreover, there are many low-resourced languages that are on verge of extinction which can be preserved with help of Automatic Speech Recognition Technology. Urdu is the $10^{th}$ most widely spoken language in the world, with 231,295,440 worldwide still remains a resource constrained language in ASR. Regional call-center conversations operate in local language, with a mix of English numbers and technical terms generally causing a "code-switching" problem. Hence, this paper describes an implementation framework of a resource efficient Automatic Speech Recognition/ Speech to Text System in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. Using Hybrid HMM-DNN approach allowed us to utilize the advantages of Neural Network with less labelled data. Adding CNN with TDNN has shown to work better in noisy environment due to CNN's additional frequency dimension which captures extra information from noisy speech, thus improving accuracy. We collected data from various open sources and labelled some of the unlabelled data after analysing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English and were able to achieve WER of 5.2% with noisy as well as clean environment in isolated words or numbers as well as in continuous spontaneous speech.
</details>
<details>
<summary>摘要</summary>
call centers 有庞大的音频数据，可以用于获得有价值的商业意见和电话交流的自动识别，以便搜索历史记录中的特定上下文和内容，进行自动监控、关键词搜索和情感分析。为了在电话交流中提高质量，需要一个更加可靠的自动识别系统，因为电话环境通常噪音。此外，有很多资源受限的语言正在濒临灭绝，可以通过自动识别技术来保存它们。urd 是全球第10大最流行的语言，有231295440名使用者，但它仍然是资源受限的语言。本文描述了在噪音电话交流环境中实现资源有效的自动识别/文本转语系统的实现框架，使用链式混合HMM和CNN-TDNN进行混合语言识别。通过将HMM和DNN结合使用，我们可以利用神经网络的优势，而不需要大量标注数据。另外，通过添加CNN和TDNN，我们可以在噪音环境中提高准确率，因为CNN的额外频率维度可以捕捉更多的噪音语音信息。我们从多个开源资源中收集数据，并对一些未标注的数据进行分析和标注，以获得urd语言的WER为5.2%，包括噪音和清晰环境下的隔离单词或数字以及连续自由语言。
</details></li>
</ul>
<hr>
<h2 id="IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models"><a href="#IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models" class="headerlink" title="IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models"></a>IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13005">http://arxiv.org/abs/2307.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiromu Yakura, Masataka Goto</li>
<li>for: 帮助 novice 用户自由生成音乐音频，即使他们没有音乐知识，如和声进程和乐器知识。</li>
<li>methods: 我们使用 text-to-audio 生成技术，并提供了一个特定的 interface 以帮助用户逐步调整文本提示和选择有利的音频先导。</li>
<li>results: 通过这种双重探索方式，用户可以了解不同文本提示和音频先导对生成结果的影响，并逐步实现他们的模糊定义目标。<details>
<summary>Abstract</summary>
Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.
</details>
<details>
<summary>摘要</summary>
现代文本到音频生成技术具有让新手可以自由生成音频的潜力。即使他们没有音乐知识，如和声进程和乐器，用户仍可以尝试不同的文本提示来生成音频。然而，与图像领域不同，了解生成音频的可能性空间是困难的，因为用户无法同时听到生成音频的变化。我们因此为用户提供了探索不同文本提示和音频先前的机会。这种双重探索允许用户通过比较不同的文本提示和音频先前来了解不同的生成结果的影响。我们开发的界面IteraTTA专门为用户帮助制定文本提示和选择生成音频中有利的先前。通过这种方式，用户可以逐步实现自己的抽象目标，同时了解和探索生成结果的可能性空间。我们的实现和讨论探讨了特定于文本到音频模型的设计考虑因素，以及如何通过互动技术来提高其效iveness。
</details></li>
</ul>
<hr>
<h2 id="A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization"><a href="#A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization" class="headerlink" title="A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization"></a>A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12659">http://arxiv.org/abs/2307.12659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Fish, Umberto Michieli, Mete Ozay</li>
<li>for: 这篇论文目的是提出一种可以个别化的数字化模型优化方法，以提高自动语音识别（ASR）模型在移动设备上的部署。</li>
<li>methods: 本文提出了一种混合精度量化方法（myQASR），可以根据不同的用户和目标领域，生成特化的数字化方案，并且不需要精确调整。myQASR通过分析全精度活动值，自动评估网络层的数字化敏感度，然后生成个别化的混合精度量化方案。</li>
<li>results: 本文的实验结果显示，myQASR可以对大规模ASR模型进行个别化优化，以提高特定的性别、语言和说话者的表现。<details>
<summary>Abstract</summary>
Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.
</details>
<details>
<summary>摘要</summary>
myQASR uses mixed-precision quantization to generate personalized schemes for each user. The method evaluates the quantization sensitivity of network layers by analyzing full-precision activation values. This allows for the creation of a personalized mixed-precision quantization scheme for any pre-determined memory budget.Results for large-scale ASR models show that myQASR improves performance for specific genders, languages, and speakers. This demonstrates the effectiveness of personalized quantization for ASR models, and highlights the potential of myQASR for improving the performance of ASR systems in a wide range of applications.Here is the text in Simplified Chinese:最近的自动语音识别（ASR）技术发展，导致大型AI模型的出现，但这些模型在移动设备上部署是不实际的。模型压缩是一种解决方案，但它只能部署到一个限制的子domain中。我们提出myQASR方法，它可以在压缩过程中为不同用户personal化ASR模型，而无需细化。myQASR使用混合精度压缩生成用户化的压缩方案，并通过分析全精度活动值来评估网络层的压缩敏感度。这allow for生成任何预先确定的内存预算的个性化混合精度压缩方案。results表明，myQASR可以对大规模ASR模型进行特定性别、语言和发音人的改进。这表明个性化压缩对ASR模型的性能有积极的影响，并 highlights myQASR在各种应用场景中的潜在应用前景。
</details></li>
</ul>
<hr>
<h2 id="Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training"><a href="#Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training" class="headerlink" title="Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training"></a>Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12498">http://arxiv.org/abs/2307.12498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WAPATASR/WAPAT">https://github.com/WAPATASR/WAPAT</a></li>
<li>paper_authors: Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue</li>
<li>for: 提高自动语音识别（ASR）模型在小量干扰和大域转移下的稳定性。</li>
<li>methods: 使用phoneme空间中的对抗例进行听话示例的挤压，使模型对phoneme表示具有抗衰减性，并通过使用挤压示例的phoneme表示来引导对抗例生成，以找到更稳定和多样的梯度方向，提高总体性。</li>
<li>results: 在End-to-end Speech Challenge Benchmark（ESB）上实现了6.28%的WRER降低，超过原始模型，达到新的领域最优。<details>
<summary>Abstract</summary>
Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (wapat). wapat use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, wapat utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of wapat on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-wapat outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
发展一个实用robust的自动语音识别（ASR）模型是挑战，因为模型不仅需要保持干净样本上的原始性能，还需要在小量干扰和大域变化下达到一致的效果。为解决这问题，我们提出了一种新的WavAugment导向的phoneme adversarial Training（wapat）方法。wapat使用phoneme空间中的 adversarial example作为增强素，以使模型对phoneme表示的小变化免疫，并保持干净样本上的性能。此外，wapat使用增强后的phoneme表示来导引敌对生成，以找到更稳定和多样的梯度方向，从而提高了总体的一致性。我们在End-to-end Speech Challenge Benchmark（ESB）上进行了广泛的实验，并证明了wapat的有效性。特别是，SpeechLM-wapat比原始模型减少了6.28%的WRR，达到了新的状态态-of-the-art。
</details></li>
</ul>
<hr>
<h2 id="SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces"><a href="#SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces" class="headerlink" title="SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces"></a>SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12445">http://arxiv.org/abs/2307.12445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Vallés-Pérez, Grzegorz Beringer, Piotr Bilinski, Gary Cook, Roberto Barra-Chicote</li>
<li>for: This paper aims to learn shared representations of phonetic and acoustic spaces in the speech domain using a CLIP-based model.</li>
<li>methods: The proposed model is trained using the CLIP framework, which enables deep learning systems to learn shared latent spaces between images and text descriptions.</li>
<li>results: The model shows sensitivity to phonetic changes and robustness against different types of noise, with a 91% score drop when replacing 20% of the phonemes at random and a 10% performance drop when mixing the audio with 75% of Gaussian noise. The resulting embeddings are also found to be useful for downstream applications such as intelligibility evaluation and speech generation.<details>
<summary>Abstract</summary>
Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility evaluation and the ability to leverage rich pre-trained phonetic embeddings in speech generation task. Finally, we discuss potential applications with interesting implications for the speech generation and recognition fields.
</details>
<details>
<summary>摘要</summary>
多种例子在 literatur 中证明深度学习模型可以处理多模态数据。近期，CLIP 使得深度学习系统可以学习共同的封闭空间 между图像和文本描述， obtained outstanding zero- or few-shot results in downstream tasks。在这篇文章中，我们探索了同样的想法，但是应用到语音领域， где phonetic 和 acoustic 空间通常共存。我们使用 CLIP 基于的模型， aiming to learn shared representations of phonetic and acoustic spaces。结果显示，我们的模型对 phonetic 变化敏感，对于Randomly replacing 20% of phonemes 的情况下，得分下降了 91%，而对于不同类型的噪音混合情况下，得分下降了 10%。我们还提供了实验证明， embedding 是对下游应用场景有用，如智能识别和speech generation 任务中的质量评估。最后，我们讨论了可能的应用，具有 interessing 的潜在应用于语音生成和识别领域。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.SD_2023_07_24/" data-id="clpxp6c5w00wwee88fjgt22er" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/eess.AS_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T14:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/eess.AS_2023_07_24/">eess.AS - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Adaptation-of-Whisper-models-to-child-speech-recognition"><a href="#Adaptation-of-Whisper-models-to-child-speech-recognition" class="headerlink" title="Adaptation of Whisper models to child speech recognition"></a>Adaptation of Whisper models to child speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13008">http://arxiv.org/abs/2307.13008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c3imaging/whisper_child_speech">https://github.com/c3imaging/whisper_child_speech</a></li>
<li>paper_authors: Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, Horia Cucu</li>
<li>for: 提高儿童语音识别（ASR）系统对儿童语音的识别精度。</li>
<li>methods: 利用大量的成人语音数据创建多语言ASR模型，如Whisper，并对其进行适应化以适应儿童语音。</li>
<li>results: 对Whisper模型进行finetuning后，对儿童语音识别表现出显著改善，而使用自然语言生成模型wav2vec2进行finetuning则超过了Whisper模型的表现。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.
</details>
<details>
<summary>摘要</summary>
自动语音识别（ASR）系统经常遇到儿童语音识别问题，原因在于缺乏儿童语音数据集，以训练适合儿童的 ASR 模型。然而，有大量已注解的成人语音数据集，用于创建多语言 ASR 模型，如呐喊（Whisper）。我们的工作旨在探讨是否可以将这些模型适应儿童语音，以提高 ASR 性能。此外，我们还比较了呐喊儿童化的模型与自动学习的 wav2vec2 模型，并证明了后者在儿童语音识别中表现更优。
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-Between-VoLTE-and-non-VoLTE-Voice-Calls-During-Mobility-in-Commercial-Deployment-A-Drive-Test-Based-Analysis"><a href="#Performance-Comparison-Between-VoLTE-and-non-VoLTE-Voice-Calls-During-Mobility-in-Commercial-Deployment-A-Drive-Test-Based-Analysis" class="headerlink" title="Performance Comparison Between VoLTE and non-VoLTE Voice Calls During Mobility in Commercial Deployment: A Drive Test-Based Analysis"></a>Performance Comparison Between VoLTE and non-VoLTE Voice Calls During Mobility in Commercial Deployment: A Drive Test-Based Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12397">http://arxiv.org/abs/2307.12397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashed Hasan Ratul, Muhammad Iqbal, Jen-Yi Pan, Mohammad Mahadi Al Deen, Mohammad Tawhid Kawser, Mohammad Masum Billah</li>
<li>for: 这个论文主要研究了 Voice over LTE (VoLTE) 技术对移动通信网络的性能优化的影响，尤其是 call setup delay 和用户设备 (UE) 电池寿命。</li>
<li>methods: 该研究使用了 XCAL 驱动测试工具收集实时网络参数数据，并对 VoLTE 和非 VoLTE 语音呼电中的实时网络特性进行分析。</li>
<li>results: 研究发现，使用 VoLTE 技术可以提高 call setup delay 的速度和 UE 电池寿命，并且在 VoLTE 和非 VoLTE 语音呼电中比较研究了 DRX 机制。这些结果可以帮助优化移动通信网络的质量服务 (QoS)。<details>
<summary>Abstract</summary>
The optimization of network performance is vital for the delivery of services using standard cellular technologies for mobile communications. Call setup delay and User Equipment (UE) battery savings significantly influence network performance. Improving these factors is vital for ensuring optimal service delivery. In comparison to traditional circuit-switched voice calls, VoLTE (Voice over LTE) technology offers faster call setup durations and better battery-saving performance. To validate these claims, a drive test was carried out using the XCAL drive test tool to collect real-time network parameter details in VoLTE and non-VoLTE voice calls. The findings highlight the analysis of real-time network characteristics, such as the call setup delay calculation, battery-saving performance, and DRX mechanism. The study contributes to the understanding of network optimization strategies and provides insights for enhancing the quality of service (QoS) in mobile communication networks. Examining VoLTE and non-VoLTE operations, this research highlights the substantial energy savings obtained by VoLTE. Specifically, VoLTE saves approximately 60.76% of energy before the Service Request and approximately 38.97% of energy after the Service Request. Moreover, VoLTE to VoLTE calls have a 72.6% faster call setup delay than non-VoLTE-based LTE to LTE calls, because of fewer signaling messages required. Furthermore, as compared to non-VoLTE to non-VoLTE calls, VoLTE to non-VoLTE calls offer an 18.6% faster call setup delay. These results showcase the performance advantages of VoLTE and reinforce its potential for offering better services in wireless communication networks.
</details>
<details>
<summary>摘要</summary>
网络性能优化对于通过标准无线通信技术提供服务是非常重要。启动延迟和用户设备（UE）电池寿命具有重要影响力。提高这些因素对于确保优质服务的提供是非常重要。与传统的循环连接语音电话相比，VoLTE（音声在LTE）技术提供了更快的启动延迟和更好的电池寿命性能。为了证明这些声明，我们使用了XCAL驱动测试工具来收集实时网络参数详细信息在VoLTE和非VoLTE语音电话中。研究结果显示了实时网络特性的分析，包括启动延迟计算、电池寿命性能和DRX机制。这项研究对网络优化策略的理解和服务质量（QoS）的提高做出了贡献。对VoLTE和非VoLTE操作进行比较，这项研究显示了VoLTE可以获得约60.76%的电源储存和约38.97%的电源储存。此外，VoLTE到VoLTE电话的启动延迟比非VoLTE基于LTE到LTE电话的启动延迟更快，这是因为VoLTE需要 fewer signaling messages。此外，VoLTE到非VoLTE电话的启动延迟比非VoLTE到非VoLTE电话的启动延迟更快，这是因为VoLTE需要更少的信号处理。这些结果显示了VoLTE的性能优势，并证明它在无线通信网络中可以提供更好的服务。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/eess.AS_2023_07_24/" data-id="clpxp6c8l013zee889uo245ti" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.CV_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T13:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.CV_2023_07_24/">cs.CV - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Automotive-Object-Detection-via-Learning-Sparse-Events-by-Temporal-Dynamics-of-Spiking-Neurons"><a href="#Automotive-Object-Detection-via-Learning-Sparse-Events-by-Temporal-Dynamics-of-Spiking-Neurons" class="headerlink" title="Automotive Object Detection via Learning Sparse Events by Temporal Dynamics of Spiking Neurons"></a>Automotive Object Detection via Learning Sparse Events by Temporal Dynamics of Spiking Neurons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12900">http://arxiv.org/abs/2307.12900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hu Zhang, Luziwei Leng, Kaiwei Che, Qian Liu, Jie Cheng, Qinghai Guo, Jiangxing Liao, Ran Cheng</li>
<li>for: 这篇论文旨在应用事件驱动神经网络（SNN）来进行事件基数据的物体探测。</li>
<li>methods: 这篇论文使用了SNN的传入层、遗传层和调节层，并使用了脉幅动力学来调节网络活动。</li>
<li>results: 这篇论文使用了SNN取得了47.7%的精确率（map50）在Gen1标准dataset上，比前一代SNN提高9.7%，并且比使用注意力机制的对照模型还要好。<details>
<summary>Abstract</summary>
Event-based sensors, with their high temporal resolution (1us) and dynamical range (120dB), have the potential to be deployed in high-speed platforms such as vehicles and drones. However, the highly sparse and fluctuating nature of events poses challenges for conventional object detection techniques based on Artificial Neural Networks (ANNs). In contrast, Spiking Neural Networks (SNNs) are well-suited for representing event-based data due to their inherent temporal dynamics. In particular, we demonstrate that the membrane potential dynamics can modulate network activity upon fluctuating events and strengthen features of sparse input. In addition, the spike-triggered adaptive threshold can stabilize training which further improves network performance. Based on this, we develop an efficient spiking feature pyramid network for event-based object detection. Our proposed SNN outperforms previous SNNs and sophisticated ANNs with attention mechanisms, achieving a mean average precision (map50) of 47.7% on the Gen1 benchmark dataset. This result significantly surpasses the previous best SNN by 9.7% and demonstrates the potential of SNNs for event-based vision. Our model has a concise architecture while maintaining high accuracy and much lower computation cost as a result of sparse computation. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
Event-based 感测器，具有高度的时间分辨率（1us）和动态范围（120dB），有可能在高速平台 such as 车辆和无人机上进行部署。然而，事件的高度稀疏和波动性带来了传统的人工神经网络（ANNs）中的挑战。相比之下，脉冲神经网络（SNNs）因其内置的时间动力学而适用于表示事件基本数据。具体来说，我们表明了膜电位动力学可以在事件波动时调整网络活动，并强制特征的稀疏输入。此外，使用触发适应阈值可以稳定训练，从而进一步提高网络性能。基于这些，我们开发了高效的脉冲特征峰网络，用于事件基本对象检测。我们的提出的 SNN 在 Gen1 测试集上达到了47.7%的 Mean Average Precision（map50），这比前一代最佳 SNN 高出9.7%，并证明了 SNN 在事件基本视觉中的潜力。我们的模型具有简洁的架构，同时保持高度准确和较低的计算成本。我们的代码将公开。
</details></li>
</ul>
<hr>
<h2 id="Data-free-Black-box-Attack-based-on-Diffusion-Model"><a href="#Data-free-Black-box-Attack-based-on-Diffusion-Model" class="headerlink" title="Data-free Black-box Attack based on Diffusion Model"></a>Data-free Black-box Attack based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12872">http://arxiv.org/abs/2307.12872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo</li>
<li>for: 增强数据隐身攻击的效率和准确性，使用扩散模型生成数据来训练代理模型。</li>
<li>methods: 使用扩散模型生成数据，并提出了干扰码修饰（LCA）方法来指导扩散模型生成数据，使得生成的数据可以更好地满足目标模型的批判标准。</li>
<li>results: 通过使用干扰码修饰方法，可以使扩散模型生成的数据更加符合目标模型的批判标准，并且可以提高黑盒攻击的成功率和减少查询预算。<details>
<summary>Abstract</summary>
Since the training data for the target model in a data-free black-box attack is not available, most recent schemes utilize GANs to generate data for training substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training. Despite the data generated by the diffusion model exhibits high quality, it presents diverse domain distributions and contains many samples that do not meet the discriminative criteria of the target model. To further facilitate the diffusion model to generate data suitable for the target model, we propose a Latent Code Augmentation (LCA) method to guide the diffusion model in generating data. With the guidance of LCA, the data generated by the diffusion model not only meets the discriminative criteria of the target model but also exhibits high diversity. By utilizing this data, it is possible to train substitute model that closely resemble the target model more efficiently. Extensive experiments demonstrate that our LCA achieves higher attack success rates and requires fewer query budgets compared to GANs-based schemes for different target models.
</details>
<details>
<summary>摘要</summary>
由于目标模型的训练数据不可获取，最近的方案多利用GANs生成数据进行训练代理模型。然而，这些GANs基本方案受到低训练效率和低生成质量的限制。为了突破这些限制，我们考虑使用扩散模型来生成数据，并提出了基于扩散模型的数据自由黑盒攻击方案，以提高代理训练的效率和准确率。尽管扩散模型生成的数据具有高质量，但它们具有多样化的领域分布和含有大量不符合目标模型的检准标准的样本。为了更好地使用扩散模型生成数据，我们提出了秘密代码增强（LCA）方法，以导引扩散模型生成数据。通过LCA的引导，扩散模型生成的数据不仅符合目标模型的检准标准，而且具有高多样性。通过利用这些数据，我们可以更高效地训练代理模型，并且需要更少的查询预算。我们的LCA在不同的目标模型上实现了更高的攻击成功率和更低的查询预算。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Latent-Space-of-Diffusion-Models-through-the-Lens-of-Riemannian-Geometry"><a href="#Understanding-the-Latent-Space-of-Diffusion-Models-through-the-Lens-of-Riemannian-Geometry" class="headerlink" title="Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry"></a>Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12868">http://arxiv.org/abs/2307.12868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, Youngjung Uh</li>
<li>for: 这 paper 旨在更好地理解Diffusion Model（DM）的潜在空间，从几何学角度进行分析。</li>
<li>methods: 这 paper 使用了 pullback 度量来找到 $\mathcal{X}$ 中的本地几何基底和其对应的 $\mathcal{H}$ 中的中间特征图。</li>
<li>results: 这 paper 发现了一种不supervised image editing能力，通过 traversal 在 $\mathbf{x}$ 空间中。此外，paper 还分析了这些结构如何随 diffusion 步骤的变化，以及如何基于文本条件进行修改。<details>
<summary>Abstract</summary>
Despite the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\mathbf{x}_t \in \mathcal{X}$, we analyze them from a geometrical perspective. Specifically, we utilize the pullback metric to find the local latent basis in $\mathcal{X}$ and their corresponding local tangent basis in $\mathcal{H}$, the intermediate feature maps of DMs. The discovered latent basis enables unsupervised image editing capability through latent space traversal. We investigate the discovered structure from two perspectives. First, we examine how geometric structure evolves over diffusion timesteps. Through analysis, we show that 1) the model focuses on low-frequency components early in the generative process and attunes to high-frequency details later; 2) At early timesteps, different samples share similar tangent spaces; and 3) The simpler datasets that DMs trained on, the more consistent the tangent space for each timestep. Second, we investigate how the geometric structure changes based on text conditioning in Stable Diffusion. The results show that 1) similar prompts yield comparable tangent spaces; and 2) the model depends less on text conditions in later timesteps. To the best of our knowledge, this paper is the first to present image editing through $\mathbf{x}$-space traversal and provide thorough analyses of the latent structure of DMs.
</details>
<details>
<summary>摘要</summary>
不withstanding the success of diffusion models (DMs), we still lack a thorough understanding of their latent space. To understand the latent space $\mathbf{x}_t \in \mathcal{X}$, we analyze them from a geometrical perspective. Specifically, we utilize the pullback metric to find the local latent basis in $\mathcal{X}$ and their corresponding local tangent basis in $\mathcal{H}$, the intermediate feature maps of DMs. The discovered latent basis enables unsupervised image editing capability through latent space traversal. We investigate the discovered structure from two perspectives. First, we examine how geometric structure evolves over diffusion timesteps. Through analysis, we show that 1) the model focuses on low-frequency components early in the generative process and attunes to high-frequency details later; 2) At early timesteps, different samples share similar tangent spaces; and 3) The simpler datasets that DMs trained on, the more consistent the tangent space for each timestep. Second, we investigate how the geometric structure changes based on text conditioning in Stable Diffusion. The results show that 1) similar prompts yield comparable tangent spaces; and 2) the model depends less on text conditions in later timesteps. To the best of our knowledge, this paper is the first to present image editing through $\mathbf{x}$-space traversal and provide thorough analyses of the latent structure of DMs.Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. The traditional Chinese form of the translation is slightly different due to the difference in word order and character usage.
</details></li>
</ul>
<hr>
<h2 id="Treatment-Outcome-Prediction-for-Intracerebral-Hemorrhage-via-Generative-Prognostic-Model-with-Imaging-and-Tabular-Data"><a href="#Treatment-Outcome-Prediction-for-Intracerebral-Hemorrhage-via-Generative-Prognostic-Model-with-Imaging-and-Tabular-Data" class="headerlink" title="Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data"></a>Treatment Outcome Prediction for Intracerebral Hemorrhage via Generative Prognostic Model with Imaging and Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12858">http://arxiv.org/abs/2307.12858</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/top-gpm">https://github.com/med-air/top-gpm</a></li>
<li>paper_authors: Wenao Ma, Cheng Chen, Jill Abrigo, Calvin Hoi-Kwan Mak, Yuqi Gong, Nga Yan Chan, Chu Han, Zaiyi Liu, Qi Dou</li>
<li>For: 预测 интра脑出血(ICH) 治疗结果* Methods: 使用 imaging 和 tabular 数据建立报告模型，并使用 variational autoencoder 模型生成低维度预测分数，以Address selection bias* Results: 对实际临床数据进行了广泛的实验，并显示了substantial improvement 在治疗结果预测 compared to 现有的状态 искусственный智能方法。Here’s the breakdown of each point:* For: 预测 ICH 治疗结果 (What the paper is written for)* Methods: 使用 imaging 和 tabular 数据建立报告模型，并使用 variational autoencoder 模型生成低维度预测分数，以Address selection bias (What methods the paper uses)* Results: 对实际临床数据进行了广泛的实验，并显示了substantial improvement 在治疗结果预测 compared to 现有的状态 искусственный智能方法。 (What results the paper gets)<details>
<summary>Abstract</summary>
Intracerebral hemorrhage (ICH) is the second most common and deadliest form of stroke. Despite medical advances, predicting treat ment outcomes for ICH remains a challenge. This paper proposes a novel prognostic model that utilizes both imaging and tabular data to predict treatment outcome for ICH. Our model is trained on observational data collected from non-randomized controlled trials, providing reliable predictions of treatment success. Specifically, we propose to employ a variational autoencoder model to generate a low-dimensional prognostic score, which can effectively address the selection bias resulting from the non-randomized controlled trials. Importantly, we develop a variational distributions combination module that combines the information from imaging data, non-imaging clinical data, and treatment assignment to accurately generate the prognostic score. We conducted extensive experiments on a real-world clinical dataset of intracerebral hemorrhage. Our proposed method demonstrates a substantial improvement in treatment outcome prediction compared to existing state-of-the-art approaches. Code is available at https://github.com/med-air/TOP-GPM
</details>
<details>
<summary>摘要</summary>
中风血盖（ICH）是第二常见且最致命的 stroke 型。医学进步不withstanding，预测 IC 的治疗结果仍然是一大挑战。本文提出了一种新的 прогности 模型，该模型利用了 Both imaging 和 tabular data 预测 IC 的治疗结果。我们的模型在非随机化控制试验中收集的 observational 数据上进行训练，以提供可靠的治疗成功预测。特别是，我们提出了一种 variational autoencoder 模型，用于生成低维度的 прогности 分数，以有效地 Addressing the selection bias  resulting from non-randomized controlled trials。我们还开发了一种 variational distributions combination module，用于组合 imaging 数据、非 imaging клиниче数据和治疗分配信息，以准确生成 prognostic 分数。我们对实际患有中风血盖的临床数据进行了广泛的实验，并证明了我们的提出方法可以具有显著改善的治疗结果预测效果，比现有的状态 искусственный智能方法更好。代码可以在 https://github.com/med-air/TOP-GPM 上找到。
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Video-Pretraining-for-Long-Term-Activity-Forecasting"><a href="#Multiscale-Video-Pretraining-for-Long-Term-Activity-Forecasting" class="headerlink" title="Multiscale Video Pretraining for Long-Term Activity Forecasting"></a>Multiscale Video Pretraining for Long-Term Activity Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12854">http://arxiv.org/abs/2307.12854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reuben Tan, Matthias De Lange, Michael Iuzzolino, Bryan A. Plummer, Kate Saenko, Karl Ridgeway, Lorenzo Torresani</li>
<li>for: 预测人类活动长期趋势，提高机器学习模型对未看过数据的泛化能力。</li>
<li>methods: 提出了一种新的自我监督预训练方法—多 scales video pretraining (MVP)，通过学习预测视频片断的上下文化表示来学习Robust Representation。</li>
<li>results: 与现有方法进行比较，MVP在长期动作预测和视频概要预测任务上表现出了显著的性能优势，对于视频概要预测任务，MVP的表现提高了Relative Performance Gain超过20%。<details>
<summary>Abstract</summary>
Long-term activity forecasting is an especially challenging research problem because it requires understanding the temporal relationships between observed actions, as well as the variability and complexity of human activities. Despite relying on strong supervision via expensive human annotations, state-of-the-art forecasting approaches often generalize poorly to unseen data. To alleviate this issue, we propose Multiscale Video Pretraining (MVP), a novel self-supervised pretraining approach that learns robust representations for forecasting by learning to predict contextualized representations of future video clips over multiple timescales. MVP is based on our observation that actions in videos have a multiscale nature, where atomic actions typically occur at a short timescale and more complex actions may span longer timescales. We compare MVP to state-of-the-art self-supervised video learning approaches on downstream long-term forecasting tasks including long-term action anticipation and video summary prediction. Our comprehensive experiments across the Ego4D and Epic-Kitchens-55/100 datasets demonstrate that MVP out-performs state-of-the-art methods by significant margins. Notably, MVP obtains a relative performance gain of over 20% accuracy in video summary forecasting over existing methods.
</details>
<details>
<summary>摘要</summary>
长期活动预测是一个特别困难的研究问题，因为它需要理解视频中观察到的动作之间的时间关系，以及人类活动的多样性和复杂性。尽管通过昂贵的人工纠正，使用现状的预测方法可以取得良好的性能，但这些方法在未看到的数据上 generalize 很差。为了解决这个问题，我们提出了多Scale Video Pretraining（MVP），一种新的自我监督预训练方法，该方法通过学习预测视频clip的多Scale representation来学习Robust的表示。MVP基于我们观察到的动作在视频中有多Scale性质， atomic 动作通常发生在短时间尺度，而更复杂的动作可能 span 更长的时间尺度。我们与现状的自我监督视频学习方法进行比较，在下游长期预测任务中，包括长期动作预测和视频概要预测。我们在Ego4D和Epic-Kitchens-55/100 datasets上进行了广泛的实验，结果表明MVP在性能上超过了现状的方法，具体来说，MVP在视频概要预测任务中取得了20%以上的相对性能提升。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion"><a href="#Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion" class="headerlink" title="Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion"></a>Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12853">http://arxiv.org/abs/2307.12853</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. I. Ugwu, S. Casarin, O. Lanz</li>
<li>for: 这paper的目的是提出一种基于2DConvolutional Neural Networks的多模态脏器部分 segmentation方法，以提高诊断和治疗的效率和准确性。</li>
<li>methods: 这paper使用了一种基于多视图的2DConvolutional Neural Networks方法，通过在多个方向上进行2D convolution来学习多维特征，并通过卷积缓存共享机制来重新inteporate第三维信息。</li>
<li>results: 对于Multi-Modality Abdominal Multi-Organ Segmentation (AMOS)和Multi-Atlas Labeling Beyond the Cranial Vault (BTCV)数据集，这paper的方法得到了较高的效率和相当于顶尖模型的性能。<details>
<summary>Abstract</summary>
As a fundamental part of computational healthcare, Computer Tomography (CT) and Magnetic Resonance Imaging (MRI) provide volumetric data, making the development of algorithms for 3D image analysis a necessity. Despite being computationally cheap, 2D Convolutional Neural Networks can only extract spatial information. In contrast, 3D CNNs can extract three-dimensional features, but they have higher computational costs and latency, which is a limitation for clinical practice that requires fast and efficient models. Inspired by the field of video action recognition we propose a new 2D-based model dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensional features at 2D CNN's complexity. More precisely multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. The effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details>
<details>
<summary>摘要</summary>
为computational healthcare的基本部分，计算机断层成像（CT）和核磁共振成像（MRI）提供了体积数据，因此开发3D图像分析算法是必要的。然而，使用2D卷积神经网络（CNN）只能提取空间信息，而3D CNN则可以提取三维特征，但它们的计算成本和延迟较高，这限制了临床实践中需要快速和高效的模型。 drawing inspiration from the field of video action recognition, we propose a new 2D-based model called Slice SHift UNet (SSH-UNet) that encodes three-dimensional features at the complexity of 2D CNNs. Specifically, we perform 2D convolutions along the three orthogonal planes of a volume and share weights across different planes to collaboratively learn multi-view features. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. Our approach is validated on the Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Vertebra-Localization-and-Identification-from-CT-Images"><a href="#Multi-View-Vertebra-Localization-and-Identification-from-CT-Images" class="headerlink" title="Multi-View Vertebra Localization and Identification from CT Images"></a>Multi-View Vertebra Localization and Identification from CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12845">http://arxiv.org/abs/2307.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images">https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images</a></li>
<li>paper_authors: Han Wu, Jiadong Zhang, Yu Fang, Zhentao Liu, Nizhuan Wang, Zhiming Cui, Dinggang Shen</li>
<li>for: 本研究旨在提高CT图像中骨架的准确位置和识别率，为各种临床应用提供技术支持。</li>
<li>methods: 本研究提出了一种基于多视图的骨架localization和识别方法，将3D问题转化为2D的本地化和识别任务。而无需裁剪patch操作，我们的方法可以自然地学习多视图全局信息。此外，为更好地捕捉不同视角的骨架结构信息，我们还提出了一种多视图对比学习策略来预训练后处理网络。</li>
<li>results: 我们的方法可以在只使用两个2D网络的情况下，准确地位置和识别骨架在CT图像中。并且与状态艺术方法相比，我们的方法显著超越了其表现。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images上获取。</a><details>
<summary>Abstract</summary>
Accurately localizing and identifying vertebrae from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information. In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views. Without the limitation of the 3D cropped patch, our method can learn the multi-view global information naturally. Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone. Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae. Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently. Our code is available at https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images.
</details>
<details>
<summary>摘要</summary>
通过CT图像进行精准的骨vertebrae的本地化和识别是许多临床应用中的关键。然而，大多数现有的尝试都是在3D中进行，通过剪辑patch操作，它们受到了大量计算成本和局部信息的限制。在这篇论文中，我们提出了基于多视图的骨vertebrae本地化和识别方法，将3D问题转化为2D本地化和识别任务。不同于剪辑patch的限制，我们的方法可以自然地学习多视图的全局信息。此外，为了更好地捕捉不同视图角度的骨 vertebrae结构信息，我们开发了一种多视图冲击学习策略来预训练干部。此外，我们还提出了一种序列损失来保持骨vertebrae中的顺序结构嵌入。评估结果表明，只有两个2D网络，我们的方法可以在CT图像中精准地本地化和识别骨vertebrae，并在状态艺术方法上一直保持领先。我们的代码可以在https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images中找到。
</details></li>
</ul>
<hr>
<h2 id="Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering"><a href="#Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering" class="headerlink" title="Learning Provably Robust Estimators for Inverse Problems via Jittering"></a>Learning Provably Robust Estimators for Inverse Problems via Jittering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12822">http://arxiv.org/abs/2307.12822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mli-lab/robust_reconstructors_via_jittering">https://github.com/mli-lab/robust_reconstructors_via_jittering</a></li>
<li>paper_authors: Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</li>
<li>for: 本研究 investigate whether jittering, a simple regularization technique, can be used to train efficient worst-case robust deep neural networks for inverse problems.</li>
<li>methods: 本研究使用了一种简单的正则化技术，即在训练时添加均匀的高斯噪声，来提高深度神经网络的最坏情况Robustness。</li>
<li>results: 研究发现，使用jittering可以实现最佳的 $\ell_2$-worst-case robust estimator for linear denoising,并且通过训练深度神经网络（U-net）对自然图像减 noise、deconvolution和加速Magnetic Resonance Imaging（MRI）进行了实验，结果表明，jittering可以增强最坏情况Robustness，但可能不适用于 inverse problems beyond denoising。此外，研究还发现，训练在真实数据上，通常包含一些噪声，可以提高模型的Robustness。<details>
<summary>Abstract</summary>
Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.
</details>
<details>
<summary>摘要</summary>
深度神经网络在逆问题中提供了优秀的性能，但神经网络可能会受到敌意或最坏情况的攻击。这引发了我们是否可以有效地训练神经网络以适应最坏情况的问题。在这篇论文中，我们研究了加入 ISO 型 Gaussian 噪声 durante 训练是否可以有效地学习最坏情况Robust 估计器。虽然这种技术在预测 classification 任务中已经得到了广泛的研究，但对于逆问题的研究尚未得到系统的探讨。在这篇论文中，我们提出了一种新的分析方法，用于计算最佳 $\ell_2$ 最坏情况Robust 估计器。我们发现，在线性噪声降解 задании下，加入噪声可以实现最佳的Robust 性能。此外，我们通过训练深度神经网络（U-net）进行自然图像降解、减 convolution 和加速核磁共振成像（MRI）任务，实验结果表明，加入噪声可以显著提高最坏情况Robust 性能，但在涉及到逆问题的更高级别上可能不是最佳的方法。此外，我们的结果还 imply 训练在真实数据上，常常包含一定的噪声，可能会有一定的 robustness 提升。
</details></li>
</ul>
<hr>
<h2 id="Exposing-the-Troublemakers-in-Described-Object-Detection"><a href="#Exposing-the-Troublemakers-in-Described-Object-Detection" class="headerlink" title="Exposing the Troublemakers in Described Object Detection"></a>Exposing the Troublemakers in Described Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12813">http://arxiv.org/abs/2307.12813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shikras/d-cube">https://github.com/shikras/d-cube</a></li>
<li>paper_authors: Chi Xie, Zhao Zhang, Yixuan Wu, Feng Zhu, Rui Zhao, Shuang Liang</li>
<li>for: 本研究旨在提高Open-Vocabulary object Detection (OVD)和Referring Expression Comprehension (REC)的实际应用，通过扩展分类名称为描述语言表达来提高DOD任务的研究基础。</li>
<li>methods: 我们采用了Description Detection Dataset ($D^3$)，其特点是包含flexible language expressions和全面的描述对象标注，并评估了之前的SOTA方法在$D^3$上的性能。此外，我们还提出了一种基线方法，通过重新构建训练数据和引入二分类子任务来大幅提高REC方法的性能。</li>
<li>results: 我们发现现有REC方法在$D^3$上存在许多问题，包括信任分数、拒绝负例、多目标场景等。此外，最新的bi-functional方法也不太适用于DOD任务，因为它们在训练和测试过程中分离的程序和推理策略。我们的基线方法在$D^3$上表现出色，大幅超越现有方法。<details>
<summary>Abstract</summary>
Detecting objects based on language descriptions is a popular task that includes Open-Vocabulary object Detection (OVD) and Referring Expression Comprehension (REC). In this paper, we advance them to a more practical setting called Described Object Detection (DOD) by expanding category names to flexible language expressions for OVD and overcoming the limitation of REC to only grounding the pre-existing object. We establish the research foundation for DOD tasks by constructing a Description Detection Dataset ($D^3$), featuring flexible language expressions and annotating all described objects without omission. By evaluating previous SOTA methods on $D^3$, we find some troublemakers that fail current REC, OVD, and bi-functional methods. REC methods struggle with confidence scores, rejecting negative instances, and multi-target scenarios, while OVD methods face constraints with long and complex descriptions. Recent bi-functional methods also do not work well on DOD due to their separated training procedures and inference strategies for REC and OVD tasks. Building upon the aforementioned findings, we propose a baseline that largely improves REC methods by reconstructing the training data and introducing a binary classification sub-task, outperforming existing methods. Data and code is available at https://github.com/shikras/d-cube.
</details>
<details>
<summary>摘要</summary>
检测基于语言描述的对象是一个受欢迎的任务，包括开放词汇对象检测（OVD）和引用表达理解（REC）。在这篇论文中，我们将其推进到更实用的设定中，称为描述对象检测（DOD），通过扩展类别名称到 flexible language expression 来进一步提高 OVD 的精度。我们建立了描述检测任务的研究基础，constructing a Description Detection Dataset ($D^3$), featuring flexible language expressions and annotating all described objects without omission。通过评估先前的 SOTA 方法在 $D^3$ 上，我们发现了一些“困难者”，其中 REC 方法受到负样本、多目标场景和 confidence score 等限制，而 OVD 方法则面临长和复杂的描述句所带来的限制。最近的 bi-functional 方法也不太适合 DOD 任务，因为它们在 REC 和 OVD 任务之间具有分开的训练过程和推理策略。基于以上发现，我们提出了一个基线方案，可以大幅提高 REC 方法的性能，通过重新构建训练数据和引入 binary classification 子任务。数据和代码可以在 https://github.com/shikras/d-cube 上获取。
</details></li>
</ul>
<hr>
<h2 id="Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification"><a href="#Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification" class="headerlink" title="Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification"></a>Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12790">http://arxiv.org/abs/2307.12790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonrepo-keeper/gcnn-ec">https://github.com/anonrepo-keeper/gcnn-ec</a></li>
<li>paper_authors: Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</li>
<li>for: 这个研究探讨了图像分类 tasks 上的图形学习模型，具体来说是使用 Graph Neural Networks (GNNs) 和 edge convolution 来强化图像之间的连接。</li>
<li>methods: 本研究提出了一个新的 GNN 模型，融合了 GNNs 和 edge convolution，以利用 RGB 通道特征值之间的连接来强化图像的表现。</li>
<li>results: 比较 GNN 和 pre-trained DNNs，GNN 能够在 MedMNIST 数据集上显示出与 DNNs 相似的表现，并且仅需要1000个参数，相较于 DNNs 的训练时间和数据量。<details>
<summary>Abstract</summary>
Graph-based neural network models are gaining traction in the field of representation learning due to their ability to uncover latent topological relationships between entities that are otherwise challenging to identify. These models have been employed across a diverse range of domains, encompassing drug discovery, protein interactions, semantic segmentation, and fluid dynamics research. In this study, we investigate the potential of Graph Neural Networks (GNNs) for medical image classification. We introduce a novel model that combines GNNs and edge convolution, leveraging the interconnectedness of RGB channel feature values to strongly represent connections between crucial graph nodes. Our proposed model not only performs on par with state-of-the-art Deep Neural Networks (DNNs) but does so with 1000 times fewer parameters, resulting in reduced training time and data requirements. We compare our Graph Convolutional Neural Network (GCNN) to pre-trained DNNs for classifying MedMNIST dataset classes, revealing promising prospects for GNNs in medical image analysis. Our results also encourage further exploration of advanced graph-based models such as Graph Attention Networks (GAT) and Graph Auto-Encoders in the medical imaging domain. The proposed model yields more reliable, interpretable, and accurate outcomes for tasks like semantic segmentation and image classification compared to simpler GCNNs
</details>
<details>
<summary>摘要</summary>
“基于图的神经网络模型在 Representation Learning 领域受到广泛应用，因为它们可以捕捉到难以识别的实体之间的隐藏 topological 关系。这些模型在药物发现、蛋白结合、 semantic segmentation 和 fluid dynamics 等领域都有应用。在这项研究中，我们研究了医学图像分类中 Graph Neural Networks（GNNs）的潜力。我们提出了一种新的模型，它将 GNNs 和边 convolution 结合起来，利用 RGB 通道特征值之间的连接来强大地表示图像中关键节点之间的连接。我们的提出的模型不仅与 state-of-the-art Deep Neural Networks（DNNs）的性能相当，而且只需要1000倍 fewer 参数，从而减少了训练时间和数据需求。我们对 MedMNIST 数据集类型进行比较，发现 GNNs 在医学图像分类中有极好的前景。我们的结果还鼓励了在医学图像分类和 semantic segmentation 等任务中进一步探索更高级别的图基于模型，如 Graph Attention Networks（GAT）和 Graph Auto-Encoders。我们的模型在 semantic segmentation 和图像分类任务中比 simpler GNNs 更可靠、更加可解释、更高精度。”
</details></li>
</ul>
<hr>
<h2 id="Fast-Full-frame-Video-Stabilization-with-Iterative-Optimization"><a href="#Fast-Full-frame-Video-Stabilization-with-Iterative-Optimization" class="headerlink" title="Fast Full-frame Video Stabilization with Iterative Optimization"></a>Fast Full-frame Video Stabilization with Iterative Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12774">http://arxiv.org/abs/2307.12774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyue Zhao, Xin Li, Zhan Peng, Xianrui Luo, Xinyi Ye, Hao Lu, Zhiguo Cao</li>
<li>for: 提高视频稳定性和计算效率的交互优化方法</li>
<li>methods: 使用Synthetic datasets、两级（粗到细）稳定算法、可信度地图和反射推断等技术</li>
<li>results: 提供了高效且视觉质量高的视频稳定方法，并通过实验证明其在计算速度和视觉质量两个方面的优势<details>
<summary>Abstract</summary>
Video stabilization refers to the problem of transforming a shaky video into a visually pleasing one. The question of how to strike a good trade-off between visual quality and computational speed has remained one of the open challenges in video stabilization. Inspired by the analogy between wobbly frames and jigsaw puzzles, we propose an iterative optimization-based learning approach using synthetic datasets for video stabilization, which consists of two interacting submodules: motion trajectory smoothing and full-frame outpainting. First, we develop a two-level (coarse-to-fine) stabilizing algorithm based on the probabilistic flow field. The confidence map associated with the estimated optical flow is exploited to guide the search for shared regions through backpropagation. Second, we take a divide-and-conquer approach and propose a novel multiframe fusion strategy to render full-frame stabilized views. An important new insight brought about by our iterative optimization approach is that the target video can be interpreted as the fixed point of nonlinear mapping for video stabilization. We formulate video stabilization as a problem of minimizing the amount of jerkiness in motion trajectories, which guarantees convergence with the help of fixed-point theory. Extensive experimental results are reported to demonstrate the superiority of the proposed approach in terms of computational speed and visual quality. The code will be available on GitHub.
</details>
<details>
<summary>摘要</summary>
视频稳定化问题是将晃动视频转化为美观的视频。计算速度和视觉质量之间的平衡问题一直是视频稳定化领域的开放问题。我们提出了基于iterative optimization-based learning方法的视频稳定化方法，该方法包括两个互动子模块：运动轨迹缓和全帧补充。首先，我们开发了一种两级（粗细到细）稳定化算法，基于概率流场。利用估算的光学流场的信息来引导搜索共享区域，我们使用回传propagation。其次，我们提出了一种新的分解和融合策略，以生成稳定视频全帧视图。我们发现，通过iterative optimization方法，目标视频可以被视为非线性映射的稳定点，我们将视频稳定化问题解释为减少运动轨迹中的抖动量，以 garantía converge。我们通过实验报告了我们的方法的计算速度和视觉质量的超越性。代码将在GitHub上发布。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-Meta-Depth-Completion"><a href="#LiDAR-Meta-Depth-Completion" class="headerlink" title="LiDAR Meta Depth Completion"></a>LiDAR Meta Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12761">http://arxiv.org/abs/2307.12761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wbkit/reslan">https://github.com/wbkit/reslan</a></li>
<li>paper_authors: Wolfgang Boettcher, Lukas Hoyer, Ozan Unal, Ke Li, Dengxin Dai</li>
<li>for: 提高移动自适应系统中的深度估计精度</li>
<li>methods: 动态适应LiDAR扫描模式的深度完成网络</li>
<li>results: 与专业LiDAR模型相比，单个模型可以在多个LiDAR扫描模式上显示更好的性能，并且可以扩展到未经训练的扫描模式<details>
<summary>Abstract</summary>
Depth estimation is one of the essential tasks to be addressed when creating mobile autonomous systems. While monocular depth estimation methods have improved in recent times, depth completion provides more accurate and reliable depth maps by additionally using sparse depth information from other sensors such as LiDAR. However, current methods are specifically trained for a single LiDAR sensor. As the scanning pattern differs between sensors, every new sensor would require re-training a specialized depth completion model, which is computationally inefficient and not flexible. Therefore, we propose to dynamically adapt the depth completion model to the used sensor type enabling LiDAR adaptive depth completion. Specifically, we propose a meta depth completion network that uses data patterns derived from the data to learn a task network to alter weights of the main depth completion network to solve a given depth completion task effectively. The method demonstrates a strong capability to work on multiple LiDAR scanning patterns and can also generalize to scanning patterns that are unseen during training. While using a single model, our method yields significantly better results than a non-adaptive baseline trained on different LiDAR patterns. It outperforms LiDAR-specific expert models for very sparse cases. These advantages allow flexible deployment of a single depth completion model on different sensors, which could also prove valuable to process the input of nascent LiDAR technology with adaptive instead of fixed scanning patterns.
</details>
<details>
<summary>摘要</summary>
深度估算是创建移动自主系统中的一项重要任务。虽然单投射深度估算方法在最近有所进步，但深度完成提供更加准确和可靠的深度地图，并使用其他感知器 such as LiDAR 上的稀疏深度信息。然而，当前的方法都是特定的 LiDAR 扫描模式进行训练。因此，我们提出了动态适应 LiDAR 类型的深度完成模型，以便在不同的 LiDAR 扫描模式上进行适应性的深度完成。具体来说，我们提出了一个元深度完成网络，通过数据模式来学习一个任务网络，以修改主深度完成网络中的权重，以解决给定的深度完成任务。该方法能够在多个 LiDAR 扫描模式上工作，并且还能够对未在训练中看到的扫描模式进行泛化。使用单一模型，我们的方法能够在不同 LiDAR 扫描模式上显著提高结果，并且在非常稀疏的情况下也能够超越特定 LiDAR 模型。这些优势使得我们的方法可以在不同感知器上进行灵活的部署，这也可以证明有用于处理新型 LiDAR 技术的适应式扫描模式。
</details></li>
</ul>
<hr>
<h2 id="ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution"><a href="#ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution" class="headerlink" title="ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution"></a>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12751">http://arxiv.org/abs/2307.12751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee</li>
<li>for: 这个论文是为了解决单图超解析（SISR）问题，即将低分辨率（LR）图像提升到高分辨率（HR）图像的问题。</li>
<li>methods: 该论文提出了一种新的归一化可能函数（ICF），可以将输入图像缩放到不同的比例条件下，并且可以还原原始输入图像。基于该ICF，该论文提出了一种新的无监督SISR框架（ICF-SRSR），可以在实际世界中进行SR任务无需使用任何对应&#x2F;无对应的训练数据。</li>
<li>results: 该论文的实验表明，ICF-SRSR可以在无监督情况下处理SISR问题，并且在实际世界中表现更好于使用synthesized paired图像进行训练的方法。此外，ICF-SRSR还可以生成更加真实和可行的LR-HR对，使现有的监督SISR网络更加可靠。<details>
<summary>Abstract</summary>
Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice because of the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. By leveraging the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.
</details>
<details>
<summary>摘要</summary>
单图超分辨 (SISR) 是一个具有挑战性的不定性问题，旨在将给定的低分辨 (LR) 图像提升到高分辨 (HR) 对应的图像。由于实际获得LR-HR训练对的困难，现有的方法通常是使用简化下采样算法，如二次方程，进行模拟LR图像。这种方法在实践中可能存在大量的差异，这使得SR任务变得更加困难。为了解决这个问题，我们提议一种新的归一化可逆函数 (ICF)，可以将输入图像缩放，然后将原始输入图像还原，并且可以在不同的比例条件下进行缩放。通过利用我们提议的ICF，我们建立了一种新的自适应SISR框架 (ICF-SRSR)，可以在实际的SR任务中处理不需要使用任何配对/非配对训练数据。此外，我们的ICF-SRSR可以生成真实可行的LR-HR对，这可以使得现有的supervised SISR网络更加强大。我们进行了广泛的实验，并证明了我们的ICF-SRSR可以在不需要任何配对训练数据的情况下 Handle SISR任务。我们的ICF-SRSR在实际场景中表现出了supervised/非配对方法的相当性，并且在公共的benchmark datasets上达到了相当的性能。
</details></li>
</ul>
<hr>
<h2 id="CLIP-KD-An-Empirical-Study-of-Distilling-CLIP-Models"><a href="#CLIP-KD-An-Empirical-Study-of-Distilling-CLIP-Models" class="headerlink" title="CLIP-KD: An Empirical Study of Distilling CLIP Models"></a>CLIP-KD: An Empirical Study of Distilling CLIP Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12732">http://arxiv.org/abs/2307.12732</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/winycg/CLIP-KD">https://github.com/winycg/CLIP-KD</a></li>
<li>paper_authors: Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Yongjun Xu</li>
<li>for: 本文旨在静态小CLIP模型，通过与大教师CLIP模型的导师学习来提高性能。</li>
<li>methods: 本文提出了多种静态CLIP模型的精简策略，包括关系、特征、梯度和对比方式，以评估其影响CLIP精简。</li>
<li>results: 研究发现最简单的特征模仿与MSE损失最佳，并且交互对比学习和关系基于精简也对性能有益。在应用于多个学生网络，CLIP精简在零shotImageNet分类和跨模态检索benchmark上具有consistent的改进。<details>
<summary>Abstract</summary>
CLIP has become a promising language-supervised visual pre-training framework and achieves excellent performance over a wide range of tasks. This paper aims to distill small CLIP models supervised by a large teacher CLIP model. We propose several distillation strategies, including relation, feature, gradient and contrastive paradigm, to examine the impact on CLIP distillation. We show that the simplest feature mimicry with MSE loss performs best. Moreover, interactive contrastive learning and relation-based distillation are also critical in performance improvement. We apply the unified method to distill several student networks trained on 15 million (image, text) pairs. Distillation improves the student CLIP models consistently over zero-shot ImageNet classification and cross-modal retrieval benchmarks. We hope our empirical study will become an important baseline for future CLIP distillation research. The code is available at \url{https://github.com/winycg/CLIP-KD}.
</details>
<details>
<summary>摘要</summary>
CLIP 已成为一个有前途的语言监督 visual 预训练框架，在各种任务上表现出色。本文目的是将大 teacher CLIP 模型监督小 CLIP 模型进行学习减少。我们提出了多种减少策略，包括关系、特征、梯度和对比方法，以确定 CLIP 减少的影响。我们发现最简单的特征模仿方法使用 MSE 损失最佳。此外，交互式对比学习和关系基于减少也对性能进行了贡献。我们应用这种简化方法到多个学生网络，每个网络都在 15 百万（图像、文本）对的训练下进行学习。减少提高了学生 CLIP 模型在零shot ImageNet 分类和 cross-modal 检索标准准则上的表现。我们希望我们的实验研究将成为未来 CLIP 减少研究的重要基准。代码可以在 \url{https://github.com/winycg/CLIP-KD} 上获取。
</details></li>
</ul>
<hr>
<h2 id="COCO-O-A-Benchmark-for-Object-Detectors-under-Natural-Distribution-Shifts"><a href="#COCO-O-A-Benchmark-for-Object-Detectors-under-Natural-Distribution-Shifts" class="headerlink" title="COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts"></a>COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12730">http://arxiv.org/abs/2307.12730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba/easyrobust">https://github.com/alibaba/easyrobust</a></li>
<li>paper_authors: Xiaofeng Mao, Yuefeng Chen, Yao Zhu, Da Chen, Hang Su, Rong Zhang, Hui Xue<br>for:This paper aims to provide a comprehensive assessment of the robustness of object detection models under natural distribution shifts, and to introduce a new test dataset called COCO-O to benchmark the OOD robustness of detectors.methods:The authors use a large-scale dataset called COCO-O, which contains six types of natural distribution shifts, to evaluate the OOD robustness of more than 100 modern object detection models. They also conduct experiments to study the effect of different components of the detector, such as the backbone, detection head, and augmentation techniques, on its OOD robustness.results:The authors find that most classic detectors do not exhibit strong OOD generalization, and that the backbone is the most important part for robustness. They also find that an end-to-end detection transformer design does not bring any enhancement, and may even reduce robustness. Finally, they show that large-scale foundation models have made a great leap on robust object detection. The dataset will be available at <a target="_blank" rel="noopener" href="https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o.Here">https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o.Here</a> is the Chinese version of the three key points:for:这篇论文的目的是为了提供一个广泛的对象检测模型在自然分布变化下的robustness的评估，并为了 benchmarking 对象检测模型的OOD robustness而引入了COCO-O测试集。methods:作者使用了COCO-O测试集，这个测试集包含6种自然分布变化，来评估现代对象检测模型的OOD robustness。他们还进行了不同组件的检测器，如背bone、检测头和增强技术的效果的研究。results:作者发现大多数经典的检测器在自然分布变化下并没有强大的OOD普适性，背bone是检测器的robustness中最重要的部分。他们还发现，检测transformer设计并不提供任何改进，可能甚至会降低robustness。最后，他们发现大规模基础模型在robust对象检测方面做出了很大的进步。<details>
<summary>Abstract</summary>
Practical object detection application can lose its effectiveness on image inputs with natural distribution shifts. This problem leads the research community to pay more attention on the robustness of detectors under Out-Of-Distribution (OOD) inputs. Existing works construct datasets to benchmark the detector's OOD robustness for a specific application scenario, e.g., Autonomous Driving. However, these datasets lack universality and are hard to benchmark general detectors built on common tasks such as COCO. To give a more comprehensive robustness assessment, we introduce COCO-O(ut-of-distribution), a test dataset based on COCO with 6 types of natural distribution shifts. COCO-O has a large distribution gap with training data and results in a significant 55.7% relative performance drop on a Faster R-CNN detector. We leverage COCO-O to conduct experiments on more than 100 modern object detectors to investigate if their improvements are credible or just over-fitting to the COCO test set. Unfortunately, most classic detectors in early years do not exhibit strong OOD generalization. We further study the robustness effect on recent breakthroughs of detector's architecture design, augmentation and pre-training techniques. Some empirical findings are revealed: 1) Compared with detection head or neck, backbone is the most important part for robustness; 2) An end-to-end detection transformer design brings no enhancement, and may even reduce robustness; 3) Large-scale foundation models have made a great leap on robust object detection. We hope our COCO-O could provide a rich testbed for robustness study of object detection. The dataset will be available at https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o.
</details>
<details>
<summary>摘要</summary>
实际应用中的物体检测应用可能会在自然分布变化下失效。这个问题导致研究人员更加关注检测器在不同分布下的Robustness。现有的 dataset 用于测试检测器的 OOD Robustness，如 autonomous driving 应用场景。然而，这些 dataset 缺乏通用性，难以测试基于 common task 如 COCO 的检测器。为了给出更加全面的 Robustness 评估，我们引入 COCO-O（out-of-distribution）测试集，基于 COCO 的6种自然分布变化。COCO-O 与训练数据之间存在大的分布差，导致 Faster R-CNN 检测器的Relative performance drop 为 55.7%。我们使用 COCO-O 对 более than 100 种现代 object detection 算法进行实验，以查找他们是否具有可靠的 OOD 通用性。结果显示，大多数早期的检测器不具有强的 OOD 泛化能力。我们进一步研究了检测器的architecture design，增强技术和预训练技术的Robustness效果。我们发现：1）与检测头或 neck 相比，后凹是检测器 Robustness 的关键部分; 2） end-to-end 检测转换设计不具有改善效果，可能 même 减少 Robustness; 3）大规模基础模型在 Robust object detection 方面做出了很大的进步。我们希望 COCO-O 可以为 Robustness 研究提供一个丰富的测试平台。 dataset 将在 https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o 上提供。
</details></li>
</ul>
<hr>
<h2 id="Persistent-Transient-Duality-A-Multi-mechanism-Approach-for-Modeling-Human-Object-Interaction"><a href="#Persistent-Transient-Duality-A-Multi-mechanism-Approach-for-Modeling-Human-Object-Interaction" class="headerlink" title="Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction"></a>Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12729">http://arxiv.org/abs/2307.12729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hung Tran, Vuong Le, Svetha Venkatesh, Truyen Tran</li>
<li>for: 这个论文旨在解释人类在人机交互（HOI）活动中的多机制性，以及如何使用Persistent-Transient Duality来模型这些机制。</li>
<li>methods: 这篇论文使用了Parent-Child neural network，其中Persistent频道和Transient频道是两个独立的神经网络，用于模型不同的机制。此外，还使用了一个神经网络模块来实现机制之间的动态切换。</li>
<li>results: 在两个丰富的数据集和多种设置下，这个模型在HOI动作预测中具有了superior的性能，证明了其适用性。<details>
<summary>Abstract</summary>
Humans are highly adaptable, swiftly switching between different modes to progressively handle different tasks, situations and contexts. In Human-object interaction (HOI) activities, these modes can be attributed to two mechanisms: (1) the large-scale consistent plan for the whole activity and (2) the small-scale children interactive actions that start and end along the timeline. While neuroscience and cognitive science have confirmed this multi-mechanism nature of human behavior, machine modeling approaches for human motion are trailing behind. While attempted to use gradually morphing structures (e.g., graph attention networks) to model the dynamic HOI patterns, they miss the expeditious and discrete mode-switching nature of the human motion. To bridge that gap, this work proposes to model two concurrent mechanisms that jointly control human motion: the Persistent process that runs continually on the global scale, and the Transient sub-processes that operate intermittently on the local context of the human while interacting with objects. These two mechanisms form an interactive Persistent-Transient Duality that synergistically governs the activity sequences. We model this conceptual duality by a parent-child neural network of Persistent and Transient channels with a dedicated neural module for dynamic mechanism switching. The framework is trialed on HOI motion forecasting. On two rich datasets and a wide variety of settings, the model consistently delivers superior performances, proving its suitability for the challenge.
</details>
<details>
<summary>摘要</summary>
人类具有高度适应性，快速 switching  между不同模式以逐渐处理不同任务、情况和上下文。在人机交互（HOI）活动中，这些模式可以归结为两种机制：（1）整体活动的大规模一致计划，和（2）在时间轴上开始和结束的小规模儿童交互动作。而神经科学和认知科学已经证实了人类行为的多机制性，但机器模型人体运动的方法尚未跟上。尝试使用渐变变换结构（如图像注意力网络）来模型动态 HOI 模式，但它们缺乏人类运动快速和精细的模式转换特点。为了bridging这个差距，本工作提出了同时控制人体运动的两个机制：持续过程，该过程在全局范围内一直运行，和间歇性子进程，该子进程在人类与对象交互时在本地上运行。这两种机制组成了互动的持续-间歇性双重性，这种双重性同时控制活动序列。我们使用父母-孩子神经网络，其中持续通道和间歇性通道各自拥有特定的神经元模块，以实现动态机制转换。这种概念双重性在 HOI 动作预测中得到证明，在两个丰富的数据集和多种设置下，模型一致地表现出优秀的表现，证明其适用性。
</details></li>
</ul>
<hr>
<h2 id="AMAE-Adaptation-of-Pre-Trained-Masked-Autoencoder-for-Dual-Distribution-Anomaly-Detection-in-Chest-X-Rays"><a href="#AMAE-Adaptation-of-Pre-Trained-Masked-Autoencoder-for-Dual-Distribution-Anomaly-Detection-in-Chest-X-Rays" class="headerlink" title="AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays"></a>AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12721">http://arxiv.org/abs/2307.12721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behzad Bozorgtabar, Dwarikanath Mahapatra, Jean-Philippe Thiran</li>
<li>for: 这个研究是针对医疗影像中的无Supervised anomaly detection问题，尤其是胸部X-ray影像。</li>
<li>methods: 这个研究使用了一个名为AMAE的两阶段算法，它首先将normal类别的训练图像 sintethesize为假想的异常图像，然后使用内部的lightweight分类器来训练。接着，它利用了一个pseudo-labeling scheme来利用无标注图像中的异常。</li>
<li>results: AMAE在不同的异常比例下评估了其效果，并与其他自动标注和双分布异常检测方法进行比较。结果显示，AMAE在三个公开的胸部X-raybenchmark上设置了新的State-of-the-art。<details>
<summary>Abstract</summary>
Unsupervised anomaly detection in medical images such as chest radiographs is stepping into the spotlight as it mitigates the scarcity of the labor-intensive and costly expert annotation of anomaly data. However, nearly all existing methods are formulated as a one-class classification trained only on representations from the normal class and discard a potentially significant portion of the unlabeled data. This paper focuses on a more practical setting, dual distribution anomaly detection for chest X-rays, using the entire training data, including both normal and unlabeled images. Inspired by a modern self-supervised vision transformer model trained using partial image inputs to reconstruct missing image regions -- we propose AMAE, a two-stage algorithm for adaptation of the pre-trained masked autoencoder (MAE). Starting from MAE initialization, AMAE first creates synthetic anomalies from only normal training images and trains a lightweight classifier on frozen transformer features. Subsequently, we propose an adaptation strategy to leverage unlabeled images containing anomalies. The adaptation scheme is accomplished by assigning pseudo-labels to unlabeled images and using two separate MAE based modules to model the normative and anomalous distributions of pseudo-labeled images. The effectiveness of the proposed adaptation strategy is evaluated with different anomaly ratios in an unlabeled training set. AMAE leads to consistent performance gains over competing self-supervised and dual distribution anomaly detection methods, setting the new state-of-the-art on three public chest X-ray benchmarks: RSNA, NIH-CXR, and VinDr-CXR.
</details>
<details>
<summary>摘要</summary>
不监督异常检测在医学影像中，如胸部X射线图像，正在受到关注，因为它解决了专业人员对异常数据的繁琐和成本的批注短缺。然而，大多数现有方法都是基于一个类型的分类，只使用正常类的表示来训练。这篇论文关注了更实际的设定：对胸部X射线图像进行双分布异常检测，使用整个训练数据，包括正常和无标记图像。 Drawing inspiration from a modern self-supervised vision transformer model trained using partial image inputs to reconstruct missing image regions，我们提出了AMAE算法，一个两个阶段的算法，用于适应预先训练的封闭自动encoder（MAE）。从MAE初始化开始，AMAE首先将正常训练图像中生成假异常，并训练一个轻量级分类器。接着，我们提出了适应策略，使用无标记图像中含异常的图像进行适应。这种适应策略是通过将无标记图像 assign pseudo-labels，并使用两个基于MAE模块来模型无标记图像的正常和异常分布。我们对不同异常比例的无标记训练集进行评估，AMAE在自我超vised和双分布异常检测方法中表现出优异，创造了新的state-of-the-art在三个公共胸部X射线标准benchmark上：RSNA、NIH-CXR和VinDr-CXR。
</details></li>
</ul>
<hr>
<h2 id="CarPatch-A-Synthetic-Benchmark-for-Radiance-Field-Evaluation-on-Vehicle-Components"><a href="#CarPatch-A-Synthetic-Benchmark-for-Radiance-Field-Evaluation-on-Vehicle-Components" class="headerlink" title="CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle Components"></a>CarPatch: A Synthetic Benchmark for Radiance Field Evaluation on Vehicle Components</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12718">http://arxiv.org/abs/2307.12718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Di Nucci, Alessandro Simoni, Matteo Tomei, Luca Ciuffreda, Roberto Vezzani, Rita Cucchiara</li>
<li>for: 本研究旨在提供一个Synthetic benchmark dataset for vehicle inspection, 以便用于评估和比较不同技术的效果。</li>
<li>methods: 本研究使用NeRFs技术来生成3D重建图像，并提供了相应的深度地图和semantic segmentationmask。</li>
<li>results: 本研究提供了一个公共可用的Synthetic benchmark dataset，可以作为评估和比较不同技术的导向。<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRFs) have gained widespread recognition as a highly effective technique for representing 3D reconstructions of objects and scenes derived from sets of images. Despite their efficiency, NeRF models can pose challenges in certain scenarios such as vehicle inspection, where the lack of sufficient data or the presence of challenging elements (e.g. reflections) strongly impact the accuracy of the reconstruction. To this aim, we introduce CarPatch, a novel synthetic benchmark of vehicles. In addition to a set of images annotated with their intrinsic and extrinsic camera parameters, the corresponding depth maps and semantic segmentation masks have been generated for each view. Global and part-based metrics have been defined and used to evaluate, compare, and better characterize some state-of-the-art techniques. The dataset is publicly released at https://aimagelab.ing.unimore.it/go/carpatch and can be used as an evaluation guide and as a baseline for future work on this challenging topic.
</details>
<details>
<summary>摘要</summary>
neural radiance fields (NeRFs) 已经得到广泛的认可作为用于表示基于图像集的物体和场景三维重建的高效技术。尽管它们有效，但 NeRF 模型在某些情况下可能会遇到挑战，如车辆检查，因为缺乏足够的数据或存在挑战性的元素（例如反射）会强烈影响重建的准确性。为此，我们引入了 CarPatch，一个新的人工测试集。除了每个视图的图像、摄像机参数、深度地图和semantic segmentation映射之外，还包括了每个视图的全球和部分评价指标。这些指标用于评估、比较和更好地描述一些现状技术的性能。该数据集publicly released at <https://aimagelab.ing.unimore.it/go/carpatch>，可以用作评估指南和未来工作的基准。
</details></li>
</ul>
<hr>
<h2 id="Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction"><a href="#Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction" class="headerlink" title="Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction"></a>Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12717">http://arxiv.org/abs/2307.12717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangduo Xie, Matthew B. Blaschko</li>
<li>for: 这篇论文的目的是提出一种不需要组织数据的自动化阴极 artifact 除除法，以帮助 CT 影像诊断中受到阴极 artifact 的影响。</li>
<li>methods: 本论文提出了一个基于 dense transformer 的增强编码网络 (DTEC-Net)，包括一个层次分解对接oder 和 transformer 来取得高密度编码序列，以及一个第二阶分解方法来改善密度序列的解码过程。</li>
<li>results: 实验和模型讨论表明，DTEC-Net 能够优于前一代方法，对 CT 影像进行自动化阴极 artifact 除除法，同时保留 CT 影像的结构资讯。<details>
<summary>Abstract</summary>
CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence's decoding process. Extensive experiments and model discussions illustrate DTEC-Net's effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Damage-Vision-Mining-Opportunity-for-Imbalanced-Anomaly-Detection"><a href="#Damage-Vision-Mining-Opportunity-for-Imbalanced-Anomaly-Detection" class="headerlink" title="Damage Vision Mining Opportunity for Imbalanced Anomaly Detection"></a>Damage Vision Mining Opportunity for Imbalanced Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12676">http://arxiv.org/abs/2307.12676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takato Yasuno</li>
<li>for: 这篇论文是为了探讨受损测量数据的挑战和解决方案，尤其是在工业应用中使用深度学习方法进行预测维护和应急维护。</li>
<li>methods: 本研究使用了深度学习方法，包括回归、图像分类、物件检测和 semantic segmentation，以解决受损测量数据的问题。</li>
<li>results: 研究发现，在受损测量数据中，使用深度学习方法可以提高预测维护和应急维护的精度。此外，研究还发现了一个有用的正常检测应用，可以在受损测量数据中检测到异常情况。<details>
<summary>Abstract</summary>
In past decade, previous balanced datasets have been used to advance algorithms for classification, object detection, semantic segmentation, and anomaly detection in industrial applications. Specifically, for condition-based maintenance, automating visual inspection is crucial to ensure high quality. Deterioration prognostic attempts to optimize the fine decision process for predictive maintenance and proactive repair. In civil infrastructure and living environment, damage data mining cannot avoid the imbalanced data issue because of rare unseen events and high quality status by improved operations. For visual inspection, deteriorated class acquired from the surface of concrete and steel components are occasionally imbalanced. From numerous related surveys, we summarize that imbalanced data problems can be categorized into four types; 1) missing range of target and label valuables, 2) majority-minority class imbalance, 3) foreground-background of spatial imbalance, 4) long-tailed class of pixel-wise imbalance. Since 2015, there has been many imbalanced studies using deep learning approaches that includes regression, image classification, object detection, semantic segmentation. However, anomaly detection for imbalanced data is not yet well known. In the study, we highlight one-class anomaly detection application whether anomalous class or not, and demonstrate clear examples on imbalanced vision datasets: blood smear, lung infection, hazardous driving, wooden, concrete deterioration, river sludge, and disaster damage. Illustrated in Fig.1, we provide key results on damage vision mining advantage, hypothesizing that the more effective range of positive ratio, the higher accuracy gain of anomaly detection application. In our imbalanced studies, compared with the balanced case of positive ratio 1/1, we find that there is applicable positive ratio, where the accuracy are consistently high.
</details>
<details>
<summary>摘要</summary>
过去一个 décennial，以前的平衡数据集被用于提高分类、物体检测、semantic segmentation和异常检测在工业应用中的算法。具体来说，为condition-based maintenance，自动化视觉检查是关键以确保高质量。预测维护和抢修的决策进程优化。在公共基础设施和生活环境中，损害数据挖掘无法避免偏移数据问题，因为罕见的未看到事件和高质量状态的提高。对于视觉检查，坏化类型从混凝土和钢结构的表面获得的数据 occasionally imbalanced。从多个相关的调查中，我们总结出四种偏移数据问题：1）目标和标签值的缺失范围，2）多数少数类别偏移，3）前景背景的空间偏移，4）像素级偏移。自2015年以来，有很多关于偏移数据的研究使用深度学习方法，包括回归、图像分类、物体检测和semantic segmentation。然而，异常检测对偏移数据还未得到充分的研究。在本研究中，我们强调一类异常检测应用，无论异常类或不，并提供了清晰的示例，包括血癌、肺感染、危险驾驶、木材、混凝土衰老、河流淤泥和灾害损害。图1中，我们提供了危害视觉矿物优势，假设更高的正确率，异常检测应用的更高精度。在我们的偏移研究中，与平衡情况相比，我们发现了可采用的正确比例，其准确率一致高。
</details></li>
</ul>
<hr>
<h2 id="Industrial-Segment-Anything-–-a-Case-Study-in-Aircraft-Manufacturing-Intralogistics-Maintenance-Repair-and-Overhaul"><a href="#Industrial-Segment-Anything-–-a-Case-Study-in-Aircraft-Manufacturing-Intralogistics-Maintenance-Repair-and-Overhaul" class="headerlink" title="Industrial Segment Anything – a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul"></a>Industrial Segment Anything – a Case Study in Aircraft Manufacturing, Intralogistics, Maintenance, Repair, and Overhaul</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12674">http://arxiv.org/abs/2307.12674</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keno Moenck, Arne Wendt, Philipp Prünte, Julian Koch, Arne Sahrhage, Johann Gierecker, Ole Schmedemann, Falko Kähler, Dirk Holst, Martin Gomse, Thorsten Schüppstuhl, Daniel Schoepflin</li>
<li>for: 这篇论文旨在探讨在飞机生产专业领域中应用深度学习基于应用程序的问题。</li>
<li>methods: 该论文使用视觉基础模型（VFM）的零shot能力来解决数据、上下文和感知器多样性的问题。</li>
<li>results: 论文对飞机生产专业领域中的制造、内部物流、维护、维修和更换过程进行了应用，并考虑了领域知识的投入。<details>
<summary>Abstract</summary>
Deploying deep learning-based applications in specialized domains like the aircraft production industry typically suffers from the training data availability problem. Only a few datasets represent non-everyday objects, situations, and tasks. Recent advantages in research around Vision Foundation Models (VFM) opened a new area of tasks and models with high generalization capabilities in non-semantic and semantic predictions. As recently demonstrated by the Segment Anything Project, exploiting VFM's zero-shot capabilities is a promising direction in tackling the boundaries spanned by data, context, and sensor variety. Although, investigating its application within specific domains is subject to ongoing research. This paper contributes here by surveying applications of the SAM in aircraft production-specific use cases. We include manufacturing, intralogistics, as well as maintenance, repair, and overhaul processes, also representing a variety of other neighboring industrial domains. Besides presenting the various use cases, we further discuss the injection of domain knowledge.
</details>
<details>
<summary>摘要</summary>
通常在特殊领域 like 飞机生产 industri에서推广深度学习基于应用typically suffers from the training data availability problem。只有一些数据集表示不同的对象、情况和任务。 current Advantages in Research on Vision Foundation Models (VFM) opened a new area of tasks and models with high generalization capabilities in non-semantic and semantic predictions。 As recently demonstrated by the Segment Anything Project， exploiting VFM's zero-shot capabilities is a promising direction in tackling the boundaries spanned by data, context, and sensor variety。Although，investigating its application within specific domains is subject to ongoing research。This paper contributes here by surveying applications of the SAM in aircraft production-specific use cases。We include manufacturing, intralogistics, as well as maintenance, repair, and overhaul processes，also representing a variety of other neighboring industrial domains。Besides presenting the various use cases，we further discuss the injection of domain knowledge。
</details></li>
</ul>
<hr>
<h2 id="Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling"><a href="#Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling" class="headerlink" title="Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling"></a>Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12672">http://arxiv.org/abs/2307.12672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Pan, Suprosanna Shit, Özgün Turgut, Wenqi Huang, Hongwei Bran Li, Nil Stolt-Ansó, Thomas Küstner, Kerstin Hammernik, Daniel Rueckert</li>
<li>for: This paper focuses on improving dynamic magnetic resonance imaging (MRI) reconstruction by interpolating undersampled k-space data before obtaining images with Fourier transform.</li>
<li>methods: The proposed approach uses a Transformer-based k-space Global Interpolation Network (k-GIN) to learn global dependencies among low- and high-frequency components of 2D+t k-space, and a novel k-space Iterative Refinement Module (k-IRM) to enhance high-frequency components learning.</li>
<li>results: The proposed method outperforms baseline methods in terms of both quantitative and qualitative measures, and achieves higher robustness and generalizability in highly-undersampled MR data.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文关注改进动态磁共振成像重建，通过在傅里叶变换前 interpolate 受损的 k-空间数据。</li>
<li>methods: 提议的方法使用 Transformer 型 k-空间全球 interpolating 网络 (k-GIN) 学习 k-空间低频和高频成分之间的全球相互关系，并使用 novel k-space Iterative Refinement Module (k-IRM) 进一步提高高频成分学习。</li>
<li>results: 提议的方法相比基eline方法，在量化和质量上都有显著提高，并在高度受损的 MR 数据中具有更高的Robustness和普适性。<details>
<summary>Abstract</summary>
In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data.
</details>
<details>
<summary>摘要</summary>
在动态磁共振成像（MRI）中，通常因为扫描时间有限，因此会出现嵌套artefacts在图像领域。因此，动态MR重建需要不仅考虑图像频谱中的空间频率组件，还需要考虑时间重复性。大多数前一些工作都是通过图像频谱约束（约束）来进行MR重建。在这种情况下，我们将掩码图像模型与嵌套空间 interpolate的方法相连接，并提出了一种新的Transformer基于的嵌套空间全球 interpolate网络，称为k-GIN。我们的k-GIN可以学习2D+t嵌套空间中低频和高频组件之间的全球依赖关系，并使用它来 interpolate不扫描的数据。此外，我们还提出了一种新的嵌套空间迭代优化模块（k-IRM），以提高高频组件的学习。我们对92个自有2D+t心脏MR数据进行了评估，并与图像频谱约束的MR重建方法进行比较。实验结果表明，我们的提posed方法在量化和质量上都超过了基eline方法。其中，我们的方法在高度压缩MR数据的情况下具有显著更高的Robustness和普适性。
</details></li>
</ul>
<hr>
<h2 id="A-Theoretically-Guaranteed-Quaternion-Weighted-Schatten-p-norm-Minimization-Method-for-Color-Image-Restoration"><a href="#A-Theoretically-Guaranteed-Quaternion-Weighted-Schatten-p-norm-Minimization-Method-for-Color-Image-Restoration" class="headerlink" title="A Theoretically Guaranteed Quaternion Weighted Schatten p-norm Minimization Method for Color Image Restoration"></a>A Theoretically Guaranteed Quaternion Weighted Schatten p-norm Minimization Method for Color Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12656">http://arxiv.org/abs/2307.12656</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuxuanzhizi/qwsnm">https://github.com/qiuxuanzhizi/qwsnm</a></li>
<li>paper_authors: Qing-Hua Zhang, Liang-Tian He, Yi-Lun Wang, Liang-Jian Deng, Jun Liu</li>
<li>for: 这篇论文主要针对的是颜色图像修复（CIR）问题，具体来说是使用Weighted Nuclear Norm Minimization（WNNM）和Weighted Schatten $p$-norm Minimization（WSNM）方法来解决CIR问题。</li>
<li>methods: 这篇论文提出了一种基于四元数的WNNM方法（QWNNM），该方法可以将颜色图像 Represented as a whole in the quaternion domain，并且保持了颜色通道之间的自然协同关系。此外，该论文还提出了一种基于四元数的WSNM模型（QWSNM），用于解决CIR问题。</li>
<li>results: 该论文通过对两种CIR任务， namely color image denoising和deblurring，进行了广泛的实验，并证明了QWSNM方法在量化和质量上都有优于许多现有的方法。此外，论文还提供了一种初步的理论收敛分析，表明QWNNM和QWSNM的解决方案都具有固定点收敛保证。<details>
<summary>Abstract</summary>
Inspired by the fact that the matrix formulated by nonlocal similar patches in a natural image is of low rank, the rank approximation issue have been extensively investigated over the past decades, among which weighted nuclear norm minimization (WNNM) and weighted Schatten $p$-norm minimization (WSNM) are two prevailing methods have shown great superiority in various image restoration (IR) problems. Due to the physical characteristic of color images, color image restoration (CIR) is often a much more difficult task than its grayscale image counterpart. However, when applied to CIR, the traditional WNNM/WSNM method only processes three color channels individually and fails to consider their cross-channel correlations. Very recently, a quaternion-based WNNM approach (QWNNM) has been developed to mitigate this issue, which is capable of representing the color image as a whole in the quaternion domain and preserving the inherent correlation among the three color channels. Despite its empirical success, unfortunately, the convergence behavior of QWNNM has not been strictly studied yet. In this paper, on the one side, we extend the WSNM into quaternion domain and correspondingly propose a novel quaternion-based WSNM model (QWSNM) for tackling the CIR problems. Extensive experiments on two representative CIR tasks, including color image denoising and deblurring, demonstrate that the proposed QWSNM method performs favorably against many state-of-the-art alternatives, in both quantitative and qualitative evaluations. On the other side, more importantly, we preliminarily provide a theoretical convergence analysis, that is, by modifying the quaternion alternating direction method of multipliers (QADMM) through a simple continuation strategy, we theoretically prove that both the solution sequences generated by the QWNNM and QWSNM have fixed-point convergence guarantees.
</details>
<details>
<summary>摘要</summary>
基于自然图像中非local相似区域矩阵的低级数据结构，过去几十年内，对矩阵近似问题进行了广泛的研究，其中包括权重核函数最小化（WNNM）和权重斜率p- norm最小化（WSNM）等两种方法，在不同的图像修复（IR）问题中显示出了优异的表现。然而，由于图像的物理特性，对于颜色图像的修复（CIR）是对灰度图像修复的一个非常更加困难的任务。然而，传统的WNNM/WSNM方法只是对每个色道进行独立处理，而忽略了它们之间的相互关系。最近，一种基于四元数的WNNM方法（QWNNM）已经开发出来，可以将颜色图像作为一个整体在四元数域中表示，并保留它们之间的自然相互关系。虽然它在实际中表现了良好，但它们的减法性还没有得到严格的研究。在这篇论文中，我们首先将WSNM扩展到四元数域，并对此提出了一种新的四元数基于WNNM模型（QWSNM），用于解决CIR问题。我们在两个代表性的CIR任务上进行了广泛的实验，包括颜色图像噪声去除和颜色图像补做。结果表明，我们提出的QWSNM方法在量化和质量上的评价中表现出色，胜过许多当前的状态艺术。此外，我们还提供了一种初步的理论减法分析，即通过修改四元数alternating direction method of multipliers（QADMM）的简单继续策略，我们 theoretically proves that both the solution sequences generated by QWNNM and QWSNM have fixed-point convergence guarantees.
</details></li>
</ul>
<hr>
<h2 id="PG-RCNN-Semantic-Surface-Point-Generation-for-3D-Object-Detection"><a href="#PG-RCNN-Semantic-Surface-Point-Generation-for-3D-Object-Detection" class="headerlink" title="PG-RCNN: Semantic Surface Point Generation for 3D Object Detection"></a>PG-RCNN: Semantic Surface Point Generation for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12637">http://arxiv.org/abs/2307.12637</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/quotation2520/pg-rcnn">https://github.com/quotation2520/pg-rcnn</a></li>
<li>paper_authors: Inyong Koo, Inyoung Lee, Se-Ho Kim, Hee-Seon Kim, Woo-jin Jeon, Changick Kim</li>
<li>for: 该 paper 是为了解决 LiDAR 数据中 объек的三维检测困难而写的。</li>
<li>methods: 该 paper 使用了点云补充方法，包括使用预训练网络生成 RoI 中的点云。</li>
<li>results: 该 paper 提出了 Point Generation R-CNN（PG-RCNN），一种新的端到端检测器，可以生成准确的前景对象的 semantic surface points。<details>
<summary>Abstract</summary>
One of the main challenges in LiDAR-based 3D object detection is that the sensors often fail to capture the complete spatial information about the objects due to long distance and occlusion. Two-stage detectors with point cloud completion approaches tackle this problem by adding more points to the regions of interest (RoIs) with a pre-trained network. However, these methods generate dense point clouds of objects for all region proposals, assuming that objects always exist in the RoIs. This leads to the indiscriminate point generation for incorrect proposals as well. Motivated by this, we propose Point Generation R-CNN (PG-RCNN), a novel end-to-end detector that generates semantic surface points of foreground objects for accurate detection. Our method uses a jointly trained RoI point generation module to process the contextual information of RoIs and estimate the complete shape and displacement of foreground objects. For every generated point, PG-RCNN assigns a semantic feature that indicates the estimated foreground probability. Extensive experiments show that the point clouds generated by our method provide geometrically and semantically rich information for refining false positive and misaligned proposals. PG-RCNN achieves competitive performance on the KITTI benchmark, with significantly fewer parameters than state-of-the-art models. The code is available at https://github.com/quotation2520/PG-RCNN.
</details>
<details>
<summary>摘要</summary>
Motivated by this, we propose Point Generation R-CNN (PG-RCNN), a novel end-to-end detector that generates semantic surface points of foreground objects for accurate detection. Our method uses a jointly trained RoI point generation module to process the contextual information of RoIs and estimate the complete shape and displacement of foreground objects. For every generated point, PG-RCNN assigns a semantic feature that indicates the estimated foreground probability.Extensive experiments show that the point clouds generated by our method provide geometrically and semantically rich information for refining false positive and misaligned proposals. PG-RCNN achieves competitive performance on the KITTI benchmark, with significantly fewer parameters than state-of-the-art models. The code is available at https://github.com/quotation2520/PG-RCNN.Translated into Simplified Chinese:一个主要挑战在LiDAR基于3D物体检测中是感知器通常无法捕捉物体的完整空间信息，这主要是因为距离较远和遮挡。两Stage检测器通过添加更多的点云来补充RoI中的点云，但这些方法生成的点云都是对所有的区域提案中的物体，假设物体总是存在于RoI中。这会导致无用的点云生成和预测错误的提案。我们提出了Point Generation R-CNN（PG-RCNN），一种新的端到端检测器，它可以生成对象的语义表面点，以提高检测的准确性。我们的方法使用一个同时训练的RoI点生成模块，以处理RoI中的上下文信息，并估计前景对象的完整形状和偏移。每个生成的点都被PG-RCNN分配一个语义特征，这个特征指示了对象的预测概率。广泛的实验表明，PG-RCNN生成的点云具有很好的准确性和语义特征，可以用于修正错误的提案和偏移。PG-RCNN在KITTI测试benchmark上达到了竞争性性能，并且 Parameters 比 state-of-the-art 模型少得多。代码可以在https://github.com/quotation2520/PG-RCNN中下载。
</details></li>
</ul>
<hr>
<h2 id="Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation"><a href="#Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation" class="headerlink" title="Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation"></a>Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12634">http://arxiv.org/abs/2307.12634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Qi Su, Na Wang, Jiawen Xie, Yinan Chen, Xiaofan Zhang</li>
<li>for:  automatic lung lobe segmentation algorithm for the diagnosis and treatment of lung diseases</li>
<li>methods:  task-specific loss function to pay attention to the area around the pulmonary fissure, end-to-end pulmonary fissure generation method, registration-based loss function to alleviate convergence difficulty</li>
<li>results:  dice scores of 97.83% on private dataset STLB and 94.75% on public LUNA16 dataset<details>
<summary>Abstract</summary>
The automatic lung lobe segmentation algorithm is of great significance for the diagnosis and treatment of lung diseases, however, which has great challenges due to the incompleteness of pulmonary fissures in lung CT images and the large variability of pathological features. Therefore, we propose a new automatic lung lobe segmentation framework, in which we urge the model to pay attention to the area around the pulmonary fissure during the training process, which is realized by a task-specific loss function. In addition, we introduce an end-to-end pulmonary fissure generation method in the auxiliary pulmonary fissure segmentation task, without any additional network branch. Finally, we propose a registration-based loss function to alleviate the convergence difficulty of the Dice loss supervised pulmonary fissure segmentation task. We achieve 97.83% and 94.75% dice scores on our private dataset STLB and public LUNA16 dataset respectively.
</details>
<details>
<summary>摘要</summary>
“自动肺lobus分割算法具有诊断和治疗肺病的重要意义，但受到肺 CT 影像中肺裂的不完整性和病理特征的大幅度variability所困扰。因此，我们提出了一个新的自动肺lobus分割框架，其中我们要求模型在训练过程中对肺裂附近区域做出优化。此外，我们引入了一个独立的辅助肺裂分割任务，并在这个任务中使用了一个统一的损失函数。最后，我们提出了一个注册损失函数，以解决基于 Dice 损失函数的肺裂分割任务中的整合问题。我们在私人数据集 STLB 和公共数据集 LUNA16 上实现了97.83% 和 94.75%的 Dice 分数。”
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Medical-Image-Segmentation-with-Co-Distribution-Alignment"><a href="#Semi-Supervised-Medical-Image-Segmentation-with-Co-Distribution-Alignment" class="headerlink" title="Semi-Supervised Medical Image Segmentation with Co-Distribution Alignment"></a>Semi-Supervised Medical Image Segmentation with Co-Distribution Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12630">http://arxiv.org/abs/2307.12630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Wang, Zhongzheng Huang, Jiawei Wu, Yuanzheng Cai, Zuoyong Li</li>
<li>for: 这篇论文主要是为了提出一种基于半指导学习的医学影像分割方法，以便在缺乏大量标注数据的情况下进行医学影像分割。</li>
<li>methods: 这篇论文提出了一种名为Co-Distribution Alignment（Co-DA）的方法，它可以在半指导学习情况下提高医学影像分割的性能。Co-DA方法包括使用两个不同初始化的模型进行类别匹配，并使用一个模型生成的pseudo-labels来监督另一个模型。此外，论文还提出了一种过期预期极限似一个权重函数来降低无效的pseudo-labels噪音。</li>
<li>results: 根据论文的实验结果，Co-DA方法在三个公共dataset上都有较好的性能，尤其是在2D CaDIS dataset和3D LGE-MRI和ACDC dataset上，它可以仅使用24%的标注数据而 достиieving an mIoU of 0.8515和Dice score of 0.8824和0.8773，即使只使用20%的数据。<details>
<summary>Abstract</summary>
Medical image segmentation has made significant progress when a large amount of labeled data are available. However, annotating medical image segmentation datasets is expensive due to the requirement of professional skills. Additionally, classes are often unevenly distributed in medical images, which severely affects the classification performance on minority classes. To address these problems, this paper proposes Co-Distribution Alignment (Co-DA) for semi-supervised medical image segmentation. Specifically, Co-DA aligns marginal predictions on unlabeled data to marginal predictions on labeled data in a class-wise manner with two differently initialized models before using the pseudo-labels generated by one model to supervise the other. Besides, we design an over-expectation cross-entropy loss for filtering the unlabeled pixels to reduce noise in their pseudo-labels. Quantitative and qualitative experiments on three public datasets demonstrate that the proposed approach outperforms existing state-of-the-art semi-supervised medical image segmentation methods on both the 2D CaDIS dataset and the 3D LGE-MRI and ACDC datasets, achieving an mIoU of 0.8515 with only 24% labeled data on CaDIS, and a Dice score of 0.8824 and 0.8773 with only 20% data on LGE-MRI and ACDC, respectively.
</details>
<details>
<summary>摘要</summary>
医疗图像分割技术在有大量标注数据时已经做出了 significiant进步。然而，为了创建医疗图像分割数据集， annotating 医疗图像分割数据集是昂贵的，主要因为需要专业技能。此外，医疗图像中的类别经常不均匀分布，这会严重地影响少数类别的分类性能。为了解决这些问题，这篇论文提出了Co-Distribution Alignment（Co-DA）方法，用于 semi-supervised 医疗图像分割。具体来说，Co-DA 方法将未标注数据中的边缘预测与已标注数据中的边缘预测进行类别匹配，使用两个不同初始化的模型来实现。此外，我们还设计了过期cross-entropy损失函数，用于筛选未标注的像素，以降低它们的 Pseudo-labels 中的噪音。我们对三个公共数据集进行了量化和质量测试，结果显示，我们的方法在 CaDIS 数据集上的 mIoU 为 0.8515，只使用 24% 的标注数据；在 LGE-MRI 和 ACDC 数据集上，我们的方法的 Dice 分数分别为 0.8824 和 0.8773，只使用 20% 的数据。
</details></li>
</ul>
<hr>
<h2 id="Phase-Matching-for-Out-of-Distribution-Generalization"><a href="#Phase-Matching-for-Out-of-Distribution-Generalization" class="headerlink" title="Phase Matching for Out-of-Distribution Generalization"></a>Phase Matching for Out-of-Distribution Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12622">http://arxiv.org/abs/2307.12622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengming Hu, Yeqian Du, Rui Wang, Hao Chen</li>
<li>for: 本研究旨在解释卷积神经网络（CNNs）在不同分布下的泛化行为，并提出一种基于傅ри映射的频谱层次结构的方法来解决频谱预测问题。</li>
<li>methods: 本研究使用傅ри映射来分解视觉信号，并提出了一种基于频谱的相匹配方法（PhaMa）来 Address Domain Generalization（DG）问题。</li>
<li>results: 经过实验证明，提出的方法可以在多个标准准 benchmark上达到领先的性能水平，并且在不同分布下的泛化和Out-of-distribution（OOD） robustness任务中表现出色。<details>
<summary>Abstract</summary>
The Fourier transform, serving as an explicit decomposition method for visual signals, has been employed to explain the out-of-distribution generalization behaviors of Convolutional Neural Networks (CNNs). Previous studies have indicated that the amplitude spectrum is susceptible to the disturbance caused by distribution shifts. On the other hand, the phase spectrum preserves highly-structured spatial information, which is crucial for robust visual representation learning. However, the spatial relationships of phase spectrum remain unexplored in previous researches. In this paper, we aim to clarify the relationships between Domain Generalization (DG) and the frequency components, and explore the spatial relationships of the phase spectrum. Specifically, we first introduce a Fourier-based structural causal model which interprets the phase spectrum as semi-causal factors and the amplitude spectrum as non-causal factors. Then, we propose Phase Matching (PhaMa) to address DG problems. Our method introduces perturbations on the amplitude spectrum and establishes spatial relationships to match the phase components. Through experiments on multiple benchmarks, we demonstrate that our proposed method achieves state-of-the-art performance in domain generalization and out-of-distribution robustness tasks.
</details>
<details>
<summary>摘要</summary>
《傅里叶变换在视觉信号中的明确分解方法》，已经用于解释深度学习模型在不同分布下的泛化行为。先前的研究表明，振荡спектrum容易受到分布变化的影响。然而，相对于振荡спектrum，频率спектrum preserve了高度结构化的空间信息，这是重要的视觉表示学习的基础。然而，频率спектrum中的空间关系尚未在先前的研究中得到了探讨。在这篇论文中，我们想要解释频率спектrum和频率组件之间的关系，并探讨频率спектrum中的空间关系。我们首先介绍了一种基于傅里叶变换的结构 causal模型，其中 interprets频率спектrum为半 causal因素，而振荡спектrum为非 causal因素。然后，我们提出了phasematching（PhaMa）方法，用于解决频率特征泛化问题。我们的方法在振荡spectrum中引入了干扰并建立了空间关系，以匹配频率спектrum的组件。通过多个标准benchmark experiment表明，我们提出的方法可以在频率特征泛化和out-of-distribution Robustness任务中达到状态之巅的性能。
</details></li>
</ul>
<hr>
<h2 id="Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI"><a href="#Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI" class="headerlink" title="Sparse annotation strategies for segmentation of short axis cardiac MRI"></a>Sparse annotation strategies for segmentation of short axis cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12619">http://arxiv.org/abs/2307.12619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Stein, Maxime Di Folco, Julia Schnabel</li>
<li>for: 这个论文的目的是研究使用减少数据量和标注量来实现高精度的心脏MRI分割。</li>
<li>methods: 这个论文使用了减少数据量和标注量来降低标注量的方法，包括训练 sparse volumes 和 sparse annotations。</li>
<li>results: 研究发现，训练 sparse volumes 和 sparse annotations 可以获得高达 0.85 的 Dice 分数，并且比使用完整数据集（160 和 240 个数据集）更好。此外，中部的剖面标注对分割性能最有利，而脊梁区域的标注对分割性能最差。<details>
<summary>Abstract</summary>
Short axis cardiac MRI segmentation is a well-researched topic, with excellent results achieved by state-of-the-art models in a supervised setting. However, annotating MRI volumes is time-consuming and expensive. Many different approaches (e.g. transfer learning, data augmentation, few-shot learning, etc.) have emerged in an effort to use fewer annotated data and still achieve similar performance as a fully supervised model. Nevertheless, to the best of our knowledge, none of these works focus on which slices of MRI volumes are most important to annotate for yielding the best segmentation results. In this paper, we investigate the effects of training with sparse volumes, i.e. reducing the number of cases annotated, and sparse annotations, i.e. reducing the number of slices annotated per case. We evaluate the segmentation performance using the state-of-the-art nnU-Net model on two public datasets to identify which slices are the most important to annotate. We have shown that training on a significantly reduced dataset (48 annotated volumes) can give a Dice score greater than 0.85 and results comparable to using the full dataset (160 and 240 volumes for each dataset respectively). In general, training on more slice annotations provides more valuable information compared to training on more volumes. Further, annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, and the apical region the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details>
<details>
<summary>摘要</summary>
短轴心臓MRI分割是已有广泛研究的话题，现有前沿模型在监督环境下实现了出色的结果。然而，对MRI卷积的标注是时间consuming和昂贵的。多种方法（如转移学习、数据扩展、少数案例学习等）在尝试使用 fewer annotated data 并 still achieve similar performance as a fully supervised model 中出现。然而，到目前为止，这些工作没有关注在哪些MRI卷积中最重要的标注，以获得最佳分割结果。在这篇论文中，我们investigate the effects of training with sparse volumes和 sparse annotations。我们使用state-of-the-art nnU-Net模型在两个公共数据集上评估分割性能，以确定哪些卷积是最重要的标注。我们发现，使用很少的数据（48个标注卷积）可以达到 Dice 分数大于 0.85 和与全数据集（160和240卷积）的结果相当。总的来说，训练更多的卷积标注提供更多有价值的信息，而不是训练更多的卷积。此外，从中心部分标注MRI卷积可以提供最佳分割结果，而Apical区域则是最差。当评估标注卷积与卷积之间的权衡时，可以看到， annotating as many slices as possible instead of annotating more volumes 是一个更好的策略。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains"><a href="#Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains" class="headerlink" title="Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains"></a>Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12618">http://arxiv.org/abs/2307.12618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Di Folco, Cosmin Bercea, Julia A. Schnabel</li>
<li>for: 这篇论文旨在提高深度生成模型的可控性，通过修改数据特征来控制数据生成。</li>
<li>methods: 这篇论文提出了Attribute-Regularized Soft Introspective VAE（Attri-SIVAE）模型，通过添加特征规范损失来提高VAE的可控性。</li>
<li>results: 实验表明，Attri-SIVAE模型在不同的MRI数据集上表现相当于Attributed regularized VAE，并且可以在不同的数据集上保持同等的规范水平。<details>
<summary>Abstract</summary>
Deep generative models have emerged as influential instruments for data generation and manipulation. Enhancing the controllability of these models by selectively modifying data attributes has been a recent focus. Variational Autoencoders (VAEs) have shown promise in capturing hidden attributes but often produce blurry reconstructions. Controlling these attributes through different imaging domains is difficult in medical imaging. Recently, Soft Introspective VAE leverage the benefits of both VAEs and Generative Adversarial Networks (GANs), which have demonstrated impressive image synthesis capabilities, by incorporating an adversarial loss into VAE training. In this work, we propose the Attributed Soft Introspective VAE (Attri-SIVAE) by incorporating an attribute regularized loss, into the Soft-Intro VAE framework. We evaluate experimentally the proposed method on cardiac MRI data from different domains, such as various scanner vendors and acquisition centers. The proposed method achieves similar performance in terms of reconstruction and regularization compared to the state-of-the-art Attributed regularized VAE but additionally also succeeds in keeping the same regularization level when tested on a different dataset, unlike the compared method.
</details>
<details>
<summary>摘要</summary>
深度生成模型已经成为数据生成和修改的重要工具。提高这些模型的可控性，通过选择性地修改数据特性，是最近的焦点。变量自编码器（VAEs）已经表现出捕捉隐藏特性的抑或，但它们经常生成模糊的重建。在医学成像中，控制这些特性通过不同的成像频谱是困难的。最近，软 introspective VAE 利用 VAEs 和生成对抗网络（GANs）的优点，通过在 VAE 训练中添加对抗损失来提高图像合成能力。在这项工作中，我们提议 incorporating  attribute regularized loss 到 Soft-Intro VAE 框架中，并对其进行实验评估。我们发现，提议的方法在不同的 MRI 数据集上实现了相似的重建和规范性能，与 state-of-the-art  attributed regularized VAE 相比，同时还能在不同的数据集上保持同等的规范水平。
</details></li>
</ul>
<hr>
<h2 id="ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays"><a href="#ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays" class="headerlink" title="ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays"></a>ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12607">http://arxiv.org/abs/2307.12607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akanksha Dixit, Yashashwee Chakrabarty, Smruti R. Sarangi</li>
<li>For: The paper aims to increase the frame rate of high-frequency displays by 4x with minimal reduction in perceived image quality.* Methods: The paper proposes using reinforcement learning (RL) to intelligently choose between slower DNN-based extrapolation and faster warping-based methods.* Results: The proposed approach, called Exwarp, achieves a 4x increase in frame rate with minimal reduction in image quality.<details>
<summary>Abstract</summary>
High-frequency displays are gaining immense popularity because of their increasing use in video games and virtual reality applications. However, the issue is that the underlying GPUs cannot continuously generate frames at this high rate -- this results in a less smooth and responsive experience. Furthermore, if the frame rate is not synchronized with the refresh rate, the user may experience screen tearing and stuttering. Previous works propose increasing the frame rate to provide a smooth experience on modern displays by predicting new frames based on past or future frames. Interpolation and extrapolation are two widely used algorithms that predict new frames. Interpolation requires waiting for the future frame to make a prediction, which adds additional latency. On the other hand, extrapolation provides a better quality of experience because it relies solely on past frames -- it does not incur any additional latency. The simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme. Past work has used DNNs to get good accuracy, however, these approaches are slow. This paper proposes Exwarp -- an approach based on reinforcement learning (RL) to intelligently choose between the slower DNN-based extrapolation and faster warping-based methods to increase the frame rate by 4x with an almost negligible reduction in the perceived image quality.
</details>
<details>
<summary>摘要</summary>
高频显示器在游戏和虚拟现实应用中的使用越来越普遍，但是下面的 GPU 无法不断生成这高速帧，这会导致用户体验不平滑和不响应。此外，如果帧率与刷新率不同步，用户可能会经历屏扑和停顿。先前的工作建议通过预测新帧来提高现代显示器的帧率，以提供平滑的用户体验。 interpolate 和 extrapolate 是两种广泛使用的预测算法。 interpolate 需要等待未来的帧来作预测，这会添加额外的延迟。 extrapolate 提供了更好的用户体验，因为它仅基于过去的帧，无需添加额外的延迟。在 extrapolate 帧时，最简单的方法是通过运动向量来扭曲上一帧，但是扭曲后的帧可能会包含不正确渲染的视觉artefacts，这使得设计这种方案非常困难。过去的工作使用 DNN 来获得好的准确性，但这些方法比较慢。这篇论文提出了 Exwarp，一种基于 reinforcement learning（RL）的方法，可以智能地选择 slower DNN-based extrapolation 和 faster warping-based方法，以提高帧率4倍，并且几乎无法感受到图像质量的下降。
</details></li>
</ul>
<hr>
<h2 id="SwinMM-Masked-Multi-view-with-Swin-Transformers-for-3D-Medical-Image-Segmentation"><a href="#SwinMM-Masked-Multi-view-with-Swin-Transformers-for-3D-Medical-Image-Segmentation" class="headerlink" title="SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation"></a>SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12591">http://arxiv.org/abs/2307.12591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucsc-vlaa/swinmm">https://github.com/ucsc-vlaa/swinmm</a></li>
<li>paper_authors: Yiqing Wang, Zihan Li, Jieru Mei, Zihao Wei, Li Liu, Chen Wang, Shengtian Sang, Alan Yuille, Cihang Xie, Yuyin Zhou<br>for:这篇论文主要目的是提高自主学习方法 для医疗影像分割。methods:论文使用的方法包括masked multi-view encoder和cross-view decoder，以及一种新的多视图学习方法。results:论文比前一个状态的自主学习方法Swin UNITR显示了明显的优势，能够更好地 интегрирова多视图信息，提高模型的准确率和数据效率。<details>
<summary>Abstract</summary>
Recent advancements in large-scale Vision Transformers have made significant strides in improving pre-trained models for medical image segmentation. However, these methods face a notable challenge in acquiring a substantial amount of pre-training data, particularly within the medical field. To address this limitation, we present Masked Multi-view with Swin Transformers (SwinMM), a novel multi-view pipeline for enabling accurate and data-efficient self-supervised medical image analysis. Our strategy harnesses the potential of multi-view information by incorporating two principal components. In the pre-training phase, we deploy a masked multi-view encoder devised to concurrently train masked multi-view observations through a range of diverse proxy tasks. These tasks span image reconstruction, rotation, contrastive learning, and a novel task that employs a mutual learning paradigm. This new task capitalizes on the consistency between predictions from various perspectives, enabling the extraction of hidden multi-view information from 3D medical data. In the fine-tuning stage, a cross-view decoder is developed to aggregate the multi-view information through a cross-attention block. Compared with the previous state-of-the-art self-supervised learning method Swin UNETR, SwinMM demonstrates a notable advantage on several medical image segmentation tasks. It allows for a smooth integration of multi-view information, significantly boosting both the accuracy and data-efficiency of the model. Code and models are available at https://github.com/UCSC-VLAA/SwinMM/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PRIOR-Prototype-Representation-Joint-Learning-from-Medical-Images-and-Reports"><a href="#PRIOR-Prototype-Representation-Joint-Learning-from-Medical-Images-and-Reports" class="headerlink" title="PRIOR: Prototype Representation Joint Learning from Medical Images and Reports"></a>PRIOR: Prototype Representation Joint Learning from Medical Images and Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12577">http://arxiv.org/abs/2307.12577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qtacierp/prior">https://github.com/qtacierp/prior</a></li>
<li>paper_authors: Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo, Xiaoying Tang</li>
<li>for: 本文提出了一种基于对比学习的视频语言共同预训练框架，用于学习医学图像和报告之间的对应关系。</li>
<li>methods: 该方法使用了全球对对比方法，以及一种细致的本地对对比模块，以便学习高级клиниче语言特征和低级视觉特征。此外，一种跨Modalities的条件重建模块是用于在训练阶段交换modalities之间的信息。</li>
<li>results: 实验结果表明，提出的方法在五个下游任务中（包括监督分类、零扩展分类、图像到文本检索、semantic segmentation和物体检测）均表现出色，并且在不同的数据集大小设置下也具有优异性。<details>
<summary>Abstract</summary>
Contrastive learning based vision-language joint pre-training has emerged as a successful representation learning strategy. In this paper, we present a prototype representation learning framework incorporating both global and local alignment between medical images and reports. In contrast to standard global multi-modality alignment methods, we employ a local alignment module for fine-grained representation. Furthermore, a cross-modality conditional reconstruction module is designed to interchange information across modalities in the training phase by reconstructing masked images and reports. For reconstructing long reports, a sentence-wise prototype memory bank is constructed, enabling the network to focus on low-level localized visual and high-level clinical linguistic features. Additionally, a non-auto-regressive generation paradigm is proposed for reconstructing non-sequential reports. Experimental results on five downstream tasks, including supervised classification, zero-shot classification, image-to-text retrieval, semantic segmentation, and object detection, show the proposed method outperforms other state-of-the-art methods across multiple datasets and under different dataset size settings. The code is available at https://github.com/QtacierP/PRIOR.
</details>
<details>
<summary>摘要</summary>
医疗图像和报告的共同预训练基于对比学习已经成为一种成功的表示学习策略。在这篇论文中，我们提出了一种原型学习框架，其中包括医疗图像和报告之间的全局和局部对齐。与标准的全局多Modalities对齐方法不同，我们使用了局部对齐模块，以获得细化的表示。此外，我们还设计了跨Modalities的Conditional重建模块，用于在训练阶段交换modalities之间的信息，通过重建遮盖的图像和报告来进行交换。为恢复长报告，我们构建了句子级prototype记忆银行，使得网络能够关注低级的本地视觉和高级的医学语言特征。此外，我们还提出了一种非自动生成 paradigma，用于恢复非顺序报告。实验结果表明，我们的方法在五个下游任务中，包括supervised分类、零shot分类、图像到文本检索、semantic segmentation和物体检测中，都超过了其他当前state-of-the-art方法。代码可以在https://github.com/QtacierP/PRIOR上获取。
</details></li>
</ul>
<hr>
<h2 id="A-Good-Student-is-Cooperative-and-Reliable-CNN-Transformer-Collaborative-Learning-for-Semantic-Segmentation"><a href="#A-Good-Student-is-Cooperative-and-Reliable-CNN-Transformer-Collaborative-Learning-for-Semantic-Segmentation" class="headerlink" title="A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation"></a>A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12574">http://arxiv.org/abs/2307.12574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinjing Zhu, Yunhao Luo, Xu Zheng, Hao Wang, Lin Wang</li>
<li>for: 本研究的目的是解决如何使 convolutional neural network (CNN) 和 vision transformer (ViT) 模型之间进行协同学习，以实现 semantic segmentation 中的可靠知识选择和交换？</li>
<li>methods: 我们提出了一个在线知识distillation (KD) 框架，可以同时学习高效 yet 紧凑的 CNN 和 ViT 模型，并通过两个关键技术突破 CNN 和 ViT 的局限性。首先，我们提出了不同类征distillation (HFD)，以提高学生在低层特征空间的一致性，模仿 CNN 和 ViT 之间的不同特征。其次，我们提出了双向选择distillation (BSD)，可以动态地将选择的知识传递给对方。这包括1) Region-wise BSD 确定知识传递的方向在特征空间中，2) Pixel-wise BSD 确定在极值空间中传递哪些预测知识。</li>
<li>results: 我们的提出的框架在三个 benchmark 数据集上进行了广泛的实验，并与现有的在线distillation方法相比，表现出了很大的提升。此外，我们的方法还证明了在学习 ViT 和 CNN 模型之间协同学习的可能性。<details>
<summary>Abstract</summary>
In this paper, we strive to answer the question "how to collaboratively learn convolutional neural network (CNN)-based and vision transformer (ViT)-based models by selecting and exchanging the reliable knowledge between them for semantic segmentation?" Accordingly, we propose an online knowledge distillation (KD) framework that can simultaneously learn compact yet effective CNN-based and ViT-based models with two key technical breakthroughs to take full advantage of CNNs and ViT while compensating their limitations. Firstly, we propose heterogeneous feature distillation (HFD) to improve students' consistency in low-layer feature space by mimicking heterogeneous features between CNNs and ViT. Secondly, to facilitate the two students to learn reliable knowledge from each other, we propose bidirectional selective distillation (BSD) that can dynamically transfer selective knowledge. This is achieved by 1) region-wise BSD determining the directions of knowledge transferred between the corresponding regions in the feature space and 2) pixel-wise BSD discerning which of the prediction knowledge to be transferred in the logit space. Extensive experiments on three benchmark datasets demonstrate that our proposed framework outperforms the state-of-the-art online distillation methods by a large margin, and shows its efficacy in learning collaboratively between ViT-based and CNN-based models.
</details>
<details>
<summary>摘要</summary>
在本文中，我们努力回答“如何通过选择和交换可靠知识来协同学习卷积神经网络（CNN）和视觉 трансформа（ViT）模型以进行semantic segmentation？”为此，我们提出了在线知识储存（KD）框架，可同时学习高效又紧凑的CNN和ViT模型，并且具有两个关键技术突破，以全面利用CNN和ViT的优势，同时补做它们的局限性。首先，我们提出了不同类征储存（HFD），以提高学生在低层特征空间的一致性，模仿CNN和ViT之间的不同特征。其次，为了让两个学生之间学习可靠的知识，我们提出了双向选择储存（BSD），可动态传递选择的知识。这实现了1）在特征空间中确定 переда知识的方向，2）在逻辑空间中选择要传递的预测知识。我们对三个标准测试集进行了广泛的实验，结果显示，我们提出的框架在在线储存方法中升级了状态之差，并在学习协同CNN和ViT模型方面表现出了 efficacy。
</details></li>
</ul>
<hr>
<h2 id="MataDoc-Margin-and-Text-Aware-Document-Dewarping-for-Arbitrary-Boundary"><a href="#MataDoc-Margin-and-Text-Aware-Document-Dewarping-for-Arbitrary-Boundary" class="headerlink" title="MataDoc: Margin and Text Aware Document Dewarping for Arbitrary Boundary"></a>MataDoc: Margin and Text Aware Document Dewarping for Arbitrary Boundary</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12571">http://arxiv.org/abs/2307.12571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beiya Dai, Xing li, Qunyi Xie, Yulin Li, Xiameng Qin, Chengquan Zhang, Kun Yao, Junyu Han</li>
<li>for:  DocUNet, DIR300, WarpDoc datasets</li>
<li>methods:  margin regularization, background consistency, word position consistency</li>
<li>results:  superior performance on documents with incomplete boundaries<details>
<summary>Abstract</summary>
Document dewarping from a distorted camera-captured image is of great value for OCR and document understanding. The document boundary plays an important role which is more evident than the inner region in document dewarping. Current learning-based methods mainly focus on complete boundary cases, leading to poor document correction performance of documents with incomplete boundaries. In contrast to these methods, this paper proposes MataDoc, the first method focusing on arbitrary boundary document dewarping with margin and text aware regularizations. Specifically, we design the margin regularization by explicitly considering background consistency to enhance boundary perception. Moreover, we introduce word position consistency to keep text lines straight in rectified document images. To produce a comprehensive evaluation of MataDoc, we propose a novel benchmark ArbDoc, mainly consisting of document images with arbitrary boundaries in four typical scenarios. Extensive experiments confirm the superiority of MataDoc with consideration for the incomplete boundary on ArbDoc and also demonstrate the effectiveness of the proposed method on DocUNet, DIR300, and WarpDoc datasets.
</details>
<details>
<summary>摘要</summary>
文档去扭曲从扭曲捕捉的图像中是很有价值的，尤其是文档边界的角色更加重要。现有的学习型方法主要关注完整边界的文档去扭曲，导致文档修正性能较差。与这些方法不同，本文提出了MataDoc，第一种专门关注任意边界文档去扭曲方法，并添加了边界追求和文本意识regularization。具体来说，我们设计了边界追求的margin regularization，通过考虑背景一致性来增强边界感知。此外，我们引入了文本位置一致的regularization，以保持文本线条在修正后的图像中 straight。为了对MataDoc进行全面的评估，我们提出了一个新的benchmark ArbDoc，主要包括四种典型的文档场景，其中文档边界具有任意形态。广泛的实验表明，MataDoc在ArbDoc上的性能卓越，同时也在DocUNet、DIR300和WarpDoc数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Interpolating-between-Images-with-Diffusion-Models"><a href="#Interpolating-between-Images-with-Diffusion-Models" class="headerlink" title="Interpolating between Images with Diffusion Models"></a>Interpolating between Images with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12560">http://arxiv.org/abs/2307.12560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clinton J. Wang, Polina Golland</li>
<li>for: 这篇论文旨在探讨图像生成和编辑中缺失的特性 interpolating between two input images，以扩展图像生成模型的创作应用。</li>
<li>methods: 该论文提出了一种基于潜在扩散模型的零shot interpolating方法，通过在潜在空间中逐渐减少噪声水平进行 interpolating，然后使用文本排序和主体姿态来进行denoising。</li>
<li>results: 该论文通过在多种主体姿态、图像风格和图像内容中进行 interpolating，并通过CLIP选择最高质量图像，以证明该方法可以获得有力的 interpolating结果。<details>
<summary>Abstract</summary>
One little-explored frontier of image generation and editing is the task of interpolating between two input images, a feature missing from all currently deployed image generation pipelines. We argue that such a feature can expand the creative applications of such models, and propose a method for zero-shot interpolation using latent diffusion models. We apply interpolation in the latent space at a sequence of decreasing noise levels, then perform denoising conditioned on interpolated text embeddings derived from textual inversion and (optionally) subject poses. For greater consistency, or to specify additional criteria, we can generate several candidates and use CLIP to select the highest quality image. We obtain convincing interpolations across diverse subject poses, image styles, and image content, and show that standard quantitative metrics such as FID are insufficient to measure the quality of an interpolation. Code and data are available at https://clintonjwang.github.io/interpolation.
</details>
<details>
<summary>摘要</summary>
一个未经探索的前ier是将两个输入图像之间进行 interpolating，这是现有的图像生成管道中缺失的一个特性。我们认为这样的特性可以扩展图像生成模型的创作应用，并提出了采用潜在扩散模型进行零 shot interpolating的方法。我们在潜在空间中逐渐减少噪声水平进行 interpolating，然后使用文本倒转和（可选）主体姿态来进行杜然处理。为了更好地保持一致性，或者设置其他参数，我们可以生成多个候选图像，并使用 CLIP 选择最高质量的图像。我们在不同的主体姿态、图像风格和图像内容中获得了令人满意的 interpolating，并证明了标准的量化指标如 FID 不够Measure  interpolating 的质量。代码和数据可以在 https://clintonjwang.github.io/interpolation 上获取。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Event-based-Video-Frame-Interpolation"><a href="#Revisiting-Event-based-Video-Frame-Interpolation" class="headerlink" title="Revisiting Event-based Video Frame Interpolation"></a>Revisiting Event-based Video Frame Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12558">http://arxiv.org/abs/2307.12558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaben Chen, Yichen Zhu, Dongze Lian, Jiaqi Yang, Yifu Wang, Renrui Zhang, Xinhang Liu, Shenhan Qian, Laurent Kneip, Shenghua Gao</li>
<li>for: 用于提高视频插值的精度和真实性</li>
<li>methods: 使用事件摄像头提供的高时间密度和高噪声特征进行事件导向的Optical flow refinement策略，以及一种分解器-并发的事件核心Synthesis策略</li>
<li>results: 比前方法更加可靠和真实地生成中间帧结果，并且在实验中表明了考虑事件特征的重要性<details>
<summary>Abstract</summary>
Dynamic vision sensors or event cameras provide rich complementary information for video frame interpolation. Existing state-of-the-art methods follow the paradigm of combining both synthesis-based and warping networks. However, few of those methods fully respect the intrinsic characteristics of events streams. Given that event cameras only encode intensity changes and polarity rather than color intensities, estimating optical flow from events is arguably more difficult than from RGB information. We therefore propose to incorporate RGB information in an event-guided optical flow refinement strategy. Moreover, in light of the quasi-continuous nature of the time signals provided by event cameras, we propose a divide-and-conquer strategy in which event-based intermediate frame synthesis happens incrementally in multiple simplified stages rather than in a single, long stage. Extensive experiments on both synthetic and real-world datasets show that these modifications lead to more reliable and realistic intermediate frame results than previous video frame interpolation methods. Our findings underline that a careful consideration of event characteristics such as high temporal density and elevated noise benefits interpolation accuracy.
</details>
<details>
<summary>摘要</summary>
“动态视觉传感器或事件摄像机可提供丰富的补充信息，以帮助视频帧 interpolate。现有的state-of-the-art方法通常采用组合synthesis-based和折叠网络的方法。然而，这些方法很少充分尊重事件流的内在特征。因为事件摄像机只记录了INTENSITY变化和方向，而不是颜色强度，因此从事件中估算光流 arguably 更加困难 than from RGB信息。我们因此提议将RGB信息 integrate into event-guided optical flow refinement策略。此外，由于事件摄像机提供的时间信号具有 quasi-连续性，我们提议采用分段的 divide-and-conquer策略，在多个简化的阶段中进行事件基本中间帧synthesis，而不是在单一、长阶段中进行。广泛的实验表明，这些修改可以更加可靠和真实地 interpolate 视频帧结果，than previous video frame interpolation方法。我们的发现也 подчеркивает，对事件特征，如高时间密度和提高的噪声，的仔细考虑，可以提高插值精度。”
</details></li>
</ul>
<hr>
<h2 id="MFMAN-YOLO-A-Method-for-Detecting-Pole-like-Obstacles-in-Complex-Environment"><a href="#MFMAN-YOLO-A-Method-for-Detecting-Pole-like-Obstacles-in-Complex-Environment" class="headerlink" title="MFMAN-YOLO: A Method for Detecting Pole-like Obstacles in Complex Environment"></a>MFMAN-YOLO: A Method for Detecting Pole-like Obstacles in Complex Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12548">http://arxiv.org/abs/2307.12548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Cai, Hao Wang, Congling Zhou, Yongqiang Wang, Boyu Liu</li>
<li>for: 解决复杂环境中杆体物体特征信息易丢失问题，提高探测精度和实时性。</li>
<li>methods: 提出了一种多尺度混合注意力机制探测算法，通过最优运输函数孟柯诺夫（MK）函数进行匹配，并将多尺度特征分割和混合注意力机制应用于复杂环境中。</li>
<li>results: 实验结果显示，方法的检测精度、回归率和均值精度分别为94.7%、93.1%和97.4%，检测帧率达400f&#x2F;s。这种方法可以在实时和精度高的情况下探测复杂环境中的杆体物体。<details>
<summary>Abstract</summary>
In real-world traffic, there are various uncertainties and complexities in road and weather conditions. To solve the problem that the feature information of pole-like obstacles in complex environments is easily lost, resulting in low detection accuracy and low real-time performance, a multi-scale hybrid attention mechanism detection algorithm is proposed in this paper. First, the optimal transport function Monge-Kantorovich (MK) is incorporated not only to solve the problem of overlapping multiple prediction frames with optimal matching but also the MK function can be regularized to prevent model over-fitting; then, the features at different scales are up-sampled separately according to the optimized efficient multi-scale feature pyramid. Finally, the extraction of multi-scale feature space channel information is enhanced in complex environments based on the hybrid attention mechanism, which suppresses the irrelevant complex environment background information and focuses the feature information of pole-like obstacles. Meanwhile, this paper conducts real road test experiments in a variety of complex environments. The experimental results show that the detection precision, recall, and average precision of the method are 94.7%, 93.1%, and 97.4%, respectively, and the detection frame rate is 400 f/s. This research method can detect pole-like obstacles in a complex road environment in real time and accurately, which further promotes innovation and progress in the field of automatic driving.
</details>
<details>
<summary>摘要</summary>
实际交通中有各种不确定性和复杂性，以致 pole-like obstacles 的特征信息在复杂环境中易丢失，导致检测精度低下、实时性低下。为解决这个问题，本文提出了一种多尺度混合注意力机制检测算法。首先，通过 Monge-Kantorovich（MK）函数进行最佳运输函数，不仅可以解决多个预测帧的最佳匹配问题，还可以将 MK 函数进行正则化，以防止模型过拟合；然后，在不同尺度上分别更新独立的高效多尺度特征 pyramid。最后，在复杂环境中提高多尺度特征空间通道信息抽取的能力，通过混合注意力机制，抑制不相关的复杂环境背景信息，专注于检测 pole-like obstacles 的特征信息。同时，本文在实际公路测试中进行了多种复杂环境的实验，实验结果表明，该方法的检测精度、回归率和平均精度分别为 94.7%、93.1% 和 97.4%，检测帧率为 400 f/s。这种检测方法可以在复杂交通环境中实时和准确地检测 pole-like obstacles，为自动驾驶技术的进一步创新和发展做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-Deepfake-Detection-by-Primary-Region-Regularization"><a href="#Towards-Generalizable-Deepfake-Detection-by-Primary-Region-Regularization" class="headerlink" title="Towards Generalizable Deepfake Detection by Primary Region Regularization"></a>Towards Generalizable Deepfake Detection by Primary Region Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12534">http://arxiv.org/abs/2307.12534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harry Cheng, Yangyang Guo, Tianyi Wang, Liqiang Nie, Mohan Kankanhalli</li>
<li>for: 提高深伪检测方法的泛化能力，对于未见过的伪造和修改方法进行扩展</li>
<li>methods: 使用新的调整方法来提高深伪检测方法的泛化能力，包括删除主要区域图像的调整</li>
<li>results: 与多个基eline比较，提高了平均性能表现6%，并且与一些现有的基eline竞争Here’s the breakdown of each sentence:* “for”: This sentence indicates the purpose of the paper, which is to improve the generalization ability of deepfake detection methods.* “methods”: This sentence describes the approach used in the paper to achieve the purpose, which is to use a novel regularization perspective to enhance the generalization capability of deepfake detectors.* “results”: This sentence summarizes the performance of the proposed method compared to baseline methods, showing an average improvement of 6% and competitive performance with state-of-the-art baselines.<details>
<summary>Abstract</summary>
The existing deepfake detection methods have reached a bottleneck in generalizing to unseen forgeries and manipulation approaches. Based on the observation that the deepfake detectors exhibit a preference for overfitting the specific primary regions in input, this paper enhances the generalization capability from a novel regularization perspective. This can be simply achieved by augmenting the images through primary region removal, thereby preventing the detector from over-relying on data bias. Our method consists of two stages, namely the static localization for primary region maps, as well as the dynamic exploitation of primary region masks. The proposed method can be seamlessly integrated into different backbones without affecting their inference efficiency. We conduct extensive experiments over three widely used deepfake datasets - DFDC, DF-1.0, and Celeb-DF with five backbones. Our method demonstrates an average performance improvement of 6% across different backbones and performs competitively with several state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
现有的深伪检测方法已达到泛化到未经见 forgery 和 manipulation 方法的瓶颈。基于观察到深伪检测器偏好特定的主要区域在输入中过拟合的观察，这篇文章提高了泛化能力从一个新的规范化视角。这可以简单地通过除去输入图像的主要区域来实现，以防止检测器过于依赖数据偏好。我们的方法包括两个阶段：首先，静态地LOCALIZATION FOR PRIMARY REGION MAPS，然后是动态利用主要区域面罩。我们的方法可以轻松地与不同的背bone结合使用，无需影响其推理效率。我们在DFDC、DF-1.0和Celeb-DF三个广泛使用的深伪数据集上进行了广泛的实验，我们的方法在不同的背bone上显示了平均提高6%的性能，并与一些状态机器人的基准值竞争。
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness"><a href="#On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness" class="headerlink" title="On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"></a>On the Connection between Pre-training Data Diversity and Fine-tuning Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12532">http://arxiv.org/abs/2307.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ludwig Schmidt, Ali Farhadi<br>for: 这个论文的目的是研究预训练策略对深度学习模型的泛化性的影响。methods: 作者使用了多种自然和 sintetic 数据源来生成不同的预训练分布，并通过评估这些预训练分布对下游模型的抗衰假设来研究预训练分布的影响。results: 研究发现，预训练分布中数据量的变化是主要影响下游模型的抗衰假设的因素，而其他因素对下游模型的抗衰假设具有有限的影响。例如，减少 ImageNet 预训练类别的数量，同时增加每个类别中的图像数量（即保持总数据量不变）并不会影响下游模型的抗衰假设。<details>
<summary>Abstract</summary>
Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.
</details>
<details>
<summary>摘要</summary>
pré-entraînement a été largement adopté dans l'apprentissage profond pour améliorer les performances des modèles, especialment lorsque les données d'entraînement pour une tâche cible sont limitées. Dans notre travail, nous voulons comprendre les implications de cette stratégie d'entraînement sur les propriétés de généralisation des modèles downstream. Plus spécifiquement, nous posons la question suivante : comment les propriétés de la distribution de pré-entraînement affectent-elles la robustesse des modèles fine-tunés ? Les propriétés que nous explorons comprennent l'espace de labels, les semantiques de labels, la diversité d'images, les domaines de données et la quantité de données de la distribution de pré-entraînement. Nous trouvons que le facteur primordial influençant la robustesse effective downstream (Taori et al., 2020) est la quantité de données, tandis que les autres facteurs ont un impact limité. Par exemple, en réduisant le nombre de classes de pré-entraînement d'ImageNet de 4 fois while augmentant le nombre d'images par classe de 4 fois (c'est-à-dire en gardant la quantité totale de données fixée), n'a pas d'impact sur la robustesse des modèles fine-tunés. Nous démontrons nos résultats sur des distributions de pré-entraînement tirées de divers sources de données naturelles et synthétiques, en utilisant principalement la distribution shift iWildCam-WILDS comme un test pour la robustesse downstream.
</details></li>
</ul>
<hr>
<h2 id="Entropy-Transformer-Networks-A-Learning-Approach-via-Tangent-Bundle-Data-Manifold"><a href="#Entropy-Transformer-Networks-A-Learning-Approach-via-Tangent-Bundle-Data-Manifold" class="headerlink" title="Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold"></a>Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12517">http://arxiv.org/abs/2307.12517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ijcnn2023/ESTN">https://github.com/ijcnn2023/ESTN</a></li>
<li>paper_authors: Pourya Shamsolmoali, Masoumeh Zareapoor</li>
<li>for: 该 paper ocuses on developing an accurate and efficient image transformation approach for Convolutional Neural Networks (CNNs) architectures.</li>
<li>methods: 该 paper 用 novel Entropy Spatial Transformer Networks (ESTN)  interpolate on the data manifold distributions, 使用 random samples 和 tangent space 计算 transformer parameters. 同时， authors 还提出了一种简单 yet effective technique 来 normalize the non-zero values of convolution operation.</li>
<li>results:  experiments 表明， ESTN 可以在多种 computer vision tasks 中提高预测精度， 包括图像重建和分类， 而减少计算成本。<details>
<summary>Abstract</summary>
This paper focuses on an accurate and fast interpolation approach for image transformation employed in the design of CNN architectures. Standard Spatial Transformer Networks (STNs) use bilinear or linear interpolation as their interpolation, with unrealistic assumptions about the underlying data distributions, which leads to poor performance under scale variations. Moreover, STNs do not preserve the norm of gradients in propagation due to their dependency on sparse neighboring pixels. To address this problem, a novel Entropy STN (ESTN) is proposed that interpolates on the data manifold distributions. In particular, random samples are generated for each pixel in association with the tangent space of the data manifold and construct a linear approximation of their intensity values with an entropy regularizer to compute the transformer parameters. A simple yet effective technique is also proposed to normalize the non-zero values of the convolution operation, to fine-tune the layers for gradients' norm-regularization during training. Experiments on challenging benchmarks show that the proposed ESTN can improve predictive accuracy over a range of computer vision tasks, including image reconstruction, and classification, while reducing the computational cost.
</details>
<details>
<summary>摘要</summary>
To address these problems, a novel Entropy STN (ESTN) is proposed that interpolates on the data manifold distributions. Specifically, random samples are generated for each pixel in association with the tangent space of the data manifold, and a linear approximation of their intensity values is computed with an entropy regularizer to compute the transformer parameters. Additionally, a simple yet effective technique is proposed to normalize the non-zero values of the convolution operation to fine-tune the layers for gradients' norm-regularization during training.Experiments on challenging benchmarks show that the proposed ESTN can improve predictive accuracy over a range of computer vision tasks, including image reconstruction and classification, while reducing the computational cost.
</details></li>
</ul>
<hr>
<h2 id="Cross-Contrasting-Feature-Perturbation-for-Domain-Generalization"><a href="#Cross-Contrasting-Feature-Perturbation-for-Domain-Generalization" class="headerlink" title="Cross Contrasting Feature Perturbation for Domain Generalization"></a>Cross Contrasting Feature Perturbation for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12502">http://arxiv.org/abs/2307.12502</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hackmebroo/ccfp">https://github.com/hackmebroo/ccfp</a></li>
<li>paper_authors: Chenming Li, Daoan Zhang, Wenjian Huang, Jianguo Zhang<br>for: This paper focuses on the problem of domain generalization (DG) and proposes a novel framework called Cross Contrasting Feature Perturbation (CCFP) to simulate domain shift and improve the robustness of the model.methods: The proposed CCFP framework uses an online one-stage approach and generates perturbed features in the latent space while regularizing the model prediction against domain shift. The framework includes learnable feature perturbations and semantic consistency constraints to improve the quality of the perturbed features.results: The proposed method outperforms the previous state-of-the-art in the standard DomainBed benchmark with a strict evaluation protocol. Quantitative analyses show that the method can effectively alleviate the domain shift problem in out-of-distribution (OOD) scenarios.<details>
<summary>Abstract</summary>
Domain generalization (DG) aims to learn a robust model from source domains that generalize well on unseen target domains. Recent studies focus on generating novel domain samples or features to diversify distributions complementary to source domains. Yet, these approaches can hardly deal with the restriction that the samples synthesized from various domains can cause semantic distortion. In this paper, we propose an online one-stage Cross Contrasting Feature Perturbation (CCFP) framework to simulate domain shift by generating perturbed features in the latent space while regularizing the model prediction against domain shift. Different from the previous fixed synthesizing strategy, we design modules with learnable feature perturbations and semantic consistency constraints. In contrast to prior work, our method does not use any generative-based models or domain labels. We conduct extensive experiments on a standard DomainBed benchmark with a strict evaluation protocol for a fair comparison. Comprehensive experiments show that our method outperforms the previous state-of-the-art, and quantitative analyses illustrate that our approach can alleviate the domain shift problem in out-of-distribution (OOD) scenarios.
</details>
<details>
<summary>摘要</summary>
域间泛化（DG）目标是从源域学习一个可以在未看过的目标域中进行泛化的模型。最近的研究主要关注生成新的域样本或特征以增加分布的多样性，但这些方法很难处理源域样本生成的semantic扭曲问题。在这篇论文中，我们提出了在线一阶段 Cross Contrasting Feature Perturbation（CCFP）框架，通过在幂空间生成受损特征来模拟域转移，同时对模型预测进行域转移 regularization。与前一代固定生成策略不同，我们设计了可学习的特征干扰和semantic一致约束。与先前的工作不同，我们的方法不使用任何生成型模型或域标签。我们在DomainBed标准测试床上进行了广泛的实验，并对比评估准则进行严格的评估。广泛的实验结果表明，我们的方法超过了先前的最佳实现，并且量化分析表明，我们的方法可以在OOD（out-of-distribution）场景中缓解域转移问题。
</details></li>
</ul>
<hr>
<h2 id="AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models"><a href="#AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models" class="headerlink" title="AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models"></a>AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12499">http://arxiv.org/abs/2307.12499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelong Dai, Kaisheng Liang, Bin Xiao</li>
<li>for: 本研究旨在提出一种新的方法，即AdvDiff，用于生成不受限制的反击示例，以攻击深度学习模型和反击技术。</li>
<li>methods: 本研究使用了两种新的反击指导技术，即反击扩散模型的梯度导航和反击扩散模型的反向生成过程。这两种技术可以生成高质量、实用的反击示例，并且可以兼顾target类фика器的解释性。</li>
<li>results: 实验结果表明，AdvDiff可以高效地生成不受限制的反击示例，并且在MNIST和ImageNet datasets上的实验结果表明，AdvDiff的攻击性和生成质量都高于基于GAN的方法。<details>
<summary>Abstract</summary>
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective to generate unrestricted adversarial examples, which outperforms GAN-based methods in terms of attack performance and generation quality.
</details>
<details>
<summary>摘要</summary>
深度学习模型面临无限制 adversarial 攻击的威胁，这些攻击可以快速绕过防御机制。然而，过去的攻击方法常用生成式对抗网络（GAN），这些网络不能有效地证明其可行性，特别是在大规模的数据集如 ImageNet 上。在本文中，我们提出一种新的方法，称为 AdvDiff，以使用扩散模型生成无限制 adversarial 示例。我们设计了两种新的对抗导航技术，以在扩散模型的反生成过程中进行对抗采样。这两种技术能够生成高质量、实用的 adversarial 示例，通过可见的梯度来具体化目标分类器的解释。实验结果表明，AdvDiff 在 MNIST 和 ImageNet 数据集上效果地生成了无限制 adversarial 示例，其性能和生成质量都高于基于 GAN 的方法。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Distillation-Do-Not-Overlook-Calibration"><a href="#Rethinking-Data-Distillation-Do-Not-Overlook-Calibration" class="headerlink" title="Rethinking Data Distillation: Do Not Overlook Calibration"></a>Rethinking Data Distillation: Do Not Overlook Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12463">http://arxiv.org/abs/2307.12463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets">https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets</a></li>
<li>paper_authors: Dongyao Zhu, Bowen Lei, Jie Zhang, Yanbo Fang, Ruqi Zhang, Yiqun Xie, Dongkuan Xu</li>
<li>for: 本文旨在解决由大型源数据抽取而得到的神经网络经常产生过于自信的输出问题，通过calibration方法来修正。</li>
<li>methods: 本文提出了Masked Temperature Scaling (MTS)和Masked Distillation Training (MDT)两种方法，可以在神经网络训练过程中进行数据缩写和混合，以提高神经网络的准确率和可靠性。</li>
<li>results: 本文的实验结果表明，使用MTS和MDT可以有效地修正由大型源数据抽取而得到的神经网络，提高其准确率和可靠性，同时保持数据缩写的效率。<details>
<summary>Abstract</summary>
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
</details>
<details>
<summary>摘要</summary>
neural networks 经过精炼数据训练后常会产生过度自信的输出，需要使用均衡方法进行调整。现有的均衡方法，如温度升降和混合方法，对原始大规模数据训练的网络具有良好的效果。然而，我们发现这些方法对含拟合数据训练的网络无法进行均衡。在这篇论文中，我们发现了含拟合数据导致网络无法均衡的两个问题：（一）含拟合数据导致网络的最大幂值分布更加集中，（二）含拟合数据丢失了与分类任务无关 yet semantically meaningful的信息。为解决这问题，我们提出了Masked Temperature Scaling（MTS）和Masked Distillation Training（MDT）两种方法，它们可以缓解含拟合数据的限制，实现更好的均衡结果，同时保持数据精炼的效率。
</details></li>
</ul>
<hr>
<h2 id="Robust-face-anti-spoofing-framework-with-Convolutional-Vision-Transformer"><a href="#Robust-face-anti-spoofing-framework-with-Convolutional-Vision-Transformer" class="headerlink" title="Robust face anti-spoofing framework with Convolutional Vision Transformer"></a>Robust face anti-spoofing framework with Convolutional Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12459">http://arxiv.org/abs/2307.12459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunseung Lee, Youngjun Kwak, Jinho Shin</li>
<li>for: 本研究旨在提高人脸验证过程中的防御性能，对抗真实的演示攻击。</li>
<li>methods: 本研究使用自注意力和卷积层对人脸图像进行全球和局部学习，以提高人脸识别性能。</li>
<li>results: 该模型在不同数据集中的跨域设定中表现出了7.3%$p$和12.9%$p$的提升，并在九个参考模型中得到了最高的平均排名。<details>
<summary>Abstract</summary>
Owing to the advances in image processing technology and large-scale datasets, companies have implemented facial authentication processes, thereby stimulating increased focus on face anti-spoofing (FAS) against realistic presentation attacks. Recently, various attempts have been made to improve face recognition performance using both global and local learning on face images; however, to the best of our knowledge, this is the first study to investigate whether the robustness of FAS against domain shifts is improved by considering global information and local cues in face images captured using self-attention and convolutional layers. This study proposes a convolutional vision transformer-based framework that achieves robust performance for various unseen domain data. Our model resulted in 7.3%$p$ and 12.9%$p$ increases in FAS performance compared to models using only a convolutional neural network or vision transformer, respectively. It also shows the highest average rank in sub-protocols of cross-dataset setting over the other nine benchmark models for domain generalization.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为图像处理技术的进步和大规模数据集，公司已经实施了人脸验证过程，从而促使更多关注面对攻击（FAS）的真实演示攻击。最近，有很多尝试来提高人脸识别性能使用全球和地方学习方法，但到目前为止，这是第一个研究是否可以通过考虑全球信息和地方指示来提高人脸验证性能对域shift。这种研究提出了基于 convolutional vision transformer 框架的方法，实现了对不同数据集的 Robust 性能。我们的模型比只使用 convolutional neural network 或 vision transformer 模型使用时提高了7.3%$p$ 和 12.9%$p$ 的 FAS性能。它还在横跨数据集设定下的各个子协议中显示了最高的平均排名。
</details></li>
</ul>
<hr>
<h2 id="EnTri-Ensemble-Learning-with-Tri-level-Representations-for-Explainable-Scene-Recognition"><a href="#EnTri-Ensemble-Learning-with-Tri-level-Representations-for-Explainable-Scene-Recognition" class="headerlink" title="EnTri: Ensemble Learning with Tri-level Representations for Explainable Scene Recognition"></a>EnTri: Ensemble Learning with Tri-level Representations for Explainable Scene Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12442">http://arxiv.org/abs/2307.12442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Aminimehr, Amirali Molaei, Erik Cambria</li>
<li>for: 提高Scene recognition的可读性和可解释性，同时提高分类精度。</li>
<li>methods: 使用ensemble学习，包括Pixel-level、Semantic segmentation-level和Object class和frequency level的特征编码，以及不同复杂度的特征编码策略。</li>
<li>results: 在MIT67、SUN397和UIUC8 datasets上实现了87.69%、75.56%和99.17%的分类精度，比 estado-of-the-art方法有竞争力。<details>
<summary>Abstract</summary>
Scene recognition based on deep-learning has made significant progress, but there are still limitations in its performance due to challenges posed by inter-class similarities and intra-class dissimilarities. Furthermore, prior research has primarily focused on improving classification accuracy, yet it has given less attention to achieving interpretable, precise scene classification. Therefore, we are motivated to propose EnTri, an ensemble scene recognition framework that employs ensemble learning using a hierarchy of visual features. EnTri represents features at three distinct levels of detail: pixel-level, semantic segmentation-level, and object class and frequency level. By incorporating distinct feature encoding schemes of differing complexity and leveraging ensemble strategies, our approach aims to improve classification accuracy while enhancing transparency and interpretability via visual and textual explanations. To achieve interpretability, we devised an extension algorithm that generates both visual and textual explanations highlighting various properties of a given scene that contribute to the final prediction of its category. This includes information about objects, statistics, spatial layout, and textural details. Through experiments on benchmark scene classification datasets, EnTri has demonstrated superiority in terms of recognition accuracy, achieving competitive performance compared to state-of-the-art approaches, with an accuracy of 87.69%, 75.56%, and 99.17% on the MIT67, SUN397, and UIUC8 datasets, respectively.
</details>
<details>
<summary>摘要</summary>
EnTri represents features at three distinct levels of detail: pixel-level, semantic segmentation-level, and object class and frequency level. By incorporating distinct feature encoding schemes of differing complexity and leveraging ensemble strategies, our approach aims to improve classification accuracy while enhancing transparency and interpretability via visual and textual explanations.To achieve interpretability, we devised an extension algorithm that generates both visual and textual explanations highlighting various properties of a given scene that contribute to the final prediction of its category. This includes information about objects, statistics, spatial layout, and textural details.Through experiments on benchmark scene classification datasets, EnTri has demonstrated superiority in terms of recognition accuracy, achieving competitive performance compared to state-of-the-art approaches, with an accuracy of 87.69%, 75.56%, and 99.17% on the MIT67, SUN397, and UIUC8 datasets, respectively.
</details></li>
</ul>
<hr>
<h2 id="SwIPE-Efficient-and-Robust-Medical-Image-Segmentation-with-Implicit-Patch-Embeddings"><a href="#SwIPE-Efficient-and-Robust-Medical-Image-Segmentation-with-Implicit-Patch-Embeddings" class="headerlink" title="SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings"></a>SwIPE: Efficient and Robust Medical Image Segmentation with Implicit Patch Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12429">http://arxiv.org/abs/2307.12429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yejia Zhang, Pengfei Gu, Nishchal Sapkota, Danny Z. Chen</li>
<li>for: 这个研究的目的是为了提出一个新的医疗影像分类方法，以改善现有的离散表示方法，并且能够获得更好的本地细节和全局形状匹配。</li>
<li>methods: 这个方法使用了隐藏 нейрон网络（INR）来学习连续表示，并且预测形状在图像级层次，而不是点级层次或全图像级层次，以获得更好的本地边界定义和全局形状匹配。</li>
<li>results: 实验结果显示，这个方法可以与现有的离散方法进行比较，并且在两个任务（2D肿瘤分类和3D腹部器官分类）上获得了更好的结果，并且需要较少的参数。此外，这个方法也展示了较好的数据效率和数据类型的适应性。<details>
<summary>Abstract</summary>
Modern medical image segmentation methods primarily use discrete representations in the form of rasterized masks to learn features and generate predictions. Although effective, this paradigm is spatially inflexible, scales poorly to higher-resolution images, and lacks direct understanding of object shapes. To address these limitations, some recent works utilized implicit neural representations (INRs) to learn continuous representations for segmentation. However, these methods often directly adopted components designed for 3D shape reconstruction. More importantly, these formulations were also constrained to either point-based or global contexts, lacking contextual understanding or local fine-grained details, respectively--both critical for accurate segmentation. To remedy this, we propose a novel approach, SwIPE (Segmentation with Implicit Patch Embeddings), that leverages the advantages of INRs and predicts shapes at the patch level--rather than at the point level or image level--to enable both accurate local boundary delineation and global shape coherence. Extensive evaluations on two tasks (2D polyp segmentation and 3D abdominal organ segmentation) show that SwIPE significantly improves over recent implicit approaches and outperforms state-of-the-art discrete methods with over 10x fewer parameters. Our method also demonstrates superior data efficiency and improved robustness to data shifts across image resolutions and datasets. Code is available on Github.
</details>
<details>
<summary>摘要</summary>
现代医学图像分割方法主要使用精度为矩阵的批处理来学习特征和生成预测。虽然有效，但这种方法具有不可修复的局限性，包括空间不灵活、高分辨率图像扩展不良、直接没有对物体形状的理解。为了解决这些限制，一些最近的研究使用了卷积神经网络（INR）来学习连续表示，以提高分割精度。然而，这些方法通常直接采用了设计 для三维形态重建的组件，而且受限于点级或全局上下文，缺乏当地细节或形态准确性。为了改善这个问题，我们提出了一种新的方法：SwIPE（分割with Implicit Patch Embeddings），它利用INR的优点，预测形状在patch水平（而不是点级或图像级），以实现准确的本地边界定义和全局形态协调。我们对两个任务（2D菌体分割和3D腹部器官分割）进行了广泛的评估，结果表明SwIPE在最近的隐式方法中显著提高，并在精度和数据效率方面超过了状态机 discrete方法。我们的方法还在数据偏移和图像分辨率之间具有更好的数据效率和数据弹性。代码可以在Github上获取。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection"><a href="#Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection" class="headerlink" title="Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection"></a>Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12427">http://arxiv.org/abs/2307.12427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuyangSunshine/ABR_IOD">https://github.com/YuyangSunshine/ABR_IOD</a></li>
<li>paper_authors: Liu Yuyang, Cong Yang, Goswami Dipam, Liu Xialei, Joost van de Weijer<br>for: 这篇论文的目的是解决对incremental object detection（IOD）中的catastrophic forgetting问题。methods: 本文使用了一个称为Augmented Box Replay（ABR）的新方法，它将仅储存和重复过去任务中的前景物体，以避免预义遗传问题。此外，本文也提出了一种创新的注意力捕捉RoI特征的Attentive RoI Distillation损失，它使用领域特征的空间注意力来锁定现在的模型对过去模型的重要信息的注意。results: ABR有效地降低了之前任务的遗传，同时保持了目前任务的高柔养性。此外，ABR还能够储存和重复的减少储存需求，相比于标准的图像重复。实验结果表明，本文的模型在Pascal-VOC和COCO dataset上具有现场的表现。<details>
<summary>Abstract</summary>
In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model.
</details>
<details>
<summary>摘要</summary>
在增量学习中，重新播放之前任务的样本和当前任务的样本是解决快速卷积承忘的一种最有效的方法。然而，与增量分类不同，图像重新播放在增量物体检测（IOD）中尚未得到成功。在这篇论文中，我们认为背景变化导致的前景偏移是主要的问题。背景变化仅发生在重新播放前任务的图像时，并且指的是图像的背景中可能包含当前任务的前景对象。为解决这个问题，我们开发了一种新的和高效的增量盒子重播（ABR）方法，该方法仅存储和重播前景对象，因此可以避免前景偏移问题。此外，我们提出了一种创新的注意力捕捉的区域特征练习损失（Attentive RoI Distillation loss），该损失使当前模型通过区域特征中的注意力来约束当前模型关注到最重要的信息。ABR显著减少了之前类的忘记，同时保持当前类的高灵活性。此外，它比标准图像重新播放要减少存储需求。我们在 Pascal-VOC 和 COCO 数据集上进行了广泛的实验，并证明了我们的模型的状态级表现。
</details></li>
</ul>
<hr>
<h2 id="TransNet-Transparent-Object-Manipulation-Through-Category-Level-Pose-Estimation"><a href="#TransNet-Transparent-Object-Manipulation-Through-Category-Level-Pose-Estimation" class="headerlink" title="TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation"></a>TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12400">http://arxiv.org/abs/2307.12400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huijie Zhang, Anthony Opipari, Xiaotong Chen, Jiyue Zhu, Zeren Yu, Odest Chadwicke Jenkins</li>
<li>for: 本研究旨在提高自动化透明物体检测和操作系统的可靠性和精度，特别是在透明物体上进行Category-levelpose estimation。</li>
<li>methods: 本研究提出了一种两stage管道 named TransNet，包括本地化深度完成和表面法向估计两个阶段。TransNet使用了一种新的surface normal estimation方法，并且使用了一种新的depth completion方法来提高pose estimation的准确性。</li>
<li>results: 对于一个大规模的透明物体 dataset，TransNet achieved improved pose estimation accuracy compared to a state-of-the-art category-level pose estimation approach. In addition, TransNet was used to build an autonomous transparent object manipulation system for robotic pick-and-place and pouring tasks, which demonstrated its effectiveness in real-world applications.<details>
<summary>Abstract</summary>
Transparent objects present multiple distinct challenges to visual perception systems. First, their lack of distinguishing visual features makes transparent objects harder to detect and localize than opaque objects. Even humans find certain transparent surfaces with little specular reflection or refraction, like glass doors, difficult to perceive. A second challenge is that depth sensors typically used for opaque object perception cannot obtain accurate depth measurements on transparent surfaces due to their unique reflective properties. Stemming from these challenges, we observe that transparent object instances within the same category, such as cups, look more similar to each other than to ordinary opaque objects of that same category. Given this observation, the present paper explores the possibility of category-level transparent object pose estimation rather than instance-level pose estimation. We propose \textit{\textbf{TransNet}, a two-stage pipeline that estimates category-level transparent object pose using localized depth completion and surface normal estimation. TransNet is evaluated in terms of pose estimation accuracy on a large-scale transparent object dataset and compared to a state-of-the-art category-level pose estimation approach. Results from this comparison demonstrate that TransNet achieves improved pose estimation accuracy on transparent objects. Moreover, we use TransNet to build an autonomous transparent object manipulation system for robotic pick-and-place and pouring tasks.
</details>
<details>
<summary>摘要</summary>
trasparent objects present multiple distinct challenges to visual perception systems. First, their lack of distinguishing visual features makes transparent objects harder to detect and localize than opaque objects. Even humans find certain transparent surfaces with little specular reflection or refraction, like glass doors, difficult to perceive. A second challenge is that depth sensors typically used for opaque object perception cannot obtain accurate depth measurements on transparent surfaces due to their unique reflective properties. Stemming from these challenges, we observe that transparent object instances within the same category, such as cups, look more similar to each other than to ordinary opaque objects of that same category. Given this observation, the present paper explores the possibility of category-level transparent object pose estimation rather than instance-level pose estimation. We propose \textbf{\textit{TransNet}， a two-stage pipeline that estimates category-level transparent object pose using localized depth completion and surface normal estimation. TransNet is evaluated in terms of pose estimation accuracy on a large-scale transparent object dataset and compared to a state-of-the-art category-level pose estimation approach. Results from this comparison demonstrate that TransNet achieves improved pose estimation accuracy on transparent objects. Moreover, we use TransNet to build an autonomous transparent object manipulation system for robotic pick-and-place and pouring tasks.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Robust-Visual-Grounding-with-Masked-Reference-based-Centerpoint-Supervision"><a href="#Iterative-Robust-Visual-Grounding-with-Masked-Reference-based-Centerpoint-Supervision" class="headerlink" title="Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision"></a>Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12392">http://arxiv.org/abs/2307.12392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cv516buaa/ir-vg">https://github.com/cv516buaa/ir-vg</a></li>
<li>paper_authors: Menghao Li, Chunlei Wang, Wenquan Feng, Shuchang Lyu, Guangliang Cheng, Xiangtai Li, Binghao Liu, Qi Zhao<br>for: 本研究旨在解决现有的视觉固定问题，即基于给定的描述生成假阳性对象。methods: 本研究提出了一种Iterative Robust Visual Grounding（IR-VG）框架，包括多层视语融合（IMVF）和掩码参考中心点监督（MRCS）等技术，以提高对描述的匹配和对图像中的细节特征的捕捉。results: 对五个常见的视觉固定数据集和两个新提出的鲁棒视觉固定数据集进行了广泛的实验，并得到了新的最佳性能记录，相比之前的最佳方法在两个新提出的鲁棒视觉固定数据集上提高了25%和10%。此外，该方法还在五个常见的视觉固定数据集上得到了证明。<details>
<summary>Abstract</summary>
Visual Grounding (VG) aims at localizing target objects from an image based on given expressions and has made significant progress with the development of detection and vision transformer. However, existing VG methods tend to generate false-alarm objects when presented with inaccurate or irrelevant descriptions, which commonly occur in practical applications. Moreover, existing methods fail to capture fine-grained features, accurate localization, and sufficient context comprehension from the whole image and textual descriptions. To address both issues, we propose an Iterative Robust Visual Grounding (IR-VG) framework with Masked Reference based Centerpoint Supervision (MRCS). The framework introduces iterative multi-level vision-language fusion (IMVF) for better alignment. We use MRCS to ahieve more accurate localization with point-wised feature supervision. Then, to improve the robustness of VG, we also present a multi-stage false-alarm sensitive decoder (MFSD) to prevent the generation of false-alarm objects when presented with inaccurate expressions. The proposed framework is evaluated on five regular VG datasets and two newly constructed robust VG datasets. Extensive experiments demonstrate that IR-VG achieves new state-of-the-art (SOTA) results, with improvements of 25\% and 10\% compared to existing SOTA approaches on the two newly proposed robust VG datasets. Moreover, the proposed framework is also verified effective on five regular VG datasets. Codes and models will be publicly at https://github.com/cv516Buaa/IR-VG.
</details>
<details>
<summary>摘要</summary>
Visual Grounding (VG) target  objetcs  from  an  image  based  on  given  expressions  and  has  made  significant  progress  with  the  development  of  detection  and  vision  transformer.  However,  existing  VG  methods  tend  to  generate  false-alarm  objects  when  presented  with  inaccurate  or  irrelevant  descriptions,  which  commonly  occur  in  practical  applications.  Moreover,  existing  methods  fail  to  capture  fine-grained  features,  accurate  localization,  and  sufficient  context  comprehension  from  the  whole  image  and  textual  descriptions.  To  address  both  issues,  we  propose  an  Iterative  Robust  Visual  Grounding  (IR-VG)  framework  with  Masked  Reference  based  Centerpoint  Supervision  (MRCS).  The  framework  introduces  iterative  multi-level  vision-language  fusion  (IMVF)  for  better  alignment.  We  use  MRCS  to  achieve  more  accurate  localization  with  point-wised  feature  supervision.  Then,  to  improve  the  robustness  of  VG,  we  also  present  a  multi-stage  false-alarm  sensitive  decoder  (MFSD)  to  prevent  the  generation  of  false-alarm  objects  when  presented  with  inaccurate  expressions.  The  proposed  framework  is  evaluated  on  five  regular  VG  datasets  and  two  newly  constructed  robust  VG  datasets.  Extensive  experiments  demonstrate  that  IR-VG  achieves  new  state-of-the-art  (SOTA)  results,  with  improvements  of  25%  and  10%  compared  to  existing  SOTA  approaches  on  the  two  newly  proposed  robust  VG  datasets.  Moreover,  the  proposed  framework  is  also  verified  effective  on  five  regular  VG  datasets.  Codes  and  models  will  be  publicly  available  at  https://github.com/cv516Buaa/IR-VG.
</details></li>
</ul>
<hr>
<h2 id="Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting"><a href="#Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting" class="headerlink" title="Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting"></a>Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02505">http://arxiv.org/abs/2308.02505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly</li>
<li>for: This paper aims to evaluate the effectiveness of using Generative Adversarial Networks (GANs) for data augmentation in biomedical image analysis, and to investigate the impact of different sample sizes on the diversity and quality of synthetic images.</li>
<li>methods: The paper uses Multi-scale Structural Similarity Index Measure, Cosine Distance, and Frechet Inception Distance to evaluate the diversity and quality of synthetic images generated by a Deep Convolutional GAN in both biomedical and non-biomedical imaging modalities.</li>
<li>results: The results show that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities, and that the diversity and quality of synthetic images are affected by the sample size used for training the GAN.<details>
<summary>Abstract</summary>
In biomedical image analysis, data imbalance is common across several imaging modalities. Data augmentation is one of the key solutions in addressing this limitation. Generative Adversarial Networks (GANs) are increasingly being relied upon for data augmentation tasks. Biomedical image features are sensitive to evaluating the efficacy of synthetic images. These features can have a significant impact on metric scores when evaluating synthetic images across different biomedical imaging modalities. Synthetically generated images can be evaluated by comparing the diversity and quality of real images. Multi-scale Structural Similarity Index Measure and Cosine Distance are used to evaluate intra-class diversity, while Frechet Inception Distance is used to evaluate the quality of synthetic images. Assessing these metrics for biomedical and non-biomedical imaging is important to investigate an informed strategy in evaluating the diversity and quality of synthetic images. In this work, an empirical assessment of these metrics is conducted for the Deep Convolutional GAN in a biomedical and non-biomedical setting. The diversity and quality of synthetic images are evaluated using different sample sizes. This research intends to investigate the variance in diversity and quality across biomedical and non-biomedical imaging modalities. Results demonstrate that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities.
</details>
<details>
<summary>摘要</summary>
在生物医学影像分析中，数据不均衡是广泛存在的问题，而数据扩充是解决这个问题的关键方法之一。生成敌对网络（GANs）在数据扩充任务中得到了越来越多的应用。生物医学影像特征对于评估 sintetic 图像的效果非常敏感。这些特征在评估不同生物医学成像模式下的 sintetic 图像时可以有显著的影响。 sintetic 图像可以通过比较真实图像的多样性和质量来评估。多尺度结构相似度指标和夹角距离是评估内部多样性的方法，而干扰抽象距离则是评估 sintetic 图像质量的方法。为了了解 informed 策略，需要对生物医学和非生物医学成像进行评估。本研究通过对这些指标进行实际评估，研究了不同样本大小下的多样性和质量的变化。结果表明，在生物医学到生物医学和生物医学到非生物医学的转换中，指标分数差异显著。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.CV_2023_07_24/" data-id="clpxp6c0j00h2ee8837kh5obf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.AI_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T12:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.AI_2023_07_24/">cs.AI - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network"><a href="#QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network" class="headerlink" title="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network"></a>QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12906">http://arxiv.org/abs/2307.12906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Md. Saiful Islam, Jungpil Shin, M. F. Mridha, Yuichi Okuyama<br>for: 这个研究是为了提高供应链管理中的货物预先预测，以便优化存储控制、减少成本和提高顾客满意度。methods: 本研究提出了一个新的方法ológical framework，使用量子概念的启发法来预测货物预先，并且使用量子-классиiral neural network来预测货物预先效果。results: 实验评估表明，QAmplifyNet模型在短时间和不寻常的数据集上预测货物预先的性能比 классиical models、量子组合、量子神经网和深度强化学习模型更好。此外，QAmplifyNet模型的可读性也得到了改进，使用可读性的人工智能技术。实际应用中，QAmplifyNet模型可以实现有效的存储控制、减少货物预先和提高操作效率。未来的工作包括进一步探索量子概念启发法，扩大数据集和探索其他供应链应用。<details>
<summary>Abstract</summary>
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence techniques. Practical implications include improved inventory control, reduced backorders, and enhanced operational efficiency. QAmplifyNet seamlessly integrates into real-world supply chain management systems, enabling proactive decision-making and efficient resource allocation. Future work involves exploring additional quantum-inspired techniques, expanding the dataset, and investigating other supply chain applications. This research unlocks the potential of quantum computing in supply chain optimization and paves the way for further exploration of quantum-inspired machine learning models in supply chain management. Our framework and QAmplifyNet model offer a breakthrough approach to supply chain backorder prediction, providing superior performance and opening new avenues for leveraging quantum-inspired techniques in supply chain management.
</details>
<details>
<summary>摘要</summary>
供应链管理需要准确预测落后订单，以优化存储控制、降低成本和提高客户满意度。然而，传统的机器学习模型在巨量数据和复杂关系下难以取得实际数据收集。本研究提出了一种新的方法ológica framework for supply chain backorder prediction，解决了处理巨量数据的挑战。我们的提出的模型，QAmplifyNet，利用量子启发技术在量子-классиical neural network中预测落后订单，效果更高于经典模型、量子集合、量子神经网络和深度奖励学习。实验评估表明，QAmplifyNet在短时间和不均衡数据上的预测性能较高，适用于实际供应链管理。为提高模型可读性，我们使用Explainable Artificial Intelligence技术。实际应用包括改善存储控制、减少落后订单和提高操作效率。QAmplifyNet可顺利 интегра到实际供应链管理系统中，允许批处理决策和有效资源分配。未来工作包括进一步探索量子启发技术、扩大数据集和探索其他供应链应用。本研究解锁了量子计算在供应链优化中的潜力，开辟了量子启发机器学习模型在供应链管理中的新 Avenues。我们的框架和QAmplifyNet模型提供了落后订单预测的突破方法，提供了更高的性能，开启了新的可能性 для利用量子启发技术在供应链管理中。
</details></li>
</ul>
<hr>
<h2 id="Towards-Bridging-the-FL-Performance-Explainability-Trade-Off-A-Trustworthy-6G-RAN-Slicing-Use-Case"><a href="#Towards-Bridging-the-FL-Performance-Explainability-Trade-Off-A-Trustworthy-6G-RAN-Slicing-Use-Case" class="headerlink" title="Towards Bridging the FL Performance-Explainability Trade-Off: A Trustworthy 6G RAN Slicing Use-Case"></a>Towards Bridging the FL Performance-Explainability Trade-Off: A Trustworthy 6G RAN Slicing Use-Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12903">http://arxiv.org/abs/2307.12903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swastika Roy, Hatim Chergui, Christos Verikoukis</li>
<li>for:  sixth-generation (6G) networks 与多元网络slice 共存下，AI驱动的零touch管理和orchestration (MANO) 成为重要的。但是，确保AI黑盒子在实际应用中的可靠性是问题。</li>
<li>methods:  this paper presents a novel explanation-guided in-hoc federated learning (FL) approach, which combines a constrained resource allocation model and an explainer exchange in a closed loop (CL) fashion to achieve transparent 6G network slicing resource management in a RAN-Edge setup under non-independent identically distributed (non-IID) datasets.</li>
<li>results:  the proposed approach achieves a balance between AI performance and explainability, and outperforms the unconstrained Integrated-Gradient post-hoc FL baseline in terms of faithfulness of explanations and overall training process.Here is the full answer in Simplified Chinese:</li>
<li>for:  sixth-generation (6G) networks 与多元网络slice 共存下，AI驱动的零touch管理和orchestration (MANO) 成为重要的。但是，确保AI黑盒子在实际应用中的可靠性是问题。</li>
<li>methods:  this paper presents a novel explanation-guided in-hoc federated learning (FL) approach, which combines a constrained resource allocation model and an explainer exchange in a closed loop (CL) fashion to achieve transparent 6G network slicing resource management in a RAN-Edge setup under non-independent identically distributed (non-IID) datasets.</li>
<li>results:  the proposed approach achieves a balance between AI performance and explainability, and outperforms the unconstrained Integrated-Gradient post-hoc FL baseline in terms of faithfulness of explanations and overall training process.<details>
<summary>Abstract</summary>
In the context of sixth-generation (6G) networks, where diverse network slices coexist, the adoption of AI-driven zero-touch management and orchestration (MANO) becomes crucial. However, ensuring the trustworthiness of AI black-boxes in real deployments is challenging. Explainable AI (XAI) tools can play a vital role in establishing transparency among the stakeholders in the slicing ecosystem. But there is a trade-off between AI performance and explainability, posing a dilemma for trustworthy 6G network slicing because the stakeholders require both highly performing AI models for efficient resource allocation and explainable decision-making to ensure fairness, accountability, and compliance. To balance this trade off and inspired by the closed loop automation and XAI methodologies, this paper presents a novel explanation-guided in-hoc federated learning (FL) approach where a constrained resource allocation model and an explainer exchange -- in a closed loop (CL) fashion -- soft attributions of the features as well as inference predictions to achieve a transparent 6G network slicing resource management in a RAN-Edge setup under non-independent identically distributed (non-IID) datasets. In particular, we quantitatively validate the faithfulness of the explanations via the so-called attribution-based confidence metric that is included as a constraint to guide the overall training process in the run-time FL optimization task. In this respect, Integrated-Gradient (IG) as well as Input $\times$ Gradient and SHAP are used to generate the attributions for our proposed in-hoc scheme, wherefore simulation results under different methods confirm its success in tackling the performance-explainability trade-off and its superiority over the unconstrained Integrated-Gradient post-hoc FL baseline.
</details>
<details>
<summary>摘要</summary>
在 sixth-generation（6G）网络上，多种网络slice共存的情况下，采用AI驱动的零 touched Management和Orchestration（MANO）变得非常重要。然而，在实际部署中确保AI黑obox的可靠性是一项挑战。 Explainable AI（XAI）工具可以在slice ecosystem中建立透明度，但是存在AI性能和解释性之间的负担，这种负担对于可靠的6G网络slice进行分配是一个悖论。为了平衡这个负担，我们提出了一种基于closed loop自动化和XAI方法的新的解释导向 federated learning（FL）方法。在这种方法中，一个受限的资源分配模型和一个解释器在closed loop（CL）方式交换软属性和推理预测结果，以实现透明的6G网络slice资源管理。特别是，我们在运行时FL优化任务中包含了一个受限的权重矩阵，以确保解释的准确性。在这种情况下，我们使用Integrated-Gradient（IG）、Input × Gradient和SHAP等方法生成解释，并通过在不同方法下的 simulate结果证明了我们的方法的成功。
</details></li>
</ul>
<hr>
<h2 id="As-Time-Goes-By-Adding-a-Temporal-Dimension-Towards-Resolving-Delegations-in-Liquid-Democracy"><a href="#As-Time-Goes-By-Adding-a-Temporal-Dimension-Towards-Resolving-Delegations-in-Liquid-Democracy" class="headerlink" title="As Time Goes By: Adding a Temporal Dimension Towards Resolving Delegations in Liquid Democracy"></a>As Time Goes By: Adding a Temporal Dimension Towards Resolving Delegations in Liquid Democracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12898">http://arxiv.org/abs/2307.12898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evangelos Markakis, Georgios Papasotiropoulos</li>
<li>for: This paper aims to integrate a time horizon into decision-making problems in Liquid Democracy systems to enhance participation.</li>
<li>methods: The paper uses temporal graph theory to analyze the computational complexity of Liquid Democracy systems with a time horizon.</li>
<li>results: The paper shows that adding a time horizon can increase the number of possible delegation paths and reduce the loss of votes due to delegation cycles or abstaining agents, ultimately enhancing participation in Liquid Democracy systems.<details>
<summary>Abstract</summary>
In recent years, the study of various models and questions related to Liquid Democracy has been of growing interest among the community of Computational Social Choice. A concern that has been raised, is that current academic literature focuses solely on static inputs, concealing a key characteristic of Liquid Democracy: the right for a voter to change her mind as time goes by, regarding her options of whether to vote herself or delegate her vote to other participants, till the final voting deadline. In real life, a period of extended deliberation preceding the election-day motivates voters to adapt their behaviour over time, either based on observations of the remaining electorate or on information acquired for the topic at hand. By adding a temporal dimension to Liquid Democracy, such adaptations can increase the number of possible delegation paths and reduce the loss of votes due to delegation cycles or delegating paths towards abstaining agents, ultimately enhancing participation. Our work takes a first step to integrate a time horizon into decision-making problems in Liquid Democracy systems. Our approach, via a computational complexity analysis, exploits concepts and tools from temporal graph theory which turn out to be convenient for our framework.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anytime-Model-Selection-in-Linear-Bandits"><a href="#Anytime-Model-Selection-in-Linear-Bandits" class="headerlink" title="Anytime Model Selection in Linear Bandits"></a>Anytime Model Selection in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12897">http://arxiv.org/abs/2307.12897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, Andreas Krause</li>
<li>for: 本文研究了带剑优化中的模型选择问题，即在搜索和利用之间进行平衡，以便在不同的模型中选择最佳的一个。</li>
<li>methods: 作者使用了在线学习算法，将不同的模型当做专家进行处理。然而，现有的方法具有$\text{poly}M$的复杂度，与模型数量 $M$ 成直接相关。</li>
<li>results: 作者提出了ALEXP方法，它具有$\log M$ 的依赖关系，并且具有任何时间保证的减册。此外，ALEXP方法不需要知道预测horizon $n$，也不需要早期充分探索阶段。<details>
<summary>Abstract</summary>
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details>
<details>
<summary>摘要</summary>
模型选择在带刺优化上是一个复杂的问题，因为它需要平衡探索和利用，不仅 для动作选择，而且还为模型选择。一个自然的方法是通过在线学习算法来处理不同的模型。现有方法，然而， scales poorly（$\text{poly}M$）于模型数量 $M$ 的 regret。我们的关键发现是，在线选择器中的模型选择可以通过利用全信息反馈来让在线学习者具有有利的偏差-variance质量。这允许我们开发 ALEXP，它的 regret 有 exponentially 改进的（$\log M$）依赖于 $M$。ALEXP 具有任何时间 guarantees 的 regret，并不需要知道天数 $n$，也不需要初始阶段的纯探索阶段。我们的方法利用了一种新的时间uniform 分析，建立了在线学习和高维统计之间的新连接。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Stereotype-Identification-through-Reasoning"><a href="#Interpretable-Stereotype-Identification-through-Reasoning" class="headerlink" title="Interpretable Stereotype Identification through Reasoning"></a>Interpretable Stereotype Identification through Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00071">http://arxiv.org/abs/2308.00071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak<br>for: 这篇研究的目的是探讨语模型中的偏见，并在其开发中整合公平性，以确保这些模型是无偏见的和公平的。methods: 本研究使用了Vicuna-13B-v1.3进行零执行刻 sterotype 识别，并评估了将13B扩展到33B的影响。results: 研究发现，从理解到执行的改善 exceeds 从扩展到33B的改善，这表明了理解可以帮助语模型在离域任务上超越扩展的法则。此外，通过选择性分析一些理解迹象，我们显示了如何理解可以提高决策的解释性。<details>
<summary>Abstract</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details>
<details>
<summary>摘要</summary>
language models 可能会含有隐性偏见，这可能会导致不必要地扩大系统歧视。因此，检查和解决语言模型中的偏见变得非常重要，以确保这些模型是公正的。在这种情况下，我们展示了在 Vicuna-13B-v1.3 基础上进行零 shot 刻板印象的重要性。虽然我们确实观察到了从 13B 缩放到 33B 的性能提升，但我们发现，理解的性能提升远超过缩放的提升。我们的发现表明，理解可以使 LLMS 突破预期的性能增长。此外，通过分析 select 的理解轨迹，我们指出了如何使用理解来提高决策的可读性。
</details></li>
</ul>
<hr>
<h2 id="A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis"><a href="#A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis" class="headerlink" title="A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"></a>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12856">http://arxiv.org/abs/2307.12856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</li>
<li>for: 这篇论文的目的是提出一种基于大语言模型（LLM）的自主网络浏览器，可以根据自然语言指令完成真实网站上的任务。</li>
<li>methods: 这篇论文使用了Flan-U-PaLM和HTML-T5两种大语言模型，其中Flan-U-PaLM用于固有代码生成，HTML-T5用于长HTML文档的规划和摘要。这两种模型都使用了地方和全局注意力机制，并使用了混合长时间杂化目标来进行规划和摘要。</li>
<li>results: 论文的实验表明，使用WebAgent可以在真实网站上提高成功率超过50%，并且HTML-T5模型在解决HTML基于任务上的精度高于之前的SoTA。<details>
<summary>Abstract</summary>
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that can complete the tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via generated Python programs from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our recipe improves the success on a real website by over 50%, and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9% higher success rate than prior SoTA on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details>
<details>
<summary>摘要</summary>
Recently, pre-trained large language models (LLMs) have achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites is still affected by three factors: open domain, limited context length, and lack of inductive bias on HTML. To address these issues, we introduce WebAgent, an LLM-driven agent that can complete tasks on real websites based on natural language instructions.WebAgent plans ahead by breaking down instructions into canonical sub-instructions, summarizing long HTML documents into task-relevant snippets, and acting on websites through generated Python programs. We use Flan-U-PaLM for grounded code generation and HTML-T5, a new pre-trained LLM that utilizes local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.Our empirical results show that our recipe improves the success rate on a real website by over 50%, and HTML-T5 is the best model for solving HTML-based tasks, achieving a 14.9% higher success rate than the prior state-of-the-art on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details></li>
</ul>
<hr>
<h2 id="EPIC-KITCHENS-100-Unsupervised-Domain-Adaptation-Challenge-Mixed-Sequences-Prediction"><a href="#EPIC-KITCHENS-100-Unsupervised-Domain-Adaptation-Challenge-Mixed-Sequences-Prediction" class="headerlink" title="EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge: Mixed Sequences Prediction"></a>EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge: Mixed Sequences Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12837">http://arxiv.org/abs/2307.12837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirshayan Nasirimajd, Simone Alberto Peirone, Chiara Plizzari, Barbara Caputo</li>
<li>For: 本研究是为了解决Unsupervised Domain Adaptation (UDA) Challenge in Action Recognition 中的问题。* Methods: 我们采用了一种基于序列的方法，即将源频率和目标频率 randomly combine 生成一个修改后的序列，然后使用标准的 pseudo-labeling 策略提取目标频率中的动作标签。* Results: 我们的提交（名为 ‘sshayan’）可以在领导人员中找到，目前在 ‘verb’ 和 ‘noun’ 两个分类中排名第二和第四。<details>
<summary>Abstract</summary>
This report presents the technical details of our approach for the EPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action Recognition. Our approach is based on the idea that the order in which actions are performed is similar between the source and target domains. Based on this, we generate a modified sequence by randomly combining actions from the source and target domains. As only unlabelled target data are available under the UDA setting, we use a standard pseudo-labeling strategy for extracting action labels for the target. We then ask the network to predict the resulting action sequence. This allows to integrate information from both domains during training and to achieve better transfer results on target. Additionally, to better incorporate sequence information, we use a language model to filter unlikely sequences. Lastly, we employed a co-occurrence matrix to eliminate unseen combinations of verbs and nouns. Our submission, labeled as 'sshayan', can be found on the leaderboard, where it currently holds the 2nd position for 'verb' and the 4th position for both 'noun' and 'action'.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这份报告介绍了我们在EPIC-Kitchens-100Unsupervised Domain Adaptation（UDA）挑战中的动作识别技术细节。我们的方法基于源频率和目标频率中动作的顺序相似性。我们随机将源频率和目标频率中的动作组合在一起，然后使用标准的 Pseudo-labeling 策略提取目标频率中的动作标签。我们然后问网络预测结果的动作序列。这allow我们在训练中 integrating信息从两个频率中，以实现更好的传输结果。此外，我们还使用语言模型筛选不可能的序列，以及一个co-occurrence Matrix来消除未看到的动词和名词的组合。我们的提交，标记为'sshayan'，可以在排行榜上找到，现在在'verb'、'noun'和'action'三个分类中分别排名第二和第四。
</details></li>
</ul>
<hr>
<h2 id="Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains"><a href="#Joint-speech-and-overlap-detection-a-benchmark-over-multiple-audio-setup-and-speech-domains" class="headerlink" title="Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains"></a>Joint speech and overlap detection: a benchmark over multiple audio setup and speech domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13012">http://arxiv.org/abs/2307.13012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Lebourdais, Théo Mariotte, Marie Tahon, Anthony Larcher, Antoine Laurent, Silvio Montresor, Sylvain Meignier, Jean-Hugh Thomas<br>for: 这篇论文主要针对的是voice activity detection和 overlap speech detection的预处理任务，以提高speaker diarization的最终 segmentation性能。methods: 这篇论文提出了一个全新的benchmark，用于评估不同的voice activity detection和overlap speech detection模型，在多个音频设置和语音频道上。这些模型结合了一个Temporal Convolutional Network和适应于设置的语音表示，可以达到 state-of-the-art的性能水平。results: 这篇论文的实验结果显示，将voice activity detection和overlap speech detection作为一个 jointly trained模型，可以提高预处理性能，同时降低了训练成本。此外，这种unique的架构还可以用于单和多通道speech处理。<details>
<summary>Abstract</summary>
Voice activity and overlapped speech detection (respectively VAD and OSD) are key pre-processing tasks for speaker diarization. The final segmentation performance highly relies on the robustness of these sub-tasks. Recent studies have shown VAD and OSD can be trained jointly using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字符串。<</SYS>>声音活动和重叠说话检测（简称VAD和OSD）是Speaker Diagnosis的关键预处理任务。最终分 segmentation 性能强度取决于这两个子任务的稳定性。 current studies have shown that VAD and OSD can be trained together using a multi-class classification model. However, these works are often restricted to a specific speech domain, lacking information about the generalization capacities of the systems. This paper proposes a complete and new benchmark of different VAD and OSD models, on multiple audio setups (single/multi-channel) and speech domains (e.g. media, meeting...). Our 2/3-class systems, which combine a Temporal Convolutional Network with speech representations adapted to the setup, outperform state-of-the-art results. We show that the joint training of these two tasks offers similar performances in terms of F1-score to two dedicated VAD and OSD systems while reducing the training cost. This unique architecture can also be used for single and multichannel speech processing.
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Deep-Transfer-Learning-for-Calibration-free-Motor-Imagery-Brain-Computer-Interfaces"><a href="#End-to-End-Deep-Transfer-Learning-for-Calibration-free-Motor-Imagery-Brain-Computer-Interfaces" class="headerlink" title="End-to-End Deep Transfer Learning for Calibration-free Motor Imagery Brain Computer Interfaces"></a>End-to-End Deep Transfer Learning for Calibration-free Motor Imagery Brain Computer Interfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12827">http://arxiv.org/abs/2307.12827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Alimardani, Steven Kocken, Nikki Leeuwis<br>for:这个研究的目的是开发一种可以在各种应用场景中使用的无需参数调整的motor imagery brain-computer interface（MI-BCI）分类器。methods:这个研究使用了深度训练学习，并在 Raw EEG 信号上进行了一种端到端的深度学习方法。三种深度学习模型（MIN2Net、EEGNet 和 DeepConvNet）被训练并比较使用了一个公开available的数据集。results:在离势一个subject出的cross-validation中，MIN2Net 无法在新用户中分辨右手和左手的motor imagery，具有51.7%的 median 准确率。而 EEGNet 和 DeepConvNet 两种模型在其他数据集上没有进行过训练，但在这个数据集上的 median 准确率分别为62.5% 和 59.2%。这些准确率虽然不足70%，但与其他数据集上的准确率相似。<details>
<summary>Abstract</summary>
A major issue in Motor Imagery Brain-Computer Interfaces (MI-BCIs) is their poor classification accuracy and the large amount of data that is required for subject-specific calibration. This makes BCIs less accessible to general users in out-of-the-lab applications. This study employed deep transfer learning for development of calibration-free subject-independent MI-BCI classifiers. Unlike earlier works that applied signal preprocessing and feature engineering steps in transfer learning, this study adopted an end-to-end deep learning approach on raw EEG signals. Three deep learning models (MIN2Net, EEGNet and DeepConvNet) were trained and compared using an openly available dataset. The dataset contained EEG signals from 55 subjects who conducted a left- vs. right-hand motor imagery task. To evaluate the performance of each model, a leave-one-subject-out cross validation was used. The results of the models differed significantly. MIN2Net was not able to differentiate right- vs. left-hand motor imagery of new users, with a median accuracy of 51.7%. The other two models performed better, with median accuracies of 62.5% for EEGNet and 59.2% for DeepConvNet. These accuracies do not reach the required threshold of 70% needed for significant control, however, they are similar to the accuracies of these models when tested on other datasets without transfer learning.
</details>
<details>
<summary>摘要</summary>
一个主要问题在肌动幻象潜意计算机界面（MI-BCI）是其低精度分类和大量的数据需要用于特定用户的卡 Liping。这使得BCI对通用用户在室外应用中 menos  accessible。这项研究使用了深度传输学习开发了无需特定用户 calibration的Ml-BCI分类器。与之前的工作一样，这项研究不对信号预处理和特征工程步骤进行传输学习，而是使用了 Raw EEG 信号的端到端深度学习方法。研究使用了三个深度学习模型（MIN2Net、EEGNet 和 DeepConvNet），并对其进行了比较。数据集包含55名用户完成左手 vs. 右手肌动幻象任务的EEG信号。为评估每个模型的性能，使用了留一个用户之外的交叉验证。结果表明，MIN2Net 无法识别新用户的左手 vs. 右手肌动幻象，具有51.7%的 median 精度。另外两个模型则表现更好， median 精度分别为62.5% 和 59.2%。这些精度没有达到所需的70%的阈值，但与不使用传输学习的其他数据集测试的精度类似。
</details></li>
</ul>
<hr>
<h2 id="Performance-of-Large-Language-Models-in-a-Computer-Science-Degree-Program"><a href="#Performance-of-Large-Language-Models-in-a-Computer-Science-Degree-Program" class="headerlink" title="Performance of Large Language Models in a Computer Science Degree Program"></a>Performance of Large Language Models in a Computer Science Degree Program</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02432">http://arxiv.org/abs/2308.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Krüger, Michael Gref</li>
<li>for: 这个论文的目的是评估不同大型自然语言模型在大学应用科学学士学位课程中的效iveness。</li>
<li>methods: 这个论文使用了不同大型自然语言模型作为教学工具，并通过提示模型 lecture material、运动任务和过去考试来评估它们在不同计算机科学领域的能力。</li>
<li>results: 研究发现，ChatGPT-3.5在10个测试模块中的平均分为79.9%，BingAI为68.4%，LLaMa（65亿参数变量）为20%。尽管这些结果非常有力，但even GPT-4.0不能通过学位课程 - 因为它在数学计算中存在限制。<details>
<summary>Abstract</summary>
Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter variant, 20%. Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.
</details>
<details>
<summary>摘要</summary>
大型语言模型如ChatGPT-3.5和GPT-4.0在当前的讨论中占据主导地位，其转换能力导致了信息处理方式的 paradigm shift。每天，新的可能性 emerge 以利用这些模型的能力。本文介绍了不同大型语言模型在大学应用科学学士学位课程中的表现。我们的主要目标是通过使用这些模型作为教育工具来评估它们在课程中的效iveness。我们在讲义材料、作业任务和过往考试中提问这些模型，以评估它们在不同的计算机科学领域中的技能。我们显示了当前大型语言模型的强大表现，并 highlighted 其中的限制和约束在这种学位课程中。我们发现ChatGPT-3.5在10个测试模块中的平均分为79.9%，BingAI为68.4%，LLaMa（65亿参数变量）为20%。尽管这些结果非常吸引人，但even GPT-4.0不能通过这种学位课程 - 因为它们在数学计算中的限制。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks"><a href="#Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks" class="headerlink" title="Maximal Independent Sets for Pooling in Graph Neural Networks"></a>Maximal Independent Sets for Pooling in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13011">http://arxiv.org/abs/2307.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stevan Stanovic, Benoit Gaüzère, Luc Brun</li>
<li>for: 本文为了解决图像Pooling问题，提出了三种基于最大独立集的Pooling方法，以避免传统图像Pooling方法的缺点。</li>
<li>methods: 本文使用了三种基于最大独立集的Pooling方法，即Maximal Independent Set Pooling（MISP）、Maximal Independent Set Aggregation（MISA）和Maximal Independent Set Based Pooling（MIBP）。</li>
<li>results: 实验结果表明，这三种Pooling方法能够减少图像的缺失和杂乱，并且能够保持图像的连通性。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNNs)  hanno permesso di grandi avanzamenti nella classificazione di immagini attraverso la convoluzione e la pooling. In particolare, la pooling di immagini trasforma una rete discreta connesse in una rete ridotta con la stessa connessione e consente alle funzioni di riduzione di considerare tutti i pixel dell'immagine. Tuttavia, non esiste pooling per grafi che soddisfi queste proprietà. Infatti, i metodi di pooling tradizionali per grafi soffrono almeno uno dei seguenti difetti: disconnessione del grafico o eccessiva connessione, bassa riduzione del numero di node e cancellazione di grandi porzioni del grafico. In questo paper, presentiamo tre metodi di pooling basati sulla nozione di set indipendenti massimi che evitano questi inconvenienti. I nostri risultati sperimentali confermano la rilevanza delle restrizioni di set indipendenti massimi per il pooling di grafi.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine"><a href="#Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine" class="headerlink" title="Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine"></a>Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12788">http://arxiv.org/abs/2307.12788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominique Geissler, Stefan Feuerriegel</li>
<li>for: This study aims to understand the strategy behind the pro-Russian propaganda campaign on social media during the 2022 Russian invasion of Ukraine.</li>
<li>methods: The study uses an inverse reinforcement learning (IRL) approach to model online behavior as a Markov decision process and infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion.</li>
<li>results: The study finds that bots and humans follow different strategies in responding to pro-Russian propaganda. Bots primarily respond to pro-invasion messages, suggesting they seek to drive virality, while messages indicating opposition primarily elicit responses from humans, suggesting they tend to engage in critical discussions.<details>
<summary>Abstract</summary>
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details>
<details>
<summary>摘要</summary>
俄罗斯入侵乌克兰的2022年社交媒体宣传活动中，涉及到了大规模的俄罗斯支持者在社交媒体上的宣传活动。然而，这些宣传活动的战略具体如何实施，特别是如何通过在用户之间的互动来推动俄罗斯宣传的普及，这些问题仍未得到了清楚的回答。在这里，我们使用 inverse reinforcement learning（IRL）方法来分析推特社区的宣传策略。具体来说，IRL方法允许我们将在线行为模型为Markov决策过程，并且目的是从推特用户的支持或反对姿态来推断宣传者在互动中的奖励结构。因此，我们可以理解propagandists在互动中是如何用between-user互动来推动俄罗斯宣传的。为此，我们利用了349,455条推特帖子和132,131名用户的大规模数据集。我们发现，机器人和人类使用者采取了不同的策略：机器人尽量回应支持入侵的消息，表明它们想要驱动病毒性；而反对消息主要引起人类用户的回应，表明人类用户更倾向于进行批评讨论。到目前为止，这是我们分析2022年俄罗斯入侵乌克兰的宣传策略的第一项研究，通过IRL镜像来分析。
</details></li>
</ul>
<hr>
<h2 id="Is-attention-all-you-need-in-medical-image-analysis-A-review"><a href="#Is-attention-all-you-need-in-medical-image-analysis-A-review" class="headerlink" title="Is attention all you need in medical image analysis? A review"></a>Is attention all you need in medical image analysis? A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12775">http://arxiv.org/abs/2307.12775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Papanastasiou, Nikolaos Dikaios, Jiahao Huang, Chengjia Wang, Guang Yang</li>
<li>for: 这篇论文旨在概述现有的hybrid CNN-Transf&#x2F;Attention模型，以及对这些模型的架构设计、突破点和应用前景。</li>
<li>methods: 这篇论文使用了系统性的文献综述方法，对 hybrid CNN-Transf&#x2F;Attention模型进行了架构分析和综述，并提出了一种基于数据驱动的领域泛化和适应方法。</li>
<li>results: 论文发现了hybrid CNN-Transf&#x2F;Attention模型在医学图像分析领域的应用前景，并提出了一种基于数据驱动的领域泛化和适应方法，可以优化模型的泛化能力和适应能力。<details>
<summary>Abstract</summary>
Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.
</details>
<details>
<summary>摘要</summary>
医疗影像是诊断、治疗规划和临床试验设计中的关键组成部分，占全部医疗数据的近90%。过去几年，Convolutional Neural Networks（CNN）在医疗影像分析（MIA）中获得了性能提升。CNN可以有效地模型图像中的局部像素互动，并可以在小规模的MI数据上进行训练。然而，典型的CNN模型却忽略图像中的全局像素关系，这限制了它们对不同'全球'信息的总体化能力。随着人工智能的发展，Transformers（Transformers）产生了，它可以从数据中学习全球关系。然而，全Transformers模型需要大规模的训练和巨大的计算复杂度。Attention和Transformers组件（Transf/Attention），可以保持模型全球关系的性能，被提议为轻量级的Alternative。最近，有一个增长的趋势，将Complementary local-global properties（CLGP）从CNN和Transf/Attention架构中搬运到新的混合模型中。过去几年，混合CNN-Transf/Attention模型在多种MIA问题上表现出了明显的增长。在这个系统性评论中，我们对现有的混合CNN-Transf/Attention模型进行了评论和探讨，分析了重要的架构设计、突破性和现有和未来的机遇和挑战。此外，我们还提出了一种基于总结机会的科学和临床影响分析框架，可以驱动新的数据驱动领域总结和适应方法。
</details></li>
</ul>
<hr>
<h2 id="Adaptation-of-Whisper-models-to-child-speech-recognition"><a href="#Adaptation-of-Whisper-models-to-child-speech-recognition" class="headerlink" title="Adaptation of Whisper models to child speech recognition"></a>Adaptation of Whisper models to child speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13008">http://arxiv.org/abs/2307.13008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c3imaging/whisper_child_speech">https://github.com/c3imaging/whisper_child_speech</a></li>
<li>paper_authors: Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, Horia Cucu</li>
<li>for: 提高儿童语音识别（ASR）系统对儿童语音的识别精度</li>
<li>methods: 使用现有的大量成人语音数据集来适应儿童语音，并对Whisper模型进行finetuning和自动监督学习</li>
<li>results: 在儿童语音上，使用finetuning Whisper模型和自动监督学习的wav2vec2模型可以获得显著改善的ASR性能，相比非finetuning Whisper模型<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.
</details>
<details>
<summary>摘要</summary>
自动话语识别（ASR）系统经常因缺乏儿童语音数据而困难地识别儿童语音。然而，存在巨量的注解的成人语音数据，这些数据被用来创建多语言的ASR模型，如呼叫。我们的工作是探索是否可以将这些模型适应儿童语音，以提高ASR的性能。此外，我们还比较了使用自动注解的wav2vec2模型和Whisper模型。我们的结果显示，对儿童语音进行finetuning可以大幅提高ASR性能，相比非finetuning Whisper模型。此外，使用自动注解的wav2vec2模型，经过finetuning在儿童语音上表现更好，超越了Whisper finetuning。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation"><a href="#Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation" class="headerlink" title="Nonparametric Linear Feature Learning in Regression Through Regularisation"></a>Nonparametric Linear Feature Learning in Regression Through Regularisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12754">http://arxiv.org/abs/2307.12754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bertillefollain/regfeal">https://github.com/bertillefollain/regfeal</a></li>
<li>paper_authors: Bertille Follain, Umut Simsekli, Francis Bach</li>
<li>for: 这个论文主要针对高维数据的自动特征选择问题，具体来说是多指标模型中的适用学习问题。</li>
<li>methods: 该论文提出了一种新的非 Parametric 方法，可以同时估计预测函数和相关的直方几何空间。该方法使用了Empirical risk minimization，并添加了函数导数的约束，以保证方法的多元性。</li>
<li>results: 该论文提供了一些实验结果，证明了RegFeaL 可以在不同的实际场景中表现出色，并且可以准确地估计相关维度。<details>
<summary>Abstract</summary>
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with leading directions and accurately estimate the relevant dimension in practical settings. We establish that our method yields a consistent estimator of the prediction function with explicit rates. Additionally, we provide empirical results demonstrating the performance of RegFeaL in various experiments.
</details>
<details>
<summary>摘要</summary>
《学习表示在自动选择特征中扮演重要角色，特别是在高维数据的情况下，非 Parametric 方法经常陷入困难。在这项研究中，我们关注supervised learning情况下，关键信息都集中在数据中的一个lower-dimensional linear subspace，即多index模型。如果这个subspace已知，它会大大提高预测、计算和解释。为解决这个挑战，我们提出了一种新的方法 для线性特征学习，同时估算预测函数和linear subspace。我们的方法使用empirical risk minimization，并添加了函数导数的罚因，以确保多样化。利用 Hermite  polynomials的正交性和旋转不变性，我们引入了我们的估计器，名为RegFeaL。通过使用alternative minimization，我们可以逐步旋转数据，以便更好地与主导方向相align并准确地估算实际情况中的相关维度。我们证明了我们的方法可以生成一个consistent的预测函数估计器，并且提供了explicit rates。此外，我们还提供了一系列实验结果，证明RegFeaL的性能。
</details></li>
</ul>
<hr>
<h2 id="Introducing-CALMED-Multimodal-Annotated-Dataset-for-Emotion-Detection-in-Children-with-Autism"><a href="#Introducing-CALMED-Multimodal-Annotated-Dataset-for-Emotion-Detection-in-Children-with-Autism" class="headerlink" title="Introducing CALMED: Multimodal Annotated Dataset for Emotion Detection in Children with Autism"></a>Introducing CALMED: Multimodal Annotated Dataset for Emotion Detection in Children with Autism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13706">http://arxiv.org/abs/2307.13706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Annanda Sousa, Karen Young, Mathieu D’aquin, Manel Zarrouk, Jennifer Holloway</li>
<li>for: 这个论文的目的是提高人际交流的自动情感识别系统，以提供个性化的用户体验。</li>
<li>methods: 本论文使用了多种数据收集和处理技术，包括录音和视频特征提取和分类。</li>
<li>results: 本论文描述了一个基于多Modal的情感识别数据集，包括8-12岁的儿童患有Autism Spectrum Disorder（ASD）的记录例子。该数据集包括4个target类别的注解，共计57,012个示例，每个示例代表200ms（0.2秒）的时间窗口。<details>
<summary>Abstract</summary>
Automatic Emotion Detection (ED) aims to build systems to identify users' emotions automatically. This field has the potential to enhance HCI, creating an individualised experience for the user. However, ED systems tend to perform poorly on people with Autism Spectrum Disorder (ASD). Hence, the need to create ED systems tailored to how people with autism express emotions. Previous works have created ED systems tailored for children with ASD but did not share the resulting dataset. Sharing annotated datasets is essential to enable the development of more advanced computer models for ED within the research community. In this paper, we describe our experience establishing a process to create a multimodal annotated dataset featuring children with a level 1 diagnosis of autism. In addition, we introduce CALMED (Children, Autism, Multimodal, Emotion, Detection), the resulting multimodal emotion detection dataset featuring children with autism aged 8-12. CALMED includes audio and video features extracted from recording files of study sessions with participants, together with annotations provided by their parents into four target classes. The generated dataset includes a total of 57,012 examples, with each example representing a time window of 200ms (0.2s). Our experience and methods described here, together with the dataset shared, aim to contribute to future research applications of affective computing in ASD, which has the potential to create systems to improve the lives of people with ASD.
</details>
<details>
<summary>摘要</summary>
自动情感检测（ED）目标是建立自动识别用户情感的系统。这个领域有可能提高人机交互（HCI），创造个性化的用户体验。然而，ED系统通常在Autism Spectrum Disorder（ASD）人群表现不佳。因此，需要开发特化于人们表达情感的ED系统。先前的工作已经创建了特化于儿童ASD的ED系统，但没有公布数据集。分享标注数据集是开发更先进的计算机模型的关键。在这篇论文中，我们描述了我们在建立一个多模态注释数据集的过程中的经验。此外，我们还介绍了CALMED（儿童Autism、多模态、情感检测）数据集，这是8-12岁的儿童ASD的多模态情感检测数据集。CALMED包括录音和视频特征，从参与者录制的文件中提取，以及由参与者的父母提供的四个目标类别的注释。生成的数据集包括57,012个示例，每个示例表示200毫秒（0.2秒）的时间窗口。我们的经验和方法描述以及分享的数据，希望能为ASD的情感计算机科学研究提供贡献，以创造用于改善ASD人群生活质量的系统。
</details></li>
</ul>
<hr>
<h2 id="MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features"><a href="#MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features" class="headerlink" title="MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features"></a>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12698">http://arxiv.org/abs/2307.12698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Bardes, Jean Ponce, Yann LeCun</li>
<li>for: 学习视觉表示，强调了学习内容特征，而不是捕捉物体运动或位置信息。</li>
<li>methods: 我们提出了 MC-JEPA 方法，即共同嵌入预测架构和自动学习方法，以同时学习光流和内容特征，并证明了这两个关联目标互助彼此，从而学习了包含运动信息的内容特征。</li>
<li>results: 我们的方法可以与现有的无监督光流标准做比较，以及与常见的自动学习方法在图像和视频 semantic segmentation 任务上表现相当。<details>
<summary>Abstract</summary>
Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.
</details>
<details>
<summary>摘要</summary>
自领导学习的视觉表示学习把注意力集中在学习内容特征上，这些特征不包括物体运动或位置信息，而是通过识别和区分图像和视频中的 объек 来学习。然而，流体计算是一个不需要理解图像内容的任务。我们将这两种方法联合起来，并引入 MC-JEPA，一种共享编码器中的联合预测建筑和自领导学习方法，以便同时学习流体计算和内容特征。我们发现这两个关联的目标，即流体计算目标和自领导学习目标，互相帮助和学习，因此学习的内容特征包含运动信息。我们的方法可以与现有的无监督光流标准准确比较，以及常见的自领导学习方法在图像和视频 semantic 分割任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks"><a href="#Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks" class="headerlink" title="Addressing the Impact of Localized Training Data in Graph Neural Networks"></a>Addressing the Impact of Localized Training Data in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12689">http://arxiv.org/abs/2307.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akanshaaga/reg_appnp">https://github.com/akanshaaga/reg_appnp</a></li>
<li>paper_authors: Singh Akansha</li>
<li>for: 本文旨在评估 Graph Neural Networks (GNNs) 在不同区域的图数据上的性能，以及如何在受限的训练数据情况下提高 GNN 的适应性和泛化能力。</li>
<li>methods: 本文提出了一种基于 distribution alignment 的正则化方法，用于解决 GNN 在本地化训练数据上的性能下降问题。</li>
<li>results: 经过广泛测试， Results 表明该正则化方法可以有效地提高 GNN 在异常数据上的性能，并且可以帮助 GNN 更好地适应和泛化到不同的图数据上。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details>
<details>
<summary>摘要</summary>
граф neural networks (GNNs) 已经取得了很大的成功，它们可以从图структуре数据中学习，因为它们可以捕捉图中节点之间的复杂关系和依赖关系。它们在半supervised node classification、链接预测和图生成等应用中表现出色。然而，我们需要注意的是，大多数当前的状态部署GNN模型是基于图结构的内部分布式Setting中建模，这会限制它们在实际图中的性能。在这篇文章中，我们试图评估在本地化Subset中训练GNN模型的影响。这种局部训练数据可能会导致模型在特定区域中表现出色，但是它无法总结整个图中的准确预测。在图基于半supervised learning (SSL) 中，资源约束常常导致数据集很大，但只有一部分可以被标记，这会影响模型的性能。这种限制会对任务 like anomaly detection 或 spam detection 产生影响，因为标记过程可能受到人类主观性的影响。为了解决本地化训练数据所带来的挑战，我们将这种问题看作一个out-of-distribution (OOD) 数据问题，并通过对本地化训练数据和图推理过程之间的分布匹配来解决问题。我们提出一种Regularization方法，以降低本地化训练数据和图推理之间的分布差异，从而提高OOD数据上的模型表现。我们在popular GNN模型上进行了广泛的测试，并发现这种Regularization方法可以在三个文献GNN benchmark数据集上显著提高表现。这种Regularization方法可以增强模型的适应和泛化能力，从而超越OOD数据的挑战。
</details></li>
</ul>
<hr>
<h2 id="IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models"><a href="#IteraTTA-An-interface-for-exploring-both-text-prompts-and-audio-priors-in-generating-music-with-text-to-audio-models" class="headerlink" title="IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models"></a>IteraTTA: An interface for exploring both text prompts and audio priors in generating music with text-to-audio models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13005">http://arxiv.org/abs/2307.13005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hiromu Yakura, Masataka Goto</li>
<li>for: 帮助用户自由生成音乐音频，不需具备音乐知识，通过不同文本提示和音频先导来探索音频生成空间。</li>
<li>methods: 使用文本-到-音频生成技术，并提供用户可以进行双重探索（文本提示和音频先导），以便理解不同文本提示和音频先导对生成结果的影响，并逐渐实现用户的模糊化目标。</li>
<li>results: 通过提供特定的音频先导和文本提示，用户可以逐渐理解和探索音频生成空间，并通过反复比较不同的文本提示和音频先导来了解它们对生成结果的影响。<details>
<summary>Abstract</summary>
Recent text-to-audio generation techniques have the potential to allow novice users to freely generate music audio. Even if they do not have musical knowledge, such as about chord progressions and instruments, users can try various text prompts to generate audio. However, compared to the image domain, gaining a clear understanding of the space of possible music audios is difficult because users cannot listen to the variations of the generated audios simultaneously. We therefore facilitate users in exploring not only text prompts but also audio priors that constrain the text-to-audio music generation process. This dual-sided exploration enables users to discern the impact of different text prompts and audio priors on the generation results through iterative comparison of them. Our developed interface, IteraTTA, is specifically designed to aid users in refining text prompts and selecting favorable audio priors from the generated audios. With this, users can progressively reach their loosely-specified goals while understanding and exploring the space of possible results. Our implementation and discussions highlight design considerations that are specifically required for text-to-audio models and how interaction techniques can contribute to their effectiveness.
</details>
<details>
<summary>摘要</summary>
现代文本到音频生成技术具有让新手无需 musical knowledge 可以自由生成音乐音频的潜力。用户可以通过不同的文本提示来尝试生成音频，而不需要了解和器械进程和和谐进程。然而，与图像领域相比，了解音乐频谱中可能的音频空间是Difficult的，因为用户无法同时听到生成的音频变化。我们因此为用户提供了探索不仅文本提示而且受限于文本到音频生成过程的音频先例的机会。这种双重探索使得用户可以通过相互比较不同的文本提示和音频先例来了解它们对生成结果的影响。我们开发的界面IteraTTA特别设计为帮助用户精细调整文本提示和选择生成音频中有利的先例。通过这种方式，用户可以逐步实现自己的模糊化目标，同时了解和探索可能的结果空间。我们的实现和讨论探讨了特定于文本到音频模型的设计考虑和交互技术如何提高其效果。
</details></li>
</ul>
<hr>
<h2 id="Control-and-Monitoring-of-Artificial-Intelligence-Algorithms"><a href="#Control-and-Monitoring-of-Artificial-Intelligence-Algorithms" class="headerlink" title="Control and Monitoring of Artificial Intelligence Algorithms"></a>Control and Monitoring of Artificial Intelligence Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13705">http://arxiv.org/abs/2307.13705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Carlos Mario Braga Ortuño, Blanza Martinez Donoso, Belén Muñiz Villanueva</li>
<li>for: 本研究阐述了在训练完成后，如何监督人工智能模型的运行，并处理可能出现的数据分布的变化。</li>
<li>methods: 本研究介绍了一些用于评估模型表现的指标，以及适当的数据分布基础。</li>
<li>results: 研究发现，监督模型的运行可以帮助检测和处理数据分布的变化，并且可以提高模型的表现。<details>
<summary>Abstract</summary>
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
</details>
<details>
<summary>摘要</summary>
Translation Notes:* "post-deployment" is translated as "后部署" (hòu bù zhì)* "potential fluctuations" is translated as "潜在的波动" (pán zài de bō dòng)* "data drift" is translated as "数据漂移" (shù jí qiáo yí)* "concept drift" is translated as "概念漂移" (gài yán qiáo yí)* "foundational distributions" is translated as "基础分布" (jī zhì fēn zhòu)* "scrutinize" is translated as "检查" (jiǎn chá)* "concerning potential temporal variations" is translated as "关于潜在的时间变化" (guān yù pán zài de shí huan bìng xiàng)
</details></li>
</ul>
<hr>
<h2 id="Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG"><a href="#Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG" class="headerlink" title="Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG"></a>Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12644">http://arxiv.org/abs/2307.12644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remotebiosensing/rppg">https://github.com/remotebiosensing/rppg</a></li>
<li>paper_authors: Dae-Yeol Kim, Eunsu Goh, KwangKee Lee, JongEui Chae, JongHyeon Mun, Junyeong Na, Chae-bong Sohn, Do-Yup Kim</li>
<li>For: This paper aims to provide a benchmarking framework for evaluating the performance of remote photoplethysmography (rPPG) techniques across a wide range of datasets, to facilitate fair comparison and progress in the field.* Methods: The paper uses a variety of datasets and benchmarking metrics to evaluate the performance of both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods for rPPG.* Results: The paper provides a comprehensive evaluation of the performance of different rPPG techniques on a wide range of datasets, and highlights the need for fair and evaluable benchmarking to overcome challenges in the field and make meaningful progress.<details>
<summary>Abstract</summary>
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overcome these challenges and make meaningful progress from both academic and commercial perspectives. In most existing work, models are trained, tested, and validated only on limited datasets. Even worse, some studies lack available code or reproducibility, making it difficult to fairly evaluate and compare performance. Therefore, the purpose of this study is to provide a benchmarking framework to evaluate various rPPG techniques across a wide range of datasets for fair evaluation and comparison, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods. GitHub URL: https://github.com/remotebiosensing/rppg
</details>
<details>
<summary>摘要</summary>
remote 血液氧测技术 (rPPG) 是一种利用摄像机捕捉到血液中内氧滤过特性，并分析血液量脉搏 (BVP) 的技术。通过分析测量的 BVP，可以 derivate 多种生理信号，如心率、压力和压力等，这些信号可以应用于多个应用，如远程医疗、远程患者监控和早期心血管疾病预后评估。 rPPG 在学术和业界中受到广泛关注，因为它提供了很好的使用性和便利性，可以通过摄像机设备而不需要医疗器械或戴式设备来量测生物信号。然而，这个领域仍然面临许多挑战，包括皮肤颜色、摄像机特性、环境照明和其他干扰和错误的问题，这些问题会影响性能。我们认为，对于这些挑战的公平和评估是非常重要的，以实现学术和商业上的进步。大多数现有的工作都是将模型训练、测试和验证在有限的数据集上。甚至更糟糕，一些研究缺乏可用的代码或重现性，使得公平评估和比较性能的问题更加困难。因此，本研究的目的是提供一个 benchmarking 框架，以评估不同的 rPPG 技术在广泛的数据集上的性能，以便公平评估和比较，包括非深度神经网络 (non-DNN) 和深度神经网络 (DNN) 方法。GitHub URL: <https://github.com/remotebiosensing/rppg>
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Human-like-Multi-Modal-Reasoning-A-New-Challenging-Dataset-and-Comprehensive-Framework"><a href="#Enhancing-Human-like-Multi-Modal-Reasoning-A-New-Challenging-Dataset-and-Comprehensive-Framework" class="headerlink" title="Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework"></a>Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12626">http://arxiv.org/abs/2307.12626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingxuan Wei, Cheng Tan, Zhangyang Gao, Linzhuang Sun, Siyuan Li, Bihui Yu, Ruifeng Guo, Stan Z. Li</li>
<li>for:  This paper aims to address the lack of comprehensive evaluation of diverse approaches in multimodal scientific question answering, by presenting a novel dataset (COCO Multi-Modal Reasoning Dataset) that includes open-ended questions, rationales, and answers derived from the large object dataset COCO.</li>
<li>methods:  The proposed dataset pioneers the use of open-ended questions in the context of multimodal chain-of-thought, which introduces a more challenging problem that effectively assesses the reasoning capability of CoT models. The authors propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders.</li>
<li>results:  Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning. The proposed methods and dataset provide valuable insights and offer a more challenging problem for advancing the field of multimodal reasoning.<details>
<summary>Abstract</summary>
Multimodal reasoning is a critical component in the pursuit of artificial intelligence systems that exhibit human-like intelligence, especially when tackling complex tasks. While the chain-of-thought (CoT) technique has gained considerable attention, the existing ScienceQA dataset, which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning Dataset(COCO-MMRD), a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide valuable insights and propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders. Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning.
</details>
<details>
<summary>摘要</summary>
多Modal重要组成部分在人工智能系统具有人类智能的追求中，尤其是在较复杂的任务上。而链式思考（CoT）技术已经受到了广泛关注，但现有的科学QA数据集（ScienceQA），which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning Dataset(COCO-MMRD), a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide valuable insights and propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders. Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning.Here is the word-for-word translation of the text into Simplified Chinese:多Modal重要组成部分在人工智能系统具有人类智能的追求中，尤其是在较复杂的任务上。而链式思考（CoT）技术已经受到了广泛关注，但现有的科学QA数据集（ScienceQA），which focuses on multimodal scientific questions and explanations from elementary and high school textbooks, lacks a comprehensive evaluation of diverse approaches. To address this gap, we present COCO Multi-Modal Reasoning Dataset(COCO-MMRD), a novel dataset that encompasses an extensive collection of open-ended questions, rationales, and answers derived from the large object dataset COCO. Unlike previous datasets that rely on multiple-choice questions, our dataset pioneers the use of open-ended questions in the context of multimodal CoT, introducing a more challenging problem that effectively assesses the reasoning capability of CoT models. Through comprehensive evaluations and detailed analyses, we provide valuable insights and propose innovative techniques, including multi-hop cross-modal attention and sentence-level contrastive learning, to enhance the image and text encoders. Extensive experiments demonstrate the efficacy of the proposed dataset and techniques, offering novel perspectives for advancing multimodal reasoning.
</details></li>
</ul>
<hr>
<h2 id="De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network"><a href="#De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network" class="headerlink" title="De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network"></a>De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12625">http://arxiv.org/abs/2307.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Haolong Zeng, Yun Pen, Huiyan Sun</li>
<li>for: 这种论文主要针对的是如何对连续型干预变量进行Counterfactual推断，而现实世界中更常见的是连续型干预变量的Counterfactual推断任务。</li>
<li>methods: 我们提出了一种基于De-confounding Representation Learning（DRL）的框架，通过生成与干预变量分离的covariate表示来消除干预变量与covariate之间的相关性。DRL是一种非 Parametric 模型，可以消除连续型干预变量与covariate之间的线性和非线性相关性。</li>
<li>results: 在 synthetic 数据集上进行了广泛的实验，发现 DRL 模型在学习分离表示的同时，也可以超越当前Counterfactual推断模型的性能。此外，我们还应用了 DRL 模型到一个实际的医疗数据集 MIMIC，并显示出了连续型红细胞宽度分布和死亡率之间的详细 causal 关系。<details>
<summary>Abstract</summary>
Counterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.
</details>
<details>
<summary>摘要</summary>
<SYS> translate-into-simplified-chineseCounterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.</SYS>Here's the text in Simplified Chinese:<SYS>对于连续型干预变量，Counterfactual inference更常用于实际世界 causal inference 任务中。虽然现有一些基于Marginal Structural Model的样本重新权重方法可以消除干预的相关性，但这些方法通常只是消除干预对隐藏变量的直线相关性，并且它们的假设模型通常是不可验证的。在这篇论文中，我们提出了一种基于 representation learning 的 de-confounding 框架，用于计算连续型干预下的可靠的结果。该框架使用非 Parametric 模型来消除干预和隐藏变量之间的线性和非线性相关性。具体来说，我们在 DRL 框架中训练了对干预变量和隐藏变量之间的相关性，并与隐藏变量和干预变量之间的相关性进行对比，以消除干预偏见。此外，我们还在框架中嵌入了一个 counterfactual inference 网络，以使得学习的表示可以服务于 both de-confounding 和可靠的推理。在 synthetic 数据集上进行了广泛的实验，显示 DRL 模型在学习 de-confounding 表示方面表现出色，并且超越了现有的 state-of-the-art counterfactual inference 模型。此外，我们还应用了 DRL 模型到了一个实际医疗数据集 MIMIC，并通过示出红细胞宽度分布和死亡之间的详细 causal 关系，以证明 DRL 模型的效果。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Past-present-temporal-programs-over-finite-traces"><a href="#Past-present-temporal-programs-over-finite-traces" class="headerlink" title="Past-present temporal programs over finite traces"></a>Past-present temporal programs over finite traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12620">http://arxiv.org/abs/2307.12620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pedro Cabalar, Martín Diéguez, François Laferrière, Torsten Schaub</li>
<li>for: 这个论文旨在探讨逻辑编程的扩展，使用时间逻辑的语言结构来模型动态应用。</li>
<li>methods: 这篇论文使用了TELf语义，对过去和当前的逻辑编程规则进行研究，并将过去和当前分为不同的语义级别。</li>
<li>results: 论文提出了一种基于LTLf表达式的完成和循环式表达式的定义，以捕捉过去和当前 temporal 稳定模型。<details>
<summary>Abstract</summary>
Extensions of Answer Set Programming with language constructs from temporal logics, such as temporal equilibrium logic over finite traces (TELf), provide an expressive computational framework for modeling dynamic applications. In this paper, we study the so-called past-present syntactic subclass, which consists of a set of logic programming rules whose body references to the past and head to the present. Such restriction ensures that the past remains independent of the future, which is the case in most dynamic domains. We extend the definitions of completion and loop formulas to the case of past-present formulas, which allows capturing the temporal stable models of a set of past-present temporal programs by means of an LTLf expression.
</details>
<details>
<summary>摘要</summary>
扩展回答集编程的语言结构，如基于时间逻辑的时间平衡逻辑（TELf），提供了一种表达强大的计算框架，用于模型动态应用。本文研究一种叫做过去今天的 sintactic subclass，它包含一组逻辑编程规则，其体中参考过去，头部参考当前。这种限制保证了过去不会受到未来的影响，这是动态领域中的典型情况。我们将完成和循环式的定义扩展到过去今天的表达中，以便通过 LTLf 表达捕捉过去今天的时间稳定模型。
</details></li>
</ul>
<hr>
<h2 id="CTVIS-Consistent-Training-for-Online-Video-Instance-Segmentation"><a href="#CTVIS-Consistent-Training-for-Online-Video-Instance-Segmentation" class="headerlink" title="CTVIS: Consistent Training for Online Video Instance Segmentation"></a>CTVIS: Consistent Training for Online Video Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12616">http://arxiv.org/abs/2307.12616</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kainingying/ctvis">https://github.com/kainingying/ctvis</a></li>
<li>paper_authors: Kaining Ying, Qing Zhong, Weian Mao, Zhenhua Wang, Hao Chen, Lin Yuanbo Wu, Yifan Liu, Chengxiang Fan, Yunzhi Zhuge, Chunhua Shen</li>
<li>for: 本研究旨在提高在线视频实例分割（VIS）中的实例嵌入差异，以便在不同时刻进行实例关联。</li>
<li>methods: 本研究使用了对比损失来直接监督实例嵌入学习，并使用了一种叫做“一致训练”的简单而有效的训练策略，以提高实例嵌入的可靠性。</li>
<li>results: 实验表明，使用“一致训练”策略可以提高实例嵌入的可靠性，并在三个 VIS 测试套件中提高了 SOTA 模型的性能，包括 YTVIS19（55.1% AP）、YTVIS21（50.1% AP）和 OVIS（35.5% AP）。此外，我们还发现使用 pseudo-video 从图像转换而来的模型可以训练出比 Fully-supervised 模型更加强大的模型。<details>
<summary>Abstract</summary>
The discrimination of instance embeddings plays a vital role in associating instances across time for online video instance segmentation (VIS). Instance embedding learning is directly supervised by the contrastive loss computed upon the contrastive items (CIs), which are sets of anchor/positive/negative embeddings. Recent online VIS methods leverage CIs sourced from one reference frame only, which we argue is insufficient for learning highly discriminative embeddings. Intuitively, a possible strategy to enhance CIs is replicating the inference phase during training. To this end, we propose a simple yet effective training strategy, called Consistent Training for Online VIS (CTVIS), which devotes to aligning the training and inference pipelines in terms of building CIs. Specifically, CTVIS constructs CIs by referring inference the momentum-averaged embedding and the memory bank storage mechanisms, and adding noise to the relevant embeddings. Such an extension allows a reliable comparison between embeddings of current instances and the stable representations of historical instances, thereby conferring an advantage in modeling VIS challenges such as occlusion, re-identification, and deformation. Empirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on three VIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS (35.5% AP). Furthermore, we find that pseudo-videos transformed from images can train robust models surpassing fully-supervised ones.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于在线视频实例分割（VIS）中，实例嵌入的歧视扮演着关键的角色。我们使用对比损失来直接监督实例嵌入学习，其中对比项（CI）是一组锚点/正例/负例嵌入的集合。在线 VIS 方法通常使用一个参考帧中的 CIs，我们认为这是学习高度抽象的嵌入的不够。我们提出了一种简单 yet 有效的训练策略，called Consistent Training for Online VIS（CTVIS），它的目的是在训练和推理管道中对实例嵌入进行对应。具体来说，CTVIS 使用暂停聚合的嵌入和存储机制，并将它们添加到相关的嵌入中。这种扩展使得可以在训练和推理中对实例嵌入进行可靠的比较，从而提高对 occlusion、重复、和变形等挑战的适应能力。实验证明，CTVIS 可以超越当前 SOTA VIS 模型，提高 YTVIS19（55.1% AP）、YTVIS21（50.1% AP）和 OVIS（35.5% AP）等三个 VIS 标准套件中的成绩。此外，我们发现在图像转换成 pseudo-video 后训练的模型可以超过完全监督的模型。>>>
</details></li>
</ul>
<hr>
<h2 id="Regulating-AI-manipulation-Applying-Insights-from-behavioral-economics-and-psychology-to-enhance-the-practicality-of-the-EU-AI-Act"><a href="#Regulating-AI-manipulation-Applying-Insights-from-behavioral-economics-and-psychology-to-enhance-the-practicality-of-the-EU-AI-Act" class="headerlink" title="Regulating AI manipulation: Applying Insights from behavioral economics and psychology to enhance the practicality of the EU AI Act"></a>Regulating AI manipulation: Applying Insights from behavioral economics and psychology to enhance the practicality of the EU AI Act</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02041">http://arxiv.org/abs/2308.02041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huixin Zhong</li>
<li>For: 这篇论文的目的是为了解释和增强欧盟人工智能法规第五条的执行，以防止人工智能操纵的可能有害后果。* Methods: 这篇论文使用了认知心理学和行为经济学的研究来解释潜意识技术和相关表达的概念，并将行为经济学中的决策简化技巧推广到操纵技术的领域。* Results: 这篇论文提出了五种经典的决策简化技巧和其相应的示例，以便用户、开发者、算法审核人和法律专业人员识别操纵技术并采取对策。此外，论文还对欧盟人工智能法规第五条的保护效果进行了批判性评估，并提出了特定的修改建议以增强保护效果。<details>
<summary>Abstract</summary>
The EU AI Act Article 5 is designed to regulate AI manipulation to prevent potential harmful consequences. However, the practical implementation of this legislation is challenging due to the ambiguous terminologies and the unclear presentations of manipulative techniques. Moreover, the Article 5 also suffers criticize of inadequate protective efficacy. This paper attempts to clarify terminologies and to enhance the protective efficacy by integrating insights from psychology and behavioral economics. Firstly, this paper employs cognitive psychology research to elucidate the term subliminal techniques and its associated representation. Additionally, this paper extends the study of heuristics: a set of thinking shortcuts which can be aroused for behavior changing from behavior economics to the realm of manipulative techniques. The elucidation and expansion of terminologies not only provide a more accurate understanding of the legal provision but also enhance its protective efficacy. Secondly, this paper proposes five classical heuristics and their associated examples to illustrate how can AI arouse those heuristics to alter users behavior. The enumeration of heuristics serves as a practical guide for stakeholders such as AI developers, algorithm auditors, users, and legal practitioners, enabling them to identify manipulative techniques and implement countermeasures. Finally, this paper critically evaluates the protective efficacy of Article 5 for both the general public and vulnerable groups. This paper argues that the current protective efficacy of Article 5 is insufficient and thus proposes specific revision suggestions to terms a and b in Article 5 to enhance its protective efficacy. This work contributes to the ongoing discourse on AI ethics and legal regulations, providing a practical guide for interpreting and applying the EU AI Act Article 5.
</details>
<details>
<summary>摘要</summary>
欧盟人工智能法 Article 5 是为了规范人工智能操纵，避免可能的有害后果。然而，实施这一法律的具体方法是困难的，因为涉及的术语抽象，以及操纵技巧的不清楚表述。此外， Article 5 还受到了不充分的保护效果的批评。这篇论文试图使用认知心理学和行为经济学的意识来减轻这些问题。首先，这篇论文使用认知心理学研究来解释“潜意识技巧”的概念，并与其相关的表达相结合。此外，这篇论文将行为经济学中的决策简化技巧（heuristics）扩展到了操纵技巧的领域。通过这种方式，不仅可以提供更加准确的法律规定的理解，还可以提高其保护效果。其次，这篇论文提出五种经典的决策简化技巧和其相应的示例，以示AI如何使用这些技巧来改变用户的行为。这些技巧的列表 serves as a practical guide for stakeholders such as AI developers, algorithm auditors, users, and legal practitioners，allowing them to identify manipulative techniques and implement countermeasures。最后，这篇论文 kritisch evaluates the protective efficacy of Article 5 for both the general public and vulnerable groups。这篇论文 argue that the current protective efficacy of Article 5 is insufficient, and thus proposes specific revision suggestions to terms a and b in Article 5 to enhance its protective efficacy。这篇论文的成果将贡献到欧盟人工智能伦理法规的继续讨论中，并提供了一份实用的指南，用于解读和应用欧盟人工智能法 Article 5。
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-Focus-Attention-for-Efficient-DETR"><a href="#Less-is-More-Focus-Attention-for-Efficient-DETR" class="headerlink" title="Less is More: Focus Attention for Efficient DETR"></a>Less is More: Focus Attention for Efficient DETR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12612">http://arxiv.org/abs/2307.12612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/noah-research">https://github.com/huawei-noah/noah-research</a></li>
<li>paper_authors: Dehua Zheng, Wenhui Dong, Hailin Hu, Xinghao Chen, Yunhe Wang<br>for: 这个论文的目的是提高DETR-like模型的计算效率，同时保持模型的准确率。methods: 这个论文使用了一种名为Focus-DETR的方法，它使用了双重注意 Mechanism来注意更有用的 токен，从而提高计算效率。具体来说，它首先使用了一种名为Token Scoring Mechanism来评估每个 токен的重要性，然后使用了一种名为Enhanced Semantic Interaction Mechanism来提高对象的 semantic interaction。results:  Comparing with state-of-the-art sparse DETR-like detectors under the same setting, Focus-DETR achieves 50.4AP (+2.2) on COCO, with comparable complexity.<details>
<summary>Abstract</summary>
DETR-like models have significantly boosted the performance of detectors and even outperformed classical convolutional models. However, all tokens are treated equally without discrimination brings a redundant computational burden in the traditional encoder structure. The recent sparsification strategies exploit a subset of informative tokens to reduce attention complexity maintaining performance through the sparse encoder. But these methods tend to rely on unreliable model statistics. Moreover, simply reducing the token population hinders the detection performance to a large extent, limiting the application of these sparse models. We propose Focus-DETR, which focuses attention on more informative tokens for a better trade-off between computation efficiency and model accuracy. Specifically, we reconstruct the encoder with dual attention, which includes a token scoring mechanism that considers both localization and category semantic information of the objects from multi-scale feature maps. We efficiently abandon the background queries and enhance the semantic interaction of the fine-grained object queries based on the scores. Compared with the state-of-the-art sparse DETR-like detectors under the same setting, our Focus-DETR gets comparable complexity while achieving 50.4AP (+2.2) on COCO. The code is available at https://github.com/huawei-noah/noah-research/tree/master/Focus-DETR and https://gitee.com/mindspore/models/tree/master/research/cv/Focus-DETR.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SL-Stable-Learning-in-Source-Free-Domain-Adaption-for-Medical-Image-Segmentation"><a href="#SL-Stable-Learning-in-Source-Free-Domain-Adaption-for-Medical-Image-Segmentation" class="headerlink" title="SL: Stable Learning in Source-Free Domain Adaption for Medical Image Segmentation"></a>SL: Stable Learning in Source-Free Domain Adaption for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12580">http://arxiv.org/abs/2307.12580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Chen, Yan Wang</li>
<li>for: 这篇论文是针对医疗影像分析中的深度学习技术，尤其是面临域名变化问题。</li>
<li>methods: 本研究提出了一个名为“稳定学习”（Stable Learning）策略，用于解决“长期训练，差化表现”的问题。这个策略包括对预测重量进行整合和增加熵。</li>
<li>results: 比较实验显示了这个策略的有效性。此外，研究者还进行了广泛的剥离实验，以评估不同方法之间的比较。<details>
<summary>Abstract</summary>
Deep learning techniques for medical image analysis usually suffer from the domain shift between source and target data. Most existing works focus on unsupervised domain adaptation (UDA). However, in practical applications, privacy issues are much more severe. For example, the data of different hospitals have domain shifts due to equipment problems, and data of the two domains cannot be available simultaneously because of privacy. In this challenge defined as Source-Free UDA, the previous UDA medical methods are limited. Although a variety of medical source-free unsupervised domain adaption (MSFUDA) methods have been proposed, we found they fall into an over-fitting dilemma called "longer training, worse performance." Therefore, we propose the Stable Learning (SL) strategy to address the dilemma. SL is a scalable method and can be integrated with other research, which consists of Weight Consolidation and Entropy Increase. First, we apply Weight Consolidation to retain domain-invariant knowledge and then we design Entropy Increase to avoid over-learning. Comparative experiments prove the effectiveness of SL. We also have done extensive ablation experiments. Besides, We will release codes including a variety of MSFUDA methods.
</details>
<details>
<summary>摘要</summary>
深度学习技术 для医疗影像分析通常受到源数据和目标数据之间的领域转移的影响。大多数现有的工作集中在无监督领域适应（UDA）。然而，在实际应用中，隐私问题更加严重。例如，医院数据之间存在领域转移，因为设备问题，两个领域的数据无法同时可用。这种挑战被称为源无法适应（Source-Free UDA），前一代的UDA医疗方法有限。虽然一些医疗源无法无监督领域适应（MSFUDA）方法已经被提出，但我们发现它们受到“长期训练，性能下降”的困惑。因此，我们提出稳定学习（SL）策略来解决这个困惑。SL是可扩展的方法，可以与其他研究集成，它包括权重团聚和Entropy增加。首先，我们将权重团聚应用于保留领域不变知识，然后我们设计Entropy增加以避免过度学习。对比性实验证明SL的效果。此外，我们还进行了广泛的减少实验。此外，我们将发布代码，包括多种MSFUDA方法。
</details></li>
</ul>
<hr>
<h2 id="Continuation-Path-Learning-for-Homotopy-Optimization"><a href="#Continuation-Path-Learning-for-Homotopy-Optimization" class="headerlink" title="Continuation Path Learning for Homotopy Optimization"></a>Continuation Path Learning for Homotopy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12551">http://arxiv.org/abs/2307.12551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xi-l/cpl">https://github.com/xi-l/cpl</a></li>
<li>paper_authors: Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, Qingfu Zhang</li>
<li>for: 解决复杂优化问题，提高homotopy优化的效果和可靠性。</li>
<li>methods: 提出了一种基于模型的方法，可以同时优化原始问题和所有优化子问题，并在实时生成任何中间解决方案。</li>
<li>results: 实验表明，提议的方法可以大幅提高homotopy优化的性能，并提供更多有用信息，以支持更好的决策。<details>
<summary>Abstract</summary>
Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which includes infinite intermediate solutions for any surrogate subproblems. Our method optimizes the original problem and all surrogate subproblems in a collaborative manner, rather than the classic unidirectional easy-to-hard optimization. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications.Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details></li>
</ul>
<hr>
<h2 id="Knapsack-Connectedness-Path-and-Shortest-Path"><a href="#Knapsack-Connectedness-Path-and-Shortest-Path" class="headerlink" title="Knapsack: Connectedness, Path, and Shortest-Path"></a>Knapsack: Connectedness, Path, and Shortest-Path</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12547">http://arxiv.org/abs/2307.12547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Palash Dey, Sudeshna Kolay, Sipra Singh</li>
<li>for: 这个论文研究了带有图理解的背包问题。 specifically, it aims to find a connected subset of items of maximum value that satisfies the knapsack constraint.</li>
<li>methods: 这个论文使用了图论的方法来解决这个问题。 specifically, it shows that the problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs.</li>
<li>results: 这个论文得到了一个时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(\min{s^2,d^2})\right)$ 的算法，以及一个 $(1-\epsilon)$ 因数逼近算法时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(n,1&#x2F;\epsilon)\right)$  для每个 $\epsilon&gt;0$。 Additionally, it shows that connected-knapsack is computationally hardest followed by path-knapsack and shortestpath-knapsack.<details>
<summary>Abstract</summary>
We study the knapsack problem with graph theoretic constraints. That is, we assume that there exists a graph structure on the set of items of knapsack and the solution also needs to satisfy certain graph theoretic properties on top of knapsack constraints. In particular, we need to compute in the connected knapsack problem a connected subset of items which has maximum value subject to the size of knapsack constraint. We show that this problem is strongly NP-complete even for graphs of maximum degree four and NP-complete even for star graphs. On the other hand, we develop an algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$ are respectively treewidth of the graph, size, and target value of the knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for every $\epsilon>0$. We show similar results for several other graph theoretic properties, namely path and shortest-path under the problem names path-knapsack and shortestpath-knapsack. Our results seems to indicate that connected-knapsack is computationally hardest followed by path-knapsack and shortestpath-knapsack.
</details>
<details>
<summary>摘要</summary>
我们研究了带有图 teoretic 约束的零件包问题。具体来说，我们假设存在一个图结构，其中每个零件都有一定的价值，并且解决方案还需要满足一定的图 theoretica 性质。在特定情况下，我们需要计算一个连接的零件集，其中每个零件的价值最大，并且满足零件包的大小约束。我们证明了这个问题是强NP完全的，甚至对于最大度为四的图和星型图。然而，我们开发了一个时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ 的算法，其中 $tw,s,d$ 分别是图的树幂，零件包的大小，和目标值。此外，我们还提出了一个 $(1-\epsilon)$ 因子近似算法，其时间复杂度为 $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$，其中 $\epsilon>0$。我们还证明了对于一些其他的图 teoretic 性质，例如路径和最短路，我们可以通过对应的问题名称来描述它们，例如路径-零件包和最短路-零件包。我们的结果表明，连接-零件包是计算最为困难的，然后是路径-零件包和最短路-零件包。
</details></li>
</ul>
<hr>
<h2 id="Towards-Video-Anomaly-Retrieval-from-Video-Anomaly-Detection-New-Benchmarks-and-Model"><a href="#Towards-Video-Anomaly-Retrieval-from-Video-Anomaly-Detection-New-Benchmarks-and-Model" class="headerlink" title="Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model"></a>Towards Video Anomaly Retrieval from Video Anomaly Detection: New Benchmarks and Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12545">http://arxiv.org/abs/2307.12545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wu, Jing Liu, Xiangteng He, Yuxin Peng, Peng Wang, Yanning Zhang</li>
<li>for: 这个研究的目的是为了提出一个新的错误检测任务，即错误视频搜寻（VAR），这个任务的目的是实用地搜寻错误的视频，并且使用语言描述和同步的声音进行跨Modalities的搜寻。</li>
<li>methods: 这个研究使用了一个名为“错误引导排序”的类别学习模型，它使用了一个新的错误排序方法来将错误的部分对应到视频中的特定时间点。此外，这个模型还使用了一个内建的类别排序方法来将视频中的内容与语言描述进行对应。</li>
<li>results: 实验结果显示，这个新的错误检测任务（VAR）是一个非常有挑战性的任务，并且证明了这个任务的重要性。此外，实验结果还显示了这个模型的优秀性，它可以实现高度的准确率和高效率。<details>
<summary>Abstract</summary>
Video anomaly detection (VAD) has been paid increasing attention due to its potential applications, its current dominant tasks focus on online detecting anomalies% at the frame level, which can be roughly interpreted as the binary or multiple event classification. However, such a setup that builds relationships between complicated anomalous events and single labels, e.g., ``vandalism'', is superficial, since single labels are deficient to characterize anomalous events. In reality, users tend to search a specific video rather than a series of approximate videos. Therefore, retrieving anomalous events using detailed descriptions is practical and positive but few researches focus on this. In this context, we propose a novel task called Video Anomaly Retrieval (VAR), which aims to pragmatically retrieve relevant anomalous videos by cross-modalities, e.g., language descriptions and synchronous audios. Unlike the current video retrieval where videos are assumed to be temporally well-trimmed with short duration, VAR is devised to retrieve long untrimmed videos which may be partially relevant to the given query. To achieve this, we present two large-scale VAR benchmarks, UCFCrime-AR and XDViolence-AR, constructed on top of prevalent anomaly datasets. Meanwhile, we design a model called Anomaly-Led Alignment Network (ALAN) for VAR. In ALAN, we propose an anomaly-led sampling to focus on key segments in long untrimmed videos. Then, we introduce an efficient pretext task to enhance semantic associations between video-text fine-grained representations. Besides, we leverage two complementary alignments to further match cross-modal contents. Experimental results on two benchmarks reveal the challenges of VAR task and also demonstrate the advantages of our tailored method.
</details>
<details>
<summary>摘要</summary>
视频异常检测（VAD）在过去几年中受到了越来越多的关注，因为它在实际应用中具有潜在的价值。目前主流的VAD任务都是在帧级别上进行异常检测，可以简单地 interpreted为多个事件的二分类或多类别分类。但是，这种设置不仅缺乏异常事件之间的关系，而且单个标签（如“违法行为”）无法描述异常事件的复杂性。在实际应用中，用户通常会搜索特定的视频而不是一系列相似的视频。因此，使用详细的描述来检测异常事件是有用的。在这个上下文中，我们提出了一个新的任务：视频异常检索（VAR），该任务的目标是在多modalities（如语言描述和同步声音）之间实用地检索相关的异常视频。不同于现有的视频检索，在VAR中视频不再被假设为短暂和有效的，而是可以是长不trimmed的，这些视频可能只有部分相关于给定查询。为了实现这一目标，我们提出了两个大规模的VAR benchmark，UCFCrime-AR和XDViolence-AR，它们基于常见的异常数据集。同时，我们设计了一种名为异常领导Alignment网络（ALAN）的模型，用于VAR。在ALAN中，我们提出了一种异常领导采样方法，以吸引关注关键部分在长不trimmed视频中。然后，我们引入了一种有效的假任务，以增强视频-文本细致的semantic关系。此外，我们利用了两种补偿匹配来进一步匹配cross-modal内容。实验结果表明，VAR任务具有挑战性，同时我们的定制方法也得到了优势。
</details></li>
</ul>
<hr>
<h2 id="Client-Level-Differential-Privacy-via-Adaptive-Intermediary-in-Federated-Medical-Imaging"><a href="#Client-Level-Differential-Privacy-via-Adaptive-Intermediary-in-Federated-Medical-Imaging" class="headerlink" title="Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging"></a>Client-Level Differential Privacy via Adaptive Intermediary in Federated Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12542">http://arxiv.org/abs/2307.12542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/client-dp-fl">https://github.com/med-air/client-dp-fl</a></li>
<li>paper_authors: Meirui Jiang, Yuan Zhong, Anjie Le, Xiaoxiao Li, Qi Dou</li>
<li>for: This paper aims to optimize the trade-off between privacy protection and performance in federated learning (FL) for medical imaging under the context of client-level differential privacy (DP).</li>
<li>methods: The proposed approach is based on an adaptive intermediary strategy that splits clients into sub-clients, which serve as intermediaries between hospitals and the server to mitigate the noises introduced by DP without harming privacy.</li>
<li>results: The proposed approach is empirically evaluated on both classification and segmentation tasks using two public datasets, and its effectiveness is demonstrated with significant performance improvements and comprehensive analytical studies.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文的目标是在医疗镜像 Federated Learning（FL）中优化 differential privacy（DP）的质量和性能的负担平衡。</li>
<li>methods: 提议的方法是基于 adaptive intermediary strategy，将客户端分成子客户端，这些子客户端将作为客户端和服务器之间的中间人，以减少 DP 引入的噪声而不危害隐私。</li>
<li>results: 提议的方法被 Empirical 评估在 two 个公共数据集上，并通过 comprehensive analytical studies 证明其效果。<details>
<summary>Abstract</summary>
Despite recent progress in enhancing the privacy of federated learning (FL) via differential privacy (DP), the trade-off of DP between privacy protection and performance is still underexplored for real-world medical scenario. In this paper, we propose to optimize the trade-off under the context of client-level DP, which focuses on privacy during communications. However, FL for medical imaging involves typically much fewer participants (hospitals) than other domains (e.g., mobile devices), thus ensuring clients be differentially private is much more challenging. To tackle this problem, we propose an adaptive intermediary strategy to improve performance without harming privacy. Specifically, we theoretically find splitting clients into sub-clients, which serve as intermediaries between hospitals and the server, can mitigate the noises introduced by DP without harming privacy. Our proposed approach is empirically evaluated on both classification and segmentation tasks using two public datasets, and its effectiveness is demonstrated with significant performance improvements and comprehensive analytical studies. Code is available at: https://github.com/med-air/Client-DP-FL.
</details>
<details>
<summary>摘要</summary>
尽管最近的进展已经提高了联合学习（FL）的隐私保护（DP），但是实际世界医疗场景中DP与性能之间的负担仍未得到充分探讨。在这篇论文中，我们提议优化DP与隐私保护之间的负担，在客户端DP上进行优化。然而，医疗影像FL通常有比其他领域（如移动设备）更少的参与者（医院），因此保持客户端的隐私是更加挑战性的。为解决这个问题，我们提议使用可适应担保策略，以提高性能而不危害隐私。具体来说，我们通过将客户端分为子客户端，使其作为医院和服务器之间的中间人，可以减少DP引入的噪声而不危害隐私。我们的提议方法在两个公共数据集上进行了实际评估，并通过了重要性能改进和完整的分析研究。代码可以在以下链接中找到：https://github.com/med-air/Client-DP-FL。
</details></li>
</ul>
<hr>
<h2 id="SelFormaly-Towards-Task-Agnostic-Unified-Anomaly-Detection"><a href="#SelFormaly-Towards-Task-Agnostic-Unified-Anomaly-Detection" class="headerlink" title="SelFormaly: Towards Task-Agnostic Unified Anomaly Detection"></a>SelFormaly: Towards Task-Agnostic Unified Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12540">http://arxiv.org/abs/2307.12540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujin Lee, Harin Lim, Hyunsoo Yoon</li>
<li>for: 这篇论文旨在提出一个通用且强大的问题检测框架，以扩展previous任务特定的问题检测方法。</li>
<li>methods: 这篇论文使用了自我supervised ViTs，以及back-patch masking和top k-ratio feature matching等技术来实现通用的问题检测。</li>
<li>results: 这篇论文在不同的数据集上都 achieved state-of-the-art 的结果，并且适用于多种任务，包括问题检测、Semantic anomaly detection、多类问题检测和问题聚合。<details>
<summary>Abstract</summary>
The core idea of visual anomaly detection is to learn the normality from normal images, but previous works have been developed specifically for certain tasks, leading to fragmentation among various tasks: defect detection, semantic anomaly detection, multi-class anomaly detection, and anomaly clustering. This one-task-one-model approach is resource-intensive and incurs high maintenance costs as the number of tasks increases. This paper presents SelFormaly, a universal and powerful anomaly detection framework. We emphasize the necessity of our off-the-shelf approach by pointing out a suboptimal issue with fluctuating performance in previous online encoder-based methods. In addition, we question the effectiveness of using ConvNets as previously employed in the literature and confirm that self-supervised ViTs are suitable for unified anomaly detection. We introduce back-patch masking and discover the new role of top k-ratio feature matching to achieve unified and powerful anomaly detection. Back-patch masking eliminates irrelevant regions that possibly hinder target-centric detection with representations of the scene layout. The top k-ratio feature matching unifies various anomaly levels and tasks. Finally, SelFormaly achieves state-of-the-art results across various datasets for all the aforementioned tasks.
</details>
<details>
<summary>摘要</summary>
核心思想是可视异常检测是学习正常图像的 normality，但之前的工作是为特定任务而开发，导致不同任务之间的分化。这种一个任务一个模型的方法是资源占用和维护成本高。这篇论文介绍了 SelFormaly，一个通用和强大的异常检测框架。我们强调我们的卖外方法的必要性，指出了在线编码器基于方法中的性能波动问题。此外，我们质疑了使用 ConvNets 以前在文献中使用的效果，并证明了不同类异常检测和泛化异常检测可以使用自适应 ViTs。我们引入后贴布覆盖和发现了新的顶部 k-比例特征匹配，以实现统一和强大的异常检测。后贴布覆盖 eliminates 不相关的区域，可能干扰目标中心检测表示场景布局。顶部 k-比例特征匹配 统一了不同异常水平和任务。最后，SelFormaly 在不同数据集上实现了所有上述任务的州OF-the-art 结果。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph"><a href="#Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph" class="headerlink" title="Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph"></a>Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12526">http://arxiv.org/abs/2307.12526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyixinxin/mrg-kg">https://github.com/wangyixinxin/mrg-kg</a></li>
<li>paper_authors: Yixin Wang, Zihao Lin, Haoyu Dong</li>
<li>for: 这个研究的目的是提高医疗报告生成（MRG）过程中的知识图（KG）的完整性和应用。</li>
<li>methods: 这个研究使用了一个完整的知识图，包括137种疾病和异常性，以帮助指导MRG过程。此外，研究还引入了一种新的增强策略，以便增强疾病类型在分布的表现。</li>
<li>results: 研究发现，使用提案的两个阶段生成方法和增强策略，可以显著提高生成报告中的疾病匹配度和多样性。这表明，这种方法可以有效地减少对疾病分布的长尾问题。<details>
<summary>Abstract</summary>
Knowledge Graph (KG) plays a crucial role in Medical Report Generation (MRG) because it reveals the relations among diseases and thus can be utilized to guide the generation process. However, constructing a comprehensive KG is labor-intensive and its applications on the MRG process are under-explored. In this study, we establish a complete KG on chest X-ray imaging that includes 137 types of diseases and abnormalities. Based on this KG, we find that the current MRG data sets exhibit a long-tailed problem in disease distribution. To mitigate this problem, we introduce a novel augmentation strategy that enhances the representation of disease types in the tail-end of the distribution. We further design a two-stage MRG approach, where a classifier is first trained to detect whether the input images exhibit any abnormalities. The classified images are then independently fed into two transformer-based generators, namely, ``disease-specific generator" and ``disease-free generator" to generate the corresponding reports. To enhance the clinical evaluation of whether the generated reports correctly describe the diseases appearing in the input image, we propose diverse sensitivity (DS), a new metric that checks whether generated diseases match ground truth and measures the diversity of all generated diseases. Results show that the proposed two-stage generation framework and augmentation strategies improve DS by a considerable margin, indicating a notable reduction in the long-tailed problem associated with under-represented diseases.
</details>
<details>
<summary>摘要</summary>
医学报告生成（MRG）中知识图грам（KG）发挥关键作用，因为它揭示疾病之间的关系，可以用于导航生成过程。然而，建立完整的KG是劳动密集的，其在MRG过程中的应用还尚未得到充分探索。在这种研究中，我们建立了包括137种疾病和异常的完整KG，基于这个KG，我们发现现有的MRG数据集表现出长尾问题。为了解决这个问题，我们提出了一种新的扩充策略，该策略可以增强疾病类型在分布的尾部的表达。我们还设计了一种两阶段的MRG方法，其中一个是使用一个分类器来检测输入图像是否存在任何异常。经过分类后，输入图像被独立地传递给两个基于转换器的生成器，即“疾病特定生成器”和“疾病无效生成器”，以生成相应的报告。为了提高生成的严肃评估，我们提出了多样敏感度（DS），一种新的度量，该度量检查生成的疾病与真实的疾病是否匹配，并且度量所有生成的疾病的多样性。结果表明，我们的两阶段生成框架和扩充策略可以大幅提高DS， indicating a remarkable reduction in the long-tailed problem associated with under-represented diseases.
</details></li>
</ul>
<hr>
<h2 id="FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks"><a href="#FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks" class="headerlink" title="FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks"></a>FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12518">http://arxiv.org/abs/2307.12518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Shaojie Zhao, Juan Cheng, Xingquan Li, Ri Su, Muzhou Hou, Cong Cao</li>
<li>for: 本研究旨在解决应用深度学习&#x2F;机器学习方法于疾病分类任务中的两个基本问题，即训练样本数量和质量的不足，以及如何有效地融合多源特征并训练稳定的分类模型。</li>
<li>methods: 我们提出了一种基于人类学习知识的Feature-aware Fusion Correlation Neural Network (FaFCNN)框架，包括特征意识互动模块和域对抗学习基于特征对齐模块。</li>
<li>results: 实验结果表明，通过使用预训练梯度提升树的扩充特征，FaFCNN在低质量 dataset 上实现了更高的性能提升，并且对于竞争对手基线方法进行了一致性优化。此外，广泛的实验还证明了提案的方法的稳定性和模型中每个组件的有效性。<details>
<summary>Abstract</summary>
There are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model\footnote{Accepted in IEEE SMC2023}.
</details>
<details>
<summary>摘要</summary>
“there are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model”。Here is the breakdown of the translation:* “two fundamental problems”(二大问题) - This phrase is translated as "two fundamental problems" to emphasize the importance of the issues being discussed.* “insufficient number and poor quality of training samples”(训练样本数量和质量不足) - This phrase is translated as "insufficient number and poor quality of training samples" to accurately convey the idea that there are not enough high-quality training samples available for training deep learning models.* “how to effectively fuse multiple source features”(如何有效地融合多源特征) - This phrase is translated as "how to effectively fuse multiple source features" to emphasize the importance of combining multiple sources of data to improve the accuracy of deep learning models.* “and thus train robust classification models”(并因此训练Robust分类模型) - This phrase is translated as "and thus train robust classification models" to emphasize the goal of training deep learning models that are robust and accurate.* “Feature-aware Fusion Correlation Neural Network”(特征意识融合相关神经网络) - This phrase is translated as "Feature-aware Fusion Correlation Neural Network" to accurately convey the name of the proposed method and its key features.* “based on domain adversarial learning”(基于域对抗学习) - This phrase is translated as "based on domain adversarial learning" to emphasize the key technique used in the proposed method.* “This is a general framework for disease classification”(这是一种普遍的疾病分类框架) - This phrase is translated as "This is a general framework for disease classification" to emphasize that the proposed method is a generalizable framework that can be applied to a wide range of disease classification tasks.* “and FaFCNN improves the way existing methods obtain sample correlation features”(而FaFCNN改进了现有方法获取样本相关特征) - This phrase is translated as "and FaFCNN improves the way existing methods obtain sample correlation features" to emphasize the key advantage of the proposed method.* “The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods”(实验结果表明，使用由前期逻辑树决策树提取的增强特征进行训练，比Random Forest基于方法更多地提高性能) - This phrase is translated as "The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods" to accurately convey the key findings of the experiments.* “On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines”(在我们的设置下，具有大量缺失数据的低质量数据集上，FaFCNN对于竞争对手基线表现出一致优化的性能) - This phrase is translated as "On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines" to emphasize the key finding that the proposed method performs well even on low-quality datasets with a large amount of missing data.* “In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model”(此外，广泛的实验还证明了提议方法的稳定性和模型每个组件的有效性) - This phrase is translated as "In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model" to emphasize the key finding that the proposed method is robust and effective, and to highlight the importance of each component of the model.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Based-Word-Substitution-for-Obstinate-Adversarial-Examples-Generation-in-Language-Models"><a href="#Gradient-Based-Word-Substitution-for-Obstinate-Adversarial-Examples-Generation-in-Language-Models" class="headerlink" title="Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models"></a>Gradient-Based Word Substitution for Obstinate Adversarial Examples Generation in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12507">http://arxiv.org/abs/2307.12507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimu Wang, Peng Shi, Hongyang Zhang</li>
<li>for:  This paper aims to address the problem of generating obstinate adversarial examples in NLP by introducing a novel word substitution method named GradObstinate, which automatically generates obstinate adversarial examples without any constraints on the search space or the need for manual design principles.</li>
<li>methods:  The proposed GradObstinate method uses a gradient-based approach to automatically generate obstinate adversarial examples. It does not rely on any manual design principles or constraints on the search space, making it more practical and applicable in real-world scenarios.</li>
<li>results:  The proposed GradObstinate method is evaluated on five representative NLP models and four benchmarks, and the results show that it generates more powerful obstinate adversarial examples with a higher attack success rate compared to antonym-based methods. Additionally, the obstinate substitutions found by GradObstinate are transferable to other models in black-box settings, including even GPT-3 and ChatGPT.Here is the answer in Simplified Chinese:</li>
<li>for: 这篇论文目的是解决NLP中的阻挡(过稳)攻击例的生成问题，提出了一种名为GradObstinate的新的词替换方法，可以自动生成阻挡攻击例而不需要人工设计原则或搜索空间的约束。</li>
<li>methods:  GradObstinate方法使用梯度基础来自动生成阻挡攻击例，不需要人工设计原则或搜索空间的约束，使其在实际应用中更加实际。</li>
<li>results:  GradObstinate方法在五种代表性的NLP模型和四个benchmark上进行了广泛的实验，结果显示，它可以生成更加强大的阻挡攻击例，攻击成功率高于antonym-based方法。此外，GradObstinate发现的阻挡替换还可以在黑盒设置下转移到其他模型中，包括GPT-3和ChatGPT。<details>
<summary>Abstract</summary>
In this paper, we study the problem of generating obstinate (over-stability) adversarial examples by word substitution in NLP, where input text is meaningfully changed but the model's prediction does not, even though it should. Previous word substitution approaches have predominantly focused on manually designed antonym-based strategies for generating obstinate adversarial examples, which hinders its application as these strategies can only find a subset of obstinate adversarial examples and require human efforts. To address this issue, in this paper, we introduce a novel word substitution method named GradObstinate, a gradient-based approach that automatically generates obstinate adversarial examples without any constraints on the search space or the need for manual design principles. To empirically evaluate the efficacy of GradObstinate, we conduct comprehensive experiments on five representative models (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP benchmarks (SST-2, MRPC, SNLI, and SQuAD) and a language-grounding benchmark (MSCOCO). Extensive experiments show that our proposed GradObstinate generates more powerful obstinate adversarial examples, exhibiting a higher attack success rate compared to antonym-based methods. Furthermore, to show the transferability of obstinate word substitutions found by GradObstinate, we replace the words in four representative NLP benchmarks with their obstinate substitutions. Notably, obstinate substitutions exhibit a high success rate when transferred to other models in black-box settings, including even GPT-3 and ChatGPT. Examples of obstinate adversarial examples found by GradObstinate are available at https://huggingface.co/spaces/anonauthors/SecretLanguage.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了对话语言处理（NLP）领域中生成顽固（过度稳定）攻击示例的问题，通过单词替换而生成这些示例。现有的单词替换方法主要采用人工设计的反义策略来生成顽固攻击示例，这限制了其应用，因为这些策略只能找到一部分顽固攻击示例，并且需要人工劳动。为解决这问题，在这篇论文中，我们提出了一种新的单词替换方法，即GradObstinate，它是基于梯度的方法，可以自动生成顽固攻击示例，不需要任何限制或人工设计原则。为证明GradObstinate的有效性，我们在五种代表性模型（Electra、ALBERT、Roberta、DistillBERT和CLIP）上进行了广泛的实验，这些模型在四个NLPBenchmark（SST-2、MRPC、SNLI和SQuAD）和一个语言固定 benchmark（MSCOCO）上进行了finetuning。实验结果表明，我们提出的GradObstinate可以更好地生成顽固攻击示例，对于反义策略来说，攻击成功率更高。此外，我们还证明了GradObstinate生成的顽固替换示例在黑盒Setting中的传送性，包括GPT-3和ChatGPT等模型。详细的顽固攻击示例可以在https://huggingface.co/spaces/anonauthors/SecretLanguage上找到。
</details></li>
</ul>
<hr>
<h2 id="TF-ICON-Diffusion-Based-Training-Free-Cross-Domain-Image-Composition"><a href="#TF-ICON-Diffusion-Based-Training-Free-Cross-Domain-Image-Composition" class="headerlink" title="TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition"></a>TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12493">http://arxiv.org/abs/2307.12493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Shilin-LU/TF-ICON">https://github.com/Shilin-LU/TF-ICON</a></li>
<li>paper_authors: Shilin Lu, Yanzhu Liu, Adams Wai-Kin Kong</li>
<li>for: 这个论文旨在提出一个无需训练的图像调和框架，将文本驱动的填充模型应用于跨领域图像导向作业。</li>
<li>methods: 这个框架使用的方法是使用文本驱动的填充模型，不需要进一步的训练、调整或优化。</li>
<li>results: 实验结果显示，将Stable Diffusion与特别提示（Exceptional Prompt）搭配可以超越现有的对应方法，而TF-ICON在多个视觉领域中也表现出优越的可 versatility。<details>
<summary>Abstract</summary>
Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains. Code is available at https://github.com/Shilin-LU/TF-ICON
</details>
<details>
<summary>摘要</summary>
文本驱动的扩散模型已经展示出了吸引人的生成能力，可以完成多种图像编辑任务。在这篇论文中，我们提出了TF-ICON，一种新的培成freeImage COmpositioN框架，利用文本驱动扩散模型进行交域图像引导组合。这个任务的目标是将用户提供的对象顺利地 интеGRATE到特定的视觉上下文中。当前的扩散基于方法通常需要费时的实例基于优化或特定数据集上的Finetuning pretrained模型，这可能会损害它们的丰富先天知识。相比之下，TF-ICON可以不需要额外的培成或优化，通过使用off-the-shelf扩散模型来完成交域图像引导组合。此外，我们引入了Exceptional prompt，它不含任何信息，以便文本驱动扩散模型可以准确地将真实图像转化为幂等表示，这为组合提供了基础。我们的实验表明，在不同的图像 datasets（CelebA-HQ、COCO和ImageNet）上，使用 Exceptional prompt 的 Stable Diffusion 方法可以超越现有的抽象方法，而TF-ICON 也可以在多种视觉领域中超越先前的基eline。代码可以在https://github.com/Shilin-LU/TF-ICON 中找到。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-Software-Security-Exploring-the-Strengths-and-Limitations-of-ChatGPT-in-the-Security-Applications"><a href="#ChatGPT-for-Software-Security-Exploring-the-Strengths-and-Limitations-of-ChatGPT-in-the-Security-Applications" class="headerlink" title="ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications"></a>ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12488">http://arxiv.org/abs/2307.12488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhilong Wang, Lan Zhang, Peng Liu</li>
<li>for: The paper aims to evaluate ChatGPT’s capabilities in security-oriented program analysis, specifically from the perspectives of both attackers and security analysts.</li>
<li>methods: The paper uses a case study approach, presenting several security-oriented program analysis tasks and deliberately introducing challenges to assess ChatGPT’s responses.</li>
<li>results: The paper examines the quality of answers provided by ChatGPT and gains a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.Here’s the same information in Simplified Chinese text:</li>
<li>for: 本文旨在评估ChatGPT在安全关注程序分析方面的能力，具体来说是从攻击者和安全分析员两个角度出发。</li>
<li>methods: 本文采用 caso study方法，通过提出多个安全关注程序分析任务，故意引入挑战来评估ChatGPT的回答质量。</li>
<li>results: 本文通过分析ChatGPT的回答来了解它在安全关注程序分析方面的优劣点。<details>
<summary>Abstract</summary>
ChatGPT, as a versatile large language model, has demonstrated remarkable potential in addressing inquiries across various domains. Its ability to analyze, comprehend, and synthesize information from both online sources and user inputs has garnered significant attention. Previous research has explored ChatGPT's competence in code generation and code reviews. In this paper, we delve into ChatGPT's capabilities in security-oriented program analysis, focusing on perspectives from both attackers and security analysts. We present a case study involving several security-oriented program analysis tasks while deliberately introducing challenges to assess ChatGPT's responses. Through an examination of the quality of answers provided by ChatGPT, we gain a clearer understanding of its strengths and limitations in the realm of security-oriented program analysis.
</details>
<details>
<summary>摘要</summary>
chatgpt 作为一种多能语言模型，在各个领域的问题上表现出了惊人的潜力。它可以分析、理解和合成来自线上源和用户输入的信息，吸引了广泛的关注。以前的研究探讨了 chatgpt 在代码生成和代码审查方面的能力。在这篇论文中，我们探究 chatgpt 在安全关注程序分析方面的能力，具体来说是从攻击者和安全分析员的视角来评估 chatgpt 的回答质量。我们通过对多个安全关注程序分析任务的挑战性评估，了解 chatgpt 在安全关注程序分析领域的优势和局限性。
</details></li>
</ul>
<hr>
<h2 id="ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation"><a href="#ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation" class="headerlink" title="ProtoFL: Unsupervised Federated Learning via Prototypical Distillation"></a>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12450">http://arxiv.org/abs/2307.12450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansol Kim, Youngjun Kwak, Minyoung Jung, Jinho Shin, Youngsung Kim, Changick Kim</li>
<li>for: 提高数据隐私保护和一类分类性能</li>
<li>methods: 提出了基于原型 repreentation 的 federated learning 方法，以增强全球模型的表示能力，降低通信成本</li>
<li>results: 对五种广泛使用的基准数据集进行了广泛的实验，证明了提议的框架在先前的方法中表现出色<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
</details>
<details>
<summary>摘要</summary>
《联合学习（FL）是一种有前途的方法，能够提高数据隐私保护，特别是 для 身份验证系统。然而，有限的回合通信，珍贵的表示和扩展性带来了其部署的挑战，使其全部潜力受限。在本文中，我们提出了“ProtoFL”，基于 прототипиаль表示储存的无监督联合学习，以提高全球模型的表示力和减少回合通信成本。此外，我们还引入了基于 нормализа函数的本地一类分类器，以提高有限数据下的性能。我们的研究是首次利用FL提高一类分类性能的研究。我们在五种广泛使用的标准数据集上进行了广泛的实验，包括MNIST、CIFAR-10、CIFAR-100、ImageNet-30和键盘动作，以示出我们提posed的框架在过去的方法中的超越性。》Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces"><a href="#SCRAPS-Speech-Contrastive-Representations-of-Acoustic-and-Phonetic-Spaces" class="headerlink" title="SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces"></a>SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12445">http://arxiv.org/abs/2307.12445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan Vallés-Pérez, Grzegorz Beringer, Piotr Bilinski, Gary Cook, Roberto Barra-Chicote</li>
<li>for: 本研究旨在将CLIP模型应用到语音领域，以学习共同的phonetic和acoustic空间表示。</li>
<li>methods: 本研究使用CLIP模型，通过对语音资料进行对映和调整，以学习共同的phonetic和acoustic空间表示。</li>
<li>results: 研究获得的结果显示，提案的模型具有辨识phonetic变化的能力，并且具有对不同类型噪音的抗响性。此外，研究还证明了模型的对下游应用的有用性，如语音识别和生成等。<details>
<summary>Abstract</summary>
Numerous examples in the literature proved that deep learning models have the ability to work well with multimodal data. Recently, CLIP has enabled deep learning systems to learn shared latent spaces between images and text descriptions, with outstanding zero- or few-shot results in downstream tasks. In this paper we explore the same idea proposed by CLIP but applied to the speech domain, where the phonetic and acoustic spaces usually coexist. We train a CLIP-based model with the aim to learn shared representations of phonetic and acoustic spaces. The results show that the proposed model is sensible to phonetic changes, with a 91% of score drops when replacing 20% of the phonemes at random, while providing substantial robustness against different kinds of noise, with a 10% performance drop when mixing the audio with 75% of Gaussian noise. We also provide empirical evidence showing that the resulting embeddings are useful for a variety of downstream applications, such as intelligibility evaluation and the ability to leverage rich pre-trained phonetic embeddings in speech generation task. Finally, we discuss potential applications with interesting implications for the speech generation and recognition fields.
</details>
<details>
<summary>摘要</summary>
多种例子在文献中证明深度学习模型可以好地处理多Modal数据。近期，CLIP使得深度学习系统可以学习图像和文本描述之间的共享幂等空间，显示出 Zero-或几个Shot结果在下游任务中。在这篇论文中，我们探索了与CLIP相同的想法，但是应用到语音频域中， где音频和语音空间通常共存。我们使用CLIP基于模型，以学习共享phonetic和Acoustic空间的表示。结果显示，我们的提posed模型对phonetic变化敏感，在Random中替换20%的Phoneemes时，得分下降91%，同时在混合音频75%的加aussian噪音时，表现下降10%。我们还提供了实验证明，表明得到的嵌入是下游应用中有用，如智能评估和Speech生成任务中的Rich预训练phonetic嵌入。最后，我们讨论了可能的应用，具有Speech生成和识别领域的 interessante implikationen。
</details></li>
</ul>
<hr>
<h2 id="AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection"><a href="#AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection" class="headerlink" title="AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection"></a>AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03766">http://arxiv.org/abs/2308.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Mall, Sanchit Kabra, Ankur Lhila, Pawan Ajmera</li>
<li>for:  automatización de la detección de enfermedades en maíz utilizando imágenes multiespectrales obtenidas desde drones.</li>
<li>methods: combina redes neuronales convolucionales (CNNs) como extractores de características y técnicas de segmentación para identificar las plantas de maíz y sus enfermedades asociadas.</li>
<li>results: detecta una variedad de enfermedades de maíz, incluyendo la roya, el antracnosis y la podredumbre foliar, con un rendimiento estado del arte en el conjunto de datos personalizado.<details>
<summary>Abstract</summary>
This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, including powdery mildew, anthracnose, and leaf blight. The framework achieves state-of-the-art performance on the custom hand-collected dataset and contributes to the field of automated disease detection in agriculture, offering a practical solution for early identification of diseases in maize crops advanced machine learning techniques and deep learning architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Framing-Relevance-for-Safety-Critical-Autonomous-Systems"><a href="#Framing-Relevance-for-Safety-Critical-Autonomous-Systems" class="headerlink" title="Framing Relevance for Safety-Critical Autonomous Systems"></a>Framing Relevance for Safety-Critical Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14355">http://arxiv.org/abs/2307.14355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Astrid Rakow</li>
<li>for: 这个论文是为了研究如何确定自动化系统在当前任务中需要的信息，以建立适当的世界观并实现任务目标。</li>
<li>methods: 这篇论文使用了正式方法来确定自动化系统需要的信息，包括对各种信息源的分类和选择，以及如何将信息整合到自动化系统中。</li>
<li>results: 这篇论文的结果表明，使用正式方法可以有效地确定自动化系统需要的信息，并且可以帮助自动化系统在充满信息的环境中做出更加有效的决策。<details>
<summary>Abstract</summary>
We are in the process of building complex highly autonomous systems that have build-in beliefs, perceive their environment and exchange information. These systems construct their respective world view and based on it they plan their future manoeuvres, i.e., they choose their actions in order to establish their goals based on their prediction of the possible futures. Usually these systems face an overwhelming flood of information provided by a variety of sources where by far not everything is relevant. The goal of our work is to develop a formal approach to determine what is relevant for a safety critical autonomous system at its current mission, i.e., what information suffices to build an appropriate world view to accomplish its mission goals.
</details>
<details>
<summary>摘要</summary>
我们正在建设复杂高自动化系统，这些系统具有内置的信念和环境感知功能，并且能够交换信息。这些系统根据自己的世界观建立未来行动计划，即选择行动以实现目标基于预测未来的可能性。通常这些系统面临着极大的信息泛洪，其中大多数信息并不相关。我们的工作目标是开发一种正式的方法，以确定一个安全关键自动化系统当前任务中需要的信息，以建立合适的世界观以实现任务目标。
</details></li>
</ul>
<hr>
<h2 id="Implementing-Smart-Contracts-The-case-of-NFT-rental-with-pay-per-like"><a href="#Implementing-Smart-Contracts-The-case-of-NFT-rental-with-pay-per-like" class="headerlink" title="Implementing Smart Contracts: The case of NFT-rental with pay-per-like"></a>Implementing Smart Contracts: The case of NFT-rental with pay-per-like</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02424">http://arxiv.org/abs/2308.02424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asopi/rental-project">https://github.com/asopi/rental-project</a></li>
<li>paper_authors: Alfred Sopi, Johannes Schneider, Jan vom Brocke</li>
<li>For: The paper aims to address the challenges of lending and renting non-fungible tokens (NFTs) for marketing purposes, such as the risk of items not being returned and the difficulty in anticipating the impact of artworks.* Methods: The paper introduces an NFT rental solution based on a pay-per-like pricing model using blockchain technology and smart contracts based on the Ethereum chain.* Results: The paper finds that blockchain solutions enjoy many advantages, but also observes dark sides such as large blockchain fees, which can be unfair to niche artists and potentially hamper cultural diversity. Additionally, a trust-cost tradeoff arises to handle fraud caused by manipulation from parties outside the blockchain.Here are the three points in Simplified Chinese text:* For: 论文目的是解决非 fungible tokens（NFTs）的借领和租赁问题，如物品不返还和艺术作品的影响预测困难。* Methods: 论文提出了基于 pays-per-like 价格模式的 NFT 租赁解决方案，使用了区块链技术和基于 Ethereum 链的智能合约。* Results: 论文发现区块链解决方案具有许多优点，但也注意到了一些黑暗的面向，如大量区块链费用，可能对专业艺术家不公平，可能妨碍文化多样性。此外，面临滥用和 manipulate 等问题，需要考虑信任成本tradeoff。<details>
<summary>Abstract</summary>
Non-fungible tokens(NFTs) are on the rise. They can represent artworks exhibited for marketing purposes on webpages of companies or online stores -- analogously to physical artworks. Lending of NFTs is an attractive form of passive income for owners but comes with risks (e.g., items are not returned) and costs for escrow agents. Similarly, renters have difficulties in anticipating the impact of artworks, e.g., how spectators of NFTs perceive them. To address these challenges, we introduce an NFT rental solution based on a pay-per-like pricing model using blockchain technology, i.e., smart contracts based on the Ethereum chain. We find that blockchain solutions enjoy many advantages also reported for other applications, but interestingly, we also observe dark sides of (large) blockchain fees. Blockchain solutions appear unfair to niche artists and potentially hamper cultural diversity. Furthermore, a trust-cost tradeoff arises to handle fraud caused by manipulation from parties outside the blockchain. All code for the solution is publicly available at: https://github.com/asopi/rental-project
</details>
<details>
<summary>摘要</summary>
非可转换token(NFT)在升温。它们可以代表公司或在线商店的网页上展示的艺术作品，类似于 físichen艺术作品。NFT的借用是持有者的有利预想的 passive income，但是也有风险（例如，物品不返回）和代理人的成本。同时，租户困难预测NFT的影响，例如，如何评估NF的观众。为解决这些挑战，我们介绍了基于付费模式的NFT租赁解决方案，使用区块链技术和Ethereum链上的智能合约。我们发现，区块链解决方案在其他应用程序中报道的优点也存在，但是有趣的是，我们还观察到大型区块链费用的黑暗面。区块链解决方案可能不公平对尼希艺术家和文化多样性。此外，为了处理外部干扰所引起的诈骗，需要处理信任成本。所有的代码都可以在GitHub上找到：https://github.com/asopi/rental-project。
</details></li>
</ul>
<hr>
<h2 id="Validation-of-a-Zero-Shot-Learning-Natural-Language-Processing-Tool-for-Data-Abstraction-from-Unstructured-Healthcare-Data"><a href="#Validation-of-a-Zero-Shot-Learning-Natural-Language-Processing-Tool-for-Data-Abstraction-from-Unstructured-Healthcare-Data" class="headerlink" title="Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data"></a>Validation of a Zero-Shot Learning Natural Language Processing Tool for Data Abstraction from Unstructured Healthcare Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00107">http://arxiv.org/abs/2308.00107</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaufmannb/PDF-Extractor">https://github.com/kaufmannb/PDF-Extractor</a></li>
<li>paper_authors: Basil Kaufmann, Dallin Busby, Chandan Krushna Das, Neeraja Tillu, Mani Menon, Ashutosh K. Tewari, Michael A. Gorin</li>
<li>For: The paper is written to describe the development and validation of a zero-shot learning natural language processing (NLP) tool for abstracting data from unstructured text contained within PDF documents, such as those found within electronic health records.* Methods: The data abstraction tool was based on the GPT-3.5 model from OpenAI, and was compared to three physician human abstractors in terms of time to task completion and accuracy for abstracting data on 14 unique variables from a set of 199 de-identified radical prostatectomy pathology reports. The reports were processed by the software tool in vectorized and scanned formats to establish the impact of optical character recognition on data abstraction.* Results: The software tool required a mean of 12.8 s to process the vectorized reports and a mean of 15.8 s to process the scanned reports, which was significantly faster than the human abstractors (mean time of 101 s). The tool had an overall accuracy of 94.2% for the vectorized reports and 88.7% for the scanned reports, which was non-inferior to 2 out of 3 human abstractors.<details>
<summary>Abstract</summary>
Objectives: To describe the development and validation of a zero-shot learning natural language processing (NLP) tool for abstracting data from unstructured text contained within PDF documents, such as those found within electronic health records. Materials and Methods: A data abstraction tool based on the GPT-3.5 model from OpenAI was developed and compared to three physician human abstractors in terms of time to task completion and accuracy for abstracting data on 14 unique variables from a set of 199 de-identified radical prostatectomy pathology reports. The reports were processed by the software tool in vectorized and scanned formats to establish the impact of optical character recognition on data abstraction. The tool was assessed for superiority for data abstraction speed and non-inferiority for accuracy. Results: The human abstractors required a mean of 101s per report for data abstraction, with times varying from 15 to 284 s. In comparison, the software tool required a mean of 12.8 s to process the vectorized reports and a mean of 15.8 to process the scanned reports (P < 0.001). The overall accuracies of the three human abstractors were 94.7%, 97.8%, and 96.4% for the combined set of 2786 datapoints. The software tool had an overall accuracy of 94.2% for the vectorized reports, proving to be non-inferior to the human abstractors at a margin of -10% ($\alpha$=0.025). The tool had a slightly lower accuracy of 88.7% using the scanned reports, proving to be non-inferiority to 2 out of 3 human abstractors. Conclusion: The developed zero-shot learning NLP tool affords researchers comparable levels of accuracy to that of human abstractors, with significant time savings benefits. Because of the lack of need for task-specific model training, the developed tool is highly generalizable and can be used for a wide variety of data abstraction tasks, even outside the field of medicine.
</details>
<details>
<summary>摘要</summary>
Materials and Methods:  We developed a data abstraction tool based on the GPT-3.5 model from OpenAI and compared its performance to three human abstractors in terms of time and accuracy for abstracting data from 14 variables in 199 de-identified radical prostatectomy pathology reports. The reports were processed in both vectorized and scanned formats to assess the impact of optical character recognition (OCR) on data abstraction. We evaluated the tool's superiority in speed and non-inferiority in accuracy.Results:  The human abstractors took a mean of 101 seconds per report, with times ranging from 15 to 284 seconds. In contrast, the software tool took a mean of 12.8 seconds to process vectorized reports and 15.8 seconds for scanned reports (p < 0.001). The tool's overall accuracy was 94.2% for vectorized reports, proving non-inferiority to the human abstractors at a margin of -10% (α = 0.025). For scanned reports, the tool's accuracy was 88.7%, proving non-inferiority to two out of three human abstractors.Conclusion:  The developed zero-shot learning NLP tool provides researchers with a time-saving solution that affords comparable levels of accuracy to human abstractors. The tool's lack of need for task-specific model training makes it highly generalizable and suitable for a wide range of data abstraction tasks, both within and outside the field of medicine.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control"><a href="#Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control" class="headerlink" title="Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control"></a>Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12388">http://arxiv.org/abs/2307.12388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longchao Da, Hao Mei, Romir Sharma, Hua Wei</li>
<li>for: 提高RL在实际道路上的应用性能</li>
<li>methods: 使用 simulations-to-real-world (sim-to-real) 转移方法，动态将模拟环境中学习的策略转移到实际环境中，以 Mitigate domain gap</li>
<li>results: 在模拟交通环境中评估了UGAT方法，显示UGAT方法可以在实际环境中提高RL策略的性能<details>
<summary>Abstract</summary>
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
</details>
<details>
<summary>摘要</summary>
交通信号控制（TSC）是一项复杂且重要的任务，影响了数百万人的日常生活。人工智能学习（RL）已经在优化交通信号控制方面表现出了扎实的成果，但现有RL基于TSC方法主要在模拟环境中训练，它们在实际世界中的性能差异很大。在这篇论文中，我们提出了一种从模拟环境到实际世界（sim-to-real）的转移方法，称为UGAT，它通过在模拟环境中动态地转换行动，以减少领域差距，将学习的RL策略在实际世界中表现出较好的性能。我们对一个模拟交通环境进行评估，并显示了UGAT方法在实际世界中的性能提升。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning"><a href="#In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning" class="headerlink" title="In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"></a>In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12375">http://arxiv.org/abs/2307.12375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Kossen, Tom Rainforth, Yarin Gal</li>
<li>for: 这种研究旨在探讨大语言模型（LLMs）在下游任务中表现提高的原因，以及如何更好地理解和调控LLMs的行为。</li>
<li>methods: 这篇论文使用了实验方法，检查了LLMs在不同情况下如何处理输入和标签的关系，并分析了模型如何在不同阶段学习和使用标签信息。</li>
<li>results: 研究发现，LLMs通常会在输入中使用标签信息，但是在预训练和输入中的标签关系是不同的，并且模型不会对所有输入信息进行平等处理。这些结论可以帮助我们更好地理解和调控LLMs的行为。<details>
<summary>Abstract</summary>
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into understanding and aligning LLM behavior.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在下游任务中表现往往会得到显著改善，当包含输入-标签关系的示例在内部时。然而，目前没有一致的看法，即如何实现这种在 контексте学习（ICL）能力：例如，希耶等（2021）将 ICL 比作一种通用学习算法，而民等（2022b）则认为 ICL 不会从内部示例中学习标签关系。在这篇论文中，我们研究以下几点：1. 如何 Labels of in-context examples affect predictions.2. 如何在预训练时学习的标签关系与输入-标签示例提供在内部交互。3. ICL 如何对各个内部示例的标签信息进行汇总。我们的发现表明，LLMs 通常会在内部示例中使用标签信息，但是预训练和内部标签关系被处理不同，并且模型不会对所有内部信息进行平等考虑。我们的结果为理解和调整 LLM 行为提供了新的视角。
</details></li>
</ul>
<hr>
<h2 id="Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans"><a href="#Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans" class="headerlink" title="Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans"></a>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12369">http://arxiv.org/abs/2307.12369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rumeng Li, Xun Wang, Dan Berlowitz, Brian Silver, Wen Hu, Heather Keating, Raelene Goodwin, Weisong Liu, Honghuang Lin, Hong Yu<br>for: 这个研究的目的是使用机器学习方法来分析患有阿尔茨杰病（AD）的长期电子医疗纪录（EHR），以预测AD的发病更早。methods: 这个研究使用了一种case-control设计，使用了来自美国卫生部VA卫生管理局（VHA）的长期EHR数据从2004年到2021年。研究使用了一组AD相关关键词，并分析了这些词的时间发展特征以预测AD的发病。results: 研究发现，与AD诊断日期相关的AD相关关键词的出现频率在患有AD的患者中增长迅速，从约10年增长到超过40年。此外，研究还发现了一些年龄、性别和种族&#x2F;民族 subgroup的差异。最佳模型可以具有高度的排除率（ROCAUC 0.997），并且在不同的 subgroup 中具有良好的准确率和均匀性。<details>
<summary>Abstract</summary>
Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
</details>
<details>
<summary>摘要</summary>
早期预测阿尔茨海默病（AD）非常重要，以便在时间上采取措施和治疗。这项研究的目的是使用机器学习方法分析患者的长期电子医疗纪录（EHR），以预测AD的发病。我们采用了一种case-control设计，使用了2004年至2021年美国卫生部 veterans Health Administration（VHA）的长期EHR数据。cases是在2016年1月1日以后根据ICD-10-CM代码被诊断出AD的VHA患者，与年龄、性别和临床使用相同的9名控制组进行匹配。我们使用了AD相关关键词的Panel，并分析这些关键词在患者的长期EHR中的出现情况，以预测AD的发病。我们进行了年龄、性别和种族/民族 subgroup分析，并在一个封闭和“未看到”的VHA站组中验证了模型。我们测试了四种机器学习模型，并计算了预测正确率、准确率和其他相关指标。研究人口包括16,701个case和39,097个匹配控制组。case的AD相关关键词每年增长迅速，从约10到超过40，而控制组保持在10个。最佳模型在使用至少10年前ICD-based诊断时的预测中 дости到了高度的抗抑词率（ROCAUC 0.997）。模型具有良好的准确率（Hosmer-Lemeshow好准确性值 = 0.99）和年龄、性别和种族/民族 subgroup中的一致性，除了年龄少于65的患者（ROCAUC 0.746）。机器学习模型使用从EHR笔记中提取的AD相关关键词可以预测未来AD诊断，这表明了这种方法的潜在应用价值，可以通过分析EHR笔记来预测AD风险，这是一种可靠且经济的预测方法。
</details></li>
</ul>
<hr>
<h2 id="Deployment-of-Leader-Follower-Automated-Vehicle-Systems-for-Smart-Work-Zone-Applications-with-a-Queuing-based-Traffic-Assignment-Approach"><a href="#Deployment-of-Leader-Follower-Automated-Vehicle-Systems-for-Smart-Work-Zone-Applications-with-a-Queuing-based-Traffic-Assignment-Approach" class="headerlink" title="Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach"></a>Deployment of Leader-Follower Automated Vehicle Systems for Smart Work Zone Applications with a Queuing-based Traffic Assignment Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03764">http://arxiv.org/abs/2308.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Tang, Xianbiao Hu</li>
<li>for: 这篇论文旨在优化Autonomous Truck Mounted Attenuator（ATMA）车辆系统中的routing，以最小化在交通基础设施维护期间的系统成本。</li>
<li>methods: 这篇论文使用了连接和自动化车辆技术，并提出了一种基于队列的交通分配方法，以考虑ATMA车辆的运行速度差异。</li>
<li>results: 研究发现，通过模拟不同路线的选择，ATMA车辆系统可以减少交通系统的成本，并且可以通过队列基于的交通分配方法来实现这一目标。<details>
<summary>Abstract</summary>
The emerging technology of the Autonomous Truck Mounted Attenuator (ATMA), a leader-follower style vehicle system, utilizes connected and automated vehicle capabilities to enhance safety during transportation infrastructure maintenance in work zones. However, the speed difference between ATMA vehicles and general vehicles creates a moving bottleneck that reduces capacity and increases queue length, resulting in additional delays. The different routes taken by ATMA cause diverse patterns of time-varying capacity drops, which may affect the user equilibrium traffic assignment and lead to different system costs. This manuscript focuses on optimizing the routing for ATMA vehicles in a network to minimize the system cost associated with the slow-moving operation.   To achieve this, a queuing-based traffic assignment approach is proposed to identify the system cost caused by the ATMA system. A queuing-based time-dependent (QBTD) travel time function, considering capacity drop, is introduced and applied in the static user equilibrium traffic assignment problem, with a result of adding dynamic characteristics. Subsequently, we formulate the queuing-based traffic assignment problem and solve it using a modified path-based algorithm. The methodology is validated using a small-size and a large-size network and compared with two benchmark models to analyze the benefit of capacity drop modeling and QBTD travel time function. Furthermore, the approach is applied to quantify the impact of different routes on the traffic system and identify an optimal route for ATMA vehicles performing maintenance work. Finally, sensitivity analysis is conducted to explore how the impact changes with variations in traffic demand and capacity reduction.
</details>
<details>
<summary>摘要</summary>
新兴技术自动化卡车拥挤器（ATMA），一种领头随员式车辆系统，通过连接和自动化车辆能力来提高交通基础设施维护工区的安全性。然而，ATMA车辆的速度与普通车辆的速度差距创造了运动瓶颈，导致交通容量下降和排队较长，从而增加延迟。ATMA车辆采取不同的路线，导致时间变化的容量下降，这可能影响用户均衡交通分配和导致不同的系统成本。本文关注优化ATMA车辆网络路径，以最小化由慢速运行引起的系统成本。为此，我们提出了基于队列的交通分配方法，以识别由ATMA系统引起的系统成本。我们引入了考虑容量下降的队列基于时间依赖（QBTD）旅行时间函数，并应用于静态用户均衡交通分配问题。通过修改的路径基本算法，我们解决了队列基于交通分配问题。我们验证了方法使用小型和大型网络，并与两个参考模型进行比较，以分析容器下降模型和QBTD旅行时间函数的利好。此外，我们还应用该方法来评估不同路线对交通系统的影响，并确定最佳维护工区路线。最后，我们进行敏感分析，以explore系统成本变化的影响因素。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.AI_2023_07_24/" data-id="clpxp6bvs0015ee882k5q0qtl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.CL_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T11:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.CL_2023_07_24/">cs.CL - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Corrections-of-Zipf’s-and-Heaps’-Laws-Derived-from-Hapax-Rate-Models"><a href="#Corrections-of-Zipf’s-and-Heaps’-Laws-Derived-from-Hapax-Rate-Models" class="headerlink" title="Corrections of Zipf’s and Heaps’ Laws Derived from Hapax Rate Models"></a>Corrections of Zipf’s and Heaps’ Laws Derived from Hapax Rate Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12896">http://arxiv.org/abs/2307.12896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lukasz-debowski/zipfanatomy">https://github.com/lukasz-debowski/zipfanatomy</a></li>
<li>paper_authors: Łukasz Dębowski</li>
<li>for: 这篇论文旨在修正Zipf和Hep的法则，基于系统模型的废弃率。</li>
<li>methods: 这篇论文使用了两个假设：第一个是标准杯模型，认为短文本的边缘频率分布与长文本中的词汇频率分布相同；第二个假设是废弃率的函数是文本大小的简单函数。</li>
<li>results: 这篇论文显示，使用了Logistic模型可以得到最佳的适应。<details>
<summary>Abstract</summary>
The article introduces corrections to Zipf's and Heaps' laws based on systematic models of the hapax rate. The derivation rests on two assumptions: The first one is the standard urn model which predicts that marginal frequency distributions for shorter texts look as if word tokens were sampled blindly from a given longer text. The second assumption posits that the rate of hapaxes is a simple function of the text size. Four such functions are discussed: the constant model, the Davis model, the linear model, and the logistic model. It is shown that the logistic model yields the best fit.
</details>
<details>
<summary>摘要</summary>
文章介绍了对Zipf和堆法的修正，基于系统性模型的唯一urn模型和文本大小的函数模型。两个假设是：首先，假设短文本中的单词分布遵循着 longer text中的样本采样；其次，假设 hapax 的速率是文本大小的简单函数。文章提出了四种函数模型：常数模型、Davis模型、线性模型和ilogistic模型。结果表明，ilogistic模型得到了最佳的适应。
</details></li>
</ul>
<hr>
<h2 id="Joint-Dropout-Improving-Generalizability-in-Low-Resource-Neural-Machine-Translation-through-Phrase-Pair-Variables"><a href="#Joint-Dropout-Improving-Generalizability-in-Low-Resource-Neural-Machine-Translation-through-Phrase-Pair-Variables" class="headerlink" title="Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables"></a>Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12835">http://arxiv.org/abs/2307.12835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Araabi, Vlad Niculae, Christof Monz</li>
<li>for: 提高低资源语言对翻译机器翻译的性能</li>
<li>methods: 使用联合Dropout方法，将短语替换为变量，提高翻译机器翻译的可组合性</li>
<li>results: 对低资源语言对翻译机器翻译进行了重要改进，为语言对翻译机器翻译带来了显著提高，并且在不同领域中也具有了更好的鲁棒性和适应性。<details>
<summary>Abstract</summary>
Despite the tremendous success of Neural Machine Translation (NMT), its performance on low-resource language pairs still remains subpar, partly due to the limited ability to handle previously unseen inputs, i.e., generalization. In this paper, we propose a method called Joint Dropout, that addresses the challenge of low-resource neural machine translation by substituting phrases with variables, resulting in significant enhancement of compositionality, which is a key aspect of generalization. We observe a substantial improvement in translation quality for language pairs with minimal resources, as seen in BLEU and Direct Assessment scores. Furthermore, we conduct an error analysis, and find Joint Dropout to also enhance generalizability of low-resource NMT in terms of robustness and adaptability across different domains
</details>
<details>
<summary>摘要</summary>
尽管神经机器翻译（NMT）已经取得了很大的成功，但它在低资源语言对的表现仍然较差，一个原因是对未经见过的输入的处理能力有限，即通用性。在这篇论文中，我们提出了一种方法called Joint Dropout，该方法通过将短语替换为变量，从而提高了语言对的复合性，这是通用性的关键特征。我们发现，对具有最少资源的语言对，使用Joint Dropout可以得到显著提高翻译质量，按照BLEU和直接评估得分来看。此外，我们进行了错误分析，发现Joint Dropout还可以提高低资源NMT的通用性，包括鲁棒性和适应性 across different domains。
</details></li>
</ul>
<hr>
<h2 id="Guidance-in-Radiology-Report-Summarization-An-Empirical-Evaluation-and-Error-Analysis"><a href="#Guidance-in-Radiology-Report-Summarization-An-Empirical-Evaluation-and-Error-Analysis" class="headerlink" title="Guidance in Radiology Report Summarization: An Empirical Evaluation and Error Analysis"></a>Guidance in Radiology Report Summarization: An Empirical Evaluation and Error Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12803">http://arxiv.org/abs/2307.12803</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jantrienes/inlg2023-radsum">https://github.com/jantrienes/inlg2023-radsum</a></li>
<li>paper_authors: Jan Trienes, Paul Youssef, Jörg Schlötterer, Christin Seifert</li>
<li>for:  automatization of radiology report summarization to reduce clinicians’ manual work and improve reporting consistency</li>
<li>methods:  variable-length extractive summaries as a domain-agnostic guidance signal, competitive with domain-specific methods</li>
<li>results:  improved summarization quality compared to unguided summarization, but still limited by content selection and corpus-level inconsistencies<details>
<summary>Abstract</summary>
Automatically summarizing radiology reports into a concise impression can reduce the manual burden of clinicians and improve the consistency of reporting. Previous work aimed to enhance content selection and factuality through guided abstractive summarization. However, two key issues persist. First, current methods heavily rely on domain-specific resources to extract the guidance signal, limiting their transferability to domains and languages where those resources are unavailable. Second, while automatic metrics like ROUGE show progress, we lack a good understanding of the errors and failure modes in this task. To bridge these gaps, we first propose a domain-agnostic guidance signal in form of variable-length extractive summaries. Our empirical results on two English benchmarks demonstrate that this guidance signal improves upon unguided summarization while being competitive with domain-specific methods. Additionally, we run an expert evaluation of four systems according to a taxonomy of 11 fine-grained errors. We find that the most pressing differences between automatic summaries and those of radiologists relate to content selection including omissions (up to 52%) and additions (up to 57%). We hypothesize that latent reporting factors and corpus-level inconsistencies may limit models to reliably learn content selection from the available data, presenting promising directions for future work.
</details>
<details>
<summary>摘要</summary>
自动概括 radiology 报告可以减少临床医生的手动劳动和提高报告的一致性。过去的工作是通过引导抽象SUMMARIZATION提高内容选择和事实性。然而，两个关键问题仍然存在。首先，当前的方法听命于域特定资源提取指导信号，限制其在领域和语言中的传输性。其次，虽然自动度量器Like ROUGE表现出进步，但我们对这个任务中的错误和失败模式几乎没有良好的理解。为了bridging这些差距，我们首先提议一种域无关的引导信号，即变量长抽取SUMMARIES。我们的实验结果表明，这种引导信号可以超过无引导抽取SUMMARIES，并与域特定方法竞争。此外，我们运行了四种系统的专家评估，根据报告11种细腻错误的税onomy。我们发现，自动报告与医生的报告之间最主要的差异在于内容选择，包括漏掉（最多52%）和添加（最多57%）。我们推测，隐藏的报告因素和 corpus 级别的不一致性可能限制模型从可用数据中学习内容选择，提供了可能的未来工作方向。
</details></li>
</ul>
<hr>
<h2 id="RRAML-Reinforced-Retrieval-Augmented-Machine-Learning"><a href="#RRAML-Reinforced-Retrieval-Augmented-Machine-Learning" class="headerlink" title="RRAML: Reinforced Retrieval Augmented Machine Learning"></a>RRAML: Reinforced Retrieval Augmented Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12798">http://arxiv.org/abs/2307.12798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Bacciu, Florin Cuconasu, Federico Siciliano, Fabrizio Silvestri, Nicola Tonellotto, Giovanni Trappolini</li>
<li>for: 本研究旨在推广人工智能领域中的大语言模型（LLMs）的应用，提高其在理解、生成和修改人语言方面的能力。</li>
<li>methods: 本研究提出了一种新的框架，即强化检索增强机器学习（RRAML），它将LLMs的理解能力与一个特制的检索器连接起来，从一个大量的用户提供的数据库中提取支持信息。</li>
<li>results: RRAML可以减少LLMs的训练和重新训练的需求，同时也可以避免访问LLMs的梯度，从而提高其应用的效率和可扩展性。此外，RRAML还可以减少检索结果中的幻见和不相关信息，提高检索的准确率和有用性。<details>
<summary>Abstract</summary>
The emergence of large language models (LLMs) has revolutionized machine learning and related fields, showcasing remarkable abilities in comprehending, generating, and manipulating human language. However, their conventional usage through API-based text prompt submissions imposes certain limitations in terms of context constraints and external source availability. To address these challenges, we propose a novel framework called Reinforced Retrieval Augmented Machine Learning (RRAML). RRAML integrates the reasoning capabilities of LLMs with supporting information retrieved by a purpose-built retriever from a vast user-provided database. By leveraging recent advancements in reinforcement learning, our method effectively addresses several critical challenges. Firstly, it circumvents the need for accessing LLM gradients. Secondly, our method alleviates the burden of retraining LLMs for specific tasks, as it is often impractical or impossible due to restricted access to the model and the computational intensity involved. Additionally we seamlessly link the retriever's task with the reasoner, mitigating hallucinations and reducing irrelevant, and potentially damaging retrieved documents. We believe that the research agenda outlined in this paper has the potential to profoundly impact the field of AI, democratizing access to and utilization of LLMs for a wide range of entities.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的出现对机器学习和相关领域产生了革命性的变革，展示了人类语言理解、生成和修改的强大能力。然而，通过 API 提交文本提示来使用 LLM 存在一些限制，包括上下文约束和外部资源的可用性。为解决这些挑战，我们提出了一个新的框架 called Reinforced Retrieval Augmented Machine Learning（RRAML）。RRAML 将 LLM 的理解能力与用户提供的大量数据库中的支持信息结合起来，通过利用最近的回归学术进行有效地解决多个关键问题。首先，它绕过了访问 LLM 的梯度的需求。其次，我们的方法减轻了特定任务的 LLM 重新训练的压力，因为在访问模型和计算浩瀚性方面存在限制。此外，我们将检索器的任务与理解者联系在一起，以避免幻想和减少不相关和可能有害的检索文档。我们认为这篇论文的研究议程具有潜在的影响力，可以广泛影响 AI 领域，使 LLM 的访问和利用更加普遍和便捷。
</details></li>
</ul>
<hr>
<h2 id="Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN"><a href="#Code-Switched-Urdu-ASR-for-Noisy-Telephonic-Environment-using-Data-Centric-Approach-with-Hybrid-HMM-and-CNN-TDNN" class="headerlink" title="Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN"></a>Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12759">http://arxiv.org/abs/2307.12759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sage-khan/code-switched-noisy-urdu-asr">https://github.com/sage-khan/code-switched-noisy-urdu-asr</a></li>
<li>paper_authors: Muhammad Danyal Khan, Raheem Ali, Arshad Aziz</li>
<li>for: 这个论文是为了提出一种资源有效的自动语音识别系统，以便在呼叫中心环境中准确地转录电话对话，并且能够进行自动化的电话监测、关键词搜索和情感分析。</li>
<li>methods: 这个论文使用了链式混合HMM和CNN-TDNN来实现资源有效的自动语音识别系统，并且在呼叫中心环境中进行了测试和评估。</li>
<li>results: 根据论文的描述，在呼叫中心环境中，使用链式混合HMM和CNN-TDNN来实现自动语音识别系统，可以达到5.2%的Word Error Rate（WER），包括干净环境和噪音环境下的 isolated words和连续杂音speech。<details>
<summary>Abstract</summary>
Call Centers have huge amount of audio data which can be used for achieving valuable business insights and transcription of phone calls is manually tedious task. An effective Automated Speech Recognition system can accurately transcribe these calls for easy search through call history for specific context and content allowing automatic call monitoring, improving QoS through keyword search and sentiment analysis. ASR for Call Center requires more robustness as telephonic environment are generally noisy. Moreover, there are many low-resourced languages that are on verge of extinction which can be preserved with help of Automatic Speech Recognition Technology. Urdu is the $10^{th}$ most widely spoken language in the world, with 231,295,440 worldwide still remains a resource constrained language in ASR. Regional call-center conversations operate in local language, with a mix of English numbers and technical terms generally causing a "code-switching" problem. Hence, this paper describes an implementation framework of a resource efficient Automatic Speech Recognition/ Speech to Text System in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. Using Hybrid HMM-DNN approach allowed us to utilize the advantages of Neural Network with less labelled data. Adding CNN with TDNN has shown to work better in noisy environment due to CNN's additional frequency dimension which captures extra information from noisy speech, thus improving accuracy. We collected data from various open sources and labelled some of the unlabelled data after analysing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English and were able to achieve WER of 5.2% with noisy as well as clean environment in isolated words or numbers as well as in continuous spontaneous speech.
</details>
<details>
<summary>摘要</summary>
Call Centers possess vast amounts of audio data that can be leveraged for gaining valuable business insights, and the manual transcription of phone calls is a tedious task. An effective Automatic Speech Recognition (ASR) system can accurately transcribe these calls, enabling easy search through call history for specific context and content, and allowing for automatic call monitoring, improving quality of service (QoS) through keyword search and sentiment analysis. However, ASR systems for call centers must be more robust due to the noisy telephonic environment. Moreover, there are many low-resource languages that are on the verge of extinction, and ASR technology can help preserve these languages. Urdu, the 10th most widely spoken language in the world with 231,295,440 speakers, remains a resource-constrained language in ASR. Regional call-center conversations often operate in local languages, with a mix of English and technical terms, causing a "code-switching" problem.To address these challenges, this paper proposes an implementation framework for a resource-efficient ASR/Speech-to-Text system in a noisy call-center environment using Chain Hybrid HMM and CNN-TDNN for Code-Switched Urdu Language. By combining Hybrid HMM-DNN and CNN-TDNN, we can leverage the advantages of neural networks with less labeled data. Additionally, the CNN-TDNN approach has shown to work better in noisy environments due to the CNN's additional frequency dimension, which captures extra information from noisy speech, improving accuracy.We collected data from various open sources and labeled some of the unlabeled data after analyzing its general context and content from Urdu language as well as from commonly used words from other languages, primarily English. Our results achieved a Word Error Rate (WER) of 5.2% with both noisy and clean environments in isolated words or numbers as well as in continuous spontaneous speech.
</details></li>
</ul>
<hr>
<h2 id="A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization"><a href="#A-Model-for-Every-User-and-Budget-Label-Free-and-Personalized-Mixed-Precision-Quantization" class="headerlink" title="A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization"></a>A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12659">http://arxiv.org/abs/2307.12659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Fish, Umberto Michieli, Mete Ozay</li>
<li>for: 这个研究是为了提高自动语音识别（ASR）模型的实时部署，以及实现个人化模型适应。</li>
<li>methods: 这个研究使用了混合精度优化方法（myQASR），可以根据不同的用户和内存需求生成个性化的混合精度优化方案。myQASR 使用了全精度活动值分析来自动评估网络层的优化感受，并生成个性化的混合精度优化方案。</li>
<li>results: 研究结果显示，使用 myQASR 可以提高特定的性别、语言和说话者的表现，并且不需要组数调整。<details>
<summary>Abstract</summary>
Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.
</details>
<details>
<summary>摘要</summary>
myQASR evaluates the quantization sensitivity of network layers by analyzing full-precision activation values, and generates a personalized mixed-precision quantization scheme for any pre-determined memory budget. Our results show that myQASR improves performance for specific genders, languages, and speakers.
</details></li>
</ul>
<hr>
<h2 id="Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey"><a href="#Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey" class="headerlink" title="Fake News Detection Through Graph-based Neural Networks: A Survey"></a>Fake News Detection Through Graph-based Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12639">http://arxiv.org/abs/2307.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris</li>
<li>for: 本研究主要旨在探讨基于图структуры和深度学习技术的假新闻检测方法。</li>
<li>methods: 图结构基于的方法包括知识驱动方法、传播基于方法和多元社交 контекст基于方法，它们根据不同的图结构来模型新闻相关信息的流动。</li>
<li>results: 研究发现，图结构基于方法在假新闻检测中得到了显著的成果，特别是在模elling社交媒体宣传过程中。但是，还存在一些挑战和未解决的问题，如假新闻的定义和识别、社交媒体平台的不同性和数据的可靠性等。<details>
<summary>Abstract</summary>
The popularity of online social networks has enabled rapid dissemination of information. People now can share and consume information much more rapidly than ever before. However, low-quality and/or accidentally/deliberately fake information can also spread rapidly. This can lead to considerable and negative impacts on society. Identifying, labelling and debunking online misinformation as early as possible has become an increasingly urgent problem. Many methods have been proposed to detect fake news including many deep learning and graph-based approaches. In recent years, graph-based methods have yielded strong results, as they can closely model the social context and propagation process of online news. In this paper, we present a systematic review of fake news detection studies based on graph-based and deep learning-based techniques. We classify existing graph-based methods into knowledge-driven methods, propagation-based methods, and heterogeneous social context-based methods, depending on how a graph structure is constructed to model news related information flows. We further discuss the challenges and open problems in graph-based fake news detection and identify future research directions.
</details>
<details>
<summary>摘要</summary>
在线社交网络的流行化使得信息的传播变得非常快速，人们可以更快地分享和消耗信息。然而，低质量和/或意外或故意假的信息也可以快速传播，这可能会对社会产生重大和负面的影响。正确地识别、标注和驳斥在线谣言已成为一项急需解决的问题。许多方法已经被提议来检测假新闻，其中包括深度学习和图基于的方法。在过去几年中，图基于的方法在检测假新闻方面取得了强劲的结果，因为它们可以准确地模拟在线新闻的社交上下文和传播过程。本文提供一个系统性的审查，检测基于图和深度学习的假新闻检测研究。我们将现有的图基于方法分为知识驱动的方法、传播基于方法和多元社交上下文基于方法，根据如何构建图来模型新闻相关信息的流动。我们还讨论了假新闻检测中的挑战和未解决的问题，并确定了未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Tachikuma-Understading-Complex-Interactions-with-Multi-Character-and-Novel-Objects-by-Large-Language-Models"><a href="#Tachikuma-Understading-Complex-Interactions-with-Multi-Character-and-Novel-Objects-by-Large-Language-Models" class="headerlink" title="Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models"></a>Tachikuma: Understading Complex Interactions with Multi-Character and Novel Objects by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12573">http://arxiv.org/abs/2307.12573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanzhi Liang, Linchao Zhu, Yi Yang<br>for:这篇论文旨在提高人工智能代理人在虚拟世界中的互动复杂性和灵活性，特别是在多个角色和新型对象的情况下。methods:该论文提出了在人工智能代理人世界模型中引入虚拟游戏主持人（GM）的想法，以增强信息把关、估计玩家的意图、提供环境描述和给予反馈等功能，从而补做当前世界模型的缺陷。results:该论文提出了一个名为Tachikuma的 benchmark，包括一个多个角色和新型对象基于互动 estimation（MOE）任务和一个相关的数据集。MOE挑战模型理解角色的意图并准确地确定他们在复杂情况下的行为。此外，数据集capture了在游戏即时通信中的实际交流记录，为未来的探索提供了多样、根据实际情况的复杂互动。最后，论文提出了一个简单的提示基线，并评估了其性能，示出其在促进互动理解方面的效果。<details>
<summary>Abstract</summary>
Recent advancements in natural language and Large Language Models (LLMs) have enabled AI agents to simulate human-like interactions within virtual worlds. However, these interactions still face limitations in complexity and flexibility, particularly in scenarios involving multiple characters and novel objects. Pre-defining all interactable objects in the agent's world model presents challenges, and conveying implicit intentions to multiple characters through complex interactions remains difficult. To address these issues, we propose integrating virtual Game Masters (GMs) into the agent's world model, drawing inspiration from Tabletop Role-Playing Games (TRPGs). GMs play a crucial role in overseeing information, estimating players' intentions, providing environment descriptions, and offering feedback, compensating for current world model deficiencies. To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations. Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.
</details>
<details>
<summary>摘要</summary>
To facilitate future explorations for complex interactions, we introduce a benchmark named Tachikuma, comprising a Multiple character and novel Object based interaction Estimation (MOE) task and a supporting dataset. MOE challenges models to understand characters' intentions and accurately determine their actions within intricate contexts involving multi-character and novel object interactions. Besides, the dataset captures log data from real-time communications during gameplay, providing diverse, grounded, and complex interactions for further explorations.Finally, we present a simple prompting baseline and evaluate its performance, demonstrating its effectiveness in enhancing interaction understanding. We hope that our dataset and task will inspire further research in complex interactions with natural language, fostering the development of more advanced AI agents.Translation in Simplified Chinese:最近的自然语言和大型语言模型（LLMs）的进步，使得AI代理人能够在虚拟世界中模拟人类化的互动。然而，这些互动仍面临复杂性和灵活性的限制，特别是在多个角色和新的物品的情况下。将所有互动的物品都嵌入代理人的世界模型中存在挑战，而且通过复杂的互动传递多个角色的意图仍然具有挑战性。为解决这些问题，我们提出了在代理人的世界模型中 integrate 虚拟游戏大师（GMs）的想法， draw  inspirations from 桌上角色扮演游戏（TRPGs）。GMs 在虚拟世界中扮演着重要的角色，负责资讯的监督、玩家的意图的估计、环境描述和回应，以补偿现有世界模型的不足。为了促进未来的复杂互动探索，我们提出了一个名为 Tachikuma 的benchmark，包括一个多个角色和新的物品基本互动Estimation（MOE）任务和一个支持 datasets。MOE 挑战模型能够理解角色的意图和精确地决定他们在复杂的多个角色和新的物品互动中的动作。此外， datasets  capture 游戏中的实时通讯记录，提供多样化、根据现实的互动进行探索。最后，我们提出了一个简单的提示基eline，评估其表现，证明其能够增强互动理解。我们希望这个dataset和任务能够鼓励更多的研究在复杂互动中的自然语言，推动更进步的 AI 代理人的发展。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalising-Neural-Topical-Representations"><a href="#Towards-Generalising-Neural-Topical-Representations" class="headerlink" title="Towards Generalising Neural Topical Representations"></a>Towards Generalising Neural Topical Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12564">http://arxiv.org/abs/2307.12564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohao Yang, He Zhao, Dinh Phung, Lan Du</li>
<li>for: 提高神经话题模型（NTM）的泛化能力，使其在不同文库和任务中产生质量话题表示。</li>
<li>methods: 使用数据扩充 durante el entrenamiento para模型 similar documents，并使用 Hierarchical Topic Transport Distance (HOTT) 测量文档之间的semantical distance。</li>
<li>results: 对多个NTMs进行了广泛的实验，并证明了框架可以significantly improve neural topical representation的泛化能力 across corpora。<details>
<summary>Abstract</summary>
Topic models have evolved from conventional Bayesian probabilistic models to Neural Topic Models (NTMs) over the last two decays. Although NTMs have achieved promising performance when trained and tested on a specific corpus, their generalisation ability across corpora is rarely studied. In practice, we often expect that an NTM trained on a source corpus can still produce quality topical representation for documents in a different target corpus without retraining. In this work, we aim to improve NTMs further so that their benefits generalise reliably across corpora and tasks. To do so, we propose to model similar documents by minimising their semantical distance when training NTMs. Specifically, similar documents are created by data augmentation during training; The semantical distance between documents is measured by the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be readily applied to most NTMs as a plug-and-play module. Extensive experiments show that our framework significantly improves the generalisation ability regarding neural topical representation across corpora.
</details>
<details>
<summary>摘要</summary>
To achieve this, we propose to model similar documents by minimizing their semantic distance during training. Specifically, we create similar documents by performing data augmentation during training, and we measure the semantic distance between documents using the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be easily applied to most NTMs as a plug-and-play module.Extensive experiments show that our framework significantly improves the generalization ability of neural topical representation across corpora.
</details></li>
</ul>
<hr>
<h2 id="Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation"><a href="#Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation" class="headerlink" title="Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation"></a>Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12520">http://arxiv.org/abs/2307.12520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelbhandari6/nmt_text_attack">https://github.com/neelbhandari6/nmt_text_attack</a></li>
<li>paper_authors: Neel Bhandari, Pin-Yu Chen</li>
<li>for: This paper aims to study the robustness of current text adversarial attacks to round-trip translation and to introduce an intervention-based solution to improve the robustness of adversarial examples.</li>
<li>methods: The paper uses six state-of-the-art text-based adversarial attacks and integrates machine translation into the process of adversarial example generation to improve the robustness of adversarial examples.</li>
<li>results: The paper demonstrates that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是研究当前文本 adversarial 攻击的翻译循环Robustness，并提出一种干预方法来提高攻击示例的Robustness。</li>
<li>methods: 论文使用了六种当前最佳文本基于攻击方法，并将机器翻译integrated into the process of adversarial example generation以提高攻击示例的Robustness。</li>
<li>results: 论文表明，找到可以在翻译中维持Robustness的攻击示例可以帮助发现语言模型的共同缺陷，并促进多语言攻击的研究。<details>
<summary>Abstract</summary>
Language Models today provide a high accuracy across a large number of downstream tasks. However, they remain susceptible to adversarial attacks, particularly against those where the adversarial examples maintain considerable similarity to the original text. Given the multilingual nature of text, the effectiveness of adversarial examples across translations and how machine translations can improve the robustness of adversarial examples remain largely unexplored. In this paper, we present a comprehensive study on the robustness of current text adversarial attacks to round-trip translation. We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation. Furthermore, we introduce an intervention-based solution to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation. Our results indicate that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.
</details>
<details>
<summary>摘要</summary>
现代语言模型在许多下游任务上具有高准确率，但它们仍然易受到恶意攻击，特别是那些保留了原文的相似性。由于文本的多语言特性，对翻译后的恶意攻击的效iveness和机器翻译如何提高恶意攻击的Robustness remains largely unexplored。在这篇论文中，我们提供了round-trip translation对当前文本恶意攻击的全面研究。我们发现了6种现状顶尖文本基于攻击不具有翻译后的效力。此外，我们还介绍了一种利用机器翻译的解决方案，通过将机器翻译 integrate into the process of generating adversarial examples，并证明了该方法可以提高恶意攻击的Robustness。我们的结果表明，找到可以抵抗翻译的恶意攻击可以帮助发现语言模型的共同缺陷，并促进更多的关于多语言恶意攻击的研究。
</details></li>
</ul>
<hr>
<h2 id="Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training"><a href="#Robust-Automatic-Speech-Recognition-via-WavAugment-Guided-Phoneme-Adversarial-Training" class="headerlink" title="Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training"></a>Robust Automatic Speech Recognition via WavAugment Guided Phoneme Adversarial Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12498">http://arxiv.org/abs/2307.12498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WAPATASR/WAPAT">https://github.com/WAPATASR/WAPAT</a></li>
<li>paper_authors: Gege Qi, Yuefeng Chen, Xiaofeng Mao, Xiaojun Jia, Ranjie Duan, Rong Zhang, Hui Xue</li>
<li>for: 提高自动话语识别（ASR）模型的实际抗坏性，使其在小量干扰和大范围频率变化下保持原有性能。</li>
<li>methods: 提出了一种新的WavAugment导向的phoneme adversarial Training（wapat）方法，通过在phoneme空间中使用对抗示例来使模型具有小量干扰和大范围频率变化下的抗坏性，并且通过使用振荡示例来引导对抗生成，以获得更好的普适性。</li>
<li>results: 在End-to-end Speech Challenge Benchmark（ESB）上进行了广泛的实验，结果表明，SpeechLM-wapat模型比原始模型减少了6.28%的Word Error Rate（WER），达到了新的状态态-of-the-art。<details>
<summary>Abstract</summary>
Developing a practically-robust automatic speech recognition (ASR) is challenging since the model should not only maintain the original performance on clean samples, but also achieve consistent efficacy under small volume perturbations and large domain shifts. To address this problem, we propose a novel WavAugment Guided Phoneme Adversarial Training (wapat). wapat use adversarial examples in phoneme space as augmentation to make the model invariant to minor fluctuations in phoneme representation and preserve the performance on clean samples. In addition, wapat utilizes the phoneme representation of augmented samples to guide the generation of adversaries, which helps to find more stable and diverse gradient-directions, resulting in improved generalization. Extensive experiments demonstrate the effectiveness of wapat on End-to-end Speech Challenge Benchmark (ESB). Notably, SpeechLM-wapat outperforms the original model by 6.28% WER reduction on ESB, achieving the new state-of-the-art.
</details>
<details>
<summary>摘要</summary>
开发一个实用robust的自动语音识别（ASR）系统是具有搅乱的挑战，因为模型需要不仅保持干净样本的原始性能，还需要在小量扰动和大域转换下实现一致的效果。为解决这个问题，我们提出了一种新的WavAugment导向的phoneme adversarial training（wapat）方法。wapat使用phoneme空间的对抗样本作为增强元素，使模型对phoneme表示的小变化具有抗衰减性，并保持干净样本的性能。此外，wapat利用增强后的phoneme表示导向对抗生成，以找到更稳定和多样的梯度方向，从而提高泛化能力。广泛的实验表明，wapat在End-to-end Speech Challenge Benchmark（ESB）上具有显著的效果，SpeechLM-wapat比原始模型减少6.28%的WRR，实现新的州际顶峰性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Offline-RL-for-Dialogue-Response-Generation"><a href="#On-the-Effectiveness-of-Offline-RL-for-Dialogue-Response-Generation" class="headerlink" title="On the Effectiveness of Offline RL for Dialogue Response Generation"></a>On the Effectiveness of Offline RL for Dialogue Response Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12425">http://arxiv.org/abs/2307.12425</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asappresearch/dialogue-offline-rl">https://github.com/asappresearch/dialogue-offline-rl</a></li>
<li>paper_authors: Paloma Sodhi, Felix Wu, Ethan R. Elenberg, Kilian Q. Weinberger, Ryan McDonald</li>
<li>for: 研究 teacher forcing 的替代方法，以提高对话响应生成的性能。</li>
<li>methods: 使用了多种离线束规学学习（RL）方法，以优化对话响应生成的序列水平目标。</li>
<li>results: 研究发现，离线RL可以明显提高对话响应生成的性能，而不会导致训练不稳定或减少实际训练时间。<details>
<summary>Abstract</summary>
A common training technique for language models is teacher forcing (TF). TF attempts to match human language exactly, even though identical meanings can be expressed in different ways. This motivates use of sequence-level objectives for dialogue response generation. In this paper, we study the efficacy of various offline reinforcement learning (RL) methods to maximize such objectives. We present a comprehensive evaluation across multiple datasets, models, and metrics. Offline RL shows a clear performance improvement over teacher forcing while not inducing training instability or sacrificing practical training budgets.
</details>
<details>
<summary>摘要</summary>
一种常见的语言模型训练技巧是教师强制（TF）。TF尝试匹配人类语言完全一致，即使同一个意思可以表达在不同的方式。这种motivation使我们使用序列级目标来生成对话响应。在这篇论文中，我们研究了多种离线强化学习（RL）方法，以最大化这些目标。我们在多个数据集、模型和指标上进行了全面的评估。离线RL显示了与教师强制相比的表现提升，而不会导致训练不稳定或浪费实际训练预算。
</details></li>
</ul>
<hr>
<h2 id="Testing-Hateful-Speeches-against-Policies"><a href="#Testing-Hateful-Speeches-against-Policies" class="headerlink" title="Testing Hateful Speeches against Policies"></a>Testing Hateful Speeches against Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12418">http://arxiv.org/abs/2307.12418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Jiangrui Zheng, Xueqing Liu, Girish Budhrani, Wei Yang, Ravishka Rathnasuriya</li>
<li>for: 这个论文的目的是研究基于深度学习技术的 AI 系统如何对于基于自然语言规则的需求或政策进行行为。</li>
<li>methods: 该论文使用了人工批准和 OpenAI 的大语言模型自动匹配新的示例和政策来扩展 HateModerate 数据集。</li>
<li>results: 研究发现现有的 hate speech 检测软件对于某些政策有高失败率，而自动匹配新的示例和政策可以提高 AI 系统对于需求或政策的traceability。<details>
<summary>Abstract</summary>
In the recent years, many software systems have adopted AI techniques, especially deep learning techniques. Due to their black-box nature, AI-based systems brought challenges to traceability, because AI system behaviors are based on models and data, whereas the requirements or policies are rules in the form of natural or programming language. To the best of our knowledge, there is a limited amount of studies on how AI and deep neural network-based systems behave against rule-based requirements/policies. This experience paper examines deep neural network behaviors against rule-based requirements described in natural language policies. In particular, we focus on a case study to check AI-based content moderation software against content moderation policies. First, using crowdsourcing, we collect natural language test cases which match each moderation policy, we name this dataset HateModerate; second, using the test cases in HateModerate, we test the failure rates of state-of-the-art hate speech detection software, and we find that these models have high failure rates for certain policies; finally, since manual labeling is costly, we further proposed an automated approach to augument HateModerate by finetuning OpenAI's large language models to automatically match new examples to policies. The dataset and code of this work can be found on our anonymous website: \url{https://sites.google.com/view/content-moderation-project}.
</details>
<details>
<summary>摘要</summary>
Recently, many software systems have adopted AI techniques, especially deep learning techniques. Due to their black-box nature, AI-based systems have brought challenges to traceability, as their behaviors are based on models and data, whereas the requirements or policies are rules in the form of natural or programming language. To the best of our knowledge, there is a limited amount of studies on how AI and deep neural network-based systems behave against rule-based requirements/policies. This experience paper examines deep neural network behaviors against rule-based requirements described in natural language policies. In particular, we focus on a case study to check AI-based content moderation software against content moderation policies. First, using crowdsourcing, we collect natural language test cases that match each moderation policy, which we name HateModerate; second, using the test cases in HateModerate, we test the failure rates of state-of-the-art hate speech detection software and find that these models have high failure rates for certain policies; finally, since manual labeling is costly, we further propose an automated approach to augment HateModerate by finetuning OpenAI's large language models to automatically match new examples to policies. The dataset and code of this work can be found on our anonymous website: [https://sites.google.com/view/content-moderation-project](https://sites.google.com/view/content-moderation-project).Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CommonsenseVIS-Visualizing-and-Understanding-Commonsense-Reasoning-Capabilities-of-Natural-Language-Models"><a href="#CommonsenseVIS-Visualizing-and-Understanding-Commonsense-Reasoning-Capabilities-of-Natural-Language-Models" class="headerlink" title="CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models"></a>CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning Capabilities of Natural Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12382">http://arxiv.org/abs/2307.12382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingbo Wang, Renfei Huang, Zhihua Jin, Tianqing Fang, Huamin Qu</li>
<li>for: 本研究旨在提供一个可视化的解释系统，以协助NLPT专家对模型对概念的关系进行可视化分析。</li>
<li>methods: 本研究使用了外部常识库来将模型的行为与人类知识相互对映，以提高模型的可视化解释。</li>
<li>results: 经过User Study，我们发现CommonsenseVIS可以帮助NLPT专家在不同情况下进行系统性和批量的可视化分析，从而更好地理解模型对概念的关系。<details>
<summary>Abstract</summary>
Recently, large pretrained language models have achieved compelling performance on commonsense benchmarks. Nevertheless, it is unclear what commonsense knowledge the models learn and whether they solely exploit spurious patterns. Feature attributions are popular explainability techniques that identify important input concepts for model outputs. However, commonsense knowledge tends to be implicit and rarely explicitly presented in inputs. These methods cannot infer models' implicit reasoning over mentioned concepts. We present CommonsenseVIS, a visual explanatory system that utilizes external commonsense knowledge bases to contextualize model behavior for commonsense question-answering. Specifically, we extract relevant commonsense knowledge in inputs as references to align model behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations. Through a user study, we show that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of models' relational reasoning over concepts in different situations.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we propose CommonsenseVIS, a visual explanatory system that leverages external common sense knowledge bases to contextualize model behavior for common sense question-answering. Specifically, we extract relevant common sense knowledge from inputs and use it to align the model's behavior with human knowledge. Our system features multi-level visualization and interactive model probing and editing for different concepts and their underlying relations.Through a user study, we demonstrate that CommonsenseVIS helps NLP experts conduct a systematic and scalable visual analysis of the models' relational reasoning over concepts in different situations. By providing a visual interface for exploring the models' behavior, CommonsenseVIS enables experts to gain a deeper understanding of how the models are using common sense knowledge to make predictions. This can help improve the models' performance and ensure that they are making accurate and informed decisions.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Emotional-Nuances-in-Dialogue-Summarization"><a href="#Evaluating-Emotional-Nuances-in-Dialogue-Summarization" class="headerlink" title="Evaluating Emotional Nuances in Dialogue Summarization"></a>Evaluating Emotional Nuances in Dialogue Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12371">http://arxiv.org/abs/2307.12371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxin Zhou, Fabien Ringeval, François Portet</li>
<li>for: 本研究旨在提高对人工对话的自动概要summarization，以保留对话中情感内容的信息。</li>
<li>methods: 本文提出了一组名为$PEmo$的衡量量表，用于衡量对话概要中情感内容的保留情况。</li>
<li>results: 研究发现，现有的概要模型不太好地保留对话中情感内容，而且通过减少训练集中不情感对话，可以更好地保留情感内容，同时保留最重要的事实信息。<details>
<summary>Abstract</summary>
Automatic dialogue summarization is a well-established task that aims to identify the most important content from human conversations to create a short textual summary. Despite recent progress in the field, we show that most of the research has focused on summarizing the factual information, leaving aside the affective content, which can yet convey useful information to analyse, monitor, or support human interactions. In this paper, we propose and evaluate a set of measures $PEmo$, to quantify how much emotion is preserved in dialog summaries. Results show that, summarization models of the state-of-the-art do not preserve well the emotional content in the summaries. We also show that by reducing the training set to only emotional dialogues, the emotional content is better preserved in the generated summaries, while conserving the most salient factual information.
</details>
<details>
<summary>摘要</summary>
自动对话摘要是一个已经成熟的任务，目的是从人类对话中提取最重要的内容，创建简短的文本摘要。尽管最近的进步在这个领域，但大多数研究仍然专注于摘要的事实信息，忽略了情感内容，这种内容可以带来有用的信息，分析、监测或支持人类交流。在这篇论文中，我们提出并评估了一组测量方法$PEmo$,以量化对话摘要中情感内容的保留程度。结果表明，现有的摘要模型并不能很好地保留对话中的情感内容。我们还表明，通过将训练集限制为只包含情感对话，可以更好地保留对话摘要中的情感内容，同时保留最重要的事实信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.CL_2023_07_24/" data-id="clpxp6by60095ee8804y6hjej" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.LG_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T10:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.LG_2023_07_24/">cs.LG - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network"><a href="#QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network" class="headerlink" title="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network"></a>QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12906">http://arxiv.org/abs/2307.12906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Md. Saiful Islam, Jungpil Shin, M. F. Mridha, Yuichi Okuyama</li>
<li>for: 这个研究的目的是为供应链管理系统提供精确的货物预测，以便优化存储控制、降低成本和提高顾客满意度。</li>
<li>methods: 本研究提出了一个新的方法olographical framework，使用量子灵感技术实现供应链货物预测，并且可以处理短期和不寻常的数据集。</li>
<li>results: 实验结果显示，QAmplifyNet模型在短期和不寻常的数据集上的预测效果比 класиical models、量子对集、量子神经网和深度强化学习模型更好。这个模型的可读性和可扩展性使其成为供应链管理中的理想解决方案。<details>
<summary>Abstract</summary>
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence techniques. Practical implications include improved inventory control, reduced backorders, and enhanced operational efficiency. QAmplifyNet seamlessly integrates into real-world supply chain management systems, enabling proactive decision-making and efficient resource allocation. Future work involves exploring additional quantum-inspired techniques, expanding the dataset, and investigating other supply chain applications. This research unlocks the potential of quantum computing in supply chain optimization and paves the way for further exploration of quantum-inspired machine learning models in supply chain management. Our framework and QAmplifyNet model offer a breakthrough approach to supply chain backorder prediction, providing superior performance and opening new avenues for leveraging quantum-inspired techniques in supply chain management.
</details>
<details>
<summary>摘要</summary>
供应链管理需要准确预测营销订单，以优化存储控制、降低成本和提高客户满意度。然而，传统的机器学习模型在大规模数据集和复杂关系下难以处理实际数据收集。本研究提出了一种新的方法olo Framework for Supply Chain Backorder Prediction，解决大数据集处理的挑战。我们的提议模型，QAmplifyNet，在短时间和不均衡数据集上预测营销订单非常有效。实验评估表明QAmplifyNet在经典模型、量子ensemble、量子神经网络和深度强化学习方面具有突出的优势。由于它可以处理短时间和不均衡数据集，因此在供应链管理中是一个 идеal的解决方案。为了提高模型可读性，我们使用了可解释人工智能技术。实际应用包括改善存储控制、减少营销订单和提高运营效率。QAmplifyNet可以轻松整合到实际供应链管理系统中，允许执行投入式决策和有效资源分配。未来的工作包括探索更多的量子静止技术、扩大数据集和探索其他供应链应用。本研究开启了量子计算在供应链优化中的潜力，为了 leveraging量子静止机器学习模型在供应链管理中提供了一个突破性的方法。我们的框架和QAmplifyNet模型为供应链营销订单预测提供了超越性能，开创了新的可能性，以及可以在供应链管理中应用量子静止技术。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs"><a href="#Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs" class="headerlink" title="Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs"></a>Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12904">http://arxiv.org/abs/2307.12904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Gonon, Antoine Jacquier</li>
<li>for: 这个论文是为了证明Quantum Neural Network可以用于精确地预测函数的目的。</li>
<li>methods: 这篇论文使用了Parameterised quantum circuits和随机量子Circuits来 aproximate classical functions。</li>
<li>results: 这篇论文提供了具体的错误 bound，证明一个Quantum Neural Network可以在某些情况下以 $\mathcal{O}(\varepsilon^{-2})$ 的参数和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ 个量子比特来实现精度 $\varepsilon&gt;0$ 的函数预测。<details>
<summary>Abstract</summary>
Universal approximation theorems are the foundations of classical neural networks, providing theoretical guarantees that the latter are able to approximate maps of interest. Recent results have shown that this can also be achieved in a quantum setting, whereby classical functions can be approximated by parameterised quantum circuits. We provide here precise error bounds for specific classes of functions and extend these results to the interesting new setup of randomised quantum circuits, mimicking classical reservoir neural networks. Our results show in particular that a quantum neural network with $\mathcal{O}(\varepsilon^{-2})$ weights and $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve accuracy $\varepsilon>0$ when approximating functions with integrable Fourier transform.
</details>
<details>
<summary>摘要</summary>
“ universal approximation 定理 是 classical neural network 的基础，提供了理论保证这些 later 能够 approximate  interessant 的映射。 recent results 表明这也可以在量子设置下实现，其中 classical 函数可以通过参数化 quantum circuit 的方式进行approximation。我们在这里提供了具体的误差 bound  для特定的函数类型，并将其扩展到 randomized quantum circuit 中，模拟 classical reservoir neural network。我们的结果显示，一个 quantum neural network  WITH $\mathcal{O}(\varepsilon^{-2})$  weights 和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits 就能够达到 $\varepsilon>0$ 的精度，当approximating functions with integrable Fourier transform。”Note: "Simplified Chinese" is a romanization of Chinese that uses simpler characters and grammar to facilitate typing and reading. It is not a standardized translation of Chinese, and the actual translation may vary depending on the context and the translator's preference.
</details></li>
</ul>
<hr>
<h2 id="Anytime-Model-Selection-in-Linear-Bandits"><a href="#Anytime-Model-Selection-in-Linear-Bandits" class="headerlink" title="Anytime Model Selection in Linear Bandits"></a>Anytime Model Selection in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12897">http://arxiv.org/abs/2307.12897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, Andreas Krause</li>
<li>for:  This paper is written for solving the problem of model selection in the context of bandit optimization, which is a challenging problem that requires balancing exploration and exploitation not only for action selection, but also for model selection.</li>
<li>methods:  The paper proposes a new method called ALEXP, which uses online learning algorithms that treat different models as experts and emulates full-information feedback to the online learner with a favorable bias-variance trade-off.</li>
<li>results:  The paper shows that ALEXP has an exponentially improved ($\log M$) dependence on the number of models $M$ for its regret, and has anytime guarantees on its regret without requiring knowledge of the horizon $n$ or relying on an initial purely exploratory stage.<details>
<summary>Abstract</summary>
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details>
<details>
<summary>摘要</summary>
Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved dependence on $M$ for its regret, with a logarithmic dependence on $M$ (log$M$).ALEXP has anytime guarantees on its regret, and does not require knowledge of the horizon $n$ or an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details></li>
</ul>
<hr>
<h2 id="A-Statistical-View-of-Column-Subset-Selection"><a href="#A-Statistical-View-of-Column-Subset-Selection" class="headerlink" title="A Statistical View of Column Subset Selection"></a>A Statistical View of Column Subset Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12892">http://arxiv.org/abs/2307.12892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anavsood/css">https://github.com/anavsood/css</a></li>
<li>paper_authors: Anav Sood, Trevor Hastie</li>
<li>for: 选择一小集合的表变量从大数据集中</li>
<li>methods: 使用某些简统统计方法，如某些简单的概率模型，来实现维度减少</li>
<li>results: 该paper表明了CSS和主要变量选择是等价的，并且两者都可以视为最大化信息的最优化问题。此外，paper还介绍了如何使用这些连接来快速完成CSS，包括使用摘要统计数据进行CSS和在缺失和&#x2F;或抑制数据的情况下进行CSS。<details>
<summary>Abstract</summary>
We consider the problem of selecting a small subset of representative variables from a large dataset. In the computer science literature, this dimensionality reduction problem is typically formalized as Column Subset Selection (CSS). Meanwhile, the typical statistical formalization is to find an information-maximizing set of Principal Variables. This paper shows that these two approaches are equivalent, and moreover, both can be viewed as maximum likelihood estimation within a certain semi-parametric model. Using these connections, we show how to efficiently (1) perform CSS using only summary statistics from the original dataset; (2) perform CSS in the presence of missing and/or censored data; and (3) select the subset size for CSS in a hypothesis testing framework.
</details>
<details>
<summary>摘要</summary>
我团队正在考虑一个大数据集中选择一小子集的代表变量问题。在计算机科学文献中，这个维度减少问题通常被称为列子集选择（CSS）。在统计学文献中，这个问题通常被формализова为找到最优化信息的主要变量。这篇论文表明了这两种方法是等价的，并且它们都可以被视为在某种半 parametic 模型中的最大 LIKELIHOOD估计。使用这些连接，我们展示了如何：1. 使用原始数据集的摘要统计来快速完成 CSS。2. 在缺失和/或截断数据存在的情况下进行 CSS。3. 在假设检测框架中选择 CSS 中的子集大小。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Stereotype-Identification-through-Reasoning"><a href="#Interpretable-Stereotype-Identification-through-Reasoning" class="headerlink" title="Interpretable Stereotype Identification through Reasoning"></a>Interpretable Stereotype Identification through Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00071">http://arxiv.org/abs/2308.00071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak</li>
<li>for: 这篇研究的目的是探讨语言模型中的偏见，并将公平性集成到语言模型的开发过程中，以确保这些模型是不偏不倾的。</li>
<li>methods: 本研究使用了Vicuna-13B-v1.3进行零扩展 sterotype 识别 tasks，并 comparing the performance of scaling up from 13B to 33B 和 reasoning 的效果。</li>
<li>results: 研究发现，reasoning 可以帮助语言模型在离 Domain tasks 中提高精度，并且可以增加模型的解释力。<details>
<summary>Abstract</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details>
<details>
<summary>摘要</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.Here's the translation in Traditional Chinese: given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details></li>
</ul>
<hr>
<h2 id="Data-free-Black-box-Attack-based-on-Diffusion-Model"><a href="#Data-free-Black-box-Attack-based-on-Diffusion-Model" class="headerlink" title="Data-free Black-box Attack based on Diffusion Model"></a>Data-free Black-box Attack based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12872">http://arxiv.org/abs/2307.12872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo</li>
<li>for: 本研究旨在提高数据free黑盒攻击的效率和准确性，通过使用扩散模型生成数据进行训练代理模型。</li>
<li>methods: 本研究使用扩散模型生成数据，并提出了一种干扰代码增强（LCA）方法，以指导扩散模型生成数据。LCA方法可以使得生成的数据符合目标模型的批判标准，同时保持高的多样性。</li>
<li>results: 对于不同的目标模型，我们的LCA方法可以获得更高的攻击成功率，并且需要更少的查询预算。EXTensive experiments表明，我们的LCA方法可以提高数据free黑盒攻击的效率和准确性。<details>
<summary>Abstract</summary>
Since the training data for the target model in a data-free black-box attack is not available, most recent schemes utilize GANs to generate data for training substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training. Despite the data generated by the diffusion model exhibits high quality, it presents diverse domain distributions and contains many samples that do not meet the discriminative criteria of the target model. To further facilitate the diffusion model to generate data suitable for the target model, we propose a Latent Code Augmentation (LCA) method to guide the diffusion model in generating data. With the guidance of LCA, the data generated by the diffusion model not only meets the discriminative criteria of the target model but also exhibits high diversity. By utilizing this data, it is possible to train substitute model that closely resemble the target model more efficiently. Extensive experiments demonstrate that our LCA achieves higher attack success rates and requires fewer query budgets compared to GANs-based schemes for different target models.
</details>
<details>
<summary>摘要</summary>
因为目标模型的训练数据不可获得，大多数最新的方案使用GANs生成数据来训练代理模型。然而，这些GANs基于的方案受到低训练效率和低生成质量的限制。为了突破这些限制，我们考虑使用扩散模型生成数据，并提出了基于扩散模型的数据 свобо black-box攻击方案，以提高代理训练的效率和准确性。尽管扩散模型生成的数据具有高质量，但它们具有多样的领域分布和含有许多不符合目标模型的扩散标准的样本。为了使扩散模型更加适合目标模型，我们提出了幽默代码修饰（LCA）方法，以导引扩散模型生成数据。通过LCA的引导，扩散模型生成的数据不仅满足目标模型的扩散标准，而且具有高多样性。通过这些数据，我们可以更加快速地训练符合目标模型的代理模型。我们的LCA在不同的目标模型上实现了更高的攻击成功率和更少的查询预算。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs"><a href="#Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs" class="headerlink" title="Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)"></a>Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12862">http://arxiv.org/abs/2307.12862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helal El-Zaatari, Fei Yu, Michael R Kosorok</li>
<li>for: 这 paper 的目的是提供一种改进的 exponential random graph models (ERGMs) 模型，以更好地Capture 社交网络中的依赖关系。</li>
<li>methods: 该 paper 使用了一种新的方法，即选择内生变量（endogenous variable selection），以解决 ERGMs 中的degeneracy问题，并提高了网络模型的准确性。</li>
<li>results: 经验测试表明，该方法可以有效地避免 ERGMs 中的degeneracy问题，并提高了网络模型的准确性和可靠性。<details>
<summary>Abstract</summary>
Statistical analysis of social networks provides valuable insights into complex network interactions across various scientific disciplines. However, accurate modeling of networks remains challenging due to the heavy computational burden and the need to account for observed network dependencies. Exponential Random Graph Models (ERGMs) have emerged as a promising technique used in social network modeling to capture network dependencies by incorporating endogenous variables. Nevertheless, using ERGMs poses multiple challenges, including the occurrence of ERGM degeneracy, which generates unrealistic and meaningless network structures. To address these challenges and enhance the modeling of collaboration networks, we propose and test a novel approach that focuses on endogenous variable selection within ERGMs. Our method aims to overcome the computational burden and improve the accommodation of observed network dependencies, thereby facilitating more accurate and meaningful interpretations of network phenomena in various scientific fields. We conduct empirical testing and rigorous analysis to contribute to the advancement of statistical techniques and offer practical insights for network analysis.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)社交网络统计分析提供了许多有价值的网络互动现象的理解，但是准确地模型网络仍然是一项挑战，因为计算负担重要和需要考虑观察到的网络依赖关系。扩展随机图模型（ERGMs）在社交网络模型中表现出了扩展的潜力，可以通过包含内生变量来捕捉网络依赖关系。然而，使用ERGMs也存在多种挑战，包括ERGM异常性，这会生成无意义和不切实际的网络结构。为了解决这些挑战并改进协作网络的模型，我们提出了一种新的方法，即内生变量选择在ERGMs中。我们的方法目的是减少计算负担和更好地考虑观察到的网络依赖关系，从而为不同科学领域中的网络现象提供更准确和有意义的解释。我们进行了实际测试和严格分析，以贡献到统计技术的进步和为网络分析提供实用的指导。
</details></li>
</ul>
<hr>
<h2 id="A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis"><a href="#A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis" class="headerlink" title="A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"></a>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12856">http://arxiv.org/abs/2307.12856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</li>
<li>for: 本研究旨在提高自主浏览器的自然语言指令下的实际website上的性能。</li>
<li>methods: 研究人员提出了WebAgent Agent，该Agent可以根据自然语言指令完成实际website上的任务。WebAgent使用Flan-U-PaLM进行code生成和HTML-T5进行规划和摘要，以及使用本地和全局听力机制和杂xture-span噪声目标来解决HTML文档中的长 Span问题。</li>
<li>results: 研究人员通过实验表明，WebAgent在真实的website上提高了成功率超过50%，并且HTML-T5在解决HTML基本任务方面的成功率高于之前的SoTA，达到14.9%。<details>
<summary>Abstract</summary>
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that can complete the tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via generated Python programs from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our recipe improves the success on a real website by over 50%, and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9% higher success rate than prior SoTA on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details>
<details>
<summary>摘要</summary>
各种大型自然语言模型（LLM）在自主网络浏览方面最近很有进步，但实际网站上的性能仍然受到以下三种因素的影响：开放性、限制的上下文长度和HTML的适应性。我们介绍了WebAgent，一个根据自然语言指令完成实际网站上的任务的LML-驱动的代理人。WebAgent通过将指令分解成标准化的子指令、将长HTML文档摘要成任务相关的短报道，并通过生成的Python程序来操作网站。我们为WebAgent设计了Flan-U-PaLM，用于锚定代码生成，以及HTML-T5，一种新的适应HTML文档的预训练语言模型，使用本地和全球注意力机制，并结合长时间排除目标来计划和摘要。我们实际证明了我们的配方可以在真实网站上提高成功率高于50%，并且HTML-T5是解决HTML基本任务的最佳模型，比前一个SoTA在小型网站浏览 benchmark上的成功率高14.9%。
</details></li>
</ul>
<hr>
<h2 id="Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization"><a href="#Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization" class="headerlink" title="Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization"></a>Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12851">http://arxiv.org/abs/2307.12851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hancheng Min, René Vidal, Enrique Mallada</li>
<li>for: 这 paper 研究了使用梯度流的两层 ReLU 网络进行二分类训练，并且使用小初值。</li>
<li>methods: 我们分析了在训练集中的输入向量之间的相互关系，并且通过对神经元的方向动态进行仔细分析，提供了 $\mathcal{O}(\frac{\log n}{\sqrt{\mu})$ 上限 bounds on 训练时间，其中 $n$ 是数据点的数量，$\mu$ 是数据点之间的相互关系。</li>
<li>results: 我们的分析表明，在训练的早期阶段，神经元在第一层会尝试与输入数据进行对齐，并且在训练的晚期阶段，损失函数会逐渐逼近零，并且第一层的权重矩阵会变得相对低矩。数据实验表明了我们的理论发现。<details>
<summary>Abstract</summary>
This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文研究了使用梯度流的两层ReLU网络 для二分类问题的训练，并采用小初始化。我们考虑了一个具有良好分离的训练集：任何同样标签的输入数据对都是正相关的，任何不同标签的输入数据对都是负相关的。我们的分析表明，在训练的早期阶段，第一层神经元尝试与输入数据进行对齐，具体来说，它们会对应于第二层神经元的权重进行对齐。通过仔细分析神经元的方向动态，我们可以提供一个 $\mathcal{O}(\frac{\log n}{\sqrt{\mu})$ 上限于时间内所有神经元达到好的对齐度，其中 $n$ 是数据点的数量，$\mu$ 是数据分离度。训练过程后期，损失函数会随着时间的增长而逐渐减少，并且第一层神经元的权重矩阵会 aproximately 变为低级 matrix。实验结果表明，这些理论发现都能够在 MNIST 数据集上得到支持。
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials"><a href="#Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials" class="headerlink" title="Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials"></a>Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12840">http://arxiv.org/abs/2307.12840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Daniel M. Kane</li>
<li>for: 学习一个 linear combination of $k$ ReLU 活化器在标准 Gaussian 分布上的 $\mathbb{R}^d$ 上的问题，使用标准损失函数。</li>
<li>methods: 使用 tensor  decomposition 技术来识别一个子空间，使得所有 $O(k)$-order  моменты在正交方向上都很小。</li>
<li>results: 提供了一个高效的算法，其复杂度为 $(dk&#x2F;\epsilon)^{O(k)}$, 比之前的算法更为优化。<details>
<summary>Abstract</summary>
We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
</details>
<details>
<summary>摘要</summary>
我们研究一个PAC学习问题，即以$k$个ReLU激活函数为线性结构，在标准 Gaussian 分布下的 $\mathbb{R}^d$ 上对对于方差损失函数进行学习。我们的主要结果是一个有效的学习算法，其sample和computational Complexity为 $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ 是目标精度。对比之下，先前的算法的复杂度为 $(dk/\epsilon)^{h(k)}$, where $h(k)$  scales 超 polynomial 地增长。有趣的是，我们的算法的复杂度几乎是near-optimal  within the class of Correlational Statistical Query algorithms。在高阶概念上，我们的算法使用了维度分解来识别一个子空间，使得所有 $O(k)$-order moments 在这个orthogonal direction 上都是小的。其分析将使用Schur多项式理论来显示，在这个子空间上，更高阶的error tensors 是小的，只要Lower-order ones 是。
</details></li>
</ul>
<hr>
<h2 id="Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering"><a href="#Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering" class="headerlink" title="Learning Provably Robust Estimators for Inverse Problems via Jittering"></a>Learning Provably Robust Estimators for Inverse Problems via Jittering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12822">http://arxiv.org/abs/2307.12822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mli-lab/robust_reconstructors_via_jittering">https://github.com/mli-lab/robust_reconstructors_via_jittering</a></li>
<li>paper_authors: Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</li>
<li>for: 这篇论文 investigate whether jittering, a simple regularization technique, can be used to train deep neural networks to be worst-case robust for inverse problems.</li>
<li>methods: 论文使用了jittering regularization technique during training, and presents a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising.</li>
<li>results: 研究发现，jittering可以增强worst-case robustness，但可能不适用于 inverse problems beyond denoising。同时，论文还发现，使用实际数据进行训练可以提供一定的 robustness enhancement.<details>
<summary>Abstract</summary>
Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.
</details>
<details>
<summary>摘要</summary>
深度神经网络在反向问题中表现出色，但是神经网络可能对抗性或最坏情况的扰动敏感。这引起了训练神经网络是否可以有效地培养最坏情况Robust的问题。在这篇论文中，我们调查了在反向问题中是否可以使用扰动，一种简单的规范技术，来学习最坏情况Robust的估计器。虽然在预测类型任务中well studied，但是反向问题中扰动的效iveness尚未系统地研究。在这篇论文中，我们提供了一种新的分析 Characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising，并证明了扰动可以生成最优的Robust denoiser。此外，我们通过训练深度神经网络（U-net）对自然图像杂谔、减 convolution和加速核磁共振成像（MRI）进行实验。结果表明，扰动可以强化最坏情况的Robust性，但可能不适用于反向问题 beyond denoising。此外，我们的结果也表明，训练在真实数据上，通常含有些许噪声，可以提高Robust性。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks"><a href="#Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks" class="headerlink" title="Maximal Independent Sets for Pooling in Graph Neural Networks"></a>Maximal Independent Sets for Pooling in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13011">http://arxiv.org/abs/2307.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stevan Stanovic, Benoit Gaüzère, Luc Brun</li>
<li>for: 图像分类领域的进步</li>
<li>methods: 基于最大独立集的三种图像池化方法</li>
<li>results: 实验结果证明最大独立集约束对图像池化有 relevance<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNNs）已经为图像分类带来重要的进步，通过卷积和聚合。特别是图像聚合将连接的离散网络转换为减少的网络，让减少函数考虑整个图像中的所有像素。然而，为图集而设计的pooling方法存在一些缺点，包括图集分离或过度连接、低减少比率和删除大量图集。在这篇论文中，我们提出了基于最大独立集的三种pooling方法，避免了这些缺点。我们的实验结果证明了最大独立集约束对图集聚合具有重要性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions"><a href="#Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions" class="headerlink" title="Causal Fair Machine Learning via Rank-Preserving Interventional Distributions"></a>Causal Fair Machine Learning via Rank-Preserving Interventional Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12797">http://arxiv.org/abs/2307.12797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/paper_2023_cfml">https://github.com/slds-lmu/paper_2023_cfml</a></li>
<li>paper_authors: Ludwig Bothmann, Susanne Dandl, Michael Schomaker</li>
<li>for: 该论文旨在设计机器学习模型，以减少自动决策系统中的不公正性。</li>
<li>methods: 该论文提出了一种基于 causal thinking 的方法，通过引入保护属性来定义个体是否是normatively equal。该方法使用rank-preserving interventional distributions来定义一个FiND世界，并使用扭曲方法进行估计。</li>
<li>results: 该论文通过实验和实际数据 validate了该方法和模型的评价标准，并显示了该方法能够减少不公正性。<details>
<summary>Abstract</summary>
A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
</details>
<details>
<summary>摘要</summary>
一个决策可以被定义为公平的，如果对等的人进行对等的待遇，不同的人则不同的待遇。在设计自动化决策系统中减少不公的方面，我们应该采用 causal 思维。根据最近的提议，我们定义了一个人为在一个虚拟、normatively 愿望的 (FiND) 世界中是否是等值的。我们提议使用排名保持分布来定义这个FiND世界的估计量，并使用扭曲方法进行估计。我们对方法和模型的评价标准和验证结果进行了说明和验证，并通过实验数据和仿真数据来显示我们的扭曲方法能够有效地找到最受歧视的个体并减少不公。
</details></li>
</ul>
<hr>
<h2 id="Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification"><a href="#Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification" class="headerlink" title="Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification"></a>Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12790">http://arxiv.org/abs/2307.12790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonrepo-keeper/gcnn-ec">https://github.com/anonrepo-keeper/gcnn-ec</a></li>
<li>paper_authors: Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</li>
<li>for: 这个研究探索了图形神经网络（Graph Neural Network，GNN）在医疗影像分类中的潜力。</li>
<li>methods: 我们提出了一种新的模型，具有结合图形神经网络和边弹击的特点，通过RGB通道特征值之间的连接强化关系，实现更好地表示关键图形节点之间的连接。</li>
<li>results: 我们的模型不仅与现有的深度神经网络（Deep Neural Network，DNN）相比，表现优化，且仅需1000个参数，训练时间和数据需求都被降低了。我们将这个GCNN模型与预训练的DNN进行比较，发现GCNN在医疗影像分类任务中表现出色，并鼓励进一步探索更进阶的图形基于模型，如图形注意力网络（Graph Attention Network，GAT）和图形自动编码器（Graph Auto-Encoder）在医疗影像领域的应用。<details>
<summary>Abstract</summary>
Graph-based neural network models are gaining traction in the field of representation learning due to their ability to uncover latent topological relationships between entities that are otherwise challenging to identify. These models have been employed across a diverse range of domains, encompassing drug discovery, protein interactions, semantic segmentation, and fluid dynamics research. In this study, we investigate the potential of Graph Neural Networks (GNNs) for medical image classification. We introduce a novel model that combines GNNs and edge convolution, leveraging the interconnectedness of RGB channel feature values to strongly represent connections between crucial graph nodes. Our proposed model not only performs on par with state-of-the-art Deep Neural Networks (DNNs) but does so with 1000 times fewer parameters, resulting in reduced training time and data requirements. We compare our Graph Convolutional Neural Network (GCNN) to pre-trained DNNs for classifying MedMNIST dataset classes, revealing promising prospects for GNNs in medical image analysis. Our results also encourage further exploration of advanced graph-based models such as Graph Attention Networks (GAT) and Graph Auto-Encoders in the medical imaging domain. The proposed model yields more reliable, interpretable, and accurate outcomes for tasks like semantic segmentation and image classification compared to simpler GCNNs
</details>
<details>
<summary>摘要</summary>
“基于图的神经网络模型在知识学习领域受到广泛应用，因为它们可以捕捉难以识别的实体之间的隐藏 topological 关系。这些模型在药物发现、蛋白质交互、semantic segmentation 和 fluid dynamics 等领域中得到应用。在本研究中，我们调查了医学图像分类中的可能性，并提出了一种新的模型，该模型将基于图的神经网络（GCNN）和边 convolution 结合在一起，通过RGB通道特征值之间的连接来强大地表示关键图节点之间的连接。我们的提出的模型不仅与现有的深度神经网络（DNN）性能相似，而且具有1000倍少的参数，从而减少了训练时间和数据需求。我们对MedMNIST 数据集类别进行比较，发现GCNN在医学图像分类中有良好的前景，并且鼓励进一步探索更高级的图基于模型，如图注意力网络（GAT）和图自动编码器（GAE）在医学图像分类领域。GCNN 模型在 semantic segmentation 和图像分类任务中提供了更可靠、可解释、高精度的结果，相比于简单的 GCNN 模型”
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer"><a href="#Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer" class="headerlink" title="Deep neural network improves the estimation of polygenic risk scores for breast cancer"></a>Deep neural network improves the estimation of polygenic risk scores for breast cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13010">http://arxiv.org/abs/2307.13010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Badré, Li Zhang, Wellington Muchero, Justin C. Reynolds, Chongle Pan<br>for: 这个研究用于比较多种计算模型来计算乳腺癌风险分数（PRS）。methods: 这个研究使用了深度神经网络（DNN）和其他机器学习技术以及统计学方法，包括BLUP、BayesA和LDpred。results: DNN在测试群体中表现出色，其AUC为67.4%，比其他方法高。此外，DNN还能够分化患者群体，并且可以达到18.8%的感知率 при 90%的准确率。这些结果表明，DNN可以更好地预测乳腺癌风险。<details>
<summary>Abstract</summary>
Polygenic risk scores (PRS) estimate the genetic risk of an individual for a complex disease based on many genetic variants across the whole genome. In this study, we compared a series of computational models for estimation of breast cancer PRS. A deep neural network (DNN) was found to outperform alternative machine learning techniques and established statistical algorithms, including BLUP, BayesA and LDpred. In the test cohort with 50% prevalence, the Area Under the receiver operating characteristic Curve (AUC) were 67.4% for DNN, 64.2% for BLUP, 64.5% for BayesA, and 62.4% for LDpred. BLUP, BayesA, and LPpred all generated PRS that followed a normal distribution in the case population. However, the PRS generated by DNN in the case population followed a bi-modal distribution composed of two normal distributions with distinctly different means. This suggests that DNN was able to separate the case population into a high-genetic-risk case sub-population with an average PRS significantly higher than the control population and a normal-genetic-risk case sub-population with an average PRS similar to the control population. This allowed DNN to achieve 18.8% recall at 90% precision in the test cohort with 50% prevalence, which can be extrapolated to 65.4% recall at 20% precision in a general population with 12% prevalence. Interpretation of the DNN model identified salient variants that were assigned insignificant p-values by association studies, but were important for DNN prediction. These variants may be associated with the phenotype through non-linear relationships.
</details>
<details>
<summary>摘要</summary>
多因素风险分数（PRS）用于估计个体复杂疾病的遗传风险，基于整个基因组中的多个遗传变异。本研究比较了多种计算模型来估计乳腺癌PRS。深度神经网络（DNN）被发现超过了其他机器学习技术和确立的统计算法，包括BLUP、BayesA和LDpred。在测试群中的50%预测率下，DNN的AUC分数为67.4%，BLUP的AUC分数为64.2%，BayesA的AUC分数为64.5%，LDpred的AUC分数为62.4%。BLUP、BayesA和LPpred都生成的PRS在正例群中遵循正态分布。然而，DNN在疾病群中生成的PRS遵循了二元分布，由两个正态分布组成，其中一个分布的mean值明显高于控制群的mean值，另一个分布的mean值与控制群的mean值类似。这表明DNN能够将疾病群分为高遗传风险子群和正常遗传风险子群，其中高遗传风险子群的PRS平均值明显高于控制群的PRS，而正常遗传风险子群的PRS与控制群的PRS类似。这使得DNN在50%预测率下实现了18.8%的回归率，在20%预测率下可以扩展到65.4%的回归率。DNN模型的解释发现了一些被关键性评估为无关的变异，但对DNN预测是重要的。这些变异可能与现象之间存在非线性关系。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine"><a href="#Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine" class="headerlink" title="Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine"></a>Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12788">http://arxiv.org/abs/2307.12788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominique Geissler, Stefan Feuerriegel</li>
<li>for: 这个研究旨在分析2022年俄罗斯入侵乌克兰的社交媒体宣传活动的战略。</li>
<li>methods: 这个研究使用反强化学习（IRL）方法来分析社交媒体上的宣传行为。</li>
<li>results: 研究发现，负面宣传的机器人和人类用户采取不同策略：机器人主要回应支持入侵的消息，而人类用户主要回应反对消息，这表明机器人寻求把消息推广，而人类用户更倾向于进行批评讨论。<details>
<summary>Abstract</summary>
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details>
<details>
<summary>摘要</summary>
俄罗斯入侵乌克兰的2022年社交媒体宣传活动拥有了大规模的俄罗斯支持者宣传运动。然而，这些宣传的执行策略仍然不清楚，特别是在线媒体互动如何被推动者社区战略性地形成。在这里，我们使用反向强化学习（IRL）方法来分析推特社区的策略。具体来说，IRL允许我们将在线行为视为Markov决策过程，其中的目标是推断推特用户在与支持或反对入侵的用户互动时的奖励结构。由此，我们希望理解在线互动是如何被推动者用于推广俄罗斯宣传的。为此，我们利用了349,455条推特文章和132,131名用户的大规模数据集。我们发现，机器人和人类采取不同策略：机器人主要回应支持入侵的消息，表明它们想要驱动病毒性;而表达反对的消息主要引起人类的回应，表明人类更倾向于进行批评讨论。根据我们所知，这是第一篇分析2022年俄罗斯入侵乌克兰宣传策略的IRL研究。
</details></li>
</ul>
<hr>
<h2 id="Is-attention-all-you-need-in-medical-image-analysis-A-review"><a href="#Is-attention-all-you-need-in-medical-image-analysis-A-review" class="headerlink" title="Is attention all you need in medical image analysis? A review"></a>Is attention all you need in medical image analysis? A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12775">http://arxiv.org/abs/2307.12775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Papanastasiou, Nikolaos Dikaios, Jiahao Huang, Chengjia Wang, Guang Yang</li>
<li>For: This paper reviews and analyzes existing hybrid CNN-Transf&#x2F;Attention models for medical image analysis (MIA) problems, and discusses their generalization opportunities for scientific and clinical impact.* Methods: The paper uses a comprehensive analysis framework to evaluate the architectural designs, breakthroughs, and opportunities of hybrid CNN-Transf&#x2F;Attention models in MIA.* Results: The paper provides a systematic review of existing hybrid CNN-Transf&#x2F;Attention models, and discusses their strengths and limitations in terms of generalization ability and clinical impact.<details>
<summary>Abstract</summary>
Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.
</details>
<details>
<summary>摘要</summary>
医疗影像是诊断、治疗规划和临床试验设计中的关键组成部分，占健康保健数据的大约90%。过去几年，深度学习（CNN）在医疗影像分析（MIA）中获得了性能提升。CNN可以高效地模型影像中的局部像素互动，并可以在小规模的MI数据上进行训练。然而，典型的CNN模型忽略了影像中的全局像素关系，这限制了它们的泛化能力，不能理解不同的全局信息。随着人工智能的发展，转换器（Transformers）在数据中学习全局关系的能力得到了提升。然而，全Transformers模型需要大规模的训练数据和巨大的计算复杂度。为了维护模型的全局性和可扩展性，人们提出了Attention和Transformers组件（Transf/Attention）。最近几年， hybrid CNN-Transf/Attention模型在多个MIA问题上得到了广泛应用。在这篇系统评影卷中，我们对现有的hybrid CNN-Transf/Attention模型进行了抽样、回顾和分析，并评估了这些模型的当前和未来的机遇和挑战。此外，我们还提出了一种全面的分析框架，以便根据这些模型的泛化机会，推动数据驱动的领域泛化和适应方法的发展。
</details></li>
</ul>
<hr>
<h2 id="Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning"><a href="#Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning" class="headerlink" title="Detecting disturbances in network-coupled dynamical systems with machine learning"></a>Detecting disturbances in network-coupled dynamical systems with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12771">http://arxiv.org/abs/2307.12771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Per Sebastian Skardal, Juan G. Restrepo</li>
<li>for: identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics</li>
<li>methods: model-free method based on machine learning using prior observations of the system when forced by a known training function</li>
<li>results: able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions, both with linear and nonlinear disturbances using food web and neuronal activity models.Here’s the full translation in Simplified Chinese:</li>
<li>for:  Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics</li>
<li>methods:  Model-free method based on machine learning using prior observations of the system when forced by a known training function</li>
<li>results:  Able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions, both with linear and nonlinear disturbances using food web and neuronal activity models.<details>
<summary>Abstract</summary>
Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics is a problem with a wide range of applications. For example, one might want to know which nodes in the network are being disturbed and identify the type of disturbance. Here we present a model-free method based on machine learning to identify such unknown disturbances based only on prior observations of the system when forced by a known training function. We find that this method is able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions. We illustrate our results both with linear and nonlinear disturbances using food web and neuronal activity models. Finally, we discuss how to scale our method to large networks.
</details>
<details>
<summary>摘要</summary>
<<sys.translation.activate("zh-Hans")>>无知的网络相互作用系统中的干扰的识别问题具有广泛的应用领域。例如，我们可能想知道哪些节点在网络中受到干扰，并识别干扰的类型。在这里，我们提出了一种无模型的方法，基于机器学习来识别未知干扰，只基于先前观察到的系统强制函数。我们发现这种方法可以识别多种不同类型的未知干扰，使用多种已知强制函数。我们使用食物网和神经活动模型来ILLUSTRATE我们的结果。最后，我们讨论如何扩展我们的方法到大型网络。<<sys.translation.deactivate()>>
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation"><a href="#Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation" class="headerlink" title="Nonparametric Linear Feature Learning in Regression Through Regularisation"></a>Nonparametric Linear Feature Learning in Regression Through Regularisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12754">http://arxiv.org/abs/2307.12754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bertillefollain/regfeal">https://github.com/bertillefollain/regfeal</a></li>
<li>paper_authors: Bertille Follain, Umut Simsekli, Francis Bach</li>
<li>for: 这个论文的目的是提出一种新的非 Parametric 特征选择方法，用于在高维数据中进行预测、计算和解释。</li>
<li>methods: 该方法使用了Empirical Risk Minimization 算法，并在其中添加了一个 penalty term 来保证方法的 versatility。在使用 Hermite 波幅的时候，我们引入了一个新的估计器名为 RegFeaL。</li>
<li>results: 我们的实验结果表明，RegFeaL 可以在各种实验中达到高效的预测性和精度。此外，我们还提供了一些实验结果，证明了我们的方法的可靠性和稳定性。<details>
<summary>Abstract</summary>
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with leading directions and accurately estimate the relevant dimension in practical settings. We establish that our method yields a consistent estimator of the prediction function with explicit rates. Additionally, we provide empirical results demonstrating the performance of RegFeaL in various experiments.
</details>
<details>
<summary>摘要</summary>
学习表示在自动选择特征中扮演着关键角色，尤其在高维数据的情况下。在这种情况下，非 Parametric 方法经常陷入困难。在这项研究中，我们关注supervised学习场景，其中相关信息归结于数据中的一个低维线性子空间，即多指标模型。如果这个子空间知道，那么预测、计算和解释都将得到极大提高。为解决这个挑战，我们提出了一种新的方法，即linear feature学习方法，该方法同时估算预测函数和线性子空间。我们的方法使用empirical risk minimization，加上函数导数的罚函数，以确保多样性。通过 Hermite  polynomials 的正交性和旋转不变性，我们引入了我们的估计器，名为RegFeaL。通过alternative minimization，我们可以逐步旋转数据，以便更好地与主要方向align，并准确地估算实际情况中的相关维度。我们证明了我们的方法可以得到一个consistent的预测函数估算器，并且提供了explicit rates。此外，我们还提供了许多实际 экспериментов的结果，以证明 RegFeaL 的性能。
</details></li>
</ul>
<hr>
<h2 id="Concept-based-explainability-for-an-EEG-transformer-model"><a href="#Concept-based-explainability-for-an-EEG-transformer-model" class="headerlink" title="Concept-based explainability for an EEG transformer model"></a>Concept-based explainability for an EEG transformer model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12745">http://arxiv.org/abs/2307.12745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andersgmadsen/tcav-bendr">https://github.com/andersgmadsen/tcav-bendr</a></li>
<li>paper_authors: Anders Gjølbye Madsen, William Theodor Lehn-Schiøler, Áshildur Jónsdóttir, Bergdís Arnardóttir, Lars Kai Hansen</li>
<li>for: 这个论文的目的是解释深度学习模型内部的状态，以便更好地理解它们如何处理数据。</li>
<li>methods: 该论文使用了Concept Activation Vectors（CAVs）方法来解释深度学习模型。CAVs是基于人类可理解的概念的方法，通过利用欧几何分布来定义内部状态。</li>
<li>results: 研究人员通过使用外部标注的EEG数据集和基于生物学结构的概念来定义概念，并证明这两种方法都可以提供深度EEG模型学习的有价值信息。<details>
<summary>Abstract</summary>
Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation: the use of externally labeled EEG datasets, and the application of anatomically defined concepts. The former approach is a straightforward generalization of methods used in image classification, while the latter is novel and specific to EEG. We present evidence that both approaches to concept formation yield valuable insights into the representations learned by deep EEG models.
</details>
<details>
<summary>摘要</summary>
深度学习模型因其大小、结构和训练过程中的随机性而复杂。这些复杂性来自数据选择和印uctive bias。为了解释这些复杂性，金等人（2018）提出了概念活化向量（CAV），该方法通过将深度模型内部状态转化为人类可理解的概念来解释深度模型的行为。这些概念与 latent space 中的方向相对应，通过使用线性投影来确定。这种方法最初应用于图像分类 зада务，后来扩展到其他领域，包括自然语言处理。在这项工作中，我们尝试将该方法应用于 Kostas 等人（2021）的 BENDR 模型，这是一个大规模的 transformer 模型。我们的注重点在于选择合适的解释概念和使用外部标注的 EEG 数据集来固定概念。前一种方法是一种直观的推广，而后一种方法是特定于 EEG 的新领域。我们显示两种方法都可以为深度 EEG 模型学习的表征提供有价值的解释。
</details></li>
</ul>
<hr>
<h2 id="Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding"><a href="#Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding" class="headerlink" title="Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding"></a>Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13007">http://arxiv.org/abs/2307.13007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakemi, Kakei Yamamoto, Takeo Hosomi, Kazuyuki Aihara</li>
<li>for: 这种研究旨在提高多层脉冲神经网络（SNN）的训练效果，特别是使用错误反射算法来实现理想的时间编码。</li>
<li>methods: 这种方法使用时间到初始脉冲（TTFS）编码，每个神经元只能发射一次，这种限制使得信息可以在非常低的脉冲频率下处理。</li>
<li>results: 通过两种基于脉冲时间的稀发（SSR）规范方法来进一步降低TTFS-编码SNNs的脉冲频率，并在MNIST、Fashion-MNIST和CIFAR-10 datasets上使用多层感知器网络和卷积神经网络结构进行研究。<details>
<summary>Abstract</summary>
The training of multilayer spiking neural networks (SNNs) using the error backpropagation algorithm has made significant progress in recent years. Among the various training schemes, the error backpropagation method that directly uses the firing time of neurons has attracted considerable attention because it can realize ideal temporal coding. This method uses time-to-first spike (TTFS) coding, in which each neuron fires at most once, and this restriction on the number of firings enables information to be processed at a very low firing frequency. This low firing frequency increases the energy efficiency of information processing in SNNs, which is important not only because of its similarity with information processing in the brain, but also from an engineering point of view. However, only an upper limit has been provided for TTFS-coded SNNs, and the information-processing capability of SNNs at lower firing frequencies has not been fully investigated. In this paper, we propose two spike timing-based sparse-firing (SSR) regularization methods to further reduce the firing frequency of TTFS-coded SNNs. The first is the membrane potential-aware SSR (M-SSR) method, which has been derived as an extreme form of the loss function of the membrane potential value. The second is the firing condition-aware SSR (F-SSR) method, which is a regularization function obtained from the firing conditions. Both methods are characterized by the fact that they only require information about the firing timing and associated weights. The effects of these regularization methods were investigated on the MNIST, Fashion-MNIST, and CIFAR-10 datasets using multilayer perceptron networks and convolutional neural network structures.
</details>
<details>
<summary>摘要</summary>
多层脉冲神经网络（SNN）的训练使用错误归散算法在过去几年来有了 significiant progress。多种训练方案中，使用神经元发射时间的错误归散方法吸引了较大的关注，因为它可以实现理想的时间编码。这种方法使用时间到第一脉冲（TTFS）编码，每个神经元只能发射一次，这种限制神经元发射数量使得信息可以在非常低的发射频率下处理。这种低发射频率提高了SNNs中信息处理的能效性，这不仅与神经元处理信息的方式相似，还从工程角度来看是非常重要。然而，只有提供了TTFS编码SNNs的Upper bound，它们在lower firing frequency下的信息处理能力还未得到了全面的研究。在这篇论文中，我们提出了两种基于发射时间的稀发射（SSR）规范，以进一步降低TTFS编码SNNs的发射频率。第一种是膜电压意识SSR（M-SSR）方法，它是膜电压值的极限形式的损失函数。第二种是发射条件意识SSR（F-SSR）方法，它是基于发射条件获得的规范函数。两种方法都是基于发射时间和相关权重的信息。我们在MNIST、Fashion-MNIST和CIFAR-10 datasets上使用多层报告网络和卷积神经网络结构来研究这两种规范的效果。
</details></li>
</ul>
<hr>
<h2 id="Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift"><a href="#Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift" class="headerlink" title="Safety Performance of Neural Networks in the Presence of Covariate Shift"></a>Safety Performance of Neural Networks in the Presence of Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12716">http://arxiv.org/abs/2307.12716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Harald Ruess, Konstantinos Theodorou</li>
<li>for:  This paper aims to address the issue of covariate shift’s impact on the operational safety performance of neural networks, and proposes a method to reshape the initial test set based on an approximation of the operational data.</li>
<li>methods: The proposed method uses finite binning and static dataflow analysis to derive conservative bounds on the values of neurons, and formulates a mixed integer linear programming (MILP) constraint to construct the minimum set of data points to be removed in the test set.</li>
<li>results: The proposed method can re-evaluate the safety performance of neural networks in the presence of covariate shift by using the reshaped test set, and can potentially reduce the need for collecting new operational data and creating corresponding ground truth labels.<details>
<summary>Abstract</summary>
Covariate shift may impact the operational safety performance of neural networks. A re-evaluation of the safety performance, however, requires collecting new operational data and creating corresponding ground truth labels, which often is not possible during operation. We are therefore proposing to reshape the initial test set, as used for the safety performance evaluation prior to deployment, based on an approximation of the operational data. This approximation is obtained by observing and learning the distribution of activation patterns of neurons in the network during operation. The reshaped test set reflects the distribution of neuron activation values as observed during operation, and may therefore be used for re-evaluating safety performance in the presence of covariate shift. First, we derive conservative bounds on the values of neurons by applying finite binning and static dataflow analysis. Second, we formulate a mixed integer linear programming (MILP) constraint for constructing the minimum set of data points to be removed in the test set, such that the difference between the discretized test and operational distributions is bounded. We discuss potential benefits and limitations of this constraint-based approach based on our initial experience with an implemented research prototype.
</details>
<details>
<summary>摘要</summary>
covariate shift可能会影响神经网络的操作安全性表现。然而，为了重新评估安全性表现，通常需要收集新的操作数据并创建相应的地面真实标签，这并不是在运行时可行。我们因此提议将初始测试集重新分配，基于运行时神经网络活动 patrerns的approximation。这种approximation可以通过观察和学习神经网络在运行时的活动模式来获得。重新分配的测试集尝试反映了在运行时神经网络活动值的分布，可以用于重新评估安全性表现在covariate shift的情况下。首先，我们通过finite binning和静态数据流分析来 derive保守的神经元值 bounds。其次，我们将mix integer linear programming（MILP）约束构造最小的数据点删除集，以使得测试集和运行时分布之间的差异保持在bound。我们对实际研究版本中的初步体验提出了可能的优点和限制。
</details></li>
</ul>
<hr>
<h2 id="Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport"><a href="#Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport" class="headerlink" title="Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport"></a>Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12703">http://arxiv.org/abs/2307.12703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Bras, Gilles Pagès</li>
<li>for: 估计 $f(X_T)$ 的方法，即使 $X$ 是 Stochastic Differential Equation 的解。</li>
<li>methods: 使用 $(f(X^1_T) + f(X^2_T))&#x2F;2$ 作为新的估计器，其中 $X^1$ 和 $X^2$ 具有同样的分布，但是具有相对的路径相关性，以降低方差。采用深度神经网络来 aproximate 优化函数 $\rho$，并使用策略梯度和奖励学习技术来准确调整 $\rho$。</li>
<li>results: 通过Policy Gradient和奖励学习技术来准确地调整优化函数 $\rho$，实现了降低方差的目标。<details>
<summary>Abstract</summary>
We propose a new algorithm for variance reduction when estimating $f(X_T)$ where $X$ is the solution to some stochastic differential equation and $f$ is a test function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and $X^2$ have same marginal law as $X$ but are pathwise correlated so that to reduce the variance. The optimal correlation function $\rho$ is approximated by a deep neural network and is calibrated along the trajectories of $(X^1, X^2)$ by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法来降低方差时估计 $f(X_T)$，其中 $X$ 是一个随机 diferencial equation 的解，$f$ 是一个测试函数。新的估计器是 $(f(X^1_T) + f(X^2_T))/2$，其中 $X^1$ 和 $X^2$ 具有同样的分布，但是它们的路径相关，以降低方差。我们使用深度神经网络来 aproximate 优化函数 $\rho$，并通过政策梯度和强化学习技术来调整 $\rho$ 的参数。找到最佳对接给定分布有关系于最大优化运输。
</details></li>
</ul>
<hr>
<h2 id="MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features"><a href="#MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features" class="headerlink" title="MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features"></a>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12698">http://arxiv.org/abs/2307.12698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Bardes, Jean Ponce, Yann LeCun</li>
<li>for: 本研究旨在jointly学习视觉表示和流体动作 estimation，并证明这两个目标互相帮助进行学习，从而学习出包含运动信息的内容特征。</li>
<li>methods: 本研究提出了MC-JEPA模型，即共同嵌入预测建模和自然学习方法，在共同编码器中同时学习视觉表示和流体动作 estimation。</li>
<li>results: 实验结果表明，MC-JEPA模型可以在无监督的情况下实现视觉表示和流体动作 estimation的同时学习，并且在下游任务中，如图像和视频Semantic segmentation等任务中，可以达到与现有无监督光流估计 benchmark和常见自然学习方法相当的性能。<details>
<summary>Abstract</summary>
Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.
</details>
<details>
<summary>摘要</summary>
自适应学习视觉表示法中心在学习内容特征，这些特征不包括物体运动或位置信息，而是通过识别和区分图像和视频中的对象来学习。相反，光流估计是一个不需要理解图像内容的任务。我们将这两种方法结合起来，并介绍MC-JEPA，一种共享编码器中的共同预测建筑和自适应学习方法，以jointly学习光流和内容特征。我们发现这两个相关的目标，即光流估计目标和自适应学习目标，在共同学习中互相帮助，因此学习的内容特征包含运动信息。我们的方法可以与现有的无监督光流标准做比较，以及常见的自适应学习方法在图像和视频Semantic segmentation任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks"><a href="#Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks" class="headerlink" title="Addressing the Impact of Localized Training Data in Graph Neural Networks"></a>Addressing the Impact of Localized Training Data in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12689">http://arxiv.org/abs/2307.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akanshaaga/reg_appnp">https://github.com/akanshaaga/reg_appnp</a></li>
<li>paper_authors: Singh Akansha</li>
<li>for: 本研究旨在评估图神经网络（GNNs）在本地化训练数据下的性能。</li>
<li>methods: 我们提出了一种常见GNN模型的补做方法，以适应本地化训练数据下的挑战。</li>
<li>results: 我们在三个标准图神经网络 benchmark dataset上进行了广泛的测试，并得到了显著的性能提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details></li>
</ul>
<hr>
<h2 id="An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks"><a href="#An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks" class="headerlink" title="An Estimator for the Sensitivity to Perturbations of Deep Neural Networks"></a>An Estimator for the Sensitivity to Perturbations of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12679">http://arxiv.org/abs/2307.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naman Maheshwari, Nicholas Malaya, Scott Moe, Jaydeep P. Kulkarni, Sudhanva Gurumurthi</li>
<li>for: 这篇论文的目的是为了评估深度神经网络（DNNs）在安全关键应用中的稳定性，如自动驾驶车和疾病诊断。</li>
<li>methods: 这篇论文使用了一种能够预测DNN对输入和模型参数的敏感性的估计器。该估计器基于不等式和矩阵范数，其结果类似于神经网络的condition number。</li>
<li>results: 在测试了AlexNet和VGG-19 convolutional neural networks（CNNs）以及ImageNet dataset时，这种估计器能够准确地预测DNN对输入和模型参数的敏感性。此外，通过随机偏移和攻击测试，这种估计器的紧密性也得到了证明。<details>
<summary>Abstract</summary>
For Deep Neural Networks (DNNs) to become useful in safety-critical applications, such as self-driving cars and disease diagnosis, they must be stable to perturbations in input and model parameters. Characterizing the sensitivity of a DNN to perturbations is necessary to determine minimal bit-width precision that may be used to safely represent the network. However, no general result exists that is capable of predicting the sensitivity of a given DNN to round-off error, noise, or other perturbations in input. This paper derives an estimator that can predict such quantities. The estimator is derived via inequalities and matrix norms, and the resulting quantity is roughly analogous to a condition number for the entire neural network. An approximation of the estimator is tested on two Convolutional Neural Networks, AlexNet and VGG-19, using the ImageNet dataset. For each of these networks, the tightness of the estimator is explored via random perturbations and adversarial attacks.
</details>
<details>
<summary>摘要</summary>
（ Deep Neural Networks 必须在安全关键应用中稳定，如自动驾驶车和疾病诊断。因此，必须了解 DNN 对输入和模型参数的敏感度，以确定安全地表示网络所需的最小位数准确性。然而，没有一个通用的结果可以预测给定 DNN 对轮减错误、噪声或其他输入中的敏感度。这篇文章提出了一个估计器，可以预测这些量。估计器是通过不等式和矩阵范数 derive，其结果类似于整个神经网络的condition number。这个估计器的紧密性在使用 ImageNet 数据集上对 AlexNet 和 VGG-19 两个卷积神经网络进行随机扰动和攻击性测试中被探索。）
</details></li>
</ul>
<hr>
<h2 id="Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling"><a href="#Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling" class="headerlink" title="Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling"></a>Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12672">http://arxiv.org/abs/2307.12672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Pan, Suprosanna Shit, Özgün Turgut, Wenqi Huang, Hongwei Bran Li, Nil Stolt-Ansó, Thomas Küstner, Kerstin Hammernik, Daniel Rueckert</li>
<li>for: 这篇论文的目的是为了提高动力磁共振成像（MRI）中的数据探测率，以解决因时间限制而导致的抽象项目残影。</li>
<li>methods: 本文使用的方法是将受测空间探测短缺的数据进行插值，并且使用一个新的Transformer-based k-space Global Interpolation Network（k-GIN）来学习全球的低频和高频成像结构。此外，我们还提出了一个k-space Iterative Refinement Module（k-IRM）来强化高频成像的学习。</li>
<li>results: 我们的方法与基准方法相比，在92个内部2D+t心脏MRI试验中表现出了优化的成像质量和更高的类别化能力。特别是在具有高度受测空间探测短缺的情况下，我们的方法具有更高的类别化能力和普遍性。<details>
<summary>Abstract</summary>
In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data.
</details>
<details>
<summary>摘要</summary>
在动态磁共振成像（MRI）中，通常因为扫描时间有限，会导致卷积空间下折射样本受到假象 artifacts。因此，动态MR重建需要不仅考虑 x 和 y 方向的空间频率组件，还需要考虑时间重复性。大多数前一些工作都是通过图像领域的正则化（约束）来进行MR重建。相比之下，我们注意到 interpolating 未折射的卷积空间，并提出了一种基于 Transformer 的全域卷积global interpolation network，称之为 k-GIN。我们的 k-GIN 学习了 2D+t 卷积空间中低频和高频组件之间的全局依赖关系，并使用其来 interpolate 未折射数据。此外，我们还提出了一种 k-space 迭代优化模块（k-IRM），以提高高频组件的学习。我们对 92 个室内 2D+t 心脏 MRI 测试数据进行了评估，并与使用图像领域正则化的 MR 重建方法进行比较。实验表明，我们提出的方法在量化和质量上都有显著提高，并且在高度受折射影响的 MR 数据中具有更高的robustness和普适性。
</details></li>
</ul>
<hr>
<h2 id="Control-and-Monitoring-of-Artificial-Intelligence-Algorithms"><a href="#Control-and-Monitoring-of-Artificial-Intelligence-Algorithms" class="headerlink" title="Control and Monitoring of Artificial Intelligence Algorithms"></a>Control and Monitoring of Artificial Intelligence Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13705">http://arxiv.org/abs/2307.13705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Carlos Mario Braga Ortuño, Blanza Martinez Donoso, Belén Muñiz Villanueva</li>
<li>for: 这篇论文强调了在人工智能模型部署后进行监管和评估数据分布的变化。</li>
<li>methods: 文章介绍了数据漂移和概念漂移的概念，以及他们的基础分布。同时，文章还提出了一些用于评估模型性能对于时间变化的指标。</li>
<li>results: 文章通过介绍不同的指标和方法，探讨了模型在不同情况下的性能。<details>
<summary>Abstract</summary>
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
</details>
<details>
<summary>摘要</summary>
这篇论文强调了在人工智能模型部署后的管理和监测数据分布的可能变化，而不是只是在训练数据上。文中介绍了数据漂移和概念漂移的概念，并详细介绍了它们的基础分布。此外，文中还提出了一些指标，可以用来评估模型在可能时间变化的情况下的性能。Here's a breakdown of the translation:* 这篇论文 (zhè běn tiān) - This paper* 强调 (qiáng dì) - emphasize* 在人工智能模型部署后 (zài rénsheng zhìyì módel bùdào hòu) - after the deployment of the artificial intelligence model* 管理 (guǎn lí) - management* 和监测 (hé jìng chá) - and monitoring* 数据分布 (shùdā fāngchēng) - data distribution* 可能变化 (kěnéng biànhùa) - potential variations* 而不是只是在训练数据上 (ér bùshì zhīshì zài xiǎngyìng shūjuè) - rather than only on the training data* 文中介绍 (wén zhōng jièshì) - the paper introduces* 数据漂移 (shùdā qùyì) - data drift* 和概念漂移 (hè guījiān qùyì) - and concept drift* 概念 (guījiān) - concept* 漂移 (qùyì) - drift* 基础分布 (jīshì fāngchēng) - fundamental distribution* 此外 (qíwài) - furthermore* 文中还提出 (wén zhōng hái tímzhěng) - the paper also proposes* 一些指标 (yīxiē zhǐdài) - some metrics* 可以用来 (kěyǐ yòu lái) - can be used to* 评估模型 (píngjì módel) - evaluate the model* 在可能时间变化的情况下 (zài kěnéng shíjiān biànhùa de qíngkè) - in the case of possible temporal variations.
</details></li>
</ul>
<hr>
<h2 id="TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers"><a href="#TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers" class="headerlink" title="TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers"></a>TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12667">http://arxiv.org/abs/2307.12667</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fahim-sikder/TransFusion">https://github.com/fahim-sikder/TransFusion</a></li>
<li>paper_authors: Md Fahim Sikder, Resmi Ramachandranpillai, Fredrik Heintz</li>
<li>for: 本研究旨在生成高质量、长序时间序数据，应用广泛。</li>
<li>methods: 我们提出了一种基于协同扩散和变换器的生成模型，称为TransFusion。</li>
<li>results: TransFusion可以生成高质量的长序时间序数据，并且在许多视觉和实验性指标上表现出优于之前的状态 искусственный智能。<details>
<summary>Abstract</summary>
The generation of high-quality, long-sequenced time-series data is essential due to its wide range of applications. In the past, standalone Recurrent and Convolutional Neural Network-based Generative Adversarial Networks (GAN) were used to synthesize time-series data. However, they are inadequate for generating long sequences of time-series data due to limitations in the architecture. Furthermore, GANs are well known for their training instability and mode collapse problem. To address this, we propose TransFusion, a diffusion, and transformers-based generative model to generate high-quality long-sequence time-series data. We have stretched the sequence length to 384, and generated high-quality synthetic data. To the best of our knowledge, this is the first study that has been done with this long-sequence length. Also, we introduce two evaluation metrics to evaluate the quality of the synthetic data as well as its predictive characteristics. We evaluate TransFusion with a wide variety of visual and empirical metrics, and TransFusion outperforms the previous state-of-the-art by a significant margin.
</details>
<details>
<summary>摘要</summary>
“高质量、长序时间序数据的生成是非常重要，因为它具有广泛的应用领域。在过去，单独的循环神经网和卷积神经网基于的生成对抗网（GAN）被用来合成时间序数据。但是，它们因架构限制而无法生成长序时间序数据，并且GAN在训练时会出现不稳定和模式崩溃问题。为解决这问题，我们提出了TransFusion，一个扩散和卷积变数基于的生成模型，可以生成高质量的长序时间序数据。我们已经将序列长度延长到384，并生成了高质量的 sintetic数据。到目前为止，这是第一篇使用这长序长度的研究。此外，我们也引入了两个评估 metric 来评估生成的质量和预测特性。我们将TransFusion评估使用广泛的视觉和实验 metric，并证明TransFusion在前一代的state-of-the-art上出现著标准的差异。”
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics"><a href="#Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics" class="headerlink" title="Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics"></a>Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12660">http://arxiv.org/abs/2307.12660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umbertomichieli/tap-slda">https://github.com/umbertomichieli/tap-slda</a></li>
<li>paper_authors: Umberto Michieli, Pablo Peso Parada, Mete Ozay</li>
<li>for: 本研究旨在提高附加在设备上的语音识别（KWS）模型的适应速度，使其能够快速适应用户定义的新词语，而不会忘记之前的词语。</li>
<li>methods: 本研究提出了一种名为Temporal Aware Pooling（TAP）的方法，它在采用冻结的后向传播模型（backbone）的基础上，通过计算高阶语音特征的时间相关特征空间来扩充特征空间。然后，对这个扩充后的特征空间进行Gaussian模型的更新，以便更好地利用语音表示。</li>
<li>results: 实验分析表明，TAP-SLDA方法在几个设置、后向传播模型和基础上都显示出了明显的优异性，相对于竞争者的平均提升率为11.3%。<details>
<summary>Abstract</summary>
Keyword Spotting (KWS) models on embedded devices should adapt fast to new user-defined words without forgetting previous ones. Embedded devices have limited storage and computational resources, thus, they cannot save samples or update large models. We consider the setup of embedded online continual learning (EOCL), where KWS models with frozen backbone are trained to incrementally recognize new words from a non-repeated stream of samples, seen one at a time. To this end, we propose Temporal Aware Pooling (TAP) which constructs an enriched feature space computing high-order moments of speech features extracted by a pre-trained backbone. Our method, TAP-SLDA, updates a Gaussian model for each class on the enriched feature space to effectively use audio representations. In experimental analyses, TAP-SLDA outperforms competitors on several setups, backbones, and baselines, bringing a relative average gain of 11.3% on the GSC dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable id="1" 键 слова检测（KWS）模型在嵌入式设备上应该快速适应新用户定义的词语，而不会忘记之前的词语。嵌入式设备具有有限的存储和计算资源，因此无法保存样本或更新大型模型。我们考虑了嵌入式在线继续学习（EOCL）的设置，其中KWS模型具有冻结的脊梁被训练以逐渐认可新的词语从一个不重复的流量中，每个样本一个个。为此，我们提议使用时间意识汇聚（TAP），它在预训练后的听语特征空间中构建了丰富的特征空间，并计算高阶时域特征以实现有效的听语表示。我们的方法TAP-SLDA将对每个类在汇聚特征空间上更新加aussian模型，以使用听语表示。在实验分析中，TAP-SLDA比竞争者在多种设置、脊梁和基础上表现出较高的平均提升率，达到11.3%的相对平均提升率在GSC数据集上。
</details></li>
</ul>
<hr>
<h2 id="Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG"><a href="#Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG" class="headerlink" title="Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG"></a>Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12644">http://arxiv.org/abs/2307.12644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remotebiosensing/rppg">https://github.com/remotebiosensing/rppg</a></li>
<li>paper_authors: Dae-Yeol Kim, Eunsu Goh, KwangKee Lee, JongEui Chae, JongHyeon Mun, Junyeong Na, Chae-bong Sohn, Do-Yup Kim</li>
<li>For: This study provides a benchmarking framework for evaluating the performance of remote photoplethysmography (rPPG) techniques across a wide range of datasets, to ensure fair and meaningful comparison and progress in the field.* Methods: The study uses a variety of datasets, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods, to evaluate the performance of rPPG techniques and provide a comprehensive benchmarking framework.* Results: The study aims to provide a fair and evaluable benchmarking framework for rPPG techniques, addressing the challenges of skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, to make meaningful progress in the field.<details>
<summary>Abstract</summary>
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overcome these challenges and make meaningful progress from both academic and commercial perspectives. In most existing work, models are trained, tested, and validated only on limited datasets. Even worse, some studies lack available code or reproducibility, making it difficult to fairly evaluate and compare performance. Therefore, the purpose of this study is to provide a benchmarking framework to evaluate various rPPG techniques across a wide range of datasets for fair evaluation and comparison, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods. GitHub URL: https://github.com/remotebiosensing/rppg
</details>
<details>
<summary>摘要</summary>
rPPG (远程血液抑血光谱) 是一种技术，通过使用摄像头捕捉光谱特性来测量和分析血液脉冲（BVP）。通过分析测量的BVP，可以 derivate 多种生物physiological signals，如心率、剂量压力和 стресс水平，这些信号可以应用于telemedicine、远程病人监测和早期心血管疾病预测等领域。rPPG 在学术和产业界 rapidly evolving 和吸引广泛关注，因为它可以通过摄像头设备测量生物信号，无需医疗或佩戴设备，提供了很好的可用性和便利性。然而，它还存在严重的挑战，包括皮肤颜色、摄像头特性、 ambient 照明和其他干扰和噪声的问题，这些问题会降低精度性能。我们认为，准确和评估可能的benchmarking是 urgently required ，以超越这些挑战和取得学术和商业上的进步。现有的大多数工作都是在有限的数据集上进行训练、测试和验证，甚至有些研究缺乏可用的代码或可重现性，使得准确评估和比较困难。因此，本研究的目的是提供一个 benchmarking 框架，以评估不同的 rPPG 技术在各种数据集上的性能，包括非深度神经网络（non-DNN）和深度神经网络（DNN）方法。GitHub URL：https://github.com/remotebiosensing/rppg
</details></li>
</ul>
<hr>
<h2 id="Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey"><a href="#Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey" class="headerlink" title="Fake News Detection Through Graph-based Neural Networks: A Survey"></a>Fake News Detection Through Graph-based Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12639">http://arxiv.org/abs/2307.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris</li>
<li>for: 本研究评估了基于图структуры的新闻假消息检测方法和深度学习方法，以及它们在新闻传播过程中的应用。</li>
<li>methods: 本研究分类了现有的图结构基于的新闻假消息检测方法，包括知识驱动方法、传播基于方法和多元社交 контекст基于方法。</li>
<li>results: 本研究评估了现有的图结构基于的新闻假消息检测方法，并提出了未来研究方向。<details>
<summary>Abstract</summary>
The popularity of online social networks has enabled rapid dissemination of information. People now can share and consume information much more rapidly than ever before. However, low-quality and/or accidentally/deliberately fake information can also spread rapidly. This can lead to considerable and negative impacts on society. Identifying, labelling and debunking online misinformation as early as possible has become an increasingly urgent problem. Many methods have been proposed to detect fake news including many deep learning and graph-based approaches. In recent years, graph-based methods have yielded strong results, as they can closely model the social context and propagation process of online news. In this paper, we present a systematic review of fake news detection studies based on graph-based and deep learning-based techniques. We classify existing graph-based methods into knowledge-driven methods, propagation-based methods, and heterogeneous social context-based methods, depending on how a graph structure is constructed to model news related information flows. We further discuss the challenges and open problems in graph-based fake news detection and identify future research directions.
</details>
<details>
<summary>摘要</summary>
“在线社交网络的广泛散布信息的受欢迎程度，使得人们可以更加快速地分享和消耗信息。但是，低品质和/或意外或故意伪造的信息也可以快速散布，导致社会产生了负面的影响。为了早为社会做出负面影响的预防和控制，识别、标识和驳斥网络伪信息的检测已经成为一个非常紧迫的问题。许多方法已经被提出来检测伪新闻，包括深度学习和agraph基的方法。在过去的几年中，agraph基的方法具有优秀的成绩，因为它们可以将在线新闻的社交内容和传播过程模型得到更加精确地。在这篇文章中，我们提出了一种系统性的审查伪新闻检测研究，依据graph基的方法进行分类，并讨论了伪新闻检测中的挑战和未来研究方向。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI"><a href="#Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI" class="headerlink" title="Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI"></a>Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12636">http://arxiv.org/abs/2307.12636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurizio Titz, Sebastian Pütz, Dirk Witthaut</li>
<li>for: 这篇论文旨在分析德国传输电网中的压力峰值和对负面影响，以及可能的市场设计变更以缓解压力峰值。</li>
<li>methods: 该论文使用可解释的机器学习模型来预测每小时的重新配置和对贸易量。模型分析了压力峰值的驱动因素和缓解因素，并评估了它们的影响。</li>
<li>results: 研究发现，风力电力生产是压力峰值的主要驱动因素，而水力电力和跨国电力贸易也扮演着重要的缓解作用。然而，太阳能电力没有缓解压力峰值的效果。结果表明，市场设计的变更可以缓解压力峰值。<details>
<summary>Abstract</summary>
The transition to a sustainable energy supply challenges the operation of electric power systems in manifold ways. Transmission grid loads increase as wind and solar power are often installed far away from the consumers. In extreme cases, system operators must intervene via countertrading or redispatch to ensure grid stability. In this article, we provide a data-driven analysis of congestion in the German transmission grid. We develop an explainable machine learning model to predict the volume of redispatch and countertrade on an hourly basis. The model reveals factors that drive or mitigate grid congestion and quantifies their impact. We show that, as expected, wind power generation is the main driver, but hydropower and cross-border electricity trading also play an essential role. Solar power, on the other hand, has no mitigating effect. Our results suggest that a change to the market design would alleviate congestion.
</details>
<details>
<summary>摘要</summary>
“将可再生能源纳入可持续能源供应的过程对电力系统运行带来多种挑战。透传网络荷载增加，因为风力和太阳能经常在消费者处 instal 远 away。在极端情况下，系统运维人员需要通过对贸易或重新分配来确保网格稳定。在这篇文章中，我们提供了一个可解释的机器学习模型，用于预测每小时的重新分配和对贸易量。该模型表明风力发电是主要驱动力，而水力发电和跨国电力贸易也扮演着关键性的角色。而太阳能发电却没有缓解效果。我们的结果表明，修改市场设计可以缓解压力。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network"><a href="#De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network" class="headerlink" title="De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network"></a>De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12625">http://arxiv.org/abs/2307.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Haolong Zeng, Yun Pen, Huiyan Sun</li>
<li>for: This paper aims to address the problem of counterfactual inference for continuous treatment variables, which is more common in real-world causal inference tasks.</li>
<li>methods: The proposed method is called de-confounding representation learning (DRL), which generates representations of covariates that are disentangled from the treatment variables. The DRL model is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates.</li>
<li>results: The DRL model outperforms state-of-the-art counterfactual inference models for continuous treatment variables in extensive experiments on synthetic datasets. Additionally, the DRL model is applied to a real-world medical dataset MIMIC and demonstrates a detailed causal relationship between red cell width distribution and mortality.<details>
<summary>Abstract</summary>
Counterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.
</details>
<details>
<summary>摘要</summary>
常用的Counterfactual推论 для连续而不是二进制的治疗变量更常见在实际世界的 causal推论任务中。现有一些基于Marginal Structural Model的样本重新权重方法，可以消除干扰的偏见，但这些方法通常假设了治疗变量和干扰变量之间的线性关系，并且这些模型通常是不可证明的。在这篇论文中，我们提出了一种基于de-confounding representation learning（DRL）的推论框架，用于连续治疗变量的 counterfactual 结果估计。DRL 是一种非 Parametric 模型，可以消除治疗变量和干扰变量之间的线性和非线性关系。具体来说，我们在框架中训练了干扰变量和治疗变量之间的相关性和干扰变量和治疗变量之间的相关性，以消除干扰偏见。此外，我们还将 counterfactual 推论网络 embedding 到框架中，以使得学习的表示可以用于 both de-confounding 和可靠的推论。我们在 synthetic 数据上进行了广泛的实验，发现 DRL 模型在学习 de-confounding 表示方面表现出色，并且超过了当前的 counterfactual 推论模型。此外，我们还应用了 DRL 模型到实际的医疗数据集 MIMIC，并显示了红细胞宽度分布和死亡的明确 causal 关系。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ordinary-Differential-Equations-with-Transformers"><a href="#Predicting-Ordinary-Differential-Equations-with-Transformers" class="headerlink" title="Predicting Ordinary Differential Equations with Transformers"></a>Predicting Ordinary Differential Equations with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12617">http://arxiv.org/abs/2307.12617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sören Becker, Michal Klein, Alexander Neitz, Giambattista Parascandolo, Niki Kilbertus</li>
<li>for:  recuperates scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory</li>
<li>methods:  transformer-based sequence-to-sequence model</li>
<li>results:  better or on par with existing methods in terms of accurate recovery, and efficiently scalable after one-time pretraining on a large set of ODEs<details>
<summary>Abstract</summary>
We develop a transformer-based sequence-to-sequence model that recovers scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory. We demonstrate in extensive empirical evaluations that our model performs better or on par with existing methods in terms of accurate recovery across various settings. Moreover, our method is efficiently scalable: after one-time pretraining on a large set of ODEs, we can infer the governing law of a new observed solution in a few forward passes of the model.
</details>
<details>
<summary>摘要</summary>
我们开发了一种基于转换器的序列到序列模型，可以从不规则采样和噪声观测数据中精确地回归Scalar常微方程（ODEs）的符号形式。我们在广泛的实验中证明了我们的模型与现有方法相比，在不同的设置下都能够更高效地回归精度。另外，我们的方法可以高效扩展：只需一次预训练于大量ODEs后，我们就可以在几个前向传播中快速地推断新观测数据的管理法律。
</details></li>
</ul>
<hr>
<h2 id="ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays"><a href="#ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays" class="headerlink" title="ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays"></a>ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12607">http://arxiv.org/abs/2307.12607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akanksha Dixit, Yashashwee Chakrabarty, Smruti R. Sarangi<br>for: 提高高频显示器的帧率，提供更平滑和响应的用户体验。methods: 使用动态网络（DNN）和强化学习（RL）算法，选择适合的插值和扩展方法，以提高帧率，同时保持图像质量。results: 比较传统方法，Exwarp可以提高帧率4倍，且图像质量几乎不受影响。<details>
<summary>Abstract</summary>
High-frequency displays are gaining immense popularity because of their increasing use in video games and virtual reality applications. However, the issue is that the underlying GPUs cannot continuously generate frames at this high rate -- this results in a less smooth and responsive experience. Furthermore, if the frame rate is not synchronized with the refresh rate, the user may experience screen tearing and stuttering. Previous works propose increasing the frame rate to provide a smooth experience on modern displays by predicting new frames based on past or future frames. Interpolation and extrapolation are two widely used algorithms that predict new frames. Interpolation requires waiting for the future frame to make a prediction, which adds additional latency. On the other hand, extrapolation provides a better quality of experience because it relies solely on past frames -- it does not incur any additional latency. The simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme. Past work has used DNNs to get good accuracy, however, these approaches are slow. This paper proposes Exwarp -- an approach based on reinforcement learning (RL) to intelligently choose between the slower DNN-based extrapolation and faster warping-based methods to increase the frame rate by 4x with an almost negligible reduction in the perceived image quality.
</details>
<details>
<summary>摘要</summary>
高频显示器目前在游戏和虚拟现实应用中得到了广泛的推广，但是这些显示器的后置GPU无法持续生成这高的帧率，这会导致用户体验不平滑和不响应。此外，如果帧率与刷新率不同步，用户可能会经历屏渲染和颤抖现象。以往的工作建议通过预测新帧来提高现代显示器的帧率，以提供柔顺的用户体验。插值和拟合是两种广泛使用的预测算法。插值需要等待未来帧来作预测，这会添加额外的延迟。拟合则提供了更高质量的用户体验，因为它仅基于过去帧进行预测，不增加额外的延迟。最简单的拟合方法是通过运动向量来扭曲上一帧，以生成下一帧。但是，扭曲后的帧可能包含不正确渲染的视觉artifacts，这使得设计这种方案非常困难。过去的工作使用深度神经网络（DNN）来获得高精度，但这些方法较慢。这篇论文提出了Exwarp方法，基于强化学习（RL）来智能选择 slower DNN-based extrapolation和 faster warping-based方法，以提高帧率4倍，并且几乎无法感受到图像质量的下降。
</details></li>
</ul>
<hr>
<h2 id="Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models"><a href="#Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models" class="headerlink" title="Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models"></a>Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12601">http://arxiv.org/abs/2307.12601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrik-ha/concept-backpropagation">https://github.com/patrik-ha/concept-backpropagation</a></li>
<li>paper_authors: Patrik Hammersborg, Inga Strümke</li>
<li>for:  This paper aims to provide a method for visualizing the information that a neural network model depends on to represent a given concept.</li>
<li>methods: The method used in this paper is called concept backpropagation, which involves perturbing the model input in a way that maximizes the detected concept.</li>
<li>results: The paper presents results for this method applied to a variety of input modalities, and discusses how the method can be used to visualize the information that trained concept probes use and the degree to which the representation of the probed concept is entangled within the neural network model.<details>
<summary>Abstract</summary>
Neural network models are widely used in a variety of domains, often as black-box solutions, since they are not directly interpretable for humans. The field of explainable artificial intelligence aims at developing explanation methods to address this challenge, and several approaches have been developed over the recent years, including methods for investigating what type of knowledge these models internalise during the training process. Among these, the method of concept detection, investigates which \emph{concepts} neural network models learn to represent in order to complete their tasks. In this work, we present an extension to the method of concept detection, named \emph{concept backpropagation}, which provides a way of analysing how the information representing a given concept is internalised in a given neural network model. In this approach, the model input is perturbed in a manner guided by a trained concept probe for the described model, such that the concept of interest is maximised. This allows for the visualisation of the detected concept directly in the input space of the model, which in turn makes it possible to see what information the model depends on for representing the described concept. We present results for this method applied to a various set of input modalities, and discuss how our proposed method can be used to visualise what information trained concept probes use, and the degree as to which the representation of the probed concept is entangled within the neural network model itself.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning"><a href="#Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning" class="headerlink" title="Optimized data collection and analysis process for studying solar-thermal desalination by machine learning"></a>Optimized data collection and analysis process for studying solar-thermal desalination by machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12594">http://arxiv.org/abs/2307.12594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guilong Peng, Senshan Sun, Yangjun Qin, Zhenwei Xu, Juxin Du, Swellam W. sharshir, A. W. Kandel, A. E. Kabeel, Nuo Yang</li>
<li>for: 这个研究的目的是提高机器学习在太阳蒸馏净水方面的应用，通过大量的实验数据收集和分析。</li>
<li>methods: 这个研究使用了修改后的实验数据收集和分析过程，通过加速数据收集和减少时间83.3%来收集超过一千个实验数据，比前一个研究的平均数据量大得多。同时，研究者使用了三种算法，包括人工神经网络、多ivariate 回归和随机森林，来研究数据特征的影响。</li>
<li>results: 研究结果表明，使用人工神经网络和随机森林算法时，大量数据可以显著提高预测精度。此外，研究还发现数据规模和范围对预测精度和影响因素排名的影响很大。同时，研究发现人工神经网络在推广范围上的描述性能受到数据范围的影响。这些结果表明，大量的实验数据收集和分析，以及数据特征的影响分析是机器学习在太阳蒸馏净水领域的重要步骤，可以推广机器学习在这个领域的应用。<details>
<summary>Abstract</summary>
An effective interdisciplinary study between machine learning and solar-thermal desalination requires a sufficiently large and well-analyzed experimental datasets. This study develops a modified dataset collection and analysis process for studying solar-thermal desalination by machine learning. Based on the optimized water condensation and collection process, the proposed experimental method collects over one thousand datasets, which is ten times more than the average number of datasets in previous works, by accelerating data collection and reducing the time by 83.3%. On the other hand, the effects of dataset features are investigated by using three different algorithms, including artificial neural networks, multiple linear regressions, and random forests. The investigation focuses on the effects of dataset size and range on prediction accuracy, factor importance ranking, and the model's generalization ability. The results demonstrate that a larger dataset can significantly improve prediction accuracy when using artificial neural networks and random forests. Additionally, the study highlights the significant impact of dataset size and range on ranking the importance of influence factors. Furthermore, the study reveals that the extrapolation data range significantly affects the extrapolation accuracy of artificial neural networks. Based on the results, massive dataset collection and analysis of dataset feature effects are important steps in an effective and consistent machine learning process flow for solar-thermal desalination, which can promote machine learning as a more general tool in the field of solar-thermal desalination.
</details>
<details>
<summary>摘要</summary>
要有效地结合机器学习和太阳蒸馈淡水，需要一个足够大、且具有分析力的实验数据集。本研究提出了一种修改后的数据采集和分析过程，用于通过机器学习研究太阳蒸馈淡水。基于优化的水蒸馈和收集过程，该方法收集了超过一千个数据集，比前一个平均数据集的十倍多，并将采集时间减少了83.3%。而且，该研究通过使用三种不同的算法，包括人工神经网络、多元线性回归和随机森林，研究数据集大小和范围对预测精度、因素重要性排名和模型泛化能力的影响。结果表明，大量数据集可以在使用人工神经网络和随机森林时显著提高预测精度。此外，研究还发现数据集大小和范围对因素重要性排名产生了重要影响。此外，研究还发现人工神经网络抽象数据范围对抽象预测精度产生了重要影响。根据结果，大量数据采集和分析数据集特征效果是机器学习过程中不可或缺的一步，可以推动机器学习在太阳蒸馈淡水领域的普遍应用。
</details></li>
</ul>
<hr>
<h2 id="InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis"><a href="#InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis" class="headerlink" title="InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis"></a>InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12586">http://arxiv.org/abs/2307.12586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxiang Grayson Tong, Carlos A. Sing Long, Daniele E. Schiavazzi</li>
<li>for: 本研究旨在推广使用生成模型和深度学习来解决物理系统的设计和分析问题，而不仅仅是模拟任务。</li>
<li>methods: 该研究提出了一种名为inVAErt网络的框架，该框架使用确定性编码器和解码器来表示前向和反向解决 Map，使用流变换模型来捕捉系统输出的概率分布，并使用变量编码器来学习减少输入和输出之间的不一致性。</li>
<li>results: 研究人员通过数值实验证明了inVAErt网络的可行性和灵活性，并发现选择罚分 coefficient和积分空间抽取策略对训练和测试性能有重要影响。<details>
<summary>Abstract</summary>
Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We validate our framework through extensive numerical examples, including simple linear, nonlinear, and periodic maps, dynamical systems, and spatio-temporal PDEs.
</details>
<details>
<summary>摘要</summary>
使用生成模型和深度学习来处理物理系统的应用主要是 emulator。然而，这些数据驱动架构的灵活性表示可以扩展到其他系统设计方面，包括模型反转和可识别性。我们介绍inVAErt（pronounced  inverse）网络，一个涵盖数据驱动分析和设计参数物理系统的框架，使用决定性编码器和解码器表示前向和反向解决Map，使用正态流捕捉系统输出的概率分布，并使用可变编码器学习减少输入和输出之间的不一致。我们正式调查损害征素在损失函数中的选择和latent空间抽样策略，因为我们发现这些对训练和测试性能有很大影响。我们通过大量的数字例子验证了我们的框架，包括简单的线性、非线性和 periodic  maps，动力系统和时空PDEs。
</details></li>
</ul>
<hr>
<h2 id="Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data"><a href="#Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data" class="headerlink" title="Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data"></a>Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12576">http://arxiv.org/abs/2307.12576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghyun Koo, Yunkee Chae, Chang-Bin Jeon, Kyogu Lee</li>
<li>for: 提高音乐源分离（MSS）性能，增加大数据集来改进MSS模型的训练</li>
<li>methods: 自动地对含有噪声标签的数据集进行自我反射，提高MSS模型的识别精度</li>
<li>results: 使用自我反射的数据集可以达到与使用干净标签的数据集相同的识别精度，而且在只有噪声标签数据集的情况下，MSS模型训练在自我反射数据集上可以超过使用干净标签数据集训练的性能。<details>
<summary>Abstract</summary>
Music source separation (MSS) faces challenges due to the limited availability of correctly-labeled individual instrument tracks. With the push to acquire larger datasets to improve MSS performance, the inevitability of encountering mislabeled individual instrument tracks becomes a significant challenge to address. This paper introduces an automated technique for refining the labels in a partially mislabeled dataset. Our proposed self-refining technique, employed with a noisy-labeled dataset, results in only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data in MSS model training and shows that utilizing the refined dataset leads to comparable results derived from a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on a self-refined dataset even outperform those trained on a dataset refined with a classifier trained on clean labels.
</details>
<details>
<summary>摘要</summary>
音乐源分离（MSS）面临限量正确标注个 instrumente 轨迹的问题。随着提高 MSS性能的努力，遇到带有错误标注的个 instrumente 轨迹的可能性变得非常重要。这篇文章介绍了一种自动刷新标注的技术，可以在带有噪声标注的 dataset 上进行刷新。我们的提议的自我刷新技术与噪声标注 dataset 上的类ifier 结合使用，对多个标签 instrumente 识别中的准确率进行了1%的下降。这种研究表明了刷新噪声标注数据的重要性，并证明了使用刷新后的数据可以达到与清晰标注数据相同的结果。甚至只有带有噪声标注的数据，MSS模型在使用自我刷新数据进行训练后会比使用刷新后的数据进行训练后更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalising-Neural-Topical-Representations"><a href="#Towards-Generalising-Neural-Topical-Representations" class="headerlink" title="Towards Generalising Neural Topical Representations"></a>Towards Generalising Neural Topical Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12564">http://arxiv.org/abs/2307.12564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohao Yang, He Zhao, Dinh Phung, Lan Du</li>
<li>for: 提高 neural topic model（NTM）的通用能力，使其可以在不同的资料集中具有可靠的泛化能力。</li>
<li>methods: 使用数据扩充和层次话题交通距离（HOTT）计算优化运输（OT）距离，以iminimize similar documents的semantic distance during training NTMs。</li>
<li>results: 对NTMs进行了扩展，使其在不同的资料集中具有显著提高的泛化能力。<details>
<summary>Abstract</summary>
Topic models have evolved from conventional Bayesian probabilistic models to Neural Topic Models (NTMs) over the last two decays. Although NTMs have achieved promising performance when trained and tested on a specific corpus, their generalisation ability across corpora is rarely studied. In practice, we often expect that an NTM trained on a source corpus can still produce quality topical representation for documents in a different target corpus without retraining. In this work, we aim to improve NTMs further so that their benefits generalise reliably across corpora and tasks. To do so, we propose to model similar documents by minimising their semantical distance when training NTMs. Specifically, similar documents are created by data augmentation during training; The semantical distance between documents is measured by the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be readily applied to most NTMs as a plug-and-play module. Extensive experiments show that our framework significantly improves the generalisation ability regarding neural topical representation across corpora.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction"><a href="#DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction" class="headerlink" title="DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction"></a>DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13004">http://arxiv.org/abs/2307.13004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Li, Changkun Jiang, Jianqiang Li</li>
<li>for: automatic protein function prediction (AFP)</li>
<li>methods:  sequence-based hierarchical prediction method using graph attention networks (GATs) and contrastive learning</li>
<li>results: better scalability in GO term enrichment analysis on large-scale datasets<details>
<summary>Abstract</summary>
Automatic protein function prediction (AFP) is classified as a large-scale multi-label classification problem aimed at automating protein enrichment analysis to eliminate the current reliance on labor-intensive wet-lab methods. Currently, popular methods primarily combine protein-related information and Gene Ontology (GO) terms to generate final functional predictions. For example, protein sequences, structural information, and protein-protein interaction networks are integrated as prior knowledge to fuse with GO term embeddings and generate the ultimate prediction results. However, these methods are limited by the difficulty in obtaining structural information or network topology information, as well as the accuracy of such data. Therefore, more and more methods that only use protein sequences for protein function prediction have been proposed, which is a more reliable and computationally cheaper approach. However, the existing methods fail to fully extract feature information from protein sequences or label data because they do not adequately consider the intrinsic characteristics of the data itself. Therefore, we propose a sequence-based hierarchical prediction method, DeepGATGO, which processes protein sequences and GO term labels hierarchically, and utilizes graph attention networks (GATs) and contrastive learning for protein function prediction. Specifically, we compute embeddings of the sequence and label data using pre-trained models to reduce computational costs and improve the embedding accuracy. Then, we use GATs to dynamically extract the structural information of non-Euclidean data, and learn general features of the label dataset with contrastive learning by constructing positive and negative example samples. Experimental results demonstrate that our proposed model exhibits better scalability in GO term enrichment analysis on large-scale datasets.
</details>
<details>
<summary>摘要</summary>
自动蛋白功能预测（AFP）被分类为大规模多标签分类问题，旨在自动化蛋白聚集分析，以消除现有的人工劳动密集方法。现有的popular方法主要结合蛋白质相关信息和生物学功能 ontology（GO）标签来生成最终的功能预测结果。例如，蛋白序列、结构信息和蛋白蛋白交互网络被融合到GO标签嵌入中，以生成最终的预测结果。然而，这些方法受到蛋白质结构信息或网络拓扑信息的困难性和准确性的限制。因此，越来越多的方法只使用蛋白序列进行蛋白功能预测，这是一种更可靠和计算成本更低的方法。然而，现有的方法无法充分EXTRACT蛋白序列和标签数据中的特征信息。因此，我们提出了一种遵循蛋白序列层次预测方法，深度GATGO，该方法可以处理蛋白序列和GO标签数据层次，并使用图注意力网络（GATs）和对比学习来进行蛋白功能预测。具体来说，我们使用预训练模型计算蛋白序列和标签数据的嵌入，以降低计算成本并提高嵌入精度。然后，我们使用GATs动态提取蛋白序列非几何数据的结构信息，并通过对比学习学习标签数据的通用特征。实验结果表明，我们提出的模型在大规模GO标签浸泡分析中展现出较好的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning"><a href="#Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning" class="headerlink" title="Homophily-Driven Sanitation View for Robust Graph Contrastive Learning"></a>Homophily-Driven Sanitation View for Robust Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12555">http://arxiv.org/abs/2307.12555</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Yulin Zhu, Xing Ai, Yevgeniy Vorobeychik, Kai Zhou</li>
<li>for: 这个论文旨在探讨Graph Contrastive Learning（GCL）对于结构攻击的 adversarial robustness。</li>
<li>methods: 这篇论文使用了一系列的攻击分析和理论分析，揭示了现有攻击的弱点和如何降低GCL的性能。此外，它还提出了一种robust GCL框架，该框架通过 integrate homophily-driven sanitation view来增强GCL的鲁棒性。然而，sanitation objective的非导数性带来了一些挑战，以下是一些解决这些挑战的技巧。</li>
<li>results: 我们的实验结果表明，GCHS（Graph Contrastive Learning with Homophily-driven Sanitation View）在两种状态之前的顶尖模型面前占据了优势，并在生成节点 embedding 和两个重要的下游任务上表现出色。<details>
<summary>Abstract</summary>
We investigate adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. First, we provide a comprehensive empirical and theoretical analysis of existing attacks, revealing how and why they downgrade the performance of GCL. Inspired by our analytic results, we present a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. A key challenge this poses, however, is the non-differentiable nature of the sanitation objective. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method which, unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state of the art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state of the art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details>
<details>
<summary>摘要</summary>
我们研究不监督图像对比学习（GCL）的抗击力，特别是对于结构性攻击。首先，我们提供了广泛的实验和理论分析，揭示了现有攻击的如何和为什么会下降GCL性能。 inspirited by our analytic results, we present a Robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. However, the non-differentiable nature of the sanitation objective poses a key challenge. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method, which unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state-of-the-art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state-of-the-art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.Here's the translation of the text in Traditional Chinese:我们研究不监督图像对比学习（GCL）的抗击力，特别是对于结构性攻击。首先，我们提供了广泛的实验和理论分析，揭示了现有攻击的如何和为什么会下降GCL性能。 inspirited by our analytic results, we present a Robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. However, the non-differentiable nature of the sanitation objective poses a key challenge. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method, which unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state-of-the-art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state-of-the-art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details></li>
</ul>
<hr>
<h2 id="Continuation-Path-Learning-for-Homotopy-Optimization"><a href="#Continuation-Path-Learning-for-Homotopy-Optimization" class="headerlink" title="Continuation Path Learning for Homotopy Optimization"></a>Continuation Path Learning for Homotopy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12551">http://arxiv.org/abs/2307.12551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xi-l/cpl">https://github.com/xi-l/cpl</a></li>
<li>paper_authors: Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, Qingfu Zhang</li>
<li>for: 提高Homotopy优化的效果和可用性，并提供更多的解决方案选择机会。</li>
<li>methods: 提出了一种基于模型的方法，可以同时优化原始问题和所有优化子问题，并实时生成任意中间解决方案。</li>
<li>results: 实验表明，该方法可以明显提高Homotopy优化的性能，并提供更多的有用信息支持更好的决策。<details>
<summary>Abstract</summary>
Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which includes infinite intermediate solutions for any surrogate subproblems. Unlike classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which can be desirable for many applications.Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness"><a href="#On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness" class="headerlink" title="On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"></a>On the Connection between Pre-training Data Diversity and Fine-tuning Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12532">http://arxiv.org/abs/2307.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ludwig Schmidt, Ali Farhadi</li>
<li>for: 了解预训练策略对下游模型的泛化性质的影响。</li>
<li>methods: 研究预训练分布的属性对下游模型的可靠性的影响，包括标签空间、标签 semantics、图像多样性、数据领域和数据量等因素。</li>
<li>results: 发现预训练数据量是下游模型的可靠性的关键因素，其他因素具有有限的影响。例如，将 ImageNet 预训练类减少到 4 倍，同时将每个类的图像数量增加到 4 倍（即保持总数据量不变）不会影响 fine-tuned 模型的可靠性。通过使用不同的自然和Synthetic 数据源预训练分布，主要通过 iWildCam-WILDS 分布转换测试下游模型的可靠性。<details>
<summary>Abstract</summary>
Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>预训练已广泛应用于深度学习中，以提高模型性能，特别是当目标任务的训练数据scarce时。在我们的工作中，我们想要了解预训练策略对下游模型的泛化性质产生的影响。更 Specifically，我们问的问题是：预训练分布的属性如何影响下游模型的可靠性？我们探讨的属性包括标签空间、标签 semantics、图像多样性、数据领域和数据量。我们发现预训练数据量是下游可靠性的主要因素，而其他因素具有有限的意义。例如，将 ImageNet 预训练类别数量减少到 4 倍，同时图像每类数量增加 4 倍（即保持总数据量不变），不会影响 Fine-tune 模型的可靠性。我们通过不同的自然和 sintetic 数据源中的预训练分布，主要使用 iWildCam-WILDS 分布转换为下游可靠性的测试。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph"><a href="#Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph" class="headerlink" title="Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph"></a>Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12526">http://arxiv.org/abs/2307.12526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyixinxin/mrg-kg">https://github.com/wangyixinxin/mrg-kg</a></li>
<li>paper_authors: Yixin Wang, Zihao Lin, Haoyu Dong</li>
<li>for: 这个研究旨在提高医疗报告生成（MRG）的品质，特别是透过知识图（KG）来导向生成过程。</li>
<li>methods: 本研究使用了一个完整的KG，包括137种疾病和问题，并导入了一个新的增强描述疾病类型的增强策略，以解决长条形分布问题。</li>
<li>results: 研究发现，提案的两阶段生成框架和增强策略可以提高生成的多样性和准确性，并有着显著的改善效果。<details>
<summary>Abstract</summary>
Knowledge Graph (KG) plays a crucial role in Medical Report Generation (MRG) because it reveals the relations among diseases and thus can be utilized to guide the generation process. However, constructing a comprehensive KG is labor-intensive and its applications on the MRG process are under-explored. In this study, we establish a complete KG on chest X-ray imaging that includes 137 types of diseases and abnormalities. Based on this KG, we find that the current MRG data sets exhibit a long-tailed problem in disease distribution. To mitigate this problem, we introduce a novel augmentation strategy that enhances the representation of disease types in the tail-end of the distribution. We further design a two-stage MRG approach, where a classifier is first trained to detect whether the input images exhibit any abnormalities. The classified images are then independently fed into two transformer-based generators, namely, ``disease-specific generator" and ``disease-free generator" to generate the corresponding reports. To enhance the clinical evaluation of whether the generated reports correctly describe the diseases appearing in the input image, we propose diverse sensitivity (DS), a new metric that checks whether generated diseases match ground truth and measures the diversity of all generated diseases. Results show that the proposed two-stage generation framework and augmentation strategies improve DS by a considerable margin, indicating a notable reduction in the long-tailed problem associated with under-represented diseases.
</details>
<details>
<summary>摘要</summary>
医疗报告生成（MRG）中知识图（KG）扮演着关键性的角色，因为它揭示疾病之间的关系，可以用于导航生成过程。然而，建立全面的KG是劳动密集的，而其在MRG过程中的应用还尚未得到了充分的探索。本研究中，我们建立了包含137种疾病和异常的完整KG，基于这个KG，我们发现现有的MRG数据集具有长尾分布问题。为了解决这个问题，我们提出了一种新的增强策略，增强疾病类型在分布尾部的表示。此外，我们设计了两个阶段的MRG方法，其中第一阶段使用分类器来检测输入图像是否具有任何异常。经过分类后，图像分别被独立地传递到两个基于转换器的生成器，即“疾病特定生成器”和“疾病无效生成器”，以生成对应的报告。为了提高生成报告的临床评估，我们提出了多样性敏感度（DS），一种新的指标，用于检查生成的疾病与实际情况是否匹配，并测量所有生成的疾病的多样性。结果显示，我们的两个阶段生成框架和增强策略可以大幅提高DS， indicating a considerable reduction in the long-tailed problem associated with under-represented diseases.
</details></li>
</ul>
<hr>
<h2 id="Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm"><a href="#Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm" class="headerlink" title="Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm"></a>Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12524">http://arxiv.org/abs/2307.12524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ruichen Li, Fan Liu, Xingquan Li, Juan Cheng, Muzhou Hou, Cong Cao</li>
<li>for: 预测地面滑坡表面变位</li>
<li>methods: 基于变形模式分解（VMD）、SegSigmoid函数、XGBoost算法和嵌入LSTM neural network的时序预测框架（VSXC-LSTM）</li>
<li>results: 在测试集上，模型表现良好，除了随机项 subsequences 难以适应外， periodic item subsequence 和 trend item subsequence 的 RMSE 和 MAPE 都小于 0.1， periodic item prediction module 基于 XGBoost 的 RMSE 为 0.006。<details>
<summary>Abstract</summary>
Landslide is a natural disaster that can easily threaten local ecology, people's lives and property. In this paper, we conduct modelling research on real unidirectional surface displacement data of recent landslides in the research area and propose a time series prediction framework named VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM) based on variational mode decomposition, which can predict the landslide surface displacement more accurately. The model performs well on the test set. Except for the random item subsequence that is hard to fit, the root mean square error (RMSE) and the mean absolute percentage error (MAPE) of the trend item subsequence and the periodic item subsequence are both less than 0.1, and the RMSE is as low as 0.006 for the periodic item prediction module based on XGBoost\footnote{Accepted in ICANN2023}.
</details>
<details>
<summary>摘要</summary>
地面滑坡是自然灾害，容易威胁当地生态、人们生命和财产。在这篇论文中，我们基于实际的单向表面偏移数据进行模拟研究，并提出了一种基于变幅模式分解的时间序列预测框架，称为VMD-SegSigmoid-XGBoost-ClusterLSTM（VSXC-LSTM）。这种模型可以更准确地预测滑坡表面偏移。测试集上的表现良好，只有随机项子序列难以适应，RMSE和MAPE值均小于0.1， periodic item prediction module based on XGBoost的RMSE值为0.006（ Accepted in ICANN2023）。
</details></li>
</ul>
<hr>
<h2 id="Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation"><a href="#Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation" class="headerlink" title="Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation"></a>Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12520">http://arxiv.org/abs/2307.12520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelbhandari6/nmt_text_attack">https://github.com/neelbhandari6/nmt_text_attack</a></li>
<li>paper_authors: Neel Bhandari, Pin-Yu Chen</li>
<li>for: 研究文章探讨了现有文本 adversarial 攻击的稳定性，特别是对于保持了 considerable similarity 的文本 adversarial examples。</li>
<li>methods: 文章使用了 six state-of-the-art text-based adversarial attacks，并对它们进行了 round-trip translation 测试。此外，文章还提出了一种基于 machine translation 的解决方案，以增强 adversarial example 的稳定性。</li>
<li>results: 研究发现，six state-of-the-art text-based adversarial attacks 在 round-trip translation 下失效，而 integrate machine translation into adversarial example generation 可以提高稳定性。这些结果表明，找到可以抗 translation 的 adversarial examples 可以帮助找到语言模型的缺陷，并促进更多关于多语言 adversarial attacks 的研究。<details>
<summary>Abstract</summary>
Language Models today provide a high accuracy across a large number of downstream tasks. However, they remain susceptible to adversarial attacks, particularly against those where the adversarial examples maintain considerable similarity to the original text. Given the multilingual nature of text, the effectiveness of adversarial examples across translations and how machine translations can improve the robustness of adversarial examples remain largely unexplored. In this paper, we present a comprehensive study on the robustness of current text adversarial attacks to round-trip translation. We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation. Furthermore, we introduce an intervention-based solution to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation. Our results indicate that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.
</details>
<details>
<summary>摘要</summary>
现代语言模型在许多下游任务上具有高精度。然而，它们仍然容易受到敌意攻击，特别是那些维持了原文和敌意例子之间的相似性。由于文本的多语言性，攻击者可以利用不同语言的文本来攻击语言模型。在这篇论文中，我们展示了现有的文本基于攻击的六种状态体验的不稳定性，并证明它们在翻译后不再有效。此外，我们还介绍了一种基于机器翻译的解决方案，并证明该方法可以提高攻击例子的翻译稳定性。我们的结果表明，找到可以抵抗翻译的攻击例子可以帮助我们发现语言模型的共同缺陷，并促进多语言攻击的研究。
</details></li>
</ul>
<hr>
<h2 id="DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning"><a href="#DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning" class="headerlink" title="DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning"></a>DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12519">http://arxiv.org/abs/2307.12519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ri Su, Shaojie Zhao, Muzhou Hou</li>
<li>for: This paper proposes a new method for multi-task learning (MTL) recommendation systems to better understand user behavior in complex scenarios.</li>
<li>methods: The proposed method, called Different Expression Parallel Heterogeneous Network (DEPHN), uses different feature interaction methods to improve the generalization ability of shared information flow and adaptively adjusts the learning intensity of gated units based on task correlation.</li>
<li>results: Extensive experiments on artificial and real-world datasets demonstrate that DEPHN can capture task correlation in complex situations and achieve better performance than baseline models.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文提出了一种基于多任务学习（MTL）的推荐系统，以更好地理解在复杂情况下用户行为。</li>
<li>methods: 提议的方法是多表达并发异构网络（DEPHN），通过不同的特征交互方法提高共享信息流的泛化能力，并在训练过程中通过特征显式映射和虚梯度系数进行专家阀控，以适应不同任务信息流的差异。</li>
<li>results: 对于人工和实际数据集的广泛实验表明，DEPHN可以在复杂情况下捕捉任务相关性，并比基线模型表现更好。<details>
<summary>Abstract</summary>
Recommendation system algorithm based on multi-task learning (MTL) is the major method for Internet operators to understand users and predict their behaviors in the multi-behavior scenario of platform. Task correlation is an important consideration of MTL goals, traditional models use shared-bottom models and gating experts to realize shared representation learning and information differentiation. However, The relationship between real-world tasks is often more complex than existing methods do not handle properly sharing information. In this paper, we propose an Different Expression Parallel Heterogeneous Network (DEPHN) to model multiple tasks simultaneously. DEPHN constructs the experts at the bottom of the model by using different feature interaction methods to improve the generalization ability of the shared information flow. In view of the model's differentiating ability for different task information flows, DEPHN uses feature explicit mapping and virtual gradient coefficient for expert gating during the training process, and adaptively adjusts the learning intensity of the gated unit by considering the difference of gating values and task correlation. Extensive experiments on artificial and real-world datasets demonstrate that our proposed method can capture task correlation in complex situations and achieve better performance than baseline models\footnote{Accepted in IJCNN2023}.
</details>
<details>
<summary>摘要</summary>
互联网运营商可以通过多任务学习（MTL）来理解用户和预测他们在多行为场景中的行为。传统模型通过共享底部模型和阻塞专家来实现共享表示学习和信息差异化。然而，现实世界中任务之间的关系经常比既有方法不够好地处理共享信息。在这篇论文中，我们提出了不同表达平行多样性网络（DEPHN），以同时模型多个任务。DEPHN通过不同的特征互动方法来提高共享信息流的泛化能力。对于模型对不同任务信息流的分化能力，DEPHN使用特征显式映射和虚拟梯度系数进行专家闭合 durante 训练过程中，并根据计算任务相互关系的差异来自适应学习Intensity of gated unit。经过了人工和实际世界的广泛实验，我们的提议方法可以在复杂的情况下捕捉任务相互关系，并在基eline模型的基础上提高表现。
</details></li>
</ul>
<hr>
<h2 id="FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks"><a href="#FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks" class="headerlink" title="FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks"></a>FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12518">http://arxiv.org/abs/2307.12518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Shaojie Zhao, Juan Cheng, Xingquan Li, Ri Su, Muzhou Hou, Cong Cao</li>
<li>for: 本研究旨在 Addressing two fundamental problems in applying deep learning&#x2F;machine learning methods to disease classification tasks, namely insufficient training samples and feature fusion.</li>
<li>methods: 提出了一种基于域对抗学习的Feature-aware Fusion Correlation Neural Network (FaFCNN)，具有特点增强样本相关性和特征对齐。</li>
<li>results: 实验结果表明，使用增强的特征获得融合特征后，FaFCNN可以更好地提高疾病分类性能，特别是在低质量数据集上。此外，广泛的实验还证明了模型的稳定性和每个组件的有效性。<details>
<summary>Abstract</summary>
There are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model\footnote{Accepted in IEEE SMC2023}.
</details>
<details>
<summary>摘要</summary>
“有两个基本问题在应用深度学习/机器学习方法进行疾病分类任务中，一个是训练样本数量和质量不足; 另一个是如何有效地融合多个来源特征，以训练可靠的分类模型。为了解决这些问题，我们提出了基于人类学习知识的Feature-aware Fusion Correlation Neural Network（FaFCNN）框架。这是一个通用的疾病分类框架，并且FaFCNN可以将现有方法中取得的样本相互联系特征改进。实验结果显示，使用增强决策树进行预训练后的扩展特征可以实现更多的性能提升，而且在我们的设置中，FaFCNN在低质量样本大量遗传数据上取得了一致性的最佳性能。此外，广泛的实验显示了提案的方法的稳定性和每个模型 ком成分的有效性。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Evaluation-of-Temporal-Graph-Benchmark"><a href="#An-Empirical-Evaluation-of-Temporal-Graph-Benchmark" class="headerlink" title="An Empirical Evaluation of Temporal Graph Benchmark"></a>An Empirical Evaluation of Temporal Graph Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12510">http://arxiv.org/abs/2307.12510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yule-BUAA/DyGLib_TGB">https://github.com/yule-BUAA/DyGLib_TGB</a></li>
<li>paper_authors: Le Yu</li>
<li>for: 本研究是一个empirical evaluation of Temporal Graph Benchmark (TGB)，通过扩展我们的Dynamic Graph Library (DyGLib)来对TGB进行评估。</li>
<li>methods: 本研究使用了11种流行的动态图学习方法进行更加广泛的比较，包括TGB中所report的基eline。</li>
<li>results: 通过实验发现，不同的模型在不同的数据集上表现出了不同的性能，与之前的观察一致；同时，使用DyGLib可以对一些基eline进行显著改进，超过TGB的报告结果。<details>
<summary>Abstract</summary>
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们进行了emporical评估Temporal Graph Benchmark（TGB）的扩展，使用我们的动态图库（DyGLib）来对TGB进行评估。相比TGB，我们包括了 eleven 种流行的动态图学习方法，以便进行更加详细的比较。通过实验，我们发现了以下两点：1. 不同的模型在不同的数据集上表现出了不同的性能，这与之前的观察一致。2. 一些基elines的性能可以通过使用DyGLib进行改进，这与TGB中report的结果相比有所提高。这项工作的目标是为研究者提供一个便利的评估多种动态图学习方法的平台，并提供可直接引用的结果。我们使用的所有资源都公开可用于https://github.com/yule-BUAA/DyGLib_TGB。这项工作正在进行中，欢迎社区提供反馈以便进行改进。
</details></li>
</ul>
<hr>
<h2 id="AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models"><a href="#AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models" class="headerlink" title="AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models"></a>AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12499">http://arxiv.org/abs/2307.12499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelong Dai, Kaisheng Liang, Bin Xiao</li>
<li>for: 防御深度学习模型受到攻击的攻击方法研究</li>
<li>methods: 使用扩散模型生成不受限制的攻击示例，并提出两种新的反向生成导航技术来进行攻击采样</li>
<li>results: 对MNIST和ImageNet dataset进行实验，得到了高质量、真实的攻击示例，并超越了基于GAN的方法在攻击性和生成质量上In English, this means:</li>
<li>for: Research on adversarial attacks against deep learning models and defense techniques</li>
<li>methods: Using diffusion models to generate unrestricted adversarial examples, and proposing two novel adversarial guidance techniques for reverse generation</li>
<li>results: Experimental results on MNIST and ImageNet datasets show that AdvDiff is effective in generating high-quality, realistic adversarial examples that outperform GAN-based methods in attack performance and generation quality.<details>
<summary>Abstract</summary>
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective to generate unrestricted adversarial examples, which outperforms GAN-based methods in terms of attack performance and generation quality.
</details>
<details>
<summary>摘要</summary>
深度学习模型面临了无限制敌意攻击的威胁，这些攻击可以有效绕过防御机制。然而，先前的攻击方法 часто使用生成对抗网络（GAN），这些网络不是理论可证明的，因此会生成不实际的例子，特别是对于大规模的数据集如ImageNet。在这篇论文中，我们提出了一种新的方法，称为AdvDiff，用于生成无限制敌意例子。我们设计了两种新的对抗导航技术，用于在扩散模型的反生成过程中进行对抗采样。这两种技术可以生成高质量、实际的敌意例子，通过可视化目标分类器的梯度来 интегрирова。实验结果表明，AdvDiff在MNIST和ImageNet数据集上效果地生成了无限制敌意例子，其性能和生成质量都高于基于GAN的方法。
</details></li>
</ul>
<hr>
<h2 id="A-faster-and-simpler-algorithm-for-learning-shallow-networks"><a href="#A-faster-and-simpler-algorithm-for-learning-shallow-networks" class="headerlink" title="A faster and simpler algorithm for learning shallow networks"></a>A faster and simpler algorithm for learning shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12496">http://arxiv.org/abs/2307.12496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Sitan Chen, Shyam Narayanan</li>
<li>for: 学习一个线性组合中的ReLU活化器，给出标注的例子来自标准的$d$-维高斯分布。</li>
<li>methods: 使用Chen et al.的算法， runtime在$\text{poly}(d,1&#x2F;\varepsilon)$时间内运行，并在多个阶段学习。</li>
<li>results: 显示了一个简单的一阶版本的算法可以 suffices，并且其运行时间只是 $(d&#x2F;\varepsilon)^{O(k^2)} $。<details>
<summary>Abstract</summary>
We revisit the well-studied problem of learning a linear combination of $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional Gaussian measure. Chen et al. [CDG+23] recently gave the first algorithm for this problem to run in $\text{poly}(d,1/\varepsilon)$ time when $k = O(1)$, where $\varepsilon$ is the target error. More precisely, their algorithm runs in time $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$ and learns over multiple stages. Here we show that a much simpler one-stage version of their algorithm suffices, and moreover its runtime is only $(d/\varepsilon)^{O(k^2)}$.
</details>
<details>
<summary>摘要</summary>
我们回到了已经很受研究的问题：学习一个线性 комбінаción of $k$ ReLU 激活函数， given labeled examples 从标准 $d$-dimensional Gaussian 分布中获取。陈等人 [CDG+23] 最近提出了首个这个问题可以在 $\text{poly}(d,1/\varepsilon)$ 时间内解决的算法，其中 $k = O(1)$，$\varepsilon$ 是目标错误。更加精确地说，他们的算法在多个阶段中执行， runtime 为 $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$。我们现在显示出一个 much simpler 的一阶版本的他们的算法，并且其时间复杂度仅为 $(d/\varepsilon)^{O(k^2)}$。
</details></li>
</ul>
<hr>
<h2 id="Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks"><a href="#Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks" class="headerlink" title="Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks"></a>Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12491">http://arxiv.org/abs/2307.12491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Zhang, Yang Liu, Li Xie, Lei Xie</li>
<li>for: 用于学习分子的准确表示，需要考虑分子的化学和几何特征。</li>
<li>methods: 基于分子图表示的方向节点对（DNP）描述器，Robust Molecular Graph Convolutional Network（RoM-GCN）模型可以同时考虑节点和边特征。</li>
<li>results: 对蛋白质和小分子数据集进行评估，研究表明DNP描述器能够具有3D分子几何信息的Robust性，RoM-GCN模型在比较基eline上表现出色。<details>
<summary>Abstract</summary>
To learn accurate representations of molecules, it is essential to consider both chemical and geometric features. To encode geometric information, many descriptors have been proposed in constrained circumstances for specific types of molecules and do not have the properties to be ``robust": 1. Invariant to rotations and translations; 2. Injective when embedding molecular structures. In this work, we propose a universal and robust Directional Node Pair (DNP) descriptor based on the graph representations of 3D molecules. Our DNP descriptor is robust compared to previous ones and can be applied to multiple molecular types. To combine the DNP descriptor and chemical features in molecules, we construct the Robust Molecular Graph Convolutional Network (RoM-GCN) which is capable to take both node and edge features into consideration when generating molecule representations. We evaluate our model on protein and small molecule datasets. Our results validate the superiority of the DNP descriptor in incorporating 3D geometric information of molecules. RoM-GCN outperforms all compared baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN"><a href="#Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN" class="headerlink" title="Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?"></a>Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12480">http://arxiv.org/abs/2307.12480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Peng, Jia Guo, Chenyang Yang</li>
<li>for: 本文研究了基于图神经网络（Graph Neural Networks，GNNs）的无线资源分配策略学习。</li>
<li>methods: 本文分析了顶点神经网络（Vertex-GNNs）和边神经网络（Edge-GNNs）在学习无线策略时的表达能力。</li>
<li>results: 研究发现，顶点神经网络和边神经网络的表达能力取决于处理和组合函数的线性和输出维度。顶点神经网络在使用线性处理器时无法分辨所有通道矩阵，而边神经网络可以。在学习precoding策略时，即使使用非线性处理器，顶点神经网络的表达能力仍然有限。研究还提出了必要的条件，以确保GNNs可以好好地学习precoding策略。实验结果证明了分析结论，并表明了边神经网络可以与顶点神经网络相比，具有更低的训练和推断时间。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) update the hidden representations of vertices (called Vertex-GNNs) or hidden representations of edges (called Edge-GNNs) by processing and pooling the information of neighboring vertices and edges and combining to incorporate graph topology. When learning resource allocation policies, GNNs cannot perform well if their expressive power are weak, i.e., if they cannot differentiate all input features such as channel matrices. In this paper, we analyze the expressive power of the Vertex-GNNs and Edge-GNNs for learning three representative wireless policies: link scheduling, power control, and precoding policies. We find that the expressive power of the GNNs depend on the linearity and output dimensions of the processing and combination functions. When linear processors are used, the Vertex-GNNs cannot differentiate all channel matrices due to the loss of channel information, while the Edge-GNNs can. When learning the precoding policy, even the Vertex-GNNs with non-linear processors may not be with strong expressive ability due to the dimension compression. We proceed to provide necessary conditions for the GNNs to well learn the precoding policy. Simulation results validate the analyses and show that the Edge-GNNs can achieve the same performance as the Vertex-GNNs with much lower training and inference time.
</details>
<details>
<summary>摘要</summary>
图 нейрон网络（GNNs）更新隐藏表示的顶点（称为顶点GNNs）或隐藏表示的边（称为边GNNs），通过处理和汇聚邻近顶点和边的信息，并将其组合以利用图STRUCTURE。在学习资源分配策略时，GNNs如果表达力强不足，例如不能分辨输入特征集如渠道矩阵，则不能表现出好的性能。在这篇论文中，我们分析顶点GNNs和边GNNs在学习三种代表性无线策略：链接调度策略、功率控制策略和排序策略时的表达力。我们发现顶点GNNs和边GNNs的表达力取决于处理和组合函数的线性和输出维度。当使用线性处理器时，顶点GNNs无法分辨所有渠道矩阵，而边GNNs可以。在学习预处理策略时， même avec les processeurs non linéaires, les GNNs peut ne pas avoir une capacité d'expression suffisante en raison de la compression de la dimension. Nous avons fourni les conditions nécessaires pour que les GNNs apprennent efficacement la stratégie de pré-traitement. Les résultats de simulation valident les analyses et montrent que les GNNs peuvent atteindre le même niveau de performance que les GNNs avec beaucoup moins de temps d'entraînement et d'inférence.
</details></li>
</ul>
<hr>
<h2 id="Model-free-generalized-fiducial-inference"><a href="#Model-free-generalized-fiducial-inference" class="headerlink" title="Model-free generalized fiducial inference"></a>Model-free generalized fiducial inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12472">http://arxiv.org/abs/2307.12472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan P Williams</li>
<li>for: 这项研究的目的是为了开发一种安全可靠的机器学习 uncertainty quantification 方法。</li>
<li>methods: 这项研究使用了一种 model-free 统计框架，以实现 imprecise probabilistic prediction inference。这个框架可以提供 finite sample control of type 1 errors，同时也可以提供更多的 versatile tools for imprecise probabilistic reasoning。</li>
<li>results: 这项研究提出了一种新的 preciseness probability approximation，用于 approximate belief&#x2F;plausibility measure pair。这个 aproximation 是一种 optima in some sense 的 probability measure in the credal set，可以解决 imprecise probabilistic approaches to inference 的问题。<details>
<summary>Abstract</summary>
Motivated by the need for the development of safe and reliable methods for uncertainty quantification in machine learning, I propose and develop ideas for a model-free statistical framework for imprecise probabilistic prediction inference. This framework facilitates uncertainty quantification in the form of prediction sets that offer finite sample control of type 1 errors, a property shared with conformal prediction sets, but this new approach also offers more versatile tools for imprecise probabilistic reasoning. Furthermore, I propose and consider the theoretical and empirical properties of a precise probabilistic approximation to the model-free imprecise framework. Approximating a belief/plausibility measure pair by an [optimal in some sense] probability measure in the credal set is a critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in statistical and machine learning communities. It is largely undetermined in the statistical and machine learning literatures, more generally, how to properly quantify uncertainty in that there is no generally accepted standard of accountability of stated uncertainties. The research I present in this manuscript is aimed at motivating a framework for statistical inference with reliability and accountability as the guiding principles.
</details>
<details>
<summary>摘要</summary>
我受到机器学习中无certainty量化的需求而努力提出和开发一个无模型的统计框架，以便实现precise probabilistic prediction inference中的uncertainty量化。这个框架可以提供finite sample控制type 1 error的prediction set，和conformal prediction set相似，但这个新的方法可以提供更多的versatile工具 дляimprecise probabilistic reasoning。此外，我还提出了一个精确的 probabilistic approximation，用于对无模型的imprecise framework进行approximation。在这个框架中，一个belief/plausibility measure pair的抽象是一个optimal的probability measure在credal set中，这是critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in statistical and machine learning communities。在统计和机器学习文献中，更加一般地，无法properly quantify uncertainty，因为没有一个通行的标准 accountability of stated uncertainties。我在这个著作中的研究是对 statistical inference的一个框架，以实现可靠性和责任性为引导 principl。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Distillation-Do-Not-Overlook-Calibration"><a href="#Rethinking-Data-Distillation-Do-Not-Overlook-Calibration" class="headerlink" title="Rethinking Data Distillation: Do Not Overlook Calibration"></a>Rethinking Data Distillation: Do Not Overlook Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12463">http://arxiv.org/abs/2307.12463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets">https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets</a></li>
<li>paper_authors: Dongyao Zhu, Bowen Lei, Jie Zhang, Yanbo Fang, Ruqi Zhang, Yiqun Xie, Dongkuan Xu</li>
<li>for: 本研究旨在解决因数据压缩而导致的神经网络输出过于自信的问题，提出了两种新的纠正方法：Masked Temperature Scaling (MTS) 和 Masked Distillation Training (MDT)。</li>
<li>methods: 本研究使用了数据压缩后的神经网络进行训练，并采用了温度扩大和混合方法来纠正神经网络的输出。</li>
<li>results: 研究发现，使用Masked Temperature Scaling (MTS) 和 Masked Distillation Training (MDT) 可以更好地纠正数据压缩后的神经网络输出，同时保持数据压缩的效率。<details>
<summary>Abstract</summary>
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
</details>
<details>
<summary>摘要</summary>
neural networks 经过精炼数据训练后通常会生成过度自信的输出，需要进行减强方法来修正。现有的减强方法，如温度Scaling 和 mixup，对于基于原始大规模数据的网络进行训练时工作良好。然而，我们发现这些方法无法调整基于大源数据集的数据精炼后的网络。在这篇论文中，我们发现了精炼数据导致的网络无法减强的两个问题：（i）精炼数据集中最大 logits 的更集中分布，以及（ii）semantic意义强度不相关的信息的丢失。为解决这问题，我们提出了Masked Temperature Scaling (MTS) 和 Masked Distillation Training (MDT)，这两种方法可以缓解精炼数据的局限性，实现更好的减强结果，同时保持数据精炼的效率。
</details></li>
</ul>
<hr>
<h2 id="Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks"><a href="#Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks" class="headerlink" title="Rates of Approximation by ReLU Shallow Neural Networks"></a>Rates of Approximation by ReLU Shallow Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12461">http://arxiv.org/abs/2307.12461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Mao, Ding-Xuan Zhou</li>
<li>For: The paper is written to investigate the efficiency of shallow neural networks with one hidden layer in approximating functions from H&quot;older spaces.* Methods: The paper uses ReLU neural networks with $m$ hidden neurons to approximate functions from $W_\infty^r([-1, 1]^d)$ and provides rates of uniform approximation.* Results: The paper shows that ReLU shallow neural networks can uniformly approximate functions from $W_\infty^r([-1, 1]^d)$ with rates $O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4})$ when $r&lt;d&#x2F;2 +2$, which is very close to the optimal rate $O(m^{-\frac{r}{d})$ when the dimension $d$ is large.<details>
<summary>Abstract</summary>
Neural networks activated by the rectified linear unit (ReLU) play a central role in the recent development of deep learning. The topic of approximating functions from H\"older spaces by these networks is crucial for understanding the efficiency of the induced learning algorithms. Although the topic has been well investigated in the setting of deep neural networks with many layers of hidden neurons, it is still open for shallow networks having only one hidden layer. In this paper, we provide rates of uniform approximation by these networks. We show that ReLU shallow neural networks with $m$ hidden neurons can uniformly approximate functions from the H\"older space $W_\infty^r([-1, 1]^d)$ with rates $O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4})$ when $r<d/2 +2$. Such rates are very close to the optimal one $O(m^{-\frac{r}{d})$ in the sense that $\frac{d+2}{d+4}$ is close to $1$, when the dimension $d$ is large.
</details>
<details>
<summary>摘要</summary>
“射预统计学中的神经网络，尤其是使用Rectified Linear Unit（ReLU）启动的神经网络，在深度学习的发展中扮演着中心作用。关于使用这些神经网络来近似Holder空间中函数的问题，是深度学习算法的效率的关键因素。虽然在多层神经网络的情况下已经得到了广泛的研究，但是对于单层神经网络还未有充分的研究。在这篇论文中，我们提供了uniform近似率。我们证明了ReLU单层神经网络可以将-$m$个隐藏神经元uniform近似$W_\infty^r([-1, 1]^d)$中的函数， rates为$O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4})$，当$r<d/2 +2$时。这些率与最佳率$O(m^{-\frac{r}{d})$几乎相等，即$\frac{d+2}{d+4}$与$1$几乎相同，当维度$d$很大时。”
</details></li>
</ul>
<hr>
<h2 id="Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty"><a href="#Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty" class="headerlink" title="Information-theoretic Analysis of Test Data Sensitivity in Uncertainty"></a>Information-theoretic Analysis of Test Data Sensitivity in Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12456">http://arxiv.org/abs/2307.12456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Futoshi Futami, Tomoharu Iwata</li>
<li>for: 这篇论文的目的是对 bayesian 推断中的不确定性进行量化，并分析了这种不确定性的两种类型：aleatoric 不确定性和 epistemic 不确定性。</li>
<li>methods: 该论文使用了 bayesian 推断，并rigorously 分解了 predictive uncertainty 到 two 种不确定性。它们分别表示数据生成过程中的内在随机性和数据不充分导致的多样性。</li>
<li>results: 该论文成功地定义了 uncertainty sensitivity，并extend 了现有的 bayesian meta-learning 分析。它首次显示了任务之间的新的sensitivity。<details>
<summary>Abstract</summary>
Bayesian inference is often utilized for uncertainty quantification tasks. A recent analysis by Xu and Raginsky 2022 rigorously decomposed the predictive uncertainty in Bayesian inference into two uncertainties, called aleatoric and epistemic uncertainties, which represent the inherent randomness in the data-generating process and the variability due to insufficient data, respectively. They analyzed those uncertainties in an information-theoretic way, assuming that the model is well-specified and treating the model's parameters as latent variables. However, the existing information-theoretic analysis of uncertainty cannot explain the widely believed property of uncertainty, known as the sensitivity between the test and training data. It implies that when test data are similar to training data in some sense, the epistemic uncertainty should become small. In this work, we study such uncertainty sensitivity using our novel decomposition method for the predictive uncertainty. Our analysis successfully defines such sensitivity using information-theoretic quantities. Furthermore, we extend the existing analysis of Bayesian meta-learning and show the novel sensitivities among tasks for the first time.
</details>
<details>
<summary>摘要</summary>
某些任务中，泊然推理 often 用于 uncertainty quantification 任务。据 Xu 和 Raginsky （2022）的分析， Bayesian 推理中的 predictive uncertainty 可以分为两种不确定性，即 aleatoric 和 epistemic 不确定性，它们表示数据生成过程中的内在随机性和数据不充分导致的多样性。他们通过信息论方式分析这些不确定性，假设模型是正确的并将模型参数看作隐藏变量。然而，现有的信息论分析不能解释uncertainty 中的一个广泛信奉的性质，即测试数据与训练数据之间的敏感性。这意味着当测试数据与训练数据相似时， epistemic 不确定性应该减少。在这项工作中，我们通过我们的新的分解方法来研究这种敏感性。我们的分析成功地定义了这种敏感性使用信息论量表示。此外，我们将 Bayesian meta-learning 的现有分析扩展到新的任务，并首次研究这些任务之间的新的敏感性。
</details></li>
</ul>
<hr>
<h2 id="DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces"><a href="#DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces" class="headerlink" title="DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces"></a>DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12451">http://arxiv.org/abs/2307.12451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferg-lab/diamondback">https://github.com/ferg-lab/diamondback</a></li>
<li>paper_authors: Michael S. Jones, Kirill Shmilovich, Andrew L. Ferguson</li>
<li>for: 这个论文的目的是提出一种基于杂化的分子模型，以便在长时间步骤上模拟蛋白质的均质化和折叠过程。</li>
<li>methods: 这个论文使用了一种名为Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping（DiAMoNDBack）的杂化模型，用于从粗粒度模型中恢复到原子级模型。这种模型基于一种杂化扩散过程，通过 conditioned on the Cα trace和当地的蛋白质结构来生成原子级模型。</li>
<li>results: 这个论文的实验结果表明，DiAMoNDBack模型可以在蛋白质结构模拟中实现高水平的重建性，包括正确的键形成、避免侧链冲突以及配置态的多样性。此外，模型还可以在不同的蛋白质结构和 simulate 中进行传输性。<details>
<summary>Abstract</summary>
Coarse-grained molecular models of proteins permit access to length and time scales unattainable by all-atom models and the simulation of processes that occur on long-time scales such as aggregation and folding. The reduced resolution realizes computational accelerations but an atomistic representation can be vital for a complete understanding of mechanistic details. Backmapping is the process of restoring all-atom resolution to coarse-grained molecular models. In this work, we report DiAMoNDBack (Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping) as an autoregressive denoising diffusion probability model to restore all-atom details to coarse-grained protein representations retaining only C{\alpha} coordinates. The autoregressive generation process proceeds from the protein N-terminus to C-terminus in a residue-by-residue fashion conditioned on the C{\alpha} trace and previously backmapped backbone and side chain atoms within the local neighborhood. The local and autoregressive nature of our model makes it transferable between proteins. The stochastic nature of the denoising diffusion process means that the model generates a realistic ensemble of backbone and side chain all-atom configurations consistent with the coarse-grained C{\alpha} trace. We train DiAMoNDBack over 65k+ structures from Protein Data Bank (PDB) and validate it in applications to a hold-out PDB test set, intrinsically-disordered protein structures from the Protein Ensemble Database (PED), molecular dynamics simulations of fast-folding mini-proteins from DE Shaw Research, and coarse-grained simulation data. We achieve state-of-the-art reconstruction performance in terms of correct bond formation, avoidance of side chain clashes, and diversity of the generated side chain configurational states. We make DiAMoNDBack model publicly available as a free and open source Python package.
</details>
<details>
<summary>摘要</summary>
习微观模型可以访问到长度和时间尺度，不可能由所有原子模型 achieve。它允许模拟在长时间尺度上发生的过程，如聚集和折叠。减少分辨率实现计算加速，但原子尺度的表示是完整理解机制的必要条件。回映是将高级别的分辨率复制到低级别的分辨率模型中的过程。在这种工作中，我们报道了Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping（Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping，简称DiAMoNDBack）作为一种推荐模型，用于在保留Cα坐标的情况下，将高级别的分辨率详细信息还原到低级别的分辨率模型中。这种推荐过程从蛋白质的N端开始，以每个残基为单位，通过Cα轨迹和以前已经回映的背bone和副链原子来驱动。本地和自适应的特点使得模型可以转移到不同的蛋白质上。由于杂化的推荐过程，模型可以生成一个真实的蛋白质详细配置，包括背bone和副链原子的全原子配置，并且与高级别的分辨率详细信息保持一致。我们在65000多个PDB结构数据集上训练了DiAMoNDBack模型，并在一个PDB测试集上验证了它。我们还在Protein Ensemble Database（PED）中的自发布蛋白质结构数据集、DE Shaw Research的分子动力学 simulations和减少分辨率 simulation数据上应用了这种模型。我们达到了当前最佳的重建性表现，包括正确的键形成、避免副链冲突和副链配置状态的多样性。我们将DiAMoNDBack模型作为一种免费和开源的Python包公开发布。
</details></li>
</ul>
<hr>
<h2 id="ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation"><a href="#ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation" class="headerlink" title="ProtoFL: Unsupervised Federated Learning via Prototypical Distillation"></a>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12450">http://arxiv.org/abs/2307.12450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansol Kim, Youngjun Kwak, Minyoung Jung, Jinho Shin, Youngsung Kim, Changick Kim</li>
<li>for: 提高数据隐私保护和验证系统性能</li>
<li>methods: 提出了基于原型表示缩短的 federated learning（ProtoFL）和基于正则化流的本地一类分类器</li>
<li>results: 在五个广泛使用的标准评估 dataset 上，证明了我们提出的框架在先前Literature中的表现优于其他方法<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种有前途的方法，可以增强数据隐私保护，特别是 для 身份验证系统。然而，有限的回合通信，珍贵的表示和扩展性带来了大量的挑战，这些挑战妨碍了其全面发挥。在这篇论文中，我们提议了“ProtoFL”，基于原型表示抽象的 Federated Learning，以提高全球模型的表示力并降低回合通信成本。此外，我们还引入了一种基于正规流的本地一类分类器，以提高有限数据下的性能。这是文献中第一篇使用 Federated Learning 提高一类分类性能的研究。我们在五个广泛使用的 benchmark 上进行了广泛的实验，包括 MNIST、CIFAR-10、CIFAR-100、ImageNet-30 和 Keystroke-Dynamics，以示出我们提posed框架的超过先前方法的优秀性。
</details></li>
</ul>
<hr>
<h2 id="WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms"><a href="#WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms" class="headerlink" title="WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms"></a>WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12449">http://arxiv.org/abs/2307.12449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satwik Kundu, Debarshi Kundu, Swaroop Ghosh</li>
<li>for: 加速量子 neural network 和量子矩阵问题的训练，提高量子矩阵算法的精度和效率。</li>
<li>methods: 提出了一种新的方法 called WEPRO，利用量子矩阵参数 weights 的常见趋势来加速量子矩阵的训练。并提出了两种优化技术 Naive Prediction 和 Adaptive Prediction。</li>
<li>results: 通过对多个量子 neural network 模型的训练和测试，显示了 WEPRO 可以提高训练速度约 2.25 倍，同时提高精度和预测性能，且具有低存储和计算开销。在量子矩阵问题中，WEPRO 也能够提高训练速度和精度。<details>
<summary>Abstract</summary>
The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational overheads. We also evaluate WEPRO's effectiveness in VQE for molecular ground-state energy estimation and in QAOA for graph MaxCut. Our results show that WEPRO leads to speed improvements of up to $3.1\times$ for VQE and $2.91\times$ for QAOA, compared to traditional optimization techniques, while using up to $3.3\times$ less number of shots (i.e., repeated circuit executions) per training iteration.
</details>
<details>
<summary>摘要</summary>
traditional training methods for Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE), and Quantum Approximate Optimization Algorithm (QAOA) face significant challenges due to the exponential run time on classical machines and the long queue depths and high costs of real quantum devices. To address these limitations, we propose a new approach called WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance, namely Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately 2.25 times compared to standard training methods, while also providing improved accuracy (up to 2.3% higher) and loss (up to 6.1% lower) with low storage and computational overheads. We also evaluate WEPRO's effectiveness in VQE for molecular ground-state energy estimation and in QAOA for graph MaxCut. Our results show that WEPRO leads to speed improvements of up to 3.1 times for VQE and 2.91 times for QAOA, compared to traditional optimization techniques, while using up to 3.3 times less number of shots (i.e., repeated circuit executions) per training iteration.
</details></li>
</ul>
<hr>
<h2 id="Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices"><a href="#Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices" class="headerlink" title="Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices"></a>Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12438">http://arxiv.org/abs/2307.12438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aimee Maurais, Terrence Alsup, Benjamin Peherstorfer, Youssef Marzouk</li>
<li>for: 这篇论文是为了提出一种多信度估计器，用于估计协方差矩阵。</li>
<li>methods: 这篇论文使用了拟合问题的方法，在协方差矩阵的拟合空间上进行估计。</li>
<li>results: 论文的实验结果表明，使用这种多信度估计器可以大幅降低估计误差，相比单信度和其他多信度估计器。此外，这种估计器还保持了正定定义性，使其适用于后续任务，如数据吸收和 метри学学习。<details>
<summary>Abstract</summary>
We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.
</details>
<details>
<summary>摘要</summary>
我们介绍一个多域确度估计器，它是解决在对称正定矩阵构造的应变问题中的解。这个估计器由建构而成，并且在 Mahalanobis 距离下实现了实用的计算。我们显示了我们的数据融合多域确度估计器（MRMF）是一个最大 LIKELIHOOD 估计器，在某些错误模型上的拓扑 tangent space 上。更一般地说，我们的里敦热投影框架包含了现有的多域确度估计器，它们是由控制值构成的。我们通过数据示例显示了我们的估计器可以对单域和其他多域确度估计器的平方误差做出很大减少，达到一个次的减少。此外，保持正定性的保证，使得我们的估计器可以与下游任务，如数据融合和度量学习，进行Compatible。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks"><a href="#A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks" class="headerlink" title="A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks"></a>A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12435">http://arxiv.org/abs/2307.12435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hipersimlab/pecann">https://github.com/hipersimlab/pecann</a></li>
<li>paper_authors: Shamsulhaq Basir, Inanc Senocak</li>
<li>For: The paper is written for solving forward and inverse problems involving partial differential equations (PDEs) using a meshless Schwarz-type non-overlapping domain decomposition method based on artificial neural networks.* Methods: The paper uses a generalized Robin-type interface condition, where unique Robin parameters are assigned to each subdomain and learned to minimize the mismatch on the Robin interface condition. The method uses an independent neural network model trained to minimize the loss on the governing PDE while strictly enforcing boundary and interface conditions through an augmented Lagrangian formalism.* Results: The paper demonstrates the versatility and performance of the proposed approach through extensive experiments on forward and inverse problems, including one-way and two-way decompositions with crosspoints. The learned Robin parameters adapt to the local behavior of the solution, domain partitioning, and subdomain location relative to the overall domain.<details>
<summary>Abstract</summary>
We present a meshless Schwarz-type non-overlapping domain decomposition method based on artificial neural networks for solving forward and inverse problems involving partial differential equations (PDEs). To ensure the consistency of solutions across neighboring subdomains, we adopt a generalized Robin-type interface condition, assigning unique Robin parameters to each subdomain. These subdomain-specific Robin parameters are learned to minimize the mismatch on the Robin interface condition, facilitating efficient information exchange during training. Our method is applicable to both the Laplace's and Helmholtz equations. It represents local solutions by an independent neural network model which is trained to minimize the loss on the governing PDE while strictly enforcing boundary and interface conditions through an augmented Lagrangian formalism. A key strength of our method lies in its ability to learn a Robin parameter for each subdomain, thereby enhancing information exchange with its neighboring subdomains. We observe that the learned Robin parameters adapt to the local behavior of the solution, domain partitioning and subdomain location relative to the overall domain. Extensive experiments on forward and inverse problems, including one-way and two-way decompositions with crosspoints, demonstrate the versatility and performance of our proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无缝Schwarz类非重叠域分解方法，基于人工神经网络来解决部分� differential 方程（PDEs）中的前向和反向问题。为确保邻居子域解的一致性，我们采用一种通用的Robin类型界面条件，将每个子域分配特定的Robin参数。这些子域特定的Robin参数通过在训练中最小化Robin界面条件的差异，以便有效地交换信息。我们的方法适用于拉普拉斯方程和哈尔曼方程。它使用独立的神经网络模型来表示本地解，并通过一种扩展的拉格朗日 formalism来严格执行边界和界面条件。我们发现，我们的方法可以学习每个子域的Robin参数，从而提高邻居子域之间信息交换的能力。我们在多个实验中证明了我们的提出的方法的多样性和性能。这些实验包括一个方向和二个方向的分解，以及跨点的分解。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection"><a href="#Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection" class="headerlink" title="Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection"></a>Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12427">http://arxiv.org/abs/2307.12427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuyangSunshine/ABR_IOD">https://github.com/YuyangSunshine/ABR_IOD</a></li>
<li>paper_authors: Liu Yuyang, Cong Yang, Goswami Dipam, Liu Xialei, Joost van de Weijer</li>
<li>for: 这篇论文的目的是解决在增量学习中的忘记问题，特别是在增量物体检测（IOD）领域中，通过重新播放之前任务的图像和当前任务的图像。</li>
<li>methods: 这篇论文提出了一种新的和高效的增量Box Replay（ABR）方法，该方法仅将前一任务的背景图像中的前景物体存储并重新播放，从而解决了背景shift问题。此外，该论文还提出了一种新的注意力捕捉的RoI填充损失，该损失使当前模型在旧模型中捕捉到重要信息。</li>
<li>results: 实验结果表明，ABR方法可以有效地避免忘记前一任务的类别，同时保持当前任务的柔软性。此外，ABR方法还可以减少存储需求，并在 Pascal-VOC 和 COCO 数据集上达到了顶尖性能。<details>
<summary>Abstract</summary>
In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model.
</details>
<details>
<summary>摘要</summary>
增量学习中，重新播放之前任务中的样本和当前任务中的样本是解决忘却折架的最有效方法之一。然而，与增量分类不同，图像重新播放在增量物体检测（IOD）中尚未得到成功。在这篇论文中，我们认为过looked problem of foreground shift是主要的原因。foreground shift只发生在重新播放之前任务的图像时，并且指的是这些背景可能包含当前任务中的前景对象。为解决这个问题，我们开发了一种新的和高效的增强盒子重新播放（ABR）方法，只将前景对象存储和重新播放，因此绕过了前景shift问题。此外，我们提出了一种创新的关注点 RoI 特征整合损失，使当前模型从老模型中提取最重要的信息，并将其用于现有模型的约束。ABR 能够减少之前类型的忘却，同时保持当前类型的高柔性。此外，它也可以significantly reduce the storage requirements when compared to standard image replay。我们在 Pascal-VOC 和 COCO 数据集上进行了全面的实验，并证明了我们的模型的状态-of-the-art表现。
</details></li>
</ul>
<hr>
<h2 id="Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction"><a href="#Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction" class="headerlink" title="Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction"></a>Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12417">http://arxiv.org/abs/2307.12417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasidis Arunruangsirilert, Jiro Katto</li>
<li>for: 这个论文的目的是预测5G NR网络中用户设备（UE）的未来上行吞吐量，以优化用户体验（QoE）。</li>
<li>methods: 这个论文使用ConvLSTM神经网络预测未来上行吞吐量，基于过去的上行吞吐量和RF参数。网络通过实际的5G SA网络驱动测试数据进行训练，并限制了模型只使用Android API中提供的信息。</li>
<li>results: 这个论文的结果表明，使用ConvLSTM神经网络预测未来上行吞吐量的方法可以达到98.9%的准确率， average RMSE为1.80 Mbps。<details>
<summary>Abstract</summary>
While the 5G New Radio (NR) network promises a huge uplift of the uplink throughput, the improvement can only be seen when the User Equipment (UE) is connected to the high-frequency millimeter wave (mmWave) band. With the rise of uplink-intensive smartphone applications such as the real-time transmission of UHD 4K/8K videos, and Virtual Reality (VR)/Augmented Reality (AR) contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience (QoE). In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, then evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9\% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.
</details>
<details>
<summary>摘要</summary>
5G新Radio（NR）网络承诺会带来巨大的上行吞吐量提高，但是这种提高只能在用户设备（UE）与高频毫米波（mmWave）频率带连接时得到。随着上行吞吐量占用应用程序如实时传输UHD 4K/8K视频和虚拟现实（VR）/增强现实（AR）内容的普及，上行吞吐量预测在maximizing用户体验质量（QoE）中扮演着关键的角色。在这篇论文中，我们提议使用ConvLSTM神经网络预测未来上行吞吐量，基于过去上行吞吐量和RF参数。网络通过实际驱动测试数据 collected from commercial 5G SA 网络而验证，该数据包括不同频率带、过渡和隐私。为确保我们的模型能够实际应用，我们然后限制了我们的模型仅使用可以通过 Android API 获得的信息。我们使用了不同交通工具进行评估，并发现我们的模型在所有未seen评估场景中达到了98.9%的预测精度，平均Relative Mean Squared Error（RMSE）为1.80 Mbps。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization"><a href="#A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization" class="headerlink" title="A Machine Learning Approach to Two-Stage Adaptive Robust Optimization"></a>A Machine Learning Approach to Two-Stage Adaptive Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12409">http://arxiv.org/abs/2307.12409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 解决两阶段线性适应Robust优化问题（ARO）中的 binary 现在变量和多面uncertainty sets问题。</li>
<li>methods: 使用机器学习方法，编码优化的现在决策、最差情况相关的优化决策和等待决策为策略。使用列和约束生成算法提取优化策略，并使用机器学习模型预测高质量策略。</li>
<li>results: 应用方法到facility location、multi-item 存储控制和单位启动问题，可以快速解决ARO问题，高精度。<details>
<summary>Abstract</summary>
We propose an approach based on machine learning to solve two-stage linear adaptive robust optimization (ARO) problems with binary here-and-now variables and polyhedral uncertainty sets. We encode the optimal here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the optimal wait-and-see decisions into what we denote as the strategy. We solve multiple similar ARO instances in advance using the column and constraint generation algorithm and extract the optimal strategies to generate a training set. We train a machine learning model that predicts high-quality strategies for the here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the wait-and-see decisions. We also introduce an algorithm to reduce the number of different target classes the machine learning algorithm needs to be trained on. We apply the proposed approach to the facility location, the multi-item inventory control and the unit commitment problems. Our approach solves ARO problems drastically faster than the state-of-the-art algorithms with high accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach"><a href="#Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach" class="headerlink" title="Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach"></a>Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12405">http://arxiv.org/abs/2307.12405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 这篇论文是为了提出一种机器学习方法来控制多类流体队列网络（MFQNET），并提供了明确和深入的控制策略。</li>
<li>methods: 这篇论文使用了优化类型的机器学习方法，即Optimal Classification Trees with hyperplane splits（OCT-H）来学习MFQNET的控制策略。</li>
<li>results: 实验结果表明，使用OCT-H学习的控制策略可以在大规模网络中实现100%的准确率，而在线应用只需几毫秒。<details>
<summary>Abstract</summary>
We propose a machine learning approach to the optimal control of multiclass fluid queueing networks (MFQNETs) that provides explicit and insightful control policies. We prove that a threshold type optimal policy exists for MFQNET control problems, where the threshold curves are hyperplanes passing through the origin. We use Optimal Classification Trees with hyperplane splits (OCT-H) to learn an optimal control policy for MFQNETs. We use numerical solutions of MFQNET control problems as a training set and apply OCT-H to learn explicit control policies. We report experimental results with up to 33 servers and 99 classes that demonstrate that the learned policies achieve 100\% accuracy on the test set. While the offline training of OCT-H can take days in large networks, the online application takes milliseconds.
</details>
<details>
<summary>摘要</summary>
我们提出了一种机器学习方法来优化多类流体队列网络（MFQNET）的控制问题，该方法提供了明确和深入的控制策略。我们证明了多类流体队列网络控制问题中存在一种阈值类型的优化策略，其阈值曲线都是通过起点的 hyperplanes。我们使用Optimal Classification Trees with hyperplane splits（OCT-H）来学习MFQNET的控制策略。我们使用 numerically solved MFQNET control problems作为训练集，并通过OCT-H来学习明确的控制策略。我们发现在33个服务器和99个类型的 эксперименталь结果中，学习的策略可以达到100%的准确率。虽然在大型网络中的离线训练可能需要几天的时间，但在线应用只需毫秒钟。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control"><a href="#Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control" class="headerlink" title="Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control"></a>Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12388">http://arxiv.org/abs/2307.12388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longchao Da, Hao Mei, Romir Sharma, Hua Wei</li>
<li>for: 提高RL在实际道路上的应用性能</li>
<li>methods: 使用 simulations-to-real-world（sim-to-real）转移方法，动态将模拟环境中学习的策略转移到实际环境中，以抑制域的差异</li>
<li>results: 在模拟交通环境中评估了UGAT方法，并显示其在实际环境中显著提高RL策略的性能<details>
<summary>Abstract</summary>
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
</details>
<details>
<summary>摘要</summary>
交通信号控制（TSC）是一项复杂重要的任务，影响了数百万人的日常生活。强化学习（RL）已经在优化交通信号控制方面显示了扎实的成果，但现有RL基于TSC方法主要在模拟环境中训练，它们在真实世界中的性能差距很大。在这篇论文中，我们提出了一种从模拟环境到真实世界（sim-to-real）传输方法，称为UGAT，它可以在模拟环境中学习的策略在真实世界中被转移并且在不同的环境中保持良好的性能。我们对一个模拟交通环境进行了评估，并证明了UGAT方法在真实世界中可以大幅提高RL策略的性能。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning"><a href="#In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning" class="headerlink" title="In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"></a>In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12375">http://arxiv.org/abs/2307.12375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Kossen, Tom Rainforth, Yarin Gal</li>
<li>for: 本研究旨在 investigating large language models (LLMs) 在下游任务中的启发式学习能力，特别是如何在 Context 中提供的示例对 Label 之间的关系对 LLMs 的预测造成影响。</li>
<li>methods: 本研究使用了一种 combine 的方法，包括分析 LLMs 在预训练和 Context 中的行为，以及如何将 Context 中的示例和 Label 相互关联。</li>
<li>results: 研究发现，LLMs 通常会在 Context 中使用示例 Label 的信息，但是预训练和 Context 中的 Label 关系是不同的，并且模型不会对所有 Context 中的信息进行平等考虑。这些结论可以帮助我们更好地理解和调节 LLM 的行为。<details>
<summary>Abstract</summary>
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into understanding and aligning LLM behavior.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在下游任务中的表现经常会有显著改善，当 inclusion 输入-标签关系的例子在上下文中时。然而，目前没有一致的共识，如何解释 LLM 的上下文学习（ICL）能力。例如，希xsd et al.（2021）认为 ICL 类似于一种通用学习算法，而 Min et al.（2022b）则认为 ICL 不会从上下文中学习标签关系。在这篇文章中，我们研究了以下几点：1. 输入-标签例子中的标签如何影响预测结果。2. 在预训练中学习的标签关系如何与输入-标签例子在上下文中相互作用。3. ICL 如何平均处理上下文中的标签信息。我们发现 LLM 通常会在上下文中利用标签信息，但是预训练和上下文中的标签关系被处理不同，而且模型不会对所有上下文中的信息进行平均处理。我们的发现可以帮助理解和调整 LLM 的行为。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting"><a href="#Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting" class="headerlink" title="Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting"></a>Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02505">http://arxiv.org/abs/2308.02505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly</li>
<li>for: 这种研究是为了评估生成 adversarial Networks (GANs) 在生成难以 obtain 的医学影像数据 augmentation 任务中的效果。</li>
<li>methods: 这种研究使用了多Scale Structural Similarity Index Measure 和 Cosine Distance 评估生成图像的内部多样性，以及 Frechet Inception Distance 评估生成图像的质量。</li>
<li>results: 研究发现，在不同的医学影像模式下，生成图像的多样性和质量得分异常大。不同的采样大小也会影响生成图像的质量和多样性。这种研究旨在探讨生成图像的多样性和质量在医学和非医学影像模式之间的差异。<details>
<summary>Abstract</summary>
In biomedical image analysis, data imbalance is common across several imaging modalities. Data augmentation is one of the key solutions in addressing this limitation. Generative Adversarial Networks (GANs) are increasingly being relied upon for data augmentation tasks. Biomedical image features are sensitive to evaluating the efficacy of synthetic images. These features can have a significant impact on metric scores when evaluating synthetic images across different biomedical imaging modalities. Synthetically generated images can be evaluated by comparing the diversity and quality of real images. Multi-scale Structural Similarity Index Measure and Cosine Distance are used to evaluate intra-class diversity, while Frechet Inception Distance is used to evaluate the quality of synthetic images. Assessing these metrics for biomedical and non-biomedical imaging is important to investigate an informed strategy in evaluating the diversity and quality of synthetic images. In this work, an empirical assessment of these metrics is conducted for the Deep Convolutional GAN in a biomedical and non-biomedical setting. The diversity and quality of synthetic images are evaluated using different sample sizes. This research intends to investigate the variance in diversity and quality across biomedical and non-biomedical imaging modalities. Results demonstrate that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities.
</details>
<details>
<summary>摘要</summary>
在生物医学影像分析中，数据偏好是广泛存在的问题，而生成对抗网络（GANs）在解决这一问题上逐渐被广泛应用。生物医学影像特征对评估合成图像的效果非常敏感，这些特征可以带来评估合成图像的纪录分数变化。合成图像可以通过与真实图像进行比较来评估其多样性和质量。在不同的生物医学成像modalities中，使用多尺度结构相似性指标和夹角距离来评估内类多样性，而使用彩色征波距离来评估合成图像的质量。为了调查这些指标在生物医学和非生物医学成像modalities中的效果，这里进行了一项实验性的评估。研究表明，在不同的生物医学和非生物医学成像modalities中，多样性和质量指标的分数差异很大。
</details></li>
</ul>
<hr>
<h2 id="Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans"><a href="#Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans" class="headerlink" title="Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans"></a>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12369">http://arxiv.org/abs/2307.12369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rumeng Li, Xun Wang, Dan Berlowitz, Brian Silver, Wen Hu, Heather Keating, Raelene Goodwin, Weisong Liu, Honghuang Lin, Hong Yu</li>
<li>for: 预测阿尔茨海默病（AD）的早期诊断非常重要，以便于时间性的 intervención和治疗。这项研究使用机器学习方法分析患者的长期电子医疗纪录（EHR），以找出可以预测AD诊断的标志和症状。</li>
<li>methods: 这项研究使用了一种 случа控制设计，使用从2004年到2021年的美国卫生部卫生管理局（VHA）的长期EHR数据进行分析。实验中的患者是由2016年以后根据ICD-10-CM代码诊断出的AD患者，与年龄、性别和临床使用相同的9名控制人进行匹配。研究使用了AD相关关键词的时间序列分析，以预测AD诊断。</li>
<li>results: 研究发现，患者的AD相关关键词的时间序列分析可以预测AD诊断，特别是在诊断附近的时间段。模型的拟合度（ROCAUC）为0.997，准确性很高。研究还发现，年龄、性别和种族&#x2F;民族子组的预测结果几乎一致，只有年龄 younger than 65的 subgroup的预测结果不太准确（ROCAUC 0.746）。这种机器学习模型可以使用EHR数据预测AD诊断，提供一种可靠的、便宜的方式，用于早期诊断大量人口。<details>
<summary>Abstract</summary>
Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
</details>
<details>
<summary>摘要</summary>
早期预测阿尔ツ海默病（AD）是非常重要，以便在时间上采取措施和治疗。这项研究目的是使用机器学习方法分析患者的长期电子医疗记录（EHR），以确定患者患阿尔ツ海默病的预测指标。我们采用了一种case-control设计，使用2004年至2021年美国卫生部老年军人医疗管理局（VHA）的长期EHR数据，确定患者是在2016年1月1日后被诊断为阿尔ツ海默病（根据ICD-10-CM代码），并与年龄、性别和临床使用相同的9名控制人群进行匹配。我们使用了一组阿尔ツ海默病相关关键词，并跟踪这些词语在患者的长期EHR记录中的出现情况，采用4种机器学习模型进行预测。我们进行了年龄、性别和种族/民族 subgroup分析，并在一个“隐藏”的VHA站点上验证模型。模型的准确率、均衡和其他相关指标都被报告，用于预测距离ICD-基本诊断的诊断。研究人口包括16,701例患者和39,097名匹配的控制人群。患者的AD相关关键词每年增加速度很快，从约10个增加到超过40个，而控制人群保持在10个，而且在诊断 approached时，AD相关关键词的增加速度加剧。最佳模型在使用至少10年之前的ICD-基本诊断数据时，达到了0.997的报告准确率。模型均衡好（Hosmer-Lemeshow准确度测试值为0.99），并在年龄、性别和种族/民族 subgroup中保持一致，除了年龄小于65岁的患者（ROCAUC为0.746）。机器学习模型使用从EHR记录中提取的AD相关关键词可以预测未来的AD诊断，表明其可能用于通过EHR记录来预测AD风险，提供一种可靠且有效的大规模屏检方式。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.LG_2023_07_24/" data-id="clpxp6c3500p2ee88b7bp4j3b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/eess.IV_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T09:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/eess.IV_2023_07_24/">eess.IV - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding"><a href="#Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding" class="headerlink" title="Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding"></a>Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12864">http://arxiv.org/abs/2307.12864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Brand, Jürgen Seiler, André Kaup</li>
<li>for: 这个论文是为了提出一种新的视频编码方法，即基于神经网络的 conditional coding，以提高视频编码的效率。</li>
<li>methods: 这个论文使用了 conditional coding 和 residual coding 两种编码方法进行比较，并提出了一种新的 conditional residual coding 方法，以解决 conditional coding 中的信息瓶颈问题。</li>
<li>results: 论文通过 theoretically 和实际例子的分析，证明 conditional residual coding 可以减少信息瓶颈的影响，同时保持 conditional coding 的理论性能。这种编码方法可以 viewed as “the best from both worlds” 在 residual 和 conditional coding 之间。<details>
<summary>Abstract</summary>
Conditional coding is a new video coding paradigm enabled by neural-network-based compression. It can be shown that conditional coding is in theory better than the traditional residual coding, which is widely used in video compression standards like HEVC or VVC. However, on closer inspection, it becomes clear that conditional coders can suffer from information bottlenecks in the prediction path, i.e., that due to the data processing inequality not all information from the prediction signal can be passed to the reconstructed signal, thereby impairing the coder performance. In this paper we propose the conditional residual coding concept, which we derive from information theoretical properties of the conditional coder. This coder significantly reduces the influence of bottlenecks, while maintaining the theoretical performance of the conditional coder. We provide a theoretical analysis of the coding paradigm and demonstrate the performance of the conditional residual coder in a practical example. We show that conditional residual coders alleviate the disadvantages of conditional coders while being able to maintain their advantages over residual coders. In the spectrum of residual and conditional coding, we can therefore consider them as ``the best from both worlds''.
</details>
<details>
<summary>摘要</summary>
新的条件编码方式是基于神经网络的压缩，可以证明这种条件编码在理论上比传统的差异编码（如HEVC或VVC中的差异编码）更好。然而，在更加仔细的分析下，可以发现条件编码器可能会在预测路径中遇到信息瓶颈，即由数据处理不对称性导致的信息无法传递到重建信号中，从而影响编码器性能。在这篇论文中，我们提出了条件差异编码概念，该概念基于条件编码器的信息学性质。这种编码器可以减少预测路径中的瓶颈影响，同时保持条件编码器的理论性能。我们对这种编码器进行了理论分析，并在实践中示出了其性能。我们发现，条件差异编码器可以消除条件编码器的缺点，同时保持条件编码器比差异编码器更好的优势。因此，在差异和条件编码之间的谱spectrum中，我们可以视之为“最佳的两个世界”。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion"><a href="#Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion" class="headerlink" title="Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion"></a>Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12853">http://arxiv.org/abs/2307.12853</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. I. Ugwu, S. Casarin, O. Lanz</li>
<li>for: 这paper的目的是提出一种基于2D Convolutional Neural Networks的多模态脐椎像分割模型，以提高计算医学中的图像分析效能。</li>
<li>methods: 这paper使用了一种名为Slice SHift UNet（SSH-UNet）的新模型，它通过在多个视角上进行2D卷积，共同学习多个视角的特征，并通过在层次轴上偏移特征图来重新包含第三维度信息。</li>
<li>results: 该paper在Multi-Modality Abdominal Multi-Organ Segmentation（AMOS）和Multi-Atlas Labeling Beyond the Cranial Vault（BTCV） datasets上进行了实验，并证明了SSH-UNet的效果与现有的模型相当，而且更高效。<details>
<summary>Abstract</summary>
As a fundamental part of computational healthcare, Computer Tomography (CT) and Magnetic Resonance Imaging (MRI) provide volumetric data, making the development of algorithms for 3D image analysis a necessity. Despite being computationally cheap, 2D Convolutional Neural Networks can only extract spatial information. In contrast, 3D CNNs can extract three-dimensional features, but they have higher computational costs and latency, which is a limitation for clinical practice that requires fast and efficient models. Inspired by the field of video action recognition we propose a new 2D-based model dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensional features at 2D CNN's complexity. More precisely multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. The effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details>
<details>
<summary>摘要</summary>
computer tomography (CT) 和 магнитная резонансная томография (MRI) 提供了体积数据，因此开发三维图像分析算法是必需的基础部分。 although 2D convolutional neural networks (CNNs) 可以提取空间信息，但它们只能提取二维特征。 相比之下，三维 CNNs 可以提取三维特征，但它们的计算成本和延迟更高，这限制了临床实践中的快速和高效模型。  inspirited  by the field of video action recognition, we propose a new 2D-based model called Slice SHift UNet (SSH-UNet)，它在 2D CNN 的复杂性下编码三维特征。 more precisely, multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. the third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. the effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Vertebra-Localization-and-Identification-from-CT-Images"><a href="#Multi-View-Vertebra-Localization-and-Identification-from-CT-Images" class="headerlink" title="Multi-View Vertebra Localization and Identification from CT Images"></a>Multi-View Vertebra Localization and Identification from CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12845">http://arxiv.org/abs/2307.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images">https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images</a></li>
<li>paper_authors: Han Wu, Jiadong Zhang, Yu Fang, Zhentao Liu, Nizhuan Wang, Zhiming Cui, Dinggang Shen</li>
<li>for: 本研究旨在提出一种基于多视图的 vertebra 定位和识别方法，以解决现有方法的大量计算成本和局部信息有限问题。</li>
<li>methods: 该方法将3D问题转化为2D定位和识别任务，并采用多视图对准学习策略来学习全局信息。此外，还提出了一种序列损失来保持vertebrae中的序列结构。</li>
<li>results: 评估结果表明，只使用两个2D网络，该方法可以准确地定位和识别CT图像中的vertebrae，并在比较现有方法的情况下卓越表现。<details>
<summary>Abstract</summary>
Accurately localizing and identifying vertebrae from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information. In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views. Without the limitation of the 3D cropped patch, our method can learn the multi-view global information naturally. Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone. Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae. Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently. Our code is available at https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images.
</details>
<details>
<summary>摘要</summary>
通过CT图像进行精准地Localizing和识别脊梗是许多临床应用中的关键。然而，大多数现有的尝试都是基于3D的剪辑补丁操作，它们受到大量计算成本和有限的全局信息的限制。在这篇论文中，我们提出了基于多视图的脊梗Localization和识别方法，将3D问题转化为2D的Localization和识别任务。不同于剪辑补丁限制，我们的方法可以自然地学习多视图的全局信息。此外，为了更好地捕捉不同视角的解剖结构信息，我们还提出了一种多视图对比学习策略来预训练脊梗。此外，我们还提出了一种序列损失，以维护链接在脊梗上的序列结构。评估结果表明，只有两个2D网络，我们的方法可以在CT图像中准确地Localizing和识别脊梗，并在状态艺术方法上一致性地表现出优于其他方法。我们的代码可以在https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning"><a href="#Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning" class="headerlink" title="Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning"></a>Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12792">http://arxiv.org/abs/2307.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Huber, Sebastien Ourselin, Christos Bergeles, Tom Vercauteren</li>
<li>for: 这个研究探讨了透过从逆向录影中学习自动化 Laparoscopic 镜头运动。</li>
<li>methods: 研究将从 retrospective 录影中学习对象运动空间的增强，运用 homographies 进行对象运动不变对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对<details>
<summary>Abstract</summary>
In this work, we investigate laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions. A novel method is introduced that learns to augment a surgeon's behavior in image space through object motion invariant image registration via homographies. Contrary to existing approaches, no geometric assumptions are made and no depth information is necessary, enabling immediate translation to a robotic setup. Deviating from the dominant approach in the literature which consist of following a surgical tool, we do not handcraft the objective and no priors are imposed on the surgical scene, allowing the method to discover unbiased policies. In this new research field, significant improvements are demonstrated over two baselines on the Cholec80 and HeiChole datasets, showcasing an improvement of 47% over camera motion continuation. The method is further shown to indeed predict camera motion correctly on the public motion classification labels of the AutoLaparo dataset. All code is made accessible on GitHub.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了通过imitating Learning自逆 Laparoscopic 摄像头运动的自动化。我们提出了一种新的方法，可以在图像空间通过对象运动不变的图像 регистрациюvia homographies来增强 Surgeon 的行为。与现有方法不同，我们没有做任何几何假设，也没有需要深度信息，因此可以立即翻译到Robotic 设置。与文献中主流的方法不同，我们没有手动定义目标，也没有对手术场景做任何假设，因此方法可以发现无偏的策略。在这个新的研究领域中，我们示出了在Cholec80 和 HeiChole 数据集上显著提高，比对照续摄像头运动的Camera Motion Continuation 提高47%。此外，我们还证明了该方法可以正确预测摄像头运动在AutoLaparo 数据集上的公共运动分类标签上。所有代码都已经公开在 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging"><a href="#Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging" class="headerlink" title="Synthetic white balancing for intra-operative hyperspectral imaging"></a>Synthetic white balancing for intra-operative hyperspectral imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12791">http://arxiv.org/abs/2307.12791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Bahl, Conor C. Horgan, Mirek Janatka, Oscar J. MacCormac, Philip Noonan, Yijing Xie, Jianrong Qiu, Nicola Cavalcanti, Philipp Fürnstahl, Michael Ebner, Mads S. Bergholt, Jonathan Shapey, Tom Vercauteren</li>
<li>For: The paper is written for the purpose of demonstrating the need for in situ white references in hyperspectral imaging for surgical applications, and proposing a novel, sterile, synthetic reference construction algorithm to address this need.* Methods: The paper uses a composite image from a video of a standard sterile ruler to create the synthetic reference, and models the reference as the product of independent spatial and spectral components, with a scalar factor accounting for gain, exposure, and light intensity.* Results: The paper shows that the synthetic references achieve median pixel-by-pixel errors lower than 6.5% and produce similar reconstructions and errors to an ideal reference, and that the algorithm integrated well into surgical workflow with median pixel-by-pixel errors of 4.77%, while maintaining good spectral and color reconstruction.<details>
<summary>Abstract</summary>
Hyperspectral imaging shows promise for surgical applications to non-invasively provide spatially-resolved, spectral information. For calibration purposes, a white reference image of a highly-reflective Lambertian surface should be obtained under the same imaging conditions. Standard white references are not sterilizable, and so are unsuitable for surgical environments. We demonstrate the necessity for in situ white references and address this by proposing a novel, sterile, synthetic reference construction algorithm. The use of references obtained at different distances and lighting conditions to the subject were examined. Spectral and color reconstructions were compared with standard measurements qualitatively and quantitatively, using $\Delta E$ and normalised RMSE respectively. The algorithm forms a composite image from a video of a standard sterile ruler, whose imperfect reflectivity is compensated for. The reference is modelled as the product of independent spatial and spectral components, and a scalar factor accounting for gain, exposure, and light intensity. Evaluation of synthetic references against ideal but non-sterile references is performed using the same metrics alongside pixel-by-pixel errors. Finally, intraoperative integration is assessed though cadaveric experiments. Improper white balancing leads to increases in all quantitative and qualitative errors. Synthetic references achieve median pixel-by-pixel errors lower than 6.5% and produce similar reconstructions and errors to an ideal reference. The algorithm integrated well into surgical workflow, achieving median pixel-by-pixel errors of 4.77%, while maintaining good spectral and color reconstruction.
</details>
<details>
<summary>摘要</summary>
高spectral成像显示在手术应用中具有潜在的优势，能够非侵入式地在空间上提供 spectral信息。为了进行准确的均衡，需要在同一种 imaging 条件下获得一个白色参照图像，但标准的白色参照图像不能sterilizable，因此不适用于手术环境。我们提出了一种新的、sterile、Synthetic参照图像建构算法。我们测试了不同距离和照明条件下的参照图像的使用，并与标准测量进行比较。我们使用了ΔE和normalized RMSE两种指标进行评估。我们的算法使用了一个标准 sterile 的测量仪表，并对其进行了补做。参照图像被视为独立的空间和spectral组分的乘积，以及一个权值补做照明、曝光和光强。我们对synthetic参照图像与理想 pero non-sterile 参照图像进行了比较，并使用了相同的指标进行评估。最后，我们通过实验评估了这种算法在手术过程中的integrability。不当的白平衡会导致所有量化和质量错误的增加。synthetic参照图像的 median 像素误差低于6.5%，并且生成了与理想参照图像类似的重建和错误。我们的算法在手术工作流中融合了良好的 spectral和color重建，并且 median 像素误差为4.77%。
</details></li>
</ul>
<hr>
<h2 id="ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution"><a href="#ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution" class="headerlink" title="ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution"></a>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12751">http://arxiv.org/abs/2307.12751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee</li>
<li>for: 提高单张图像超分辨率（SISR）的性能，不使用任何对应的训练数据。</li>
<li>methods: 提出了一种新的可逆扩率函数（ICF），可以扩大输入图像，然后使用不同的扩率条件恢复原始输入图像。基于该ICF，提出了一种新的无监督SISR框架（ICF-SRSR）。</li>
<li>results: 经验表明，提出的ICF-SRSR方法在实际世界 scenarios中可以很好地处理SISR任务，并且与现有的监督&#x2F;无监督方法在公共 benchmark datasets上展现了相似的性能。<details>
<summary>Abstract</summary>
Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice because of the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. By leveraging the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.
</details>
<details>
<summary>摘要</summary>
Single image super-resolution (SISR) 是一个具有挑战性的不定系数问题，旨在将给定的低分辨率 (LR) 图像提升到高分辨率 (HR) 对应的图像。由于实际获得LR-HR训练对的困难，现有的方法通常是通过简化的下采样算法，如比 Example: bicubic，进行训练。这种方法在实践中可能会存在问题，因为生成的Synthesized和实际世界LR图像之间存在很大的差距。为了解决这个问题，我们提出了一种新的减少函数 (ICF)，可以将输入图像缩放，然后使用不同的缩放比例来恢复原始输入。通过利用我们提出的ICF，我们建立了一种新的自动编码SR框架 (ICF-SRSR)，可以在不使用任何paired/unpaired训练数据的情况下进行SR任务。此外，我们的ICF-SRSR可以生成可靠和可行的LR-HR对，这可以使现有的supervised SR网络更加可靠。我们的实验表明，我们的ICF-SRSR可以在不使用任何训练数据的情况下处理SR任务，并且在实际世界 scenario 中表现出色。我们的ICF-SRSR在与现有的方法进行比较时，在公共的benchmark datasets上表现出了相当的性能。
</details></li>
</ul>
<hr>
<h2 id="Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction"><a href="#Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction" class="headerlink" title="Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction"></a>Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12717">http://arxiv.org/abs/2307.12717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangduo Xie, Matthew B. Blaschko</li>
<li>for: 针对CT图像损坏的金属artifacts，提高临床诊断的精度。</li>
<li>methods: 提出了一种基于Dense Transformer的增强编码网络（DTEC-Net），利用高阶杂分解编码器和转换器来获得长距离匹配的紧密编码序列。然后，提出了第二阶杂分解方法来改进密集序列的解码过程。</li>
<li>results: 对一个标准测试集进行了广泛的实验和模型说明，证明DTEC-Net的有效性，其在降低金属artifacts的同时保留了更多的细节Texture。与之前的状态统计方法相比，DTEC-Net显著提高了图像质量。<details>
<summary>Abstract</summary>
CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence's decoding process. Extensive experiments and model discussions illustrate DTEC-Net's effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Low-complexity-Overfitted-Neural-Image-Codec"><a href="#Low-complexity-Overfitted-Neural-Image-Codec" class="headerlink" title="Low-complexity Overfitted Neural Image Codec"></a>Low-complexity Overfitted Neural Image Codec</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12706">http://arxiv.org/abs/2307.12706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Orange-OpenSource/Cool-Chic">https://github.com/Orange-OpenSource/Cool-Chic</a></li>
<li>paper_authors: Thomas Leguay, Théo Ladune, Pierrick Philippe, Gordon Clare, Félix Henry</li>
<li>for: 这个论文是为了提出一种具有减少复杂度的神经网络图像编码器，该编码器可以对输入图像进行适应参数过滤。</li>
<li>methods: 该论文使用了自适应神经网络，并通过优化训练过程和使用轻量级模块来降低编码器的复杂度。</li>
<li>results: 该论文的方法可以与 autoencoder 和 HEVC 比肩，并且在不同的编码条件下具有14%的rate reduction，同时保持相似的复杂度。<details>
<summary>Abstract</summary>
We propose a neural image codec at reduced complexity which overfits the decoder parameters to each input image. While autoencoders perform up to a million multiplications per decoded pixel, the proposed approach only requires 2300 multiplications per pixel. Albeit low-complexity, the method rivals autoencoder performance and surpasses HEVC performance under various coding conditions. Additional lightweight modules and an improved training process provide a 14% rate reduction with respect to previous overfitted codecs, while offering a similar complexity. This work is made open-source at https://orange-opensource.github.io/Cool-Chic/
</details>
<details>
<summary>摘要</summary>
我们提出了一种减少复杂性的神经图像编码器，其将解码器参数过拟合到输入图像。而自动编码器可能需要每个解码ixel进行数百万次乘法运算，而我们的方法只需要每个解码ixel进行2300次乘法运算。虽然具有较低的复杂性，我们的方法与自动编码器的性能相当，甚至超过HEVC的编码性能在不同的编码条件下。此外，我们还提供了一些轻量级模块和改进的训练过程，可以对前一代过拟合编码器进行14%的比较率减少，同时保持相似的复杂性。该工作将在https://orange-opensource.github.io/Cool-Chic/上开源。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems"><a href="#Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems" class="headerlink" title="Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems"></a>Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12700">http://arxiv.org/abs/2307.12700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abderrahim Halimi, Jakeoung Koo, Stephen McLaughlin</li>
<li>for: 这篇论文主要用于描述一种基于深度学习的3D单光子探测器的重建和超分辨率方法。</li>
<li>methods: 该方法基于一种卷积 Bayesian 模型，可以在高噪音环境下提供最佳估计，同时具有改进的网络解释性。</li>
<li>results: 与现有的学习基于方法相比，该算法具有减少可训练参数数量、更高的噪音耐受度和系统响应函数模型化不足的问题，同时提供了更多的估计信息，包括不确定性度量。  Synthetic and real data 比较表明，该算法可以与现有算法相比，提供类似的推理质量和计算复杂度。<details>
<summary>Abstract</summary>
Deploying 3D single-photon Lidar imaging in real world applications faces several challenges due to imaging in high noise environments and with sensors having limited resolution. This paper presents a deep learning algorithm based on unrolling a Bayesian model for the reconstruction and super-resolution of 3D single-photon Lidar. The resulting algorithm benefits from the advantages of both statistical and learning based frameworks, providing best estimates with improved network interpretability. Compared to existing learning-based solutions, the proposed architecture requires a reduced number of trainable parameters, is more robust to noise and mismodelling of the system impulse response function, and provides richer information about the estimates including uncertainty measures. Results on synthetic and real data show competitive results regarding the quality of the inference and computational complexity when compared to state-of-the-art algorithms. This short paper is based on contributions published in [1] and [2].
</details>
<details>
<summary>摘要</summary>
<<SYS>>将3D单 фотоン探测技术应用于实际场景中存在多种挑战，包括高噪声环境和探测器有限分辨率。这篇论文提出了基于深度学习的bayesian模型的推算和超Resolution算法，以解决3D单 фотоン探测中的重要问题。该算法利用了统计和学习两个框架的优点，提供了最佳估计值，同时具有改进的网络解释性。与现有的学习型解决方案相比，提出的架构具有较少的可训练参数量、更高的噪声和系统响应函数模型化不正确率，并提供了更多的估计值和不确定度测量。对于synthetic和实际数据进行了比较，结果显示了与当前状态艺术算法相当的质量和计算复杂度。这篇短文基于[1]和[2]的贡献。
</details></li>
</ul>
<hr>
<h2 id="Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation"><a href="#Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation" class="headerlink" title="Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation"></a>Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12634">http://arxiv.org/abs/2307.12634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Qi Su, Na Wang, Jiawen Xie, Yinan Chen, Xiaofan Zhang</li>
<li>For:  automatic lung lobe segmentation algorithm for the diagnosis and treatment of lung diseases* Methods:  task-specific loss function to pay attention to the area around the pulmonary fissure, end-to-end pulmonary fissure generation method, registration-based loss function to alleviate convergence difficulty* Results:  achieved 97.83% and 94.75% dice scores on private dataset STLB and public LUNA16 dataset respectively.<details>
<summary>Abstract</summary>
The automatic lung lobe segmentation algorithm is of great significance for the diagnosis and treatment of lung diseases, however, which has great challenges due to the incompleteness of pulmonary fissures in lung CT images and the large variability of pathological features. Therefore, we propose a new automatic lung lobe segmentation framework, in which we urge the model to pay attention to the area around the pulmonary fissure during the training process, which is realized by a task-specific loss function. In addition, we introduce an end-to-end pulmonary fissure generation method in the auxiliary pulmonary fissure segmentation task, without any additional network branch. Finally, we propose a registration-based loss function to alleviate the convergence difficulty of the Dice loss supervised pulmonary fissure segmentation task. We achieve 97.83% and 94.75% dice scores on our private dataset STLB and public LUNA16 dataset respectively.
</details>
<details>
<summary>摘要</summary>
自动肺lobSeg算法对肺病诊断和治疗具有很大的重要性，但是受到肺CT图像的杏仁缺失和疾病特征的大量变化所带来的挑战。因此，我们提出了一种新的自动肺lobSeg框架，其中我们要求模型在训练过程中对杏仁附近区域进行注意力。我们实现了这一点通过任务特定的损失函数。此外，我们还提出了一种不含额外网络分支的杏仁生成方法，以及一种基于准确Registration的损失函数，以解决约瑟分解损失supervised杏仁分 segmentation任务的困难。在我们的私有数据集STLB和公共数据集LUNA16上，我们实现了97.83%和94.75%的 dice分数。
</details></li>
</ul>
<hr>
<h2 id="Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI"><a href="#Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI" class="headerlink" title="Sparse annotation strategies for segmentation of short axis cardiac MRI"></a>Sparse annotation strategies for segmentation of short axis cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12619">http://arxiv.org/abs/2307.12619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Stein, Maxime Di Folco, Julia Schnabel</li>
<li>for: 本研究旨在探讨使用少量标注数据进行心脏MRI分割的方法，以优化标注成本和提高分割性能。</li>
<li>methods: 我们采用了减少数据量和标注数量的方法，包括减少数据量和标注数量，以及使用转移学习和数据增强技术。</li>
<li>results: 我们的实验结果表明，训练使用少量标注数据可以达到0.85的Dice分数和与全数据集相当的性能。此外，我们发现，在中部层的标注更加有价值，而胸部区域的标注最差。在评估量据集对比中，更多的层标注比更多的量据集具有更高的分割性能。因此，建议在标注时尽量标注中部层，而不是标注更多的量据集。<details>
<summary>Abstract</summary>
Short axis cardiac MRI segmentation is a well-researched topic, with excellent results achieved by state-of-the-art models in a supervised setting. However, annotating MRI volumes is time-consuming and expensive. Many different approaches (e.g. transfer learning, data augmentation, few-shot learning, etc.) have emerged in an effort to use fewer annotated data and still achieve similar performance as a fully supervised model. Nevertheless, to the best of our knowledge, none of these works focus on which slices of MRI volumes are most important to annotate for yielding the best segmentation results. In this paper, we investigate the effects of training with sparse volumes, i.e. reducing the number of cases annotated, and sparse annotations, i.e. reducing the number of slices annotated per case. We evaluate the segmentation performance using the state-of-the-art nnU-Net model on two public datasets to identify which slices are the most important to annotate. We have shown that training on a significantly reduced dataset (48 annotated volumes) can give a Dice score greater than 0.85 and results comparable to using the full dataset (160 and 240 volumes for each dataset respectively). In general, training on more slice annotations provides more valuable information compared to training on more volumes. Further, annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, and the apical region the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details>
<details>
<summary>摘要</summary>
短轴心臓MRI分割是一个广泛研究的话题，现有一些最新的模型在指导下达到了出色的结果。然而，对MRIVolume进行标注是时间consuming和expensive。许多不同的方法（如转移学习、数据扩展、少数学习等）在尝试使用 fewer annotated data 并且达到类似于全指导模型的性能。然而，据我们所知，这些工作没有关注于哪些MRI Volume slice是最重要的标注，以达到最佳分割结果。在这篇文章中，我们 investigate了在减少 annotated volume 和 sparse annotations 下的训练效果。我们使用了state-of-the-art nnU-Net模型对两个公共数据集进行评估，以确定哪些slice是最重要的标注。我们发现，通过减少数据集至48个标注Volume可以达到Dice分数大于0.85，并且与使用全数据集（160和240个Volume）的结果相当。总的来说，训练更多的slice标注比训练更多的Volume更有价值的信息。此外，从MRI Volume 中间部分标注slice最有利于分割性能，而apical区域最差。当评估 annotating Volume 和 slice 之间的负担比，更好的策略是annotating as many slices as possible 而不是 annotating more Volume。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains"><a href="#Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains" class="headerlink" title="Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains"></a>Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12618">http://arxiv.org/abs/2307.12618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Di Folco, Cosmin Bercea, Julia A. Schnabel</li>
<li>for: 本研究旨在提高深度生成模型的控制性，通过选择性地修改数据特征进行数据生成和修饰。</li>
<li>methods: 本研究使用了Variational Autoencoders (VAEs)，并通过添加对偏好损失的限制来提高模型的控制性。</li>
<li>results: 实验表明，提出的Attributed Soft Introspective VAE（Attri-SIVAE）方法可以在不同的MRI数据集上达到同等的重建和规范化性，而且在不同的数据集上也可以保持同等的规范化水平，不同于相比方法。<details>
<summary>Abstract</summary>
Deep generative models have emerged as influential instruments for data generation and manipulation. Enhancing the controllability of these models by selectively modifying data attributes has been a recent focus. Variational Autoencoders (VAEs) have shown promise in capturing hidden attributes but often produce blurry reconstructions. Controlling these attributes through different imaging domains is difficult in medical imaging. Recently, Soft Introspective VAE leverage the benefits of both VAEs and Generative Adversarial Networks (GANs), which have demonstrated impressive image synthesis capabilities, by incorporating an adversarial loss into VAE training. In this work, we propose the Attributed Soft Introspective VAE (Attri-SIVAE) by incorporating an attribute regularized loss, into the Soft-Intro VAE framework. We evaluate experimentally the proposed method on cardiac MRI data from different domains, such as various scanner vendors and acquisition centers. The proposed method achieves similar performance in terms of reconstruction and regularization compared to the state-of-the-art Attributed regularized VAE but additionally also succeeds in keeping the same regularization level when tested on a different dataset, unlike the compared method.
</details>
<details>
<summary>摘要</summary>
深度生成模型已经成为数据生成和修饰的重要工具。提高这些模型的可控性，通过选择性地修改数据属性，是最近的研究焦点。变量自动编码器（VAEs）可以捕捉隐藏属性，但经常生成模糊的重建。在医学成像中，控制这些属性通过不同的成像频谱是困难的。最近，软 introspective VAE 利用了 VAEs 和生成对抗网络（GANs）的优点，通过在 VAE 训练中添加对抗损失来提高图像生成能力。在这项工作中，我们提出了具有属性规则化损失的 Attributed Soft Introspective VAE（Attri-SIVAE）。我们通过实验评估该方法在不同的cardiac MRI数据集上的性能。该方法与状态uset-of-the-art Attributed regularized VAE 相似的重建和规则化性能，并且在不同的数据集上保持了同等的规则化水平，不同于相比方法。
</details></li>
</ul>
<hr>
<h2 id="AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection"><a href="#AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection" class="headerlink" title="AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection"></a>AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03766">http://arxiv.org/abs/2308.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Mall, Sanchit Kabra, Ankur Lhila, Pawan Ajmera</li>
<li>for: 这个研究论文旨在提供一个自动化的豇豉病诊断框架，用于早期检测豇豉作物中的病诊断。</li>
<li>methods: 该框架使用多spectral图像，结合了深度学习网络来提取特征和分割方法，以识别豇豉作物和其相关的病诊断。</li>
<li>results: 实验结果表明，该框架可以有效地检测豇豉作物中的多种病诊断，包括粉刺虫、芽虫和叶褪病等。<details>
<summary>Abstract</summary>
This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, including powdery mildew, anthracnose, and leaf blight. The framework achieves state-of-the-art performance on the custom hand-collected dataset and contributes to the field of automated disease detection in agriculture, offering a practical solution for early identification of diseases in maize crops advanced machine learning techniques and deep learning architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal"><a href="#Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal" class="headerlink" title="Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal"></a>Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02405">http://arxiv.org/abs/2308.02405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arpita Paul, Avik Kumar Das, Manas Rakshit, Ankita Ray Chowdhury, Susmita Saha, Hrishin Roy, Sajal Sarkar, Dongiri Prasanth, Eravelli Saicharan<br>for:多种心脏病的自动检测和分类可能会减少心脏疾病的死亡率。本研究提出了基于单通道电cardiogram（ECG）信号的多类刺激识别算法。methods:在本研究中，使用心脏自变性（HRV）、形态特征和wavelet幂特征，通过机器学习基于Random Forest分类器进行检测。results:使用HRV和时域形态特征时，获得了85.11%的准确率、85.11%的敏感度、85.07%的精度和85.00%的F1分数。使用HRV和wavelet幂特征时，性能提高到90.91%的准确率、90.91%的敏感度、90.96%的精度和90.87%的F1分数。实验结果表明，提出的方案可以有效地从单通道ECG记录中检测多种刺激。<details>
<summary>Abstract</summary>
Arrhythmia, an abnormal cardiac rhythm, is one of the most common types of cardiac disease. Automatic detection and classification of arrhythmia can be significant in reducing deaths due to cardiac diseases. This work proposes a multi-class arrhythmia detection algorithm using single channel electrocardiogram (ECG) signal. In this work, heart rate variability (HRV) along with morphological features and wavelet coefficient features are utilized for detection of 9 classes of arrhythmia. Statistical, entropy and energy-based features are extracted and applied to machine learning based random forest classifiers. Data used in both works is taken from 4 broad databases (CPSC and CPSC extra, PTB-XL, G12EC and Chapman-Shaoxing and Ningbo Database) made available by Physionet. With HRV and time domain morphological features, an average accuracy of 85.11%, sensitivity of 85.11%, precision of 85.07% and F1 score of 85.00% is obtained whereas with HRV and wavelet coefficient features, the performance obtained is 90.91% accuracy, 90.91% sensitivity, 90.96% precision and 90.87% F1 score. The detailed analysis of simulation results affirms that the presented scheme effectively detects broad categories of arrhythmia from single-channel ECG records. In the last part of the work, the proposed classification schemes are implemented on hardware using Raspberry Pi for real time ECG signal classification.
</details>
<details>
<summary>摘要</summary>
心动过速病（Arrhythmia）是心血管疾病中最常见的一种。自动检测和识别Arrhythmia可以有效降低心血管疾病的死亡率。这项工作提出了基于单通道电cardiogram（ECG）信号的多类Arrhythmia检测算法。在这项工作中，利用心跳变化（HRV）以及形态特征和wavelet幅特征来检测9种类型的Arrhythmia。通过提取统计、熵和能量基本特征，并应用机器学习基于Random Forest分类器，实现了高精度的Arrhythmia检测。数据来源于Physionet提供的4个广泛数据库（CPSC和CPSC extra、PTB-XL、G12EC和Chapman-Shaoxing和Ningbo数据库）。使用HRV和时域形态特征时，取得了85.11%的准确率、85.11%的敏感度、85.07%的精度和85.00%的F1分数，而使用HRV和wavelet幅特征时，取得了90.91%的准确率、90.91%的敏感度、90.96%的精度和90.87%的F1分数。etailed分析结果表明，提出的方案可以有效地从单通道ECG记录中检测广泛的Arrhythmia类型。最后，提出的分类方案在硬件上使用Raspberry Pi实现了实时ECG信号分类。
</details></li>
</ul>
<hr>
<h2 id="4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network"><a href="#4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network" class="headerlink" title="4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network"></a>4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12377">http://arxiv.org/abs/2307.12377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzam Tajdari, Toon Huysmans, Xinhe Yao, Jun Xu, Yu Song</li>
<li>for: 该论文旨在帮助研究人员更好地理解动态弹性人体部件的特征，通过基于多个异步摄像机捕获的4D扫描数据进行重建。</li>
<li>methods: 该论文提出了一种通用框架，包括：1）使用非RIGID迭代最近最远点对精度找到和对准不同摄像机捕获的3D扫描数据中的动态特征；2）使用一种新型的ADGC-LSTM网络将不同摄像机捕获的3D扫描数据同步到特定摄像机的时间轴上；3）使用非RIGID注准方法将同步化的3D扫描数据注准到高质量模板中。</li>
<li>results: 该论文采用了一种新开发的4D脚部扫描仪，并将数据集分为58名参与者的15帧&#x2F;秒4D形态数据集（共116个脚部，包括5147帧的3D扫描数据），覆盖了脚步征的重要阶段。结果表明提出的方法有效地同步异步的4D扫描数据，特别是通过使用提出的ADGC-LSTM网络进行同步。<details>
<summary>Abstract</summary>
4D scans of dynamic deformable human body parts help researchers have a better understanding of spatiotemporal features. However, reconstructing 4D scans based on multiple asynchronous cameras encounters two main challenges: 1) finding the dynamic correspondences among different frames captured by each camera at the timestamps of the camera in terms of dynamic feature recognition, and 2) reconstructing 3D shapes from the combined point clouds captured by different cameras at asynchronous timestamps in terms of multi-view fusion. In this paper, we introduce a generic framework that is able to 1) find and align dynamic features in the 3D scans captured by each camera using the nonrigid iterative closest-farthest points algorithm; 2) synchronize scans captured by asynchronous cameras through a novel ADGC-LSTM-based network, which is capable of aligning 3D scans captured by different cameras to the timeline of a specific camera; and 3) register a high-quality template to synchronized scans at each timestamp to form a high-quality 3D mesh model using a non-rigid registration method. With a newly developed 4D foot scanner, we validate the framework and create the first open-access data-set, namely the 4D feet. It includes 4D shapes (15 fps) of the right and left feet of 58 participants (116 feet in total, including 5147 3D frames), covering significant phases of the gait cycle. The results demonstrate the effectiveness of the proposed framework, especially in synchronizing asynchronous 4D scans using the proposed ADGC-LSTM network.
</details>
<details>
<summary>摘要</summary>
4D扫描技术为研究人体动态变形带来了更好的认知，但是通过多个异步相机重建4D扫描存在两大挑战：1）在不同相机拍摄时间点找到动态匹配，并通过动态特征识别将它们相互对应；2）将不同相机拍摄的点云数据 fusion 到一起，以便形成高质量的3D模型。在这篇论文中，我们提出了一种通用的框架，可以1）使用非RIGID迭代最近最远点算法来在不同相机拍摄的3D扫描中找到和对应动态特征；2）使用一种新型的ADGC-LSTM网络将不同相机拍摄的3D扫描同步到同一个时间轴上；3）使用非RIGID注册方法将同步化后的3D扫描与高质量模板进行对应，以形成高质量的3D mesh模型。我们使用一种新开发的4D脚部扫描仪来验证该框架，并创建了首个公共数据集，即4D脚部（15帧/秒），包括58名参与者的右和左脚的4D形状（共116个脚，包括5147帧），覆盖了走势过程中重要的阶段。结果表明提出的框架具有良好的效果，特别是在同步异步4D扫描中使用提出的ADGC-LSTM网络。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/eess.IV_2023_07_24/" data-id="clpxp6ca60181ee88b64z71e7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.SD_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T15:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.SD_2023_07_23/">cs.SD - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition"><a href="#A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition" class="headerlink" title="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition"></a>A meta learning scheme for fast accent domain expansion in Mandarin speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12262">http://arxiv.org/abs/2307.12262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang, Jian Yu</li>
<li>for: 这 paper 是为了解决中文识别技术中的方言域扩展问题，提高方言识别精度。</li>
<li>methods: 这 paper 使用了元学习技术，包括模型冻结和多元学习，实现快速方言域扩展。</li>
<li>results: 该方法在方言域扩展任务上达到了3%的相对提升，相比基线模型，在同样的测试集上提高了37%。此外，该方法也在大量数据上实现了4%的相对提升。<details>
<summary>Abstract</summary>
Spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details>
<details>
<summary>摘要</summary>
spoken languages show significant variation across mandarin and accent. despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. in this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. so we select meta-learning in the domain expansion task. this more essential learning will cause improved performance on accent domain extension tasks. we combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. in addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details></li>
</ul>
<hr>
<h2 id="MyVoice-Arabic-Speech-Resource-Collaboration-Platform"><a href="#MyVoice-Arabic-Speech-Resource-Collaboration-Platform" class="headerlink" title="MyVoice: Arabic Speech Resource Collaboration Platform"></a>MyVoice: Arabic Speech Resource Collaboration Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02503">http://arxiv.org/abs/2308.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</li>
<li>for: 增强阿拉伯语言技术的发展，收集和整理阿拉伯语言的语音数据。</li>
<li>methods: 使用互联网平台，征集大量的阿拉伯语言口音录音，并提供城市&#x2F;国家精细的口音选择功能。用户可以 switching roles，从记录者变成评估者，并提供反馈。平台还包括质量检查系统，过滤掉低质量和假 recording。</li>
<li>results: 实现了收集大量的阿拉伯语言口音数据，提供城市&#x2F;国家精细的口音选择功能，并且可以进行多方合作，汇集多种阿拉伯语言数据。<details>
<summary>Abstract</summary>
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
</details>
<details>
<summary>摘要</summary>
我团队介绍MyVoice，一个招待人寄语的平台，用于提高阿拉伯语言口音技术。该平台提供了大量地方口音数据的设计机会，并将其公共地发布。MyVoice让参与者可以选择城市/国家精细口音，并录制显示的语音。用户可以在角色之间切换，包括参与者和注释者。平台包含一个质量保证系统，过滤掉低质量和假语音记录，然后将其发送给验证。在验证阶段，参与者可以评估语音质量，注释和提供反馈，这些反馈会被管理员审核。此外，平台允许管理员添加新的数据或任务，以外语言和词汇收集，这些任务将被显示给参与者。因此，MyVoice平台可以促进多方合作，收集多样化和大量的阿拉伯语言数据。
</details></li>
</ul>
<hr>
<h2 id="Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase"><a href="#Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase" class="headerlink" title="Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase"></a>Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12232">http://arxiv.org/abs/2307.12232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram">https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram</a></li>
<li>paper_authors: Yoshiki Masuyama, Natsuki Ueno, Nobutaka Ono</li>
<li>for: 重建时域信号从低维спектроgram中</li>
<li>methods: 利用丰富的听取关系和时域信号之间的双层关系，并使用优化问题的形式来重建全带幅强度和相位信息</li>
<li>results: 对话、音乐和环境信号都有较好的重建效果<details>
<summary>Abstract</summary>
We propose an optimization-based method for reconstructing a time-domain signal from a low-dimensional spectral representation such as a mel-spectrogram. Phase reconstruction has been studied to reconstruct a time-domain signal from the full-band short-time Fourier transform (STFT) magnitude. The Griffin-Lim algorithm (GLA) has been widely used because it relies only on the redundancy of STFT and is applicable to various audio signals. In this paper, we jointly reconstruct the full-band magnitude and phase by considering the bi-level relationships among the time-domain signal, its STFT coefficients, and its mel-spectrogram. The proposed method is formulated as a rigorous optimization problem and estimates the full-band magnitude based on the criterion used in GLA. Our experiments demonstrate the effectiveness of the proposed method on speech, music, and environmental signals.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于优化的方法，用于从低维特征表示（如MEL spectrogram）重建时域信号。 phase reconstruction 已经研究过了从全带快时域傅立叙Transform（STFT）大小取得时域信号的重建方法。格里菲恩-林算法（GLA）在广泛使用，因为它只凭借 STFT 的重复性而工作，适用于各种音频信号。在这篇论文中，我们同时重建全带大小和频谱图中的相对关系，并基于这些关系进行优化问题的形式化表述。我们的实验表明，提议的方法在语音、音乐和环境信号上具有效果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation"><a href="#Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation" class="headerlink" title="Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation"></a>Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12231">http://arxiv.org/abs/2307.12231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe</li>
<li>for: 这个论文是为了研究听话筛选和识别的集成，以提高多个人识别性能。</li>
<li>methods: 本论文使用了多通道分离方法、面积基于的扩展射频映射和复杂spectral mapping，以及最佳的后续模型特征。</li>
<li>results: 研究人员使用了最新的自动学习表示(SSLR)来改进filterbank特征，并通过合理的训练策略来集成语音分离和识别。结果显示，该策略在噪音混响WHAMR!测试集上实现了2.5%单词错误率，与现有的面积基于MVDR扩展射频映射和filterbank集成的28.9%相比，显著提高了多个人识别性能。<details>
<summary>Abstract</summary>
Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
</details>
<details>
<summary>摘要</summary>
neuronal speech separation 已经取得了很大的进步，其与自动语音识别（ASR）的结合是实现多 speaker ASR 的重要方向。本工作提供了深入的Investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end。具体来说，我们探讨了多通道分离方法，Mask-based beamforming和复杂的 spectral mapping，以及ASR back-end模型中最佳的特征。我们使用了最近的自然语言学习表示（SSLR）作为特征，并从filterbank特征中提高了认知性能。为了进一步提高多 speaker recognition性能，我们提出了一种优化的训练策略，将 speech separation和recognition与SSLR结合使用。我们使用TF-GridNet-based complex spectral mapping和WavLM-based SSLR，在抗噪抗干扰 WHAMR! 测试集上实现了2.5% 词错率，与现有的mask-based MVDR beamforming和filterbank结合（28.9%）相比，显著超越了。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey"><a href="#Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey" class="headerlink" title="Backdoor Attacks against Voice Recognition Systems: A Survey"></a>Backdoor Attacks against Voice Recognition Systems: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13643">http://arxiv.org/abs/2307.13643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baochen Yan, Jiahe Lan, Zheng Yan</li>
<li>for: This paper aims to provide a comprehensive survey on backdoor attacks against Voice Recognition Systems (VRSs) and to discuss the feasibility of deploying classic backdoor defense methods and generic audio defense techniques on VRSs.</li>
<li>methods: The paper employs a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives, and analyzes the characteristic of different categories. It also reviews existing attack methods and classic backdoor defense methods, and discusses the feasibility of deploying them on VRSs.</li>
<li>results: The paper provides a thorough review of backdoor attacks against VRSs, and discusses the open issues and future research directions in this field. It also provides a comprehensive understanding of the vulnerabilities of VRSs to backdoor attacks and the potential solutions to mitigate these attacks.<details>
<summary>Abstract</summary>
Voice Recognition Systems (VRSs) employ deep learning for speech recognition and speaker recognition. They have been widely deployed in various real-world applications, from intelligent voice assistance to telephony surveillance and biometric authentication. However, prior research has revealed the vulnerability of VRSs to backdoor attacks, which pose a significant threat to the security and privacy of VRSs. Unfortunately, existing literature lacks a thorough review on this topic. This paper fills this research gap by conducting a comprehensive survey on backdoor attacks against VRSs. We first present an overview of VRSs and backdoor attacks, elucidating their basic knowledge. Then we propose a set of evaluation criteria to assess the performance of backdoor attack methods. Next, we present a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives and analyze the characteristic of different categories. After that, we comprehensively review existing attack methods and analyze their pros and cons based on the proposed criteria. Furthermore, we review classic backdoor defense methods and generic audio defense techniques. Then we discuss the feasibility of deploying them on VRSs. Finally, we figure out several open issues and further suggest future research directions to motivate the research of VRSs security.
</details>
<details>
<summary>摘要</summary>
声认系统（VRS）利用深度学习进行语音识别和说话人识别。它们在各种现实应用中广泛应用，从智能语音助手到电信监测和生物认证。然而，先前的研究表明，VRS受到后门攻击的威胁，这对VRS的安全性和隐私具有重要性。然而，现有的文献缺乏对这个话题的全面审查。这篇论文填补了这个研究空白，通过进行VRS对后门攻击的全面评估。我们首先提供VRS和后门攻击的概述，并提出评估后门攻击方法的评价标准。然后，我们提出了VRS对后门攻击的多维分类，并分析不同类别的特点。接着，我们对现有的攻击方法进行了全面的审查，并分析了它们的优缺点。此外，我们还评估了经典的后门防御方法和通用音频防御技术，并评估了它们在VRS上的可行性。最后，我们提出了一些未解决的问题，并建议未来研究VRS的安全性。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding"><a href="#Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding"></a>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer</li>
<li>for: 提高 END-to-END（E2E）语音理解（SLU）系统的 Robustness，使其在听写识别（ASR）错误时仍能准确理解语音。</li>
<li>methods: 我们提出了一种新的 E2E SLU 系统，利用 audio 和文本表示，并基于 ASR 假设的模态信息确定精度。我们采用了两种新技术：1）有效地编码 ASR 假设质量，2）有效地将其集成到 E2E SLU 模型中。</li>
<li>results: 我们在 STOP 数据集上进行了实验，并发现我们的方法可以提高准确率。我们还进行了分析，以证明我们的方法的有效性。<details>
<summary>Abstract</summary>
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
最近，端到端（E2E）的语音理解（SLU）系统在使用预训练的语音识别（ASR）模型时，变得更加有前途。这种方法使用单个模型，利用语音和文本表示从预训练的ASR模型中获取，并在设备上流动enario下超越传统的管道SLU系统。然而，E2E SLU系统仍然在文本表示质量低下时表现弱，这是因为ASR识别错误。为了解决这个问题，我们提出了一种新的E2E SLU系统，增强了对ASR错误的抗钝性。我们介绍了两种新技术：1）一种有效的ASR假设质量编码方法，2）一种有效的将其集成到E2E SLU模型中的方法。我们在STOP数据集上显示了准确性改进，并提供分析，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals"><a href="#Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals" class="headerlink" title="Estimating speaker direction on a humanoid robot with binaural acoustic signals"></a>Estimating speaker direction on a humanoid robot with binaural acoustic signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12129">http://arxiv.org/abs/2307.12129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Barot, Katja Mombaur, Ewen MacDonald</li>
<li>for: 这篇论文主要用于讲述一种用于人类对话者的位置估计方法，以实现人类与机器人之间的对话。</li>
<li>methods: 这篇论文使用了一种基于眼睛音源的方法来估计对话者的位置，并考虑了实时应用场景。这种方法在机器人人类头上实现了双耳声音源定位框架。</li>
<li>results: 经过实验和分析，这种方法可以在实时应用场景中提供有效的位置估计结果，并且可以适应不同的对话场景。同时，这种方法也可以减少延迟时间，以便实现实时对话。<details>
<summary>Abstract</summary>
To achieve human-like behaviour during speech interactions, it is necessary for a humanoid robot to estimate the location of a human talker. Here, we present a method to optimize the parameters used for the direction of arrival (DOA) estimation, while also considering real-time applications for human-robot interaction scenarios. This method is applied to binaural sound source localization framework on a humanoid robotic head. Real data is collected and annotated for this work. Optimizations are performed via a brute force method and a Bayesian model based method, results are validated and discussed, and effects on latency for real-time use are also explored.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为实现人类样式的语音互动， robot需要估计人类说话者的位置。我们现在提出一种优化DOA估计参数的方法，同时考虑实时应用场景。这种方法应用于人型机器人头部上的双耳声源定位框架。实际数据收集和标注，并对其进行优化。我们使用枚举方法和 Bayesian 模型基于方法进行优化，并对结果进行验证和讨论。我们还探讨了在实时使用中的延迟影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.SD_2023_07_23/" data-id="clpxp6c5w00wuee88equa9757" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/81/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/80/">80</a><a class="page-number" href="/page/81/">81</a><span class="page-number current">82</span><a class="page-number" href="/page/83/">83</a><a class="page-number" href="/page/84/">84</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/83/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
