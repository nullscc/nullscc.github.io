
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/82/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/cs.LG_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T10:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/cs.LG_2023_07_24/">cs.LG - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network"><a href="#QAmplifyNet-Pushing-the-Boundaries-of-Supply-Chain-Backorder-Prediction-Using-Interpretable-Hybrid-Quantum-Classical-Neural-Network" class="headerlink" title="QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network"></a>QAmplifyNet: Pushing the Boundaries of Supply Chain Backorder Prediction Using Interpretable Hybrid Quantum - Classical Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12906">http://arxiv.org/abs/2307.12906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abrar Jahin, Md Sakib Hossain Shovon, Md. Saiful Islam, Jungpil Shin, M. F. Mridha, Yuichi Okuyama</li>
<li>for: 这个研究的目的是为供应链管理系统提供精确的货物预测，以便优化存储控制、降低成本和提高顾客满意度。</li>
<li>methods: 本研究提出了一个新的方法olographical framework，使用量子灵感技术实现供应链货物预测，并且可以处理短期和不寻常的数据集。</li>
<li>results: 实验结果显示，QAmplifyNet模型在短期和不寻常的数据集上的预测效果比 класиical models、量子对集、量子神经网和深度强化学习模型更好。这个模型的可读性和可扩展性使其成为供应链管理中的理想解决方案。<details>
<summary>Abstract</summary>
Supply chain management relies on accurate backorder prediction for optimizing inventory control, reducing costs, and enhancing customer satisfaction. However, traditional machine-learning models struggle with large-scale datasets and complex relationships, hindering real-world data collection. This research introduces a novel methodological framework for supply chain backorder prediction, addressing the challenge of handling large datasets. Our proposed model, QAmplifyNet, employs quantum-inspired techniques within a quantum-classical neural network to predict backorders effectively on short and imbalanced datasets. Experimental evaluations on a benchmark dataset demonstrate QAmplifyNet's superiority over classical models, quantum ensembles, quantum neural networks, and deep reinforcement learning. Its proficiency in handling short, imbalanced datasets makes it an ideal solution for supply chain management. To enhance model interpretability, we use Explainable Artificial Intelligence techniques. Practical implications include improved inventory control, reduced backorders, and enhanced operational efficiency. QAmplifyNet seamlessly integrates into real-world supply chain management systems, enabling proactive decision-making and efficient resource allocation. Future work involves exploring additional quantum-inspired techniques, expanding the dataset, and investigating other supply chain applications. This research unlocks the potential of quantum computing in supply chain optimization and paves the way for further exploration of quantum-inspired machine learning models in supply chain management. Our framework and QAmplifyNet model offer a breakthrough approach to supply chain backorder prediction, providing superior performance and opening new avenues for leveraging quantum-inspired techniques in supply chain management.
</details>
<details>
<summary>摘要</summary>
供应链管理需要准确预测营销订单，以优化存储控制、降低成本和提高客户满意度。然而，传统的机器学习模型在大规模数据集和复杂关系下难以处理实际数据收集。本研究提出了一种新的方法olo Framework for Supply Chain Backorder Prediction，解决大数据集处理的挑战。我们的提议模型，QAmplifyNet，在短时间和不均衡数据集上预测营销订单非常有效。实验评估表明QAmplifyNet在经典模型、量子ensemble、量子神经网络和深度强化学习方面具有突出的优势。由于它可以处理短时间和不均衡数据集，因此在供应链管理中是一个 идеal的解决方案。为了提高模型可读性，我们使用了可解释人工智能技术。实际应用包括改善存储控制、减少营销订单和提高运营效率。QAmplifyNet可以轻松整合到实际供应链管理系统中，允许执行投入式决策和有效资源分配。未来的工作包括探索更多的量子静止技术、扩大数据集和探索其他供应链应用。本研究开启了量子计算在供应链优化中的潜力，为了 leveraging量子静止机器学习模型在供应链管理中提供了一个突破性的方法。我们的框架和QAmplifyNet模型为供应链营销订单预测提供了超越性能，开创了新的可能性，以及可以在供应链管理中应用量子静止技术。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs"><a href="#Universal-Approximation-Theorem-and-error-bounds-for-quantum-neural-networks-and-quantum-reservoirs" class="headerlink" title="Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs"></a>Universal Approximation Theorem and error bounds for quantum neural networks and quantum reservoirs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12904">http://arxiv.org/abs/2307.12904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Gonon, Antoine Jacquier</li>
<li>for: 这个论文是为了证明Quantum Neural Network可以用于精确地预测函数的目的。</li>
<li>methods: 这篇论文使用了Parameterised quantum circuits和随机量子Circuits来 aproximate classical functions。</li>
<li>results: 这篇论文提供了具体的错误 bound，证明一个Quantum Neural Network可以在某些情况下以 $\mathcal{O}(\varepsilon^{-2})$ 的参数和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ 个量子比特来实现精度 $\varepsilon&gt;0$ 的函数预测。<details>
<summary>Abstract</summary>
Universal approximation theorems are the foundations of classical neural networks, providing theoretical guarantees that the latter are able to approximate maps of interest. Recent results have shown that this can also be achieved in a quantum setting, whereby classical functions can be approximated by parameterised quantum circuits. We provide here precise error bounds for specific classes of functions and extend these results to the interesting new setup of randomised quantum circuits, mimicking classical reservoir neural networks. Our results show in particular that a quantum neural network with $\mathcal{O}(\varepsilon^{-2})$ weights and $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve accuracy $\varepsilon>0$ when approximating functions with integrable Fourier transform.
</details>
<details>
<summary>摘要</summary>
“ universal approximation 定理 是 classical neural network 的基础，提供了理论保证这些 later 能够 approximate  interessant 的映射。 recent results 表明这也可以在量子设置下实现，其中 classical 函数可以通过参数化 quantum circuit 的方式进行approximation。我们在这里提供了具体的误差 bound  для特定的函数类型，并将其扩展到 randomized quantum circuit 中，模拟 classical reservoir neural network。我们的结果显示，一个 quantum neural network  WITH $\mathcal{O}(\varepsilon^{-2})$  weights 和 $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits 就能够达到 $\varepsilon>0$ 的精度，当approximating functions with integrable Fourier transform。”Note: "Simplified Chinese" is a romanization of Chinese that uses simpler characters and grammar to facilitate typing and reading. It is not a standardized translation of Chinese, and the actual translation may vary depending on the context and the translator's preference.
</details></li>
</ul>
<hr>
<h2 id="Anytime-Model-Selection-in-Linear-Bandits"><a href="#Anytime-Model-Selection-in-Linear-Bandits" class="headerlink" title="Anytime Model Selection in Linear Bandits"></a>Anytime Model Selection in Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12897">http://arxiv.org/abs/2307.12897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parnian Kassraie, Aldo Pacchiano, Nicolas Emmenegger, Andreas Krause</li>
<li>for:  This paper is written for solving the problem of model selection in the context of bandit optimization, which is a challenging problem that requires balancing exploration and exploitation not only for action selection, but also for model selection.</li>
<li>methods:  The paper proposes a new method called ALEXP, which uses online learning algorithms that treat different models as experts and emulates full-information feedback to the online learner with a favorable bias-variance trade-off.</li>
<li>results:  The paper shows that ALEXP has an exponentially improved ($\log M$) dependence on the number of models $M$ for its regret, and has anytime guarantees on its regret without requiring knowledge of the horizon $n$ or relying on an initial purely exploratory stage.<details>
<summary>Abstract</summary>
Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details>
<details>
<summary>摘要</summary>
Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved dependence on $M$ for its regret, with a logarithmic dependence on $M$ (log$M$).ALEXP has anytime guarantees on its regret, and does not require knowledge of the horizon $n$ or an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
</details></li>
</ul>
<hr>
<h2 id="A-Statistical-View-of-Column-Subset-Selection"><a href="#A-Statistical-View-of-Column-Subset-Selection" class="headerlink" title="A Statistical View of Column Subset Selection"></a>A Statistical View of Column Subset Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12892">http://arxiv.org/abs/2307.12892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anavsood/css">https://github.com/anavsood/css</a></li>
<li>paper_authors: Anav Sood, Trevor Hastie</li>
<li>for: 选择一小集合的表变量从大数据集中</li>
<li>methods: 使用某些简统统计方法，如某些简单的概率模型，来实现维度减少</li>
<li>results: 该paper表明了CSS和主要变量选择是等价的，并且两者都可以视为最大化信息的最优化问题。此外，paper还介绍了如何使用这些连接来快速完成CSS，包括使用摘要统计数据进行CSS和在缺失和&#x2F;或抑制数据的情况下进行CSS。<details>
<summary>Abstract</summary>
We consider the problem of selecting a small subset of representative variables from a large dataset. In the computer science literature, this dimensionality reduction problem is typically formalized as Column Subset Selection (CSS). Meanwhile, the typical statistical formalization is to find an information-maximizing set of Principal Variables. This paper shows that these two approaches are equivalent, and moreover, both can be viewed as maximum likelihood estimation within a certain semi-parametric model. Using these connections, we show how to efficiently (1) perform CSS using only summary statistics from the original dataset; (2) perform CSS in the presence of missing and/or censored data; and (3) select the subset size for CSS in a hypothesis testing framework.
</details>
<details>
<summary>摘要</summary>
我团队正在考虑一个大数据集中选择一小子集的代表变量问题。在计算机科学文献中，这个维度减少问题通常被称为列子集选择（CSS）。在统计学文献中，这个问题通常被формализова为找到最优化信息的主要变量。这篇论文表明了这两种方法是等价的，并且它们都可以被视为在某种半 parametic 模型中的最大 LIKELIHOOD估计。使用这些连接，我们展示了如何：1. 使用原始数据集的摘要统计来快速完成 CSS。2. 在缺失和/或截断数据存在的情况下进行 CSS。3. 在假设检测框架中选择 CSS 中的子集大小。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Stereotype-Identification-through-Reasoning"><a href="#Interpretable-Stereotype-Identification-through-Reasoning" class="headerlink" title="Interpretable Stereotype Identification through Reasoning"></a>Interpretable Stereotype Identification through Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00071">http://arxiv.org/abs/2308.00071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak</li>
<li>for: 这篇研究的目的是探讨语言模型中的偏见，并将公平性集成到语言模型的开发过程中，以确保这些模型是不偏不倾的。</li>
<li>methods: 本研究使用了Vicuna-13B-v1.3进行零扩展 sterotype 识别 tasks，并 comparing the performance of scaling up from 13B to 33B 和 reasoning 的效果。</li>
<li>results: 研究发现，reasoning 可以帮助语言模型在离 Domain tasks 中提高精度，并且可以增加模型的解释力。<details>
<summary>Abstract</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details>
<details>
<summary>摘要</summary>
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.Here's the translation in Traditional Chinese: given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
</details></li>
</ul>
<hr>
<h2 id="Data-free-Black-box-Attack-based-on-Diffusion-Model"><a href="#Data-free-Black-box-Attack-based-on-Diffusion-Model" class="headerlink" title="Data-free Black-box Attack based on Diffusion Model"></a>Data-free Black-box Attack based on Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12872">http://arxiv.org/abs/2307.12872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo</li>
<li>for: 本研究旨在提高数据free黑盒攻击的效率和准确性，通过使用扩散模型生成数据进行训练代理模型。</li>
<li>methods: 本研究使用扩散模型生成数据，并提出了一种干扰代码增强（LCA）方法，以指导扩散模型生成数据。LCA方法可以使得生成的数据符合目标模型的批判标准，同时保持高的多样性。</li>
<li>results: 对于不同的目标模型，我们的LCA方法可以获得更高的攻击成功率，并且需要更少的查询预算。EXTensive experiments表明，我们的LCA方法可以提高数据free黑盒攻击的效率和准确性。<details>
<summary>Abstract</summary>
Since the training data for the target model in a data-free black-box attack is not available, most recent schemes utilize GANs to generate data for training substitute model. However, these GANs-based schemes suffer from low training efficiency as the generator needs to be retrained for each target model during the substitute training process, as well as low generation quality. To overcome these limitations, we consider utilizing the diffusion model to generate data, and propose a data-free black-box attack scheme based on diffusion model to improve the efficiency and accuracy of substitute training. Despite the data generated by the diffusion model exhibits high quality, it presents diverse domain distributions and contains many samples that do not meet the discriminative criteria of the target model. To further facilitate the diffusion model to generate data suitable for the target model, we propose a Latent Code Augmentation (LCA) method to guide the diffusion model in generating data. With the guidance of LCA, the data generated by the diffusion model not only meets the discriminative criteria of the target model but also exhibits high diversity. By utilizing this data, it is possible to train substitute model that closely resemble the target model more efficiently. Extensive experiments demonstrate that our LCA achieves higher attack success rates and requires fewer query budgets compared to GANs-based schemes for different target models.
</details>
<details>
<summary>摘要</summary>
因为目标模型的训练数据不可获得，大多数最新的方案使用GANs生成数据来训练代理模型。然而，这些GANs基于的方案受到低训练效率和低生成质量的限制。为了突破这些限制，我们考虑使用扩散模型生成数据，并提出了基于扩散模型的数据 свобо black-box攻击方案，以提高代理训练的效率和准确性。尽管扩散模型生成的数据具有高质量，但它们具有多样的领域分布和含有许多不符合目标模型的扩散标准的样本。为了使扩散模型更加适合目标模型，我们提出了幽默代码修饰（LCA）方法，以导引扩散模型生成数据。通过LCA的引导，扩散模型生成的数据不仅满足目标模型的扩散标准，而且具有高多样性。通过这些数据，我们可以更加快速地训练符合目标模型的代理模型。我们的LCA在不同的目标模型上实现了更高的攻击成功率和更少的查询预算。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs"><a href="#Stochastic-Step-wise-Feature-Selection-for-Exponential-Random-Graph-Models-ERGMs" class="headerlink" title="Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)"></a>Stochastic Step-wise Feature Selection for Exponential Random Graph Models (ERGMs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12862">http://arxiv.org/abs/2307.12862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helal El-Zaatari, Fei Yu, Michael R Kosorok</li>
<li>for: 这 paper 的目的是提供一种改进的 exponential random graph models (ERGMs) 模型，以更好地Capture 社交网络中的依赖关系。</li>
<li>methods: 该 paper 使用了一种新的方法，即选择内生变量（endogenous variable selection），以解决 ERGMs 中的degeneracy问题，并提高了网络模型的准确性。</li>
<li>results: 经验测试表明，该方法可以有效地避免 ERGMs 中的degeneracy问题，并提高了网络模型的准确性和可靠性。<details>
<summary>Abstract</summary>
Statistical analysis of social networks provides valuable insights into complex network interactions across various scientific disciplines. However, accurate modeling of networks remains challenging due to the heavy computational burden and the need to account for observed network dependencies. Exponential Random Graph Models (ERGMs) have emerged as a promising technique used in social network modeling to capture network dependencies by incorporating endogenous variables. Nevertheless, using ERGMs poses multiple challenges, including the occurrence of ERGM degeneracy, which generates unrealistic and meaningless network structures. To address these challenges and enhance the modeling of collaboration networks, we propose and test a novel approach that focuses on endogenous variable selection within ERGMs. Our method aims to overcome the computational burden and improve the accommodation of observed network dependencies, thereby facilitating more accurate and meaningful interpretations of network phenomena in various scientific fields. We conduct empirical testing and rigorous analysis to contribute to the advancement of statistical techniques and offer practical insights for network analysis.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)社交网络统计分析提供了许多有价值的网络互动现象的理解，但是准确地模型网络仍然是一项挑战，因为计算负担重要和需要考虑观察到的网络依赖关系。扩展随机图模型（ERGMs）在社交网络模型中表现出了扩展的潜力，可以通过包含内生变量来捕捉网络依赖关系。然而，使用ERGMs也存在多种挑战，包括ERGM异常性，这会生成无意义和不切实际的网络结构。为了解决这些挑战并改进协作网络的模型，我们提出了一种新的方法，即内生变量选择在ERGMs中。我们的方法目的是减少计算负担和更好地考虑观察到的网络依赖关系，从而为不同科学领域中的网络现象提供更准确和有意义的解释。我们进行了实际测试和严格分析，以贡献到统计技术的进步和为网络分析提供实用的指导。
</details></li>
</ul>
<hr>
<h2 id="A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis"><a href="#A-Real-World-WebAgent-with-Planning-Long-Context-Understanding-and-Program-Synthesis" class="headerlink" title="A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"></a>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12856">http://arxiv.org/abs/2307.12856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust</li>
<li>for: 本研究旨在提高自主浏览器的自然语言指令下的实际website上的性能。</li>
<li>methods: 研究人员提出了WebAgent Agent，该Agent可以根据自然语言指令完成实际website上的任务。WebAgent使用Flan-U-PaLM进行code生成和HTML-T5进行规划和摘要，以及使用本地和全局听力机制和杂xture-span噪声目标来解决HTML文档中的长 Span问题。</li>
<li>results: 研究人员通过实验表明，WebAgent在真实的website上提高了成功率超过50%，并且HTML-T5在解决HTML基本任务方面的成功率高于之前的SoTA，达到14.9%。<details>
<summary>Abstract</summary>
Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web navigation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that can complete the tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via generated Python programs from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our recipe improves the success on a real website by over 50%, and that HTML-T5 is the best model to solve HTML-based tasks; achieving 14.9% higher success rate than prior SoTA on the MiniWoB web navigation benchmark and better accuracy on offline task planning evaluation.
</details>
<details>
<summary>摘要</summary>
各种大型自然语言模型（LLM）在自主网络浏览方面最近很有进步，但实际网站上的性能仍然受到以下三种因素的影响：开放性、限制的上下文长度和HTML的适应性。我们介绍了WebAgent，一个根据自然语言指令完成实际网站上的任务的LML-驱动的代理人。WebAgent通过将指令分解成标准化的子指令、将长HTML文档摘要成任务相关的短报道，并通过生成的Python程序来操作网站。我们为WebAgent设计了Flan-U-PaLM，用于锚定代码生成，以及HTML-T5，一种新的适应HTML文档的预训练语言模型，使用本地和全球注意力机制，并结合长时间排除目标来计划和摘要。我们实际证明了我们的配方可以在真实网站上提高成功率高于50%，并且HTML-T5是解决HTML基本任务的最佳模型，比前一个SoTA在小型网站浏览 benchmark上的成功率高14.9%。
</details></li>
</ul>
<hr>
<h2 id="Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization"><a href="#Early-Neuron-Alignment-in-Two-layer-ReLU-Networks-with-Small-Initialization" class="headerlink" title="Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization"></a>Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12851">http://arxiv.org/abs/2307.12851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hancheng Min, René Vidal, Enrique Mallada</li>
<li>for: 这 paper 研究了使用梯度流的两层 ReLU 网络进行二分类训练，并且使用小初值。</li>
<li>methods: 我们分析了在训练集中的输入向量之间的相互关系，并且通过对神经元的方向动态进行仔细分析，提供了 $\mathcal{O}(\frac{\log n}{\sqrt{\mu})$ 上限 bounds on 训练时间，其中 $n$ 是数据点的数量，$\mu$ 是数据点之间的相互关系。</li>
<li>results: 我们的分析表明，在训练的早期阶段，神经元在第一层会尝试与输入数据进行对齐，并且在训练的晚期阶段，损失函数会逐渐逼近零，并且第一层的权重矩阵会变得相对低矩。数据实验表明了我们的理论发现。<details>
<summary>Abstract</summary>
This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文研究了使用梯度流的两层ReLU网络 для二分类问题的训练，并采用小初始化。我们考虑了一个具有良好分离的训练集：任何同样标签的输入数据对都是正相关的，任何不同标签的输入数据对都是负相关的。我们的分析表明，在训练的早期阶段，第一层神经元尝试与输入数据进行对齐，具体来说，它们会对应于第二层神经元的权重进行对齐。通过仔细分析神经元的方向动态，我们可以提供一个 $\mathcal{O}(\frac{\log n}{\sqrt{\mu})$ 上限于时间内所有神经元达到好的对齐度，其中 $n$ 是数据点的数量，$\mu$ 是数据分离度。训练过程后期，损失函数会随着时间的增长而逐渐减少，并且第一层神经元的权重矩阵会 aproximately 变为低级 matrix。实验结果表明，这些理论发现都能够在 MNIST 数据集上得到支持。
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials"><a href="#Efficiently-Learning-One-Hidden-Layer-ReLU-Networks-via-Schur-Polynomials" class="headerlink" title="Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials"></a>Efficiently Learning One-Hidden-Layer ReLU Networks via Schur Polynomials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12840">http://arxiv.org/abs/2307.12840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Daniel M. Kane</li>
<li>for: 学习一个 linear combination of $k$ ReLU 活化器在标准 Gaussian 分布上的 $\mathbb{R}^d$ 上的问题，使用标准损失函数。</li>
<li>methods: 使用 tensor  decomposition 技术来识别一个子空间，使得所有 $O(k)$-order  моменты在正交方向上都很小。</li>
<li>results: 提供了一个高效的算法，其复杂度为 $(dk&#x2F;\epsilon)^{O(k)}$, 比之前的算法更为优化。<details>
<summary>Abstract</summary>
We study the problem of PAC learning a linear combination of $k$ ReLU activations under the standard Gaussian distribution on $\mathbb{R}^d$ with respect to the square loss. Our main result is an efficient algorithm for this learning task with sample and computational complexity $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ is the target accuracy. Prior work had given an algorithm for this problem with complexity $(dk/\epsilon)^{h(k)}$, where the function $h(k)$ scales super-polynomially in $k$. Interestingly, the complexity of our algorithm is near-optimal within the class of Correlational Statistical Query algorithms. At a high-level, our algorithm uses tensor decomposition to identify a subspace such that all the $O(k)$-order moments are small in the orthogonal directions. Its analysis makes essential use of the theory of Schur polynomials to show that the higher-moment error tensors are small given that the lower-order ones are.
</details>
<details>
<summary>摘要</summary>
我们研究一个PAC学习问题，即以$k$个ReLU激活函数为线性结构，在标准 Gaussian 分布下的 $\mathbb{R}^d$ 上对对于方差损失函数进行学习。我们的主要结果是一个有效的学习算法，其sample和computational Complexity为 $(dk/\epsilon)^{O(k)}$, where $\epsilon>0$ 是目标精度。对比之下，先前的算法的复杂度为 $(dk/\epsilon)^{h(k)}$, where $h(k)$  scales 超 polynomial 地增长。有趣的是，我们的算法的复杂度几乎是near-optimal  within the class of Correlational Statistical Query algorithms。在高阶概念上，我们的算法使用了维度分解来识别一个子空间，使得所有 $O(k)$-order moments 在这个orthogonal direction 上都是小的。其分析将使用Schur多项式理论来显示，在这个子空间上，更高阶的error tensors 是小的，只要Lower-order ones 是。
</details></li>
</ul>
<hr>
<h2 id="Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering"><a href="#Learning-Provably-Robust-Estimators-for-Inverse-Problems-via-Jittering" class="headerlink" title="Learning Provably Robust Estimators for Inverse Problems via Jittering"></a>Learning Provably Robust Estimators for Inverse Problems via Jittering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12822">http://arxiv.org/abs/2307.12822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mli-lab/robust_reconstructors_via_jittering">https://github.com/mli-lab/robust_reconstructors_via_jittering</a></li>
<li>paper_authors: Anselm Krainovic, Mahdi Soltanolkotabi, Reinhard Heckel</li>
<li>for: 这篇论文 investigate whether jittering, a simple regularization technique, can be used to train deep neural networks to be worst-case robust for inverse problems.</li>
<li>methods: 论文使用了jittering regularization technique during training, and presents a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising.</li>
<li>results: 研究发现，jittering可以增强worst-case robustness，但可能不适用于 inverse problems beyond denoising。同时，论文还发现，使用实际数据进行训练可以提供一定的 robustness enhancement.<details>
<summary>Abstract</summary>
Deep neural networks provide excellent performance for inverse problems such as denoising. However, neural networks can be sensitive to adversarial or worst-case perturbations. This raises the question of whether such networks can be trained efficiently to be worst-case robust. In this paper, we investigate whether jittering, a simple regularization technique that adds isotropic Gaussian noise during training, is effective for learning worst-case robust estimators for inverse problems. While well studied for prediction in classification tasks, the effectiveness of jittering for inverse problems has not been systematically investigated. In this paper, we present a novel analytical characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising and show that jittering yields optimal robust denoisers. Furthermore, we examine jittering empirically via training deep neural networks (U-nets) for natural image denoising, deconvolution, and accelerated magnetic resonance imaging (MRI). The results show that jittering significantly enhances the worst-case robustness, but can be suboptimal for inverse problems beyond denoising. Moreover, our results imply that training on real data which often contains slight noise is somewhat robustness enhancing.
</details>
<details>
<summary>摘要</summary>
深度神经网络在反向问题中表现出色，但是神经网络可能对抗性或最坏情况的扰动敏感。这引起了训练神经网络是否可以有效地培养最坏情况Robust的问题。在这篇论文中，我们调查了在反向问题中是否可以使用扰动，一种简单的规范技术，来学习最坏情况Robust的估计器。虽然在预测类型任务中well studied，但是反向问题中扰动的效iveness尚未系统地研究。在这篇论文中，我们提供了一种新的分析 Characterization of the optimal $\ell_2$-worst-case robust estimator for linear denoising，并证明了扰动可以生成最优的Robust denoiser。此外，我们通过训练深度神经网络（U-net）对自然图像杂谔、减 convolution和加速核磁共振成像（MRI）进行实验。结果表明，扰动可以强化最坏情况的Robust性，但可能不适用于反向问题 beyond denoising。此外，我们的结果也表明，训练在真实数据上，通常含有些许噪声，可以提高Robust性。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks"><a href="#Maximal-Independent-Sets-for-Pooling-in-Graph-Neural-Networks" class="headerlink" title="Maximal Independent Sets for Pooling in Graph Neural Networks"></a>Maximal Independent Sets for Pooling in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13011">http://arxiv.org/abs/2307.13011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stevan Stanovic, Benoit Gaüzère, Luc Brun</li>
<li>for: 图像分类领域的进步</li>
<li>methods: 基于最大独立集的三种图像池化方法</li>
<li>results: 实验结果证明最大独立集约束对图像池化有 relevance<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have enabled major advances in image classification through convolution and pooling. In particular, image pooling transforms a connected discrete lattice into a reduced lattice with the same connectivity and allows reduction functions to consider all pixels in an image. However, there is no pooling that satisfies these properties for graphs. In fact, traditional graph pooling methods suffer from at least one of the following drawbacks: Graph disconnection or overconnection, low decimation ratio, and deletion of large parts of graphs. In this paper, we present three pooling methods based on the notion of maximal independent sets that avoid these pitfalls. Our experimental results confirm the relevance of maximal independent set constraints for graph pooling.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNNs）已经为图像分类带来重要的进步，通过卷积和聚合。特别是图像聚合将连接的离散网络转换为减少的网络，让减少函数考虑整个图像中的所有像素。然而，为图集而设计的pooling方法存在一些缺点，包括图集分离或过度连接、低减少比率和删除大量图集。在这篇论文中，我们提出了基于最大独立集的三种pooling方法，避免了这些缺点。我们的实验结果证明了最大独立集约束对图集聚合具有重要性。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions"><a href="#Causal-Fair-Machine-Learning-via-Rank-Preserving-Interventional-Distributions" class="headerlink" title="Causal Fair Machine Learning via Rank-Preserving Interventional Distributions"></a>Causal Fair Machine Learning via Rank-Preserving Interventional Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12797">http://arxiv.org/abs/2307.12797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/slds-lmu/paper_2023_cfml">https://github.com/slds-lmu/paper_2023_cfml</a></li>
<li>paper_authors: Ludwig Bothmann, Susanne Dandl, Michael Schomaker</li>
<li>for: 该论文旨在设计机器学习模型，以减少自动决策系统中的不公正性。</li>
<li>methods: 该论文提出了一种基于 causal thinking 的方法，通过引入保护属性来定义个体是否是normatively equal。该方法使用rank-preserving interventional distributions来定义一个FiND世界，并使用扭曲方法进行估计。</li>
<li>results: 该论文通过实验和实际数据 validate了该方法和模型的评价标准，并显示了该方法能够减少不公正性。<details>
<summary>Abstract</summary>
A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
</details>
<details>
<summary>摘要</summary>
一个决策可以被定义为公平的，如果对等的人进行对等的待遇，不同的人则不同的待遇。在设计自动化决策系统中减少不公的方面，我们应该采用 causal 思维。根据最近的提议，我们定义了一个人为在一个虚拟、normatively 愿望的 (FiND) 世界中是否是等值的。我们提议使用排名保持分布来定义这个FiND世界的估计量，并使用扭曲方法进行估计。我们对方法和模型的评价标准和验证结果进行了说明和验证，并通过实验数据和仿真数据来显示我们的扭曲方法能够有效地找到最受歧视的个体并减少不公。
</details></li>
</ul>
<hr>
<h2 id="Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification"><a href="#Compact-Capable-Harnessing-Graph-Neural-Networks-and-Edge-Convolution-for-Medical-Image-Classification" class="headerlink" title="Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification"></a>Compact &amp; Capable: Harnessing Graph Neural Networks and Edge Convolution for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12790">http://arxiv.org/abs/2307.12790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonrepo-keeper/gcnn-ec">https://github.com/anonrepo-keeper/gcnn-ec</a></li>
<li>paper_authors: Aryan Singh, Pepijn Van de Ven, Ciarán Eising, Patrick Denny</li>
<li>for: 这个研究探索了图形神经网络（Graph Neural Network，GNN）在医疗影像分类中的潜力。</li>
<li>methods: 我们提出了一种新的模型，具有结合图形神经网络和边弹击的特点，通过RGB通道特征值之间的连接强化关系，实现更好地表示关键图形节点之间的连接。</li>
<li>results: 我们的模型不仅与现有的深度神经网络（Deep Neural Network，DNN）相比，表现优化，且仅需1000个参数，训练时间和数据需求都被降低了。我们将这个GCNN模型与预训练的DNN进行比较，发现GCNN在医疗影像分类任务中表现出色，并鼓励进一步探索更进阶的图形基于模型，如图形注意力网络（Graph Attention Network，GAT）和图形自动编码器（Graph Auto-Encoder）在医疗影像领域的应用。<details>
<summary>Abstract</summary>
Graph-based neural network models are gaining traction in the field of representation learning due to their ability to uncover latent topological relationships between entities that are otherwise challenging to identify. These models have been employed across a diverse range of domains, encompassing drug discovery, protein interactions, semantic segmentation, and fluid dynamics research. In this study, we investigate the potential of Graph Neural Networks (GNNs) for medical image classification. We introduce a novel model that combines GNNs and edge convolution, leveraging the interconnectedness of RGB channel feature values to strongly represent connections between crucial graph nodes. Our proposed model not only performs on par with state-of-the-art Deep Neural Networks (DNNs) but does so with 1000 times fewer parameters, resulting in reduced training time and data requirements. We compare our Graph Convolutional Neural Network (GCNN) to pre-trained DNNs for classifying MedMNIST dataset classes, revealing promising prospects for GNNs in medical image analysis. Our results also encourage further exploration of advanced graph-based models such as Graph Attention Networks (GAT) and Graph Auto-Encoders in the medical imaging domain. The proposed model yields more reliable, interpretable, and accurate outcomes for tasks like semantic segmentation and image classification compared to simpler GCNNs
</details>
<details>
<summary>摘要</summary>
“基于图的神经网络模型在知识学习领域受到广泛应用，因为它们可以捕捉难以识别的实体之间的隐藏 topological 关系。这些模型在药物发现、蛋白质交互、semantic segmentation 和 fluid dynamics 等领域中得到应用。在本研究中，我们调查了医学图像分类中的可能性，并提出了一种新的模型，该模型将基于图的神经网络（GCNN）和边 convolution 结合在一起，通过RGB通道特征值之间的连接来强大地表示关键图节点之间的连接。我们的提出的模型不仅与现有的深度神经网络（DNN）性能相似，而且具有1000倍少的参数，从而减少了训练时间和数据需求。我们对MedMNIST 数据集类别进行比较，发现GCNN在医学图像分类中有良好的前景，并且鼓励进一步探索更高级的图基于模型，如图注意力网络（GAT）和图自动编码器（GAE）在医学图像分类领域。GCNN 模型在 semantic segmentation 和图像分类任务中提供了更可靠、可解释、高精度的结果，相比于简单的 GCNN 模型”
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer"><a href="#Deep-neural-network-improves-the-estimation-of-polygenic-risk-scores-for-breast-cancer" class="headerlink" title="Deep neural network improves the estimation of polygenic risk scores for breast cancer"></a>Deep neural network improves the estimation of polygenic risk scores for breast cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13010">http://arxiv.org/abs/2307.13010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Badré, Li Zhang, Wellington Muchero, Justin C. Reynolds, Chongle Pan<br>for: 这个研究用于比较多种计算模型来计算乳腺癌风险分数（PRS）。methods: 这个研究使用了深度神经网络（DNN）和其他机器学习技术以及统计学方法，包括BLUP、BayesA和LDpred。results: DNN在测试群体中表现出色，其AUC为67.4%，比其他方法高。此外，DNN还能够分化患者群体，并且可以达到18.8%的感知率 при 90%的准确率。这些结果表明，DNN可以更好地预测乳腺癌风险。<details>
<summary>Abstract</summary>
Polygenic risk scores (PRS) estimate the genetic risk of an individual for a complex disease based on many genetic variants across the whole genome. In this study, we compared a series of computational models for estimation of breast cancer PRS. A deep neural network (DNN) was found to outperform alternative machine learning techniques and established statistical algorithms, including BLUP, BayesA and LDpred. In the test cohort with 50% prevalence, the Area Under the receiver operating characteristic Curve (AUC) were 67.4% for DNN, 64.2% for BLUP, 64.5% for BayesA, and 62.4% for LDpred. BLUP, BayesA, and LPpred all generated PRS that followed a normal distribution in the case population. However, the PRS generated by DNN in the case population followed a bi-modal distribution composed of two normal distributions with distinctly different means. This suggests that DNN was able to separate the case population into a high-genetic-risk case sub-population with an average PRS significantly higher than the control population and a normal-genetic-risk case sub-population with an average PRS similar to the control population. This allowed DNN to achieve 18.8% recall at 90% precision in the test cohort with 50% prevalence, which can be extrapolated to 65.4% recall at 20% precision in a general population with 12% prevalence. Interpretation of the DNN model identified salient variants that were assigned insignificant p-values by association studies, but were important for DNN prediction. These variants may be associated with the phenotype through non-linear relationships.
</details>
<details>
<summary>摘要</summary>
多因素风险分数（PRS）用于估计个体复杂疾病的遗传风险，基于整个基因组中的多个遗传变异。本研究比较了多种计算模型来估计乳腺癌PRS。深度神经网络（DNN）被发现超过了其他机器学习技术和确立的统计算法，包括BLUP、BayesA和LDpred。在测试群中的50%预测率下，DNN的AUC分数为67.4%，BLUP的AUC分数为64.2%，BayesA的AUC分数为64.5%，LDpred的AUC分数为62.4%。BLUP、BayesA和LPpred都生成的PRS在正例群中遵循正态分布。然而，DNN在疾病群中生成的PRS遵循了二元分布，由两个正态分布组成，其中一个分布的mean值明显高于控制群的mean值，另一个分布的mean值与控制群的mean值类似。这表明DNN能够将疾病群分为高遗传风险子群和正常遗传风险子群，其中高遗传风险子群的PRS平均值明显高于控制群的PRS，而正常遗传风险子群的PRS与控制群的PRS类似。这使得DNN在50%预测率下实现了18.8%的回归率，在20%预测率下可以扩展到65.4%的回归率。DNN模型的解释发现了一些被关键性评估为无关的变异，但对DNN预测是重要的。这些变异可能与现象之间存在非线性关系。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine"><a href="#Analyzing-the-Strategy-of-Propaganda-using-Inverse-Reinforcement-Learning-Evidence-from-the-2022-Russian-Invasion-of-Ukraine" class="headerlink" title="Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine"></a>Analyzing the Strategy of Propaganda using Inverse Reinforcement Learning: Evidence from the 2022 Russian Invasion of Ukraine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12788">http://arxiv.org/abs/2307.12788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominique Geissler, Stefan Feuerriegel</li>
<li>for: 这个研究旨在分析2022年俄罗斯入侵乌克兰的社交媒体宣传活动的战略。</li>
<li>methods: 这个研究使用反强化学习（IRL）方法来分析社交媒体上的宣传行为。</li>
<li>results: 研究发现，负面宣传的机器人和人类用户采取不同策略：机器人主要回应支持入侵的消息，而人类用户主要回应反对消息，这表明机器人寻求把消息推广，而人类用户更倾向于进行批评讨论。<details>
<summary>Abstract</summary>
The 2022 Russian invasion of Ukraine was accompanied by a large-scale, pro-Russian propaganda campaign on social media. However, the strategy behind the dissemination of propaganda has remained unclear, particularly how the online discourse was strategically shaped by the propagandists' community. Here, we analyze the strategy of the Twitter community using an inverse reinforcement learning (IRL) approach. Specifically, IRL allows us to model online behavior as a Markov decision process, where the goal is to infer the underlying reward structure that guides propagandists when interacting with users with a supporting or opposing stance toward the invasion. Thereby, we aim to understand empirically whether and how between-user interactions are strategically used to promote the proliferation of Russian propaganda. For this, we leverage a large-scale dataset with 349,455 posts with pro-Russian propaganda from 132,131 users. We show that bots and humans follow a different strategy: bots respond predominantly to pro-invasion messages, suggesting that they seek to drive virality; while messages indicating opposition primarily elicit responses from humans, suggesting that they tend to engage in critical discussions. To the best of our knowledge, this is the first study analyzing the strategy behind propaganda from the 2022 Russian invasion of Ukraine through the lens of IRL.
</details>
<details>
<summary>摘要</summary>
俄罗斯入侵乌克兰的2022年社交媒体宣传活动拥有了大规模的俄罗斯支持者宣传运动。然而，这些宣传的执行策略仍然不清楚，特别是在线媒体互动如何被推动者社区战略性地形成。在这里，我们使用反向强化学习（IRL）方法来分析推特社区的策略。具体来说，IRL允许我们将在线行为视为Markov决策过程，其中的目标是推断推特用户在与支持或反对入侵的用户互动时的奖励结构。由此，我们希望理解在线互动是如何被推动者用于推广俄罗斯宣传的。为此，我们利用了349,455条推特文章和132,131名用户的大规模数据集。我们发现，机器人和人类采取不同策略：机器人主要回应支持入侵的消息，表明它们想要驱动病毒性;而表达反对的消息主要引起人类的回应，表明人类更倾向于进行批评讨论。根据我们所知，这是第一篇分析2022年俄罗斯入侵乌克兰宣传策略的IRL研究。
</details></li>
</ul>
<hr>
<h2 id="Is-attention-all-you-need-in-medical-image-analysis-A-review"><a href="#Is-attention-all-you-need-in-medical-image-analysis-A-review" class="headerlink" title="Is attention all you need in medical image analysis? A review"></a>Is attention all you need in medical image analysis? A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12775">http://arxiv.org/abs/2307.12775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giorgos Papanastasiou, Nikolaos Dikaios, Jiahao Huang, Chengjia Wang, Guang Yang</li>
<li>For: This paper reviews and analyzes existing hybrid CNN-Transf&#x2F;Attention models for medical image analysis (MIA) problems, and discusses their generalization opportunities for scientific and clinical impact.* Methods: The paper uses a comprehensive analysis framework to evaluate the architectural designs, breakthroughs, and opportunities of hybrid CNN-Transf&#x2F;Attention models in MIA.* Results: The paper provides a systematic review of existing hybrid CNN-Transf&#x2F;Attention models, and discusses their strengths and limitations in terms of generalization ability and clinical impact.<details>
<summary>Abstract</summary>
Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.
</details>
<details>
<summary>摘要</summary>
医疗影像是诊断、治疗规划和临床试验设计中的关键组成部分，占健康保健数据的大约90%。过去几年，深度学习（CNN）在医疗影像分析（MIA）中获得了性能提升。CNN可以高效地模型影像中的局部像素互动，并可以在小规模的MI数据上进行训练。然而，典型的CNN模型忽略了影像中的全局像素关系，这限制了它们的泛化能力，不能理解不同的全局信息。随着人工智能的发展，转换器（Transformers）在数据中学习全局关系的能力得到了提升。然而，全Transformers模型需要大规模的训练数据和巨大的计算复杂度。为了维护模型的全局性和可扩展性，人们提出了Attention和Transformers组件（Transf/Attention）。最近几年， hybrid CNN-Transf/Attention模型在多个MIA问题上得到了广泛应用。在这篇系统评影卷中，我们对现有的hybrid CNN-Transf/Attention模型进行了抽样、回顾和分析，并评估了这些模型的当前和未来的机遇和挑战。此外，我们还提出了一种全面的分析框架，以便根据这些模型的泛化机会，推动数据驱动的领域泛化和适应方法的发展。
</details></li>
</ul>
<hr>
<h2 id="Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning"><a href="#Detecting-disturbances-in-network-coupled-dynamical-systems-with-machine-learning" class="headerlink" title="Detecting disturbances in network-coupled dynamical systems with machine learning"></a>Detecting disturbances in network-coupled dynamical systems with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12771">http://arxiv.org/abs/2307.12771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Per Sebastian Skardal, Juan G. Restrepo</li>
<li>for: identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics</li>
<li>methods: model-free method based on machine learning using prior observations of the system when forced by a known training function</li>
<li>results: able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions, both with linear and nonlinear disturbances using food web and neuronal activity models.Here’s the full translation in Simplified Chinese:</li>
<li>for:  Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics</li>
<li>methods:  Model-free method based on machine learning using prior observations of the system when forced by a known training function</li>
<li>results:  Able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions, both with linear and nonlinear disturbances using food web and neuronal activity models.<details>
<summary>Abstract</summary>
Identifying disturbances in network-coupled dynamical systems without knowledge of the disturbances or underlying dynamics is a problem with a wide range of applications. For example, one might want to know which nodes in the network are being disturbed and identify the type of disturbance. Here we present a model-free method based on machine learning to identify such unknown disturbances based only on prior observations of the system when forced by a known training function. We find that this method is able to identify the locations and properties of many different types of unknown disturbances using a variety of known forcing functions. We illustrate our results both with linear and nonlinear disturbances using food web and neuronal activity models. Finally, we discuss how to scale our method to large networks.
</details>
<details>
<summary>摘要</summary>
<<sys.translation.activate("zh-Hans")>>无知的网络相互作用系统中的干扰的识别问题具有广泛的应用领域。例如，我们可能想知道哪些节点在网络中受到干扰，并识别干扰的类型。在这里，我们提出了一种无模型的方法，基于机器学习来识别未知干扰，只基于先前观察到的系统强制函数。我们发现这种方法可以识别多种不同类型的未知干扰，使用多种已知强制函数。我们使用食物网和神经活动模型来ILLUSTRATE我们的结果。最后，我们讨论如何扩展我们的方法到大型网络。<<sys.translation.deactivate()>>
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation"><a href="#Nonparametric-Linear-Feature-Learning-in-Regression-Through-Regularisation" class="headerlink" title="Nonparametric Linear Feature Learning in Regression Through Regularisation"></a>Nonparametric Linear Feature Learning in Regression Through Regularisation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12754">http://arxiv.org/abs/2307.12754</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bertillefollain/regfeal">https://github.com/bertillefollain/regfeal</a></li>
<li>paper_authors: Bertille Follain, Umut Simsekli, Francis Bach</li>
<li>for: 这个论文的目的是提出一种新的非 Parametric 特征选择方法，用于在高维数据中进行预测、计算和解释。</li>
<li>methods: 该方法使用了Empirical Risk Minimization 算法，并在其中添加了一个 penalty term 来保证方法的 versatility。在使用 Hermite 波幅的时候，我们引入了一个新的估计器名为 RegFeaL。</li>
<li>results: 我们的实验结果表明，RegFeaL 可以在各种实验中达到高效的预测性和精度。此外，我们还提供了一些实验结果，证明了我们的方法的可靠性和稳定性。<details>
<summary>Abstract</summary>
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with leading directions and accurately estimate the relevant dimension in practical settings. We establish that our method yields a consistent estimator of the prediction function with explicit rates. Additionally, we provide empirical results demonstrating the performance of RegFeaL in various experiments.
</details>
<details>
<summary>摘要</summary>
学习表示在自动选择特征中扮演着关键角色，尤其在高维数据的情况下。在这种情况下，非 Parametric 方法经常陷入困难。在这项研究中，我们关注supervised学习场景，其中相关信息归结于数据中的一个低维线性子空间，即多指标模型。如果这个子空间知道，那么预测、计算和解释都将得到极大提高。为解决这个挑战，我们提出了一种新的方法，即linear feature学习方法，该方法同时估算预测函数和线性子空间。我们的方法使用empirical risk minimization，加上函数导数的罚函数，以确保多样性。通过 Hermite  polynomials 的正交性和旋转不变性，我们引入了我们的估计器，名为RegFeaL。通过alternative minimization，我们可以逐步旋转数据，以便更好地与主要方向align，并准确地估算实际情况中的相关维度。我们证明了我们的方法可以得到一个consistent的预测函数估算器，并且提供了explicit rates。此外，我们还提供了许多实际 экспериментов的结果，以证明 RegFeaL 的性能。
</details></li>
</ul>
<hr>
<h2 id="Concept-based-explainability-for-an-EEG-transformer-model"><a href="#Concept-based-explainability-for-an-EEG-transformer-model" class="headerlink" title="Concept-based explainability for an EEG transformer model"></a>Concept-based explainability for an EEG transformer model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12745">http://arxiv.org/abs/2307.12745</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andersgmadsen/tcav-bendr">https://github.com/andersgmadsen/tcav-bendr</a></li>
<li>paper_authors: Anders Gjølbye Madsen, William Theodor Lehn-Schiøler, Áshildur Jónsdóttir, Bergdís Arnardóttir, Lars Kai Hansen</li>
<li>for: 这个论文的目的是解释深度学习模型内部的状态，以便更好地理解它们如何处理数据。</li>
<li>methods: 该论文使用了Concept Activation Vectors（CAVs）方法来解释深度学习模型。CAVs是基于人类可理解的概念的方法，通过利用欧几何分布来定义内部状态。</li>
<li>results: 研究人员通过使用外部标注的EEG数据集和基于生物学结构的概念来定义概念，并证明这两种方法都可以提供深度EEG模型学习的有价值信息。<details>
<summary>Abstract</summary>
Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation: the use of externally labeled EEG datasets, and the application of anatomically defined concepts. The former approach is a straightforward generalization of methods used in image classification, while the latter is novel and specific to EEG. We present evidence that both approaches to concept formation yield valuable insights into the representations learned by deep EEG models.
</details>
<details>
<summary>摘要</summary>
深度学习模型因其大小、结构和训练过程中的随机性而复杂。这些复杂性来自数据选择和印uctive bias。为了解释这些复杂性，金等人（2018）提出了概念活化向量（CAV），该方法通过将深度模型内部状态转化为人类可理解的概念来解释深度模型的行为。这些概念与 latent space 中的方向相对应，通过使用线性投影来确定。这种方法最初应用于图像分类 зада务，后来扩展到其他领域，包括自然语言处理。在这项工作中，我们尝试将该方法应用于 Kostas 等人（2021）的 BENDR 模型，这是一个大规模的 transformer 模型。我们的注重点在于选择合适的解释概念和使用外部标注的 EEG 数据集来固定概念。前一种方法是一种直观的推广，而后一种方法是特定于 EEG 的新领域。我们显示两种方法都可以为深度 EEG 模型学习的表征提供有价值的解释。
</details></li>
</ul>
<hr>
<h2 id="Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding"><a href="#Sparse-firing-regularization-methods-for-spiking-neural-networks-with-time-to-first-spike-coding" class="headerlink" title="Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding"></a>Sparse-firing regularization methods for spiking neural networks with time-to-first spike coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13007">http://arxiv.org/abs/2307.13007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakemi, Kakei Yamamoto, Takeo Hosomi, Kazuyuki Aihara</li>
<li>for: 这种研究旨在提高多层脉冲神经网络（SNN）的训练效果，特别是使用错误反射算法来实现理想的时间编码。</li>
<li>methods: 这种方法使用时间到初始脉冲（TTFS）编码，每个神经元只能发射一次，这种限制使得信息可以在非常低的脉冲频率下处理。</li>
<li>results: 通过两种基于脉冲时间的稀发（SSR）规范方法来进一步降低TTFS-编码SNNs的脉冲频率，并在MNIST、Fashion-MNIST和CIFAR-10 datasets上使用多层感知器网络和卷积神经网络结构进行研究。<details>
<summary>Abstract</summary>
The training of multilayer spiking neural networks (SNNs) using the error backpropagation algorithm has made significant progress in recent years. Among the various training schemes, the error backpropagation method that directly uses the firing time of neurons has attracted considerable attention because it can realize ideal temporal coding. This method uses time-to-first spike (TTFS) coding, in which each neuron fires at most once, and this restriction on the number of firings enables information to be processed at a very low firing frequency. This low firing frequency increases the energy efficiency of information processing in SNNs, which is important not only because of its similarity with information processing in the brain, but also from an engineering point of view. However, only an upper limit has been provided for TTFS-coded SNNs, and the information-processing capability of SNNs at lower firing frequencies has not been fully investigated. In this paper, we propose two spike timing-based sparse-firing (SSR) regularization methods to further reduce the firing frequency of TTFS-coded SNNs. The first is the membrane potential-aware SSR (M-SSR) method, which has been derived as an extreme form of the loss function of the membrane potential value. The second is the firing condition-aware SSR (F-SSR) method, which is a regularization function obtained from the firing conditions. Both methods are characterized by the fact that they only require information about the firing timing and associated weights. The effects of these regularization methods were investigated on the MNIST, Fashion-MNIST, and CIFAR-10 datasets using multilayer perceptron networks and convolutional neural network structures.
</details>
<details>
<summary>摘要</summary>
多层脉冲神经网络（SNN）的训练使用错误归散算法在过去几年来有了 significiant progress。多种训练方案中，使用神经元发射时间的错误归散方法吸引了较大的关注，因为它可以实现理想的时间编码。这种方法使用时间到第一脉冲（TTFS）编码，每个神经元只能发射一次，这种限制神经元发射数量使得信息可以在非常低的发射频率下处理。这种低发射频率提高了SNNs中信息处理的能效性，这不仅与神经元处理信息的方式相似，还从工程角度来看是非常重要。然而，只有提供了TTFS编码SNNs的Upper bound，它们在lower firing frequency下的信息处理能力还未得到了全面的研究。在这篇论文中，我们提出了两种基于发射时间的稀发射（SSR）规范，以进一步降低TTFS编码SNNs的发射频率。第一种是膜电压意识SSR（M-SSR）方法，它是膜电压值的极限形式的损失函数。第二种是发射条件意识SSR（F-SSR）方法，它是基于发射条件获得的规范函数。两种方法都是基于发射时间和相关权重的信息。我们在MNIST、Fashion-MNIST和CIFAR-10 datasets上使用多层报告网络和卷积神经网络结构来研究这两种规范的效果。
</details></li>
</ul>
<hr>
<h2 id="Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift"><a href="#Safety-Performance-of-Neural-Networks-in-the-Presence-of-Covariate-Shift" class="headerlink" title="Safety Performance of Neural Networks in the Presence of Covariate Shift"></a>Safety Performance of Neural Networks in the Presence of Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12716">http://arxiv.org/abs/2307.12716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Harald Ruess, Konstantinos Theodorou</li>
<li>for:  This paper aims to address the issue of covariate shift’s impact on the operational safety performance of neural networks, and proposes a method to reshape the initial test set based on an approximation of the operational data.</li>
<li>methods: The proposed method uses finite binning and static dataflow analysis to derive conservative bounds on the values of neurons, and formulates a mixed integer linear programming (MILP) constraint to construct the minimum set of data points to be removed in the test set.</li>
<li>results: The proposed method can re-evaluate the safety performance of neural networks in the presence of covariate shift by using the reshaped test set, and can potentially reduce the need for collecting new operational data and creating corresponding ground truth labels.<details>
<summary>Abstract</summary>
Covariate shift may impact the operational safety performance of neural networks. A re-evaluation of the safety performance, however, requires collecting new operational data and creating corresponding ground truth labels, which often is not possible during operation. We are therefore proposing to reshape the initial test set, as used for the safety performance evaluation prior to deployment, based on an approximation of the operational data. This approximation is obtained by observing and learning the distribution of activation patterns of neurons in the network during operation. The reshaped test set reflects the distribution of neuron activation values as observed during operation, and may therefore be used for re-evaluating safety performance in the presence of covariate shift. First, we derive conservative bounds on the values of neurons by applying finite binning and static dataflow analysis. Second, we formulate a mixed integer linear programming (MILP) constraint for constructing the minimum set of data points to be removed in the test set, such that the difference between the discretized test and operational distributions is bounded. We discuss potential benefits and limitations of this constraint-based approach based on our initial experience with an implemented research prototype.
</details>
<details>
<summary>摘要</summary>
covariate shift可能会影响神经网络的操作安全性表现。然而，为了重新评估安全性表现，通常需要收集新的操作数据并创建相应的地面真实标签，这并不是在运行时可行。我们因此提议将初始测试集重新分配，基于运行时神经网络活动 patrerns的approximation。这种approximation可以通过观察和学习神经网络在运行时的活动模式来获得。重新分配的测试集尝试反映了在运行时神经网络活动值的分布，可以用于重新评估安全性表现在covariate shift的情况下。首先，我们通过finite binning和静态数据流分析来 derive保守的神经元值 bounds。其次，我们将mix integer linear programming（MILP）约束构造最小的数据点删除集，以使得测试集和运行时分布之间的差异保持在bound。我们对实际研究版本中的初步体验提出了可能的优点和限制。
</details></li>
</ul>
<hr>
<h2 id="Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport"><a href="#Policy-Gradient-Optimal-Correlation-Search-for-Variance-Reduction-in-Monte-Carlo-simulation-and-Maximum-Optimal-Transport" class="headerlink" title="Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport"></a>Policy Gradient Optimal Correlation Search for Variance Reduction in Monte Carlo simulation and Maximum Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12703">http://arxiv.org/abs/2307.12703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Bras, Gilles Pagès</li>
<li>for: 估计 $f(X_T)$ 的方法，即使 $X$ 是 Stochastic Differential Equation 的解。</li>
<li>methods: 使用 $(f(X^1_T) + f(X^2_T))&#x2F;2$ 作为新的估计器，其中 $X^1$ 和 $X^2$ 具有同样的分布，但是具有相对的路径相关性，以降低方差。采用深度神经网络来 aproximate 优化函数 $\rho$，并使用策略梯度和奖励学习技术来准确调整 $\rho$。</li>
<li>results: 通过Policy Gradient和奖励学习技术来准确地调整优化函数 $\rho$，实现了降低方差的目标。<details>
<summary>Abstract</summary>
We propose a new algorithm for variance reduction when estimating $f(X_T)$ where $X$ is the solution to some stochastic differential equation and $f$ is a test function. The new estimator is $(f(X^1_T) + f(X^2_T))/2$, where $X^1$ and $X^2$ have same marginal law as $X$ but are pathwise correlated so that to reduce the variance. The optimal correlation function $\rho$ is approximated by a deep neural network and is calibrated along the trajectories of $(X^1, X^2)$ by policy gradient and reinforcement learning techniques. Finding an optimal coupling given marginal laws has links with maximum optimal transport.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法来降低方差时估计 $f(X_T)$，其中 $X$ 是一个随机 diferencial equation 的解，$f$ 是一个测试函数。新的估计器是 $(f(X^1_T) + f(X^2_T))/2$，其中 $X^1$ 和 $X^2$ 具有同样的分布，但是它们的路径相关，以降低方差。我们使用深度神经网络来 aproximate 优化函数 $\rho$，并通过政策梯度和强化学习技术来调整 $\rho$ 的参数。找到最佳对接给定分布有关系于最大优化运输。
</details></li>
</ul>
<hr>
<h2 id="MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features"><a href="#MC-JEPA-A-Joint-Embedding-Predictive-Architecture-for-Self-Supervised-Learning-of-Motion-and-Content-Features" class="headerlink" title="MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features"></a>MC-JEPA: A Joint-Embedding Predictive Architecture for Self-Supervised Learning of Motion and Content Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12698">http://arxiv.org/abs/2307.12698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Bardes, Jean Ponce, Yann LeCun</li>
<li>for: 本研究旨在jointly学习视觉表示和流体动作 estimation，并证明这两个目标互相帮助进行学习，从而学习出包含运动信息的内容特征。</li>
<li>methods: 本研究提出了MC-JEPA模型，即共同嵌入预测建模和自然学习方法，在共同编码器中同时学习视觉表示和流体动作 estimation。</li>
<li>results: 实验结果表明，MC-JEPA模型可以在无监督的情况下实现视觉表示和流体动作 estimation的同时学习，并且在下游任务中，如图像和视频Semantic segmentation等任务中，可以达到与现有无监督光流估计 benchmark和常见自然学习方法相当的性能。<details>
<summary>Abstract</summary>
Self-supervised learning of visual representations has been focusing on learning content features, which do not capture object motion or location, and focus on identifying and differentiating objects in images and videos. On the other hand, optical flow estimation is a task that does not involve understanding the content of the images on which it is estimated. We unify the two approaches and introduce MC-JEPA, a joint-embedding predictive architecture and self-supervised learning approach to jointly learn optical flow and content features within a shared encoder, demonstrating that the two associated objectives; the optical flow estimation objective and the self-supervised learning objective; benefit from each other and thus learn content features that incorporate motion information. The proposed approach achieves performance on-par with existing unsupervised optical flow benchmarks, as well as with common self-supervised learning approaches on downstream tasks such as semantic segmentation of images and videos.
</details>
<details>
<summary>摘要</summary>
自适应学习视觉表示法中心在学习内容特征，这些特征不包括物体运动或位置信息，而是通过识别和区分图像和视频中的对象来学习。相反，光流估计是一个不需要理解图像内容的任务。我们将这两种方法结合起来，并介绍MC-JEPA，一种共享编码器中的共同预测建筑和自适应学习方法，以jointly学习光流和内容特征。我们发现这两个相关的目标，即光流估计目标和自适应学习目标，在共同学习中互相帮助，因此学习的内容特征包含运动信息。我们的方法可以与现有的无监督光流标准做比较，以及常见的自适应学习方法在图像和视频Semantic segmentation任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks"><a href="#Addressing-the-Impact-of-Localized-Training-Data-in-Graph-Neural-Networks" class="headerlink" title="Addressing the Impact of Localized Training Data in Graph Neural Networks"></a>Addressing the Impact of Localized Training Data in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12689">http://arxiv.org/abs/2307.12689</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akanshaaga/reg_appnp">https://github.com/akanshaaga/reg_appnp</a></li>
<li>paper_authors: Singh Akansha</li>
<li>for: 本研究旨在评估图神经网络（GNNs）在本地化训练数据下的性能。</li>
<li>methods: 我们提出了一种常见GNN模型的补做方法，以适应本地化训练数据下的挑战。</li>
<li>results: 我们在三个标准图神经网络 benchmark dataset上进行了广泛的测试，并得到了显著的性能提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details>
<details>
<summary>摘要</summary>
GRAPH NEURAL NETWORKS (GNNs) have achieved notable success in learning from graph-structured data, owing to their ability to capture intricate dependencies and relationships between nodes. They excel in various applications, including semi-supervised node classification, link prediction, and graph generation. However, it is important to acknowledge that the majority of state-of-the-art GNN models are built upon the assumption of an in-distribution setting, which hinders their performance on real-world graphs with dynamic structures. In this article, we aim to assess the impact of training GNNs on localized subsets of the graph. Such restricted training data may lead to a model that performs well in the specific region it was trained on but fails to generalize and make accurate predictions for the entire graph. In the context of graph-based semi-supervised learning (SSL), resource constraints often lead to scenarios where the dataset is large, but only a portion of it can be labeled, affecting the model's performance. This limitation affects tasks like anomaly detection or spam detection when labeling processes are biased or influenced by human subjectivity. To tackle the challenges posed by localized training data, we approach the problem as an out-of-distribution (OOD) data issue by aligning the distributions between the training data, which represents a small portion of labeled data, and the graph inference process that involves making predictions for the entire graph. We propose a regularization method to minimize distributional discrepancies between localized training data and graph inference, improving model performance on OOD data. Extensive tests on popular GNN models show significant performance improvement on three citation GNN benchmark datasets. The regularization approach effectively enhances model adaptation and generalization, overcoming challenges posed by OOD data.
</details></li>
</ul>
<hr>
<h2 id="An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks"><a href="#An-Estimator-for-the-Sensitivity-to-Perturbations-of-Deep-Neural-Networks" class="headerlink" title="An Estimator for the Sensitivity to Perturbations of Deep Neural Networks"></a>An Estimator for the Sensitivity to Perturbations of Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12679">http://arxiv.org/abs/2307.12679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naman Maheshwari, Nicholas Malaya, Scott Moe, Jaydeep P. Kulkarni, Sudhanva Gurumurthi</li>
<li>for: 这篇论文的目的是为了评估深度神经网络（DNNs）在安全关键应用中的稳定性，如自动驾驶车和疾病诊断。</li>
<li>methods: 这篇论文使用了一种能够预测DNN对输入和模型参数的敏感性的估计器。该估计器基于不等式和矩阵范数，其结果类似于神经网络的condition number。</li>
<li>results: 在测试了AlexNet和VGG-19 convolutional neural networks（CNNs）以及ImageNet dataset时，这种估计器能够准确地预测DNN对输入和模型参数的敏感性。此外，通过随机偏移和攻击测试，这种估计器的紧密性也得到了证明。<details>
<summary>Abstract</summary>
For Deep Neural Networks (DNNs) to become useful in safety-critical applications, such as self-driving cars and disease diagnosis, they must be stable to perturbations in input and model parameters. Characterizing the sensitivity of a DNN to perturbations is necessary to determine minimal bit-width precision that may be used to safely represent the network. However, no general result exists that is capable of predicting the sensitivity of a given DNN to round-off error, noise, or other perturbations in input. This paper derives an estimator that can predict such quantities. The estimator is derived via inequalities and matrix norms, and the resulting quantity is roughly analogous to a condition number for the entire neural network. An approximation of the estimator is tested on two Convolutional Neural Networks, AlexNet and VGG-19, using the ImageNet dataset. For each of these networks, the tightness of the estimator is explored via random perturbations and adversarial attacks.
</details>
<details>
<summary>摘要</summary>
（ Deep Neural Networks 必须在安全关键应用中稳定，如自动驾驶车和疾病诊断。因此，必须了解 DNN 对输入和模型参数的敏感度，以确定安全地表示网络所需的最小位数准确性。然而，没有一个通用的结果可以预测给定 DNN 对轮减错误、噪声或其他输入中的敏感度。这篇文章提出了一个估计器，可以预测这些量。估计器是通过不等式和矩阵范数 derive，其结果类似于整个神经网络的condition number。这个估计器的紧密性在使用 ImageNet 数据集上对 AlexNet 和 VGG-19 两个卷积神经网络进行随机扰动和攻击性测试中被探索。）
</details></li>
</ul>
<hr>
<h2 id="Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling"><a href="#Global-k-Space-Interpolation-for-Dynamic-MRI-Reconstruction-using-Masked-Image-Modeling" class="headerlink" title="Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling"></a>Global k-Space Interpolation for Dynamic MRI Reconstruction using Masked Image Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12672">http://arxiv.org/abs/2307.12672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiazhen Pan, Suprosanna Shit, Özgün Turgut, Wenqi Huang, Hongwei Bran Li, Nil Stolt-Ansó, Thomas Küstner, Kerstin Hammernik, Daniel Rueckert</li>
<li>for: 这篇论文的目的是为了提高动力磁共振成像（MRI）中的数据探测率，以解决因时间限制而导致的抽象项目残影。</li>
<li>methods: 本文使用的方法是将受测空间探测短缺的数据进行插值，并且使用一个新的Transformer-based k-space Global Interpolation Network（k-GIN）来学习全球的低频和高频成像结构。此外，我们还提出了一个k-space Iterative Refinement Module（k-IRM）来强化高频成像的学习。</li>
<li>results: 我们的方法与基准方法相比，在92个内部2D+t心脏MRI试验中表现出了优化的成像质量和更高的类别化能力。特别是在具有高度受测空间探测短缺的情况下，我们的方法具有更高的类别化能力和普遍性。<details>
<summary>Abstract</summary>
In dynamic Magnetic Resonance Imaging (MRI), k-space is typically undersampled due to limited scan time, resulting in aliasing artifacts in the image domain. Hence, dynamic MR reconstruction requires not only modeling spatial frequency components in the x and y directions of k-space but also considering temporal redundancy. Most previous works rely on image-domain regularizers (priors) to conduct MR reconstruction. In contrast, we focus on interpolating the undersampled k-space before obtaining images with Fourier transform. In this work, we connect masked image modeling with k-space interpolation and propose a novel Transformer-based k-space Global Interpolation Network, termed k-GIN. Our k-GIN learns global dependencies among low- and high-frequency components of 2D+t k-space and uses it to interpolate unsampled data. Further, we propose a novel k-space Iterative Refinement Module (k-IRM) to enhance the high-frequency components learning. We evaluate our approach on 92 in-house 2D+t cardiac MR subjects and compare it to MR reconstruction methods with image-domain regularizers. Experiments show that our proposed k-space interpolation method quantitatively and qualitatively outperforms baseline methods. Importantly, the proposed approach achieves substantially higher robustness and generalizability in cases of highly-undersampled MR data.
</details>
<details>
<summary>摘要</summary>
在动态磁共振成像（MRI）中，通常因为扫描时间有限，会导致卷积空间下折射样本受到假象 artifacts。因此，动态MR重建需要不仅考虑 x 和 y 方向的空间频率组件，还需要考虑时间重复性。大多数前一些工作都是通过图像领域的正则化（约束）来进行MR重建。相比之下，我们注意到 interpolating 未折射的卷积空间，并提出了一种基于 Transformer 的全域卷积global interpolation network，称之为 k-GIN。我们的 k-GIN 学习了 2D+t 卷积空间中低频和高频组件之间的全局依赖关系，并使用其来 interpolate 未折射数据。此外，我们还提出了一种 k-space 迭代优化模块（k-IRM），以提高高频组件的学习。我们对 92 个室内 2D+t 心脏 MRI 测试数据进行了评估，并与使用图像领域正则化的 MR 重建方法进行比较。实验表明，我们提出的方法在量化和质量上都有显著提高，并且在高度受折射影响的 MR 数据中具有更高的robustness和普适性。
</details></li>
</ul>
<hr>
<h2 id="Control-and-Monitoring-of-Artificial-Intelligence-Algorithms"><a href="#Control-and-Monitoring-of-Artificial-Intelligence-Algorithms" class="headerlink" title="Control and Monitoring of Artificial Intelligence Algorithms"></a>Control and Monitoring of Artificial Intelligence Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13705">http://arxiv.org/abs/2307.13705</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Carlos Mario Braga Ortuño, Blanza Martinez Donoso, Belén Muñiz Villanueva</li>
<li>for: 这篇论文强调了在人工智能模型部署后进行监管和评估数据分布的变化。</li>
<li>methods: 文章介绍了数据漂移和概念漂移的概念，以及他们的基础分布。同时，文章还提出了一些用于评估模型性能对于时间变化的指标。</li>
<li>results: 文章通过介绍不同的指标和方法，探讨了模型在不同情况下的性能。<details>
<summary>Abstract</summary>
This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
</details>
<details>
<summary>摘要</summary>
这篇论文强调了在人工智能模型部署后的管理和监测数据分布的可能变化，而不是只是在训练数据上。文中介绍了数据漂移和概念漂移的概念，并详细介绍了它们的基础分布。此外，文中还提出了一些指标，可以用来评估模型在可能时间变化的情况下的性能。Here's a breakdown of the translation:* 这篇论文 (zhè běn tiān) - This paper* 强调 (qiáng dì) - emphasize* 在人工智能模型部署后 (zài rénsheng zhìyì módel bùdào hòu) - after the deployment of the artificial intelligence model* 管理 (guǎn lí) - management* 和监测 (hé jìng chá) - and monitoring* 数据分布 (shùdā fāngchēng) - data distribution* 可能变化 (kěnéng biànhùa) - potential variations* 而不是只是在训练数据上 (ér bùshì zhīshì zài xiǎngyìng shūjuè) - rather than only on the training data* 文中介绍 (wén zhōng jièshì) - the paper introduces* 数据漂移 (shùdā qùyì) - data drift* 和概念漂移 (hè guījiān qùyì) - and concept drift* 概念 (guījiān) - concept* 漂移 (qùyì) - drift* 基础分布 (jīshì fāngchēng) - fundamental distribution* 此外 (qíwài) - furthermore* 文中还提出 (wén zhōng hái tímzhěng) - the paper also proposes* 一些指标 (yīxiē zhǐdài) - some metrics* 可以用来 (kěyǐ yòu lái) - can be used to* 评估模型 (píngjì módel) - evaluate the model* 在可能时间变化的情况下 (zài kěnéng shíjiān biànhùa de qíngkè) - in the case of possible temporal variations.
</details></li>
</ul>
<hr>
<h2 id="TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers"><a href="#TransFusion-Generating-Long-High-Fidelity-Time-Series-using-Diffusion-Models-with-Transformers" class="headerlink" title="TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers"></a>TransFusion: Generating Long, High Fidelity Time Series using Diffusion Models with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12667">http://arxiv.org/abs/2307.12667</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fahim-sikder/TransFusion">https://github.com/fahim-sikder/TransFusion</a></li>
<li>paper_authors: Md Fahim Sikder, Resmi Ramachandranpillai, Fredrik Heintz</li>
<li>for: 本研究旨在生成高质量、长序时间序数据，应用广泛。</li>
<li>methods: 我们提出了一种基于协同扩散和变换器的生成模型，称为TransFusion。</li>
<li>results: TransFusion可以生成高质量的长序时间序数据，并且在许多视觉和实验性指标上表现出优于之前的状态 искусственный智能。<details>
<summary>Abstract</summary>
The generation of high-quality, long-sequenced time-series data is essential due to its wide range of applications. In the past, standalone Recurrent and Convolutional Neural Network-based Generative Adversarial Networks (GAN) were used to synthesize time-series data. However, they are inadequate for generating long sequences of time-series data due to limitations in the architecture. Furthermore, GANs are well known for their training instability and mode collapse problem. To address this, we propose TransFusion, a diffusion, and transformers-based generative model to generate high-quality long-sequence time-series data. We have stretched the sequence length to 384, and generated high-quality synthetic data. To the best of our knowledge, this is the first study that has been done with this long-sequence length. Also, we introduce two evaluation metrics to evaluate the quality of the synthetic data as well as its predictive characteristics. We evaluate TransFusion with a wide variety of visual and empirical metrics, and TransFusion outperforms the previous state-of-the-art by a significant margin.
</details>
<details>
<summary>摘要</summary>
“高质量、长序时间序数据的生成是非常重要，因为它具有广泛的应用领域。在过去，单独的循环神经网和卷积神经网基于的生成对抗网（GAN）被用来合成时间序数据。但是，它们因架构限制而无法生成长序时间序数据，并且GAN在训练时会出现不稳定和模式崩溃问题。为解决这问题，我们提出了TransFusion，一个扩散和卷积变数基于的生成模型，可以生成高质量的长序时间序数据。我们已经将序列长度延长到384，并生成了高质量的 sintetic数据。到目前为止，这是第一篇使用这长序长度的研究。此外，我们也引入了两个评估 metric 来评估生成的质量和预测特性。我们将TransFusion评估使用广泛的视觉和实验 metric，并证明TransFusion在前一代的state-of-the-art上出现著标准的差异。”
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics"><a href="#Online-Continual-Learning-in-Keyword-Spotting-for-Low-Resource-Devices-via-Pooling-High-Order-Temporal-Statistics" class="headerlink" title="Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics"></a>Online Continual Learning in Keyword Spotting for Low-Resource Devices via Pooling High-Order Temporal Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12660">http://arxiv.org/abs/2307.12660</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/umbertomichieli/tap-slda">https://github.com/umbertomichieli/tap-slda</a></li>
<li>paper_authors: Umberto Michieli, Pablo Peso Parada, Mete Ozay</li>
<li>for: 本研究旨在提高附加在设备上的语音识别（KWS）模型的适应速度，使其能够快速适应用户定义的新词语，而不会忘记之前的词语。</li>
<li>methods: 本研究提出了一种名为Temporal Aware Pooling（TAP）的方法，它在采用冻结的后向传播模型（backbone）的基础上，通过计算高阶语音特征的时间相关特征空间来扩充特征空间。然后，对这个扩充后的特征空间进行Gaussian模型的更新，以便更好地利用语音表示。</li>
<li>results: 实验分析表明，TAP-SLDA方法在几个设置、后向传播模型和基础上都显示出了明显的优异性，相对于竞争者的平均提升率为11.3%。<details>
<summary>Abstract</summary>
Keyword Spotting (KWS) models on embedded devices should adapt fast to new user-defined words without forgetting previous ones. Embedded devices have limited storage and computational resources, thus, they cannot save samples or update large models. We consider the setup of embedded online continual learning (EOCL), where KWS models with frozen backbone are trained to incrementally recognize new words from a non-repeated stream of samples, seen one at a time. To this end, we propose Temporal Aware Pooling (TAP) which constructs an enriched feature space computing high-order moments of speech features extracted by a pre-trained backbone. Our method, TAP-SLDA, updates a Gaussian model for each class on the enriched feature space to effectively use audio representations. In experimental analyses, TAP-SLDA outperforms competitors on several setups, backbones, and baselines, bringing a relative average gain of 11.3% on the GSC dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable id="1" 键 слова检测（KWS）模型在嵌入式设备上应该快速适应新用户定义的词语，而不会忘记之前的词语。嵌入式设备具有有限的存储和计算资源，因此无法保存样本或更新大型模型。我们考虑了嵌入式在线继续学习（EOCL）的设置，其中KWS模型具有冻结的脊梁被训练以逐渐认可新的词语从一个不重复的流量中，每个样本一个个。为此，我们提议使用时间意识汇聚（TAP），它在预训练后的听语特征空间中构建了丰富的特征空间，并计算高阶时域特征以实现有效的听语表示。我们的方法TAP-SLDA将对每个类在汇聚特征空间上更新加aussian模型，以使用听语表示。在实验分析中，TAP-SLDA比竞争者在多种设置、脊梁和基础上表现出较高的平均提升率，达到11.3%的相对平均提升率在GSC数据集上。
</details></li>
</ul>
<hr>
<h2 id="Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG"><a href="#Remote-Bio-Sensing-Open-Source-Benchmark-Framework-for-Fair-Evaluation-of-rPPG" class="headerlink" title="Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG"></a>Remote Bio-Sensing: Open Source Benchmark Framework for Fair Evaluation of rPPG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12644">http://arxiv.org/abs/2307.12644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/remotebiosensing/rppg">https://github.com/remotebiosensing/rppg</a></li>
<li>paper_authors: Dae-Yeol Kim, Eunsu Goh, KwangKee Lee, JongEui Chae, JongHyeon Mun, Junyeong Na, Chae-bong Sohn, Do-Yup Kim</li>
<li>For: This study provides a benchmarking framework for evaluating the performance of remote photoplethysmography (rPPG) techniques across a wide range of datasets, to ensure fair and meaningful comparison and progress in the field.* Methods: The study uses a variety of datasets, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods, to evaluate the performance of rPPG techniques and provide a comprehensive benchmarking framework.* Results: The study aims to provide a fair and evaluable benchmarking framework for rPPG techniques, addressing the challenges of skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, to make meaningful progress in the field.<details>
<summary>Abstract</summary>
rPPG (Remote photoplethysmography) is a technology that measures and analyzes BVP (Blood Volume Pulse) by using the light absorption characteristics of hemoglobin captured through a camera. Analyzing the measured BVP can derive various physiological signals such as heart rate, stress level, and blood pressure, which can be applied to various applications such as telemedicine, remote patient monitoring, and early prediction of cardiovascular disease. rPPG is rapidly evolving and attracting great attention from both academia and industry by providing great usability and convenience as it can measure biosignals using a camera-equipped device without medical or wearable devices. Despite extensive efforts and advances in this field, serious challenges remain, including issues related to skin color, camera characteristics, ambient lighting, and other sources of noise and artifacts, which degrade accuracy performance. We argue that fair and evaluable benchmarking is urgently required to overcome these challenges and make meaningful progress from both academic and commercial perspectives. In most existing work, models are trained, tested, and validated only on limited datasets. Even worse, some studies lack available code or reproducibility, making it difficult to fairly evaluate and compare performance. Therefore, the purpose of this study is to provide a benchmarking framework to evaluate various rPPG techniques across a wide range of datasets for fair evaluation and comparison, including both conventional non-deep neural network (non-DNN) and deep neural network (DNN) methods. GitHub URL: https://github.com/remotebiosensing/rppg
</details>
<details>
<summary>摘要</summary>
rPPG (远程血液抑血光谱) 是一种技术，通过使用摄像头捕捉光谱特性来测量和分析血液脉冲（BVP）。通过分析测量的BVP，可以 derivate 多种生物physiological signals，如心率、剂量压力和 стресс水平，这些信号可以应用于telemedicine、远程病人监测和早期心血管疾病预测等领域。rPPG 在学术和产业界 rapidly evolving 和吸引广泛关注，因为它可以通过摄像头设备测量生物信号，无需医疗或佩戴设备，提供了很好的可用性和便利性。然而，它还存在严重的挑战，包括皮肤颜色、摄像头特性、 ambient 照明和其他干扰和噪声的问题，这些问题会降低精度性能。我们认为，准确和评估可能的benchmarking是 urgently required ，以超越这些挑战和取得学术和商业上的进步。现有的大多数工作都是在有限的数据集上进行训练、测试和验证，甚至有些研究缺乏可用的代码或可重现性，使得准确评估和比较困难。因此，本研究的目的是提供一个 benchmarking 框架，以评估不同的 rPPG 技术在各种数据集上的性能，包括非深度神经网络（non-DNN）和深度神经网络（DNN）方法。GitHub URL：https://github.com/remotebiosensing/rppg
</details></li>
</ul>
<hr>
<h2 id="Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey"><a href="#Fake-News-Detection-Through-Graph-based-Neural-Networks-A-Survey" class="headerlink" title="Fake News Detection Through Graph-based Neural Networks: A Survey"></a>Fake News Detection Through Graph-based Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12639">http://arxiv.org/abs/2307.12639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuzhi Gong, Richard O. Sinnott, Jianzhong Qi, Cecile Paris</li>
<li>for: 本研究评估了基于图структуры的新闻假消息检测方法和深度学习方法，以及它们在新闻传播过程中的应用。</li>
<li>methods: 本研究分类了现有的图结构基于的新闻假消息检测方法，包括知识驱动方法、传播基于方法和多元社交 контекст基于方法。</li>
<li>results: 本研究评估了现有的图结构基于的新闻假消息检测方法，并提出了未来研究方向。<details>
<summary>Abstract</summary>
The popularity of online social networks has enabled rapid dissemination of information. People now can share and consume information much more rapidly than ever before. However, low-quality and/or accidentally/deliberately fake information can also spread rapidly. This can lead to considerable and negative impacts on society. Identifying, labelling and debunking online misinformation as early as possible has become an increasingly urgent problem. Many methods have been proposed to detect fake news including many deep learning and graph-based approaches. In recent years, graph-based methods have yielded strong results, as they can closely model the social context and propagation process of online news. In this paper, we present a systematic review of fake news detection studies based on graph-based and deep learning-based techniques. We classify existing graph-based methods into knowledge-driven methods, propagation-based methods, and heterogeneous social context-based methods, depending on how a graph structure is constructed to model news related information flows. We further discuss the challenges and open problems in graph-based fake news detection and identify future research directions.
</details>
<details>
<summary>摘要</summary>
“在线社交网络的广泛散布信息的受欢迎程度，使得人们可以更加快速地分享和消耗信息。但是，低品质和/或意外或故意伪造的信息也可以快速散布，导致社会产生了负面的影响。为了早为社会做出负面影响的预防和控制，识别、标识和驳斥网络伪信息的检测已经成为一个非常紧迫的问题。许多方法已经被提出来检测伪新闻，包括深度学习和agraph基的方法。在过去的几年中，agraph基的方法具有优秀的成绩，因为它们可以将在线新闻的社交内容和传播过程模型得到更加精确地。在这篇文章中，我们提出了一种系统性的审查伪新闻检测研究，依据graph基的方法进行分类，并讨论了伪新闻检测中的挑战和未来研究方向。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI"><a href="#Identifying-drivers-and-mitigators-for-congestion-and-redispatch-in-the-German-electric-power-system-with-explainable-AI" class="headerlink" title="Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI"></a>Identifying drivers and mitigators for congestion and redispatch in the German electric power system with explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12636">http://arxiv.org/abs/2307.12636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurizio Titz, Sebastian Pütz, Dirk Witthaut</li>
<li>for: 这篇论文旨在分析德国传输电网中的压力峰值和对负面影响，以及可能的市场设计变更以缓解压力峰值。</li>
<li>methods: 该论文使用可解释的机器学习模型来预测每小时的重新配置和对贸易量。模型分析了压力峰值的驱动因素和缓解因素，并评估了它们的影响。</li>
<li>results: 研究发现，风力电力生产是压力峰值的主要驱动因素，而水力电力和跨国电力贸易也扮演着重要的缓解作用。然而，太阳能电力没有缓解压力峰值的效果。结果表明，市场设计的变更可以缓解压力峰值。<details>
<summary>Abstract</summary>
The transition to a sustainable energy supply challenges the operation of electric power systems in manifold ways. Transmission grid loads increase as wind and solar power are often installed far away from the consumers. In extreme cases, system operators must intervene via countertrading or redispatch to ensure grid stability. In this article, we provide a data-driven analysis of congestion in the German transmission grid. We develop an explainable machine learning model to predict the volume of redispatch and countertrade on an hourly basis. The model reveals factors that drive or mitigate grid congestion and quantifies their impact. We show that, as expected, wind power generation is the main driver, but hydropower and cross-border electricity trading also play an essential role. Solar power, on the other hand, has no mitigating effect. Our results suggest that a change to the market design would alleviate congestion.
</details>
<details>
<summary>摘要</summary>
“将可再生能源纳入可持续能源供应的过程对电力系统运行带来多种挑战。透传网络荷载增加，因为风力和太阳能经常在消费者处 instal 远 away。在极端情况下，系统运维人员需要通过对贸易或重新分配来确保网格稳定。在这篇文章中，我们提供了一个可解释的机器学习模型，用于预测每小时的重新分配和对贸易量。该模型表明风力发电是主要驱动力，而水力发电和跨国电力贸易也扮演着关键性的角色。而太阳能发电却没有缓解效果。我们的结果表明，修改市场设计可以缓解压力。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network"><a href="#De-confounding-Representation-Learning-for-Counterfactual-Inference-on-Continuous-Treatment-via-Generative-Adversarial-Network" class="headerlink" title="De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network"></a>De-confounding Representation Learning for Counterfactual Inference on Continuous Treatment via Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12625">http://arxiv.org/abs/2307.12625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Qiang Huang, Haolong Zeng, Yun Pen, Huiyan Sun</li>
<li>for: This paper aims to address the problem of counterfactual inference for continuous treatment variables, which is more common in real-world causal inference tasks.</li>
<li>methods: The proposed method is called de-confounding representation learning (DRL), which generates representations of covariates that are disentangled from the treatment variables. The DRL model is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates.</li>
<li>results: The DRL model outperforms state-of-the-art counterfactual inference models for continuous treatment variables in extensive experiments on synthetic datasets. Additionally, the DRL model is applied to a real-world medical dataset MIMIC and demonstrates a detailed causal relationship between red cell width distribution and mortality.<details>
<summary>Abstract</summary>
Counterfactual inference for continuous rather than binary treatment variables is more common in real-world causal inference tasks. While there are already some sample reweighting methods based on Marginal Structural Model for eliminating the confounding bias, they generally focus on removing the treatment's linear dependence on confounders and rely on the accuracy of the assumed parametric models, which are usually unverifiable. In this paper, we propose a de-confounding representation learning (DRL) framework for counterfactual outcome estimation of continuous treatment by generating the representations of covariates disentangled with the treatment variables. The DRL is a non-parametric model that eliminates both linear and nonlinear dependence between treatment and covariates. Specifically, we train the correlations between the de-confounded representations and the treatment variables against the correlations between the covariate representations and the treatment variables to eliminate confounding bias. Further, a counterfactual inference network is embedded into the framework to make the learned representations serve both de-confounding and trusted inference. Extensive experiments on synthetic datasets show that the DRL model performs superiorly in learning de-confounding representations and outperforms state-of-the-art counterfactual inference models for continuous treatment variables. In addition, we apply the DRL model to a real-world medical dataset MIMIC and demonstrate a detailed causal relationship between red cell width distribution and mortality.
</details>
<details>
<summary>摘要</summary>
常用的Counterfactual推论 для连续而不是二进制的治疗变量更常见在实际世界的 causal推论任务中。现有一些基于Marginal Structural Model的样本重新权重方法，可以消除干扰的偏见，但这些方法通常假设了治疗变量和干扰变量之间的线性关系，并且这些模型通常是不可证明的。在这篇论文中，我们提出了一种基于de-confounding representation learning（DRL）的推论框架，用于连续治疗变量的 counterfactual 结果估计。DRL 是一种非 Parametric 模型，可以消除治疗变量和干扰变量之间的线性和非线性关系。具体来说，我们在框架中训练了干扰变量和治疗变量之间的相关性和干扰变量和治疗变量之间的相关性，以消除干扰偏见。此外，我们还将 counterfactual 推论网络 embedding 到框架中，以使得学习的表示可以用于 both de-confounding 和可靠的推论。我们在 synthetic 数据上进行了广泛的实验，发现 DRL 模型在学习 de-confounding 表示方面表现出色，并且超过了当前的 counterfactual 推论模型。此外，我们还应用了 DRL 模型到实际的医疗数据集 MIMIC，并显示了红细胞宽度分布和死亡的明确 causal 关系。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Ordinary-Differential-Equations-with-Transformers"><a href="#Predicting-Ordinary-Differential-Equations-with-Transformers" class="headerlink" title="Predicting Ordinary Differential Equations with Transformers"></a>Predicting Ordinary Differential Equations with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12617">http://arxiv.org/abs/2307.12617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sören Becker, Michal Klein, Alexander Neitz, Giambattista Parascandolo, Niki Kilbertus</li>
<li>for:  recuperates scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory</li>
<li>methods:  transformer-based sequence-to-sequence model</li>
<li>results:  better or on par with existing methods in terms of accurate recovery, and efficiently scalable after one-time pretraining on a large set of ODEs<details>
<summary>Abstract</summary>
We develop a transformer-based sequence-to-sequence model that recovers scalar ordinary differential equations (ODEs) in symbolic form from irregularly sampled and noisy observations of a single solution trajectory. We demonstrate in extensive empirical evaluations that our model performs better or on par with existing methods in terms of accurate recovery across various settings. Moreover, our method is efficiently scalable: after one-time pretraining on a large set of ODEs, we can infer the governing law of a new observed solution in a few forward passes of the model.
</details>
<details>
<summary>摘要</summary>
我们开发了一种基于转换器的序列到序列模型，可以从不规则采样和噪声观测数据中精确地回归Scalar常微方程（ODEs）的符号形式。我们在广泛的实验中证明了我们的模型与现有方法相比，在不同的设置下都能够更高效地回归精度。另外，我们的方法可以高效扩展：只需一次预训练于大量ODEs后，我们就可以在几个前向传播中快速地推断新观测数据的管理法律。
</details></li>
</ul>
<hr>
<h2 id="ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays"><a href="#ExWarp-Extrapolation-and-Warping-based-Temporal-Supersampling-for-High-frequency-Displays" class="headerlink" title="ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays"></a>ExWarp: Extrapolation and Warping-based Temporal Supersampling for High-frequency Displays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12607">http://arxiv.org/abs/2307.12607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akanksha Dixit, Yashashwee Chakrabarty, Smruti R. Sarangi<br>for: 提高高频显示器的帧率，提供更平滑和响应的用户体验。methods: 使用动态网络（DNN）和强化学习（RL）算法，选择适合的插值和扩展方法，以提高帧率，同时保持图像质量。results: 比较传统方法，Exwarp可以提高帧率4倍，且图像质量几乎不受影响。<details>
<summary>Abstract</summary>
High-frequency displays are gaining immense popularity because of their increasing use in video games and virtual reality applications. However, the issue is that the underlying GPUs cannot continuously generate frames at this high rate -- this results in a less smooth and responsive experience. Furthermore, if the frame rate is not synchronized with the refresh rate, the user may experience screen tearing and stuttering. Previous works propose increasing the frame rate to provide a smooth experience on modern displays by predicting new frames based on past or future frames. Interpolation and extrapolation are two widely used algorithms that predict new frames. Interpolation requires waiting for the future frame to make a prediction, which adds additional latency. On the other hand, extrapolation provides a better quality of experience because it relies solely on past frames -- it does not incur any additional latency. The simplest method to extrapolate a frame is to warp the previous frame using motion vectors; however, the warped frame may contain improperly rendered visual artifacts due to dynamic objects -- this makes it very challenging to design such a scheme. Past work has used DNNs to get good accuracy, however, these approaches are slow. This paper proposes Exwarp -- an approach based on reinforcement learning (RL) to intelligently choose between the slower DNN-based extrapolation and faster warping-based methods to increase the frame rate by 4x with an almost negligible reduction in the perceived image quality.
</details>
<details>
<summary>摘要</summary>
高频显示器目前在游戏和虚拟现实应用中得到了广泛的推广，但是这些显示器的后置GPU无法持续生成这高的帧率，这会导致用户体验不平滑和不响应。此外，如果帧率与刷新率不同步，用户可能会经历屏渲染和颤抖现象。以往的工作建议通过预测新帧来提高现代显示器的帧率，以提供柔顺的用户体验。插值和拟合是两种广泛使用的预测算法。插值需要等待未来帧来作预测，这会添加额外的延迟。拟合则提供了更高质量的用户体验，因为它仅基于过去帧进行预测，不增加额外的延迟。最简单的拟合方法是通过运动向量来扭曲上一帧，以生成下一帧。但是，扭曲后的帧可能包含不正确渲染的视觉artifacts，这使得设计这种方案非常困难。过去的工作使用深度神经网络（DNN）来获得高精度，但这些方法较慢。这篇论文提出了Exwarp方法，基于强化学习（RL）来智能选择 slower DNN-based extrapolation和 faster warping-based方法，以提高帧率4倍，并且几乎无法感受到图像质量的下降。
</details></li>
</ul>
<hr>
<h2 id="Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models"><a href="#Concept-backpropagation-An-Explainable-AI-approach-for-visualising-learned-concepts-in-neural-network-models" class="headerlink" title="Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models"></a>Concept backpropagation: An Explainable AI approach for visualising learned concepts in neural network models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12601">http://arxiv.org/abs/2307.12601</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrik-ha/concept-backpropagation">https://github.com/patrik-ha/concept-backpropagation</a></li>
<li>paper_authors: Patrik Hammersborg, Inga Strümke</li>
<li>for:  This paper aims to provide a method for visualizing the information that a neural network model depends on to represent a given concept.</li>
<li>methods: The method used in this paper is called concept backpropagation, which involves perturbing the model input in a way that maximizes the detected concept.</li>
<li>results: The paper presents results for this method applied to a variety of input modalities, and discusses how the method can be used to visualize the information that trained concept probes use and the degree to which the representation of the probed concept is entangled within the neural network model.<details>
<summary>Abstract</summary>
Neural network models are widely used in a variety of domains, often as black-box solutions, since they are not directly interpretable for humans. The field of explainable artificial intelligence aims at developing explanation methods to address this challenge, and several approaches have been developed over the recent years, including methods for investigating what type of knowledge these models internalise during the training process. Among these, the method of concept detection, investigates which \emph{concepts} neural network models learn to represent in order to complete their tasks. In this work, we present an extension to the method of concept detection, named \emph{concept backpropagation}, which provides a way of analysing how the information representing a given concept is internalised in a given neural network model. In this approach, the model input is perturbed in a manner guided by a trained concept probe for the described model, such that the concept of interest is maximised. This allows for the visualisation of the detected concept directly in the input space of the model, which in turn makes it possible to see what information the model depends on for representing the described concept. We present results for this method applied to a various set of input modalities, and discuss how our proposed method can be used to visualise what information trained concept probes use, and the degree as to which the representation of the probed concept is entangled within the neural network model itself.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning"><a href="#Optimized-data-collection-and-analysis-process-for-studying-solar-thermal-desalination-by-machine-learning" class="headerlink" title="Optimized data collection and analysis process for studying solar-thermal desalination by machine learning"></a>Optimized data collection and analysis process for studying solar-thermal desalination by machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12594">http://arxiv.org/abs/2307.12594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guilong Peng, Senshan Sun, Yangjun Qin, Zhenwei Xu, Juxin Du, Swellam W. sharshir, A. W. Kandel, A. E. Kabeel, Nuo Yang</li>
<li>for: 这个研究的目的是提高机器学习在太阳蒸馏净水方面的应用，通过大量的实验数据收集和分析。</li>
<li>methods: 这个研究使用了修改后的实验数据收集和分析过程，通过加速数据收集和减少时间83.3%来收集超过一千个实验数据，比前一个研究的平均数据量大得多。同时，研究者使用了三种算法，包括人工神经网络、多ivariate 回归和随机森林，来研究数据特征的影响。</li>
<li>results: 研究结果表明，使用人工神经网络和随机森林算法时，大量数据可以显著提高预测精度。此外，研究还发现数据规模和范围对预测精度和影响因素排名的影响很大。同时，研究发现人工神经网络在推广范围上的描述性能受到数据范围的影响。这些结果表明，大量的实验数据收集和分析，以及数据特征的影响分析是机器学习在太阳蒸馏净水领域的重要步骤，可以推广机器学习在这个领域的应用。<details>
<summary>Abstract</summary>
An effective interdisciplinary study between machine learning and solar-thermal desalination requires a sufficiently large and well-analyzed experimental datasets. This study develops a modified dataset collection and analysis process for studying solar-thermal desalination by machine learning. Based on the optimized water condensation and collection process, the proposed experimental method collects over one thousand datasets, which is ten times more than the average number of datasets in previous works, by accelerating data collection and reducing the time by 83.3%. On the other hand, the effects of dataset features are investigated by using three different algorithms, including artificial neural networks, multiple linear regressions, and random forests. The investigation focuses on the effects of dataset size and range on prediction accuracy, factor importance ranking, and the model's generalization ability. The results demonstrate that a larger dataset can significantly improve prediction accuracy when using artificial neural networks and random forests. Additionally, the study highlights the significant impact of dataset size and range on ranking the importance of influence factors. Furthermore, the study reveals that the extrapolation data range significantly affects the extrapolation accuracy of artificial neural networks. Based on the results, massive dataset collection and analysis of dataset feature effects are important steps in an effective and consistent machine learning process flow for solar-thermal desalination, which can promote machine learning as a more general tool in the field of solar-thermal desalination.
</details>
<details>
<summary>摘要</summary>
要有效地结合机器学习和太阳蒸馈淡水，需要一个足够大、且具有分析力的实验数据集。本研究提出了一种修改后的数据采集和分析过程，用于通过机器学习研究太阳蒸馈淡水。基于优化的水蒸馈和收集过程，该方法收集了超过一千个数据集，比前一个平均数据集的十倍多，并将采集时间减少了83.3%。而且，该研究通过使用三种不同的算法，包括人工神经网络、多元线性回归和随机森林，研究数据集大小和范围对预测精度、因素重要性排名和模型泛化能力的影响。结果表明，大量数据集可以在使用人工神经网络和随机森林时显著提高预测精度。此外，研究还发现数据集大小和范围对因素重要性排名产生了重要影响。此外，研究还发现人工神经网络抽象数据范围对抽象预测精度产生了重要影响。根据结果，大量数据采集和分析数据集特征效果是机器学习过程中不可或缺的一步，可以推动机器学习在太阳蒸馈淡水领域的普遍应用。
</details></li>
</ul>
<hr>
<h2 id="InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis"><a href="#InVAErt-networks-a-data-driven-framework-for-emulation-inference-and-identifiability-analysis" class="headerlink" title="InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis"></a>InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12586">http://arxiv.org/abs/2307.12586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxiang Grayson Tong, Carlos A. Sing Long, Daniele E. Schiavazzi</li>
<li>for: 本研究旨在推广使用生成模型和深度学习来解决物理系统的设计和分析问题，而不仅仅是模拟任务。</li>
<li>methods: 该研究提出了一种名为inVAErt网络的框架，该框架使用确定性编码器和解码器来表示前向和反向解决 Map，使用流变换模型来捕捉系统输出的概率分布，并使用变量编码器来学习减少输入和输出之间的不一致性。</li>
<li>results: 研究人员通过数值实验证明了inVAErt网络的可行性和灵活性，并发现选择罚分 coefficient和积分空间抽取策略对训练和测试性能有重要影响。<details>
<summary>Abstract</summary>
Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We validate our framework through extensive numerical examples, including simple linear, nonlinear, and periodic maps, dynamical systems, and spatio-temporal PDEs.
</details>
<details>
<summary>摘要</summary>
使用生成模型和深度学习来处理物理系统的应用主要是 emulator。然而，这些数据驱动架构的灵活性表示可以扩展到其他系统设计方面，包括模型反转和可识别性。我们介绍inVAErt（pronounced  inverse）网络，一个涵盖数据驱动分析和设计参数物理系统的框架，使用决定性编码器和解码器表示前向和反向解决Map，使用正态流捕捉系统输出的概率分布，并使用可变编码器学习减少输入和输出之间的不一致。我们正式调查损害征素在损失函数中的选择和latent空间抽样策略，因为我们发现这些对训练和测试性能有很大影响。我们通过大量的数字例子验证了我们的框架，包括简单的线性、非线性和 periodic  maps，动力系统和时空PDEs。
</details></li>
</ul>
<hr>
<h2 id="Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data"><a href="#Self-refining-of-Pseudo-Labels-for-Music-Source-Separation-with-Noisy-Labeled-Data" class="headerlink" title="Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data"></a>Self-refining of Pseudo Labels for Music Source Separation with Noisy Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12576">http://arxiv.org/abs/2307.12576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghyun Koo, Yunkee Chae, Chang-Bin Jeon, Kyogu Lee</li>
<li>for: 提高音乐源分离（MSS）性能，增加大数据集来改进MSS模型的训练</li>
<li>methods: 自动地对含有噪声标签的数据集进行自我反射，提高MSS模型的识别精度</li>
<li>results: 使用自我反射的数据集可以达到与使用干净标签的数据集相同的识别精度，而且在只有噪声标签数据集的情况下，MSS模型训练在自我反射数据集上可以超过使用干净标签数据集训练的性能。<details>
<summary>Abstract</summary>
Music source separation (MSS) faces challenges due to the limited availability of correctly-labeled individual instrument tracks. With the push to acquire larger datasets to improve MSS performance, the inevitability of encountering mislabeled individual instrument tracks becomes a significant challenge to address. This paper introduces an automated technique for refining the labels in a partially mislabeled dataset. Our proposed self-refining technique, employed with a noisy-labeled dataset, results in only a 1% accuracy degradation in multi-label instrument recognition compared to a classifier trained on a clean-labeled dataset. The study demonstrates the importance of refining noisy-labeled data in MSS model training and shows that utilizing the refined dataset leads to comparable results derived from a clean-labeled dataset. Notably, upon only access to a noisy dataset, MSS models trained on a self-refined dataset even outperform those trained on a dataset refined with a classifier trained on clean labels.
</details>
<details>
<summary>摘要</summary>
音乐源分离（MSS）面临限量正确标注个 instrumente 轨迹的问题。随着提高 MSS性能的努力，遇到带有错误标注的个 instrumente 轨迹的可能性变得非常重要。这篇文章介绍了一种自动刷新标注的技术，可以在带有噪声标注的 dataset 上进行刷新。我们的提议的自我刷新技术与噪声标注 dataset 上的类ifier 结合使用，对多个标签 instrumente 识别中的准确率进行了1%的下降。这种研究表明了刷新噪声标注数据的重要性，并证明了使用刷新后的数据可以达到与清晰标注数据相同的结果。甚至只有带有噪声标注的数据，MSS模型在使用自我刷新数据进行训练后会比使用刷新后的数据进行训练后更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalising-Neural-Topical-Representations"><a href="#Towards-Generalising-Neural-Topical-Representations" class="headerlink" title="Towards Generalising Neural Topical Representations"></a>Towards Generalising Neural Topical Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12564">http://arxiv.org/abs/2307.12564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohao Yang, He Zhao, Dinh Phung, Lan Du</li>
<li>for: 提高 neural topic model（NTM）的通用能力，使其可以在不同的资料集中具有可靠的泛化能力。</li>
<li>methods: 使用数据扩充和层次话题交通距离（HOTT）计算优化运输（OT）距离，以iminimize similar documents的semantic distance during training NTMs。</li>
<li>results: 对NTMs进行了扩展，使其在不同的资料集中具有显著提高的泛化能力。<details>
<summary>Abstract</summary>
Topic models have evolved from conventional Bayesian probabilistic models to Neural Topic Models (NTMs) over the last two decays. Although NTMs have achieved promising performance when trained and tested on a specific corpus, their generalisation ability across corpora is rarely studied. In practice, we often expect that an NTM trained on a source corpus can still produce quality topical representation for documents in a different target corpus without retraining. In this work, we aim to improve NTMs further so that their benefits generalise reliably across corpora and tasks. To do so, we propose to model similar documents by minimising their semantical distance when training NTMs. Specifically, similar documents are created by data augmentation during training; The semantical distance between documents is measured by the Hierarchical Topic Transport Distance (HOTT), which computes the Optimal Transport (OT) distance between the topical representations. Our framework can be readily applied to most NTMs as a plug-and-play module. Extensive experiments show that our framework significantly improves the generalisation ability regarding neural topical representation across corpora.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction"><a href="#DeepGATGO-A-Hierarchical-Pretraining-Based-Graph-Attention-Model-for-Automatic-Protein-Function-Prediction" class="headerlink" title="DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction"></a>DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13004">http://arxiv.org/abs/2307.13004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Li, Changkun Jiang, Jianqiang Li</li>
<li>for: automatic protein function prediction (AFP)</li>
<li>methods:  sequence-based hierarchical prediction method using graph attention networks (GATs) and contrastive learning</li>
<li>results: better scalability in GO term enrichment analysis on large-scale datasets<details>
<summary>Abstract</summary>
Automatic protein function prediction (AFP) is classified as a large-scale multi-label classification problem aimed at automating protein enrichment analysis to eliminate the current reliance on labor-intensive wet-lab methods. Currently, popular methods primarily combine protein-related information and Gene Ontology (GO) terms to generate final functional predictions. For example, protein sequences, structural information, and protein-protein interaction networks are integrated as prior knowledge to fuse with GO term embeddings and generate the ultimate prediction results. However, these methods are limited by the difficulty in obtaining structural information or network topology information, as well as the accuracy of such data. Therefore, more and more methods that only use protein sequences for protein function prediction have been proposed, which is a more reliable and computationally cheaper approach. However, the existing methods fail to fully extract feature information from protein sequences or label data because they do not adequately consider the intrinsic characteristics of the data itself. Therefore, we propose a sequence-based hierarchical prediction method, DeepGATGO, which processes protein sequences and GO term labels hierarchically, and utilizes graph attention networks (GATs) and contrastive learning for protein function prediction. Specifically, we compute embeddings of the sequence and label data using pre-trained models to reduce computational costs and improve the embedding accuracy. Then, we use GATs to dynamically extract the structural information of non-Euclidean data, and learn general features of the label dataset with contrastive learning by constructing positive and negative example samples. Experimental results demonstrate that our proposed model exhibits better scalability in GO term enrichment analysis on large-scale datasets.
</details>
<details>
<summary>摘要</summary>
自动蛋白功能预测（AFP）被分类为大规模多标签分类问题，旨在自动化蛋白聚集分析，以消除现有的人工劳动密集方法。现有的popular方法主要结合蛋白质相关信息和生物学功能 ontology（GO）标签来生成最终的功能预测结果。例如，蛋白序列、结构信息和蛋白蛋白交互网络被融合到GO标签嵌入中，以生成最终的预测结果。然而，这些方法受到蛋白质结构信息或网络拓扑信息的困难性和准确性的限制。因此，越来越多的方法只使用蛋白序列进行蛋白功能预测，这是一种更可靠和计算成本更低的方法。然而，现有的方法无法充分EXTRACT蛋白序列和标签数据中的特征信息。因此，我们提出了一种遵循蛋白序列层次预测方法，深度GATGO，该方法可以处理蛋白序列和GO标签数据层次，并使用图注意力网络（GATs）和对比学习来进行蛋白功能预测。具体来说，我们使用预训练模型计算蛋白序列和标签数据的嵌入，以降低计算成本并提高嵌入精度。然后，我们使用GATs动态提取蛋白序列非几何数据的结构信息，并通过对比学习学习标签数据的通用特征。实验结果表明，我们提出的模型在大规模GO标签浸泡分析中展现出较好的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning"><a href="#Homophily-Driven-Sanitation-View-for-Robust-Graph-Contrastive-Learning" class="headerlink" title="Homophily-Driven Sanitation View for Robust Graph Contrastive Learning"></a>Homophily-Driven Sanitation View for Robust Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12555">http://arxiv.org/abs/2307.12555</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Yulin Zhu, Xing Ai, Yevgeniy Vorobeychik, Kai Zhou</li>
<li>for: 这个论文旨在探讨Graph Contrastive Learning（GCL）对于结构攻击的 adversarial robustness。</li>
<li>methods: 这篇论文使用了一系列的攻击分析和理论分析，揭示了现有攻击的弱点和如何降低GCL的性能。此外，它还提出了一种robust GCL框架，该框架通过 integrate homophily-driven sanitation view来增强GCL的鲁棒性。然而，sanitation objective的非导数性带来了一些挑战，以下是一些解决这些挑战的技巧。</li>
<li>results: 我们的实验结果表明，GCHS（Graph Contrastive Learning with Homophily-driven Sanitation View）在两种状态之前的顶尖模型面前占据了优势，并在生成节点 embedding 和两个重要的下游任务上表现出色。<details>
<summary>Abstract</summary>
We investigate adversarial robustness of unsupervised Graph Contrastive Learning (GCL) against structural attacks. First, we provide a comprehensive empirical and theoretical analysis of existing attacks, revealing how and why they downgrade the performance of GCL. Inspired by our analytic results, we present a robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. A key challenge this poses, however, is the non-differentiable nature of the sanitation objective. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method which, unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state of the art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state of the art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details>
<details>
<summary>摘要</summary>
我们研究不监督图像对比学习（GCL）的抗击力，特别是对于结构性攻击。首先，我们提供了广泛的实验和理论分析，揭示了现有攻击的如何和为什么会下降GCL性能。 inspirited by our analytic results, we present a Robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. However, the non-differentiable nature of the sanitation objective poses a key challenge. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method, which unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state-of-the-art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state-of-the-art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.Here's the translation of the text in Traditional Chinese:我们研究不监督图像对比学习（GCL）的抗击力，特别是对于结构性攻击。首先，我们提供了广泛的实验和理论分析，揭示了现有攻击的如何和为什么会下降GCL性能。 inspirited by our analytic results, we present a Robust GCL framework that integrates a homophily-driven sanitation view, which can be learned jointly with contrastive learning. However, the non-differentiable nature of the sanitation objective poses a key challenge. To address this challenge, we propose a series of techniques to enable gradient-based end-to-end robust GCL. Moreover, we develop a fully unsupervised hyperparameter tuning method, which unlike prior approaches, does not require knowledge of node labels. We conduct extensive experiments to evaluate the performance of our proposed model, GCHS (Graph Contrastive Learning with Homophily-driven Sanitation View), against two state-of-the-art structural attacks on GCL. Our results demonstrate that GCHS consistently outperforms all state-of-the-art baselines in terms of the quality of generated node embeddings as well as performance on two important downstream tasks.
</details></li>
</ul>
<hr>
<h2 id="Continuation-Path-Learning-for-Homotopy-Optimization"><a href="#Continuation-Path-Learning-for-Homotopy-Optimization" class="headerlink" title="Continuation Path Learning for Homotopy Optimization"></a>Continuation Path Learning for Homotopy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12551">http://arxiv.org/abs/2307.12551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xi-l/cpl">https://github.com/xi-l/cpl</a></li>
<li>paper_authors: Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, Qingfu Zhang</li>
<li>for: 提高Homotopy优化的效果和可用性，并提供更多的解决方案选择机会。</li>
<li>methods: 提出了一种基于模型的方法，可以同时优化原始问题和所有优化子问题，并实时生成任意中间解决方案。</li>
<li>results: 实验表明，该方法可以明显提高Homotopy优化的性能，并提供更多的有用信息支持更好的决策。<details>
<summary>Abstract</summary>
Homotopy optimization is a traditional method to deal with a complicated optimization problem by solving a sequence of easy-to-hard surrogate subproblems. However, this method can be very sensitive to the continuation schedule design and might lead to a suboptimal solution to the original problem. In addition, the intermediate solutions, often ignored by classic homotopy optimization, could be useful for many real-world applications. In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which contains infinite intermediate solutions for any surrogate subproblems. Rather than the classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which could be desirable for many applications. Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel model-based approach to learn the whole continuation path for homotopy optimization, which includes infinite intermediate solutions for any surrogate subproblems. Unlike classic unidirectional easy-to-hard optimization, our method can simultaneously optimize the original problem and all surrogate subproblems in a collaborative manner. The proposed model also supports real-time generation of any intermediate solution, which can be desirable for many applications.Experimental studies on different problems show that our proposed method can significantly improve the performance of homotopy optimization and provide extra helpful information to support better decision-making.
</details></li>
</ul>
<hr>
<h2 id="On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness"><a href="#On-the-Connection-between-Pre-training-Data-Diversity-and-Fine-tuning-Robustness" class="headerlink" title="On the Connection between Pre-training Data Diversity and Fine-tuning Robustness"></a>On the Connection between Pre-training Data Diversity and Fine-tuning Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12532">http://arxiv.org/abs/2307.12532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Ramanujan, Thao Nguyen, Sewoong Oh, Ludwig Schmidt, Ali Farhadi</li>
<li>for: 了解预训练策略对下游模型的泛化性质的影响。</li>
<li>methods: 研究预训练分布的属性对下游模型的可靠性的影响，包括标签空间、标签 semantics、图像多样性、数据领域和数据量等因素。</li>
<li>results: 发现预训练数据量是下游模型的可靠性的关键因素，其他因素具有有限的影响。例如，将 ImageNet 预训练类减少到 4 倍，同时将每个类的图像数量增加到 4 倍（即保持总数据量不变）不会影响 fine-tuned 模型的可靠性。通过使用不同的自然和Synthetic 数据源预训练分布，主要通过 iWildCam-WILDS 分布转换测试下游模型的可靠性。<details>
<summary>Abstract</summary>
Pre-training has been widely adopted in deep learning to improve model performance, especially when the training data for a target task is limited. In our work, we seek to understand the implications of this training strategy on the generalization properties of downstream models. More specifically, we ask the following question: how do properties of the pre-training distribution affect the robustness of a fine-tuned model? The properties we explore include the label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution. We find that the primary factor influencing downstream effective robustness (Taori et al., 2020) is data quantity, while other factors have limited significance. For example, reducing the number of ImageNet pre-training classes by 4x while increasing the number of images per class by 4x (that is, keeping total data quantity fixed) does not impact the robustness of fine-tuned models. We demonstrate our findings on pre-training distributions drawn from various natural and synthetic data sources, primarily using the iWildCam-WILDS distribution shift as a test for downstream robustness.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。</SYS>>预训练已广泛应用于深度学习中，以提高模型性能，特别是当目标任务的训练数据scarce时。在我们的工作中，我们想要了解预训练策略对下游模型的泛化性质产生的影响。更 Specifically，我们问的问题是：预训练分布的属性如何影响下游模型的可靠性？我们探讨的属性包括标签空间、标签 semantics、图像多样性、数据领域和数据量。我们发现预训练数据量是下游可靠性的主要因素，而其他因素具有有限的意义。例如，将 ImageNet 预训练类别数量减少到 4 倍，同时图像每类数量增加 4 倍（即保持总数据量不变），不会影响 Fine-tune 模型的可靠性。我们通过不同的自然和 sintetic 数据源中的预训练分布，主要使用 iWildCam-WILDS 分布转换为下游可靠性的测试。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph"><a href="#Rethinking-Medical-Report-Generation-Disease-Revealing-Enhancement-with-Knowledge-Graph" class="headerlink" title="Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph"></a>Rethinking Medical Report Generation: Disease Revealing Enhancement with Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12526">http://arxiv.org/abs/2307.12526</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyixinxin/mrg-kg">https://github.com/wangyixinxin/mrg-kg</a></li>
<li>paper_authors: Yixin Wang, Zihao Lin, Haoyu Dong</li>
<li>for: 这个研究旨在提高医疗报告生成（MRG）的品质，特别是透过知识图（KG）来导向生成过程。</li>
<li>methods: 本研究使用了一个完整的KG，包括137种疾病和问题，并导入了一个新的增强描述疾病类型的增强策略，以解决长条形分布问题。</li>
<li>results: 研究发现，提案的两阶段生成框架和增强策略可以提高生成的多样性和准确性，并有着显著的改善效果。<details>
<summary>Abstract</summary>
Knowledge Graph (KG) plays a crucial role in Medical Report Generation (MRG) because it reveals the relations among diseases and thus can be utilized to guide the generation process. However, constructing a comprehensive KG is labor-intensive and its applications on the MRG process are under-explored. In this study, we establish a complete KG on chest X-ray imaging that includes 137 types of diseases and abnormalities. Based on this KG, we find that the current MRG data sets exhibit a long-tailed problem in disease distribution. To mitigate this problem, we introduce a novel augmentation strategy that enhances the representation of disease types in the tail-end of the distribution. We further design a two-stage MRG approach, where a classifier is first trained to detect whether the input images exhibit any abnormalities. The classified images are then independently fed into two transformer-based generators, namely, ``disease-specific generator" and ``disease-free generator" to generate the corresponding reports. To enhance the clinical evaluation of whether the generated reports correctly describe the diseases appearing in the input image, we propose diverse sensitivity (DS), a new metric that checks whether generated diseases match ground truth and measures the diversity of all generated diseases. Results show that the proposed two-stage generation framework and augmentation strategies improve DS by a considerable margin, indicating a notable reduction in the long-tailed problem associated with under-represented diseases.
</details>
<details>
<summary>摘要</summary>
医疗报告生成（MRG）中知识图（KG）扮演着关键性的角色，因为它揭示疾病之间的关系，可以用于导航生成过程。然而，建立全面的KG是劳动密集的，而其在MRG过程中的应用还尚未得到了充分的探索。本研究中，我们建立了包含137种疾病和异常的完整KG，基于这个KG，我们发现现有的MRG数据集具有长尾分布问题。为了解决这个问题，我们提出了一种新的增强策略，增强疾病类型在分布尾部的表示。此外，我们设计了两个阶段的MRG方法，其中第一阶段使用分类器来检测输入图像是否具有任何异常。经过分类后，图像分别被独立地传递到两个基于转换器的生成器，即“疾病特定生成器”和“疾病无效生成器”，以生成对应的报告。为了提高生成报告的临床评估，我们提出了多样性敏感度（DS），一种新的指标，用于检查生成的疾病与实际情况是否匹配，并测量所有生成的疾病的多样性。结果显示，我们的两个阶段生成框架和增强策略可以大幅提高DS， indicating a considerable reduction in the long-tailed problem associated with under-represented diseases.
</details></li>
</ul>
<hr>
<h2 id="Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm"><a href="#Landslide-Surface-Displacement-Prediction-Based-on-VSXC-LSTM-Algorithm" class="headerlink" title="Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm"></a>Landslide Surface Displacement Prediction Based on VSXC-LSTM Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12524">http://arxiv.org/abs/2307.12524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ruichen Li, Fan Liu, Xingquan Li, Juan Cheng, Muzhou Hou, Cong Cao</li>
<li>for: 预测地面滑坡表面变位</li>
<li>methods: 基于变形模式分解（VMD）、SegSigmoid函数、XGBoost算法和嵌入LSTM neural network的时序预测框架（VSXC-LSTM）</li>
<li>results: 在测试集上，模型表现良好，除了随机项 subsequences 难以适应外， periodic item subsequence 和 trend item subsequence 的 RMSE 和 MAPE 都小于 0.1， periodic item prediction module 基于 XGBoost 的 RMSE 为 0.006。<details>
<summary>Abstract</summary>
Landslide is a natural disaster that can easily threaten local ecology, people's lives and property. In this paper, we conduct modelling research on real unidirectional surface displacement data of recent landslides in the research area and propose a time series prediction framework named VMD-SegSigmoid-XGBoost-ClusterLSTM (VSXC-LSTM) based on variational mode decomposition, which can predict the landslide surface displacement more accurately. The model performs well on the test set. Except for the random item subsequence that is hard to fit, the root mean square error (RMSE) and the mean absolute percentage error (MAPE) of the trend item subsequence and the periodic item subsequence are both less than 0.1, and the RMSE is as low as 0.006 for the periodic item prediction module based on XGBoost\footnote{Accepted in ICANN2023}.
</details>
<details>
<summary>摘要</summary>
地面滑坡是自然灾害，容易威胁当地生态、人们生命和财产。在这篇论文中，我们基于实际的单向表面偏移数据进行模拟研究，并提出了一种基于变幅模式分解的时间序列预测框架，称为VMD-SegSigmoid-XGBoost-ClusterLSTM（VSXC-LSTM）。这种模型可以更准确地预测滑坡表面偏移。测试集上的表现良好，只有随机项子序列难以适应，RMSE和MAPE值均小于0.1， periodic item prediction module based on XGBoost的RMSE值为0.006（ Accepted in ICANN2023）。
</details></li>
</ul>
<hr>
<h2 id="Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation"><a href="#Lost-In-Translation-Generating-Adversarial-Examples-Robust-to-Round-Trip-Translation" class="headerlink" title="Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation"></a>Lost In Translation: Generating Adversarial Examples Robust to Round-Trip Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12520">http://arxiv.org/abs/2307.12520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neelbhandari6/nmt_text_attack">https://github.com/neelbhandari6/nmt_text_attack</a></li>
<li>paper_authors: Neel Bhandari, Pin-Yu Chen</li>
<li>for: 研究文章探讨了现有文本 adversarial 攻击的稳定性，特别是对于保持了 considerable similarity 的文本 adversarial examples。</li>
<li>methods: 文章使用了 six state-of-the-art text-based adversarial attacks，并对它们进行了 round-trip translation 测试。此外，文章还提出了一种基于 machine translation 的解决方案，以增强 adversarial example 的稳定性。</li>
<li>results: 研究发现，six state-of-the-art text-based adversarial attacks 在 round-trip translation 下失效，而 integrate machine translation into adversarial example generation 可以提高稳定性。这些结果表明，找到可以抗 translation 的 adversarial examples 可以帮助找到语言模型的缺陷，并促进更多关于多语言 adversarial attacks 的研究。<details>
<summary>Abstract</summary>
Language Models today provide a high accuracy across a large number of downstream tasks. However, they remain susceptible to adversarial attacks, particularly against those where the adversarial examples maintain considerable similarity to the original text. Given the multilingual nature of text, the effectiveness of adversarial examples across translations and how machine translations can improve the robustness of adversarial examples remain largely unexplored. In this paper, we present a comprehensive study on the robustness of current text adversarial attacks to round-trip translation. We demonstrate that 6 state-of-the-art text-based adversarial attacks do not maintain their efficacy after round-trip translation. Furthermore, we introduce an intervention-based solution to this problem, by integrating Machine Translation into the process of adversarial example generation and demonstrating increased robustness to round-trip translation. Our results indicate that finding adversarial examples robust to translation can help identify the insufficiency of language models that is common across languages, and motivate further research into multilingual adversarial attacks.
</details>
<details>
<summary>摘要</summary>
现代语言模型在许多下游任务上具有高精度。然而，它们仍然容易受到敌意攻击，特别是那些维持了原文和敌意例子之间的相似性。由于文本的多语言性，攻击者可以利用不同语言的文本来攻击语言模型。在这篇论文中，我们展示了现有的文本基于攻击的六种状态体验的不稳定性，并证明它们在翻译后不再有效。此外，我们还介绍了一种基于机器翻译的解决方案，并证明该方法可以提高攻击例子的翻译稳定性。我们的结果表明，找到可以抵抗翻译的攻击例子可以帮助我们发现语言模型的共同缺陷，并促进多语言攻击的研究。
</details></li>
</ul>
<hr>
<h2 id="DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning"><a href="#DEPHN-Different-Expression-Parallel-Heterogeneous-Network-using-virtual-gradient-optimization-for-Multi-task-Learning" class="headerlink" title="DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning"></a>DEPHN: Different Expression Parallel Heterogeneous Network using virtual gradient optimization for Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12519">http://arxiv.org/abs/2307.12519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Ri Su, Shaojie Zhao, Muzhou Hou</li>
<li>for: This paper proposes a new method for multi-task learning (MTL) recommendation systems to better understand user behavior in complex scenarios.</li>
<li>methods: The proposed method, called Different Expression Parallel Heterogeneous Network (DEPHN), uses different feature interaction methods to improve the generalization ability of shared information flow and adaptively adjusts the learning intensity of gated units based on task correlation.</li>
<li>results: Extensive experiments on artificial and real-world datasets demonstrate that DEPHN can capture task correlation in complex situations and achieve better performance than baseline models.Here’s the simplified Chinese version:</li>
<li>for: 这篇论文提出了一种基于多任务学习（MTL）的推荐系统，以更好地理解在复杂情况下用户行为。</li>
<li>methods: 提议的方法是多表达并发异构网络（DEPHN），通过不同的特征交互方法提高共享信息流的泛化能力，并在训练过程中通过特征显式映射和虚梯度系数进行专家阀控，以适应不同任务信息流的差异。</li>
<li>results: 对于人工和实际数据集的广泛实验表明，DEPHN可以在复杂情况下捕捉任务相关性，并比基线模型表现更好。<details>
<summary>Abstract</summary>
Recommendation system algorithm based on multi-task learning (MTL) is the major method for Internet operators to understand users and predict their behaviors in the multi-behavior scenario of platform. Task correlation is an important consideration of MTL goals, traditional models use shared-bottom models and gating experts to realize shared representation learning and information differentiation. However, The relationship between real-world tasks is often more complex than existing methods do not handle properly sharing information. In this paper, we propose an Different Expression Parallel Heterogeneous Network (DEPHN) to model multiple tasks simultaneously. DEPHN constructs the experts at the bottom of the model by using different feature interaction methods to improve the generalization ability of the shared information flow. In view of the model's differentiating ability for different task information flows, DEPHN uses feature explicit mapping and virtual gradient coefficient for expert gating during the training process, and adaptively adjusts the learning intensity of the gated unit by considering the difference of gating values and task correlation. Extensive experiments on artificial and real-world datasets demonstrate that our proposed method can capture task correlation in complex situations and achieve better performance than baseline models\footnote{Accepted in IJCNN2023}.
</details>
<details>
<summary>摘要</summary>
互联网运营商可以通过多任务学习（MTL）来理解用户和预测他们在多行为场景中的行为。传统模型通过共享底部模型和阻塞专家来实现共享表示学习和信息差异化。然而，现实世界中任务之间的关系经常比既有方法不够好地处理共享信息。在这篇论文中，我们提出了不同表达平行多样性网络（DEPHN），以同时模型多个任务。DEPHN通过不同的特征互动方法来提高共享信息流的泛化能力。对于模型对不同任务信息流的分化能力，DEPHN使用特征显式映射和虚拟梯度系数进行专家闭合 durante 训练过程中，并根据计算任务相互关系的差异来自适应学习Intensity of gated unit。经过了人工和实际世界的广泛实验，我们的提议方法可以在复杂的情况下捕捉任务相互关系，并在基eline模型的基础上提高表现。
</details></li>
</ul>
<hr>
<h2 id="FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks"><a href="#FaFCNN-A-General-Disease-Classification-Framework-Based-on-Feature-Fusion-Neural-Networks" class="headerlink" title="FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks"></a>FaFCNN: A General Disease Classification Framework Based on Feature Fusion Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12518">http://arxiv.org/abs/2307.12518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglin Kong, Shaojie Zhao, Juan Cheng, Xingquan Li, Ri Su, Muzhou Hou, Cong Cao</li>
<li>for: 本研究旨在 Addressing two fundamental problems in applying deep learning&#x2F;machine learning methods to disease classification tasks, namely insufficient training samples and feature fusion.</li>
<li>methods: 提出了一种基于域对抗学习的Feature-aware Fusion Correlation Neural Network (FaFCNN)，具有特点增强样本相关性和特征对齐。</li>
<li>results: 实验结果表明，使用增强的特征获得融合特征后，FaFCNN可以更好地提高疾病分类性能，特别是在低质量数据集上。此外，广泛的实验还证明了模型的稳定性和每个组件的有效性。<details>
<summary>Abstract</summary>
There are two fundamental problems in applying deep learning/machine learning methods to disease classification tasks, one is the insufficient number and poor quality of training samples; another one is how to effectively fuse multiple source features and thus train robust classification models. To address these problems, inspired by the process of human learning knowledge, we propose the Feature-aware Fusion Correlation Neural Network (FaFCNN), which introduces a feature-aware interaction module and a feature alignment module based on domain adversarial learning. This is a general framework for disease classification, and FaFCNN improves the way existing methods obtain sample correlation features. The experimental results show that training using augmented features obtained by pre-training gradient boosting decision tree yields more performance gains than random-forest based methods. On the low-quality dataset with a large amount of missing data in our setup, FaFCNN obtains a consistently optimal performance compared to competitive baselines. In addition, extensive experiments demonstrate the robustness of the proposed method and the effectiveness of each component of the model\footnote{Accepted in IEEE SMC2023}.
</details>
<details>
<summary>摘要</summary>
“有两个基本问题在应用深度学习/机器学习方法进行疾病分类任务中，一个是训练样本数量和质量不足; 另一个是如何有效地融合多个来源特征，以训练可靠的分类模型。为了解决这些问题，我们提出了基于人类学习知识的Feature-aware Fusion Correlation Neural Network（FaFCNN）框架。这是一个通用的疾病分类框架，并且FaFCNN可以将现有方法中取得的样本相互联系特征改进。实验结果显示，使用增强决策树进行预训练后的扩展特征可以实现更多的性能提升，而且在我们的设置中，FaFCNN在低质量样本大量遗传数据上取得了一致性的最佳性能。此外，广泛的实验显示了提案的方法的稳定性和每个模型 ком成分的有效性。”Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Evaluation-of-Temporal-Graph-Benchmark"><a href="#An-Empirical-Evaluation-of-Temporal-Graph-Benchmark" class="headerlink" title="An Empirical Evaluation of Temporal Graph Benchmark"></a>An Empirical Evaluation of Temporal Graph Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12510">http://arxiv.org/abs/2307.12510</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yule-BUAA/DyGLib_TGB">https://github.com/yule-BUAA/DyGLib_TGB</a></li>
<li>paper_authors: Le Yu</li>
<li>for: 本研究是一个empirical evaluation of Temporal Graph Benchmark (TGB)，通过扩展我们的Dynamic Graph Library (DyGLib)来对TGB进行评估。</li>
<li>methods: 本研究使用了11种流行的动态图学习方法进行更加广泛的比较，包括TGB中所report的基eline。</li>
<li>results: 通过实验发现，不同的模型在不同的数据集上表现出了不同的性能，与之前的观察一致；同时，使用DyGLib可以对一些基eline进行显著改进，超过TGB的报告结果。<details>
<summary>Abstract</summary>
In this paper, we conduct an empirical evaluation of Temporal Graph Benchmark (TGB) by extending our Dynamic Graph Library (DyGLib) to TGB. Compared with TGB, we include eleven popular dynamic graph learning methods for more exhaustive comparisons. Through the experiments, we find that (1) different models depict varying performance across various datasets, which is in line with previous observations; (2) the performance of some baselines can be significantly improved over the reported results in TGB when using DyGLib. This work aims to ease the researchers' efforts in evaluating various dynamic graph learning methods on TGB and attempts to offer results that can be directly referenced in the follow-up research. All the used resources in this project are publicly available at https://github.com/yule-BUAA/DyGLib_TGB. This work is in progress, and feedback from the community is welcomed for improvements.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们进行了emporical评估Temporal Graph Benchmark（TGB）的扩展，使用我们的动态图库（DyGLib）来对TGB进行评估。相比TGB，我们包括了 eleven 种流行的动态图学习方法，以便进行更加详细的比较。通过实验，我们发现了以下两点：1. 不同的模型在不同的数据集上表现出了不同的性能，这与之前的观察一致。2. 一些基elines的性能可以通过使用DyGLib进行改进，这与TGB中report的结果相比有所提高。这项工作的目标是为研究者提供一个便利的评估多种动态图学习方法的平台，并提供可直接引用的结果。我们使用的所有资源都公开可用于https://github.com/yule-BUAA/DyGLib_TGB。这项工作正在进行中，欢迎社区提供反馈以便进行改进。
</details></li>
</ul>
<hr>
<h2 id="AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models"><a href="#AdvDiff-Generating-Unrestricted-Adversarial-Examples-using-Diffusion-Models" class="headerlink" title="AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models"></a>AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12499">http://arxiv.org/abs/2307.12499</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuelong Dai, Kaisheng Liang, Bin Xiao</li>
<li>for: 防御深度学习模型受到攻击的攻击方法研究</li>
<li>methods: 使用扩散模型生成不受限制的攻击示例，并提出两种新的反向生成导航技术来进行攻击采样</li>
<li>results: 对MNIST和ImageNet dataset进行实验，得到了高质量、真实的攻击示例，并超越了基于GAN的方法在攻击性和生成质量上In English, this means:</li>
<li>for: Research on adversarial attacks against deep learning models and defense techniques</li>
<li>methods: Using diffusion models to generate unrestricted adversarial examples, and proposing two novel adversarial guidance techniques for reverse generation</li>
<li>results: Experimental results on MNIST and ImageNet datasets show that AdvDiff is effective in generating high-quality, realistic adversarial examples that outperform GAN-based methods in attack performance and generation quality.<details>
<summary>Abstract</summary>
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective to generate unrestricted adversarial examples, which outperforms GAN-based methods in terms of attack performance and generation quality.
</details>
<details>
<summary>摘要</summary>
深度学习模型面临了无限制敌意攻击的威胁，这些攻击可以有效绕过防御机制。然而，先前的攻击方法 часто使用生成对抗网络（GAN），这些网络不是理论可证明的，因此会生成不实际的例子，特别是对于大规模的数据集如ImageNet。在这篇论文中，我们提出了一种新的方法，称为AdvDiff，用于生成无限制敌意例子。我们设计了两种新的对抗导航技术，用于在扩散模型的反生成过程中进行对抗采样。这两种技术可以生成高质量、实际的敌意例子，通过可视化目标分类器的梯度来 интегрирова。实验结果表明，AdvDiff在MNIST和ImageNet数据集上效果地生成了无限制敌意例子，其性能和生成质量都高于基于GAN的方法。
</details></li>
</ul>
<hr>
<h2 id="A-faster-and-simpler-algorithm-for-learning-shallow-networks"><a href="#A-faster-and-simpler-algorithm-for-learning-shallow-networks" class="headerlink" title="A faster and simpler algorithm for learning shallow networks"></a>A faster and simpler algorithm for learning shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12496">http://arxiv.org/abs/2307.12496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Sitan Chen, Shyam Narayanan</li>
<li>for: 学习一个线性组合中的ReLU活化器，给出标注的例子来自标准的$d$-维高斯分布。</li>
<li>methods: 使用Chen et al.的算法， runtime在$\text{poly}(d,1&#x2F;\varepsilon)$时间内运行，并在多个阶段学习。</li>
<li>results: 显示了一个简单的一阶版本的算法可以 suffices，并且其运行时间只是 $(d&#x2F;\varepsilon)^{O(k^2)} $。<details>
<summary>Abstract</summary>
We revisit the well-studied problem of learning a linear combination of $k$ ReLU activations given labeled examples drawn from the standard $d$-dimensional Gaussian measure. Chen et al. [CDG+23] recently gave the first algorithm for this problem to run in $\text{poly}(d,1/\varepsilon)$ time when $k = O(1)$, where $\varepsilon$ is the target error. More precisely, their algorithm runs in time $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$ and learns over multiple stages. Here we show that a much simpler one-stage version of their algorithm suffices, and moreover its runtime is only $(d/\varepsilon)^{O(k^2)}$.
</details>
<details>
<summary>摘要</summary>
我们回到了已经很受研究的问题：学习一个线性 комбінаción of $k$ ReLU 激活函数， given labeled examples 从标准 $d$-dimensional Gaussian 分布中获取。陈等人 [CDG+23] 最近提出了首个这个问题可以在 $\text{poly}(d,1/\varepsilon)$ 时间内解决的算法，其中 $k = O(1)$，$\varepsilon$ 是目标错误。更加精确地说，他们的算法在多个阶段中执行， runtime 为 $(d/\varepsilon)^{\mathrm{quasipoly}(k)}$。我们现在显示出一个 much simpler 的一阶版本的他们的算法，并且其时间复杂度仅为 $(d/\varepsilon)^{O(k^2)}$。
</details></li>
</ul>
<hr>
<h2 id="Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks"><a href="#Learning-Universal-and-Robust-3D-Molecular-Representations-with-Graph-Convolutional-Networks" class="headerlink" title="Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks"></a>Learning Universal and Robust 3D Molecular Representations with Graph Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12491">http://arxiv.org/abs/2307.12491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Zhang, Yang Liu, Li Xie, Lei Xie</li>
<li>for: 用于学习分子的准确表示，需要考虑分子的化学和几何特征。</li>
<li>methods: 基于分子图表示的方向节点对（DNP）描述器，Robust Molecular Graph Convolutional Network（RoM-GCN）模型可以同时考虑节点和边特征。</li>
<li>results: 对蛋白质和小分子数据集进行评估，研究表明DNP描述器能够具有3D分子几何信息的Robust性，RoM-GCN模型在比较基eline上表现出色。<details>
<summary>Abstract</summary>
To learn accurate representations of molecules, it is essential to consider both chemical and geometric features. To encode geometric information, many descriptors have been proposed in constrained circumstances for specific types of molecules and do not have the properties to be ``robust": 1. Invariant to rotations and translations; 2. Injective when embedding molecular structures. In this work, we propose a universal and robust Directional Node Pair (DNP) descriptor based on the graph representations of 3D molecules. Our DNP descriptor is robust compared to previous ones and can be applied to multiple molecular types. To combine the DNP descriptor and chemical features in molecules, we construct the Robust Molecular Graph Convolutional Network (RoM-GCN) which is capable to take both node and edge features into consideration when generating molecule representations. We evaluate our model on protein and small molecule datasets. Our results validate the superiority of the DNP descriptor in incorporating 3D geometric information of molecules. RoM-GCN outperforms all compared baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN"><a href="#Learning-Resource-Allocation-Policy-Vertex-GNN-or-Edge-GNN" class="headerlink" title="Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?"></a>Learning Resource Allocation Policy: Vertex-GNN or Edge-GNN?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12480">http://arxiv.org/abs/2307.12480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Peng, Jia Guo, Chenyang Yang</li>
<li>for: 本文研究了基于图神经网络（Graph Neural Networks，GNNs）的无线资源分配策略学习。</li>
<li>methods: 本文分析了顶点神经网络（Vertex-GNNs）和边神经网络（Edge-GNNs）在学习无线策略时的表达能力。</li>
<li>results: 研究发现，顶点神经网络和边神经网络的表达能力取决于处理和组合函数的线性和输出维度。顶点神经网络在使用线性处理器时无法分辨所有通道矩阵，而边神经网络可以。在学习precoding策略时，即使使用非线性处理器，顶点神经网络的表达能力仍然有限。研究还提出了必要的条件，以确保GNNs可以好好地学习precoding策略。实验结果证明了分析结论，并表明了边神经网络可以与顶点神经网络相比，具有更低的训练和推断时间。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) update the hidden representations of vertices (called Vertex-GNNs) or hidden representations of edges (called Edge-GNNs) by processing and pooling the information of neighboring vertices and edges and combining to incorporate graph topology. When learning resource allocation policies, GNNs cannot perform well if their expressive power are weak, i.e., if they cannot differentiate all input features such as channel matrices. In this paper, we analyze the expressive power of the Vertex-GNNs and Edge-GNNs for learning three representative wireless policies: link scheduling, power control, and precoding policies. We find that the expressive power of the GNNs depend on the linearity and output dimensions of the processing and combination functions. When linear processors are used, the Vertex-GNNs cannot differentiate all channel matrices due to the loss of channel information, while the Edge-GNNs can. When learning the precoding policy, even the Vertex-GNNs with non-linear processors may not be with strong expressive ability due to the dimension compression. We proceed to provide necessary conditions for the GNNs to well learn the precoding policy. Simulation results validate the analyses and show that the Edge-GNNs can achieve the same performance as the Vertex-GNNs with much lower training and inference time.
</details>
<details>
<summary>摘要</summary>
图 нейрон网络（GNNs）更新隐藏表示的顶点（称为顶点GNNs）或隐藏表示的边（称为边GNNs），通过处理和汇聚邻近顶点和边的信息，并将其组合以利用图STRUCTURE。在学习资源分配策略时，GNNs如果表达力强不足，例如不能分辨输入特征集如渠道矩阵，则不能表现出好的性能。在这篇论文中，我们分析顶点GNNs和边GNNs在学习三种代表性无线策略：链接调度策略、功率控制策略和排序策略时的表达力。我们发现顶点GNNs和边GNNs的表达力取决于处理和组合函数的线性和输出维度。当使用线性处理器时，顶点GNNs无法分辨所有渠道矩阵，而边GNNs可以。在学习预处理策略时， même avec les processeurs non linéaires, les GNNs peut ne pas avoir une capacité d'expression suffisante en raison de la compression de la dimension. Nous avons fourni les conditions nécessaires pour que les GNNs apprennent efficacement la stratégie de pré-traitement. Les résultats de simulation valident les analyses et montrent que les GNNs peuvent atteindre le même niveau de performance que les GNNs avec beaucoup moins de temps d'entraînement et d'inférence.
</details></li>
</ul>
<hr>
<h2 id="Model-free-generalized-fiducial-inference"><a href="#Model-free-generalized-fiducial-inference" class="headerlink" title="Model-free generalized fiducial inference"></a>Model-free generalized fiducial inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12472">http://arxiv.org/abs/2307.12472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan P Williams</li>
<li>for: 这项研究的目的是为了开发一种安全可靠的机器学习 uncertainty quantification 方法。</li>
<li>methods: 这项研究使用了一种 model-free 统计框架，以实现 imprecise probabilistic prediction inference。这个框架可以提供 finite sample control of type 1 errors，同时也可以提供更多的 versatile tools for imprecise probabilistic reasoning。</li>
<li>results: 这项研究提出了一种新的 preciseness probability approximation，用于 approximate belief&#x2F;plausibility measure pair。这个 aproximation 是一种 optima in some sense 的 probability measure in the credal set，可以解决 imprecise probabilistic approaches to inference 的问题。<details>
<summary>Abstract</summary>
Motivated by the need for the development of safe and reliable methods for uncertainty quantification in machine learning, I propose and develop ideas for a model-free statistical framework for imprecise probabilistic prediction inference. This framework facilitates uncertainty quantification in the form of prediction sets that offer finite sample control of type 1 errors, a property shared with conformal prediction sets, but this new approach also offers more versatile tools for imprecise probabilistic reasoning. Furthermore, I propose and consider the theoretical and empirical properties of a precise probabilistic approximation to the model-free imprecise framework. Approximating a belief/plausibility measure pair by an [optimal in some sense] probability measure in the credal set is a critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in statistical and machine learning communities. It is largely undetermined in the statistical and machine learning literatures, more generally, how to properly quantify uncertainty in that there is no generally accepted standard of accountability of stated uncertainties. The research I present in this manuscript is aimed at motivating a framework for statistical inference with reliability and accountability as the guiding principles.
</details>
<details>
<summary>摘要</summary>
我受到机器学习中无certainty量化的需求而努力提出和开发一个无模型的统计框架，以便实现precise probabilistic prediction inference中的uncertainty量化。这个框架可以提供finite sample控制type 1 error的prediction set，和conformal prediction set相似，但这个新的方法可以提供更多的versatile工具 дляimprecise probabilistic reasoning。此外，我还提出了一个精确的 probabilistic approximation，用于对无模型的imprecise framework进行approximation。在这个框架中，一个belief/plausibility measure pair的抽象是一个optimal的probability measure在credal set中，这是critical resolution needed for the broader adoption of imprecise probabilistic approaches to inference in statistical and machine learning communities。在统计和机器学习文献中，更加一般地，无法properly quantify uncertainty，因为没有一个通行的标准 accountability of stated uncertainties。我在这个著作中的研究是对 statistical inference的一个框架，以实现可靠性和责任性为引导 principl。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Distillation-Do-Not-Overlook-Calibration"><a href="#Rethinking-Data-Distillation-Do-Not-Overlook-Calibration" class="headerlink" title="Rethinking Data Distillation: Do Not Overlook Calibration"></a>Rethinking Data Distillation: Do Not Overlook Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12463">http://arxiv.org/abs/2307.12463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets">https://github.com/dongyaozhu/calibrate-networks-trained-on-distilled-datasets</a></li>
<li>paper_authors: Dongyao Zhu, Bowen Lei, Jie Zhang, Yanbo Fang, Ruqi Zhang, Yiqun Xie, Dongkuan Xu</li>
<li>for: 本研究旨在解决因数据压缩而导致的神经网络输出过于自信的问题，提出了两种新的纠正方法：Masked Temperature Scaling (MTS) 和 Masked Distillation Training (MDT)。</li>
<li>methods: 本研究使用了数据压缩后的神经网络进行训练，并采用了温度扩大和混合方法来纠正神经网络的输出。</li>
<li>results: 研究发现，使用Masked Temperature Scaling (MTS) 和 Masked Distillation Training (MDT) 可以更好地纠正数据压缩后的神经网络输出，同时保持数据压缩的效率。<details>
<summary>Abstract</summary>
Neural networks trained on distilled data often produce over-confident output and require correction by calibration methods. Existing calibration methods such as temperature scaling and mixup work well for networks trained on original large-scale data. However, we find that these methods fail to calibrate networks trained on data distilled from large source datasets. In this paper, we show that distilled data lead to networks that are not calibratable due to (i) a more concentrated distribution of the maximum logits and (ii) the loss of information that is semantically meaningful but unrelated to classification tasks. To address this problem, we propose Masked Temperature Scaling (MTS) and Masked Distillation Training (MDT) which mitigate the limitations of distilled data and achieve better calibration results while maintaining the efficiency of dataset distillation.
</details>
<details>
<summary>摘要</summary>
neural networks 经过精炼数据训练后通常会生成过度自信的输出，需要进行减强方法来修正。现有的减强方法，如温度Scaling 和 mixup，对于基于原始大规模数据的网络进行训练时工作良好。然而，我们发现这些方法无法调整基于大源数据集的数据精炼后的网络。在这篇论文中，我们发现了精炼数据导致的网络无法减强的两个问题：（i）精炼数据集中最大 logits 的更集中分布，以及（ii）semantic意义强度不相关的信息的丢失。为解决这问题，我们提出了Masked Temperature Scaling (MTS) 和 Masked Distillation Training (MDT)，这两种方法可以缓解精炼数据的局限性，实现更好的减强结果，同时保持数据精炼的效率。
</details></li>
</ul>
<hr>
<h2 id="Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks"><a href="#Rates-of-Approximation-by-ReLU-Shallow-Neural-Networks" class="headerlink" title="Rates of Approximation by ReLU Shallow Neural Networks"></a>Rates of Approximation by ReLU Shallow Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12461">http://arxiv.org/abs/2307.12461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Mao, Ding-Xuan Zhou</li>
<li>For: The paper is written to investigate the efficiency of shallow neural networks with one hidden layer in approximating functions from H&quot;older spaces.* Methods: The paper uses ReLU neural networks with $m$ hidden neurons to approximate functions from $W_\infty^r([-1, 1]^d)$ and provides rates of uniform approximation.* Results: The paper shows that ReLU shallow neural networks can uniformly approximate functions from $W_\infty^r([-1, 1]^d)$ with rates $O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4})$ when $r&lt;d&#x2F;2 +2$, which is very close to the optimal rate $O(m^{-\frac{r}{d})$ when the dimension $d$ is large.<details>
<summary>Abstract</summary>
Neural networks activated by the rectified linear unit (ReLU) play a central role in the recent development of deep learning. The topic of approximating functions from H\"older spaces by these networks is crucial for understanding the efficiency of the induced learning algorithms. Although the topic has been well investigated in the setting of deep neural networks with many layers of hidden neurons, it is still open for shallow networks having only one hidden layer. In this paper, we provide rates of uniform approximation by these networks. We show that ReLU shallow neural networks with $m$ hidden neurons can uniformly approximate functions from the H\"older space $W_\infty^r([-1, 1]^d)$ with rates $O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4})$ when $r<d/2 +2$. Such rates are very close to the optimal one $O(m^{-\frac{r}{d})$ in the sense that $\frac{d+2}{d+4}$ is close to $1$, when the dimension $d$ is large.
</details>
<details>
<summary>摘要</summary>
“射预统计学中的神经网络，尤其是使用Rectified Linear Unit（ReLU）启动的神经网络，在深度学习的发展中扮演着中心作用。关于使用这些神经网络来近似Holder空间中函数的问题，是深度学习算法的效率的关键因素。虽然在多层神经网络的情况下已经得到了广泛的研究，但是对于单层神经网络还未有充分的研究。在这篇论文中，我们提供了uniform近似率。我们证明了ReLU单层神经网络可以将-$m$个隐藏神经元uniform近似$W_\infty^r([-1, 1]^d)$中的函数， rates为$O((\log m)^{\frac{1}{2} +d}m^{-\frac{r}{d}\frac{d+2}{d+4})$，当$r<d/2 +2$时。这些率与最佳率$O(m^{-\frac{r}{d})$几乎相等，即$\frac{d+2}{d+4}$与$1$几乎相同，当维度$d$很大时。”
</details></li>
</ul>
<hr>
<h2 id="Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty"><a href="#Information-theoretic-Analysis-of-Test-Data-Sensitivity-in-Uncertainty" class="headerlink" title="Information-theoretic Analysis of Test Data Sensitivity in Uncertainty"></a>Information-theoretic Analysis of Test Data Sensitivity in Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12456">http://arxiv.org/abs/2307.12456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Futoshi Futami, Tomoharu Iwata</li>
<li>for: 这篇论文的目的是对 bayesian 推断中的不确定性进行量化，并分析了这种不确定性的两种类型：aleatoric 不确定性和 epistemic 不确定性。</li>
<li>methods: 该论文使用了 bayesian 推断，并rigorously 分解了 predictive uncertainty 到 two 种不确定性。它们分别表示数据生成过程中的内在随机性和数据不充分导致的多样性。</li>
<li>results: 该论文成功地定义了 uncertainty sensitivity，并extend 了现有的 bayesian meta-learning 分析。它首次显示了任务之间的新的sensitivity。<details>
<summary>Abstract</summary>
Bayesian inference is often utilized for uncertainty quantification tasks. A recent analysis by Xu and Raginsky 2022 rigorously decomposed the predictive uncertainty in Bayesian inference into two uncertainties, called aleatoric and epistemic uncertainties, which represent the inherent randomness in the data-generating process and the variability due to insufficient data, respectively. They analyzed those uncertainties in an information-theoretic way, assuming that the model is well-specified and treating the model's parameters as latent variables. However, the existing information-theoretic analysis of uncertainty cannot explain the widely believed property of uncertainty, known as the sensitivity between the test and training data. It implies that when test data are similar to training data in some sense, the epistemic uncertainty should become small. In this work, we study such uncertainty sensitivity using our novel decomposition method for the predictive uncertainty. Our analysis successfully defines such sensitivity using information-theoretic quantities. Furthermore, we extend the existing analysis of Bayesian meta-learning and show the novel sensitivities among tasks for the first time.
</details>
<details>
<summary>摘要</summary>
某些任务中，泊然推理 often 用于 uncertainty quantification 任务。据 Xu 和 Raginsky （2022）的分析， Bayesian 推理中的 predictive uncertainty 可以分为两种不确定性，即 aleatoric 和 epistemic 不确定性，它们表示数据生成过程中的内在随机性和数据不充分导致的多样性。他们通过信息论方式分析这些不确定性，假设模型是正确的并将模型参数看作隐藏变量。然而，现有的信息论分析不能解释uncertainty 中的一个广泛信奉的性质，即测试数据与训练数据之间的敏感性。这意味着当测试数据与训练数据相似时， epistemic 不确定性应该减少。在这项工作中，我们通过我们的新的分解方法来研究这种敏感性。我们的分析成功地定义了这种敏感性使用信息论量表示。此外，我们将 Bayesian meta-learning 的现有分析扩展到新的任务，并首次研究这些任务之间的新的敏感性。
</details></li>
</ul>
<hr>
<h2 id="DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces"><a href="#DiAMoNDBack-Diffusion-denoising-Autoregressive-Model-for-Non-Deterministic-Backmapping-of-Cα-Protein-Traces" class="headerlink" title="DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces"></a>DiAMoNDBack: Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping of Cα Protein Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12451">http://arxiv.org/abs/2307.12451</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ferg-lab/diamondback">https://github.com/ferg-lab/diamondback</a></li>
<li>paper_authors: Michael S. Jones, Kirill Shmilovich, Andrew L. Ferguson</li>
<li>for: 这个论文的目的是提出一种基于杂化的分子模型，以便在长时间步骤上模拟蛋白质的均质化和折叠过程。</li>
<li>methods: 这个论文使用了一种名为Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping（DiAMoNDBack）的杂化模型，用于从粗粒度模型中恢复到原子级模型。这种模型基于一种杂化扩散过程，通过 conditioned on the Cα trace和当地的蛋白质结构来生成原子级模型。</li>
<li>results: 这个论文的实验结果表明，DiAMoNDBack模型可以在蛋白质结构模拟中实现高水平的重建性，包括正确的键形成、避免侧链冲突以及配置态的多样性。此外，模型还可以在不同的蛋白质结构和 simulate 中进行传输性。<details>
<summary>Abstract</summary>
Coarse-grained molecular models of proteins permit access to length and time scales unattainable by all-atom models and the simulation of processes that occur on long-time scales such as aggregation and folding. The reduced resolution realizes computational accelerations but an atomistic representation can be vital for a complete understanding of mechanistic details. Backmapping is the process of restoring all-atom resolution to coarse-grained molecular models. In this work, we report DiAMoNDBack (Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping) as an autoregressive denoising diffusion probability model to restore all-atom details to coarse-grained protein representations retaining only C{\alpha} coordinates. The autoregressive generation process proceeds from the protein N-terminus to C-terminus in a residue-by-residue fashion conditioned on the C{\alpha} trace and previously backmapped backbone and side chain atoms within the local neighborhood. The local and autoregressive nature of our model makes it transferable between proteins. The stochastic nature of the denoising diffusion process means that the model generates a realistic ensemble of backbone and side chain all-atom configurations consistent with the coarse-grained C{\alpha} trace. We train DiAMoNDBack over 65k+ structures from Protein Data Bank (PDB) and validate it in applications to a hold-out PDB test set, intrinsically-disordered protein structures from the Protein Ensemble Database (PED), molecular dynamics simulations of fast-folding mini-proteins from DE Shaw Research, and coarse-grained simulation data. We achieve state-of-the-art reconstruction performance in terms of correct bond formation, avoidance of side chain clashes, and diversity of the generated side chain configurational states. We make DiAMoNDBack model publicly available as a free and open source Python package.
</details>
<details>
<summary>摘要</summary>
习微观模型可以访问到长度和时间尺度，不可能由所有原子模型 achieve。它允许模拟在长时间尺度上发生的过程，如聚集和折叠。减少分辨率实现计算加速，但原子尺度的表示是完整理解机制的必要条件。回映是将高级别的分辨率复制到低级别的分辨率模型中的过程。在这种工作中，我们报道了Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping（Diffusion-denoising Autoregressive Model for Non-Deterministic Backmapping，简称DiAMoNDBack）作为一种推荐模型，用于在保留Cα坐标的情况下，将高级别的分辨率详细信息还原到低级别的分辨率模型中。这种推荐过程从蛋白质的N端开始，以每个残基为单位，通过Cα轨迹和以前已经回映的背bone和副链原子来驱动。本地和自适应的特点使得模型可以转移到不同的蛋白质上。由于杂化的推荐过程，模型可以生成一个真实的蛋白质详细配置，包括背bone和副链原子的全原子配置，并且与高级别的分辨率详细信息保持一致。我们在65000多个PDB结构数据集上训练了DiAMoNDBack模型，并在一个PDB测试集上验证了它。我们还在Protein Ensemble Database（PED）中的自发布蛋白质结构数据集、DE Shaw Research的分子动力学 simulations和减少分辨率 simulation数据上应用了这种模型。我们达到了当前最佳的重建性表现，包括正确的键形成、避免副链冲突和副链配置状态的多样性。我们将DiAMoNDBack模型作为一种免费和开源的Python包公开发布。
</details></li>
</ul>
<hr>
<h2 id="ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation"><a href="#ProtoFL-Unsupervised-Federated-Learning-via-Prototypical-Distillation" class="headerlink" title="ProtoFL: Unsupervised Federated Learning via Prototypical Distillation"></a>ProtoFL: Unsupervised Federated Learning via Prototypical Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12450">http://arxiv.org/abs/2307.12450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hansol Kim, Youngjun Kwak, Minyoung Jung, Jinho Shin, Youngsung Kim, Changick Kim</li>
<li>for: 提高数据隐私保护和验证系统性能</li>
<li>methods: 提出了基于原型表示缩短的 federated learning（ProtoFL）和基于正则化流的本地一类分类器</li>
<li>results: 在五个广泛使用的标准评估 dataset 上，证明了我们提出的框架在先前Literature中的表现优于其他方法<details>
<summary>Abstract</summary>
Federated learning (FL) is a promising approach for enhancing data privacy preservation, particularly for authentication systems. However, limited round communications, scarce representation, and scalability pose significant challenges to its deployment, hindering its full potential. In this paper, we propose 'ProtoFL', Prototypical Representation Distillation based unsupervised Federated Learning to enhance the representation power of a global model and reduce round communication costs. Additionally, we introduce a local one-class classifier based on normalizing flows to improve performance with limited data. Our study represents the first investigation of using FL to improve one-class classification performance. We conduct extensive experiments on five widely used benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, and Keystroke-Dynamics, to demonstrate the superior performance of our proposed framework over previous methods in the literature.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种有前途的方法，可以增强数据隐私保护，特别是 для 身份验证系统。然而，有限的回合通信，珍贵的表示和扩展性带来了大量的挑战，这些挑战妨碍了其全面发挥。在这篇论文中，我们提议了“ProtoFL”，基于原型表示抽象的 Federated Learning，以提高全球模型的表示力并降低回合通信成本。此外，我们还引入了一种基于正规流的本地一类分类器，以提高有限数据下的性能。这是文献中第一篇使用 Federated Learning 提高一类分类性能的研究。我们在五个广泛使用的 benchmark 上进行了广泛的实验，包括 MNIST、CIFAR-10、CIFAR-100、ImageNet-30 和 Keystroke-Dynamics，以示出我们提posed框架的超过先前方法的优秀性。
</details></li>
</ul>
<hr>
<h2 id="WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms"><a href="#WEPRO-Weight-Prediction-for-Efficient-Optimization-of-Hybrid-Quantum-Classical-Algorithms" class="headerlink" title="WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms"></a>WEPRO: Weight Prediction for Efficient Optimization of Hybrid Quantum-Classical Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12449">http://arxiv.org/abs/2307.12449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satwik Kundu, Debarshi Kundu, Swaroop Ghosh</li>
<li>for: 加速量子 neural network 和量子矩阵问题的训练，提高量子矩阵算法的精度和效率。</li>
<li>methods: 提出了一种新的方法 called WEPRO，利用量子矩阵参数 weights 的常见趋势来加速量子矩阵的训练。并提出了两种优化技术 Naive Prediction 和 Adaptive Prediction。</li>
<li>results: 通过对多个量子 neural network 模型的训练和测试，显示了 WEPRO 可以提高训练速度约 2.25 倍，同时提高精度和预测性能，且具有低存储和计算开销。在量子矩阵问题中，WEPRO 也能够提高训练速度和精度。<details>
<summary>Abstract</summary>
The exponential run time of quantum simulators on classical machines and long queue depths and high costs of real quantum devices present significant challenges in the effective training of Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA). To address these limitations, we propose a new approach, WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance namely, Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately $2.25\times$ compared to standard training methods, while also providing improved accuracy (up to $2.3\%$ higher) and loss (up to $6.1\%$ lower) with low storage and computational overheads. We also evaluate WEPRO's effectiveness in VQE for molecular ground-state energy estimation and in QAOA for graph MaxCut. Our results show that WEPRO leads to speed improvements of up to $3.1\times$ for VQE and $2.91\times$ for QAOA, compared to traditional optimization techniques, while using up to $3.3\times$ less number of shots (i.e., repeated circuit executions) per training iteration.
</details>
<details>
<summary>摘要</summary>
traditional training methods for Variational Quantum Algorithms (VQAs) like Quantum Neural Networks (QNNs), Variational Quantum Eigensolver (VQE), and Quantum Approximate Optimization Algorithm (QAOA) face significant challenges due to the exponential run time on classical machines and the long queue depths and high costs of real quantum devices. To address these limitations, we propose a new approach called WEPRO (Weight Prediction), which accelerates the convergence of VQAs by exploiting regular trends in the parameter weights. We introduce two techniques for optimal prediction performance, namely Naive Prediction (NaP) and Adaptive Prediction (AdaP). Through extensive experimentation and training of multiple QNN models on various datasets, we demonstrate that WEPRO offers a speedup of approximately 2.25 times compared to standard training methods, while also providing improved accuracy (up to 2.3% higher) and loss (up to 6.1% lower) with low storage and computational overheads. We also evaluate WEPRO's effectiveness in VQE for molecular ground-state energy estimation and in QAOA for graph MaxCut. Our results show that WEPRO leads to speed improvements of up to 3.1 times for VQE and 2.91 times for QAOA, compared to traditional optimization techniques, while using up to 3.3 times less number of shots (i.e., repeated circuit executions) per training iteration.
</details></li>
</ul>
<hr>
<h2 id="Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices"><a href="#Multifidelity-Covariance-Estimation-via-Regression-on-the-Manifold-of-Symmetric-Positive-Definite-Matrices" class="headerlink" title="Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices"></a>Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12438">http://arxiv.org/abs/2307.12438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aimee Maurais, Terrence Alsup, Benjamin Peherstorfer, Youssef Marzouk</li>
<li>for: 这篇论文是为了提出一种多信度估计器，用于估计协方差矩阵。</li>
<li>methods: 这篇论文使用了拟合问题的方法，在协方差矩阵的拟合空间上进行估计。</li>
<li>results: 论文的实验结果表明，使用这种多信度估计器可以大幅降低估计误差，相比单信度和其他多信度估计器。此外，这种估计器还保持了正定定义性，使其适用于后续任务，如数据吸收和 метри学学习。<details>
<summary>Abstract</summary>
We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.
</details>
<details>
<summary>摘要</summary>
我们介绍一个多域确度估计器，它是解决在对称正定矩阵构造的应变问题中的解。这个估计器由建构而成，并且在 Mahalanobis 距离下实现了实用的计算。我们显示了我们的数据融合多域确度估计器（MRMF）是一个最大 LIKELIHOOD 估计器，在某些错误模型上的拓扑 tangent space 上。更一般地说，我们的里敦热投影框架包含了现有的多域确度估计器，它们是由控制值构成的。我们通过数据示例显示了我们的估计器可以对单域和其他多域确度估计器的平方误差做出很大减少，达到一个次的减少。此外，保持正定性的保证，使得我们的估计器可以与下游任务，如数据融合和度量学习，进行Compatible。
</details></li>
</ul>
<hr>
<h2 id="A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks"><a href="#A-Generalized-Schwarz-type-Non-overlapping-Domain-Decomposition-Method-using-Physics-constrained-Neural-Networks" class="headerlink" title="A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks"></a>A Generalized Schwarz-type Non-overlapping Domain Decomposition Method using Physics-constrained Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12435">http://arxiv.org/abs/2307.12435</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hipersimlab/pecann">https://github.com/hipersimlab/pecann</a></li>
<li>paper_authors: Shamsulhaq Basir, Inanc Senocak</li>
<li>For: The paper is written for solving forward and inverse problems involving partial differential equations (PDEs) using a meshless Schwarz-type non-overlapping domain decomposition method based on artificial neural networks.* Methods: The paper uses a generalized Robin-type interface condition, where unique Robin parameters are assigned to each subdomain and learned to minimize the mismatch on the Robin interface condition. The method uses an independent neural network model trained to minimize the loss on the governing PDE while strictly enforcing boundary and interface conditions through an augmented Lagrangian formalism.* Results: The paper demonstrates the versatility and performance of the proposed approach through extensive experiments on forward and inverse problems, including one-way and two-way decompositions with crosspoints. The learned Robin parameters adapt to the local behavior of the solution, domain partitioning, and subdomain location relative to the overall domain.<details>
<summary>Abstract</summary>
We present a meshless Schwarz-type non-overlapping domain decomposition method based on artificial neural networks for solving forward and inverse problems involving partial differential equations (PDEs). To ensure the consistency of solutions across neighboring subdomains, we adopt a generalized Robin-type interface condition, assigning unique Robin parameters to each subdomain. These subdomain-specific Robin parameters are learned to minimize the mismatch on the Robin interface condition, facilitating efficient information exchange during training. Our method is applicable to both the Laplace's and Helmholtz equations. It represents local solutions by an independent neural network model which is trained to minimize the loss on the governing PDE while strictly enforcing boundary and interface conditions through an augmented Lagrangian formalism. A key strength of our method lies in its ability to learn a Robin parameter for each subdomain, thereby enhancing information exchange with its neighboring subdomains. We observe that the learned Robin parameters adapt to the local behavior of the solution, domain partitioning and subdomain location relative to the overall domain. Extensive experiments on forward and inverse problems, including one-way and two-way decompositions with crosspoints, demonstrate the versatility and performance of our proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无缝Schwarz类非重叠域分解方法，基于人工神经网络来解决部分� differential 方程（PDEs）中的前向和反向问题。为确保邻居子域解的一致性，我们采用一种通用的Robin类型界面条件，将每个子域分配特定的Robin参数。这些子域特定的Robin参数通过在训练中最小化Robin界面条件的差异，以便有效地交换信息。我们的方法适用于拉普拉斯方程和哈尔曼方程。它使用独立的神经网络模型来表示本地解，并通过一种扩展的拉格朗日 formalism来严格执行边界和界面条件。我们发现，我们的方法可以学习每个子域的Robin参数，从而提高邻居子域之间信息交换的能力。我们在多个实验中证明了我们的提出的方法的多样性和性能。这些实验包括一个方向和二个方向的分解，以及跨点的分解。
</details></li>
</ul>
<hr>
<h2 id="Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection"><a href="#Augmented-Box-Replay-Overcoming-Foreground-Shift-for-Incremental-Object-Detection" class="headerlink" title="Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection"></a>Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12427">http://arxiv.org/abs/2307.12427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuyangSunshine/ABR_IOD">https://github.com/YuyangSunshine/ABR_IOD</a></li>
<li>paper_authors: Liu Yuyang, Cong Yang, Goswami Dipam, Liu Xialei, Joost van de Weijer</li>
<li>for: 这篇论文的目的是解决在增量学习中的忘记问题，特别是在增量物体检测（IOD）领域中，通过重新播放之前任务的图像和当前任务的图像。</li>
<li>methods: 这篇论文提出了一种新的和高效的增量Box Replay（ABR）方法，该方法仅将前一任务的背景图像中的前景物体存储并重新播放，从而解决了背景shift问题。此外，该论文还提出了一种新的注意力捕捉的RoI填充损失，该损失使当前模型在旧模型中捕捉到重要信息。</li>
<li>results: 实验结果表明，ABR方法可以有效地避免忘记前一任务的类别，同时保持当前任务的柔软性。此外，ABR方法还可以减少存储需求，并在 Pascal-VOC 和 COCO 数据集上达到了顶尖性能。<details>
<summary>Abstract</summary>
In incremental learning, replaying stored samples from previous tasks together with current task samples is one of the most efficient approaches to address catastrophic forgetting. However, unlike incremental classification, image replay has not been successfully applied to incremental object detection (IOD). In this paper, we identify the overlooked problem of foreground shift as the main reason for this. Foreground shift only occurs when replaying images of previous tasks and refers to the fact that their background might contain foreground objects of the current task. To overcome this problem, a novel and efficient Augmented Box Replay (ABR) method is developed that only stores and replays foreground objects and thereby circumvents the foreground shift problem. In addition, we propose an innovative Attentive RoI Distillation loss that uses spatial attention from region-of-interest (RoI) features to constrain current model to focus on the most important information from old model. ABR significantly reduces forgetting of previous classes while maintaining high plasticity in current classes. Moreover, it considerably reduces the storage requirements when compared to standard image replay. Comprehensive experiments on Pascal-VOC and COCO datasets support the state-of-the-art performance of our model.
</details>
<details>
<summary>摘要</summary>
增量学习中，重新播放之前任务中的样本和当前任务中的样本是解决忘却折架的最有效方法之一。然而，与增量分类不同，图像重新播放在增量物体检测（IOD）中尚未得到成功。在这篇论文中，我们认为过looked problem of foreground shift是主要的原因。foreground shift只发生在重新播放之前任务的图像时，并且指的是这些背景可能包含当前任务中的前景对象。为解决这个问题，我们开发了一种新的和高效的增强盒子重新播放（ABR）方法，只将前景对象存储和重新播放，因此绕过了前景shift问题。此外，我们提出了一种创新的关注点 RoI 特征整合损失，使当前模型从老模型中提取最重要的信息，并将其用于现有模型的约束。ABR 能够减少之前类型的忘却，同时保持当前类型的高柔性。此外，它也可以significantly reduce the storage requirements when compared to standard image replay。我们在 Pascal-VOC 和 COCO 数据集上进行了全面的实验，并证明了我们的模型的状态-of-the-art表现。
</details></li>
</ul>
<hr>
<h2 id="Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction"><a href="#Practical-Commercial-5G-Standalone-SA-Uplink-Throughput-Prediction" class="headerlink" title="Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction"></a>Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12417">http://arxiv.org/abs/2307.12417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasidis Arunruangsirilert, Jiro Katto</li>
<li>for: 这个论文的目的是预测5G NR网络中用户设备（UE）的未来上行吞吐量，以优化用户体验（QoE）。</li>
<li>methods: 这个论文使用ConvLSTM神经网络预测未来上行吞吐量，基于过去的上行吞吐量和RF参数。网络通过实际的5G SA网络驱动测试数据进行训练，并限制了模型只使用Android API中提供的信息。</li>
<li>results: 这个论文的结果表明，使用ConvLSTM神经网络预测未来上行吞吐量的方法可以达到98.9%的准确率， average RMSE为1.80 Mbps。<details>
<summary>Abstract</summary>
While the 5G New Radio (NR) network promises a huge uplift of the uplink throughput, the improvement can only be seen when the User Equipment (UE) is connected to the high-frequency millimeter wave (mmWave) band. With the rise of uplink-intensive smartphone applications such as the real-time transmission of UHD 4K/8K videos, and Virtual Reality (VR)/Augmented Reality (AR) contents, uplink throughput prediction plays a huge role in maximizing the users' quality of experience (QoE). In this paper, we propose using a ConvLSTM-based neural network to predict the future uplink throughput based on past uplink throughput and RF parameters. The network is trained using the data from real-world drive tests on commercial 5G SA networks while riding commuter trains, which accounted for various frequency bands, handover, and blind spots. To make sure our model can be practically implemented, we then limited our model to only use the information available via Android API, then evaluate our model using the data from both commuter trains and other methods of transportation. The results show that our model reaches an average prediction accuracy of 98.9\% with an average RMSE of 1.80 Mbps across all unseen evaluation scenarios.
</details>
<details>
<summary>摘要</summary>
5G新Radio（NR）网络承诺会带来巨大的上行吞吐量提高，但是这种提高只能在用户设备（UE）与高频毫米波（mmWave）频率带连接时得到。随着上行吞吐量占用应用程序如实时传输UHD 4K/8K视频和虚拟现实（VR）/增强现实（AR）内容的普及，上行吞吐量预测在maximizing用户体验质量（QoE）中扮演着关键的角色。在这篇论文中，我们提议使用ConvLSTM神经网络预测未来上行吞吐量，基于过去上行吞吐量和RF参数。网络通过实际驱动测试数据 collected from commercial 5G SA 网络而验证，该数据包括不同频率带、过渡和隐私。为确保我们的模型能够实际应用，我们然后限制了我们的模型仅使用可以通过 Android API 获得的信息。我们使用了不同交通工具进行评估，并发现我们的模型在所有未seen评估场景中达到了98.9%的预测精度，平均Relative Mean Squared Error（RMSE）为1.80 Mbps。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization"><a href="#A-Machine-Learning-Approach-to-Two-Stage-Adaptive-Robust-Optimization" class="headerlink" title="A Machine Learning Approach to Two-Stage Adaptive Robust Optimization"></a>A Machine Learning Approach to Two-Stage Adaptive Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12409">http://arxiv.org/abs/2307.12409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 解决两阶段线性适应Robust优化问题（ARO）中的 binary 现在变量和多面uncertainty sets问题。</li>
<li>methods: 使用机器学习方法，编码优化的现在决策、最差情况相关的优化决策和等待决策为策略。使用列和约束生成算法提取优化策略，并使用机器学习模型预测高质量策略。</li>
<li>results: 应用方法到facility location、multi-item 存储控制和单位启动问题，可以快速解决ARO问题，高精度。<details>
<summary>Abstract</summary>
We propose an approach based on machine learning to solve two-stage linear adaptive robust optimization (ARO) problems with binary here-and-now variables and polyhedral uncertainty sets. We encode the optimal here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the optimal wait-and-see decisions into what we denote as the strategy. We solve multiple similar ARO instances in advance using the column and constraint generation algorithm and extract the optimal strategies to generate a training set. We train a machine learning model that predicts high-quality strategies for the here-and-now decisions, the worst-case scenarios associated with the optimal here-and-now decisions, and the wait-and-see decisions. We also introduce an algorithm to reduce the number of different target classes the machine learning algorithm needs to be trained on. We apply the proposed approach to the facility location, the multi-item inventory control and the unit commitment problems. Our approach solves ARO problems drastically faster than the state-of-the-art algorithms with high accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach"><a href="#Optimal-Control-of-Multiclass-Fluid-Queueing-Networks-A-Machine-Learning-Approach" class="headerlink" title="Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach"></a>Optimal Control of Multiclass Fluid Queueing Networks: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12405">http://arxiv.org/abs/2307.12405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Bertsimas, Cheol Woo Kim</li>
<li>for: 这篇论文是为了提出一种机器学习方法来控制多类流体队列网络（MFQNET），并提供了明确和深入的控制策略。</li>
<li>methods: 这篇论文使用了优化类型的机器学习方法，即Optimal Classification Trees with hyperplane splits（OCT-H）来学习MFQNET的控制策略。</li>
<li>results: 实验结果表明，使用OCT-H学习的控制策略可以在大规模网络中实现100%的准确率，而在线应用只需几毫秒。<details>
<summary>Abstract</summary>
We propose a machine learning approach to the optimal control of multiclass fluid queueing networks (MFQNETs) that provides explicit and insightful control policies. We prove that a threshold type optimal policy exists for MFQNET control problems, where the threshold curves are hyperplanes passing through the origin. We use Optimal Classification Trees with hyperplane splits (OCT-H) to learn an optimal control policy for MFQNETs. We use numerical solutions of MFQNET control problems as a training set and apply OCT-H to learn explicit control policies. We report experimental results with up to 33 servers and 99 classes that demonstrate that the learned policies achieve 100\% accuracy on the test set. While the offline training of OCT-H can take days in large networks, the online application takes milliseconds.
</details>
<details>
<summary>摘要</summary>
我们提出了一种机器学习方法来优化多类流体队列网络（MFQNET）的控制问题，该方法提供了明确和深入的控制策略。我们证明了多类流体队列网络控制问题中存在一种阈值类型的优化策略，其阈值曲线都是通过起点的 hyperplanes。我们使用Optimal Classification Trees with hyperplane splits（OCT-H）来学习MFQNET的控制策略。我们使用 numerically solved MFQNET control problems作为训练集，并通过OCT-H来学习明确的控制策略。我们发现在33个服务器和99个类型的 эксперименталь结果中，学习的策略可以达到100%的准确率。虽然在大型网络中的离线训练可能需要几天的时间，但在线应用只需毫秒钟。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control"><a href="#Uncertainty-aware-Grounded-Action-Transformation-towards-Sim-to-Real-Transfer-for-Traffic-Signal-Control" class="headerlink" title="Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control"></a>Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12388">http://arxiv.org/abs/2307.12388</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longchao Da, Hao Mei, Romir Sharma, Hua Wei</li>
<li>for: 提高RL在实际道路上的应用性能</li>
<li>methods: 使用 simulations-to-real-world（sim-to-real）转移方法，动态将模拟环境中学习的策略转移到实际环境中，以抑制域的差异</li>
<li>results: 在模拟交通环境中评估了UGAT方法，并显示其在实际环境中显著提高RL策略的性能<details>
<summary>Abstract</summary>
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
</details>
<details>
<summary>摘要</summary>
交通信号控制（TSC）是一项复杂重要的任务，影响了数百万人的日常生活。强化学习（RL）已经在优化交通信号控制方面显示了扎实的成果，但现有RL基于TSC方法主要在模拟环境中训练，它们在真实世界中的性能差距很大。在这篇论文中，我们提出了一种从模拟环境到真实世界（sim-to-real）传输方法，称为UGAT，它可以在模拟环境中学习的策略在真实世界中被转移并且在不同的环境中保持良好的性能。我们对一个模拟交通环境进行了评估，并证明了UGAT方法在真实世界中可以大幅提高RL策略的性能。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning"><a href="#In-Context-Learning-in-Large-Language-Models-Learns-Label-Relationships-but-Is-Not-Conventional-Learning" class="headerlink" title="In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning"></a>In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12375">http://arxiv.org/abs/2307.12375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannik Kossen, Tom Rainforth, Yarin Gal</li>
<li>for: 本研究旨在 investigating large language models (LLMs) 在下游任务中的启发式学习能力，特别是如何在 Context 中提供的示例对 Label 之间的关系对 LLMs 的预测造成影响。</li>
<li>methods: 本研究使用了一种 combine 的方法，包括分析 LLMs 在预训练和 Context 中的行为，以及如何将 Context 中的示例和 Label 相互关联。</li>
<li>results: 研究发现，LLMs 通常会在 Context 中使用示例 Label 的信息，但是预训练和 Context 中的 Label 关系是不同的，并且模型不会对所有 Context 中的信息进行平等考虑。这些结论可以帮助我们更好地理解和调节 LLM 的行为。<details>
<summary>Abstract</summary>
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into understanding and aligning LLM behavior.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在下游任务中的表现经常会有显著改善，当 inclusion 输入-标签关系的例子在上下文中时。然而，目前没有一致的共识，如何解释 LLM 的上下文学习（ICL）能力。例如，希xsd et al.（2021）认为 ICL 类似于一种通用学习算法，而 Min et al.（2022b）则认为 ICL 不会从上下文中学习标签关系。在这篇文章中，我们研究了以下几点：1. 输入-标签例子中的标签如何影响预测结果。2. 在预训练中学习的标签关系如何与输入-标签例子在上下文中相互作用。3. ICL 如何平均处理上下文中的标签信息。我们发现 LLM 通常会在上下文中利用标签信息，但是预训练和上下文中的标签关系被处理不同，而且模型不会对所有上下文中的信息进行平均处理。我们的发现可以帮助理解和调整 LLM 的行为。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting"><a href="#Assessing-Intra-class-Diversity-and-Quality-of-Synthetically-Generated-Images-in-a-Biomedical-and-Non-biomedical-Setting" class="headerlink" title="Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting"></a>Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02505">http://arxiv.org/abs/2308.02505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Muneeb Saad, Mubashir Husain Rehmani, Ruairi O’Reilly</li>
<li>for: 这种研究是为了评估生成 adversarial Networks (GANs) 在生成难以 obtain 的医学影像数据 augmentation 任务中的效果。</li>
<li>methods: 这种研究使用了多Scale Structural Similarity Index Measure 和 Cosine Distance 评估生成图像的内部多样性，以及 Frechet Inception Distance 评估生成图像的质量。</li>
<li>results: 研究发现，在不同的医学影像模式下，生成图像的多样性和质量得分异常大。不同的采样大小也会影响生成图像的质量和多样性。这种研究旨在探讨生成图像的多样性和质量在医学和非医学影像模式之间的差异。<details>
<summary>Abstract</summary>
In biomedical image analysis, data imbalance is common across several imaging modalities. Data augmentation is one of the key solutions in addressing this limitation. Generative Adversarial Networks (GANs) are increasingly being relied upon for data augmentation tasks. Biomedical image features are sensitive to evaluating the efficacy of synthetic images. These features can have a significant impact on metric scores when evaluating synthetic images across different biomedical imaging modalities. Synthetically generated images can be evaluated by comparing the diversity and quality of real images. Multi-scale Structural Similarity Index Measure and Cosine Distance are used to evaluate intra-class diversity, while Frechet Inception Distance is used to evaluate the quality of synthetic images. Assessing these metrics for biomedical and non-biomedical imaging is important to investigate an informed strategy in evaluating the diversity and quality of synthetic images. In this work, an empirical assessment of these metrics is conducted for the Deep Convolutional GAN in a biomedical and non-biomedical setting. The diversity and quality of synthetic images are evaluated using different sample sizes. This research intends to investigate the variance in diversity and quality across biomedical and non-biomedical imaging modalities. Results demonstrate that the metrics scores for diversity and quality vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities.
</details>
<details>
<summary>摘要</summary>
在生物医学影像分析中，数据偏好是广泛存在的问题，而生成对抗网络（GANs）在解决这一问题上逐渐被广泛应用。生物医学影像特征对评估合成图像的效果非常敏感，这些特征可以带来评估合成图像的纪录分数变化。合成图像可以通过与真实图像进行比较来评估其多样性和质量。在不同的生物医学成像modalities中，使用多尺度结构相似性指标和夹角距离来评估内类多样性，而使用彩色征波距离来评估合成图像的质量。为了调查这些指标在生物医学和非生物医学成像modalities中的效果，这里进行了一项实验性的评估。研究表明，在不同的生物医学和非生物医学成像modalities中，多样性和质量指标的分数差异很大。
</details></li>
</ul>
<hr>
<h2 id="Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans"><a href="#Early-Prediction-of-Alzheimers-Disease-Leveraging-Symptom-Occurrences-from-Longitudinal-Electronic-Health-Records-of-US-Military-Veterans" class="headerlink" title="Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans"></a>Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12369">http://arxiv.org/abs/2307.12369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rumeng Li, Xun Wang, Dan Berlowitz, Brian Silver, Wen Hu, Heather Keating, Raelene Goodwin, Weisong Liu, Honghuang Lin, Hong Yu</li>
<li>for: 预测阿尔茨海默病（AD）的早期诊断非常重要，以便于时间性的 intervención和治疗。这项研究使用机器学习方法分析患者的长期电子医疗纪录（EHR），以找出可以预测AD诊断的标志和症状。</li>
<li>methods: 这项研究使用了一种 случа控制设计，使用从2004年到2021年的美国卫生部卫生管理局（VHA）的长期EHR数据进行分析。实验中的患者是由2016年以后根据ICD-10-CM代码诊断出的AD患者，与年龄、性别和临床使用相同的9名控制人进行匹配。研究使用了AD相关关键词的时间序列分析，以预测AD诊断。</li>
<li>results: 研究发现，患者的AD相关关键词的时间序列分析可以预测AD诊断，特别是在诊断附近的时间段。模型的拟合度（ROCAUC）为0.997，准确性很高。研究还发现，年龄、性别和种族&#x2F;民族子组的预测结果几乎一致，只有年龄 younger than 65的 subgroup的预测结果不太准确（ROCAUC 0.746）。这种机器学习模型可以使用EHR数据预测AD诊断，提供一种可靠的、便宜的方式，用于早期诊断大量人口。<details>
<summary>Abstract</summary>
Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and "unseen" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., "concentration", "speaking") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.
</details>
<details>
<summary>摘要</summary>
早期预测阿尔ツ海默病（AD）是非常重要，以便在时间上采取措施和治疗。这项研究目的是使用机器学习方法分析患者的长期电子医疗记录（EHR），以确定患者患阿尔ツ海默病的预测指标。我们采用了一种case-control设计，使用2004年至2021年美国卫生部老年军人医疗管理局（VHA）的长期EHR数据，确定患者是在2016年1月1日后被诊断为阿尔ツ海默病（根据ICD-10-CM代码），并与年龄、性别和临床使用相同的9名控制人群进行匹配。我们使用了一组阿尔ツ海默病相关关键词，并跟踪这些词语在患者的长期EHR记录中的出现情况，采用4种机器学习模型进行预测。我们进行了年龄、性别和种族/民族 subgroup分析，并在一个“隐藏”的VHA站点上验证模型。模型的准确率、均衡和其他相关指标都被报告，用于预测距离ICD-基本诊断的诊断。研究人口包括16,701例患者和39,097名匹配的控制人群。患者的AD相关关键词每年增加速度很快，从约10个增加到超过40个，而控制人群保持在10个，而且在诊断 approached时，AD相关关键词的增加速度加剧。最佳模型在使用至少10年之前的ICD-基本诊断数据时，达到了0.997的报告准确率。模型均衡好（Hosmer-Lemeshow准确度测试值为0.99），并在年龄、性别和种族/民族 subgroup中保持一致，除了年龄小于65岁的患者（ROCAUC为0.746）。机器学习模型使用从EHR记录中提取的AD相关关键词可以预测未来的AD诊断，表明其可能用于通过EHR记录来预测AD风险，提供一种可靠且有效的大规模屏检方式。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/cs.LG_2023_07_24/" data-id="clp8zxr9300otn6885734768i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/24/eess.IV_2023_07_24/" class="article-date">
  <time datetime="2023-07-24T09:00:00.000Z" itemprop="datePublished">2023-07-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/24/eess.IV_2023_07_24/">eess.IV - 2023-07-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding"><a href="#Conditional-Residual-Coding-A-Remedy-for-Bottleneck-Problems-in-Conditional-Inter-Frame-Coding" class="headerlink" title="Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding"></a>Conditional Residual Coding: A Remedy for Bottleneck Problems in Conditional Inter Frame Coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12864">http://arxiv.org/abs/2307.12864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Brand, Jürgen Seiler, André Kaup</li>
<li>for: 这个论文是为了提出一种新的视频编码方法，即基于神经网络的 conditional coding，以提高视频编码的效率。</li>
<li>methods: 这个论文使用了 conditional coding 和 residual coding 两种编码方法进行比较，并提出了一种新的 conditional residual coding 方法，以解决 conditional coding 中的信息瓶颈问题。</li>
<li>results: 论文通过 theoretically 和实际例子的分析，证明 conditional residual coding 可以减少信息瓶颈的影响，同时保持 conditional coding 的理论性能。这种编码方法可以 viewed as “the best from both worlds” 在 residual 和 conditional coding 之间。<details>
<summary>Abstract</summary>
Conditional coding is a new video coding paradigm enabled by neural-network-based compression. It can be shown that conditional coding is in theory better than the traditional residual coding, which is widely used in video compression standards like HEVC or VVC. However, on closer inspection, it becomes clear that conditional coders can suffer from information bottlenecks in the prediction path, i.e., that due to the data processing inequality not all information from the prediction signal can be passed to the reconstructed signal, thereby impairing the coder performance. In this paper we propose the conditional residual coding concept, which we derive from information theoretical properties of the conditional coder. This coder significantly reduces the influence of bottlenecks, while maintaining the theoretical performance of the conditional coder. We provide a theoretical analysis of the coding paradigm and demonstrate the performance of the conditional residual coder in a practical example. We show that conditional residual coders alleviate the disadvantages of conditional coders while being able to maintain their advantages over residual coders. In the spectrum of residual and conditional coding, we can therefore consider them as ``the best from both worlds''.
</details>
<details>
<summary>摘要</summary>
新的条件编码方式是基于神经网络的压缩，可以证明这种条件编码在理论上比传统的差异编码（如HEVC或VVC中的差异编码）更好。然而，在更加仔细的分析下，可以发现条件编码器可能会在预测路径中遇到信息瓶颈，即由数据处理不对称性导致的信息无法传递到重建信号中，从而影响编码器性能。在这篇论文中，我们提出了条件差异编码概念，该概念基于条件编码器的信息学性质。这种编码器可以减少预测路径中的瓶颈影响，同时保持条件编码器的理论性能。我们对这种编码器进行了理论分析，并在实践中示出了其性能。我们发现，条件差异编码器可以消除条件编码器的缺点，同时保持条件编码器比差异编码器更好的优势。因此，在差异和条件编码之间的谱spectrum中，我们可以视之为“最佳的两个世界”。
</details></li>
</ul>
<hr>
<h2 id="Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion"><a href="#Spatiotemporal-Modeling-Encounters-3D-Medical-Image-Analysis-Slice-Shift-UNet-with-Multi-View-Fusion" class="headerlink" title="Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion"></a>Spatiotemporal Modeling Encounters 3D Medical Image Analysis: Slice-Shift UNet with Multi-View Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12853">http://arxiv.org/abs/2307.12853</a></li>
<li>repo_url: None</li>
<li>paper_authors: C. I. Ugwu, S. Casarin, O. Lanz</li>
<li>for: 这paper的目的是提出一种基于2D Convolutional Neural Networks的多模态脐椎像分割模型，以提高计算医学中的图像分析效能。</li>
<li>methods: 这paper使用了一种名为Slice SHift UNet（SSH-UNet）的新模型，它通过在多个视角上进行2D卷积，共同学习多个视角的特征，并通过在层次轴上偏移特征图来重新包含第三维度信息。</li>
<li>results: 该paper在Multi-Modality Abdominal Multi-Organ Segmentation（AMOS）和Multi-Atlas Labeling Beyond the Cranial Vault（BTCV） datasets上进行了实验，并证明了SSH-UNet的效果与现有的模型相当，而且更高效。<details>
<summary>Abstract</summary>
As a fundamental part of computational healthcare, Computer Tomography (CT) and Magnetic Resonance Imaging (MRI) provide volumetric data, making the development of algorithms for 3D image analysis a necessity. Despite being computationally cheap, 2D Convolutional Neural Networks can only extract spatial information. In contrast, 3D CNNs can extract three-dimensional features, but they have higher computational costs and latency, which is a limitation for clinical practice that requires fast and efficient models. Inspired by the field of video action recognition we propose a new 2D-based model dubbed Slice SHift UNet (SSH-UNet) which encodes three-dimensional features at 2D CNN's complexity. More precisely multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. The third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. The effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details>
<details>
<summary>摘要</summary>
computer tomography (CT) 和 магнитная резонансная томография (MRI) 提供了体积数据，因此开发三维图像分析算法是必需的基础部分。 although 2D convolutional neural networks (CNNs) 可以提取空间信息，但它们只能提取二维特征。 相比之下，三维 CNNs 可以提取三维特征，但它们的计算成本和延迟更高，这限制了临床实践中的快速和高效模型。  inspirited  by the field of video action recognition, we propose a new 2D-based model called Slice SHift UNet (SSH-UNet)，它在 2D CNN 的复杂性下编码三维特征。 more precisely, multi-view features are collaboratively learned by performing 2D convolutions along the three orthogonal planes of a volume and imposing a weights-sharing mechanism. the third dimension, which is neglected by the 2D convolution, is reincorporated by shifting a portion of the feature maps along the slices' axis. the effectiveness of our approach is validated in Multi-Modality Abdominal Multi-Organ Segmentation (AMOS) and Multi-Atlas Labeling Beyond the Cranial Vault (BTCV) datasets, showing that SSH-UNet is more efficient while on par in performance with state-of-the-art architectures.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Vertebra-Localization-and-Identification-from-CT-Images"><a href="#Multi-View-Vertebra-Localization-and-Identification-from-CT-Images" class="headerlink" title="Multi-View Vertebra Localization and Identification from CT Images"></a>Multi-View Vertebra Localization and Identification from CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12845">http://arxiv.org/abs/2307.12845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images">https://github.com/shanghaitech-impact/multi-view-vertebra-localization-and-identification-from-ct-images</a></li>
<li>paper_authors: Han Wu, Jiadong Zhang, Yu Fang, Zhentao Liu, Nizhuan Wang, Zhiming Cui, Dinggang Shen</li>
<li>for: 本研究旨在提出一种基于多视图的 vertebra 定位和识别方法，以解决现有方法的大量计算成本和局部信息有限问题。</li>
<li>methods: 该方法将3D问题转化为2D定位和识别任务，并采用多视图对准学习策略来学习全局信息。此外，还提出了一种序列损失来保持vertebrae中的序列结构。</li>
<li>results: 评估结果表明，只使用两个2D网络，该方法可以准确地定位和识别CT图像中的vertebrae，并在比较现有方法的情况下卓越表现。<details>
<summary>Abstract</summary>
Accurately localizing and identifying vertebrae from CT images is crucial for various clinical applications. However, most existing efforts are performed on 3D with cropping patch operation, suffering from the large computation costs and limited global information. In this paper, we propose a multi-view vertebra localization and identification from CT images, converting the 3D problem into a 2D localization and identification task on different views. Without the limitation of the 3D cropped patch, our method can learn the multi-view global information naturally. Moreover, to better capture the anatomical structure information from different view perspectives, a multi-view contrastive learning strategy is developed to pre-train the backbone. Additionally, we further propose a Sequence Loss to maintain the sequential structure embedded along the vertebrae. Evaluation results demonstrate that, with only two 2D networks, our method can localize and identify vertebrae in CT images accurately, and outperforms the state-of-the-art methods consistently. Our code is available at https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images.
</details>
<details>
<summary>摘要</summary>
通过CT图像进行精准地Localizing和识别脊梗是许多临床应用中的关键。然而，大多数现有的尝试都是基于3D的剪辑补丁操作，它们受到大量计算成本和有限的全局信息的限制。在这篇论文中，我们提出了基于多视图的脊梗Localization和识别方法，将3D问题转化为2D的Localization和识别任务。不同于剪辑补丁限制，我们的方法可以自然地学习多视图的全局信息。此外，为了更好地捕捉不同视角的解剖结构信息，我们还提出了一种多视图对比学习策略来预训练脊梗。此外，我们还提出了一种序列损失，以维护链接在脊梗上的序列结构。评估结果表明，只有两个2D网络，我们的方法可以在CT图像中准确地Localizing和识别脊梗，并在状态艺术方法上一致性地表现出优于其他方法。我们的代码可以在https://github.com/ShanghaiTech-IMPACT/Multi-View-Vertebra-Localization-and-Identification-from-CT-Images上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning"><a href="#Deep-Homography-Prediction-for-Endoscopic-Camera-Motion-Imitation-Learning" class="headerlink" title="Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning"></a>Deep Homography Prediction for Endoscopic Camera Motion Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12792">http://arxiv.org/abs/2307.12792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Huber, Sebastien Ourselin, Christos Bergeles, Tom Vercauteren</li>
<li>for: 这个研究探讨了透过从逆向录影中学习自动化 Laparoscopic 镜头运动。</li>
<li>methods: 研究将从 retrospective 录影中学习对象运动空间的增强，运用 homographies 进行对象运动不变对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对照对<details>
<summary>Abstract</summary>
In this work, we investigate laparoscopic camera motion automation through imitation learning from retrospective videos of laparoscopic interventions. A novel method is introduced that learns to augment a surgeon's behavior in image space through object motion invariant image registration via homographies. Contrary to existing approaches, no geometric assumptions are made and no depth information is necessary, enabling immediate translation to a robotic setup. Deviating from the dominant approach in the literature which consist of following a surgical tool, we do not handcraft the objective and no priors are imposed on the surgical scene, allowing the method to discover unbiased policies. In this new research field, significant improvements are demonstrated over two baselines on the Cholec80 and HeiChole datasets, showcasing an improvement of 47% over camera motion continuation. The method is further shown to indeed predict camera motion correctly on the public motion classification labels of the AutoLaparo dataset. All code is made accessible on GitHub.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究了通过imitating Learning自逆 Laparoscopic 摄像头运动的自动化。我们提出了一种新的方法，可以在图像空间通过对象运动不变的图像 регистрациюvia homographies来增强 Surgeon 的行为。与现有方法不同，我们没有做任何几何假设，也没有需要深度信息，因此可以立即翻译到Robotic 设置。与文献中主流的方法不同，我们没有手动定义目标，也没有对手术场景做任何假设，因此方法可以发现无偏的策略。在这个新的研究领域中，我们示出了在Cholec80 和 HeiChole 数据集上显著提高，比对照续摄像头运动的Camera Motion Continuation 提高47%。此外，我们还证明了该方法可以正确预测摄像头运动在AutoLaparo 数据集上的公共运动分类标签上。所有代码都已经公开在 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging"><a href="#Synthetic-white-balancing-for-intra-operative-hyperspectral-imaging" class="headerlink" title="Synthetic white balancing for intra-operative hyperspectral imaging"></a>Synthetic white balancing for intra-operative hyperspectral imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12791">http://arxiv.org/abs/2307.12791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anisha Bahl, Conor C. Horgan, Mirek Janatka, Oscar J. MacCormac, Philip Noonan, Yijing Xie, Jianrong Qiu, Nicola Cavalcanti, Philipp Fürnstahl, Michael Ebner, Mads S. Bergholt, Jonathan Shapey, Tom Vercauteren</li>
<li>For: The paper is written for the purpose of demonstrating the need for in situ white references in hyperspectral imaging for surgical applications, and proposing a novel, sterile, synthetic reference construction algorithm to address this need.* Methods: The paper uses a composite image from a video of a standard sterile ruler to create the synthetic reference, and models the reference as the product of independent spatial and spectral components, with a scalar factor accounting for gain, exposure, and light intensity.* Results: The paper shows that the synthetic references achieve median pixel-by-pixel errors lower than 6.5% and produce similar reconstructions and errors to an ideal reference, and that the algorithm integrated well into surgical workflow with median pixel-by-pixel errors of 4.77%, while maintaining good spectral and color reconstruction.<details>
<summary>Abstract</summary>
Hyperspectral imaging shows promise for surgical applications to non-invasively provide spatially-resolved, spectral information. For calibration purposes, a white reference image of a highly-reflective Lambertian surface should be obtained under the same imaging conditions. Standard white references are not sterilizable, and so are unsuitable for surgical environments. We demonstrate the necessity for in situ white references and address this by proposing a novel, sterile, synthetic reference construction algorithm. The use of references obtained at different distances and lighting conditions to the subject were examined. Spectral and color reconstructions were compared with standard measurements qualitatively and quantitatively, using $\Delta E$ and normalised RMSE respectively. The algorithm forms a composite image from a video of a standard sterile ruler, whose imperfect reflectivity is compensated for. The reference is modelled as the product of independent spatial and spectral components, and a scalar factor accounting for gain, exposure, and light intensity. Evaluation of synthetic references against ideal but non-sterile references is performed using the same metrics alongside pixel-by-pixel errors. Finally, intraoperative integration is assessed though cadaveric experiments. Improper white balancing leads to increases in all quantitative and qualitative errors. Synthetic references achieve median pixel-by-pixel errors lower than 6.5% and produce similar reconstructions and errors to an ideal reference. The algorithm integrated well into surgical workflow, achieving median pixel-by-pixel errors of 4.77%, while maintaining good spectral and color reconstruction.
</details>
<details>
<summary>摘要</summary>
高spectral成像显示在手术应用中具有潜在的优势，能够非侵入式地在空间上提供 spectral信息。为了进行准确的均衡，需要在同一种 imaging 条件下获得一个白色参照图像，但标准的白色参照图像不能sterilizable，因此不适用于手术环境。我们提出了一种新的、sterile、Synthetic参照图像建构算法。我们测试了不同距离和照明条件下的参照图像的使用，并与标准测量进行比较。我们使用了ΔE和normalized RMSE两种指标进行评估。我们的算法使用了一个标准 sterile 的测量仪表，并对其进行了补做。参照图像被视为独立的空间和spectral组分的乘积，以及一个权值补做照明、曝光和光强。我们对synthetic参照图像与理想 pero non-sterile 参照图像进行了比较，并使用了相同的指标进行评估。最后，我们通过实验评估了这种算法在手术过程中的integrability。不当的白平衡会导致所有量化和质量错误的增加。synthetic参照图像的 median 像素误差低于6.5%，并且生成了与理想参照图像类似的重建和错误。我们的算法在手术工作流中融合了良好的 spectral和color重建，并且 median 像素误差为4.77%。
</details></li>
</ul>
<hr>
<h2 id="ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution"><a href="#ICF-SRSR-Invertible-scale-Conditional-Function-for-Self-Supervised-Real-world-Single-Image-Super-Resolution" class="headerlink" title="ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution"></a>ICF-SRSR: Invertible scale-Conditional Function for Self-Supervised Real-world Single Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12751">http://arxiv.org/abs/2307.12751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee</li>
<li>for: 提高单张图像超分辨率（SISR）的性能，不使用任何对应的训练数据。</li>
<li>methods: 提出了一种新的可逆扩率函数（ICF），可以扩大输入图像，然后使用不同的扩率条件恢复原始输入图像。基于该ICF，提出了一种新的无监督SISR框架（ICF-SRSR）。</li>
<li>results: 经验表明，提出的ICF-SRSR方法在实际世界 scenarios中可以很好地处理SISR任务，并且与现有的监督&#x2F;无监督方法在公共 benchmark datasets上展现了相似的性能。<details>
<summary>Abstract</summary>
Single image super-resolution (SISR) is a challenging ill-posed problem that aims to up-sample a given low-resolution (LR) image to a high-resolution (HR) counterpart. Due to the difficulty in obtaining real LR-HR training pairs, recent approaches are trained on simulated LR images degraded by simplified down-sampling operators, e.g., bicubic. Such an approach can be problematic in practice because of the large gap between the synthesized and real-world LR images. To alleviate the issue, we propose a novel Invertible scale-Conditional Function (ICF), which can scale an input image and then restore the original input with different scale conditions. By leveraging the proposed ICF, we construct a novel self-supervised SISR framework (ICF-SRSR) to handle the real-world SR task without using any paired/unpaired training data. Furthermore, our ICF-SRSR can generate realistic and feasible LR-HR pairs, which can make existing supervised SISR networks more robust. Extensive experiments demonstrate the effectiveness of the proposed method in handling SISR in a fully self-supervised manner. Our ICF-SRSR demonstrates superior performance compared to the existing methods trained on synthetic paired images in real-world scenarios and exhibits comparable performance compared to state-of-the-art supervised/unsupervised methods on public benchmark datasets.
</details>
<details>
<summary>摘要</summary>
Single image super-resolution (SISR) 是一个具有挑战性的不定系数问题，旨在将给定的低分辨率 (LR) 图像提升到高分辨率 (HR) 对应的图像。由于实际获得LR-HR训练对的困难，现有的方法通常是通过简化的下采样算法，如比 Example: bicubic，进行训练。这种方法在实践中可能会存在问题，因为生成的Synthesized和实际世界LR图像之间存在很大的差距。为了解决这个问题，我们提出了一种新的减少函数 (ICF)，可以将输入图像缩放，然后使用不同的缩放比例来恢复原始输入。通过利用我们提出的ICF，我们建立了一种新的自动编码SR框架 (ICF-SRSR)，可以在不使用任何paired/unpaired训练数据的情况下进行SR任务。此外，我们的ICF-SRSR可以生成可靠和可行的LR-HR对，这可以使现有的supervised SR网络更加可靠。我们的实验表明，我们的ICF-SRSR可以在不使用任何训练数据的情况下处理SR任务，并且在实际世界 scenario 中表现出色。我们的ICF-SRSR在与现有的方法进行比较时，在公共的benchmark datasets上表现出了相当的性能。
</details></li>
</ul>
<hr>
<h2 id="Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction"><a href="#Dense-Transformer-based-Enhanced-Coding-Network-for-Unsupervised-Metal-Artifact-Reduction" class="headerlink" title="Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction"></a>Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12717">http://arxiv.org/abs/2307.12717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wangduo Xie, Matthew B. Blaschko</li>
<li>for: 针对CT图像损坏的金属artifacts，提高临床诊断的精度。</li>
<li>methods: 提出了一种基于Dense Transformer的增强编码网络（DTEC-Net），利用高阶杂分解编码器和转换器来获得长距离匹配的紧密编码序列。然后，提出了第二阶杂分解方法来改进密集序列的解码过程。</li>
<li>results: 对一个标准测试集进行了广泛的实验和模型说明，证明DTEC-Net的有效性，其在降低金属artifacts的同时保留了更多的细节Texture。与之前的状态统计方法相比，DTEC-Net显著提高了图像质量。<details>
<summary>Abstract</summary>
CT images corrupted by metal artifacts have serious negative effects on clinical diagnosis. Considering the difficulty of collecting paired data with ground truth in clinical settings, unsupervised methods for metal artifact reduction are of high interest. However, it is difficult for previous unsupervised methods to retain structural information from CT images while handling the non-local characteristics of metal artifacts. To address these challenges, we proposed a novel Dense Transformer based Enhanced Coding Network (DTEC-Net) for unsupervised metal artifact reduction. Specifically, we introduce a Hierarchical Disentangling Encoder, supported by the high-order dense process, and transformer to obtain densely encoded sequences with long-range correspondence. Then, we present a second-order disentanglement method to improve the dense sequence's decoding process. Extensive experiments and model discussions illustrate DTEC-Net's effectiveness, which outperforms the previous state-of-the-art methods on a benchmark dataset, and greatly reduces metal artifacts while restoring richer texture details.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Low-complexity-Overfitted-Neural-Image-Codec"><a href="#Low-complexity-Overfitted-Neural-Image-Codec" class="headerlink" title="Low-complexity Overfitted Neural Image Codec"></a>Low-complexity Overfitted Neural Image Codec</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12706">http://arxiv.org/abs/2307.12706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Orange-OpenSource/Cool-Chic">https://github.com/Orange-OpenSource/Cool-Chic</a></li>
<li>paper_authors: Thomas Leguay, Théo Ladune, Pierrick Philippe, Gordon Clare, Félix Henry</li>
<li>for: 这个论文是为了提出一种具有减少复杂度的神经网络图像编码器，该编码器可以对输入图像进行适应参数过滤。</li>
<li>methods: 该论文使用了自适应神经网络，并通过优化训练过程和使用轻量级模块来降低编码器的复杂度。</li>
<li>results: 该论文的方法可以与 autoencoder 和 HEVC 比肩，并且在不同的编码条件下具有14%的rate reduction，同时保持相似的复杂度。<details>
<summary>Abstract</summary>
We propose a neural image codec at reduced complexity which overfits the decoder parameters to each input image. While autoencoders perform up to a million multiplications per decoded pixel, the proposed approach only requires 2300 multiplications per pixel. Albeit low-complexity, the method rivals autoencoder performance and surpasses HEVC performance under various coding conditions. Additional lightweight modules and an improved training process provide a 14% rate reduction with respect to previous overfitted codecs, while offering a similar complexity. This work is made open-source at https://orange-opensource.github.io/Cool-Chic/
</details>
<details>
<summary>摘要</summary>
我们提出了一种减少复杂性的神经图像编码器，其将解码器参数过拟合到输入图像。而自动编码器可能需要每个解码ixel进行数百万次乘法运算，而我们的方法只需要每个解码ixel进行2300次乘法运算。虽然具有较低的复杂性，我们的方法与自动编码器的性能相当，甚至超过HEVC的编码性能在不同的编码条件下。此外，我们还提供了一些轻量级模块和改进的训练过程，可以对前一代过拟合编码器进行14%的比较率减少，同时保持相似的复杂性。该工作将在https://orange-opensource.github.io/Cool-Chic/上开源。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems"><a href="#Bayesian-Based-Unrolling-for-Reconstruction-and-Super-resolution-of-Single-Photon-Lidar-Systems" class="headerlink" title="Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems"></a>Bayesian Based Unrolling for Reconstruction and Super-resolution of Single-Photon Lidar Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12700">http://arxiv.org/abs/2307.12700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abderrahim Halimi, Jakeoung Koo, Stephen McLaughlin</li>
<li>for: 这篇论文主要用于描述一种基于深度学习的3D单光子探测器的重建和超分辨率方法。</li>
<li>methods: 该方法基于一种卷积 Bayesian 模型，可以在高噪音环境下提供最佳估计，同时具有改进的网络解释性。</li>
<li>results: 与现有的学习基于方法相比，该算法具有减少可训练参数数量、更高的噪音耐受度和系统响应函数模型化不足的问题，同时提供了更多的估计信息，包括不确定性度量。  Synthetic and real data 比较表明，该算法可以与现有算法相比，提供类似的推理质量和计算复杂度。<details>
<summary>Abstract</summary>
Deploying 3D single-photon Lidar imaging in real world applications faces several challenges due to imaging in high noise environments and with sensors having limited resolution. This paper presents a deep learning algorithm based on unrolling a Bayesian model for the reconstruction and super-resolution of 3D single-photon Lidar. The resulting algorithm benefits from the advantages of both statistical and learning based frameworks, providing best estimates with improved network interpretability. Compared to existing learning-based solutions, the proposed architecture requires a reduced number of trainable parameters, is more robust to noise and mismodelling of the system impulse response function, and provides richer information about the estimates including uncertainty measures. Results on synthetic and real data show competitive results regarding the quality of the inference and computational complexity when compared to state-of-the-art algorithms. This short paper is based on contributions published in [1] and [2].
</details>
<details>
<summary>摘要</summary>
<<SYS>>将3D单 фотоン探测技术应用于实际场景中存在多种挑战，包括高噪声环境和探测器有限分辨率。这篇论文提出了基于深度学习的bayesian模型的推算和超Resolution算法，以解决3D单 фотоン探测中的重要问题。该算法利用了统计和学习两个框架的优点，提供了最佳估计值，同时具有改进的网络解释性。与现有的学习型解决方案相比，提出的架构具有较少的可训练参数量、更高的噪声和系统响应函数模型化不正确率，并提供了更多的估计值和不确定度测量。对于synthetic和实际数据进行了比较，结果显示了与当前状态艺术算法相当的质量和计算复杂度。这篇短文基于[1]和[2]的贡献。
</details></li>
</ul>
<hr>
<h2 id="Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation"><a href="#Automatic-lobe-segmentation-using-attentive-cross-entropy-and-end-to-end-fissure-generation" class="headerlink" title="Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation"></a>Automatic lobe segmentation using attentive cross entropy and end-to-end fissure generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12634">http://arxiv.org/abs/2307.12634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/htytewx/softcam">https://github.com/htytewx/softcam</a></li>
<li>paper_authors: Qi Su, Na Wang, Jiawen Xie, Yinan Chen, Xiaofan Zhang</li>
<li>For:  automatic lung lobe segmentation algorithm for the diagnosis and treatment of lung diseases* Methods:  task-specific loss function to pay attention to the area around the pulmonary fissure, end-to-end pulmonary fissure generation method, registration-based loss function to alleviate convergence difficulty* Results:  achieved 97.83% and 94.75% dice scores on private dataset STLB and public LUNA16 dataset respectively.<details>
<summary>Abstract</summary>
The automatic lung lobe segmentation algorithm is of great significance for the diagnosis and treatment of lung diseases, however, which has great challenges due to the incompleteness of pulmonary fissures in lung CT images and the large variability of pathological features. Therefore, we propose a new automatic lung lobe segmentation framework, in which we urge the model to pay attention to the area around the pulmonary fissure during the training process, which is realized by a task-specific loss function. In addition, we introduce an end-to-end pulmonary fissure generation method in the auxiliary pulmonary fissure segmentation task, without any additional network branch. Finally, we propose a registration-based loss function to alleviate the convergence difficulty of the Dice loss supervised pulmonary fissure segmentation task. We achieve 97.83% and 94.75% dice scores on our private dataset STLB and public LUNA16 dataset respectively.
</details>
<details>
<summary>摘要</summary>
自动肺lobSeg算法对肺病诊断和治疗具有很大的重要性，但是受到肺CT图像的杏仁缺失和疾病特征的大量变化所带来的挑战。因此，我们提出了一种新的自动肺lobSeg框架，其中我们要求模型在训练过程中对杏仁附近区域进行注意力。我们实现了这一点通过任务特定的损失函数。此外，我们还提出了一种不含额外网络分支的杏仁生成方法，以及一种基于准确Registration的损失函数，以解决约瑟分解损失supervised杏仁分 segmentation任务的困难。在我们的私有数据集STLB和公共数据集LUNA16上，我们实现了97.83%和94.75%的 dice分数。
</details></li>
</ul>
<hr>
<h2 id="Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI"><a href="#Sparse-annotation-strategies-for-segmentation-of-short-axis-cardiac-MRI" class="headerlink" title="Sparse annotation strategies for segmentation of short axis cardiac MRI"></a>Sparse annotation strategies for segmentation of short axis cardiac MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12619">http://arxiv.org/abs/2307.12619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Stein, Maxime Di Folco, Julia Schnabel</li>
<li>for: 本研究旨在探讨使用少量标注数据进行心脏MRI分割的方法，以优化标注成本和提高分割性能。</li>
<li>methods: 我们采用了减少数据量和标注数量的方法，包括减少数据量和标注数量，以及使用转移学习和数据增强技术。</li>
<li>results: 我们的实验结果表明，训练使用少量标注数据可以达到0.85的Dice分数和与全数据集相当的性能。此外，我们发现，在中部层的标注更加有价值，而胸部区域的标注最差。在评估量据集对比中，更多的层标注比更多的量据集具有更高的分割性能。因此，建议在标注时尽量标注中部层，而不是标注更多的量据集。<details>
<summary>Abstract</summary>
Short axis cardiac MRI segmentation is a well-researched topic, with excellent results achieved by state-of-the-art models in a supervised setting. However, annotating MRI volumes is time-consuming and expensive. Many different approaches (e.g. transfer learning, data augmentation, few-shot learning, etc.) have emerged in an effort to use fewer annotated data and still achieve similar performance as a fully supervised model. Nevertheless, to the best of our knowledge, none of these works focus on which slices of MRI volumes are most important to annotate for yielding the best segmentation results. In this paper, we investigate the effects of training with sparse volumes, i.e. reducing the number of cases annotated, and sparse annotations, i.e. reducing the number of slices annotated per case. We evaluate the segmentation performance using the state-of-the-art nnU-Net model on two public datasets to identify which slices are the most important to annotate. We have shown that training on a significantly reduced dataset (48 annotated volumes) can give a Dice score greater than 0.85 and results comparable to using the full dataset (160 and 240 volumes for each dataset respectively). In general, training on more slice annotations provides more valuable information compared to training on more volumes. Further, annotating slices from the middle of volumes yields the most beneficial results in terms of segmentation performance, and the apical region the worst. When evaluating the trade-off between annotating volumes against slices, annotating as many slices as possible instead of annotating more volumes is a better strategy.
</details>
<details>
<summary>摘要</summary>
短轴心臓MRI分割是一个广泛研究的话题，现有一些最新的模型在指导下达到了出色的结果。然而，对MRIVolume进行标注是时间consuming和expensive。许多不同的方法（如转移学习、数据扩展、少数学习等）在尝试使用 fewer annotated data 并且达到类似于全指导模型的性能。然而，据我们所知，这些工作没有关注于哪些MRI Volume slice是最重要的标注，以达到最佳分割结果。在这篇文章中，我们 investigate了在减少 annotated volume 和 sparse annotations 下的训练效果。我们使用了state-of-the-art nnU-Net模型对两个公共数据集进行评估，以确定哪些slice是最重要的标注。我们发现，通过减少数据集至48个标注Volume可以达到Dice分数大于0.85，并且与使用全数据集（160和240个Volume）的结果相当。总的来说，训练更多的slice标注比训练更多的Volume更有价值的信息。此外，从MRI Volume 中间部分标注slice最有利于分割性能，而apical区域最差。当评估 annotating Volume 和 slice 之间的负担比，更好的策略是annotating as many slices as possible 而不是 annotating more Volume。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains"><a href="#Attribute-Regularized-Soft-Introspective-VAE-Towards-Cardiac-Attribute-Regularization-Through-MRI-Domains" class="headerlink" title="Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains"></a>Attribute Regularized Soft Introspective VAE: Towards Cardiac Attribute Regularization Through MRI Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12618">http://arxiv.org/abs/2307.12618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxime Di Folco, Cosmin Bercea, Julia A. Schnabel</li>
<li>for: 本研究旨在提高深度生成模型的控制性，通过选择性地修改数据特征进行数据生成和修饰。</li>
<li>methods: 本研究使用了Variational Autoencoders (VAEs)，并通过添加对偏好损失的限制来提高模型的控制性。</li>
<li>results: 实验表明，提出的Attributed Soft Introspective VAE（Attri-SIVAE）方法可以在不同的MRI数据集上达到同等的重建和规范化性，而且在不同的数据集上也可以保持同等的规范化水平，不同于相比方法。<details>
<summary>Abstract</summary>
Deep generative models have emerged as influential instruments for data generation and manipulation. Enhancing the controllability of these models by selectively modifying data attributes has been a recent focus. Variational Autoencoders (VAEs) have shown promise in capturing hidden attributes but often produce blurry reconstructions. Controlling these attributes through different imaging domains is difficult in medical imaging. Recently, Soft Introspective VAE leverage the benefits of both VAEs and Generative Adversarial Networks (GANs), which have demonstrated impressive image synthesis capabilities, by incorporating an adversarial loss into VAE training. In this work, we propose the Attributed Soft Introspective VAE (Attri-SIVAE) by incorporating an attribute regularized loss, into the Soft-Intro VAE framework. We evaluate experimentally the proposed method on cardiac MRI data from different domains, such as various scanner vendors and acquisition centers. The proposed method achieves similar performance in terms of reconstruction and regularization compared to the state-of-the-art Attributed regularized VAE but additionally also succeeds in keeping the same regularization level when tested on a different dataset, unlike the compared method.
</details>
<details>
<summary>摘要</summary>
深度生成模型已经成为数据生成和修饰的重要工具。提高这些模型的可控性，通过选择性地修改数据属性，是最近的研究焦点。变量自动编码器（VAEs）可以捕捉隐藏属性，但经常生成模糊的重建。在医学成像中，控制这些属性通过不同的成像频谱是困难的。最近，软 introspective VAE 利用了 VAEs 和生成对抗网络（GANs）的优点，通过在 VAE 训练中添加对抗损失来提高图像生成能力。在这项工作中，我们提出了具有属性规则化损失的 Attributed Soft Introspective VAE（Attri-SIVAE）。我们通过实验评估该方法在不同的cardiac MRI数据集上的性能。该方法与状态uset-of-the-art Attributed regularized VAE 相似的重建和规则化性能，并且在不同的数据集上保持了同等的规则化水平，不同于相比方法。
</details></li>
</ul>
<hr>
<h2 id="AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection"><a href="#AMaizeD-An-End-to-End-Pipeline-for-Automatic-Maize-Disease-Detection" class="headerlink" title="AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection"></a>AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03766">http://arxiv.org/abs/2308.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anish Mall, Sanchit Kabra, Ankur Lhila, Pawan Ajmera</li>
<li>for: 这个研究论文旨在提供一个自动化的豇豉病诊断框架，用于早期检测豇豉作物中的病诊断。</li>
<li>methods: 该框架使用多spectral图像，结合了深度学习网络来提取特征和分割方法，以识别豇豉作物和其相关的病诊断。</li>
<li>results: 实验结果表明，该框架可以有效地检测豇豉作物中的多种病诊断，包括粉刺虫、芽虫和叶褪病等。<details>
<summary>Abstract</summary>
This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize growth and disease progression. By leveraging multispectral imagery, the framework benefits from improved spectral resolution and increased sensitivity to subtle changes in plant health. The proposed framework employs a combination of convolutional neural networks (CNNs) as feature extractors and segmentation techniques to identify both the maize plants and their associated diseases. Experimental results demonstrate the effectiveness of the framework in detecting a range of maize diseases, including powdery mildew, anthracnose, and leaf blight. The framework achieves state-of-the-art performance on the custom hand-collected dataset and contributes to the field of automated disease detection in agriculture, offering a practical solution for early identification of diseases in maize crops advanced machine learning techniques and deep learning architectures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal"><a href="#Development-Of-Automated-Cardiac-Arrhythmia-Detection-Methods-Using-Single-Channel-ECG-Signal" class="headerlink" title="Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal"></a>Development Of Automated Cardiac Arrhythmia Detection Methods Using Single Channel ECG Signal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02405">http://arxiv.org/abs/2308.02405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arpita Paul, Avik Kumar Das, Manas Rakshit, Ankita Ray Chowdhury, Susmita Saha, Hrishin Roy, Sajal Sarkar, Dongiri Prasanth, Eravelli Saicharan<br>for:多种心脏病的自动检测和分类可能会减少心脏疾病的死亡率。本研究提出了基于单通道电cardiogram（ECG）信号的多类刺激识别算法。methods:在本研究中，使用心脏自变性（HRV）、形态特征和wavelet幂特征，通过机器学习基于Random Forest分类器进行检测。results:使用HRV和时域形态特征时，获得了85.11%的准确率、85.11%的敏感度、85.07%的精度和85.00%的F1分数。使用HRV和wavelet幂特征时，性能提高到90.91%的准确率、90.91%的敏感度、90.96%的精度和90.87%的F1分数。实验结果表明，提出的方案可以有效地从单通道ECG记录中检测多种刺激。<details>
<summary>Abstract</summary>
Arrhythmia, an abnormal cardiac rhythm, is one of the most common types of cardiac disease. Automatic detection and classification of arrhythmia can be significant in reducing deaths due to cardiac diseases. This work proposes a multi-class arrhythmia detection algorithm using single channel electrocardiogram (ECG) signal. In this work, heart rate variability (HRV) along with morphological features and wavelet coefficient features are utilized for detection of 9 classes of arrhythmia. Statistical, entropy and energy-based features are extracted and applied to machine learning based random forest classifiers. Data used in both works is taken from 4 broad databases (CPSC and CPSC extra, PTB-XL, G12EC and Chapman-Shaoxing and Ningbo Database) made available by Physionet. With HRV and time domain morphological features, an average accuracy of 85.11%, sensitivity of 85.11%, precision of 85.07% and F1 score of 85.00% is obtained whereas with HRV and wavelet coefficient features, the performance obtained is 90.91% accuracy, 90.91% sensitivity, 90.96% precision and 90.87% F1 score. The detailed analysis of simulation results affirms that the presented scheme effectively detects broad categories of arrhythmia from single-channel ECG records. In the last part of the work, the proposed classification schemes are implemented on hardware using Raspberry Pi for real time ECG signal classification.
</details>
<details>
<summary>摘要</summary>
心动过速病（Arrhythmia）是心血管疾病中最常见的一种。自动检测和识别Arrhythmia可以有效降低心血管疾病的死亡率。这项工作提出了基于单通道电cardiogram（ECG）信号的多类Arrhythmia检测算法。在这项工作中，利用心跳变化（HRV）以及形态特征和wavelet幅特征来检测9种类型的Arrhythmia。通过提取统计、熵和能量基本特征，并应用机器学习基于Random Forest分类器，实现了高精度的Arrhythmia检测。数据来源于Physionet提供的4个广泛数据库（CPSC和CPSC extra、PTB-XL、G12EC和Chapman-Shaoxing和Ningbo数据库）。使用HRV和时域形态特征时，取得了85.11%的准确率、85.11%的敏感度、85.07%的精度和85.00%的F1分数，而使用HRV和wavelet幅特征时，取得了90.91%的准确率、90.91%的敏感度、90.96%的精度和90.87%的F1分数。etailed分析结果表明，提出的方案可以有效地从单通道ECG记录中检测广泛的Arrhythmia类型。最后，提出的分类方案在硬件上使用Raspberry Pi实现了实时ECG信号分类。
</details></li>
</ul>
<hr>
<h2 id="4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network"><a href="#4D-Feet-Registering-Walking-Foot-Shapes-Using-Attention-Enhanced-Dynamic-Synchronized-Graph-Convolutional-LSTM-Network" class="headerlink" title="4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network"></a>4D Feet: Registering Walking Foot Shapes Using Attention Enhanced Dynamic-Synchronized Graph Convolutional LSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12377">http://arxiv.org/abs/2307.12377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzam Tajdari, Toon Huysmans, Xinhe Yao, Jun Xu, Yu Song</li>
<li>for: 该论文旨在帮助研究人员更好地理解动态弹性人体部件的特征，通过基于多个异步摄像机捕获的4D扫描数据进行重建。</li>
<li>methods: 该论文提出了一种通用框架，包括：1）使用非RIGID迭代最近最远点对精度找到和对准不同摄像机捕获的3D扫描数据中的动态特征；2）使用一种新型的ADGC-LSTM网络将不同摄像机捕获的3D扫描数据同步到特定摄像机的时间轴上；3）使用非RIGID注准方法将同步化的3D扫描数据注准到高质量模板中。</li>
<li>results: 该论文采用了一种新开发的4D脚部扫描仪，并将数据集分为58名参与者的15帧&#x2F;秒4D形态数据集（共116个脚部，包括5147帧的3D扫描数据），覆盖了脚步征的重要阶段。结果表明提出的方法有效地同步异步的4D扫描数据，特别是通过使用提出的ADGC-LSTM网络进行同步。<details>
<summary>Abstract</summary>
4D scans of dynamic deformable human body parts help researchers have a better understanding of spatiotemporal features. However, reconstructing 4D scans based on multiple asynchronous cameras encounters two main challenges: 1) finding the dynamic correspondences among different frames captured by each camera at the timestamps of the camera in terms of dynamic feature recognition, and 2) reconstructing 3D shapes from the combined point clouds captured by different cameras at asynchronous timestamps in terms of multi-view fusion. In this paper, we introduce a generic framework that is able to 1) find and align dynamic features in the 3D scans captured by each camera using the nonrigid iterative closest-farthest points algorithm; 2) synchronize scans captured by asynchronous cameras through a novel ADGC-LSTM-based network, which is capable of aligning 3D scans captured by different cameras to the timeline of a specific camera; and 3) register a high-quality template to synchronized scans at each timestamp to form a high-quality 3D mesh model using a non-rigid registration method. With a newly developed 4D foot scanner, we validate the framework and create the first open-access data-set, namely the 4D feet. It includes 4D shapes (15 fps) of the right and left feet of 58 participants (116 feet in total, including 5147 3D frames), covering significant phases of the gait cycle. The results demonstrate the effectiveness of the proposed framework, especially in synchronizing asynchronous 4D scans using the proposed ADGC-LSTM network.
</details>
<details>
<summary>摘要</summary>
4D扫描技术为研究人体动态变形带来了更好的认知，但是通过多个异步相机重建4D扫描存在两大挑战：1）在不同相机拍摄时间点找到动态匹配，并通过动态特征识别将它们相互对应；2）将不同相机拍摄的点云数据 fusion 到一起，以便形成高质量的3D模型。在这篇论文中，我们提出了一种通用的框架，可以1）使用非RIGID迭代最近最远点算法来在不同相机拍摄的3D扫描中找到和对应动态特征；2）使用一种新型的ADGC-LSTM网络将不同相机拍摄的3D扫描同步到同一个时间轴上；3）使用非RIGID注册方法将同步化后的3D扫描与高质量模板进行对应，以形成高质量的3D mesh模型。我们使用一种新开发的4D脚部扫描仪来验证该框架，并创建了首个公共数据集，即4D脚部（15帧/秒），包括58名参与者的右和左脚的4D形状（共116个脚，包括5147帧），覆盖了走势过程中重要的阶段。结果表明提出的框架具有良好的效果，特别是在同步异步4D扫描中使用提出的ADGC-LSTM网络。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/24/eess.IV_2023_07_24/" data-id="clp8zxrfy017rn688h35p764n" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.SD_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T15:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.SD_2023_07_23/">cs.SD - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition"><a href="#A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition" class="headerlink" title="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition"></a>A meta learning scheme for fast accent domain expansion in Mandarin speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12262">http://arxiv.org/abs/2307.12262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang, Jian Yu</li>
<li>for: 这 paper 是为了解决中文识别技术中的方言域扩展问题，提高方言识别精度。</li>
<li>methods: 这 paper 使用了元学习技术，包括模型冻结和多元学习，实现快速方言域扩展。</li>
<li>results: 该方法在方言域扩展任务上达到了3%的相对提升，相比基线模型，在同样的测试集上提高了37%。此外，该方法也在大量数据上实现了4%的相对提升。<details>
<summary>Abstract</summary>
Spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details>
<details>
<summary>摘要</summary>
spoken languages show significant variation across mandarin and accent. despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. in this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. so we select meta-learning in the domain expansion task. this more essential learning will cause improved performance on accent domain extension tasks. we combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. in addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details></li>
</ul>
<hr>
<h2 id="MyVoice-Arabic-Speech-Resource-Collaboration-Platform"><a href="#MyVoice-Arabic-Speech-Resource-Collaboration-Platform" class="headerlink" title="MyVoice: Arabic Speech Resource Collaboration Platform"></a>MyVoice: Arabic Speech Resource Collaboration Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02503">http://arxiv.org/abs/2308.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</li>
<li>for: 增强阿拉伯语言技术的发展，收集和整理阿拉伯语言的语音数据。</li>
<li>methods: 使用互联网平台，征集大量的阿拉伯语言口音录音，并提供城市&#x2F;国家精细的口音选择功能。用户可以 switching roles，从记录者变成评估者，并提供反馈。平台还包括质量检查系统，过滤掉低质量和假 recording。</li>
<li>results: 实现了收集大量的阿拉伯语言口音数据，提供城市&#x2F;国家精细的口音选择功能，并且可以进行多方合作，汇集多种阿拉伯语言数据。<details>
<summary>Abstract</summary>
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
</details>
<details>
<summary>摘要</summary>
我团队介绍MyVoice，一个招待人寄语的平台，用于提高阿拉伯语言口音技术。该平台提供了大量地方口音数据的设计机会，并将其公共地发布。MyVoice让参与者可以选择城市/国家精细口音，并录制显示的语音。用户可以在角色之间切换，包括参与者和注释者。平台包含一个质量保证系统，过滤掉低质量和假语音记录，然后将其发送给验证。在验证阶段，参与者可以评估语音质量，注释和提供反馈，这些反馈会被管理员审核。此外，平台允许管理员添加新的数据或任务，以外语言和词汇收集，这些任务将被显示给参与者。因此，MyVoice平台可以促进多方合作，收集多样化和大量的阿拉伯语言数据。
</details></li>
</ul>
<hr>
<h2 id="Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase"><a href="#Signal-Reconstruction-from-Mel-spectrogram-Based-on-Bi-level-Consistency-of-Full-band-Magnitude-and-Phase" class="headerlink" title="Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase"></a>Signal Reconstruction from Mel-spectrogram Based on Bi-level Consistency of Full-band Magnitude and Phase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12232">http://arxiv.org/abs/2307.12232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram">https://github.com/YoshikiMas/signal-reconstruction-from-mel-spectrogram</a></li>
<li>paper_authors: Yoshiki Masuyama, Natsuki Ueno, Nobutaka Ono</li>
<li>for: 重建时域信号从低维спектроgram中</li>
<li>methods: 利用丰富的听取关系和时域信号之间的双层关系，并使用优化问题的形式来重建全带幅强度和相位信息</li>
<li>results: 对话、音乐和环境信号都有较好的重建效果<details>
<summary>Abstract</summary>
We propose an optimization-based method for reconstructing a time-domain signal from a low-dimensional spectral representation such as a mel-spectrogram. Phase reconstruction has been studied to reconstruct a time-domain signal from the full-band short-time Fourier transform (STFT) magnitude. The Griffin-Lim algorithm (GLA) has been widely used because it relies only on the redundancy of STFT and is applicable to various audio signals. In this paper, we jointly reconstruct the full-band magnitude and phase by considering the bi-level relationships among the time-domain signal, its STFT coefficients, and its mel-spectrogram. The proposed method is formulated as a rigorous optimization problem and estimates the full-band magnitude based on the criterion used in GLA. Our experiments demonstrate the effectiveness of the proposed method on speech, music, and environmental signals.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于优化的方法，用于从低维特征表示（如MEL spectrogram）重建时域信号。 phase reconstruction 已经研究过了从全带快时域傅立叙Transform（STFT）大小取得时域信号的重建方法。格里菲恩-林算法（GLA）在广泛使用，因为它只凭借 STFT 的重复性而工作，适用于各种音频信号。在这篇论文中，我们同时重建全带大小和频谱图中的相对关系，并基于这些关系进行优化问题的形式化表述。我们的实验表明，提议的方法在语音、音乐和环境信号上具有效果。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation"><a href="#Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation" class="headerlink" title="Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation"></a>Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12231">http://arxiv.org/abs/2307.12231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe</li>
<li>for: 这个论文是为了研究听话筛选和识别的集成，以提高多个人识别性能。</li>
<li>methods: 本论文使用了多通道分离方法、面积基于的扩展射频映射和复杂spectral mapping，以及最佳的后续模型特征。</li>
<li>results: 研究人员使用了最新的自动学习表示(SSLR)来改进filterbank特征，并通过合理的训练策略来集成语音分离和识别。结果显示，该策略在噪音混响WHAMR!测试集上实现了2.5%单词错误率，与现有的面积基于MVDR扩展射频映射和filterbank集成的28.9%相比，显著提高了多个人识别性能。<details>
<summary>Abstract</summary>
Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
</details>
<details>
<summary>摘要</summary>
neuronal speech separation 已经取得了很大的进步，其与自动语音识别（ASR）的结合是实现多 speaker ASR 的重要方向。本工作提供了深入的Investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end。具体来说，我们探讨了多通道分离方法，Mask-based beamforming和复杂的 spectral mapping，以及ASR back-end模型中最佳的特征。我们使用了最近的自然语言学习表示（SSLR）作为特征，并从filterbank特征中提高了认知性能。为了进一步提高多 speaker recognition性能，我们提出了一种优化的训练策略，将 speech separation和recognition与SSLR结合使用。我们使用TF-GridNet-based complex spectral mapping和WavLM-based SSLR，在抗噪抗干扰 WHAMR! 测试集上实现了2.5% 词错率，与现有的mask-based MVDR beamforming和filterbank结合（28.9%）相比，显著超越了。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey"><a href="#Backdoor-Attacks-against-Voice-Recognition-Systems-A-Survey" class="headerlink" title="Backdoor Attacks against Voice Recognition Systems: A Survey"></a>Backdoor Attacks against Voice Recognition Systems: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13643">http://arxiv.org/abs/2307.13643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baochen Yan, Jiahe Lan, Zheng Yan</li>
<li>for: This paper aims to provide a comprehensive survey on backdoor attacks against Voice Recognition Systems (VRSs) and to discuss the feasibility of deploying classic backdoor defense methods and generic audio defense techniques on VRSs.</li>
<li>methods: The paper employs a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives, and analyzes the characteristic of different categories. It also reviews existing attack methods and classic backdoor defense methods, and discusses the feasibility of deploying them on VRSs.</li>
<li>results: The paper provides a thorough review of backdoor attacks against VRSs, and discusses the open issues and future research directions in this field. It also provides a comprehensive understanding of the vulnerabilities of VRSs to backdoor attacks and the potential solutions to mitigate these attacks.<details>
<summary>Abstract</summary>
Voice Recognition Systems (VRSs) employ deep learning for speech recognition and speaker recognition. They have been widely deployed in various real-world applications, from intelligent voice assistance to telephony surveillance and biometric authentication. However, prior research has revealed the vulnerability of VRSs to backdoor attacks, which pose a significant threat to the security and privacy of VRSs. Unfortunately, existing literature lacks a thorough review on this topic. This paper fills this research gap by conducting a comprehensive survey on backdoor attacks against VRSs. We first present an overview of VRSs and backdoor attacks, elucidating their basic knowledge. Then we propose a set of evaluation criteria to assess the performance of backdoor attack methods. Next, we present a comprehensive taxonomy of backdoor attacks against VRSs from different perspectives and analyze the characteristic of different categories. After that, we comprehensively review existing attack methods and analyze their pros and cons based on the proposed criteria. Furthermore, we review classic backdoor defense methods and generic audio defense techniques. Then we discuss the feasibility of deploying them on VRSs. Finally, we figure out several open issues and further suggest future research directions to motivate the research of VRSs security.
</details>
<details>
<summary>摘要</summary>
声认系统（VRS）利用深度学习进行语音识别和说话人识别。它们在各种现实应用中广泛应用，从智能语音助手到电信监测和生物认证。然而，先前的研究表明，VRS受到后门攻击的威胁，这对VRS的安全性和隐私具有重要性。然而，现有的文献缺乏对这个话题的全面审查。这篇论文填补了这个研究空白，通过进行VRS对后门攻击的全面评估。我们首先提供VRS和后门攻击的概述，并提出评估后门攻击方法的评价标准。然后，我们提出了VRS对后门攻击的多维分类，并分析不同类别的特点。接着，我们对现有的攻击方法进行了全面的审查，并分析了它们的优缺点。此外，我们还评估了经典的后门防御方法和通用音频防御技术，并评估了它们在VRS上的可行性。最后，我们提出了一些未解决的问题，并建议未来研究VRS的安全性。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding"><a href="#Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding"></a>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer</li>
<li>for: 提高 END-to-END（E2E）语音理解（SLU）系统的 Robustness，使其在听写识别（ASR）错误时仍能准确理解语音。</li>
<li>methods: 我们提出了一种新的 E2E SLU 系统，利用 audio 和文本表示，并基于 ASR 假设的模态信息确定精度。我们采用了两种新技术：1）有效地编码 ASR 假设质量，2）有效地将其集成到 E2E SLU 模型中。</li>
<li>results: 我们在 STOP 数据集上进行了实验，并发现我们的方法可以提高准确率。我们还进行了分析，以证明我们的方法的有效性。<details>
<summary>Abstract</summary>
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
最近，端到端（E2E）的语音理解（SLU）系统在使用预训练的语音识别（ASR）模型时，变得更加有前途。这种方法使用单个模型，利用语音和文本表示从预训练的ASR模型中获取，并在设备上流动enario下超越传统的管道SLU系统。然而，E2E SLU系统仍然在文本表示质量低下时表现弱，这是因为ASR识别错误。为了解决这个问题，我们提出了一种新的E2E SLU系统，增强了对ASR错误的抗钝性。我们介绍了两种新技术：1）一种有效的ASR假设质量编码方法，2）一种有效的将其集成到E2E SLU模型中的方法。我们在STOP数据集上显示了准确性改进，并提供分析，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals"><a href="#Estimating-speaker-direction-on-a-humanoid-robot-with-binaural-acoustic-signals" class="headerlink" title="Estimating speaker direction on a humanoid robot with binaural acoustic signals"></a>Estimating speaker direction on a humanoid robot with binaural acoustic signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12129">http://arxiv.org/abs/2307.12129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Barot, Katja Mombaur, Ewen MacDonald</li>
<li>for: 这篇论文主要用于讲述一种用于人类对话者的位置估计方法，以实现人类与机器人之间的对话。</li>
<li>methods: 这篇论文使用了一种基于眼睛音源的方法来估计对话者的位置，并考虑了实时应用场景。这种方法在机器人人类头上实现了双耳声音源定位框架。</li>
<li>results: 经过实验和分析，这种方法可以在实时应用场景中提供有效的位置估计结果，并且可以适应不同的对话场景。同时，这种方法也可以减少延迟时间，以便实现实时对话。<details>
<summary>Abstract</summary>
To achieve human-like behaviour during speech interactions, it is necessary for a humanoid robot to estimate the location of a human talker. Here, we present a method to optimize the parameters used for the direction of arrival (DOA) estimation, while also considering real-time applications for human-robot interaction scenarios. This method is applied to binaural sound source localization framework on a humanoid robotic head. Real data is collected and annotated for this work. Optimizations are performed via a brute force method and a Bayesian model based method, results are validated and discussed, and effects on latency for real-time use are also explored.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:为实现人类样式的语音互动， robot需要估计人类说话者的位置。我们现在提出一种优化DOA估计参数的方法，同时考虑实时应用场景。这种方法应用于人型机器人头部上的双耳声源定位框架。实际数据收集和标注，并对其进行优化。我们使用枚举方法和 Bayesian 模型基于方法进行优化，并对结果进行验证和讨论。我们还探讨了在实时使用中的延迟影响。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.SD_2023_07_23/" data-id="clp8zxrbs00wnn688fxwaaoul" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.CV_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T13:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.CV_2023_07_23/">cs.CV - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ComPtr-Towards-Diverse-Bi-source-Dense-Prediction-Tasks-via-A-Simple-yet-General-Complementary-Transformer"><a href="#ComPtr-Towards-Diverse-Bi-source-Dense-Prediction-Tasks-via-A-Simple-yet-General-Complementary-Transformer" class="headerlink" title="ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer"></a>ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12349">http://arxiv.org/abs/2307.12349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lartpang/comptr">https://github.com/lartpang/comptr</a></li>
<li>paper_authors: Youwei Pang, Xiaoqi Zhao, Lihe Zhang, Huchuan Lu</li>
<li>for: 这个研究是为了开发一个能够同时处理多种不同任务的复合式对话模型，以提高深度学习（DL）在紧密预测领域的表现。</li>
<li>methods: 本研究使用了一种名为ComPtr的复合式对话模型，它基于信息补充的概念，并具有两个组件：协调强化和差异意识部分。这两个组件可以帮助ComPtr从不同的图像来源中获取重要的视觉 semantic 讯号，并将其转换为多个任务中的有用信息。</li>
<li>results: 在多个代表性的视觉任务中，例如离散检测、RGB-T人数掌握、RGB-D&#x2F;T焦点物探测以及RGB-D semantics 类别分类，ComPtr 都能够获得良好的表现。<details>
<summary>Abstract</summary>
Deep learning (DL) has advanced the field of dense prediction, while gradually dissolving the inherent barriers between different tasks. However, most existing works focus on designing architectures and constructing visual cues only for the specific task, which ignores the potential uniformity introduced by the DL paradigm. In this paper, we attempt to construct a novel \underline{ComP}lementary \underline{tr}ansformer, \textbf{ComPtr}, for diverse bi-source dense prediction tasks. Specifically, unlike existing methods that over-specialize in a single task or a subset of tasks, ComPtr starts from the more general concept of bi-source dense prediction. Based on the basic dependence on information complementarity, we propose consistency enhancement and difference awareness components with which ComPtr can evacuate and collect important visual semantic cues from different image sources for diverse tasks, respectively. ComPtr treats different inputs equally and builds an efficient dense interaction model in the form of sequence-to-sequence on top of the transformer. This task-generic design provides a smooth foundation for constructing the unified model that can simultaneously deal with various bi-source information. In extensive experiments across several representative vision tasks, i.e. remote sensing change detection, RGB-T crowd counting, RGB-D/T salient object detection, and RGB-D semantic segmentation, the proposed method consistently obtains favorable performance. The code will be available at \url{https://github.com/lartpang/ComPtr}.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）在密集预测方面取得了 significiant 进步，逐渐消除了不同任务之间的自然障碍。然而，大多数现有的工作都是为特定任务或子集任务设计特有的建筑和视觉提示，忽视了深度学习 парадиг中的可能性。在这篇论文中，我们尝试构建一种新的 ComPlementary trasnformer，即 ComPtr，用于多种生物源密集预测任务。Specifically， unlike existing methods that over-specialize in a single task or a subset of tasks, ComPtr starts from the more general concept of bi-source dense prediction. Based on the basic dependence on information complementarity, we propose consistency enhancement and difference awareness components with which ComPtr can evacuate and collect important visual semantic cues from different image sources for diverse tasks, respectively. ComPtr treats different inputs equally and builds an efficient dense interaction model in the form of sequence-to-sequence on top of the transformer. This task-generic design provides a smooth foundation for constructing the unified model that can simultaneously deal with various bi-source information. In extensive experiments across several representative vision tasks, i.e. 远程感知变化检测、RGB-T人群计数、RGB-D/T突出物检测和RGB-D semantic segmentation, the proposed method consistently obtains favorable performance. Code will be available at \url{https://github.com/lartpang/ComPtr}.
</details></li>
</ul>
<hr>
<h2 id="ResShift-Efficient-Diffusion-Model-for-Image-Super-resolution-by-Residual-Shifting"><a href="#ResShift-Efficient-Diffusion-Model-for-Image-Super-resolution-by-Residual-Shifting" class="headerlink" title="ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting"></a>ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12348">http://arxiv.org/abs/2307.12348</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zsyoaoa/resshift">https://github.com/zsyoaoa/resshift</a></li>
<li>paper_authors: Zongsheng Yue, Jianyi Wang, Chen Change Loy</li>
<li>for: 提高图像超分辨率（SR）方法的执行速度，解决现有方法的执行慢速问题。</li>
<li>methods: 提出一种新的和高效的扩散模型，通过减少扩散步数，消除post加速的需求和相关性能下降。</li>
<li>results: 实验表明，提出的方法在 synthetic 和实际数据集上具有较高或相当于当前状态艺术方法的性能，只需15步扩散。<details>
<summary>Abstract</summary>
Diffusion-based image super-resolution (SR) methods are mainly limited by the low inference speed due to the requirements of hundreds or even thousands of sampling steps. Existing acceleration sampling techniques inevitably sacrifice performance to some extent, leading to over-blurry SR results. To address this issue, we propose a novel and efficient diffusion model for SR that significantly reduces the number of diffusion steps, thereby eliminating the need for post-acceleration during inference and its associated performance deterioration. Our method constructs a Markov chain that transfers between the high-resolution image and the low-resolution image by shifting the residual between them, substantially improving the transition efficiency. Additionally, an elaborate noise schedule is developed to flexibly control the shifting speed and the noise strength during the diffusion process. Extensive experiments demonstrate that the proposed method obtains superior or at least comparable performance to current state-of-the-art methods on both synthetic and real-world datasets, even only with 15 sampling steps. Our code and model are available at https://github.com/zsyOAOA/ResShift.
</details>
<details>
<summary>摘要</summary>
Diffusion-based图像超分辨 (SR) 方法主要受限于推断速度较低，因为需要数百或者千个抽象步骤。现有的加速抽象技术无论如何，都会 sacrificing performance一定程度，导致SR结果过度模糊。为解决这个问题，我们提出了一种新的和高效的Diffusion模型，可以大幅减少抽象步骤数量，从而消除推断过程中的后加速和其相关的性能下降。我们的方法构建了一个Markov链，将高分辨图像和低分辨图像之间的差异转移到高分辨图像上，大幅提高了转移效率。此外，我们还开发了一种灵活控制抽象速度和噪声强度的附加noise schedule。广泛的实验表明，我们的方法可以在现有的State-of-the-art方法的基础上实现更好的或者相当的性能，只需要15个抽象步骤。我们的代码和模型可以在https://github.com/zsyOAOA/ResShift上下载。
</details></li>
</ul>
<hr>
<h2 id="Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction"><a href="#Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction" class="headerlink" title="Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction"></a>Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12341">http://arxiv.org/abs/2307.12341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lykourgos Chiniadis, Petros Tamvakis<br>for:* 提高农业生产和土壤属性分析，为生态可持续发展提供关键前提。methods:* 使用FT NIR reflectance спектроскопия和深度学习方法来预测土壤碳酸含量。* 利用多种机器学习算法，如：多层感知网络（MLP）回归器和卷积神经网络（CNN），并与传统的多变量分析（PLSR）、矩阵分析（Cubist）和支持向量机（SVM）进行比较。results:* 使用FT NIR reflectance спектроскопия和深度学习方法可以快速和高效地预测土壤碳酸含量，并且在未看过的土壤样本上达到了良好的预测性能。* 与X射Diffraction量测相比，MLP预测值和实际值之间的相对误差在5%以内，表明深度学习模型可以准确地预测土壤碳酸含量。* 本研究的结果表明，深度学习模型可以作为快速和高效的预测工具，用于预测土壤碳酸含量，特别是在没有量imetric方法可用的情况下。<details>
<summary>Abstract</summary>
Soil NIR spectral absorbance/reflectance libraries are utilized towards improving agricultural production and analysis of soil properties which are key prerequisite for agroecological balance and environmental sustainability. Carbonates in particular, represent a soil property which is mostly affected even by mild, let alone extreme, changes of environmental conditions during climate change. In this study we propose a rapid and efficient way to predict carbonates content in soil by means of FT NIR reflectance spectroscopy and by use of deep learning methods. We exploited multiple machine learning methods, such as: 1) a MLP Regressor and 2) a CNN and compare their performance with other traditional ML algorithms such as PLSR, Cubist and SVM on the combined dataset of two NIR spectral libraries: KSSL (USDA), a dataset of soil samples reflectance spectra collected nationwide, and LUCAS TopSoil (European Soil Library) which contains soil sample absorbance spectra from all over the European Union, and use them to predict carbonate content on never before seen soil samples. Soil samples in KSSL and in TopSoil spectral libraries were acquired in the spectral region of visNIR, however in this study, only the NIR spectral region was utilized. Quantification of carbonates by means of Xray Diffraction is in good agreement with the volumetric method and the MLP prediction. Our work contributes to rapid carbonates content prediction in soil samples in cases where: 1) no volumetric method is available and 2) only NIR spectra absorbance data are available. Up till now and to the best of our knowledge, there exists no other study, that presents a prediction model trained on such an extensive dataset with such promising results on unseen data, undoubtedly supporting the notion that deep learning models present excellent prediction tools for soil carbonates content.
</details>
<details>
<summary>摘要</summary>
soil NIR spectral absorbance/reflectance 图书馆是用于提高农业生产和土壤属性分析的重要途径，这些属性是生态平衡和环境可持续性的关键因素。碳酸盐 particularly 是在气候变化中环境条件轻度到极端变化时受到影响的土壤属性。本研究提出了一种快速和高效的碳酸盐含量预测方法，通过FT NIR 反射спектроскопия和深度学习方法。我们利用了多种机器学习方法，如：1）多层感知网络（MLP）回归器和2）卷积神经网络（CNN），并与传统的机器学习算法如：PLSR、Cubist和SVM进行比较，使用了合并的KSSL（美国农业部）和LUCAS TopSoil（欧盟土壤图书馆）的两个NIR spectral库，用于预测碳酸盐含量。KSSL和TopSoil spectral库中的土壤样本反射спектроскопия数据收集在VisNIRspectral区间内，但在本研究中仅使用NIRspectral区间。X射 diffraction 测量和MLP预测结果表明，我们的方法可以准确预测碳酸盐含量。我们的工作支持在没有体积方法可用时，只有NIR Spectra absorbance数据可用时，可以快速预测碳酸盐含量。到目前为止，我们知道没有其他研究，可以在such an extensive dataset上提出类似的预测模型，并且模型在未见数据上表现了惊人的好准确性，无疑地支持深度学习模型在土壤碳酸盐含量预测中的出色表现。
</details></li>
</ul>
<hr>
<h2 id="Learning-Navigational-Visual-Representations-with-Semantic-Map-Supervision"><a href="#Learning-Navigational-Visual-Representations-with-Semantic-Map-Supervision" class="headerlink" title="Learning Navigational Visual Representations with Semantic Map Supervision"></a>Learning Navigational Visual Representations with Semantic Map Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12335">http://arxiv.org/abs/2307.12335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiconghong/ego2map-navit">https://github.com/yiconghong/ego2map-navit</a></li>
<li>paper_authors: Yicong Hong, Yang Zhou, Ruiyi Zhang, Franck Dernoncourt, Trung Bui, Stephen Gould, Hao Tan</li>
<li>for: 本研究旨在提高家用机器人视觉导航能力，尤其是在室内环境中。</li>
<li>methods: 我们提出了一种基于 egocentric 视图和 semantic 地图（Ego$^2$-Map）的视觉表示学习方法，以增强机器人的导航能力。</li>
<li>results: 我们的实验表明，使用我们学习的表示可以在 object-goal 导航任务中表现出优于最新的视觉预训练方法，并在 continuous 环境中实现新的state-of-the-art 结果。<details>
<summary>Abstract</summary>
Being able to perceive the semantics and the spatial structure of the environment is essential for visual navigation of a household robot. However, most existing works only employ visual backbones pre-trained either with independent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that humans naturally build semantically and spatially meaningful cognitive maps in their brains during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent's egocentric views and semantic maps (Ego$^2$-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego$^2$-Map learning transfers the compact and rich information from a map, such as objects, structure and transition, to the agent's egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperform recent visual pre-training methods. Moreover, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.
</details>
<details>
<summary>摘要</summary>
“能够感受到环境的 semantics 和空间结构是家用机器人视觉导航的重要前提。然而，现有的大多数工作仅将视觉背bone 进行独立图像的分类或自我类型学习方法进行适应室内导航领域，忽略了 Navigation 中所需的空间关系。 Drawing inspiration from humans' natural ability to build semantically and spatially meaningful cognitive maps in their brains during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent's egocentric views and semantic maps (Ego$^2$-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale Habitat-Matterport3D environments. Ego$^2$-Map learning transfers the compact and rich information from a map, such as objects, structure, and transition, to the agent's egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperform recent visual pre-training methods. Moreover, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.”Note that Simplified Chinese is the official standard for Chinese writing in mainland China, and it is used in this translation. Traditional Chinese is used in Taiwan and Hong Kong, and it may have slightly different grammar and character usage.
</details></li>
</ul>
<hr>
<h2 id="ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection"><a href="#ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection" class="headerlink" title="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection"></a>ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12327">http://arxiv.org/abs/2307.12327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang<br>For:* The paper is written for hyperspectral image change detection (HSI-CD) and aims to identify differences in bitemporal HSIs.Methods:* The paper proposes an end-to-end efficient spectral-spatial change detection network (ES2Net) that includes a learnable band selection module and a cluster-wise spatial attention mechanism.Results:* The paper demonstrates the effectiveness and superiority of the proposed method compared with other state-of-the-art methods through experiments on three widely used HSI-CD datasets.<details>
<summary>Abstract</summary>
Hyperspectral image change detection (HSI-CD) aims to identify the differences in bitemporal HSIs. To mitigate spectral redundancy and improve the discriminativeness of changing features, some methods introduced band selection technology to select bands conducive for CD. However, these methods are limited by the inability to end-to-end training with the deep learning-based feature extractor and lack considering the complex nonlinear relationship among bands. In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we devised a learnable band selection module to automatically select bands conducive to CD. It can be jointly optimized with a feature extraction network and capture the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we design the cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band. Experiments on three widely used HSI-CD datasets demonstrate the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
干elijah�image Change Detection (HSI-CD) 的目标是 Identify the differences between bitemporal hyperspectral images (HSIs). To reduce spectral redundancy and improve the discriminativeness of changing features, some methods have introduced band selection technology to select bands that are conducive to CD. However, these methods are limited by their inability to perform end-to-end training with deep learning-based feature extractors and their failure to consider the complex nonlinear relationships among bands.In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we have devised a learnable band selection module that automatically selects bands that are conducive to CD. This module can be jointly optimized with a feature extraction network and captures the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we have designed a cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band.Experiments on three widely used HSI-CD datasets have demonstrated the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models"><a href="#Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models" class="headerlink" title="Development of pericardial fat count images using a combination of three different deep-learning models"></a>Development of pericardial fat count images using a combination of three different deep-learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12316">http://arxiv.org/abs/2307.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takaaki Matsunaga, Atsushi Kono, Hidetoshi Matsuo, Kaoru Kitagawa, Mizuho Nishio, Hiromi Hashimura, Yu Izawa, Takayoshi Toba, Kazuki Ishikawa, Akie Katsuki, Kazuyuki Ohmura, Takamichi Murakami</li>
<li>for: The paper aims to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model, in order to evaluate pericardial fat (PF) and its potential role in the development of coronary artery disease.</li>
<li>methods: The proposed method uses three different deep-learning models, including CycleGAN, to generate PFCIs from CXRs. The method first projects the three-dimensional CT images onto a two-dimensional plane, and then uses the deep-learning models to generate PFCIs from the projected images. The performance of the proposed method is evaluated using structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE).</li>
<li>results: The results show that the PFCIs generated using the proposed method have better performance than those generated using a single CycleGAN-based model, as measured by SSIM, MSE, and MAE. The proposed method also shows the potential for evaluating PF without the need for CT scans.<details>
<summary>Abstract</summary>
Rationale and Objectives: Pericardial fat (PF), the thoracic visceral fat surrounding the heart, promotes the development of coronary artery disease by inducing inflammation of the coronary arteries. For evaluating PF, this study aimed to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model.   Materials and Methods: The data of 269 consecutive patients who underwent coronary computed tomography (CT) were reviewed. Patients with metal implants, pleural effusion, history of thoracic surgery, or that of malignancy were excluded. Thus, the data of 191 patients were used. PFCIs were generated from the projection of three-dimensional CT images, where fat accumulation was represented by a high pixel value. Three different deep-learning models, including CycleGAN, were combined in the proposed method to generate PFCIs from CXRs. A single CycleGAN-based model was used to generate PFCIs from CXRs for comparison with the proposed method. To evaluate the image quality of the generated PFCIs, structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE) of (i) the PFCI generated using the proposed method and (ii) the PFCI generated using the single model were compared.   Results: The mean SSIM, MSE, and MAE were as follows: 0.856, 0.0128, and 0.0357, respectively, for the proposed model; and 0.762, 0.0198, and 0.0504, respectively, for the single CycleGAN-based model.   Conclusion: PFCIs generated from CXRs with the proposed model showed better performance than those with the single model. PFCI evaluation without CT may be possible with the proposed method.
</details>
<details>
<summary>摘要</summary>
目的和目标：胸膜脂肪（PF），脏膜内附近心脏的脂肪，可以促进心脏疾病的发展，并导致心脏粥玢病变。为评估PF，本研究想要从胸部X射线图像（CXR）中生成胸膜脂肪计数图像（PFCIs）。材料和方法：本研究审查了269例 consecutively admitted patients的 coronary computed tomography（CT）数据。排除了 метал制品、肿胀、历史上的胸部手术和肿瘤等因素，因此使用了191例的数据。PFCIs由三维CT图像的投影生成，其中脂肪堆积表示高像素值。本研究使用了三种深度学习模型，包括 CycleGAN，来生成PFCIs从CXR。单个 CycleGAN-based 模型用于生成PFCIs从CXR，并与提案方法进行比较。为评估生成的PFCIs的图像质量，使用了结构相似度指数（SSIM）、平均平方误差（MSE）和平均绝对误差（MAE）。结果：* SSIM：0.856* MSE：0.0128* MAE：0.0357比较结果：* SSIM：0.762* MSE：0.0198* MAE：0.0504结论：提案方法生成的PFCIs显示与单个模型生成的PFCIs有更好的性能。PFCI评估可能不需要CT扫描。
</details></li>
</ul>
<hr>
<h2 id="Building-Extraction-from-Remote-Sensing-Images-via-an-Uncertainty-Aware-Network"><a href="#Building-Extraction-from-Remote-Sensing-Images-via-an-Uncertainty-Aware-Network" class="headerlink" title="Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network"></a>Building Extraction from Remote Sensing Images via an Uncertainty-Aware Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12309">http://arxiv.org/abs/2307.12309</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henryjiepanli/uncertainty-aware-network">https://github.com/henryjiepanli/uncertainty-aware-network</a></li>
<li>paper_authors: Wei He, Jiepan Li, Weinan Cao, Liangpei Zhang, Hongyan Zhang</li>
<li>for: 减少建筑物识别错误率</li>
<li>methods: 使用uncertainty-aware网络（UANet）</li>
<li>results: 比其他状态对照方法减少误差率<details>
<summary>Abstract</summary>
Building extraction aims to segment building pixels from remote sensing images and plays an essential role in many applications, such as city planning and urban dynamic monitoring. Over the past few years, deep learning methods with encoder-decoder architectures have achieved remarkable performance due to their powerful feature representation capability. Nevertheless, due to the varying scales and styles of buildings, conventional deep learning models always suffer from uncertain predictions and cannot accurately distinguish the complete footprints of the building from the complex distribution of ground objects, leading to a large degree of omission and commission. In this paper, we realize the importance of uncertain prediction and propose a novel and straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To verify the performance of our proposed UANet, we conduct extensive experiments on three public building datasets, including the WHU building dataset, the Massachusetts building dataset, and the Inria aerial image dataset. Results demonstrate that the proposed UANet outperforms other state-of-the-art algorithms by a large margin.
</details>
<details>
<summary>摘要</summary>
traditional deep learning models always suffer from uncertain predictions and cannot accurately distinguish the complete footprints of the building from the complex distribution of ground objects, leading to a large degree of omission and commission. To address this problem, we propose a novel and straightforward Uncertainty-Aware Network (UANet) to alleviate this problem. To verify the performance of our proposed UANet, we conduct extensive experiments on three public building datasets, including the WHU building dataset, the Massachusetts building dataset, and the Inria aerial image dataset. Results demonstrate that the proposed UANet outperforms other state-of-the-art algorithms by a large margin.Here's the text with some additional information about the Simplified Chinese translation:The text is translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation is done using a machine translation tool, and the result is a more literal translation of the original text.Some notes about the translation:* "uncertain prediction" is translated as "uncertain predictions" (uncertain predictions is a phrase that is commonly used in machine learning to describe the situation where a model is not sure about its predictions).* "complete footprints" is translated as "complete footprint" (singular form) to match the original text.* "ground objects" is translated as "ground objects" (literal translation), but it could be translated as "ground features" or "ground objects" depending on the context.* "state-of-the-art algorithms" is translated as "state-of-the-art algorithm" (singular form) to match the original text.I hope this helps! Let me know if you have any other questions.
</details></li>
</ul>
<hr>
<h2 id="RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC"><a href="#RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC" class="headerlink" title="RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC"></a>RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12301">http://arxiv.org/abs/2307.12301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mxtsai/ransac-nn">https://github.com/mxtsai/ransac-nn</a></li>
<li>paper_authors: Chen-Han Tsai, Yu-Shao Peng</li>
<li>for:  This paper proposes an unsupervised outlier detection algorithm specifically designed for image data, called RANSAC-NN.</li>
<li>methods: The proposed algorithm uses a RANSAC-based approach to compare images and predict the outlier score without additional training or label information.</li>
<li>results: The proposed algorithm consistently performs favorably against state-of-the-art outlier detection algorithms on 15 diverse datasets without any hyperparameter tuning, and it has potential applications in image mislabeled detection.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文提出了一种特制于图像数据的无监督异常检测算法，名为RANSAC-NN。</li>
<li>methods: 该算法使用RANSAC-based方法比较图像，自动预测每个图像的异常分数，无需额外训练或标签信息。</li>
<li>results: 该算法在15个多样化的数据集中，与现状异常检测算法相比，一般性高，而且无需调整Hyperparameter，并且具有图像推理检测的潜在应用。<details>
<summary>Abstract</summary>
Image outlier detection (OD) is crucial for ensuring the quality and accuracy of image datasets used in computer vision tasks. The majority of OD algorithms, however, have not been targeted toward image data. Consequently, the results of applying such algorithms to images are often suboptimal. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets. Without any hyperparameter tuning, RANSAC-NN consistently performs favorably in contrast to other algorithms in almost every dataset category. Furthermore, we provide a detailed analysis to understand each RANSAC-NN component, and we demonstrate its potential applications in image mislabeled detection. Code for RANSAC-NN is provided at https://github.com/mxtsai/ransac-nn
</details>
<details>
<summary>摘要</summary>
Image outlier detection (OD) 是 Ensure the quality and accuracy of image datasets used in computer vision tasks 的 crucial step. However, most OD algorithms have not been designed specifically for images, leading to suboptimal results when applied to images. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm tailored for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without requiring additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets and show that it consistently performs well without any hyperparameter tuning. Furthermore, we provide a detailed analysis of each RANSAC-NN component and demonstrate its potential applications in image mislabeled detection. The code for RANSAC-NN is available at https://github.com/mxtsai/ransac-nn.
</details></li>
</ul>
<hr>
<h2 id="Hybrid-CSR-Coupling-Explicit-and-Implicit-Shape-Representation-for-Cortical-Surface-Reconstruction"><a href="#Hybrid-CSR-Coupling-Explicit-and-Implicit-Shape-Representation-for-Cortical-Surface-Reconstruction" class="headerlink" title="Hybrid-CSR: Coupling Explicit and Implicit Shape Representation for Cortical Surface Reconstruction"></a>Hybrid-CSR: Coupling Explicit and Implicit Shape Representation for Cortical Surface Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12299">http://arxiv.org/abs/2307.12299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanlin Sun, Thanh-Tung Le, Chenyu You, Hao Tang, Kun Han, Haoyu Ma, Deying Kong, Xiangyi Yan, Xiaohui Xie</li>
<li>for:  cortical surface reconstruction</li>
<li>methods:  geometric deep-learning model combining explicit and implicit shape representations, mesh-based deformation module, optimization-based diffeomorphic surface registration</li>
<li>results:  surpasses existing implicit and explicit cortical surface reconstruction methods in numeric metrics, including accuracy, regularity, and consistency.<details>
<summary>Abstract</summary>
We present Hybrid-CSR, a geometric deep-learning model that combines explicit and implicit shape representations for cortical surface reconstruction. Specifically, Hybrid-CSR begins with explicit deformations of template meshes to obtain coarsely reconstructed cortical surfaces, based on which the oriented point clouds are estimated for the subsequent differentiable poisson surface reconstruction. By doing so, our method unifies explicit (oriented point clouds) and implicit (indicator function) cortical surface reconstruction. Compared to explicit representation-based methods, our hybrid approach is more friendly to capture detailed structures, and when compared with implicit representation-based methods, our method can be topology aware because of end-to-end training with a mesh-based deformation module. In order to address topology defects, we propose a new topology correction pipeline that relies on optimization-based diffeomorphic surface registration. Experimental results on three brain datasets show that our approach surpasses existing implicit and explicit cortical surface reconstruction methods in numeric metrics in terms of accuracy, regularity, and consistency.
</details>
<details>
<summary>摘要</summary>
我们介绍Hybrid-CSR，一种几何深度学习模型，将显式和隐式形态表示结合用于脑表面重建。具体来说，Hybrid-CSR从template mesh的显式变形开始，以获取粗略重建的脑表面，然后根据这些oriented point clouds进行后续的可 differentiable Poisson surface reconstruction。这样，我们的方法将显式（oriented point clouds）和隐式（指示函数）脑表面重建方法融合在一起。与显式表示基于方法相比，我们的半结合方法更友好地捕捉细节结构，而与隐式表示基于方法相比，我们的方法可以保持topology意识，这是因为我们通过粘性变换模块进行终端训练。为了解决topology问题，我们提出了一种新的topology更正管道，该管道基于数据优化的diffusion surface registration。实验结果表明，我们的方法在三个脑数据集上的数据指标上胜过现有的隐式和显式脑表面重建方法。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames"><a href="#Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames" class="headerlink" title="Simultaneous temperature estimation and nonuniformity correction from multiple frames"></a>Simultaneous temperature estimation and nonuniformity correction from multiple frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12297">http://arxiv.org/abs/2307.12297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Omri Berman, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 这个论文的目的是提出一种同时进行温度估计和不均匀性修正的方法，以提高低成本的红外摄像机在各种应用中的精度和效率。</li>
<li>methods: 该方法基于深度学习核函数网络（KPN），利用摄像机的物理图像捕获模型，并通过一个新的偏移块来 incorporate  ambient temperature。</li>
<li>results: 对实际数据进行测试，该方法可以 achieve 高精度和高效率的温度估计和不均匀性修正，相比 vanilla KPN 有显著的改善。<details>
<summary>Abstract</summary>
Infrared (IR) cameras are widely used for temperature measurements in various applications, including agriculture, medicine, and security. Low-cost IR camera have an immense potential to replace expansive radiometric cameras in these applications, however low-cost microbolometer-based IR cameras are prone to spatially-variant nonuniformity and to drift in temperature measurements, which limits their usability in practical scenarios.   To address these limitations, we propose a novel approach for simultaneous temperature estimation and nonuniformity correction from multiple frames captured by low-cost microbolometer-based IR cameras. We leverage the physical image acquisition model of the camera and incorporate it into a deep learning architecture called kernel estimation networks (KPN), which enables us to combine multiple frames despite imperfect registration between them. We also propose a novel offset block that incorporates the ambient temperature into the model and enables us to estimate the offset of the camera, which is a key factor in temperature estimation.   Our findings demonstrate that the number of frames has a significant impact on the accuracy of temperature estimation and nonuniformity correction. Moreover, our approach achieves a significant improvement in performance compared to vanilla KPN, thanks to the offset block. The method was tested on real data collected by a low-cost IR camera mounted on a UAV, showing only a small average error of $0.27^\circ C-0.54^\circ C$ relative to costly scientific-grade radiometric cameras.   Our method provides an accurate and efficient solution for simultaneous temperature estimation and nonuniformity correction, which has important implications for a wide range of practical applications.
</details>
<details>
<summary>摘要</summary>
infrared (IR) 摄像机广泛应用于温度测量多个应用场景中，如农业、医学和安全。低成本IR摄像机具有取代昂贵 радиометрические摄像机的潜在优势，但低成本微博拉ometer-based IR摄像机受到空间不均和温度测量中的偏差所限制。为解决这些限制，我们提出了一种新的方法，即同时进行温度估计和不均差修正，使用多帧 captured by low-cost microbolometer-based IR摄像机。我们利用摄像机物理捕获模型，并将其integrated into a deep learning architecture called kernel estimation networks (KPN)，这使得我们可以将多帧组合成一起，即使这些帧之间没有完美对齐。我们还提出了一个新的偏移块，即将室外温度 incorporated into the model，这使得我们可以估计摄像机的偏移，这是温度估计中的关键因素。我们的发现表明，帧数对温度估计和不均差修正的精度有着显著的影响。此外，我们的方法在比 vanilla KPN 的情况下具有显著的改善，这是因为偏移块的存在。我们的方法在实际数据 collected by a low-cost IR摄像机 mounted on a UAV 上进行测试，显示只有小于 $0.27^\circ C-0.54^\circ C$ 的平均误差，相比昂贵的科学级 radiometric 摄像机。我们的方法为温度估计和不均差修正提供了一种准确和高效的解决方案，这有着广泛的实际应用前景。
</details></li>
</ul>
<hr>
<h2 id="TransHuman-A-Transformer-based-Human-Representation-for-Generalizable-Neural-Human-Rendering"><a href="#TransHuman-A-Transformer-based-Human-Representation-for-Generalizable-Neural-Human-Rendering" class="headerlink" title="TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering"></a>TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12291">http://arxiv.org/abs/2307.12291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pansanity666/TransHuman">https://github.com/pansanity666/TransHuman</a></li>
<li>paper_authors: Xiao Pan, Zongxin Yang, Jianxin Ma, Chang Zhou, Yi Yang</li>
<li>for: 本研究强调的任务是通过 conditional Neural Radiance Fields (NeRF) 来实现一致的人类渲染，从多视图视频中训练 conditional NeRF 模型，以便在不同的人物上进行一致的渲染。</li>
<li>methods: 本研究使用了一种brand-new的框架，名为 TransHuman，它通过 Transformer-based Human Encoding (TransHE)、Deformable Partial Radiance Fields (DPaRF) 和 Fine-grained Detail Integration (FDI) 来学习涂抹 SMPL 图像，并捕捉人体部件之间的全局关系。</li>
<li>results: 经验表明，TransHuman 可以在 ZJU-MoCap 和 H36M 数据集上达到新的州态艺之绩，同时具有高效性。项目页面：<a target="_blank" rel="noopener" href="https://pansanity666.github.io/TransHuman/">https://pansanity666.github.io/TransHuman/</a><details>
<summary>Abstract</summary>
In this paper, we focus on the task of generalizable neural human rendering which trains conditional Neural Radiance Fields (NeRF) from multi-view videos of different characters. To handle the dynamic human motion, previous methods have primarily used a SparseConvNet (SPC)-based human representation to process the painted SMPL. However, such SPC-based representation i) optimizes under the volatile observation space which leads to the pose-misalignment between training and inference stages, and ii) lacks the global relationships among human parts that is critical for handling the incomplete painted SMPL. Tackling these issues, we present a brand-new framework named TransHuman, which learns the painted SMPL under the canonical space and captures the global relationships between human parts with transformers. Specifically, TransHuman is mainly composed of Transformer-based Human Encoding (TransHE), Deformable Partial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI). TransHE first processes the painted SMPL under the canonical space via transformers for capturing the global relationships between human parts. Then, DPaRF binds each output token with a deformable radiance field for encoding the query point under the observation space. Finally, the FDI is employed to further integrate fine-grained information from reference images. Extensive experiments on ZJU-MoCap and H36M show that our TransHuman achieves a significantly new state-of-the-art performance with high efficiency. Project page: https://pansanity666.github.io/TransHuman/
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注通用神经人类渲染任务，这个任务是通过多视图视频来训练 conditional Neural Radiance Fields（NeRF）来生成不同人物的图像。为了处理人类动态运动，之前的方法主要使用了SparseConvNet（SPC）来表示人类，但这种SPC-based表示方式有两个问题：一是优化在不稳定的观察空间下，导致训练和执行阶段的姿势不一致；二是缺乏人类部分之间的全局关系，这是在处理部分SMPL的时候非常重要。为了解决这些问题，我们提出了一个全新的框架名为TransHuman，它在 canonical space 中学习涂抹 SMPL，并且 capture 人类部分之间的全局关系。TransHuman 的主要组成部分包括 Transformer-based Human Encoding（TransHE）、Deformable Partial Radiance Fields（DPaRF）和 Fine-grained Detail Integration（FDI）。TransHE 首先在 canonical space 中使用 transformers 处理涂抹 SMPL，以 capture 人类部分之间的全局关系。然后，DPaRF 将每个输出 Token 绑定到一个可变的辐射场，以在观察空间中编码查询点。最后，FDI 被使用来进一步 интеGRATE 细节信息。我们对 ZJU-MoCap 和 H36M 进行了广泛的实验，并证明了我们的 TransHuman 可以达到新的 estado del arte 性能，同时具有高效性。项目页面：https://pansanity666.github.io/TransHuman/
</details></li>
</ul>
<hr>
<h2 id="Downstream-agnostic-Adversarial-Examples"><a href="#Downstream-agnostic-Adversarial-Examples" class="headerlink" title="Downstream-agnostic Adversarial Examples"></a>Downstream-agnostic Adversarial Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12280">http://arxiv.org/abs/2307.12280</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cgcl-codes/advencoder">https://github.com/cgcl-codes/advencoder</a></li>
<li>paper_authors: Ziqi Zhou, Shengshan Hu, Ruizhi Zhao, Qian Wang, Leo Yu Zhang, Junhui Hou, Hai Jin</li>
<li>for: 本研究旨在提出一个基于预训模型的攻击框架，可以对具有预训模型的下游任务进行 Universial Adversarial Examples 攻击。</li>
<li>methods: 本研究使用了高频率成分信息来引导生成攻击例子，然后设计了一个生成攻击框架，以学习攻击类别dataset的分布，以提高攻击成功率和传播性。</li>
<li>results: 研究结果显示，攻击者可以成功攻击下游任务，不需要知道预训dataset或下游dataset的详细信息。此外，研究者还提出了四种防护方法，其结果进一步证明了 AdvEncoder 的攻击能力。<details>
<summary>Abstract</summary>
Self-supervised learning usually uses a large amount of unlabeled data to pre-train an encoder which can be used as a general-purpose feature extractor, such that downstream users only need to perform fine-tuning operations to enjoy the benefit of "large model". Despite this promising prospect, the security of pre-trained encoder has not been thoroughly investigated yet, especially when the pre-trained encoder is publicly available for commercial use.   In this paper, we propose AdvEncoder, the first framework for generating downstream-agnostic universal adversarial examples based on the pre-trained encoder. AdvEncoder aims to construct a universal adversarial perturbation or patch for a set of natural images that can fool all the downstream tasks inheriting the victim pre-trained encoder. Unlike traditional adversarial example works, the pre-trained encoder only outputs feature vectors rather than classification labels. Therefore, we first exploit the high frequency component information of the image to guide the generation of adversarial examples. Then we design a generative attack framework to construct adversarial perturbations/patches by learning the distribution of the attack surrogate dataset to improve their attack success rates and transferability. Our results show that an attacker can successfully attack downstream tasks without knowing either the pre-training dataset or the downstream dataset. We also tailor four defenses for pre-trained encoders, the results of which further prove the attack ability of AdvEncoder.
</details>
<details>
<summary>摘要</summary>
自我监督学习通常使用大量未标注数据来预训练一个编码器，以便下游用户只需进行精细调整来获得“大型模型”的好处。然而，预训练编码器的安全性尚未得到全面的调查，尤其是在公开可用于商业用途的情况下。在这篇论文中，我们提出了 AdvEncoder，第一个基于预训练编码器的下游agnostic通用攻击示例生成框架。AdvEncoder的目标是为一组自然图像构建一个通用攻击杂音或贴图，可以让所有继承于受害者预训练编码器的下游任务受到攻击。与传统攻击示例工作不同，预训练编码器只输出图像特征 вектор而不是分类标签。因此，我们首先利用图像高频成分信息来引导攻击示例生成。然后，我们设计了一个生成攻击框架，通过学习攻击代理数据集的分布来提高攻击成功率和传输性。我们的结果表明，攻击者可以成功攻击下游任务，不需要知道预训练数据集或下游数据集。我们还适应四种防御措施 для预训练编码器，结果证明了 AdvEncoder 的攻击能力。
</details></li>
</ul>
<hr>
<h2 id="FDCT-Fast-Depth-Completion-for-Transparent-Objects"><a href="#FDCT-Fast-Depth-Completion-for-Transparent-Objects" class="headerlink" title="FDCT: Fast Depth Completion for Transparent Objects"></a>FDCT: Fast Depth Completion for Transparent Objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12274">http://arxiv.org/abs/2307.12274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nonmy/fdct">https://github.com/nonmy/fdct</a></li>
<li>paper_authors: Tianan Li, Zhehan Chen, Huan Liu, Chen Wang</li>
<li>for: 这篇论文的目的是提出一种快速的深度完成框架，用于处理透明物体的RGB-D图像。</li>
<li>methods: 该方法使用了一种新的拼接分支和短circuit来抓取低级特征，并使用了一种损失函数来抑制过拟合。</li>
<li>results: 对比之前的方法，该方法可以在70帧&#x2F;秒的速度下提供更高精度的深度修复结果，并且可以改善物体抓取任务中的姿态估计。Here’s the full summary in Simplified Chinese:</li>
<li>for: 这篇论文的目的是提出一种快速的深度完成框架，用于处理透明物体的RGB-D图像。</li>
<li>methods: 该方法使用了一种新的拼接分支和短circuit来抓取低级特征，并使用了一种损失函数来抑制过拟合。</li>
<li>results: 对比之前的方法，该方法可以在70帧&#x2F;秒的速度下提供更高精度的深度修复结果，并且可以改善物体抓取任务中的姿态估计。I hope that helps!<details>
<summary>Abstract</summary>
Depth completion is crucial for many robotic tasks such as autonomous driving, 3-D reconstruction, and manipulation. Despite the significant progress, existing methods remain computationally intensive and often fail to meet the real-time requirements of low-power robotic platforms. Additionally, most methods are designed for opaque objects and struggle with transparent objects due to the special properties of reflection and refraction. To address these challenges, we propose a Fast Depth Completion framework for Transparent objects (FDCT), which also benefits downstream tasks like object pose estimation. To leverage local information and avoid overfitting issues when integrating it with global information, we design a new fusion branch and shortcuts to exploit low-level features and a loss function to suppress overfitting. This results in an accurate and user-friendly depth rectification framework which can recover dense depth estimation from RGB-D images alone. Extensive experiments demonstrate that FDCT can run about 70 FPS with a higher accuracy than the state-of-the-art methods. We also demonstrate that FDCT can improve pose estimation in object grasping tasks. The source code is available at https://github.com/Nonmy/FDCT
</details>
<details>
<summary>摘要</summary>
深度完成是许多 робо类任务的关键，如自动驾驶、3D重建和机械 manipulate。尽管已有很大的进步，现有方法仍然具有计算投入性和时间约束，并且大多数方法只适用于不透明物体，对于透明物体呈现特殊的反射和折射特性具有困难。为解决这些挑战，我们提出了高速深度完成框架 для透明物体（FDCT），该框架还有利于下游任务如对象 pose 估计。为了利用本地信息和避免过拟合问题，我们设计了新的融合分支和短cut 来利用低级特征，并设计了一个损失函数来抑制过拟合。这导致了一个准确和用户友好的深度修正框架，可以从RGB-D图像中恢复精密的深度估计。广泛的实验表明，FDCT 可以在70 FPS 下运行，并且与现状态艺术方法相比具有更高的准确率。我们还示出了 FDCT 可以改善对象抓取任务中的姿态估计。源代码可以在 <https://github.com/Nonmy/FDCT> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Context-Perception-Parallel-Decoder-for-Scene-Text-Recognition"><a href="#Context-Perception-Parallel-Decoder-for-Scene-Text-Recognition" class="headerlink" title="Context Perception Parallel Decoder for Scene Text Recognition"></a>Context Perception Parallel Decoder for Scene Text Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12270">http://arxiv.org/abs/2307.12270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Chenxia Li, Yuning Du, Yu-Gang Jiang</li>
<li>for: 这篇论文主要研究Scene Text Recognition（STR）方法的高精度和快速推断问题。</li>
<li>methods: 本文提出了一种新的AR模型，并进行了实验研究，发现AR模型的成功不仅归功于语言模型，还归功于视觉上下文的感知。为此，本文提出了Context Perception Parallel Decoder（CPPD）模型，通过计算字符出现频率和字符顺序来提供上下文信息，并且与字符预测任务结合，以准确地推断字符序列。</li>
<li>results: 实验结果表明，CPPD模型在英文和中文benchmark上达到了非常竞争性的准确率，并且比AR模型快约7倍。此外，CPPD模型也是当前最快的recognizer之一。代码将很快发布。<details>
<summary>Abstract</summary>
Scene text recognition (STR) methods have struggled to attain high accuracy and fast inference speed. Autoregressive (AR)-based STR model uses the previously recognized characters to decode the next character iteratively. It shows superiority in terms of accuracy. However, the inference speed is slow also due to this iteration. Alternatively, parallel decoding (PD)-based STR model infers all the characters in a single decoding pass. It has advantages in terms of inference speed but worse accuracy, as it is difficult to build a robust recognition context in such a pass. In this paper, we first present an empirical study of AR decoding in STR. In addition to constructing a new AR model with the top accuracy, we find out that the success of AR decoder lies also in providing guidance on visual context perception rather than language modeling as claimed in existing studies. As a consequence, we propose Context Perception Parallel Decoder (CPPD) to decode the character sequence in a single PD pass. CPPD devises a character counting module and a character ordering module. Given a text instance, the former infers the occurrence count of each character, while the latter deduces the character reading order and placeholders. Together with the character prediction task, they construct a context that robustly tells what the character sequence is and where the characters appear, well mimicking the context conveyed by AR decoding. Experiments on both English and Chinese benchmarks demonstrate that CPPD models achieve highly competitive accuracy. Moreover, they run approximately 7x faster than their AR counterparts, and are also among the fastest recognizers. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
Scene文本识别（STR）方法一直受到高精度和快速推理速度的限制。基于排序（AR）的STR模型利用已经识别的字符来逐个解码下一个字符，其精度较高。然而，推理速度又相对较慢，主要因为这种迭代过程。相反，并行推理（PD）基于STR模型在单个推理过程中推理所有字符，它在推理速度方面具有优势，但精度相对较差，因为在这种情况下难以建立强大的识别Context。在这篇论文中，我们首先进行了AR推理在STR方面的实验研究。此外，我们还构建了一个新的AR模型，并发现AR推理的成功不仅归结于语言模型化，还需要VisualContext的感知。因此，我们提出了Context Perception并行推理器（CPPD），可以在单个PD过程中推理字符序列。CPPD包括字符出现频次预测模块和字符顺序预测模块。对于每个文本实例，前者预测每个字符的出现频次，而后者预测字符的顺序和占位符。与字符预测任务相结合，它们构建了一个Robust的识别Context，能够准确地表示文本序列和字符的位置，与AR推理准确相似。实验表明，CPPD模型在英文和中文标准套件上达到了非常竞争的精度水平，并且运行速度约为AR模型的7倍，同时也是推理速度最快的一部分。代码即将发布。
</details></li>
</ul>
<hr>
<h2 id="ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder"><a href="#ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder" class="headerlink" title="ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder"></a>ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12255">http://arxiv.org/abs/2307.12255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youzhi Liang, Wen Liang</li>
<li>for: 该文章的目的是提出一种轻量级和可靠的深度学习架构，用于解决小型互联网设备中的指纹图像噪声问题。</li>
<li>methods: 该架构基于差分卷积束的残差整合（Res-WCAE），包括两个Encoder和一个Decoder。image Encoder使用残差连接来保持细腻的空间特征，而wavelet Encoder使用卷积束来提取特征。</li>
<li>results: 对比多种现有方法，RES-WCAE在噪声水平较高的指纹图像权重级别上表现出优异性，特别是对于严重损坏的指纹图像。总的来说，RES-WCAE显示出了解决小型互联网设备中生物认证系统中的噪声问题的潜力。<details>
<summary>Abstract</summary>
The utilization of biometric authentication with pattern images is increasingly popular in compact Internet of Things (IoT) devices. However, the reliability of such systems can be compromised by image quality issues, particularly in the presence of high levels of noise. While state-of-the-art deep learning algorithms designed for generic image denoising have shown promise, their large number of parameters and lack of optimization for unique biometric pattern retrieval make them unsuitable for these devices and scenarios. In response to these challenges, this paper proposes a lightweight and robust deep learning architecture, the Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE) with a Kullback-Leibler divergence (KLD) regularization, designed specifically for fingerprint image denoising. Res-WCAE comprises two encoders - an image encoder and a wavelet encoder - and one decoder. Residual connections between the image encoder and decoder are leveraged to preserve fine-grained spatial features, where the bottleneck layer conditioned on the compressed representation of features obtained from the wavelet encoder using approximation and detail subimages in the wavelet-transform domain. The effectiveness of Res-WCAE is evaluated against several state-of-the-art denoising methods, and the experimental results demonstrate that Res-WCAE outperforms these methods, particularly for heavily degraded fingerprint images in the presence of high levels of noise. Overall, Res-WCAE shows promise as a solution to the challenges faced by biometric authentication systems in compact IoT devices.
</details>
<details>
<summary>摘要</summary>
互联网物联网（IoT）设备中的生物识别系统对于图像生物识别的使用越来越普遍。然而，这些系统的可靠性可能受到图像质量问题的影响，特别是在高水平的噪声存在下。现有的深度学习算法，设计用于普通图像数据的数据条件，即使在生物识别领域中显示了损害，但它们的参数数量过多，并且不适合专门适应生物识别图像的条件下进行数据条件。为了解决这些挑战，本文提出了一个轻量级和可靠的深度学习架构，即对应涡回图像条件的内积条件（KLD）调整的内积条件对称卷积数位对称（Res-WCAE）。Res-WCAE包括两个卷积数位：一个图像卷积数位和一个波лет卷积数位，以及一个解oder。内积条件组件使用对应涡回图像条件的条件对称卷积数位，以维持细节的空间特征。实验结果显示，Res-WCAE在多个州数据条件下与其他竞争方法进行比较，特别是在高水平的噪声存在下，Res-WCAE表现出色。总的来说，Res-WCAE具有优秀的应用潜力，用于生物识别系统中的图像数据条件。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Depression-Detection-via-Head-Motion-Patterns"><a href="#Explainable-Depression-Detection-via-Head-Motion-Patterns" class="headerlink" title="Explainable Depression Detection via Head Motion Patterns"></a>Explainable Depression Detection via Head Motion Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12241">http://arxiv.org/abs/2307.12241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Gahalawat, Raul Fernandez Rojas, Tanaya Guha, Ramanathan Subramanian, Roland Goecke</li>
<li>for: 检测抑郁症状</li>
<li>methods: 基于head motion数据的基本运动单元（kinemes）和机器学习方法</li>
<li>results: 头部运动模式是识别抑郁症状的有效标记，并且可以找到与先前发现的解释性kineme patrernsHere’s a more detailed explanation of each point:</li>
<li>for: The paper is written to detect depression using head motion data and machine learning methods.</li>
<li>methods: The paper uses two approaches to analyze head motion data: (a) discovering kinemes from head motion data of both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls and computing statistics derived from reconstruction errors for both the patient and control classes. The paper employs machine learning methods to evaluate depression classification performance on two datasets: BlackDog and AVEC2013.</li>
<li>results: The paper finds that head motion patterns are effective biomarkers for detecting depressive symptoms, and that explanatory kineme patterns consistent with prior findings can be observed for the two classes. The paper achieves peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic thin-slices, and a peak F1 of 0.72 over videos for AVEC2013.<details>
<summary>Abstract</summary>
While depression has been studied via multimodal non-verbal behavioural cues, head motion behaviour has not received much attention as a biomarker. This study demonstrates the utility of fundamental head-motion units, termed \emph{kinemes}, for depression detection by adopting two distinct approaches, and employing distinctive features: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes. Employing machine learning methods, we evaluate depression classification performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings indicate that: (1) head motion patterns are effective biomarkers for detecting depressive symptoms, and (2) explanatory kineme patterns consistent with prior findings can be observed for the two classes. Overall, we achieve peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over videos for AVEC2013.
</details>
<details>
<summary>摘要</summary>
在识别抑郁症的研究中，脑部运动尚未得到过多的关注，作为生物标志。这项研究表明了基本头部运动单元（kinemes）在抑郁检测中的有用性，通过采用两种不同的方法和特征：（a）从头部运动数据中挖掘出kinemes，并对both depressed patients和健康Control进行比较；（b）从健康Control中学习kineme模式，并计算来自重建错误的统计，用于分类patient和control类。通过机器学习方法，我们评估了在BlackDog和AVEC2013 datasets上的抑郁分类性能。我们发现：（1）头部运动模式是有效的生物标志，可以检测抑郁症状；（2）可以看到与之前发现相符的解释性kineme模式。总的来说，我们在BlackDog和AVEC2013上取得了最高的F1分数为0.79和0.82，并在AVEC2013上取得了最高的F1分数为0.72。
</details></li>
</ul>
<hr>
<h2 id="Learning-Dynamic-Query-Combinations-for-Transformer-based-Object-Detection-and-Segmentation"><a href="#Learning-Dynamic-Query-Combinations-for-Transformer-based-Object-Detection-and-Segmentation" class="headerlink" title="Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation"></a>Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12239">http://arxiv.org/abs/2307.12239</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bytedance/dq-det">https://github.com/bytedance/dq-det</a></li>
<li>paper_authors: Yiming Cui, Linjie Yang, Haichao Yu</li>
<li>for: 这种方法用于提高DETR基本模型的性能，包括对象检测、实例分割、精准分割和视频实例分割等多个任务。</li>
<li>methods: 该方法使用一个列表学习的检测查询来从变换器网络中提取信息，并学习预测图像中对象的位置和类别。然后，通过随机几何融合这些学习的查询来生成动态的融合查询，以更好地捕捉图像中对象的先前知识。</li>
<li>results: 通过使用我们的模ulated查询，DETR基本模型在多个任务上取得了一致和稳定的高性能。这些任务包括对象检测、实例分割、精准分割和视频实例分割等。<details>
<summary>Abstract</summary>
Transformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queries, named modulated queries, better capture the prior of object locations and categories in the different images. Equipped with our modulated queries, a wide range of DETR-based models achieve consistent and superior performance across multiple tasks including object detection, instance segmentation, panoptic segmentation, and video instance segmentation.
</details>
<details>
<summary>摘要</summary>
transformer-based 检测和分割方法使用一个学习的检测查询列表从 transformer 网络中获取信息并学习预测图像中的对象位置和类别。我们实际发现，随机几何的检测查询仍然可以为相应的模型提供好的性能。然后，我们提议通过图像的高级 semantics 学习动态的查询组合，称为modulated queries。这些生成的动态查询更好地捕捉图像中对象的先驱知识，使得各种基于 DE TR 的模型在多个任务上具有一致性和superior的表现，包括对象检测、实例分割、泛化分割和视频实例分割。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO"><a href="#Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO" class="headerlink" title="Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO"></a>Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12236">http://arxiv.org/abs/2307.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longxiang Zhang, Wenping Wang</li>
<li>for: 这种研究是为了评估在视频流处理中的游戏技能，以便向流服务提供者提供个性化推荐和服务促销。</li>
<li>methods: 该研究使用了最新的终端模型，以学习多Modalities的联合表示。在数据集中，研究人员首先识别数据集中的漏洞，然后手动清理数据。</li>
<li>results: 经过广泛的实验，研究人员证明了他们的提议的有效性。然而，研究人员还发现了他们的模型偏向用户标识而不是学习有意义的表示。<details>
<summary>Abstract</summary>
Online streaming is an emerging market that address much attention. Assessing gaming skills from videos is an important task for streaming service providers to discover talented gamers. Service providers require the information to offer customized recommendation and service promotion to their customers. Meanwhile, this is also an important multi-modal machine learning tasks since online streaming combines vision, audio and text modalities. In this study we begin by identifying flaws in the dataset and proceed to clean it manually. Then we propose several variants of latest end-to-end models to learn joint representation of multiple modalities. Through our extensive experimentation, we demonstrate the efficacy of our proposals. Moreover, we identify that our proposed models is prone to identifying users instead of learning meaningful representations. We purpose future work to address the issue in the end.
</details>
<details>
<summary>摘要</summary>
在线串流是一个崛起的市场，吸引了大量的注意力。从视频中评估玩家技巧是串流服务提供商们需要了解潜在的才华玩家的重要任务。服务提供商需要这些信息以为客户提供个性化推荐和服务促销。同时，这也是一个重要的多modal机器学习任务，因为在线串流结合了视觉、音频和文本模式。在这个研究中，我们开始通过手动清理数据集来发现问题，然后我们提出了多种最新的端到端模型，以学习多Modalities的联合表示。通过我们的广泛的实验，我们证明了我们的提议的效果。此外，我们发现我们的提议模型很容易被用户的身份识别，而不是学习有意义的表示。我们未来的工作是解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms"><a href="#EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms" class="headerlink" title="EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms"></a>EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12229">http://arxiv.org/abs/2307.12229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masoudmo/echoglad">https://github.com/masoudmo/echoglad</a></li>
<li>paper_authors: Masoud Mokhtari, Mobina Mahdavi, Hooman Vaseli, Christina Luong, Purang Abolmaesumi, Teresa S. M. Tsang, Renjie Liao</li>
<li>For: The paper aims to automate the task of detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle in the heart, using machine learning.* Methods: The proposed method uses an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection, which includes a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs, and induced hierarchical supervision at different levels of granularity using a multi-level loss.* Results: The paper achieves the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on two datasets under the in-distribution (ID) setting, and shows better out-of-distribution (OOD) generalization than prior works with a testing MAE of 4.3 mm.<details>
<summary>Abstract</summary>
The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details>
<details>
<summary>摘要</summary>
左心室功能评估需要检测四个标志点和测量左心室内部维度以及周围肌肉的约重。难点在机器学习自动化这个任务是严重缺乏临床标注，即只有一些标注像素在高维度图像中，导致许多前作重要关注于均勋标注。然而，这种标注策略忽视图像的解剖信息并且带来一定偏见。为解决这个挑战，我们提出了一种用电子心室图像（EchoGLAD）来检测左心室标志点的模型。我们的主要贡献包括：1. 基于层次图像表示学习框架，通过层次graph neural network（GNN）实现多分辨率标志点检测。2. 通过多级监督，在不同的级别上实现层次监督。我们在公共和私人数据集上进行了测试，在内 distribuition（ID）和外部 distribuition（OOD）两种设置下。在ID设置下，我们实现了左心室标志点检测的状态机器学习（SOTA）的mean absolute error（MAE）为1.46mm和1.86mm。我们的模型还在OOD设置下表现更好，测试MAE为4.3mm。
</details></li>
</ul>
<hr>
<h2 id="The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery"><a href="#The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery" class="headerlink" title="The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery"></a>The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02502">http://arxiv.org/abs/2308.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Keith Wilkinson</li>
<li>For: The paper aims to investigate the use of artificial intelligence techniques and satellite imagery to identify illegal garbage dumps in rural areas of Cyprus.* Methods: The paper uses a novel dataset of images, data augmentation techniques, and an artificial neural network (specifically, a convolutional neural network) to recognize the presence or absence of garbage in new images.* Results: The resulting deep learning model can correctly identify images containing garbage in approximately 90% of cases, and could form the basis of a future system for systematically analyzing the entire landscape of Cyprus to build a comprehensive “garbage” map of the island.Here are the three points in Simplified Chinese text:* For: 这篇论文目标是使用人工智能技术和卫星影像来识别资产报废在cyprus的农村地区。* Methods: 论文使用了一个新的图像集，数据增强技术和人工神经网络来识别新图像中是否包含垃圾。* Results: 结果显示，使用这种方法可以在约90%的情况下正确地识别垃圾图像，并可能成为未来Cyprus岛上系统性地分析整个地图的基础。<details>
<summary>Abstract</summary>
Garbage disposal is a challenging problem throughout the developed world. In Cyprus, as elsewhere, illegal ``fly-tipping" is a significant issue, especially in rural areas where few legal garbage disposal options exist. However, there is a lack of studies that attempt to measure the scale of this problem, and few resources available to address it. A method of automating the process of identifying garbage dumps would help counter this and provide information to the relevant authorities. The aim of this study was to investigate the degree to which artificial intelligence techniques, together with satellite imagery, can be used to identify illegal garbage dumps in the rural areas of Cyprus. This involved collecting a novel dataset of images that could be categorised as either containing, or not containing, garbage. The collection of such datasets in sufficient raw quantities is time consuming and costly. Therefore a relatively modest baseline set of images was collected, then data augmentation techniques used to increase the size of this dataset to a point where useful machine learning could occur. From this set of images an artificial neural network was trained to recognise the presence or absence of garbage in new images. A type of neural network especially suited to this task known as ``convolutional neural networks" was used. The efficacy of the resulting model was evaluated using an independently collected dataset of test images. The result was a deep learning model that could correctly identify images containing garbage in approximately 90\% of cases. It is envisaged that this model could form the basis of a future system that could systematically analyse the entire landscape of Cyprus to build a comprehensive ``garbage" map of the island.
</details>
<details>
<summary>摘要</summary>
垃圾处理是发达国家的一个挑战，在塞浦路斯也是如此。非法投射（fly-tipping）是一个严重的问题，尤其在农村地区，因为有限的法定垃圾处理选择。然而，有很少的研究 Trying to measure the scale of this problem and few resources available to address it. This study aimed to investigate the extent to which artificial intelligence techniques, combined with satellite imagery, can be used to identify illegal garbage dumps in rural areas of Cyprus.To do this, we collected a novel dataset of images that could be categorized as either containing or not containing garbage. However, collecting such datasets in large quantities is time-consuming and costly, so we used data augmentation techniques to increase the size of our dataset to a point where useful machine learning could occur. We then trained an artificial neural network on this dataset to recognize the presence or absence of garbage in new images. We used a type of neural network well-suited to this task, called convolutional neural networks (CNNs).We evaluated the efficacy of our resulting model using an independently collected dataset of test images. The model was able to correctly identify images containing garbage in approximately 90% of cases. We envision this model as the basis for a future system that could systematically analyze the entire landscape of Cyprus to create a comprehensive "garbage" map of the island.
</details></li>
</ul>
<hr>
<h2 id="ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising"><a href="#ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising" class="headerlink" title="ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising"></a>ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12225">http://arxiv.org/abs/2307.12225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao1635/ASCON">https://github.com/hao1635/ASCON</a></li>
<li>paper_authors: Zhihao Chen, Qi Gao, Yi Zhang, Hongming Shan</li>
<li>for: 低剂量 computed tomography（CT）静止图像减噪</li>
<li>methods: 提出了一种新的 Anatomy-aware Supervised CONtrastive learning框架（ASCON），可以利用静止图像的生物学信息进行减噪，同时提供生物学解释。</li>
<li>results: 对两个公共的低剂量 CT 静止图像减噪数据集进行了广泛的实验，并证明了 ASCON 的超过状态艺术模型的性能。另外，ASCON 提供了低剂量 CT 静止图像减噪中的生物学解释，这是首次实现的。<details>
<summary>Abstract</summary>
While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.
</details>
<details>
<summary>摘要</summary>
而んどの深度学习方法已经被提出用于低剂量计算机断层成像（CT）去噪，大多数其中 leverages the normal-dose CT影像作为ground truth来监督去噪过程。这些方法通常忽略单个CT影像内的自然 correlation，特别是人体组织学的 semantics，而且缺乏去噪过程的解释性。在这篇论文中，我们提出了一种新的Anatomy-aware Supervised CONtrastive learning框架，称为ASCON，可以利用人体组织学来低剂量CT去噪，同时提供解释性。我们的ASCON包括两个新的设计：一种高效的自我注意力基于U-Net（ESAU-Net）和一种多尺度的组织学对比网络（MAC-Net）。首先，为了更好地捕捉全局-地方交互和适应高分辨率输入，我们使用了通道级别的自我注意力机制。其次，MAC-Net包括一个patch-wise非对比模块，用于捕捉内在的组织信息，以及一个像素级别的对比模块，用于保持内在的组织一致性。我们对公共的两个低剂量CT去噪数据集进行了广泛的实验，结果显示ASCON在状态机器上表现出了superior性。吸取onders, our ASCON为低剂量CT去噪提供了解释性，这是首次。源代码可以在https://github.com/hao1635/ASCON中获取。
</details></li>
</ul>
<hr>
<h2 id="LoLep-Single-View-View-Synthesis-with-Locally-Learned-Planes-and-Self-Attention-Occlusion-Inference"><a href="#LoLep-Single-View-View-Synthesis-with-Locally-Learned-Planes-and-Self-Attention-Occlusion-Inference" class="headerlink" title="LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference"></a>LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12217">http://arxiv.org/abs/2307.12217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Wang, Yu-Ping Wang, Dinesh Manocha</li>
<li>for: 本研究旨在提出一种新的方法，即LoLep，可以从单个RGB图像中推断高精度的场景表示，并生成更好的新视图。</li>
<li>methods: 为解决不具有深度信息的情况下推断合适的平面位置的问题，我们采用了预分 partition disparity 空间为 bins，并设计了一种disparity sampler来推断多个平面在每个bin中的本地偏移。此外，我们还提出了两种优化策略，其中一种是与不同的 disparity distribution 集合结合，另一种是在不同的 dataset 上添加 occlusion-aware reprojection loss 作为简单 yet effective 的 геометрической监视技术。</li>
<li>results: 我们的方法可以生成高精度的场景表示，并在不同的数据集上达到了领先的状态。与MINE相比，我们的方法具有 LPIPS 减少量为4.8%-9.0%和 RV 减少量为73.9%-83.5%。此外，我们还评估了实际图像上的性能，并证明了 LoLep 的优势。<details>
<summary>Abstract</summary>
We propose a novel method, LoLep, which regresses Locally-Learned planes from a single RGB image to represent scenes accurately, thus generating better novel views. Without the depth information, regressing appropriate plane locations is a challenging problem. To solve this issue, we pre-partition the disparity space into bins and design a disparity sampler to regress local offsets for multiple planes in each bin. However, only using such a sampler makes the network not convergent; we further propose two optimizing strategies that combine with different disparity distributions of datasets and propose an occlusion-aware reprojection loss as a simple yet effective geometric supervision technique. We also introduce a self-attention mechanism to improve occlusion inference and present a Block-Sampling Self-Attention (BS-SA) module to address the problem of applying self-attention to large feature maps. We demonstrate the effectiveness of our approach and generate state-of-the-art results on different datasets. Compared to MINE, our approach has an LPIPS reduction of 4.8%-9.0% and an RV reduction of 73.9%-83.5%. We also evaluate the performance on real-world images and demonstrate the benefits.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新方法，LoLep，该方法从单个RGB图像中回归地方精度的计划，以便更加准确地表示场景，并生成更好的新视图。在不知道深度信息的情况下，回归合适的平面位置是一个具有挑战性的问题。为解决这个问题，我们先对差分空间进行预分区，并设计了差分抽样器来回归多个平面在每个分区中的本地偏移。然而，只使用这种抽样器将网络训练不整合;我们还提出了两种优化策略，其中一种是基于不同差分分布的数据集的优化策略，另一种是一种简单 yet有效的干扰抑制损失技术。我们还引入了自注意机制，以改善干扰推断，并提出了块抽样自注意模块（BS-SA），以解决应用自注意到大特征地图时存在的问题。我们证明了我们的方法的有效性，并在不同的数据集上达到了领先的结果。相比于MINE，我们的方法有LPIPS减少4.8%-9.0%和RV减少73.9%-83.5%。我们还评估了实际图像上的性能，并证明了其利好。
</details></li>
</ul>
<hr>
<h2 id="LIST-Learning-Implicitly-from-Spatial-Transformers-for-Single-View-3D-Reconstruction"><a href="#LIST-Learning-Implicitly-from-Spatial-Transformers-for-Single-View-3D-Reconstruction" class="headerlink" title="LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction"></a>LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12194">http://arxiv.org/abs/2307.12194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Samiul Arshad, William J. Beksi</li>
<li>for: 用于重构3D对象的几何和 topological结构 from a single 2D图像</li>
<li>methods: 利用本地和全局图像特征，通过一种新的神经网络架构来准确重构3D对象的几何和 topological结构</li>
<li>results: 对比现有方法，本研究的模型能够更高精度地重构3D对象的几何和 topological结构，不需要摄像头估计或像素对齐。<details>
<summary>Abstract</summary>
Accurate reconstruction of both the geometric and topological details of a 3D object from a single 2D image embodies a fundamental challenge in computer vision. Existing explicit/implicit solutions to this problem struggle to recover self-occluded geometry and/or faithfully reconstruct topological shape structures. To resolve this dilemma, we introduce LIST, a novel neural architecture that leverages local and global image features to accurately reconstruct the geometric and topological structure of a 3D object from a single image. We utilize global 2D features to predict a coarse shape of the target object and then use it as a base for higher-resolution reconstruction. By leveraging both local 2D features from the image and 3D features from the coarse prediction, we can predict the signed distance between an arbitrary point and the target surface via an implicit predictor with great accuracy. Furthermore, our model does not require camera estimation or pixel alignment. It provides an uninfluenced reconstruction from the input-view direction. Through qualitative and quantitative analysis, we show the superiority of our model in reconstructing 3D objects from both synthetic and real-world images against the state of the art.
</details>
<details>
<summary>摘要</summary>
通过单个2D图像掌握3D物体的几何和拓扑细节是计算机视觉领域的基本挑战。现有的解决方案往往无法恢复自我遮盖的几何结构和/或准确地重建物体的拓扑形态。为解决这个问题，我们介绍了LIST，一种新的神经网络架构，它利用本地和全局图像特征来准确地从单个图像中重建3D物体的几何和拓扑结构。我们使用全局2D特征预测目标对象的抽象形状，然后使用它作为高分辨率重建的基础。通过利用图像中的本地特征和3D预测结果，我们可以使用隐式预测器来准确地预测目标表面上任意点的负距离。此外，我们的模型不需要相机估计或像素对齐。它可以从输入视图角度提供不受束缚的重建。通过质量和量化分析，我们证明了我们的模型在对真实图像和 sintetic图像进行3D物体重建时具有优势。
</details></li>
</ul>
<hr>
<h2 id="An-X3D-Neural-Network-Analysis-for-Runner’s-Performance-Assessment-in-a-Wild-Sporting-Environment"><a href="#An-X3D-Neural-Network-Analysis-for-Runner’s-Performance-Assessment-in-a-Wild-Sporting-Environment" class="headerlink" title="An X3D Neural Network Analysis for Runner’s Performance Assessment in a Wild Sporting Environment"></a>An X3D Neural Network Analysis for Runner’s Performance Assessment in a Wild Sporting Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12183">http://arxiv.org/abs/2307.12183</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Freire-Obregón, Javier Lorenzo-Navarro, Oliverio J. Santana, Daniel Hernández-Sosa, Modesto Castrillón-Santana</li>
<li>for: 这个研究是为了应用传输学习技术在运动环境中进行3D神经网络的分析。</li>
<li>methods: 该方法使用一个动作识别网络来估计长跑运动员的总赛时（CRT）。</li>
<li>results: 研究发现，使用X3D神经网络可以提供出色的表现，对于短视频输入， Mean Absolute Error为12分钟半。此外，X3D神经网络需要的内存比前一作少得多，可以达到更高的精度。<details>
<summary>Abstract</summary>
We present a transfer learning analysis on a sporting environment of the expanded 3D (X3D) neural networks. Inspired by action quality assessment methods in the literature, our method uses an action recognition network to estimate athletes' cumulative race time (CRT) during an ultra-distance competition. We evaluate the performance considering the X3D, a family of action recognition networks that expand a small 2D image classification architecture along multiple network axes, including space, time, width, and depth. We demonstrate that the resulting neural network can provide remarkable performance for short input footage, with a mean absolute error of 12 minutes and a half when estimating the CRT for runners who have been active from 8 to 20 hours. Our most significant discovery is that X3D achieves state-of-the-art performance while requiring almost seven times less memory to achieve better precision than previous work.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于转移学习的分析，涉及到扩展三维神经网络（X3D）环境中的运动环境。我们的方法使用动作识别网络来估算Runner在长跑比赛中的累累时间（CRT）。我们评估性能时考虑了X3D家族中的动作识别网络，该网络扩展了小于2D图像分类架构的维度，包括空间、时间、宽度和深度。我们表明，这种神经网络可以提供短输入视频时具有很好的性能，其中平均绝对误差为12分钟半，当估算 runner在8到20个小时内活动时。我们最重要的发现是，X3D可以达到现有最佳性能，而且需要只有七分之一的内存，以提高精度。
</details></li>
</ul>
<hr>
<h2 id="Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation"><a href="#Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation" class="headerlink" title="Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation"></a>Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12180">http://arxiv.org/abs/2307.12180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linzy0227/pdminet">https://github.com/linzy0227/pdminet</a></li>
<li>paper_authors: Yafei Zhang, Zhiyuan Li, Huafeng Li, Dapeng Tao</li>
<li>for: 这个论文的目的是提出一种多模态核磁共振（MR）脑肿瘤图像分割方法，以便更好地确定和地理位置脑肿瘤子区域。</li>
<li>methods: 该方法首先提取输入图像中的特征，然后使用脑肿瘤prototype来引导和融合不同模态特征，以便高亮每个脑肿瘤子区域的特征。</li>
<li>results: 实验结果表明，提出的方法在三个竞赛脑肿瘤分割数据集上具有更高的分割精度和稳定性。<details>
<summary>Abstract</summary>
For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion method with the learned prototypes, which implants the prototypes into tumor features and generates corresponding activation maps. With the activation maps, the sub-region features consistent with the prototype category can be highlighted. A key information enhancement and fusion strategy with multi-expert integration is designed to further improve the segmentation performance. The strategy can integrate the features from different layers of the extra feature extraction network and the features highlighted by the prototypes. Experimental results on three competition brain tumor segmentation datasets prove the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
现有的多Modal MR脑肿吸引图像分割方法通常直接从输入图像中提取出特征，以确定和 lokalisieren tumor sub-region。然而，信息抖动所引起的影响通常被忽略。此外，现有的方法通常不会采取特化的努力来强调单个 tumor sub-region 的特征。为此，我们提出了一种基于 tumor 原型的多Modal MR脑肿吸引图像分割方法。它可以在 tumor 原型的指导下强调每个 tumor sub-region 的特征。具体来说，为了获得完整的信息，我们提出了一种互传机制，将不同模态特征传递给每个模态，以解决单modal特征不具备的问题。此外，我们还提出了一种基于原型的特征表示和融合方法，通过learned原型来嵌入 tumor 特征，并生成对应的活化地图。通过活化地图，可以强调与原型类别相符的子区域特征。为了进一步提高分割性能，我们还设计了一种多 экспер特性融合策略。该策略可以将不同层次的特征和由原型引导的特征融合在一起。实验结果表明，我们的提议方法在三个竞赛脑肿吸引图像分割数据集上具有优越性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Knowledge-Graphs-for-Zero-Shot-Object-agnostic-State-Classification"><a href="#Leveraging-Knowledge-Graphs-for-Zero-Shot-Object-agnostic-State-Classification" class="headerlink" title="Leveraging Knowledge Graphs for Zero-Shot Object-agnostic State Classification"></a>Leveraging Knowledge Graphs for Zero-Shot Object-agnostic State Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12179">http://arxiv.org/abs/2307.12179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filipos Gouidis, Theodore Patkos, Antonis Argyros, Dimitris Plexousakis</li>
<li>for: 本研究强调解决对象状态分类（OSC）问题，具体来说是一种零例学习问题，即不需要知道对象的类别来预测对象的状态。</li>
<li>methods: 我们提出了首个对象agnosticState Classification（OaSC）方法，即不需要对象类知识或估计来预测对象的状态。我们利用知识 graphs（KGs）来结构化和组织知识，并与视觉信息结合，以便在对象&#x2F;状态对的训练集中未经遭遇的情况下预测对象的状态。</li>
<li>results: 实验结果表明，对象类知识不是预测对象状态的决定因素。此外，我们的OaSC方法在所有数据集和benchmark中都超越了现有方法，差距很大。<details>
<summary>Abstract</summary>
We investigate the problem of Object State Classification (OSC) as a zero-shot learning problem. Specifically, we propose the first Object-agnostic State Classification (OaSC) method that infers the state of a certain object without relying on the knowledge or the estimation of the object class. In that direction, we capitalize on Knowledge Graphs (KGs) for structuring and organizing knowledge, which, in combination with visual information, enable the inference of the states of objects in object/state pairs that have not been encountered in the method's training set. A series of experiments investigate the performance of the proposed method in various settings, against several hypotheses and in comparison with state of the art approaches for object attribute classification. The experimental results demonstrate that the knowledge of an object class is not decisive for the prediction of its state. Moreover, the proposed OaSC method outperforms existing methods in all datasets and benchmarks by a great margin.
</details>
<details>
<summary>摘要</summary>
我们研究对象状态分类（OSC）问题，并将其视为零例学习问题。具体来说，我们提出了首个对象不依赖类别知识的对象状态分类方法（OaSC）。这种方法可以基于知识图（KGs）结构和组织知识，并结合视觉信息，对未在方法训练集中出现的对象/状态对进行状态推理。我们进行了一系列实验，测试方法在不同的设置、假设和现有的对象属性分类方法的比较中的性能。实验结果表明，对象类知识不是决定对象状态预测的关键因素。此外，我们的OaSC方法在所有数据集和标准准则上都超越了现有方法，差距很大。
</details></li>
</ul>
<hr>
<h2 id="Challenges-for-Monocular-6D-Object-Pose-Estimation-in-Robotics"><a href="#Challenges-for-Monocular-6D-Object-Pose-Estimation-in-Robotics" class="headerlink" title="Challenges for Monocular 6D Object Pose Estimation in Robotics"></a>Challenges for Monocular 6D Object Pose Estimation in Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12172">http://arxiv.org/abs/2307.12172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Thalhammer, Dominik Bauer, Peter Hönig, Jean-Baptiste Weibel, José García-Rodríguez, Markus Vincze</li>
<li>for: 本研究旨在探讨单视模式下的物体 pose 估算问题，即 robotics 应用中的核心识别任务。</li>
<li>methods: 该研究使用了现成的 RGB 感知器和 CNN 进行快速推理，以及提出了一种综合视角和数据集的评估方法。</li>
<li>results: 研究发现，虽然现有的多Modal 和单视方法已经达到了 estado del arte，但是它们在 robotics 应用中仍面临着 occlusion 处理、新的 pose 表示方法、Category-level pose estimation 的形式化和改进等挑战。此外，大量对象集、新型对象、干涉物质、不确定性估计等问题仍未得到解决。<details>
<summary>Abstract</summary>
Object pose estimation is a core perception task that enables, for example, object grasping and scene understanding. The widely available, inexpensive and high-resolution RGB sensors and CNNs that allow for fast inference based on this modality make monocular approaches especially well suited for robotics applications. We observe that previous surveys on object pose estimation establish the state of the art for varying modalities, single- and multi-view settings, and datasets and metrics that consider a multitude of applications. We argue, however, that those works' broad scope hinders the identification of open challenges that are specific to monocular approaches and the derivation of promising future challenges for their application in robotics. By providing a unified view on recent publications from both robotics and computer vision, we find that occlusion handling, novel pose representations, and formalizing and improving category-level pose estimation are still fundamental challenges that are highly relevant for robotics. Moreover, to further improve robotic performance, large object sets, novel objects, refractive materials, and uncertainty estimates are central, largely unsolved open challenges. In order to address them, ontological reasoning, deformability handling, scene-level reasoning, realistic datasets, and the ecological footprint of algorithms need to be improved.
</details>
<details>
<summary>摘要</summary>
We argue that there are still several fundamental challenges that need to be addressed in order to improve the performance of monocular object pose estimation in robotics. These challenges include occlusion handling, the development of novel pose representations, and the formalization and improvement of category-level pose estimation. Additionally, there are several central, largely unsolved open challenges that must be addressed, including the need for large object sets, the ability to handle novel objects and refractive materials, and the inclusion of uncertainty estimates.To address these challenges, we propose several areas of improvement, including ontological reasoning, deformability handling, scene-level reasoning, the creation of realistic datasets, and the reduction of the ecological footprint of algorithms. By focusing on these areas, we believe that the performance of monocular object pose estimation in robotics can be significantly improved, enabling more advanced and capable robots.
</details></li>
</ul>
<hr>
<h2 id="Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification"><a href="#Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification" class="headerlink" title="Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification"></a>Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12159">http://arxiv.org/abs/2307.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nícolas Barbosa Gomes, Arissa Yoshida, Mateus Roder, Guilherme Camargo de Oliveira, João Paulo Papa</li>
<li>for: 早期诊断阿LS（amyotrophic lateral sclerosis）有助于确定治疗的开始、改善病人的前途和整体健康状况。</li>
<li>methods: 该论文提出使用计算机方法分析病人的脸部表达来自动识别阿LS。</li>
<li>results: 实验结果表明，该方法在渥太华神经面数据集上表现出色，超过了现有的最佳成绩，为早期诊断阿LS带来了有前途的发展。<details>
<summary>Abstract</summary>
Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
</details>
<details>
<summary>摘要</summary>
早期诊断阿底士病（ALS）对患者的治疗结果有着重要的影响，能够提高生活质量和整体健康状况。然而，早期诊断和识别病种的症状不是一件容易的事情。这项研究提出使用计算机方法分析病人的面部表达来自动识别ALS。研究发现，当患者进行特定的动作时，如开口嘴巴，健康人的面部肌肉运动与ALS患者不同。该研究使用面部点Graph学习face图像的几何信息，并在渥太华神经面数据集上进行实验，结果表明该方法在比较当前的结果之上出色，这些结果表明了这种方法在诊断ALS方面的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices"><a href="#Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices" class="headerlink" title="Real-Time Neural Video Recovery and Enhancement on Mobile Devices"></a>Real-Time Neural Video Recovery and Enhancement on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12152">http://arxiv.org/abs/2307.12152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Lili Qiu, Kyoungjun Park</li>
<li>for: 提高移动设备上视频流式传输的流畅体验</li>
<li>methods: 提出一种新的视频帧恢复算法、一种新的超分辨率算法和一种接受器增强视频比特率调整算法</li>
<li>results: 实现了30帧&#x2F;秒的实时增强，在不同的网络环境下测试，实现了视频流经验质量（Quality of Experience，QoE）的显著提高（24%-82%）<details>
<summary>Abstract</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.   To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24\% - 82\% in our video streaming system.
</details>
<details>
<summary>摘要</summary>
“随着移动设备在影像流媒体中的普及，实时优化影像流媒体的体验成为了非常重要的。深度学习基本的影像增强技术在获得注目，但大多数这些技术无法在移动设备上支持实时优化。此外，许多这些技术仅专注于超解析，而无法处理部分或完全的影像帧损失或腐败，这是互联网和无线网络上很常见的问题。”“为了解决这些挑战，我们在这篇论文中提出了一个新的方法。我们的方法包括：（i）一个新的影像帧恢复算法，（ii）一个新的超解析算法，以及（iii）一个受到接收端优化影像比特率改变算法的影像流媒体实时优化系统。我们在iPhone 12上实现了我们的方法，并且可以支持30帧每秒（FPS）。我们在WiFi、3G、4G和5G网络中进行了评估，我们的评估结果表明，我们的方法可以实现实时优化，并且导致影像流媒体系统中的影像质量经验（Quality of Experience，QoE）增加了24%-82%。”
</details></li>
</ul>
<hr>
<h2 id="Does-color-modalities-affect-handwriting-recognition-An-empirical-study-on-Persian-handwritings-using-convolutional-neural-networks"><a href="#Does-color-modalities-affect-handwriting-recognition-An-empirical-study-on-Persian-handwritings-using-convolutional-neural-networks" class="headerlink" title="Does color modalities affect handwriting recognition? An empirical study on Persian handwritings using convolutional neural networks"></a>Does color modalities affect handwriting recognition? An empirical study on Persian handwritings using convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12150">http://arxiv.org/abs/2307.12150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abbas Zohrevand, Zahra Imani, Javad Sadri, Ching Y. Suen</li>
<li>for: 这篇论文是 investigate whether color modalities of handwritten digits and words affect their recognition accuracy or speed.</li>
<li>methods: 使用 Convolutional Neural Networks (CNNs) 作为眼动模拟器，在一个新的波斯语手写数据库中进行测试。</li>
<li>results: 结果表明，使用 CNN 对黑白字体和字符图像进行训练后，对测试集进行识别时，比其他两种颜色模式具有更高的性能。然而，在不同颜色模式下的训练时间比较，发现使用 BW 图像进行训练是最效率的。<details>
<summary>Abstract</summary>
Most of the methods on handwritten recognition in the literature are focused and evaluated on Black and White (BW) image databases. In this paper we try to answer a fundamental question in document recognition. Using Convolutional Neural Networks (CNNs), as eye simulator, we investigate to see whether color modalities of handwritten digits and words affect their recognition accuracy or speed? To the best of our knowledge, so far this question has not been answered due to the lack of handwritten databases that have all three color modalities of handwritings. To answer this question, we selected 13,330 isolated digits and 62,500 words from a novel Persian handwritten database, which have three different color modalities and are unique in term of size and variety. Our selected datasets are divided into training, validation, and testing sets. Afterwards, similar conventional CNN models are trained with the training samples. While the experimental results on the testing set show that CNN on the BW digit and word images has a higher performance compared to the other two color modalities, in general there are no significant differences for network accuracy in different color modalities. Also, comparisons of training times in three color modalities show that recognition of handwritten digits and words in BW images using CNN is much more efficient.
</details>
<details>
<summary>摘要</summary>
大多数现成的手写识别方法都是在黑白图像库中进行研究和评估。在这篇论文中，我们试图回答一个基本的问题：使用卷积神经网络（CNN）作为眼动模拟器，我们调查了手写数字和字符的三种颜色模式是否影响了识别精度或速度？至于这个问题，我们认为现有的手写库缺乏三种颜色模式的手写数据，因此这个问题尚未得到解答。为了回答这个问题，我们选择了13330个隔离的数字和62500个字符从一个新的波斯语手写库中，这些数据库具有三种颜色模式和各种大小和样式。我们选择的数据库被分成了训练集、验证集和测试集。然后，我们使用同样的传统CNN模型在训练样本上进行训练。在测试集上的实验结果表明，使用CNN对黑白数字和字符图像进行识别的精度较高，而在其他两种颜色模式下的识别精度相对较低。此外，在三种颜色模式下的训练时间进行比较，发现使用黑白图像进行识别的训练时间相对较短。
</details></li>
</ul>
<hr>
<h2 id="Learned-Gridification-for-Efficient-Point-Cloud-Processing"><a href="#Learned-Gridification-for-Efficient-Point-Cloud-Processing" class="headerlink" title="Learned Gridification for Efficient Point Cloud Processing"></a>Learned Gridification for Efficient Point Cloud Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14354">http://arxiv.org/abs/2307.14354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/computri/gridifier">https://github.com/computri/gridifier</a></li>
<li>paper_authors: Putri A. van der Linden, David W. Romero, Erik J. Bekkers</li>
<li>for: 将点云数据转换为可以进行操作的稠密grid数据，以提高操作效率和可扩展性。</li>
<li>methods: 提出了一种名为”learnable gridification”的方法，用于将点云数据转换为稠密grid数据，并在后续层使用常见的grid-based操作，如Conv3D。此外，还提出了一种名为”learnable de-gridification”的方法，用于将稠密grid数据还原回原始点云数据。</li>
<li>results: 经过理论和实验分析，显示了gridified网络在内存和时间方面的扩展性和可扩展性，而且可以达到竞争性的结果。<details>
<summary>Abstract</summary>
Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.   In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a learnable de-gridification step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that gridified networks scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at https://github.com/computri/gridifier.
</details>
<details>
<summary>摘要</summary>
神经操作需要邻域信息的时候在点云上比在网格数据上更加昂贵，因为点云中点的距离不规则。在网格上，我们可以一次计算核心，然后重复使用它们 для所有查询位置。因此，基于邻域信息的操作在点云上比网格数据更加慢速，尤其是对于大输入和大邻域。在这项工作中，我们解决了点云方法的扩展性问题，通过learned gridification来将点云转换成一个紧凑的、规则的网格。然后，我们扩展了gridification到点云到点云任务，例如分割，通过添加一个学习的de-gridification步骤来将紧凑的网格还原回原始点云形式。我们通过理论和实验分析表明，gridified网络在内存和时间方面比直接应用于原始点云数据更加高效，同时能够实现竞争性的结果。我们的代码可以在https://github.com/computri/gridifier上下载。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter"><a href="#A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter" class="headerlink" title="A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter"></a>A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12145">http://arxiv.org/abs/2307.12145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/river-lab/hyperspectral_macro_plastic_detection">https://github.com/river-lab/hyperspectral_macro_plastic_detection</a></li>
<li>paper_authors: Nathaniel Hanson, Ahmet Demirkaya, Deniz Erdoğmuş, Aron Stubbins, Taşkın Padır, Tales Imbiriba</li>
<li>for: 这个研究旨在解决水体中废弃 пласти克废弃物的监测问题，以提高当地生态和经济环境的健康性。</li>
<li>methods: 这个研究使用计算机成像技术来检测水体中的巨大废弃 пласти克废弃物。研究人员使用快照可见短波辐射 hyperspectral成像技术，并利用机器学习分类方法来实现高精度检测。</li>
<li>results: 实验结果表明，使用 hyperspectral 数据和非线性分类方法可以在具有挑战性的场景下实现高精度的检测精度，特别是在检测部分潜没的废弃 пласти克废弃物时。<details>
<summary>Abstract</summary>
Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
</details>
<details>
<summary>摘要</summary>
塑料垃圾进入河流环境会对当地生态系统造成负面影响，导致生态和经济 Both positive and negative impacts. Large amounts of plastic waste are transported from inland to oceans, causing a global problem of floating debris fields. In this context, efficient and automated monitoring of mismanaged plastic waste is crucial. To address this problem, we explore the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments show that imaging strategies combined with machine learning classification approaches can achieve high detection accuracy, especially in challenging scenarios, by leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online at <https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection>.Here's the breakdown of the translation:* "塑料垃圾" (plastic waste) is translated as "塑料垃圾" (plastic waste)* "进入河流环境" (entering the riverine environment) is translated as "进入河流环境" (entering the riverine environment)* "会对当地生态系统造成负面影响" (leading to negative ecological and economic impacts) is translated as "会对当地生态系统造成负面影响" (leading to negative ecological and economic impacts)* "Large amounts of plastic waste are transported from inland to oceans" is translated as "大量塑料垃圾从内陆流入海洋" (large amounts of plastic waste are transported from inland to oceans)* "causing a global problem of floating debris fields" is translated as "导致全球漂泊垃圾场的问题" (causing a global problem of floating debris fields)* "In this context, efficient and automated monitoring of mismanaged plastic waste is crucial" is translated as "在这种情况下，有效和自动化的塑料垃圾监测是关键" (in this context, efficient and automated monitoring of mismanaged plastic waste is crucial)* "To address this problem, we explore the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios" is translated as "为解决这个问题，我们在河流类场景中explore了计算成像方法的可能性" (to address this problem, we explore the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios)* "We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging" is translated as "我们通过使用快照可见短波infrared hyperspectral成像来实现近实时跟踪部分浸没在水中的塑料" (we enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging)* "Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy" is translated as "我们的实验表明，与机器学习分类方法相关的成像策略可以实现高的检测精度" (our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy)* "especially in challenging scenarios, by leveraging hyperspectral data and nonlinear classifiers" is translated as "特别是在挑战性的场景下，通过利用快照数据和非线性分类器来提高检测精度" (especially in challenging scenarios, by leveraging hyperspectral data and nonlinear classifiers)* "All code, data, and models are available online" is translated as "所有代码、数据和模型都可以在线获取" (all code, data, and models are available online)
</details></li>
</ul>
<hr>
<h2 id="SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images"><a href="#SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images" class="headerlink" title="SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images"></a>SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12138">http://arxiv.org/abs/2307.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Hongshan Liu, Xiaoyu Song, Brigitta C. Brott, Silvio H. Litovsky, Yu Gan</li>
<li>for: 对于摄影镜像胆管疾病的诊断和治疗提供虚拟 histological 信息</li>
<li>methods: 使用 transformer 生成数学类型的 GAN 模型，将 OCT 图像转换为虚拟显色 H&amp;E 厚比例图像</li>
<li>results: 实现了对摄影镜像胆管疾病的诊断和治疗中的虚拟 histological 信息生成，并且可以对应胆管疾病的病理特征Here’s the simplified Chinese text:</li>
<li>for: 为摄影镜像胆管疾病的诊断和治疗提供虚拟 histological 信息</li>
<li>methods: 使用 transformer 生成数学类型的 GAN 模型，将 OCT 图像转换为虚拟显色 H&amp;E 厚比例图像</li>
<li>results: 实现了对摄影镜像胆管疾病的诊断和治疗中的虚拟 histological 信息生成，并且可以对应胆管疾病的病理特征<details>
<summary>Abstract</summary>
There is a significant need for the generation of virtual histological information from coronary optical coherence tomography (OCT) images to better guide the treatment of coronary artery disease. However, existing methods either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions. To address these issues, we proposed a structural constrained, pathology aware, transformer generative adversarial network, namely SCPAT-GAN, to generate virtual stained H&E histology from OCT images. The proposed SCPAT-GAN advances existing methods via a novel design to impose pathological guidance on structural layers using transformer-based network.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> coronary optical coherence tomography (OCT) 图像中的虚拟 histological 信息的生成很需要用于更好地治疗 coronary artery disease。然而，现有的方法 Either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions。为解决这些问题，我们提出了一种基于 transformer 的 generator adversarial network，即 SCPAT-GAN，用于从 OCT 图像中生成虚拟染色 H&E  histology。我们的提议的 SCPAT-GAN 会使现有的方法得到一种新的设计，通过将 pathological guidance 添加到结构层以使用 transformer-based network。Here's the breakdown of the translation:*  coronary optical coherence tomography (OCT) 图像中的虚拟 histological 信息 (Virtual histological information in OCT images)* Either require a large pixel-wisely paired training dataset (Existing methods either require a large pixel-wisely paired training dataset)* or have limited capability to map pathological regions (or have limited capability to map pathological regions)* 用于更好地治疗 coronary artery disease (To better guide the treatment of coronary artery disease)* 基于 transformer 的 generator adversarial network (Based on transformer, a generator adversarial network)* 即 SCPAT-GAN (i.e., SCPAT-GAN)* 用于从 OCT 图像中生成虚拟染色 H&E  histology (To generate virtual stained H&E histology from OCT images)* 我们的提议的 SCPAT-GAN (Our proposed SCPAT-GAN)* 会使现有的方法得到一种新的设计 (Will make existing methods obtain a new design)* 通过将 pathological guidance 添加到结构层 (By adding pathological guidance to the structural layers)* 以使用 transformer-based network (To use transformer-based network)
</details></li>
</ul>
<hr>
<h2 id="Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks"><a href="#Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks" class="headerlink" title="Improving temperature estimation in low-cost infrared cameras using deep neural networks"></a>Improving temperature estimation in low-cost infrared cameras using deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12130">http://arxiv.org/abs/2307.12130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 提高低成本热相机的温度精度和修正不均匀性</li>
<li>methods: 开发了一个考虑 ambient temperature 的非均匀性模拟器，并提出了一种基于批处理神经网络的方法，通过使用单个图像和摄像头自身测量的 ambient temperature 来估算对象的温度和修正不均匀性</li>
<li>results: 比前一些方法更低 ($1^\circ C$) 的mean temperature error，并且通过Physical constraint 降低了错误率 ($4%$)。 总的来说，mean temperature error 在广泛验证集上为 $0.37^\circ C$，并在实际场景中得到了等效的结果。<details>
<summary>Abstract</summary>
Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have space-variant nonuniformity across their detector. Both inaccuracy and nonuniformity are dependent on the ambient temperature of the camera. The main goal of this work was to improve the temperature accuracy of low-cost cameras and rectify the nonuniformity.   A nonuniformity simulator that accounts for the ambient temperature was developed. An end-to-end neural network that incorporates the ambient temperature at image acquisition was introduced. The neural network was trained with the simulated nonuniformity data to estimate the object's temperature and correct the nonuniformity, using only a single image and the ambient temperature measured by the camera itself. Results show that the proposed method lowered the mean temperature error by approximately $1^\circ C$ compared to previous works. In addition, applying a physical constraint on the network lowered the error by an additional $4\%$.   The mean temperature error over an extensive validation dataset was $0.37^\circ C$. The method was verified on real data in the field and produced equivalent results.
</details>
<details>
<summary>摘要</summary>
低成本热相机的精度受到 ambient temperature 的影响（通常在 $\pm 3^\circ C$ 之间），并且具有空间不均的非均匀性，这两个问题都与热相机的环境温度相关。本工作的主要目标是提高低成本热相机的温度精度和修正非均匀性。我们开发了一个考虑 ambient temperature 的非均匀性模拟器，并引入了一个结合 ambient temperature 的末端到终端神经网络。这个神经网络通过使用模拟的非均匀数据进行训练，以估算对象的温度并修正非均匀性，只需要使用单个图像和摄像头自带的温度值。结果显示，我们的方法可以比前一些工作降低mean温度错误约 $1^\circ C$。此外，通过应用物理约束，降低错误约 $4\%$。整体来说，我们的方法在广泛验证数据集上的mean温度错误为 $0.37^\circ C$。此方法还在实际场景中进行验证，并获得相同的结果。
</details></li>
</ul>
<hr>
<h2 id="InFusion-Inject-and-Attention-Fusion-for-Multi-Concept-Zero-Shot-Text-based-Video-Editing"><a href="#InFusion-Inject-and-Attention-Fusion-for-Multi-Concept-Zero-Shot-Text-based-Video-Editing" class="headerlink" title="InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing"></a>InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.00135">http://arxiv.org/abs/2308.00135</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/infusion-zero-edit/InFusion">https://github.com/infusion-zero-edit/InFusion</a></li>
<li>paper_authors: Anant Khandelwal</li>
<li>for: 这篇论文的目的是提出一个框架，以便透过文本提示进行类型控制的视频编辑，并且在不需要训练的情况下实现高品质的视频编辑。</li>
<li>methods: 这篇论文使用了大型预训的文本扩散模型，并且提出了一个内部插入（Injection）的方法，允许在视频中编辑多个概念，并且具有像素级的控制。</li>
<li>results: 论文的实验结果显示，这个框架可以实现高品质和时间含意的视频编辑，并且可以与现有的图像扩散技术进行整合。<details>
<summary>Abstract</summary>
Large text-to-image diffusion models have achieved remarkable success in generating diverse, high-quality images. Additionally, these models have been successfully leveraged to edit input images by just changing the text prompt. But when these models are applied to videos, the main challenge is to ensure temporal consistency and coherence across frames. In this paper, we propose InFusion, a framework for zero-shot text-based video editing leveraging large pre-trained image diffusion models. Our framework specifically supports editing of multiple concepts with pixel-level control over diverse concepts mentioned in the editing prompt. Specifically, we inject the difference in features obtained with source and edit prompts from U-Net residual blocks of decoder layers. When these are combined with injected attention features, it becomes feasible to query the source contents and scale edited concepts along with the injection of unedited parts. The editing is further controlled in a fine-grained manner with mask extraction and attention fusion, which cut the edited part from the source and paste it into the denoising pipeline for the editing prompt. Our framework is a low-cost alternative to one-shot tuned models for editing since it does not require training. We demonstrated complex concept editing with a generalised image model (Stable Diffusion v1.5) using LoRA. Adaptation is compatible with all the existing image diffusion techniques. Extensive experimental results demonstrate the effectiveness of existing methods in rendering high-quality and temporally consistent videos.
</details>
<details>
<summary>摘要</summary>
大型文本到图像扩散模型已经实现了惊人的成功，可以生成多样化、高质量的图像。此外，这些模型还可以通过修改输入文本来编辑图像。但是在视频 editing 中，主要挑战是保证时间协调一致和各帧的一致性。在这篇论文中，我们提出了 InFusion 框架，一种基于大型预训练的图像扩散模型来实现零搅 diffusion 的文本基于视频编辑。我们的框架具有多个概念编辑、像素级控制的特点，可以在编辑提示中提出多个概念，并且可以在精细化的方式下控制编辑。具体来说，我们将源和编辑提示中的特征差拟合到 U-Net 径弧层的解码层中，并与注入的注意力特征相结合，使得可以查询源内容并将编辑概念扩大到不变的部分。此外，我们还使用抽取Mask和注意力融合来进一步控制编辑。我们的框架是一种低成本的替代方案，不需要训练。我们使用了 Stable Diffusion v1.5 总体图像模型进行复杂概念编辑，并证明了与现有图像扩散技术相容。我们的实验结果表明，InFusion 可以生成高质量和时间协调一致的视频。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network"><a href="#Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network" class="headerlink" title="Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network"></a>Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12122">http://arxiv.org/abs/2307.12122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/octadion/diffusion-stylegan2-ada-pytorch">https://github.com/octadion/diffusion-stylegan2-ada-pytorch</a></li>
<li>paper_authors: One Octadion, Novanto Yudistira, Diva Kurnianingtyas<br>for:本研究旨在帮助 batik 设计师或手工艺术家创造独特和高品质的 batik 模样，并实现有效率的生产时间和成本。methods:本研究使用了 StyleGAN2-Ada 和扩散技术，实现了生成高品质和真实的 synthetic batik 模样。 StyleGAN2-Ada 是一种分离 Style 和 Content 两个方面的 GAN 模型，而扩散技术则引入了随机噪音到数据中。results:根据质量和量度评估，模型测试过程中生成的 batik 模样具有细节丰富和艺术多样性，并且能够实现有效率的生产时间和成本。<details>
<summary>Abstract</summary>
Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs. Based on qualitative and quantitative evaluations, the results show that the model tested is capable of producing authentic and quality batik patterns, with finer details and rich artistic variations. The dataset and code can be accessed here:https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details>
<details>
<summary>摘要</summary>
《独特的抽象艺术——batik的研究》batik是印度尼西亚社会独特的艺术和手工艺术材料，研究主要集中在纹理的分类。然而，进一步的研究可能会扩展到纹理的合成。生成对抗网络（GANs）是深度学习模型，用于生成合成数据，但经常面临稳定性和一致性问题。本研究使用StyleGAN2-Ada和扩散技术生成真实和高质量的合成纹理图案。StyleGAN2-Ada是GAN模型中分离风格和内容的变种，而扩散技术引入随机噪声到数据中。在batik中，StyleGAN2-Ada和扩散被用生成真实的合成纹理图案。本研究还对模型结构进行了调整，使用了优化的batik数据集。主要目标是协助batik设计师或手工艺术家生成独特和高质量的纹理图案，减少生产时间和成本。根据质量和量度评价，结果显示，试用的模型能够生成authentic和高质量的纹理图案，细节更加细腻，艺术变化更加丰富。数据集和代码可以在以下链接获取：https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details></li>
</ul>
<hr>
<h2 id="Pyramid-Semantic-Graph-based-Global-Point-Cloud-Registration-with-Low-Overlap"><a href="#Pyramid-Semantic-Graph-based-Global-Point-Cloud-Registration-with-Low-Overlap" class="headerlink" title="Pyramid Semantic Graph-based Global Point Cloud Registration with Low Overlap"></a>Pyramid Semantic Graph-based Global Point Cloud Registration with Low Overlap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12116">http://arxiv.org/abs/2307.12116</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-aerial-robotics/pagor">https://github.com/hkust-aerial-robotics/pagor</a></li>
<li>paper_authors: Zhijian Qiao, Zehuan Yu, Huan Yin, Shaojie Shen</li>
<li>for: 这篇论文是关于全球点云注册的，用于绕过视点变化和 occlusion 等问题，以实现 loop closing 和 relocalization。</li>
<li>methods: 该论文提出了一种基于图论的全球点云注册方法，使用了 robust 的数据关联和可靠的姿态估计，以及semantic 信息来减少点云数据的精度。</li>
<li>results: 实验结果表明，该方法在自行收集的indoor数据集和公共的 KITTI 数据集上具有最高成功率，即使点云之间的重叠率低、semantic质量低。代码已经开源在 GitHub 上（<a target="_blank" rel="noopener" href="https://github.com/HKUST-Aerial-Robotics/Pagor">https://github.com/HKUST-Aerial-Robotics/Pagor</a>）。<details>
<summary>Abstract</summary>
Global point cloud registration is essential in many robotics tasks like loop closing and relocalization. Unfortunately, the registration often suffers from the low overlap between point clouds, a frequent occurrence in practical applications due to occlusion and viewpoint change. In this paper, we propose a graph-theoretic framework to address the problem of global point cloud registration with low overlap. To this end, we construct a consistency graph to facilitate robust data association and employ graduated non-convexity (GNC) for reliable pose estimation, following the state-of-the-art (SoTA) methods.   Unlike previous approaches, we use semantic cues to scale down the dense point clouds, thus reducing the problem size. Moreover, we address the ambiguity arising from the consistency threshold by constructing a pyramid graph with multi-level consistency thresholds. Then we propose a cascaded gradient ascend method to solve the resulting densest clique problem and obtain multiple pose candidates for every consistency threshold. Finally, fast geometric verification is employed to select the optimal estimation from multiple pose candidates. Our experiments, conducted on a self-collected indoor dataset and the public KITTI dataset, demonstrate that our method achieves the highest success rate despite the low overlap of point clouds and low semantic quality. We have open-sourced our code https://github.com/HKUST-Aerial-Robotics/Pagor for this project.
</details>
<details>
<summary>摘要</summary>
To achieve this, we construct a consistency graph to facilitate robust data association and employ graduated non-convexity (GNC) for reliable pose estimation, following state-of-the-art (SoTA) methods. Unlike previous approaches, we use semantic cues to scale down the dense point clouds, reducing the problem size. Additionally, we address the ambiguity arising from the consistency threshold by constructing a pyramid graph with multi-level consistency thresholds.We then propose a cascaded gradient ascend method to solve the resulting densest clique problem and obtain multiple pose candidates for every consistency threshold. Finally, fast geometric verification is employed to select the optimal estimation from multiple pose candidates. Our experiments, conducted on a self-collected indoor dataset and the public KITTI dataset, show that our method achieves the highest success rate despite the low overlap of point clouds and low semantic quality. We have open-sourced our code at https://github.com/HKUST-Aerial-Robotics/Pagor for this project.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.CV_2023_07_23/" data-id="clp8zxr6k00gxn68855avcefw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.AI_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T12:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.AI_2023_07_23/">cs.AI - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations"><a href="#Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations" class="headerlink" title="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?"></a>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12344">http://arxiv.org/abs/2307.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ss-sun/right-for-the-wrong-reason">https://github.com/ss-sun/right-for-the-wrong-reason</a></li>
<li>paper_authors: Susu Sun, Lisa M. Koch, Christian F. Baumgartner</li>
<li>for: 本研究旨在评估不同半结构化解释技术的检测潜在的假 correlate 能力。</li>
<li>methods: 本研究使用了 five 种post-hoc解释技术和一种自然语言解释技术来检测在胸部X射影诊断任务中 искусificially添加的三种干扰因素。</li>
<li>results: 研究发现，使用 SHAP 技术和自然语言解释技术 Attri-Net 可以准确地检测到胸部X射影诊断中的假 correlate，并且这些技术可以被用来可靠地检测模型的异常行为。<details>
<summary>Abstract</summary>
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.
</details>
<details>
<summary>摘要</summary>
深度神经网络模型具有无可比的分类性能，但它们容易学习潜在的 correlate 信息。这些依赖于潜在信息的关系可能难以通过性能指标来探测，尤其如果测试数据来自同一个分布。可解释的机器学习方法，如后期解释或内置可解释的分类器，承诺可以识别模型的错误思维。然而，有混合的证据表明许多这些技术是否能够做到这一点。在这篇论文中，我们提出了一种严格的评估策略，用于评估一种解释技术的能力 Correctly 识别人工添加的三种隐藏因素在胸部X射影诊断任务中。我们发现，使用 SHAP 和 Attri-Net 的 Post-hoc 解释技术，以及内置可解释的 Attri-Net 可以准确地识别模型的错误行为。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generic-and-Controllable-Attacks-Against-Object-Detection"><a href="#Towards-Generic-and-Controllable-Attacks-Against-Object-Detection" class="headerlink" title="Towards Generic and Controllable Attacks Against Object Detection"></a>Towards Generic and Controllable Attacks Against Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12342">http://arxiv.org/abs/2307.12342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liguopeng0923/LGP">https://github.com/liguopeng0923/LGP</a></li>
<li>paper_authors: Guopeng Li, Yue Xu, Jian Ding, Gui-Song Xia</li>
<li>for: 这个论文的目的是设计一种可控的攻击方法来攻击主流的物件探测器（Object Detectors，OD），以实现对OD的攻击。</li>
<li>methods: 这个论文使用了一种称为LGP（Local Perturbations with Adaptively Global Attacks）的白盒攻击方法，它可以对OD进行攻击，并且可以控制攻击的方向和大小。LGP使用了高品质的提案和三种不同的损失函数来实现攻击。</li>
<li>results: 实验结果显示，LGP可以成功攻击十六种主流的物件探测器，包括MS-COCO和DOTA datasets。此外，LGP也可以实现了不可见和传递性的攻击。codes可以在<a target="_blank" rel="noopener" href="https://github.com/liguopeng0923/LGP.git%E4%B8%AD%E5%8F%96%E5%BE%97%E3%80%82">https://github.com/liguopeng0923/LGP.git中取得。</a><details>
<summary>Abstract</summary>
Existing adversarial attacks against Object Detectors (ODs) suffer from two inherent limitations. Firstly, ODs have complicated meta-structure designs, hence most advanced attacks for ODs concentrate on attacking specific detector-intrinsic structures, which makes it hard for them to work on other detectors and motivates us to design a generic attack against ODs. Secondly, most works against ODs make Adversarial Examples (AEs) by generalizing image-level attacks from classification to detection, which brings redundant computations and perturbations in semantically meaningless areas (e.g., backgrounds) and leads to an emergency for seeking controllable attacks for ODs. To this end, we propose a generic white-box attack, LGP (local perturbations with adaptively global attacks), to blind mainstream object detectors with controllable perturbations. For a detector-agnostic attack, LGP tracks high-quality proposals and optimizes three heterogeneous losses simultaneously. In this way, we can fool the crucial components of ODs with a part of their outputs without the limitations of specific structures. Regarding controllability, we establish an object-wise constraint that exploits foreground-background separation adaptively to induce the attachment of perturbations to foregrounds. Experimentally, the proposed LGP successfully attacked sixteen state-of-the-art object detectors on MS-COCO and DOTA datasets, with promising imperceptibility and transferability obtained. Codes are publicly released in https://github.com/liguopeng0923/LGP.git
</details>
<details>
<summary>摘要</summary>
现有的对象检测器（OD）的敌对攻击受到两种内在的限制。首先，OD有复杂的元结构设计，因此大多数对OD的高级攻击都是针对特定的检测器结构，这使得它们难以在其他检测器上工作，激励我们设计一种通用的攻击方法。其次，大多数对OD的攻击是将图像级别的攻击扩展到检测，这会带来 redundant computations和在意义不明的区域（如背景）中的扰动，从而导致对OD的攻击控制不足。为此，我们提出了一种通用的白盒攻击方法——本地加工攻击（LGP），可以让主流的对象检测器感受到控制的扰动。为了实现检测器无关的攻击，LGP跟踪高质量的提案并同时优化三种多元损失。这样，我们可以使对OD的核心组件的输出中的一部分被扰动，而不是特定的结构。在控制性方面，我们建立了一种对象强制约束，以便随时 Adaptively 调整扰动的分布。实验结果表明，我们提出的LGP方法成功地击败了MS-COCO和DOTA数据集上十六个状态对象检测器，并且在透明度和传输性方面取得了惊人的成绩。代码在https://github.com/liguopeng0923/LGP.git中公开发布。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks"><a href="#Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks" class="headerlink" title="Tackling the Curse of Dimensionality with Physics-Informed Neural Networks"></a>Tackling the Curse of Dimensionality with Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12306">http://arxiv.org/abs/2307.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi</li>
<li>for: 解决高维精度数学模型的计算问题，提高计算效率和可扩展性。</li>
<li>methods: 使用Stochastic Dimension Gradient Descent（SDGD）方法，即将梯度分解成不同维度的部分，然后随机选择这些维度的部分进行每次训练。</li>
<li>results: 能够快速解决许多困难高维度精度数学问题，如 Hamilton-Jacobi-Bellman 方程和Schrödinger方程，并且可以在单个GPU上进行计算。例如，在100,000维度中解决了一个非线性HJB方程和一个Black-Scholes方程，并且在6小时内完成了计算。<details>
<summary>Abstract</summary>
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed method. We experimentally demonstrate that the proposed method allows us to solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. For instance, we solve nontrivial nonlinear PDEs (one HJB equation and one Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
科学研究中的困难（CoD）会使计算资源受到极大的挑战，计算成本随着维度的增加而呈指数增长。这在60多年前，理查德·贝尔曼首次提出的高维度 partial differential equations（PDEs）解决问题中 pose great challenges. Although there has been some recent success in numerically solving PDEs in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved.在这篇论文中，我们开发了一种新的方法，即随机维度加速（SDGD），用于扩展物理学信息神经网络（PINNs）解决任意高维度 PDEs。SDGD方法将 PDE 的梯度分解成不同维度的部分，然后在训练 PINNs 时随机选择这些维度的部分。我们证明了该方法的收敛保证和其他愿望的性质。我们通过实验表明，使用 SDGD 方法可以很快地解决许多知名度很高的高维度 PDEs，例如 Hamilton-Jacobi-Bellman 方程和 Schrödinger 方程在千个维度中。例如，我们在6个小时内使用单个 GPU 解决了一个非线性 HJB 方程和一个黑色-肖勒斯方程在10,000个维度中。由于 SDGD 是一种通用的 PINNs 训练方法，因此可以应用于任何当前和未来的 PINNs 变体，以扩展它们的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Controller-Synthesis-for-Timeline-based-Games"><a href="#Controller-Synthesis-for-Timeline-based-Games" class="headerlink" title="Controller Synthesis for Timeline-based Games"></a>Controller Synthesis for Timeline-based Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12289">http://arxiv.org/abs/2307.12289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renato Acampora, Luca Geatti, Nicola Gigante, Angelo Montanari, Valentino Picotti</li>
<li>for: 这篇论文旨在提供一种有效和计算优化的控制器生成方法来解决时间轴基于游戏中的游戏策略问题。</li>
<li>methods: 该论文使用的方法包括时间轴基于游戏和控制器生成。</li>
<li>results: 该论文提出的控制器生成方法可以有效地解决时间轴基于游戏中的游戏策略问题，并且computational complexity是2EXPTIME-complete。<details>
<summary>Abstract</summary>
In the timeline-based approach to planning, the evolution over time of a set of state variables (the timelines) is governed by a set of temporal constraints. Traditional timeline-based planning systems excel at the integration of planning with execution by handling temporal uncertainty. In order to handle general nondeterminism as well, the concept of timeline-based games has been recently introduced. It has been proved that finding whether a winning strategy exists for such games is 2EXPTIME-complete. However, a concrete approach to synthesize controllers implementing such strategies is missing. This paper fills this gap, by providing an effective and computationally optimal approach to controller synthesis for timeline-based games.
</details>
<details>
<summary>摘要</summary>
在时间轴基本方法中，时间变量集（时间轴）的演化遵循一组时间约束。传统的时间轴基本方法具有融合规划与执行的能力，可以处理时间不确定性。为了处理通用非决定性，时间轴基本方法中的游戏概念被最近引入。已证明找到赢家策略的存在是2EXPTIME-完善。但是，实现这种策略的控制器合成方法缺失。这篇论文填补了这个空白，提供了有效和计算优化的控制器合成方法 для时间轴基本方法中的游戏。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Adaptive-Formation-via-Consensus-Oriented-Multi-Agent-Communication"><a href="#Decentralized-Adaptive-Formation-via-Consensus-Oriented-Multi-Agent-Communication" class="headerlink" title="Decentralized Adaptive Formation via Consensus-Oriented Multi-Agent Communication"></a>Decentralized Adaptive Formation via Consensus-Oriented Multi-Agent Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12287">http://arxiv.org/abs/2307.12287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuming Xiang, Sizhao Li, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</li>
<li>for: 这 paper 的目的是提出一种适应多 аген特式控制方法，以适应多 аген系统中 agent 的数量变化，并在有限通信环境下实现高速稳定的formation control.</li>
<li>methods: 该 paper 使用了一种新的 multi-agent reinforcement learning 方法，即 consensus-oriented multi-agent communication (ConsMAC)，以使 agents 能够 perceive 全局信息并在本地状态下达成 consensus。此外，paper 还使用了 policy distillation 来实现 Adaptive formation adjustment.</li>
<li>results: 实验结果表明，提出的方法在高速稳定性方面具有出色的表现，并且可以快速适应多 аген系统中 agent 的数量变化。<details>
<summary>Abstract</summary>
Adaptive multi-agent formation control, which requires the formation to flexibly adjust along with the quantity variations of agents in a decentralized manner, belongs to one of the most challenging issues in multi-agent systems, especially under communication-limited constraints. In this paper, we propose a novel Consensus-based Decentralized Adaptive Formation (Cons-DecAF) framework. Specifically, we develop a novel multi-agent reinforcement learning method, Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish the consensus from local states by effectively aggregating neighbor messages. Afterwards, we leverage policy distillation to accomplish the adaptive formation adjustment. Meanwhile, instead of pre-assigning specific positions of agents, we employ a displacement-based formation by Hausdorff distance to significantly improve the formation efficiency. The experimental results through extensive simulations validate that the proposed method has achieved outstanding performance in terms of both speed and stability.
</details>
<details>
<summary>摘要</summary>
《适应多智能体formation控制》是多智能体系统中最为复杂的问题之一，尤其是在有限通信环境下。在这篇论文中，我们提出了一种新的Consensus-based Decentralized Adaptive Formation（Cons-DecAF）框架。具体来说，我们开发了一种新的多智能体学习方法——Consensus-oriented Multi-Agent Communication（ConsMAC），使智能体能够从本地状态中获得全局信息并达成一致。接着，我们通过策略填充来实现形态调整。而不是先行指定智能体的具体位置，我们employs a displacement-based formation by Hausdorff distance，以大幅提高形态效率。实验结果表明，我们提出的方法在速度和稳定性两个方面具有出色的表现。
</details></li>
</ul>
<hr>
<h2 id="Towards-Automatic-Boundary-Detection-for-Human-AI-Collaborative-Hybrid-Essay-in-Education"><a href="#Towards-Automatic-Boundary-Detection-for-Human-AI-Collaborative-Hybrid-Essay-in-Education" class="headerlink" title="Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education"></a>Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12267">http://arxiv.org/abs/2307.12267</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/douglashiwo/BoundaryDetectionFromHybridText">https://github.com/douglashiwo/BoundaryDetectionFromHybridText</a></li>
<li>paper_authors: Zijie Zeng, Lele Sha, Yuheng Li, Kaixun Yang, Dragan Gašević, Guanliang Chen</li>
<li>for: 本研究旨在探讨AI生成文本检测在 hybrid 文本中的应用，即检测人类和生成LLMs共同写作的文本中的AI生成部分。</li>
<li>methods: 本研究提出了一种两步方法，包括在encoder训练过程中分离AI生成内容和人类写作内容，然后计算每两个相邻的聚合函数间的距离，并假设存在两个聚合函数间最远的距离处的边界。</li>
<li>results: 实验结果表明，提出的方法在不同的实验设置下 consistently 超过基eline方法的性能，并且 encoder 训练过程可以明显提高方法的性能。 在检测单边 hybrid 文本中的边界时，可以采用一定的 prototype 大小来进一步提高方法的性能，升师22% 在 Domain 评估中和18% 在 Out-of-Domain 评估中。<details>
<summary>Abstract</summary>
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary detection). Then we proposed a two-step approach where we (1) separated AI-generated content from human-written content during the encoder training process; and (2) calculated the distances between every two adjacent prototypes and assumed that the boundaries exist between the two adjacent prototypes that have the furthest distance from each other. Through extensive experiments, we observed the following main findings: (1) the proposed approach consistently outperformed the baseline methods across different experiment settings; (2) the encoder training process can significantly boost the performance of the proposed approach; (3) when detecting boundaries for single-boundary hybrid essays, the proposed approach could be enhanced by adopting a relatively large prototype size, leading to a 22% improvement in the In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
</details>
<details>
<summary>摘要</summary>
Recent large language models (LLMs), such as ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. However, educators have concerns that students may use LLMs to complete their writing assignments and pass them off as their own work. To address this issue, many AI content detection studies have been conducted, but most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated.In this study, we investigated AI content detection in a realistic setting where the text to be detected is collaboratively written by humans and generative LLMs (hybrid text). We formalized the detection task as identifying the transition points between human-written content and AI-generated content in a given hybrid text (boundary detection).To solve this problem, we proposed a two-step approach:1. Separate AI-generated content from human-written content during the encoder training process.2. Calculate the distances between every two adjacent prototypes and assume that the boundaries exist between the two adjacent prototypes with the furthest distance from each other.Through extensive experiments, we found the following main findings:1. Our proposed approach consistently outperformed baseline methods across different experiment settings.2. The encoder training process can significantly boost the performance of our proposed approach.3. When detecting boundaries for single-boundary hybrid essays, our proposed approach can be enhanced by adopting a relatively large prototype size, leading to a 22% improvement in the In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
</details></li>
</ul>
<hr>
<h2 id="Building-road-Collaborative-Extraction-from-Remotely-Sensed-Images-via-Cross-Interaction"><a href="#Building-road-Collaborative-Extraction-from-Remotely-Sensed-Images-via-Cross-Interaction" class="headerlink" title="Building-road Collaborative Extraction from Remotely Sensed Images via Cross-Interaction"></a>Building-road Collaborative Extraction from Remotely Sensed Images via Cross-Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12256">http://arxiv.org/abs/2307.12256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haonan Guo, Xin Su, Chen Wu, Bo Du, Liangpei Zhang</li>
<li>for: 本研究旨在提高从高分辨率遥感图像中提取建筑和道路的精度和效率，通过建筑和道路之间的协同抽取方法。</li>
<li>methods: 本研究提出了一种基于多任务和跨比例特征互动的建筑-道路共同抽取方法，通过交互信息 между任务和自适应收发场所来提高每个任务的准确性。</li>
<li>results: 实验表明，提出的方法可以在各种城市和农村场景下实现出色的建筑-道路抽取性能和效率。<details>
<summary>Abstract</summary>
Buildings are the basic carrier of social production and human life; roads are the links that interconnect social networks. Building and road information has important application value in the frontier fields of regional coordinated development, disaster prevention, auto-driving, etc. Mapping buildings and roads from very high-resolution (VHR) remote sensing images have become a hot research topic. However, the existing methods often ignore the strong spatial correlation between roads and buildings and extract them in isolation. To fully utilize the complementary advantages between buildings and roads, we propose a building-road collaborative extraction method based on multi-task and cross-scale feature interaction to improve the accuracy of both tasks in a complementary way. A multi-task interaction module is proposed to interact information across tasks and preserve the unique information of each task, which tackle the seesaw phenomenon in multitask learning. By considering the variation in appearance and structure between buildings and roads, a cross-scale interaction module is designed to automatically learn the optimal reception field for different tasks. Compared with many existing methods that train each task individually, the proposed collaborative extraction method can utilize the complementary advantages between buildings and roads by the proposed inter-task and inter-scale feature interactions, and automatically select the optimal reception field for different tasks. Experiments on a wide range of urban and rural scenarios show that the proposed algorithm can achieve building-road extraction with outstanding performance and efficiency.
</details>
<details>
<summary>摘要</summary>
建筑和路径是社会生产和人类生活的基础载体，路径是社会网络之间的连接。建筑和路径信息在前沿领域的区域协调发展、灾害预防、自动驾驶等领域有重要应用价值。从very high-resolution（VHR）Remote sensing图像中提取建筑和路径信息已成为热点研究话题。然而，现有方法 часто忽略了道路和建筑之间的强相关性，单独提取它们。为了充分利用建筑和道路之间的补做优势，我们提议一种建筑道路共同提取方法，基于多任务和跨比例特征互动来提高两个任务的准确率。我们提出的多任务互动模块可以在不同任务之间交换信息，保持每个任务的独特信息，解决多任务学习中的摇摆现象。通过考虑建筑和道路之间的外观和结构变化，我们设计了一种跨比例互动模块，自动学习不同任务的最佳接收频率。与许多现有方法不同，我们的共同提取方法可以利用建筑和道路之间的补做优势，通过我们提出的交互特征互动和自动选择最佳接收频率，提高两个任务的准确率。在各种都市和农村场景下进行了广泛的实验，我们的算法可以实现出色的建筑道路提取和高效率。
</details></li>
</ul>
<hr>
<h2 id="Nature-and-the-Machines"><a href="#Nature-and-the-Machines" class="headerlink" title="Nature and the Machines"></a>Nature and the Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04440">http://arxiv.org/abs/2308.04440</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/face-analysis/emonet">https://github.com/face-analysis/emonet</a></li>
<li>paper_authors: Huw Price, Matthew Connolly</li>
<li>for: 这篇论文是关于人工智能（AI）对人类是否 pose existential risk 的问题，而critics认为这个问题 receiving 太多关注，希望把其推迟到一边，Focus on 当前 AI poses 的风险。</li>
<li>methods: 论文使用的方法是 argument 和 persuasion，指出 Nature 期刊在这个问题上的错误判断，并阐述了 AI 的风险和 Its potential consequences.</li>
<li>results: 论文的结论是，Nature 期刊的 error  judgement 是 serious，因为 AI 的风险不仅是今天的问题，也是未来的问题。论文 argues that we should not ignore the potential risks of AI, but instead, we should consider the consequences of error and take appropriate measures to mitigate them.<details>
<summary>Abstract</summary>
Does artificial intelligence (AI) pose existential risks to humanity? Some critics feel this question is getting too much attention, and want to push it aside in favour of conversations about the immediate risks of AI. These critics now include the journal Nature, where a recent editorial urges us to 'stop talking about tomorrow's AI doomsday when AI poses risks today.' We argue that this is a serious failure of judgement, on Nature's part. In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
</details>
<details>
<summary>摘要</summary>
人类是否面临人工智能（AI）的极大风险？一些批评者认为这个问题Receiving too much attention，希望把其推迟到一Side和讨论当前AI的风险。这些批评者现在包括《Nature》杂志，其latest editorial呼吁我们“停止讨论明天的AIArmageddon，因为AI今天已经存在风险”。我们认为，《Nature》在这个问题上manifestly failed to consider the cost of error。In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
</details></li>
</ul>
<hr>
<h2 id="MARS-Exploiting-Multi-Level-Parallelism-for-DNN-Workloads-on-Adaptive-Multi-Accelerator-Systems"><a href="#MARS-Exploiting-Multi-Level-Parallelism-for-DNN-Workloads-on-Adaptive-Multi-Accelerator-Systems" class="headerlink" title="MARS: Exploiting Multi-Level Parallelism for DNN Workloads on Adaptive Multi-Accelerator Systems"></a>MARS: Exploiting Multi-Level Parallelism for DNN Workloads on Adaptive Multi-Accelerator Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12234">http://arxiv.org/abs/2307.12234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guan Shen, Jieru Zhao, Zeke Wang, Zhe Lin, Wenchao Ding, Chentao Wu, Quan Chen, Minyi Guo</li>
<li>for: 这个研究旨在提出一个名为 MARS 的新的映射框架，用于在多个加速器系统中选择适当的加速器设计，并对 DNN 进行有效的分割策略，以最大化并行度。</li>
<li>methods: 这个研究使用了 computation-aware 加速器选择策略和 communication-aware 分割策略，以提高 DNN 的执行效率。</li>
<li>results: 实验结果显示，MARS 可以在常见的 DNN 负载中实现32.2% 的延迟增加，并在多核心模型中实现59.4% 的延迟增加，较基eline方法和相关的现有方法更高。<details>
<summary>Abstract</summary>
Along with the fast evolution of deep neural networks, the hardware system is also developing rapidly. As a promising solution achieving high scalability and low manufacturing cost, multi-accelerator systems widely exist in data centers, cloud platforms, and SoCs. Thus, a challenging problem arises in multi-accelerator systems: selecting a proper combination of accelerators from available designs and searching for efficient DNN mapping strategies. To this end, we propose MARS, a novel mapping framework that can perform computation-aware accelerator selection, and apply communication-aware sharding strategies to maximize parallelism. Experimental results show that MARS can achieve 32.2% latency reduction on average for typical DNN workloads compared to the baseline, and 59.4% latency reduction on heterogeneous models compared to the corresponding state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
“深度神经网络的快速演化，硬件系统也在快速发展。作为数据中心、云平台和SoC中高扩展性低生产成本的解决方案，多加速器系统广泛存在。因此，多加速器系统中的一个挑战是选择合适的加速器设计，并搜索高度平行的DNN映射策略。为此，我们提出了MARS，一种新的映射框架，可以实现计算意识加速器选择，以及通信意识分割策略，以最大化并行性。实验结果显示，MARS可以相对基eline方法平均减少32.2%的延迟，对于常见的DNN任务，并且相对相对国际先进方法，对于多元模型，可以减少59.4%的延迟。”
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Adaptation-for-Pretrained-Models"><a href="#Geometry-Aware-Adaptation-for-Pretrained-Models" class="headerlink" title="Geometry-Aware Adaptation for Pretrained Models"></a>Geometry-Aware Adaptation for Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12226">http://arxiv.org/abs/2307.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala<br>for:The paper is written to improve the performance of machine learning models, specifically zero-shot models, in predicting new classes without any additional training.methods:The proposed approach, called Loki, uses a simple technique to adapt the trained model to predict new classes by swapping the standard prediction rule with the Fréchet mean. The approach is a drop-in replacement and does not require any additional training data.results:The paper shows that Loki achieves up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no external metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.<details>
<summary>Abstract</summary>
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.
</details>
<details>
<summary>摘要</summary>
机器学习模型 -- 包括知名的零 shot 模型 -- 常常在具有小 proportion 的标签空间上进行训练。这些空间通常具有一个将标签相关的度量。我们提议一种简单的方法，利用这些信息来适应训练后的模型，以可靠地预测新类 -- 或在零 shot 预测情况下，提高其性能 -- 无需进一步训练。我们的技术是将 argmax 替换为 Fréchet 平均。我们提供了完整的理论分析，包括（i）交互学习理论，考虑标签空间径、样本复杂度和模型维度之间的负反关系，（ii）对预测任何未知类的全范围情况的Characterizations，以及（iii）用于获取优化训练类的优化活动学习类选择方法。Empirically, 我们的提议方法，Loki，在 ImageNet 上对 SimCLR 进行了29.7% 相对提升，并可扩展到数万个类。当没有外部度量时，Loki 可以使用自己计算出的类嵌入度量，并在预测零 shot 模型 CLIP 时获得10.5% 的提升。
</details></li>
</ul>
<hr>
<h2 id="FATRER-Full-Attention-Topic-Regularizer-for-Accurate-and-Robust-Conversational-Emotion-Recognition"><a href="#FATRER-Full-Attention-Topic-Regularizer-for-Accurate-and-Robust-Conversational-Emotion-Recognition" class="headerlink" title="FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition"></a>FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12221">http://arxiv.org/abs/2307.12221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ludybupt/FATRER">https://github.com/ludybupt/FATRER</a></li>
<li>paper_authors: Yuzhao Mao, Di Lu, Xiaojie Wang, Yang Zhang</li>
<li>for: 这 paper 的目的是理解对话中的情感引起的情绪。</li>
<li>methods: 这 paper 使用了一种基于全注意力话题规范的情绪识别器，以提高模型的Robustness 和准确性。</li>
<li>results: 实验显示，这 paper 的模型比现有模型更好地抵抗三种 adversarial 攻击，并且在情绪识别 tasks 上得到了更高的效果。<details>
<summary>Abstract</summary>
This paper concentrates on the understanding of interlocutors' emotions evoked in conversational utterances. Previous studies in this literature mainly focus on more accurate emotional predictions, while ignoring model robustness when the local context is corrupted by adversarial attacks. To maintain robustness while ensuring accuracy, we propose an emotion recognizer augmented by a full-attention topic regularizer, which enables an emotion-related global view when modeling the local context in a conversation. A joint topic modeling strategy is introduced to implement regularization from both representation and loss perspectives. To avoid over-regularization, we drop the constraints on prior distributions that exist in traditional topic modeling and perform probabilistic approximations based entirely on attention alignment. Experiments show that our models obtain more favorable results than state-of-the-art models, and gain convincing robustness under three types of adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Expediting-Building-Footprint-Segmentation-from-High-resolution-Remote-Sensing-Images-via-progressive-lenient-supervision"><a href="#Expediting-Building-Footprint-Segmentation-from-High-resolution-Remote-Sensing-Images-via-progressive-lenient-supervision" class="headerlink" title="Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision"></a>Expediting Building Footprint Segmentation from High-resolution Remote Sensing Images via progressive lenient supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12220">http://arxiv.org/abs/2307.12220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework">https://github.com/haonanguo/bfseg-efficient-building-footprint-segmentation-framework</a></li>
<li>paper_authors: Haonan Guo, Bo Du, Chen Wu, Xin Su, Liangpei Zhang</li>
<li>for: 本研究旨在提高从遥感图像中提取建筑印迹的效率和准确率。</li>
<li>methods: 本文提出了一种高效的建筑印迹分割框架，包括一种精密的均匀连接粗细度特征融合网络，以及一种宽泛的深度监督和采样策略。</li>
<li>results: 根据实验结果，提出的建筑印迹分割框架可以高效地提取建筑印迹，并且可以在不同的encoder网络上达到出色的性能和效率。<details>
<summary>Abstract</summary>
The efficacy of building footprint segmentation from remotely sensed images has been hindered by model transfer effectiveness. Many existing building segmentation methods were developed upon the encoder-decoder architecture of U-Net, in which the encoder is finetuned from the newly developed backbone networks that are pre-trained on ImageNet. However, the heavy computational burden of the existing decoder designs hampers the successful transfer of these modern encoder networks to remote sensing tasks. Even the widely-adopted deep supervision strategy fails to mitigate these challenges due to its invalid loss in hybrid regions where foreground and background pixels are intermixed. In this paper, we conduct a comprehensive evaluation of existing decoder network designs for building footprint segmentation and propose an efficient framework denoted as BFSeg to enhance learning efficiency and effectiveness. Specifically, a densely-connected coarse-to-fine feature fusion decoder network that facilitates easy and fast feature fusion across scales is proposed. Moreover, considering the invalidity of hybrid regions in the down-sampled ground truth during the deep supervision process, we present a lenient deep supervision and distillation strategy that enables the network to learn proper knowledge from deep supervision. Building upon these advancements, we have developed a new family of building segmentation networks, which consistently surpass prior works with outstanding performance and efficiency across a wide range of newly developed encoder networks. The code will be released on https://github.com/HaonanGuo/BFSeg-Efficient-Building-Footprint-Segmentation-Framework.
</details>
<details>
<summary>摘要</summary>
“ remote sensing 图像中的建筑印迹分 segmentation 的效果受到模型传递效果的限制。现有的建筑分 segmentation 方法大多基于encoder-decoder架构的 U-Net，其中 encoder 是从新开发的背景网络中精度调整的。但是，现有的 decoder 设计带来了大量计算负担，使得现代 encoder 网络不能成功传递到 remote sensing 任务中。即使广泛采用的深度监测策略也无法缓解这些挑战，因为它在混合区域中的无效损失。在这篇论文中，我们对现有 decoder 网络设计进行了全面的评估，并提出了一种高效的框架 denoted as BFSeg，以提高学习效率和效果。具体来说，我们提出了一种紧密连接的粗细尺度混合特征卷积网络，以便轻松地实现特征之间的快速混合。此外，我们认为在下采样的真实图像中的混合区域无效的损失问题，我们提出了一种宽松的深度监测和采样策略，使得网络能够从深度监测中学习正确的知识。基于这些进步，我们开发了一个新的建筑分 segmentation 网络家族，这些网络在各种新开发的 encoder 网络上表现出色，在效率和性能方面都超越了先前的工作。代码将在 https://github.com/HaonanGuo/BFSeg-Efficient-Building-Footprint-Segmentation-Framework 上发布。”
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Review-and-Systematic-Analysis-of-Artificial-Intelligence-Regulation-Policies"><a href="#A-Comprehensive-Review-and-Systematic-Analysis-of-Artificial-Intelligence-Regulation-Policies" class="headerlink" title="A Comprehensive Review and Systematic Analysis of Artificial Intelligence Regulation Policies"></a>A Comprehensive Review and Systematic Analysis of Artificial Intelligence Regulation Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12218">http://arxiv.org/abs/2307.12218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyue Wu, Shaoshan Liu</li>
<li>for: This paper aims to help governing bodies understand and regulate AI technologies in a chaotic global regulatory space.</li>
<li>methods: The paper presents a comprehensive review of AI regulation proposals from different geographical locations and cultural backgrounds, and develops a framework for analyzing these proposals.</li>
<li>results: The paper performs a systematic analysis of AI regulation proposals to identify potential failures and provide insights for governing bodies to untangle the AI regulatory chaos.In Simplified Chinese text, the three key points would be:</li>
<li>for: 这篇论文目标是帮助管理机构理解和调控全球AI regulatory空间中的混乱。</li>
<li>methods: 论文首先提供了AI regulatory proposal的全面回顾，然后开发了一个分析这些提案的框架。</li>
<li>results: 论文进行了系统性的AI regulatory proposal分析，以便发现可能的失败并为管理机构提供干预措施。<details>
<summary>Abstract</summary>
Due to the cultural and governance differences of countries around the world, there currently exists a wide spectrum of AI regulation policy proposals that have created a chaos in the global AI regulatory space. Properly regulating AI technologies is extremely challenging, as it requires a delicate balance between legal restrictions and technological developments. In this article, we first present a comprehensive review of AI regulation proposals from different geographical locations and cultural backgrounds. Then, drawing from historical lessons, we develop a framework to facilitate a thorough analysis of AI regulation proposals. Finally, we perform a systematic analysis of these AI regulation proposals to understand how each proposal may fail. This study, containing historical lessons and analysis methods, aims to help governing bodies untangling the AI regulatory chaos through a divide-and-conquer manner.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为世界各国文化和管理差异的存在，目前在全球AI规制空间中存在广泛的AI规制政策提议，创造了混乱。正确地规制AI技术是极其困难的，因为它需要绝对的法律限制和技术发展之间的细致平衡。在这篇文章中，我们首先提供了AI规制提议的全面回顾，然后，Drawing from historical lessons，我们开发了一个框架，以便对AI规制提议进行全面的分析。最后，我们对这些AI规制提议进行了系统性的分析，以了解每一个提议可能会失败的原因。这篇文章，包含历史评论和分析方法， hopes to help governing bodies untangle the AI regulatory chaos through a divide-and-conquer manner.
</details></li>
</ul>
<hr>
<h2 id="Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models"><a href="#Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models" class="headerlink" title="Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models"></a>Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02409">http://arxiv.org/abs/2308.02409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim</li>
<li>for: 这篇论文旨在预测 mental workload 的三个状态和级别，以便提高 mental health 评估的准确性。</li>
<li>methods: 该论文使用 Temporal Convolutional Networks 和 Multi-Dimensional Residual Block 等方法，将多维度空间综合利用以实现最佳的 mental 估计。</li>
<li>results: 该论文通过将 mental workload 分类为三个状态并估计级别，帮助早期发现 mental health 问题，从而预防严重的健康问题并提高生活质量。<details>
<summary>Abstract</summary>
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
</details>
<details>
<summary>摘要</summary>
人脑在工作和休息时都处于不断活跃状态。心理活动是每日的过程，当脑部过度劳累时，可能有负面影响于人类健康。在最近几年，对于早期发现心理问题的检测已经受到了广泛关注，因为它可以帮助预防严重的健康问题并提高生活质量。几种信号都用于评估心理状态，但是电enzephalogram（EEG）在研究人员中广泛使用，因为它可以提供大量关于脑部的信息。本文目的是将心理劳重分为三个状态，并估算维度水平。我们的方法将多个空间维度组合起来，以获得最佳的心理估算结果。在时域方法中，我们使用Temporal Convolutional Networks，在频域中，我们提出了一种新的架构——多维度征存块，这种架构将征存块结合在一起。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Flow-Simulation-for-Autonomous-Driving"><a href="#Traffic-Flow-Simulation-for-Autonomous-Driving" class="headerlink" title="Traffic Flow Simulation for Autonomous Driving"></a>Traffic Flow Simulation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16762">http://arxiv.org/abs/2307.16762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junfeng Li, Changqing Yan</li>
<li>for: This paper aims to provide a simulation environment for testing and evaluating the development of autonomous driving technology.</li>
<li>methods: The paper uses micro-traffic flow modeling and cellular automata to build the simulation environment, and it also employs a vehicle motion model based on bicycle intelligence.</li>
<li>results: The paper develops a simulation environment for autonomous vehicle flow, which can accurately control the acceleration, braking, steering, and lighting actions of the vehicle based on the bus instructions issued by the decision-making system.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了提供自动驾驶技术的测试和评估技术的实验环境。</li>
<li>methods: 这篇论文使用微车流模型和细胞自动机来构建实验环境，同时还使用基于自行车智能的车辆动态模型。</li>
<li>results: 这篇论文建立了一个可以准确控制车辆加速、减速、转向和灯光动作的自动驾驶流 simulator。<details>
<summary>Abstract</summary>
A traffic system is a random and complex large system, which is difficult to conduct repeated modelling and control research in a real traffic environment. With the development of automatic driving technology, the requirements for testing and evaluating the development of automatic driving technology are getting higher and higher, so the application of computer technology for traffic simulation has become a very effective technical means. Based on the micro-traffic flow modelling, this paper adopts the vehicle motion model based on cellular automata and the theory of bicycle intelligence to build the simulation environment of autonomous vehicle flow. The architecture of autonomous vehicles is generally divided into a perception system, decision system and control system. The perception system is generally divided into many subsystems, responsible for autonomous vehicle positioning, obstacle recognition, traffic signal detection and recognition and other tasks. Decision systems are typically divided into many subsystems that are responsible for tasks such as path planning, path planning, behavior selection, motion planning, and control. The control system is the basis of the selfdriving car, and each control system of the vehicle needs to be connected with the decision-making system through the bus, and can accurately control the acceleration degree, braking degree, steering amplitude, lighting control and other driving actions according to the bus instructions issued by the decision-making system, so as to achieve the autonomous driving of the vehicle.
</details>
<details>
<summary>摘要</summary>
traffic system 是一个随机和复杂的大型系统，在实际交通环境中进行重复模拟和控制研究非常困难。随着自动驾驶技术的发展，测试和评估自动驾驶技术的要求越来越高，因此计算机技术在交通模拟中发挥了非常有效的技术作用。基于微流量模拟，本文采用基于细胞自动机和自行车智能理论建立自动车流 simulation 环境。自动车的架构通常分为感知系统、决策系统和控制系统。感知系统通常分为多个子系统，负责自动车定位、障碍物识别、交通信号检测和识别等任务。决策系统通常分为多个子系统，负责任务such as 路径规划、行为选择、动作规划和控制。控制系统是自动车的基础，每个控制系统需要与决策系统通过公共总线连接，并可以准确控制车辆加速度、缓冲度、转向强度、灯光控制和其他驾驶动作，以实现自动驾驶。
</details></li>
</ul>
<hr>
<h2 id="DeepCL-Deep-Change-Feature-Learning-on-Remote-Sensing-Images-in-the-Metric-Space"><a href="#DeepCL-Deep-Change-Feature-Learning-on-Remote-Sensing-Images-in-the-Metric-Space" class="headerlink" title="DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space"></a>DeepCL: Deep Change Feature Learning on Remote Sensing Images in the Metric Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12208">http://arxiv.org/abs/2307.12208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haonanguo/deepcl">https://github.com/haonanguo/deepcl</a></li>
<li>paper_authors: Haonan Guo, Bo Du, Chen Wu, Chengxi Han, Liangpei Zhang</li>
<li>for: 本研究旨在提高自动变化检测（CD）的精度和可解释性，以便更好地监测地表动态变化。</li>
<li>methods: 我们结合度量学习的强时间关系模型和分割的优势，提出了深度变化特征学习（DeepCL）框架。我们设计了一种具有强时间相关性的强对比损失函数，以显著提高对比样本的重要性。此外，我们利用模型的时间关系知识来引导分割过程，以更好地检测变化区域。</li>
<li>results: 我们对 DeepCL 框架进行了严格的理论和实验评估，结果显示，DeepCL 在特征识别率、鲁棒性和可解释性等方面具有明显的优势，并且可以与多种 CD 方法进行结合使用。广泛的比较实验证明 DeepCL 在 CD 领域的精度和可解释性都达到了国际先进水平。<details>
<summary>Abstract</summary>
Change detection (CD) is an important yet challenging task in the Earth observation field for monitoring Earth surface dynamics. The advent of deep learning techniques has recently propelled automatic CD into a technological revolution. Nevertheless, deep learning-based CD methods are still plagued by two primary issues: 1) insufficient temporal relationship modeling and 2) pseudo-change misclassification. To address these issues, we complement the strong temporal modeling ability of metric learning with the prominent fitting ability of segmentation and propose a deep change feature learning (DeepCL) framework for robust and explainable CD. Firstly, we designed a hard sample-aware contrastive loss, which reweights the importance of hard and simple samples. This loss allows for explicit modeling of the temporal correlation between bi-temporal remote sensing images. Furthermore, the modeled temporal relations are utilized as knowledge prior to guide the segmentation process for detecting change regions. The DeepCL framework is thoroughly evaluated both theoretically and experimentally, demonstrating its superior feature discriminability, resilience against pseudo changes, and adaptability to a variety of CD algorithms. Extensive comparative experiments substantiate the quantitative and qualitative superiority of DeepCL over state-of-the-art CD approaches.
</details>
<details>
<summary>摘要</summary>
优化检测 (CD) 是地球观测领域中重要且挑战性强的任务，用于监测地球表面动态。深度学习技术的出现已经推动了自动化 CD 技术到了技术革命的水平。然而，深度学习基于 CD 方法仍然受到两个主要问题的困扰：1）不充分的时间关系模型化和2）假变误分类。为了解决这些问题，我们将强度表示学习的准确性能与分割的优劣点相结合，并提出了深度变化特征学习 (DeepCL) 框架。首先，我们设计了一种坚持样本感知的对比损失函数，该函数重新评估硬件和简单样本的重要性。这种损失函数允许明确模型 биitemporal 遥感图像之间的时间相关性。其次，模型的时间关系知识被用作引导分割过程，以便检测变化区域。DeepCL 框架进行了严格的理论和实验测试，并证明其具有更高的特征抽象能力、鲁棒性 against 假变和适应性。广泛的比较实验证明 DeepCL 在 CD 领域的超过状态之前的性能。
</details></li>
</ul>
<hr>
<h2 id="Monadic-Deep-Learning"><a href="#Monadic-Deep-Learning" class="headerlink" title="Monadic Deep Learning"></a>Monadic Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12187">http://arxiv.org/abs/2307.12187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoughtWorksInc/monadic-deep-learning">https://github.com/ThoughtWorksInc/monadic-deep-learning</a></li>
<li>paper_authors: Bo Yang, Zhihao Zhang Kirisame Marisa, Kai Shi</li>
<li>for: 本研究旨在提供一个可靠的、类型安全的深度学习框架，用于在 Scala 中实现动态神经网络。</li>
<li>methods: 本研究使用了一种新的准确方法，可以在静态类型函数中自动进行微分，并且可以跨语言交互。此外，研究还使用了一些套件和套件变换，以便用户可以创建具有动态神经网络表示的幂等表达式。</li>
<li>results: 研究得到了一个可靠的、类型安全的深度学习框架，可以在 Scala 中实现复杂的神经网络表示。用户可以使用这个框架来创建具有多变量的神经网络，并且仍然保持类型安全性。<details>
<summary>Abstract</summary>
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.   Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.   We solved this problem in DeepLearning.scala 2. Our contributions are:   1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.   2. We designed a set of monads and monad transformers, which allow users to create monadic expressions that represent dynamic neural networks.   3. Along with these monads, we provide some applicative functors, to perform multiple calculations in parallel.   With these features, users of DeepLearning.scala were able to create complex neural networks in an intuitive and concise way, and still maintain type safety.
</details>
<details>
<summary>摘要</summary>
Java和Scala社区已经建立了一个非常成功的大数据生态系统。然而，大多数运行在其上的神经网络都是使用动态类型编程语言模型的。这些动态类型深度学习框架将神经网络视为可导的表达，并在训练时自动进行差分。直到2019年，静态类型语言的学习框架中没有提供表达能力相当于传统框架的。其用户无法使用自定义算法，除非创建大量的简单代码来实现硬编码的反向差分。我们解决了这个问题，我们在DeepLearning.scala 2中提供了以下贡献：1. 我们发现了一种新的方法，可以在静态类型函数中自动进行差分，并且可以与金属语言自由交互。2. 我们设计了一组幂kk和幂Transformer，这些幂kk可以让用户创建幂表达式，表示动态神经网络。3. 同时，我们还提供了一些应用函数，可以并发地执行多个计算。通过这些特性，DeepLearning.scala 2 的用户可以创建复杂的神经网络，并且保持类型安全性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-discovers-invariants-of-braids-and-flat-braids"><a href="#Machine-learning-discovers-invariants-of-braids-and-flat-braids" class="headerlink" title="Machine learning discovers invariants of braids and flat braids"></a>Machine learning discovers invariants of braids and flat braids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12185">http://arxiv.org/abs/2307.12185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexei Lisitsa, Mateo Salles, Alexei Vernitski</li>
<li>for: 这个论文用机器学习分类扑斗织（或平坦扑斗）的例子，以确定它们是否是平凡的。</li>
<li>methods: 这个论文使用了指导学习使用神经网络（多层感知器）进行超visisted学习，以达到好的分类结果。</li>
<li>results: 通过这个论文，我们发现了新的便利的扑斗 invariants，包括完全的平坦扑斗 invariants。<details>
<summary>Abstract</summary>
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
</details>
<details>
<summary>摘要</summary>
我们使用机器学习来分类拥有螺旋或平板拥的例子（拥有螺旋或平板拥）。我们的机器学习是指监督学习使用神经网络（多层感知器）。当它们在分类中达到好的结果时，我们可以解释它们的结构为数学假设，然后证明这些假设为定理。因此，我们发现了新的便利的拥 invariants，包括完整的平板拥 invariants。Note: "拥" (wò) in the text refers to "braids" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="On-the-Expressivity-of-Multidimensional-Markov-Reward"><a href="#On-the-Expressivity-of-Multidimensional-Markov-Reward" class="headerlink" title="On the Expressivity of Multidimensional Markov Reward"></a>On the Expressivity of Multidimensional Markov Reward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12184">http://arxiv.org/abs/2307.12184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuwa Miura</li>
<li>for: 本研究考虑了Markov奖励表示在Sequential Decision Making下的表达性。</li>
<li>methods: 我们使用Markov决策过程（MDPs）中的奖励函数来描述代理人希望的行为。我们研究了一个可能性空间中的奖励函数是否可以使得所有希望的策略更加抽象。我们的主要结果表明，如果存在一个可能性空间中的奖励函数，那么这个奖励函数必须满足一定的必要和 suficient conditions。</li>
<li>results: 我们还证明了，对于任何非杂分 deterministic策略集合，存在一个多维度Markov奖励函数可以描述它。<details>
<summary>Abstract</summary>
We consider the expressivity of Markov rewards in sequential decision making under uncertainty. We view reward functions in Markov Decision Processes (MDPs) as a means to characterize desired behaviors of agents. Assuming desired behaviors are specified as a set of acceptable policies, we investigate if there exists a scalar or multidimensional Markov reward function that makes the policies in the set more desirable than the other policies. Our main result states both necessary and sufficient conditions for the existence of such reward functions. We also show that for every non-degenerate set of deterministic policies, there exists a multidimensional Markov reward function that characterizes it
</details>
<details>
<summary>摘要</summary>
我们考虑了马尔可夫奖励在随机决策中的表达性。我们看作奖励函数在马尔可夫决策过程（MDP）中是代表机器人所愿意行为的方式。假设所希望的行为是一个集合的可接受政策，我们实际寻找是否存在一个数值或多维马尔可夫奖励函数，使得这些政策在集合中更加愿意被选择。我们的主要结果表明了这些奖励函数的必要和充分条件，并且显示了每个非当ode deterministic政策都存在一个多维马尔可夫奖励函数，可以Characterize它。
</details></li>
</ul>
<hr>
<h2 id="Security-and-Privacy-Issues-of-Federated-Learning"><a href="#Security-and-Privacy-Issues-of-Federated-Learning" class="headerlink" title="Security and Privacy Issues of Federated Learning"></a>Security and Privacy Issues of Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12181">http://arxiv.org/abs/2307.12181</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JiangChSo/PFLM">https://github.com/JiangChSo/PFLM</a></li>
<li>paper_authors: Jahid Hasan</li>
<li>for: 本研究旨在提供一个涵盖多种机器学习模型的 Federated Learning（FL）安全性和隐私性挑战的全面分类。</li>
<li>methods: 本研究使用了多种攻击和防御策略，包括投毒攻击、后门攻击、会员推理攻击、生成对抗网络（GAN）基于攻击和均衡隐私攻击。</li>
<li>results: 本研究提出了一些新的研究方向，旨在强化FL系统的安全性和隐私性，以保护分布式学习环境中的敏感数据confidentiality。<details>
<summary>Abstract</summary>
Federated Learning (FL) has emerged as a promising approach to address data privacy and confidentiality concerns by allowing multiple participants to construct a shared model without centralizing sensitive data. However, this decentralized paradigm introduces new security challenges, necessitating a comprehensive identification and classification of potential risks to ensure FL's security guarantees. This paper presents a comprehensive taxonomy of security and privacy challenges in Federated Learning (FL) across various machine learning models, including large language models. We specifically categorize attacks performed by the aggregator and participants, focusing on poisoning attacks, backdoor attacks, membership inference attacks, generative adversarial network (GAN) based attacks, and differential privacy attacks. Additionally, we propose new directions for future research, seeking innovative solutions to fortify FL systems against emerging security risks and uphold sensitive data confidentiality in distributed learning environments.
</details>
<details>
<summary>摘要</summary>
受领 learning（FL）已经出现为一种有前途的方法，以解决数据隐私和安全性问题，使得多个参与者可以构建共享模型，而不需要中央化敏感数据。然而，这种分布式模式带来了新的安全挑战，需要进行全面的风险识别和分类，以确保FL的安全保证。这篇论文提出了FL中安全和隐私挑战的完整分类，包括聚合器和参与者的攻击，以及毒ous attacks、后门攻击、会员推理攻击、基于GAN的攻击和权威隐私攻击。此外，我们还提出了未来研究的新方向，寻找创新的解决方案，以固化FL系统，防止数据泄露和分布式学习环境中的安全风险。
</details></li>
</ul>
<hr>
<h2 id="Named-Entity-Resolution-in-Personal-Knowledge-Graphs"><a href="#Named-Entity-Resolution-in-Personal-Knowledge-Graphs" class="headerlink" title="Named Entity Resolution in Personal Knowledge Graphs"></a>Named Entity Resolution in Personal Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12173">http://arxiv.org/abs/2307.12173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayank Kejriwal</li>
<li>for: 本文主要针对个人知识图（PKG）中的命名实体解析（ER）问题。</li>
<li>methods: 本文首先提供了高质量和高效的ER问题定义和必需组件。然后，总结了现有技术在PKG中的应用可能性，以及预期在Web级数据中出现的挑战。</li>
<li>results: 本文结束后，提供了一些应用和未来研究的可能性。<details>
<summary>Abstract</summary>
Entity Resolution (ER) is the problem of determining when two entities refer to the same underlying entity. The problem has been studied for over 50 years, and most recently, has taken on new importance in an era of large, heterogeneous 'knowledge graphs' published on the Web and used widely in domains as wide ranging as social media, e-commerce and search. This chapter will discuss the specific problem of named ER in the context of personal knowledge graphs (PKGs). We begin with a formal definition of the problem, and the components necessary for doing high-quality and efficient ER. We also discuss some challenges that are expected to arise for Web-scale data. Next, we provide a brief literature review, with a special focus on how existing techniques can potentially apply to PKGs. We conclude the chapter by covering some applications, as well as promising directions for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>实体归一化（ER）问题是确定两个实体指向同一个基础实体的问题。该问题已经被研究了50多年，并在最近在大量、多元知识图（knowledge graphs）在社交媒体、电商和搜索等领域广泛使用后，又得到了新的重要性。本章将讨论个人知识图（PKGs）中的命名实体归一化问题。我们开始于形式定义问题和完成高质量和高效的实体归一化所需的组件。我们还讨论了预期出现在网络规模数据上的挑战。接下来，我们提供了一 brief文献综述，强调如何现有技术可能应用于PKGs。我们结束本章，涵盖一些应用以及未来研究的承诺。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters"><a href="#Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters" class="headerlink" title="Optimized Network Architectures for Large Language Model Training with Billions of Parameters"></a>Optimized Network Architectures for Large Language Model Training with Billions of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12169">http://arxiv.org/abs/2307.12169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani</li>
<li>for: 这个论文挑战了训练大语言模型（LLM）的传统建立任意对任意网络的思路。</li>
<li>methods: 我们表明了 LLM 的通信模式，只有小组 GPU 需要高带宽任意对任意通信，以达到训练性能的近似最优点。在这些小组 GPU 之间，通信几乎不存在，稀疏、同质化。我们提议一种新的网络架构，与 LLM 的通信需求相匹配。我们将集群分为 HB Domain 中的 GPU，并使用非堵塞任意对任意高速交换机连接这些 GPU。在 HB Domain 内，网络只连接了需要通信的 GPU。我们称这种网络为 “铁路仅” 连接，并证明我们的提议架构可以将网络成本降低至最多 75%，不对 LLM 训练性能产生影响。</li>
<li>results: 我们的实验结果表明，我们的提议架构可以减少网络成本至最多 75%，而无需增加训练时间或缓存大小。此外，我们的架构还可以在不同的 GPU 分布和通信占比情况下保持较高的性能稳定性。<details>
<summary>Abstract</summary>
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Hallucination-Improves-the-Performance-of-Unsupervised-Visual-Representation-Learning"><a href="#Hallucination-Improves-the-Performance-of-Unsupervised-Visual-Representation-Learning" class="headerlink" title="Hallucination Improves the Performance of Unsupervised Visual Representation Learning"></a>Hallucination Improves the Performance of Unsupervised Visual Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12168">http://arxiv.org/abs/2307.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Wu, Jennifer Hobbs, Naira Hovakimyan</li>
<li>for: 提高自主学习中的对比学习性能</li>
<li>methods: 基于Siamese结构的对比学习模型，加入Hallucinator来生成额外正例样本，提高对比学习的semantic对比和鲁棒性</li>
<li>results: 在不同的对比学习模型和数据集上，通过Hallucinator来生成额外正例样本，可以提高对比学习模型的稳定性和对应性，并且在下游任务中也可以看到明显的提升。<details>
<summary>Abstract</summary>
Contrastive learning models based on Siamese structure have demonstrated remarkable performance in self-supervised learning. Such a success of contrastive learning relies on two conditions, a sufficient number of positive pairs and adequate variations between them. If the conditions are not met, these frameworks will lack semantic contrast and be fragile on overfitting. To address these two issues, we propose Hallucinator that could efficiently generate additional positive samples for further contrast. The Hallucinator is differentiable and creates new data in the feature space. Thus, it is optimized directly with the pre-training task and introduces nearly negligible computation. Moreover, we reduce the mutual information of hallucinated pairs and smooth them through non-linear operations. This process helps avoid over-confident contrastive learning models during the training and achieves more transformation-invariant feature embeddings. Remarkably, we empirically prove that the proposed Hallucinator generalizes well to various contrastive learning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linear classification protocol, a stable accuracy gain is achieved, ranging from 0.3% to 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement is also observed in transferring pre-train encoders to the downstream tasks, including object detection and segmentation.
</details>
<details>
<summary>摘要</summary>
对比学习模型基于单胞结构的表现备受关注，特别是在自监督学习中。然而，这些框架的成功受到两个条件的限制：一个是足够多的正例对，另一个是正例对之间的差异。如果这两个条件不充分满足，这些框架将缺乏含义的对比，并且容易过滤。为了解决这两个问题，我们提出了“幻觉”（Hallucinator），它可以快速生成更多的正例对，并且让这些对之间的差异更加明显。幻觉是可微的，并且在预训任务中直接优化。此外，我们还将幻觉的对应对采用非线性操作，以避免模型在训练过程中过滤。我们实际上证明，提案的幻觉可以跨多种对比学习模型，包括MoCoV1&V2、SimCLR和SimSiam，在线性分类协议下表现稳定，从CIFAR10&100、Tiny ImageNet、STL-10和ImageNet上获得了0.3%到3.0%的稳定精度提升。此外，幻觉也可以在预训对象检测和分类等下游任务中实现稳定的提升。
</details></li>
</ul>
<hr>
<h2 id="The-Imitation-Game-Detecting-Human-and-AI-Generated-Texts-in-the-Era-of-Large-Language-Models"><a href="#The-Imitation-Game-Detecting-Human-and-AI-Generated-Texts-in-the-Era-of-Large-Language-Models" class="headerlink" title="The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models"></a>The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12166">http://arxiv.org/abs/2307.12166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kadhim Hayawi, Sakib Shahriar, Sujith Samuel Mathew</li>
<li>for: 本研究旨在探讨人工智能（AI）基于大型语言模型（LLM）在教育、研究和实践中的潜在潜力，但分辨人类写作和AI生成文本已成为一项重要任务。</li>
<li>methods: 本研究采用了多种机器学习模型来分类文本，并 introduce了一个新的人类写作和LLM生成文本的数据集，包括了不同类型的文章、短篇小说、诗歌和Python代码。</li>
<li>results: 结果表明这些机器学习模型在分类文本时表现出色，即使数据集的样本数较少，但在分类GPT生成文本时，特别是在故事写作方面，任务变得更加困难。结果还表明这些模型在二分类任务中，如人类写作与特定LLM之间的分类，表现出更高的性能，而在多类任务中，如分辨人类写作和多个LLM之间的分类，任务变得更加复杂。<details>
<summary>Abstract</summary>
The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionizing education, research, and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This paper presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry, and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the dataset's limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-generated and multiple LLMs. Our findings provide insightful implications for AI text detection while our dataset paves the way for future research in this evolving area.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）基于大型语言模型（LLM）的潜力在教育、研究和实践中具有巨大的推动力。然而，分辨人类写作和AI生成文本已成为一项重要的任务。这篇论文介绍了一项比较研究，推出了一个新的人类写作和LLM生成文本的数据集，包括了不同类型的文章、故事、诗歌和Python代码。我们使用了多种机器学习模型来分类文本。结果表明这些模型在分类人类写作和AI生成文本的任务中具有remarkable的表现，即使数据集的样本数较少。然而，当分类GPT生成文本时，特别是在故事创作中，任务变得更加困难。结果表明这些模型在二分类任务中，如人类写作与特定LLM的分类，表现更加出色，与多类任务，如分类人类写作和多个LLM之间的分类，相比较，任务变得更加复杂。我们的发现对AI文本检测具有深刻的意义，而我们的数据集也为未来这一领域的研究开辟了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft"><a href="#DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft" class="headerlink" title="DIP-RL: Demonstration-Inferred Preference Learning in Minecraft"></a>DIP-RL: Demonstration-Inferred Preference Learning in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12158">http://arxiv.org/abs/2307.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellen Novoseller, Vinicius G. Goecks, David Watkins, Josh Miller, Nicholas Waytowich</li>
<li>For:	+ The paper is written for researchers and practitioners in the field of reinforcement learning and machine learning, particularly those interested in using human demonstrations to guide learning in unstructured and open-ended environments.* Methods:	+ The paper presents Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), a novel algorithm that leverages human demonstrations in three distinct ways: training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL.* Results:	+ The paper evaluates DIP-RL in a tree-chopping task in Minecraft, and finds that the method can guide an RL agent to learn a reward function that reflects human preferences. Additionally, DIP-RL performs competitively relative to baselines. Example trajectory rollouts of DIP-RL and baselines are available online.<details>
<summary>Abstract</summary>
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspired by our previous work on combining demonstrations and pairwise preferences in Minecraft, which was awarded a research prize at the 2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in Minecraft. Example trajectory rollouts of DIP-RL and baselines are located at https://sites.google.com/view/dip-rl.
</details>
<details>
<summary>摘要</summary>
机器学习中的决策过程中，一个算法代理人会在环境中互动，并从环境接收回报信号作为反馈。但在许多不结构化的实际场景中，这种奖励信号是未知的，人们无法可靠地制定一个正确捕捉愿望行为的奖励信号。为解决这类无结构和开放的环境中的任务，我们提出了人示示 preference reinforcement learning（DIP-RL）算法，它利用人类示例的三种方式，包括训练自动编码器、使用示例数据种子RL训练批次，以及从示例数据中推断行为偏好，以学习一个奖励函数，以引导RL。我们在Minecraft中进行了树割任务的评估，结果表明，DIP-RL可以引导一个RL代理人学习一个奖励函数，满足人类的偏好，并且DIP-RL与基elines相比，表现竞争力强。DIP-RL的灵感来自我们在Minecraft中结合示例和对比式偏好的研究，赢得2022年NeurIPS MineRL BASALT大赛的研究奖，《从人类反馈学习在Minecraft》。有关DIP-RL和基elines的示例轨迹演示，请参考https://sites.google.com/view/dip-rl。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning"><a href="#Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning" class="headerlink" title="Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning"></a>Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12143">http://arxiv.org/abs/2307.12143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqeel13932/mn_project">https://github.com/aqeel13932/mn_project</a></li>
<li>paper_authors: Aqeel Labash, Florian Fletzer, Daniel Majoral, Raul Vicente</li>
<li>for: 这个论文的目的是研究深度学习代理人在有可靠 periodic 变化的环境中学习寻食任务时，是否可以emerge circadian-like rhythms。</li>
<li>methods: 作者使用了深度学习代理人，在一个可靠 periodic 变化的环境中解决寻食任务。在学习过程中，作者系统地Characterize了代理人的行为，并证明了代理人内部的rhythm是自适应的和可调整的。</li>
<li>results: 研究发现，代理人在学习过程中emerge一种自适应的rhythm，这种rhythm可以适应环境的signal phase的变化，而无需重新训练。此外，作者通过分析bifurcation和相对应 Curve的方法，表明了人工神经元的动力学特性是支持内部 rhythm 的内化的关键。<details>
<summary>Abstract</summary>
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows an optimal phase synchronisation between the agent's dynamics and the environmental rhythm.
</details>
<details>
<summary>摘要</summary>
适应环境的规律是生物体可能预测事件和规划的关键。一个明显的例子是Circadian rhythm，即生物体内化地球的24小时轮征。在这项工作中，我们研究了深度强化学习代理人在环境中的Circadian-like rhythm的出现。特别是，我们在一个可靠的周期变化环境中部署了代理人，并解决了捕食任务。我们系统地描述了代理人在学习过程中的行为，并证明了代理人内生的频率可以适应环境的阶段变化，而无需重新训练。此外，我们通过分支和相对响应曲线分析，展示了人工神经元发展的动力学支持内化环境的频率。从动态系统的视角来看，我们证明了适应过程中的稳定 periodic orbit 在神经元动力学中出现，并且该频率允许代理人的动力学和环境频率进行优化的相对同步。
</details></li>
</ul>
<hr>
<h2 id="Route-Planning-Using-Nature-Inspired-Algorithms"><a href="#Route-Planning-Using-Nature-Inspired-Algorithms" class="headerlink" title="Route Planning Using Nature-Inspired Algorithms"></a>Route Planning Using Nature-Inspired Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12133">http://arxiv.org/abs/2307.12133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyansh Saxena, Raahat Gupta, Akshat Maheshwari</li>
<li>for: 本文主要用于介绍 Nature-Inspired Algorithms（NIAs），以及它们在路径规划问题中的应用。</li>
<li>methods: 本文使用了多种 Nature-Inspired Algorithms，包括遗传算法、社会间气候算法、蜂群算法等。</li>
<li>results: 本文通过对路径规划问题的解决，显示了 NIAs 的优化性和可靠性。<details>
<summary>Abstract</summary>
There are many different heuristic algorithms for solving combinatorial optimization problems that are commonly described as Nature-Inspired Algorithms (NIAs). Generally, they are inspired by some natural phenomenon, and due to their inherent converging and stochastic nature, they are known to give optimal results when compared to classical approaches. There are a large number of applications of NIAs, perhaps the most popular being route planning problems in robotics - problems that require a sequence of translation and rotation steps from the start to the goal in an optimized manner while avoiding obstacles in the environment. In this chapter, we will first give an overview of Nature-Inspired Algorithms, followed by their classification and common examples. We will then discuss how the NIAs have applied to solve the route planning problem.
</details>
<details>
<summary>摘要</summary>
《自然引导算法》是解决 combinatorial optimization 问题的多种不同规则算法。通常它们是根据自然现象 inspirited，因为它们的内置的叠合和随机性，可以给出优化的结果，比 класси方法更好。有很多应用的NIAs，最受欢迎的是机器人路径规划问题——需要从起始点到目标点按一个优化的轨迹和旋转步骤，避免环境中的障碍物。本章首先给出了 Nature-Inspired Algorithms 的概述，然后分类和常见的例子，最后讨论了 NIAs 如何应用于路径规划问题。
</details></li>
</ul>
<hr>
<h2 id="AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities"><a href="#AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities" class="headerlink" title="AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities"></a>AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12128">http://arxiv.org/abs/2307.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Victoria Wangia-Anderson, Ahmed Abdelgawad</li>
<li>for: 这篇论文主要是为了提高交通管理和交通事故预防。</li>
<li>methods: 该论文提出了一种基于交通监测摄像头和动作识别系统的交通事故探测和应对方案。</li>
<li>results: 该方案可以减少交通事故的频率和严重程度，提高交通管理的效率和安全性。<details>
<summary>Abstract</summary>
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve traffic management and traffic accident severity. Overall, this study provides valuable insights into traffic accidents in the US and presents a practical solution to enhance the safety and efficiency of transportation systems.
</details>
<details>
<summary>摘要</summary>
意外探测和交通分析是智能城市和自动化交通系统的关键组成部分，可以降低意外频率、严重程度并改善总体交通管理。这篇论文对美国各地的交通意外进行了全面分析，使用国家公路安全管理局（NHTSA）的交通事故报告采样系统（CRSS）的数据。为了解决意外探测和交通分析的挑战，这篇论文提出了一个框架，该框架使用交通监测摄像头和动作认知系统来自动探测和应对交通意外。将该框架与急救服务集成，可以利用交通摄像头和机器学习算法来创造一个高效的交通意外应急处理解决方案。智能技术，如提议的交通意外探测系统，将改善交通管理和交通意外严重程度。总之，这篇研究对美国交通意外提供了有价值的意见，并提出了一个实用的解决方案，以提高交通系统的安全性和效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.AI_2023_07_23/" data-id="clp8zxr1n0013n6888jz26603" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.CL_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T11:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.CL_2023_07_23/">cs.CL - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="X-CapsNet-For-Fake-News-Detection"><a href="#X-CapsNet-For-Fake-News-Detection" class="headerlink" title="X-CapsNet For Fake News Detection"></a>X-CapsNet For Fake News Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12332">http://arxiv.org/abs/2307.12332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Hadi Goldani, Reza Safabakhsh, Saeedeh Momtazi</li>
<li>for: 本研究旨在帮助减少社交媒体和网络论坛上的谣言对用户决策产生的影响，通过自动检测和抵御假新闻。</li>
<li>methods: 该研究提出了一种基于变换器的模型，称为X-CapsNet，该模型包括一个带有动态路由算法的 capsule神经网络（CapsNet），以及一个大小基于的分类器。</li>
<li>results: 研究使用了 Covid-19 和 Liar 数据集进行评估，结果表明，模型在 Covid-19 数据集上的 F1 分数和 Liar 数据集上的准确率都高于现有基线。<details>
<summary>Abstract</summary>
News consumption has significantly increased with the growing popularity and use of web-based forums and social media. This sets the stage for misinforming and confusing people. To help reduce the impact of misinformation on users' potential health-related decisions and other intents, it is desired to have machine learning models to detect and combat fake news automatically. This paper proposes a novel transformer-based model using Capsule neural Networks(CapsNet) called X-CapsNet. This model includes a CapsNet with dynamic routing algorithm paralyzed with a size-based classifier for detecting short and long fake news statements. We use two size-based classifiers, a Deep Convolutional Neural Network (DCNN) for detecting long fake news statements and a Multi-Layer Perceptron (MLP) for detecting short news statements. To resolve the problem of representing short news statements, we use indirect features of news created by concatenating the vector of news speaker profiles and a vector of polarity, sentiment, and counting words of news statements. For evaluating the proposed architecture, we use the Covid-19 and the Liar datasets. The results in terms of the F1-score for the Covid-19 dataset and accuracy for the Liar dataset show that models perform better than the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
新闻消耗量已经明显增加，这与网络讨论平台和社交媒体的普及和使用有着直接关系。这种情况设置了误导和混淆人们的场景，为了帮助用户避免基于误information的决策，自动检测和抗误information的机器学习模型变得越来越重要。本文提出了一种基于变换器的新型模型，称为X-CapsNet，该模型包括一个具有动态路由算法的Capsule神经网络（CapsNet）和一个大小分类器。我们使用了两个大小分类器，一个是深度卷积神经网络（DCNN）用于检测长 fake news 声明，另一个是多层感知神经网络（MLP）用于检测短新闻声明。为解决短新闻声明的表示问题，我们使用了 indirect 特征，即新闻发布人的 Profile 向量和新闻声明中的负面情感和计数词向量。为评估提议的体系，我们使用了 Covid-19 和 Liar 数据集。结果表明，模型在 Covid-19 数据集上的 F1 分数和 Liar 数据集上的准确率都高于当前基eline。
</details></li>
</ul>
<hr>
<h2 id="Milimili-Collecting-Parallel-Data-via-Crowdsourcing"><a href="#Milimili-Collecting-Parallel-Data-via-Crowdsourcing" class="headerlink" title="Milimili. Collecting Parallel Data via Crowdsourcing"></a>Milimili. Collecting Parallel Data via Crowdsourcing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12282">http://arxiv.org/abs/2307.12282</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alantonov/milimili">https://github.com/alantonov/milimili</a></li>
<li>paper_authors: Alexander Antonov</li>
<li>for: 这个研究是为了提出一种通过协同劳动来收集并构建平行 Corpora的方法，比聘用专业翻译人员更加经济。</li>
<li>methods: 这种方法利用了互联网平台，通过吸引志愿者参与翻译来收集数据，并使用机器学习算法来进行自动评分。</li>
<li>results: 研究人员通过实验对Chechen-Russian和Fula-English语种的平行数据进行了收集和分析，并发现这种方法可以提供高质量的平行数据，但需要进一步的优化和纠正。<details>
<summary>Abstract</summary>
We present a methodology for gathering a parallel corpus through crowdsourcing, which is more cost-effective than hiring professional translators, albeit at the expense of quality. Additionally, we have made available experimental parallel data collected for Chechen-Russian and Fula-English language pairs.
</details>
<details>
<summary>摘要</summary>
我们提出了一种使用人工协助收集并行文献的方法，这比聘请专业翻译人员更加经济，然而品质可能受到影响。此外，我们已经为Chechen-Russian和Fula-English语种对 experimental parallel数据进行了收集。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-Joint-Source-Channel-Coding-for-Textual-Semantic-Communication"><a href="#Transformer-based-Joint-Source-Channel-Coding-for-Textual-Semantic-Communication" class="headerlink" title="Transformer-based Joint Source Channel Coding for Textual Semantic Communication"></a>Transformer-based Joint Source Channel Coding for Textual Semantic Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12266">http://arxiv.org/abs/2307.12266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shicong Liu, Zhen Gao, Gaojie Chen, Yu Su, Lu Peng</li>
<li>for: 本文提出了一种文本semantic传输框架，以提高在雷达环境下的文本传输可靠性和效率。</li>
<li>methods: 本文使用了高级自然语言处理技术，将文本句子分解成单词，并使用Transformer编码器进行semantic提取。编码后的数据被归一化为固定长度二进制序列，并对这些二进制序列进行了模拟隐藏 Markov chain 的扩展。</li>
<li>results:  simulationResults表明，提出的模型在semantic传输中具有较高的可靠性和效率，并且在雷达环境下能够有效地抗抗干扰。<details>
<summary>Abstract</summary>
The Space-Air-Ground-Sea integrated network calls for more robust and secure transmission techniques against jamming. In this paper, we propose a textual semantic transmission framework for robust transmission, which utilizes the advanced natural language processing techniques to model and encode sentences. Specifically, the textual sentences are firstly split into tokens using wordpiece algorithm, and are embedded to token vectors for semantic extraction by Transformer-based encoder. The encoded data are quantized to a fixed length binary sequence for transmission, where binary erasure, symmetric, and deletion channels are considered for transmission. The received binary sequences are further decoded by the transformer decoders into tokens used for sentence reconstruction. Our proposed approach leverages the power of neural networks and attention mechanism to provide reliable and efficient communication of textual data in challenging wireless environments, and simulation results on semantic similarity and bilingual evaluation understudy prove the superiority of the proposed model in semantic transmission.
</details>
<details>
<summary>摘要</summary>
天空地海集成网络呼吁更加robust和安全的传输技术以抗干扰。在这篇论文中，我们提出了文本semantic传输框架，利用高级自然语言处理技术来模型和编码句子。具体来说，文本句子首先使用wordpiece算法拆分成单词，然后将单词embedding到token vector中进行semantic提取。编码后的数据被归一化为固定长度二进制序列进行传输，并考虑了三种渠道（ binary erasure、symmetric 和deletion）的传输。接收到的二进制序列被transformer解码器解码成原始句子中的单词，并用于句子重建。我们提出的方法利用神经网络和注意机制提供了可靠和高效的文本数据在困难无线环境中的传输，并在语义传输方面进行了严格的验证和评估。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition"><a href="#A-meta-learning-scheme-for-fast-accent-domain-expansion-in-Mandarin-speech-recognition" class="headerlink" title="A meta learning scheme for fast accent domain expansion in Mandarin speech recognition"></a>A meta learning scheme for fast accent domain expansion in Mandarin speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12262">http://arxiv.org/abs/2307.12262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziwei Zhu, Changhao Shan, Bihong Zhang, Jian Yu</li>
<li>for: 这篇论文主要用于探讨普通话自动语音识别（ASR）中腔点域扩展问题。</li>
<li>methods: 该论文使用元学习技术来实现快速普通话腔点扩展，包括冻结模型参数以及元学习。</li>
<li>results: 相比基eline模型，该方法在腔点扩展任务中显示出3%的相对提升，并在大量数据下达到4%的相对提升。<details>
<summary>Abstract</summary>
Spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set.
</details>
<details>
<summary>摘要</summary>
⟨SYS⟩ spoken languages show significant variation across mandarin and accent. Despite the high performance of mandarin automatic speech recognition (ASR), accent ASR is still a challenge task. In this paper, we introduce meta-learning techniques for fast accent domain expansion in mandarin speech recognition, which expands the field of accents without deteriorating the performance of mandarin ASR. Meta-learning or learn-to-learn can learn general relation in multi domains not only for over-fitting a specific domain. So we select meta-learning in the domain expansion task. This more essential learning will cause improved performance on accent domain extension tasks. We combine the methods of meta learning and freeze of model parameters, which makes the recognition performance more stable in different cases and the training faster about 20%. Our approach significantly outperforms other methods about 3% relatively in the accent domain expansion task. Compared to the baseline model, it improves relatively 37% under the condition that the mandarin test set remains unchanged. In addition, it also proved this method to be effective on a large amount of data with a relative performance improvement of 4% on the accent test set. traslated by Google Translate
</details></li>
</ul>
<hr>
<h2 id="MyVoice-Arabic-Speech-Resource-Collaboration-Platform"><a href="#MyVoice-Arabic-Speech-Resource-Collaboration-Platform" class="headerlink" title="MyVoice: Arabic Speech Resource Collaboration Platform"></a>MyVoice: Arabic Speech Resource Collaboration Platform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02503">http://arxiv.org/abs/2308.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yousseif Elshahawy, Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali</li>
<li>for: 增强阿拉伯语言技术的研究</li>
<li>methods: 使用拥有者参与的人群征集平台收集阿拉伯语言录音，并提供公共数据集</li>
<li>results: 成功创建了大量的 диалект语言录音数据集，并提供了可Switch用户角色的功能，以及一个质量筛选和反馈系统，以保证数据的质量。<details>
<summary>Abstract</summary>
We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.
</details>
<details>
<summary>摘要</summary>
我们介绍MyVoice，一个人工智能平台，旨在收集阿拉伯语言的口语，以提高方言技术。这个平台提供了大规模的方言语音数据的设计机会，并将其公开提供。MyVoice让参与者选择城市/国家精细方言，并录制显示的句子。用户可以在参与者和注释者之间进行Switch角色。平台包含一个质量保证系统，将低质量和假的录音过滤掉，然后将其发送到验证。在验证阶段，参与者可以评估录音质量，注释和提供反馈，这些反馈会被管理员审核。此外，平台还允许管理员添加新的数据或任务，超出方言语音和单词收集，这些任务将被显示给参与者。因此，MyVoice可以促进多方合作，收集多样化和大规模的阿拉伯语言数据。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation"><a href="#Exploring-the-Integration-of-Speech-Separation-and-Recognition-with-Self-Supervised-Learning-Representation" class="headerlink" title="Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation"></a>Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12231">http://arxiv.org/abs/2307.12231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe</li>
<li>for: 这个论文的目的是提出一种基于自适应学习表示的多 speaker自然语音识别系统。</li>
<li>methods: 本文使用多通道分离方法、封闭抑制映射和复杂 spectral mapping，以及最佳的ASR后端模型特征。</li>
<li>results: 研究人员通过使用最新的自我超级vised学习表示（SSLR）来提高filterbank特征下的识别性能，并通过合理的训练策略将speech separation和识别 integrate into一个系统，实现了WHAMR! reverberation测试集的2.5%字幕误差率，与现有的mask-based MVDR抽样抑制和filterbank综合integration（28.9%）相比，表现显著提高。<details>
<summary>Abstract</summary>
Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
</details>
<details>
<summary>摘要</summary>
“神经语音分离技术在过去几年中已经做出了很大的进步，其与自动语音识别（ASR）的结合是实现多个说话人ASR的重要方向。本文提供了详细的语音分离在噪音混响和噪音混响 scenarios中的研究，作为ASR前端。 Specifically, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Identifying-Misinformation-on-YouTube-through-Transcript-Contextual-Analysis-with-Transformer-Models"><a href="#Identifying-Misinformation-on-YouTube-through-Transcript-Contextual-Analysis-with-Transformer-Models" class="headerlink" title="Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models"></a>Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12155">http://arxiv.org/abs/2307.12155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/christoschr97/misinf-detection-llms">https://github.com/christoschr97/misinf-detection-llms</a></li>
<li>paper_authors: Christos Christodoulou, Nikos Salamanos, Pantelitsa Leonidou, Michail Papadakis, Michael Sirivianos</li>
<li>for: 本研究旨在提出一种新的视频分类方法，以确定视频内容的真实性。</li>
<li>methods: 本方法利用视频转cript中的文本内容，将传统的视频分类任务转化为文本分类任务。采用高级机器学习技术，如传输学习和少量学习。</li>
<li>results: 在三个dataset上进行评估，包括YouTube疫苗谣言相关视频、YouTube pseudoscience视频和一个新闻假消息集合。 fine-tuned模型的 Matthews Correlation Coefficient&gt;0.81，准确率&gt;0.90和F1 score&gt;0.90。而少量学习模型在YouTube pseudoscience数据集上比 fine-tuned模型高20%的准确率和F1 score。<details>
<summary>Abstract</summary>
Misinformation on YouTube is a significant concern, necessitating robust detection strategies. In this paper, we introduce a novel methodology for video classification, focusing on the veracity of the content. We convert the conventional video classification task into a text classification task by leveraging the textual content derived from the video transcripts. We employ advanced machine learning techniques like transfer learning to solve the classification challenge. Our approach incorporates two forms of transfer learning: (a) fine-tuning base transformer models such as BERT, RoBERTa, and ELECTRA, and (b) few-shot learning using sentence-transformers MPNet and RoBERTa-large. We apply the trained models to three datasets: (a) YouTube Vaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and (c) Fake-News dataset (a collection of articles). Including the Fake-News dataset extended the evaluation of our approach beyond YouTube videos. Using these datasets, we evaluated the models distinguishing valid information from misinformation. The fine-tuned models yielded Matthews Correlation Coefficient>0.81, accuracy>0.90, and F1 score>0.90 in two of three datasets. Interestingly, the few-shot models outperformed the fine-tuned ones by 20% in both Accuracy and F1 score for the YouTube Pseudoscience dataset, highlighting the potential utility of this approach -- especially in the context of limited training data.
</details>
<details>
<summary>摘要</summary>
伪信息在YouTube上是一项重要的问题，需要 robust的检测策略。在这篇论文中，我们介绍了一种新的方法ology for video classification, ocus on 视频内容的真实性。我们将传统的视频分类任务转化为文本分类任务，通过利用视频字幕中的文本内容。我们采用了先进的机器学习技术，如传输学习，解决分类挑战。我们的方法包括两种形式的传输学习：（a）精度调整基于BERT、RoBERTa和ELECTRA的transformer模型，以及（b）几shot学习使用 sentence-transformers MPNet和RoBERTa-large。我们对三个数据集进行了应用：（a）YouTube疫苗谣言相关视频，（b）YouTube pseudo科学视频，以及（c） fake news数据集（一个收集了文章）。通过这些数据集，我们评估了我们的方法可以分辨真实信息和伪信息。精度调整模型的 Matthews Correlation Coefficient>0.81，准确率>0.90，和F1分数>0.90在三个数据集中都达到了。有意思的是，几shot模型在YouTube pseudo科学数据集上比精度调整模型高出20%的准确率和F1分数，这highlights了这种方法在有限的训练数据情况下的潜在实用性。
</details></li>
</ul>
<hr>
<h2 id="Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding"><a href="#Modality-Confidence-Aware-Training-for-Robust-End-to-End-Spoken-Language-Understanding" class="headerlink" title="Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding"></a>Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12134">http://arxiv.org/abs/2307.12134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, Michael L. Seltzer</li>
<li>for: 提高 END-to-END 语言理解系统的稳定性，增强对听写错误的耐受能力。</li>
<li>methods: 提出一种新的 END-to-END 语言理解系统，通过融合音频和文本表示来增强对听写错误的耐受能力，并采用两种新技术：1）有效地编码听写错误的质量信息，2）有效地将其集成到 END-to-END 语言理解模型中。</li>
<li>results: 在 STOP 数据集上实现了准确率的提高，并进行了分析，证明了我们的方法的有效性。<details>
<summary>Abstract</summary>
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
最近，终端到终端（E2E）的语音理解系统（SLU）已经变得更加有前途。这种方法使用单个模型，利用采样和文本表示从预训练的语音识别模型（ASR）中获得，并在设备流动中超越传统的管道式SLU系统。然而，E2E SLU系统仍然在文本表示质量低下表现不佳，即使使用ASR转译错误。为解决这个问题，我们提出了一种新的E2E SLU系统，增强了ASR错误的鲁棒性。我们介绍了两种新技术：1）有效的ASR假设质量编码方法和2）有效的E2E SLU模型集成方法。我们在STOP数据集上显示了准确率改善，并进行分析，以证明我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Topic-Enhanced-Argument-Mining-from-Heterogeneous-Sources"><a href="#Explainable-Topic-Enhanced-Argument-Mining-from-Heterogeneous-Sources" class="headerlink" title="Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources"></a>Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12131">http://arxiv.org/abs/2307.12131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiasheng Si, Yingjie Zhu, Xingyu Shi, Deyu Zhou, Yulan He</li>
<li>for: 本文提出了一种新的可解释话题增强的论据挖掘方法，以提高论据挖掘的精度和效果。</li>
<li>methods: 本文使用了神经网络话题模型和语言模型，将目标信息补充了可解释话题表示，并通过共同学习来捕捉在论据中的句子水平话题信息。</li>
<li>results: 实验结果表明，提出的方法在benchmark数据集上在各种设置下都有显著优势，与现有基线模型相比。<details>
<summary>Abstract</summary>
Given a controversial target such as ``nuclear energy'', argument mining aims to identify the argumentative text from heterogeneous sources. Current approaches focus on exploring better ways of integrating the target-associated semantic information with the argumentative text. Despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argument mining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argument mining approach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic representation through mutual learning. Experiments have been conducted on the benchmark dataset in both the in-target setting and the cross-target setting. Results demonstrate the superiority of the proposed model against the state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
Given a controversial target such as "核能源" (nuclear energy), argument mining aims to identify the argumentative text from heterogeneous sources. Current approaches focus on exploring better ways of integrating the target-associated semantic information with the argumentative text. Despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argument mining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argument mining approach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic representation through mutual learning. Experiments have been conducted on the benchmark dataset in both the in-target setting and the cross-target setting. Results demonstrate the superiority of the proposed model against the state-of-the-art baselines.Here's the word-for-word translation:给一个争议性目标，如“核能源”(nuclear energy), argument mining 目标是从不同来源中提取Argumentative text。现有的方法主要关注在更好地将目标相关的语义信息与 argumentative text 集成。despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argument mining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argument mining approach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic representation through mutual learning. Experiments have been conducted on the benchmark dataset in both the in-target setting and the cross-target setting. Results demonstrate the superiority of the proposed model against the state-of-the-art baselines.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.CL_2023_07_23/" data-id="clp8zxr41008wn6881n9yc5nf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/cs.LG_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T10:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/cs.LG_2023_07_23/">cs.LG - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations"><a href="#Right-for-the-Wrong-Reason-Can-Interpretable-ML-Techniques-Detect-Spurious-Correlations" class="headerlink" title="Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?"></a>Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12344">http://arxiv.org/abs/2307.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ss-sun/right-for-the-wrong-reason">https://github.com/ss-sun/right-for-the-wrong-reason</a></li>
<li>paper_authors: Susu Sun, Lisa M. Koch, Christian F. Baumgartner</li>
<li>for:  This paper aims to evaluate the ability of various explanation techniques to identify spurious correlations in deep neural network models.</li>
<li>methods:  The paper proposes a rigorous evaluation strategy to assess the effectiveness of post-hoc explanation techniques and inherently interpretable classifiers in detecting artificially added confounders in a chest x-ray diagnosis task.</li>
<li>results:  The paper finds that the post-hoc technique SHAP and the inherently interpretable Attri-Net provide the best performance in identifying faulty model behavior and can be used to reliably detect spurious correlations.<details>
<summary>Abstract</summary>
While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique's ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.
</details>
<details>
<summary>摘要</summary>
深度神经网络模型可以提供无可比拟的分类性能，但它们容易学习假设关系。这些关系可能是由干扰信息引起的，这些干扰信息可以难以通过性能指标来探测，尤其如果测试数据来自同一个分布。可解释Machine Learning方法，如后处解释或自然可解释的分类器，承诺可以识别模型的错误思维。然而，现有证据表明，许多这些技术并没有充分能力完成这一任务。在这篇文章中，我们提出了一种严格的评估策略，用于评估解释技术的能力是否可以正确地识别假设关系。使用这种策略，我们评估了五种后处解释技术和一种自然可解释的分类器，以 Detect three types of artificially added confounders in a chest x-ray diagnosis task。我们发现，使用SHAP的后处解释技术以及自然可解释的Attri-Net可以提供最好的性能，可以可靠地识别模型的错误行为。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition"><a href="#Self-Supervised-Learning-for-Audio-Based-Emotion-Recognition" class="headerlink" title="Self-Supervised Learning for Audio-Based Emotion Recognition"></a>Self-Supervised Learning for Audio-Based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12343">http://arxiv.org/abs/2307.12343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peranut Nimitsurachat, Peter Washington<br>for: 这个研究的目的是发展一个基于音频资料的情绪识别模型，以便在心理健康、市场营销、游戏和社交媒体分析等领域中建立互动系统。methods: 这个研究使用了自我超级vised learning（SSL）方法，通过预测资料本身的特性来学习，不需要大量的指导标签。results: 这个研究发现，使用SSL方法可以在小量 annotated data 上提高模型的性能，特别是当情绪较易分类时。此外，这个研究还证明了SSL方法在嵌入特征表示空间中进行自我超级vised learning可以实现更好的表现。<details>
<summary>Abstract</summary>
Emotion recognition models using audio input data can enable the development of interactive systems with applications in mental healthcare, marketing, gaming, and social media analysis. While the field of affective computing using audio data is rich, a major barrier to achieve consistently high-performance models is the paucity of available training labels. Self-supervised learning (SSL) is a family of methods which can learn despite a scarcity of supervised labels by predicting properties of the data itself. To understand the utility of self-supervised learning for audio-based emotion recognition, we have applied self-supervised learning pre-training to the classification of emotions from the CMU- MOSEI's acoustic modality. Unlike prior papers that have experimented with raw acoustic data, our technique has been applied to encoded acoustic data. Our model is first pretrained to uncover the randomly-masked timestamps of the acoustic data. The pre-trained model is then fine-tuned using a small sample of annotated data. The performance of the final model is then evaluated via several evaluation metrics against a baseline deep learning model with an identical backbone architecture. We find that self-supervised learning consistently improves the performance of the model across all metrics. This work shows the utility of self-supervised learning for affective computing, demonstrating that self-supervised learning is most useful when the number of training examples is small, and that the effect is most pronounced for emotions which are easier to classify such as happy, sad and anger. This work further demonstrates that self-supervised learning works when applied to embedded feature representations rather than the traditional approach of pre-training on the raw input space.
</details>
<details>
<summary>摘要</summary>
这个文章探讨了使用语音资料进行情感识别的模型，并 explore了这些模型在心理健康、市场营销、游戏和社交媒体分析等领域的应用。然而，对于情感识别模型的训练label scarcity是一个主要的阻碍因素。自动学习（SSL）是一家 мето�odo，可以在训练labels的缺乏情况下培养出高性能的模型。为了了解SSL在语音基本情感识别中的使用效果，我们将运用SSL预训练在CMU-MOSEI的语音模式上进行分组。不同于先前的研究，我们的技术是对编码语音资料进行预训练。我们的模型首先预训练以探索随机遮盾的语音资料时间戳。预训练后，模型会被精确地调整使用一小sample的标注数据。最终的模型性能会通过一些评估度量与基准深度学习模型进行比较。我们发现，透过SSL预训练可以对情感识别模型进行改进，并且这个改进效果是随着标注数据的数量增加而加强。此外，我们发现这个效果尤其明显在易于分组的情感方面，例如：快乐、沮丧和愤怒。这个研究显示了自动学习在情感识别中的 utility，并且显示了这种方法在语音嵌入特征表现上进行预训练的效果。
</details></li>
</ul>
<hr>
<h2 id="Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction"><a href="#Rapid-detection-of-soil-carbonates-by-means-of-NIR-spectroscopy-deep-learning-methods-and-phase-quantification-by-powder-Xray-diffraction" class="headerlink" title="Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction"></a>Rapid detection of soil carbonates by means of NIR spectroscopy, deep learning methods and phase quantification by powder Xray diffraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12341">http://arxiv.org/abs/2307.12341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lykourgos Chiniadis, Petros Tamvakis<br>for: 这个研究旨在提高农业生产和土壤物理特性分析，以实现农业均衡和环境可持续性。methods: 本研究使用FT NIR reflectanceспектроскопія和深度学习方法来预测土壤碳酸含量。results: 研究获得了优异的预测结果，并且在无法使用量imetric方法的情况下，可以快速和有效地预测土壤碳酸含量。<details>
<summary>Abstract</summary>
Soil NIR spectral absorbance/reflectance libraries are utilized towards improving agricultural production and analysis of soil properties which are key prerequisite for agroecological balance and environmental sustainability. Carbonates in particular, represent a soil property which is mostly affected even by mild, let alone extreme, changes of environmental conditions during climate change. In this study we propose a rapid and efficient way to predict carbonates content in soil by means of FT NIR reflectance spectroscopy and by use of deep learning methods. We exploited multiple machine learning methods, such as: 1) a MLP Regressor and 2) a CNN and compare their performance with other traditional ML algorithms such as PLSR, Cubist and SVM on the combined dataset of two NIR spectral libraries: KSSL (USDA), a dataset of soil samples reflectance spectra collected nationwide, and LUCAS TopSoil (European Soil Library) which contains soil sample absorbance spectra from all over the European Union, and use them to predict carbonate content on never before seen soil samples. Soil samples in KSSL and in TopSoil spectral libraries were acquired in the spectral region of visNIR, however in this study, only the NIR spectral region was utilized. Quantification of carbonates by means of Xray Diffraction is in good agreement with the volumetric method and the MLP prediction. Our work contributes to rapid carbonates content prediction in soil samples in cases where: 1) no volumetric method is available and 2) only NIR spectra absorbance data are available. Up till now and to the best of our knowledge, there exists no other study, that presents a prediction model trained on such an extensive dataset with such promising results on unseen data, undoubtedly supporting the notion that deep learning models present excellent prediction tools for soil carbonates content.
</details>
<details>
<summary>摘要</summary>
soil NIR  spectral absorbance/reflectance 图书馆是用于提高农业生产和土壤属性分析的重要前提，这些属性是生态平衡和环境可持续性的关键因素。碳酸盐是土壤属性中受到气候变化的影响最大的一种，因此在这种情况下，我们提出了一种快速和高效的碳酸盐含量预测方法，使用FT NIR 谐振谱分析和深度学习方法。我们利用了多种机器学习方法，如：1）多层感知网络（MLP）回归器，2）卷积神经网络（CNN），并与传统的机器学习算法如PLSR、Cubist和SVM进行比较，用于预测碳酸盐含量。我们使用了KSSL（USDA）和LUCAS TopSoil（欧盟土壤图书馆）两个 spectral 图书馆的共同数据集，其中KSSL包含了全美国的土壤样本谐振谱pectra，而LUCAS TopSoil包含了欧盟各国的土壤样本吸收谱spectra。我们只使用了NIR spectral 区域。我们通过X射晶 diffraction 测量和MLP 预测相比，发现了含碳酸盐的量与volume 方法具有良好一致性。我们的工作可以帮助在没有volume 方法可用时，只有NIR spectra 吸收数据可用时，快速预测土壤中碳酸盐含量。在我们所知道的范围内，没有其他研究可以在这样的广泛数据集上提出类似的预测模型，并且模型在未seen数据上的表现是非常出色，证明了深度学习模型在土壤碳酸盐含量预测中的优秀表现。
</details></li>
</ul>
<hr>
<h2 id="TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models"><a href="#TabADM-Unsupervised-Tabular-Anomaly-Detection-with-Diffusion-Models" class="headerlink" title="TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models"></a>TabADM: Unsupervised Tabular Anomaly Detection with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12336">http://arxiv.org/abs/2307.12336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Zamberg, Moshe Salhov, Ofir Lindenbaum, Amir Averbuch</li>
<li>for: 本研究旨在提出一种基于扩散的 probabilistic 模型，用于不监督的异常检测。</li>
<li>methods: 我们的模型通过使用特殊的拒绝机制，使正常样本的浓度估计免受异常样本的影响。在推断阶段，我们可以通过查找低浓度区域的样本来识别异常样本。</li>
<li>results: 我们使用实际数据进行测试，发现我们的方法可以提高异常检测的能力，并且相比基eline，我们的方法更加稳定和不需要较多的超参数调整。<details>
<summary>Abstract</summary>
Tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.
</details>
<details>
<summary>摘要</summary>
tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.Here's the translation in Traditional Chinese as well:tables are an abundant form of data with use cases across all scientific fields. Real-world datasets often contain anomalous samples that can negatively affect downstream analysis. In this work, we only assume access to contaminated data and present a diffusion-based probabilistic model effective for unsupervised anomaly detection. Our model is trained to learn the density of normal samples by utilizing a unique rejection scheme to attenuate the influence of anomalies on the density estimation. At inference, we identify anomalies as samples in low-density regions. We use real data to demonstrate that our method improves detection capabilities over baselines. Furthermore, our method is relatively stable to the dimension of the data and does not require extensive hyperparameter tuning.
</details></li>
</ul>
<hr>
<h2 id="An-axiomatized-PDE-model-of-deep-neural-networks"><a href="#An-axiomatized-PDE-model-of-deep-neural-networks" class="headerlink" title="An axiomatized PDE model of deep neural networks"></a>An axiomatized PDE model of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12333">http://arxiv.org/abs/2307.12333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tangjun Wang, Wenqi Tao, Chenglong Bao, Zuoqiang Shi</li>
<li>for: 研究深度神经网络（DNN）与 partial differential equations（PDEs）之间的关系，尤其是 DNN 的普遍形式 PDE 模型。</li>
<li>methods: 将 DNN 视为一个进化Operator，从简单的基模型出发，根据一些合理的假设，证明了演化Operator 实际上是受湍涨-扩散方程的推动。</li>
<li>results: 根据演化Operator 的推动，提出了一种新的训练方法，用于改进 ResNet 的性能。实验 validate 了提出的方法的效果。<details>
<summary>Abstract</summary>
Inspired by the relation between deep neural network (DNN) and partial differential equations (PDEs), we study the general form of the PDE models of deep neural networks. To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Based on several reasonable assumptions, we prove that the evolution operator is actually determined by convection-diffusion equation. This convection-diffusion equation model gives mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we design a new training method for ResNets. Experiments validate the performance of the proposed method.
</details>
<details>
<summary>摘要</summary>
Based on the relation between deep neural networks (DNN) and partial differential equations (PDEs), we investigate the general form of PDE models of DNN. To achieve this goal, we formulate DNN as an evolution operator from a simple base model. Under several reasonable assumptions, we prove that the evolution operator is actually determined by a convection-diffusion equation. This convection-diffusion equation model provides a mathematical explanation for several effective networks. Moreover, we show that the convection-diffusion model improves the robustness and reduces the Rademacher complexity. Based on the convection-diffusion equation, we propose a new training method for ResNets. Experimental results validate the performance of the proposed method.Here's the word-for-word translation of the text into Simplified Chinese: Based on Deep Neural Network (DNN) 和 Partial Differential Equations (PDEs) 之间的关系，我们研究 DNN 的总体形式 PDE 模型。为达到这个目标，我们将 DNN 表示为一个从简单基本模型的演化运算器。根据一些合理的假设，我们证明了演化运算器实际上是受湍振-漫步方程的决定。这个湍振-漫步方程模型为许多有效网络提供了数学解释。此外，我们还证明了湍振-漫步模型可以提高稳定性并降低 Rademacher 复杂度。基于湍振-漫步方程，我们提出了一种新的训练方法 для ResNets。实验结果证明了我们的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks"><a href="#Tackling-the-Curse-of-Dimensionality-with-Physics-Informed-Neural-Networks" class="headerlink" title="Tackling the Curse of Dimensionality with Physics-Informed Neural Networks"></a>Tackling the Curse of Dimensionality with Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12306">http://arxiv.org/abs/2307.12306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi</li>
<li>for: 解决高维纬度的物理学定义问题 (solving high-dimensional physical definition problems)</li>
<li>methods: 使用Stochastic Dimension Gradient Descent (SDGD)方法，即将梯度分解成不同维度的部分，并随机选择每个训练轮中的一部分维度进行训练physics-informed neural networks (PINNs)。 (using Stochastic Dimension Gradient Descent (SDGD) method, which decomposes the gradient into parts corresponding to different dimensions and randomly selects a subset of these dimensional parts for training physics-informed neural networks (PINNs))</li>
<li>results: 可以很快地解决很多难以解决的高维度纬度的非线性Partial Differential Equations (PDEs)，例如Hamilton-Jacobi-Bellman (HJB)和Schrödinger方程在千个维度中的解决。 (can solve many notoriously hard high-dimensional PDEs, such as the Hamilton-Jacobi-Bellman (HJB) and the Schrödinger equations in thousands of dimensions very fast)<details>
<summary>Abstract</summary>
The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed method. We experimentally demonstrate that the proposed method allows us to solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. For instance, we solve nontrivial nonlinear PDEs (one HJB equation and one Black-Scholes equation) in 100,000 dimensions in 6 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, SDGD can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.
</details>
<details>
<summary>摘要</summary>
“几何约束”（CoD）会卷用计算资源，计算成本随着维度的增加而呈指数增长。这会对解决高维 partial differential equations（PDEs） pose 大量挑战，Richard Bellman 在60年前就这样注意到。虽然在过去几年，有些研究者通过数值方法解决高维 PDEs，但这些计算却是非常昂贵的，并且true scaling of general nonlinear PDEs to high dimensions 从未实现过。在这篇论文中，我们开发了一种新的方法，即随机维度梯度下降（SDGD），用于扩展 physics-informed neural networks（PINNs）来解决任意高维 PDEs。SDGD 方法将 PDE 的梯度分解成不同维度的部分，并在训练 PINNs 时随机选择这些维度的部分。我们理论上证明了该方法的收敛性和其他愿望的性质。我们实验表明，该方法可以很快地解决许多知名度的高维 PDEs，包括 Hamilton-Jacobi-Bellman 方程和 Schrödinger 方程，并且可以在单个 GPU 上完成。例如，我们在 100,000 维度中解决了一些非线性 PDEs（一个 HJB 方程和一个 Black-Scholes 方程），只用了 6 个 GPU 上的 6 小时。由于 SDGD 是 PINNs 的一种通用训练方法，SDGD 可以应用于任何当前和未来的 PINNs 变体，以扩展它们到任意高维 PDEs。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics"><a href="#Physics-Informed-Machine-Learning-of-Argon-Gas-Driven-Melt-Pool-Dynamics" class="headerlink" title="Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics"></a>Physics-Informed Machine Learning of Argon Gas-Driven Melt Pool Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12304">http://arxiv.org/abs/2307.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Sharma, W. Grace Guo, M. Raissi, Y. B. Guo</li>
<li>for: 这 paper 是关于 metal 添加印制 (AM) 过程中溶融池动态的研究，它们的目的是提高过程的稳定性、微结构形成和印制物的性能。</li>
<li>methods: 这 paper 使用了物理学习 (PIML) 方法，通过将神经网络与物理法律相结合来预测溶融池动态，包括温度、速度和压力等参数。PIML 方法可以避免使用数学模拟方法，从而大幅降低计算成本。</li>
<li>results: 该 paper 通过数据驱动发现了模型常数，并且通过优化 PINN 模型来提高模型训练效率。PIML 方法可以高效地预测溶融池动态，并且可以提供更好的初始条件和边界条件。<details>
<summary>Abstract</summary>
Melt pool dynamics in metal additive manufacturing (AM) is critical to process stability, microstructure formation, and final properties of the printed materials. Physics-based simulation including computational fluid dynamics (CFD) is the dominant approach to predict melt pool dynamics. However, the physics-based simulation approaches suffer from the inherent issue of very high computational cost. This paper provides a physics-informed machine learning (PIML) method by integrating neural networks with the governing physical laws to predict the melt pool dynamics such as temperature, velocity, and pressure without using any training data on velocity. This approach avoids solving the highly non-linear Navier-Stokes equation numerically, which significantly reduces the computational cost. The difficult-to-determine model constants of the governing equations of the melt pool can also be inferred through data-driven discovery. In addition, the physics-informed neural network (PINN) architecture has been optimized for efficient model training. The data-efficient PINN model is attributed to the soft penalty by incorporating governing partial differential equations (PDEs), initial conditions, and boundary conditions in the PINN model.
</details>
<details>
<summary>摘要</summary>
金属加料制造（AM）中的熔 pool 动力学是制造过程稳定性、微结构形成和打印物质的关键因素。基于物理定律的数学模拟（CFD）是预测熔 pool 动力学的主要方法。然而，物理基础的模拟方法受到内置的计算成本高峰问题。这篇论文提出了基于物理学习（PIML）方法，通过将神经网络与管理物理法律相结合来预测熔 pool 动力学的温度、速度和压力，不需要使用任何很速度训练数据。这种方法可以避免数值方法中的高级非线性 Navier-Stokes 方程的解算问题，从而减少计算成本。此外，通过数据驱动发现，可以通过推断模型常数来确定管理方程的困难常数。此外，基于物理学习（PINN）架构已经优化了模型训练效率。通过软约束 penalty，将管理的partial differential equations（PDEs）、初始条件和边界条件 integrate 到 PINN 模型中，使得数据效率的 PINN 模型。
</details></li>
</ul>
<hr>
<h2 id="RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC"><a href="#RANSAC-NN-Unsupervised-Image-Outlier-Detection-using-RANSAC" class="headerlink" title="RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC"></a>RANSAC-NN: Unsupervised Image Outlier Detection using RANSAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12301">http://arxiv.org/abs/2307.12301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mxtsai/ransac-nn">https://github.com/mxtsai/ransac-nn</a></li>
<li>paper_authors: Chen-Han Tsai, Yu-Shao Peng</li>
<li>for: 这个论文旨在提出一种专门为图像数据设计的异常检测算法，以确保计算机视觉任务中使用的图像数据质量和准确性。</li>
<li>methods: 该算法基于RANSAC的方法进行比较图像，自动预测每个图像的异常分数而无需额外训练或标签信息。</li>
<li>results: 对于15种多样化的数据集，RANSAC-NN在与当前状态艺算法进行比较时，无需任何超参数调整，一致地表现出优异性。此外，文章还提供了每个RANSAC-NN组件的详细分析，并展示了其在图像涂抹检测中的潜在应用。<details>
<summary>Abstract</summary>
Image outlier detection (OD) is crucial for ensuring the quality and accuracy of image datasets used in computer vision tasks. The majority of OD algorithms, however, have not been targeted toward image data. Consequently, the results of applying such algorithms to images are often suboptimal. In this work, we propose RANSAC-NN, a novel unsupervised OD algorithm specifically designed for images. By comparing images in a RANSAC-based approach, our algorithm automatically predicts the outlier score of each image without additional training or label information. We evaluate RANSAC-NN against state-of-the-art OD algorithms on 15 diverse datasets. Without any hyperparameter tuning, RANSAC-NN consistently performs favorably in contrast to other algorithms in almost every dataset category. Furthermore, we provide a detailed analysis to understand each RANSAC-NN component, and we demonstrate its potential applications in image mislabeled detection. Code for RANSAC-NN is provided at https://github.com/mxtsai/ransac-nn
</details>
<details>
<summary>摘要</summary>
图像异常检测（OD）是计算机视觉任务中至关重要的质量和准确性因素。大多数OD算法 however，没有特地针对图像数据。因此，将这些算法应用于图像时的结果通常不佳。在这项工作中，我们提出了一种新的无监督OD算法，即RANSAC-NN。我们通过对图像进行RANSAC-based的比较，自动地对每个图像预测异常分数，无需额外的训练或标签信息。我们对RANSAC-NN与现有OD算法进行了15种多样化的数据集评估。无需任何超参数调整，RANSAC-NN在大多数数据集类别中一直表现优于其他算法。此外，我们还提供了每个RANSAC-NN组件的详细分析，并证明其在图像涂抹检测中的潜在应用。RANSAC-NN代码可以在https://github.com/mxtsai/ransac-nn上获取。
</details></li>
</ul>
<hr>
<h2 id="ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder"><a href="#ResWCAE-Biometric-Pattern-Image-Denoising-Using-Residual-Wavelet-Conditioned-Autoencoder" class="headerlink" title="ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder"></a>ResWCAE: Biometric Pattern Image Denoising Using Residual Wavelet-Conditioned Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12255">http://arxiv.org/abs/2307.12255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youzhi Liang, Wen Liang</li>
<li>For: This paper proposes a deep learning architecture for fingerprint image denoising in compact IoT devices, aiming to improve the reliability of biometric authentication systems.* Methods: The proposed method, called Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE), combines image and wavelet encoders with a Kullback-Leibler divergence regularization. It leverages residual connections and wavelet-transform domain features to preserve fine-grained spatial information.* Results: The experimental results show that Res-WCAE outperforms several state-of-the-art denoising methods, particularly for heavily degraded fingerprint images with high levels of noise. The proposed method demonstrates promise for improving the reliability of biometric authentication systems in compact IoT devices.Here’s the simplified Chinese text for the three key points:* For: 这篇论文提出了一种用于 compact IoT 设备中的指纹图像干扰 removing 深度学习架构，以提高生物识别系统的可靠性。* Methods: 提议的方法是 Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE)，它组合了图像编码器和波峰编码器，并使用 Kullback-Leibler 分布regularization。它利用了剩余连接和波峰变换域特征来保留细腻的空间信息。* Results: 实验结果表明，Res-WCAE 比许多状态机器人的干扰方法更高效，特别是对于受到高水平噪声的指纹图像。提议的方法表明，可以提高 compact IoT 设备中生物识别系统的可靠性。<details>
<summary>Abstract</summary>
The utilization of biometric authentication with pattern images is increasingly popular in compact Internet of Things (IoT) devices. However, the reliability of such systems can be compromised by image quality issues, particularly in the presence of high levels of noise. While state-of-the-art deep learning algorithms designed for generic image denoising have shown promise, their large number of parameters and lack of optimization for unique biometric pattern retrieval make them unsuitable for these devices and scenarios. In response to these challenges, this paper proposes a lightweight and robust deep learning architecture, the Residual Wavelet-Conditioned Convolutional Autoencoder (Res-WCAE) with a Kullback-Leibler divergence (KLD) regularization, designed specifically for fingerprint image denoising. Res-WCAE comprises two encoders - an image encoder and a wavelet encoder - and one decoder. Residual connections between the image encoder and decoder are leveraged to preserve fine-grained spatial features, where the bottleneck layer conditioned on the compressed representation of features obtained from the wavelet encoder using approximation and detail subimages in the wavelet-transform domain. The effectiveness of Res-WCAE is evaluated against several state-of-the-art denoising methods, and the experimental results demonstrate that Res-WCAE outperforms these methods, particularly for heavily degraded fingerprint images in the presence of high levels of noise. Overall, Res-WCAE shows promise as a solution to the challenges faced by biometric authentication systems in compact IoT devices.
</details>
<details>
<summary>摘要</summary>
现在互联网物联网设备中越来越普遍使用生物特征认证，特别是使用图像特征进行身份验证。然而，这些系统的可靠性可能受到图像质量问题的影响，特别是在高噪声水平下。当前的深度学习算法，专门为普通图像除噪设计的深度学习模型，虽然已经达到了一定的成绩，但它们的参数数量很大，并且没有特定生物特征检索优化，因此不适合这些设备和场景。为了解决这些挑战，本文提出了一种轻量级和可靠的深度学习架构——差分波let-conditioned Convolutional Autoencoder（Res-WCAE），并在其中添加了Kullback-Leibler异质（KLD）正则化。Res-WCAE包括两个编码器——图像编码器和波лет编码器——以及一个解码器。图像编码器和解码器之间的差分连接，使得图像的细腻特征得到保留，而波лет编码器使用波лет变换获得的特征的压缩表示，并通过差分连接和权重Conditioning来与图像编码器进行交互。对比多种现有的去噪方法，Res-WCAE的效果得到了证明，特别是在高噪声水平下的极大噪声图像认证 task。总的来说，Res-WCAE表现出了在compact IoT设备中的可靠性和灵活性，并且有望成为生物认证系统中的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Depression-Detection-via-Head-Motion-Patterns"><a href="#Explainable-Depression-Detection-via-Head-Motion-Patterns" class="headerlink" title="Explainable Depression Detection via Head Motion Patterns"></a>Explainable Depression Detection via Head Motion Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12241">http://arxiv.org/abs/2307.12241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Gahalawat, Raul Fernandez Rojas, Tanaya Guha, Ramanathan Subramanian, Roland Goecke</li>
<li>for: 检测抑郁症状</li>
<li>methods: 基于head motion数据的基本运动单元（kinemes）和机器学习方法</li>
<li>results:  head motion patterns 可以作为抑郁症状的生物标志，并且可以通过基于征料的方法来分类抑郁和健康控制组Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to detect depression symptoms using head motion data and machine learning methods.</li>
<li>methods: The paper uses two approaches to detect depression: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes.</li>
<li>results: The paper finds that head motion patterns are effective biomarkers for detecting depressive symptoms, and that explanatory kineme patterns consistent with prior findings can be observed for the two classes. The paper achieves peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 datasets for binary classification over episodic thin-slices, and a peak F1 of 0.72 over videos for AVEC2013.<details>
<summary>Abstract</summary>
While depression has been studied via multimodal non-verbal behavioural cues, head motion behaviour has not received much attention as a biomarker. This study demonstrates the utility of fundamental head-motion units, termed \emph{kinemes}, for depression detection by adopting two distinct approaches, and employing distinctive features: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy controls, and computing statistics derived from reconstruction errors for both the patient and control classes. Employing machine learning methods, we evaluate depression classification performance on the \emph{BlackDog} and \emph{AVEC2013} datasets. Our findings indicate that: (1) head motion patterns are effective biomarkers for detecting depressive symptoms, and (2) explanatory kineme patterns consistent with prior findings can be observed for the two classes. Overall, we achieve peak F1 scores of 0.79 and 0.82, respectively, over BlackDog and AVEC2013 for binary classification over episodic \emph{thin-slices}, and a peak F1 of 0.72 over videos for AVEC2013.
</details>
<details>
<summary>摘要</summary>
研究表示，诊断抑郁症可以通过多modal非语言行为迹象来进行。然而，头部运动行为尚未得到很多关注，作为生物标记。本研究演示了基本头部运动单元（kinemes）在抑郁检测中的有用性，通过采用两种不同的方法和特征：（a）从头部运动数据中提取与抑郁患者和健康控制人群相对应的kinemes，以及（b）通过健康控制人群学习kineme模式，并计算来自重建错误的统计。使用机器学习方法，我们评估了抑郁分类性能在BlackDog和AVEC2013数据集上。我们发现：（1）头部运动模式是抑郁症的有效生物标记，和（2）可以观察到健康控制人群和抑郁患者之间的准确的kineme模式。总的来说，我们在BlackDog和AVEC2013数据集上实现了最高的F1分数为0.79和0.82，分别为 binary 分类EPISODE 薄片和视频。
</details></li>
</ul>
<hr>
<h2 id="Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems"><a href="#Demonstration-of-a-Response-Time-Based-Remaining-Useful-Life-RUL-Prediction-for-Software-Systems" class="headerlink" title="Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems"></a>Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12237">http://arxiv.org/abs/2307.12237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ray Islam, Peter Sandborn</li>
<li>for: 这个论文旨在应用PHM概念到软件系统中，以预测问题和计算系统的RUL。</li>
<li>methods: 本论文使用了usage参数（例如发布数量和类别）和性能参数（例如响应时间）来预测RUL。</li>
<li>results: 研究人员通过对实际数据进行比较，发现PHM概念可以应用于软件系统，并且可以计算出RUL来做系统管理决策。<details>
<summary>Abstract</summary>
Prognostic and Health Management (PHM) has been widely applied to hardware systems in the electronics and non-electronics domains but has not been explored for software. While software does not decay over time, it can degrade over release cycles. Software health management is confined to diagnostic assessments that identify problems, whereas prognostic assessment potentially indicates when in the future a problem will become detrimental. Relevant research areas such as software defect prediction, software reliability prediction, predictive maintenance of software, software degradation, and software performance prediction, exist, but all of these represent diagnostic models built upon historical data, none of which can predict an RUL for software. This paper addresses the application of PHM concepts to software systems for fault predictions and RUL estimation. Specifically, this paper addresses how PHM can be used to make decisions for software systems such as version update and upgrade, module changes, system reengineering, rejuvenation, maintenance scheduling, budgeting, and total abandonment. This paper presents a method to prognostically and continuously predict the RUL of a software system based on usage parameters (e.g., the numbers and categories of releases) and performance parameters (e.g., response time). The model developed has been validated by comparing actual data, with the results that were generated by predictive models. Statistical validation (regression validation, and k-fold cross validation) has also been carried out. A case study, based on publicly available data for the Bugzilla application is presented. This case study demonstrates that PHM concepts can be applied to software systems and RUL can be calculated to make system management decisions.
</details>
<details>
<summary>摘要</summary>
预测和健康管理（PHM）已广泛应用于硬件系统中，但尚未探讨软件领域。虽然软件不会逝减，但可能会逐渐下降。软件健康管理仅仅是诊断评估，而预测评估可能会预测未来哪一天问题会变得严重。有关研究领域包括软件缺陷预测、软件可靠性预测、软件维护预测、软件衰老和软件性能预测，但这些都是基于历史数据建立的诊断模型，无法预测软件的寿命。本文探讨将PHM概念应用于软件系统中，以预测问题和计算软件系统的寿命。具体来说，本文探讨了如何使用PHM来做软件系统的决策，如版本更新和升级、模块更改、系统重构、重新生成、维护计划、预算和完全废弃。本文提出了一种基于使用量和性能参数预测软件系统的寿命的方法。该模型已经验证了，并通过与预测模型生成的结果进行比较。此外，还进行了统计验证（回归验证和Kfold跨验证）。一个基于公共数据的 Bugzilla 应用程序的案例研究也被提出，这个案例示出了PHM概念可以应用于软件系统，并且可以计算软件系统的寿命以进行系统管理决策。
</details></li>
</ul>
<hr>
<h2 id="Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO"><a href="#Multi-Modal-Machine-Learning-for-Assessing-Gaming-Skills-in-Online-Streaming-A-Case-Study-with-CS-GO" class="headerlink" title="Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO"></a>Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12236">http://arxiv.org/abs/2307.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longxiang Zhang, Wenping Wang</li>
<li>for: 本研究旨在为串流服务提供商评估电竞技巧，以便为客户提供个性化推荐和服务促销。</li>
<li>methods: 本研究使用最新的端到端模型学习joint representation of multiple modalities，并进行了大量的实验证明其效果。</li>
<li>results: 研究发现，提议的模型具有识别用户的弱点，而不是学习有意义的表示。未来工作将解决这个问题。<details>
<summary>Abstract</summary>
Online streaming is an emerging market that address much attention. Assessing gaming skills from videos is an important task for streaming service providers to discover talented gamers. Service providers require the information to offer customized recommendation and service promotion to their customers. Meanwhile, this is also an important multi-modal machine learning tasks since online streaming combines vision, audio and text modalities. In this study we begin by identifying flaws in the dataset and proceed to clean it manually. Then we propose several variants of latest end-to-end models to learn joint representation of multiple modalities. Through our extensive experimentation, we demonstrate the efficacy of our proposals. Moreover, we identify that our proposed models is prone to identifying users instead of learning meaningful representations. We purpose future work to address the issue in the end.
</details>
<details>
<summary>摘要</summary>
在线流媒体是一个崛起的市场，吸引了很多注意。通过视频评估游戏技巧是流媒体服务提供者为发掘才华的玩家提供重要的任务。服务提供者需要这些信息以为客户提供个性化推荐和服务促销。同时，这也是一项重要的多modal机器学习任务，因为在线流媒体结合了视觉、音频和文本模式。在本研究中，我们首先发现数据集中的缺陷，然后手动清理数据。然后，我们提出了多种最新的端到端模型，以学习多Modalities的共同表示。通过我们的广泛实验，我们证明了我们的提议的有效性。另外，我们发现我们的提议模型容易被用户认出，而不是学习有意义的表示。我们在结尾提出了未来的工作，以解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms"><a href="#EchoGLAD-Hierarchical-Graph-Neural-Networks-for-Left-Ventricle-Landmark-Detection-on-Echocardiograms" class="headerlink" title="EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms"></a>EchoGLAD: Hierarchical Graph Neural Networks for Left Ventricle Landmark Detection on Echocardiograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12229">http://arxiv.org/abs/2307.12229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masoudmo/echoglad">https://github.com/masoudmo/echoglad</a></li>
<li>paper_authors: Masoud Mokhtari, Mobina Mahdavi, Hooman Vaseli, Christina Luong, Purang Abolmaesumi, Teresa S. M. Tsang, Renjie Liao</li>
<li>for: 这 paper 的目的是自动检测心脏左心室的四个标志点和测量左心室内部的尺寸和周围肌肉的大约质量。</li>
<li>methods: 这 paper 使用了一种基于 echo cardiogram 的层次 graph neural network (GNN)，以实现左心室标志点检测。</li>
<li>results: 这 paper 在一个公共数据集和一个私有数据集上进行了评估，在内分布 (ID) 和外分布 (OOD) 两种设置下， achieved  state-of-the-art 的 Mean Absolute Error (MAE) 值为 1.46 mm 和 1.86 mm，并且在 OOD 设置下表现更好。<details>
<summary>Abstract</summary>
The functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details>
<details>
<summary>摘要</summary>
functional assessment of the left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.Here's the translation in Traditional Chinese:函数评估左心室的 left ventricle chamber of the heart requires detecting four landmark locations and measuring the internal dimension of the left ventricle and the approximate mass of the surrounding muscle. The key challenge of automating this task with machine learning is the sparsity of clinical labels, i.e., only a few landmark pixels in a high-dimensional image are annotated, leading many prior works to heavily rely on isotropic label smoothing. However, such a label smoothing strategy ignores the anatomical information of the image and induces some bias. To address this challenge, we introduce an echocardiogram-based, hierarchical graph neural network (GNN) for left ventricle landmark detection (EchoGLAD). Our main contributions are: 1) a hierarchical graph representation learning framework for multi-resolution landmark detection via GNNs; 2) induced hierarchical supervision at different levels of granularity using a multi-level loss. We evaluate our model on a public and a private dataset under the in-distribution (ID) and out-of-distribution (OOD) settings. For the ID setting, we achieve the state-of-the-art mean absolute errors (MAEs) of 1.46 mm and 1.86 mm on the two datasets. Our model also shows better OOD generalization than prior works with a testing MAE of 4.3 mm.
</details></li>
</ul>
<hr>
<h2 id="The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery"><a href="#The-identification-of-garbage-dumps-in-the-rural-areas-of-Cyprus-through-the-application-of-deep-learning-to-satellite-imagery" class="headerlink" title="The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery"></a>The identification of garbage dumps in the rural areas of Cyprus through the application of deep learning to satellite imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02502">http://arxiv.org/abs/2308.02502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Keith Wilkinson</li>
<li>for: 这个研究旨在使用人工智能技术和卫星图像来识别Cyprus农村地区的非法垃圾弃置。</li>
<li>methods: 这个研究使用了人工智能技术和卫星图像来识别垃圾，首先收集了一个小型数据集，然后使用数据扩展技术来增加数据量，然后训练了一个 convolutional neural network（CNN）来识别垃圾。</li>
<li>results: 这个研究得到了一个深度学习模型，可以在90%的情况下正确地识别垃圾图像。这个模型可以成为未来Cyprus岛上的垃圾映射系统的基础。<details>
<summary>Abstract</summary>
Garbage disposal is a challenging problem throughout the developed world. In Cyprus, as elsewhere, illegal ``fly-tipping" is a significant issue, especially in rural areas where few legal garbage disposal options exist. However, there is a lack of studies that attempt to measure the scale of this problem, and few resources available to address it. A method of automating the process of identifying garbage dumps would help counter this and provide information to the relevant authorities. The aim of this study was to investigate the degree to which artificial intelligence techniques, together with satellite imagery, can be used to identify illegal garbage dumps in the rural areas of Cyprus. This involved collecting a novel dataset of images that could be categorised as either containing, or not containing, garbage. The collection of such datasets in sufficient raw quantities is time consuming and costly. Therefore a relatively modest baseline set of images was collected, then data augmentation techniques used to increase the size of this dataset to a point where useful machine learning could occur. From this set of images an artificial neural network was trained to recognise the presence or absence of garbage in new images. A type of neural network especially suited to this task known as ``convolutional neural networks" was used. The efficacy of the resulting model was evaluated using an independently collected dataset of test images. The result was a deep learning model that could correctly identify images containing garbage in approximately 90\% of cases. It is envisaged that this model could form the basis of a future system that could systematically analyse the entire landscape of Cyprus to build a comprehensive ``garbage" map of the island.
</details>
<details>
<summary>摘要</summary>
垃圾处理是发达国家的一个挑战。在塞浦路斯，如其他地方一样，非法“飞tipping”是一个严重的问题，特别是在农村地区，其法定垃圾处理选择较少。然而，有很少的研究尝试量化这个问题，而且有限的资源来解决它。这项研究的目的是使用人工智能技术和卫星图像来识别塞浦路斯农村地区非法垃圾排放。这包括收集一个新的图像集，这些图像可以分为含垃圾和不含垃圾两类。收集这些图像集的过程是时间consuming和成本高的。因此，我们只收集了一个相对较小的基线集的图像，然后使用数据扩展技术来增加这个集的大小，以便进行有用的机器学习。从这些图像中，我们用人工神经网络来识别新图像中是否含垃圾。我们使用的是一种适合这种任务的特殊类型的神经网络，即“卷积神经网络”。我们评估了这种模型的效果，使用独立收集的测试集。结果是一个深度学习模型，可以在90%的情况下正确地识别含垃圾的图像。我们可以基于这个模型，建立一个将系统地分析整个塞浦路斯岛的系统，并建立一个“垃圾”地图。
</details></li>
</ul>
<hr>
<h2 id="Geometry-Aware-Adaptation-for-Pretrained-Models"><a href="#Geometry-Aware-Adaptation-for-Pretrained-Models" class="headerlink" title="Geometry-Aware Adaptation for Pretrained Models"></a>Geometry-Aware Adaptation for Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12226">http://arxiv.org/abs/2307.12226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas Roberts, Xintong Li, Dyah Adila, Sonia Cromp, Tzu-Heng Huang, Jitian Zhao, Frederic Sala</li>
<li>for: 提高零shot预测性能和适应新类预测</li>
<li>methods: 利用度量学空间信息进行适应和预测改进</li>
<li>results: 在ImageNet上实现了29.7%的相对改进，并且可以扩展到千万类的预测任务。当无外部度量时，可以使用自动生成的度量从类嵌入中获得10.5%的改进。<details>
<summary>Abstract</summary>
Machine learning models -- including prominent zero-shot models -- are often trained on datasets whose labels are only a small proportion of a larger label space. Such spaces are commonly equipped with a metric that relates the labels via distances between them. We propose a simple approach to exploit this information to adapt the trained model to reliably predict new classes -- or, in the case of zero-shot prediction, to improve its performance -- without any additional training. Our technique is a drop-in replacement of the standard prediction rule, swapping argmax with the Fr\'echet mean. We provide a comprehensive theoretical analysis for this approach, studying (i) learning-theoretic results trading off label space diameter, sample complexity, and model dimension, (ii) characterizations of the full range of scenarios in which it is possible to predict any unobserved class, and (iii) an optimal active learning-like next class selection procedure to obtain optimal training classes for when it is not possible to predict the entire range of unobserved classes. Empirically, using easily-available external metrics, our proposed approach, Loki, gains up to 29.7% relative improvement over SimCLR on ImageNet and scales to hundreds of thousands of classes. When no such metric is available, Loki can use self-derived metrics from class embeddings and obtains a 10.5% improvement on pretrained zero-shot models such as CLIP.
</details>
<details>
<summary>摘要</summary>
机器学习模型 -- 包括知名的零批量模型 -- 常常在具有小比例标签空间的数据集上训练。这些空间通常具有一个度量关系标签之间的距离。我们提出了一种简单的方法，利用这些信息来适应训练过的模型预测新类 -- 或者在零批量预测中提高性能 --  без需要额外训练。我们的技术是将欧几何均值换取作为标准预测规则的替换。我们提供了全面的理论分析，研究（i）标签空间径距、样本复杂度和模型维度之间的学习理论结果，（ii）可预测任何未见类的全范围描述，以及（iii）针对不能预测整个未见类范围时的优化的活动学习样本选择方法。实际上，我们的提议方法Loki在ImageNet上实现了Relative improvement为29.7%，并且可以扩展到万个类。当没有外部度量时，Loki可以使用自带的类嵌入度量，实现预测零批量模型CLIP的10.5%提高。
</details></li>
</ul>
<hr>
<h2 id="Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation"><a href="#Improving-Out-of-Distribution-Robustness-of-Classifiers-via-Generative-Interpolation" class="headerlink" title="Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation"></a>Improving Out-of-Distribution Robustness of Classifiers via Generative Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12219">http://arxiv.org/abs/2307.12219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyue Bai, Ceyuan Yang, Yinghao Xu, S. -H. Gary Chan, Bolei Zhou</li>
<li>for: 提高神经网络模型对于不同分布数据的鲁棒性</li>
<li>methods: 使用生成模型作为数据增强源，通过混合多个域的生成模型并在 interpolate 模型参数来生成多元的OoD样本</li>
<li>results: 实验结果显示，提出的方法可以明显提高神经网络模型对于不同分布数据的鲁棒性，并且可以控制增强的方向和强度<details>
<summary>Abstract</summary>
Deep neural networks achieve superior performance for learning from independent and identically distributed (i.i.d.) data. However, their performance deteriorates significantly when handling out-of-distribution (OoD) data, where the training and test are drawn from different distributions. In this paper, we explore utilizing the generative models as a data augmentation source for improving out-of-distribution robustness of neural classifiers. Specifically, we develop a simple yet effective method called Generative Interpolation to fuse generative models trained from multiple domains for synthesizing diverse OoD samples. Training a generative model directly on the source domains tends to suffer from mode collapse and sometimes amplifies the data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators where their model parameters have the same initialization thus are aligned. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength. In addition, a style-mixing mechanism is applied to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details>
<details>
<summary>摘要</summary>
Training a generative model directly on the source domains can lead to mode collapse and amplify data bias. Instead, we first train a StyleGAN model on one source domain and then fine-tune it on the other domains, resulting in many correlated generators with aligned model parameters. We then linearly interpolate the model parameters of the generators to spawn new sets of generators. Such interpolated generators are used as an extra data augmentation source to train the classifiers. The interpolation coefficients can flexibly control the augmentation direction and strength.In addition, we apply a style-mixing mechanism to further improve the diversity of the generated OoD samples. Our experiments show that the proposed method explicitly increases the diversity of training domains and achieves consistent improvements over baselines across datasets and multiple different distribution shifts.
</details></li>
</ul>
<hr>
<h2 id="Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models"><a href="#Mental-Workload-Estimation-with-Electroencephalogram-Signals-by-Combining-Multi-Space-Deep-Models" class="headerlink" title="Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models"></a>Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02409">http://arxiv.org/abs/2308.02409</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim</li>
<li>for: 本研究旨在 классифицироватьMENTAL工作负担为三种状态和估算综合指数。</li>
<li>methods: 该方法 combinestemultiple空间维度来获得最佳的心理估算结果。在时域方法中，我们使用Temporal Convolutional Networks，而在频域方法中，我们提出了一种新的Multi-Dimensional Residual Block架构。</li>
<li>results: 我们的方法可以准确地分类MENTAL工作负担为三种状态，并且可以准确地估算综合指数。<details>
<summary>Abstract</summary>
The human brain is in a continuous state of activity during both work and rest. Mental activity is a daily process, and when the brain is overworked, it can have negative effects on human health. In recent years, great attention has been paid to early detection of mental health problems because it can help prevent serious health problems and improve quality of life. Several signals are used to assess mental state, but the electroencephalogram (EEG) is widely used by researchers because of the large amount of information it provides about the brain. This paper aims to classify mental workload into three states and estimate continuum levels. Our method combines multiple dimensions of space to achieve the best results for mental estimation. In the time domain approach, we use Temporal Convolutional Networks, and in the frequency domain, we propose a new architecture called the Multi-Dimensional Residual Block, which combines residual blocks.
</details>
<details>
<summary>摘要</summary>
人类大脑在工作和休息时都处于不断活跃的状态。心理活动是每日的过程，当大脑过度劳累时，可能会对人类健康产生负面影响。在最近几年，关注早期识别心理健康问题的注意力凝固了，因为它可以帮助预防严重的健康问题并提高生活质量。多种信号都可以评估心理状态，但是电enzephalogram（EEG）在研究人员中广泛使用，因为它可以提供大量关于大脑的信息。本文的目标是将心理劳动力分为三个状态，并估计连续水平。我们的方法将多维空间综合使用，以达到最佳的心理估计结果。在时域方法中，我们使用Temporal Convolutional Networks，在频域中，我们提出了一种新的建筑方案called Multi-Dimensional Residual Block，这个方案将residual blocks综合使用。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices"><a href="#Adversarial-Agents-For-Attacking-Inaudible-Voice-Activated-Devices" class="headerlink" title="Adversarial Agents For Attacking Inaudible Voice Activated Devices"></a>Adversarial Agents For Attacking Inaudible Voice Activated Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12204">http://arxiv.org/abs/2307.12204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Forrest McKee, David Noever</li>
<li>For: 这个论文探讨了基于互联网物联网的听不到攻击的威胁风险。* Methods: 该论文使用了强化学习来解决这些听不到攻击的问题。* Results: 研究发现，使用深度强化学习的方法可以快速拥有所有节点，并且在 fewer steps 中完成。In English, this means:* For: This paper explores the risk of inaudible attacks on voice-activated devices.* Methods: The paper uses reinforcement learning to solve the problem of inaudible attacks.* Results: The study finds that using deep reinforcement learning can quickly gain control of all nodes, and achieve this in fewer steps.<details>
<summary>Abstract</summary>
The paper applies reinforcement learning to novel Internet of Thing configurations. Our analysis of inaudible attacks on voice-activated devices confirms the alarming risk factor of 7.6 out of 10, underlining significant security vulnerabilities scored independently by NIST National Vulnerability Database (NVD). Our baseline network model showcases a scenario in which an attacker uses inaudible voice commands to gain unauthorized access to confidential information on a secured laptop. We simulated many attack scenarios on this baseline network model, revealing the potential for mass exploitation of interconnected devices to discover and own privileged information through physical access without adding new hardware or amplifying device skills. Using Microsoft's CyberBattleSim framework, we evaluated six reinforcement learning algorithms and found that Deep-Q learning with exploitation proved optimal, leading to rapid ownership of all nodes in fewer steps. Our findings underscore the critical need for understanding non-conventional networks and new cybersecurity measures in an ever-expanding digital landscape, particularly those characterized by mobile devices, voice activation, and non-linear microphones susceptible to malicious actors operating stealth attacks in the near-ultrasound or inaudible ranges. By 2024, this new attack surface might encompass more digital voice assistants than people on the planet yet offer fewer remedies than conventional patching or firmware fixes since the inaudible attacks arise inherently from the microphone design and digital signal processing.
</details>
<details>
<summary>摘要</summary>
文章应用再强化学习解决新互联网设备配置中的攻击问题。我们对无声攻击voice控制设备进行分析，确认了攻击性风险因子为7.6/10，强调了设备安全漏洞的独立评分。我们的基线网络模型显示了一种攻击者通过无声voice命令窃取机密信息的场景，我们在这个基线网络模型上进行了许多攻击场景的模拟，发现了大规模攻击INTERNET OF THINGS设备，以获取特权信息，不需要新硬件或设备技能升级。使用Microsoft的CyberBattleSim框架，我们评估了六种再强化学习算法，发现deep Q学习与利用最佳，可以在 fewer steps 内快速拥有所有节点。我们的发现强调了非常 conventional 网络和新的cybersecurity措施在不断扩大的数字ландшаф具 особен需要，特别是包括移动设备、voice控制和非线性 Microphone 在内的设备，遭受恶势力操作的隐藏攻击。到2024年，这个新的攻击表面可能会包括更多的数字voice助手 than人类 на 地球， yet offer fewer remedies than conventional patching or firmware fixes，因为无声攻击来自 Microphone 设计和数字信号处理。
</details></li>
</ul>
<hr>
<h2 id="NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data"><a href="#NCART-Neural-Classification-and-Regression-Tree-for-Tabular-Data" class="headerlink" title="NCART: Neural Classification and Regression Tree for Tabular Data"></a>NCART: Neural Classification and Regression Tree for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12198">http://arxiv.org/abs/2307.12198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Luo, Shixin Xu</li>
<li>for: 这 paper 旨在提出一种可解释性强的深度学习模型，以解决深度学习模型在大规模或高维数据集中的计算成本高和可解释性差的问题。</li>
<li>methods: 该 paper 提出了一种名为 Neural Classification and Regression Tree (NCART) 的新型可解释性神经网络，它将多层感知网络替换为多个可导的无知决策树。通过将决策树 integrate 到网络架构中，NCART 保持了可解释性，同时具有神经网络的综合能力。</li>
<li>results: 数值实验表明，NCART 比现有的深度学习模型具有更高的性能，并且在不同的数据集中表现出色，建立了 NCART 作为树状模型的强大竞争对手。<details>
<summary>Abstract</summary>
Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally expensive when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models.
</details>
<details>
<summary>摘要</summary>
深度学习模型在表格数据分析中变得越来越受欢迎，因为它们可以解决决策树的限制，并实现有价值的应用，如半监督学习、在线学习和传输学习。然而，这些深度学习方法经常面临一种负担。一方面，它们在处理大规模或高维度数据时可能会变得计算昂贵。另一方面，它们可能缺乏可解性，并且不适用于小规模数据。在本研究中，我们提出一种新的可解的神经网络模型，即神经分类和回归树（NCART），以解决这些挑战。NCART是基于差分网络的修改版本，它将完全连接层 replaced with 多个可微分的无知决策树。通过将决策树 integrate into 神经网络架构，NCART可以保持其可解性，同时受益于神经网络的终端能力。NCART 的简单架构使其适用于不同规模的数据集，并 reduc 计算成本与现有的深度学习模型相比。我们的数字实验证明 NCART 的表现比现有的深度学习模型更优，从而成为树状模型的强竞争对手。
</details></li>
</ul>
<hr>
<h2 id="Monadic-Deep-Learning"><a href="#Monadic-Deep-Learning" class="headerlink" title="Monadic Deep Learning"></a>Monadic Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12187">http://arxiv.org/abs/2307.12187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThoughtWorksInc/monadic-deep-learning">https://github.com/ThoughtWorksInc/monadic-deep-learning</a></li>
<li>paper_authors: Bo Yang, Zhihao Zhang Kirisame Marisa, Kai Shi</li>
<li>for: 这个论文的目的是解决 dynamically typed 编程语言中的神经网络模型问题，使得用户可以使用 statically typed 语言来创建神经网络模型。</li>
<li>methods: 这篇论文使用了一种新的方法，即在静态类型函数中自动对含有多个可训练变量的神经网络模型进行差分。它还使用了一些幺athed 和 monad transformers，以便让用户创建具有 intuition 和简洁性的神经网络模型。</li>
<li>results: 该论文的实验结果表明，使用 DeepLearning.scala 可以帮助用户创建复杂的神经网络模型，并且仍然保持类型安全性。<details>
<summary>Abstract</summary>
The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.   Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.   We solved this problem in DeepLearning.scala 2. Our contributions are:   1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.   2. We designed a set of monads and monad transformers, which allow users to create monadic expressions that represent dynamic neural networks.   3. Along with these monads, we provide some applicative functors, to perform multiple calculations in parallel.   With these features, users of DeepLearning.scala were able to create complex neural networks in an intuitive and concise way, and still maintain type safety.
</details>
<details>
<summary>摘要</summary>
直到2019年，静态类型语言中的学习框架都没有提供传统框架的表达力。其用户无法使用自定义算法，除非创建大量的 boilerplate 代码来实现硬编码的反射传播。我们在 DeepLearning.scala 2 中解决了这个问题。我们的贡献包括：1. 我们发现了一种新的方法，可以在静态类型函数中自动进行反Mode微分，并且可以与金属语言进行自由交互。2. 我们设计了一组幂等和幂等转换，这些幂等可以让用户创建幂等表达式，表示动态神经网络。3. 同时，我们还提供了一些应用程序函数，可以在平行计算中进行多个计算。通过这些特性，DeepLearning.scala 的用户可以创建复杂的神经网络，使用 intuition 和简洁的方式来进行训练，并且仍保持类型安全性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-discovers-invariants-of-braids-and-flat-braids"><a href="#Machine-learning-discovers-invariants-of-braids-and-flat-braids" class="headerlink" title="Machine learning discovers invariants of braids and flat braids"></a>Machine learning discovers invariants of braids and flat braids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12185">http://arxiv.org/abs/2307.12185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexei Lisitsa, Mateo Salles, Alexei Vernitski</li>
<li>for: 用机器学习分类布里论（或平坦布里论）的例子为轻量级或非轻量级。</li>
<li>methods: 使用超visisted学习神经网络（多层感知器）进行分类。</li>
<li>results: 发现新的便利 invariants of braids, including a complete invariant of flat braids.Here’s the translation in English:</li>
<li>for: Using machine learning to classify examples of braids (or flat braids) as trivial or non-trivial.</li>
<li>methods: Using supervised learning with neural networks (multilayer perceptrons).</li>
<li>results: Discovering new convenient invariants of braids, including a complete invariant of flat braids.<details>
<summary>Abstract</summary>
We use machine learning to classify examples of braids (or flat braids) as trivial or non-trivial. Our ML takes form of supervised learning using neural networks (multilayer perceptrons). When they achieve good results in classification, we are able to interpret their structure as mathematical conjectures and then prove these conjectures as theorems. As a result, we find new convenient invariants of braids, including a complete invariant of flat braids.
</details>
<details>
<summary>摘要</summary>
我们使用机器学习来分类拥有布里亚的示例（或平板布里亚）为致命或非致命。我们的机器学习形式为指导学习使用神经网络（多层感知器）。当它们在分类中获得良好的结果时，我们可以解释它们的结构为数学假设，然后证明这些假设为定理。因此，我们发现新的便利 invariants of braids，包括完整的平板布里亚 invariants。Note: "布里亚" (braids) is a word in Chinese that refers to a type of mathematical object, and "平板布里亚" (flat braids) is a specific type of braid that is flat and has no crossing points.
</details></li>
</ul>
<hr>
<h2 id="Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation"><a href="#Prototype-Driven-and-Multi-Expert-Integrated-Multi-Modal-MR-Brain-Tumor-Image-Segmentation" class="headerlink" title="Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation"></a>Prototype-Driven and Multi-Expert Integrated Multi-Modal MR Brain Tumor Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12180">http://arxiv.org/abs/2307.12180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linzy0227/pdminet">https://github.com/linzy0227/pdminet</a></li>
<li>paper_authors: Yafei Zhang, Zhiyuan Li, Huafeng Li, Dapeng Tao</li>
<li>for: 这种多Modal MR brain tumor imaging segmentation方法是为了解决现有方法 Directly extracting discriminative features from input images for tumor sub-region category determination and localization，忽略了信息杂化的影响。</li>
<li>methods: 该方法提议使用肿瘤原型驱动的多专家结合，使得每个肿瘤子区域特征得到高亮显示。具体来说，我们提出了一种互传机制，将不同modal的特征传递给每个modal，以解决单modal特征不充分的问题。此外，我们还提出了一种使用学习的肿瘤原型来驱动特征表示和融合方法，使得肿瘤特征得到了融合。</li>
<li>results: 实验结果表明，该方法在三个竞赛肿瘤分割数据集上具有优秀的性能。<details>
<summary>Abstract</summary>
For multi-modal magnetic resonance (MR) brain tumor image segmentation, current methods usually directly extract the discriminative features from input images for tumor sub-region category determination and localization. However, the impact of information aliasing caused by the mutual inclusion of tumor sub-regions is often ignored. Moreover, existing methods usually do not take tailored efforts to highlight the single tumor sub-region features. To this end, a multi-modal MR brain tumor segmentation method with tumor prototype-driven and multi-expert integration is proposed. It could highlight the features of each tumor sub-region under the guidance of tumor prototypes. Specifically, to obtain the prototypes with complete information, we propose a mutual transmission mechanism to transfer different modal features to each other to address the issues raised by insufficient information on single-modal features. Furthermore, we devise a prototype-driven feature representation and fusion method with the learned prototypes, which implants the prototypes into tumor features and generates corresponding activation maps. With the activation maps, the sub-region features consistent with the prototype category can be highlighted. A key information enhancement and fusion strategy with multi-expert integration is designed to further improve the segmentation performance. The strategy can integrate the features from different layers of the extra feature extraction network and the features highlighted by the prototypes. Experimental results on three competition brain tumor segmentation datasets prove the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
对多Modal MR脑肿刺激图像分割，当前方法通常直接从输入图像中提取特征来确定肿瘤子区划分和定位。然而，现有方法通常忽略了肿瘤子区之间的信息冲突的影响。此外，现有方法通常不会特意强调单个肿瘤子区域的特征。为此，我们提议一种基于肿瘤原型的多Modal MR脑肿刺激分割方法。它可以在肿瘤原型的指导下高亮单个肿瘤子区域的特征。具体来说，为了获取完整信息的肿瘤原型，我们提议一种互传机制来传递不同modal特征之间的信息，以解决单modal特征不具备的问题。此外，我们设计了一种基于肿瘤原型的特征表示和融合方法，其可以在肿瘤特征中嵌入肿瘤原型，并生成相应的激活图。通过激活图，可以高亮与肿瘤原型类别相符的子区域特征。此外，我们设计了一种多 экспер特征融合策略，可以将不同层次的特征和肿瘤原型高亮的特征融合在一起，以提高分割性能。实验结果表明，提议的方法在三个竞赛脑肿刺激分割数据集上显示出了超越性。
</details></li>
</ul>
<hr>
<h2 id="Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics"><a href="#Learn-to-Compress-LtC-Efficient-Learning-based-Streaming-Video-Analytics" class="headerlink" title="Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics"></a>Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12171">http://arxiv.org/abs/2307.12171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quazi Mishkatul Alam, Israat Haque, Nael Abu-Ghazaleh<br>for: 这个论文的目的是建立一个高效的云端视频分析框架，以减少视频流的带宽和能源消耗。methods: 这个框架使用了一个轻量级神经网络，通过教师模型在服务器端进行学习，以减少视频流中不必要的信息。此外，它还使用了一种基于特征差分的时间滤波算法，以便快速忽略不必要的帧。results: 这个框架可以使用28-35% less bandwidth和45% shorter response delay，与最近发布的相关研究框架相比，而且保持了类似的分析性能。<details>
<summary>Abstract</summary>
Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The student network is trained to comprehend the semantic significance of various regions within the videos, which is used to differentially preserve the crucial regions in high quality while the remaining regions undergo aggressive compression. Furthermore, LtC also incorporates a novel temporal filtering algorithm based on feature-differencing to omit transmitting frames that do not contribute new information. Overall, LtC is able to use 28-35% less bandwidth and has up to 45% shorter response delay compared to recently published state of the art streaming frameworks while achieving similar analytics performance.
</details>
<details>
<summary>摘要</summary>
视频分析通常在云服务中进行，主要是为了减轻计算负担，以及在视频感知不直接在视频传感器上进行处理。往往将高质量视频数据从边缘设备传输到云服务器可能会占用很多带宽和电力资源。为建立高效的流动视频分析管道，因此是非常重要减小视频流。传统的视频压缩算法不了解视频的 semantics，可能会导致不fficient和对分析性能有害。在这篇论文中，我们介绍了 LtC，一个协同框架，其中视频源和分析服务器之间协同减小视频流。特别是，LtC 使用全功能的分析算法作为老师来训练一个轻量级神经网络，并将其部署到视频源上。学生网络被训练以理解视频中各个区域的semantic Significance，并使用这些区域来差分保留高质量视频，而其他区域则进行了激进压缩。此外，LtC 还包括一种基于特征差异的时间滤波算法，以便快速忽略不包含新信息的帧。总之，LtC 可以使用28-35%的带宽和45%的响应延迟，相比之下最新的流动框架，而 achieved similar analytics performance。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters"><a href="#Optimized-Network-Architectures-for-Large-Language-Model-Training-with-Billions-of-Parameters" class="headerlink" title="Optimized Network Architectures for Large Language Model Training with Billions of Parameters"></a>Optimized Network Architectures for Large Language Model Training with Billions of Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12169">http://arxiv.org/abs/2307.12169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani</li>
<li>for: 这篇论文挑战了在训练大型自然语言模型（LLM）时建立任意对任意网络的惯例。</li>
<li>methods: 我们表明了 LLM 训练中唯一特点的通信模式，只有小组内 GPU 需要高带宽任意对任意通信，以达到训练性能的近似最优。这些小组内 GPU 之间的通信费量极低、稀疏和均匀。我们提议一种新的网络架构，它与 LLM 的通信需求高度相似。我们将集群分为 HB domain，其中每个 HB domain 由非堵塞的任意对任意高带宽互连器连接。在 HB domain 内，网络只与需要通信的 GPU 连接。我们称这种网络为 “铁路仅” 连接，并证明我们的提议架构可以将网络成本降低至最多 75%，不会影响 LLM 训练的性能。</li>
<li>results: 我们的实验结果表明，我们的提议架构可以减少网络成本，同时保持 LLM 训练的性能。<details>
<summary>Abstract</summary>
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification"><a href="#Facial-Point-Graphs-for-Amyotrophic-Lateral-Sclerosis-Identification" class="headerlink" title="Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification"></a>Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12159">http://arxiv.org/abs/2307.12159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nícolas Barbosa Gomes, Arissa Yoshida, Mateus Roder, Guilherme Camargo de Oliveira, João Paulo Papa</li>
<li>for: 这篇论文的目的是找到早期诊断amyotrophic lateral sclerosis (ALS)的方法，以提高病人的预后和生活质量。</li>
<li>methods: 这篇论文使用computational方法来分析病人的脸部表情，以检测ALS的症状。具体来说，这篇论文使用Facial Point Graphs来学习脸部图像的几何特征，以自动识别ALS。</li>
<li>results: 论文的实验结果显示，提案的方法在测试数据集Toronto Neuroface Dataset中，与现有方法相比，有着更高的准确性和效率。这些结果显示出这种方法的潜力，并带来了领域的发展。<details>
<summary>Abstract</summary>
Identifying Amyotrophic Lateral Sclerosis (ALS) in its early stages is essential for establishing the beginning of treatment, enriching the outlook, and enhancing the overall well-being of those affected individuals. However, early diagnosis and detecting the disease's signs is not straightforward. A simpler and cheaper way arises by analyzing the patient's facial expressions through computational methods. When a patient with ALS engages in specific actions, e.g., opening their mouth, the movement of specific facial muscles differs from that observed in a healthy individual. This paper proposes Facial Point Graphs to learn information from the geometry of facial images to identify ALS automatically. The experimental outcomes in the Toronto Neuroface dataset show the proposed approach outperformed state-of-the-art results, fostering promising developments in the area.
</details>
<details>
<summary>摘要</summary>
早期识别amyotrophic lateral sclerosis（ALS）是非常重要，可以提高治疗的开始，改善患者的生活质量和总体情况。然而，早期诊断和识别病状的标准方法并不是很直forward。本文提出了一种使用计算机方法分析患者的面部表情来自动识别ALS的方法。当患者进行特定的动作时，如打开嘴巴，特定的面部肌肉的运动会与健康人的不同。这篇论文使用面部点图学习geometry of facial images来自动识别ALS，实验结果表明该方法在多伦多Neuroface dataset中超越了现有的最佳结果，为您的发展提供了有希望的前景。
</details></li>
</ul>
<hr>
<h2 id="DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft"><a href="#DIP-RL-Demonstration-Inferred-Preference-Learning-in-Minecraft" class="headerlink" title="DIP-RL: Demonstration-Inferred Preference Learning in Minecraft"></a>DIP-RL: Demonstration-Inferred Preference Learning in Minecraft</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12158">http://arxiv.org/abs/2307.12158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellen Novoseller, Vinicius G. Goecks, David Watkins, Josh Miller, Nicholas Waytowich</li>
<li>for: 本研究旨在解决在无结构化真实世界中，RL算法学习Sequential Decision-Making时，因缺乏奖励信号而难以学习的问题。</li>
<li>methods: 本研究提出了Demonstration-Inferred Preference Reinforcement Learning（DIP-RL）算法，利用人类示范来推导RL算法学习。DIP-RL在三种不同的方式使用示范数据，包括训练自动编码器、RL训练批处理中使用示范数据，以及推导RL奖励函数。</li>
<li>results: 实验结果表明，DIP-RL可以引导RL算法学习人类偏好，并且与基elines相比，DIP-RL在树割任务中表现竞争力强。例如轨迹满足扩展可以在<a target="_blank" rel="noopener" href="https://sites.google.com/view/dip-rl%E3%80%82">https://sites.google.com/view/dip-rl。</a><details>
<summary>Abstract</summary>
In machine learning for sequential decision-making, an algorithmic agent learns to interact with an environment while receiving feedback in the form of a reward signal. However, in many unstructured real-world settings, such a reward signal is unknown and humans cannot reliably craft a reward signal that correctly captures desired behavior. To solve tasks in such unstructured and open-ended environments, we present Demonstration-Inferred Preference Reinforcement Learning (DIP-RL), an algorithm that leverages human demonstrations in three distinct ways, including training an autoencoder, seeding reinforcement learning (RL) training batches with demonstration data, and inferring preferences over behaviors to learn a reward function to guide RL. We evaluate DIP-RL in a tree-chopping task in Minecraft. Results suggest that the method can guide an RL agent to learn a reward function that reflects human preferences and that DIP-RL performs competitively relative to baselines. DIP-RL is inspired by our previous work on combining demonstrations and pairwise preferences in Minecraft, which was awarded a research prize at the 2022 NeurIPS MineRL BASALT competition, Learning from Human Feedback in Minecraft. Example trajectory rollouts of DIP-RL and baselines are located at https://sites.google.com/view/dip-rl.
</details>
<details>
<summary>摘要</summary>
机器学习 дляsequential decision-making中的算法式代理可以在环境中学习并接受反馈形式为奖signal。但在许多无结构的实际场景中，这种奖signal是未知的，人们无法可靠地制定一个正确捕捉所愿行为的奖 signal。为解决这些无结构和开放的环境中的任务，我们提出了Demonstration-Inferred Preference Reinforcement Learning（DIP-RL）算法，该算法利用人类示范在三种方式：在训练 autoencoder 中，在RL训练批次中使用示范数据，以及推导RL agent 对行为的偏好来学习一个奖函数来导引RL。我们在 Minecraft 中进行了树割任务的评估，结果表明DIP-RL可以引导RL agent 学习一个符合人类偏好的奖函数，并且DIP-RL与基elines相比表现竞争性。DIP-RL是基于我们在 Minecraft 中结合示范和对比偏好的前一项研究，该研究在2022年 NeurIPS MineRL BASALT 比赛中获得了研究奖，Learning from Human Feedback in Minecraft。DIP-RL的示例轨迹扩展位于 <https://sites.google.com/view/dip-rl>。
</details></li>
</ul>
<hr>
<h2 id="Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach"><a href="#Identifying-contributors-to-supply-chain-outcomes-in-a-multi-echelon-setting-a-decentralised-approach" class="headerlink" title="Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach"></a>Identifying contributors to supply chain outcomes in a multi-echelon setting: a decentralised approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12157">http://arxiv.org/abs/2307.12157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Jack Foster, Alexandra Brintrup</li>
<li>for: 本研究旨在帮助企业快速准确地确定生产过程中metric变化的原因，尤其是在多层供应链中，只能部分可见。</li>
<li>methods: 本研究提议使用可解释人工智能来实现分布式计算估算变量的贡献，以解决数据隐私问题。</li>
<li>results: 实验结果表明，分布式计算可以更有效地检测质量变化的起源，比中央化方法使用Shapley添加itive解释。<details>
<summary>Abstract</summary>
Organisations often struggle to identify the causes of change in metrics such as product quality and delivery duration. This task becomes increasingly challenging when the cause lies outside of company borders in multi-echelon supply chains that are only partially observable. Although traditional supply chain management has advocated for data sharing to gain better insights, this does not take place in practice due to data privacy concerns. We propose the use of explainable artificial intelligence for decentralised computing of estimated contributions to a metric of interest in a multi-stage production process. This approach mitigates the need to convince supply chain actors to share data, as all computations occur in a decentralised manner. Our method is empirically validated using data collected from a real multi-stage manufacturing process. The results demonstrate the effectiveness of our approach in detecting the source of quality variations compared to a centralised approach using Shapley additive explanations.
</details>
<details>
<summary>摘要</summary>
企业们经常难以确定生产质量和交付时间的变化的原因。这个任务在多层供应链中，只能半 observability 的情况下变得更加困难。传统的供应链管理推荐数据共享，以获得更好的洞察力，但在实践中，由于数据隐私问题，这并没有实现。我们建议使用可解释人工智能 для分布式计算估算的贡献因素，以解决不必要地让供应链 aktör 分享数据的问题。我们的方法在实验 validate 使用实际的多Stage生产过程中的数据，结果显示，我们的方法可以更好地探测质量变化的来源，比中央化使用Shapley加itive解释法。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices"><a href="#Real-Time-Neural-Video-Recovery-and-Enhancement-on-Mobile-Devices" class="headerlink" title="Real-Time Neural Video Recovery and Enhancement on Mobile Devices"></a>Real-Time Neural Video Recovery and Enhancement on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12152">http://arxiv.org/abs/2307.12152</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Zhaoyuan He, Yifan Yang, Lili Qiu, Kyoungjun Park</li>
<li>for: 提高移动设备上视频流式的流畅体验</li>
<li>methods: 提出了一种新的视频帧恢复方案、一种新的超分辨率算法和一种接受器增强视频比特率调整算法</li>
<li>results: 实现了30帧&#x2F;秒的实时增强，在不同的网络环境下提高了视频流程的质量经验（QoE），具体的提高率为24%-82%。<details>
<summary>Abstract</summary>
As mobile devices become increasingly popular for video streaming, it's crucial to optimize the streaming experience for these devices. Although deep learning-based video enhancement techniques are gaining attention, most of them cannot support real-time enhancement on mobile devices. Additionally, many of these techniques are focused solely on super-resolution and cannot handle partial or complete loss or corruption of video frames, which is common on the Internet and wireless networks.   To overcome these challenges, we present a novel approach in this paper. Our approach consists of (i) a novel video frame recovery scheme, (ii) a new super-resolution algorithm, and (iii) a receiver enhancement-aware video bit rate adaptation algorithm. We have implemented our approach on an iPhone 12, and it can support 30 frames per second (FPS). We have evaluated our approach in various networks such as WiFi, 3G, 4G, and 5G networks. Our evaluation shows that our approach enables real-time enhancement and results in a significant increase in video QoE (Quality of Experience) of 24\% - 82\% in our video streaming system.
</details>
<details>
<summary>摘要</summary>
为了优化移动设备上的视频流处理，随着移动设备的普及，现在已经非常重要。虽然深度学习基于视频提升技术在获得关注，但大多数这些技术无法在移动设备上实时进行提升。此外，许多这些技术都是专注于超解像，而不是处理部分或完全丢失的视频帧，这是网络和无线网络中的常见问题。为了解决这些挑战，我们在本文中提出了一种新的方法。我们的方法包括以下三个部分：(i) 一种新的视频帧恢复算法，(ii) 一种新的超解像算法，(iii) 一种基于接收器提升的视频比特率自适应算法。我们在iPhone 12上实现了我们的方法，并可以支持30帧/秒。我们在WiFi、3G、4G和5G网络中进行了评估，我们的评估结果表明，我们的方法可以实现实时提升，并导致视频Quality of Experience（QoE）提高24%-82%。
</details></li>
</ul>
<hr>
<h2 id="Learned-Gridification-for-Efficient-Point-Cloud-Processing"><a href="#Learned-Gridification-for-Efficient-Point-Cloud-Processing" class="headerlink" title="Learned Gridification for Efficient Point Cloud Processing"></a>Learned Gridification for Efficient Point Cloud Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.14354">http://arxiv.org/abs/2307.14354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/computri/gridifier">https://github.com/computri/gridifier</a></li>
<li>paper_authors: Putri A. van der Linden, David W. Romero, Erik J. Bekkers</li>
<li>for: 这篇论文主要用于解决点云处理领域中的可插入性问题，提高点云处理的缩放性和可扩展性。</li>
<li>methods: 该论文提出了一种名为”学习gridification”的方法，即将点云转化为一个紧凑、规则的网格，以便在网格上使用已有的深度学习方法。</li>
<li>results: 经过 teorтиче 和实验分析，该论文表明，使用学习gridification方法可以提高点云处理的缩放性和可扩展性，同时保持与原始点云数据相比的竞争性。<details>
<summary>Abstract</summary>
Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods.   In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a learnable de-gridification step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that gridified networks scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at https://github.com/computri/gridifier.
</details>
<details>
<summary>摘要</summary>
神经操作依赖地域信息在点云上比在格子数据上更加昂贵，因为点云中点的距离不规则。在格子中，我们可以一次计算核心，然后将其重复使用所有查询位置。因此，基于地域信息的操作在点云上缩放比格子数据更差，特别是对于大输入和大地域。在这项工作中，我们解决点云方法的扩展性问题，通过将点云转换为可 compact、规则的格子。感谢gridification，后续层可以使用定义在规则格子上的操作，例如Conv3D，这些操作在点云数据上缩放更好。我们还扩展gridification来点云到点云任务，例如分割，通过在点云处理管道的末端添加学习的de-gridification步骤，将紧凑的规则格子映射回原始点云形式。通过理论和实验分析，我们表明gridified网络在内存和时间上比直接应用于原始点云数据更好的扩展性，同时能够达到竞争性的结果。我们的代码公开在https://github.com/computri/gridifier。
</details></li>
</ul>
<hr>
<h2 id="CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment"><a href="#CorrFL-Correlation-Based-Neural-Network-Architecture-for-Unavailability-Concerns-in-a-Heterogeneous-IoT-Environment" class="headerlink" title="CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment"></a>CorrFL: Correlation-Based Neural Network Architecture for Unavailability Concerns in a Heterogeneous IoT Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12149">http://arxiv.org/abs/2307.12149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Western-OC2-Lab/CorrFL">https://github.com/Western-OC2-Lab/CorrFL</a></li>
<li>paper_authors: Ibrahim Shaer, Abdallah Shami</li>
<li>For: 解决 Federated Learning（FL）中模型策略的差异和物联网（IoT）节点的缺失问题。* Methods: 提出了一种基于相关学习（Correlation-based FL，CorrFL）的方法，通过将不同模型权重映射到共同的准则空间来处理模型策略的差异。* Results: 通过对一个真实的使用场景进行评估，发现CorrFL模型在缺失IoT节点和高活动水平时的预测性能较为出色，并且与不同的使用场景中的标准模型进行比较。<details>
<summary>Abstract</summary>
The Federated Learning (FL) paradigm faces several challenges that limit its application in real-world environments. These challenges include the local models' architecture heterogeneity and the unavailability of distributed Internet of Things (IoT) nodes due to connectivity problems. These factors posit the question of "how can the available models fill the training gap of the unavailable models?". This question is referred to as the "Oblique Federated Learning" problem. This problem is encountered in the studied environment that includes distributed IoT nodes responsible for predicting CO2 concentrations. This paper proposes the Correlation-based FL (CorrFL) approach influenced by the representational learning field to address this problem. CorrFL projects the various model weights to a common latent space to address the model heterogeneity. Its loss function minimizes the reconstruction loss when models are absent and maximizes the correlation between the generated models. The latter factor is critical because of the intersection of the feature spaces of the IoT devices. CorrFL is evaluated on a realistic use case, involving the unavailability of one IoT device and heightened activity levels that reflect occupancy. The generated CorrFL models for the unavailable IoT device from the available ones trained on the new environment are compared against models trained on different use cases, referred to as the benchmark model. The evaluation criteria combine the mean absolute error (MAE) of predictions and the impact of the amount of exchanged data on the prediction performance improvement. Through a comprehensive experimental procedure, the CorrFL model outperformed the benchmark model in every criterion.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）模式面临许多实际环境中的挑战，这些挑战包括本地模型的架构多样性和分布式互联网络端的网络问题，这们问题使得“如何让可用的模型填充缺失的模型？”这个问题被称为“偏角联邦学习”问题。这个问题在分散式互联网络端负责预测CO2浓度的环境中被研究。这篇文章提出了基于相互关联学习（CorrFL）方法，它将多个模型的weight投射到共同的潜在空间以解决模型多样性问题。CorrFL的损失函数将缺失的模型的重建损失降低至最小，并将可用模型生成的模型之间的相互相关性提高。这个因素是critical，因为分散式互联网络端的特征空间 intersection。通过一个实际的使用情况，这篇文章评估了CorrFL在一个 IoT 设备缺失和活动水平增加的情况下的表现。生成的CorrFL模型与不可用的 IoT 设备进行比较，并与不同的使用情况下的模型进行比较，这些模型被称为底线模型。评估标准包括预测误差的总平均误差（MAE）和预测性能改善的资料交换量影响。通过一个完整的实验程序，CorrFL 模型在每个标准中都表现出优于底线模型。
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems"><a href="#Applications-of-Machine-Learning-to-Modelling-and-Analysing-Dynamical-Systems" class="headerlink" title="Applications of Machine Learning to Modelling and Analysing Dynamical Systems"></a>Applications of Machine Learning to Modelling and Analysing Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03763">http://arxiv.org/abs/2308.03763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedanta Thapar</li>
<li>for: 本研究使用物理学 Informed Neural Networks 分析非线性哈密顿动力系统，具有一个动力平衡方程的第一 интеграル。</li>
<li>methods: 本文提出了一种结合现有哈密顿神经网络结构的 Adaptable Symplectic Recurrent Neural Networks 架构，能够保持哈密顿方程和相互作用的 symplectic 结构，并在整个参数空间预测动力学行为。此架构在预测哈密顿动力学中，特别是在含有多个参数的潜能中表现出了显著的优势。</li>
<li>results: 本研究表明，使用该架构可以高效地预测哈密顿动力学，尤其是在含有多个参数的潜能中。此外， authors 还证明了该方法在单参数潜能中的稳定性和长期预测性。<details>
<summary>Abstract</summary>
We explore the use of Physics Informed Neural Networks to analyse nonlinear Hamiltonian Dynamical Systems with a first integral of motion. In this work, we propose an architecture which combines existing Hamiltonian Neural Network structures into Adaptable Symplectic Recurrent Neural Networks which preserve Hamilton's equations as well as the symplectic structure of phase space while predicting dynamics for the entire parameter space. This architecture is found to significantly outperform previously proposed neural networks when predicting Hamiltonian dynamics especially in potentials which contain multiple parameters. We demonstrate its robustness using the nonlinear Henon-Heiles potential under chaotic, quasiperiodic and periodic conditions.   The second problem we tackle is whether we can use the high dimensional nonlinear capabilities of neural networks to predict the dynamics of a Hamiltonian system given only partial information of the same. Hence we attempt to take advantage of Long Short Term Memory networks to implement Takens' embedding theorem and construct a delay embedding of the system followed by mapping the topologically invariant attractor to the true form. This architecture is then layered with Adaptable Symplectic nets to allow for predictions which preserve the structure of Hamilton's equations. We show that this method works efficiently for single parameter potentials and provides accurate predictions even over long periods of time.
</details>
<details>
<summary>摘要</summary>
我们探讨使用物理 Informed Neural Networks 分析非线性汉密尔顿动力系统，其具有一个动力的第一Integral of motion。在这项工作中，我们提议一种结合现有汉密尔顿神经网络结构的可适应 симплектиче Recurrent Neural Networks 结构，该结构保留汉密尔顿方程以及相对空间的 симплектиче结构，同时预测动力的整个参数空间。这种结构在预测汉密尔顿动力方面表现出了明显的优异，尤其是在含有多个参数的潜能中。我们通过使用非线性 Henon-Heiles 潜能函数进行了robustness测试，并在各种不同的conditions下进行了验证。第二个问题是可以使用高维非线性神经网络来预测汉密尔顿系统的动力，只要知道一部分系统的信息吗。因此，我们尝试使用 Long Short Term Memory 网络实现 Takens 嵌入定理，并将系统的延迟嵌入映射到真正的形式。然后，我们层加 Adaptable Symplectic nets 以使预测保留汉密尔顿方程的结构。我们发现这种方法可以高效地预测单参数潜能中的动力，并且可以在长时间内提供高度准确的预测。
</details></li>
</ul>
<hr>
<h2 id="A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter"><a href="#A-Vision-for-Cleaner-Rivers-Harnessing-Snapshot-Hyperspectral-Imaging-to-Detect-Macro-Plastic-Litter" class="headerlink" title="A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter"></a>A Vision for Cleaner Rivers: Harnessing Snapshot Hyperspectral Imaging to Detect Macro-Plastic Litter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12145">http://arxiv.org/abs/2307.12145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/river-lab/hyperspectral_macro_plastic_detection">https://github.com/river-lab/hyperspectral_macro_plastic_detection</a></li>
<li>paper_authors: Nathaniel Hanson, Ahmet Demirkaya, Deniz Erdoğmuş, Aron Stubbins, Taşkın Padır, Tales Imbiriba</li>
<li>for: 本研究旨在开发一种高效自动化的浮游垃圾监测方法，以解决水体中垃圾杂物的监测问题。</li>
<li>methods: 本研究使用计算成像技术进行浮游垃圾物质的检测，包括可见短波谱成像和可见短波谱识别方法。</li>
<li>results: 实验结果表明，使用Snapshot可见短波谱成像和机器学习分类方法可以在实际场景中实现高检测精度，特别是在具有挑战性的场景下。<details>
<summary>Abstract</summary>
Plastic waste entering the riverine harms local ecosystems leading to negative ecological and economic impacts. Large parcels of plastic waste are transported from inland to oceans leading to a global scale problem of floating debris fields. In this context, efficient and automatized monitoring of mismanaged plastic waste is paramount. To address this problem, we analyze the feasibility of macro-plastic litter detection using computational imaging approaches in river-like scenarios. We enable near-real-time tracking of partially submerged plastics by using snapshot Visible-Shortwave Infrared hyperspectral imaging. Our experiments indicate that imaging strategies associated with machine learning classification approaches can lead to high detection accuracy even in challenging scenarios, especially when leveraging hyperspectral data and nonlinear classifiers. All code, data, and models are available online: https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection.
</details>
<details>
<summary>摘要</summary>
塑料废弃物进入河流环境会对当地生态系统造成负面影响，导致生态和经济问题。大量塑料废弃物从陆地传输到海洋，导致全球范围内漂浮垃圾场景。在这种情况下，高效和自动化的废弃塑料监测变得非常重要。为解决这个问题，我们分析了使用计算成像方法检测大型塑料废弃物的可能性。我们使用快照可见短波谱 hyperspectral成像进行近实时检测半潜水塑料。我们的实验表明，通过使用机器学习分类方法和非线性分类器，可以在具有挑战性的情况下实现高检测精度。所有代码、数据和模型都可以在 GitHub 上下载：https://github.com/RIVeR-Lab/hyperspectral_macro_plastic_detection。
</details></li>
</ul>
<hr>
<h2 id="Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning"><a href="#Emergence-of-Adaptive-Circadian-Rhythms-in-Deep-Reinforcement-Learning" class="headerlink" title="Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning"></a>Emergence of Adaptive Circadian Rhythms in Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12143">http://arxiv.org/abs/2307.12143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aqeel13932/mn_project">https://github.com/aqeel13932/mn_project</a></li>
<li>paper_authors: Aqeel Labash, Florian Fletzer, Daniel Majoral, Raul Vicente</li>
<li>For: The paper explores the emergence of circadian-like rhythms in deep reinforcement learning agents.* Methods: The authors use a foraging task and a reliable periodic variation in the environment to train the agents. They systematically characterize the agents’ behavior during learning and analyze the emergence of an endogenous rhythm using bifurcation and phase response curve analyses.* Results: The paper shows that the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training, and demonstrates how artificial neurons develop dynamics to support the internalization of the environmental rhythm. The adaptation proceeds through the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows optimal phase synchronization between the agent’s dynamics and the environmental rhythm.Here’s the Chinese translation of the three points:* For: 这篇论文研究了深度奖励学习代理人是如何适应环境的 rhythm。* Methods: 作者使用了一个捕食任务和一个可预测的环境变化来训练代理人。他们系统地描述了代理人在学习过程中的行为，并通过分支和相位响应曲线分析来分析内在的 rhythm 的出现。* Results: 论文显示了内在 rhythm 可以适应环境信号的相位变化，而不需要任何再训练。它还表明了人工神经元的动力学发展了一种支持内在 rhythm 的动力学特性，并且在代理人动力学和环境 rhythm 之间进行了优化的相位同步。从动力学视角来看， adaptive 进程是通过内在 rhythm 的稳定 periodic orbit 的出现来实现的，该 periodic orbit 的相位响应允许代理人动力学和环境 rhythm 之间的优化相位同步。<details>
<summary>Abstract</summary>
Adapting to regularities of the environment is critical for biological organisms to anticipate events and plan. A prominent example is the circadian rhythm corresponding to the internalization by organisms of the $24$-hour period of the Earth's rotation. In this work, we study the emergence of circadian-like rhythms in deep reinforcement learning agents. In particular, we deployed agents in an environment with a reliable periodic variation while solving a foraging task. We systematically characterize the agent's behavior during learning and demonstrate the emergence of a rhythm that is endogenous and entrainable. Interestingly, the internal rhythm adapts to shifts in the phase of the environmental signal without any re-training. Furthermore, we show via bifurcation and phase response curve analyses how artificial neurons develop dynamics to support the internalization of the environmental rhythm. From a dynamical systems view, we demonstrate that the adaptation proceeds by the emergence of a stable periodic orbit in the neuron dynamics with a phase response that allows an optimal phase synchronisation between the agent's dynamics and the environmental rhythm.
</details>
<details>
<summary>摘要</summary>
适应环境的规律是生物体预测事件和规划的关键。一个明显的例子是生物体内部的 circadian 频率，即通过生物体内部内化地球的24小时转动周期。在这项工作中，我们研究了深度学习Agent中的 circadian-like 频率的出现。特别是，我们在一个可靠 periodic 变化的环境中部署了 Agent，并在寻食任务中学习。我们系统地描述了 Agent 的行为 durante 学习，并证明了 Agent 内部的频率可以自动适应环境的阶段偏移。此外，我们通过杂化和相对响应曲线分析表明，人工神经元发展了 dynamics 来支持内部化环境的频率。从动力系统视角来看，适应进程由神经元动力学中的稳定 periodic 轨迹的出现和相应的相位协调导致。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem"><a href="#Unlocking-Carbon-Reduction-Potential-with-Reinforcement-Learning-for-the-Three-Dimensional-Loading-Capacitated-Vehicle-Routing-Problem" class="headerlink" title="Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem"></a>Unlocking Carbon Reduction Potential with Reinforcement Learning for the Three-Dimensional Loading Capacitated Vehicle Routing Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12136">http://arxiv.org/abs/2307.12136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Schoepf, Stephen Mak, Julian Senoner, Liming Xu, Netland Torbjörn, Alexandra Brintrup</li>
<li>for: 增加效率，提高运输效率</li>
<li>methods: 使用强化学习模型</li>
<li>results: 与现有方法相比，平均差距在3.83%到8.10%之间<details>
<summary>Abstract</summary>
Heavy goods vehicles are vital backbones of the supply chain delivery system but also contribute significantly to carbon emissions with only 60% loading efficiency in the United Kingdom. Collaborative vehicle routing has been proposed as a solution to increase efficiency, but challenges remain to make this a possibility. One key challenge is the efficient computation of viable solutions for co-loading and routing. Current operations research methods suffer from non-linear scaling with increasing problem size and are therefore bound to limited geographic areas to compute results in time for day-to-day operations. This only allows for local optima in routing and leaves global optimisation potential untouched. We develop a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. While this problem has been studied extensively in operations research, no publications on solving it with reinforcement learning exist. We demonstrate the favourable scaling of our reinforcement learning model and benchmark our routing performance against state-of-the-art methods. The model performs within an average gap of 3.83% to 8.10% compared to established methods. Our model not only represents a promising first step towards large-scale logistics optimisation with reinforcement learning but also lays the foundation for this research stream.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we have developed a reinforcement learning model to solve the three-dimensional loading capacitated vehicle routing problem in approximately linear time. This problem has been extensively studied in operations research, but no publications on solving it with reinforcement learning exist. We demonstrate the favourable scaling of our reinforcement learning model and benchmark our routing performance against state-of-the-art methods. Our model performs within an average gap of 3.83% to 8.10% compared to established methods.Our model not only represents a promising first step towards large-scale logistics optimization with reinforcement learning but also lays the foundation for this research stream. With the ability to efficiently compute viable solutions for co-loading and routing, we can significantly reduce carbon emissions from heavy goods vehicles and improve the overall efficiency of the supply chain delivery system.
</details></li>
</ul>
<hr>
<h2 id="The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes"><a href="#The-Sample-Complexity-of-Multi-Distribution-Learning-for-VC-Classes" class="headerlink" title="The Sample Complexity of Multi-Distribution Learning for VC Classes"></a>The Sample Complexity of Multi-Distribution Learning for VC Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12135">http://arxiv.org/abs/2307.12135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Awasthi, Nika Haghtalab, Eric Zhao</li>
<li>for: 多 Distribution Learning 是一种自然推广 PAC 学习到多个数据分布的设置中。</li>
<li>methods: 使用游戏动力学来解决这个问题，并讨论了一些基本障碍。</li>
<li>results: 研究表明，现有的最佳下界是 $\Omega(\epsilon^{-2}(d + k \ln(k)))$, 而实际 Sample Complexity 为 $O(\epsilon^{-2} \ln(k)(d + k) + \min{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d})$.<details>
<summary>Abstract</summary>
Multi-distribution learning is a natural generalization of PAC learning to settings with multiple data distributions. There remains a significant gap between the known upper and lower bounds for PAC-learnable classes. In particular, though we understand the sample complexity of learning a VC dimension d class on $k$ distributions to be $O(\epsilon^{-2} \ln(k)(d + k) + \min\{\epsilon^{-1} dk, \epsilon^{-4} \ln(k) d\})$, the best lower bound is $\Omega(\epsilon^{-2}(d + k \ln(k)))$. We discuss recent progress on this problem and some hurdles that are fundamental to the use of game dynamics in statistical learning.
</details>
<details>
<summary>摘要</summary>
多分布学习是自然推广PAC学习的设置中的多个数据分布的一种自然推广。现存在较大的知识上下文和下界之间的差距。具体来说，虽然我们理解了VC阶数d在k个分布上学习的样本复杂度为O（ε^-2 \* ln(k) (d + k) + MIN（ε^-1 dk, ε^-4 \* ln(k) d）），但最好的下界是Ω（ε^-2 (d + k \* ln(k))）。我们讨论了这个问题的最新进展和使用游戏动力学在统计学习中的核心障碍。
</details></li>
</ul>
<hr>
<h2 id="AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities"><a href="#AI-on-the-Road-A-Comprehensive-Analysis-of-Traffic-Accidents-and-Accident-Detection-System-in-Smart-Cities" class="headerlink" title="AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities"></a>AI on the Road: A Comprehensive Analysis of Traffic Accidents and Accident Detection System in Smart Cities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12128">http://arxiv.org/abs/2307.12128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Victoria Wangia-Anderson, Ahmed Abdelgawad</li>
<li>for: 本研究旨在提高交通管理和交通事故减少，通过分析不同地区的交通事故数据，提出一个基于交通监控摄像头和动作识别系统的交通事故探测和应答框架。</li>
<li>methods: 本研究使用了国家公路交通安全管理局（NHTSA）的交通事故报告采样系统（CRSS）数据进行交通事故分析，并提出了一种基于机器学习算法和交通监控摄像头的交通事故探测和应答框架。</li>
<li>results: 本研究发现了不同地区的交通事故特征和趋势，并提出了一种基于交通监控摄像头和动作识别系统的交通事故探测和应答框架，可以减少交通事故的频率和严重程度，提高交通管理的效率和安全性。<details>
<summary>Abstract</summary>
Accident detection and traffic analysis is a critical component of smart city and autonomous transportation systems that can reduce accident frequency, severity and improve overall traffic management. This paper presents a comprehensive analysis of traffic accidents in different regions across the United States using data from the National Highway Traffic Safety Administration (NHTSA) Crash Report Sampling System (CRSS). To address the challenges of accident detection and traffic analysis, this paper proposes a framework that uses traffic surveillance cameras and action recognition systems to detect and respond to traffic accidents spontaneously. Integrating the proposed framework with emergency services will harness the power of traffic cameras and machine learning algorithms to create an efficient solution for responding to traffic accidents and reducing human errors. Advanced intelligence technologies, such as the proposed accident detection systems in smart cities, will improve traffic management and traffic accident severity. Overall, this study provides valuable insights into traffic accidents in the US and presents a practical solution to enhance the safety and efficiency of transportation systems.
</details>
<details>
<summary>摘要</summary>
智能城市和自动交通系统中的事故探测和交通分析是一个关键组成部分，可以降低事故频率、严重程度并改善总体交通管理。这篇论文对美国各地的交通事故进行了全面的分析，使用国家公路安全管理局（NHTSA）的事故报告采样系统（CRSS）的数据。为了解决事故探测和交通分析的挑战，该论文提出了一个框架，使用交通监控摄像头和动作认知系统来自动探测和应对交通事故。将该框架与紧急服务集成，可以利用交通摄像头和机器学习算法创造一种高效的交通事故应对解决方案，减少人类错误。高级智能技术，如智能城市中的事故探测系统，将改善交通管理和交通事故严重程度。总的来说，这篇研究提供了美国交通事故的有价值的视角，并提出了实用的解决方案，以提高交通系统的安全和效率。
</details></li>
</ul>
<hr>
<h2 id="Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network"><a href="#Synthesis-of-Batik-Motifs-using-a-Diffusion-–-Generative-Adversarial-Network" class="headerlink" title="Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network"></a>Synthesis of Batik Motifs using a Diffusion – Generative Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12122">http://arxiv.org/abs/2307.12122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/octadion/diffusion-stylegan2-ada-pytorch">https://github.com/octadion/diffusion-stylegan2-ada-pytorch</a></li>
<li>paper_authors: One Octadion, Novanto Yudistira, Diva Kurnianingtyas</li>
<li>for:  assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs.</li>
<li>methods:  using StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns, with adjustments to the model architecture and a well-curated batik dataset.</li>
<li>results:  capable of producing authentic and quality batik patterns, with finer details and rich artistic variations.<details>
<summary>Abstract</summary>
Batik, a unique blend of art and craftsmanship, is a distinct artistic and technological creation for Indonesian society. Research on batik motifs is primarily focused on classification. However, further studies may extend to the synthesis of batik patterns. Generative Adversarial Networks (GANs) have been an important deep learning model for generating synthetic data, but often face challenges in the stability and consistency of results. This research focuses on the use of StyleGAN2-Ada and Diffusion techniques to produce realistic and high-quality synthetic batik patterns. StyleGAN2-Ada is a variation of the GAN model that separates the style and content aspects in an image, whereas diffusion techniques introduce random noise into the data. In the context of batik, StyleGAN2-Ada and Diffusion are used to produce realistic synthetic batik patterns. This study also made adjustments to the model architecture and used a well-curated batik dataset. The main goal is to assist batik designers or craftsmen in producing unique and quality batik motifs with efficient production time and costs. Based on qualitative and quantitative evaluations, the results show that the model tested is capable of producing authentic and quality batik patterns, with finer details and rich artistic variations. The dataset and code can be accessed here:https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details>
<details>
<summary>摘要</summary>
《独特的抽象艺术》——巴迪克的研究巴迪克是印度尼西亚社会独特的艺术和手工艺术品。研究巴迪克图案主要集中在分类方面，但可能会扩展到synthesize batik patterns。生成对抗网络（GANs）是深度学习模型，可以生成 sintetic data，但经常面临稳定性和一致性的挑战。本研究使用StyleGAN2-Ada和扩散技术生成高质量和真实的 sintetic batik patterns。StyleGAN2-Ada分离图像中的风格和内容两个方面，而扩散技术引入随机噪音。在batik中，StyleGAN2-Ada和扩散被用来生成真实的 sintetic batik patterns。本研究还对模型结构进行了调整，使用了高质量的batik dataset。主要目标是帮助batik设计师或手工艺术家生成独特和高质量的batik图案，以及减少生产时间和成本。根据质量和量的评估，研究结果表明模型能够生成authentic和高质量的batik patterns，具有细节和艺术变化。数据集和代码可以在以下链接获取：https://github.com/octadion/diffusion-stylegan2-ada-pytorch
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/cs.LG_2023_07_23/" data-id="clp8zxr9100opn688569z2sob" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_07_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/23/eess.IV_2023_07_23/" class="article-date">
  <time datetime="2023-07-23T09:00:00.000Z" itemprop="datePublished">2023-07-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/23/eess.IV_2023_07_23/">eess.IV - 2023-07-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection"><a href="#ES2Net-An-Efficient-Spectral-Spatial-Network-for-Hyperspectral-Image-Change-Detection" class="headerlink" title="ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection"></a>ES2Net: An Efficient Spectral-Spatial Network for Hyperspectral Image Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12327">http://arxiv.org/abs/2307.12327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingren Yao, Yuan Zhou, Wei Xiang</li>
<li>for: 本研究旨在提高迁移图像特征检测精度，具体目的是针对高spectralresolution干扰图像（HSIs）进行迁移特征检测。</li>
<li>methods: 本研究提出了一种综合利用深度学习和频谱筛选技术的迁移特征检测网络（ES2Net），其中包括一个学习式频谱筛选模块，可以自动选择适合迁移检测的频谱 bands。此外，为了考虑不同频谱 bands 之间的复杂非线性关系，我们还提出了一种帧度级空间注意力机制。</li>
<li>results: 实验表明，ES2Net 比其他当前领域state-of-the-art方法更高效和精度。<details>
<summary>Abstract</summary>
Hyperspectral image change detection (HSI-CD) aims to identify the differences in bitemporal HSIs. To mitigate spectral redundancy and improve the discriminativeness of changing features, some methods introduced band selection technology to select bands conducive for CD. However, these methods are limited by the inability to end-to-end training with the deep learning-based feature extractor and lack considering the complex nonlinear relationship among bands. In this paper, we propose an end-to-end efficient spectral-spatial change detection network (ES2Net) to address these issues. Specifically, we devised a learnable band selection module to automatically select bands conducive to CD. It can be jointly optimized with a feature extraction network and capture the complex nonlinear relationships among bands. Moreover, considering the large spatial feature distribution differences among different bands, we design the cluster-wise spatial attention mechanism that assigns a spatial attention factor to each individual band to individually improve the feature discriminativeness for each band. Experiments on three widely used HSI-CD datasets demonstrate the effectiveness and superiority of this method compared with other state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
干支图像变化检测（HSI-CD）的目标是确定双时间干支图像之间的差异。为了减少 спектраль的重复性和提高变化特征的抑制力，一些方法引入了频率选择技术，以选择适合变化检测的频率。然而，这些方法受到练习深度学习基于特征提取器的端到端训练的限制，以及各频率之间的复杂非线性关系的忽略。在本文中，我们提出了一种练习效率的 spectral-spatial 变化检测网络（ES2Net），以解决这些问题。具体来说，我们设计了一个学习型频率选择模块，可以自动选择适合变化检测的频率。这个模块可以与特征提取网络jointly 优化，并 capture 各频率之间的复杂非线性关系。此外，考虑到不同频率之间的大规模空间特征分布差异，我们设计了各个频率的各自精度注意力机制，以进一步提高每个频率的特征抑制力。在三个广泛使用的HSI-CD数据集上进行了实验，我们发现该方法与其他当前状态的方法相比，具有更高的效iveness和优势。
</details></li>
</ul>
<hr>
<h2 id="Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models"><a href="#Development-of-pericardial-fat-count-images-using-a-combination-of-three-different-deep-learning-models" class="headerlink" title="Development of pericardial fat count images using a combination of three different deep-learning models"></a>Development of pericardial fat count images using a combination of three different deep-learning models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12316">http://arxiv.org/abs/2307.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takaaki Matsunaga, Atsushi Kono, Hidetoshi Matsuo, Kaoru Kitagawa, Mizuho Nishio, Hiromi Hashimura, Yu Izawa, Takayoshi Toba, Kazuki Ishikawa, Akie Katsuki, Kazuyuki Ohmura, Takamichi Murakami</li>
<li>for: 这个研究旨在使用深度学习模型从胸部X射线图像中生成胸部脂肪计数图像（PFCIs），以评估胸部脂肪的水平。</li>
<li>methods: 这个研究使用了3种不同的深度学习模型，包括CycleGAN，将胸部CT图像投影到2D图像上，并使用高像素值表示脂肪聚集。</li>
<li>results: 比较了使用提案方法生成的PFCIs和单一CycleGAN-based模型生成的PFCIs，发现提案方法生成的PFCIs具有更高的图像质量指标（SSIM、MSE、MAE）。<details>
<summary>Abstract</summary>
Rationale and Objectives: Pericardial fat (PF), the thoracic visceral fat surrounding the heart, promotes the development of coronary artery disease by inducing inflammation of the coronary arteries. For evaluating PF, this study aimed to generate pericardial fat count images (PFCIs) from chest radiographs (CXRs) using a dedicated deep-learning model.   Materials and Methods: The data of 269 consecutive patients who underwent coronary computed tomography (CT) were reviewed. Patients with metal implants, pleural effusion, history of thoracic surgery, or that of malignancy were excluded. Thus, the data of 191 patients were used. PFCIs were generated from the projection of three-dimensional CT images, where fat accumulation was represented by a high pixel value. Three different deep-learning models, including CycleGAN, were combined in the proposed method to generate PFCIs from CXRs. A single CycleGAN-based model was used to generate PFCIs from CXRs for comparison with the proposed method. To evaluate the image quality of the generated PFCIs, structural similarity index measure (SSIM), mean squared error (MSE), and mean absolute error (MAE) of (i) the PFCI generated using the proposed method and (ii) the PFCI generated using the single model were compared.   Results: The mean SSIM, MSE, and MAE were as follows: 0.856, 0.0128, and 0.0357, respectively, for the proposed model; and 0.762, 0.0198, and 0.0504, respectively, for the single CycleGAN-based model.   Conclusion: PFCIs generated from CXRs with the proposed model showed better performance than those with the single model. PFCI evaluation without CT may be possible with the proposed method.
</details>
<details>
<summary>摘要</summary>
理解和目标：胸膈脂肪（PF），脊梗内脂肪环绕心脏，促进了折射病变的发展。为评估PF，本研究旨在通过专门的深度学习模型生成胸膈脂肪计数图像（PFCIs）从胸部X射线图（CXRs）中。材料和方法：本研究查阅了269例 consecutively患者的折射 computed tomography（CT）数据。患有金属设备、肿胀、历史折射手术或肿瘤患者被排除。因此，该研究使用了191例患者的数据。PFCIs通过三维CT图像的投影，表示脂肪堆积的高像素值来生成。三种不同的深度学习模型，包括CycleGAN，被组合在提议的方法中来生成PFCIs从CXRs。单独使用CycleGAN基于模型生成PFCIs从CXRs作为比较。为评估生成的PFCIs的图像质量，使用了结构相似度指标（SSIM）、平均方差（MSE）和平均绝对错误（MAE）进行比较。结果：生成的PFCIs的SSIM、MSE和MAE分别为：0.856、0.0128和0.0357；而单独使用CycleGAN基于模型生成的PFCIs的SSIM、MSE和MAE分别为：0.762、0.0198和0.0504。结论：提议的方法生成的PFCIs在SSIM、MSE和MAE指标上表现更好于单独使用CycleGAN基于模型生成的PFCIs。PFCI评估可能不需要CT成像。
</details></li>
</ul>
<hr>
<h2 id="Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames"><a href="#Simultaneous-temperature-estimation-and-nonuniformity-correction-from-multiple-frames" class="headerlink" title="Simultaneous temperature estimation and nonuniformity correction from multiple frames"></a>Simultaneous temperature estimation and nonuniformity correction from multiple frames</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12297">http://arxiv.org/abs/2307.12297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Omri Berman, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 用于温度测量的低成本紫外线相机中的非均匀性和温度测量偏差问题的解决方案。</li>
<li>methods: 基于物理捕获模型和深度学习核函数网络（KPN）的方法，通过将多帧图像 fusion 来实现同时的温度估计和非均匀性修正。另外，还提出了一个新的偏移块，可以将 ambient 温度包含在模型中，以便估计相机的偏移。</li>
<li>results: 通过对实际数据进行测试，得到了与高科技质量的放射计相机相比，仅有0.27-0.54℃的小差异。这种方法可以提供高精度和高效的同时温度估计和非均匀性修正解决方案，对各种实际应用场景具有重要意义。<details>
<summary>Abstract</summary>
Infrared (IR) cameras are widely used for temperature measurements in various applications, including agriculture, medicine, and security. Low-cost IR camera have an immense potential to replace expansive radiometric cameras in these applications, however low-cost microbolometer-based IR cameras are prone to spatially-variant nonuniformity and to drift in temperature measurements, which limits their usability in practical scenarios.   To address these limitations, we propose a novel approach for simultaneous temperature estimation and nonuniformity correction from multiple frames captured by low-cost microbolometer-based IR cameras. We leverage the physical image acquisition model of the camera and incorporate it into a deep learning architecture called kernel estimation networks (KPN), which enables us to combine multiple frames despite imperfect registration between them. We also propose a novel offset block that incorporates the ambient temperature into the model and enables us to estimate the offset of the camera, which is a key factor in temperature estimation.   Our findings demonstrate that the number of frames has a significant impact on the accuracy of temperature estimation and nonuniformity correction. Moreover, our approach achieves a significant improvement in performance compared to vanilla KPN, thanks to the offset block. The method was tested on real data collected by a low-cost IR camera mounted on a UAV, showing only a small average error of $0.27^\circ C-0.54^\circ C$ relative to costly scientific-grade radiometric cameras.   Our method provides an accurate and efficient solution for simultaneous temperature estimation and nonuniformity correction, which has important implications for a wide range of practical applications.
</details>
<details>
<summary>摘要</summary>
低成本红外线（IR）镜头在不同应用中广泛使用，包括农业、医学和安全领域。低成本微波温度计IR镜头具有巨大的潜在可能性，以取代昂贵的几何学测量镜头，但是它们受到空间不均匀和测量偏差的限制，这限制了它们在实际应用中的可用性。为了解决这些限制，我们提出了一个新的方法，可以同时进行测量温度和非均匀调正。我们利用镜头的物理摄取模型，并将其 integrate into a deep learning architecture called kernel estimation networks (KPN)，这使得我们可以融合多帧影像，即使它们不具有完美的对齐。我们还提出了一个 novel offset block，它包含了环境温度，并允许我们估计镜头的偏移，这是温度估计中的关键因素。我们的研究表明，影像数量有显著的影响温度估计和非均匀调正的精度。此外，我们的方法在比vanilla KPN更好的性能，感谢偏移层的存在。我们的方法在实际应用中使用低成本IR镜头，与较贵的科学级测量镜头相比， пока有小平均误差为0.27-0.54℃。我们的方法提供了一个精确和高效的温度估计和非均匀调正方法，具有广泛的实际应用。
</details></li>
</ul>
<hr>
<h2 id="ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising"><a href="#ASCON-Anatomy-aware-Supervised-Contrastive-Learning-Framework-for-Low-dose-CT-Denoising" class="headerlink" title="ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising"></a>ASCON: Anatomy-aware Supervised Contrastive Learning Framework for Low-dose CT Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12225">http://arxiv.org/abs/2307.12225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hao1635/ASCON">https://github.com/hao1635/ASCON</a></li>
<li>paper_authors: Zhihao Chen, Qi Gao, Yi Zhang, Hongming Shan</li>
<li>for: 这个论文是设计来进行低剂量 Computed Tomography（CT）扫描图像干扰除掉噪声的方法。</li>
<li>methods: 这个方法使用了两个新的设计：一个高效的自我注意力-based U-Net（ESAU-Net）和一个多尺度解剖对比网络（MAC-Net）。ESAU-Net使用了通道对自我注意力的机制来更好地捕捉全域-地方互动，而MAC-Net则包括一个单元非对比模组来捕捉解剖信息和一个像素对比模组来维持自体解剖一致性。</li>
<li>results: 实验结果显示，ASCON在两个公共的低剂量 CT 扫描干扰 dataset 上表现出色，较之前的模型更好。此外，ASCON还提供了解剖解释，允许在低剂量 CT 扫描中进行解剖可读性检查。<details>
<summary>Abstract</summary>
While various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, most of them leverage the normal-dose CT images as the ground-truth to supervise the denoising process. These methods typically ignore the inherent correlation within a single CT image, especially the anatomical semantics of human tissues, and lack the interpretability on the denoising process. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to the high-resolution input, an efficient ESAU-Net is introduced by using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models. Remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.
</details>
<details>
<summary>摘要</summary>
“Various deep learning methods have been proposed for low-dose computed tomography (CT) denoising, but most of them rely on normal-dose CT images as ground truth to supervise the denoising process, ignoring the inherent correlation within a single CT image and lacking interpretability. In this paper, we propose a novel Anatomy-aware Supervised CONtrastive learning framework, termed ASCON, which can explore the anatomical semantics for low-dose CT denoising while providing anatomical interpretability. The proposed ASCON consists of two novel designs: an efficient self-attention-based U-Net (ESAU-Net) and a multi-scale anatomical contrastive network (MAC-Net). First, to better capture global-local interactions and adapt to high-resolution input, an efficient ESAU-Net is introduced using a channel-wise self-attention mechanism. Second, MAC-Net incorporates a patch-wise non-contrastive module to capture inherent anatomical information and a pixel-wise contrastive module to maintain intrinsic anatomical consistency. Extensive experimental results on two public low-dose CT denoising datasets demonstrate superior performance of ASCON over state-of-the-art models, and remarkably, our ASCON provides anatomical interpretability for low-dose CT denoising for the first time. Source code is available at https://github.com/hao1635/ASCON.”Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images"><a href="#SCPAT-GAN-Structural-Constrained-and-Pathology-Aware-Convolutional-Transformer-GAN-for-Virtual-Histology-Staining-of-Human-Coronary-OCT-images" class="headerlink" title="SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images"></a>SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12138">http://arxiv.org/abs/2307.12138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueshen Li, Hongshan Liu, Xiaoyu Song, Brigitta C. Brott, Silvio H. Litovsky, Yu Gan</li>
<li>for: 用于生成基于OCT图像的虚拟病理信息，以更好地指导心血管疾病的治疗。</li>
<li>methods: 使用 transformer生成对抗网络，并在结构层进行病理导向的制约，以生成虚拟染色H&amp;E压痕。</li>
<li>results: 提高了现有方法的病理准确率和结构准确率，并可以在不需要大量 paired 训练数据的情况下生成虚拟病理信息。<details>
<summary>Abstract</summary>
There is a significant need for the generation of virtual histological information from coronary optical coherence tomography (OCT) images to better guide the treatment of coronary artery disease. However, existing methods either require a large pixel-wisely paired training dataset or have limited capability to map pathological regions. To address these issues, we proposed a structural constrained, pathology aware, transformer generative adversarial network, namely SCPAT-GAN, to generate virtual stained H&E histology from OCT images. The proposed SCPAT-GAN advances existing methods via a novel design to impose pathological guidance on structural layers using transformer-based network.
</details>
<details>
<summary>摘要</summary>
“ coronary optical coherence tomography（OCT）图像中的虚拟 histological 信息的生成具有抑制 coronary artery disease 的治疗方法的重要需求。然而，现有的方法 either require a large paired training dataset or have limited capability to map pathological regions。为解决这些问题，我们提出了一种基于 transformer 的权重约束、病理相关的生成对抗网络，即 SCPAT-GAN，用于从 OCT 图像中生成虚拟染色 H&E 病理图像。我们的提议的 SCPAT-GAN 在现有方法中提供了一种新的设计，通过 transformer 基于的网络来强制 paths 层中的病理指导。”Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you have any further questions or need more accurate translations, please feel free to ask!
</details></li>
</ul>
<hr>
<h2 id="Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks"><a href="#Improving-temperature-estimation-in-low-cost-infrared-cameras-using-deep-neural-networks" class="headerlink" title="Improving temperature estimation in low-cost infrared cameras using deep neural networks"></a>Improving temperature estimation in low-cost infrared cameras using deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12130">http://arxiv.org/abs/2307.12130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navot Oz, Nir Sochen, David Mendelovich, Iftach Klapp</li>
<li>for: 提高低成本热相机的温度准确性和纠正非均匀性。</li>
<li>methods: 开发了一个考虑 ambient temperature 的非均匀性模拟器，并提出了一种基于全连接神经网络的热体温度估算方法，通过使用单个图像和摄像头自身测量的 ambient temperature 来纠正非均匀性。</li>
<li>results: 比前作下降了约 $1^\circ C$ 的平均温度误差，并且通过应用物理约束降低了误差的 $4%$。 验证数据集上的平均温度误差为 $0.37^\circ C$，并在实际场景中也得到了相当的结果。<details>
<summary>Abstract</summary>
Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have space-variant nonuniformity across their detector. Both inaccuracy and nonuniformity are dependent on the ambient temperature of the camera. The main goal of this work was to improve the temperature accuracy of low-cost cameras and rectify the nonuniformity.   A nonuniformity simulator that accounts for the ambient temperature was developed. An end-to-end neural network that incorporates the ambient temperature at image acquisition was introduced. The neural network was trained with the simulated nonuniformity data to estimate the object's temperature and correct the nonuniformity, using only a single image and the ambient temperature measured by the camera itself. Results show that the proposed method lowered the mean temperature error by approximately $1^\circ C$ compared to previous works. In addition, applying a physical constraint on the network lowered the error by an additional $4\%$.   The mean temperature error over an extensive validation dataset was $0.37^\circ C$. The method was verified on real data in the field and produced equivalent results.
</details>
<details>
<summary>摘要</summary>
低成本热相机的精度受到 ambient temperature 的影响（通常在 $\pm 3^\circ C$ 范围内），并且具有空间不均的非统一性，这两个问题都与热相机的 ambient temperature 相关。本研究的主要目标是提高低成本热相机的温度精度和修正非统一性。我们开发了一个考虑 ambient temperature 的非统一性模拟器，并提出了一种基于 neural network 的方法，该方法可以使用单个图像和热相机自己测量的 ambient temperature 来估计物体温度并修正非统一性。实验结果表明，我们的方法可以降低mean温度错误约为 $1^\circ C$  compared to 前一代方法。此外，通过 físical 约束对网络进行限制，可以降低错误约为 $4\%$。整体来说，我们的方法在广泛验证数据集上的 mean 温度错误为 $0.37^\circ C$。此外，我们的方法在实际场景中也得到了Equivalent 的结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/23/eess.IV_2023_07_23/" data-id="clp8zxrfx017nn6888n39f478" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.CV_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T13:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.CV_2023_07_22/">cs.CV - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Spatial-Self-Distillation-for-Object-Detection-with-Inaccurate-Bounding-Boxes"><a href="#Spatial-Self-Distillation-for-Object-Detection-with-Inaccurate-Bounding-Boxes" class="headerlink" title="Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes"></a>Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12101">http://arxiv.org/abs/2307.12101</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucas-vg/pointtinybenchmark">https://github.com/ucas-vg/pointtinybenchmark</a></li>
<li>paper_authors: Di Wu, Pengfei Chen, Xuehui Yu, Guorong Li, Zhenjun Han, Jianbin Jiao</li>
<li>for: 提高object detection的精度和效率，使用低质量 bounding box 监督。</li>
<li>methods: 使用 Spatial Position Self-Distillation (SPSD) 模块和 Spatial Identity Self-Distillation (SISD) 模块， combinig spatial information 和 category information，以提高提档箱的选择过程。</li>
<li>results: 在 MS-COCO 和 VOC  datasets 上，与无isy box 监督实现 state-of-the-art 性能。<details>
<summary>Abstract</summary>
Object detection via inaccurate bounding boxes supervision has boosted a broad interest due to the expensive high-quality annotation data or the occasional inevitability of low annotation quality (\eg tiny objects). The previous works usually utilize multiple instance learning (MIL), which highly depends on category information, to select and refine a low-quality box. Those methods suffer from object drift, group prediction and part domination problems without exploring spatial information. In this paper, we heuristically propose a \textbf{Spatial Self-Distillation based Object Detector (SSD-Det)} to mine spatial information to refine the inaccurate box in a self-distillation fashion. SSD-Det utilizes a Spatial Position Self-Distillation \textbf{(SPSD)} module to exploit spatial information and an interactive structure to combine spatial information and category information, thus constructing a high-quality proposal bag. To further improve the selection procedure, a Spatial Identity Self-Distillation \textbf{(SISD)} module is introduced in SSD-Det to obtain spatial confidence to help select the best proposals. Experiments on MS-COCO and VOC datasets with noisy box annotation verify our method's effectiveness and achieve state-of-the-art performance. The code is available at https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.
</details>
<details>
<summary>摘要</summary>
Object detection via inaccurate bounding boxes supervision has aroused broad interest due to the high cost of high-quality annotation data or the occasional low annotation quality (e.g., tiny objects). Previous works usually rely on multiple instance learning (MIL), which heavily relies on category information, to select and refine a low-quality box. These methods are plagued by object drift, group prediction, and part domination problems without exploring spatial information. In this paper, we propose a Spatial Self-Distillation based Object Detector (SSD-Det) to exploit spatial information to refine the inaccurate box in a self-distillation manner. SSD-Det utilizes a Spatial Position Self-Distillation (SPSD) module to leverage spatial information and an interactive structure to combine spatial information and category information, thus constructing a high-quality proposal bag. To further improve the selection procedure, a Spatial Identity Self-Distillation (SISD) module is introduced in SSD-Det to obtain spatial confidence to help select the best proposals. Experiments on MS-COCO and VOC datasets with noisy box annotation demonstrate the effectiveness of our method and achieve state-of-the-art performance. The code is available at https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.
</details></li>
</ul>
<hr>
<h2 id="Edge-Guided-GANs-with-Multi-Scale-Contrastive-Learning-for-Semantic-Image-Synthesis"><a href="#Edge-Guided-GANs-with-Multi-Scale-Contrastive-Learning-for-Semantic-Image-Synthesis" class="headerlink" title="Edge Guided GANs with Multi-Scale Contrastive Learning for Semantic Image Synthesis"></a>Edge Guided GANs with Multi-Scale Contrastive Learning for Semantic Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12084">http://arxiv.org/abs/2307.12084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ha0tang/ecgan">https://github.com/ha0tang/ecgan</a></li>
<li>paper_authors: Hao Tang, Guolei Sun, Nicu Sebe, Luc Van Gool</li>
<li>for: 提出了一种新的ECGAN方法，用于解决 semantic image synthesis 问题。</li>
<li>methods: 使用 edge 作为中间表示，并通过提案的注意力导向Edge传输模块来引导图像生成。同时，设计了一种选择性高亮类征图像的模块，以保持 semantic 信息。</li>
<li>results: 提出了一种新的对比学习方法，用于让同类像素内容更相似，并在多个输入Semantic layout中捕捉更多的semantic关系。<details>
<summary>Abstract</summary>
We propose a novel ECGAN for the challenging semantic image synthesis task. Although considerable improvements have been achieved by the community in the recent period, the quality of synthesized images is far from satisfactory due to three largely unresolved challenges. 1) The semantic labels do not provide detailed structural information, making it challenging to synthesize local details and structures; 2) The widely adopted CNN operations such as convolution, down-sampling, and normalization usually cause spatial resolution loss and thus cannot fully preserve the original semantic information, leading to semantically inconsistent results (e.g., missing small objects); 3) Existing semantic image synthesis methods focus on modeling 'local' semantic information from a single input semantic layout. However, they ignore 'global' semantic information of multiple input semantic layouts, i.e., semantic cross-relations between pixels across different input layouts. To tackle 1), we propose to use the edge as an intermediate representation which is further adopted to guide image generation via a proposed attention guided edge transfer module. To tackle 2), we design an effective module to selectively highlight class-dependent feature maps according to the original semantic layout to preserve the semantic information. To tackle 3), inspired by current methods in contrastive learning, we propose a novel contrastive learning method, which aims to enforce pixel embeddings belonging to the same semantic class to generate more similar image content than those from different classes. We further propose a novel multi-scale contrastive learning method that aims to push same-class features from different scales closer together being able to capture more semantic relations by explicitly exploring the structures of labeled pixels from multiple input semantic layouts from different scales.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的ECGAN，用于解决 semantic image synthesis 任务中的三大挑战。尽管社区在最近一段时间内已经取得了显著的进步，但是synthesized图像的质量仍然远远不够满意，主要因为以下三个因素：1）semantic labels 不含细节信息，因此难以synthesize  мест Details和结构; 2）通用的CNN操作，如 convolution、下采样和normalization，通常会导致空间分辨率损失，从而无法完全保留原始semantic信息，导致结果不一致（例如缺少小对象）; 3）现有的semantic image synthesis方法都是基于单个输入semantic layout的本地semantic信息模型化。然而，它们忽略了多个输入semantic layout的semantic信息之间的关系，即多个输入semantic layout中的semantic cross-relations。为了解决1），我们提议使用边为 intermediate representation，并将其采用提取模块来导引图像生成。为了解决2），我们设计了一种有效的模块，可以根据原始semantic layout选择性地强调类型相关的特征图。为了解决3），我们提出了一种基于contrastive learning的新方法，该方法 aimsto enforce pixel embeddings belonging to the same semantic class to generate more similar image content than those from different classes。我们还提出了一种多尺度contrastive learning方法，该方法 aimsto push same-class features from different scales closer together，以便捕捉多个输入semantic layout中的semantic关系。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Reconstruction-Based-on-Latent-Diffusion-Model-for-Sparse-Data-Reconstruction"><a href="#Iterative-Reconstruction-Based-on-Latent-Diffusion-Model-for-Sparse-Data-Reconstruction" class="headerlink" title="Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction"></a>Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12070">http://arxiv.org/abs/2307.12070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linchao He, Hongyu Yan, Mengting Luo, Kunming Luo, Wang Wang, Wenchao Du, Hu Chen, Hongyu Yang, Yi Zhang</li>
<li>For: The paper is written for reconstructing computed tomography (CT) images from sparse measurements, which is an ill-posed inverse problem. The paper proposes a new method called Latent Diffusion Iterative Reconstruction (LDIR) to solve this problem.* Methods: LDIR uses a pre-trained Latent Diffusion Model (LDM) as a data prior to extend the Iterative Reconstruction (IR) method. The LDM is used to approximate the prior distribution of the CT images, and the gradient from the data-fidelity term is used to guide the sampling process. This allows LDIR to integrate iterative reconstruction and LDM in an unsupervised manner, making the reconstruction of high-resolution images more efficient.* Results: The paper shows that LDIR outperforms other state-of-the-art unsupervised and even exceeds supervised methods in terms of both quantity and quality on extremely sparse CT data reconstruction tasks. Additionally, LDIR achieves competitive performance on nature image tasks and exhibits significantly faster execution times and lower memory consumption compared to methods with similar network settings.<details>
<summary>Abstract</summary>
Reconstructing Computed tomography (CT) images from sparse measurement is a well-known ill-posed inverse problem. The Iterative Reconstruction (IR) algorithm is a solution to inverse problems. However, recent IR methods require paired data and the approximation of the inverse projection matrix. To address those problems, we present Latent Diffusion Iterative Reconstruction (LDIR), a pioneering zero-shot method that extends IR with a pre-trained Latent Diffusion Model (LDM) as a accurate and efficient data prior. By approximating the prior distribution with an unconditional latent diffusion model, LDIR is the first method to successfully integrate iterative reconstruction and LDM in an unsupervised manner. LDIR makes the reconstruction of high-resolution images more efficient. Moreover, LDIR utilizes the gradient from the data-fidelity term to guide the sampling process of the LDM, therefore, LDIR does not need the approximation of the inverse projection matrix and can solve various CT reconstruction tasks with a single model. Additionally, for enhancing the sample consistency of the reconstruction, we introduce a novel approach that uses historical gradient information to guide the gradient. Our experiments on extremely sparse CT data reconstruction tasks show that LDIR outperforms other state-of-the-art unsupervised and even exceeds supervised methods, establishing it as a leading technique in terms of both quantity and quality. Furthermore, LDIR also achieves competitive performance on nature image tasks. It is worth noting that LDIR also exhibits significantly faster execution times and lower memory consumption compared to methods with similar network settings. Our code will be publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将计算Tomography（CT）图像重建问题转化为简单的无监测问题。iterative Reconstruction（IR）算法是解决无监测问题的一种方法。然而，最近的IR方法需要匹配数据和近似投影矩阵的approximation。为了解决这些问题，我们介绍Latent Diffusion Iterative Reconstruction（LDIR），一种革新的零shot方法，通过将iterative Reconstruction和Latent Diffusion Model（LDM）集成在一起，以获得高效和准确的数据先天。LDIR通过近似 latent diffusion模型来表示先天分布，因此不需要近似投影矩阵的approximation，可以解决多种CT重建任务。此外，我们还引入了一种新的方法，使用历史梯度信息来导引抽象过程。我们的实验表明，LDIR在极端稀畴CT数据重建任务中表现出色，超过了其他无监测和监测方法，成为当前领导技术。此外，LDIR还在自然图像任务中表现竞争力强。值得注意的是，LDIR也显示出了相对较快的执行时间和较低的内存占用量，相比于与相同网络设置的方法。我们将代码公开。
</details></li>
</ul>
<hr>
<h2 id="Replay-Multi-modal-Multi-view-Acted-Videos-for-Casual-Holography"><a href="#Replay-Multi-modal-Multi-view-Acted-Videos-for-Casual-Holography" class="headerlink" title="Replay: Multi-modal Multi-view Acted Videos for Casual Holography"></a>Replay: Multi-modal Multi-view Acted Videos for Casual Holography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12067">http://arxiv.org/abs/2307.12067</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/replay_dataset">https://github.com/facebookresearch/replay_dataset</a></li>
<li>paper_authors: Roman Shapovalov, Yanir Kleiman, Ignacio Rocco, David Novotny, Andrea Vedaldi, Changan Chen, Filippos Kokkinos, Ben Graham, Natalia Neverova</li>
<li>for: 这个论文主要用于提供一个多视角多Modal视频人类社交交互的收集，可以用于多种应用，如新视角合成、3D重建、人体和脸部分析等。</li>
<li>methods: 这个论文使用了多个高品质摄像头和仪器来捕捉和记录人类社交交互的视频，并对视频进行了高精度时间戳和相机pose的标注。</li>
<li>results: 这个论文提供了一个包含超过4000分钟视频和700万个高分辨率帧的大规模数据集，并提供了一个基准测试集用于训练和评估新视角合成方法。<details>
<summary>Abstract</summary>
We introduce Replay, a collection of multi-view, multi-modal videos of humans interacting socially. Each scene is filmed in high production quality, from different viewpoints with several static cameras, as well as wearable action cameras, and recorded with a large array of microphones at different positions in the room. Overall, the dataset contains over 4000 minutes of footage and over 7 million timestamped high-resolution frames annotated with camera poses and partially with foreground masks. The Replay dataset has many potential applications, such as novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and training generative models. We provide a benchmark for training and evaluating novel-view synthesis, with two scenarios of different difficulty. Finally, we evaluate several baseline state-of-the-art methods on the new benchmark.
</details>
<details>
<summary>摘要</summary>
我们介绍Replay数据集，这是一个多视点多Modal人类社交互动的集合。每场场景由高品质摄像机拍摄，包括固定摄像机和穿戴式动作摄像机，以及不同位置的听音器。总的来说，数据集包含超过4000分钟的视频和700万个时间戳的高分辨率帧，并有部分帧附有相机姿态和部分人体或脸部掩码。Replay数据集有许多应用 potential，如新视图合成、3D重建、新视图声音合成、人体和脸部分析，以及训练生成模型。我们提供了一个对novel-view synthesis进行训练和评估的标准准。最后，我们评估了一些基eline状态的先进方法在新的标准上。
</details></li>
</ul>
<hr>
<h2 id="Discovering-Spatio-Temporal-Rationales-for-Video-Question-Answering"><a href="#Discovering-Spatio-Temporal-Rationales-for-Video-Question-Answering" class="headerlink" title="Discovering Spatio-Temporal Rationales for Video Question Answering"></a>Discovering Spatio-Temporal Rationales for Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12058">http://arxiv.org/abs/2307.12058</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yicong Li, Junbin Xiao, Chun Feng, Xiang Wang, Tat-Seng Chua</li>
<li>for: 解决复杂的视频问答（VideoQA）问题，其中视频具有多个物体和事件，发生在不同的时间点。</li>
<li>methods: 提出了一种Spatio-Temporal Rationalization（STR），这是一种可 diferenciable 选择模块，可以适应тив地从视频内容中收集问题关键的时间点和空间对象。此外，还提出了一种基于STR的Transformer-style neural network架构，名为TranSTR，它还强调了一种新的答案交互机制来协调STR。</li>
<li>results: 实验结果表明，TranSTR在四个dataset上达到了新的State-of-the-art（SoTA）水平，特别是在NExT-QA和Causal-VidQA上，它超过了之前的SoTA by 5.8%和6.8%。此外，我们还进行了广泛的研究来证明STR的重要性以及提出的答案交互机制的重要性。<details>
<summary>Abstract</summary>
This paper strives to solve complex video question answering (VideoQA) which features long video containing multiple objects and events at different time. To tackle the challenge, we highlight the importance of identifying question-critical temporal moments and spatial objects from the vast amount of video content. Towards this, we propose a Spatio-Temporal Rationalization (STR), a differentiable selection module that adaptively collects question-critical moments and objects using cross-modal interaction. The discovered video moments and objects are then served as grounded rationales to support answer reasoning. Based on STR, we further propose TranSTR, a Transformer-style neural network architecture that takes STR as the core and additionally underscores a novel answer interaction mechanism to coordinate STR for answer decoding. Experiments on four datasets show that TranSTR achieves new state-of-the-art (SoTA). Especially, on NExT-QA and Causal-VidQA which feature complex VideoQA, it significantly surpasses the previous SoTA by 5.8\% and 6.8\%, respectively. We then conduct extensive studies to verify the importance of STR as well as the proposed answer interaction mechanism. With the success of TranSTR and our comprehensive analysis, we hope this work can spark more future efforts in complex VideoQA. Code will be released at https://github.com/yl3800/TranSTR.
</details>
<details>
<summary>摘要</summary>
这篇论文目标解决复杂的视频问答（VideoQA）问题，该问题具有长视频内容中的多个物体和事件，并且发生在不同的时间点。为了解决这个挑战，我们强调了问题关键的时间刻和空间对象的标识，并提出了一种空间时间合理化（STR）模块，该模块通过交叉模式互动来适应性地收集问题关键的时间刻和空间对象。得到的视频刻和对象将被用作问题理解的基础理据。基于STR，我们还提出了TransSTR，一种基于Transformer的神经网络架构，该架构将STR作为核心，并强调了一种新的答案互动机制以协调STR进行答案解码。实验结果表明，TransSTR在四个数据集上达到了新的状态时刻（SoTA），尤其是在NExT-QA和Causal-VidQA这两个复杂的VideoQA数据集上，与之前的SoTA相比，它提高了5.8%和6.8%。我们还进行了广泛的研究来证明STR的重要性以及我们提议的答案互动机制的重要性。通过TransSTR和我们的全面分析，我们希望这项工作可以激发更多的未来的VideoQA研究。代码将在GitHub上发布。
</details></li>
</ul>
<hr>
<h2 id="Patch-Wise-Point-Cloud-Generation-A-Divide-and-Conquer-Approach"><a href="#Patch-Wise-Point-Cloud-Generation-A-Divide-and-Conquer-Approach" class="headerlink" title="Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach"></a>Patch-Wise Point Cloud Generation: A Divide-and-Conquer Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12049">http://arxiv.org/abs/2307.12049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wenc13/patchgeneration">https://github.com/wenc13/patchgeneration</a></li>
<li>paper_authors: Cheng Wen, Baosheng Yu, Rao Fu, Dacheng Tao</li>
<li>for: 本研究旨在生成高精度点云，用于自动驾驶和机器人等应用。</li>
<li>methods: 提出了一种新的点云生成框架，使用分割和聚合的方法，将整个生成过程分解成多个小区域生成任务。每个小区域生成器都基于学习的先验，用于捕捉点云的几何结构信息。还引入了点和小区域之间的交互transformer，以便在不同尺度上进行交互。</li>
<li>results: 实验结果表明，提出的小区域生成方法可以准确地生成高精度点云，并且在ShapeNet数据集上表现出色，与现有的状态对点云生成方法进行比较。<details>
<summary>Abstract</summary>
A generative model for high-fidelity point clouds is of great importance in synthesizing 3d environments for applications such as autonomous driving and robotics. Despite the recent success of deep generative models for 2d images, it is non-trivial to generate 3d point clouds without a comprehensive understanding of both local and global geometric structures. In this paper, we devise a new 3d point cloud generation framework using a divide-and-conquer approach, where the whole generation process can be divided into a set of patch-wise generation tasks. Specifically, all patch generators are based on learnable priors, which aim to capture the information of geometry primitives. We introduce point- and patch-wise transformers to enable the interactions between points and patches. Therefore, the proposed divide-and-conquer approach contributes to a new understanding of point cloud generation from the geometry constitution of 3d shapes. Experimental results on a variety of object categories from the most popular point cloud dataset, ShapeNet, show the effectiveness of the proposed patch-wise point cloud generation, where it clearly outperforms recent state-of-the-art methods for high-fidelity point cloud generation.
</details>
<details>
<summary>摘要</summary>
一个高级别点云生成模型对于 sintesizing 3D 环境而言是非常重要的，特别是在自动驾驶和机器人应用中。虽然最近的深度生成模型在 2D 图像方面已经取得了成功，但是生成 3D 点云则不是一件容易的事情，需要全面了解点云的本地和全局 геометрической结构。在这篇论文中，我们提出了一种新的点云生成框架，使用分治方法，整个生成过程可以分解为一系列的小区域生成任务。具体来说，所有的小区域生成器都基于学习的先验，旨在捕捉点云中的几何基本元素。我们引入了点云和小区域之间的交互，使得我们的分治方法在点云生成中做出了新的贡献，帮助我们更好地理解点云生成的几何结构。我们的实验结果表明，在ShapeNet 上的多种物体类别上，我们的裂解方法可以高效地生成高级别的点云，并且明显超过了最近的状态 искусственный风格方法。
</details></li>
</ul>
<hr>
<h2 id="FSDiffReg-Feature-wise-and-Score-wise-Diffusion-guided-Unsupervised-Deformable-Image-Registration-for-Cardiac-Images"><a href="#FSDiffReg-Feature-wise-and-Score-wise-Diffusion-guided-Unsupervised-Deformable-Image-Registration-for-Cardiac-Images" class="headerlink" title="FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images"></a>FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12035">http://arxiv.org/abs/2307.12035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/fsdiffreg">https://github.com/xmed-lab/fsdiffreg</a></li>
<li>paper_authors: Yi Qin, Xiaomeng Li</li>
<li>for: 这篇论文主要针对医疗影像注册 зада задачу，尤其是实现高品质的扭转场，同时保持扭转学径的可靠性。</li>
<li>methods: 本论文提出了两个模组，分别是对于Semantic Diffusion Model的多对多映射和Score-wise Diffusion-Guided Module，以利用扩散模型的Semantic Feature空间来帮助注册任务。</li>
<li>results: 实验结果显示，本论文的模型能够对3D医疗心脏影像注册任务提供精确的扭转场，并具有保持扭转学径的可靠性。<details>
<summary>Abstract</summary>
Unsupervised deformable image registration is one of the challenging tasks in medical imaging. Obtaining a high-quality deformation field while preserving deformation topology remains demanding amid a series of deep-learning-based solutions. Meanwhile, the diffusion model's latent feature space shows potential in modeling the deformation semantics. To fully exploit the diffusion model's ability to guide the registration task, we present two modules: Feature-wise Diffusion-Guided Module (FDG) and Score-wise Diffusion-Guided Module (SDG). Specifically, FDG uses the diffusion model's multi-scale semantic features to guide the generation of the deformation field. SDG uses the diffusion score to guide the optimization process for preserving deformation topology with barely any additional computation. Experiment results on the 3D medical cardiac image registration task validate our model's ability to provide refined deformation fields with preserved topology effectively. Code is available at: https://github.com/xmed-lab/FSDiffReg.git.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>医疗影像注册不监督式扭变是一项具有挑战性的任务。在一系列深度学习基于解决方案中，获得高质量扭变场并保持扭变 topology remains demanding. 同时，扩散模型的隐藏特征空间表现出了模型扭变 semantics 的潜力。为了充分利用扩散模型对注册任务的导航，我们提出了两个模块：特征 wise Diffusion-Guided Module (FDG) 和 Score-wise Diffusion-Guided Module (SDG)。具体来说，FDG 使用扩散模型的多尺度semantic特征来导航生成扭变场。SDG 使用扩散分数来导航优化过程，以保持扭变 topology with barely any additional computation。实验结果表明，我们的模型能够提供高精度的扭变场，并具有保持扭变 topology 的能力。代码可以在：https://github.com/xmed-lab/FSDiffReg.git 中找到。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-and-Semi-Supervised-Polyp-Segmentation-using-Synthetic-Data"><a href="#Self-Supervised-and-Semi-Supervised-Polyp-Segmentation-using-Synthetic-Data" class="headerlink" title="Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data"></a>Self-Supervised and Semi-Supervised Polyp Segmentation using Synthetic Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12033">http://arxiv.org/abs/2307.12033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enric Moreu, Eric Arazo, Kevin McGuinness, Noel E. O’Connor</li>
<li>for: 早期检测Rectal Polyps是肝肠癌预防中非常重要的一步，手术镜检查是 manually carried out to examine the entirety of the patient’s colon的主要方法。</li>
<li>methods: 我们使用计算机视觉技术来助力 профессионаls在诊断阶段，并利用Synthetic Data和自动生成的图像来增加数据量，以便更好地利用无标注数据。</li>
<li>results: 我们的模型Pl-CUT-Seg在标准的Polyp Segmentation benchmark上达到了自动标注和 semi-supervised setup中的state-of-the-art Results，并且我们还提出了PL-CUT-Seg+，一种通过targeted regularization来Address the domain gap between real and synthetic images的改进版本。<details>
<summary>Abstract</summary>
Early detection of colorectal polyps is of utmost importance for their treatment and for colorectal cancer prevention. Computer vision techniques have the potential to aid professionals in the diagnosis stage, where colonoscopies are manually carried out to examine the entirety of the patient's colon. The main challenge in medical imaging is the lack of data, and a further challenge specific to polyp segmentation approaches is the difficulty of manually labeling the available data: the annotation process for segmentation tasks is very time-consuming. While most recent approaches address the data availability challenge with sophisticated techniques to better exploit the available labeled data, few of them explore the self-supervised or semi-supervised paradigm, where the amount of labeling required is greatly reduced. To address both challenges, we leverage synthetic data and propose an end-to-end model for polyp segmentation that integrates real and synthetic data to artificially increase the size of the datasets and aid the training when unlabeled samples are available. Concretely, our model, Pl-CUT-Seg, transforms synthetic images with an image-to-image translation module and combines the resulting images with real images to train a segmentation model, where we use model predictions as pseudo-labels to better leverage unlabeled samples. Additionally, we propose PL-CUT-Seg+, an improved version of the model that incorporates targeted regularization to address the domain gap between real and synthetic images. The models are evaluated on standard benchmarks for polyp segmentation and reach state-of-the-art results in the self- and semi-supervised setups.
</details>
<details>
<summary>摘要</summary>
早期检测肠RectalPolyp非常重要，以采取治疗和预防肠RectalCancer。计算机视觉技术有可能帮助专业人员在诊断阶段进行手动检查患者的整个肠肠Rectal。主要挑战在医疗影像领域是数据不足，而特定于肠Polyp分割方法的另一个挑战是手动标注可用数据的困难。大多数最新的方法解决数据不足的挑战，使用了复杂的技术来更好地利用可用的标注数据。然而，只有几个方法探讨了不supervised或semi-supervised模式，其中可以大幅减少标注数量。为了解决这两个挑战，我们利用生成的数据和提议一个综合模型，即Pl-CUT-Seg，将生成的图像与实际图像结合以训练一个分割模型。我们使用模型预测结果作为pseudo-标注，以更好地利用无标注样本。此外，我们还提出了PL-CUT-Seg+，一个改进的模型，其中包括targeted regularization，以解决实际和生成图像之间的领域差异。模型在标准的肠Polyp分割测试benchmark上进行评估，并在自supervised和semi-supervised setup中达到了状态的末点结果。
</details></li>
</ul>
<hr>
<h2 id="Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space"><a href="#Flight-Contrail-Segmentation-via-Augmented-Transfer-Learning-with-Novel-SR-Loss-Function-in-Hough-Space" class="headerlink" title="Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space"></a>Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12032">http://arxiv.org/abs/2307.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junzis/contrail-net">https://github.com/junzis/contrail-net</a></li>
<li>paper_authors: Junzi Sun, Esther Roosenbrand</li>
<li>for: 这篇论文旨在提出一种基于增强传输学习的新模型，用于检测飞机烟囱从卫星图像中。</li>
<li>methods: 该模型使用了增强传输学习技术，以及一种新的损失函数SR损失，以提高烟囱线检测精度。</li>
<li>results: 研究发现，该模型能够准确地检测飞机烟囱，并且只需要 minimal data。这些成果开创了机器学习基于烟囱检测的新途径，并可以解决航空研究中烟囱检测模型的缺乏大量手动标注数据问题。<details>
<summary>Abstract</summary>
Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detection models.
</details>
<details>
<summary>摘要</summary>
飞行交通对环境造成重大挑战，特别是飞机烟尘对气候变化的贡献，由于其可能对全球暖化产生影响。传统的计算机视觉技术在不同的图像条件下有限制，机器学习方法使用典型的卷积神经网络受到手动标注烟尘数据的罕见和烟尘特化学习过程的限制。在这篇论文中，我们介绍了一种创新的模型，基于增强转移学习，可以准确地检测烟尘。我们还提出了一种新的损失函数，SR损失，它在图像空间转换到抽象空间，从而改善烟尘线检测。我们的研究开创了机器学习基于烟尘检测的新途径，解决了航空研究中缺乏大量手动标注数据的问题，并有效地提高烟尘检测模型。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement"><a href="#On-the-Effectiveness-of-Spectral-Discriminators-for-Perceptual-Quality-Improvement" class="headerlink" title="On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement"></a>On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12027">http://arxiv.org/abs/2307.12027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luciennnnnnn/dualformer">https://github.com/luciennnnnnn/dualformer</a></li>
<li>paper_authors: Xin Luo, Yunan Zhu, Shunxin Xu, Dong Liu</li>
<li>for: 本研究旨在解释spectral discriminator在生成模型中的效果，特别是在图像超解像(GAN-based SR)中。</li>
<li>methods: 作者使用了spectral discriminator和ordinary discriminator进行比较，并提出了使用这两种权值 simultaneously。另外，作者还提出了一种基于Transformer的方法来协调spectral discriminator。</li>
<li>results: 作者发现spectral discriminator在高频范围内的性能更高，而ordinary discriminator在低频范围内的性能更高。这些结果表明，使用spectral discriminator和ordinary discriminator simultaneously可以提高SR图像的质量。此外，作者还发现，使用这种方法可以更好地评估图像的品质。<details>
<summary>Abstract</summary>
Several recent studies advocate the use of spectral discriminators, which evaluate the Fourier spectra of images for generative modeling. However, the effectiveness of the spectral discriminators is not well interpreted yet. We tackle this issue by examining the spectral discriminators in the context of perceptual image super-resolution (i.e., GAN-based SR), as SR image quality is susceptible to spectral changes. Our analyses reveal that the spectral discriminator indeed performs better than the ordinary (a.k.a. spatial) discriminator in identifying the differences in the high-frequency range; however, the spatial discriminator holds an advantage in the low-frequency range. Thus, we suggest that the spectral and spatial discriminators shall be used simultaneously. Moreover, we improve the spectral discriminators by first calculating the patch-wise Fourier spectrum and then aggregating the spectra by Transformer. We verify the effectiveness of the proposed method twofold. On the one hand, thanks to the additional spectral discriminator, our obtained SR images have their spectra better aligned to those of the real images, which leads to a better PD tradeoff. On the other hand, our ensembled discriminator predicts the perceptual quality more accurately, as evidenced in the no-reference image quality assessment task.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:一些最近的研究提出了使用spectral discriminator，它们评估图像的快 Fourier spectrum进行生成模型。然而，spectral discriminator的效果还不够理解。我们在perceptual image super-resolution（i.e., GAN-based SR）中研究spectral discriminator，因为SR图像质量对快 Fourier spectrum的变化敏感。我们的分析发现，spectral discriminator在高频范围内比普通的（即空间）discriminator更好地识别图像的差异，但是空间discriminator在低频范围内有优势。因此，我们建议同时使用spectral和空间discriminator。此外，我们改进了spectral discriminator，首先计算每个 patch的快 Fourier spectrum，然后使用Transformer聚合spectrum。我们验证了我们的提议方法的有效性通过两种方式：一是我们的SR图像的spectrum更加靠近真实图像的spectrum，导致PD质量更好的权衡；二是我们的ensembled discriminator在无参图像质量评估任务中预测了更加准确的Perceptual质量。
</details></li>
</ul>
<hr>
<h2 id="Simple-parameter-free-self-attention-approximation"><a href="#Simple-parameter-free-self-attention-approximation" class="headerlink" title="Simple parameter-free self-attention approximation"></a>Simple parameter-free self-attention approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12018">http://arxiv.org/abs/2307.12018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exploita123/charmedforfree">https://github.com/exploita123/charmedforfree</a></li>
<li>paper_authors: Yuwen Zhai, Jing Hao, Liang Gao, Xinyu Li, Yiping Gao, Shumin Han</li>
<li>for: 用于提高ViT的效率，适用于边缘设备。</li>
<li>methods: 使用自注意力和卷积的混合模型，以及一种无需训练参数的自注意力 aproximation方法（SPSA），用于捕捉全局空间特征。</li>
<li>results: 通过对图像分类和对象检测任务进行广泛的实验，证明了SPSA与卷积的组合的效果。<details>
<summary>Abstract</summary>
The hybrid model of self-attention and convolution is one of the methods to lighten ViT. The quadratic computational complexity of self-attention with respect to token length limits the efficiency of ViT on edge devices. We propose a self-attention approximation without training parameters, called SPSA, which captures global spatial features with linear complexity. To verify the effectiveness of SPSA combined with convolution, we conduct extensive experiments on image classification and object detection tasks.
</details>
<details>
<summary>摘要</summary>
“半自动化模型，即自注意和卷积的混合模型，是用于轻量化ViT的一种方法。自注意的二次计算复杂度与字符串长度成正比，限制了ViT在边缘设备上的效率。我们提出了一种不需要训练参数的自注意简化方法，称为SPSA，它可以 capture global spatial features with linear complexity。为验证SPSA与卷积结合的效果，我们进行了广泛的图像分类和对象检测任务的实验。”Note that Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details"><a href="#NLCUnet-Single-Image-Super-Resolution-Network-with-Hairline-Details" class="headerlink" title="NLCUnet: Single-Image Super-Resolution Network with Hairline Details"></a>NLCUnet: Single-Image Super-Resolution Network with Hairline Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12014">http://arxiv.org/abs/2307.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancong Feng, Yuan-Gen Wang, Fengchuang Xing</li>
<li>for: 提高单张超解像图像质量</li>
<li>methods: 提出了一种基于非本地注意力机制的单张超解像网络（NLCUnet），包括三种核心设计。具体来说，首先引入了一种基于全图区域学习的非本地注意力机制，以恢复本地碎片。然后，我们发现现有工作中的卷积权重学习是无需的，因此我们创建了一种新的网络架构，通过对每个通道进行深度卷积并添加通道注意力，从而实现性能提升。最后，我们提议在中心2K图像中随机选择64×64区域，以便尽可能包含 semantic信息。</li>
<li>results: 经过多次实验 validate 的DF2K数据集上，我们的NLCUnet表现比state-of-the-art更高，按PSNR和SSIM指标评估。同时，它也可以呈现出更加有利的毛细处理细节。<details>
<summary>Abstract</summary>
Pursuing the precise details of super-resolution images is challenging for single-image super-resolution tasks. This paper presents a single-image super-resolution network with hairline details (termed NLCUnet), including three core designs. Specifically, a non-local attention mechanism is first introduced to restore local pieces by learning from the whole image region. Then, we find that the blur kernel trained by the existing work is unnecessary. Based on this finding, we create a new network architecture by integrating depth-wise convolution with channel attention without the blur kernel estimation, resulting in a performance improvement instead. Finally, to make the cropped region contain as much semantic information as possible, we propose a random 64$\times$64 crop inside the central 512$\times$512 crop instead of a direct random crop inside the whole image of 2K size. Numerous experiments conducted on the benchmark DF2K dataset demonstrate that our NLCUnet performs better than the state-of-the-art in terms of the PSNR and SSIM metrics and yields visually favorable hairline details.
</details>
<details>
<summary>摘要</summary>
追求超高清照片细节精度是单图超解像任务中的挑战。这篇论文提出了一种单图超解像网络（NLCUnet），包括三个核心设计。具体来说，我们首先引入非本地注意力机制，以全图区域学习恢复本地块。然后，我们发现现有工作中的模糊核心训练是不必要的，因此我们创建了一种不包含模糊核心的网络架构，通过混合深度卷积和通道注意力，从而实现性能提升。最后，我们提议在中心256x256区域内随机选择64x64区域，而不是直接随机选择2K图像中的整个区域，以便尽可能地包含中心区域的semantic信息。经过多次实验，我们发现NLCUnet在DF2K数据集上的PSNR和SSIM指标上表现比前者更好，并且视觉效果更佳。
</details></li>
</ul>
<hr>
<h2 id="SCOL-Supervised-Contrastive-Ordinal-Loss-for-Abdominal-Aortic-Calcification-Scoring-on-Vertebral-Fracture-Assessment-Scans"><a href="#SCOL-Supervised-Contrastive-Ordinal-Loss-for-Abdominal-Aortic-Calcification-Scoring-on-Vertebral-Fracture-Assessment-Scans" class="headerlink" title="SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans"></a>SCOL: Supervised Contrastive Ordinal Loss for Abdominal Aortic Calcification Scoring on Vertebral Fracture Assessment Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12006">http://arxiv.org/abs/2307.12006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/afsahs/supervised-contrastive-ordinal-loss">https://github.com/afsahs/supervised-contrastive-ordinal-loss</a></li>
<li>paper_authors: Afsah Saleem, Zaid Ilyas, David Suter, Ghulam Mubashar Hassan, Siobhan Reid, John T. Schousboe, Richard Prince, William D. Leslie, Joshua R. Lewis, Syed Zulqarnain Gilani<br>for: 这个研究的目的是开发一种自动评估胸部动脉calcification的方法，以检测 asymptomatic atherosclerotic cardiovascular diseases (ASCVDs) 的风险。methods: 这个研究使用了一种新的Supervised Contrastive Ordinal Loss (SCOL) 函数，并开发了一种 Dual-encoder Contrastive Ordinal Learning (DCOL) 框架，以利用 AAC  regression 标签中的顺序信息。results: 研究结果表明，该方法可以提高 AAC 的 интер-класс分化度和内部一致性，并且可以准确地预测高风险 AAC 类别。<details>
<summary>Abstract</summary>
Abdominal Aortic Calcification (AAC) is a known marker of asymptomatic Atherosclerotic Cardiovascular Diseases (ASCVDs). AAC can be observed on Vertebral Fracture Assessment (VFA) scans acquired using Dual-Energy X-ray Absorptiometry (DXA) machines. Thus, the automatic quantification of AAC on VFA DXA scans may be used to screen for CVD risks, allowing early interventions. In this research, we formulate the quantification of AAC as an ordinal regression problem. We propose a novel Supervised Contrastive Ordinal Loss (SCOL) by incorporating a label-dependent distance metric with existing supervised contrastive loss to leverage the ordinal information inherent in discrete AAC regression labels. We develop a Dual-encoder Contrastive Ordinal Learning (DCOL) framework that learns the contrastive ordinal representation at global and local levels to improve the feature separability and class diversity in latent space among the AAC-24 genera. We evaluate the performance of the proposed framework using two clinical VFA DXA scan datasets and compare our work with state-of-the-art methods. Furthermore, for predicted AAC scores, we provide a clinical analysis to predict the future risk of a Major Acute Cardiovascular Event (MACE). Our results demonstrate that this learning enhances inter-class separability and strengthens intra-class consistency, which results in predicting the high-risk AAC classes with high sensitivity and high accuracy.
</details>
<details>
<summary>摘要</summary>
《腹部动脉钙化（AAC）是无症状栓塞动脉疾病（ASCVD）的知名标志。AAC可以在骨折评估（VFA）扫描机上观察到，因此自动量化AAC on VFA DXA扫描机可能用来检测心血管风险，允许早期干预。在这个研究中，我们将AAC量化作为一个Ordinal regression问题。我们提出了一种名为Supervised Contrastive Ordinal Loss（SCOL）的新的损失函数，它通过融合现有的Supervised contrastive loss和标签висимый距离度量来利用AAC regression标签中的排序信息。我们开发了一个名为Dual-encoder Contrastive Ordinal Learning（DCOL）的框架，它可以在全球和本地两个水平上学习对照的排序ORDINAL表示，以提高在隐藏空间中的特征分类和类别多样性。我们使用两个来自临床的VFA DXA扫描数据集进行评估，并与现有的方法进行比较。此外，我们还对预测的AAC分数进行临床分析，以预测未来的主要急性心血管事件（MACE）的风险。我们的结果显示，这种学习可以提高间隔分类的标准差和内部一致性，从而预测高风险AAC类别的敏感性和准确性。
</details></li>
</ul>
<hr>
<h2 id="COLosSAL-A-Benchmark-for-Cold-start-Active-Learning-for-3D-Medical-Image-Segmentation"><a href="#COLosSAL-A-Benchmark-for-Cold-start-Active-Learning-for-3D-Medical-Image-Segmentation" class="headerlink" title="COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation"></a>COLosSAL: A Benchmark for Cold-start Active Learning for 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12004">http://arxiv.org/abs/2307.12004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medicl-vu/colossal">https://github.com/medicl-vu/colossal</a></li>
<li>paper_authors: Han Liu, Hao Li, Xing Yao, Yubo Fan, Dewei Hu, Benoit Dawant, Vishwesh Nath, Zhoubing Xu, Ipek Oguz</li>
<li>for: 本研究旨在解决医疗图像分割任务中数据标注瓶颈问题，提出了一个叫做COLosSAL的数据集和评估框架，用于评估不同的启动式活动学习策略。</li>
<li>methods: 本研究使用了六种不同的启动式活动学习策略，并对五个3D医疗图像分割任务进行了评估。</li>
<li>results: 研究发现，启动式活动学习仍然是3D分割任务中未解决的问题，但是一些重要的趋势有被观察到。<details>
<summary>Abstract</summary>
Medical image segmentation is a critical task in medical image analysis. In recent years, deep learning based approaches have shown exceptional performance when trained on a fully-annotated dataset. However, data annotation is often a significant bottleneck, especially for 3D medical images. Active learning (AL) is a promising solution for efficient annotation but requires an initial set of labeled samples to start active selection. When the entire data pool is unlabeled, how do we select the samples to annotate as our initial set? This is also known as the cold-start AL, which permits only one chance to request annotations from experts without access to previously annotated data. Cold-start AL is highly relevant in many practical scenarios but has been under-explored, especially for 3D medical segmentation tasks requiring substantial annotation effort. In this paper, we present a benchmark named COLosSAL by evaluating six cold-start AL strategies on five 3D medical image segmentation tasks from the public Medical Segmentation Decathlon collection. We perform a thorough performance analysis and explore important open questions for cold-start AL, such as the impact of budget on different strategies. Our results show that cold-start AL is still an unsolved problem for 3D segmentation tasks but some important trends have been observed. The code repository, data partitions, and baseline results for the complete benchmark are publicly available at https://github.com/MedICL-VU/COLosSAL.
</details>
<details>
<summary>摘要</summary>
医学像素化是医学图像分析中的关键任务。在过去几年，基于深度学习的方法在完全标注的数据集上训练后表现出色。然而，数据标注却是一个重要的瓶颈，特别是 для 3D 医学图像。活动学习（AL）是一种可能的解决方案，但它需要一个初始化标注的样本集来开始活动选择。当整个数据池都是未标注的时候，如何选择要标注的样本呢？这也被称为冷启动 AL，它允许在专家无法访问前一次已经标注的数据时，仅请求一次标注。冷启动 AL 在许多实际场景中是非常有价值的，特别是 для 3D 医学分割任务，需要很大的标注努力。在这篇论文中，我们提出了一个名为 COLosSAL 的标准套件，通过评估六种冷启动 AL 策略在五个 3D 医学图像分割任务上进行了全面性的性能分析。我们进行了详细的性能分析，并探讨了冷启动 AL 中重要的开放问题，如预算对不同策略的影响。我们的结果表明，冷启动 AL 仍然是未解决的问题，但我们在不同任务上观察到了一些重要的趋势。我们在 GitHub 上公开了代码库、数据分区和基线结果，欢迎您在 <https://github.com/MedICL-VU/COLosSAL> 上查看。
</details></li>
</ul>
<hr>
<h2 id="A-Stronger-Stitching-Algorithm-for-Fisheye-Images-based-on-Deblurring-and-Registration"><a href="#A-Stronger-Stitching-Algorithm-for-Fisheye-Images-based-on-Deblurring-and-Registration" class="headerlink" title="A Stronger Stitching Algorithm for Fisheye Images based on Deblurring and Registration"></a>A Stronger Stitching Algorithm for Fisheye Images based on Deblurring and Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11997">http://arxiv.org/abs/2307.11997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Hao, Jingming Xie, Jinyuan Zhang, Moyun Liu</li>
<li>for: 解决 fisheye 图像中的 геометрической扭曲问题，提高 fisheye 图像的融合质量。</li>
<li>methods:  combines 传统图像处理技术和深度学习，提出了 Attention-based Nonlinear Activation Free Network (ANAFNet) 和 ORB-FREAK-GMS (OFG) 两种算法。</li>
<li>results: 实验结果表明，通过我们的方法可以获得高质量的排 compose 图像。<details>
<summary>Abstract</summary>
Fisheye lens, which is suitable for panoramic imaging, has the prominent advantage of a large field of view and low cost. However, the fisheye image has a severe geometric distortion which may interfere with the stage of image registration and stitching. Aiming to resolve this drawback, we devise a stronger stitching algorithm for fisheye images by combining the traditional image processing method with deep learning. In the stage of fisheye image correction, we propose the Attention-based Nonlinear Activation Free Network (ANAFNet) to deblur fisheye images corrected by Zhang calibration method. Specifically, ANAFNet adopts the classical single-stage U-shaped architecture based on convolutional neural networks with soft-attention technique and it can restore a sharp image from a blurred image effectively. In the part of image registration, we propose the ORB-FREAK-GMS (OFG), a comprehensive image matching algorithm, to improve the accuracy of image registration. Experimental results demonstrate that panoramic images of superior quality stitching by fisheye images can be obtained through our method.
</details>
<details>
<summary>摘要</summary>
鱼眼镜，适用于全景拍摄，具有大视野和低成本的优点。然而，鱼眼图像受到严重的几何扭曲的影响，可能会干扰图像registraton和组合stage。为了解决这个缺点，我们提出了一种基于传统图像处理技术和深度学习的更强大的缝合算法 для鱼眼图像。在鱼眼图像修正阶段，我们提出了Attention-based Nonlinear Activation Free Network（ANAFNet），用于deburring鱼眼图像，并可以有效地还原锐化图像。在图像registraton阶段，我们提出了ORB-FREAK-GMS（OFG），一种全面的图像匹配算法，以提高图像registraton的准确性。实验结果表明，通过我们的方法可以得到高质量的全景图像组合。
</details></li>
</ul>
<hr>
<h2 id="Morphology-inspired-Unsupervised-Gland-Segmentation-via-Selective-Semantic-Grouping"><a href="#Morphology-inspired-Unsupervised-Gland-Segmentation-via-Selective-Semantic-Grouping" class="headerlink" title="Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping"></a>Morphology-inspired Unsupervised Gland Segmentation via Selective Semantic Grouping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11989">http://arxiv.org/abs/2307.11989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/mssg">https://github.com/xmed-lab/mssg</a></li>
<li>paper_authors: Qixiang Zhang, Yi Li, Cheng Xue, Xiaomeng Li</li>
<li>for: 这个论文的目的是开发一种不需要人工标注的深度学习方法来自动分类腺体，以推动自动癌症诊断和预后评估。</li>
<li>methods: 我们提出了一种新的 Morphology-inspired 方法，将实验资讯引入为额外知识，以指导分类过程。我们首先利用这个实验资讯选择出腺体子区域的提案，然后使用具有形式知识的 Semantic Grouping 模组集成全局资讯。</li>
<li>results: 我们在 GlaS dataset 和 CRAG dataset 上进行实验，结果显示我们的方法在 mIOU 上超过第二名的对手10.56%。<details>
<summary>Abstract</summary>
Designing deep learning algorithms for gland segmentation is crucial for automatic cancer diagnosis and prognosis, yet the expensive annotation cost hinders the development and application of this technology. In this paper, we make a first attempt to explore a deep learning method for unsupervised gland segmentation, where no manual annotations are required. Existing unsupervised semantic segmentation methods encounter a huge challenge on gland images: They either over-segment a gland into many fractions or under-segment the gland regions by confusing many of them with the background. To overcome this challenge, our key insight is to introduce an empirical cue about gland morphology as extra knowledge to guide the segmentation process. To this end, we propose a novel Morphology-inspired method via Selective Semantic Grouping. We first leverage the empirical cue to selectively mine out proposals for gland sub-regions with variant appearances. Then, a Morphology-aware Semantic Grouping module is employed to summarize the overall information about the gland by explicitly grouping the semantics of its sub-region proposals. In this way, the final segmentation network could learn comprehensive knowledge about glands and produce well-delineated, complete predictions. We conduct experiments on GlaS dataset and CRAG dataset. Our method exceeds the second-best counterpart over 10.56% at mIOU.
</details>
<details>
<summary>摘要</summary>
深度学习算法的设计对腺体分 segmentation是抑肿癌诊断和诊断的关键，但是手动标注的高投入成本限制了这种技术的发展和应用。在这篇论文中，我们首次尝试探索一种不需要手动标注的深度学习方法 для腺体分 segmentation。现有的无监督语义分割方法在腺体图像上遇到一大问题：它们都会过分 segment 腺体，或者将腺体区域与背景相混淆。为了解决这个问题，我们的关键思想是通过引入腺体形态的辅助知识来导向分 segmentation 过程。我们首先利用这个辅助知识来选择出腺体各个子区域的提议，然后使用具有形态意识的语义组合模块来总结腺体的全部信息。这样，最终的分 segmentation 网络可以学习腺体的全面信息，并生成完整、清晰的预测。我们在 GlaS 数据集和 CRAG 数据集上进行实验，我们的方法在 mIOU 上超过第二最佳对手的 10.56%。
</details></li>
</ul>
<hr>
<h2 id="Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering"><a href="#Expert-Knowledge-Aware-Image-Difference-Graph-Representation-Learning-for-Difference-Aware-Medical-Visual-Question-Answering" class="headerlink" title="Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"></a>Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11986">http://arxiv.org/abs/2307.11986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holipori/mimic-diff-vqa">https://github.com/holipori/mimic-diff-vqa</a></li>
<li>paper_authors: Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald M. Summers, Yingying Zhu</li>
<li>for: 提高医疗机器人识别能力，推动医学视觉语言模型的自动化。</li>
<li>methods: 基于专家知识，构建图像差异视Question Answering（VQA）任务，使用新收集的MIMIC-Diff-VQA数据集。</li>
<li>results: 提出了一种新的图像差异VQA任务，并提供了一种基于专家知识的图像差异GRAPH表示学习模型，以提高医学视觉语言模型的性能。<details>
<summary>Abstract</summary>
To contribute to automating the medical vision-language model, we propose a novel Chest-Xray Difference Visual Question Answering (VQA) task. Given a pair of main and reference images, this task attempts to answer several questions on both diseases and, more importantly, the differences between them. This is consistent with the radiologist's diagnosis practice that compares the current image with the reference before concluding the report. We collect a new dataset, namely MIMIC-Diff-VQA, including 700,703 QA pairs from 164,324 pairs of main and reference images. Compared to existing medical VQA datasets, our questions are tailored to the Assessment-Diagnosis-Intervention-Evaluation treatment procedure used by clinical professionals. Meanwhile, we also propose a novel expert knowledge-aware graph representation learning model to address this task. The proposed baseline model leverages expert knowledge such as anatomical structure prior, semantic, and spatial knowledge to construct a multi-relationship graph, representing the image differences between two images for the image difference VQA task. The dataset and code can be found at https://github.com/Holipori/MIMIC-Diff-VQA. We believe this work would further push forward the medical vision language model.
</details>
<details>
<summary>摘要</summary>
为推进医疗视语模型的自动化，我们提出了一个新的胸部X射影异常问答任务（VQA）任务。给定一对主要和参照图像，这个任务的目标是回答几个疾病和图像之间的差异相关的问题。这与辐射医生的诊断实践相一致，即比较当前图像与参照图像进行诊断报告。我们收集了一个新的数据集，即MIMIC-Diff-VQA，包含700703个问答对从164324对主要和参照图像的对。与现有的医疗VQA数据集相比，我们的问题更加适合诊断过程中的评估-诊断- interven-评估策略。同时，我们也提出了一种基于专家知识的图像差异学习模型来解决这个任务。我们的基eline模型利用专家知识，如解剖结构优先知识、语义知识和空间知识，构建多关系图，表示图像差异问题中的图像差异。数据集和代码可以在 GitHub 上找到：https://github.com/Holipori/MIMIC-Diff-VQA。我们认为这项工作将进一步推动医疗视语模型的发展。
</details></li>
</ul>
<hr>
<h2 id="Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model"><a href="#Simulation-of-Arbitrary-Level-Contrast-Dose-in-MRI-Using-an-Iterative-Global-Transformer-Model" class="headerlink" title="Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model"></a>Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11980">http://arxiv.org/abs/2307.11980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayang Wang, Srivathsa Pasumarthi, Greg Zaharchuk, Ryan Chamberlain</li>
<li>for: 这个论文是为了提出一种基于深度学习的扬化剂抑制和消除技术，以减少或完全消除荷尔蒙酸盐（GBCAs）的负面影响。</li>
<li>methods: 这种技术使用了一种名为Gformer的变换器，它是一种迭代模型，通过扫描和注意力机制来Synthesize图像，并且可以模拟不同的剑药剂和疾病水平。</li>
<li>results: 根据量化评估结果，提出的Gformer模型在比较其他现状技术时表现更好，而且在下游任务如剂量减少和肿瘤分割等方面也进行了评估，以证明这种技术的临床实用性。<details>
<summary>Abstract</summary>
Deep learning (DL) based contrast dose reduction and elimination in MRI imaging is gaining traction, given the detrimental effects of Gadolinium-based Contrast Agents (GBCAs). These DL algorithms are however limited by the availability of high quality low dose datasets. Additionally, different types of GBCAs and pathologies require different dose levels for the DL algorithms to work reliably. In this work, we formulate a novel transformer (Gformer) based iterative modelling approach for the synthesis of images with arbitrary contrast enhancement that corresponds to different dose levels. The proposed Gformer incorporates a sub-sampling based attention mechanism and a rotational shift module that captures the various contrast related features. Quantitative evaluation indicates that the proposed model performs better than other state-of-the-art methods. We further perform quantitative evaluation on downstream tasks such as dose reduction and tumor segmentation to demonstrate the clinical utility.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）基于对比剂减少和消除在MRI成像中得到了进展，由于质子剂基因链（GBCAs）的负面影响。这些DL算法 however 受到低质量低剂量数据的可用性的限制。另外，不同类型的GBCAs和疾病需要不同的剂量水平以便DL算法可靠地工作。在这种工作中，我们提出了一种基于转换器（Gformer）的迭代模型方法，用于生成具有任意对比强化的图像。我们的Gformer模型包括一种归一化基于抽样的注意力机制和一种旋转变换模块，以捕捉不同的对比相关特征。量化评估表明，我们的模型在其他状态对比较好的方法。我们进一步进行了下游任务的量化评估，如剂量减少和肿瘤分 segmentation，以证明我们的临床实用性。
</details></li>
</ul>
<hr>
<h2 id="Two-stream-Multi-level-Dynamic-Point-Transformer-for-Two-person-Interaction-Recognition"><a href="#Two-stream-Multi-level-Dynamic-Point-Transformer-for-Two-person-Interaction-Recognition" class="headerlink" title="Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition"></a>Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11973">http://arxiv.org/abs/2307.11973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Liu, Gangfeng Cui, Jiahui Luo, Lina Yao, Xiaojun Chang</li>
<li>for: 本研究的目的是提出一种基于点云网络的两人交互识别方法，以满足人工智能应用中的个人隐私要求。</li>
<li>methods: 我们提出了一种名为Interval Frame Sampling（IFS）的框选Method，以及一种两树多级特征聚合模块，以提取全局和部分特征。</li>
<li>results: 我们的网络在NTU RGB+D 60和NTU RGB+D 120的交互子集上进行了广泛的实验，并显示了与状态前方法进行比较的优异性。<details>
<summary>Abstract</summary>
As a fundamental aspect of human life, two-person interactions contain meaningful information about people's activities, relationships, and social settings. Human action recognition serves as the foundation for many smart applications, with a strong focus on personal privacy. However, recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions. In this paper, we propose a point cloud-based network named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition. Our model addresses the challenge of recognizing two-person interactions by incorporating local-region spatial information, appearance information, and motion information. To achieve this, we introduce a designed frame selection method named Interval Frame Sampling (IFS), which efficiently samples frames from videos, capturing more discriminative information in a relatively short processing time. Subsequently, a frame features learning module and a two-stream multi-level feature aggregation module extract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions. Finally, we apply a transformer to perform self-attention on the learned features for the final classification. Extensive experiments are conducted on two large-scale datasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. The results show that our network outperforms state-of-the-art approaches across all standard evaluation settings.
</details>
<details>
<summary>摘要</summary>
人类生活中的基本方面之一是两个人之间的交互，这些交互含有人们的活动、关系和社会环境中的有用信息。人工智能应用中强调个人隐私，人体动作识别作为基础技术，但Recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions。在这篇论文中，我们提出了一种基于点云的网络 named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition。我们的模型解决了认izing two-person interactions的挑战，通过 integrate local-region spatial information, appearance information, and motion information。为了实现这一点，我们提出了一种设计的Frame Selection Method named Interval Frame Sampling (IFS)，该方法高效地从视频中选择Frame， capture more discriminative information in a relatively short processing time。然后，我们引入了一个Frame Features Learning Module和Two-stream Multi-level Feature Aggregation Module，这两个模块通过EXTract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions。最后，我们使用 transformer 进行自注意力，以实现最终的分类。我们在 NTU RGB+D 60 和 NTU RGB+D 120 两个大规模数据集上进行了广泛的实验，结果显示，我们的网络在所有标准评估环境下都超过了现有方法。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Remote-Sensing-Image-Quality-Inspection-System"><a href="#Intelligent-Remote-Sensing-Image-Quality-Inspection-System" class="headerlink" title="Intelligent Remote Sensing Image Quality Inspection System"></a>Intelligent Remote Sensing Image Quality Inspection System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11965">http://arxiv.org/abs/2307.11965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijiong Yu, Tao Wang, Kang Ran, Chang Li, Hao Wu</li>
<li>for: 这篇论文旨在提出一个新的两步智能系统，用于Remote Sensing图像质量检查，以提高检查效率。</li>
<li>methods: 本论文提出的方法首先使用多 modelo 进行图像分类，然后使用最佳的方法来ocalize多种图像质量问题。</li>
<li>results:  результа业表示，提出的方法在Remote Sensing图像质量检查中表现出色，比一些一步方法更高效。此外，本论文初步探讨了多modal模型在Remote Sensing图像质量检查中的可行性和潜力。<details>
<summary>Abstract</summary>
Quality inspection is a necessary task before putting any remote sensing image into practical application. However, traditional manual inspection methods suffer from low efficiency. Hence, we propose a novel two-step intelligent system for remote sensing image quality inspection that combines multiple models, which first performs image classification and then employs the most appropriate methods to localize various forms of quality problems in the image. Results demonstrate that the proposed method exhibits excellent performance and efficiency in remote sensing image quality inspection, surpassing the performance of those one-step methods. Furthermore, we conduct an initial exploration of the feasibility and potential of applying multimodal models to remote sensing image quality inspection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Quality inspection is a necessary task before putting any remote sensing image into practical application. However, traditional manual inspection methods suffer from low efficiency. Hence, we propose a novel two-step intelligent system for remote sensing image quality inspection that combines multiple models, which first performs image classification and then employs the most appropriate methods to localize various forms of quality problems in the image. Results demonstrate that the proposed method exhibits excellent performance and efficiency in remote sensing image quality inspection, surpassing the performance of those one-step methods. Furthermore, we conduct an initial exploration of the feasibility and potential of applying multimodal models to remote sensing image quality inspection." into Simplified Chinese.翻译：在应用 remote sensing 图像之前，质量检测是一项必需的任务。然而，传统的手动检测方法具有低效率。因此，我们提出了一种新的两步智能系统，用于 remote sensing 图像质量检测，这个系统首先使用多个模型进行图像分类，然后使用最有效的方法来地址图像中的质量问题。结果显示，我们的方法在 remote sensing 图像质量检测中表现出色，高于一键方法的性能。此外，我们还进行了初步的多模态模型在 remote sensing 图像质量检测中的可行性和潜力的探索。>>
</details></li>
</ul>
<hr>
<h2 id="MIMONet-Multi-Input-Multi-Output-On-Device-Deep-Learning"><a href="#MIMONet-Multi-Input-Multi-Output-On-Device-Deep-Learning" class="headerlink" title="MIMONet: Multi-Input Multi-Output On-Device Deep Learning"></a>MIMONet: Multi-Input Multi-Output On-Device Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11962">http://arxiv.org/abs/2307.11962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexin Li, Xiaoxi He, Yufei Li, Shahab Nikkhoo, Wei Yang, Lothar Thiele, Cong Liu</li>
<li>for: This paper aims to improve the performance of intelligent robotic systems by proposing a novel on-device multi-input multi-output deep neural network (MIMO DNN) framework called MIMONet.</li>
<li>methods: MIMONet leverages existing single-input single-output (SISO) model compression techniques and develops a new deep-compression method tailored to MIMO models, which explores unique properties of the MIMO model to achieve boosted accuracy and on-device efficiency.</li>
<li>results: Extensive experiments on three embedded platforms and a case study using the TurtleBot3 robot demonstrate that MIMONet achieves higher accuracy and superior on-device efficiency compared to state-of-the-art SISO and MISO models, as well as a baseline MIMO model.<details>
<summary>Abstract</summary>
Future intelligent robots are expected to process multiple inputs simultaneously (such as image and audio data) and generate multiple outputs accordingly (such as gender and emotion), similar to humans. Recent research has shown that multi-input single-output (MISO) deep neural networks (DNN) outperform traditional single-input single-output (SISO) models, representing a significant step towards this goal. In this paper, we propose MIMONet, a novel on-device multi-input multi-output (MIMO) DNN framework that achieves high accuracy and on-device efficiency in terms of critical performance metrics such as latency, energy, and memory usage. Leveraging existing SISO model compression techniques, MIMONet develops a new deep-compression method that is specifically tailored to MIMO models. This new method explores unique yet non-trivial properties of the MIMO model, resulting in boosted accuracy and on-device efficiency. Extensive experiments on three embedded platforms commonly used in robotic systems, as well as a case study using the TurtleBot3 robot, demonstrate that MIMONet achieves higher accuracy and superior on-device efficiency compared to state-of-the-art SISO and MISO models, as well as a baseline MIMO model we constructed. Our evaluation highlights the real-world applicability of MIMONet and its potential to significantly enhance the performance of intelligent robotic systems.
</details>
<details>
<summary>摘要</summary>
将来的智能机器人将能同时处理多个输入（如图像和音频数据），并生成相应的多个输出（如性别和情感），类似于人类。最新的研究表明，多输入单输出（MISO）深度神经网络（DNN）的性能比单输入单输出（SISO）模型更高，这表示了大的一步。在这篇论文中，我们提出了一种名为MIMONet的在设备上运行的多输入多输出（MIMO）DNN框架，实现了高准确率和设备上的高效性。我们利用了现有的SISO模型压缩技术，开发了一种专门适应MIMO模型的深度压缩方法。这种新方法利用了MIMO模型独特 yet non-trivial 的性质，从而提高了准确率和设备上的效率。我们在三种常用于机器人系统的嵌入式平台上进行了广泛的实验，以及使用了TurtleBot3机器人进行了一个案例研究。我们的评估表明，MIMONet在与状态对照模型和基eline MIMO模型进行比较时，在准确率和设备上的性能均高于其他模型。我们的评估还表明，MIMONet在实际应用中具有广泛的应用前景和可能性。
</details></li>
</ul>
<hr>
<h2 id="DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation"><a href="#DHC-Dual-debiased-Heterogeneous-Co-training-Framework-for-Class-imbalanced-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation"></a>DHC: Dual-debiased Heterogeneous Co-training Framework for Class-imbalanced Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11960">http://arxiv.org/abs/2307.11960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/dhc">https://github.com/xmed-lab/dhc</a></li>
<li>paper_authors: Haonan Wang, Xiaomeng Li</li>
<li>for: 这个研究旨在提出一个基于semi-supervised learning的3D医疗影像分类框架，以解决对于医疗影像分类的专门技能和时间负担问题。</li>
<li>methods: 这个框架使用了一个名为Dual-debiased Heterogeneous Co-training（DHC）的新方法，包括两种损失衡量策略，即Distribution-aware Debiased Weighting（DistDW）和Difficulty-aware Debiased Weighting（DiffDW），以透过对于 pseudo labels的动态指导，帮助模型解决数据和学习偏见。</li>
<li>results: 实验结果显示，提出的方法可以将 pseudo labels 用于偏见调整和纾解类别偏见问题，并且与现有的SSL方法相比，提高了模型的性能。此外，我们还提出了更加代表的医疗影像分类 semi-supervised 测试标准，以全面显示这些类别偏见的设计的效果。<details>
<summary>Abstract</summary>
The volume-wise labeling of 3D medical images is expertise-demanded and time-consuming; hence semi-supervised learning (SSL) is highly desirable for training with limited labeled data. Imbalanced class distribution is a severe problem that bottlenecks the real-world application of these methods but was not addressed much. Aiming to solve this issue, we present a novel Dual-debiased Heterogeneous Co-training (DHC) framework for semi-supervised 3D medical image segmentation. Specifically, we propose two loss weighting strategies, namely Distribution-aware Debiased Weighting (DistDW) and Difficulty-aware Debiased Weighting (DiffDW), which leverage the pseudo labels dynamically to guide the model to solve data and learning biases. The framework improves significantly by co-training these two diverse and accurate sub-models. We also introduce more representative benchmarks for class-imbalanced semi-supervised medical image segmentation, which can fully demonstrate the efficacy of the class-imbalance designs. Experiments show that our proposed framework brings significant improvements by using pseudo labels for debiasing and alleviating the class imbalance problem. More importantly, our method outperforms the state-of-the-art SSL methods, demonstrating the potential of our framework for the more challenging SSL setting. Code and models are available at: https://github.com/xmed-lab/DHC.
</details>
<details>
<summary>摘要</summary>
“三维医疗影像的量化标签是专业需求又是时间耗尽的，因此半监督学习（SSL）是非常有优点的。然而，实际应用中存在资料分布不均的问题，这对实际应用带来了很大的阻碍。为解决这个问题，我们提出了一个新的双重偏见随时执行（DHC）框架，用于半监督三维医疗影像分类。具体来说，我们提出了两种损失调整策略，namely Distribution-aware Debiased Weighting（DistDW）和 Difficulty-aware Debiased Weighting（DiffDW），这些策略可以在执行时将 pseudo labels 用于导引模型解决数据和学习偏见。这个框架在半监督下执行这两个多标的子模型，从而提高了性能。我们还提出了更多的代表性的标签数据集，用于测试实际中的半监督医疗影像分类。实验结果显示，我们的提案的框架可以将 pseudo labels 用于偏见调整和缓和资料分布不均问题，并且超越了现有的SSL方法。代码和模型可以在：https://github.com/xmed-lab/DHC 中找到。”
</details></li>
</ul>
<hr>
<h2 id="Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier"><a href="#Topology-Preserving-Automatic-Labeling-of-Coronary-Arteries-via-Anatomy-aware-Connection-Classifier" class="headerlink" title="Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier"></a>Topology-Preserving Automatic Labeling of Coronary Arteries via Anatomy-aware Connection Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11959">http://arxiv.org/abs/2307.11959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zutsusemi/miccai2023-topolab-labels">https://github.com/zutsusemi/miccai2023-topolab-labels</a></li>
<li>paper_authors: Zhixing Zhang, Ziwei Zhao, Dong Wang, Shishuang Zhao, Yuhang Liu, Jia Liu, Liwei Wang</li>
<li>for: 这个论文主要是为了提高心血管疾病诊断过程中自动标注 coronary artery 的精度和效率。</li>
<li>methods: 该方法使用了新的 TopoLab 框架，该框架包括明确表征动脉连接的方法，以及层次结构特征提取和动脉间特征互动的策略。</li>
<li>results: 实验结果表明，使用 TopoLab 可以在 orCaScore 数据集和一个内部数据集上达到状态机器人的表现，提高了自动标注 coronary artery 的精度和效率。<details>
<summary>Abstract</summary>
Automatic labeling of coronary arteries is an essential task in the practical diagnosis process of cardiovascular diseases. For experienced radiologists, the anatomically predetermined connections are important for labeling the artery segments accurately, while this prior knowledge is barely explored in previous studies. In this paper, we present a new framework called TopoLab which incorporates the anatomical connections into the network design explicitly. Specifically, the strategies of intra-segment feature aggregation and inter-segment feature interaction are introduced for hierarchical segment feature extraction. Moreover, we propose the anatomy-aware connection classifier to enable classification for each connected segment pair, which effectively exploits the prior topology among the arteries with different categories. To validate the effectiveness of our method, we contribute high-quality annotations of artery labeling to the public orCaScore dataset. The experimental results on both the orCaScore dataset and an in-house dataset show that our TopoLab has achieved state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
自动标注 coronary arteries 是诊断冠状动脉疾病的重要任务。经验丰富的医生们认为，将预先确定的 анатомиче连接 explicitly  incorporated into the network design 是标注artery 段 accurately。在这篇论文中，我们提出了一种新的框架 called TopoLab，它使用了 hierarchical segment feature extraction 和 anatomy-aware connection classifier 来实现这一目标。此外，我们还提供了高质量的 artery 标注数据，以 validate 我们的方法。实验结果表明，我们的 TopoLab 已经达到了领先的性能。Here's the text with some additional explanations and notes:自动标注 coronary arteries 是诊断冠状动脉疾病的重要任务。（1） coronary arteries 是冠状动脉系统中的重要组成部分，它们的血流很重要，一旦发生疾病，会对身体产生严重的影响。（2） 为了诊断 coronary arteries 的疾病，需要进行精准的标注，但这是一项复杂的任务，需要具备很好的知识和技能。经验丰富的医生们认为，将预先确定的 anatomical connections  explicitly  incorporated into the network design 是标注 coronary arteries 段 accurately。（3） anatomical connections 是指冠状动脉系统中各个组成部分之间的连接关系，这些连接关系可以帮助医生更好地理解冠状动脉系统的结构和功能。在这篇论文中，我们提出了一种新的框架 called TopoLab，它使用了 hierarchical segment feature extraction 和 anatomy-aware connection classifier 来实现标注 coronary arteries 的任务。（4） TopoLab 的核心思想是利用 hierarchical segment feature extraction 来提取 coronary arteries 的特征，并通过 anatomy-aware connection classifier 来确定各个连接关系的类别。此外，我们还提供了高质量的 artery 标注数据，以 validate 我们的方法。（5） 我们使用了一个高质量的 dataset，并对其进行了严格的验证和验收，以确保我们的方法的可靠性和精准性。实验结果表明，我们的 TopoLab 已经达到了领先的性能。（6） 我们对比了 TopoLab 与其他方法的实验结果，发现 TopoLab 的性能明显超过了其他方法。这表明，我们的方法可以帮助医生更好地诊断 coronary arteries 的疾病。
</details></li>
</ul>
<hr>
<h2 id="Pick-the-Best-Pre-trained-Model-Towards-Transferability-Estimation-for-Medical-Image-Segmentation"><a href="#Pick-the-Best-Pre-trained-Model-Towards-Transferability-Estimation-for-Medical-Image-Segmentation" class="headerlink" title="Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation"></a>Pick the Best Pre-trained Model: Towards Transferability Estimation for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11958">http://arxiv.org/abs/2307.11958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/endoluminalsurgicalvision-imr/ccfv">https://github.com/endoluminalsurgicalvision-imr/ccfv</a></li>
<li>paper_authors: Yuncheng Yang, Meng Wei, Junjun He, Jie Yang, Jin Ye, Yun Gu</li>
<li>for: 这种论文主要用于适用于医疗图像分割任务中的深度神经网络训练，以便更好地利用庞大的医疗图像数据。</li>
<li>methods: 该论文提出了一种新的转移性能度估计（TE）方法，该方法考虑了类别一致性和特征多样性，以更好地估计转移性能度。</li>
<li>results: 对比现有的TE算法，该方法在医疗图像分割任务中的转移性能度估计表现出色，并且在实验中胜过所有现有的TE算法。<details>
<summary>Abstract</summary>
Transfer learning is a critical technique in training deep neural networks for the challenging medical image segmentation task that requires enormous resources. With the abundance of medical image data, many research institutions release models trained on various datasets that can form a huge pool of candidate source models to choose from. Hence, it's vital to estimate the source models' transferability (i.e., the ability to generalize across different downstream tasks) for proper and efficient model reuse. To make up for its deficiency when applying transfer learning to medical image segmentation, in this paper, we therefore propose a new Transferability Estimation (TE) method. We first analyze the drawbacks of using the existing TE algorithms for medical image segmentation and then design a source-free TE framework that considers both class consistency and feature variety for better estimation. Extensive experiments show that our method surpasses all current algorithms for transferability estimation in medical image segmentation. Code is available at https://github.com/EndoluminalSurgicalVision-IMR/CCFV
</details>
<details>
<summary>摘要</summary>
启用转移学习是训练深度神经网络的关键技术，用于具有巨大资源的医学图像分割任务。由于医学图像数据的备受，许多研究机构发布了基于不同数据集的模型，这些模型可以形成一个庞大的候选源模型库。因此，对于正确和高效地 reuse 模型，估计源模型的传输性（即在不同下游任务中generalization）是非常重要的。为了解决医学图像分割中转移学习的不足，本文提出了一种新的传输性估计（TE）方法。我们首先分析了现有TE算法在医学图像分割中的缺点，然后设计了一个无源TE框架，考虑了类含义一致和特征多样性，以更好地估计传输性。经验表明，我们的方法在医学图像分割中的传输性估计比现有算法更高。代码可以在https://github.com/EndoluminalSurgicalVision-IMR/CCFV 中找到。
</details></li>
</ul>
<hr>
<h2 id="High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization"><a href="#High-performance-real-world-optical-computing-trained-by-in-situ-model-free-optimization" class="headerlink" title="High-performance real-world optical computing trained by in situ model-free optimization"></a>High-performance real-world optical computing trained by in situ model-free optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11957">http://arxiv.org/abs/2307.11957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyuan Zhao, Xin Shu, Renjie Zhou</li>
<li>for: 提高光学计算系统的高速和低能耗数据处理能力，但面临计算吃力和现实模拟差距问题。</li>
<li>methods: 使用模型独立的光学计算系统优化方法，基于排名评分算法直接倒敲光学权重的概率分布，不需要计算吃力和偏见的系统模拟。</li>
<li>results: 在MNIST和FMNIST数据集上实现了高精度分类，并在单层折射光学计算系统上实现了图像自由和高速细胞分析的潜力。<details>
<summary>Abstract</summary>
Optical computing systems can provide high-speed and low-energy data processing but face deficiencies in computationally demanding training and simulation-to-reality gap. We propose a model-free solution for lightweight in situ optimization of optical computing systems based on the score gradient estimation algorithm. This approach treats the system as a black box and back-propagates loss directly to the optical weights' probabilistic distributions, hence circumventing the need for computation-heavy and biased system simulation. We demonstrate a superior classification accuracy on the MNIST and FMNIST datasets through experiments on a single-layer diffractive optical computing system. Furthermore, we show its potential for image-free and high-speed cell analysis. The inherent simplicity of our proposed method, combined with its low demand for computational resources, expedites the transition of optical computing from laboratory demonstrations to real-world applications.
</details>
<details>
<summary>摘要</summary>
光学计算系统可以提供高速和低能耗数据处理，但面临 computationally demanding 训练和实际与模拟之间的差距。我们提议一种模型自由的解决方案，基于分布式权重的推估算法，用于优化光学计算系统。这种方法将系统视为黑盒，将损失反射直接到光学权重的概率分布中，因此不需要 computationally 复杂和偏见的系统模拟。我们通过在单层散射光学计算系统上进行实验，示出了在 MNIST 和 FMNIST 数据集上的高精度分类。此外，我们还展示了它的潜在应用于无图像和高速细胞分析。我们的提议的简单性和计算资源的低需求，将推动光学计算从实验室示范向实际应用的过渡。
</details></li>
</ul>
<hr>
<h2 id="Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System"><a href="#Puioio-On-device-Real-Time-Smartphone-Based-Automated-Exercise-Repetition-Counting-System" class="headerlink" title="Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System"></a>Pūioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02420">http://arxiv.org/abs/2308.02420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Sinclair, Kayla Kautai, Seyed Reza Shahamiri</li>
<li>for: 这个研究旨在开发一个基于深度学习的智能手机应用程序，用于实时评估运动重复次数。</li>
<li>methods: 这个系统包括五个 ком成分：投射估计、阈值、Optical flow、状态机器和计数器。</li>
<li>results: 这个系统在实际测试中获得了98.89%的准确率，并且在预先录取的资料集上获得了98.85%的准确率。<details>
<summary>Abstract</summary>
Automated exercise repetition counting has applications across the physical fitness realm, from personal health to rehabilitation. Motivated by the ubiquity of mobile phones and the benefits of tracking physical activity, this study explored the feasibility of counting exercise repetitions in real-time, using only on-device inference, on smartphones. In this work, after providing an extensive overview of the state-of-the-art automatic exercise repetition counting methods, we introduce a deep learning based exercise repetition counting system for smartphones consisting of five components: (1) Pose estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5) Counter. The system is then implemented via a cross-platform mobile application named P\=uioio that uses only the smartphone camera to track repetitions in real time for three standard exercises: Squats, Push-ups, and Pull-ups. The proposed system was evaluated via a dataset of pre-recorded videos of individuals exercising as well as testing by subjects exercising in real time. Evaluation results indicated the system was 98.89% accurate in real-world tests and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an effective, low-cost, and convenient alternative to existing solutions since the proposed system has minimal hardware requirements without requiring any wearable or specific sensors or network connectivity.
</details>
<details>
<summary>摘要</summary>
自动化运动重复计数有应用于身体健身领域的广泛应用，从个人健康到rehabilitation。驱动于手机的普遍和跟踪物理活动的好处，本研究探讨了使用手机上的只 inference来实时计数运动重复的可能性。在这种工作中，我们首先提供了现有自动运动重复计数方法的广泛概述，然后介绍了一种基于深度学习的运动重复计数系统，该系统包括以下五个组成部分：（1）姿势估计，（2）阈值设定，（3）光流计算，（4）状态机，（5）计数器。该系统后来通过一款可在多个平台上运行的跨平台移动应用程序 named P\=uioio 实现，该应用程序只使用手机摄像头来实时跟踪运动重复。我们对三种标准运动进行测试：蹲下、推上和抓上。我们对这些测试结果进行评估，结果表明，该系统在实际测试中的准确率为98.89%，并且在预录视频数据集上的评估结果为98.85%。这使得该系统成为一种有效、低成本、便捷的代替方案，因为该系统具有最低硬件要求，不需要任何佩戴式设备或特定的感应器或网络连接。
</details></li>
</ul>
<hr>
<h2 id="LAMP-Leveraging-Language-Prompts-for-Multi-person-Pose-Estimation"><a href="#LAMP-Leveraging-Language-Prompts-for-Multi-person-Pose-Estimation" class="headerlink" title="LAMP: Leveraging Language Prompts for Multi-person Pose Estimation"></a>LAMP: Leveraging Language Prompts for Multi-person Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11934">http://arxiv.org/abs/2307.11934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shengnanh20/lamp">https://github.com/shengnanh20/lamp</a></li>
<li>paper_authors: Shengnan Hu, Ce Zheng, Zixiang Zhou, Chen Chen, Gita Sukthankar</li>
<li>for: 这篇论文目的是提高人机交互的效果，帮助社交机器人在拥挤的公共场所中穿梭。</li>
<li>methods: 该论文提出了一种新的提示基于的多人姿态估计策略，即语言助け多人姿态估计（LAMP）。该策略利用了一个已经训练好的语言模型（CLIP）生成的文本表示，以便更好地理解人体姿态，并通过文本提示来增强视觉表示的不可见部分。</li>
<li>results: 该论文表明，使用语言supervised Training可以提高单Stage多人姿态估计的性能，并且both个体级和关节级的提示都是训练中的有价值贡献。<details>
<summary>Abstract</summary>
Human-centric visual understanding is an important desideratum for effective human-robot interaction. In order to navigate crowded public places, social robots must be able to interpret the activity of the surrounding humans. This paper addresses one key aspect of human-centric visual understanding, multi-person pose estimation. Achieving good performance on multi-person pose estimation in crowded scenes is difficult due to the challenges of occluded joints and instance separation. In order to tackle these challenges and overcome the limitations of image features in representing invisible body parts, we propose a novel prompt-based pose inference strategy called LAMP (Language Assisted Multi-person Pose estimation). By utilizing the text representations generated by a well-trained language model (CLIP), LAMP can facilitate the understanding of poses on the instance and joint levels, and learn more robust visual representations that are less susceptible to occlusion. This paper demonstrates that language-supervised training boosts the performance of single-stage multi-person pose estimation, and both instance-level and joint-level prompts are valuable for training. The code is available at https://github.com/shengnanh20/LAMP.
</details>
<details>
<summary>摘要</summary>
人类中心视觉理解是人机交互中的重要需求。为了在人权拥挤的公共场所中导航，社交机器人需要能够理解周围人类的活动。这篇论文解决了人类中心视觉理解的一个关键方面，即多人pose estimation。在拥挤场景中实现好的多人pose estimation是困难的，因为隐藏的关节和实例分离是主要的挑战。为了解决这些挑战并超越图像特征在表示隐藏身体部分方面的局限性，我们提出了一种新的提示基于的pose推断策略called LAMP（语言协助多人pose estimation）。通过利用训练好的语言模型（CLIP）生成的文本表示，LAMP可以促进实例和关节水平的pose理解，并学习更加Robust的视觉表示，更少受到遮挡的影响。这篇论文示出，语言超vised训练可以提高单 Stage多人pose estimation的性能，并且实例水平和关节水平的提示都是训练的有价值。代码可以在https://github.com/shengnanh20/LAMP中获取。
</details></li>
</ul>
<hr>
<h2 id="RICo-Rotate-Inpaint-Complete-for-Generalizable-Scene-Reconstruction"><a href="#RICo-Rotate-Inpaint-Complete-for-Generalizable-Scene-Reconstruction" class="headerlink" title="RICo: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction"></a>RICo: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11932">http://arxiv.org/abs/2307.11932</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac Kasahara, Shubham Agrawal, Selim Engin, Nikhil Chavan-Dafle, Shuran Song, Volkan Isler</li>
<li>for:  scene reconstruction from a single view, with the goal of estimating the full 3D geometry and texture of a scene containing previously unseen objects.</li>
<li>methods:  leveraging large language models to inpaint missing areas of scene color images rendered from different views, and then lifting these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values.</li>
<li>results:  outperforms multiple baselines while providing generalization to novel objects and scenes, with robustness to changes in depth distributions and scale.<details>
<summary>Abstract</summary>
General scene reconstruction refers to the task of estimating the full 3D geometry and texture of a scene containing previously unseen objects. In many practical applications such as AR/VR, autonomous navigation, and robotics, only a single view of the scene may be available, making the scene reconstruction a very challenging task. In this paper, we present a method for scene reconstruction by structurally breaking the problem into two steps: rendering novel views via inpainting and 2D to 3D scene lifting. Specifically, we leverage the generalization capability of large language models to inpaint the missing areas of scene color images rendered from different views. Next, we lift these inpainted images to 3D by predicting normals of the inpainted image and solving for the missing depth values. By predicting for normals instead of depth directly, our method allows for robustness to changes in depth distributions and scale. With rigorous quantitative evaluation, we show that our method outperforms multiple baselines while providing generalization to novel objects and scenes.
</details>
<details>
<summary>摘要</summary>
全景重建指的是根据已知的2D图像来估算场景中包含未知对象的3D几何和文本ure。在许多实际应用中，如AR/VR、自动导航和机器人等，只有一个视图可用，因此场景重建变得非常困难。在这篇论文中，我们提出了一种场景重建方法，通过分解问题为两步来解决：首先，通过填充缺失部分来渲染新的视图图像；然后，使用这些渲染的图像来提升3D场景的抽象。具体来说，我们利用大型自然语言模型来填充场景颜色图像中的缺失部分。接下来，我们使用这些填充的图像来预测场景的法向量，并通过解决缺失的深度值来提升3D场景。由于预测法向量而不是直接预测深度，我们的方法具有对深度分布和比例的弹性。经过严谨的量化评估，我们表明了我们的方法在多个基eline上方法性能高，同时具有对新对象和场景的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models"><a href="#PartDiff-Image-Super-resolution-with-Partial-Diffusion-Models" class="headerlink" title="PartDiff: Image Super-resolution with Partial Diffusion Models"></a>PartDiff: Image Super-resolution with Partial Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11926">http://arxiv.org/abs/2307.11926</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Zhao, Alex Ling Yu Hung, Kaifeng Pang, Haoxin Zheng, Kyunghyun Sung</li>
<li>for: 这个论文主要针对的是图像超分辨率生成任务中的Diffusion-based生成模型，它们可以很好地生成高质量的图像。</li>
<li>methods: 这个论文提出了一种新的Partial Diffusion Model（PartDiff），它在图像扩散过程中只需要扩散到中间 latent state 而不是扩散到完全随机噪声中，从中间 latent state 开始进行恢复。</li>
<li>results: 对于MRI和自然图像的超分辨率生成任务，Partial Diffusion Models 可以significantly reduce the number of denoising steps 而不是 sacrificing the quality of generation。<details>
<summary>Abstract</summary>
Denoising diffusion probabilistic models (DDPMs) have achieved impressive performance on various image generation tasks, including image super-resolution. By learning to reverse the process of gradually diffusing the data distribution into Gaussian noise, DDPMs generate new data by iteratively denoising from random noise. Despite their impressive performance, diffusion-based generative models suffer from high computational costs due to the large number of denoising steps.In this paper, we first observed that the intermediate latent states gradually converge and become indistinguishable when diffusing a pair of low- and high-resolution images. This observation inspired us to propose the Partial Diffusion Model (PartDiff), which diffuses the image to an intermediate latent state instead of pure random noise, where the intermediate latent state is approximated by the latent of diffusing the low-resolution image. During generation, Partial Diffusion Models start denoising from the intermediate distribution and perform only a part of the denoising steps. Additionally, to mitigate the error caused by the approximation, we introduce "latent alignment", which aligns the latent between low- and high-resolution images during training. Experiments on both magnetic resonance imaging (MRI) and natural images show that, compared to plain diffusion-based super-resolution methods, Partial Diffusion Models significantly reduce the number of denoising steps without sacrificing the quality of generation.
</details>
<details>
<summary>摘要</summary>
diffusion probabilistic models (DDPMs) 拥有在不同的图像生成任务中表现出色，包括图像超解像。通过学习逆转数据分布慢慢散发的过程，DDPMs 生成新的数据，通过多次减噪来实现。尽管它们在表现方面印象深刻，但散发基于的生成模型受到高计算成本的限制，这是因为散发步骤的数量太多。在这篇论文中，我们首先发现了将两个低分辨率和高分辨率图像散发到了中间状态后，中间状态会逐渐减少差异，并变得无法区分。这一观察点我们提出了partial diffusion模型（PartDiff），它将图像散发到中间状态而不是完全随机噪声中，中间状态approximerated by low-resolution image的散发latent。在生成过程中，Partial Diffusion Models从中间分布开始减噪，并只完成一部分的减噪步骤。此外，为了减少由approximation引起的误差，我们引入"latent alignment"，在训练期间对低分辨率和高分辨率图像的latent进行对齐。实验表明，与普通的散发基于超解像方法相比，Partial Diffusion Models可以在MRI和自然图像上减少减噪步骤数量，而不是牺牲生成质量。
</details></li>
</ul>
<hr>
<h2 id="Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data"><a href="#Poverty-rate-prediction-using-multi-modal-survey-and-earth-observation-data" class="headerlink" title="Poverty rate prediction using multi-modal survey and earth observation data"></a>Poverty rate prediction using multi-modal survey and earth observation data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11921">http://arxiv.org/abs/2307.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Fobi, Manuel Cardona, Elliott Collins, Caleb Robinson, Anthony Ortiz, Tina Sederholm, Rahul Dodhia, Juan Lavista Ferres</li>
<li>For: The paper aims to predict the poverty rate of a region by combining household demographic and living standards survey questions with features derived from satellite imagery.* Methods: The paper uses a single-step featurization method to extract visual features from freely available 10m&#x2F;px Sentinel-2 surface reflectance satellite imagery, and combines these features with ten survey questions in a proxy means test (PMT) to estimate poverty rates. The paper also proposes an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery.* Results: The inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88%, and using a subset of survey questions selected based on their complementarity to the visual features results in the best performance with errors decreasing from 4.09% to 3.71%. The extracted visual features also encode geographic and urbanization differences between regions.<details>
<summary>Abstract</summary>
This work presents an approach for combining household demographic and living standards survey questions with features derived from satellite imagery to predict the poverty rate of a region. Our approach utilizes visual features obtained from a single-step featurization method applied to freely available 10m/px Sentinel-2 surface reflectance satellite imagery. These visual features are combined with ten survey questions in a proxy means test (PMT) to estimate whether a household is below the poverty line. We show that the inclusion of visual features reduces the mean error in poverty rate estimates from 4.09% to 3.88% over a nationally representative out-of-sample test set. In addition to including satellite imagery features in proxy means tests, we propose an approach for selecting a subset of survey questions that are complementary to the visual features extracted from satellite imagery. Specifically, we design a survey variable selection approach guided by the full survey and image features and use the approach to determine the most relevant set of small survey questions to include in a PMT. We validate the choice of small survey questions in a downstream task of predicting the poverty rate using the small set of questions. This approach results in the best performance -- errors in poverty rate decrease from 4.09% to 3.71%. We show that extracted visual features encode geographic and urbanization differences between regions.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这项研究提出了一种方法，将家庭调查问题与卫星成像数据特征结合起来预测地区贫困率。该方法使用单步特征化方法提取自由可用的10m/px Sentinal-2表面反射卫星成像数据中的视觉特征，然后与十个调查问题结合在一起进行代理平均测试（PMT），以估算家庭是否下于贫困线。我们发现，通过包含卫星成像特征，贫困率估算的均误率从4.09%降低到3.88%。此外，我们还提出了一种选择调查问题的方法，该方法根据全面调查和图像特征进行指导，以选择最相关的小调查问题来包含在PMT中。我们验证了这些小调查问题的选择，并发现贫困率估算的误差从4.09%降低到3.71%。我们发现，提取的视觉特征含有地域和城市化差异。
</details></li>
</ul>
<hr>
<h2 id="Building3D-An-Urban-Scale-Dataset-and-Benchmarks-for-Learning-Roof-Structures-from-Point-Clouds"><a href="#Building3D-An-Urban-Scale-Dataset-and-Benchmarks-for-Learning-Roof-Structures-from-Point-Clouds" class="headerlink" title="Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds"></a>Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11914">http://arxiv.org/abs/2307.11914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisheng Wang, Shangfeng Huang, Hongxin Yang</li>
<li>for: 这个论文主要是为了提供一个大规模的城市建筑模型 benchmark，以便进行未来城市建筑模型的研究。</li>
<li>methods: 这个论文使用了 LiDAR 点云测试获得的数据，并使用了各种手工和深度特征基础的算法进行评估。</li>
<li>results: 论文发现了城市建筑模型存在高内分组变化、数据不对称和大规模噪声等挑战，并提供了首个和最大的城市建筑模型 benchmark，以便进行未来城市建筑模型的研究。<details>
<summary>Abstract</summary>
Urban modeling from LiDAR point clouds is an important topic in computer vision, computer graphics, photogrammetry and remote sensing. 3D city models have found a wide range of applications in smart cities, autonomous navigation, urban planning and mapping etc. However, existing datasets for 3D modeling mainly focus on common objects such as furniture or cars. Lack of building datasets has become a major obstacle for applying deep learning technology to specific domains such as urban modeling. In this paper, we present a urban-scale dataset consisting of more than 160 thousands buildings along with corresponding point clouds, mesh and wire-frame models, covering 16 cities in Estonia about 998 Km2. We extensively evaluate performance of state-of-the-art algorithms including handcrafted and deep feature based methods. Experimental results indicate that Building3D has challenges of high intra-class variance, data imbalance and large-scale noises. The Building3D is the first and largest urban-scale building modeling benchmark, allowing a comparison of supervised and self-supervised learning methods. We believe that our Building3D will facilitate future research on urban modeling, aerial path planning, mesh simplification, and semantic/part segmentation etc.
</details>
<details>
<summary>摘要</summary>
城市模型从LiDAR点云是计算机视觉、计算机图形、光学测量和远程感知等领域的重要话题。3D城市模型在智能城市、自动导航、城市规划和地图等领域发现了广泛的应用。然而，现有的3D模型数据集主要集中在常见的物品 such as 家具或车辆。lack of 建筑数据集成为应用深度学习技术于特定领域 such as 城市模型的主要障碍。在这篇论文中，我们提供了一个城市级别的数据集，包括超过16万个建筑物 along with 相应的点云、网格和细线模型，覆盖了16座城市，总面积约998平方公里。我们进行了广泛的性能评估，包括手工设计和深度特征基于方法。实验结果表明，Building3D存在高内类差异、数据不均衡和大规模噪音等挑战。Building3D是首个和最大的城市级别建筑模型 benchmark，允许对supervised和自主学习方法进行比较。我们认为，我们的Building3D将促进未来对城市模型、空中路径规划、网格简化和semantic/part分割等方面的研究。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks"><a href="#Unveiling-Vulnerabilities-in-Interpretable-Deep-Learning-Systems-with-Query-Efficient-Black-box-Attacks" class="headerlink" title="Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks"></a>Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11906">http://arxiv.org/abs/2307.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eldor Abdukhamidov, Mohammed Abuhamad, Simon S. Woo, Eric Chan-Tin, Tamer Abuhmed</li>
<li>for: 防护受到攻击的深度学习系统，因为它们容易受到恶意攻击的威胁。</li>
<li>methods: 我们提出了一种基于微生物遗传学算法的黑盒攻击方法，不需要对目标模型和解释模型有任何专业知识。</li>
<li>results: 我们的实验结果显示，这种攻击方法可以实现高度的攻击成功率，使用攻击示例和属性地图，与正常样本几乎无法区别。<details>
<summary>Abstract</summary>
Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-based black-box attack against IDLSes that requires no prior knowledge of the target model and its interpretation model. The proposed attack is a query-efficient approach that combines transfer-based and score-based methods, making it a powerful tool to unveil IDLS vulnerabilities. Our experiments of the attack show high attack success rates using adversarial examples with attribution maps that are highly similar to those of benign samples which makes it difficult to detect even by human analysts. Our results highlight the need for improved IDLS security to ensure their practical reliability.
</details>
<details>
<summary>摘要</summary>
深度学习已经广泛应用在多个领域，但它们容易受到敌意攻击。这些攻击会对深度学习基于系统造成严重的威胁，对其稳定性、可靠性和信任性造成潜在的影响。可解释深度学习系统（IDLS）是为了使系统更加透明和可解释的，但它们也被证明容易受到攻击。在这种工作中，我们提出了一种基于微生物遗传算法的黑盒攻击方法，不需要攻击目标模型和其解释模型的先前知识。我们的攻击方法结合了传递基本方法和分数基本方法，使其成为攻击IDLS的强大工具。我们的实验结果显示，使用对抗示例和归属地图可以达到高攻击成功率，这些对抗示例与正常样本具有高度相似的归属地图，使其具有难以探测的特点。我们的结果 highlights the need for improved IDLS security to ensure their practical reliability.
</details></li>
</ul>
<hr>
<h2 id="Model-Compression-Methods-for-YOLOv5-A-Review"><a href="#Model-Compression-Methods-for-YOLOv5-A-Review" class="headerlink" title="Model Compression Methods for YOLOv5: A Review"></a>Model Compression Methods for YOLOv5: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11904">http://arxiv.org/abs/2307.11904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Jani, Jamil Fayyad, Younes Al-Younes, Homayoun Najjaran</li>
<li>for: 本文主要针对于增强YOLO对象检测器的研究，以提高其精度和效率。</li>
<li>methods: 本文主要采用network pruning和quantization两种方法来压缩YOLOv5模型，以适应资源有限的边缘设备。</li>
<li>results: 通过对YOLOv5模型进行压缩，可以降低内存使用量和推理时间，使其在硬件限制的边缘设备上进行部署成为可能。但是，在实施中还存在一些挑战，需要进一步的探索和优化。<details>
<summary>Abstract</summary>
Over the past few years, extensive research has been devoted to enhancing YOLO object detectors. Since its introduction, eight major versions of YOLO have been introduced with the purpose of improving its accuracy and efficiency. While the evident merits of YOLO have yielded to its extensive use in many areas, deploying it on resource-limited devices poses challenges. To address this issue, various neural network compression methods have been developed, which fall under three main categories, namely network pruning, quantization, and knowledge distillation. The fruitful outcomes of utilizing model compression methods, such as lowering memory usage and inference time, make them favorable, if not necessary, for deploying large neural networks on hardware-constrained edge devices. In this review paper, our focus is on pruning and quantization due to their comparative modularity. We categorize them and analyze the practical results of applying those methods to YOLOv5. By doing so, we identify gaps in adapting pruning and quantization for compressing YOLOv5, and provide future directions in this area for further exploration. Among several versions of YOLO, we specifically choose YOLOv5 for its excellent trade-off between recency and popularity in literature. This is the first specific review paper that surveys pruning and quantization methods from an implementation point of view on YOLOv5. Our study is also extendable to newer versions of YOLO as implementing them on resource-limited devices poses the same challenges that persist even today. This paper targets those interested in the practical deployment of model compression methods on YOLOv5, and in exploring different compression techniques that can be used for subsequent versions of YOLO.
</details>
<details>
<summary>摘要</summary>
过去几年，对 YOLO 对象检测器进行了广泛的研究，旨在提高其精度和效率。自其出现以来，共有八个主要版本的 YOLO 发布，以提高其性能和效能。虽然 YOLO 在许多领域得到了广泛的应用，但在具有限制的硬件设备上部署它存在挑战。为解决这个问题，各种神经网络压缩方法得到了开发，这些方法可以分为三类：网络剪辑、量化和知识传递。由于这些方法的实际成果，如减少内存使用量和计算时间，因此它们在硬件限制的边缘设备上部署成为了必要的。在本文中，我们主要关注剪辑和量化，因为它们在可控性方面比较高。我们按照不同的方法进行了分类和分析，并通过应用这些方法于 YOLOv5 来评估其实际效果。通过这种方式，我们可以了解剪辑和量化在 YOLOv5 上的应用存在哪些挑战，并提供未来研究的方向。在多个 YOLO 版本中，我们选择了 YOLOv5，因为它在文献中的悠久度和受欢迎程度都非常高。这是对剪辑和量化在 YOLOv5 上的实践评估的首个专题评论文。我们的研究也可以扩展到更新版本的 YOLO，因为在具有限制的硬件设备上部署它们也存在同样的挑战。这篇文章针对那些关注实际部署模型压缩方法的人，以及想要探索不同的压缩技术，以应用于更新版本的 YOLO。
</details></li>
</ul>
<hr>
<h2 id="Selecting-the-motion-ground-truth-for-loose-fitting-wearables-benchmarking-optical-MoCap-methods"><a href="#Selecting-the-motion-ground-truth-for-loose-fitting-wearables-benchmarking-optical-MoCap-methods" class="headerlink" title="Selecting the motion ground truth for loose-fitting wearables: benchmarking optical MoCap methods"></a>Selecting the motion ground truth for loose-fitting wearables: benchmarking optical MoCap methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11881">http://arxiv.org/abs/2307.11881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lalasray/dmcb">https://github.com/lalasray/dmcb</a></li>
<li>paper_authors: Lala Shakti Swarup Ray, Bo Zhou, Sungho Suh, Paul Lukowicz</li>
<li>for: 用于评估 оптиче markers 和 marker-less MoCap 的性能，以选择最佳的质量评估方法。</li>
<li>methods: 使用了大量的实际录制 MoCap 数据，并对不同的 drape 水平进行并发的 3D 物理 simulations，以评估 marker-based 和 marker-less MoCap 方法的性能。</li>
<li>results: marker-based MoCap 和 marker-less MoCap 在轻度的穿着衣服下 both exhibit  significan performance loss (&gt;10cm)，但是在日常活动中， marker-less MoCap 略微超过 marker-based MoCap，making it a favorable and cost-effective choice for wearable studies.<details>
<summary>Abstract</summary>
To help smart wearable researchers choose the optimal ground truth methods for motion capturing (MoCap) for all types of loose garments, we present a benchmark, DrapeMoCapBench (DMCB), specifically designed to evaluate the performance of optical marker-based and marker-less MoCap. High-cost marker-based MoCap systems are well-known as precise golden standards. However, a less well-known caveat is that they require skin-tight fitting markers on bony areas to ensure the specified precision, making them questionable for loose garments. On the other hand, marker-less MoCap methods powered by computer vision models have matured over the years, which have meager costs as smartphone cameras would suffice. To this end, DMCB uses large real-world recorded MoCap datasets to perform parallel 3D physics simulations with a wide range of diversities: six levels of drape from skin-tight to extremely draped garments, three levels of motions and six body type - gender combinations to benchmark state-of-the-art optical marker-based and marker-less MoCap methods to identify the best-performing method in different scenarios. In assessing the performance of marker-based and low-cost marker-less MoCap for casual loose garments both approaches exhibit significant performance loss (>10cm), but for everyday activities involving basic and fast motions, marker-less MoCap slightly outperforms marker-based MoCap, making it a favorable and cost-effective choice for wearable studies.
</details>
<details>
<summary>摘要</summary>
为帮助智能佩戴设备研究人员选择最佳的股权实验方法，我们提出了一个标准化的测试平台：DrapeMoCapBench（DMCB），用于评估光学标记基本和无标记MoCap的性能。高成本的光学标记基本MoCap系统已经被广泛认可为精度的金标准，但是它们需要在骨部位上粘贴皮肤紧密的标记，以确保 specify 的精度，这使得它们对松裤服不太可靠。而无标记MoCap方法，通过计算机视觉模型已经成熟了多年，它们的成本很低，只需要使用智能手机相机即可。因此，DMCB使用了大量的真实世界记录的MoCap数据进行并行的3D物理模拟，以评估当今最佳的光学标记基本和无标记MoCap方法的性能。在评估松裤服上的光学标记基本和低成本无标记MoCap方法的性能时，两者都表现出了明显的性能损失（>10cm）。但是在日常活动中涉及到基本和快速动作时，无标记MoCap方法尚微出perform marker-based MoCap，使其成为便宜且可靠的选择 для wearable研究。
</details></li>
</ul>
<hr>
<h2 id="Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network"><a href="#Digital-Modeling-on-Large-Kernel-Metamaterial-Neural-Network" class="headerlink" title="Digital Modeling on Large Kernel Metamaterial Neural Network"></a>Digital Modeling on Large Kernel Metamaterial Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11862">http://arxiv.org/abs/2307.11862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Liu, Hanyu Zheng, Brandon T. Swartz, Ho hin Lee, Zuhayr Asad, Ivan Kravchenko, Jason G. Valentine, Yuankai Huo<br>for: 这个论文是针对现代深度神经网络（DNNs）的物理部署（例如CPUs和GPUs）所做的研究。这种设计可能会导致重要的计算负担，延迟和耗电量问题，这些问题在物联网（IoT）、边缘 computing 和无人机（drones）应用中是critical的限制。methods: 这个论文使用了最新的光学计算单元（例如元material），实现了无源电力和光速神经网络。但是，光学设计的物理限制（例如精度、噪声和宽度）会导致物理设计的限制。此外，现有的元material神经网络（MNN）的优点（例如光速计算）并未经过标准3x3卷积核心的探索。这个论文提出了一个新的大型卷积元material神经网络（LMNN），它可以最大化现代SOTA MNN的数位容量，并考虑光学限制。results: 这个论文的实验结果显示，使用LMNN可以提高分类精度，同时降低计算延迟。实验结果表明，将 computation cost of convolutional front-end offloaded into fabricated optical hardware可以提高分类精度。这个研究表明，LMNN的开发是可能实现无源电力和光速AI的一个重要步骤。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) utilized recently are physically deployed with computational units (e.g., CPUs and GPUs). Such a design might lead to a heavy computational burden, significant latency, and intensive power consumption, which are critical limitations in applications such as the Internet of Things (IoT), edge computing, and the usage of drones. Recent advances in optical computational units (e.g., metamaterial) have shed light on energy-free and light-speed neural networks. However, the digital design of the metamaterial neural network (MNN) is fundamentally limited by its physical limitations, such as precision, noise, and bandwidth during fabrication. Moreover, the unique advantages of MNN's (e.g., light-speed computation) are not fully explored via standard 3x3 convolution kernels. In this paper, we propose a novel large kernel metamaterial neural network (LMNN) that maximizes the digital capacity of the state-of-the-art (SOTA) MNN with model re-parametrization and network compression, while also considering the optical limitation explicitly. The new digital learning scheme can maximize the learning capacity of MNN while modeling the physical restrictions of meta-optic. With the proposed LMNN, the computation cost of the convolutional front-end can be offloaded into fabricated optical hardware. The experimental results on two publicly available datasets demonstrate that the optimized hybrid design improved classification accuracy while reducing computational latency. The development of the proposed LMNN is a promising step towards the ultimate goal of energy-free and light-speed AI.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）最近几年广泛应用，通常通过计算单元（如CPU和GPU）进行物理部署。这种设计可能会导致重大的计算负担、显著的延迟和大量的电力消耗，这些限制在互联网物联网（IoT）、边缘计算和无人机应用中非常 kritisch。 latest advances in optical computational units（如元material）have shed light on energy-free and light-speed neural networks. However, the digital design of the metamaterial neural network (MNN) is fundamentally limited by its physical limitations, such as precision, noise, and bandwidth during fabrication. Moreover, the unique advantages of MNN's (e.g., light-speed computation) are not fully explored via standard 3x3 convolution kernels. In this paper, we propose a novel large kernel metamaterial neural network (LMNN) that maximizes the digital capacity of the state-of-the-art (SOTA) MNN with model re-parametrization and network compression, while also considering the optical limitation explicitly. The new digital learning scheme can maximize the learning capacity of MNN while modeling the physical restrictions of meta-optic. With the proposed LMNN, the computation cost of the convolutional front-end can be offloaded into fabricated optical hardware. The experimental results on two publicly available datasets demonstrate that the optimized hybrid design improved classification accuracy while reducing computational latency. The development of the proposed LMNN is a promising step towards the ultimate goal of energy-free and light-speed AI.Here's a word-for-word translation of the text into Simplified Chinese:深度神经网络（DNN）最近几年广泛应用，通常通过计算单元（如CPU和GPU）进行物理部署。这种设计可能会导致重大的计算负担、显著的延迟和大量的电力消耗，这些限制在互联网物联网（IoT）、边缘计算和无人机应用中非常 kritisch。 最近的光学计算单元（如元material）的进步抛光了能源免费和光速神经网络。然而，光学神经网络（MNN）的数字设计受到物理限制，如精度、雷达和带宽的制约，这些限制在fabrication中表现出来。此外，MNN的独特优点（如光速计算）通过标准3x3卷积核不充分发挥。在这篇论文中，我们提出了一种新的大 kernel metamaterial神经网络（LMNN），该模型可以最大化SOTA MNN的数字容量，同时考虑光学限制。新的数字学习方案可以在MNN中最大化学习能力，同时模拟meta-optic的物理限制。通过提出LMNN，计算前端的计算成本可以卸载到fabricated的光学硬件上。实验结果表明，使用两个公共可用的数据集，LMNN的优化型材料设计可以提高分类精度，同时减少计算延迟。LMNN的发展是前往能源免费和光速AI的普遍进步的重要步骤。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Your-Trained-DETRs-with-Box-Refinement"><a href="#Enhancing-Your-Trained-DETRs-with-Box-Refinement" class="headerlink" title="Enhancing Your Trained DETRs with Box Refinement"></a>Enhancing Your Trained DETRs with Box Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11828">http://arxiv.org/abs/2307.11828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yiqunchen1999/refinebox">https://github.com/yiqunchen1999/refinebox</a></li>
<li>paper_authors: Yiqun Chen, Qiang Chen, Peize Sun, Shoufa Chen, Jingdong Wang, Jian Cheng</li>
<li>for: 提高 DETR 和其 variants 的本地化性能</li>
<li>methods: 使用轻量级增强网络进行推理 Outputs 的精度提高</li>
<li>results: 在 COCO 和 LVIS $1.0 $ 上实验表明 RefineBox 可以提高 DETR 和其 variants 的性能，例如 DETR 的性能提高 2.4 AP，Conditinal-DETR 的性能提高 2.5 AP，DAB-DETR 的性能提高 1.9 AP，DN-DETR 的性能提高 1.6 AP。Here’s the summary in English for reference:</li>
<li>for: Improving the localization performance of DETR and its variants</li>
<li>methods: Using lightweight refinement networks to refine the outputs of DETR-like detectors</li>
<li>results: Experimental results on COCO and LVIS $1.0$ show that RefineBox can improve the performance of DETR and its variants, with performance gains of 2.4 AP, 2.5 AP, 1.9 AP, and 1.6 AP for DETR, Conditinal-DETR, DAB-DETR, and DN-DETR, respectively.<details>
<summary>Abstract</summary>
We present a conceptually simple, efficient, and general framework for localization problems in DETR-like models. We add plugins to well-trained models instead of inefficiently designing new models and training them from scratch. The method, called RefineBox, refines the outputs of DETR-like detectors by lightweight refinement networks. RefineBox is easy to implement and train as it only leverages the features and predicted boxes from the well-trained detection models. Our method is also efficient as we freeze the trained detectors during training. In addition, we can easily generalize RefineBox to various trained detection models without any modification. We conduct experiments on COCO and LVIS $1.0$. Experimental results indicate the effectiveness of our RefineBox for DETR and its representative variants (Figure 1). For example, the performance gains for DETR, Conditinal-DETR, DAB-DETR, and DN-DETR are 2.4 AP, 2.5 AP, 1.9 AP, and 1.6 AP, respectively. We hope our work will bring the attention of the detection community to the localization bottleneck of current DETR-like models and highlight the potential of the RefineBox framework. Code and models will be publicly available at: \href{https://github.com/YiqunChen1999/RefineBox}{https://github.com/YiqunChen1999/RefineBox}.
</details>
<details>
<summary>摘要</summary>
我们提出了一个概念简单、高效和通用的框位问题解决方案，用于DETR-like模型。我们在已经训练过的模型上添加插件而不是从scratch开发新的模型和重新训练。我们称之为RefineBox，它使用轻量级修正网络来精度地修正DETR-like探测器的输出。RefineBox易于实现和训练，只需利用已经训练过的检测器的特征和预测框。我们的方法也具有高效性，因为我们在训练过程中冻结了已经训练过的检测器。此外，我们可以轻松地扩展RefineBox到不同的训练过的检测器上，无需修改。我们在COCO和LVIS $1.0$上进行了实验，实验结果表明RefineBox对DETR和其相关变体（图1）具有效果。例如，对DETR、Conditinal-DETR、DAB-DETR和DN-DETR的性能提升分别为2.4 AP、2.5 AP、1.9 AP和1.6 AP。我们希望我们的工作能吸引检测社区对当前DETR-like模型的本地化瓶颈的注意，并强调RefineBox框架的潜在可能性。代码和模型将在GitHub上公开：<https://github.com/YiqunChen1999/RefineBox>。
</details></li>
</ul>
<hr>
<h2 id="BandRe-Rethinking-Band-Pass-Filters-for-Scale-Wise-Object-Detection-Evaluation"><a href="#BandRe-Rethinking-Band-Pass-Filters-for-Scale-Wise-Object-Detection-Evaluation" class="headerlink" title="BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection Evaluation"></a>BandRe: Rethinking Band-Pass Filters for Scale-Wise Object Detection Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11748">http://arxiv.org/abs/2307.11748</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shinya7y/UniverseNet">https://github.com/shinya7y/UniverseNet</a></li>
<li>paper_authors: Yosuke Shinya</li>
<li>for: 本文提出了一种新的精度评估方法，用于评估物体检测器在实际应用中的性能。</li>
<li>methods: 本文使用了一个筛 бан组合，包括三角形和梯形带通过筛，来评估物体检测器的精度。</li>
<li>results: 经过实验，本文显示了新的精度评估方法可以准确地评估物体检测器的性能，并且可以强调不同的方法和数据集之间的差异。<details>
<summary>Abstract</summary>
Scale-wise evaluation of object detectors is important for real-world applications. However, existing metrics are either coarse or not sufficiently reliable. In this paper, we propose novel scale-wise metrics that strike a balance between fineness and reliability, using a filter bank consisting of triangular and trapezoidal band-pass filters. We conduct experiments with two methods on two datasets and show that the proposed metrics can highlight the differences between the methods and between the datasets. Code is available at https://github.com/shinya7y/UniverseNet .
</details>
<details>
<summary>摘要</summary>
精度评估是对实际应用中对象检测器的评估非常重要。然而，现有的指标都是 Either too coarse or not reliable enough。在这篇论文中，我们提出了一种新的精度指标，它们能够在精度和可靠性之间做出平衡，使用了一个由三角形和梯形带滤波器组成的筛子。我们在两种方法和两个数据集上进行了实验，并证明了我们的指标可以更好地反映方法和数据集之间的差异。代码可以在 GitHub 上找到：https://github.com/shinya7y/UniverseNet。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Data-Augmentation-Learning-using-Bilevel-Optimization-for-Histopathological-Images"><a href="#Automatic-Data-Augmentation-Learning-using-Bilevel-Optimization-for-Histopathological-Images" class="headerlink" title="Automatic Data Augmentation Learning using Bilevel Optimization for Histopathological Images"></a>Automatic Data Augmentation Learning using Bilevel Optimization for Histopathological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11808">http://arxiv.org/abs/2307.11808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smounsav/bilevel_augment_histo">https://github.com/smounsav/bilevel_augment_histo</a></li>
<li>paper_authors: Saypraseuth Mounsaveng, Issam Laradji, David Vázquez, Marco Perdersoli, Ismail Ben Ayed</li>
<li>for: 用于适应深度学习模型在 histopathological 图像分类中的训练问题，因为细胞和组织的颜色和形状变化，以及有限的数据量，使模型学习这些变化困难。</li>
<li>methods: 使用数据扩大（DA）技术，在训练过程中生成更多的样本，以帮助模型对颜色和形状变化变得抗变异。</li>
<li>results: 通过自动学习 DA 参数，使用 truncated backpropagation 进行快速和高效地学习，并在六个不同的 dataset 上进行验证。实验结果表明，我们的模型可以学习更有用的颜色和旋转变换，而不需要手动选择 DA 变换。此外，我们的模型只需要微调几个方法特有的超参数，但是性能更高。<details>
<summary>Abstract</summary>
Training a deep learning model to classify histopathological images is challenging, because of the color and shape variability of the cells and tissues, and the reduced amount of available data, which does not allow proper learning of those variations. Variations can come from the image acquisition process, for example, due to different cell staining protocols or tissue deformation. To tackle this challenge, Data Augmentation (DA) can be used during training to generate additional samples by applying transformations to existing ones, to help the model become invariant to those color and shape transformations. The problem with DA is that it is not only dataset-specific but it also requires domain knowledge, which is not always available. Without this knowledge, selecting the right transformations can only be done using heuristics or through a computationally demanding search. To address this, we propose an automatic DA learning method. In this method, the DA parameters, i.e. the transformation parameters needed to improve the model training, are considered learnable and are learned automatically using a bilevel optimization approach in a quick and efficient way using truncated backpropagation. We validated the method on six different datasets. Experimental results show that our model can learn color and affine transformations that are more helpful to train an image classifier than predefined DA transformations, which are also more expensive as they need to be selected before the training by grid search on a validation set. We also show that similarly to a model trained with RandAugment, our model has also only a few method-specific hyperparameters to tune but is performing better. This makes our model a good solution for learning the best DA parameters, especially in the context of histopathological images, where defining potentially useful transformation heuristically is not trivial.
</details>
<details>
<summary>摘要</summary>
训练深度学习模型分类 histopathological 图像是具有挑战性，因为细胞和组织的颜色和形状可能具有差异，而且可用数据的量相对较少，不允许模型学习这些差异。这些差异可能来自图像获取过程中，例如不同的细胞染色协议或组织变形。为解决这个挑战，数据扩大（DA）可以在训练过程中应用转换来生成更多的样本，以帮助模型对颜色和形状变化变得抗变异。然而，DA并不是 dataset-specific，而且需要域知识，即不一定可以获得。在缺乏域知识的情况下，选择正确的转换只能通过规则或通过计算昂贵的搜索来实现。为此，我们提出了一种自动 DA 学习方法。在这种方法中，DA 参数，即需要改进模型训练的转换参数，被视为可学习的并通过归档逆传播进行自动学习，以便快速和高效地进行学习。我们在六个不同的数据集上进行验证。实验结果表明，我们的模型可以学习更有用的颜色和Affine 转换，而不需要手动选择DA转换。此外，我们的模型也只需要少量的方法特有超参数来调整，但是性能更高。这使得我们的模型成为了学习最佳 DA 参数的好解决方案，特别是在 histopathological 图像上，因为定义有用的转换方法可能不是易事。
</details></li>
</ul>
<hr>
<h2 id="3D-Skeletonization-of-Complex-Grapevines-for-Robotic-Pruning"><a href="#3D-Skeletonization-of-Complex-Grapevines-for-Robotic-Pruning" class="headerlink" title="3D Skeletonization of Complex Grapevines for Robotic Pruning"></a>3D Skeletonization of Complex Grapevines for Robotic Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11706">http://arxiv.org/abs/2307.11706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Schneider, Sushanth Jayanth, Abhisesh Silwal, George Kantor</li>
<li>for: 提高机器人剪刀技术，以便在实际商业葡萄园中进行葡萄藤梢剪刀</li>
<li>methods: 基于植物skeletonization技术，提高机器人视觉能力，以便在 denser和更复杂的葡萄藤结构中进行剪刀</li>
<li>results: 提高剪刀精度，使用3D和skeletal信息可以更 preciselly predict剪刀重量，超过了先前的工作<details>
<summary>Abstract</summary>
Robotic pruning of dormant grapevines is an area of active research in order to promote vine balance and grape quality, but so far robotic efforts have largely focused on planar, simplified vines not representative of commercial vineyards. This paper aims to advance the robotic perception capabilities necessary for pruning in denser and more complex vine structures by extending plant skeletonization techniques. The proposed pipeline generates skeletal grapevine models that have lower reprojection error and higher connectivity than baseline algorithms. We also show how 3D and skeletal information enables prediction accuracy of pruning weight for dense vines surpassing prior work, where pruning weight is an important vine metric influencing pruning site selection.
</details>
<details>
<summary>摘要</summary>
“机械剪裁棕榈葡萄是一项活跃的研究领域，以促进葡萄平衡和质量提高，但目前机械努力主要集中在平面化、简单的葡萄藤上。本文旨在提高机械识别能力，以便在更复杂和紧凑的葡萄藤结构中进行剪裁。我们提出的管道使得植物skeletonization技术得到扩展，生成的股骨葡萄模型具有较低的重oprojection误差和更高的连接率，并且我们还示出了基于3D和股骨信息的剪裁重量预测精度超过先前的工作，剪裁重量是葡萄metric关键参数，对剪裁点选择产生重要影响。”
</details></li>
</ul>
<hr>
<h2 id="SACReg-Scene-Agnostic-Coordinate-Regression-for-Visual-Localization"><a href="#SACReg-Scene-Agnostic-Coordinate-Regression-for-Visual-Localization" class="headerlink" title="SACReg: Scene-Agnostic Coordinate Regression for Visual Localization"></a>SACReg: Scene-Agnostic Coordinate Regression for Visual Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11702">http://arxiv.org/abs/2307.11702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jerome Revaud, Yohann Cabon, Romain Brégier, JongMin Lee, Philippe Weinzaepfel</li>
<li>for: 这个研究旨在提出一个可以应用于各种景象的全局坐标 regression 模型，以提高现有的Scene Regression（SCR）方法的缩减性和应用范围。</li>
<li>methods: 本研究使用了 transformer 架构，并可以处理变数数量的图像和罕见2D-3D标注。具体来说，模型通过从 off-the-shelf 图像检索技术和 Structure-from-Motion 数据库获取输入，并使用 transformer 架构进行训练。</li>
<li>results: 研究发现，这个模型可以在多个测试 benchmark 上表现出色，特别是在Scene Regression 方法中，并在 Cambridge localization benchmark 上设置了新的州立纪录，甚至超越了 feature-matching-based 方法。<details>
<summary>Abstract</summary>
Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets. In this paper, we propose a new paradigm where a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations. The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input. It is trained on a few diverse datasets and significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization. In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.CV_2023_07_22/" data-id="clp8zxr6k00gvn6886ybgaeh2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_07_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/07/22/cs.AI_2023_07_22/" class="article-date">
  <time datetime="2023-07-22T12:00:00.000Z" itemprop="datePublished">2023-07-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/07/22/cs.AI_2023_07_22/">cs.AI - 2023-07-22</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC"><a href="#A-Revolution-of-Personalized-Healthcare-Enabling-Human-Digital-Twin-with-Mobile-AIGC" class="headerlink" title="A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC"></a>A Revolution of Personalized Healthcare: Enabling Human Digital Twin with Mobile AIGC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12115">http://arxiv.org/abs/2307.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Chen, Changyan Yi, Hongyang Du, Dusit Niyato, Jiawen Kang, Jun Cai, Xuemin, Shen</li>
<li>for: 这篇论文是为了探讨移动人工智能生成内容（AIGC）技术在人类数字双（HDT）应用中的可能性和挑战而写的。</li>
<li>methods: 该论文提出了一种基于移动AIGC的HDT系统架构，并确定了相关的设计要求和挑战。同时，文章还介绍了两个使用场景，即手术规划和个性化药物。</li>
<li>results: 文章通过实验研究证明了移动AIGC驱动的HDT解决方案的效果，并在虚拟物理治疗教学平台中应用了这种解决方案。<details>
<summary>Abstract</summary>
Mobile Artificial Intelligence-Generated Content (AIGC) technology refers to the adoption of AI algorithms deployed at mobile edge networks to automate the information creation process while fulfilling the requirements of end users. Mobile AIGC has recently attracted phenomenal attentions and can be a key enabling technology for an emerging application, called human digital twin (HDT). HDT empowered by the mobile AIGC is expected to revolutionize the personalized healthcare by generating rare disease data, modeling high-fidelity digital twin, building versatile testbeds, and providing 24/7 customized medical services. To promote the development of this new breed of paradigm, in this article, we propose a system architecture of mobile AIGC-driven HDT and highlight the corresponding design requirements and challenges. Moreover, we illustrate two use cases, i.e., mobile AIGC-driven HDT in customized surgery planning and personalized medication. In addition, we conduct an experimental study to prove the effectiveness of the proposed mobile AIGC-driven HDT solution, which shows a particular application in a virtual physical therapy teaching platform. Finally, we conclude this article by briefly discussing several open issues and future directions.
</details>
<details>
<summary>摘要</summary>
Mobile artificial intelligence生成内容（AIGC）技术指的是通过移动边缘网络部署人工智能算法自动生成信息的过程，同时满足用户的需求。 Mobile AIGC在最近吸引了非常大的关注，可以是人类数字双胞虫（HDT）的关键技术。 HDT通过移动AIGC得到强化，预计将在个性化医疗方面产生极高精度的数字双胞虫，生成罕见疾病数据，建立多样化测试平台，提供24/7个性化医疗服务。在这篇文章中，我们提议了移动AIGC驱动HDT的系统架构，并 highlight了相关的设计要求和挑战。此外，我们还介绍了两个使用场景：移动AIGC驱动HDT在自定义手术规划中和个性化药物。此外，我们进行了实验研究，证明了我们提议的移动AIGC驱动HDT解决方案的有效性，该解决方案在虚拟物理治疗教学平台中得到应用。最后，我们 briefly discussed several open issues and future directions。
</details></li>
</ul>
<hr>
<h2 id="A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks"><a href="#A-Zero-shot-and-Few-shot-Study-of-Instruction-Finetuned-Large-Language-Models-Applied-to-Clinical-and-Biomedical-Tasks" class="headerlink" title="A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"></a>A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12114">http://arxiv.org/abs/2307.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanis Labrak, Mickael Rouvier, Richard Dufour</li>
<li>for: 这些论文旨在评估四种现状最佳大型自然语言处理（NLP）模型（ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca）在13种真实世界医疗和生物医学NLP任务中的性能，包括命名实体识别（NER）、问答（QA）、关系提取（RE）等。</li>
<li>methods: 这些论文使用了四种现状最佳大型NLP模型，并在英文语言上进行了13种真实世界医疗和生物医学NLP任务的测试和评估。</li>
<li>results: 研究结果表明，评估的LLMs在零和几个预测场景中对大多数任务的性能几乎相当于状态之前的模型，特别是在问答任务上表现非常出色，即使它们没有看到这些任务的示例。然而，对于分类和RE任务，模型的性能比专门为医疗领域训练的模型，如PubMedBERT，下降。此外，研究发现没有LLM可以在所有任务中占据领先地位，各模型在某些任务上表现更好。<details>
<summary>Abstract</summary>
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
</details>
<details>
<summary>摘要</summary>
我们评估了四种当前最佳的 instruciton-tuned大语言模型（LLMs）——ChatGPT、Flan-T5 UL2、Tk-Instruct和Alpaca——在英文的13种实际医疗和生物医学自然语言处理（NLP）任务上，如命名实体识别（NER）、问答（QA）、关系提取（RE）等。我们的总结结果表明，评估的LLMs在零和几个预测场景中的性能接近了当前最佳模型的水平，特别是在QA任务上表现出色，即使它们从来没有看到这些任务的示例。然而，我们发现，分类和RE任务的性能下降到了专门为医疗领域训练的模型，如PubMedBERT，所能达到的水平。最后，我们注意到，没有任何LLM在所有研究任务上表现出优于其他模型，一些模型更适合某些任务。
</details></li>
</ul>
<hr>
<h2 id="CFR-p-Counterfactual-Regret-Minimization-with-Hierarchical-Policy-Abstraction-and-its-Application-to-Two-player-Mahjong"><a href="#CFR-p-Counterfactual-Regret-Minimization-with-Hierarchical-Policy-Abstraction-and-its-Application-to-Two-player-Mahjong" class="headerlink" title="CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction, and its Application to Two-player Mahjong"></a>CFR-p: Counterfactual Regret Minimization with Hierarchical Policy Abstraction, and its Application to Two-player Mahjong</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12087">http://arxiv.org/abs/2307.12087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiheng Wang</li>
<li>for: 这篇论文是为了应用Counterfactual Regret Minimization(CFR)算法到另一款具有多种变体的 incomplete information 游戏——麻将。</li>
<li>methods: 论文使用了game theoretical analysis和层次抽象来改进CFR算法，以适应麻将游戏的复杂性。</li>
<li>results: 研究发现，这种基于赢家策略的CFR框架可以普适应其他不完整信息游戏。<details>
<summary>Abstract</summary>
Counterfactual Regret Minimization(CFR) has shown its success in Texas Hold'em poker. We apply this algorithm to another popular incomplete information game, Mahjong. Compared to the poker game, Mahjong is much more complex with many variants. We study two-player Mahjong by conducting game theoretical analysis and making a hierarchical abstraction to CFR based on winning policies. This framework can be generalized to other imperfect information games.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>Counterfactual Regret Minimization（CFR）在德州扑克游戏中表现出色。我们将这个算法应用到另一款流行的不完全信息游戏——麻将。与扑克游戏相比，麻将更加复杂，有多种变体。我们通过游戏理论分析和基于赢家策略层次化CFR的框架来研究两人麻将。这个框架可以推广到其他不完全信息游戏。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Temporal-Planning-Domains-by-Sequential-Macro-actions-Extended-Version"><a href="#Enhancing-Temporal-Planning-Domains-by-Sequential-Macro-actions-Extended-Version" class="headerlink" title="Enhancing Temporal Planning Domains by Sequential Macro-actions (Extended Version)"></a>Enhancing Temporal Planning Domains by Sequential Macro-actions (Extended Version)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12081">http://arxiv.org/abs/2307.12081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco De Bortoli, Lukáš Chrpa, Martin Gebser, Gerald Steinbauer-Wagner</li>
<li>for: 提高多代理人和资源共享的具有时间约束的计划效率。</li>
<li>methods: 使用带有恒常性的维度和扩展的约束来实现Sequential Temporal Macro-Actions，保证计划的可行性。</li>
<li>results: 在多个计划器和领域中实现提高了获得优化计划和计划质量。<details>
<summary>Abstract</summary>
Temporal planning is an extension of classical planning involving concurrent execution of actions and alignment with temporal constraints. Durative actions along with invariants allow for modeling domains in which multiple agents operate in parallel on shared resources. Hence, it is often important to avoid resource conflicts, where temporal constraints establish the consistency of concurrent actions and events. Unfortunately, the performance of temporal planning engines tends to sharply deteriorate when the number of agents and objects in a domain gets large. A possible remedy is to use macro-actions that are well-studied in the context of classical planning. In temporal planning settings, however, introducing macro-actions is significantly more challenging when the concurrent execution of actions and shared use of resources, provided the compliance to temporal constraints, should not be suppressed entirely. Our work contributes a general concept of sequential temporal macro-actions that guarantees the applicability of obtained plans, i.e., the sequence of original actions encapsulated by a macro-action is always executable. We apply our approach to several temporal planners and domains, stemming from the International Planning Competition and RoboCup Logistics League. Our experiments yield improvements in terms of obtained satisficing plans as well as plan quality for the majority of tested planners and domains.
</details>
<details>
<summary>摘要</summary>
temporal 规划是 classical 规划的扩展，具有同时执行动作和时间约束的整合。持续动作和 invariants 允许在多个代理人在共享资源上并发操作的Domain 模型。因此，通常需要避免资源冲突，而时间约束可以确定同时执行的动作和事件的一致性。然而， temporal 规划引擎的性能通常随多个代理人和对象的数量增加而逐渐下降。为了解决这个问题，我们可以使用 macro-actions，它们在 классиical 规划中非常成熟。然而，在 temporal 规划设置下，引入 macro-actions 是更加挑战，因为需要保持同时执行动作和共享资源的一致性，而不能完全终止。我们的工作提出了一种通用的顺序 temporal macro-actions 概念，该概念保证原始动作序列被封装在 macro-action 中执行的情况下，得到的计划是可靠的。我们对多个 temporal 规划器和领域进行了应用，其中包括国际规划竞赛和 RoboCup 物流联盟。我们的实验表明，我们的方法可以提高大多数测试的规划器和领域中的得到的满意计划以及计划质量。
</details></li>
</ul>
<hr>
<h2 id="Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations"><a href="#Game-Theoretic-Robust-Reinforcement-Learning-Handles-Temporally-Coupled-Perturbations" class="headerlink" title="Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"></a>Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12062">http://arxiv.org/abs/2307.12062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Tuomas Sandholm, Furong Huang, Stephen McAleer</li>
<li>for: 本研究旨在训练可以在环境干扰或敌意攻击下表现良好的RL策略。</li>
<li>methods: 我们提出了GRAD方法，它将把时间相关的干扰问题看作是一个部分可见两个玩家零 SUM 游戏，通过找到这个游戏的approximate平衡，确保agent对时间相关的干扰具有强大的Robustness。</li>
<li>results: 我们在一系列连续控制任务上进行了实验，结果表明，相比基eline，我们的提议方法在state和action空间中都具有显著的Robustness优势，特别是在面对时间相关的干扰攻击时。<details>
<summary>Abstract</summary>
Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.
</details>
<details>
<summary>摘要</summary>
STRONG REINFORCEMENT LEARNING (RL) aims to train policies that can perform well under environmental perturbations or adversarial attacks. Existing methods typically assume that the space of possible perturbations remains the same across timesteps. However, in many situations, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a new challenge for existing robust RL methods. To address this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks show that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupled attacks, in both state and action spaces.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units"><a href="#Fast-Knowledge-Graph-Completion-using-Graphics-Processing-Units" class="headerlink" title="Fast Knowledge Graph Completion using Graphics Processing Units"></a>Fast Knowledge Graph Completion using Graphics Processing Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12059">http://arxiv.org/abs/2307.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun-Hee Lee, Dong-oh Kang, Hwa Jeon Song</li>
<li>for: 本研究旨在提供一种高效的知识图谱完成框架，用于在 GPU 上获得新关系。</li>
<li>methods: 本研究使用知识图谱嵌入模型来实现知识图谱完成。首先，我们定义 “可转换为度量空间”，然后将知识图谱完成问题转换成度量空间中的相似Join问题。然后，我们利用度量空间的性质 deriv 出公式，并基于这些公式开发了一个快速的知识图谱完成算法。</li>
<li>results: 我们的研究表明，我们的框架可以高效地处理知识图谱完成问题。<details>
<summary>Abstract</summary>
Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate $N\times N \times R$ vector operations, where $N$ is the number of entities and $R$ is the number of relation types. It is very costly.   In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define "transformable to a metric space" and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is "transformable to a metric space". After that, to efficiently process the similarity join problem, we derive formulas using the properties of a metric space. Based on the formulas, we develop a fast knowledge graph completion algorithm. Finally, we experimentally show that our framework can efficiently process the knowledge graph completion problem.
</details>
<details>
<summary>摘要</summary>
知识图可以应用于数据semantics中的多个领域，如问答系统、知识基础系统。然而，现有的知识图需要补充以提高知识的关系。这被称为知识图完成。为了通过使用知识图嵌入模型添加新的关系到现有的知识图，我们需要评估 $N\times N \times R$ 矢量操作，其中 $N$ 是实体的数量，$R$ 是关系类型的数量。这非常昂贵。在这篇论文中，我们提供了一个高效的知识图完成框架在GPU上进行新关系获取。我们首先定义"可转换到一个度量空间"，然后将知识图完成问题转换成一个相似Join问题，该问题可以"可转换到一个度量空间"。然后，我们 derivate formulas使用度量空间的性质，然后我们开发了一个快速的知识图完成算法。最后，我们实验表明，我们的框架可以高效地处理知识图完成问题。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning"><a href="#Extracting-Molecular-Properties-from-Natural-Language-with-Multimodal-Contrastive-Learning" class="headerlink" title="Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning"></a>Extracting Molecular Properties from Natural Language with Multimodal Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12996">http://arxiv.org/abs/2307.12996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Lacombe, Andrew Gaut, Jeff He, David Lüdeke, Kateryna Pistunova</li>
<li>for: 本研究旨在将科学知识从文本中提取到分子图表示，以 Bridge 深度学习在计算生物化学中的图表示和文本描述之间的 gap。</li>
<li>methods: 本研究使用对比学习将神经图表示与文本描述的特征进行对应，并使用神经相关性分数策略提高文本检索。此外，我们还提出了一种基于有机反应的新的分子图数据增强策略。</li>
<li>results: 我们的模型在下游 MoleculeNet 性质分类任务上表现出色，与模型只使用图模式alone (+4.26% AUROC提升) 和 MoMu 模型（Su et al. 2022） (+1.54% 提升) 相比，均显著提高了性能。<details>
<summary>Abstract</summary>
Deep learning in computational biochemistry has traditionally focused on molecular graphs neural representations; however, recent advances in language models highlight how much scientific knowledge is encoded in text. To bridge these two modalities, we investigate how molecular property information can be transferred from natural language to graph representations. We study property prediction performance gains after using contrastive learning to align neural graph representations with representations of textual descriptions of their characteristics. We implement neural relevance scoring strategies to improve text retrieval, introduce a novel chemically-valid molecular graph augmentation strategy inspired by organic reactions, and demonstrate improved performance on downstream MoleculeNet property classification tasks. We achieve a +4.26% AUROC gain versus models pre-trained on the graph modality alone, and a +1.54% gain compared to recently proposed molecular graph/text contrastively trained MoMu model (Su et al. 2022).
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)深度学习在计算生物化学中传统上专注于分子图 neural representation; 然而，最近的语言模型发展提出了如何在文本中储存科学知识的问题。为了联系这两种模式，我们研究如何从自然语言中提取分子性质信息并将其转换为图表示。我们使用对比学习将神经图表示与文本描述中的特征表示进行对应。我们还使用神经相关分数策略来改进文本检索，并提出了一种基于有机反应的化学正确分子图增强策略。我们在下游MoleculeNet属性分类任务上达到了+4.26% AUROC提升和+1.54%提升，相比于只使用图模式预训练的模型。
</details></li>
</ul>
<hr>
<h2 id="How-to-Design-and-Deliver-Courses-for-Higher-Education-in-the-AI-Era-Insights-from-Exam-Data-Analysis"><a href="#How-to-Design-and-Deliver-Courses-for-Higher-Education-in-the-AI-Era-Insights-from-Exam-Data-Analysis" class="headerlink" title="How to Design and Deliver Courses for Higher Education in the AI Era: Insights from Exam Data Analysis"></a>How to Design and Deliver Courses for Higher Education in the AI Era: Insights from Exam Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02441">http://arxiv.org/abs/2308.02441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Samer Wazan, Imran Taj, Abdulhadi Shoufan, Romain Laborde, Rémi Venant</li>
<li>For: The paper advocates for the idea that courses and exams in the AI era should be designed based on the strengths and limitations of AI, as well as pedagogical educational objectives.* Methods: The paper explores the strengths and limitations of AI based on current advances in the field, and provides examples of how courses and exams can be designed based on these factors. The paper also describes a pedagogical approach inspired by the Socratic teaching method that was adopted from January 2023 to May 2023.* Results: The paper presents data analysis results of seven ChatGPT-authorized exams conducted between December 2022 and March 2023, which show no correlation between students’ grades and whether or not they use ChatGPT to answer their exam questions. The paper also proposes a new exam system that allows for the application of the pedagogical approach in the AI era.Here is the information in Simplified Chinese text:* For: 这篇论文提出了在人工智能时代，课程和考试应该如何设计，以便符合人工智能的优势和局限性，以及教育目标。* Methods: 论文从现有人工智能技术的发展来探讨人工智能的优势和局限性，并提供了不同领域的示例，如IT、英语和艺术等。论文还描述了一种基于索普朗教学方法的教学方法，从2023年1月至2023年5月进行了应用。* Results: 论文提供了七个使用ChatGPT作为考试工具的考试数据分析结果，显示学生的成绩与使用ChatGPT answering考试问题无关。论文还提出了一种新的考试系统，以便在人工智能时代应用教学方法。<details>
<summary>Abstract</summary>
In this position paper, we advocate for the idea that courses and exams in the AI era have to be designed based on two factors: (1) the strengths and limitations of AI, and (2) the pedagogical educational objectives. Based on insights from the Delors report on education [1], we first address the role of education and recall the main objectives that educational institutes must strive to achieve independently of any technology. We then explore the strengths and limitations of AI, based on current advances in AI. We explain how courses and exams can be designed based on these strengths and limitations of AI, providing different examples in the IT, English, and Art domains. We show how we adopted a pedagogical approach that is inspired from the Socratic teaching method from January 2023 to May 2023. Then, we present the data analysis results of seven ChatGPT-authorized exams conducted between December 2022 and March 2023. Our exam data results show that there is no correlation between students' grades and whether or not they use ChatGPT to answer their exam questions. Finally, we present a new exam system that allows us to apply our pedagogical approach in the AI era.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "AI era" is translated as "人工智能时代" (rénxīng zhìnéng shídài)* "pedagogical educational objectives" is translated as "教育目标" (jiàoyù mùbiāo)* "Delors report" is translated as "德洛尔报告" (déluō'ěr bàogāo)* "Socratic teaching method" is translated as "苏格拉底教学方法" (sūgélādī jíxué fāngfa)* "ChatGPT-authorized exams" is translated as "ChatGPT授权考试" (ChatGPT shèngquán kǎoshì)* "data analysis results" is translated as "数据分析结果" (numbers dàxīn yìjī)Note: The translation is in Simplified Chinese, as requested.
</details></li>
</ul>
<hr>
<h2 id="Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors"><a href="#Model-Predictive-Control-MPC-of-an-Artificial-Pancreas-with-Data-Driven-Learning-of-Multi-Step-Ahead-Blood-Glucose-Predictors" class="headerlink" title="Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors"></a>Model Predictive Control (MPC) of an Artificial Pancreas with Data-Driven Learning of Multi-Step-Ahead Blood Glucose Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12015">http://arxiv.org/abs/2307.12015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Maria Aiello, Mehrad Jaloli, Marzia Cescon</li>
<li>for: 这个论文旨在开发一种基于Linear Time-Varying（LTV）Model Predictive Control（MPC）框架的闭环式胰岛素输液控制算法，用于治疗型1 диабеtes（T1D）。</li>
<li>methods: 这个算法使用了一个数据驱动的多步预测器，并将预测结果用于LTV MPC控制器中。在非线性部分，我们使用了一个Long Short-Term Memory（LSTM）网络，而在线性部分，我们使用了一个线性回归模型。</li>
<li>results: 我们对这两种控制器进行了Simulation比较，并发现我们的LSTM-MPC控制器在三个场景中表现更好，即在常规情况下、随机饭物干扰情况下和降低胰岛素敏感性25%情况下。此外，我们的方法可以更好地预测未来血糖浓度，并且closed-loop性能更好。<details>
<summary>Abstract</summary>
We present the design and \textit{in-silico} evaluation of a closed-loop insulin delivery algorithm to treat type 1 diabetes (T1D) consisting in a data-driven multi-step-ahead blood glucose (BG) predictor integrated into a Linear Time-Varying (LTV) Model Predictive Control (MPC) framework. Instead of identifying an open-loop model of the glucoregulatory system from available data, we propose to directly fit the entire BG prediction over a predefined prediction horizon to be used in the MPC, as a nonlinear function of past input-ouput data and an affine function of future insulin control inputs. For the nonlinear part, a Long Short-Term Memory (LSTM) network is proposed, while for the affine component a linear regression model is chosen. To assess benefits and drawbacks when compared to a traditional linear MPC based on an auto-regressive with exogenous (ARX) input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25$\%$ decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean $\pm$ standard deviation percent time in the range 70-180 [mg/dL] was 74.99 $\pm$ 7.09 vs. 54.15 $\pm$ 14.89, the mean $\pm$ standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78$\pm$8.55 vs. 34.62 $\pm$9.04, while the mean $\pm$ standard deviation percent time in sever hypoglycemia, i.e., $<$ 54 [mg/dl] was 1.00$\pm$3.18 vs. 9.45$\pm$11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种closed-loop胰岛素输液算法，用于治疗型1 диабеtes（T1D），这种算法包括一个数据驱动的多步预测血糖（BG）预测器，integrated into a Linear Time-Varying（LTV） Model Predictive Control（MPC）框架。而不是从可用数据中直接Identify opens loop模型的静脉糖皮肤系统，我们提议直接预测整个BG预测 horizon，作为一个非线性函数，用于MPC中的预测。 For the nonlinear part, a Long Short-Term Memory（LSTM） network is proposed, while for the affine component a linear regression model is chosen. To evaluate the benefits and drawbacks of the proposed LSTM-MPC controller compared to a traditional linear MPC based on an auto-regressive with exogenous（ARX）input model identified from data, we evaluated the proposed LSTM-MPC controller in three simulation scenarios: a nominal case with 3 meals per day, a random meal disturbances case where meals were generated with a recently published meal generator, and a case with 25% decrease in the insulin sensitivity. Further, in all the scenarios, no feedforward meal bolus was administered. For the more challenging random meal generation scenario, the mean ± standard deviation percent time in the range 70-180 [mg/dL] was 74.99 ± 7.09 vs. 54.15 ± 14.89, the mean ± standard deviation percent time in the tighter range 70-140 [mg/dL] was 47.78 ± 8.55 vs. 34.62 ± 9.04, while the mean ± standard deviation percent time in severe hypoglycemia, i.e., <54 [mg/dL] was 1.00 ± 3.18 vs. 9.45 ± 11.71, for our proposed LSTM-MPC controller and the traditional ARX-MPC, respectively. Our approach provided accurate predictions of future glucose concentrations and good closed-loop performances of the overall MPC controller.
</details></li>
</ul>
<hr>
<h2 id="Psy-LLM-Scaling-up-Global-Mental-Health-Psychological-Services-with-AI-based-Large-Language-Models"><a href="#Psy-LLM-Scaling-up-Global-Mental-Health-Psychological-Services-with-AI-based-Large-Language-Models" class="headerlink" title="Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models"></a>Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11991">http://arxiv.org/abs/2307.11991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tin Lai, Yukun Shi, Zicong Du, Jiajie Wu, Ken Fu, Yichao Dou, Ziqi Wang<br>for:The paper aims to provide a novel AI-based system for online psychological consultation, which can assist healthcare professionals in providing timely and professional mental health support.methods:The proposed framework, called Psy-LLM, leverages Large Language Models (LLMs) for question-answering in online psychological consultation. The framework combines pre-trained LLMs with real-world professional Q&amp;A from psychologists and extensively crawled psychological articles.results:The authors evaluated the framework using intrinsic metrics such as perplexity and extrinsic evaluation metrics including human participant assessments of response helpfulness, fluency, relevance, and logic. The results demonstrate the effectiveness of the Psy-LLM framework in generating coherent and relevant answers to psychological questions.<details>
<summary>Abstract</summary>
The demand for psychological counseling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counseling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based system leveraging Large Language Models (LLMs) for question-answering in online psychological consultation. Our framework combines pre-trained LLMs with real-world professional Q&A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and extrinsic evaluation metrics, with human participant assessments of response helpfulness, fluency, relevance, and logic. The results demonstrate the effectiveness of the Psy-LLM framework in generating coherent and relevant answers to psychological questions. This article concludes by discussing the potential of large language models to enhance mental health support through AI technologies in online psychological consultation.
</details>
<details>
<summary>摘要</summary>
“对于心理辅导的需求在最近的几年中有了很大的增长，特别是COVID-19全球大流行，这使得心理健康支持的需求增加了。在这篇研究中，我们提出了Psy-LLM框架，这是一个基于大语言模型（LLM）的人工智能系统，用于在线心理咨询中回答问题。我们的框架结合了预训语言模型和专业心理师的问答，以及大量爬虫的心理文章。Psy-LLM框架作为健康专业人员的前端工具，可以提供即时的回答和心理活动，以减轻病人的压力。同时，它还可以作为寻找紧急案例需要进一步帮助的萤幕工具。我们使用了自类度、流畅度、相关度和逻辑性等内部评估指标，以及人类参与者的评价，来评估Psy-LLM框架的效果。结果显示，Psy-LLM框架可以生成 coherent 和相关的回答心理问题。本文结束时，讨论了大语言模型在线心理咨询中如何通过人工智能技术增强心理健康支持。”
</details></li>
</ul>
<hr>
<h2 id="Sparse-then-Prune-Toward-Efficient-Vision-Transformers"><a href="#Sparse-then-Prune-Toward-Efficient-Vision-Transformers" class="headerlink" title="Sparse then Prune: Toward Efficient Vision Transformers"></a>Sparse then Prune: Toward Efficient Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11988">http://arxiv.org/abs/2307.11988</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yogiprsty/sparse-vit">https://github.com/yogiprsty/sparse-vit</a></li>
<li>paper_authors: Yogi Prasetyo, Novanto Yudistira, Agus Wahyu Widodo</li>
<li>for: 这个研究旨在investigate the possibility of applying Sparse Regularization and Pruning methods to the Vision Transformer architecture for image classification tasks, and explore the trade-off between performance and efficiency.</li>
<li>methods: 这个研究使用了Sparse Regularization和Pruning方法，并在CIFAR-10、CIFAR-100和ImageNet-100 datasets上进行了实验。模型的训练过程包括两部分：预训练和精度调整。预训练使用了ImageNet21K数据，followed by 20 epochs of fine-tuning.</li>
<li>results: 研究发现，当使用CIFAR-100和ImageNet-100数据进行测试时，带有Sparse Regularization的模型可以提高准确率by 0.12%。此外，对带有Sparse Regularization的模型进行截割，可以更好地提高平均准确率。特别是在CIFAR-10数据集上，截割后的模型可以提高准确率by 0.568%，在CIFAR-100和ImageNet-100数据集上提高了1.764%和0.256%。<details>
<summary>Abstract</summary>
The Vision Transformer architecture is a deep learning model inspired by the success of the Transformer model in Natural Language Processing. However, the self-attention mechanism, large number of parameters, and the requirement for a substantial amount of training data still make Vision Transformers computationally burdensome. In this research, we investigate the possibility of applying Sparse Regularization to Vision Transformers and the impact of Pruning, either after Sparse Regularization or without it, on the trade-off between performance and efficiency. To accomplish this, we apply Sparse Regularization and Pruning methods to the Vision Transformer architecture for image classification tasks on the CIFAR-10, CIFAR-100, and ImageNet-100 datasets. The training process for the Vision Transformer model consists of two parts: pre-training and fine-tuning. Pre-training utilizes ImageNet21K data, followed by fine-tuning for 20 epochs. The results show that when testing with CIFAR-100 and ImageNet-100 data, models with Sparse Regularization can increase accuracy by 0.12%. Furthermore, applying pruning to models with Sparse Regularization yields even better results. Specifically, it increases the average accuracy by 0.568% on CIFAR-10 data, 1.764% on CIFAR-100, and 0.256% on ImageNet-100 data compared to pruning models without Sparse Regularization. Code can be accesed here: https://github.com/yogiprsty/Sparse-ViT
</details>
<details>
<summary>摘要</summary>
“当前的视觉 трансформер架构是一种深度学习模型，受到自然语言处理中的Transformer模型的成功所 inspirited。然而，自我对项 mechanism，大量的参数，以及需要大量的训练数据仍然使得视觉 трансформер Computationally burdensome。在这个研究中，我们 investigate了将Sparse Regularization应用到视觉 трансформер架构中，以及对其进行Prune的影响，以进行性能和效率之间的交易。为此，我们将Sparse Regularization和Prune方法应用到视觉 трансформер架构，进行图像分类任务。训练过程包括两个部分：预训练和精练。预训练使用ImageNet21K数据，接着进行20次精练。结果显示，在CIFAR-100和ImageNet-100数据上进行训练时，具有Sparse Regularization的模型可以提高精确率0.12%。此外，对Sparse Regularization的模型进行Prune操作，产生了更好的结果。具体来说，它可以在CIFAR-10数据上提高平均精确率0.568%，CIFAR-100数据上提高1.764%，ImageNet-100数据上提高0.256%。软件可以在以下github上取得：https://github.com/yogiprsty/Sparse-ViT”
</details></li>
</ul>
<hr>
<h2 id="Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels"><a href="#Why-Is-Prompt-Tuning-for-Vision-Language-Models-Robust-to-Noisy-Labels" class="headerlink" title="Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?"></a>Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11978">http://arxiv.org/abs/2307.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cewu/ptnl">https://github.com/cewu/ptnl</a></li>
<li>paper_authors: Cheng-En Wu, Yu Tian, Haichao Yu, Heng Wang, Pedro Morgado, Yu Hen Hu, Linjie Yang</li>
<li>for: 这个论文主要研究了CLIP视觉语言模型如何在几行示例下适应新的分类任务，以及这种示例调整过程对噪声标签的Robustness。</li>
<li>methods: 该论文使用了CLIP视觉语言模型，通过几行示例进行示例调整，并进行了广泛的实验研究以探索这种示例调整过程中的关键因素。</li>
<li>results: 研究发现， CLIP的示例调整过程具有很高的Robustness，这主要归因于模型中的固定类名token提供了强制的Regularization，以及CLIP学习的强大预训练图像文本嵌入，帮助提高图像分类的预测精度。<details>
<summary>Abstract</summary>
Vision-language models such as CLIP learn a generic text-image embedding from large-scale training data. A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises. This intrigues us to study the key reasons contributing to the robustness of the prompt tuning paradigm. We conducted extensive experiments to explore this property and find the key factors are: 1) the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; 2) the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. Further, we demonstrate that noisy zero-shot predictions from CLIP can be used to tune its own prompt, significantly enhancing prediction accuracy in the unsupervised setting. The code is available at https://github.com/CEWu/PTNL.
</details>
<details>
<summary>摘要</summary>
视力语模型如CLIP通过大规模训练学习通用文本图像嵌入。一个视力语模型可以通过几招提示调整来适应新的分类任务。我们发现这种提示调整过程具有很高的鲁棒性，使我们感到惊叹。我们进行了广泛的实验研究这种性能的原因，并发现关键因素有：1）固定的类名token提供了模型优化的强制 régularization，减少了噪声样本引起的梯度; 2）通过多样化和通用的网络数据学习的强大预训练图像文本嵌入，为图像分类提供了强大的先验知识。此外，我们示出了CLIP的噪声零时预测可以用来调整其自己的提示，significantly enhance预测精度在无监督Setting下。代码可以在https://github.com/CEWu/PTNL中找到。
</details></li>
</ul>
<hr>
<h2 id="Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection"><a href="#Multi-representations-Space-Separation-based-Graph-level-Anomaly-aware-Detection" class="headerlink" title="Multi-representations Space Separation based Graph-level Anomaly-aware Detection"></a>Multi-representations Space Separation based Graph-level Anomaly-aware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.12994">http://arxiv.org/abs/2307.12994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu Lin, Haonan Gong, Mingkang Li, Zitong Wang, Yue Zhang, Xuexiong Luo</li>
<li>for: 本研究的目标是检测图数据中异常的图形。</li>
<li>methods: 我们提出了一种基于多表示空间分离的图级异常检测框架，以考虑不同类型的异常图形之间的重要性。我们还设计了一个异常检测模块，以learn异常图形之间的特定权重。</li>
<li>results: 我们对基eline方法进行了广泛的评估，并获得了显著的效果。<details>
<summary>Abstract</summary>
Graph structure patterns are widely used to model different area data recently. How to detect anomalous graph information on these graph data has become a popular research problem. The objective of this research is centered on the particular issue that how to detect abnormal graphs within a graph set. The previous works have observed that abnormal graphs mainly show node-level and graph-level anomalies, but these methods equally treat two anomaly forms above in the evaluation of abnormal graphs, which is contrary to the fact that different types of abnormal graph data have different degrees in terms of node-level and graph-level anomalies. Furthermore, abnormal graphs that have subtle differences from normal graphs are easily escaped detection by the existing methods. Thus, we propose a multi-representations space separation based graph-level anomaly-aware detection framework in this paper. To consider the different importance of node-level and graph-level anomalies, we design an anomaly-aware module to learn the specific weight between them in the abnormal graph evaluation process. In addition, we learn strictly separate normal and abnormal graph representation spaces by four types of weighted graph representations against each other including anchor normal graphs, anchor abnormal graphs, training normal graphs, and training abnormal graphs. Based on the distance error between the graph representations of the test graph and both normal and abnormal graph representation spaces, we can accurately determine whether the test graph is anomalous. Our approach has been extensively evaluated against baseline methods using ten public graph datasets, and the results demonstrate its effectiveness.
</details>
<details>
<summary>摘要</summary>
graph结构模式在当今数据中广泛应用。如何检测图数据中的异常信息已成为一个流行的研究问题。本研究的目标在于特定的问题：如何在图集中检测异常图。前一些研究发现，异常图主要表现为节点水平和图水平异常，但这些方法在评估异常图时平等对待这两种异常形态，这与实际情况不符。此外，异常图具有微妙的差异，容易被现有方法检测掉。因此，我们提出了一个基于多个表示空间分离的图级异常检测框架。为了考虑节点水平和图水平异常的不同重要性，我们设计了一个异常检测模块，以学习特定的节点水平和图水平异常权重。此外，我们通过四种不同的权重图表示对彼此进行学习，以学习纯正的常见图和异常图表示空间。通过测试图表示空间与常见图表示空间和异常图表示空间之间的距离错误来准确判断测试图是否异常。我们的方法与基准方法进行比较使用了十个公共图数据集，结果表明其效果批示。
</details></li>
</ul>
<hr>
<h2 id="Pyrus-Base-An-Open-Source-Python-Framework-for-the-RoboCup-2D-Soccer-Simulation"><a href="#Pyrus-Base-An-Open-Source-Python-Framework-for-the-RoboCup-2D-Soccer-Simulation" class="headerlink" title="Pyrus Base: An Open Source Python Framework for the RoboCup 2D Soccer Simulation"></a>Pyrus Base: An Open Source Python Framework for the RoboCup 2D Soccer Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.16875">http://arxiv.org/abs/2307.16875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyrus2d/pyrus2d">https://github.com/cyrus2d/pyrus2d</a></li>
<li>paper_authors: Nader Zare, Aref Sayareh, Omid Amini, Mahtab Sarvmaili, Arad Firouzkouhi, Stan Matwin, Amilcar Soares</li>
<li>For: The paper is written to introduce Pyrus, a Python base code for the RoboCup Soccer Simulation 2D (SS2D) league, to provide a more accessible and efficient platform for researchers to develop their ideas and integrate machine learning algorithms into their teams.* Methods: The paper uses C++ base codes as the foundation and develops Pyrus, a Python base code, to overcome the challenges of C++ base codes and provide a more user-friendly platform for researchers.* Results: Pyrus is introduced as a powerful baseline for developing machine learning concepts in SS2D, and it is open-source and publicly available under MIT License on GitHub, encouraging researchers to efficiently develop their ideas and integrate machine learning algorithms into their teams.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了介绍PYRUS，一个基于Python的RoboCup足球模拟2D（SS2D）联赛的基础代码，以便更好地为研究人员提供一个访问ibility和效率的平台，以便他们可以更加快速地开发自己的想法并将机器学习算法integrated into their teams。* Methods: 这篇论文使用C++基础代码作为基础，然后开发了PYRUS，一个基于Python的基础代码，以解决C++基础代码的挑战，并提供一个更加用户友好的平台 для研究人员。* Results: PYRUS被引入为SS2D中的一个强大基线，可以帮助研究人员更加快速地开发自己的想法并integrated machine learning算法into their teams，PYRUS的基础代码公开发布在GitHub上，并且以MIT许可证进行公共可用，以便更多的研究人员可以参与到这个项目中。<details>
<summary>Abstract</summary>
Soccer, also known as football in some parts of the world, involves two teams of eleven players whose objective is to score more goals than the opposing team. To simulate this game and attract scientists from all over the world to conduct research and participate in an annual computer-based soccer world cup, Soccer Simulation 2D (SS2D) was one of the leagues initiated in the RoboCup competition. In every SS2D game, two teams of 11 players and one coach connect to the RoboCup Soccer Simulation Server and compete against each other. Over the past few years, several C++ base codes have been employed to control agents' behavior and their communication with the server. Although C++ base codes have laid the foundation for the SS2D, developing them requires an advanced level of C++ programming. C++ language complexity is a limiting disadvantage of C++ base codes for all users, especially for beginners. To conquer the challenges of C++ base codes and provide a powerful baseline for developing machine learning concepts, we introduce Pyrus, the first Python base code for SS2D. Pyrus is developed to encourage researchers to efficiently develop their ideas and integrate machine learning algorithms into their teams. Pyrus base is open-source code, and it is publicly available under MIT License on GitHub
</details>
<details>
<summary>摘要</summary>
足球（也称为足球在一些地方）是一种需要两支队伍的 eleven 名球员，目标是将更多的入球击败对手队伍。为了模拟这场游戏并吸引全球科学家来参与研究和参加每年的计算机基于足球世界杯赛，Football Simulation 2D（SS2D）是RoboCup竞赛中的一个赛事。在每场 SS2D 比赛中，两支队伍的 11 名球员和一位教练通过RoboCup足球 simulate Server 竞争对对手。过去几年，一些 C++ 基础代码被使用来控制代理的行为和与服务器的通信。虽然 C++ 基础代码已经为 SS2D 提供了基础，但是开发它们需要高级的 C++ 编程技能。 C++ 语言复杂性是 C++ 基础代码的限制性，特别是对所有用户来说，尤其是对初学者来说。为了 conquering C++ 基础代码的挑战和提供一个机器学习概念的强大基础，我们引入了 Pyrus，SS2D 的第一个 Python 基础代码。Pyrus 是为了鼓励研究人员尽可能快速地发展他们的想法，并将机器学习算法 integrate 到他们的队伍中。Pyrus 的基础代码是开源的，公开在 GitHub 上，并以 MIT 许可证进行公共发布。
</details></li>
</ul>
<hr>
<h2 id="On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs"><a href="#On-Robot-Bayesian-Reinforcement-Learning-for-POMDPs" class="headerlink" title="On-Robot Bayesian Reinforcement Learning for POMDPs"></a>On-Robot Bayesian Reinforcement Learning for POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11954">http://arxiv.org/abs/2307.11954</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Nguyen, Sammie Katt, Yuchen Xiao, Christopher Amato</li>
<li>for: 本研究旨在提高机器人学习的效率，因为收集数据的成本很高。</li>
<li>methods: 本paper使用权重学习（BRL）方法，利用专家知识和有效的算法来解决机器人学习的问题。</li>
<li>results: 本paper在两个人机交互任务中实现了近乎最佳性能，只需要几个实际世界话语。视频证明可以在<a target="_blank" rel="noopener" href="https://youtu.be/H9xp60ngOes%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://youtu.be/H9xp60ngOes中找到。</a><details>
<summary>Abstract</summary>
Robot learning is often difficult due to the expense of gathering data. The need for large amounts of data can, and should, be tackled with effective algorithms and leveraging expert information on robot dynamics. Bayesian reinforcement learning (BRL), thanks to its sample efficiency and ability to exploit prior knowledge, is uniquely positioned as such a solution method. Unfortunately, the application of BRL has been limited due to the difficulties of representing expert knowledge as well as solving the subsequent inference problem. This paper advances BRL for robotics by proposing a specialized framework for physical systems. In particular, we capture this knowledge in a factored representation, then demonstrate the posterior factorizes in a similar shape, and ultimately formalize the model in a Bayesian framework. We then introduce a sample-based online solution method, based on Monte-Carlo tree search and particle filtering, specialized to solve the resulting model. This approach can, for example, utilize typical low-level robot simulators and handle uncertainty over unknown dynamics of the environment. We empirically demonstrate its efficiency by performing on-robot learning in two human-robot interaction tasks with uncertainty about human behavior, achieving near-optimal performance after only a handful of real-world episodes. A video of learned policies is at https://youtu.be/H9xp60ngOes.
</details>
<details>
<summary>摘要</summary>
机器人学习往往困难，主要是因为获取数据的成本高昂。为了解决这个问题，我们需要使用有效的算法和利用机器人动力学专家的知识。泛bayesian学习（BRL）因其样本效率高和能够利用先验知识的特点，成为一种有优势的解决方案。然而，BRL在应用中受到了知识表示和推理问题的限制。这篇论文提出了一种特有的框架，用于解决机器人物理系统中的问题。我们捕捉了专家知识，并证明 posterior 会分解为类似的形式，最后将模型形式化为 bayesian 框架。我们then introduces 一种基于 Monte-Carlo 搜索和粒子筛选的在线解决方法，特化用于解决 resulting 模型。这种方法可以利用典型的低级机器人模拟器，并处理不确定环境中的动力学不确定性。我们实验表明，这种方法可以在两个人机器人互动任务中达到近似优化性，只需要几十个真实世界 episoden。有关学习的视频可以在 https://youtu.be/H9xp60ngOes 中找到。
</details></li>
</ul>
<hr>
<h2 id="Pathology-and-genomics-Multimodal-Transformer-for-Survival-Outcome-Prediction"><a href="#Pathology-and-genomics-Multimodal-Transformer-for-Survival-Outcome-Prediction" class="headerlink" title="Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction"></a>Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11952">http://arxiv.org/abs/2307.11952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cassie07/pathomics">https://github.com/cassie07/pathomics</a></li>
<li>paper_authors: Kexin Ding, Mu Zhou, Dimitris N. Metaxas, Shaoting Zhang</li>
<li>for: 这个研究旨在提高colon和rectum癌 survival outcome预测，通过结合pathology和genomics信息。</li>
<li>methods: 该研究提出了一种多Modal transformer（PathOmics），通过不监督预训练来捕捉组织微环境的内在相互作用，并将这些信息与许多 genomics数据（例如mRNA-sequence、copy number variant和methylation）融合。</li>
<li>results: 研究表明，提出的方法可以在TCGA colon和RECTUM癌组织中表现出优异，并且超越了现有的研究。此外，该方法还可以使用有限的finetunedamples进行数据效率的分析，从而提高预测结果的准确性。<details>
<summary>Abstract</summary>
Survival outcome assessment is challenging and inherently associated with multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer. Enabling multimodal analytics promises to reveal novel predictive patterns of patient outcomes. In this study, we propose a multimodal transformer (PathOmics) integrating pathology and genomics insights into colon-related cancer survival prediction. We emphasize the unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation). After the multimodal knowledge aggregation in pretraining, our task-specific model finetuning could expand the scope of data utility applicable to both multi- and single-modal data (e.g., image- or genomics-only). We evaluate our approach on both TCGA colon and rectum cancer cohorts, showing that the proposed approach is competitive and outperforms state-of-the-art studies. Finally, our approach is desirable to utilize the limited number of finetuned samples towards data-efficient analytics for survival outcome prediction. The code is available at https://github.com/Cassie07/PathOmics.
</details>
<details>
<summary>摘要</summary>
生存结果评估在癌症中是挑战性的，与多种临床因素（例如成像和基因表达 markers）相关。启用多modal分析承诺可以揭示新的预测性模式。在这项研究中，我们提出了一种多modal transformer（PathOmics），将pathology和基因学信息集成到colon相关癌症生存预测中。我们强调了无监督预训来捕捉材料微环境的内在交互。经过多modal知识聚合的预训后，我们的任务特定模型精度调整可以扩大数据的可用范围，包括多modal数据（例如图像或基因数据）以及单modal数据（例如图像或基因数据）。我们在TCGAcolon和rectum癌症群体上评估了我们的方法，并显示了我们的方法与当前最佳实践相比较竞争。最后，我们的方法可以使用有限的精度调整样本来实现数据效率的分析。代码可以在https://github.com/Cassie07/PathOmics中找到。
</details></li>
</ul>
<hr>
<h2 id="HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions"><a href="#HIQL-Offline-Goal-Conditioned-RL-with-Latent-States-as-Actions" class="headerlink" title="HIQL: Offline Goal-Conditioned RL with Latent States as Actions"></a>HIQL: Offline Goal-Conditioned RL with Latent States as Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11949">http://arxiv.org/abs/2307.11949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seohongpark/hiql">https://github.com/seohongpark/hiql</a></li>
<li>paper_authors: Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine</li>
<li>for: 本研究旨在开发一种基于不监督学习的目标决策策略，能够从大量未标注数据中学习。</li>
<li>methods: 该方法使用一个action-free值函数，并通过层次分解来学习两个策略：一个高级策略用于处理状态作为行为，预测子目标，以及一个低级策略用于达成这个子目标。</li>
<li>results: 该方法可以解决长期任务，并可以在高维图像观察中进行扩展。 Code可以在<a target="_blank" rel="noopener" href="https://seohong.me/projects/hiql/%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://seohong.me/projects/hiql/上下载。</a><details>
<summary>Abstract</summary>
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy that treats states as actions and predicts (a latent representation of) a subgoal and a low-level policy that predicts the action for reaching this subgoal. Through analysis and didactic examples, we show how this hierarchical decomposition makes our method robust to noise in the estimated value function. We then apply our method to offline goal-reaching benchmarks, showing that our method can solve long-horizon tasks that stymie prior methods, can scale to high-dimensional image observations, and can readily make use of action-free data. Our code is available at https://seohong.me/projects/hiql/
</details>
<details>
<summary>摘要</summary>
“无监督预训”最近已经成为计算机视觉和自然语言处理领域的基础。在征激学习（RL）中，目标conditioned RL可能提供一种自愿supervised的方法，将大量的无回奖数据给利用。然而，建立有效的对目标conditioned RL算法，从多元的过去数据中学习，是一个挑战。这是因为，过去的目标变得越远，预测其价值函数的专业程度就越高。然而，目标 raggiungere问题具有结构，即到达较远的目标需要先通过更近的子目标。这种结构可以非常有用，因为评估靠近目标的动作较 easier than评估更远的目标。基于这个想法，我们提出了一个层次架构的对目标conditioned RL算法。我们使用一个不含动作的价值函数，学习两个政策：一个高层政策，将状态视为动作，预测（一个隐藏表示）子目标，以及一个低层政策，预测将用来达到子目标的动作。我们通过分析和示例，显示了我们的层次分解对于错误估计价值函数的影响。我们然后将我们的方法应用到过去目标 raggiungere测试 benchmark，展示了我们的方法可以解决长期任务，可以扩展到高维度的影像观察，并可以轻松地使用无动作数据。我们的代码可以在https://seohong.me/projects/hiql/ 获取。”
</details></li>
</ul>
<hr>
<h2 id="Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors"><a href="#Selective-Perception-Optimizing-State-Descriptions-with-Reinforcement-Learning-for-Language-Model-Actors" class="headerlink" title="Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors"></a>Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11922">http://arxiv.org/abs/2307.11922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kolby Nottingham, Yasaman Razeghi, Kyungmin Kim, JB Lanier, Pierre Baldi, Roy Fox, Sameer Singh</li>
<li>for: 这个论文旨在探讨如何使用自然语言处理技术来帮助语言模型在决策过程中更好地处理环境状态信息。</li>
<li>methods: 这篇论文提出了一种名为“布林德”（BLINDER）的方法，它通过学习任务条件下的状态描述值函数来自动选择简洁的状态描述。</li>
<li>results: 实验结果表明，使用布林德方法可以提高任务成功率，降低输入大小和计算成本，并在不同的语言模型actor之间进行泛化。<details>
<summary>Abstract</summary>
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and generalizes between LLM actors.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）在机器人和游戏等领域被应用为序列决策任务的演员，利用其总体世界知识和规划能力。然而，先前的研究几乎没有探讨 LLM 演员所接受的环境状态信息是如何传递给语言中。描述高维状态的详细信息可能会降低性能和提高 LLM 演员的推理成本。先前的 LLM 演员通常通过靠手工设计、任务特定协议来确定要关注哪些状态特征和哪些可以被忽略。在这项工作中，我们提出了 Brief Language INputs for DEcision-making Responses（BLINDER）方法，通过学习任务条件下的状态描述值函数来自动选择简洁的状态描述。我们在 NetHack 游戏和机器人 manipulate 任务上评估 BLINDER。我们的方法可以提高任务成功率，降低输入大小和计算成本，并在不同的 LLM 演员之间进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Bibliometric-Analysis-of-Publisher-and-Journal-Instructions-to-Authors-on-Generative-AI-in-Academic-and-Scientific-Publishing"><a href="#Bibliometric-Analysis-of-Publisher-and-Journal-Instructions-to-Authors-on-Generative-AI-in-Academic-and-Scientific-Publishing" class="headerlink" title="Bibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing"></a>Bibliometric Analysis of Publisher and Journal Instructions to Authors on Generative-AI in Academic and Scientific Publishing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11918">http://arxiv.org/abs/2307.11918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Conner Ganjavi, Michael B. Eppler, Asli Pekcan, Brett Biedermann, Andre Abreu, Gary S. Collins, Inderbir S. Gill, Giovanni E. Cacciamani</li>
<li>for: The paper aims to determine the extent and content of guidance for authors regarding the use of generative-AI (GAI), Generative Pretrained models (GPTs), and Large Language Models (LLMs) powered tools among the top 100 academic publishers and journals in science.</li>
<li>methods: The study screened the websites of the top 100 publishers and journals from May 19th to May 20th, 2023, to identify guidance on the use of GAI.</li>
<li>results: The study found that 17% of the largest 100 publishers and 70% of the top 100 journals provided guidance on the use of GAI. Most publishers and journals prohibited the inclusion of GAI as an author, but there was variability in how to disclose the use of GAI and in the allowable uses of GAI. Some top publishers and journals lacked guidance on the use of GAI by authors, and there was a need for standardized guidelines to protect the integrity of scientific output.Here are the three key points in Simplified Chinese text:</li>
<li>for: 这篇论文目的是检查科学领域前100家出版社和期刊的作者指南中对生成AI（GAI）、生成预训模型（GPTs）和大语言模型（LLMs）Powered工具的使用。</li>
<li>methods: 这个研究从5月19日至5月20日，对前100家出版社和期刊的官方网站进行屏幕，以找到关于GAI的指南。</li>
<li>results: 研究发现，前100家出版社中有17%提供了GAI的指南，而前100家期刊中有70%提供了指南。大多数出版社和期刊禁止了GAI作为作者的包含，但是有一定的变化在披露GAI的方式和允许的GAI使用方式。一些顶尖出版社和期刊缺乏关于GAI的指南，需要有标准化的指南来保护科学输出的正当性。<details>
<summary>Abstract</summary>
We aim to determine the extent and content of guidance for authors regarding the use of generative-AI (GAI), Generative Pretrained models (GPTs) and Large Language Models (LLMs) powered tools among the top 100 academic publishers and journals in science. The websites of these publishers and journals were screened from between 19th and 20th May 2023. Among the largest 100 publishers, 17% provided guidance on the use of GAI, of which 12 (70.6%) were among the top 25 publishers. Among the top 100 journals, 70% have provided guidance on GAI. Of those with guidance, 94.1% of publishers and 95.7% of journals prohibited the inclusion of GAI as an author. Four journals (5.7%) explicitly prohibit the use of GAI in the generation of a manuscript, while 3 (17.6%) publishers and 15 (21.4%) journals indicated their guidance exclusively applies to the writing process. When disclosing the use of GAI, 42.8% of publishers and 44.3% of journals included specific disclosure criteria. There was variability in guidance of where to disclose the use of GAI, including in the methods, acknowledgments, cover letter, or a new section. There was also variability in how to access GAI guidance and the linking of journal and publisher instructions to authors. There is a lack of guidance by some top publishers and journals on the use of GAI by authors. Among those publishers and journals that provide guidance, there is substantial heterogeneity in the allowable uses of GAI and in how it should be disclosed, with this heterogeneity persisting among affiliated publishers and journals in some instances. The lack of standardization burdens authors and threatens to limit the effectiveness of these regulations. There is a need for standardized guidelines in order to protect the integrity of scientific output as GAI continues to grow in popularity.
</details>
<details>
<summary>摘要</summary>
我们目的是确定杂志和出版商在使用生成AI（GAI）、生成预训练模型（GPT）和大语言模型（LLM）激活的指导内容和范围。我们在2023年5月19日至20日检查了前100名学术出版商和杂志的网站。 Among the largest 100 publishers, 17% provided guidance on the use of GAI, of which 12 (70.6%) were among the top 25 publishers. Among the top 100 journals, 70% have provided guidance on GAI. Of those with guidance, 94.1% of publishers and 95.7% of journals prohibited the inclusion of GAI as an author. Four journals (5.7%) explicitly prohibit the use of GAI in the generation of a manuscript, while 3 (17.6%) publishers and 15 (21.4%) journals indicated their guidance exclusively applies to the writing process. When disclosing the use of GAI, 42.8% of publishers and 44.3% of journals included specific disclosure criteria. There was variability in guidance of where to disclose the use of GAI, including in the methods, acknowledgments, cover letter, or a new section. There was also variability in how to access GAI guidance and the linking of journal and publisher instructions to authors. There is a lack of guidance by some top publishers and journals on the use of GAI by authors. Among those publishers and journals that provide guidance, there is substantial heterogeneity in the allowable uses of GAI and in how it should be disclosed, with this heterogeneity persisting among affiliated publishers and journals in some instances. The lack of standardization burdens authors and threatens to limit the effectiveness of these regulations. There is a need for standardized guidelines in order to protect the integrity of scientific output as GAI continues to grow in popularity.
</details></li>
</ul>
<hr>
<h2 id="Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning"><a href="#Hindsight-DICE-Stable-Credit-Assignment-for-Deep-Reinforcement-Learning" class="headerlink" title="Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning"></a>Hindsight-DICE: Stable Credit Assignment for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11897">http://arxiv.org/abs/2307.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skandavaidyanath/credit-assignment">https://github.com/skandavaidyanath/credit-assignment</a></li>
<li>paper_authors: Akash Velu, Skanda Vaidyanath, Dilip Arumugam</li>
<li>for: 增强奖励学习 Agent 在缺乏评价反馈的环境中表现，尤其是在长期行为路径上只有单个终态反馈信号，导致奖励学习Agent 困难地归因到特定的行为步骤。</li>
<li>methods: 我们采用了现有的重要性抽象估计技术来改进基eline方法，以提高稳定性和效率。</li>
<li>results: 我们的方法可以在各种环境中稳定、高效地学习，并且可以缓解奖励学习Agent 在奖励分配问题上的困难。<details>
<summary>Abstract</summary>
Oftentimes, environments for sequential decision-making problems can be quite sparse in the provision of evaluative feedback to guide reinforcement-learning agents. In the extreme case, long trajectories of behavior are merely punctuated with a single terminal feedback signal, leading to a significant temporal delay between the observation of a non-trivial reward and the individual steps of behavior culpable for achieving said reward. Coping with such a credit assignment challenge is one of the hallmark characteristics of reinforcement learning. While prior work has introduced the concept of hindsight policies to develop a theoretically moxtivated method for reweighting on-policy data by impact on achieving the observed trajectory return, we show that these methods experience instabilities which lead to inefficient learning in complex environments. In this work, we adapt existing importance-sampling ratio estimation techniques for off-policy evaluation to drastically improve the stability and efficiency of these so-called hindsight policy methods. Our hindsight distribution correction facilitates stable, efficient learning across a broad range of environments where credit assignment plagues baseline methods.
</details>
<details>
<summary>摘要</summary>
常常，决策问题的环境很少提供评价反馈来引导强化学习代理人。在极端情况下，长期行为只有单个终端反馈信号，从而导致获得非致命奖励的步骤之间的时间延迟。处理这种奖励分配挑战是强化学习的一个标志特征。而优先作业已经介绍了使用影响实现观察路径返回的奖励重要性权重法，但这些方法会导致不稳定性，从而降低复杂环境中学习的效率。在这种情况下，我们采用现有的不当重要性评估技术来重要性权重法，以改善稳定性和效率。我们的往事分布修正方法可以在各种奖励分配问题中稳定、高效地学习。
</details></li>
</ul>
<hr>
<h2 id="On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise"><a href="#On-the-Vulnerability-of-Fairness-Constrained-Learning-to-Malicious-Noise" class="headerlink" title="On the Vulnerability of Fairness Constrained Learning to Malicious Noise"></a>On the Vulnerability of Fairness Constrained Learning to Malicious Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11892">http://arxiv.org/abs/2307.11892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avrim Blum, Princewill Okoroafor, Aadirupa Saha, Kevin Stangl</li>
<li>for: 本文研究了对小量恶意噪声的抗性性别平等学习。</li>
<li>methods: 本文使用了随机分类器来减轻恶意噪声的影响。</li>
<li>results: 研究发现，允许随机分类器时，性别平等学习对小量恶意噪声的抗性较为良好，例如对于人口均衡性，可以具有$\Theta(\alpha)$的准确率损失，与无性别约束的最好情况相当。对于平等机会性，可以具有$O(\sqrt{\alpha})$的准确率损失，并给出了匹配的下界$\Omega(\sqrt{\alpha})$。与 Konstantinov 和 Lampert（2021）的研究相比，这些结果表明性别平等学习对小量恶意噪声的抗性较为优秀。此外，本文还考虑了其他的公平性定义，包括平等机会性和均衡性。对这些公平性定义，残余准确率分布在$O(\alpha)$, $O(\sqrt{\alpha})$和$O(1)$三个自然区间内。这些结果为性别平等学习对 adversarial 噪声的抗性提供了更细致的视角。<details>
<summary>Abstract</summary>
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple "tricks" an adversary can use to amplify his power. We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.
</details>
<details>
<summary>摘要</summary>
我们考虑了公平性条件下的学习的易受攻击性。 Konstantinov 和 Lampert (2021) 开始了这个研究，并发现了一些数据分布下，任何合法的学习者都会受到高度易受攻击性的影响，当集合大小不对称时。 在这里，我们提供了一个更有希望的看法，表明如果允许随机分类器， то alors the landscape 会是非常复杂的。 例如，对于人口均衡，我们显示可以允许仅有 $\Theta(\alpha)$ 的精度损失，其中 $\alpha$ 是邪恶噪音率，与不具有公平性限制的情况相同。 对于平等机会，我们显示可以允许 $O(\sqrt{\alpha})$ 的精度损失，并提供了匹配的 $\Omega(\sqrt{\alpha})$ 下界。 与 Konstantinov 和 Lampert (2021) 的结果相比，我们的结果显示，允许随机分类器后，损失精度会分布在三个自然的 режимах $O(\alpha)$, $O(\sqrt{\alpha})$ 和 $O(1)$。 这些结果提供了一个更细部的看法，对于公平性限制下的学习对于噪音训练数据的敏感性。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Document-Analytics-for-Banking-Process-Automation"><a href="#Multimodal-Document-Analytics-for-Banking-Process-Automation" class="headerlink" title="Multimodal Document Analytics for Banking Process Automation"></a>Multimodal Document Analytics for Banking Process Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11845">http://arxiv.org/abs/2307.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Gerling, Stefan Lessmann</li>
<li>For: This paper aims to understand the potential of advanced document analytics, specifically using multimodal models, in banking processes to improve operational efficiency and enhance process efficiency.* Methods: The paper uses a comprehensive analysis of the diverse banking document landscape, highlighting opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. The study also employs natural language processing (NLP) techniques, including LayoutXLM, a cross-lingual, multimodal, pre-trained model, to analyze diverse documents in the banking sector.* Results: The study achieves an overall F1 score performance of around 80% on German company register extracts, demonstrating the efficiency of LayoutXLM. Additionally, the study finds that over 75% F1 score can be achieved with only 30% of the training data, highlighting the benefits of integrating image information and the potential for real-world applicability and benefits of multimodal models within banking.<details>
<summary>Abstract</summary>
In response to growing FinTech competition and the need for improved operational efficiency, this research focuses on understanding the potential of advanced document analytics, particularly using multimodal models, in banking processes. We perform a comprehensive analysis of the diverse banking document landscape, highlighting the opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. Building on the rapidly evolving field of natural language processing (NLP), we illustrate the potential of models such as LayoutXLM, a cross-lingual, multimodal, pre-trained model, for analyzing diverse documents in the banking sector. This model performs a text token classification on German company register extracts with an overall F1 score performance of around 80\%. Our empirical evidence confirms the critical role of layout information in improving model performance and further underscores the benefits of integrating image information. Interestingly, our study shows that over 75% F1 score can be achieved with only 30% of the training data, demonstrating the efficiency of LayoutXLM. Through addressing state-of-the-art document analysis frameworks, our study aims to enhance process efficiency and demonstrate the real-world applicability and benefits of multimodal models within banking.
</details>
<details>
<summary>摘要</summary>
响应金融科技竞争的增长和业务效率的需求，这项研究专注于理解进步的文档分析技术在银行业务中的潜在优势。我们进行了银行文档多样化领域的全面分析，并指出了自动化和高级分析技术的可能性，以提高客户业务的效率。基于自然语言处理（NLP）领域的快速发展，我们介绍了 LayoutXLM 模型，这是一种跨语言、多modal、预训练的模型，可以分析银行业务中的多种文档。这个模型在德国公司注册报表EXTRACTS上进行文本符号分类，其总 F1 分数为约 80%。我们的实证证明了文档中的布局信息对模型性能的重要性，并进一步强调了将图像信息integrated的利好。奇妙的是，我们的研究表明，只使用 30% 的训练数据，可以达到超过 75% F1 分数，这表明 LayoutXLM 的效率。通过对现代文档分析框架进行调查，我们的研究旨在提高业务效率，并证明在银行业务中的多Modal模型的实际可用性和优势。
</details></li>
</ul>
<hr>
<h2 id="eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review"><a href="#eXplainable-Artificial-Intelligence-XAI-in-age-prediction-A-systematic-review" class="headerlink" title="eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review"></a>eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.13704">http://arxiv.org/abs/2307.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alena Kalyakulina, Igor Yusipov</li>
<li>for: 这篇论文旨在介绍Explainable Artificial Intelligence（XAI）在年龄预测任务中的应用。</li>
<li>methods: 论文使用了多种XAI方法，包括深度学习模型和特征选择技术。</li>
<li>results: 论文通过对多个身体系统的研究，发现XAI可以帮助提高年龄预测的准确率和可解释性。<details>
<summary>Abstract</summary>
eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现代人工智能（XAI）已成为机器学习的重要和必需的一部分，允许解释模型的预测结果。XAI特别在高风险应用中需要，如医疗领域，人工智能系统的决策对人生命有着重要的影响。一个医学研究领域是年龄预测和衰老病症的生物标志物的预测。然而，XAI在年龄预测任务中的角色尚未得到直接探讨。在这篇评论中，我们讨论了XAI方法在年龄预测任务中的应用。我们按照身体系统进行了系统性的综述，并讨论了医疗应用中XAI的利点，特别是在年龄预测领域。
</details></li>
</ul>
<hr>
<h2 id="HybridAugment-Unified-Frequency-Spectra-Perturbations-for-Model-Robustness"><a href="#HybridAugment-Unified-Frequency-Spectra-Perturbations-for-Model-Robustness" class="headerlink" title="HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness"></a>HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11823">http://arxiv.org/abs/2307.11823</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mkyucel/hybrid_augment">https://github.com/mkyucel/hybrid_augment</a></li>
<li>paper_authors: Mehmet Kerim Yucel, Ramazan Gokberk Cinbis, Pinar Duygulu</li>
<li>for: 提高Convolutional Neural Networks (CNN)对分布shift的泛化性能</li>
<li>methods: 提出了一种简单 yet effective的数据增强方法 HybridAugment，以减少CNN对高频组件的依赖，提高其 robustness，保持清晰率高</li>
<li>results: HybridAugment和HybridAugment++在CIFAR-10&#x2F;100和ImageNet上达到或超过了现状的clean accuracy，在ImageNet-C、CIFAR-10-C和CIFAR-100-C中的损坏测试中达到或超过了现状，在CIFAR-10上的抗击性和多种数据集上的out-of-distribution检测中达到了竞争水平<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNN) are known to exhibit poor generalization performance under distribution shifts. Their generalization have been studied extensively, and one line of work approaches the problem from a frequency-centric perspective. These studies highlight the fact that humans and CNNs might focus on different frequency components of an image. First, inspired by these observations, we propose a simple yet effective data augmentation method HybridAugment that reduces the reliance of CNNs on high-frequency components, and thus improves their robustness while keeping their clean accuracy high. Second, we propose HybridAugment++, which is a hierarchical augmentation method that attempts to unify various frequency-spectrum augmentations. HybridAugment++ builds on HybridAugment, and also reduces the reliance of CNNs on the amplitude component of images, and promotes phase information instead. This unification results in competitive to or better than state-of-the-art results on clean accuracy (CIFAR-10/100 and ImageNet), corruption benchmarks (ImageNet-C, CIFAR-10-C and CIFAR-100-C), adversarial robustness on CIFAR-10 and out-of-distribution detection on various datasets. HybridAugment and HybridAugment++ are implemented in a few lines of code, does not require extra data, ensemble models or additional networks.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）在分布Shift下表现不佳。其泛化性已经得到了广泛的研究，其中一种方向是从频率角度出发。这些研究表明人类和CNN可能会关注不同频率成分的图像。基于这些观察，我们提出了一种简单又有效的数据扩充方法 HybridAugment，它降低了CNN对高频成分的依赖，从而提高了其Robustness，保持了清晰度高。其次，我们提出了HybridAugment++，它是一种层次扩充方法，它尝试通过不同频谱扩充来统一各种频谱扩充。HybridAugment++ builds on HybridAugment，并且降低了CNN对图像的振荡 Component 的依赖，而且促进图像的频谱信息。这种统一结果在CIFAR-10/100和ImageNet上得到了与或超过了现状的 Results，同时在ImageNet-C、CIFAR-10-C和CIFAR-100-C上的腐坏检验、鲁棒性检验和Out-of-distribution检验中也表现出色。HybridAugment和HybridAugment++都是几行代码，不需要额外数据、ensemble模型或额外网络。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense"><a href="#Mitigating-Communications-Threats-in-Decentralized-Federated-Learning-through-Moving-Target-Defense" class="headerlink" title="Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense"></a>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11730">http://arxiv.org/abs/2307.11730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/enriquetomasmb/fedstellar">https://github.com/enriquetomasmb/fedstellar</a></li>
<li>paper_authors: Enrique Tomás Martínez Beltrán, Pedro Miguel Sánchez Sánchez, Sergio López Bernal, Gérôme Bovet, Manuel Gil Pérez, Gregorio Martínez Pérez, Alberto Huertas Celdrán</li>
<li>for: 这篇研究旨在探讨对 decentralized federated learning (DFL) 的攻击性问题，并提出一个安全模组来对抗通信基础攻击。</li>
<li>methods: 这篇研究使用了 symmetric and asymmetric encryption 以及 Moving Target Defense (MTD) 技术，包括随机选择邻居和 IP&#x2F;port 变换，并在 Fedstellar 平台上实现了安全模组。</li>
<li>results: 实验结果显示，在对 MNIST 数据集和 eclipse 攻击进行评估时，这个安全模组能够提高 F1 分数的平均值至 95%，并导致 CPU 使用率（最高可达 63.2% +-3.5%）和网络流量（最高可达 230 MB +-15 MB）的moderate 增加。<details>
<summary>Abstract</summary>
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including random neighbor selection and IP/port switching. The security module is implemented in a DFL platform called Fedstellar, allowing the deployment and monitoring of the federation. A DFL scenario has been deployed, involving eight physical devices implementing three security configurations: (i) a baseline with no security, (ii) an encrypted configuration, and (iii) a configuration integrating both encryption and MTD techniques. The effectiveness of the security module is validated through experiments with the MNIST dataset and eclipse attacks. The results indicated an average F1 score of 95%, with moderate increases in CPU usage (up to 63.2% +-3.5%) and network traffic (230 MB +-15 MB) under the most secure configuration, mitigating the risks posed by eavesdropping or eclipse attacks.
</details>
<details>
<summary>摘要</summary>
DFL（分布式联合学习）的出现使得机器学习模型可以在联合参与者之间训练，从而实现分布式模型聚合和减少依赖于服务器。然而，这种方法引入了一些独特的通信安全挑战，在文献中没有得到充分解决。这些挑战主要来自联合聚合过程的分布式特性、参与者的多样化角色和责任，以及缺乏中央权限来监督和 Mitigate 威胁。为了解决这些挑战，本文首先定义了DFL通信的威胁模型，并提出了一种安全模块，用于DFL平台来防御通信基于攻击。该模块结合了加密技术和移动目标防御（MTD）技术，包括随机邻居选择和IP/端口转换。该安全模块在名为Fedstellar的DFL平台上实现， allowing the deployment and monitoring of the federation。一个DFL场景已经被部署，涉及八个物理设备实现三种安全配置：（i）无安全（基准），（ii）加密配置，（iii）加密和MTD技术的配置。安全模块的效果通过使用MNIST数据集和 eclipse 攻击进行实验验证。结果表明，在最安全配置下，F1 分数平均达到 95%，CPU 使用率（最大值63.2% +-3.5%）和网络流量（230 MB +-15 MB）增加较moderate。这些结果验证了安全模块的有效性，抵消了防止窃听或 eclipse 攻击的风险。
</details></li>
</ul>
<hr>
<h2 id="Benchmark-datasets-for-biomedical-knowledge-graphs-with-negative-statements"><a href="#Benchmark-datasets-for-biomedical-knowledge-graphs-with-negative-statements" class="headerlink" title="Benchmark datasets for biomedical knowledge graphs with negative statements"></a>Benchmark datasets for biomedical knowledge graphs with negative statements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11719">http://arxiv.org/abs/2307.11719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rita T. Sousa, Sara Silva, Catia Pesquita</li>
<li>for:  fills the lack of benchmark datasets for knowledge graphs with negative statements, especially in the biomedical domain.</li>
<li>methods:  two popular path-based methods are used to generate knowledge graph embeddings for each dataset.</li>
<li>results:  negative statements can improve the performance of knowledge graph embeddings in relation prediction tasks, such as protein-protein interaction prediction, gene-disease association prediction, and disease prediction.<details>
<summary>Abstract</summary>
Knowledge graphs represent facts about real-world entities. Most of these facts are defined as positive statements. The negative statements are scarce but highly relevant under the open-world assumption. Furthermore, they have been demonstrated to improve the performance of several applications, namely in the biomedical domain. However, no benchmark dataset supports the evaluation of the methods that consider these negative statements.   We present a collection of datasets for three relation prediction tasks - protein-protein interaction prediction, gene-disease association prediction and disease prediction - that aim at circumventing the difficulties in building benchmarks for knowledge graphs with negative statements. These datasets include data from two successful biomedical ontologies, Gene Ontology and Human Phenotype Ontology, enriched with negative statements.   We also generate knowledge graph embeddings for each dataset with two popular path-based methods and evaluate the performance in each task. The results show that the negative statements can improve the performance of knowledge graph embeddings.
</details>
<details>
<summary>摘要</summary>
知识图表示实际世界实体的事实。大多数这些事实被定义为正面声明。然而，负面声明scarce，但在开放世界假设下，它们对许多应用程序的性能有着高度相关性。例如，在生物医学领域中，它们已经被证明可以提高性能。然而，没有一个benchmark dataset来评估这些方法，这使得建立 benchmarks for knowledge graphs with negative statements 变得困难。为了解决这些问题，我们提供了三个关系预测任务的数据集 - protein-protein交互预测、基因疾病相关性预测和疾病预测 - 这些数据集包括了两个成功的生物医学 ontologies，生物学机制 Ontology 和人类现象 Ontology，这些数据集还包括了负面声明。我们还生成了每个数据集的知识图嵌入，使用两种流行的路径基方法，并评估每个任务的性能。结果表明，负面声明可以提高知识图嵌入的性能。
</details></li>
</ul>
<hr>
<h2 id="Statement-based-Memory-for-Neural-Source-Code-Summarization"><a href="#Statement-based-Memory-for-Neural-Source-Code-Summarization" class="headerlink" title="Statement-based Memory for Neural Source Code Summarization"></a>Statement-based Memory for Neural Source Code Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11709">http://arxiv.org/abs/2307.11709</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aakashba/smncode2022">https://github.com/aakashba/smncode2022</a></li>
<li>paper_authors: Aakash Bansal, Siyuan Jiang, Sakib Haque, Collin McMillan</li>
<li>For: The paper is written for programmers who want to quickly understand the behavior of source code without having to read the code itself. It aims to provide natural language descriptions of code behavior.* Methods: The paper proposes a statement-based memory encoder that learns the important elements of flow during training, allowing for a statement-based subroutine representation without the need for dynamic analysis.* Results: The paper demonstrates a significant improvement over the state-of-the-art in code summarization using the proposed statement-based memory encoder.Here is the information in Simplified Chinese text:</li>
<li>for: 该论文是为程序员们提供快速理解代码行为的自然语言描述。</li>
<li>methods: 论文提出了一种基于语句记忆的编码器，通过在训练中学习流程的重要元素，实现了基于语句的子程序表示，不需要动态分析。</li>
<li>results: 论文通过提出的语句基于编码器，实现了对代码摘要的显著改进。<details>
<summary>Abstract</summary>
Source code summarization is the task of writing natural language descriptions of source code behavior. Code summarization underpins software documentation for programmers. Short descriptions of code help programmers understand the program quickly without having to read the code itself. Lately, neural source code summarization has emerged as the frontier of research into automated code summarization techniques. By far the most popular targets for summarization are program subroutines. The idea, in a nutshell, is to train an encoder-decoder neural architecture using large sets of examples of subroutines extracted from code repositories. The encoder represents the code and the decoder represents the summary. However, most current approaches attempt to treat the subroutine as a single unit. For example, by taking the entire subroutine as input to a Transformer or RNN-based encoder. But code behavior tends to depend on the flow from statement to statement. Normally dynamic analysis may shed light on this flow, but dynamic analysis on hundreds of thousands of examples in large datasets is not practical. In this paper, we present a statement-based memory encoder that learns the important elements of flow during training, leading to a statement-based subroutine representation without the need for dynamic analysis. We implement our encoder for code summarization and demonstrate a significant improvement over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
源代码概要是指将源代码行为写入自然语言描述。代码概要支持软件文档 для程序员。简短的代码描述可以帮助程序员快速理解程序，而无需阅读代码本身。目前，神经源代码概要已经成为自动代码概要技术的前沿。目标最多是程序子循环。基本思路是使用大量的例子来训练神经网络Encoder-Decoder结构。Encoder表示代码，Decoder表示概要。但现有方法通常会将子循环视为单个单元。例如，将整个子循环作为Transformer或RNN基于Encoder的输入。但代码行为通常是从语句到语句的流动的。正常的动态分析可能会暴露这种流动，但是在大量数据集上进行动态分析是不实际的。在本文中，我们提出一个语句基 Memory Encoder，可以在训练中学习重要的流动元素，从而得到语句基的子循环表示，无需动态分析。我们实现了我们的Encoder для代码概要，并在状态前方示出了显著提升。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/07/22/cs.AI_2023_07_22/" data-id="clp8zxr1o0015n6888y5rc8ah" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/81/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/80/">80</a><a class="page-number" href="/page/81/">81</a><span class="page-number current">82</span><a class="page-number" href="/page/83/">83</a><a class="page-number" href="/page/84/">84</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/83/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
