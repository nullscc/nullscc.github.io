
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/42/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_09_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/26/cs.CL_2023_09_26/" class="article-date">
  <time datetime="2023-09-26T11:00:00.000Z" itemprop="datePublished">2023-09-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/26/cs.CL_2023_09_26/">cs.CL - 2023-09-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Unsupervised-Pre-Training-for-Vietnamese-Automatic-Speech-Recognition-in-the-HYKIST-Project"><a href="#Unsupervised-Pre-Training-for-Vietnamese-Automatic-Speech-Recognition-in-the-HYKIST-Project" class="headerlink" title="Unsupervised Pre-Training for Vietnamese Automatic Speech Recognition in the HYKIST Project"></a>Unsupervised Pre-Training for Vietnamese Automatic Speech Recognition in the HYKIST Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15869">http://arxiv.org/abs/2309.15869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khai Le-Duc</li>
<li>for: 这个研究的目的是开发一个敬礼语言翻译系统，以支持患者和医生之间的沟通。</li>
<li>methods: 该研究使用了ASR和MT技术，并 investigate了不同的训练计划和数据结合策略，以提高系统的性能。</li>
<li>results: 研究发现，使用公共可用的模型如XLSR-53可以达到比较高的识别精度，而自定义预训练模型也可以提高系统的性能。同时，该研究还 compare了不同的训练方法，包括supervised和Unsupervised方法，并使用wav2vec 2.0作为架构。<details>
<summary>Abstract</summary>
In today's interconnected globe, moving abroad is more and more prevalent, whether it's for employment, refugee resettlement, or other causes. Language difficulties between natives and immigrants present a common issue on a daily basis, especially in medical domain. This can make it difficult for patients and doctors to communicate during anamnesis or in the emergency room, which compromises patient care. The goal of the HYKIST Project is to develop a speech translation system to support patient-doctor communication with ASR and MT.   ASR systems have recently displayed astounding performance on particular tasks for which enough quantities of training data are available, such as LibriSpeech. Building a good model is still difficult due to a variety of speaking styles, acoustic and recording settings, and a lack of in-domain training data. In this thesis, we describe our efforts to construct ASR systems for a conversational telephone speech recognition task in the medical domain for Vietnamese language to assist emergency room contact between doctors and patients across linguistic barriers. In order to enhance the system's performance, we investigate various training schedules and data combining strategies. We also examine how best to make use of the little data that is available. The use of publicly accessible models like XLSR-53 is compared to the use of customized pre-trained models, and both supervised and unsupervised approaches are utilized using wav2vec 2.0 as architecture.
</details>
<details>
<summary>摘要</summary>
今天的全球化社会中，越来越多的人选择移民 abroad，无论是为了工作、难民重新安置或其他原因。在医疗领域，语言障碍问题是每天都存在的问题，特别是在医生和患者之间的交流中。这会使患者和医生在医学询问或紧急室中的交流受到干扰，从而影响病人的护理。 Project HYKIST 的目标是开发一个语音翻译系统，以支持患者和医生之间的交流。 ASR 系统在特定任务上已经表现出了惊人的表现，如 LibriSpeech。但建立好模型仍然困难，因为有很多说话风格、音频和录音设置，以及缺乏相关领域的训练数据。在这个论文中，我们描述了我们在医疗领域的语音识别任务中使用 ASR 系统的努力。我们 investigate 了不同的训练计划和数据组合策略，以提高系统的表现。我们还研究了如何利用有限的数据来提高系统的性能。我们 compare 了使用公共可用模型如 XLSR-53 和自定义预训练模型，以及使用 supervised 和 unsupervised 方法，使用 wav2vec 2.0 架构。
</details></li>
</ul>
<hr>
<h2 id="RAGAS-Automated-Evaluation-of-Retrieval-Augmented-Generation"><a href="#RAGAS-Automated-Evaluation-of-Retrieval-Augmented-Generation" class="headerlink" title="RAGAS: Automated Evaluation of Retrieval Augmented Generation"></a>RAGAS: Automated Evaluation of Retrieval Augmented Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15217">http://arxiv.org/abs/2309.15217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert</li>
<li>for: 评估 Retrieval Augmented Generation (RAG) 框架，不需要参考文本数据库。</li>
<li>methods: 使用 Retrieval 和 LLM 模块，将知识从参考文本数据库传递给 LLM，以减少用户与文本数据库之间的风险。</li>
<li>results: 提出了一组无需人工标注的评估指标，可以评估不同维度的 RAGB 架构，包括 retrieve 模块是否能够准确地标识有关焦点文本段落，LLM 模块是否能够准确地利用这些段落，以及生成结果的质量。<details>
<summary>Abstract</summary>
We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With RAGAs, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.
</details>
<details>
<summary>摘要</summary>
我们介绍了RAGAs（引用自由评估Retrieval Augmented Generation）框架，用于评估基于引用文本库的 Retrieval Augmented Generation（RAG）pipeline。RAG系统由一个检索和一个基于LLM的生成模块组成，通过将知识从参考文本库传递给LLM，使LLM能够作为自然语言层次，减少用户和文本库之间的风险假设。评估RAG体系却存在多个维度：检索系统能够寻找相关和焦点的文本段落，LLM能够充分利用这些文本段落，或生成的质量自身。我们提出了一组无需基于真实人类标注的指标，用于评估这些不同维度。我们认为这种框架可以在评估RAG体系中帮助降低评估周期的时间，特别是 LLM 的广泛采用。
</details></li>
</ul>
<hr>
<h2 id="STANCE-C3-Domain-adaptive-Cross-target-Stance-Detection-via-Contrastive-Learning-and-Counterfactual-Generation"><a href="#STANCE-C3-Domain-adaptive-Cross-target-Stance-Detection-via-Contrastive-Learning-and-Counterfactual-Generation" class="headerlink" title="STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive Learning and Counterfactual Generation"></a>STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive Learning and Counterfactual Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15176">http://arxiv.org/abs/2309.15176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nayoung Kim, David Mosallanezhad, Lu Cheng, Michelle V. Mancenido, Huan Liu</li>
<li>for: 这个研究的目的是提出一种适用于多个领域的立场推断模型，以便在不同领域和目标话题上进行高效的立场推断。</li>
<li>methods: 该模型使用了对比学习和对比生成来强化领域适应性的训练，以及修改的自然语言约束来防止过拟合和提高对多个领域的泛化能力。</li>
<li>results: 经过实验表明，该模型在多个 dataset 上表现出了性能提升，并且在不同领域和目标话题上具有较高的泛化能力。<details>
<summary>Abstract</summary>
Stance detection is the process of inferring a person's position or standpoint on a specific issue to deduce prevailing perceptions toward topics of general or controversial interest, such as health policies during the COVID-19 pandemic. Existing models for stance detection are trained to perform well for a single domain (e.g., COVID-19) and a specific target topic (e.g., masking protocols), but are generally ineffectual in other domains or targets due to distributional shifts in the data. However, constructing high-performing, domain-specific stance detection models requires an extensive corpus of labeled data relevant to the targeted domain, yet such datasets are not readily available. This poses a challenge as the process of annotating data is costly and time-consuming. To address these challenges, we introduce a novel stance detection model coined domain-adaptive Cross-target STANCE detection via Contrastive learning and Counterfactual generation (STANCE-C3) that uses counterfactual data augmentation to enhance domain-adaptive training by enriching the target domain dataset during the training process and requiring significantly less information from the new domain. We also propose a modified self-supervised contrastive learning as a component of STANCE-C3 to prevent overfitting for the existing domain and target and enable cross-target stance detection. Through experiments on various datasets, we show that STANCE-C3 shows performance improvement over existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>Stance detection是推断人的立场或看法在特定问题上，以便推断人们对一些广泛或争议性的话题（如COVID-19大流行期间的健康政策）的看法。现有的姿态检测模型通常只能在单一领域（如COVID-19）和特定目标话题（如面具协议）上表现出色，但在其他领域或话题上通常无法达到相同的水平，这是因为数据的分布shift。然而，建立高性能的领域专门的姿态检测模型需要大量的相关领域数据，但这些数据并不易 disponibles。这种情况提出了一个挑战，因为标注数据的过程是贵重的和时间consuming。为解决这些挑战，我们介绍了一种新的姿态检测模型，名为域 adapted Cross-target STANCE detection via Contrastive learning and Counterfactual generation（STANCE-C3）。STANCE-C3使用了对立数据增强，以便在训练过程中增强目标领域数据，并且需要较少的新领域信息。我们还提出了一种修改后的自我超视的对比学习，以避免过拟合现有领域和目标，并启用跨目标姿态检测。通过对多个数据集进行实验，我们表明STANCE-C3表现出了与现有状态艺技的性能提升。
</details></li>
</ul>
<hr>
<h2 id="RankVicuna-Zero-Shot-Listwise-Document-Reranking-with-Open-Source-Large-Language-Models"><a href="#RankVicuna-Zero-Shot-Listwise-Document-Reranking-with-Open-Source-Large-Language-Models" class="headerlink" title="RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models"></a>RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15088">http://arxiv.org/abs/2309.15088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/castorini/rank_llm">https://github.com/castorini/rank_llm</a></li>
<li>paper_authors: Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin</li>
<li>for: 提高信息检索中列表重新排序的质量，并使用现代大语言模型（LLM）进行列表重新排序。</li>
<li>methods: 使用开源的7B参数模型，基于GPT-3.5和GPT-4的列表重新排序方法，并进行了分布式训练和排序。</li>
<li>results: 实验结果表明，我们可以在零批训练情况下达到与GPT-3.5的列表重新排序效果相似，但效果略为落后于GPT-4。我们希望我们的工作可以为未来关于列表重新排序的研究提供基础。<details>
<summary>Abstract</summary>
Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs. All the code necessary to reproduce our results is available at https://github.com/castorini/rank_llm.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Question-Answering-Approach-to-Evaluate-Legal-Summaries"><a href="#Question-Answering-Approach-to-Evaluate-Legal-Summaries" class="headerlink" title="Question-Answering Approach to Evaluate Legal Summaries"></a>Question-Answering Approach to Evaluate Legal Summaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15016">http://arxiv.org/abs/2309.15016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huihui Xu, Kevin Ashley</li>
<li>for: 法律摘要评价框架</li>
<li>methods: GPT-4生成问答对集和答案评价</li>
<li>results: GPT-4评价与人类评价之间的相关性可以用于评估摘要质量<details>
<summary>Abstract</summary>
Traditional evaluation metrics like ROUGE compare lexical overlap between the reference and generated summaries without taking argumentative structure into account, which is important for legal summaries. In this paper, we propose a novel legal summarization evaluation framework that utilizes GPT-4 to generate a set of question-answer pairs that cover main points and information in the reference summary. GPT-4 is then used to generate answers based on the generated summary for the questions from the reference summary. Finally, GPT-4 grades the answers from the reference summary and the generated summary. We examined the correlation between GPT-4 grading with human grading. The results suggest that this question-answering approach with GPT-4 can be a useful tool for gauging the quality of the summary.
</details>
<details>
<summary>摘要</summary>
传统的评估指标如ROUGE对 lexical  overlap  между参考和生成摘要没有考虑情节结构，这是法律摘要中重要的一点。在这篇论文中，我们提出了一种新的法律摘要评估框架，利用 GPT-4 生成一组对应于参考摘要中主要点和信息的问题集。然后，GPT-4 使用生成的摘要回答这些问题。最后，GPT-4 评分来自参考摘要和生成摘要的答案。我们对 GPT-4 评分与人工评分之间的相关性进行了检验。结果表明，这种问题回答方法与 GPT-4 可以作为评估摘要质量的有用工具。
</details></li>
</ul>
<hr>
<h2 id="Updated-Corpora-and-Benchmarks-for-Long-Form-Speech-Recognition"><a href="#Updated-Corpora-and-Benchmarks-for-Long-Form-Speech-Recognition" class="headerlink" title="Updated Corpora and Benchmarks for Long-Form Speech Recognition"></a>Updated Corpora and Benchmarks for Long-Form Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15013">http://arxiv.org/abs/2309.15013</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/revdotcom/speech-datasets">https://github.com/revdotcom/speech-datasets</a></li>
<li>paper_authors: Jennifer Drexler Fox, Desh Raj, Natalie Delworth, Quinn McNamara, Corey Miller, Migüel Jetté</li>
<li>for: 这个论文主要用于研究长形语音识别（ASR）领域中的域名匹配问题。</li>
<li>methods: 该论文使用了三个标准的ASR corpora（TED-LIUM 3、Gigapeech和VoxPopuli-en），对其进行了更新的转录和对应，以便用于长形ASR研究。它们还研究了在训练和测试数据不同的情况下，逻辑架构和注意力基本encoder-decoder（AED）模型的Robustness问题。</li>
<li>results: 研究发现，AED模型更容易受到域名匹配问题的影响，而长形训练可以提高这些模型的Robustness。<details>
<summary>Abstract</summary>
The vast majority of ASR research uses corpora in which both the training and test data have been pre-segmented into utterances. In most real-word ASR use-cases, however, test audio is not segmented, leading to a mismatch between inference-time conditions and models trained on segmented utterances. In this paper, we re-release three standard ASR corpora - TED-LIUM 3, Gigapeech, and VoxPopuli-en - with updated transcription and alignments to enable their use for long-form ASR research. We use these reconstituted corpora to study the train-test mismatch problem for transducers and attention-based encoder-decoders (AEDs), confirming that AEDs are more susceptible to this issue. Finally, we benchmark a simple long-form training for these models, showing its efficacy for model robustness under this domain shift.
</details>
<details>
<summary>摘要</summary>
大多数ASR研究使用已经 pré-分 segmented的数据集进行训练和测试。然而，在实际应用中，测试音频通常没有 segmented，导致模型训练用的 condition和测试 condition 之间存在匹配问题。在这篇论文中，我们重新发布了三个标准 ASR 数据集 - TED-LIUM 3、Gigapeech 和 VoxPopuli-en - 的更新的转录和对应，以便用于长形 ASR 研究。我们使用这些重新拟合的数据集来研究训练和测试之间的匹配问题，发现 AEDs 更容易受到这种问题的影响。最后，我们测试了一种简单的长形训练方法，并证明其在这种领域移植中的效果。
</details></li>
</ul>
<hr>
<h2 id="Robustness-of-the-Random-Language-Model"><a href="#Robustness-of-the-Random-Language-Model" class="headerlink" title="Robustness of the Random Language Model"></a>Robustness of the Random Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14913">http://arxiv.org/abs/2309.14913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Fatemeh Lalegani, Eric De Giuli</li>
<li>for: 本研究探讨了人类和计算机语言之间的语法匹配。</li>
<li>methods: 该研究使用了随机语言模型（De Giuli 2019），这是一种 ensemble of stochastic context-free grammars，用于量化人类和计算机语言的 syntax。</li>
<li>results: 研究表明，在考虑到显式对称破坏的情况下，模型的enario是Robust的。与人类语言数据中的 syntax 网络划分系数相比， Observation 与24岁的儿童 обычно经历的转变相当。<details>
<summary>Abstract</summary>
The Random Language Model (De Giuli 2019) is an ensemble of stochastic context-free grammars, quantifying the syntax of human and computer languages. The model suggests a simple picture of first language learning as a type of annealing in the vast space of potential languages. In its simplest formulation, it implies a single continuous transition to grammatical syntax, at which the symmetry among potential words and categories is spontaneously broken. Here this picture is scrutinized by considering its robustness against explicit symmetry breaking, an inevitable component of learning in the real world. It is shown that the scenario is robust to such symmetry breaking. Comparison with human data on the clustering coefficient of syntax networks suggests that the observed transition is equivalent to that normally experienced by children at age 24 months.
</details>
<details>
<summary>摘要</summary>
随机语言模型（De Giuli 2019）是一个集合的随机上下文自由格式语言，量化人类和计算机语言的语法。该模型提出了一个简单的语言学习图景，认为人类语言学习是一种热化在可能语言空间中的过程。在最简式表述中，它表明了一种单一的连续变换，在潜在词汇和分类之间各自破坏 симметрии。在这种情况下，我们考虑了对显式对称破坏的Robustness，这是学习世界中不可避免的一部分。结果表明，这种情况具有Robustness。与人类语言结构网络的凝集系数相比，显示出这种过渡与24个月大的儿童常见的过渡相等。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Flawed-Data-Weakly-Supervised-Automatic-Speech-Recognition"><a href="#Learning-from-Flawed-Data-Weakly-Supervised-Automatic-Speech-Recognition" class="headerlink" title="Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition"></a>Learning from Flawed Data: Weakly Supervised Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15796">http://arxiv.org/abs/2309.15796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall">https://github.com/k2-fsa/icefall</a></li>
<li>paper_authors: Dongji Gao, Hainan Xu, Desh Raj, Leibny Paola Garcia Perera, Daniel Povey, Sanjeev Khudanpur</li>
<li>For: 提高自动语音识别（ASR）系统的训练效果，适用于大量的高质量对应数据。* Methods: 提出了 Omni-temporal Classification（OTC）训练标准，通过考虑标签不确定性，使模型能够有效地学习语音-文本对应。OTC基于不确定的Weighted Finite State Transducers（WFST）扩展了传统的 CTC 目标函数。* Results: 通过在 LibriSpeech 和 LibriVox 数据集上进行实验，表明使用 OTC 训练 ASR 模型，甚至在对应文本中含有70%错误的情况下，模型的性能不会下降。<details>
<summary>Abstract</summary>
Training automatic speech recognition (ASR) systems requires large amounts of well-curated paired data. However, human annotators usually perform "non-verbatim" transcription, which can result in poorly trained models. In this paper, we propose Omni-temporal Classification (OTC), a novel training criterion that explicitly incorporates label uncertainties originating from such weak supervision. This allows the model to effectively learn speech-text alignments while accommodating errors present in the training transcripts. OTC extends the conventional CTC objective for imperfect transcripts by leveraging weighted finite state transducers. Through experiments conducted on the LibriSpeech and LibriVox datasets, we demonstrate that training ASR models with OTC avoids performance degradation even with transcripts containing up to 70% errors, a scenario where CTC models fail completely. Our implementation is available at https://github.com/k2-fsa/icefall.
</details>
<details>
<summary>摘要</summary>
培训自动语音识别（ASR）系统需要大量的高质量对应数据。然而，人工标注员通常会进行“非文字”抄写，这可能导致模型训练不佳。在这篇论文中，我们提出了一种新的训练标准《全时分类》（OTC），该标准直接表达标注不确定性的影响。这使得模型能够有效地学习语音-文本对应，同时满足训练脚本中存在错误的情况。OTC基于权重finite state transducers扩展了传统的CTC目标，并通过实验表明，即使训练脚本中有70%的错误，ASR模型也能够保持高效性。我们的实现可以在https://github.com/k2-fsa/icefall中找到。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-Free-Streaming-Machine-Translation"><a href="#Segmentation-Free-Streaming-Machine-Translation" class="headerlink" title="Segmentation-Free Streaming Machine Translation"></a>Segmentation-Free Streaming Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14823">http://arxiv.org/abs/2309.14823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier Iranzo-Sánchez, Jorge Iranzo-Sánchez, Adrià Giménez, Jorge Civera, Alfons Juan</li>
<li>for: 提出了一种无需分 segmentation 的概率机器翻译（MT）框架，以实现在实时翻译中不需要预先分 segmentation。</li>
<li>methods: 提出了一种延迟 segmentation 决策ntil 翻译结果生成完毕的方法，使得模型可以在不需要硬件 segmentation 的情况下进行翻译。</li>
<li>results: 对比其他竞争方法，提出的 Segmentation-Free 框架在质量-延迟Trade-off中具有更好的性能。<details>
<summary>Abstract</summary>
Streaming Machine Translation (MT) is the task of translating an unbounded input text stream in real-time. The traditional cascade approach, which combines an Automatic Speech Recognition (ASR) and an MT system, relies on an intermediate segmentation step which splits the transcription stream into sentence-like units. However, the incorporation of a hard segmentation constrains the MT system and is a source of errors. This paper proposes a Segmentation-Free framework that enables the model to translate an unsegmented source stream by delaying the segmentation decision until the translation has been generated. Extensive experiments show how the proposed Segmentation-Free framework has better quality-latency trade-off than competing approaches that use an independent segmentation model. Software, data and models will be released upon paper acceptance.
</details>
<details>
<summary>摘要</summary>
流动机器翻译（MT）是将输入文本流转换成实时翻译的任务。传统的堆叠方法，将自动语音识别（ASR）和MT系统结合在一起，需要一个中间分 segmentation 步骤，将转录流分成句子样式的单元。然而，在 incorporating 硬件分 segmentation 会限制MT系统的性能，并且是错误的来源。这篇论文提出了无需分 segmentation 的框架，允许模型在翻译过程中延迟分 segmentation 决策，直到翻译结果被生成。广泛的实验表明，提议的无需分 segmentation 框架在质量-延迟质量之间有更好的质量-延迟平衡，与独立的分 segmentation 模型相比。软件、数据和模型会在论文接受后释出。
</details></li>
</ul>
<hr>
<h2 id="BLIP-Adapter-Parameter-Efficient-Transfer-Learning-for-Mobile-Screenshot-Captioning"><a href="#BLIP-Adapter-Parameter-Efficient-Transfer-Learning-for-Mobile-Screenshot-Captioning" class="headerlink" title="BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning"></a>BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14774">http://arxiv.org/abs/2309.14774</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rainyugg/blip-adapter">https://github.com/rainyugg/blip-adapter</a></li>
<li>paper_authors: Ching-Yu Chiang, I-Hua Chang, Shih-Wei Liao</li>
<li>for: 这项研究旨在探索屏幕截图captioning任务的有效调参方法。</li>
<li>methods: 本研究提议使用适应器方法，只需调参模型中的附加模块，以提高性能。</li>
<li>results: 研究表明，通过将批处理大型预训练模型的参数冻结，并仅调参适应器方法中的参数，可以实现与完全调参模型的性能相似，同时减少了大量参数的数量。<details>
<summary>Abstract</summary>
This study aims to explore efficient tuning methods for the screenshot captioning task. Recently, image captioning has seen significant advancements, but research in captioning tasks for mobile screens remains relatively scarce. Current datasets and use cases describing user behaviors within product screenshots are notably limited. Consequently, we sought to fine-tune pre-existing models for the screenshot captioning task. However, fine-tuning large pre-trained models can be resource-intensive, requiring considerable time, computational power, and storage due to the vast number of parameters in image captioning models. To tackle this challenge, this study proposes a combination of adapter methods, which necessitates tuning only the additional modules on the model. These methods are originally designed for vision or language tasks, and our intention is to apply them to address similar challenges in screenshot captioning. By freezing the parameters of the image caption models and training only the weights associated with the methods, performance comparable to fine-tuning the entire model can be achieved, while significantly reducing the number of parameters. This study represents the first comprehensive investigation into the effectiveness of combining adapters within the context of the screenshot captioning task. Through our experiments and analyses, this study aims to provide valuable insights into the application of adapters in vision-language models and contribute to the development of efficient tuning techniques for the screenshot captioning task. Our study is available at https://github.com/RainYuGG/BLIP-Adapter
</details>
<details>
<summary>摘要</summary>
这个研究的目标是探索屏幕截图标题预测任务中有效的调参方法。在图像描述领域，近年来有了 significative 的进步，但是对手机屏幕上的用户行为描述 task 的研究尚处于相对缺乏的状态。当前的数据集和用户行为描述 case 都很有限，因此我们决定使用先前训练的模型进行调参。然而，对大型预训练模型的调参可能会占用大量的时间、计算资源和存储空间，这是因为图像描述模型中有很多参数。为了解决这个挑战，本研究提出了一种将适配器方法与图像描述模型结合使用的方法。这种方法原本是设计用于视觉或语言任务的，我们想用它们来解决屏幕截图标题预测任务中的类似挑战。通过冻结图像描述模型的参数，并仅对适配器方法进行训练，可以实现与完全调参模型的性能相似，同时减少了大量参数的数量。本研究是首次对适配器在屏幕截图标题预测任务中的应用进行全面的研究。通过我们的实验和分析，本研究旨在为应用适配器在视觉语言模型中的应用提供有价值的发现，并为屏幕截图标题预测任务中的效率调参技术做出贡献。研究的数据集和代码可以在 GitHub 上找到：https://github.com/RainYuGG/BLIP-Adapter
</details></li>
</ul>
<hr>
<h2 id="KERMIT-Knowledge-Graph-Completion-of-Enhanced-Relation-Modeling-with-Inverse-Transformation"><a href="#KERMIT-Knowledge-Graph-Completion-of-Enhanced-Relation-Modeling-with-Inverse-Transformation" class="headerlink" title="KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation"></a>KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14770">http://arxiv.org/abs/2309.14770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Li, Lingzhi Wang, Yuliang Wei, Richard Yi Da Xu, Bailing Wang</li>
<li>for: 填充知识图中缺失的三元组（triple），以提高知识图完成任务的准确性。</li>
<li>methods: 利用文本描述来完成知识图 completion 任务，但可能会遇到限制，因为描述可能不准确地表达意图。为了解决这些挑战，我们提议通过两种附加机制来增强数据。首先，我们使用 ChatGPT 作为外部知识库，生成具有准确性和 coherence 的描述，以bridging semantic gap между查询和答案。其次，我们利用 inverse relations 创建对称图，生成额外标签和提供补充信息，以便链接预测。这种方法可以提供更多的关系between entities。</li>
<li>results: 通过这两种机制，我们观察到了知识图 completion 的显著改善，这些机制可以增强数据的 ricahness 和多样性，导致更准确的结果。<details>
<summary>Abstract</summary>
Knowledge graph completion is a task that revolves around filling in missing triples based on the information available in a knowledge graph. Among the current studies, text-based methods complete the task by utilizing textual descriptions of triples. However, this modeling approach may encounter limitations, particularly when the description fails to accurately and adequately express the intended meaning. To overcome these challenges, we propose the augmentation of data through two additional mechanisms. Firstly, we employ ChatGPT as an external knowledge base to generate coherent descriptions to bridge the semantic gap between the queries and answers. Secondly, we leverage inverse relations to create a symmetric graph, thereby creating extra labeling and providing supplementary information for link prediction. This approach offers additional insights into the relationships between entities. Through these efforts, we have observed significant improvements in knowledge graph completion, as these mechanisms enhance the richness and diversity of the available data, leading to more accurate results.
</details>
<details>
<summary>摘要</summary>
知识图完成任务是基于现有的知识图信息完善缺失的 triple。目前的研究主要采用文本方法来完成这项任务，但这种模型化方法可能会遇到限制，特别是当描述不准确、不完整时。为了解决这些挑战，我们提议在数据上进行两种附加机制。首先，我们使用 ChatGPT 作为外部知识库，生成具有协调性的描述， bridging 知识图中缺失的semantic gap。其次，我们利用反向关系，创建对称图，从而创建Extra labeling和提供补充信息 для链接预测。这种方法提供了更多的关系 между实体的意义，通过这些努力，我们观察到了知识图完成任务中显著的改善，这些机制增加了可用数据的丰富性和多样性，导致更加准确的结果。
</details></li>
</ul>
<hr>
<h2 id="ConPET-Continual-Parameter-Efficient-Tuning-for-Large-Language-Models"><a href="#ConPET-Continual-Parameter-Efficient-Tuning-for-Large-Language-Models" class="headerlink" title="ConPET: Continual Parameter-Efficient Tuning for Large Language Models"></a>ConPET: Continual Parameter-Efficient Tuning for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14763">http://arxiv.org/abs/2309.14763</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/raincleared-song/conpet">https://github.com/raincleared-song/conpet</a></li>
<li>paper_authors: Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, Tao Yang</li>
<li>for: 这个研究旨在提出一种应用于大型语言模型（LLM）的持续学习方法，以减少 computation costs、内存耗尽和遗传问题。</li>
<li>methods: 这个方法基于优化parameter-efficient tuning（PET），包括两个版本：静态ConPET和动态ConPET。静态ConPET可以让former continual learning方法在LMM中进行适应，并将适应成本大大减少。动态ConPET则透过分类PET模组和PET模组选择器实现动态选择最佳PET模组。</li>
<li>results: 实验结果显示，静态ConPET可以帮助多个former方法将可调参数的数量增加至3,000多倍，并在五个较小的benchmark上超过PET-只基eline以少于5分点。动态ConPET则在最大的dataset上获得了优化。codes和数据可以在<a target="_blank" rel="noopener" href="https://github.com/Raincleared-Song/ConPET%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Raincleared-Song/ConPET上获取。</a><details>
<summary>Abstract</summary>
Continual learning necessitates the continual adaptation of models to newly emerging tasks while minimizing the catastrophic forgetting of old ones. This is extremely challenging for large language models (LLMs) with vanilla full-parameter tuning due to high computation costs, memory consumption, and forgetting issue. Inspired by the success of parameter-efficient tuning (PET), we propose Continual Parameter-Efficient Tuning (ConPET), a generalizable paradigm for continual task adaptation of LLMs with task-number-independent training complexity. ConPET includes two versions with different application scenarios. First, Static ConPET can adapt former continual learning methods originally designed for relatively smaller models to LLMs through PET and a dynamic replay strategy, which largely reduces the tuning costs and alleviates the over-fitting and forgetting issue. Furthermore, to maintain scalability, Dynamic ConPET adopts separate PET modules for different tasks and a PET module selector for dynamic optimal selection. In our extensive experiments, the adaptation of Static ConPET helps multiple former methods reduce the scale of tunable parameters by over 3,000 times and surpass the PET-only baseline by at least 5 points on five smaller benchmarks, while Dynamic ConPET gains its advantage on the largest dataset. The codes and datasets are available at https://github.com/Raincleared-Song/ConPET.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> kontinuel lerning需要 kontinuel adapting模型到新出现的任务，并最大限度减少老任务的忘记。这对大语言模型（LLM）来说是极其困难的，因为它们的计算成本高、内存占用大，以及忘记问题。 Drawing inspiration from the success of parameter-efficient tuning（PET）, we propose Continual Parameter-Efficient Tuning（ConPET）， a generalizable paradigm for continual task adaptation of LLMs with task-number-independent training complexity. ConPET includes two versions with different application scenarios. First, Static ConPET can adapt former continual learning methods originally designed for relatively smaller models to LLMs through PET and a dynamic replay strategy, which largely reduces the tuning costs and alleviates the over-fitting and forgetting issue. Furthermore, to maintain scalability, Dynamic ConPET adopts separate PET modules for different tasks and a PET module selector for dynamic optimal selection. In our extensive experiments, the adaptation of Static ConPET helps multiple former methods reduce the scale of tunable parameters by over 3,000 times and surpass the PET-only baseline by at least 5 points on five smaller benchmarks, while Dynamic ConPET gains its advantage on the largest dataset. The codes and datasets are available at https://github.com/Raincleared-Song/ConPET。
</details></li>
</ul>
<hr>
<h2 id="QA-LoRA-Quantization-Aware-Low-Rank-Adaptation-of-Large-Language-Models"><a href="#QA-LoRA-Quantization-Aware-Low-Rank-Adaptation-of-Large-Language-Models" class="headerlink" title="QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"></a>QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14717">http://arxiv.org/abs/2309.14717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eltociear/qa-lora">https://github.com/eltociear/qa-lora</a></li>
<li>paper_authors: Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, Qi Tian</li>
<li>for: 这个论文的目的是提出一个量化意识掌握算法（QA-LoRA），以实现大型自然语言模型（LLM）在边缘设备上的部署。</li>
<li>methods: 这个论文使用的方法是使用群体化算子，增加量化的自由度，同时减少适应的自由度。这个方法可以轻松地实现，只需要几行代码。</li>
<li>results: 这个论文的结果显示，使用QA-LoRA可以实现量化LLM的时间和内存使用率的减少，并且不会对精度造成损害。这个方法可以在不同的精度档案和下游应用中进行适用。<details>
<summary>Abstract</summary>
Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy. We apply QA-LoRA to the LLaMA and LLaMA2 model families and validate its effectiveness in different fine-tuning datasets and downstream scenarios. Code will be made available at https://github.com/yuhuixu1993/qa-lora.
</details>
<details>
<summary>摘要</summary>
最近几年内，大型语言模型（LLM）的快速发展已经引起了广泛的关注。尽管LLM具有许多语言理解任务的强大能力，但是计算负担很大，特别是在部署到边缘设备时。在这篇论文中，我们提出了一种量化意识扩展低级化算法（QA-LoRA）。我们的动机在于量化和适应的自由度不均衡，我们的解决方案是使用群组操作符，以增加量化的自由度，同时减少适应的自由度。QA-LoRA易于实现，只需几行代码即可，它使得原始LoRA具有两种能力：（i）在练习中，LLM的参数被量化（例如INT4），以降低时间和存储使用；（ii）在练习后，LLM和辅助参数自然地被 integrate到量化模型中，无损失准确性。我们在LLaMA和LLaMA2模型家族上应用QA-LoRA，并在不同的练习数据集和下游enario中验证其效果。代码将在https://github.com/yuhuixu1993/qa-lora中提供。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Text-to-Video-Model-via-Transformer"><a href="#A-Simple-Text-to-Video-Model-via-Transformer" class="headerlink" title="A Simple Text to Video Model via Transformer"></a>A Simple Text to Video Model via Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14683">http://arxiv.org/abs/2309.14683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vividitytech/text2videogpt">https://github.com/vividitytech/text2videogpt</a></li>
<li>paper_authors: Gang Chen</li>
<li>for: 本研究旨在提出一种通用且简单的文本到视频模型，基于Transformer结构。</li>
<li>methods: 本模型使用了Transformer结构来捕捉文本和图像的时间相关性，并使用GPT2进行语言模型。</li>
<li>results: 经测试在UCF101 dataset上，本方法可以生成出promising的视频。Here’s a more detailed explanation of each point:1. For: The paper aims to propose a general and simple text-to-video model based on the Transformer architecture.2. Methods: The model uses the Transformer architecture to capture the temporal consistency between text and image sequences, and employs GPT2 as the language model.3. Results: The proposed method is tested on the UCF101 dataset and shows promising results in generating videos.<details>
<summary>Abstract</summary>
We present a general and simple text to video model based on Transformer. Since both text and video are sequential data, we encode both texts and images into the same hidden space, which are further fed into Transformer to capture the temporal consistency and then decoder to generate either text or images. Considering the image signal may become weak in the long sequence, we introduce the U-Net to reconstruct image from its noised version. Specifically, we increase the noise level to the original image in the long sequence, then use the $down$ module from U-Net to encode noised images, which are further input to transformer to predict next clear images. We also add a constraint to promote motion between any generated image pair in the video. We use GPT2 and test our approach on UCF101 dataset and show it can generate promising videos.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用、简单的文本到视频模型，基于Transformer。由于文本和视频都是序列数据，我们将文本和图像编码到同一个隐藏空间中，然后将其传递给Transformer来捕捉时间一致性。为了处理长序列中的图像信号弱化，我们引入了U-Net来重建图像。 Specifically，我们将原始图像的噪声水平提高，然后使用U-Net的$down$模块编码噪声图像，并将其输入到Transformer来预测下一帧清晰图像。此外，我们添加了一个约束来促进视频中任意生成图像对的运动。我们使用GPT2进行测试，并在UCF101 dataset上实现了可靠的视频生成。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/26/cs.CL_2023_09_26/" data-id="clpztdne800cies883by37pnm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/26/cs.LG_2023_09_26/" class="article-date">
  <time datetime="2023-09-26T10:00:00.000Z" itemprop="datePublished">2023-09-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/26/cs.LG_2023_09_26/">cs.LG - 2023-09-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DeepROCK-Error-controlled-interaction-detection-in-deep-neural-networks"><a href="#DeepROCK-Error-controlled-interaction-detection-in-deep-neural-networks" class="headerlink" title="DeepROCK: Error-controlled interaction detection in deep neural networks"></a>DeepROCK: Error-controlled interaction detection in deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15319">http://arxiv.org/abs/2309.15319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Winston Chen, William Stafford Noble, Yang Young Lu</li>
<li>for: 这 paper 的目的是提高深度神经网络 (DNN) 的解释性，使其在抗错准备领域中更加可靠。</li>
<li>methods: 这 paper 使用了“knockoffs”，即假变量，来模拟特定功能集之间的依赖关系，并使用了一种新的 DNN 架构，以控制 false discovery rate (FDR) 和最大化统计能力。</li>
<li>results:  experiments 表明，DeepROCK 可以有效地控制 FDR，并在 simulate 和实际数据上进行了广泛的验证。<details>
<summary>Abstract</summary>
The complexity of deep neural networks (DNNs) makes them powerful but also makes them challenging to interpret, hindering their applicability in error-intolerant domains. Existing methods attempt to reason about the internal mechanism of DNNs by identifying feature interactions that influence prediction outcomes. However, such methods typically lack a systematic strategy to prioritize interactions while controlling confidence levels, making them difficult to apply in practice for scientific discovery and hypothesis validation. In this paper, we introduce a method, called DeepROCK, to address this limitation by using knockoffs, which are dummy variables that are designed to mimic the dependence structure of a given set of features while being conditionally independent of the response. Together with a novel DNN architecture involving a pairwise-coupling layer, DeepROCK jointly controls the false discovery rate (FDR) and maximizes statistical power. In addition, we identify a challenge in correctly controlling FDR using off-the-shelf feature interaction importance measures. DeepROCK overcomes this challenge by proposing a calibration procedure applied to existing interaction importance measures to make the FDR under control at a target level. Finally, we validate the effectiveness of DeepROCK through extensive experiments on simulated and real datasets.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的复杂性使其具有强大的计算能力，但同时也使其难以解释，这限制了其在容错领域的应用。现有的方法通过找出特征相互作用来推理DNN的内部机制，但这些方法通常缺乏系统的策略来优先级化交互，使其在实践中困难应用于科学发现和假设验证。在本文中，我们介绍了一种方法，称为DeepROCK，以解决这一限制。DeepROCK使用“假变量”（knockoffs），这些变量模拟给定特征集的依赖结构，同时保持conditionally independent于响应变量。与一种新的DNN架构相结合，DeepROCK同时控制了false discovery rate（FDR）和最大化统计能力。此外，我们发现了控制FDR使用现有特征相互作用重要性度量的挑战。DeepROCK解决了这个挑战，通过对现有的特征相互作用重要性度量进行滤波来使FDR进行控制。最后，我们通过对模拟和实际数据进行广泛的实验验证了DeepROCK的效果。
</details></li>
</ul>
<hr>
<h2 id="Telescope-An-Automated-Hybrid-Forecasting-Approach-on-a-Level-Playing-Field"><a href="#Telescope-An-Automated-Hybrid-Forecasting-Approach-on-a-Level-Playing-Field" class="headerlink" title="Telescope: An Automated Hybrid Forecasting Approach on a Level-Playing Field"></a>Telescope: An Automated Hybrid Forecasting Approach on a Level-Playing Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15871">http://arxiv.org/abs/2309.15871</a></li>
<li>repo_url: None</li>
<li>paper_authors: André Bauer, Mark Leznik, Michael Stenger, Robert Leppich, Nikolas Herbst, Samuel Kounev, Ian Foster</li>
<li>for: 预测（forecasting）</li>
<li>methods: 使用机器学习方法自动从给定时间序列中提取有关信息，并将其分解成多个部分进行处理。</li>
<li>results: 比较其他最新方法的准确和可靠预测，而无需进行参数化或训练多个参数。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In many areas of decision-making, forecasting is an essential pillar. Consequently, many different forecasting methods have been proposed. From our experience, recently presented forecasting methods are computationally intensive, poorly automated, tailored to a particular data set, or they lack a predictable time-to-result. To this end, we introduce Telescope, a novel machine learning-based forecasting approach that automatically retrieves relevant information from a given time series and splits it into parts, handling each of them separately. In contrast to deep learning methods, our approach doesn't require parameterization or the need to train and fit a multitude of parameters. It operates with just one time series and provides forecasts within seconds without any additional setup. Our experiments show that Telescope outperforms recent methods by providing accurate and reliable forecasts while making no assumptions about the analyzed time series.
</details>
<details>
<summary>摘要</summary>
在很多决策领域中，预测是一个重要的柱子。因此，有很多不同的预测方法被提出。从我们的经验来看，最近提出的预测方法都具有计算昂贵、自动化不够、特定数据集适应性或时间到结果难以预测等缺点。为了解决这些问题，我们介绍了天镜，一种新的机器学习基于的预测方法。与深度学习方法不同，我们的方法不需要参数化或训练多个参数。它只需要一个时间序列，并在秒钟内提供准确和可靠的预测，无需任何额外设置。我们的实验表明，天镜比最近的方法提供更准确和可靠的预测，而且不会对分析的时间序列做任何假设。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Log-Concavity-Theory-and-Algorithm-for-Sum-Log-Concave-Optimization"><a href="#Beyond-Log-Concavity-Theory-and-Algorithm-for-Sum-Log-Concave-Optimization" class="headerlink" title="Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization"></a>Beyond Log-Concavity: Theory and Algorithm for Sum-Log-Concave Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15298">http://arxiv.org/abs/2309.15298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mastane Achab</li>
<li>for: 该文章扩展了经典的凸优化理论，应用于函数的最小化，其等于减法各个凸抽象函数的逻辑和。</li>
<li>methods: 该文章提出了一种新的梯度下降算法（cross gradient descent，XGD），该算法在梯度和副向量之间进行交互调整，以实现更高效的优化。</li>
<li>results: 该文章通过应用该框架，引入了一种新的分类方法（检查ered regression），该方法可以在非线性分离问题中进行分类，并且可以通过使用任意数量的准则平面，创造一种棋盘状的决策区域。<details>
<summary>Abstract</summary>
This paper extends the classic theory of convex optimization to the minimization of functions that are equal to the negated logarithm of what we term as a sum-log-concave function, i.e., a sum of log-concave functions. In particular, we show that such functions are in general not convex but still satisfy generalized convexity inequalities. These inequalities unveil the key importance of a certain vector that we call the cross-gradient and that is, in general, distinct from the usual gradient. Thus, we propose the Cross Gradient Descent (XGD) algorithm moving in the opposite direction of the cross-gradient and derive a convergence analysis. As an application of our sum-log-concave framework, we introduce the so-called checkered regression method relying on a sum-log-concave function. This classifier extends (multiclass) logistic regression to non-linearly separable problems since it is capable of tessellating the feature space by using any given number of hyperplanes, creating a checkerboard-like pattern of decision regions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multiple-Case-Physics-Informed-Neural-Network-for-Biomedical-Tube-Flows"><a href="#Multiple-Case-Physics-Informed-Neural-Network-for-Biomedical-Tube-Flows" class="headerlink" title="Multiple Case Physics-Informed Neural Network for Biomedical Tube Flows"></a>Multiple Case Physics-Informed Neural Network for Biomedical Tube Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15294">http://arxiv.org/abs/2309.15294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Shen Wong, Wei Xuan Chan, Bing Huan Li, Choon Hwai Yap</li>
<li>for: 计算血液和空气流动在管状 geometries 的生物医学评估</li>
<li>methods: 使用Physics-Informed Neural Networks (PINNs) 代替传统的计算流体力学 (CFD) 方法</li>
<li>results: 可以在实时内获得未看到的geometry cases的结果，并且可以优化网络架构、管状特有的和正则化策略以提高性能<details>
<summary>Abstract</summary>
Fluid dynamics computations for tube-like geometries are important for biomedical evaluation of vascular and airway fluid dynamics. Physics-Informed Neural Networks (PINNs) have recently emerged as a good alternative to traditional computational fluid dynamics (CFD) methods. The vanilla PINN, however, requires much longer training time than the traditional CFD methods for each specific flow scenario and thus does not justify its mainstream use. Here, we explore the use of the multi-case PINN approach for calculating biomedical tube flows, where varied geometry cases are parameterized and pre-trained on the PINN, such that results for unseen geometries can be obtained in real time. Our objective is to identify network architecture, tube-specific, and regularization strategies that can optimize this, via experiments on a series of idealized 2D stenotic tube flows.
</details>
<details>
<summary>摘要</summary>
fluid 动力计算 для管状结构是生物医学评估血液和空气流动的重要方面。物理学 Informed Neural Networks (PINNs) 最近emerge 为传统计算流体力学 (CFD) 方法的好alternative。然而，vanilla PINN 需要每个特定流场训练时间更长，因此无法 justify 其主流使用。在这里，我们探讨使用多个案例 PINN 方法计算生物管流动，其中 varied geometry cases 被参数化并在 PINN 中预训练，以便在实时内获得未经见过的geometry结果。我们的目标是通过实验 serie 的idealized 2D 狭窄管流动来优化这种方法。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Representation-Learning-from-Ubiquitous-ECG-with-State-Space-Models"><a href="#Scaling-Representation-Learning-from-Ubiquitous-ECG-with-State-Space-Models" class="headerlink" title="Scaling Representation Learning from Ubiquitous ECG with State-Space Models"></a>Scaling Representation Learning from Ubiquitous ECG with State-Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15292">http://arxiv.org/abs/2309.15292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klean2050/tiles_ecg_model">https://github.com/klean2050/tiles_ecg_model</a></li>
<li>paper_authors: Kleanthis Avramidis, Dominika Kunc, Bartosz Perz, Kranti Adsul, Tiantian Feng, Przemysław Kazienko, Stanisław Saganowski, Shrikanth Narayanan</li>
<li>For: The paper is written for enhancing human well-being through ubiquitous sensing from wearable devices in the wild, with a focus on electrocardiogram (ECG) signals.* Methods: The paper introduces a pre-trained state-space model for representation learning from ECG signals, which is trained in a self-supervised manner using a large dataset of 275,000 10s ECG recordings collected in the wild.* Results: The proposed model demonstrates competitive performance on a range of downstream tasks, including health monitoring, stress and affect estimation, and provides efficacy in low-resource regimes.Here’s the information in Simplified Chinese text format, as requested:</li>
<li>for: 帮助人类健康提升，通过抽象敏感设备在野外收集数据，包括诊断临床病种和衡量压力和情绪等。</li>
<li>methods: 提出一种基于状态空间模型的 Representation Learning 方法，通过自我超vised 方式使用大量的野外收集到的 275,000 个 10s ECG 记录。</li>
<li>results: 提出的模型在多个下游任务上显示竞争性的表现，包括健康监测、压力和情绪估计等，并在资源有限的情况下显示有效性。<details>
<summary>Abstract</summary>
Ubiquitous sensing from wearable devices in the wild holds promise for enhancing human well-being, from diagnosing clinical conditions and measuring stress to building adaptive health promoting scaffolds. But the large volumes of data therein across heterogeneous contexts pose challenges for conventional supervised learning approaches. Representation Learning from biological signals is an emerging realm catalyzed by the recent advances in computational modeling and the abundance of publicly shared databases. The electrocardiogram (ECG) is the primary researched modality in this context, with applications in health monitoring, stress and affect estimation. Yet, most studies are limited by small-scale controlled data collection and over-parameterized architecture choices. We introduce \textbf{WildECG}, a pre-trained state-space model for representation learning from ECG signals. We train this model in a self-supervised manner with 275,000 10s ECG recordings collected in the wild and evaluate it on a range of downstream tasks. The proposed model is a robust backbone for ECG analysis, providing competitive performance on most of the tasks considered, while demonstrating efficacy in low-resource regimes. The code and pre-trained weights are shared publicly at https://github.com/klean2050/tiles_ecg_model.
</details>
<details>
<summary>摘要</summary>
通过 ubique 感知设备在野外的应用，可以提高人类的健康水平，从诊断临床病种和测量压力到构建适应性健康促进架构。但是，由于这些数据在不同的CONTEXT中存在差异，因此使用传统的指导学习方法会面临挑战。生物信号的 Representation Learning 是一个出现在的领域，它受到了计算机模型的最新进步和大量公共分享数据库的推动。电cardiogram (ECG) 是这个上下文中最常研究的Modalitas，它在健康监测、压力和情绪估计等方面有着应用。然而，大多数研究都受限于小规模的控制数据收集和过参数化的建筑选择。我们介绍了 \textbf{WildECG}，一个预训练的状态空间模型，用于 Representation Learning 从 ECG 信号中获取特征。我们通过在野外收集了 275,000 个 10s ECG 记录的自助监测方式进行训练，并对其进行评估。该模型是一个强健的 ECG 分析的基础模型，在大多数任务上提供了竞争性的性能，同时在低资源 режи下也表现出了效果。代码和预训练 веса公共分享在 https://github.com/klean2050/tiles_ecg_model 上。
</details></li>
</ul>
<hr>
<h2 id="Composable-Coresets-for-Determinant-Maximization-Greedy-is-Almost-Optimal"><a href="#Composable-Coresets-for-Determinant-Maximization-Greedy-is-Almost-Optimal" class="headerlink" title="Composable Coresets for Determinant Maximization: Greedy is Almost Optimal"></a>Composable Coresets for Determinant Maximization: Greedy is Almost Optimal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15286">http://arxiv.org/abs/2309.15286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Gollapudi, Sepideh Mahabadi, Varun Sivashankar</li>
<li>For: 本研究的目标是解决一个维度为 $d$ 的集合中选择 $k$ 个向量，以最大化向量的体积。* Methods: 本研究使用了 Determinantal point processes (DPP) 的 MAP-inference 任务，并在大量数据下进行研究。* Results: 我们提出了一种基于 Greedy 算法的可 compose 核心集合，可以在 $O(k)^{3k}$ 的准确因子下解决这个问题。此外，我们还证明了 Greedy 算法的本地优化性，可以在实际数据集上实现更高的准确性。<details>
<summary>Abstract</summary>
Given a set of $n$ vectors in $\mathbb{R}^d$, the goal of the \emph{determinant maximization} problem is to pick $k$ vectors with the maximum volume. Determinant maximization is the MAP-inference task for determinantal point processes (DPP) and has recently received considerable attention for modeling diversity. As most applications for the problem use large amounts of data, this problem has been studied in the relevant \textit{composable coreset} setting. In particular, [Indyk-Mahabadi-OveisGharan-Rezaei--SODA'20, ICML'19] showed that one can get composable coresets with optimal approximation factor of $\tilde O(k)^k$ for the problem, and that a local search algorithm achieves an almost optimal approximation guarantee of $O(k)^{2k}$. In this work, we show that the widely-used Greedy algorithm also provides composable coresets with an almost optimal approximation factor of $O(k)^{3k}$, which improves over the previously known guarantee of $C^{k^2}$, and supports the prior experimental results showing the practicality of the greedy algorithm as a coreset. Our main result follows by showing a local optimality property for Greedy: swapping a single point from the greedy solution with a vector that was not picked by the greedy algorithm can increase the volume by a factor of at most $(1+\sqrt{k})$. This is tight up to the additive constant $1$. Finally, our experiments show that the local optimality of the greedy algorithm is even lower than the theoretical bound on real data sets.
</details>
<details>
<summary>摘要</summary>
给定一组 $n$ 向量在 $\mathbb{R}^d$ 中，目标是选择 $k$ 个向量以最大化体积。这个问题被称为 determinant maximization 问题，是 determinantal point processes (DPP) 的MAP-推理任务，最近受到了各种应用的关注，以模型多样性。由于大多数应用都使用大量数据，因此这个问题在相关的可composable coreset 设定下进行研究。特别是，Indyk-Mahabadi-OveisGharan-Rezaei 在 SODA'20 和 ICML'19 上显示了可composable coreset 的优化因子为 $\tilde O(k)^k$，并且一个本地搜索算法可以达到 $O(k)^{2k}$ 的相对误差 guarantee。在这个工作中，我们表明了广泛使用的 Greedy 算法也可以提供可composable coreset 的近似因子 $O(k)^{3k}$，超过之前知道的 $C^{k^2}$  guarantee，并且支持先前的实验结果，证明 Greedy 算法在实际数据集上的实用性。我们的主要结论来自于 showing Greedy 算法的本地优化性ERT：将一个点从 Greedy 解决中拿换一个未被 Greedy 选择的向量可以提高体积的因子为最多 $(1+\sqrt{k})$，这是准确到 $1$ 的上限。最后，我们的实验结果表明 Greedy 算法的本地优化性实际比 теоретиче上的 bound 低。
</details></li>
</ul>
<hr>
<h2 id="A-Physics-Enhanced-Residual-Learning-PERL-Framework-for-Traffic-State-Prediction"><a href="#A-Physics-Enhanced-Residual-Learning-PERL-Framework-for-Traffic-State-Prediction" class="headerlink" title="A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction"></a>A Physics Enhanced Residual Learning (PERL) Framework for Traffic State Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15284">http://arxiv.org/abs/2309.15284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keke Long, Haotian Shi, Zihao Sheng, Xiaopeng Li, Sikai Chen</li>
<li>for: 本文提出了一种新的框架，即物理增强剩余学习（PERL）模型，用于解决物理模型和数据驱动模型的矛盾。</li>
<li>methods: 本文使用了物理模型和剩余学习模型，并将其 integrate into a single model。该模型的预测结果为物理模型的结果加上一个预测的偏差。</li>
<li>results: 实验结果表明，PERL模型在小数据集上比物理模型、数据驱动模型和PINN模型更好地预测行驶轨迹。此外，PERL模型在训练过程中更快地 converges，需要 fewer training samples than data-driven model和PINN模型。<details>
<summary>Abstract</summary>
In vehicle trajectory prediction, physics models and data-driven models are two predominant methodologies. However, each approach presents its own set of challenges: physics models fall short in predictability, while data-driven models lack interpretability. Addressing these identified shortcomings, this paper proposes a novel framework, the Physics-Enhanced Residual Learning (PERL) model. PERL integrates the strengths of physics-based and data-driven methods for traffic state prediction. PERL contains a physics model and a residual learning model. Its prediction is the sum of the physics model result and a predicted residual as a correction to it. It preserves the interpretability inherent to physics-based models and has reduced data requirements compared to data-driven methods. Experiments were conducted using a real-world vehicle trajectory dataset. We proposed a PERL model, with the Intelligent Driver Model (IDM) as its physics car-following model and Long Short-Term Memory (LSTM) as its residual learning model. We compare this PERL model with the physics car-following model, data-driven model, and other physics-informed neural network (PINN) models. The result reveals that PERL achieves better prediction with a small dataset, compared to the physics model, data-driven model, and PINN model. Second, the PERL model showed faster convergence during training, offering comparable performance with fewer training samples than the data-driven model and PINN model. Sensitivity analysis also proves comparable performance of PERL using another residual learning model and a physics car-following model.
</details>
<details>
<summary>摘要</summary>
在车辆轨迹预测中，物理模型和数据驱动模型是两种主要的方法。然而，每个方法都有自己的缺点：物理模型对预测不够可靠，而数据驱动模型则缺乏解释性。为了解决这些缺点，这篇论文提出了一个新的框架，即物理增强遗传学习（PERL）模型。PERL结合了物理基础和数据驱动方法的优点，并提供了更好的车辆轨迹预测。PERL模型包括物理模型和遗传学习模型。其预测结果为物理模型的结果加上一个预测的差异。这样可以保留物理模型中的解释性，并且需要更少的数据。我们在实际的车辆轨迹数据集上进行了实验，比较了PERL模型与物理汽车追随模型、数据驱动模型和物理启发阶层神经网络（PINN）模型。结果显示，PERL模型在小数据集情况下表现更好，比物理模型、数据驱动模型和PINN模型更好。其次，PERL模型在训练过程中更快地趋向于平衡，需要更少的训练数据，与数据驱动模型和PINN模型相比。另外，对PERL模型使用不同的遗传学习模型和物理汽车追随模型进行了敏感性分析，结果显示PERL模型在不同的模型和物理模型下仍然具有良好的预测性。
</details></li>
</ul>
<hr>
<h2 id="Identifying-factors-associated-with-fast-visual-field-progression-in-patients-with-ocular-hypertension-based-on-unsupervised-machine-learning"><a href="#Identifying-factors-associated-with-fast-visual-field-progression-in-patients-with-ocular-hypertension-based-on-unsupervised-machine-learning" class="headerlink" title="Identifying factors associated with fast visual field progression in patients with ocular hypertension based on unsupervised machine learning"></a>Identifying factors associated with fast visual field progression in patients with ocular hypertension based on unsupervised machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15867">http://arxiv.org/abs/2309.15867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqin Huang, Asma Poursoroush, Jian Sun, Michael V. Boland, Chris Johnson, Siamak Yousefi</li>
<li>for: 本研究旨在透过不监督学习机器学习技术，确定 ocular hypertension (OHT) 的不同趋势型域视场 (VF) 进程，以及发现导致快速 VF 进程的因素。</li>
<li>methods: 研究使用了潜在类混合模型 (LCMM)，通过标准自动报测 (SAP) 的 mean deviation (MD) 轨迹，确定 OHT 个体的不同类型。基于基线测试中的人口、临床、视觉和VF 因素，我们characterized 每个类型。然后，我们使用一般估计方程 (GEE) 确定导致快速 VF 进程的因素，并对结果进行质量和量化的解释。</li>
<li>results: LCMM 模型发现了四个眼睛类型，每个类型的 MD 衰减趋势不同。794 个眼睛（25%）、1675 个眼睛（54%）、531 个眼睛（17%）和 133 个眼睛（4%）分别被分为 Improvers、Stables、Slow progressors 和 Fast progressors。这些类型的 MD 衰减平均值分别为 0.08、-0.06、-0.21 和 -0.45 dB&#x2F;年。快速 VF 进程相关的因素包括基线年龄、内分泌压力 (IOP)、 Pattern standard deviation (PSD) 和 refractive error (RE)，但是低于中央肋壁厚度 (CCT)。fast progression 与 calcium channel blockers、男性、心血管疾病历史、糖尿病历史、非洲裔美国人、心脏病历史、 migraine headaches 有关。<details>
<summary>Abstract</summary>
Purpose: To identify ocular hypertension (OHT) subtypes with different trends of visual field (VF) progression based on unsupervised machine learning and to discover factors associated with fast VF progression. Participants: A total of 3133 eyes of 1568 ocular hypertension treatment study (OHTS) participants with at least five follow-up VF tests were included in the study. Methods: We used a latent class mixed model (LCMM) to identify OHT subtypes using standard automated perimetry (SAP) mean deviation (MD) trajectories. We characterized the subtypes based on demographic, clinical, ocular, and VF factors at the baseline. We then identified factors driving fast VF progression using generalized estimating equation (GEE) and justified findings qualitatively and quantitatively. Results: The LCMM model discovered four clusters (subtypes) of eyes with different trajectories of MD worsening. The number of eyes in clusters were 794 (25%), 1675 (54%), 531 (17%) and 133 (4%). We labelled the clusters as Improvers, Stables, Slow progressors, and Fast progressors based on their mean of MD decline, which were 0.08, -0.06, -0.21, and -0.45 dB/year, respectively. Eyes with fast VF progression had higher baseline age, intraocular pressure (IOP), pattern standard deviation (PSD) and refractive error (RE), but lower central corneal thickness (CCT). Fast progression was associated with calcium channel blockers, being male, heart disease history, diabetes history, African American race, stroke history, and migraine headaches.
</details>
<details>
<summary>摘要</summary>
目的：通过无监督机器学习发现 ocular hypertension (OHT) 的不同趋势视场（VF）进程 под型，并找到加速 VF 进程的因素。参与者：全部有 3133 个眼和 1568 名 ocular hypertension treatment study（OHTS）参与者，每个参与者至少有五次视场测试。方法：我们使用潜在类混合模型（LCMM）来发现 OHT 的不同 под型，使用标准自动测测（SAP）的 Mean Deviation（MD）轨迹。我们根据基线测试时的 демографи、临床、眼科和视场因素进行分类。然后，我们使用通用估计方法（GEE）来确定加速 VF 进程的因素，并证明发现的结论。结果：LCMM 模型发现了四种眼睛的不同趋势 MD 下降。这些群体的眼睛数量分别为 794（25%）、1675（54%）、531（17%）和 133（4%）。我们将这些群体分别命名为 Improvers、Stables、Slow progressors 和 Fast progressors，根据每个群体的 MD 下降的平均值。加速 VF 进程的眼睛具有更高的基线年龄、血压（IOP）、模式标准差（PSD）和视力错觉（RE），但更低的中央肾脏厚度（CCT）。加速 VF 进程与 calcium channel blockers、男性、心血管疾病历史、糖尿病历史、非洲裔美国人种、心血管疾病历史、 migraine 和头痛历史有关。
</details></li>
</ul>
<hr>
<h2 id="Method-and-Validation-for-Optimal-Lineup-Creation-for-Daily-Fantasy-Football-Using-Machine-Learning-and-Linear-Programming"><a href="#Method-and-Validation-for-Optimal-Lineup-Creation-for-Daily-Fantasy-Football-Using-Machine-Learning-and-Linear-Programming" class="headerlink" title="Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming"></a>Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15253">http://arxiv.org/abs/2309.15253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph M. Mahoney, Tomasz B. Paniak</li>
<li>for: This paper aims to develop a method to forecast NFL player performance under uncertainty and determine an optimal lineup to maximize FPTS under a set salary limit.</li>
<li>methods: The paper uses a supervised learning neural network to project FPTS based on past player performance, and a mixed integer linear program to find the optimal lineup.</li>
<li>results: The optimal lineups outperformed randomly-created lineups on average, and fell in approximately the 31st percentile (median) compared to real-world lineups from users on DraftKings.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是开发一种能够在不确定性下预测NFL球员表现的方法，以及一种可以在一定的薪资上限下最大化FPTS的优化阵容。</li>
<li>methods: 这篇论文使用一种监督学习神经网络来预测FPTS，并使用杂合Integer Linear Programming来找到最优阵容。</li>
<li>results: 优化的阵容在 randomly-created 阵容的平均上赢得了比赛，并且与DraftKings上用户的真实阵容相比，通常在 Approximately 31st percentile (medians) 之间。<details>
<summary>Abstract</summary>
Daily fantasy sports (DFS) are weekly or daily online contests where real-game performances of individual players are converted to fantasy points (FPTS). Users select players for their lineup to maximize their FPTS within a set player salary cap. This paper focuses on (1) the development of a method to forecast NFL player performance under uncertainty and (2) determining an optimal lineup to maximize FPTS under a set salary limit. A supervised learning neural network was created and used to project FPTS based on past player performance (2018 NFL regular season for this work) prior to the upcoming week. These projected FPTS were used in a mixed integer linear program to find the optimal lineup. The performance of resultant lineups was compared to randomly-created lineups. On average, the optimal lineups outperformed the random lineups. The generated lineups were then compared to real-world lineups from users on DraftKings. The generated lineups generally fell in approximately the 31st percentile (median). The FPTS methods and predictions presented here can be further improved using this study as a baseline comparison.
</details>
<details>
<summary>摘要</summary>
每周或每天的在线日常体育竞技 (DFS) 是一种在线竞技平台， Users可以选择球员来增加他们的极限积分 (FPTS)，而不超过球员薪资限额。这篇论文将 concentrate on (1) 预测 NFL 球员表现的方法的开发和 (2) 根据薪资限额最大化 FPTS 的优补策略。一种以过去 NFL 赛季 (2018 赛季) 的球员表现数据进行预测的超vised 学习神经网络被创建并使用，以预测下一周的 FPTS。这些预测的 FPTS 然后被用在杂integer linear program中找到最佳阵容。结果的阵容与Randomly创建的阵容进行比较，并发现了最佳阵容的性能较高。然后，这些阵容与 DraftKings 上用户实际创建的阵容进行比较，并发现了这些阵容在approximately 31% 的位置 (中位数)。这种 FPTS 预测和方法可以在这个基准比较中进行进一步改进。
</details></li>
</ul>
<hr>
<h2 id="V2X-Lead-LiDAR-based-End-to-End-Autonomous-Driving-with-Vehicle-to-Everything-Communication-Integration"><a href="#V2X-Lead-LiDAR-based-End-to-End-Autonomous-Driving-with-Vehicle-to-Everything-Communication-Integration" class="headerlink" title="V2X-Lead: LiDAR-based End-to-End Autonomous Driving with Vehicle-to-Everything Communication Integration"></a>V2X-Lead: LiDAR-based End-to-End Autonomous Driving with Vehicle-to-Everything Communication Integration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15252">http://arxiv.org/abs/2309.15252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyun Deng, Yanjun Shi, Weiming Shen</li>
<li>for: 本研究提出了一种基于LiDAR的端到端自动驾驶方法，通过与所有东西（V2X）通信集成，解决城市化环境下杂化交通条件下的自动驾驶挑战。</li>
<li>methods: 提议方法利用了车辆上的LiDAR感知器和V2X通信数据进行融合处理，采用了无模型和离线学习（DRL）算法来训练驾驶代理人，并采用了精心设计的奖励函数和多任务学习技术来提高驾驶代理人的泛化性。</li>
<li>results: 实验结果表明，提议方法可以在杂化交通条件下 traverse不监控交叉路口时提高安全性和效率，并在不同的驾驶任务和场景中进行泛化。V2X通信的集成提供了让AV更好地感知周围环境的重要数据源，从而提高了驾驶行为的准确性和完整性。<details>
<summary>Abstract</summary>
This paper presents a LiDAR-based end-to-end autonomous driving method with Vehicle-to-Everything (V2X) communication integration, termed V2X-Lead, to address the challenges of navigating unregulated urban scenarios under mixed-autonomy traffic conditions. The proposed method aims to handle imperfect partial observations by fusing the onboard LiDAR sensor and V2X communication data. A model-free and off-policy deep reinforcement learning (DRL) algorithm is employed to train the driving agent, which incorporates a carefully designed reward function and multi-task learning technique to enhance generalization across diverse driving tasks and scenarios. Experimental results demonstrate the effectiveness of the proposed approach in improving safety and efficiency in the task of traversing unsignalized intersections in mixed-autonomy traffic, and its generalizability to previously unseen scenarios, such as roundabouts. The integration of V2X communication offers a significant data source for autonomous vehicles (AVs) to perceive their surroundings beyond onboard sensors, resulting in a more accurate and comprehensive perception of the driving environment and more safe and robust driving behavior.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Homotopy-Relaxation-Training-Algorithms-for-Infinite-Width-Two-Layer-ReLU-Neural-Networks"><a href="#Homotopy-Relaxation-Training-Algorithms-for-Infinite-Width-Two-Layer-ReLU-Neural-Networks" class="headerlink" title="Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks"></a>Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15244">http://arxiv.org/abs/2309.15244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahong Yang, Qipin Chen, Wenrui Hao</li>
<li>for: 加速神经网络训练过程</li>
<li>methods: 提出了一种新的训练方法—Homotopy Relaxation Training Algorithm (HRTA)，通过连接线性活动函数和ReLU活动函数的homotopy活动函数，以及对训练精度进行放松来加速训练过程。</li>
<li>results: 经过对NTK上的深度神经网络的深入分析，显示HRTA可以提高训练速度的 convergence rates，特别是在宽度更大的神经网络中。实验结果也验证了理论结论。这种提出的HRTA具有在其他活动函数和深度神经网络中的潜在应用前景。<details>
<summary>Abstract</summary>
In this paper, we present a novel training approach called the Homotopy Relaxation Training Algorithm (HRTA), aimed at accelerating the training process in contrast to traditional methods. Our algorithm incorporates two key mechanisms: one involves building a homotopy activation function that seamlessly connects the linear activation function with the ReLU activation function; the other technique entails relaxing the homotopy parameter to enhance the training refinement process. We have conducted an in-depth analysis of this novel method within the context of the neural tangent kernel (NTK), revealing significantly improved convergence rates. Our experimental results, especially when considering networks with larger widths, validate the theoretical conclusions. This proposed HRTA exhibits the potential for other activation functions and deep neural networks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的训练方法，称为Homotopy Relaxation Training Algorithm（HRTA），目的是加速训练过程，而不是使用传统方法。我们的算法包含两个关键机制：一是建立一个连续函数 activation function，将线性 activation function 与 ReLU activation function 连续连接起来；另一个技术是通过放松 homotopy 参数来提高训练精度过程。我们在NTK 的背景下进行了深入的分析，发现该新方法可以提高训练速度。我们的实验结果，特别是考虑到大width 网络，证明了我们的理论结论。这种提出的 HRTA 具有潜在的应用前提，并不仅限于 activation functions 和深度神经网络。
</details></li>
</ul>
<hr>
<h2 id="Cross-Validation-for-Training-and-Testing-Co-occurrence-Network-Inference-Algorithms"><a href="#Cross-Validation-for-Training-and-Testing-Co-occurrence-Network-Inference-Algorithms" class="headerlink" title="Cross-Validation for Training and Testing Co-occurrence Network Inference Algorithms"></a>Cross-Validation for Training and Testing Co-occurrence Network Inference Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15225">http://arxiv.org/abs/2309.15225</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EngineerDanny/CS685-Microbe-Network-Research-Code">https://github.com/EngineerDanny/CS685-Microbe-Network-Research-Code</a></li>
<li>paper_authors: Daniel Agyapong, Jeffrey Ryan Propster, Jane Marks, Toby Dylan Hocking</li>
<li>for: 本研究旨在提出一种新的评估维度网络推断算法的方法，以及应用现有算法预测测试数据。</li>
<li>methods: 本研究使用了现有的网络推断算法，并提出了一种新的评估方法来评估网络推断算法的质量。</li>
<li>results: 研究发现，提出的评估方法可以帮助选择最佳的网络推断算法和评估网络推断算法的质量。<details>
<summary>Abstract</summary>
Microorganisms are found in almost every environment, including the soil, water, air, and inside other organisms, like animals and plants. While some microorganisms cause diseases, most of them help in biological processes such as decomposition, fermentation and nutrient cycling. A lot of research has gone into studying microbial communities in various environments and how their interactions and relationships can provide insights into various diseases. Co-occurrence network inference algorithms help us understand the complex associations of micro-organisms, especially bacteria. Existing network inference algorithms employ techniques such as correlation, regularized linear regression, and conditional dependence, which have different hyper-parameters that determine the sparsity of the network. Previous methods for evaluating the quality of the inferred network include using external data, and network consistency across sub-samples, both which have several drawbacks that limit their applicability in real microbiome composition data sets. We propose a novel cross-validation method to evaluate co-occurrence network inference algorithms, and new methods for applying existing algorithms to predict on test data. Our empirical study shows that the proposed method is useful for hyper-parameter selection (training) and comparing the quality of the inferred networks between different algorithms (testing).
</details>
<details>
<summary>摘要</summary>
微生物可以在各种环境中找到，包括土壤、水、空气以及其他生物体内。虽然一些微生物会引起疾病，但大多数微生物帮助进行生物过程，如腐败、发酵和营养循环。研究微生物社区在不同环境中的相互作用和关系可以提供疾病研究的意义。存在的网络推理算法可以帮助我们理解微生物之间的复杂关系，特别是细菌。现有的网络推理算法使用技术如相关性、规则化线性回归和conditional dependence，它们的各种超参数会影响网络的稀畴程度。以前的评估推理网络质量的方法包括使用外部数据和网络在不同抽样下的一致性，但这些方法有一些缺点，限制它们在实际微生物组成数据集中的应用。我们提议一种新的验证方法来评估推理网络推理算法，以及新的方法来应用现有算法预测测试数据。我们的实验显示，我们的方法有用于权重选择（训练）和对不同算法预测测试数据的质量进行比较。
</details></li>
</ul>
<hr>
<h2 id="Auto-grading-C-programming-assignments-with-CodeBERT-and-Random-Forest-Regressor"><a href="#Auto-grading-C-programming-assignments-with-CodeBERT-and-Random-Forest-Regressor" class="headerlink" title="Auto-grading C programming assignments with CodeBERT and Random Forest Regressor"></a>Auto-grading C programming assignments with CodeBERT and Random Forest Regressor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15216">http://arxiv.org/abs/2309.15216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roshan Vasu Muddaluru, Sharvaani Ravikumar Thoguluva, Shruti Prabha, Dr. Peeta Basa Pati, Ms. Roshni M Balakrishnan</li>
<li>for: 这个论文主要是为了描述如何使用深度学习自动评分编程作业，以减轻教师的评分负担，同时确保公正和有效的评分。</li>
<li>methods: 该论文使用了多种机器学习和深度学习方法，包括回归、卷积神经网络（CNN）和长短期记忆（LSTM），以及一种名为CodeBERT的代码基于转换器词嵌入模型。</li>
<li>results: 测试结果表明，使用该方法可以准确地评分C编程作业， Root Mean Squared Error（RMSE）为1.89。 这些结果表明，使用深度学习自动评分编程作业的方法比使用统计方法更有效。<details>
<summary>Abstract</summary>
Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
</details>
<details>
<summary>摘要</summary>
“手动评分程式作业是具有复杂性和主观性的，但使用深度学习可以简化这个任务。它 объектив地评估程式的质量，检测错误，并将分数划分给学生，从而减轻教师的负担，同时确保了公正和有效的评估。本研究通过机器学习和深度学习方法（如回归、单调数网络和长期缓存）进行自动评分C语言作业的分析。使用一个称为CodeBERT的程式码基于词汇嵌入模型，将文字式程式码转换为向量，然后将向量输入到不同模型中进行评分。测试结果显示了建议的策略的有效性，RMSE为1.89。研究中也讨论了统计方法和深度学习技术之间的比较。”Note: "Simplified Chinese" refers to the standardized form of Chinese used in mainland China and Singapore, which is different from "Traditional Chinese" used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="Balancing-Computational-Efficiency-and-Forecast-Error-in-Machine-Learning-based-Time-Series-Forecasting-Insights-from-Live-Experiments-on-Meteorological-Nowcasting"><a href="#Balancing-Computational-Efficiency-and-Forecast-Error-in-Machine-Learning-based-Time-Series-Forecasting-Insights-from-Live-Experiments-on-Meteorological-Nowcasting" class="headerlink" title="Balancing Computational Efficiency and Forecast Error in Machine Learning-based Time-Series Forecasting: Insights from Live Experiments on Meteorological Nowcasting"></a>Balancing Computational Efficiency and Forecast Error in Machine Learning-based Time-Series Forecasting: Insights from Live Experiments on Meteorological Nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15207">http://arxiv.org/abs/2309.15207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elin Törnquist, Wagner Costa Santos, Timothy Pogue, Nicholas Wingle, Robert A. Caulk<br>for:This paper aims to explore the relationship between computational cost and forecast error in machine learning-based time-series forecasting, using meteorological nowcasting as an example.methods:The paper employs various popular regression techniques, including XGBoost, FC-MLP, Transformer, and LSTM, for multi-horizon, short-term forecasting of temperature, wind speed, and cloud cover at multiple locations. The authors also propose two computational cost minimization methods: a novel auto-adaptive data reduction technique called Variance Horizon and a performance-based concept drift-detection mechanism.results:The results show that using the Variance Horizon technique can reduce computational usage by more than 50%, while increasing forecast error by up to 15%. Meanwhile, performance-based retraining can reduce computational usage by up to 90%, while improving forecast error by up to 10%. The combination of both techniques outperformed other model configurations by up to 99.7% when considering error normalized to computational usage.<details>
<summary>Abstract</summary>
Machine learning for time-series forecasting remains a key area of research. Despite successful application of many machine learning techniques, relating computational efficiency to forecast error remains an under-explored domain. This paper addresses this topic through a series of real-time experiments to quantify the relationship between computational cost and forecast error using meteorological nowcasting as an example use-case. We employ a variety of popular regression techniques (XGBoost, FC-MLP, Transformer, and LSTM) for multi-horizon, short-term forecasting of three variables (temperature, wind speed, and cloud cover) for multiple locations. During a 5-day live experiment, 4000 data sources were streamed for training and inferencing 144 models per hour. These models were parameterized to explore forecast error for two computational cost minimization methods: a novel auto-adaptive data reduction technique (Variance Horizon) and a performance-based concept drift-detection mechanism. Forecast error of all model variations were benchmarked in real-time against a state-of-the-art numerical weather prediction model. Performance was assessed using classical and novel evaluation metrics. Results indicate that using the Variance Horizon reduced computational usage by more than 50\%, while increasing between 0-15\% in error. Meanwhile, performance-based retraining reduced computational usage by up to 90\% while \emph{also} improving forecast error by up to 10\%. Finally, the combination of both the Variance Horizon and performance-based retraining outperformed other model configurations by up to 99.7\% when considering error normalized to computational usage.
</details>
<details>
<summary>摘要</summary>
“机器学习 для时间序列预测仍然是研究领域的关键领域。尽管许多机器学习技术已经得到成功应用，但将计算效率与预测误差之间的关系进行研究仍然是一个未探索的领域。这篇论文通过一系列实时实验来评估这个问题，使用天气预报为例子应用场景。我们使用了多种流行的回归技术（XGBoost、FC-MLP、Transformer和LSTM）进行多个地点的多时间档期预测温度、风速和云覆盖率。在5天的实验中，我们流动了4000个数据源，每小时训练和推断144个模型。这些模型被参数化以探索预测误差的两种计算成本减少方法：一种新的自适应数据减少技术（Variance Horizon）和一种基于性能的概念漂移检测机制。所有模型变化的预测误差都在实时比较之前的状态艺术天气预报模型。我们使用了传统和新的评价指标来评估性能。结果表明，使用Variance Horizon可以降低计算使用率超过50%，而预测误差也在0-15%之间增加。同时，基于性能的重新训练可以降低计算使用率达到90%，而同时也提高预测误差达到10%。最后，将Variance Horizon和基于性能的重新训练结合使用的模型在计算使用率normalized预测误差方面表现出色，高达99.7%。”
</details></li>
</ul>
<hr>
<h2 id="ICML-2023-Topological-Deep-Learning-Challenge-Design-and-Results"><a href="#ICML-2023-Topological-Deep-Learning-Challenge-Design-and-Results" class="headerlink" title="ICML 2023 Topological Deep Learning Challenge : Design and Results"></a>ICML 2023 Topological Deep Learning Challenge : Design and Results</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15188">http://arxiv.org/abs/2309.15188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pyt-team/topomodelx">https://github.com/pyt-team/topomodelx</a></li>
<li>paper_authors: Mathilde Papillon, Mustafa Hajij, Florian Frantzen, Josef Hoppe, Helen Jenne, Johan Mathe, Audun Myers, Theodore Papamarkou, Michael T. Schaub, Ghada Zamzmi, Tolga Birdal, Tamal Dey, Tim Doster, Tegan Emerson, Gurusankar Gopalakrishnan, Devendra Govil, Vincent Grande, Aldo Guzmán-Sáenz, Henry Kvinge, Neal Livesay, Jan Meisner, Soham Mukherjee, Shreyas N. Samaga, Karthikeyan Natesan Ramamurthy, Maneel Reddy Karri, Paul Rosen, Sophia Sanborn, Michael Scholkemper, Robin Walters, Jens Agerberg, Georg Bökman, Sadrodin Barikbin, Claudio Battiloro, Gleb Bazhenov, Guillermo Bernardez, Aiden Brent, Sergio Escalera, Simone Fiorellino, Dmitrii Gavrilev, Mohammed Hassanin, Paul Häusner, Odin Hoff Gardaa, Abdelwahed Khamis, Manuel Lecha, German Magai, Tatiana Malygina, Pavlo Melnyk, Rubén Ballester, Kalyan Nadimpalli, Alexander Nikitin, Abraham Rabinowitz, Alessandro Salatiello, Simone Scardapane, Luca Scofano, Suraj Singh, Jens Sjölund, Pavel Snopov, Indro Spinelli, Lev Telyatnikov, Lucia Testa, Maosheng Yang, Yixiao Yue, Olga Zaghen, Ali Zia, Nina Miolane</li>
<li>for: 本研究是一项计算挑战，探讨了拓扑深度学习的计算问题。</li>
<li>methods: 本研究使用了开源的Python包TopoNetX和TopoModelX进行数据处理和深度学习。</li>
<li>results: 研究得到了28个合格的提交，并summarizes了挑战的主要发现。<details>
<summary>Abstract</summary>
This paper presents the computational challenge on topological deep learning that was hosted within the ICML 2023 Workshop on Topology and Geometry in Machine Learning. The competition asked participants to provide open-source implementations of topological neural networks from the literature by contributing to the python packages TopoNetX (data processing) and TopoModelX (deep learning). The challenge attracted twenty-eight qualifying submissions in its two-month duration. This paper describes the design of the challenge and summarizes its main findings.
</details>
<details>
<summary>摘要</summary>
Note:*  topological neural networks 改为  topological deep learning*  data processing 改为 数据处理*  deep learning 改为 深度学习
</details></li>
</ul>
<hr>
<h2 id="Monitoring-Machine-Learning-Models-Online-Detection-of-Relevant-Deviations"><a href="#Monitoring-Machine-Learning-Models-Online-Detection-of-Relevant-Deviations" class="headerlink" title="Monitoring Machine Learning Models: Online Detection of Relevant Deviations"></a>Monitoring Machine Learning Models: Online Detection of Relevant Deviations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15187">http://arxiv.org/abs/2309.15187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Heinrichs</li>
<li>for: 本研究旨在提供一种可靠地检测机器学习模型性能下降的方法，以维护模型的可靠性。</li>
<li>methods: 本研究提议一种顺序监测方案，通过考虑时间依赖性来减少不必要的警报和多测试问题。</li>
<li>results: 实验结果表明，本方法可以更好地检测机器学习模型性能下降，比 benchmark 方法更有效。<details>
<summary>Abstract</summary>
Machine learning models are essential tools in various domains, but their performance can degrade over time due to changes in data distribution or other factors. On one hand, detecting and addressing such degradations is crucial for maintaining the models' reliability. On the other hand, given enough data, any arbitrary small change of quality can be detected. As interventions, such as model re-training or replacement, can be expensive, we argue that they should only be carried out when changes exceed a given threshold. We propose a sequential monitoring scheme to detect these relevant changes. The proposed method reduces unnecessary alerts and overcomes the multiple testing problem by accounting for temporal dependence of the measured model quality. Conditions for consistency and specified asymptotic levels are provided. Empirical validation using simulated and real data demonstrates the superiority of our approach in detecting relevant changes in model quality compared to benchmark methods. Our research contributes a practical solution for distinguishing between minor fluctuations and meaningful degradations in machine learning model performance, ensuring their reliability in dynamic environments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SGD-Finds-then-Tunes-Features-in-Two-Layer-Neural-Networks-with-near-Optimal-Sample-Complexity-A-Case-Study-in-the-XOR-problem"><a href="#SGD-Finds-then-Tunes-Features-in-Two-Layer-Neural-Networks-with-near-Optimal-Sample-Complexity-A-Case-Study-in-the-XOR-problem" class="headerlink" title="SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem"></a>SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15111">http://arxiv.org/abs/2309.15111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Margalit Glasgow</li>
<li>for: 这个论文研究了使用小批量随机梯度下降（SGD）来有效地学习二层神经网络。</li>
<li>methods: 论文使用了标准的SGD算法和Logistic损失函数来训练神经网络。同时，论文同时训练了两层神经网络的两个层。</li>
<li>results: 论文证明了，当数据来自$d$维布尔体系，使用$d$个polylog($d$)样本来训练二层神经网络，可以达到 populaion error $o(1)$。这是首次有人提供了$\tilde{O}(d)$的样本复杂度来有效地学习XOR函数在标准神经网络上。<details>
<summary>Abstract</summary>
In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, where SGD maintains and balances the features. We leverage the simultaneous training of the layers to show that it is sufficient for only a small fraction of the neurons to learn features, since those neurons will be amplified by the simultaneous growth of their second layer weights.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑了批处理式随机梯度下降（SGD）在二层神经网络上的优化过程。我们证明了，当数据来自 $d$ 维布尔多面体，并且标注为 quadratic “XOR” 函数 $y = -x_ix_j$，然后可以在 $d$ 个polylog（d）样本上培养到 population error $o(1)$。我们的结果同时考虑了两层神经网络中的两个层的标准批处理SGD的训练。根据我们所知，这是首次为效率地学习 XOR 函数在各向同性数据上的标准神经网络进行 $\tilde{O}(d)$ 样本的证明。我们的主要技巧是显示网络在两个阶段中进行发展：一个 $\textit{signal-finding}$ 阶段，在这个阶段，网络很小，许多神经元独立地找到特征；另一个 $\textit{signal-heavy}$ 阶段，SGD在维护和平衡特征。我们利用同时训练层的技巧，显示只需要一小部分神经元学习特征，因为这些神经元将在同时增长其第二层权重时被扩大。
</details></li>
</ul>
<hr>
<h2 id="Fixing-the-NTK-From-Neural-Network-Linearizations-to-Exact-Convex-Programs"><a href="#Fixing-the-NTK-From-Neural-Network-Linearizations-to-Exact-Convex-Programs" class="headerlink" title="Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs"></a>Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15096">http://arxiv.org/abs/2309.15096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajat Vadiraj Dwaraknath, Tolga Ergen, Mert Pilanci</li>
<li>for: 本研究探讨了深度神经网络的理论分析，具体来说是对SGD训练的推理和ReLU网络的正则化训练目标的全面优化。</li>
<li>methods: 本研究使用了多kernel学习（MKL）模型和迭代重量调整来解决深度神经网络的训练问题。</li>
<li>results: 研究发现，对于某些特定的掩码量，NTK不能在训练集上达到最佳性能，而MKL kernel则可以达到最佳性能。通过iterative reweighting，我们可以从NTK中获得最佳MKLkernel，并且提供了一些数值实验来证明我们的理论。<details>
<summary>Abstract</summary>
Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the training data. A consequence of this lack of dependence on the targets is that the NTK cannot perform better than the optimal MKL kernel on the training set. By using iterative reweighting, we improve the weights induced by the NTK to obtain the optimal MKL kernel which is equivalent to the solution of the exact convex reformulation of the gated ReLU network. We also provide several numerical simulations corroborating our theory. Additionally, we provide an analysis of the prediction error of the resulting optimal kernel via consistency results for the group lasso.
</details>
<details>
<summary>摘要</summary>
近来，深度神经网络的理论分析主要集中在两个方向上：1）通过SGD训练在无穷层宽和学习率为零的极限下提供神经网络训练的洞察，使用神经 Tangent Kernel (NTK)；2）globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks。后者的研究方向还产生了一种称为闭合ReLU网络的代表形式，可以通过高效的无约束凸 програм序global optimiztion。在这个工作中，我们将这个凸程序解释为一种多重kernel学习（MKL）模型，并与NTK建立连接。具体来说，我们证明在某些掩码权重不виси于学习目标时，这个kernel与NTK相同，这个kernel是gated ReLU网络的NTK在训练数据上。由于这些掩码权重不виси于学习目标，NTK无法在训练集上做出更好的性能。通过迭代重新权重，我们可以改进由NTK引入的权重，以获得最佳的MKL kernel，这个kernel与gated ReLU网络的凸形式的正确解相同。我们还提供了一些数值实验证明我们的理论。此外，我们还提供了预测误差的分析，使用群集lasso的一致性结果。
</details></li>
</ul>
<hr>
<h2 id="Automated-Detection-of-Persistent-Inflammatory-Biomarkers-in-Post-COVID-19-Patients-Using-Machine-Learning-Techniques"><a href="#Automated-Detection-of-Persistent-Inflammatory-Biomarkers-in-Post-COVID-19-Patients-Using-Machine-Learning-Techniques" class="headerlink" title="Automated Detection of Persistent Inflammatory Biomarkers in Post-COVID-19 Patients Using Machine Learning Techniques"></a>Automated Detection of Persistent Inflammatory Biomarkers in Post-COVID-19 Patients Using Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15838">http://arxiv.org/abs/2309.15838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ghizal Fatima, Fadhil G. Al-Amran, Maitham G. Yousif<br>for: 这份研究的目的是探索机器学习技术可以自动识别潜在的急性炎症生物 markers在 COVID-19 后期病人中，以提高医疗对病人的早期诊断和个性化治疗策略。methods: 这份研究使用了多种机器学习算法，包括逻辑回传、随机森林、支持向量机器和渐进提升，将资料进行了严格的数据预processing和特征选择，以便优化资料集供机器学习分析。results: 这份研究发现，使用机器学习技术可以实现高精度和确定性地自动识别 COVID-19 后期病人中的急性炎症生物 markers，并且这些模型可以作为医疗 provider 的有用工具，帮助早期诊断和个性化治疗策略，最终对 COVID-19 后期病人的康康和生活质量有所提高。<details>
<summary>Abstract</summary>
The COVID-19 pandemic has left a lasting impact on individuals, with many experiencing persistent symptoms, including inflammation, in the post-acute phase of the disease. Detecting and monitoring these inflammatory biomarkers is critical for timely intervention and improved patient outcomes. This study employs machine learning techniques to automate the identification of persistent inflammatory biomarkers in 290 post-COVID-19 patients, based on medical data collected from hospitals in Iraq. The data encompassed a wide array of clinical parameters, such as C-reactive protein and interleukin-6 levels, patient demographics, comorbidities, and treatment histories. Rigorous data preprocessing and feature selection processes were implemented to optimize the dataset for machine learning analysis. Various machine learning algorithms, including logistic regression, random forests, support vector machines, and gradient boosting, were deployed to construct predictive models. These models exhibited promising results, showcasing high accuracy and precision in the identification of patients with persistent inflammation. The findings of this study underscore the potential of machine learning in automating the detection of persistent inflammatory biomarkers in post-COVID-19 patients. These models can serve as valuable tools for healthcare providers, facilitating early diagnosis and personalized treatment strategies for individuals at risk of persistent inflammation, ultimately contributing to improved post-acute COVID-19 care and patient well-being. Keywords: COVID-19, post-COVID-19, inflammation, biomarkers, machine learning, early detection.
</details>
<details>
<summary>摘要</summary>
COVID-19 大流行对个人有持续影响，许多人在后途期患有持续的发炎症状。检测和监测这些发炎生物标志是关键，以确定患者的病情和提高患者的结果。这项研究使用机器学习技术自动识别290名患上 COVID-19 后期的患者中的持续发炎生物标志，基于伊拉克医院收集的医疗数据。数据包括丰富的临床参数，如 C-反抗蛋白和Interleukin-6 水平、患者人口、相关疾病和治疗历史。经过严格的数据预处理和特征选择过程，以便优化数据集 для机器学习分析。不同的机器学习算法，包括逻辑回归、Random Forest、支持向量机和梯度提升，被部署来构建预测模型。这些模型表现出色，展现了高精度和准确性在识别持续发炎患者方面。这些发现反映了机器学习在自动识别持续发炎生物标志方面的潜在潜力。这些模型可以作为医疗提供者的有价值工具，帮助早期诊断和个性化治疗策略，以提高后途期 COVID-19 患者的健康状况。关键词：COVID-19, 后途期 COVID-19, 发炎, 生物标志, 机器学习, 早期诊断.
</details></li>
</ul>
<hr>
<h2 id="Identifying-Simulation-Model-Through-Alternative-Techniques-for-a-Medical-Device-Assembly-Process"><a href="#Identifying-Simulation-Model-Through-Alternative-Techniques-for-a-Medical-Device-Assembly-Process" class="headerlink" title="Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process"></a>Identifying Simulation Model Through Alternative Techniques for a Medical Device Assembly Process</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15094">http://arxiv.org/abs/2309.15094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatemeh Kakavandi</li>
<li>for: 这篇论文探讨了两种不同的方法来识别和估算生产过程模型，尤其是在医疗器械组装中关键的快照过程。</li>
<li>methods: 这两种方法分别使用spline函数和机器学习（ML）模型来识别模型。</li>
<li>results: 这些方法可以创建适应性强的模型，准确地表示快照过程，并且可以满足不同的enario。这些模型有助于进一步了解生产过程，帮助决策，特别当数据有限时。<details>
<summary>Abstract</summary>
This scientific paper explores two distinct approaches for identifying and approximating the simulation model, particularly in the context of the snap process crucial to medical device assembly. Simulation models play a pivotal role in providing engineers with insights into industrial processes, enabling experimentation and troubleshooting before physical assembly. However, their complexity often results in time-consuming computations.   To mitigate this complexity, we present two distinct methods for identifying simulation models: one utilizing Spline functions and the other harnessing Machine Learning (ML) models. Our goal is to create adaptable models that accurately represent the snap process and can accommodate diverse scenarios. Such models hold promise for enhancing process understanding and aiding in decision-making, especially when data availability is limited.
</details>
<details>
<summary>摘要</summary>
这篇科学论文探讨了两种不同的方法来识别和估算模拟模型，尤其是在医疗器械组装过程中的快照过程中。模拟模型在工程师获得工业过程的洞察力方面发挥着关键作用，允许他们在实际组装之前进行实验和排查。然而，它们的复杂性常常导致计算时间很长。为了缓解这种复杂性，我们提出了使用spline函数和机器学习（ML）模型来识别模拟模型的两种方法。我们的目标是创造一些适应性强的模型，能够准确地表示快照过程并适应多种情况。这些模型在数据有限情况下能够提供进程理解和决策支持。
</details></li>
</ul>
<hr>
<h2 id="Single-Biological-Neurons-as-Temporally-Precise-Spatio-Temporal-Pattern-Recognizers"><a href="#Single-Biological-Neurons-as-Temporally-Precise-Spatio-Temporal-Pattern-Recognizers" class="headerlink" title="Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers"></a>Single Biological Neurons as Temporally Precise Spatio-Temporal Pattern Recognizers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15090">http://arxiv.org/abs/2309.15090</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Beniaguev</li>
<li>for: 本论文主要研究的中心思想是认为单个神经元在大脑应该被视为高精度的时间特征和空间特征 Pattern recognizer。</li>
<li>methods: 本论文使用了单个神经元的计算性质和生物学性质来探讨大脑中神经元之间的各种循环和信息编码方式。</li>
<li>results: 研究表明，单个神经元的计算特征有较大的系统级别影响，并且可以用简单可靠的学习规则来模拟大脑中的非线性XOR操作。<details>
<summary>Abstract</summary>
This PhD thesis is focused on the central idea that single neurons in the brain should be regarded as temporally precise and highly complex spatio-temporal pattern recognizers. This is opposed to the prevalent view of biological neurons as simple and mainly spatial pattern recognizers by most neuroscientists today. In this thesis, I will attempt to demonstrate that this is an important distinction, predominantly because the above-mentioned computational properties of single neurons have far-reaching implications with respect to the various brain circuits that neurons compose, and on how information is encoded by neuronal activity in the brain. Namely, that these particular "low-level" details at the single neuron level have substantial system-wide ramifications. In the introduction we will highlight the main components that comprise a neural microcircuit that can perform useful computations and illustrate the inter-dependence of these components from a system perspective. In chapter 1 we discuss the great complexity of the spatio-temporal input-output relationship of cortical neurons that are the result of morphological structure and biophysical properties of the neuron. In chapter 2 we demonstrate that single neurons can generate temporally precise output patterns in response to specific spatio-temporal input patterns with a very simple biologically plausible learning rule. In chapter 3, we use the differentiable deep network analog of a realistic cortical neuron as a tool to approximate the gradient of the output of the neuron with respect to its input and use this capability in an attempt to teach the neuron to perform nonlinear XOR operation. In chapter 4 we expand chapter 3 to describe extension of our ideas to neuronal networks composed of many realistic biological spiking neurons that represent either small microcircuits or entire brain regions.
</details>
<details>
<summary>摘要</summary>
这个博士论文主要关注的中心思想是：单个神经元在脑中应该被视为高精度的空间-时间模式识别器，而不是现在大多数神经科学家视为简单的空间模式识别器。在这个论文中，我会尝试证明这是一个重要的分别，因为单个神经元的计算属性在脑内部的各种神经细胞圈和信息编码方面具有广泛的系统性影响。在引言中，我们将高亮显示神经元微circuit中的主要组件，并 illustrate它们之间的互dependent关系从系统角度。在第一章中，我们讨论了触发区域中神经元的复杂的空间-时间输入-输出关系，这些关系是由神经元的形态结构和生物物理特性所导致。在第二章中，我们展示了单个神经元可以在特定的空间-时间输入模式下生成高精度的输出模式，使用了一种简单的生物可能的学习规则。在第三章中，我们使用一个可微分的深度网络模型来估算神经元输出的梯度，并使用这种能力来教育神经元执行非线性XOR操作。在第四章中，我们扩展了上述想法，描述了使用多个真实生物快速射精神神经元组成的神经网络，代表了小微circuit或整个脑区域。
</details></li>
</ul>
<hr>
<h2 id="On-Excess-Risk-Convergence-Rates-of-Neural-Network-Classifiers"><a href="#On-Excess-Risk-Convergence-Rates-of-Neural-Network-Classifiers" class="headerlink" title="On Excess Risk Convergence Rates of Neural Network Classifiers"></a>On Excess Risk Convergence Rates of Neural Network Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15075">http://arxiv.org/abs/2309.15075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunouk Ko, Namjoon Suh, Xiaoming Huo</li>
<li>for: This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting, with a focus on their excess risks.</li>
<li>methods: The paper uses a more general scenario that resembles actual practice, with the function class including the Barron functions as a proper subset, and the neural network classifier is constructed as the minimizer of a surrogate loss.</li>
<li>results: The paper obtains a dimension-free, uniform rate of convergence for the excess risk, and shows that the rate is minimax optimal up to a logarithmic factor. The paper also demonstrates the effect of the margin assumption in this regime.<details>
<summary>Abstract</summary>
The recent success of neural networks in pattern recognition and classification problems suggests that neural networks possess qualities distinct from other more classical classifiers such as SVMs or boosting classifiers. This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting as measured by their excess risks. Compared to the typical settings imposed in the literature, we consider a more general scenario that resembles actual practice in two respects: first, the function class to be approximated includes the Barron functions as a proper subset, and second, the neural network classifier constructed is the minimizer of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based numerical optimizations can be easily applied. While the class of functions we consider is quite large that optimal rates cannot be faster than $n^{-\frac{1}{3}$, it is a regime in which dimension-free rates are possible and approximation power of neural networks can be taken advantage of. In particular, we analyze the estimation and approximation properties of neural networks to obtain a dimension-free, uniform rate of convergence for the excess risk. Finally, we show that the rate obtained is in fact minimax optimal up to a logarithmic factor, and the minimax lower bound shows the effect of the margin assumption in this regime.
</details>
<details>
<summary>摘要</summary>
近期，神经网络在图像识别和分类问题中的成功表明了神经网络具有与传统分类器不同的特点。这篇论文研究基于神经网络的插入分类器在二分类 Setting 中的性能，并且使用过程损失来衡量其剩余风险。与文献中常见的设定相比，我们在两个方面进行了更加实际的设定：首先，函数类型包括Barron函数作为一个 Correct 子集，其次，使用surrogate损失函数来构建神经网络分类器，以便使用梯度下降的数值优化。尽管我们考虑的函数集是非常大，但是我们可以在这个 regime 中获得约度独立的速度，并且利用神经网络的近似能力。特别是，我们分析神经网络的估计和抽象性特性，以获得约度独立的异常速度。最后，我们证明了获得的速度实际上是最优的，并且显示了margin假设在这个regime中的效果。
</details></li>
</ul>
<hr>
<h2 id="Targeting-Relative-Risk-Heterogeneity-with-Causal-Forests"><a href="#Targeting-Relative-Risk-Heterogeneity-with-Causal-Forests" class="headerlink" title="Targeting Relative Risk Heterogeneity with Causal Forests"></a>Targeting Relative Risk Heterogeneity with Causal Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15793">http://arxiv.org/abs/2309.15793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vshirvaikar/rrcf">https://github.com/vshirvaikar/rrcf</a></li>
<li>paper_authors: Vik Shirvaikar, Chris Holmes</li>
<li>for:  This paper focuses on the problem of treatment effect heterogeneity (TEH) in clinical trial analysis, and proposes a method for modifying causal forests to target relative risk using a novel node-splitting procedure based on generalized linear model (GLM) comparison.</li>
<li>methods: The proposed method uses a modified version of causal forests, which is a highly popular method for detecting TEH, but with a focus on relative risk instead of absolute risk. The method uses a novel node-splitting procedure based on GLM comparison to capture nuance in the relative risk.</li>
<li>results: The results of the paper show that the proposed relative risk causal forests method can capture otherwise unobserved sources of heterogeneity, as demonstrated on simulated and real-world data.<details>
<summary>Abstract</summary>
Treatment effect heterogeneity (TEH), or variability in treatment effect for different subgroups within a population, is of significant interest in clinical trial analysis. Causal forests (Wager and Athey, 2018) is a highly popular method for this problem, but like many other methods for detecting TEH, its criterion for separating subgroups focuses on differences in absolute risk. This can dilute statistical power by masking nuance in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk using a novel node-splitting procedure based on generalized linear model (GLM) comparison. We present results on simulated and real-world data that suggest relative risk causal forests can capture otherwise unobserved sources of heterogeneity.
</details>
<details>
<summary>摘要</summary>
клиниче观察数据分析中，受试者群体内部效果差异（TEH）的研究具有重要意义。 causal forests（Wager和Athey，2018）是这种问题的高度流行方法，但与其他TEH检测方法一样，它的分组标准是基于绝对风险的差异。这可能会削弱统计能力，因为它会隐藏对积分风险的细节，这通常是临床兴趣的量。在这种工作中，我们提议和实现了一种修改 causal forests 以target相对风险的方法，使用一种基于泛化线性模型（GLM）的新的节点拆分方法。我们在模拟和实际数据上的结果表明，相对风险 causal forests 可以捕捉到其他不可见的差异源。
</details></li>
</ul>
<hr>
<h2 id="QUILT-Effective-Multi-Class-Classification-on-Quantum-Computers-Using-an-Ensemble-of-Diverse-Quantum-Classifiers"><a href="#QUILT-Effective-Multi-Class-Classification-on-Quantum-Computers-Using-an-Ensemble-of-Diverse-Quantum-Classifiers" class="headerlink" title="QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers"></a>QUILT: Effective Multi-Class Classification on Quantum Computers Using an Ensemble of Diverse Quantum Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15056">http://arxiv.org/abs/2309.15056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Silver, Tirthak Patel, Devesh Tiwari</li>
<li>for: 这篇论文是用于描述一个名为 Quilt 的框架，用于进行多类别分类任务，并且能够在现有的误差多的量子计算机上进行有效运行。</li>
<li>methods: 这篇论文使用了量子计算机上的误差多的状况，并且使用了一些特别的算法来实现多类别分类任务。</li>
<li>results: 根据论文的报告，使用 Quilt 框架进行多类别分类任务，可以在现有的五边系统上达到85%的准确率，使用 MNIST 数据集。<details>
<summary>Abstract</summary>
Quantum computers can theoretically have significant acceleration over classical computers; but, the near-future era of quantum computing is limited due to small number of qubits that are also error prone. Quilt is a framework for performing multi-class classification task designed to work effectively on current error-prone quantum computers. Quilt is evaluated with real quantum machines as well as with projected noise levels as quantum machines become more noise-free. Quilt demonstrates up to 85% multi-class classification accuracy with the MNIST dataset on a five-qubit system.
</details>
<details>
<summary>摘要</summary>
量子计算机在理论上可能具有显著的加速效果比经典计算机更快;但是，近期量子计算机的时代受到有限的量子比特数和错误率的限制。Quilt是一个用于实现多类分类任务的框架，针对当前的错误率量子计算机进行设计。Quilt在真正的量子机器上以及预计的噪声水平下进行评估。Quilt在MNIST数据集上的五个量子比特系统上达到85%多类分类精度。
</details></li>
</ul>
<hr>
<h2 id="Synthia’s-Melody-A-Benchmark-Framework-for-Unsupervised-Domain-Adaptation-in-Audio"><a href="#Synthia’s-Melody-A-Benchmark-Framework-for-Unsupervised-Domain-Adaptation-in-Audio" class="headerlink" title="Synthia’s Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio"></a>Synthia’s Melody: A Benchmark Framework for Unsupervised Domain Adaptation in Audio</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15024">http://arxiv.org/abs/2309.15024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cynthpie/synthia_melody">https://github.com/cynthpie/synthia_melody</a></li>
<li>paper_authors: Chia-Hsin Lin, Charles Jones, Björn W. Schuller, Harry Coppock</li>
<li>for: 本研究的目的是提供一个不受观察 bias 的音频数据生成框架，用于测试音频深度学习模型对不同水平的分布偏移的抵抗力。</li>
<li>methods: 该研究使用了一种新的数据生成框架 called Synthia’s melody，可以生成具有用户指定的障碍结构的无数种4秒钢琴 melody。</li>
<li>results: 经测试表明，Synthia’s melody 可以提供一个不受 observation bias 的测试环境，用于评估音频深度学习模型对分布偏移的抵抗力。<details>
<summary>Abstract</summary>
Despite significant advancements in deep learning for vision and natural language, unsupervised domain adaptation in audio remains relatively unexplored. We, in part, attribute this to the lack of an appropriate benchmark dataset. To address this gap, we present Synthia's melody, a novel audio data generation framework capable of simulating an infinite variety of 4-second melodies with user-specified confounding structures characterised by musical keys, timbre, and loudness. Unlike existing datasets collected under observational settings, Synthia's melody is free of unobserved biases, ensuring the reproducibility and comparability of experiments. To showcase its utility, we generate two types of distribution shifts-domain shift and sample selection bias-and evaluate the performance of acoustic deep learning models under these shifts. Our evaluations reveal that Synthia's melody provides a robust testbed for examining the susceptibility of these models to varying levels of distribution shift.
</details>
<details>
<summary>摘要</summary>
尽管深度学习在视觉和自然语言领域取得了 significative 进步，但无监督频谱适应仍然相对未explored。我们认为这是因为缺乏适当的标准 benchmark 数据集。为了填补这个遗漏，我们提出了 Synthia 的旋律，一种新的音频数据生成框架，可以生成无数个 4 秒旋律，并且可以根据用户指定的隐藏结构（音频键、 timbre 和响度）进行定制。不同于现有的观察型数据集，Synthia 的旋律不受不观察到的偏见影响，因此可以保证实验的重复性和比较性。为了展示其 utility，我们生成了两种类型的分布转移-频谱转移和样本选择偏见-并评估了这些模型在这些转移下的性能。我们的评估结果表明，Synthia 的旋律提供了一个可靠的测试床 для检验这些模型对不同水平的分布转移的抗性。
</details></li>
</ul>
<hr>
<h2 id="Tempo-Adaptation-in-Non-stationary-Reinforcement-Learning"><a href="#Tempo-Adaptation-in-Non-stationary-Reinforcement-Learning" class="headerlink" title="Tempo Adaptation in Non-stationary Reinforcement Learning"></a>Tempo Adaptation in Non-stationary Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14989">http://arxiv.org/abs/2309.14989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunin Lee, Yuhao Ding, Jongmin Lee, Ming Jin, Javad Lavaei, Somayeh Sojoudi</li>
<li>for: 解决非站点RL中时间同步问题，提高RL在实际应用中的可行性。</li>
<li>methods: 提出了一种名为Proactively Synchronizing Tempo（$\texttt{ProST}$）框架，通过计算一个子优化的时间序列({$t_{1:K}$})来最小化动态 regret。</li>
<li>results: 实验证明，$\texttt{ProST}$框架在多维非站点环境中实现了较高的在线返点，并且比既有方法更具有可行性。<details>
<summary>Abstract</summary>
We first raise and tackle a ``time synchronization'' issue between the agent and the environment in non-stationary reinforcement learning (RL), a crucial factor hindering its real-world applications. In reality, environmental changes occur over wall-clock time ($t$) rather than episode progress ($k$), where wall-clock time signifies the actual elapsed time within the fixed duration $t \in [0, T]$. In existing works, at episode $k$, the agent rolls a trajectory and trains a policy before transitioning to episode $k+1$. In the context of the time-desynchronized environment, however, the agent at time $t_{k}$ allocates $\Delta t$ for trajectory generation and training, subsequently moves to the next episode at $t_{k+1}=t_{k}+\Delta t$. Despite a fixed total number of episodes ($K$), the agent accumulates different trajectories influenced by the choice of interaction times ($t_1,t_2,...,t_K$), significantly impacting the suboptimality gap of the policy. We propose a Proactively Synchronizing Tempo ($\texttt{ProST}$) framework that computes a suboptimal sequence {$t_1,t_2,...,t_K$} (= { $t_{1:K}$}) by minimizing an upper bound on its performance measure, i.e., the dynamic regret. Our main contribution is that we show that a suboptimal {$t_{1:K}$} trades-off between the policy training time (agent tempo) and how fast the environment changes (environment tempo). Theoretically, this work develops a suboptimal {$t_{1:K}$} as a function of the degree of the environment's non-stationarity while also achieving a sublinear dynamic regret. Our experimental evaluation on various high-dimensional non-stationary environments shows that the $\texttt{ProST}$ framework achieves a higher online return at suboptimal {$t_{1:K}$} than the existing methods.
</details>
<details>
<summary>摘要</summary>
我们首先面临到非站点学习（RL）中的时间同步问题，这是实际应用中的一个关键因素。在实际情况下，环境变化发生在墙上时间（$t$)而不是 episodenumber（$k$)，即墙上时间表示实际过去的时间在固定时间段[$0,T$]中。现有的方法在每个 episodenumber $k$ 中，Agent在 $k$  episodenumber 中生成 trajectory 并训练策略，然后在 $k+1$  episodenumber 中继续。但在时间不同步的环境中，Agent 在 $t_k$  allocate $\Delta t$  для trajectory 生成和策略训练，然后在 $t_{k+1} = t_k + \Delta t$ 中移动到下一个 episodenumber。尽管有固定的总集数 ($K$)，但 Agent 在不同的交互时间 ($t_1, t_2, ..., t_K$) 中收集了不同的 trajectory，这会对策略的优化差值产生显著的影响。我们提出了一个名为 Proactively Synchronizing Tempo（ $\texttt{ProST}$）框架，该框架计算一个不优的序列 {$t_1, t_2, ..., t_K$} (= { $t_{1:K}$})，以降低动态 regret的上限。我们的主要贡献在于我们显示了一个不优的 {$t_{1:K}$} 与策略训练时间（Agent 的拍弹）和环境变化速度（环境的拍弹）之间存在交易关系。理论上，这种工作在环境的非站点性程度的度量下计算出一个不优的 {$t_{1:K}$}，并实现了对动态 regret的下降。我们在高维非站点环境中进行了多个实验，结果表明，$\texttt{ProST}$ 框架在不优的 {$t_{1:K}$} 下实现了更高的在线返回。
</details></li>
</ul>
<hr>
<h2 id="Statistical-Analysis-of-Quantum-State-Learning-Process-in-Quantum-Neural-Networks"><a href="#Statistical-Analysis-of-Quantum-State-Learning-Process-in-Quantum-Neural-Networks" class="headerlink" title="Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks"></a>Statistical Analysis of Quantum State Learning Process in Quantum Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14980">http://arxiv.org/abs/2309.14980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenghongz/lim_learning_state">https://github.com/chenghongz/lim_learning_state</a></li>
<li>paper_authors: Hao-kai Zhang, Chenghong Zhu, Mingrui Jing, Xin Wang</li>
<li>for: 这篇论文旨在研究量子神经网络（QNNs）是否可以学习未知的量子状态，并证明在某些情况下，QNNs无法学习这种状态，即使从高精度的初始状态开始。</li>
<li>methods: 这篇论文使用了无果定理来证明，当损失值低于一定阈值时，QNNs 中的搜索空间内的概率会下降 exponentially  WITH  qubit 的数量，而只有 polynomial 增长 WITH  circuit 的深度。</li>
<li>results: 研究结果表明，QNNs 无法学习未知的量子状态，即使从高精度的初始状态开始，并且这种不可学习性与 circuit 的结构、初始化策略和 ansatz 无关。 数据 simulations  validate 了我们的理论结果。这些结果对 QNNs 的可学习性和扩展性带来了限制，同时深入了量子神经网络中备忘的优先知识的作用。<details>
<summary>Abstract</summary>
Quantum neural networks (QNNs) have been a promising framework in pursuing near-term quantum advantage in various fields, where many applications can be viewed as learning a quantum state that encodes useful data. As a quantum analog of probability distribution learning, quantum state learning is theoretically and practically essential in quantum machine learning. In this paper, we develop a no-go theorem for learning an unknown quantum state with QNNs even starting from a high-fidelity initial state. We prove that when the loss value is lower than a critical threshold, the probability of avoiding local minima vanishes exponentially with the qubit count, while only grows polynomially with the circuit depth. The curvature of local minima is concentrated to the quantum Fisher information times a loss-dependent constant, which characterizes the sensibility of the output state with respect to parameters in QNNs. These results hold for any circuit structures, initialization strategies, and work for both fixed ansatzes and adaptive methods. Extensive numerical simulations are performed to validate our theoretical results. Our findings place generic limits on good initial guesses and adaptive methods for improving the learnability and scalability of QNNs, and deepen the understanding of prior information's role in QNNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Context-Aware-Generative-Models-for-Prediction-of-Aircraft-Ground-Tracks"><a href="#Context-Aware-Generative-Models-for-Prediction-of-Aircraft-Ground-Tracks" class="headerlink" title="Context-Aware Generative Models for Prediction of Aircraft Ground Tracks"></a>Context-Aware Generative Models for Prediction of Aircraft Ground Tracks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14957">http://arxiv.org/abs/2309.14957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nick Pepper, George De Ath, Marc Thomas, Richard Everson, Tim Dodwell</li>
<li>for: 支持航空交通管理者（ATCO）决策的 trajectory prediction（TP）扮演着重要的角色。</li>
<li>methods: 这项工作提出了一种生成方法，使用机器学习的概率模型来考虑飞机飞行轨迹中的epistemicuncertainty，即пилот行为和ATCO意图的不确定性。</li>
<li>results: 使用一周的英国上空航空Surveillance数据进行训练和测试，研究发现，使用bayesian neural network和Laplaceapproximation的模型可以生成最有可能性的轨迹，以便模拟航空交通的流动。<details>
<summary>Abstract</summary>
Trajectory prediction (TP) plays an important role in supporting the decision-making of Air Traffic Controllers (ATCOs). Traditional TP methods are deterministic and physics-based, with parameters that are calibrated using aircraft surveillance data harvested across the world. These models are, therefore, agnostic to the intentions of the pilots and ATCOs, which can have a significant effect on the observed trajectory, particularly in the lateral plane. This work proposes a generative method for lateral TP, using probabilistic machine learning to model the effect of the epistemic uncertainty arising from the unknown effect of pilot behaviour and ATCO intentions. The models are trained to be specific to a particular sector, allowing local procedures such as coordinated entry and exit points to be modelled. A dataset comprising a week's worth of aircraft surveillance data, passing through a busy sector of the United Kingdom's upper airspace, was used to train and test the models. Specifically, a piecewise linear model was used as a functional, low-dimensional representation of the ground tracks, with its control points determined by a generative model conditioned on partial context. It was found that, of the investigated models, a Bayesian Neural Network using the Laplace approximation was able to generate the most plausible trajectories in order to emulate the flow of traffic through the sector.
</details>
<details>
<summary>摘要</summary>
准确预测航空器轨迹（TP）在支持空交管理员（ATCO）决策中扮演着重要角色。传统的TP方法是决定性的，基于物理学术，参数通过全球采集的飞机抽象数据进行准确。这些模型因此具有不考虑飞行员和ATCO的意图的缺陷，特别在水平面上。这项工作提出了一种生成方法，使用概率机器学习来模拟飞机轨迹中不确定因素的影响，包括飞行员和ATCO的意图。模型通过特定到某个区域的方式进行训练，以便模型当地的过程，如协调入口和出口点。使用一周内英国Upper空间的繁忙区域的飞机抽象数据进行训练和测试。Specifically, a piecewise linear model was used as a functional, low-dimensional representation of the ground tracks, with its control points determined by a generative model conditioned on partial context. It was found that, of the investigated models, a Bayesian Neural Network using the Laplace approximation was able to generate the most plausible trajectories in order to emulate the flow of traffic through the sector.Note: Simplified Chinese is a romanization of Chinese that uses simpler characters and grammar to facilitate communication. It is not a formal standard of Chinese, but it is commonly used in informal writing and online communication.
</details></li>
</ul>
<hr>
<h2 id="Learning-Generative-Models-for-Climbing-Aircraft-from-Radar-Data"><a href="#Learning-Generative-Models-for-Climbing-Aircraft-from-Radar-Data" class="headerlink" title="Learning Generative Models for Climbing Aircraft from Radar Data"></a>Learning Generative Models for Climbing Aircraft from Radar Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14941">http://arxiv.org/abs/2309.14941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nick Pepper, Marc Thomas</li>
<li>for: 这篇论文的目的是提出一个可靠的飞行器预测方法，以减少飞行器运行时的不确定性对预测结果的影响。</li>
<li>methods: 这篇论文使用了一个基于数据的生成模型，将标准的飞机数据库（BADA）模型与推进力的函数修正学习自数据。</li>
<li>results: 这篇论文的结果显示，使用这个方法可以预测飞行器的到达时间比BADA模型更加精确，并且生成的轨迹对测试数据的实际情况有着更高的吻合度。<details>
<summary>Abstract</summary>
Accurate trajectory prediction (TP) for climbing aircraft is hampered by the presence of epistemic uncertainties concerning aircraft operation, which can lead to significant misspecification between predicted and observed trajectories. This paper proposes a generative model for climbing aircraft in which the standard Base of Aircraft Data (BADA) model is enriched by a functional correction to the thrust that is learned from data. The method offers three features: predictions of the arrival time with 66.3% less error when compared to BADA; generated trajectories that are realistic when compared to test data; and a means of computing confidence bounds for minimal computational cost.
</details>
<details>
<summary>摘要</summary>
减少飞机轨迹预测错误的精准方法（TP），受飞机运行不确定性的影响，导致预测和观测轨迹之间的差异较大。这篇论文提出了一种基于飞机数据（BADA）模型的生成模型，通过学习数据来修正驱进力。该方法具有以下三个特点：1）预测到达时间的错误率比BADA低于66.3%；2）生成的轨迹与测试数据实际上具有准确性；3）可以计算出轨迹 confidence bound，而且计算成本较低。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Multi-Objective-Hyperparameter-Optimization-with-Uniform-Normalization-and-Bounded-Objectives"><a href="#Parallel-Multi-Objective-Hyperparameter-Optimization-with-Uniform-Normalization-and-Bounded-Objectives" class="headerlink" title="Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives"></a>Parallel Multi-Objective Hyperparameter Optimization with Uniform Normalization and Bounded Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14936">http://arxiv.org/abs/2309.14936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Egele, Tyler Chang, Yixuan Sun, Venkatram Vishwanath, Prasanna Balaprakash</li>
<li>for: 提高机器学习模型的多个目标性能</li>
<li>methods: 使用多目标搜索优化机器学习模型的超参数，并采用均衡约束和随机权重缩放法提高效率</li>
<li>results: 提高了多目标性能优化的效率，并且可以快速并平铺地运行多个任务。<details>
<summary>Abstract</summary>
Machine learning (ML) methods offer a wide range of configurable hyperparameters that have a significant influence on their performance. While accuracy is a commonly used performance objective, in many settings, it is not sufficient. Optimizing the ML models with respect to multiple objectives such as accuracy, confidence, fairness, calibration, privacy, latency, and memory consumption is becoming crucial. To that end, hyperparameter optimization, the approach to systematically optimize the hyperparameters, which is already challenging for a single objective, is even more challenging for multiple objectives. In addition, the differences in objective scales, the failures, and the presence of outlier values in objectives make the problem even harder. We propose a multi-objective Bayesian optimization (MoBO) algorithm that addresses these problems through uniform objective normalization and randomized weights in scalarization. We increase the efficiency of our approach by imposing constraints on the objective to avoid exploring unnecessary configurations (e.g., insufficient accuracy). Finally, we leverage an approach to parallelize the MoBO which results in a 5x speed-up when using 16x more workers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Verifiable-Learned-Behaviors-via-Motion-Primitive-Composition-Applications-to-Scooping-of-Granular-Media"><a href="#Verifiable-Learned-Behaviors-via-Motion-Primitive-Composition-Applications-to-Scooping-of-Granular-Media" class="headerlink" title="Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media"></a>Verifiable Learned Behaviors via Motion Primitive Composition: Applications to Scooping of Granular Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14894">http://arxiv.org/abs/2309.14894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Benton, Eugen Solowjow, Prithvi Akella</li>
<li>for: 提高工业机器人采用率，通过实时生成自然语言输入的机器人行为模型。</li>
<li>methods: 基于语言抽象器学习的行为抽象器，通过生成指定的运动 primitives  Synthesize  directed graph，以实现行为的可靠验证。</li>
<li>results: 在simulation中进行探索任务和硬件上使用机器人挖掘固体媒体中的示范。<details>
<summary>Abstract</summary>
A robotic behavior model that can reliably generate behaviors from natural language inputs in real time would substantially expedite the adoption of industrial robots due to enhanced system flexibility. To facilitate these efforts, we construct a framework in which learned behaviors, created by a natural language abstractor, are verifiable by construction. Leveraging recent advancements in motion primitives and probabilistic verification, we construct a natural-language behavior abstractor that generates behaviors by synthesizing a directed graph over the provided motion primitives. If these component motion primitives are constructed according to the criteria we specify, the resulting behaviors are probabilistically verifiable. We demonstrate this verifiable behavior generation capacity in both simulation on an exploration task and on hardware with a robot scooping granular media.
</details>
<details>
<summary>摘要</summary>
一种可靠生成行为的机器人行为模型，可以在实时语言输入下生成行为，将加速工业机器人的采用，因为增加系统的灵活性。为了支持这些努力，我们构建了一个框架，在该框架中，通过自然语言抽象器学习的行为被可靠地验证。利用最新的动作基本 primitives和概率验证技术，我们构建了一个基于自然语言的行为抽象器，通过将提供的动作基本 primitives синтези为导向图来生成行为。如果这些组件动作基本 primitives按照我们的要求构建，则生成的行为是可靠地验证的。我们在实验中使用一个探索任务和硬件机器人夹取粒子物质进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Credit-Card-Fraud-Detection-with-Subspace-Learning-based-One-Class-Classification"><a href="#Credit-Card-Fraud-Detection-with-Subspace-Learning-based-One-Class-Classification" class="headerlink" title="Credit Card Fraud Detection with Subspace Learning-based One-Class Classification"></a>Credit Card Fraud Detection with Subspace Learning-based One-Class Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14880">http://arxiv.org/abs/2309.14880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaffar Zaffar, Fahad Sohrab, Juho Kanniainen, Moncef Gabbouj</li>
<li>For: 这种论文旨在提出一种基于一类分类算法的自动信用卡fraud检测方法，以解决因commerce digitization而导致的信用卡fraud问题，以及随着fraud技术的不断发展，已有的检测方法的局限性。* Methods: 本文使用subspace learning-based One-Class Classification（OCC）算法，可以处理偏极分布的数据，同时具有预测未来fraud技术的能力。这种算法将数据描述于一个lower-dimensional的子空间中，从而提高了OCC的性能。* Results: 经过严格的实验和分析，本文证明了提出的方法可以有效地 mitigate credit card fraud detection中的curse of dimensionality和偏极分布问题，提高了自动检测的精度和效率。<details>
<summary>Abstract</summary>
In an increasingly digitalized commerce landscape, the proliferation of credit card fraud and the evolution of sophisticated fraudulent techniques have led to substantial financial losses. Automating credit card fraud detection is a viable way to accelerate detection, reducing response times and minimizing potential financial losses. However, addressing this challenge is complicated by the highly imbalanced nature of the datasets, where genuine transactions vastly outnumber fraudulent ones. Furthermore, the high number of dimensions within the feature set gives rise to the ``curse of dimensionality". In this paper, we investigate subspace learning-based approaches centered on One-Class Classification (OCC) algorithms, which excel in handling imbalanced data distributions and possess the capability to anticipate and counter the transactions carried out by yet-to-be-invented fraud techniques. The study highlights the potential of subspace learning-based OCC algorithms by investigating the limitations of current fraud detection strategies and the specific challenges of credit card fraud detection. These algorithms integrate subspace learning into the data description; hence, the models transform the data into a lower-dimensional subspace optimized for OCC. Through rigorous experimentation and analysis, the study validated that the proposed approach helps tackle the curse of dimensionality and the imbalanced nature of credit card data for automatic fraud detection to mitigate financial losses caused by fraudulent activities.
</details>
<details>
<summary>摘要</summary>
在数字化贸易景观中，信用卡诈骗的扩散和黑科技的不断演化，导致了严重的金融损失。自动化信用卡诈骗检测是一种可行的方法，可以加速检测，降低响应时间，最小化可能的金融损失。然而，解决这个挑战是因为数据集的高度偏好性和维度瓶颈的缘故复杂。在这篇论文中，我们调查了基于一个空间学习的一类分类算法，这些算法在处理偏好数据分布时表现出色，并具有预测和防范尚未发明的诈骗技术的能力。我们的研究探讨了现有的诈骗检测策略的局限性和信用卡诈骗检测的特定挑战。这些算法将数据描述中的subspace学习 integrate into the data description, so the models transform the data into a lower-dimensional subspace optimized for one-class classification.经过严格的实验和分析，我们的研究证明了我们提出的方法可以抗衡维度瓶颈和信用卡数据的偏好性，以便自动检测诈骗，从而减少由诈骗活动导致的金融损失。
</details></li>
</ul>
<hr>
<h2 id="Cluster-Exploration-using-Informative-Manifold-Projections"><a href="#Cluster-Exploration-using-Informative-Manifold-Projections" class="headerlink" title="Cluster Exploration using Informative Manifold Projections"></a>Cluster Exploration using Informative Manifold Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14857">http://arxiv.org/abs/2309.14857</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asem010/legend-pice">https://github.com/asem010/legend-pice</a></li>
<li>paper_authors: Stavros Gerolymatos, Xenophon Evangelopoulos, Vladimir Gusev, John Y. Goulermas</li>
<li>for: 本研究旨在提出一种基于层次结构的维度减少方法，以便利用先前知识来探索高维数据的视觉结构。</li>
<li>methods: 本方法使用了一种线性组合的目标函数，包括对偏好信息的杜尔减去和含义层次分析。</li>
<li>results: 实验表明，本方法可以效果地揭示高维数据中的层次结构，并且可以根据不同的先前知识进行自动化的视觉探索。<details>
<summary>Abstract</summary>
Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, we provide an automated framework to perform iterative visual exploration of high-dimensional data.
</details>
<details>
<summary>摘要</summary>
维度减少（DR）是数据可见化中一种关键工具，可以在两到三维空间中揭示高维数据的层次结构。大多数DR方法在文献中忽略了具体数据的先验知识。我们提出了一种新的方法，可以生成有用的嵌入，不仅抑制了不同类型的先验知识结构，还尝试揭示剩下的下面结构。我们使用了一种线性组合的两个目标：首先，对比PCA，抑制先验知识结构；其次，峰度投影约束，确保获得的嵌入是有意义的。我们将这个任务视为一个拟合优化问题，并在多个数据集上进行了验证。最后，我们提供了一个自动化的框架，可以对高维数据进行迭代可见化。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-factors-regarding-the-effects-of-COVID-19-pandemic-on-college-students’-depression-by-quantum-annealer"><a href="#Investigation-of-factors-regarding-the-effects-of-COVID-19-pandemic-on-college-students’-depression-by-quantum-annealer" class="headerlink" title="Investigation of factors regarding the effects of COVID-19 pandemic on college students’ depression by quantum annealer"></a>Investigation of factors regarding the effects of COVID-19 pandemic on college students’ depression by quantum annealer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00018">http://arxiv.org/abs/2310.00018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junggu Choi, Kion Kim, Soohyun Park, Juyoen Hur, Hyunjung Yang, Younghoon Kim, Hakbae Lee, Sanghoon Han</li>
<li>for: 本研究旨在探讨COVID-19疫情对大学生们的心理健康产生的影响，以及这些影响因素的复杂关系。</li>
<li>methods: 本研究使用量子落差（QA）特征选择算法，通过商业D-Wave量子计算机执行，确定疫情前后不同因素之间的关系变化。同时还使用多变量线性回归（MLR）和XGBoost模型来验证QA算法的可行性。</li>
<li>results: 研究结果表明，QA算法在因素分析研究中具有与MLR模型广泛使用的相同能力。此外，QA算法的重要因素结果也被验证了。疫情相关因素（如社会系统信任度）和心理因素（如不确定情况下的决策）在后疫情条件下更加重要。我们认为，本研究将为研究类似主题的研究人员提供参考。<details>
<summary>Abstract</summary>
Diverse cases regarding the impact, with its related factors, of the COVID-19 pandemic on mental health have been reported in previous studies. College student groups have been frequently selected as the target population in previous studies because they are easily affected by pandemics. In this study, multivariable datasets were collected from 751 college students based on the complex relationships between various mental health factors. We utilized quantum annealing (QA)-based feature selection algorithms that were executed by commercial D-Wave quantum computers to determine the changes in the relative importance of the associated factors before and after the pandemic. Multivariable linear regression (MLR) and XGBoost models were also applied to validate the QA-based algorithms. Based on the experimental results, we confirm that QA-based algorithms have comparable capabilities in factor analysis research to the MLR models that have been widely used in previous studies. Furthermore, the performance of the QA-based algorithms was validated through the important factor results from the algorithms. Pandemic-related factors (e.g., confidence in the social system) and psychological factors (e.g., decision-making in uncertain situations) were more important in post-pandemic conditions. We believe that our study will serve as a reference for researchers studying similar topics.
</details>
<details>
<summary>摘要</summary>
Previous studies have reported diverse cases of the impact of the COVID-19 pandemic on mental health, with various related factors. College student groups have been frequently selected as the target population due to their vulnerability to pandemics. In this study, we collected multivariable datasets from 751 college students to examine the complex relationships between mental health factors using quantum annealing (QA)-based feature selection algorithms executed by commercial D-Wave quantum computers. We also applied multivariable linear regression (MLR) and XGBoost models for validation. Our results confirm that QA-based algorithms have comparable capabilities in factor analysis research to the widely used MLR models in previous studies. Additionally, the performance of the QA-based algorithms was validated through the important factor results from the algorithms. In post-pandemic conditions, pandemic-related factors (such as confidence in the social system) and psychological factors (such as decision-making in uncertain situations) were found to be more important. We believe that our study will serve as a reference for researchers studying similar topics.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Realtime-Motion-Generation-with-Active-Perception-Using-Attention-Mechanism-for-Cooking-Robot"><a href="#Realtime-Motion-Generation-with-Active-Perception-Using-Attention-Mechanism-for-Cooking-Robot" class="headerlink" title="Realtime Motion Generation with Active Perception Using Attention Mechanism for Cooking Robot"></a>Realtime Motion Generation with Active Perception Using Attention Mechanism for Cooking Robot</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14837">http://arxiv.org/abs/2309.14837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CraftXinkali/Dungeon-Quest-HUBS">https://github.com/CraftXinkali/Dungeon-Quest-HUBS</a></li>
<li>paper_authors: Namiko Saito, Mayu Hiramoto, Ayuna Kubo, Kanata Suzuki, Hiroshi Ito, Shigeki Sugano, Tetsuya Ogata</li>
<li>for: 本研究旨在帮助机器人在日常生活中自动学习、适应物体和环境，并实现相应的动作。</li>
<li>methods: 本研究使用predictive recurrent neural network（PRNN）和注意力机制，可以快速和高效地识别感知信息中的重要信息和可靠信息，并基于这些信息进行动作生成。</li>
<li>results: 经过训练和验证，机器人通过学习人类技能，可以成功地烹饪不同的鸡蛋。机器人可以根据鸡蛋的状态进行不同的搅拌和扭转动作，例如在开始热煮鸡蛋时，机器人会搅拌整个锅，然后随着鸡蛋的热度提高，机器人会改变搅拌方式和target area，例如进行扭转和分割动作，即使没有直接指定。<details>
<summary>Abstract</summary>
To support humans in their daily lives, robots are required to autonomously learn, adapt to objects and environments, and perform the appropriate actions. We tackled on the task of cooking scrambled eggs using real ingredients, in which the robot needs to perceive the states of the egg and adjust stirring movement in real time, while the egg is heated and the state changes continuously. In previous works, handling changing objects was found to be challenging because sensory information includes dynamical, both important or noisy information, and the modality which should be focused on changes every time, making it difficult to realize both perception and motion generation in real time. We propose a predictive recurrent neural network with an attention mechanism that can weigh the sensor input, distinguishing how important and reliable each modality is, that realize quick and efficient perception and motion generation. The model is trained with learning from the demonstration, and allows the robot to acquire human-like skills. We validated the proposed technique using the robot, Dry-AIREC, and with our learning model, it could perform cooking eggs with unknown ingredients. The robot could change the method of stirring and direction depending on the status of the egg, as in the beginning it stirs in the whole pot, then subsequently, after the egg started being heated, it starts flipping and splitting motion targeting specific areas, although we did not explicitly indicate them.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we proposed a predictive recurrent neural network with an attention mechanism that can weigh the sensor input, distinguishing how important and reliable each modality is. This allows the robot to quickly and efficiently perceive and generate motion. The model is trained using learning from demonstration, allowing the robot to acquire human-like skills.We validated our proposed technique using the robot Dry-AIREC, and our learning model allowed the robot to cook eggs with unknown ingredients. The robot was able to change its method of stirring and direction depending on the status of the egg, starting with whole-pot stirring and then switching to flipping and splitting motions targeting specific areas as the egg heated up. This demonstrates the effectiveness of our proposed technique in enabling robots to adapt to changing objects and environments in real time.
</details></li>
</ul>
<hr>
<h2 id="OS-net-Orbitally-Stable-Neural-Networks"><a href="#OS-net-Orbitally-Stable-Neural-Networks" class="headerlink" title="OS-net: Orbitally Stable Neural Networks"></a>OS-net: Orbitally Stable Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14822">http://arxiv.org/abs/2309.14822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marieme Ngom, Carlo Graziani</li>
<li>for: 这篇论文是 для研究 периоди动力系统中的神经网络架构，尤其是使用运动方程式来描述动力系统的动作。</li>
<li>methods: 本论文使用了Neural Ordinary Differential Equations（NODEs）和运动方程式来建立神经网络架构，并利用ODE理论来确定网络参数的稳定性。</li>
<li>results: 本论文透过应用OS-net于罗斯勒和斯洛特的系统中，发现了 périod doubling attractors 和 chaotic 行为的动力学。<details>
<summary>Abstract</summary>
We introduce OS-net (Orbitally Stable neural NETworks), a new family of neural network architectures specifically designed for periodic dynamical data. OS-net is a special case of Neural Ordinary Differential Equations (NODEs) and takes full advantage of the adjoint method based backpropagation method. Utilizing ODE theory, we derive conditions on the network weights to ensure stability of the resulting dynamics. We demonstrate the efficacy of our approach by applying OS-net to discover the dynamics underlying the R\"{o}ssler and Sprott's systems, two dynamical systems known for their period doubling attractors and chaotic behavior.
</details>
<details>
<summary>摘要</summary>
我们介绍OS-net（Orbitally Stable neural NETworks），一新的神经网络架构，特别针对周期动力系统的资料。OS-net是NODEs（神经普通微分方程）的特殊情况，利用微分方程理论，我们得出了网络 Parameters的稳定性条件。我们透过实践OS-net，发现了R\"{o}ssler和Sprott的两个动力系统中的时间倍增吸引器和混沌行为。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Markov-Chain-Mirror-Descent-On-Data-Federation"><a href="#Markov-Chain-Mirror-Descent-On-Data-Federation" class="headerlink" title="Markov Chain Mirror Descent On Data Federation"></a>Markov Chain Mirror Descent On Data Federation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14775">http://arxiv.org/abs/2309.14775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yawei Zhao</li>
<li>for: 这个论文探讨了在联合学习中使用随机抽样 descent的方法，并提出了一个新的版本名为 MarchOn。</li>
<li>methods: 这个方法使用了随机从节点到其邻居的选择，并提出了一个新的分析框架，可以实现最好的速度传播。</li>
<li>results: 这个研究获得了实验 validate theoretical results，并证明了 MarchOn 的传播速度是最佳的。<details>
<summary>Abstract</summary>
Stochastic optimization methods such as mirror descent have wide applications due to low computational cost. Those methods have been well studied under assumption of the independent and identical distribution, and usually achieve sublinear rate of convergence. However, this assumption may be too strong and unpractical in real application scenarios. Recent researches investigate stochastic gradient descent when instances are sampled from a Markov chain. Unfortunately, few results are known for stochastic mirror descent. In the paper, we propose a new version of stochastic mirror descent termed by MarchOn in the scenario of the federated learning. Given a distributed network, the model iteratively travels from a node to one of its neighbours randomly. Furthermore, we propose a new framework to analyze MarchOn, which yields best rates of convergence for convex, strongly convex, and non-convex loss. Finally, we conduct empirical studies to evaluate the convergence of MarchOn, and validate theoretical results.
</details>
<details>
<summary>摘要</summary>
Stochastic 优化方法，如镜像下降法，具有低计算成本，因此在各种应用场景中具有广泛的应用前景。然而，这些方法通常假设数据是独立和相同分布的，这可能是一个偏要假设。现在的研究则探讨在Markov链上采样实例时，镜像下降法的性能。尽管有些结果已经得到了关注，但是对于镜像下降法，还知之不够多。在本文中，我们提出了一种新的镜像下降法，称之为MarchOn，并在联合学习场景中应用。在分布网络上，模型会随机从一个节点跳转到其近邻节点。此外，我们还提出了一种新的分析框架，可以在 convex、强Converter、非凸损函数下达到最佳的速度。最后，我们进行了实验研究，证明了MarchOn的收敛性。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Complexity-and-Formal-Hierarchy-of-Second-Order-Recurrent-Neural-Networks"><a href="#On-the-Computational-Complexity-and-Formal-Hierarchy-of-Second-Order-Recurrent-Neural-Networks" class="headerlink" title="On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks"></a>On the Computational Complexity and Formal Hierarchy of Second Order Recurrent Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14691">http://arxiv.org/abs/2309.14691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankur Mali, Alexander Ororbia, Daniel Kifer, Lee Giles</li>
<li>for: 这个论文旨在探讨基于人工神经网络（ANNs）的二阶Recurrent Neural Networks（RNNs）是否能够实现图灵完备性（TC）。</li>
<li>methods: 作者们使用了二阶RNNs的概率性和自注意力来实现TC。他们还提出了一种可解释的设计方法，可以在受限制的精度和时间下实现TC。</li>
<li>results: 作者们证明了二阶RNNs可以在受限制的精度和时间下recognize任何规则语言，并且在recognize regular grammars时表现更好于现代化RNNs和Gated Recurrent Units。他们还提供了一个Upper bound和稳定分析，证明二阶RNNs只需要一定的最大 neuron数来recognize任何规则语言。<details>
<summary>Abstract</summary>
Artificial neural networks (ANNs) with recurrence and self-attention have been shown to be Turing-complete (TC). However, existing work has shown that these ANNs require multiple turns or unbounded computation time, even with unbounded precision in weights, in order to recognize TC grammars. However, under constraints such as fixed or bounded precision neurons and time, ANNs without memory are shown to struggle to recognize even context-free languages. In this work, we extend the theoretical foundation for the $2^{nd}$-order recurrent network ($2^{nd}$ RNN) and prove there exists a class of a $2^{nd}$ RNN that is Turing-complete with bounded time. This model is capable of directly encoding a transition table into its recurrent weights, enabling bounded time computation and is interpretable by design. We also demonstrate that $2$nd order RNNs, without memory, under bounded weights and time constraints, outperform modern-day models such as vanilla RNNs and gated recurrent units in recognizing regular grammars. We provide an upper bound and a stability analysis on the maximum number of neurons required by $2$nd order RNNs to recognize any class of regular grammar. Extensive experiments on the Tomita grammars support our findings, demonstrating the importance of tensor connections in crafting computationally efficient RNNs. Finally, we show $2^{nd}$ order RNNs are also interpretable by extraction and can extract state machines with higher success rates as compared to first-order RNNs. Our results extend the theoretical foundations of RNNs and offer promising avenues for future explainable AI research.
</details>
<details>
<summary>摘要</summary>
人工神经网络（ANNs）带有回归和自注意力已被证明是图灵完备（TC）。然而，现有的工作表明，这些ANNs需要多个轮次或无限的计算时间，即使在无限精度的权重下，以recognize TC语法。然而，在受限于固定或受限的精度神经和时间的情况下，没有记忆的ANNs很难recognizeeven context-free语言。在这种工作中，我们扩展了第二阶段回归网络（2nd RNN）的理论基础，并证明存在一类的2nd RNN可以在受限时间内recognize TC语法。这种模型可以直接将转移表编码到其回归权重中，使得计算时间受限，并且可以通过设计来解释。我们还证明了没有记忆的2nd RNN，在固定权重和时间约束下，可以在较低的精度下recognize常见语法，并且在Tomita语法上进行了广泛的实验支持。最后，我们表明2nd RNN可以通过提取来解释，并且可以在比first-order RNN更高的成功率下提取状态机。我们的结果扩展了RNN的理论基础，并提供了未来可解释AI研究的有优点的方向。
</details></li>
</ul>
<hr>
<h2 id="FedCompass-Efficient-Cross-Silo-Federated-Learning-on-Heterogeneous-Client-Devices-using-a-Computing-Power-Aware-Scheduler"><a href="#FedCompass-Efficient-Cross-Silo-Federated-Learning-on-Heterogeneous-Client-Devices-using-a-Computing-Power-Aware-Scheduler" class="headerlink" title="FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler"></a>FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices using a Computing Power Aware Scheduler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14675">http://arxiv.org/abs/2309.14675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilinghan Li, Pranshu Chaturvedi, Shilan He, Han Chen, Gagandeep Singh, Volodymyr Kindratenko, E. A. Huerta, Kibaek Kim, Ravi Madduri</li>
<li>for: 这篇论文旨在解决跨节点联合学习中的客户端异相性和数据异相性问题，以实现无中央数据设施的情况下，实现强大且通用的人工智能模型训练。</li>
<li>methods: 本论文提出了一个名为FedCompass的创新的半同步联合学习算法，其在服务器端使用了资源知识来分配不同客户端的训练任务，以适应不同客户端的计算能力。这使得多个客户端的本地模型可以被收到并处理，以减少本地模型的落后性。</li>
<li>results: 使用了多种异步的非同相数据分布式集合，研究发现FedCompass可以比其他半同步算法更快地训练到准确性，并且在不同客户端的计算能力不同情况下保持高效率。<details>
<summary>Abstract</summary>
Cross-silo federated learning offers a promising solution to collaboratively train robust and generalized AI models without compromising the privacy of local datasets, e.g., healthcare, financial, as well as scientific projects that lack a centralized data facility. Nonetheless, because of the disparity of computing resources among different clients (i.e., device heterogeneity), synchronous federated learning algorithms suffer from degraded efficiency when waiting for straggler clients. Similarly, asynchronous federated learning algorithms experience degradation in the convergence rate and final model accuracy on non-identically and independently distributed (non-IID) heterogeneous datasets due to stale local models and client drift. To address these limitations in cross-silo federated learning with heterogeneous clients and data, we propose FedCompass, an innovative semi-asynchronous federated learning algorithm with a computing power aware scheduler on the server side, which adaptively assigns varying amounts of training tasks to different clients using the knowledge of the computing power of individual clients. FedCompass ensures that multiple locally trained models from clients are received almost simultaneously as a group for aggregation, effectively reducing the staleness of local models. At the same time, the overall training process remains asynchronous, eliminating prolonged waiting periods from straggler clients. Using diverse non-IID heterogeneous distributed datasets, we demonstrate that FedCompass achieves faster convergence and higher accuracy than other asynchronous algorithms while remaining more efficient than synchronous algorithms when performing federated learning on heterogeneous clients.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:跨存储 silo 联合学习提供了一个优秀的解决方案，以协同训练 robust 和泛化 AI 模型，无需妥协本地数据隐私，例如医疗、金融、以及科学项目，这些项目缺乏中央数据设施。然而，由于客户端（设备）的资源差异（device heterogeneity），同步联合学习算法受到客户端延迟的影响，而异步联合学习算法则因为客户端模型偏移和数据不均匀（non-IID）而导致衰落。为了解决这些限制，我们提出了 FedCompass，一种具有服务器端计算能力感知调度器的半同步联合学习算法。FedCompass 能够适应客户端的计算能力，并在服务器端分配不同的训练任务，以避免客户端延迟。这种方法使得多个客户端上的本地模型被接收并聚合，从而减少本地模型的衰落。同时，整个训练过程保持异步，从而消除客户端延迟。使用多种非同Kind 的非同一样 distributed datasets，我们证明了 FedCompass 在异步联合学习中实现了更快的整合速度和更高的准确率，同时保持和同步联合学习更高的效率。
</details></li>
</ul>
<hr>
<h2 id="Transformer-based-classification-of-user-queries-for-medical-consultancy-with-respect-to-expert-specialization"><a href="#Transformer-based-classification-of-user-queries-for-medical-consultancy-with-respect-to-expert-specialization" class="headerlink" title="Transformer-based classification of user queries for medical consultancy with respect to expert specialization"></a>Transformer-based classification of user queries for medical consultancy with respect to expert specialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14662">http://arxiv.org/abs/2309.14662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dmitry Lyutkin, Andrey Soloviev, Dmitry Zhukov, Denis Pozdnyakov, Muhammad Shahid Iqbal Malik, Dmitry I. Ignatov</li>
<li>for: 这个研究旨在提供一个创新的数位健康照顾方法，通过利用 RuBERT 模型进行患者查询分类，并且强调专业专长。</li>
<li>methods: 我们使用 RuBERT 模型进行微调，利用不同的数据集进行 fine-tuning，以实现具体的医疗专业与查询之间的精确对应。</li>
<li>results: 我们透过实验证明了我们的方法在不同医疗领域（如心脏科、神经科和皮肤科）中的高效性，F1 分数超过 92%。<details>
<summary>Abstract</summary>
The need for skilled medical support is growing in the era of digital healthcare. This research presents an innovative strategy, utilizing the RuBERT model, for categorizing user inquiries in the field of medical consultation with a focus on expert specialization. By harnessing the capabilities of transformers, we fine-tuned the pre-trained RuBERT model on a varied dataset, which facilitates precise correspondence between queries and particular medical specialisms. Using a comprehensive dataset, we have demonstrated our approach's superior performance with an F1-score of over 92%, calculated through both cross-validation and the traditional split of test and train datasets. Our approach has shown excellent generalization across medical domains such as cardiology, neurology and dermatology. This methodology provides practical benefits by directing users to appropriate specialists for prompt and targeted medical advice. It also enhances healthcare system efficiency, reduces practitioner burden, and improves patient care quality. In summary, our suggested strategy facilitates the attainment of specific medical knowledge, offering prompt and precise advice within the digital healthcare field.
</details>
<details>
<summary>摘要</summary>
在数字医疗时代，需求专业医疗支持的增长日益明显。本研究提出了一种创新的策略，利用 RuBERT 模型，对医疗咨询用户问题进行分类，以提高专业化水平。通过利用 transformers 的能力，我们对预训练 RuBERT 模型进行了精细调整，使得问题和医疗专业之间达到精准匹配。通过使用完整的数据集，我们已经证明了我们的方法的超过 92% 的 F1 分数，通过跨Validation和传统的测试集和训练集分割。我们的方法在医学领域such as cardiology、neurology和dermatology中展现出了优秀的泛化性。这种方法ология在数字医疗领域提供了实用的好处，可以引导用户到相应的专家获得timely和精准的医疗建议，从而提高医疗系统的效率、减轻医生的负担、提高患者的健康质量。总之，我们建议的策略可以帮助在数字医疗领域获得特定的医学知识，提供时效和精准的医疗建议。
</details></li>
</ul>
<hr>
<h2 id="Genetic-InfoMax-Exploring-Mutual-Information-Maximization-in-High-Dimensional-Imaging-Genetics-Studies"><a href="#Genetic-InfoMax-Exploring-Mutual-Information-Maximization-in-High-Dimensional-Imaging-Genetics-Studies" class="headerlink" title="Genetic InfoMax: Exploring Mutual Information Maximization in High-Dimensional Imaging Genetics Studies"></a>Genetic InfoMax: Exploring Mutual Information Maximization in High-Dimensional Imaging Genetics Studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15132">http://arxiv.org/abs/2309.15132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaochen Xie, Ziqian Xie, Sheikh Muhammad Saiful Islam, Degui Zhi, Shuiwang Ji</li>
<li>for: This paper is written for the purpose of addressing the challenges of representation learning in genome-wide association studies (GWAS) for high-dimensional medical imaging data, specifically using mutual information (MI) to identify informative representations of the data.</li>
<li>methods: The paper introduces a trans-modal learning framework called Genetic InfoMax (GIM), which includes a regularized MI estimator and a novel genetics-informed transformer to address the specific challenges of GWAS.</li>
<li>results: The paper demonstrates the effectiveness of GIM and a significantly improved performance on GWAS, as evaluated on human brain 3D MRI data using standardized evaluation protocols.<details>
<summary>Abstract</summary>
Genome-wide association studies (GWAS) are used to identify relationships between genetic variations and specific traits. When applied to high-dimensional medical imaging data, a key step is to extract lower-dimensional, yet informative representations of the data as traits. Representation learning for imaging genetics is largely under-explored due to the unique challenges posed by GWAS in comparison to typical visual representation learning. In this study, we tackle this problem from the mutual information (MI) perspective by identifying key limitations of existing methods. We introduce a trans-modal learning framework Genetic InfoMax (GIM), including a regularized MI estimator and a novel genetics-informed transformer to address the specific challenges of GWAS. We evaluate GIM on human brain 3D MRI data and establish standardized evaluation protocols to compare it to existing approaches. Our results demonstrate the effectiveness of GIM and a significantly improved performance on GWAS.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-the-Uncertainty-Sets-for-Control-Dynamics-via-Set-Membership-A-Non-Asymptotic-Analysis"><a href="#Learning-the-Uncertainty-Sets-for-Control-Dynamics-via-Set-Membership-A-Non-Asymptotic-Analysis" class="headerlink" title="Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis"></a>Learning the Uncertainty Sets for Control Dynamics via Set Membership: A Non-Asymptotic Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14648">http://arxiv.org/abs/2309.14648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingying Li, Jing Yu, Lauren Conger, Adam Wierman</li>
<li>for:  Linear dynamical systems under bounded, i.i.d. disturbances</li>
<li>methods: Set membership estimation, non-asymptotic bound on the diameter of the uncertainty sets</li>
<li>results: Robust adaptive model predictive control with performance approaching offline optimal model predictive control<details>
<summary>Abstract</summary>
Set-membership estimation is commonly used in adaptive/learning-based control algorithms that require robustness over the model uncertainty sets, e.g., online robustly stabilizing control and robust adaptive model predictive control. Despite having broad applications, non-asymptotic estimation error bounds in the stochastic setting are limited. This paper provides such a non-asymptotic bound on the diameter of the uncertainty sets generated by set membership estimation on linear dynamical systems under bounded, i.i.d. disturbances. Further, this result is applied to robust adaptive model predictive control with uncertainty sets updated by set membership. We numerically demonstrate the performance of the robust adaptive controller, which rapidly approaches the performance of the offline optimal model predictive controller, in comparison with the control design based on least square estimation's confidence regions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>设 membership 估计是通用的控制算法中的一个重要组成部分，例如在线 robustly 稳定控制和robust 适应模型预测控制中。 despite 广泛应用，非偏 asymptotic 估计误差 bound 在sto 价设置下有限。这篇文章提供了这样一个 non-asymptotic bound 在线 linear 动力系统上的 uncertainty 集 generated by set membership 估计的 diameter。此外，这个结果被应用于 robust 适应模型预测控制中，where uncertainty sets 是通过 set membership 估计更新的。我们通过数值示例表明了这种 robust 适应控制器的性能，它快速地 approached 线上最优化的模型预测控制器的性能，与 based on least square estimation 的 confidence regions 的控制设计相比。Note: "set membership" in the text refers to the estimation of the uncertainty sets of the system's parameters, and "non-asymptotic" means that the bound is valid for all time and does not rely on any assumptions about the convergence of the estimation process.
</details></li>
</ul>
<hr>
<h2 id="Gray-box-Adversarial-Attack-of-Deep-Reinforcement-Learning-based-Trading-Agents"><a href="#Gray-box-Adversarial-Attack-of-Deep-Reinforcement-Learning-based-Trading-Agents" class="headerlink" title="Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents"></a>Gray-box Adversarial Attack of Deep Reinforcement Learning-based Trading Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14615">http://arxiv.org/abs/2309.14615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Foozhan Ataiefard, Hadi Hemmati</li>
<li>For: The paper is written to demonstrate the robustness of a Deep Reinforcement Learning (Deep RL) based trading agent against adversarial attacks.* Methods: The paper uses a “gray-box” approach for attacking the Deep RL-based trading agent, which involves trading in the same stock market with no extra access to the trading agent. The adversary agent uses a hybrid Deep Neural Network as its policy, consisting of Convolutional layers and fully-connected layers.* Results: The paper shows that the adversary policy proposed in the research is able to reduce the reward values by 214.17%, which results in reducing the potential profits of the baseline by 139.4%, ensemble method by 93.7%, and an automated trading software developed by the industrial partner by 85.5%, while consuming significantly less budget than the victims.Here are the three points in Simplified Chinese text:* 为：本研究用于证明深度强化学习（Deep RL）基于的交易代理程序对抗性的可行性。* 方法：本研究使用“灰色框架”的方法进行攻击深度强化学习基于的交易代理程序，即在同一股票市场中进行交易，无需额外访问交易代理程序。敌对代理程序使用了一个混合深度神经网络作为其政策，该政策包括卷积层和全连接层。* 结果：研究显示，敌对政策提出的本研究可以将奖励值降低214.17%，这导致基准值下降139.4%，协同方法下降93.7%，并且由industrial partner开发的自动交易软件下降85.5%，同时消耗了许多更少的预算。<details>
<summary>Abstract</summary>
In recent years, deep reinforcement learning (Deep RL) has been successfully implemented as a smart agent in many systems such as complex games, self-driving cars, and chat-bots. One of the interesting use cases of Deep RL is its application as an automated stock trading agent. In general, any automated trading agent is prone to manipulations by adversaries in the trading environment. Thus studying their robustness is vital for their success in practice. However, typical mechanism to study RL robustness, which is based on white-box gradient-based adversarial sample generation techniques (like FGSM), is obsolete for this use case, since the models are protected behind secure international exchange APIs, such as NASDAQ. In this research, we demonstrate that a "gray-box" approach for attacking a Deep RL-based trading agent is possible by trading in the same stock market, with no extra access to the trading agent. In our proposed approach, an adversary agent uses a hybrid Deep Neural Network as its policy consisting of Convolutional layers and fully-connected layers. On average, over three simulated trading market configurations, the adversary policy proposed in this research is able to reduce the reward values by 214.17%, which results in reducing the potential profits of the baseline by 139.4%, ensemble method by 93.7%, and an automated trading software developed by our industrial partner by 85.5%, while consuming significantly less budget than the victims (427.77%, 187.16%, and 66.97%, respectively).
</details>
<details>
<summary>摘要</summary>
在最近几年，深度强化学习（Deep RL）在复杂的游戏、自动驾驶车和chatbot等系统中被成功应用。其中一个有趣的应用场景是作为自动交易代理。然而，通常的机制来研究RL的可靠性是基于白盒子Gradient-based adversarial sample生成技术（like FGSM），这种方法在实际应用中是无效的，因为模型被保护在安全的国际交易API中，如NASDAQ。在这个研究中，我们展示了一种“灰色盒”的攻击方法，可以在同一个股票市场上进行交易，没有额外访问交易代理。我们的提议的敌方策略使用了混合深度神经网络，其中包括卷积层和全连接层。在三个 simulate 的股票市场配置中，我们的敌方策略可以将奖励值降低214.17%，这将导致基eline的可能收益降低139.4%，集成方法降低93.7%，以及由我们的伙伴公司开发的自动交易软件降低85.5%，而消耗的预算比 víctims（427.77%, 187.16%, 66.97%）还要少。
</details></li>
</ul>
<hr>
<h2 id="Reparameterized-Variational-Rejection-Sampling"><a href="#Reparameterized-Variational-Rejection-Sampling" class="headerlink" title="Reparameterized Variational Rejection Sampling"></a>Reparameterized Variational Rejection Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14612">http://arxiv.org/abs/2309.14612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Jankowiak, Du Phan</li>
<li>for: 这篇论文的目的是扩展现有的变量推断方法，以提高变量推断的准确性和效率。</li>
<li>methods: 这篇论文使用了变量拒绝采样（VRS）技术，其将参数化提案分布与拒绝采样结合，定义了一种非参数的、高级别的变量家族。这种技术还引入了低差值重arameter化gradient估计器，以便在实际应用中更好地优化参数。</li>
<li>results: 该论文通过理论分析和实验 validate 了这种新的变量推断方法（RVRS）的效果，并证明了它在模型中存在某些特定的本地变量时表现特别好。<details>
<summary>Abstract</summary>
Traditional approaches to variational inference rely on parametric families of variational distributions, with the choice of family playing a critical role in determining the accuracy of the resulting posterior approximation. Simple mean-field families often lead to poor approximations, while rich families of distributions like normalizing flows can be difficult to optimize and usually do not incorporate the known structure of the target distribution due to their black-box nature. To expand the space of flexible variational families, we revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which combines a parametric proposal distribution with rejection sampling to define a rich non-parametric family of distributions that explicitly utilizes the known target distribution. By introducing a low-variance reparameterized gradient estimator for the parameters of the proposal distribution, we make VRS an attractive inference strategy for models with continuous latent variables. We argue theoretically and demonstrate empirically that the resulting method--Reparameterized Variational Rejection Sampling (RVRS)--offers an attractive trade-off between computational cost and inference fidelity. In experiments we show that our method performs well in practice and that it is well-suited for black-box inference, especially for models with local latent variables.
</details>
<details>
<summary>摘要</summary>
传统的变量推断方法通常基于参数化的变量分布家族，选择家族的选择对 posterior approximation 的准确性起到关键作用。简单的mean-field家族经常导致低精度的approximation，而Rich的分布家族如normalizing flows往往难以优化并不会利用target distribution的知识因为其黑盒性质。为扩展灵活的变量推断家族，我们回归到Variational Rejection Sampling（VRS）[Grover et al., 2018]，它将 parametric proposal distribution 与拒绝抽样相结合，定义一种非 Parametric 的 rich family of distributions，并且直接利用target distribution的知识。通过引入低差variance reparameterized gradient estimator 的参数，我们使VRS成为latent variables 是 kontinuous的模型中的吸引力 inference 策略。我们 theoretically 和 empirically 论证，RVRS 提供了一个吸引人的trade-off between computational cost 和推断准确性。在实验中，我们发现我们的方法在实践中表现良好，特别是适用于black-box inference，尤其是local latent variables。
</details></li>
</ul>
<hr>
<h2 id="Neuro-Visualizer-An-Auto-encoder-based-Loss-Landscape-Visualization-Method"><a href="#Neuro-Visualizer-An-Auto-encoder-based-Loss-Landscape-Visualization-Method" class="headerlink" title="Neuro-Visualizer: An Auto-encoder-based Loss Landscape Visualization Method"></a>Neuro-Visualizer: An Auto-encoder-based Loss Landscape Visualization Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14601">http://arxiv.org/abs/2309.14601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohannad Elhamod, Anuj Karpatne</li>
<li>For: This paper aims to provide a novel auto-encoder-based non-linear landscape visualization method for neural networks, called Neuro-Visualizer, to help researchers study the loss landscape of neural networks and their training process.* Methods: The proposed Neuro-Visualizer method uses an auto-encoder to learn a lower-dimensional representation of the loss landscape, and then visualizes the landscape using a 2D plot. The method is evaluated on a variety of problems in two applications of knowledge-guided machine learning (KGML).* Results: The results show that Neuro-Visualizer outperforms other linear and non-linear baselines and provides useful insights about the loss landscape of neural networks. The method is able to corroborate and sometimes challenge claims proposed by the machine learning community.Here’s the summary in Simplified Chinese:* 为: 这篇论文目标是提供一种基于自编码器的非线性损失地形可见化方法，名为Neuro-Visualizer，以帮助研究人员研究神经网络的损失地形和训练过程。* 方法: 提议的Neuro-Visualizer方法使用自编码器学习损失地形的下降维度表示，然后使用2D图表可见化地形。方法在两个知识导向机器学习（KGML）应用中进行了多种问题的实验评估。* 结果: 结果表明Neuro-Visualizer比其他线性和非线性基准方法表现更好，并为神经网络损失地形提供了有用的洞察。方法能够证实和机器学习社区提出的一些laims。所有实验代码和数据在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/NeuroVisualizer-FDD6%E4%B8%8A%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%E3%80%82">https://anonymous.4open.science/r/NeuroVisualizer-FDD6上公开发布。</a><details>
<summary>Abstract</summary>
In recent years, there has been a growing interest in visualizing the loss landscape of neural networks. Linear landscape visualization methods, such as principal component analysis, have become widely used as they intuitively help researchers study neural networks and their training process. However, these linear methods suffer from limitations and drawbacks due to their lack of flexibility and low fidelity at representing the high dimensional landscape. In this paper, we present a novel auto-encoder-based non-linear landscape visualization method called Neuro-Visualizer that addresses these shortcoming and provides useful insights about neural network loss landscapes. To demonstrate its potential, we run experiments on a variety of problems in two separate applications of knowledge-guided machine learning (KGML). Our findings show that Neuro-Visualizer outperforms other linear and non-linear baselines and helps corroborate, and sometime challenge, claims proposed by machine learning community. All code and data used in the experiments of this paper are available at an anonymous link https://anonymous.4open.science/r/NeuroVisualizer-FDD6
</details>
<details>
<summary>摘要</summary>
近年来，有越来越多的研究者关注神经网络训练过程中的损失地图的可视化。使用原理Components分析等线性可视化方法已成为广泛使用的做法，因为它们可以直观地帮助研究者研究神经网络和它的训练过程。然而，这些线性方法受到一些限制和缺陷，因为它们无法准确表达高维度的地图。在这篇论文中，我们提出了一种基于自适应Encoder的非线性地图可视化方法，称之为Neuro-Visualizer。我们运行了多个问题在两个不同的知识导向机器学习（KGML）应用中，并证明Neuro-Visualizer可以超过其他线性和非线性基准，并提供有用的意见关于神经网络损失地图。所有实验代码和数据都可以在https://anonymous.4open.science/r/NeuroVisualizer-FDD6上获取。
</details></li>
</ul>
<hr>
<h2 id="Policy-Optimization-in-a-Noisy-Neighborhood-On-Return-Landscapes-in-Continuous-Control"><a href="#Policy-Optimization-in-a-Noisy-Neighborhood-On-Return-Landscapes-in-Continuous-Control" class="headerlink" title="Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control"></a>Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14597">http://arxiv.org/abs/2309.14597</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nathanrahn/return-landscapes">https://github.com/nathanrahn/return-landscapes</a></li>
<li>paper_authors: Nate Rahn, Pierluca D’Oro, Harley Wiltzer, Pierre-Luc Bacon, Marc G. Bellemare</li>
<li>for: 这个论文的目的是研究深度强化学习代理的稳定性问题。</li>
<li>methods: 该论文使用返回地图来研究政策和返回之间的映射，并发现流行的算法在这个地图上徘徊于噪声 neighbohood，这导致政策 Parameters 更新后返回具有广泛的变化范围。</li>
<li>results: 研究发现，返回地图具有意外的结构，存在简单的路径，可以通过 navigating 这些路径来改善政策的稳定性。该论文还提出了一种分布式视角来评估政策质量，并开发了一种分布式方法来找到这些路径。<details>
<summary>Abstract</summary>
Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time. In this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping between a policy and a return. We find that popular algorithms traverse noisy neighborhoods of this landscape, in which a single update to the policy parameters leads to a wide range of returns. By taking a distributional view of these returns, we map the landscape, characterizing failure-prone regions of policy space and revealing a hidden dimension of policy quality. We show that the landscape exhibits surprising structure by finding simple paths in parameter space which improve the stability of a policy. To conclude, we develop a distribution-aware procedure which finds such paths, navigating away from noisy neighborhoods in order to improve the robustness of a policy. Taken together, our results provide new insight into the optimization, evaluation, and design of agents.
</details>
<details>
<summary>摘要</summary>
深度强化学会控制器的表现会随时间而呈现显著的不稳定性。在这个工作中，我们提供了一种新的视角，研究返回地图：策略和返回之间的映射。我们发现受欢迎的算法在策略参数空间中穿梭着噪声的 neighborhood，一次更新策略参数可以导致返回值的各种各样变化。通过对这些返回值采取分布视角，我们映射了这个地图，描述了策略空间中失败的区域，并发现了一个隐藏的策略质量维度。我们发现返回地图具有意外的结构，找到了简单的参数空间路径，可以提高策略的稳定性。最后，我们开发了一种分布意识的过程，找到这些路径，在策略参数空间中缓解噪声，以提高策略的可靠性。总之，我们的结果为代理人的优化、评估和设计提供了新的视角。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/26/cs.LG_2023_09_26/" data-id="clpztdnlb00sces88906p42ka" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/26/eess.IV_2023_09_26/" class="article-date">
  <time datetime="2023-09-26T09:00:00.000Z" itemprop="datePublished">2023-09-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/26/eess.IV_2023_09_26/">eess.IV - 2023-09-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Challenges-of-building-medical-image-datasets-for-development-of-deep-learning-software-in-stroke"><a href="#Challenges-of-building-medical-image-datasets-for-development-of-deep-learning-software-in-stroke" class="headerlink" title="Challenges of building medical image datasets for development of deep learning software in stroke"></a>Challenges of building medical image datasets for development of deep learning software in stroke</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15081">http://arxiv.org/abs/2309.15081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Fontanella, Wenwen Li, Grant Mair, Antreas Antoniou, Eleanor Platt, Chloe Martin, Paul Armitage, Emanuele Trucco, Joanna Wardlaw, Amos Storkey<br>for: This paper aims to address the challenge of preparing clinical brain CT datasets for deep learning (DL) analysis.methods: The authors propose a complete semi-automatic pipeline to standardize the heterogeneous dataset, which includes handling image sets with different orientations, image types, and dimensions, and removing redundant background.results: The final pipeline was able to process 5,868&#x2F;10,659 (45%) CT image datasets, with the majority of axial scans being accepted after adjustments such as image cropping, resizing, and scaling. However, 465 scans failed the registration process.<details>
<summary>Abstract</summary>
Despite the large amount of brain CT data generated in clinical practice, the availability of CT datasets for deep learning (DL) research is currently limited. Furthermore, the data can be insufficiently or improperly prepared for machine learning and thus lead to spurious and irreproducible analyses. This lack of access to comprehensive and diverse datasets poses a significant challenge for the development of DL algorithms. In this work, we propose a complete semi-automatic pipeline to address the challenges of preparing a clinical brain CT dataset for DL analysis and describe the process of standardising this heterogeneous dataset. Challenges include handling image sets with different orientations (axial, sagittal, coronal), different image types (to view soft tissues or bones) and dimensions, and removing redundant background. The final pipeline was able to process 5,868/10,659 (45%) CT image datasets. Reasons for rejection include non-axial data (n=1,920), bone reformats (n=687), separated skull base/vault images (n=1,226), and registration failures (n=465). Further format adjustments, including image cropping, resizing and scaling are also needed for DL processing. Of the axial scans that were not localisers, bone reformats or split brains, 5,868/6,333 (93%) were accepted, while the remaining 465 failed the registration process. Appropriate preparation of medical imaging datasets for DL is a costly and time-intensive process.
</details>
<details>
<summary>摘要</summary>
尽管在临床实践中生成了大量的脑CT数据，但现在对深度学习（DL）研究的CT数据仍然受到限制。此外，数据可能未经正确准备，导致机器学习分析出现假象和不可重复的问题。这种数据的限制对DL算法的发展带来了重大挑战。在这项工作中，我们提出了一个完整的半自动化管道，以解决在临床脑CT数据上进行DL分析前的挑战。我们描述了处理不同方向（AXIAL、SAGGITAL、CORONAL）、不同图像类型（观察软组织或骨骼）和维度等多种挑战。我们的最终管道可以处理5,868/10,659（45%）的CT图像集。拒绝原因包括非AXIAL数据（n=1,920）、骨 Reformats（n=687）、分割的颅骨基/顶层图像（n=1,226）以及注册失败（n=465）。进一步的格式调整，包括图像剪辑、缩放和缩放，还是需要DL处理。AXIAL扫描中未经本地化的、骨 Reformats或分割的脑，5,868/6,333（93%）被接受，剩下465个失败了注册过程。适当地准备医学成像数据 дляDL是一项成本高和时间投入巨大的过程。
</details></li>
</ul>
<hr>
<h2 id="Thalamic-nuclei-segmentation-from-T-1-weighted-MRI-unifying-and-benchmarking-state-of-the-art-methods-with-young-and-old-cohorts"><a href="#Thalamic-nuclei-segmentation-from-T-1-weighted-MRI-unifying-and-benchmarking-state-of-the-art-methods-with-young-and-old-cohorts" class="headerlink" title="Thalamic nuclei segmentation from T$_1$-weighted MRI: unifying and benchmarking state-of-the-art methods with young and old cohorts"></a>Thalamic nuclei segmentation from T$_1$-weighted MRI: unifying and benchmarking state-of-the-art methods with young and old cohorts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15053">http://arxiv.org/abs/2309.15053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brendan Williams, Dan Nguyen, Julie Vidal, Alzheimer’s Disease Neuroimaging Initiative, Manojkumar Saranathan</li>
<li>for: 这个研究是为了比较不同State of the art的腋带神经分 segmentation方法的效果，以及这些方法在识别健康人群和阿尔ц海默病人群之间的分化能力。</li>
<li>methods: 这个研究使用了四种State of the art的腋带神经分 segmentation方法，包括FreeSurfer、HIPS-THOMAS、SCS-CNN和T1-THOMAS。这些方法都被应用在T1 MRI图像上，并被比较使用 overlap和异同度量来评估其精度。</li>
<li>results: 研究发现，HIPS-THOMAS方法能够最好地分解各个腋带神经元的尺度，并且能够最 accurately地识别健康人群和阿尔ц海默病人群之间的分化。此外，研究还发现，这些方法的识别健康人群和阿尔ц海默病人群的精度随着疾病的进程而变化。<details>
<summary>Abstract</summary>
The thalamus and its constituent nuclei are critical for a broad range of cognitive and sensorimotor processes, and implicated in many neurological and neurodegenerative conditions. However, the functional involvement and specificity of thalamic nuclei in human neuroimaging is underappreciated and not well studied due, in part, to technical challenges of accurately identifying and segmenting nuclei. This challenge is further exacerbated by a lack of common nomenclature for comparing segmentation methods. Here, we use data from healthy young (Human Connectome Project, 100 subjects) and older healthy adults, plus those with minor cognitive impairment and Alzheimer$'$s disease (Alzheimer$'$s Disease Neuroimaging Initiative, 540 subjects), to benchmark four state of the art thalamic segmentation methods for T1 MRI (FreeSurfer, HIPS-THOMAS, SCS-CNN, and T1-THOMAS) under a single segmentation framework. Segmentations were compared using overlap and dissimilarity metrics to the Morel stereotaxic atlas. We also quantified each method$'$s estimation of thalamic nuclear degeneration across Alzheimer$'$s disease progression, and how accurately early and late mild cognitive impairment, and Alzheimers disease could be distinguished from healthy controls. We show that HIPS-THOMAS produced the most effective segmentations of individual thalamic nuclei and was also most accurate in discriminating healthy controls from those with mild cognitive impairment and Alzheimer$'$s disease using individual nucleus volumes. This work is the first to systematically compare the efficacy of anatomical thalamic segmentation approaches under a unified nomenclature. We also provide recommendations of which segmentation method to use for studying the functional relevance of specific thalamic nuclei, based on their overlap and dissimilarity with the Morel atlas.
</details>
<details>
<summary>摘要</summary>
腔室和其组成部分 nuclei 对认知和感觉过程具有关键作用，并且与许多神经病理和神经退化病种相关。然而，人类腔室 nuclei 的功能参与度和特定性在人像成像中尚未得到充分认可和研究，这主要是因为识别和分割腔室 nuclei 技术上的挑战。此外，不同的识别方法之间没有共同的命名标准，进一步增加了研究难度。本文使用100名健康年轻人（人类连接计划）和older健康成人（540名），以及急性认知障碍和阿尔茨海默病（阿尔茨海默病成像计划）的Subjects，对4种state-of-the-art腔室分割方法（FreeSurfer、HIPS-THOMAS、SCS-CNN和T1-THOMAS）进行比较，并使用单一分割框架。分割结果与Moreel颈部 Atlases 进行比较，并计算每种方法对腔室 nuclear degeneration 的估计，以及在阿尔茨海默病进程中，健康控制组和急性认知障碍、阿尔茨海默病之间的区别如何准确。结果表明HIPS-THOMAS方法生成了最有效的各个腔室 nuclei 分割，并且在健康控制组和急性认知障碍、阿尔茨海默病之间的分割结果最为准确。本研究是首次系统地比较了不同的腔室分割方法，并提供了选择具体腔室 nuclei 研究功能相关性的建议，基于它们与Moreel Atlases 的重叠和不同程度。
</details></li>
</ul>
<hr>
<h2 id="Multiplex-ultrasound-imaging-of-perfluorocarbon-nanodroplets-enabled-by-decomposition-of-post-vaporization-dynamics"><a href="#Multiplex-ultrasound-imaging-of-perfluorocarbon-nanodroplets-enabled-by-decomposition-of-post-vaporization-dynamics" class="headerlink" title="Multiplex ultrasound imaging of perfluorocarbon nanodroplets enabled by decomposition of post-vaporization dynamics"></a>Multiplex ultrasound imaging of perfluorocarbon nanodroplets enabled by decomposition of post-vaporization dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.00019">http://arxiv.org/abs/2310.00019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Austin Van Namen, Sidhartha Jandhyala, Catalina-Paula Spatarelu, Kenneth M. Tichauer, Kimberley S. Samkoe, Geoffrey P. Luke</li>
<li>for:  This paper aims to develop a new approach to multiplex ultrasound imaging using perfluorocarbon (PFC) nanodroplets as activatable contrast agents.</li>
<li>methods:  The paper uses two populations of PFC nanodroplets with different core boiling points, and leverages their unique temporal responses to an acoustic trigger to differentiate their unique contributions to the overall ultrasound signal.</li>
<li>results:  The paper demonstrates the potential of this approach for multiplex ultrasound imaging, showing that the relative concentrations of the two populations of PFC nanodroplets can be accurately measured in the same imaging volume within an average error of 1.1%.<details>
<summary>Abstract</summary>
Among the various molecular imaging modalities, ultrasound imaging benefits from its real-time, nonionizing, and cost-effective nature. Despite its benefits, there is a dearth of methods to visualize two or more populations of contrast agents simultaneously, a technique known as multiplex imaging. In this paper, we present a new approach to multiplex ultrasound imaging using perfluorocarbon (PFC) nanodroplets. The nanodroplets, which undergo a liquid-to-gas phase transition in response to an acoustic trigger, act as activatable contrast agents. By using two populations of PFC nanodroplets, each with a different core boiling point, their unique temporal responses to an acoustic trigger were leveraged to differentiate their unique contributions to the overall ultrasound signal. This work characterized the dynamic responses of two PFC nanodroplets with boiling points of 28 and 56 {\deg}C. These characteristic responses were then used to demonstrate that the relative concentrations of the two populations of PFC nanodroplets could be accurately measured in the same imaging volume within an average error of 1.1%. Overall, the findings indicate the potential of this approach for multiplex ultrasound imaging, allowing for the visualization of multiple molecular targets simultaneously.
</details>
<details>
<summary>摘要</summary>
在多种分子成像方法中，超声成像具有实时、非离子和cost-effective的特点，但 simultanously visualizing two or more populations of contrast agents still remains a challenge, a technique known as multiplex imaging. In this paper, we present a new approach to multiplex ultrasound imaging using perfluorocarbon (PFC) nanodroplets. The nanodroplets, which undergo a liquid-to-gas phase transition in response to an acoustic trigger, act as activatable contrast agents. By using two populations of PFC nanodroplets, each with a different core boiling point, their unique temporal responses to an acoustic trigger were leveraged to differentiate their unique contributions to the overall ultrasound signal. This work characterized the dynamic responses of two PFC nanodroplets with boiling points of 28 and 56 ℃. These characteristic responses were then used to demonstrate that the relative concentrations of the two populations of PFC nanodroplets could be accurately measured in the same imaging volume within an average error of 1.1%. Overall, the findings indicate the potential of this approach for multiplex ultrasound imaging, allowing for the visualization of multiple molecular targets simultaneously.
</details></li>
</ul>
<hr>
<h2 id="Depolarized-Holography-with-Polarization-multiplexing-Metasurface"><a href="#Depolarized-Holography-with-Polarization-multiplexing-Metasurface" class="headerlink" title="Depolarized Holography with Polarization-multiplexing Metasurface"></a>Depolarized Holography with Polarization-multiplexing Metasurface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14668">http://arxiv.org/abs/2309.14668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seung-Woo Nam, Youngjin Kim, Dongyeon Kim, Yoonchan Jeong</li>
<li>for: 提高投射幕技术的表现，超越物理限制</li>
<li>methods: 利用受体表面，充分利用偏振度的多样性，实现无关性的投射幕显示</li>
<li>results: 实验和 simulations 表明，通过偏振度多样性，可以减少雾点噪声，提高图像质量<details>
<summary>Abstract</summary>
The evolution of computer-generated holography (CGH) algorithms has prompted significant improvements in the performances of holographic displays. Nonetheless, they start to encounter a limited degree of freedom in CGH optimization and physical constraints stemming from the coherent nature of holograms. To surpass the physical limitations, we consider polarization as a new degree of freedom by utilizing a novel optical platform called metasurface. Polarization-multiplexing metasurfaces enable incoherent-like behavior in holographic displays due to the mutual incoherence of orthogonal polarization states. We leverage this unique characteristic of a metasurface by integrating it into a holographic display and exploiting polarization diversity to bring an additional degree of freedom for CGH algorithms. To minimize the speckle noise while maximizing the image quality, we devise a fully differentiable optimization pipeline by taking into account the metasurface proxy model, thereby jointly optimizing spatial light modulator phase patterns and geometric parameters of metasurface nanostructures. We evaluate the metasurface-enabled depolarized holography through simulations and experiments, demonstrating its ability to reduce speckle noise and enhance image quality.
</details>
<details>
<summary>摘要</summary>
计算机生成投射法（CGH）的进化已经提高了投射显示器的性能。然而，它们开始遇到物理限制，即投射的干扰性。为超越物理限制，我们利用一种新的自由度，即极化。我们使用一种新的光学平台，即元件表面（metasurface），以实现无关的行为。元件表面的多极化能力使得投射显示器的行为更加不干扰。我们利用这一特点，并通过在投射显示器中集成元件表面，以及利用极化多样性来带来一个额外的自由度，以便CGH算法进行优化。为最小化干扰噪和最大化图像质量，我们设计了一个完全可导优化管道，包括元件表面代理模型，以联合投射显示器的灵敏度模ulator相位模式和元件表面 nanostructure 的几何参数。我们通过实验和仿真来评估元件表面启用的投射极化投射，并证明其能够减少干扰噪并提高图像质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/26/eess.IV_2023_09_26/" data-id="clpztdnsp01b1es88fcw3cq12" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/26/eess.SP_2023_09_26/" class="article-date">
  <time datetime="2023-09-26T08:00:00.000Z" itemprop="datePublished">2023-09-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/26/eess.SP_2023_09_26/">eess.SP - 2023-09-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Fully-Adaptive-Time-Varying-Wave-Shape-Model-Applications-in-Biomedical-Signal-Processing"><a href="#Fully-Adaptive-Time-Varying-Wave-Shape-Model-Applications-in-Biomedical-Signal-Processing" class="headerlink" title="Fully Adaptive Time-Varying Wave-Shape Model: Applications in Biomedical Signal Processing"></a>Fully Adaptive Time-Varying Wave-Shape Model: Applications in Biomedical Signal Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15211">http://arxiv.org/abs/2309.15211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joaquinr-uner/tvwse">https://github.com/joaquinr-uner/tvwse</a></li>
<li>paper_authors: Joaquin Ruiz, Gastón Schlotthauer, Leandro Vignolo, Marcelo A. Colominas</li>
<li>for: 这篇论文的目的是提出一个时间 varying wave-shape 抽取算法，用于处理非站点信号。</li>
<li>methods: 这篇论文使用了一个修改后的适应非干扰模型，来捕捉非站点信号的时间 varying wave-shape 信息。</li>
<li>results: 这篇论文的算法可以高效地从非站点信号中提取时间 varying wave-shape 信息，并且在含高水平的噪音情况下表现更好，比较 existing wave-shape 估计算法和基于短时间傅立卷变数的检测方法。实际上，这篇论文还用于电普雷agraph 信号的静态� States 分析和静态� States 构成 waveform 的分析。<details>
<summary>Abstract</summary>
In this work, we propose a time-varying wave-shape extraction algorithm based on a modified version of the adaptive non-harmonic model for non-stationary signals. The model codifies the time-varying wave-shape information in the relative amplitude and phase of the harmonic components of the wave-shape. The algorithm was validated on both real and synthetic signals for the tasks of denoising, decomposition and adaptive segmentation. For the denoising task, both monocomponent and multicomponent synthetic signals were considered. In both cases, the proposed algorithm can accurately recover the time-varying wave-shape of non-stationary signals, even in the presence of high levels of noise, outperforming existing wave-shape estimation algorithms and denoising methods based on short-time Fourier transform thresholding. The denoising of an electroencephalograph signal was also performed, giving similar results. For decomposition, our proposal was able to recover the composing waveforms more accurately by considering the time variations from the harmonic amplitude functions when compared to existing methods. Finally, the algorithm was used for the adaptive segmentation of synthetic signals and an electrocardiograph of a patient undergoing ventricular fibrillation.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种基于修改后的适应非固定模型的时变波形抽取算法，用于处理非站ARY信号。该模型在相对幅度和相位中codifies时变波形信息。我们验证了该算法在实际和 sintetic 信号上进行了denoising、decomposition和 adaptive segmentation 任务中的性能。对于denoising任务，我们考虑了单组件和多组件的synthetic信号。在两种情况下，我们的提案可以准确地回归非站ARY信号中的时变波形，即使在高噪声水平下，超过了基于短时域快推变换的杜邦滤波法和杜邦滤波法。此外，我们还应用了该算法于电enzephalograph信号的denoising任务，获得类似的结果。对于decomposition任务，我们的提案可以更加准确地回归组成波形，因为考虑了时变幅度函数中的时变信息。与现有方法相比，我们的方法可以更好地回归非站ARY信号的组成部分。最后，我们使用了该算法进行了adaptive segmentation Synthetic信号和一个患有心脏缺陷的病人的electrocardiograph信号。
</details></li>
</ul>
<hr>
<h2 id="Wave-shape-Function-Model-Order-Estimation-by-Trigonometric-Regression"><a href="#Wave-shape-Function-Model-Order-Estimation-by-Trigonometric-Regression" class="headerlink" title="Wave-shape Function Model Order Estimation by Trigonometric Regression"></a>Wave-shape Function Model Order Estimation by Trigonometric Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15210">http://arxiv.org/abs/2309.15210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joaquin Ruiz, Marcelo A. Colominas</li>
<li>for: 这个论文旨在研究非固定幅度和相位的抽象 oscilating signal 的表示方法，并提出一种基于adaptive trigonometric regression的方法来估计波形函数（WSF）中的幂数。</li>
<li>methods: 该论文使用了适应 trigonometric regression 模型选择 criterion 来解决 estimating the number of harmonic components of WSF 问题，并将其应用到非站立波形信号中。</li>
<li>results: 实验结果表明，该方法可以有效地重construct non-stationary signals with non-sinusoidal oscillatory patterns，即使在噪声存在的情况下。  furthermore, the proposed method can denoise simulated pulse wave signals and take into account the interpatient waveform variability of ECG and respiratory signals.<details>
<summary>Abstract</summary>
The adaptive non-harmonic (ANH) model is a powerful tool to compactly represent oscillating signals with time-varying amplitude and phase, and non-sinusoidal oscillating morphology. Given good estimators of instantaneous amplitude and phase we can construct an adaptive model, where the morphology of the oscillation is described by the wave-shape function (WSF), a 2{\pi}-periodic more general periodic function. In this paper, we address the problem of estimating the number of harmonic components of the WSF, a problem that remains underresearched, by adapting trigonometric regression model selection criteria into this context. We study the application of these criteria, originally developed in the context of stationary signals, to the case of signals with time-varying amplitudes and phases. We then incorporate the order estimation to the ANH model reconstruction procedure and analyze its performance for noisy AM-FM signals. Experimental results on synthethic signals indicate that these criteria enable the adaptive estimation of the waveform of non-stationary signals with non-sinusoidal oscillatory patterns, even in the presence of considerable amount of noise. We also apply our reconstruction procedure to the task of denoising simulated pulse wave signals and determine that the proposed technique performs competitively to other denoising schemes. We conclude this work by showing that our adaptive order estimation algorithm takes into account the interpatient waveform variability of the electrocardiogram (ECG) and respiratory signals by analyzing recordings from the Fantasia Database.
</details>
<details>
<summary>摘要</summary>
“非传统的非伤害（ANH）模型是一个强大的工具，可以简洁地表示时间变化的振荡信号，包括时间变化的振幅和相位。我们可以透过适当的估计几何和相位，创建一个适应型模型，其中振荡模式由波形函数（WSF）描述，这是一个2π periodic的更一般的周期函数。在这篇文章中，我们研究了对WSF的数量估计问题，这个问题在这个 контексті仍未得到充分研究。我们运用了这些标准的 trigonometric regression 模型选择 criterion 到这个 контексті中，并研究了这些 criterion 的应用。我们然后将这些数量估计 integrate 到 ANH 模型重建程序中，并分析了它们在噪音 AM-FM 信号上的性能。实验结果表明，这些 criterion 可以在非站ARY信号上实现适应性的数量估计，即使在充斥噪音的情况下。我们还将我们的重建程序应用到实验数据中，并发现它们与其他数据去噪程序相比，表现相当竞争。最后，我们显示了我们的适应数量估计算法能够考虑各种胸部电压ogram 和呼吸信号之间的波形Variability。”
</details></li>
</ul>
<hr>
<h2 id="Eve-Said-Yes-AirBone-Authentication-for-Head-Wearable-Smart-Voice-Assistant"><a href="#Eve-Said-Yes-AirBone-Authentication-for-Head-Wearable-Smart-Voice-Assistant" class="headerlink" title="Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice Assistant"></a>Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice Assistant</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15203">http://arxiv.org/abs/2309.15203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenpei Huang, Hui Zhong, Jie Lian, Pavana Prakash, Dian Shi, Yuan Xu, Miao Pan</li>
<li>for: 本研究旨在解决智能声助手上的声音攻击问题，通过采用头环设备（如耳buds和虚拟现实头sets）进行连续监测，并利用骨射频域的声音特征进行多因素身份验证（MFA）。</li>
<li>methods: 本研究使用了两个阶段的 AirBone 验证方法，首先确定 whether air and bone conduction utterances 是时域一致（TC），然后通过骨射频域的声音特征进行骨射频speaker recognition（BC-SR）。</li>
<li>results: 实验结果表明，提posed AirBone 验证方法具有可用性和安全性，可以轻松地通过商业市场上的头环设备进行实现，并且可以提供更高的安全水平，因为它可以抗御Current acoustic attacks 和高级 cross-domain attacks。<details>
<summary>Abstract</summary>
Recent advances in machine learning and natural language processing have fostered the enormous prosperity of smart voice assistants and their services, e.g., Alexa, Google Home, Siri, etc. However, voice spoofing attacks are deemed to be one of the major challenges of voice control security, and never stop evolving such as deep-learning-based voice conversion and speech synthesis techniques. To solve this problem outside the acoustic domain, we focus on head-wearable devices, such as earbuds and virtual reality (VR) headsets, which are feasible to continuously monitor the bone-conducted voice in the vibration domain. Specifically, we identify that air and bone conduction (AC/BC) from the same vocalization are coupled (or concurrent) and user-level unique, which makes them suitable behavior and biometric factors for multi-factor authentication (MFA). The legitimate user can defeat acoustic domain and even cross-domain spoofing samples with the proposed two-stage AirBone authentication. The first stage answers \textit{whether air and bone conduction utterances are time domain consistent (TC)} and the second stage runs \textit{bone conduction speaker recognition (BC-SR)}. The security level is hence increased for two reasons: (1) current acoustic attacks on smart voice assistants cannot affect bone conduction, which is in the vibration domain; (2) even for advanced cross-domain attacks, the unique bone conduction features can detect adversary's impersonation and machine-induced vibration. Finally, AirBone authentication has good usability (the same level as voice authentication) compared with traditional MFA and those specially designed to enhance smart voice security. Our experimental results show that the proposed AirBone authentication is usable and secure, and can be easily equipped by commercial off-the-shelf head wearables with good user experience.
</details>
<details>
<summary>摘要</summary>
Our approach is based on the observation that air and bone conduction (AC/BC) from the same vocalization are coupled and unique to each user, making them suitable for use as behavior and biometric factors in multi-factor authentication (MFA). In the first stage of our proposed method, we check whether the air and bone conduction utterances are time domain consistent (TC). If the utterances are consistent, we proceed to the second stage, which involves bone conduction speaker recognition (BC-SR).The use of AirBone authentication offers several advantages over traditional MFA methods. First, current acoustic attacks on smart voice assistants cannot affect bone conduction, which is in the vibration domain. Second, even for advanced cross-domain attacks, the unique bone conduction features can detect the adversary's impersonation and machine-induced vibration. Finally, AirBone authentication has good usability compared with traditional MFA and specialized methods designed to enhance smart voice security.Our experimental results show that the proposed AirBone authentication is both usable and secure, and can be easily equipped by commercial off-the-shelf head wearables with good user experience.
</details></li>
</ul>
<hr>
<h2 id="Application-of-reciprocity-for-facilitation-of-wave-field-visualization-and-defect-detection"><a href="#Application-of-reciprocity-for-facilitation-of-wave-field-visualization-and-defect-detection" class="headerlink" title="Application of reciprocity for facilitation of wave field visualization and defect detection"></a>Application of reciprocity for facilitation of wave field visualization and defect detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15198">http://arxiv.org/abs/2309.15198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bernd Köhler, Kanta Takahashi, Kazuyuki Nakahata</li>
<li>for: 研究了STRUCTURAL ком成分中的运动可视化 для缺陷检测</li>
<li>methods: 使用了hammer impacts at multiple points to excite elastic motions, and received by an accelerometer at a fixed point</li>
<li>results: 通过reciprocity in elastodynamics theorem, obtained the dynamic motion of the structural component for fixed-point excitation from measurements performed using multipoint excitations, and observed significant additional deformation at the wall thinning inserted as an artificial defect using maximum intensity projection method.<details>
<summary>Abstract</summary>
The motion visualization in a structural component was studied for defect detection. Elastic motions were excited by hammer impacts at multiple points and received by an accelerometer at a fixed point. Reciprocity in elastodynamics is only valid under certain conditions. Its validity under given experimental conditions was derived from the elastodynamic reciprocity theorem. Based on this, the dynamic motion of the structural component was obtained for fixed-point excitation from measurements performed using multipoint excitations. In the visualized eigenmodes, significant additional deformation was observed at the wall thinning inserted as an artificial defect. To prevent the dependence of defect detection on its position within the mode shape, another approach was proposed based on the extraction of guided wave modes immediately after impact excitation. It is shown that this maximum intensity projection method works well in detecting defects.
</details>
<details>
<summary>摘要</summary>
在结构组件中的运动可视化被研究用于缺陷检测。使用锤子影响的弹性运动被测量到固定点上的加速计上，并且根据刚Dynamic motion of the structural component was obtained from measurements performed using multipoint excitation. In the visualized eigenmodes, significant additional deformation was observed at the wall thinning inserted as an artificial defect. To prevent the dependence of defect detection on its position within the mode shape, another approach was proposed based on the extraction of guided wave modes immediately after impact excitation. It is shown that this maximum intensity projection method works well in detecting defects.Here's the translation in Traditional Chinese:在结构组件中的运动可见化被研究用于缺陷检测。使用锤子影响的弹性运动被量测到固定点上的加速计上，并且根据刚Dynamic motion of the structural component was obtained from measurements performed using multipoint excitation. In the visualized eigenmodes, significant additional deformation was observed at the wall thinning inserted as an artificial defect. To prevent the dependence of defect detection on its position within the mode shape, another approach was proposed based on the extraction of guided wave modes immediately after impact excitation. It is shown that this maximum intensity projection method works well in detecting defects.
</details></li>
</ul>
<hr>
<h2 id="Reliable-Majority-Vote-Computation-with-Complementary-Sequences-for-UAV-Waypoint-Flight-Control"><a href="#Reliable-Majority-Vote-Computation-with-Complementary-Sequences-for-UAV-Waypoint-Flight-Control" class="headerlink" title="Reliable Majority Vote Computation with Complementary Sequences for UAV Waypoint Flight Control"></a>Reliable Majority Vote Computation with Complementary Sequences for UAV Waypoint Flight Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15193">http://arxiv.org/abs/2309.15193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alphan Sahin, Xiaofeng Wang</li>
<li>for: 本研究提出了一种不协调的Over-the-air computation（OAC）方案，用于可靠地计算多个参数的多数投票（MV）在滑动频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频率频<details>
<summary>Abstract</summary>
In this study, we propose a non-coherent over-the-air computation (OAC) scheme to calculate the majority vote (MV) reliably in fading channels. The proposed approach relies on modulating the amplitude of the elements of complementary sequences (CSs) based on the sign of the parameters to be aggregated. Since it does not use channel state information at the nodes, it is compatible with time-varying channels. To demonstrate the efficacy of our method, we employ it in a scenario where an unmanned aerial vehicle (UAV) is guided by distributed sensors, relying on the MV computed using our proposed scheme. We show that the proposed scheme reduces the computation error rate notably with a longer sequence length in fading channels while maintaining the peak-to-mean-envelope power ratio of the transmitted orthogonal frequency division multiplexing signals to be less than or equal to 3 dB.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种非协调的天空计算（OAC）方案，以可靠地计算多数投票（MV）在淡化通道中。我们的方法基于对填充序列元素的振幅进行模拟，根据参数的符号来决定。由于不使用节点的通道状态信息，这种方法与时变通道相容。为证明我们的方法的有效性，我们在一个由分布式感知器引导的无人飞行器（UAV）上使用了我们的方法。我们显示，我们的方法可以在淡化通道中减少计算错误率，而且可以保持发射的多射频分多路复用信号的峰峰至平均功率比不超过3 dB。
</details></li>
</ul>
<hr>
<h2 id="AsQM-Audio-streaming-Quality-Metric-based-on-Network-Impairments-and-User-Preferences"><a href="#AsQM-Audio-streaming-Quality-Metric-based-on-Network-Impairments-and-User-Preferences" class="headerlink" title="AsQM: Audio streaming Quality Metric based on Network Impairments and User Preferences"></a>AsQM: Audio streaming Quality Metric based on Network Impairments and User Preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15186">http://arxiv.org/abs/2309.15186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcelo Rodrigo dos Santos, Andreza Patrícia Batista, Renata Lopes Rosa, Muhammad Saadi, Dick Carrillo Melgarejo, Demóstenes Zegarra Rodríguez</li>
<li>for: 这篇论文主要研究了音乐流式服务中的时间中断对用户体验质量（QoE）的影响，以及用户对音乐内容的喜好如何影响 QoE。</li>
<li>methods: 研究人员采用了主观测试方法，测试了不同的应用参数对用户 QoE 的影响，并发现用户对音乐内容的喜好对 QoE 具有重要作用。</li>
<li>results: 实验结果表明，用户对音乐内容的喜好对 QoE 具有重要作用，并且提出了一种基于用户喜好的 Audio streaming Quality Metric（AsQM）来衡量音乐流式服务质量。此外，实验还表明，在用户设备中实现 AsQM 对功耗、处理和内存占用产生了较低的影响。<details>
<summary>Abstract</summary>
There are many users of audio streaming services because of the proliferation of cloud-based audio streaming services for different content. The complex networks that support these services do not always guarantee an acceptable quality on the end-user side. In this paper, the impact of temporal interruptions on the reproduction of audio streaming and the users preference in relation to audio contents are studied. In order to determine the key parameters in the audio streaming service, subjective tests were conducted, and their results show that users Quality-of-Experience (QoE) is highly correlated with the following application parameters, the number of temporal interruptions or stalls, its frequency and length, and the temporal location in which they occur. However, most important, experimental results demonstrated that users preference for audio content plays an important role in users QoE. Thus, a Preference Factor (PF) function is defined and considered in the formulation of the proposed metric named Audio streaming Quality Metric (AsQM). Considering that multimedia service providers are based on web servers, a framework to obtain user information is proposed. Furthermore, results show that the AsQM implemented in the audio player of an end users device presents a low impact on energy, processing and memory consumption.
</details>
<details>
<summary>摘要</summary>
“现有许多音乐流媒体服务的用户，因为云端音乐流媒体服务的普及，导致不同内容的音乐流媒体服务。但是，这些服务支持的复杂网络不一定能提供用户端的可接受度。本文研究了音乐流媒体服务中的时间中断对于音乐重播的影响，以及用户对于音乐内容的偏好。通过调查，发现用户的品质体验（QoE）高度与以下应用程序参数相关：时间中断或停止的次数、时间长度和时间位置。但是，最重要的是，实验结果显示用户对音乐内容的偏好对于QoE有着重要的影响。因此，我们定义了偏好因子（PF）函数，并将其包含在提案的音乐流媒体质量指标（AsQM）中。考虑到多媒体服务提供商基于网页服务器，我们提出了一个框架来获取用户信息。结果显示，在用户设备上实现的AsQM具有低影响力、处理和内存占用。”
</details></li>
</ul>
<hr>
<h2 id="STAR-RIS-Assisted-Full-Duplex-Communication-Networks"><a href="#STAR-RIS-Assisted-Full-Duplex-Communication-Networks" class="headerlink" title="STAR-RIS Assisted Full-Duplex Communication Networks"></a>STAR-RIS Assisted Full-Duplex Communication Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15037">http://arxiv.org/abs/2309.15037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelhamid Salem, Kai-Kit Wong, Chan-Byoung Chae, Yangyang Zhang</li>
<li>for: 本研究探讨了一种具有完全360度覆盖能力的同时传输和反射智能表面（STAR-RIS）助力的全双工通信系统的性能。</li>
<li>methods: 本研究使用了非对称多access（NOMA）对口 schemes和考虑了系统缺陷，如基站自身干扰和不完美的成功接续干扰（SIC）。</li>
<li>results: 我们 derivatedclosed-form表达式来描述上下行通信的质量因子，并对bidirectional通信进行了扩展分析。此外，我们还提出了一个最大化 Erdos均速率的优化问题，该问题包括调整STAR-RIS元素的振荡和相位偏移，以及有效分配总传输功率。<details>
<summary>Abstract</summary>
Different from conventional reconfigurable intelligent surfaces (RIS), a recent innovation called simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) has emerged, aimed at achieving complete 360-degree coverage in communication networks. Additionally, fullduplex (FD) technology is recognized as a potent approach for enhancing spectral efficiency by enabling simultaneous transmission and reception within the same time and frequency resources. In this study, we investigate the performance of a STAR-RIS-assisted FD communication system. The STAR-RIS is strategically placed at the cell-edge to facilitate communication for users located in this challenging region, while cell-center users can communicate directly with the FD base station (BS). We employ a non-orthogonal multiple access (NOMA) pairing scheme and account for system impairments, such as self-interference at the BS and imperfect successive interference cancellation (SIC). We derive closed-form expressions for the ergodic rates in both the up-link and down-link communications and extend our analysis to bidirectional communication between cell-center and cell-edge users. Furthermore, we formulate an optimization problem aimed at maximizing the ergodic sum-rate. This optimization involves adjusting the amplitudes and phase-shifts of the STAR-RIS elements and allocating total transmit power efficiently. To gain deeper insights into the achievable rates of STAR-RIS-aided FD systems, we explore the impact of various system parameters through numerical results.
</details>
<details>
<summary>摘要</summary>
不同于传统的可重新配置智能表面（RIS），最近的创新是同时传输和反射可重新配置智能表面（STAR-RIS），旨在实现全天猫360度的覆盖率在通信网络中。此外，全双工（FD）技术被认为是提高频率效率的强大方法，可以在同一时间和频率资源上同时进行传输和接收。在这项研究中，我们研究了STAR-RIS帮助FD通信系统的性能。STAR-RIS位于终端处，以便为位于这个困难区域的用户提供通信，而中心用户可以直接与FD基站（BS）进行通信。我们采用了不对称多接入（NOMA）对pairing schemes，并考虑了系统障碍物，如BS自身的自适应干扰和不完全的Successive Interference Cancellation（SIC）。我们 derivatedclosed-form表达式来描述在上行和下行通信中的质量因子，并将分析扩展到双向通信 между中心用户和边缘用户。此外，我们形ulated一个优化问题，旨在最大化服务器的质量因子。这个优化问题包括调整STAR-RIS元素的振荡和相位偏移，以及有效地分配总传输功率。通过numerical results，我们深入探讨STAR-RIS帮助FD系统实现的可能的速率。
</details></li>
</ul>
<hr>
<h2 id="Quadratic-Detection-in-Noncoherent-Massive-SIMO-Systems-over-Correlated-Channels"><a href="#Quadratic-Detection-in-Noncoherent-Massive-SIMO-Systems-over-Correlated-Channels" class="headerlink" title="Quadratic Detection in Noncoherent Massive SIMO Systems over Correlated Channels"></a>Quadratic Detection in Noncoherent Massive SIMO Systems over Correlated Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15030">http://arxiv.org/abs/2309.15030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Vilà-Insa, Aniol Martí, Jaume Riba, Meritxell Lamarca</li>
<li>for: 帮助实现低延迟和高可靠性无线通信 для工业互联网关键物联网（IIoT）。</li>
<li>methods: 使用能量基本模ulation在非共振性大量单输入多输出（SIMO）系统中进行研究。</li>
<li>results: 提出了一种基于最大可能性探测的非正负抽象极限的证明，并提供了一种基于统计知识的通信器抗干扰性能更好的设计框架。这种设计框架可以在中等和高SNR水平下提供更低的错误率。<details>
<summary>Abstract</summary>
With the goal of enabling ultrareliable and low-latency wireless communications for industrial internet of things (IIoT), this paper studies the use of energy-based modulations in noncoherent massive single input multiple output (SIMO) systems. We consider a one-shot communication over a channel with correlated Rayleigh fading and colored Gaussian noise. We first provide a theoretical analysis on the limitations of non-negative pulse-amplitude modulation (PAM) in systems of this kind, based on maximum likelihood detection. The existence of a fundamental error floor at high signal-to-noise ratio (SNR) regimes is proved for constellations with more than two energy levels, when no (statistical) channel state information is available at the transmitter. In the main body of the paper, we present a design framework for quadratic detectors that generalizes the widely-used energy detector, to better exploit the statistical knowledge of the channel. This allows us to design receivers optimized according to information-theoretic criteria that exhibit lower error rates at moderate and high SNR. We subsequently derive an analytic approximation for the error probability of a general class of quadratic detectors in the large array regime. Finally, we introduce an improved reception scheme based on the combination of quadratic detectors and assess its capabilities numerically.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传输goal是实现低延迟和无障碍无线通信，这篇论文研究了基于能量模式的非共振性大量单输入多输出（SIMO）系统。我们考虑了一次性通信在具有相关的徐杰尼谱折射和频率噪声的通道上。我们首先提供了非共振性PAM在这种系统中的限制分析，基于最大likelihood检测。在高信号噪声比例（SNR） regime中存在基本错误地板，当无 statistically channel state information available at the transmitter。在主要文章中，我们提供了一种泛化 quadratic detector的设计框架，以更好地利用通道的统计知识。这使得我们可以根据信息学定义的标准设计接收器，并且在中等和高SNR regime exhibit lower error rates。我们随后 derive了一个 Analytic approximation for the error probability of a general class of quadratic detectors in the large array regime。最后，我们介绍了一种基于quadratic detector和 assess its capabilities numerically。Note: Simplified Chinese is a romanization of Mandarin Chinese, which is the official language of China. The translation is written in the Simplified Chinese format, which is used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Minimizing-Energy-Consumption-for-5G-NR-Beam-Management-for-RedCap-Devices"><a href="#Minimizing-Energy-Consumption-for-5G-NR-Beam-Management-for-RedCap-Devices" class="headerlink" title="Minimizing Energy Consumption for 5G NR Beam Management for RedCap Devices"></a>Minimizing Energy Consumption for 5G NR Beam Management for RedCap Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14971">http://arxiv.org/abs/2309.14971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manishika Rawat, Matteo Pagin, Marco Giordani, Louis-Adrien Dufrene, Quentin Lampin, Michele Zorzi</li>
<li>for: 降低5G New Radio（NR）中的照射管理中的能量消耗，以满足低成本、低复杂度和电池限制的设备，如RedCap设备，支持中等市场互联网器件（IoT）应用场景。</li>
<li>methods: 我们将该问题形式化为一个优化问题，在室内工厂（InF）场景中选择最佳照射管理参数，包括照射更新频率和照射宽度，以最小化能量消耗，基于用户分布和速度。</li>
<li>results: 分析得到了可行范围，即RedCap设备的照射管理参数的Upper limit，我们可以根据这些指导方针进行设计。<details>
<summary>Abstract</summary>
In 5G New Radio (NR), beam management entails periodic and continuous transmission and reception of control signals in the form of synchronization signal blocks (SSBs), used to perform initial access and/or channel estimation. However, this procedure demands continuous energy consumption, which is particularly challenging to handle for low-cost, low-complexity, and battery-constrained devices, such as RedCap devices to support mid-market Internet of Things (IoT) use cases. In this context, this work aims at reducing the energy consumption during beam management for RedCap devices, while ensuring that the desired Quality of Service (QoS) requirements are met. To do so, we formalize an optimization problem in an Indoor Factory (InF) scenario to select the best beam management parameters, including the beam update periodicity and the beamwidth, to minimize energy consumption based on users' distribution and their speed. The analysis yields the regions of feasibility, i.e., the upper limit(s) on the beam management parameters for RedCap devices, that we use to provide design guidelines accordingly.
</details>
<details>
<summary>摘要</summary>
在5G新 Radio（NR）中，磁力管理包括 periodic和连续的控制信号传输和接收，用于初始访问和/或通道估计。但这些过程需要不断的能量消耗，尤其是 для低成本、低复杂度和电池受限的设备，如RedCap设备，以支持中高级Internet of Things（IoT）应用场景。在这个上下文中，本工作的目标是在磁力管理中降低RedCap设备的能量消耗，以确保达到所需的质量服务（QoS）要求。为此，我们将在室内工厂（InF）场景中形式化优化问题，选择最佳的磁力管理参数，包括磁力更新频率和磁力宽度，以最小化RedCap设备的能量消耗，基于用户的分布和速度。分析得到了可行范围，即RedCap设备的磁力管理参数的Upper limit，我们可以根据这些设计指南来提供相应的设计建议。
</details></li>
</ul>
<hr>
<h2 id="ML-based-PBCH-symbol-detection-and-equalization-for-5G-Non-Terrestrial-Networks"><a href="#ML-based-PBCH-symbol-detection-and-equalization-for-5G-Non-Terrestrial-Networks" class="headerlink" title="ML-based PBCH symbol detection and equalization for 5G Non-Terrestrial Networks"></a>ML-based PBCH symbol detection and equalization for 5G Non-Terrestrial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14923">http://arxiv.org/abs/2309.14923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inés Larráyoz-Arrigote, Marcele O. K. Mendonca, Alejandro Gonzalez-Garrido, Jevgenij Krivochiza, Sumit Kumar, Jorge Querol, Joel Grotz, Stefano Andrenacci, Symeon Chatzinotas</li>
<li>for: 本研究探讨了在5G无 terrestrial网络（5G-NTN）中应用机器学习（ML）技术的可能性，特别是对物理广播频道（PBCH）的符号检测和平衡。</li>
<li>methods: 本研究使用了 synthetic 和实际数据，从实际的5G通过卫星测试环境中收集到的数据进行训练。我们的分析包括在不同的信号噪听比（SNR）情况下对这些模型的性能进行评估，以及对符号增强和通道平衡任务进行评估。</li>
<li>results: 结果显示了在控制的环境中ML的性能，以及其适应实际挑战的能力。这些结果 shed light on the potential benefits of applying ML in 5G-NTN, and provide a basis for further research in this area.<details>
<summary>Abstract</summary>
This paper delves into the application of Machine Learning (ML) techniques in the realm of 5G Non-Terrestrial Networks (5G-NTN), particularly focusing on symbol detection and equalization for the Physical Broadcast Channel (PBCH). As 5G-NTN gains prominence within the 3GPP ecosystem, ML offers significant potential to enhance wireless communication performance. To investigate these possibilities, we present ML-based models trained with both synthetic and real data from a real 5G over-the-satellite testbed. Our analysis includes examining the performance of these models under various Signal-to-Noise Ratio (SNR) scenarios and evaluating their effectiveness in symbol enhancement and channel equalization tasks. The results highlight the ML performance in controlled settings and their adaptability to real-world challenges, shedding light on the potential benefits of the application of ML in 5G-NTN.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这篇论文探讨了在5G无 terrestrial 网络 (5G-NTN) 中应用机器学习 (ML) 技术，尤其是对物理广播频道 (PBCH) 的符号检测和平衡。随着5G-NTN在3GPP生态系统中的崛起，ML有可能在无线通信性能方面提供显著的提升。为了探索这些可能性，我们提出了基于 ML 的模型，使用了真实数据和验证数据从一个真实的5G过球测试平台进行训练。我们的分析包括在不同的信号噪听比 (SNR) 场景下评估这些模型的性能，以及对符号增强和频道平衡任务的评估。结果表明 ML 在控制场景下的性能和其适应实际挑战的能力，这 shedding light on the potential benefits of applying ML in 5G-NTN.
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Channel-Estimation-in-mm-Wave-MIMO-Systems-Leveraging-Integrated-Communication-and-Sensing"><a href="#Enhanced-Channel-Estimation-in-mm-Wave-MIMO-Systems-Leveraging-Integrated-Communication-and-Sensing" class="headerlink" title="Enhanced Channel Estimation in mm-Wave MIMO Systems Leveraging Integrated Communication and Sensing"></a>Enhanced Channel Estimation in mm-Wave MIMO Systems Leveraging Integrated Communication and Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14875">http://arxiv.org/abs/2309.14875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silvia Mura, Marouan Mizmizi, Umberto Spagnolini, Athina Petropulu</li>
<li>for: 这篇论文解决了indoor millimeter-wave场景中宽带MIMO通道估算的挑战。</li>
<li>methods: 该方法利用了 integrate sensing and communication paradigm，使估算信道所需的训练导航数量减少了4倍。</li>
<li>results: 实验表明，该方法可以减少4倍的训练导航数量，并且能够正确地修复感知和通信模式之间的匹配问题。<details>
<summary>Abstract</summary>
This paper tackles the challenge of wideband MIMO channel estimation within indoor millimeter-wave scenarios. Our proposed approach exploits the integrated sensing and communication paradigm, where sensing information aids in channel estimation. The key innovation consists of employing both spatial and temporal sensing modes to significantly reduce the number of required training pilots. Moreover, our algorithm addresses and corrects potential mismatches between sensing and communication modes, which can arise from differing sensing and communication propagation paths. Extensive simulations demonstrate that the proposed method requires 4x less pilots compared to the current state-of-the-art, marking a substantial advancement in channel estimation efficiency.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文研究了indoor millimeter-wave场景中广带MIMO通道估算的挑战。我们提出的方法利用了整合感知和通信的思想，通过感知信息帮助通道估算。我们的算法利用了空间和时间感知模式，可以减少需要的训练导航器数量。此外，我们的算法还解决了感知和通信传播路径之间的差异，可以提高通道估算精度。广泛的 simulations 表明，我们的方法可以比现有技术减少4倍的导航器数量，标志着通道估算效率的显著提升。
</details></li>
</ul>
<hr>
<h2 id="Multi-static-Parameter-Estimation-in-the-Near-Far-Field-Beam-Space-for-Integrated-Sensing-and-Communication-Applications"><a href="#Multi-static-Parameter-Estimation-in-the-Near-Far-Field-Beam-Space-for-Integrated-Sensing-and-Communication-Applications" class="headerlink" title="Multi-static Parameter Estimation in the Near&#x2F;Far Field Beam Space for Integrated Sensing and Communication Applications"></a>Multi-static Parameter Estimation in the Near&#x2F;Far Field Beam Space for Integrated Sensing and Communication Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14778">http://arxiv.org/abs/2309.14778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeid K. Dehkordi, Lorenzo Pucci, Peter Jung, Andrea Giorgetti, Enrico Paolini, Giuseppe Caire</li>
<li>for: 提出一种基于最大可能性（ML）估计框架，用于 millimeter 波（mmWave）集成感知通信（ISAC）系统中的多Static配置。</li>
<li>methods: 使用能效的混合数字分析阵列来实现hybrid digital-analog arrays，并在近场 режиме下使用两stage估计过程来估计目标参数。</li>
<li>results: 通过数字实验来评估提出的框架效果，并表明使用自定义的扁平增益编码字符串可以提高系统的通信性能。<details>
<summary>Abstract</summary>
This work proposes a maximum likelihood (ML)-based parameter estimation framework for a millimeter wave (mmWave) integrated sensing and communication (ISAC) system in a multi-static configuration using energy-efficient hybrid digital-analog arrays. Due to the typically large arrays deployed in the higher frequency bands to mitigate isotropic path loss, such arrays may operate in the near-field regime. The proposed parameter estimation in this work consists of a two-stage estimation process, where the first stage is based on far-field assumptions, and is used to obtain a first estimate of the target parameters. In cases where the target is determined to be in the near-field of the arrays, a second estimation based on near-field assumptions is carried out to obtain more accurate estimates. In particular, we select beamfocusing array weights designed to achieve a constant gain over an extended spatial region and re-estimate the target parameters at the receivers. We evaluate the effectiveness of the proposed framework in numerous scenarios through numerical simulations and demonstrate the impact of the custom-designed flat-gain beamfocusing codewords in increasing the communication performance of the system.
</details>
<details>
<summary>摘要</summary>
这个工作提出了基于最大可能性（ML）的参数估算框架，用于毫米波（mmWave）集成感知通信（ISAC）系统的多Static配置中。由于高频段的大型阵列在减轻各向异性视场损失，这些阵列可能会在近场区间运行。本文中的参数估算包括两个阶段的估算过程，其中第一阶段基于远场假设，用于获得初步目标参数估算。在目标确定在阵列近场区间时，进行第二阶段基于近场假设的估算，以获得更加准确的估算结果。特别是，我们选择了用于实现恒定增益的扩散焦点阵列重量，并在接收器上重新估算目标参数。我们通过数字 simulations 证明了该框架在各种场景中的效果，并示出了自定义的扁平增益扩散编码字符串在系统的通信性能中的影响。
</details></li>
</ul>
<hr>
<h2 id="Toward-Energy-Efficient-Multiuser-IRS-Assisted-URLLC-Systems-A-Novel-Rank-Relaxation-Method"><a href="#Toward-Energy-Efficient-Multiuser-IRS-Assisted-URLLC-Systems-A-Novel-Rank-Relaxation-Method" class="headerlink" title="Toward Energy Efficient Multiuser IRS-Assisted URLLC Systems: A Novel Rank Relaxation Method"></a>Toward Energy Efficient Multiuser IRS-Assisted URLLC Systems: A Novel Rank Relaxation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14606">http://arxiv.org/abs/2309.14606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jalal Jalali, Filip Lemic, Hina Tabassum, Rafael Berkvens, Jeroen Famaey</li>
<li>for: 该论文提出了一种智能反射 повер面（IRS）助け的下降ULTRA-可靠低延迟通信（URLLC）网络的能源效率资源配置设计算法。该设置包括一个多antenna基站（BS）发送数据流量到一组URLLC用户，具有短包长。我们通过BS的活动扫描器和IRS的pasive扫描器（即相位Shift）的优化，提高整个网络的能源效率（EE）。</li>
<li>methods: 该论文使用了一种分割优化方法（AO）来解决主要的非杂 convex 问题。通过成功ive convex approximation（SCA）和一种新的迭代rank relaxation方法，我们构建了一个凹降-convex 目标函数。首先，我们解决了第一个子问题，即一个分数程序，使用Dinkelbach方法和罚金方法。然后，我们解决了第二个子问题，即基于 semi-definite programming（SDP）和罚金方法。</li>
<li>results: 我们的结果表明，提案的解决方案比现有的参考解决方案更有效。<details>
<summary>Abstract</summary>
This paper proposes an energy efficient resource allocation design algorithm for an intelligent reflecting surface (IRS)-assisted downlink ultra-reliable low-latency communication (URLLC) network. This setup features a multi-antenna base station (BS) transmitting data traffic to a group of URLLC users with short packet lengths. We maximize the total network's energy efficiency (EE) through the optimization of active beamformers at the BS and passive beamformers (a.k.a. phase shifts) at the IRS. The main non-convex problem is divided into two sub-problems. An alternating optimization (AO) approach is then used to solve the problem. Through the use of the successive convex approximation (SCA) with a novel iterative rank relaxation method, we construct a concave-convex objective function for each sub-problem. The first sub-problem is a fractional program that is solved using the Dinkelbach method and a penalty-based approach. The second sub-problem is then solved based on semi-definite programming (SDP) and the penalty-based approach. The iterative solution gradually approaches the rank-one for both the active beamforming and unit modulus IRS phase-shift sub-problems. Our results demonstrate the efficacy of the proposed solution compared to existing benchmarks.
</details>
<details>
<summary>摘要</summary>
The main non-convex problem is divided into two sub-problems, and an alternating optimization (AO) approach is used to solve the problem. The first sub-problem is a fractional program that is solved using the Dinkelbach method and a penalty-based approach. The second sub-problem is then solved based on semi-definite programming (SDP) and the penalty-based approach. The iterative solution gradually approaches the rank-one for both the active beamforming and unit modulus IRS phase-shift sub-problems.The proposed solution is compared to existing benchmarks, and the results demonstrate the efficacy of the proposed solution. The key contribution of this paper is the development of an energy-efficient resource allocation design algorithm for IRS-assisted URLLC networks, which can significantly reduce the energy consumption of the network while maintaining the required reliability and latency.Here is the text in Simplified Chinese:这篇论文提出了一种智能反射Surface（IRS）助手的下行ultra-可靠低延迟通信（URLLC）网络的能源效率资源分配算法。在这种设置中，一个多antenna基站（BS）将数据流传输到一个URLLC用户群体，用户 packets的长度很短。我们通过对BS的活动扬声器和IRS的pasive扬声器（即相位Shift）进行优化，以最大化总网络的能源效率（EE）。主要非凸问题被分解成两个互相关联的优化问题。我们使用alternating optimization（AO）方法来解决问题。通过successive convex approximation（SCA）和一种新的迭代rank relaxation方法，我们构建了一个凹陷-凸函数对象函数。第一个优化问题是一个分数程序，通过Dinkelbach方法和一种罚金方法来解决。第二个优化问题是基于 semi-definite programming（SDP）和罚金方法来解决。迭代解决方案逐渐逼近rank-one для活动扬声器和IRS phase-shift优化问题。我们的结果表明，提出的解决方案比现有的标准做法更有效。这篇论文的关键贡献在于开发了一种智能反射Surface（IRS）助手的下行URLLC网络中的能源效率资源分配算法，可以减少网络的能 consumption，保持必要的可靠性和延迟。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/26/eess.SP_2023_09_26/" data-id="clpztdnuj01f4es88482y9kfw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/cs.SD_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T15:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/cs.SD_2023_09_25/">cs.SD - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NoLACE-Improving-Low-Complexity-Speech-Codec-Enhancement-Through-Adaptive-Temporal-Shaping"><a href="#NoLACE-Improving-Low-Complexity-Speech-Codec-Enhancement-Through-Adaptive-Temporal-Shaping" class="headerlink" title="NoLACE: Improving Low-Complexity Speech Codec Enhancement Through Adaptive Temporal Shaping"></a>NoLACE: Improving Low-Complexity Speech Codec Enhancement Through Adaptive Temporal Shaping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14521">http://arxiv.org/abs/2309.14521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Büthe, Ahmed Mustafa, Jean-Marc Valin, Karim Helwani, Michael M. Goodwin</li>
<li>for: 提高 speech codec 的质量，对于 Opus 编码器进行增强。</li>
<li>methods: 使用 Linear Adaptive Coding Enhancer (LACE) 模型，combines DNNs with classical long-term&#x2F;short-term postfiltering，具有低复杂性和零延迟。</li>
<li>results: 比较 Opus 基线和扩大 LACE 模型，NoLACE 实现了较高的质量表现，并且与 ASR 系统良好的合作。<details>
<summary>Abstract</summary>
Speech codec enhancement methods are designed to remove distortions added by speech codecs. While classical methods are very low in complexity and add zero delay, their effectiveness is rather limited. Compared to that, DNN-based methods deliver higher quality but they are typically high in complexity and/or require delay. The recently proposed Linear Adaptive Coding Enhancer (LACE) addresses this problem by combining DNNs with classical long-term/short-term postfiltering resulting in a causal low-complexity model. A short-coming of the LACE model is, however, that quality quickly saturates when the model size is scaled up. To mitigate this problem, we propose a novel adatpive temporal shaping module that adds high temporal resolution to the LACE model resulting in the Non-Linear Adaptive Coding Enhancer (NoLACE). We adapt NoLACE to enhance the Opus codec and show that NoLACE significantly outperforms both the Opus baseline and an enlarged LACE model at 6, 9 and 12 kb/s. We also show that LACE and NoLACE are well-behaved when used with an ASR system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通用 speech codec 优化方法是为了移除 speech codec 添加的扭曲。古典方法具有非常低的复杂性和零延迟，但是其效果很有限。相比之下，基于 DNN 的方法可以提供更高的质量，但是它们通常具有较高的复杂性和/或延迟。随后提出的 Linear Adaptive Coding Enhancer (LACE) 模型解决了这个问题，它将 DNN 与古典长期/短期 POSTfiltering 结合在一起，实现了 causal 低复杂度模型。然而，LACE 模型的缺点是，当模型大小增加时，质量快速增加。为了解决这个问题，我们提出了一种新的适应性时间形态模块，这种模块将高时间分辨率添加到 LACE 模型中，实现了 Non-Linear Adaptive Coding Enhancer (NoLACE)。我们适应 NoLACE 模型来提高 Opus 码ц，并证明 NoLACE 在 6、9 和 12 kb/s 的比特率下显著超过 Opus 基eline 和扩大 LACE 模型。我们还证明 LACE 和 NoLACE 在 ASR 系统中是合理的。
</details></li>
</ul>
<hr>
<h2 id="Noise-Robust-DSP-Assisted-Neural-Pitch-Estimation-with-Very-Low-Complexity"><a href="#Noise-Robust-DSP-Assisted-Neural-Pitch-Estimation-with-Very-Low-Complexity" class="headerlink" title="Noise-Robust DSP-Assisted Neural Pitch Estimation with Very Low Complexity"></a>Noise-Robust DSP-Assisted Neural Pitch Estimation with Very Low Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14507">http://arxiv.org/abs/2309.14507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Subramani, Jean-Marc Valin, Jan Buethe, Paris Smaragdis, Mike Goodwin</li>
<li>for: 这篇论文旨在提出一种hybrid抽取器，使得深度神经网络（DNN）和传统的信号处理（DSP）技术之间的优点得到平衡，以提高抽取器的性能和可实现性。</li>
<li>methods: 该论文使用了一种小型的DNN和传统的DSP特征来实现抽取器，并结合了这两种方法来提高抽取器的性能。</li>
<li>results: 论文表明，这种混合方法可以与纯DNN方法匹配或超越其性能，同时具有与传统DSP方法相同的复杂性和算法延迟。此外，该方法还可以提供一些优势 для神经语音编码任务。<details>
<summary>Abstract</summary>
Pitch estimation is an essential step of many speech processing algorithms, including speech coding, synthesis, and enhancement. Recently, pitch estimators based on deep neural networks (DNNs) have have been outperforming well-established DSP-based techniques. Unfortunately, these new estimators can be impractical to deploy in real-time systems, both because of their relatively high complexity, and the fact that some require significant lookahead. We show that a hybrid estimator using a small deep neural network (DNN) with traditional DSP-based features can match or exceed the performance of pure DNN-based models, with a complexity and algorithmic delay comparable to traditional DSP-based algorithms. We further demonstrate that this hybrid approach can provide benefits for a neural vocoding task.
</details>
<details>
<summary>摘要</summary>
“抽象估值是许多语音处理算法的关键步骤，包括语音编码、合成和提高。最近，基于深度神经网络（DNN）的抽象估值器已经超越了传统的数字信号处理（DSP）技术。然而，这些新的估值器在实时系统中部署可能是不实际的，因为它们的相对较高复杂度和一些需要明显的往回预测。我们表明，一种混合使用小型深度神经网络（DNN）和传统的DSP基于特征的抽象估值器可以与纯DNN基本模型匹配或超越其性能，并且与传统DSP基本算法相同的复杂性和算法延迟。我们还证明了这种混合方法可以为神经编码任务提供优势。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="On-the-Impact-of-Quantization-and-Pruning-of-Self-Supervised-Speech-Models-for-Downstream-Speech-Recognition-Tasks-“In-the-Wild’’"><a href="#On-the-Impact-of-Quantization-and-Pruning-of-Self-Supervised-Speech-Models-for-Downstream-Speech-Recognition-Tasks-“In-the-Wild’’" class="headerlink" title="On the Impact of Quantization and Pruning of Self-Supervised Speech Models for Downstream Speech Recognition Tasks “In-the-Wild’’"></a>On the Impact of Quantization and Pruning of Self-Supervised Speech Models for Downstream Speech Recognition Tasks “In-the-Wild’’</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14462">http://arxiv.org/abs/2309.14462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Pimentel, Heitor Guimarães, Anderson R. Avila, Mehdi Rezagholizadeh, Tiago H. Falk</li>
<li>for: 本研究旨在探讨基于自动编程学习的语音识别系统在不同条件下的准确率，特别是在训练和测试条件不同时的情况下。</li>
<li>methods: 本研究使用了Parameter Quantization和Model Pruning两种模型压缩方法，以及robust wav2vec 2.0模型，对语音识别精度进行了分析。</li>
<li>results: 研究发现，在噪音、抑扬和噪音+抑扬等不同条件下，Parameter Quantization和Model Pruning两种方法都能够有效地提高语音识别精度。<details>
<summary>Abstract</summary>
Recent advances with self-supervised learning have allowed speech recognition systems to achieve state-of-the-art (SOTA) word error rates (WER) while requiring only a fraction of the labeled training data needed by its predecessors. Notwithstanding, while such models achieve SOTA performance in matched train/test conditions, their performance degrades substantially when tested in unseen conditions. To overcome this problem, strategies such as data augmentation and/or domain shift training have been explored. Available models, however, are still too large to be considered for edge speech applications on resource-constrained devices, thus model compression tools are needed. In this paper, we explore the effects that train/test mismatch conditions have on speech recognition accuracy based on compressed self-supervised speech models. In particular, we report on the effects that parameter quantization and model pruning have on speech recognition accuracy based on the so-called robust wav2vec 2.0 model under noisy, reverberant, and noise-plus-reverberation conditions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Investigation-of-Distribution-Alignment-in-Multi-Genre-Speaker-Recognition"><a href="#An-Investigation-of-Distribution-Alignment-in-Multi-Genre-Speaker-Recognition" class="headerlink" title="An Investigation of Distribution Alignment in Multi-Genre Speaker Recognition"></a>An Investigation of Distribution Alignment in Multi-Genre Speaker Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14158">http://arxiv.org/abs/2309.14158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Zhou, Junhui Chen, Namin Wang, Lantian Li, Dong Wang</li>
<li>for: 本研究旨在 investigate the performance of mainstream distribution alignment methods on multi-genre data, 以便更好地 Addressing the challenges of multi-genre speaker recognition.</li>
<li>methods: 本研究使用了多种主流分布Alignment方法，包括 Within-between distribution alignment (WBDA) 等。</li>
<li>results: 实验结果表明，WBDA方法在 CN-Celeb  dataset 中表现较好，但是 None of the investigated methods consistently improved performance in all test cases. 这表明尚未发展出一种全面的解决方案。<details>
<summary>Abstract</summary>
Multi-genre speaker recognition is becoming increasingly popular due to its ability to better represent the complexities of real-world applications. However, a major challenge is the significant shift in the distribution of speaker vectors across different genres. While distribution alignment is a common approach to address this challenge, previous studies have mainly focused on aligning a source domain with a target domain, and the performance of multi-genre data is unknown.   This paper presents a comprehensive study of mainstream distribution alignment methods on multi-genre data, where multiple distributions need to be aligned. We analyze various methods both qualitatively and quantitatively. Our experiments on the CN-Celeb dataset show that within-between distribution alignment (WBDA) performs relatively better. However, we also found that none of the investigated methods consistently improved performance in all test cases. This suggests that solely aligning the distributions of speaker vectors may not fully address the challenges posed by multi-genre speaker recognition. Further investigation is necessary to develop a more comprehensive solution.
</details>
<details>
<summary>摘要</summary>
多样化 Speaker 认知正在不断受欢迎，主要是因为它能更好地反映实际应用中的复杂性。然而，一个主要挑战是多个频率域之间的分布变化。过往的研究主要集中在将源频率域与目标频率域进行分布对接，并未探讨多个频率域数据的性能。本文对多个频率域数据进行了全面的分布对接方法研究。我们分析了各种方法，包括内在between分布对接（WBDA）等。我们的实验结果表明，WBDA在CN-Celeb 数据集上表现较好。然而，我们还发现，不同的测试情况下，不同的方法的性能表现不一致。这表明，仅仅通过对 speaker  vector 的分布进行对接，并不能彻底解决多个频率域 Speaker 认知中的挑战。需要进一步的研究，以开发更加全面的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Multi-Domain-Adaptation-by-Self-Supervised-Learning-for-Speaker-Verification"><a href="#Multi-Domain-Adaptation-by-Self-Supervised-Learning-for-Speaker-Verification" class="headerlink" title="Multi-Domain Adaptation by Self-Supervised Learning for Speaker Verification"></a>Multi-Domain Adaptation by Self-Supervised Learning for Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14149">http://arxiv.org/abs/2309.14149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wan Lin, Lantian Li, Dong Wang</li>
<li>for: addressing the domain-mismatch challenge in speaker recognition models</li>
<li>methods: self-supervised learning method with three strategies (in-domain negative sampling, MoCo-like memory bank scheme, and CORAL-like distribution alignment)</li>
<li>results: outperforms the basic self-supervised adaptation method in nearly all in-domain tests and cross-domain tests, demonstrating the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
In real-world applications, speaker recognition models often face various domain-mismatch challenges, leading to a significant drop in performance. Although numerous domain adaptation techniques have been developed to address this issue, almost all present methods focus on a simple configuration where the model is trained in one domain and deployed in another. However, real-world environments are often complex and may contain multiple domains, making the methods designed for one-to-one adaptation suboptimal. In our paper, we propose a self-supervised learning method to tackle this multi-domain adaptation problem. Building upon the basic self-supervised adaptation algorithm, we designed three strategies to make it suitable for multi-domain adaptation: an in-domain negative sampling strategy, a MoCo-like memory bank scheme, and a CORAL-like distribution alignment. We conducted experiments using VoxCeleb2 as the source domain dataset and CN-Celeb1 as the target multi-domain dataset. Our results demonstrate that our method clearly outperforms the basic self-supervised adaptation method, which simply treats the data of CN-Celeb1 as a single domain. Importantly, the improvement is consistent in nearly all in-domain tests and cross-domain tests, demonstrating the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
在实际应用中，语音识别模型经常面临不同领域的挑战，导致其性能下降。虽然已有许多领域适应技术的研发，但大多数方法都是基于单个领域的训练和部署。然而，实际环境往往复杂，可能包含多个领域，这些方法在一对一适应下表现不佳。在我们的论文中，我们提出了一种基于自助学习的多领域适应方法。我们在基本的自助适应算法之上设计了三种策略，使其适应多领域适应：在领域内的负样本采样策略、MoCo-like储存银行方案以及CORAL-like分布对齐。我们使用VoxCeleb2作为源领域数据集，CN-Celeb1作为目标多领域数据集，并进行了实验。我们的结果表明，我们的方法在比较多个领域的测试中均有显著提高，而且这种改进是在几乎所有的领域测试和跨领域测试中均有，这说明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Speaker-anonymization-using-neural-audio-codec-language-models"><a href="#Speaker-anonymization-using-neural-audio-codec-language-models" class="headerlink" title="Speaker anonymization using neural audio codec language models"></a>Speaker anonymization using neural audio codec language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14129">http://arxiv.org/abs/2309.14129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Panariello, Francesco Nespoli, Massimiliano Todisco, Nicholas Evans</li>
<li>for: 隐藏发音者的匿名（Speaker Anonymization）</li>
<li>methods: 使用神经网络编码器（NAC）和语言模型来生成高质量的匿名语音（Synthetic Speech），并使用量化码来瓶颈发音者相关的信息</li>
<li>results: 通过应用voice Privacy Challenge 2022的评价框架，示出NAC语言模型可以实现高质量的匿名语音生成，并且能够有效瓶颈发音者相关的信息<details>
<summary>Abstract</summary>
The vast majority of approaches to speaker anonymization involve the extraction of fundamental frequency estimates, linguistic features and a speaker embedding which is perturbed to obfuscate the speaker identity before an anonymized speech waveform is resynthesized using a vocoder. Recent work has shown that x-vector transformations are difficult to control consistently: other sources of speaker information contained within fundamental frequency and linguistic features are re-entangled upon vocoding, meaning that anonymized speech signals still contain speaker information. We propose an approach based upon neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes, which are known to effectively bottleneck speaker-related information: we demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose an approach based on neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes that effectively bottleneck speaker-related information. We demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.Here's the text in Simplified Chinese:大多数speaker anonymization方法都包括提取基本频率估计、语言特征和一个扰乱后的speaker嵌入，然后使用vocoder重新synthesize一个匿名的语音波形。然而，最近的研究表明，x-vector变换很难控制一致地：在vocoding后，其他speaker信息包含在基本频率和语言特征中被重新杂化，导致匿名的语音信号仍然含有speaker信息。在这篇论文中，我们提出一种基于Neural Audio Codecs（NAC）的方法，NACs是在语言模型 комбиined with高质量的Synthetic Speech生成。NACs使用归一化的编码，这些编码可以有效地瓶颈speaker相关的信息。我们通过应用Voice Privacy Challenge 2022的评估框架，示出了基于NAC语言模型的speaker匿名系统的潜在能力。
</details></li>
</ul>
<hr>
<h2 id="Haha-Pod-An-Attempt-for-Laughter-based-Non-Verbal-Speaker-Verification"><a href="#Haha-Pod-An-Attempt-for-Laughter-based-Non-Verbal-Speaker-Verification" class="headerlink" title="Haha-Pod: An Attempt for Laughter-based Non-Verbal Speaker Verification"></a>Haha-Pod: An Attempt for Laughter-based Non-Verbal Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14109">http://arxiv.org/abs/2309.14109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nevermorelin/hahapod">https://github.com/nevermorelin/hahapod</a></li>
<li>paper_authors: Yuke Lin, Xiaoyi Qin, Ning Jiang, Guoqing Zhao, Ming Li</li>
<li>for:  explore speaker verification based on non-verbal vocalization, specifically laughter</li>
<li>methods:  Two-Stage Teacher-Student (2S-TS) framework to minimize the within-speaker embedding distance between verbal and non-verbal signals</li>
<li>results:  significant improvement in S2L-Eval test set performance with only minor degradation on VoxCeleb1 test set.Here’s the full summary in Simplified Chinese:</li>
<li>for: 这个研究探讨了基于非言语 vocalization 的 speaker verification，具体是 laughter。</li>
<li>methods: 这个研究提出了 Two-Stage Teacher-Student (2S-TS) 框架，以实现非言语 vocalization 和言语信号之间的内部距离最小化。</li>
<li>results: 实验结果显示，这个方法可以对 S2L-Eval 试验集的表现有所提高，仅受 VoxCeleb1 试验集的轻微下降影响。I hope that helps!<details>
<summary>Abstract</summary>
It is widely acknowledged that discriminative representation for speaker verification can be extracted from verbal speech. However, how much speaker information that non-verbal vocalization carries is still a puzzle. This paper explores speaker verification based on the most ubiquitous form of non-verbal voice, laughter. First, we use a semi-automatic pipeline to collect a new Haha-Pod dataset from open-source podcast media. The dataset contains over 240 speakers' laughter clips with corresponding high-quality verbal speech. Second, we propose a Two-Stage Teacher-Student (2S-TS) framework to minimize the within-speaker embedding distance between verbal and non-verbal (laughter) signals. Considering Haha-Pod as a test set, two trials (S2L-Eval) are designed to verify the speaker's identity through laugh sounds. Experimental results demonstrate that our method can significantly improve the performance of the S2L-Eval test set with only a minor degradation on the VoxCeleb1 test set. The resources for the Haha-Pod dataset can be found at https://github.com/nevermoreLin/HahaPod.
</details>
<details>
<summary>摘要</summary>
广泛认可的观点是，演说中的特征表达可以提取到说话中的声音中。然而，非语言声音中带有多少说话者信息仍然是一个谜。这篇论文探讨基于最普遍的非语言声音——笑声的说话者验证。我们首先使用半自动化管道收集了一个新的 Haha-Pod 数据集，该数据集包含了240个说话者的笑声clip，其中每个clip都有高质量的语音。然后，我们提出了一个 Two-Stage Teacher-Student (2S-TS) 框架，以减少语音和笑声信号之间的在说话者 embedding 距离。对于 Haha-Pod 数据集，我们设计了两次（S2L-Eval）测试来验证说话者的身份。实验结果表明，我们的方法可以显著提高 S2L-Eval 测试集的性能，只有微量地降低 VoxCeleb1 测试集的性能。Haha-Pod 数据集的资源可以在 GitHub 上找到：https://github.com/nevermoreLin/HahaPod。
</details></li>
</ul>
<hr>
<h2 id="VoiceLens-Controllable-Speaker-Generation-and-Editing-with-Flow"><a href="#VoiceLens-Controllable-Speaker-Generation-and-Editing-with-Flow" class="headerlink" title="VoiceLens: Controllable Speaker Generation and Editing with Flow"></a>VoiceLens: Controllable Speaker Generation and Editing with Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14094">http://arxiv.org/abs/2309.14094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Shi, Ming Li</li>
<li>for: 这篇论文目的是为多个说话人的语音合成和voice转换系统提供一种 semi-supervised flow-based 方法，以模型说话人的嵌入vector distribution，并在不同的条件下进行多个说话人的生成和编辑。</li>
<li>methods: 该论文提出了一种名为 VoiceLens 的方法，它将说话人嵌入vector 映射到独立的特征和差异信息中。该方法允许在已有的 TTS 模型基础上生成新的说话人voice，并可以meaningfully 编辑已知的说话人的特征。</li>
<li>results: 该论文表明，VoiceLens 在不同的条件下 display 了类似于 Tacospawn 的无条件生成能力，同时具有更高的控制性和灵活性。此外，使用 VoiceLens 模型可以在不需要重新训练 TTS 模型的情况下，将已知的噪音说话人的嵌入vector 编辑，并生成 cleaner 的语音。<details>
<summary>Abstract</summary>
Currently, many multi-speaker speech synthesis and voice conversion systems address speaker variations with an embedding vector. Modeling it directly allows new voices outside of training data to be synthesized. GMM based approaches such as Tacospawn are favored in literature for this generation task, but there are still some limitations when difficult conditionings are involved. In this paper, we propose VoiceLens, a semi-supervised flow-based approach, to model speaker embedding distributions for multi-conditional speaker generation. VoiceLens maps speaker embeddings into a combination of independent attributes and residual information. It allows new voices associated with certain attributes to be \textit{generated} for existing TTS models, and attributes of known voices to be meaningfully \textit{edited}. We show in this paper, VoiceLens displays an unconditional generation capacity that is similar to Tacospawn while obtaining higher controllability and flexibility when used in a conditional manner. In addition, we show synthesizing less noisy speech from known noisy speakers without re-training the TTS model is possible via solely editing their embeddings with a SNR conditioned VoiceLens model. Demos are available at sos1sos2sixteen.github.io/voicelens.
</details>
<details>
<summary>摘要</summary>
当前，许多多 speaker speech synthesis 和voice conversion系统使用 embedding vector来处理 speaker variations。直接模型它们允许在训练数据外部生成新的voice。文献中，GMM基于的approaches such as Tacospawn 是常见的 Generation Task 方法，但是在具有困难的conditioning时还存在一些限制。在这篇论文中，我们提出了 VoiceLens，一种半supervised flow-based方法，用于模型 speaker embedding Distributions  для多 conditional speaker Generation。VoiceLens将 speaker embeddings映射到独立的特征和差异信息中。它允许基于已有 TTS 模型的新音频 associates with certain attributes 被生成，并且可以 meaningfully edit 已知音频的特征。我们在这篇论文中表明，VoiceLens 在不conditional 的情况下 display 类似于 Tacospawn 的无条件生成能力，同时在具有条件的情况下具有更高的控制性和灵活性。此外，我们还证明可以通过只编辑 embedding 来从已知噪音speakers中生成更清晰的speech，无需重新训练 TTS 模型。示例可以在 sos1sos2sixteen.github.io/voicelens 上找到。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Accent-Adaptation-Through-Masked-Language-Model-Correction-Of-Discrete-Self-Supervised-Speech-Units"><a href="#Unsupervised-Accent-Adaptation-Through-Masked-Language-Model-Correction-Of-Discrete-Self-Supervised-Speech-Units" class="headerlink" title="Unsupervised Accent Adaptation Through Masked Language Model Correction Of Discrete Self-Supervised Speech Units"></a>Unsupervised Accent Adaptation Through Masked Language Model Correction Of Discrete Self-Supervised Speech Units</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13994">http://arxiv.org/abs/2309.13994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakob Poncelet, Hugo Van hamme</li>
<li>for: 改善预训练语音模型对不同口音和异常语音的敏感性</li>
<li>methods: 使用遮盖语言模型和小口音适应器块进行不supervised调整</li>
<li>results: 提高 HuBERT Large 模型在下游口音识别任务中的性能，无需监督<details>
<summary>Abstract</summary>
Self-supervised pre-trained speech models have strongly improved speech recognition, yet they are still sensitive to domain shifts and accented or atypical speech. Many of these models rely on quantisation or clustering to learn discrete acoustic units. We propose to correct the discovered discrete units for accented speech back to a standard pronunciation in an unsupervised manner. A masked language model is trained on discrete units from a standard accent and iteratively corrects an accented token sequence by masking unexpected cluster sequences and predicting their common variant. Small accent adapter blocks are inserted in the pre-trained model and fine-tuned by predicting the corrected clusters, which leads to an increased robustness of the pre-trained model towards a target accent, and this without supervision. We are able to improve a state-of-the-art HuBERT Large model on a downstream accented speech recognition task by altering the training regime with the proposed method.
</details>
<details>
<summary>摘要</summary>
自适应预训练音频模型已经强化了语音识别，但它们仍然敏感于频谱转移和不同口音或特殊语音。许多这些模型利用量化或归一化学习独立的声学单元。我们提议将捕捉到的特殊单元 Corrected 到标准发音的方式，以便在无监督的情况下进行改进。我们使用遮盖语言模型，并在预训练模型中插入小口音适应块，进行无监督的改进，以提高预训练模型对目标口音的抗频谱性能。我们通过修改训练方法，使用我们的方法来改进一个state-of-the-art HuBERT Large模型，并在下游受损 speech recognition 任务中获得了改进。
</details></li>
</ul>
<hr>
<h2 id="Real-Time-Emergency-Vehicle-Detection-using-Mel-Spectrograms-and-Regular-Expressions"><a href="#Real-Time-Emergency-Vehicle-Detection-using-Mel-Spectrograms-and-Regular-Expressions" class="headerlink" title="Real-Time Emergency Vehicle Detection using Mel Spectrograms and Regular Expressions"></a>Real-Time Emergency Vehicle Detection using Mel Spectrograms and Regular Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13920">http://arxiv.org/abs/2309.13920</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Pacheco-Gonzalez, Raymundo Torres, Raul Chacon, Isidro Robledo</li>
<li>for:  detecting emergency vehicle sirens in real time</li>
<li>methods:  digital signal processing techniques and signal symbolization, compared to a deep neural network audio classifier</li>
<li>results:  the developed DSP algorithm presented a greater ability to discriminate between signal and noise, compared to the CNN model.Here’s the full translation of the paper’s abstract in Simplified Chinese:</li>
<li>for: 这个论文旨在实时探测紧急车辆 siren 声音。</li>
<li>methods: 该论文使用的方法包括数字信号处理技术和信号符号化，并与深度神经网络音频分类器进行比较。</li>
<li>results: 发展的 DSP 算法在听到信号和噪声之间的分辨率比 CNN 模型更高。<details>
<summary>Abstract</summary>
In emergency situations, the movement of vehicles through city streets can be problematic due to vehicular traffic. This paper presents a method for detecting emergency vehicle sirens in real time. To derive a siren Hi-Lo audio fingerprint it was necessary to apply digital signal processing techniques and signal symbolization, contrasting against a deep neural network audio classifier feeding 280 environmental sounds and 38 Hi-Lo sirens. In both methods, their precision was evaluated based on a confusion matrix and various metrics. The precision of the developed DSP algorithm presented a greater ability to discriminate between signal and noise, compared to the CNN model.
</details>
<details>
<summary>摘要</summary>
在紧急情况下，城市街道上的车辆运动可能会受到交通堵塞的影响。本文提出了一种实时探测紧急车辆 siren 的方法。为 derivation 紧急 siren Hi-Lo 音响指纹，需要应用数字信号处理技术和音标化，并与深度神经网络音频分类器相比较，该分类器接受了 280 个环境声和 38 个 Hi-Lo  siren。在两种方法中，它们的准确率被评估基于冲激矩阵和多种指标。发展的 DSP 算法表现出更高的能力来 отли奇 Between signal 和噪声，相比 CNN 模型。
</details></li>
</ul>
<hr>
<h2 id="Frame-wise-streaming-end-to-end-speaker-diarization-with-non-autoregressive-self-attention-based-attractors"><a href="#Frame-wise-streaming-end-to-end-speaker-diarization-with-non-autoregressive-self-attention-based-attractors" class="headerlink" title="Frame-wise streaming end-to-end speaker diarization with non-autoregressive self-attention-based attractors"></a>Frame-wise streaming end-to-end speaker diarization with non-autoregressive self-attention-based attractors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13916">http://arxiv.org/abs/2309.13916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/audio-westlakeu/fs-eend">https://github.com/audio-westlakeu/fs-eend</a></li>
<li>paper_authors: Di Liang, Nian Shao, Xiaofei Li</li>
<li>for: 这种方法用于实时语音分类和说话者识别</li>
<li>methods: 使用 causal speaker embedding encoder 和 online non-autoregressive self-attention-based attractor decoder，采用 look-ahead 机制以实时检测新的说话者并自适应更新说话者吸引器</li>
<li>results: 与最近提出的块 wise online方法相比，本方法实现了状态机器的分类和说话者识别结果，并且具有低的推理延迟和计算成本<details>
<summary>Abstract</summary>
This work proposes a frame-wise online/streaming end-to-end neural diarization (FS-EEND) method in a frame-in-frame-out fashion. To frame-wisely detect a flexible number of speakers and extract/update their corresponding attractors, we propose to leverage a causal speaker embedding encoder and an online non-autoregressive self-attention-based attractor decoder. A look-ahead mechanism is adopted to allow leveraging some future frames for effectively detecting new speakers in real time and adaptively updating speaker attractors. The proposed method processes the audio stream frame by frame, and has a low inference latency caused by the look-ahead frames. Experiments show that, compared with the recently proposed block-wise online methods, our method FS-EEND achieves state-of-the-art diarization results, with a low inference latency and computational cost.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种帧级在线/流动端到端神经 диари化（FS-EEND）方法，采用帧内帧外的方式进行检测。为了在帧级检测灵活数量的说话人并提取/更新其相应的吸引器，我们提议利用 causal 说话人嵌入编码器和在线非autoregressive自注意力基本吸引器解码器。采用了 looked-ahead 机制，以便利用未来帧来有效地检测新的说话人并动态更新说话人吸引器。提posed 方法按帧处理音频流，并且具有低的推理延迟和计算成本。实验表明，相比最近提出的块级在线方法，我们的方法FS-EEND可以 achieve state-of-the-art  диари化结果，同时具有低的推理延迟和计算成本。
</details></li>
</ul>
<hr>
<h2 id="HiGNN-TTS-Hierarchical-Prosody-Modeling-with-Graph-Neural-Networks-for-Expressive-Long-form-TTS"><a href="#HiGNN-TTS-Hierarchical-Prosody-Modeling-with-Graph-Neural-Networks-for-Expressive-Long-form-TTS" class="headerlink" title="HiGNN-TTS: Hierarchical Prosody Modeling with Graph Neural Networks for Expressive Long-form TTS"></a>HiGNN-TTS: Hierarchical Prosody Modeling with Graph Neural Networks for Expressive Long-form TTS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13907">http://arxiv.org/abs/2309.13907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dake Guo, Xinfa Zhu, Liumeng Xue, Tao Li, Yuanjun Lv, Yuepeng Jiang, Lei Xie</li>
<li>for: 提高短 фор式文本译 Speech 表现力</li>
<li>methods: 使用嵌入式Global Node和上下文注意力机制，以及层次supervision来增强GNNs的表达能力</li>
<li>results: 对象和主观评估都表明，HiGNN-TTS可以显著提高长形文本译Speech的自然性和表达力<details>
<summary>Abstract</summary>
Recent advances in text-to-speech, particularly those based on Graph Neural Networks (GNNs), have significantly improved the expressiveness of short-form synthetic speech. However, generating human-parity long-form speech with high dynamic prosodic variations is still challenging. To address this problem, we expand the capabilities of GNNs with a hierarchical prosody modeling approach, named HiGNN-TTS. Specifically, we add a virtual global node in the graph to strengthen the interconnection of word nodes and introduce a contextual attention mechanism to broaden the prosody modeling scope of GNNs from intra-sentence to inter-sentence. Additionally, we perform hierarchical supervision from acoustic prosody on each node of the graph to capture the prosodic variations with a high dynamic range. Ablation studies show the effectiveness of HiGNN-TTS in learning hierarchical prosody. Both objective and subjective evaluations demonstrate that HiGNN-TTS significantly improves the naturalness and expressiveness of long-form synthetic speech.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近期文本到语音技术的进步，特别是基于图像神经网络（GNNs），有效提高了短形文本合成语音的表达力。然而，生成人工合理的长形语音，尚存在高Dynamic Prosody变化的挑战。为解决这个问题，我们扩展了GNNs的能力，通过层次听音模型策略（HiGNN-TTS）。具体来说，我们在图像中添加虚拟全球节点，强化单词节点之间的连接，并引入Contextual Attention机制，以扩展GNNs的听音模型范围从内句到间句。此外，我们在每个图像节点上进行层次监督，从听音PROSODY级别进行多层次监督，以捕捉高Dynamic Prosody变化。ablation study表明HiGNN-TTS有效学习层次听音。对象和主观评价表明，HiGNN-TTS可以显著提高长形文本合成语音的自然性和表达力。
</details></li>
</ul>
<hr>
<h2 id="AutoPrep-An-Automatic-Preprocessing-Framework-for-In-the-Wild-Speech-Data"><a href="#AutoPrep-An-Automatic-Preprocessing-Framework-for-In-the-Wild-Speech-Data" class="headerlink" title="AutoPrep: An Automatic Preprocessing Framework for In-the-Wild Speech Data"></a>AutoPrep: An Automatic Preprocessing Framework for In-the-Wild Speech Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13905">http://arxiv.org/abs/2309.13905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tomasJwYU/AutoPrepDemo">https://github.com/tomasJwYU/AutoPrepDemo</a></li>
<li>paper_authors: Jianwei Yu, Hangting Chen, Yanyao Bian, Xiang Li, Yi Luo, Jinchuan Tian, Mengyang Liu, Jiayi Jiang, Shuai Wang</li>
<li>for: 提高听说技术领域中各种大规模语音数据的使用效率</li>
<li>methods: 提出了一种自动化听说数据预处理框架AutoPrep，包括声音提升、听说段化、speaker clustering、目标听说提取、质量筛选和自动听说识别</li>
<li>results: 实验表明，提出的AutoPrep框架可以生成与多个开源TTS数据集相似的DNMS和PDNMS分数，并且可以实现0.68的在域内 speaker相似性<details>
<summary>Abstract</summary>
Recently, the utilization of extensive open-sourced text data has significantly advanced the performance of text-based large language models (LLMs). However, the use of in-the-wild large-scale speech data in the speech technology community remains constrained. One reason for this limitation is that a considerable amount of the publicly available speech data is compromised by background noise, speech overlapping, lack of speech segmentation information, missing speaker labels, and incomplete transcriptions, which can largely hinder their usefulness. On the other hand, human annotation of speech data is both time-consuming and costly. To address this issue, we introduce an automatic in-the-wild speech data preprocessing framework (AutoPrep) in this paper, which is designed to enhance speech quality, generate speaker labels, and produce transcriptions automatically. The proposed AutoPrep framework comprises six components: speech enhancement, speech segmentation, speaker clustering, target speech extraction, quality filtering and automatic speech recognition. Experiments conducted on the open-sourced WenetSpeech and our self-collected AutoPrepWild corpora demonstrate that the proposed AutoPrep framework can generate preprocessed data with similar DNSMOS and PDNSMOS scores compared to several open-sourced TTS datasets. The corresponding TTS system can achieve up to 0.68 in-domain speaker similarity.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近，通过大量开源的文本数据的使用，文本基本语言模型（LLM）的性能得到了显著提高。然而，对于语音技术社区中的大规模语音数据来说，使用尚未得到有效利用。一个原因是公共可用的语音数据中很多受到背景噪音、语音重叠、语音分割信息缺失、缺失说话人标签和不完整的转录等限制，这些限制可以很大地阻碍其使用。而人工标注语音数据则是时间consuming和costly。为解决这个问题，我们在这篇论文中介绍了一种自动化对话语音数据预处理框架（AutoPrep），用于提高语音质量、生成说话人标签和生成转录。AutoPrep框架包括6个组件：语音增强、语音分割、说话人团 clustering、目标语音提取、质量筛选和自动语音识别。在open-sourced WenetSpeech和我们自己收集的AutoPrepWild corpora上进行的实验表明，提posed AutoPrep框架可以生成与开源 TTS 数据集相似的 DNSMOS 和 PDNSMOS 分数，并且可以达到0.68 的域内说话人相似性。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Step-Approach-for-Narrowband-Source-Localization-in-Reverberant-Rooms"><a href="#A-Two-Step-Approach-for-Narrowband-Source-Localization-in-Reverberant-Rooms" class="headerlink" title="A Two-Step Approach for Narrowband Source Localization in Reverberant Rooms"></a>A Two-Step Approach for Narrowband Source Localization in Reverberant Rooms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13819">http://arxiv.org/abs/2309.13819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Ting Lai, Lachlan Birnie, Thushara Abhayapala, Amy Bastine, Shaoheng Xu, Prasanga Samarasinghe</li>
<li>for: 本研究旨在提出一种基于两步方法的窄带源localization算法，用于听觉环境中的音源定位。</li>
<li>methods: 该方法首先使用Iteratively Reweighted Least Squares（IRLS）模型干扰音场的同质分量，然后使用Orthogonal Matching Pursuit（OMP）模型干扰分量为点源分布的稀疏表示。</li>
<li>results: 实验结果表明，该方法可以减少测量量而提高定位精度，特别是在听觉环境中。此外，该方法可以不需要先知道房间边界条件和室内geometry，因此可以适用于不同的室内环境。<details>
<summary>Abstract</summary>
This paper presents a two-step approach for narrowband source localization within reverberant rooms. The first step involves dereverberation by modeling the homogeneous component of the sound field by an equivalent decomposition of planewaves using Iteratively Reweighted Least Squares (IRLS), while the second step focuses on source localization by modeling the dereverberated component as a sparse representation of point-source distribution using Orthogonal Matching Pursuit (OMP). The proposed method enhances localization accuracy with fewer measurements, particularly in environments with strong reverberation. A numerical simulation in a conference room scenario, using a uniform microphone array affixed to the wall, demonstrates real-world feasibility. Notably, the proposed method and microphone placement effectively localize sound sources within the 2D-horizontal plane without requiring prior knowledge of boundary conditions and room geometry, making it versatile for application in different room types.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/cs.SD_2023_09_25/" data-id="clpztdno80108es88e8tfdn7h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/cs.CV_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T13:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/cs.CV_2023_09_25/">cs.CV - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MEMO-Dataset-and-Methods-for-Robust-Multimodal-Retinal-Image-Registration-with-Large-or-Small-Vessel-Density-Differences"><a href="#MEMO-Dataset-and-Methods-for-Robust-Multimodal-Retinal-Image-Registration-with-Large-or-Small-Vessel-Density-Differences" class="headerlink" title="MEMO: Dataset and Methods for Robust Multimodal Retinal Image Registration with Large or Small Vessel Density Differences"></a>MEMO: Dataset and Methods for Robust Multimodal Retinal Image Registration with Large or Small Vessel Density Differences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14550">http://arxiv.org/abs/2309.14550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiao-Yi Wang, Faranguisse Kakhi Sadrieh, Yi-Ting Shen, Shih-En Chen, Sarah Kim, Victoria Chen, Achyut Raghavendra, Dongyi Wang, Osamah Saeedi, Yang Tao</li>
<li>for: 这个论文的目的是提出一种基于多Modalities的血液流量测量方法，以便早期诊断和治疗 ocular 疾病。</li>
<li>methods: 这个方法使用了 EMA 和 OCTA 两种多Modalities，并提出了一种基于 segmentation 的深度学习框架 (VDD-Reg) 和一种新的评价指标 (MSD)，以Address 多Modalities 中血液管道的不同而导致的注射挑战。</li>
<li>results: 在 CF-FA 数据集和 MEMO 数据集上，VDD-Reg 表现出了较好的性能，并且只需要三个注解的血液管道分割图来维持其精度。<details>
<summary>Abstract</summary>
The measurement of retinal blood flow (RBF) in capillaries can provide a powerful biomarker for the early diagnosis and treatment of ocular diseases. However, no single modality can determine capillary flowrates with high precision. Combining erythrocyte-mediated angiography (EMA) with optical coherence tomography angiography (OCTA) has the potential to achieve this goal, as EMA can measure the absolute 2D RBF of retinal microvasculature and OCTA can provide the 3D structural images of capillaries. However, multimodal retinal image registration between these two modalities remains largely unexplored. To fill this gap, we establish MEMO, the first public multimodal EMA and OCTA retinal image dataset. A unique challenge in multimodal retinal image registration between these modalities is the relatively large difference in vessel density (VD). To address this challenge, we propose a segmentation-based deep-learning framework (VDD-Reg) and a new evaluation metric (MSD), which provide robust results despite differences in vessel density. VDD-Reg consists of a vessel segmentation module and a registration module. To train the vessel segmentation module, we further designed a two-stage semi-supervised learning framework (LVD-Seg) combining supervised and unsupervised losses. We demonstrate that VDD-Reg outperforms baseline methods quantitatively and qualitatively for cases of both small VD differences (using the CF-FA dataset) and large VD differences (using our MEMO dataset). Moreover, VDD-Reg requires as few as three annotated vessel segmentation masks to maintain its accuracy, demonstrating its feasibility.
</details>
<details>
<summary>摘要</summary>
retinal blood flow (RBF) 的测量可以提供一个强大的生物标志物，用于早期诊断和治疗 ocular diseases。然而，没有一种单一的模式可以准确地确定 capillary flowrates。 combing erythrocyte-mediated angiography (EMA) 与 optical coherence tomography angiography (OCTA) 可以实现这个目标，因为 EMA 可以测量 retinal microvasculature 的绝对 2D RBF，而 OCTA 可以提供 capillaries 的 3D 结构图像。然而，多modal retinal image registration between these two modalities 仍然存在很大的知识 gap。 To fill this gap, we establish MEMO, the first public multimodal EMA and OCTA retinal image dataset.一个Unique challenge in multimodal retinal image registration between these modalities 是 vessel density (VD) 的相对较大的差异。 To address this challenge, we propose a segmentation-based deep-learning framework (VDD-Reg) 和 a new evaluation metric (MSD), which provide robust results despite differences in vessel density. VDD-Reg consists of a vessel segmentation module and a registration module. To train the vessel segmentation module, we further designed a two-stage semi-supervised learning framework (LVD-Seg) combining supervised and unsupervised losses. We demonstrate that VDD-Reg outperforms baseline methods quantitatively and qualitatively for cases of both small VD differences (using the CF-FA dataset) and large VD differences (using our MEMO dataset). Moreover, VDD-Reg requires as few as three annotated vessel segmentation masks to maintain its accuracy, demonstrating its feasibility.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Scene-Graph-Representation-for-Surgical-Video"><a href="#Dynamic-Scene-Graph-Representation-for-Surgical-Video" class="headerlink" title="Dynamic Scene Graph Representation for Surgical Video"></a>Dynamic Scene Graph Representation for Surgical Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14538">http://arxiv.org/abs/2309.14538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Holm, Ghazal Ghazaei, Tobias Czempiel, Ege Özsoy, Stefan Saur, Nassir Navab</li>
<li>for:  This paper aims to improve the automated understanding of surgical workflows in videos captured from microscopic or endoscopic imaging devices.</li>
<li>methods: The paper proposes using scene graphs as a more holistic and semantically meaningful way to represent surgical videos, and leverages graph convolutional networks (GCNs) to tackle surgical downstream tasks such as workflow recognition.</li>
<li>results: The paper demonstrates the benefits of surgical scene graphs in terms of explainability and robustness of model decisions, and shows competitive performance in surgical workflow recognition tasks.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是提高微scopic或endoscopic imaging设备上捕捉的手术视频自动理解的方法。</li>
<li>methods: 论文提出使用场景图表示手术视频，并利用图 convolutional neural networks (GCNs) 解决手术下沟通任务，如手术 workflow 认知。</li>
<li>results: 论文表明场景图在解释和模型决策的Robustness方面具有优势，并在手术 workflow 认知任务中显示竞争性表现。<details>
<summary>Abstract</summary>
Surgical videos captured from microscopic or endoscopic imaging devices are rich but complex sources of information, depicting different tools and anatomical structures utilized during an extended amount of time. Despite containing crucial workflow information and being commonly recorded in many procedures, usage of surgical videos for automated surgical workflow understanding is still limited.   In this work, we exploit scene graphs as a more holistic, semantically meaningful and human-readable way to represent surgical videos while encoding all anatomical structures, tools, and their interactions. To properly evaluate the impact of our solutions, we create a scene graph dataset from semantic segmentations from the CaDIS and CATARACTS datasets. We demonstrate that scene graphs can be leveraged through the use of graph convolutional networks (GCNs) to tackle surgical downstream tasks such as surgical workflow recognition with competitive performance. Moreover, we demonstrate the benefits of surgical scene graphs regarding the explainability and robustness of model decisions, which are crucial in the clinical setting.
</details>
<details>
<summary>摘要</summary>
手术录影幕 capture from microscopic or endoscopic imaging devices 是丰富且复杂的信息源，显示了不同的工具和生物结构在延时间进行了多种程度的交互。尽管这些录影幕在许多程序中很常见，但是用于自动推理手术 workflow 的使用仍然受限。在这个工作中，我们利用Scene graph来表示手术录影幕，并将所有生物结构和工具都编码在内。为了评估我们的解决方案的影响，我们创建了Scene graph dataset，并使用Graph Convolutional Networks (GCNs)来利用Scene graphs来解决手术下游任务，例如手术 workflow 识别，获得了竞争性的表现。此外，我们还证明了Scene graphs 在解释和Robustness 方面的利陵，这些是在临床设定中非常重要的。
</details></li>
</ul>
<hr>
<h2 id="Pixel-Grounded-Prototypical-Part-Networks"><a href="#Pixel-Grounded-Prototypical-Part-Networks" class="headerlink" title="Pixel-Grounded Prototypical Part Networks"></a>Pixel-Grounded Prototypical Part Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14531">http://arxiv.org/abs/2309.14531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachariah Carmichael, Suhas Lohit, Anoop Cherian, Michael Jones, Walter Scheirer</li>
<li>for: 这个论文的目的是提高protoPartNN的解释性，并解决过去的localization问题。</li>
<li>methods: 这篇论文使用了新的�ceptive field-based architectural constraint和principled pixel space mapping来实现meaningful localization，并提出了一种简化的分类头来提高解释性。</li>
<li>results: 作者们的方法PIXPNET可以 Quantifiably improve interpretability without sacrificing accuracy，并且是唯一一个真正地学习和localize到权重部分的protoPartNN。<details>
<summary>Abstract</summary>
Prototypical part neural networks (ProtoPartNNs), namely PROTOPNET and its derivatives, are an intrinsically interpretable approach to machine learning. Their prototype learning scheme enables intuitive explanations of the form, this (prototype) looks like that (testing image patch). But, does this actually look like that? In this work, we delve into why object part localization and associated heat maps in past work are misleading. Rather than localizing to object parts, existing ProtoPartNNs localize to the entire image, contrary to generated explanatory visualizations. We argue that detraction from these underlying issues is due to the alluring nature of visualizations and an over-reliance on intuition. To alleviate these issues, we devise new receptive field-based architectural constraints for meaningful localization and a principled pixel space mapping for ProtoPartNNs. To improve interpretability, we propose additional architectural improvements, including a simplified classification head. We also make additional corrections to PROTOPNET and its derivatives, such as the use of a validation set, rather than a test set, to evaluate generalization during training. Our approach, PIXPNET (Pixel-grounded Prototypical part Network), is the only ProtoPartNN that truly learns and localizes to prototypical object parts. We demonstrate that PIXPNET achieves quantifiably improved interpretability without sacrificing accuracy.
</details>
<details>
<summary>摘要</summary>
归纳部神经网络（ProtoPartNNs）是一种内在可解释的机器学习方法。它的原型学习方案允许直观的解释，例如：这个原型看起来像那个测试图像的 patch。然而，这实际上是否看起来像那个？在这项工作中，我们探究过去的对象部分Localization和相关的热图是误导的。现有的ProtoPartNNs并不是localize到对象部分，而是localize到整个图像，与生成的解释性视觉化不符。我们认为这些问题的抽象是由于视觉化的吸引力和过于依赖于直观的INTUITION。为了解决这些问题，我们设计了新的接受场景基于的建筑限制，以及一个原则正确的像素空间映射。此外，我们还提出了更多的建筑改进，包括简化的分类头。我们还对PROTOPNET和其 Derivatives进行了修正，例如使用验证集而不是测试集来评估在训练过程中的普适性。我们的方法PIXPNET（像素基于的原型部分网络）是唯一一个真正地学习和localize到prototype object part。我们示出PIXPNET可以提供量化提高的可解释性而不损失准确性。
</details></li>
</ul>
<hr>
<h2 id="UniBEV-Multi-modal-3D-Object-Detection-with-Uniform-BEV-Encoders-for-Robustness-against-Missing-Sensor-Modalities"><a href="#UniBEV-Multi-modal-3D-Object-Detection-with-Uniform-BEV-Encoders-for-Robustness-against-Missing-Sensor-Modalities" class="headerlink" title="UniBEV: Multi-modal 3D Object Detection with Uniform BEV Encoders for Robustness against Missing Sensor Modalities"></a>UniBEV: Multi-modal 3D Object Detection with Uniform BEV Encoders for Robustness against Missing Sensor Modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14516">http://arxiv.org/abs/2309.14516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiming Wang, Holger Caesar, Liangliang Nan, Julian F. P. Kooij<br>for:这个论文的目的是提高自动驾驶中多感器对象检测模型的稳定性，特别是在感器输入缺失（modalities missing）的情况下。methods:这个论文提出了一种名为UniBEV的端到端多模态3D对象检测框架，可以在LiDAR和摄像头输入下运行，而无需 retraining。UniBEV使用了 Bird’s Eye View（BEV）特征地图来确保检测器的输入组合能够处理不同的输入。这种方法与之前的BEV多模态检测方法不同，所有的感器模式都采用了一种统一的方法来从原始感器坐标系统中采样特征到BEV特征。results:在nuScenes上对所有感器输入组合进行比较，UniBEV得到了52.5%的mAP平均值，与基线值（43.5%的mAP平均值）和MetaBEV（48.7%的mAP平均值）相比有显著提高。一个ablation研究表明，通过权重平均 fusioneather than regular concatenation，以及在每个模式的BEV编码器之间共享查询，可以提高对稳定性的依赖。<details>
<summary>Abstract</summary>
Multi-sensor object detection is an active research topic in automated driving, but the robustness of such detection models against missing sensor input (modality missing), e.g., due to a sudden sensor failure, is a critical problem which remains under-studied. In this work, we propose UniBEV, an end-to-end multi-modal 3D object detection framework designed for robustness against missing modalities: UniBEV can operate on LiDAR plus camera input, but also on LiDAR-only or camera-only input without retraining. To facilitate its detector head to handle different input combinations, UniBEV aims to create well-aligned Bird's Eye View (BEV) feature maps from each available modality. Unlike prior BEV-based multi-modal detection methods, all sensor modalities follow a uniform approach to resample features from the native sensor coordinate systems to the BEV features. We furthermore investigate the robustness of various fusion strategies w.r.t. missing modalities: the commonly used feature concatenation, but also channel-wise averaging, and a generalization to weighted averaging termed Channel Normalized Weights. To validate its effectiveness, we compare UniBEV to state-of-the-art BEVFusion and MetaBEV on nuScenes over all sensor input combinations. In this setting, UniBEV achieves $52.5 \%$ mAP on average over all input combinations, significantly improving over the baselines ($43.5 \%$ mAP on average for BEVFusion, $48.7 \%$ mAP on average for MetaBEV). An ablation study shows the robustness benefits of fusing by weighted averaging over regular concatenation, and of sharing queries between the BEV encoders of each modality. Our code will be released upon paper acceptance.
</details>
<details>
<summary>摘要</summary>
多感器对象检测是自动驾驶领域的活跃研究话题，但对感器输入缺失（例如突然的感器故障）的Robustness仍然是一个尚未得到充分研究的问题。在这种情况下，我们提出了UniBEV，一个综合多Modal 3D对象检测框架，旨在提高对感器输入缺失的Robustness。UniBEV可以使用LiDAR和摄像头输入，同时也可以使用LiDAR-only或摄像头只输入，无需重新训练。为使其检测头处理不同的输入组合，UniBEV стре望在每个可用感器模式下创建匹配的Bird's Eye View（BEV）特征地图。与先前的BEV基于多Modal detection方法不同，所有感器模式都采用了一致的方式，将native感器坐标系统中的特征atures映射到BEV特征地图。我们还进行了不同模式之间的混合策略的研究，包括常见的特征 concatenation、梯度平均值和通过 Channel Normalized Weights 扩展。为证明其效果，我们与状态体系的BEVFusion和MetaBEV进行比较，在nuScenes上对所有感器输入组合进行评估。在这种设定下，UniBEV achieved $52.5\%$ mAP的平均值，significantly improving over the baselines ($43.5\%$ mAP on average for BEVFusion, $48.7\%$ mAP on average for MetaBEV).一个ablation study表明，通过权重平均混合而不是常见的特征 concatenation，以及在每个模式的BEVEncoder中共享查询，具有Robustness的优点。我们将在纸Acceptance时发布代码。
</details></li>
</ul>
<hr>
<h2 id="Accurate-and-Interactive-Visual-Inertial-Sensor-Calibration-with-Next-Best-View-and-Next-Best-Trajectory-Suggestion"><a href="#Accurate-and-Interactive-Visual-Inertial-Sensor-Calibration-with-Next-Best-View-and-Next-Best-Trajectory-Suggestion" class="headerlink" title="Accurate and Interactive Visual-Inertial Sensor Calibration with Next-Best-View and Next-Best-Trajectory Suggestion"></a>Accurate and Interactive Visual-Inertial Sensor Calibration with Next-Best-View and Next-Best-Trajectory Suggestion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14514">http://arxiv.org/abs/2309.14514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chutsu/yac">https://github.com/chutsu/yac</a></li>
<li>paper_authors: Christopher L. Choi, Binbin Xu, Stefan Leutenegger</li>
<li>for: 本研究旨在帮助非专家用户更好地使用视觉遥感（VI）传感器进行计算机视觉或状态估计任务，通过提供图形用户界面和信息理论来收集有用的准备数据，并提供下一个最佳视图和下一个最佳轨迹的建议。</li>
<li>methods: 本研究提出了一种新的VI准备管线，使用图形用户界面和信息理论来导引非专家用户收集有用的准备数据，并提供下一个最佳视图和下一个最佳轨迹的建议，以准备VI传感器的内参、外参和时间偏差。</li>
<li>results: 经过实验表明，我们的方法比现有技术更快、更准、更一致，并且可以与现有VI odometry和VI-SLAM方法结合使用，以获得更高精度的估计结果。<details>
<summary>Abstract</summary>
Visual-Inertial (VI) sensors are popular in robotics, self-driving vehicles, and augmented and virtual reality applications. In order to use them for any computer vision or state-estimation task, a good calibration is essential. However, collecting informative calibration data in order to render the calibration parameters observable is not trivial for a non-expert. In this work, we introduce a novel VI calibration pipeline that guides a non-expert with the use of a graphical user interface and information theory in collecting informative calibration data with Next-Best-View and Next-Best-Trajectory suggestions to calibrate the intrinsics, extrinsics, and temporal misalignment of a VI sensor. We show through experiments that our method is faster, more accurate, and more consistent than state-of-the-art alternatives. Specifically, we show how calibrations with our proposed method achieve higher accuracy estimation results when used by state-of-the-art VI Odometry as well as VI-SLAM approaches. The source code of our software can be found on: https://github.com/chutsu/yac.
</details>
<details>
<summary>摘要</summary>
Visual-Inertial（VI）传感器在 роботику、自动驾驶车和增强和虚拟现实应用中广泛使用。为了在计算机视觉或状态估计任务中使用它们，一个好的准备是必要的。然而，收集有用的准备数据以便计算准备参数的可见性并不是非专家的 trivial事。在这个工作中，我们介绍了一个新的 VI 准备管线，通过使用图形用户界面和信息理论来引导非专家收集有用的准备数据，并且提供 Next-Best-View 和 Next-Best-Trajectory 建议来准备 VI 传感器的内参、外参和时间偏移。我们通过实验表明，我们的方法比现有的状态之 искусственный风格更快、更准、更一致。 Specifically，我们表明使用我们提议的方法来准备准确性估计结果，与现有的 VI Odometry 以及 VI-SLAM 方法相比，具有更高的准确性。我们的软件源代码可以在：https://github.com/chutsu/yac 找到。
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-a-new-GeoAI-foundation-model-for-flood-inundation-mapping"><a href="#Assessment-of-a-new-GeoAI-foundation-model-for-flood-inundation-mapping" class="headerlink" title="Assessment of a new GeoAI foundation model for flood inundation mapping"></a>Assessment of a new GeoAI foundation model for flood inundation mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14500">http://arxiv.org/abs/2309.14500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenwen Li, Hyunho Lee, Sizhe Wang, Chia-Yu Hsu, Samantha T. Arundel</li>
<li>for: 这个研究旨在评估IBM-NASA的Prithvi模型在洪水淹没地图分析领域中的表现，以支持关键的地ospatial分析任务。</li>
<li>methods: 这篇论文使用了IBM-NASA的Prithvi模型，并与卷积神经网络和感知器transformer-based架构进行比较，以测试这些模型在洪水淹没地图分析任务中的对策精度。</li>
<li>results: 研究结果显示Prithvi模型在未见过的区域中进行洪水淹没地图分析任务时表现良好，并且在验证数据集和没有被模型视觉化的数据集上显示了良好的预测性和可读性。<details>
<summary>Abstract</summary>
Vision foundation models are a new frontier in Geospatial Artificial Intelligence (GeoAI), an interdisciplinary research area that applies and extends AI for geospatial problem solving and geographic knowledge discovery, because of their potential to enable powerful image analysis by learning and extracting important image features from vast amounts of geospatial data. This paper evaluates the performance of the first-of-its-kind geospatial foundation model, IBM-NASA's Prithvi, to support a crucial geospatial analysis task: flood inundation mapping. This model is compared with convolutional neural network and vision transformer-based architectures in terms of mapping accuracy for flooded areas. A benchmark dataset, Sen1Floods11, is used in the experiments, and the models' predictability, generalizability, and transferability are evaluated based on both a test dataset and a dataset that is completely unseen by the model. Results show the good transferability of the Prithvi model, highlighting its performance advantages in segmenting flooded areas in previously unseen regions. The findings also indicate areas for improvement for the Prithvi model in terms of adopting multi-scale representation learning, developing more end-to-end pipelines for high-level image analysis tasks, and offering more flexibility in terms of input data bands.
</details>
<details>
<summary>摘要</summary>
地球空间人工智能（GeoAI）是一个交叉学科研究领域，它应用和扩展人工智能来解决地球空间问题和地理知识发现。视频基础模型是GeoAI新领域，它们可以通过学习和提取重要的图像特征来实现强大的图像分析。本文评估了首次实现的地球空间基础模型——IBM-NASA的Prithvi，以支持重要的地球空间分析任务：洪水泛滥地图。这个模型与 convolutional neural network 和 vision transformer 基础结构相比，在泛滥区域的地图准确率方面进行了比较。使用 Sen1Floods11  benchmark 数据集进行实验，并根据测试数据集和完全新的数据集来评估模型的预测性、普适性和可转移性。结果显示 Prithvi 模型在未经见过的区域中 segments 泛滥区域的表现良好，表明其在新区域中的表现优异。发现也表明了 Prithvi 模型在采用多尺度表示学习、开发更多的端到端管道和提供更多的输入数据频谱等方面存在改进的空间。
</details></li>
</ul>
<hr>
<h2 id="Free-Bloom-Zero-Shot-Text-to-Video-Generator-with-LLM-Director-and-LDM-Animator"><a href="#Free-Bloom-Zero-Shot-Text-to-Video-Generator-with-LLM-Director-and-LDM-Animator" class="headerlink" title="Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator"></a>Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14494">http://arxiv.org/abs/2309.14494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soolab/free-bloom">https://github.com/soolab/free-bloom</a></li>
<li>paper_authors: Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, Sibei Yang</li>
<li>for: 本研究旨在实现无需视频数据和训练的情况下，生成具有Semantic coherence的高质量视频。</li>
<li>methods: 提议一种名为Free-Bloom的管道，利用大型自然语言模型（LLM）作为导演，生成Semantic coherence的提示序列，并使用预训练的潜在扩散模型（LDM）为动画师生成高效率帧。为保证时间和 identical coherence，提出了一些修改，包括共同噪声抽取、步态意识转移和双路 interpolate。</li>
<li>results: 无需任何视频数据和训练，Free-Bloom可以生成具有丰富semantic meaningful frame sequence的高质量视频，能够描绘复杂的场景。此外，Free-Bloom自然兼容LDMs-based extensions。<details>
<summary>Abstract</summary>
Text-to-video is a rapidly growing research area that aims to generate a semantic, identical, and temporal coherence sequence of frames that accurately align with the input text prompt. This study focuses on zero-shot text-to-video generation considering the data- and cost-efficient. To generate a semantic-coherent video, exhibiting a rich portrayal of temporal semantics such as the whole process of flower blooming rather than a set of "moving images", we propose a novel Free-Bloom pipeline that harnesses large language models (LLMs) as the director to generate a semantic-coherence prompt sequence, while pre-trained latent diffusion models (LDMs) as the animator to generate the high fidelity frames. Furthermore, to ensure temporal and identical coherence while maintaining semantic coherence, we propose a series of annotative modifications to adapting LDMs in the reverse process, including joint noise sampling, step-aware attention shift, and dual-path interpolation. Without any video data and training requirements, Free-Bloom generates vivid and high-quality videos, awe-inspiring in generating complex scenes with semantic meaningful frame sequences. In addition, Free-Bloom is naturally compatible with LDMs-based extensions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AiAReSeg-Catheter-Detection-and-Segmentation-in-Interventional-Ultrasound-using-Transformers"><a href="#AiAReSeg-Catheter-Detection-and-Segmentation-in-Interventional-Ultrasound-using-Transformers" class="headerlink" title="AiAReSeg: Catheter Detection and Segmentation in Interventional Ultrasound using Transformers"></a>AiAReSeg: Catheter Detection and Segmentation in Interventional Ultrasound using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14492">http://arxiv.org/abs/2309.14492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Ranne, Yordanka Velikova, Nassir Navab, Ferdinando Rodriguez y Baena<br>for: 这篇论文是为了提出一种基于深度学习的扫描器网络，以便在 intervencion Ultrasound 图像序列中探测和分割导管。methods: 该方法使用了一种基于 Attention 机制的 transformer 架构，并引入了一种新的 3D 分割头，以实现在时间上的扫描。results: 该方法在一个验证数据集上进行了验证，并在用physics-based导管插入 simulations 生成的synthetic Ultrasound 图像上进行了测试，得到了良好的效果。<details>
<summary>Abstract</summary>
To date, endovascular surgeries are performed using the golden standard of Fluoroscopy, which uses ionising radiation to visualise catheters and vasculature. Prolonged Fluoroscopic exposure is harmful for the patient and the clinician, and may lead to severe post-operative sequlae such as the development of cancer. Meanwhile, the use of interventional Ultrasound has gained popularity, due to its well-known benefits of small spatial footprint, fast data acquisition, and higher tissue contrast images. However, ultrasound images are hard to interpret, and it is difficult to localise vessels, catheters, and guidewires within them. This work proposes a solution using an adaptation of a state-of-the-art machine learning transformer architecture to detect and segment catheters in axial interventional Ultrasound image sequences. The network architecture was inspired by the Attention in Attention mechanism, temporal tracking networks, and introduced a novel 3D segmentation head that performs 3D deconvolution across time. In order to facilitate training of such deep learning networks, we introduce a new data synthesis pipeline that used physics-based catheter insertion simulations, along with a convolutional ray-casting ultrasound simulator to produce synthetic ultrasound images of endovascular interventions. The proposed method is validated on a hold-out validation dataset, thus demonstrated robustness to ultrasound noise and a wide range of scanning angles. It was also tested on data collected from silicon-based aorta phantoms, thus demonstrated its potential for translation from sim-to-real. This work represents a significant step towards safer and more efficient endovascular surgery using interventional ultrasound.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:到目前为止，endovascular手术使用金标准fluoroscopy，利用 ionizing radiation Visualize catheters 和vasculature。 prolonged fluoroscopic exposure 对 patient 和 clinician 有害，可能导致 postoperative sequelae 的发展，如 cancer。 Meanwhile，使用 interventional ultrasound 已经得到了广泛的应用，因为它的小型空间占用、快速的数据收集和高对比度图像等优点。然而，ultrasound 图像具有困难的解释和localize vessels、catheters 和 guidewires 在它们中的问题。这个工作提出了一种解决方案，利用一种基于 state-of-the-art 机器学习 transformer 架构来检测和分割 axial interventional ultrasound 图像序列中的 catheters。该网络架构 inspirited 了 Attention in Attention 机制、时间跟踪网络和引入了一个新的3D分割头，通过在时间方向上进行3D deconvolution。为了促进这些深度学习网络的训练，我们引入了一个新的数据生成管线，利用基于物理学习 catheter 插入 simulations 和一个 convolutional ray-casting ultrasound simulator 生成 synthetic ultrasound 图像。该提案在 hold-out 验证集上验证了Robustness 于 ultrasound noise 和多个扫描角度。它还在基于 silicon-based 的 aorta 模型上测试， thereby demonstrating its potential for translation from sim-to-real。这个工作表示了更安全和高效的 endovascular surgery 使用 interventional ultrasound 的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-3D-Perception-with-2D-Vision-Language-Distillation-for-Autonomous-Driving"><a href="#Unsupervised-3D-Perception-with-2D-Vision-Language-Distillation-for-Autonomous-Driving" class="headerlink" title="Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving"></a>Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14491">http://arxiv.org/abs/2309.14491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles R. Qi, Xinchen Yan, Scott Ettinger, Dragomir Anguelov</li>
<li>for: 这篇论文是为了解决关键控制自动驾驶中关键的开放集类型的3D感知问题而写的。</li>
<li>methods: 该论文提出了一种多模态自动标注管道，可以在没有3D人类标签的情况下，使用点云序列中的运动迹象和公共可用的2D图像文本对，识别和跟踪所有交通参与者。</li>
<li>results: 对于 Waymo 开放数据集的实验，该方法与先前的研究相比，在各种无监督3D感知任务上表现出了显著的优异。<details>
<summary>Abstract</summary>
Closed-set 3D perception models trained on only a pre-defined set of object categories can be inadequate for safety critical applications such as autonomous driving where new object types can be encountered after deployment. In this paper, we present a multi-modal auto labeling pipeline capable of generating amodal 3D bounding boxes and tracklets for training models on open-set categories without 3D human labels. Our pipeline exploits motion cues inherent in point cloud sequences in combination with the freely available 2D image-text pairs to identify and track all traffic participants. Compared to the recent studies in this domain, which can only provide class-agnostic auto labels limited to moving objects, our method can handle both static and moving objects in the unsupervised manner and is able to output open-vocabulary semantic labels thanks to the proposed vision-language knowledge distillation. Experiments on the Waymo Open Dataset show that our approach outperforms the prior work by significant margins on various unsupervised 3D perception tasks.
</details>
<details>
<summary>摘要</summary>
闭sets 3D 识别模型只训练在预定的对象类型上可能不够用于安全关键应用程序，如自动驾驶，因为在部署后可能会遇到新的对象类型。在这篇论文中，我们提出了一个多Modal auto Labeling 管道，可以在无人标注的情况下为训练模型提供开放集成类别的培训数据。我们的管道利用点云序列中的运动特征，并与可以得到的免费的2D图像文本对照来识别和跟踪所有交通参与者。与当前领域的研究相比，我们的方法可以不需要人工标注，并且可以自动为移动和静止对象分配开放 vocabulary 语义标签。我们的方法在 Waymo 开放数据集上进行了实验，并与之前的工作相比，在各种无监督3D识别任务上表现出了显著的优异。
</details></li>
</ul>
<hr>
<h2 id="Gastro-Intestinal-Tract-Segmentation-Using-an-Explainable-3D-Unet"><a href="#Gastro-Intestinal-Tract-Segmentation-Using-an-Explainable-3D-Unet" class="headerlink" title="Gastro-Intestinal Tract Segmentation Using an Explainable 3D Unet"></a>Gastro-Intestinal Tract Segmentation Using an Explainable 3D Unet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14474">http://arxiv.org/abs/2309.14474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Li, Jonathan Chan</li>
<li>for: 这篇论文旨在探讨对胃肠癌使用放射治疗时，辐射科医师的角色是如何实现高剂量辐射的同时避免胃肠的影响。</li>
<li>methods: 这篇论文提出了一个基于深度学习（DL）的治疗管线，并将可读性学习（XAI） integrate into the pipeline 以提高模型的透明度和可信度。</li>
<li>results: 这篇论文获得了一个可靠且高精度的辐射治疗管线，可以帮助辐射科医师更快速地处理病人。<details>
<summary>Abstract</summary>
In treating gastrointestinal cancer using radiotherapy, the role of the radiation oncologist is to administer high doses of radiation, through x-ray beams, toward the tumor while avoiding the stomach and intestines. With the advent of precise radiation treatment technology such as the MR-Linac, oncologists can visualize the daily positions of the tumors and intestines, which may vary day to day. Before delivering radiation, radio oncologists must manually outline the position of the gastrointestinal organs in order to determine position and direction of the x-ray beam. This is a time consuming and labor intensive process that may substantially prolong a patient's treatment. A deep learning (DL) method can automate and expedite the process. However, many deep neural networks approaches currently in use are black-boxes which lack interpretability which render them untrustworthy and impractical in a healthcare setting. To address this, an emergent field of AI known as Explainable AI (XAI) may be incorporated to improve the transparency and viability of a model. This paper proposes a deep learning pipeline that incorporates XAI to address the challenges of organ segmentation.
</details>
<details>
<summary>摘要</summary>
在治疗肝肠癌用电疗时，辐射生物学家的角色是通过X射线束射高剂量辐射于肿瘤，同时避免肠和肠肝。随着精细辐射治疗技术的发展，如MR-Linac，生物学家可以每天Visualize肿瘤和肠肝的位置，这些位置可能每天不同。在发射辐射之前， radio生物学家必须手动标识肠肝的位置，以确定辐射的方向和强度。这是一项时间consuming和劳动密集的过程，可能会导致患者的治疗持续时间增加。在这种情况下，一种深度学习（DL）方法可以自动和加速这个过程。然而，许多深度神经网络方法现在在使用的是黑obox，缺乏可读性，这使得它们在医疗设置中不可靠和不实用。为了解决这个问题，一个emergent的人工智能领域，称为可解释AI（XAI），可以被包含到深度学习管道中，以提高模型的透明度和实用性。本文提出了一个深度学习管道，其中包含XAI，以解决肠肿瘤分割的挑战。
</details></li>
</ul>
<hr>
<h2 id="FARSEC-A-Reproducible-Framework-for-Automatic-Real-Time-Vehicle-Speed-Estimation-Using-Traffic-Cameras"><a href="#FARSEC-A-Reproducible-Framework-for-Automatic-Real-Time-Vehicle-Speed-Estimation-Using-Traffic-Cameras" class="headerlink" title="FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras"></a>FARSEC: A Reproducible Framework for Automatic Real-Time Vehicle Speed Estimation Using Traffic Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14468">http://arxiv.org/abs/2309.14468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/porscheofficial/speed-estimation-traffic-monitoring">https://github.com/porscheofficial/speed-estimation-traffic-monitoring</a></li>
<li>paper_authors: Lucas Liebe, Franz Sauerwald, Sylwester Sawicki, Matthias Schneider, Leo Schuhmann, Tolga Buz, Paul Boes, Ahmad Ahmadov, Gerard de Melo</li>
<li>for: 该研究旨在提供一种自动实时计算车辆速度的框架，以提高交通监测和管理的精度和效果。</li>
<li>methods: 该模型使用了新的技术来预测深度地图，从而估算道路段长度，并可以自动处理实际情况如摄像头运动和不同视频流输入。</li>
<li>results: 与三种已知模型进行比较后，该模型在实际的CCTV视频上达到了竞争性的结果，同时具有更好的可重复性和可更新性。<details>
<summary>Abstract</summary>
Estimating the speed of vehicles using traffic cameras is a crucial task for traffic surveillance and management, enabling more optimal traffic flow, improved road safety, and lower environmental impact. Transportation-dependent systems, such as for navigation and logistics, have great potential to benefit from reliable speed estimation. While there is prior research in this area reporting competitive accuracy levels, their solutions lack reproducibility and robustness across different datasets. To address this, we provide a novel framework for automatic real-time vehicle speed calculation, which copes with more diverse data from publicly available traffic cameras to achieve greater robustness. Our model employs novel techniques to estimate the length of road segments via depth map prediction. Additionally, our framework is capable of handling realistic conditions such as camera movements and different video stream inputs automatically. We compare our model to three well-known models in the field using their benchmark datasets. While our model does not set a new state of the art regarding prediction performance, the results are competitive on realistic CCTV videos. At the same time, our end-to-end pipeline offers more consistent results, an easier implementation, and better compatibility. Its modular structure facilitates reproducibility and future improvements.
</details>
<details>
<summary>摘要</summary>
Our model employs novel techniques to estimate the length of road segments via depth map prediction. Additionally, our framework can automatically handle realistic conditions such as camera movements and different video stream inputs. We compare our model with three well-known models in the field using their benchmark datasets. While our model does not set a new state of the art regarding prediction performance, the results are competitive on realistic CCTV videos. Our end-to-end pipeline offers more consistent results, easier implementation, and better compatibility. Its modular structure facilitates reproducibility and future improvements.
</details></li>
</ul>
<hr>
<h2 id="Chop-Learn-Recognizing-and-Generating-Object-State-Compositions"><a href="#Chop-Learn-Recognizing-and-Generating-Object-State-Compositions" class="headerlink" title="Chop &amp; Learn: Recognizing and Generating Object-State Compositions"></a>Chop &amp; Learn: Recognizing and Generating Object-State Compositions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14339">http://arxiv.org/abs/2309.14339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nirat Saini, Hanyu Wang, Archana Swaminathan, Vinoj Jayasundara, Bo He, Kamal Gupta, Abhinav Shrivastava</li>
<li>for: 这篇论文主要研究了不同风格下对物体进行割辑和对象状态的变化。</li>
<li>methods: 该论文提出了一个新的benchmark集合Chop &amp; Learn，用于学习不同风格下的割辑和多视点下的对象状态。同时，它还提出了一个新任务：compositional image generation，可以将学习的割辑风格转移到不同的对象上，生成新的对象状态图像。</li>
<li>results: 该论文使用视频进行compositional action recognition，并证明了这些数据的多种应用。项目官网：<a target="_blank" rel="noopener" href="https://chopnlearn.github.io./">https://chopnlearn.github.io。</a><details>
<summary>Abstract</summary>
Recognizing and generating object-state compositions has been a challenging task, especially when generalizing to unseen compositions. In this paper, we study the task of cutting objects in different styles and the resulting object state changes. We propose a new benchmark suite Chop & Learn, to accommodate the needs of learning objects and different cut styles using multiple viewpoints. We also propose a new task of Compositional Image Generation, which can transfer learned cut styles to different objects, by generating novel object-state images. Moreover, we also use the videos for Compositional Action Recognition, and show valuable uses of this dataset for multiple video tasks. Project website: https://chopnlearn.github.io.
</details>
<details>
<summary>摘要</summary>
Recognizing and generating object-state compositions has been a challenging task, especially when generalizing to unseen compositions. In this paper, we study the task of cutting objects in different styles and the resulting object state changes. We propose a new benchmark suite Chop & Learn, to accommodate the needs of learning objects and different cut styles using multiple viewpoints. We also propose a new task of Compositional Image Generation, which can transfer learned cut styles to different objects, by generating novel object-state images. Moreover, we also use the videos for Compositional Action Recognition, and show valuable uses of this dataset for multiple video tasks. Project website: https://chopnlearn.github.io.Translation:recognizing和生成对象状态组合是一个挑战性任务，特别是对于未看过的组合。在这篇论文中，我们研究对象在不同风格下被剪辑的任务，以及它们所导致的对象状态变化。我们提出了一个新的benchmark集Chop & Learn，以便学习对象和不同剪辑风格的多视点学习。我们还提出了一个新的任务：compositional Image Generation，可以将学习的剪辑风格应用到不同的对象上，通过生成新的对象状态图像。此外，我们还使用视频进行compositional Action Recognition，并显示了这个数据集的多种视频任务的用途。项目网站：https://chopnlearn.github.io。
</details></li>
</ul>
<hr>
<h2 id="3D-Indoor-Instance-Segmentation-in-an-Open-World"><a href="#3D-Indoor-Instance-Segmentation-in-an-Open-World" class="headerlink" title="3D Indoor Instance Segmentation in an Open-World"></a>3D Indoor Instance Segmentation in an Open-World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14338">http://arxiv.org/abs/2309.14338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aminebdj/3d-owis">https://github.com/aminebdj/3d-owis</a></li>
<li>paper_authors: Mohamed El Amine Boudjoghra, Salwa K. Al Khatib, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan</li>
<li>for: 3D indoor instance segmentation in an open-world setting, where the model can distinguish known classes and identify unknown objects</li>
<li>methods: use an auto-labeling scheme to produce pseudo-labels during training, and adjust the unknown class probability based on objectness score distribution</li>
<li>results: promising open-world 3D instance segmentation performance with carefully curated open-world splits<details>
<summary>Abstract</summary>
Existing 3D instance segmentation methods typically assume that all semantic classes to be segmented would be available during training and only seen categories are segmented at inference. We argue that such a closed-world assumption is restrictive and explore for the first time 3D indoor instance segmentation in an open-world setting, where the model is allowed to distinguish a set of known classes as well as identify an unknown object as unknown and then later incrementally learning the semantic category of the unknown when the corresponding category labels are available. To this end, we introduce an open-world 3D indoor instance segmentation method, where an auto-labeling scheme is employed to produce pseudo-labels during training and induce separation to separate known and unknown category labels. We further improve the pseudo-labels quality at inference by adjusting the unknown class probability based on the objectness score distribution. We also introduce carefully curated open-world splits leveraging realistic scenarios based on inherent object distribution, region-based indoor scene exploration and randomness aspect of open-world classes. Extensive experiments reveal the efficacy of the proposed contributions leading to promising open-world 3D instance segmentation performance.
</details>
<details>
<summary>摘要</summary>
现有的3D实例分割方法通常假设所有需要分割的semantic类都会在训练时 disponible，只有seen类会在推理时分割。我们认为这种closed-world假设是限制性的，我们开发了第一个在开放世界设定下进行3Dindoor实例分割的方法，其中模型允许分辨知道的类别以及未知对象的类别，并在可以获得对应类别标签时逐渐学习未知类别的semanticcategory。为此，我们提出了一种开放世界3Dindoor实例分割方法，其中使用自动标签机制生成pseudo-标签 durante entrenamiento，并在推理时调整未知类别概率根据对象性分布。此外，我们还提出了仔细制定的开放世界分割，利用实际场景、indoorScene区域探索和Randomaspect of open-world类来生成准确的pseudo-标签。广泛的实验表明我们的提案具有优秀的开放世界3D实例分割性能。
</details></li>
</ul>
<hr>
<h2 id="Noise-in-Bias-out-Balanced-and-Real-time-MoCap-Solving"><a href="#Noise-in-Bias-out-Balanced-and-Real-time-MoCap-Solving" class="headerlink" title="Noise-in, Bias-out: Balanced and Real-time MoCap Solving"></a>Noise-in, Bias-out: Balanced and Real-time MoCap Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14330">http://arxiv.org/abs/2309.14330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Albanis, Nikolaos Zioulis, Spyridon Thermos, Anargyros Chatzitofis, Kostas Kolomvatsos</li>
<li>for: 这篇论文旨在提高现场摄像头采集系统的准确性和可靠性，使用机器学习算法来解决 marker 估计中的噪声和不规则性。</li>
<li>methods: 该论文使用机器学习技术，包括表示学习和不平衡回归，来解决 marker 估计中的问题。它还利用了 marker-less MoCap 技术来获取数据。</li>
<li>results: 该论文的实验结果表明，使用该方法可以在实时 MoCap 中提高 marker 估计的准确性和稳定性，并在具有极端和特殊 pose 的情况下表现出优异性。项目页面：<a target="_blank" rel="noopener" href="https://moverseai.github.io/noise-tail">https://moverseai.github.io/noise-tail</a><details>
<summary>Abstract</summary>
Real-time optical Motion Capture (MoCap) systems have not benefited from the advances in modern data-driven modeling. In this work we apply machine learning to solve noisy unstructured marker estimates in real-time and deliver robust marker-based MoCap even when using sparse affordable sensors. To achieve this we focus on a number of challenges related to model training, namely the sourcing of training data and their long-tailed distribution. Leveraging representation learning we design a technique for imbalanced regression that requires no additional data or labels and improves the performance of our model in rare and challenging poses. By relying on a unified representation, we show that training such a model is not bound to high-end MoCap training data acquisition, and exploit the advances in marker-less MoCap to acquire the necessary data. Finally, we take a step towards richer and affordable MoCap by adapting a body model-based inverse kinematics solution to account for measurement and inference uncertainty, further improving performance and robustness. Project page: https://moverseai.github.io/noise-tail
</details>
<details>
<summary>摘要</summary>
现实时光学动作捕捉（MoCap）系统没有受到现代数据驱动模型的改进。在这项工作中，我们通过机器学习解决实时噪声不结构 marker 估计中的噪声问题，并提供了可靠的 marker-based MoCap，即使使用便宜的感知器。为达到这一目标，我们关注了许多相关的挑战，包括训练数据的获取和其长尾分布。通过表示学习，我们设计了一种无需额外数据或标签的偏好回归技术，以提高我们模型在罕见和具有挑战性的姿势中的性能。由于我们的模型不依赖高级 MoCap 训练数据获取，我们可以利用 marker-less MoCap 技术获取必要的数据。最后，我们通过对体部模型基于 inverse kinematics 解决方案进行修改，以考虑测量和推理不确定性，进一步提高性能和可靠性。项目页面：https://moverseai.github.io/noise-tail
</details></li>
</ul>
<hr>
<h2 id="DeepMesh-Mesh-based-Cardiac-Motion-Tracking-using-Deep-Learning"><a href="#DeepMesh-Mesh-based-Cardiac-Motion-Tracking-using-Deep-Learning" class="headerlink" title="DeepMesh: Mesh-based Cardiac Motion Tracking using Deep Learning"></a>DeepMesh: Mesh-based Cardiac Motion Tracking using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14306">http://arxiv.org/abs/2309.14306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingjie Meng, Wenjia Bai, Declan P O’Regan, and Daniel Rueckert</li>
<li>for: 这个论文是用于评估冠动脉疾病和诊断冠动脉疾病的评估工具。</li>
<li>methods: 这个论文使用的方法是基于深度学习的DeepMesh模型，该模型可以从冠动脉CMR图像中提取冠动脉的3D运动信息。</li>
<li>results: 实验结果表明，DeepMesh方法可以高效地和量化地评估冠动脉左心室的3D运动信息，并且比其他图像基于和网格基于的冠动脉运动跟踪方法更高效。<details>
<summary>Abstract</summary>
3D motion estimation from cine cardiac magnetic resonance (CMR) images is important for the assessment of cardiac function and the diagnosis of cardiovascular diseases. Current state-of-the art methods focus on estimating dense pixel-/voxel-wise motion fields in image space, which ignores the fact that motion estimation is only relevant and useful within the anatomical objects of interest, e.g., the heart. In this work, we model the heart as a 3D mesh consisting of epi- and endocardial surfaces. We propose a novel learning framework, DeepMesh, which propagates a template heart mesh to a subject space and estimates the 3D motion of the heart mesh from CMR images for individual subjects. In DeepMesh, the heart mesh of the end-diastolic frame of an individual subject is first reconstructed from the template mesh. Mesh-based 3D motion fields with respect to the end-diastolic frame are then estimated from 2D short- and long-axis CMR images. By developing a differentiable mesh-to-image rasterizer, DeepMesh is able to leverage 2D shape information from multiple anatomical views for 3D mesh reconstruction and mesh motion estimation. The proposed method estimates vertex-wise displacement and thus maintains vertex correspondences between time frames, which is important for the quantitative assessment of cardiac function across different subjects and populations. We evaluate DeepMesh on CMR images acquired from the UK Biobank. We focus on 3D motion estimation of the left ventricle in this work. Experimental results show that the proposed method quantitatively and qualitatively outperforms other image-based and mesh-based cardiac motion tracking methods.
</details>
<details>
<summary>摘要</summary>
3D动态计算从cinéCardiac Magnetic Resonance（CMR）图像是评估心脏功能和诊断循环疾病的重要方法。当前状态艺术方法都是在图像空间进行密集像素/体积化动态场的估计，忽略了动态场只在心脏的解剖对象上是有用的事实。在这种工作中，我们模型了心脏为3D网格，包括血管和内血管表面。我们提出了一种新的学习框架，深度网格（DeepMesh），它将投影模板心脏网格到个体空间，并估计从CMR图像中心脏的3D动态。在DeepMesh中，个体心脏的结构图像的结束 диасто利Frame中的心脏网格被首先从模板网格中重construct。然后，从2D短轴和长轴CMR图像中获取心脏网格的3D动态场，并通过开发可导的网格到图像照片的映射器，以便利用多视图解剖信息来进行3D网格重建和动态场估计。提出的方法可以计算 vertex-wise 偏移量，并维护 vertex 之间的匹配关系，这是评估不同个体和人口中心脏功能的量化评估的关键。我们在UK Biobank中获取的CMR图像进行了实验，我们专注于左心室的3D动态跟踪。实验结果表明，提出的方法在图像基于和网格基于的心脏动态跟踪方法中量化和质量上有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Regress-Before-Construct-Regress-Autoencoder-for-Point-Cloud-Self-supervised-Learning"><a href="#Regress-Before-Construct-Regress-Autoencoder-for-Point-Cloud-Self-supervised-Learning" class="headerlink" title="Regress Before Construct: Regress Autoencoder for Point Cloud Self-supervised Learning"></a>Regress Before Construct: Regress Autoencoder for Point Cloud Self-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03670">http://arxiv.org/abs/2310.03670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liuyyy111/point-rae">https://github.com/liuyyy111/point-rae</a></li>
<li>paper_authors: Yang Liu, Chen Chen, Can Wang, Xulin King, Mengyuan Liu</li>
<li>for: 这篇论文的目的是提出一种新的自适应神经网络模型，即Point Regress AutoEncoder（Point-RAE），用于无监督学习三维点云数据。</li>
<li>methods: 该模型使用了一个mask regressor来预测masked patch representation，并使用了一个alignment constraint来确保预测的masked patch representation与实际的masked patch表示相一致。</li>
<li>results: 该模型在多个下游任务中表现出色，包括ScanObjectNN和ModelNet40等。Specifically, our pre-trained models achieve a high accuracy of 90.28% on the ScanObjectNN hardest split and 94.1% accuracy on ModelNet40, surpassing all the other self-supervised learning methods.<details>
<summary>Abstract</summary>
Masked Autoencoders (MAE) have demonstrated promising performance in self-supervised learning for both 2D and 3D computer vision. Nevertheless, existing MAE-based methods still have certain drawbacks. Firstly, the functional decoupling between the encoder and decoder is incomplete, which limits the encoder's representation learning ability. Secondly, downstream tasks solely utilize the encoder, failing to fully leverage the knowledge acquired through the encoder-decoder architecture in the pre-text task. In this paper, we propose Point Regress AutoEncoder (Point-RAE), a new scheme for regressive autoencoders for point cloud self-supervised learning. The proposed method decouples functions between the decoder and the encoder by introducing a mask regressor, which predicts the masked patch representation from the visible patch representation encoded by the encoder and the decoder reconstructs the target from the predicted masked patch representation. By doing so, we minimize the impact of decoder updates on the representation space of the encoder. Moreover, we introduce an alignment constraint to ensure that the representations for masked patches, predicted from the encoded representations of visible patches, are aligned with the masked patch presentations computed from the encoder. To make full use of the knowledge learned in the pre-training stage, we design a new finetune mode for the proposed Point-RAE. Extensive experiments demonstrate that our approach is efficient during pre-training and generalizes well on various downstream tasks. Specifically, our pre-trained models achieve a high accuracy of \textbf{90.28\%} on the ScanObjectNN hardest split and \textbf{94.1\%} accuracy on ModelNet40, surpassing all the other self-supervised learning methods. Our code and pretrained model are public available at: \url{https://github.com/liuyyy111/Point-RAE}.
</details>
<details>
<summary>摘要</summary>
masked autoencoders (MAE) 在自助学习中表现出色，特别是在2D和3D计算机视觉领域。然而，现有的MAE基本方法仍有一些缺点。首先，Encoder和Decoder之间的函数分离不够完善，这限制了Encoder的表征学习能力。其次，下游任务只使用Encoder，而不全面利用通过Encoder-Decoder架构在 предtext任务中获得的知识。在这篇论文中，我们提出了Point Regress AutoEncoder（Point-RAE），一种新的抽象方法 для点云自助学习。我们在Point-RAE中引入了一个mask推 regression器，该推 regression器预测从可见patch表示中编码的masked patch表示，而Decoder则使用这些预测的masked patch表示重建目标。通过这种方式，我们减少了Encoder的表征空间中Decoder的影响。此外，我们引入了一个alignment constraint，确保encoded表示中的masked patch表示与Encoder计算的masked patch表示相align。为了充分利用在预训练阶段学习的知识，我们设计了一种新的finetune模式 дляPoint-RAE。我们的实验表明，我们的方法是在预训练阶段高效，并且在多个下游任务上具有良好的泛化性。具体来说，我们的预训练模型在ScanObjectNN最难的分区上达到了90.28%的高精度，并在ModelNet40上达到了94.1%的精度，超过了所有其他自助学习方法。我们的代码和预训练模型可以在以下链接中下载：\url{https://github.com/liuyyy111/Point-RAE}.
</details></li>
</ul>
<hr>
<h2 id="Dataset-Diffusion-Diffusion-based-Synthetic-Dataset-Generation-for-Pixel-Level-Semantic-Segmentation"><a href="#Dataset-Diffusion-Diffusion-based-Synthetic-Dataset-Generation-for-Pixel-Level-Semantic-Segmentation" class="headerlink" title="Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation"></a>Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14303">http://arxiv.org/abs/2309.14303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/dataset-diffusion">https://github.com/vinairesearch/dataset-diffusion</a></li>
<li>paper_authors: Quang Nguyen, Truong Vu, Anh Tran, Khoi Nguyen<br>for:This paper aims to address the labor-intensive task of preparing training data for deep vision models by proposing a novel method for generating pixel-level semantic segmentation labels using a text-to-image generative model.methods:The proposed method utilizes the text prompts, cross-attention, and self-attention of the Stable Diffusion (SD) model to generate segmentation maps corresponding to synthetic images. The method introduces three new techniques: class-prompt appending, class-prompt cross-attention, and self-attention exponentiation.results:The proposed approach significantly outperforms concurrent work on two datasets, PASCAL VOC and MSCOCO, and provides a reliable way to generate pixel-level semantic segmentation labels without the need for labor-intensive pixel-wise annotation.<details>
<summary>Abstract</summary>
Preparing training data for deep vision models is a labor-intensive task. To address this, generative models have emerged as an effective solution for generating synthetic data. While current generative models produce image-level category labels, we propose a novel method for generating pixel-level semantic segmentation labels using the text-to-image generative model Stable Diffusion (SD). By utilizing the text prompts, cross-attention, and self-attention of SD, we introduce three new techniques: class-prompt appending, class-prompt cross-attention, and self-attention exponentiation. These techniques enable us to generate segmentation maps corresponding to synthetic images. These maps serve as pseudo-labels for training semantic segmenters, eliminating the need for labor-intensive pixel-wise annotation. To account for the imperfections in our pseudo-labels, we incorporate uncertainty regions into the segmentation, allowing us to disregard loss from those regions. We conduct evaluations on two datasets, PASCAL VOC and MSCOCO, and our approach significantly outperforms concurrent work. Our benchmarks and code will be released at https://github.com/VinAIResearch/Dataset-Diffusion
</details>
<details>
<summary>摘要</summary>
准备深度视觉模型的训练数据是一项劳动密集的任务。为了解决这个问题，生成模型在深度学习领域得到了广泛的应用。现有的生成模型可以生成图像级别的类别标签，但我们提出了一种新的方法，即使用文本生成器Stable Diffusion（SD）来生成像素级别的semantic segmentation标签。我们利用文本提示、跨处理和自处理的SD特性，提出了三种新技术：类提示附加、类提示跨处理和自处理指数。这些技术使得我们可以生成对应于合成图像的segmentation图。这些图像serve为训练semantic segmenter的pseudo标签，从而消除了对每个像素的手动标注的劳动密集任务。为了考虑我们的 pseudo标签中的不准确部分，我们将uncertainty区域纳入segmentation中，因此可以忽略这些区域中的损失。我们在PASCAL VOC和MSCOCO两个 dataset上进行了评估，并得到了与当前同类工作的显著超越。我们的标准 benchmarks和代码将在https://github.com/VinAIResearch/Dataset-Diffusion中发布。
</details></li>
</ul>
<hr>
<h2 id="Tiled-Multiplane-Images-for-Practical-3D-Photography"><a href="#Tiled-Multiplane-Images-for-Practical-3D-Photography" class="headerlink" title="Tiled Multiplane Images for Practical 3D Photography"></a>Tiled Multiplane Images for Practical 3D Photography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14291">http://arxiv.org/abs/2309.14291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Numair Khan, Douglas Lanman, Lei Xiao</li>
<li>for: 该研究旨在解决单视图图像中的三维摄影问题，有用的应用包括虚拟现实和移动计算。</li>
<li>methods: 该研究使用了多平面图像（MPI）来Estimate scene，可以模型复杂的外观效果、抗抖阈深度错误和软边缘Synthesize better than使用 текстури化的雷达或层次深度图像。</li>
<li>results: 该研究提出了一种使用分割多平面图像（TMPI）来生成单视图三维图像的方法，其中每个小区域只有几个深度层，可以提高计算效率。与state-of-the-art单视图MPI方法相比，该方法的生成结果相似，计算 overhead 比较低。<details>
<summary>Abstract</summary>
The task of synthesizing novel views from a single image has useful applications in virtual reality and mobile computing, and a number of approaches to the problem have been proposed in recent years. A Multiplane Image (MPI) estimates the scene as a stack of RGBA layers, and can model complex appearance effects, anti-alias depth errors and synthesize soft edges better than methods that use textured meshes or layered depth images. And unlike neural radiance fields, an MPI can be efficiently rendered on graphics hardware. However, MPIs are highly redundant and require a large number of depth layers to achieve plausible results. Based on the observation that the depth complexity in local image regions is lower than that over the entire image, we split an MPI into many small, tiled regions, each with only a few depth planes. We call this representation a Tiled Multiplane Image (TMPI). We propose a method for generating a TMPI with adaptive depth planes for single-view 3D photography in the wild. Our synthesized results are comparable to state-of-the-art single-view MPI methods while having lower computational overhead.
</details>
<details>
<summary>摘要</summary>
“ synthesizing novel views from a single image ”有很多实际应用，如虚拟现实和移动设备等，Recent years 有很多解决方案提出来。 Multiplane Image (MPI) 估算场景为堆叠的 RGBA 层，可以更好地模拟复杂的外观效果、抑制遮蔽depth 误差和软边缘。 不同于 neural radiance fields， MPI 可以高效地在图形硬件上运算。然而， MPI 很受重复性的限制，需要许多深度层以 дости得可靠的结果。 根据本地图像区域的深度复杂性观察，我们将 MPI 拆分为多个小、瓷砾的区域，每个区域只有几个深度平面。 我们称这个表示法为 Tiled Multiplane Image (TMPI)。 我们提出一种方法，用于从单一影像中生成 TMPI  WITH adaptive depth planes 的单眼 3D 摄影。我们的合成结果与现有的单眼 MPI 方法相似，但计算负载较低。
</details></li>
</ul>
<hr>
<h2 id="CLIP-DIY-CLIP-Dense-Inference-Yields-Open-Vocabulary-Semantic-Segmentation-For-Free"><a href="#CLIP-DIY-CLIP-Dense-Inference-Yields-Open-Vocabulary-Semantic-Segmentation-For-Free" class="headerlink" title="CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free"></a>CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14289">http://arxiv.org/abs/2309.14289</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wysoczanska/clip-diy">https://github.com/wysoczanska/clip-diy</a></li>
<li>paper_authors: Monika Wysoczańska, Michaël Ramamonjisoa, Tomasz Trzciński, Oriane Siméoni</li>
<li>for: 这个论文旨在开发一种基于 CLIP 的开放世界图像识别方法，以达到零shot  semantic segmentation 的目标。</li>
<li>methods: 该方法不需要任何额外训练或标注，而是基于现有的无监督物体定位方法，Directly 利用 CLIP 的分类能力进行多 scales 的 patch 处理，并将决策综合到一个地图中。</li>
<li>results: 在 PASCAL VOC 和 COCO 上，该方法可以达到零shot  semantic segmentation 的State-of-the-art 结果，与最佳方法在 COCO 上表现相当。<details>
<summary>Abstract</summary>
The emergence of CLIP has opened the way for open-world image perception. The zero-shot classification capabilities of the model are impressive but are harder to use for dense tasks such as image segmentation. Several methods have proposed different modifications and learning schemes to produce dense output. Instead, we propose in this work an open-vocabulary semantic segmentation method, dubbed CLIP-DIY, which does not require any additional training or annotations, but instead leverages existing unsupervised object localization approaches. In particular, CLIP-DIY is a multi-scale approach that directly exploits CLIP classification abilities on patches of different sizes and aggregates the decision in a single map. We further guide the segmentation using foreground/background scores obtained using unsupervised object localization methods. With our method, we obtain state-of-the-art zero-shot semantic segmentation results on PASCAL VOC and perform on par with the best methods on COCO.
</details>
<details>
<summary>摘要</summary>
CLIP的出现开启了开放世界图像识别的新时代。CLIP的零shot分类能力吸引了很多人，但是在密集任务如图像 segmentation 中更加困难使用。许多方法已经提出了不同的修改和学习方案来生成密集输出。而我们在这里提出了一种开放词汇 semantic segmentation 方法，称为 CLIP-DIY，不需要任何额外的训练或标注，而是利用现有的无监督物体定位方法来进行推导。具体来说，CLIP-DIY 是一种多尺度方法，直接利用 CLIP 分类器在不同大小的 patches 上进行分类，并将决定聚合到一个地图上。我们还使用无监督物体定位方法来引导分 segmentation。我们的方法可以在 PASCAL VOC 和 COCO 上达到领先的 zero-shot semantic segmentation 结果，并与最佳方法在 COCO 上表现相当。
</details></li>
</ul>
<hr>
<h2 id="Calibration-based-Dual-Prototypical-Contrastive-Learning-Approach-for-Domain-Generalization-Semantic-Segmentation"><a href="#Calibration-based-Dual-Prototypical-Contrastive-Learning-Approach-for-Domain-Generalization-Semantic-Segmentation" class="headerlink" title="Calibration-based Dual Prototypical Contrastive Learning Approach for Domain Generalization Semantic Segmentation"></a>Calibration-based Dual Prototypical Contrastive Learning Approach for Domain Generalization Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14282">http://arxiv.org/abs/2309.14282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muxin Liao, Shishun Tian, Yuhang Zhang, Guoguang Hua, Wenbin Zou, Xia Li<br>for: 这个研究是为了解决域缩推理中的域对预设问题，并提出了一个基于对照对比学习的方法来获得域标准的特征。methods: 这个方法使用了一个对照对比学习的核心思想，即使用不同域的中心价值作为域标准，并将这些中心价值与不同域的标准对照。results: 这个方法在域扩展Semantic segmentation任务中得到了superior的性能，并且可以对不同域的标准进行对照对比，以获得更好的域标准。<details>
<summary>Abstract</summary>
Prototypical contrastive learning (PCL) has been widely used to learn class-wise domain-invariant features recently. These methods are based on the assumption that the prototypes, which are represented as the central value of the same class in a certain domain, are domain-invariant. Since the prototypes of different domains have discrepancies as well, the class-wise domain-invariant features learned from the source domain by PCL need to be aligned with the prototypes of other domains simultaneously. However, the prototypes of the same class in different domains may be different while the prototypes of different classes may be similar, which may affect the learning of class-wise domain-invariant features. Based on these observations, a calibration-based dual prototypical contrastive learning (CDPCL) approach is proposed to reduce the domain discrepancy between the learned class-wise features and the prototypes of different domains for domain generalization semantic segmentation. It contains an uncertainty-guided PCL (UPCL) and a hard-weighted PCL (HPCL). Since the domain discrepancies of the prototypes of different classes may be different, we propose an uncertainty probability matrix to represent the domain discrepancies of the prototypes of all the classes. The UPCL estimates the uncertainty probability matrix to calibrate the weights of the prototypes during the PCL. Moreover, considering that the prototypes of different classes may be similar in some circumstances, which means these prototypes are hard-aligned, the HPCL is proposed to generate a hard-weighted matrix to calibrate the weights of the hard-aligned prototypes during the PCL. Extensive experiments demonstrate that our approach achieves superior performance over current approaches on domain generalization semantic segmentation tasks.
</details>
<details>
<summary>摘要</summary>
它们（Prototypical contrastive learning，PCL）在最近得到了广泛的应用，用于学习域外兼顾的特征。这些方法基于假设，各个类域的中值（prototype）是域外兼顾的。然而，不同域的中值之间可能存在差异，因此通过PCL学习的域外兼顾特征需要同时与其他域的中值进行对alignment。然而，不同类域的中值可能存在差异，而不同类域的中值可能相似，这可能影响学习域外兼顾特征的过程。基于这些观察，我们提出了一种calibration-based dual prototypical contrastive learning（CDPCL）方法，用于降低不同域的中值与学习的域外兼顾特征之间的域不一致。CDPCL包括了一种uncertainty-guided PCL（UPCL）和一种hard-weighted PCL（HPCL）。由于不同类域的中值之间可能存在不同的差异，我们提出了一个不确定性概率矩阵，用于表示不同类域中值之间的差异。UPCL用于估算这个不确定性概率矩阵，以calibrate PCL中的权重。此外，considering that prototypes of different classes may be similar in some circumstances, which means these prototypes are hard-aligned, we propose a hard-weighted matrix to calibrate the weights of the hard-aligned prototypes during the PCL.经过广泛的实验，我们发现我们的方法在域通用 semantic segmentation 任务上达到了现有方法的最高性能。
</details></li>
</ul>
<hr>
<h2 id="SINCERE-Supervised-Information-Noise-Contrastive-Estimation-REvisited"><a href="#SINCERE-Supervised-Information-Noise-Contrastive-Estimation-REvisited" class="headerlink" title="SINCERE: Supervised Information Noise-Contrastive Estimation REvisited"></a>SINCERE: Supervised Information Noise-Contrastive Estimation REvisited</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14277">http://arxiv.org/abs/2309.14277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tufts-ml/supcontrast">https://github.com/tufts-ml/supcontrast</a></li>
<li>paper_authors: Patrick Feeney, Michael C. Hughes</li>
<li>for: 提供一种正确的扩展自动学习方法，使得自动学习模型可以从可用的类别标签中学习。</li>
<li>methods: 使用InfoNCE损失函数作为基础，并通过修改SupCon损失函数来提供一种正确的扩展方法。</li>
<li>results: 比较SINCERE和SupCon损失函数的学习轨迹和终端Linear分类器性能，发现SINCERE损失函数可以更好地分离不同类别的嵌入空间，并且与SupCon损失函数相比，SINCERE损失函数可以提供更高的终端分类器性能。<details>
<summary>Abstract</summary>
The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation. Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels. This SupCon loss has been widely-used due to reports of good empirical performance. However, in this work we suggest that the specific SupCon loss formulated by prior work has questionable theoretic justification, because it can encourage images from the same class to repel one another in the learned embedding space. This problematic behavior gets worse as the number of inputs sharing one class label increases. We propose the Supervised InfoNCE REvisited (SINCERE) loss as a remedy. SINCERE is a theoretically justified solution for a supervised extension of InfoNCE that never causes images from the same class to repel one another. We further show that minimizing our new loss is equivalent to maximizing a bound on the KL divergence between class conditional embedding distributions. We compare SINCERE and SupCon losses in terms of learning trajectories during pretraining and in ultimate linear classifier performance after finetuning. Our proposed SINCERE loss better separates embeddings from different classes during pretraining while delivering competitive accuracy.
</details>
<details>
<summary>摘要</summary>
《信息干扰对照估计（InfoNCE）损失函数提供了许多自动学习深度学习方法的基础，因为它在实际上表现良好并具有理论基础。之前的工作提出了一种名为超级vised contrastive（SupCon）损失函数，用于从可用的类标签学习。这种SupCon损失函数广泛使用，但是我们认为其具体的形式不具有理论基础，因为它可能会使图像同一个类型的图像在学习的嵌入空间中抵抗对方。这种问题的严重程度随着输入图像同一个类型的数量增加。我们提议一种名为Supervised InfoNCE REvisited（SINCERE）损失函数，它是一种理论上正确的自upervised扩展，不会使图像同一个类型的图像在学习的嵌入空间中抵抗对方。我们还证明了将我们的新损失函数最小化等价于将类型 conditional嵌入分布的KL散度上升 bounds。我们比较了SINCERE和SupCon损失函数在预训练和精化后的线性分类器性能。我们的提议的SINCERE损失函数在预训练时更好地分离不同类型的嵌入，而且在精化后具有竞争力的准确率。》Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Identity-preserving-Editing-of-Multiple-Facial-Attributes-by-Learning-Global-Edit-Directions-and-Local-Adjustments"><a href="#Identity-preserving-Editing-of-Multiple-Facial-Attributes-by-Learning-Global-Edit-Directions-and-Local-Adjustments" class="headerlink" title="Identity-preserving Editing of Multiple Facial Attributes by Learning Global Edit Directions and Local Adjustments"></a>Identity-preserving Editing of Multiple Facial Attributes by Learning Global Edit Directions and Local Adjustments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14267">http://arxiv.org/abs/2309.14267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Najmeh Mohammadbagheri, Fardin Ayar, Ahmad Nickabadi, Reza Safabakhsh</li>
<li>for: 这个研究旨在解决人脸特征编辑中的identiy损失问题，提出了一个新的架构ID-Style，并在训练过程中使用两种损失函数来保持实例特征的独特性。</li>
<li>methods: 这个架构包括学习全球方向(LGD)和实例化对应强度预测器(IAIP)网络，LGD找到每个属性的共享和半独特的方向，IAIP网络在输入实例上调整全球方向。训练时使用两种损失函数，一种是来避免LGD找到过度独特的方向，另一种是来保持实例特征的独特性。</li>
<li>results: 试验结果显示，ID-Style比基于相似的state-of-the-art工作更好地保持实例特征，具体而言，与基于工作相比，ID-Style在人脸特征编辑中的Identity preserving metric(FRS)和均值修改率(mACC)分别提高了10%和7%。此外，ID-Style的网络结构比基于工作小了约95%，但是它仍然能够保持与基于工作相似的修改效果。<details>
<summary>Abstract</summary>
Semantic facial attribute editing using pre-trained Generative Adversarial Networks (GANs) has attracted a great deal of attention and effort from researchers in recent years. Due to the high quality of face images generated by StyleGANs, much work has focused on the StyleGANs' latent space and the proposed methods for facial image editing. Although these methods have achieved satisfying results for manipulating user-intended attributes, they have not fulfilled the goal of preserving the identity, which is an important challenge. We present ID-Style, a new architecture capable of addressing the problem of identity loss during attribute manipulation. The key components of ID-Style include Learnable Global Direction (LGD), which finds a shared and semi-sparse direction for each attribute, and an Instance-Aware Intensity Predictor (IAIP) network, which finetunes the global direction according to the input instance. Furthermore, we introduce two losses during training to enforce the LGD to find semi-sparse semantic directions, which along with the IAIP, preserve the identity of the input instance. Despite reducing the size of the network by roughly 95% as compared to similar state-of-the-art works, it outperforms baselines by 10% and 7% in Identity preserving metric (FRS) and average accuracy of manipulation (mACC), respectively.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>Recent years have seen a great deal of attention and effort from researchers in the field of semantic facial attribute editing using pre-trained Generative Adversarial Networks (GANs). This is due to the high quality of face images generated by StyleGANs, which has led to a focus on the latent space of StyleGANs and proposed methods for facial image editing. However, these methods have not been able to preserve the identity of the input instance, which is an important challenge. To address this challenge, we present ID-Style, a new architecture that includes Learnable Global Direction (LGD) and an Instance-Aware Intensity Predictor (IAIP) network. The LGD finds a shared and semi-sparse direction for each attribute, while the IAIP finetunes the global direction according to the input instance. Additionally, we introduce two losses during training to enforce the LGD to find semi-sparse semantic directions, which, along with the IAIP, preserve the identity of the input instance. Despite reducing the size of the network by roughly 95% compared to similar state-of-the-art works, ID-Style outperforms baselines by 10% and 7% in Identity Preserving Metric (FRS) and Average Accuracy of Manipulation (mACC), respectively.
</details></li>
</ul>
<hr>
<h2 id="Industrial-Application-of-6D-Pose-Estimation-for-Robotic-Manipulation-in-Automotive-Internal-Logistics"><a href="#Industrial-Application-of-6D-Pose-Estimation-for-Robotic-Manipulation-in-Automotive-Internal-Logistics" class="headerlink" title="Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics"></a>Industrial Application of 6D Pose Estimation for Robotic Manipulation in Automotive Internal Logistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14265">http://arxiv.org/abs/2309.14265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Quentin, Dino Knoll, Daniel Goehring</li>
<li>for: This paper aims to evaluate the current status quo of 6D pose estimation in the context of automotive parts handling tasks, and to identify the challenges and limitations of existing approaches.</li>
<li>methods: The authors built a representative 6D pose estimation pipeline using state-of-the-art components, including data generation methods and pose estimators, and evaluated its performance on automotive parts.</li>
<li>results: The authors found that the performance of the trained 6D pose estimators was promising, but did not meet industry requirements. They also revealed that the main challenge was the inability of the estimators to provide reliable uncertainties for their poses, rather than the accuracy of the poses themselves. Additionally, the authors compared RGB- and RGB-D-based approaches and showed that they are differently vulnerable to the domain gap induced by synthetic data.<details>
<summary>Abstract</summary>
Despite the advances in robotics a large proportion of the of parts handling tasks in the automotive industry's internal logistics are not automated but still performed by humans. A key component to competitively automate these processes is a 6D pose estimation that can handle a large number of different parts, is adaptable to new parts with little manual effort, and is sufficiently accurate and robust with respect to industry requirements. In this context, the question arises as to the current status quo with respect to these measures. To address this we built a representative 6D pose estimation pipeline with state-of-the-art components from economically scalable real to synthetic data generation to pose estimators and evaluated it on automotive parts with regards to a realistic sequencing process. We found that using the data generation approaches, the performance of the trained 6D pose estimators are promising, but do not meet industry requirements. We reveal that the reason for this is the inability of the estimators to provide reliable uncertainties for their poses, rather than the ability of to provide sufficiently accurate poses. In this context we further analyzed how RGB- and RGB-D-based approaches compare against this background and show that they are differently vulnerable to the domain gap induced by synthetic data.
</details>
<details>
<summary>摘要</summary>
Our results show that while the trained 6D pose estimators perform well, they do not meet industry requirements. We found that the reason for this is the inability of the estimators to provide reliable uncertainties for their poses, rather than the accuracy of the poses themselves. Additionally, we compared RGB- and RGB-D-based approaches and found that they are differently vulnerable to the domain gap induced by synthetic data.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Healthcare-with-EOG-A-Novel-Approach-to-Sleep-Stage-Classification"><a href="#Enhancing-Healthcare-with-EOG-A-Novel-Approach-to-Sleep-Stage-Classification" class="headerlink" title="Enhancing Healthcare with EOG: A Novel Approach to Sleep Stage Classification"></a>Enhancing Healthcare with EOG: A Novel Approach to Sleep Stage Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03757">http://arxiv.org/abs/2310.03757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suvadeepmaiti/EOG_Sleep_Stage_classification">https://github.com/suvadeepmaiti/EOG_Sleep_Stage_classification</a></li>
<li>paper_authors: Suvadeep Maiti, Shivam Kumar Sharma, Raju S. Bapi</li>
<li>for:  automated sleep stage classification using EOG signals</li>
<li>methods:  proposed SE-Resnet-Transformer model, 1D-GradCAM, t-SNE plots</li>
<li>results:  accurate classification of five distinct sleep stages, noteworthy performance with macro-F1 scores of 74.72, 70.63, and 69.26, respectively, excelling in identifying REM sleep.<details>
<summary>Abstract</summary>
We introduce an innovative approach to automated sleep stage classification using EOG signals, addressing the discomfort and impracticality associated with EEG data acquisition. In addition, it is important to note that this approach is untapped in the field, highlighting its potential for novel insights and contributions. Our proposed SE-Resnet-Transformer model provides an accurate classification of five distinct sleep stages from raw EOG signal. Extensive validation on publically available databases (SleepEDF-20, SleepEDF-78, and SHHS) reveals noteworthy performance, with macro-F1 scores of 74.72, 70.63, and 69.26, respectively. Our model excels in identifying REM sleep, a crucial aspect of sleep disorder investigations. We also provide insight into the internal mechanisms of our model using techniques such as 1D-GradCAM and t-SNE plots. Our method improves the accessibility of sleep stage classification while decreasing the need for EEG modalities. This development will have promising implications for healthcare and the incorporation of wearable technology into sleep studies, thereby advancing the field's potential for enhanced diagnostics and patient comfort.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种创新的自动睡眠阶段分类方法使用 EOG 信号，解决了使用 EEG 数据采集所带来的不适和实用性问题。此外，这种方法在领域中尚未被探索，因此它的潜在性和贡献很大。我们的提议的 SE-Resnet-Transformer 模型可以准确地从原始 EOG 信号中分类五个不同的睡眠阶段。我们对公共可用的数据库（SleepEDF-20、SleepEDF-78 和 SHHS）进行了广泛的验证，并发现了关键的表现，其中 macro-F1 分数分别为 74.72、70.63 和 69.26。我们的模型在识别 REM 睡眠方面表现出色，这是许多睡眠疾病研究中的关键方面。我们还使用了一些技术，如 1D-GradCAM 和 t-SNE 图表，来探索我们的模型的内部机制。我们的方法可以提高睡眠阶段分类的可accessibility，同时减少 EEG 模态的需求。这种发展将对医疗保健和睡眠研究中的睡眠疾病诊断和患者舒适具有普ROMising的影响。
</details></li>
</ul>
<hr>
<h2 id="Informative-Data-Mining-for-One-Shot-Cross-Domain-Semantic-Segmentation"><a href="#Informative-Data-Mining-for-One-Shot-Cross-Domain-Semantic-Segmentation" class="headerlink" title="Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation"></a>Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14241">http://arxiv.org/abs/2309.14241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxi Wang, Jian Liang, Jun Xiao, Shuqi Mei, Yuran Yang, Zhaoxiang Zhang<br>for: 这个研究旨在提供一个有效的一击适应方法，实现从标注源数据到无标注目数据的语意分类转换。methods: 这个方法使用了一个新的选择项目，将标注源数据中的最有用样本选择出来，以便快速适应和减少过滤训练。然后，这些选择的样本会被用来更新模型，包括裁剪和原型基于的信息增强。results: 我们的方法在实验中表现出色，比较 existing 方法高效和精度。 Specifically, our approach achieves a new state-of-the-art one-shot performance of 56.7%&#x2F;55.4% on the GTA5&#x2F;SYNTHIA to Cityscapes adaptation tasks, respectively.<details>
<summary>Abstract</summary>
Contemporary domain adaptation offers a practical solution for achieving cross-domain transfer of semantic segmentation between labeled source data and unlabeled target data. These solutions have gained significant popularity; however, they require the model to be retrained when the test environment changes. This can result in unbearable costs in certain applications due to the time-consuming training process and concerns regarding data privacy. One-shot domain adaptation methods attempt to overcome these challenges by transferring the pre-trained source model to the target domain using only one target data. Despite this, the referring style transfer module still faces issues with computation cost and over-fitting problems. To address this problem, we propose a novel framework called Informative Data Mining (IDM) that enables efficient one-shot domain adaptation for semantic segmentation. Specifically, IDM provides an uncertainty-based selection criterion to identify the most informative samples, which facilitates quick adaptation and reduces redundant training. We then perform a model adaptation method using these selected samples, which includes patch-wise mixing and prototype-based information maximization to update the model. This approach effectively enhances adaptation and mitigates the overfitting problem. In general, we provide empirical evidence of the effectiveness and efficiency of IDM. Our approach outperforms existing methods and achieves a new state-of-the-art one-shot performance of 56.7\%/55.4\% on the GTA5/SYNTHIA to Cityscapes adaptation tasks, respectively. The code will be released at \url{https://github.com/yxiwang/IDM}.
</details>
<details>
<summary>摘要</summary>
当前领域的适应采用实现了semantic segmentation中的交叉频道传输，即使在不同的测试环境下，可以将源数据标注得到目标数据上的semantic segmentation。这些解决方案在应用中得到了广泛的应用，但是它们需要模型在测试环境发生变化时重新训练，这可能会导致不可持额外高的成本，特别是在时间consuming的训练过程和数据隐私问题上。one-shot适应方法试图解决这些挑战，通过将源模型转移到目标频道上，只需要一个目标数据。然而，这些方法仍然面临计算成本和过拟合问题。为了解决这个问题，我们提出了一种新的框架，即信息挖掘（Informative Data Mining，IDM）。IDM提供了一种不确定性基于的选择 criterion，可以帮助快速适应和减少重复训练。然后，我们使用这些选择的样本进行模型适应方法，包括patch-wise混合和prototype-based信息最大化，以更新模型。这种方法有效地提高适应和减少过拟合问题。总之，我们提供了empirical evidence表明IDM的效果和效率。我们的方法比现有方法更高效，在GTA5/SYNTHIA到Cityscapes适应任务上达到了56.7%/55.4%的一射性能。我们的代码将在\url{https://github.com/yxiwang/IDM}上发布。
</details></li>
</ul>
<hr>
<h2 id="QuadricsNet-Learning-Concise-Representation-for-Geometric-Primitives-in-Point-Clouds"><a href="#QuadricsNet-Learning-Concise-Representation-for-Geometric-Primitives-in-Point-Clouds" class="headerlink" title="QuadricsNet: Learning Concise Representation for Geometric Primitives in Point Clouds"></a>QuadricsNet: Learning Concise Representation for Geometric Primitives in Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14211">http://arxiv.org/abs/2309.14211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/michaelwu99-lab/quadricsnet">https://github.com/michaelwu99-lab/quadricsnet</a></li>
<li>paper_authors: Ji Wu, Huai Yu, Wen Yang, Gui-Song Xia</li>
<li>for: 本研究目的是提出一种 novel 的抽象框架，用于学习 3D 点云中的减少精简 geometric primitive representation。</li>
<li>methods: 我们使用 quadrics 来表示多种 primitives，并提出了首个 end-to-end 学习基于 quadrics 的框架，即 QuadricsNet，用于解析 quadrics 在点云中。我们还提出了一种新的 pattern-comprehensive 数据集，用于训练和评估。</li>
<li>results: 我们的研究表明，我们的精简表示方法和 QuadricsNet 框架具有高效性和稳定性。我们的代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/MichaelWu99-lab/QuadricsNet%7D">https://github.com/MichaelWu99-lab/QuadricsNet}</a> 上获取。<details>
<summary>Abstract</summary>
This paper presents a novel framework to learn a concise geometric primitive representation for 3D point clouds. Different from representing each type of primitive individually, we focus on the challenging problem of how to achieve a concise and uniform representation robustly. We employ quadrics to represent diverse primitives with only 10 parameters and propose the first end-to-end learning-based framework, namely QuadricsNet, to parse quadrics in point clouds. The relationships between quadrics mathematical formulation and geometric attributes, including the type, scale and pose, are insightfully integrated for effective supervision of QuaidricsNet. Besides, a novel pattern-comprehensive dataset with quadrics segments and objects is collected for training and evaluation. Experiments demonstrate the effectiveness of our concise representation and the robustness of QuadricsNet. Our code is available at \url{https://github.com/MichaelWu99-lab/QuadricsNet}
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的框架，用于学习3D点云中简洁的 геометри� primitives表示。与之前每种 primitives 都被 individually 表示不同，我们在挑战性的问题上关注了如何实现一种简洁而均衡的表示。我们使用 quadrics 来表示多样化的 primitives，只需要10个参数。我们提出了第一个终端学习基于架构，即 QuadricsNet，用于解析 quadrics 在点云中。我们妥善地 интеGRATE了 quadrics 的数学表述和 geometric attribute，包括类型、比例和 Orient，以便对 QuadricsNet 进行有效的监督。此外，我们还收集了包含 quadrics 段和物体的novel pattern-comprehensive dataset，用于训练和评估。实验表明我们的简洁表示和 QuadricsNet 的稳定性。我们的代码可以在 GitHub 上找到：https://github.com/MichaelWu99-lab/QuadricsNet。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Animation-of-Hair-Blowing-in-Still-Portrait-Photos"><a href="#Automatic-Animation-of-Hair-Blowing-in-Still-Portrait-Photos" class="headerlink" title="Automatic Animation of Hair Blowing in Still Portrait Photos"></a>Automatic Animation of Hair Blowing in Still Portrait Photos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14207">http://arxiv.org/abs/2309.14207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenpeng Xiao, Wentao Liu, Yitong Wang, Bernard Ghanem, Bing Li</li>
<li>for: 这个论文是为了自动animate人类发样图片中的人发。</li>
<li>methods: 该论文使用了先进的实例分割网络，将发分解为多个实例，并提出了一种基于实例分割的发动模块，以生成自然和愉悦的发动效果。</li>
<li>results: 对比其他state-of-the-art方法，该论文的方法在量化评价中占优，并在质量测试中提供了最愉悦和最吸引人的视觉效果。<details>
<summary>Abstract</summary>
We propose a novel approach to animate human hair in a still portrait photo. Existing work has largely studied the animation of fluid elements such as water and fire. However, hair animation for a real image remains underexplored, which is a challenging problem, due to the high complexity of hair structure and dynamics. Considering the complexity of hair structure, we innovatively treat hair wisp extraction as an instance segmentation problem, where a hair wisp is referred to as an instance. With advanced instance segmentation networks, our method extracts meaningful and natural hair wisps. Furthermore, we propose a wisp-aware animation module that animates hair wisps with pleasing motions without noticeable artifacts. The extensive experiments show the superiority of our method. Our method provides the most pleasing and compelling viewing experience in the qualitative experiments and outperforms state-of-the-art still-image animation methods by a large margin in the quantitative evaluation. Project url: \url{https://nevergiveu.github.io/AutomaticHairBlowing/}
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，用于在静止画像中动画人类发型。现有的工作主要研究了流体元素的动画，如水和火。然而，对真实图像中的发型动画还具有挑战性，这是因为发型结构的复杂性和动态性。为了解决这个问题，我们创新地将发型抽取视为实例分割问题，其中每个发型被称为一个实例。使用高级实例分割网络，我们的方法可以EXTRACT meaningful和自然的发型。此外，我们还提议了一种发型感知动画模块，可以使发型动画具有愉悦的运动而不会出现显著的瑕疵。我们的实验结果表明，我们的方法可以提供最有趣和最有吸引力的视觉体验，并在量化评价中大幅超越了现有的静止图像动画方法。项目链接：<https://nevergiveu.github.io/AutomaticHairBlowing/>
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Grounding-Multi-Modal-Media-Manipulation-and-Beyond"><a href="#Detecting-and-Grounding-Multi-Modal-Media-Manipulation-and-Beyond" class="headerlink" title="Detecting and Grounding Multi-Modal Media Manipulation and Beyond"></a>Detecting and Grounding Multi-Modal Media Manipulation and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14203">http://arxiv.org/abs/2309.14203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rshaojimmy/multimodal-deepfake">https://github.com/rshaojimmy/multimodal-deepfake</a></li>
<li>paper_authors: Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, Ziwei Liu</li>
<li>for: 本研究探讨了一新的多Modal媒体伪造问题，即Detecting and Grounding Multi-Modal Media Manipulation (DGM^4)，旨在不仅检测多Modal媒体的 authenticty，还是根据多Modal媒体的不同modalities进行深入的媒体伪造推理。</li>
<li>methods: 本研究提出了一种名为 HierArchical Multi-modal Manipulation rEasoning tRansformer (HAMMER) 的新方法，该方法包括两个主要部分：1) 多Modal媒体杂化学习和深度媒体推理两个部分，以及2) 多Modal媒体聚合器。</li>
<li>results: 实验结果表明，HAMMER和HAMMER++ 两种模型具有superiority，能够准确地检测和理解多Modal媒体中的伪造 traces。<details>
<summary>Abstract</summary>
Misinformation has become a pressing issue. Fake media, in both visual and textual forms, is widespread on the web. While various deepfake detection and text fake news detection methods have been proposed, they are only designed for single-modality forgery based on binary classification, let alone analyzing and reasoning subtle forgery traces across different modalities. In this paper, we highlight a new research problem for multi-modal fake media, namely Detecting and Grounding Multi-Modal Media Manipulation (DGM^4). DGM^4 aims to not only detect the authenticity of multi-modal media, but also ground the manipulated content, which requires deeper reasoning of multi-modal media manipulation. To support a large-scale investigation, we construct the first DGM^4 dataset, where image-text pairs are manipulated by various approaches, with rich annotation of diverse manipulations. Moreover, we propose a novel HierArchical Multi-modal Manipulation rEasoning tRansformer (HAMMER) to fully capture the fine-grained interaction between different modalities. HAMMER performs 1) manipulation-aware contrastive learning between two uni-modal encoders as shallow manipulation reasoning, and 2) modality-aware cross-attention by multi-modal aggregator as deep manipulation reasoning. Dedicated manipulation detection and grounding heads are integrated from shallow to deep levels based on the interacted multi-modal information. To exploit more fine-grained contrastive learning for cross-modal semantic alignment, we further integrate Manipulation-Aware Contrastive Loss with Local View and construct a more advanced model HAMMER++. Finally, we build an extensive benchmark and set up rigorous evaluation metrics for this new research problem. Comprehensive experiments demonstrate the superiority of HAMMER and HAMMER++.
</details>
<details>
<summary>摘要</summary>
伪信息已成为一种紧迫的问题。在图像和文本之间的多modal forgery广泛存在于互联网上。虽然多种深度迷假检测和文本假新闻检测方法已经提出，但它们只是为单模态迷假进行二分类 binary classification，而不是检测和理解多modal media forgery的细节。在这篇论文中，我们提出了一个新的研究问题：多Modal Media Manipulation Detection and Grounding（DGM^4）。DGM^4的目标不仅是检测多modal media的authenticity，而且需要理解和检测受到 modificaiton的内容，这需要更深入的理解多modal media forgery。为了支持大规模的研究，我们构建了第一个DGM^4数据集，其中图像和文本组合被多种方法修改，并且有丰富的修改注释。此外，我们提出了一种 HierArchical Multi-modal Manipulation rEasoning tRansformer（HAMMER），它可以全面捕捉多modal media forgery的细节交互。HAMMER包括两种推理方法：1）在图像和文本之间进行修改意识的对比学习，以及2）在多modal信息之间进行模块相关的交互。通过将这两种推理方法集成到深度和浅度层次上，我们可以实现更细致的修改检测和修改理解。为了更好地利用跨modal semantic alignment的推理，我们还提出了Manipulation-Aware Contrastive Loss with Local View，并构建了更先进的模型HAMMER++。最后，我们建立了一个完整的 bencmarks，并设置了严格的评价指标。广泛的实验表明HAMMER和HAMMER++的优越性。
</details></li>
</ul>
<hr>
<h2 id="Predictable-Performance-Bias-in-Unsupervised-Anomaly-Detection"><a href="#Predictable-Performance-Bias-in-Unsupervised-Anomaly-Detection" class="headerlink" title="(Predictable) Performance Bias in Unsupervised Anomaly Detection"></a>(Predictable) Performance Bias in Unsupervised Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14198">http://arxiv.org/abs/2309.14198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Meissen, Svenja Breuer, Moritz Knolle, Alena Buyx, Ruth Müller, Georgios Kaissis, Benedikt Wiestler, Daniel Rückert</li>
<li>for: 本研究旨在探讨Unsupervised Anomaly Detection（UAD）模型在医疗领域中的公平性问题。</li>
<li>methods: 我们使用三个大规模公共available的胸部X射线图像 dataset进行了实验，并使用了两种 state-of-the-art UAD模型 для医疗图像。我们还引入了一个新的 subgroup-AUROC（sAUROC）指标，用于衡量机器学习的公平性。</li>
<li>results: 我们的实验发现了“公平法律”（类似于 Transformers 中的“扩大法律”），即训练集中 subgroup 的表达与该 subgroup 内 anomaly detection性能之间存在直线关系。我们的研究还发现了Balanced training data 仍然存在性能差距，并且这些差距可以叠加，使得具有多个不利影响组的主体性能更加低下。<details>
<summary>Abstract</summary>
Background: With the ever-increasing amount of medical imaging data, the demand for algorithms to assist clinicians has amplified. Unsupervised anomaly detection (UAD) models promise to aid in the crucial first step of disease detection. While previous studies have thoroughly explored fairness in supervised models in healthcare, for UAD, this has so far been unexplored.   Methods: In this study, we evaluated how dataset composition regarding subgroups manifests in disparate performance of UAD models along multiple protected variables on three large-scale publicly available chest X-ray datasets. Our experiments were validated using two state-of-the-art UAD models for medical images. Finally, we introduced a novel subgroup-AUROC (sAUROC) metric, which aids in quantifying fairness in machine learning.   Findings: Our experiments revealed empirical "fairness laws" (similar to "scaling laws" for Transformers) for training-dataset composition: Linear relationships between anomaly detection performance within a subpopulation and its representation in the training data. Our study further revealed performance disparities, even in the case of balanced training data, and compound effects that exacerbate the drop in performance for subjects associated with multiple adversely affected groups.   Interpretation: Our study quantified the disparate performance of UAD models against certain demographic subgroups. Importantly, we showed that this unfairness cannot be mitigated by balanced representation alone. Instead, the representation of some subgroups seems harder to learn by UAD models than that of others. The empirical fairness laws discovered in our study make disparate performance in UAD models easier to estimate and aid in determining the most desirable dataset composition.
</details>
<details>
<summary>摘要</summary>
Background: With the ever-increasing amount of medical imaging data, the demand for algorithms to assist clinicians has amplified. Unsupervised anomaly detection (UAD) models promise to aid in the crucial first step of disease detection. While previous studies have thoroughly explored fairness in supervised models in healthcare, for UAD, this has so far been unexplored.Methods: In this study, we evaluated how dataset composition regarding subgroups manifests in disparate performance of UAD models along multiple protected variables on three large-scale publicly available chest X-ray datasets. Our experiments were validated using two state-of-the-art UAD models for medical images. Finally, we introduced a novel subgroup-AUROC (sAUROC) metric, which aids in quantifying fairness in machine learning.Findings: Our experiments revealed empirical "fairness laws" (similar to "scaling laws" for Transformers) for training-dataset composition: Linear relationships between anomaly detection performance within a subpopulation and its representation in the training data. Our study further revealed performance disparities, even in the case of balanced training data, and compound effects that exacerbate the drop in performance for subjects associated with multiple adversely affected groups.Interpretation: Our study quantified the disparate performance of UAD models against certain demographic subgroups. Importantly, we showed that this unfairness cannot be mitigated by balanced representation alone. Instead, the representation of some subgroups seems harder to learn by UAD models than that of others. The empirical fairness laws discovered in our study make disparate performance in UAD models easier to estimate and aid in determining the most desirable dataset composition.Here's the translation in Traditional Chinese:背景：随着医疗影像数据的不断增加，诊断助手需求增加。不监督型异常检测（UAD）模型能帮助在疾病检测的首步中发掘疾病。过去的研究已经对医疗保健领域中监督模型的公平性进行了广泛的探讨，但是对UAD模型仍然是未知的。方法：这次研究中，我们评估了不同子群体的参数影响UAD模型在多个数据库中的表现差异。我们使用了三个公共可用的胸部X射影数据库，并验证了两个现有的UAD模型。最后，我们引入了一个新的子群体AUROC（sAUROC）指标，以便量测机器学习中的公平性。发现：我们的实验发现了关于训练集合的公平性法则（similar to Transformers的推广法则），这些法则表示在一个子population中的异常检测性能和训练集合中的表现之间的直线关系。我们的研究还发现了充分平衡的训练数据中的表现差异，以及两个或多个负面影响的子群体之间的互动效应。解释：我们的研究量化了UAD模型对特定民族子群体的不公平表现。我们发现了，不公平性不能通过充分平衡 alone 来减轻。相反，一些子群体的表现似乎更难由UAD模型学习，而其他子群体的表现则更容易学习。我们在这研究中发现的公平法则使得UAD模型的不公平表现更易估计，并帮助决定最佳的训练集合。
</details></li>
</ul>
<hr>
<h2 id="LAPP-Layer-Adaptive-Progressive-Pruning-for-Compressing-CNNs-from-Scratch"><a href="#LAPP-Layer-Adaptive-Progressive-Pruning-for-Compressing-CNNs-from-Scratch" class="headerlink" title="LAPP: Layer Adaptive Progressive Pruning for Compressing CNNs from Scratch"></a>LAPP: Layer Adaptive Progressive Pruning for Compressing CNNs from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14157">http://arxiv.org/abs/2309.14157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pucheng Zhai, Kailing Guo, Fang Liu, Xiaofen Xing, Xiangmin Xu<br>for:这个论文的目的是提出一种名为层 adaptive progressive pruning（LAPP）的框架，用于快速适应性地减小 convolutional Neural Network（CNN）的计算量。methods:LAPP 框架使用了一种learnable threshold和 FLOPs 约束来控制减小率，并在训练过程中动态更新这些约束，以便适应不同层的重要性分数变化。此外，在减小每层之前，我们还引入了一种轻量级的跳过来提高减小后的表达能力。results:与先前的压缩方法相比，LAPP 框架在多个数据集和后ION 背景上达到了更高的性能提升。例如，在 CIFAR-10 上，我们可以压缩 ResNet-20 到 40.3% 而无需减少精度。 ImageNet 上，我们可以减少 ResNet-18 的 FLOPs 55.6%，同时保持顶部 1 准确率和顶部 5 准确率不变。<details>
<summary>Abstract</summary>
Structured pruning is a commonly used convolutional neural network (CNN) compression approach. Pruning rate setting is a fundamental problem in structured pruning. Most existing works introduce too many additional learnable parameters to assign different pruning rates across different layers in CNN or cannot control the compression rate explicitly. Since too narrow network blocks information flow for training, automatic pruning rate setting cannot explore a high pruning rate for a specific layer. To overcome these limitations, we propose a novel framework named Layer Adaptive Progressive Pruning (LAPP), which gradually compresses the network during initial training of a few epochs from scratch. In particular, LAPP designs an effective and efficient pruning strategy that introduces a learnable threshold for each layer and FLOPs constraints for network. Guided by both task loss and FLOPs constraints, the learnable thresholds are dynamically and gradually updated to accommodate changes of importance scores during training. Therefore the pruning strategy can gradually prune the network and automatically determine the appropriate pruning rates for each layer. What's more, in order to maintain the expressive power of the pruned layer, before training starts, we introduce an additional lightweight bypass for each convolutional layer to be pruned, which only adds relatively few additional burdens. Our method demonstrates superior performance gains over previous compression methods on various datasets and backbone architectures. For example, on CIFAR-10, our method compresses ResNet-20 to 40.3% without accuracy drop. 55.6% of FLOPs of ResNet-18 are reduced with 0.21% top-1 accuracy increase and 0.40% top-5 accuracy increase on ImageNet.
</details>
<details>
<summary>摘要</summary>
“构成式剔除”是一种常用的卷积神经网络（CNN）压缩方法。剔除率设定是 convolutional neural network （CNN）压缩的基本问题。现有大多数的工作将额外的可学习参数引入到不同层的 CNN 中，以便对不同层设置不同的剔除率。另外，一些方法无法明确控制压缩率，或者对于特定层设置过高的剔除率。这些限制使得自动剔除率设定无法充分发挥其效果。为了解决这些问题，我们提出了一个名为层别进行式进行剔除（Layer Adaptive Progressive Pruning，LAPP）的新框架。LAPP 采用了一个可学习的阈值，以及 FLOPs 约束，以便在训练的初期几十轮中逐步压缩网络。具体来说，LAPP 设计了一个高效且可靠的剔除策略，通过在训练过程中逐步更新可学习的阈值，以便适应变化的重要性分数。因此，LAPP 可以逐步剔除网络，并自动决定每个层的适当剔除率。此外，为维护剔除后的表达能力，我们将每个剔除的卷积层加上一个轻量级的辅助路径，这仅增加了一些轻微的负担。我们的方法在多个数据集和背景 arquitectures 上示出了优秀的性能提升。例如，在 CIFAR-10 上，我们将 ResNet-20 压缩到 40.3% without accuracy drop。 ImageNet 上，我们将 ResNet-18 的 FLOPs 压缩到 55.6%，并且跟踪到 0.21% 顶部 1 accuracy increase 和 0.40% top-5 accuracy increase。
</details></li>
</ul>
<hr>
<h2 id="IEBins-Iterative-Elastic-Bins-for-Monocular-Depth-Estimation"><a href="#IEBins-Iterative-Elastic-Bins-for-Monocular-Depth-Estimation" class="headerlink" title="IEBins: Iterative Elastic Bins for Monocular Depth Estimation"></a>IEBins: Iterative Elastic Bins for Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14137">http://arxiv.org/abs/2309.14137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuweishao/iebins">https://github.com/shuweishao/iebins</a></li>
<li>paper_authors: Shuwei Shao, Zhongcai Pei, Xingming Wu, Zhong Liu, Weihai Chen, Zhengguo Li<br>for:* 本研究旨在提出一种基于分类回归的独眼深度估计方法（MDE），用于解决独眼深度估计中的问题。methods:* 提出了一种新的迭代弹性桶（IEBins）技术，用于在多个阶段中进行高质量深度搜索。* 利用了一种 нов的弹性目标桶技术，以适应不同的深度不确定性。results:* 对于KITTI、NYU-Depth-v2和SUN RGB-D数据集进行了广泛的实验，并证明了提出的方法可以超越先前的状态之 искусственный智能。* 源代码可以在<a target="_blank" rel="noopener" href="https://github.com/ShuweiShao/IEBins%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ShuweiShao/IEBins上获取。</a><details>
<summary>Abstract</summary>
Monocular depth estimation (MDE) is a fundamental topic of geometric computer vision and a core technique for many downstream applications. Recently, several methods reframe the MDE as a classification-regression problem where a linear combination of probabilistic distribution and bin centers is used to predict depth. In this paper, we propose a novel concept of iterative elastic bins (IEBins) for the classification-regression-based MDE. The proposed IEBins aims to search for high-quality depth by progressively optimizing the search range, which involves multiple stages and each stage performs a finer-grained depth search in the target bin on top of its previous stage. To alleviate the possible error accumulation during the iterative process, we utilize a novel elastic target bin to replace the original target bin, the width of which is adjusted elastically based on the depth uncertainty. Furthermore, we develop a dedicated framework composed of a feature extractor and an iterative optimizer that has powerful temporal context modeling capabilities benefiting from the GRU-based architecture. Extensive experiments on the KITTI, NYU-Depth-v2 and SUN RGB-D datasets demonstrate that the proposed method surpasses prior state-of-the-art competitors. The source code is publicly available at https://github.com/ShuweiShao/IEBins.
</details>
<details>
<summary>摘要</summary>
《单眼深度估计（MDE）是计算机视觉的基本领域和许多下渠应用的核心技术。近期，一些方法将MDE视为一个分类预测和回传问题，使用线性结合的概率分布和数组中心来预测深度。本文提出一个新的迭代弹性桶（IEBins）概念，用于分类预测和回传问题的MDE。提案的IEBins通过不断地优化搜寻范围，以进行多阶段的精确深度搜寻。为了避免可能的错误累累在迭代过程中，我们使用了一个新的弹性目标桶，其宽度根据深度不确定而调整。此外，我们开发了一个特别的架构，包括一个特征提取器和一个迭代优化器，具有强大的时间统计模型能力，从GRU架构中受益。广泛的实验表明，提案的方法在KITTI、NYU-Depth-v2和SUN RGB-D数据集上具有较高的性能，超过了先前的州际之径。原始代码可以在https://github.com/ShuweiShao/IEBins上取得。
</details></li>
</ul>
<hr>
<h2 id="Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers"><a href="#Masked-Image-Residual-Learning-for-Scaling-Deeper-Vision-Transformers" class="headerlink" title="Masked Image Residual Learning for Scaling Deeper Vision Transformers"></a>Masked Image Residual Learning for Scaling Deeper Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14136">http://arxiv.org/abs/2309.14136</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/russellllaputa/MIRL">https://github.com/russellllaputa/MIRL</a></li>
<li>paper_authors: Guoxi Huang, Hongtao Fu, Adrian G. Bors</li>
<li>for: 这篇研究旨在提高深度向 ViT 的训练过程，并解决深度层中的降低问题。</li>
<li>methods:  authors propose a self-supervised learning framework called Masked Image Residual Learning (MIRL), which significantly alleviates the degradation problem, making it possible to scale ViT along depth for performance upgrade.</li>
<li>results:  authors show that deeper ViTs can be effectively optimized using MIRL, and easily gain accuracy from increased depth. With the same level of computational complexity as ViT-Base and ViT-Large, they instantiate 4.5 times and 2 times deeper ViTs, dubbed ViT-S-54 and ViT-B-48. The deeper ViT-S-54 achieves performance on par with ViT-Large, while ViT-B-48 achieves 86.2% top-1 accuracy on ImageNet.<details>
<summary>Abstract</summary>
Deeper Vision Transformers (ViTs) are more challenging to train. We expose a degradation problem in deeper layers of ViT when using masked image modeling (MIM) for pre-training. To ease the training of deeper ViTs, we introduce a self-supervised learning framework called Masked Image Residual Learning (MIRL), which significantly alleviates the degradation problem, making scaling ViT along depth a promising direction for performance upgrade. We reformulate the pre-training objective for deeper layers of ViT as learning to recover the residual of the masked image. We provide extensive empirical evidence showing that deeper ViTs can be effectively optimized using MIRL and easily gain accuracy from increased depth. With the same level of computational complexity as ViT-Base and ViT-Large, we instantiate 4.5$\times$ and 2$\times$ deeper ViTs, dubbed ViT-S-54 and ViT-B-48. The deeper ViT-S-54, costing 3$\times$ less than ViT-Large, achieves performance on par with ViT-Large. ViT-B-48 achieves 86.2% top-1 accuracy on ImageNet. On one hand, deeper ViTs pre-trained with MIRL exhibit excellent generalization capabilities on downstream tasks, such as object detection and semantic segmentation. On the other hand, MIRL demonstrates high pre-training efficiency. With less pre-training time, MIRL yields competitive performance compared to other approaches.
</details>
<details>
<summary>摘要</summary>
deeper 视图变换器（ViTs）更加具有挑战性，我们揭示了深层 ViT 的降低问题，当使用掩码图像模型（MIM）进行预训练时。为了减轻深层 ViT 的训练困难，我们提出了一种自动学习框架，称为掩码图像差分学习（MIRL），它可以明显减轻降低问题，使深层 ViT 的缩放成为性能升级的可能性。我们重新表述了深层 ViT 的预训练目标，即学习恢复掩码图像中的差异。我们提供了丰富的实验证据，表明深层 ViT 可以通过 MIRL 有效地优化，并且可以轻松地从增加深度中获得性能提升。与 ViT-Base 和 ViT-Large 相同的计算复杂度下，我们实例化了 4.5 $\times$ 和 2 $\times$ 深度更深的 ViTs，称为 ViT-S-54 和 ViT-B-48。深度更深的 ViT-S-54，耗资相当于 ViT-Large 的三倍，却可以与 ViT-Large 的性能一致。ViT-B-48 在 ImageNet 上达到 86.2% 的顶部一 Accuracy。一方面，深度更深的 ViTs 预训练后在下游任务中表现出色，如物体检测和semantic segmentation。另一方面， MIRL 表现出高效的预训练能力，需要更少的预训练时间，却可以与其他方法相比肩并胜。
</details></li>
</ul>
<hr>
<h2 id="SurrogatePrompt-Bypassing-the-Safety-Filter-of-Text-To-Image-Models-via-Substitution"><a href="#SurrogatePrompt-Bypassing-the-Safety-Filter-of-Text-To-Image-Models-via-Substitution" class="headerlink" title="SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution"></a>SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14122">http://arxiv.org/abs/2309.14122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjm1900/surrogateprompt">https://github.com/zjm1900/surrogateprompt</a></li>
<li>paper_authors: Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, Kui Ren</li>
<li>for: The paper is written to address the issue of advanced text-to-image models generating unsafe content, specifically photorealistic NSFW images of political figures.</li>
<li>methods: The paper uses a novel framework called SurrogatePrompt, which utilizes large language models, image-to-text, and image-to-image modules to automate the creation of attack prompts that can bypass the safety filters of closed-source models like Midjourney.</li>
<li>results: The paper demonstrates the success of SurrogatePrompt in generating abundant photorealistic NSFW images of political figures by exploiting vulnerabilities in Midjourney’s proprietary safety filter, with an 88% success rate in bypassing the filter. The generated images are found to present significant safety hazards, both subjectively and objectively.<details>
<summary>Abstract</summary>
Advanced text-to-image models such as DALL-E 2 and Midjourney possess the capacity to generate highly realistic images, raising significant concerns regarding the potential proliferation of unsafe content. This includes adult, violent, or deceptive imagery of political figures. Despite claims of rigorous safety mechanisms implemented in these models to restrict the generation of not-safe-for-work (NSFW) content, we successfully devise and exhibit the first prompt attacks on Midjourney, resulting in the production of abundant photorealistic NSFW images. We reveal the fundamental principles of such prompt attacks and suggest strategically substituting high-risk sections within a suspect prompt to evade closed-source safety measures. Our novel framework, SurrogatePrompt, systematically generates attack prompts, utilizing large language models, image-to-text, and image-to-image modules to automate attack prompt creation at scale. Evaluation results disclose an 88% success rate in bypassing Midjourney's proprietary safety filter with our attack prompts, leading to the generation of counterfeit images depicting political figures in violent scenarios. Both subjective and objective assessments validate that the images generated from our attack prompts present considerable safety hazards.
</details>
<details>
<summary>摘要</summary>
高级文本到图像模型如DALL-E 2和Midjourney具有生成高度真实的图像能力，这引发了严重的安全问题。这包括政治人物的成人、暴力或误导性图像。尽管这些模型的生成不安全内容的安全机制已经实施了严格的安全措施，但我们成功地开发了第一个攻击示例，使得Midjourney生成了大量高度真实的不安全图像。我们揭示了攻击示例的基本原理，并建议在涉及高风险的提示中进行部分替换，以避免关闭源代码安全措施。我们的新框架SurrogatePrompt可以在大规模的攻击提示创建中自动化攻击提示的生成。我们的评估结果表明，使用我们的攻击提示可以在Midjourney的专有安全筛选器中成功绕过88%的攻击，并生成政治人物在暴力场景中的假图像。subjective和objective评估表明，生成的图像具有严重的安全风险。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-autoencoder-based-multimodal-one-class-classification"><a href="#Convolutional-autoencoder-based-multimodal-one-class-classification" class="headerlink" title="Convolutional autoencoder-based multimodal one-class classification"></a>Convolutional autoencoder-based multimodal one-class classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14090">http://arxiv.org/abs/2309.14090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Firas Laakom, Fahad Sohrab, Jenni Raitoharju, Alexandros Iosifidis, Moncef Gabbouj</li>
<li>for: 这种论文是为了提出一种适用于多Modal数据的深度学习一类分类方法，以便在异常检测中使用。</li>
<li>methods: 该方法使用了两个卷积autoencoder并在它们之间进行了 JOINT 训练，以使得输入数据在幂等空间中得到最紧凑的表示。</li>
<li>results: 对于一个多Modal图像分类数据集，该方法的实验结果表明，与单模式方法相比，该方法的多模式方法得到了更好的结果。此外，研究不同的输入图像大小和最新的特征多样性规则izers的影响，并证明这些规则izers可以提高性能。<details>
<summary>Abstract</summary>
One-class classification refers to approaches of learning using data from a single class only. In this paper, we propose a deep learning one-class classification method suitable for multimodal data, which relies on two convolutional autoencoders jointly trained to reconstruct the positive input data while obtaining the data representations in the latent space as compact as possible. During inference, the distance of the latent representation of an input to the origin can be used as an anomaly score. Experimental results using a multimodal macroinvertebrate image classification dataset show that the proposed multimodal method yields better results as compared to the unimodal approach. Furthermore, study the effect of different input image sizes, and we investigate how recently proposed feature diversity regularizers affect the performance of our approach. We show that such regularizers improve performance.
</details>
<details>
<summary>摘要</summary>
一类分类指的是使用单一类型的数据进行学习。在这篇论文中，我们提出了一种适用于多modal数据的深度学习一类分类方法，该方法基于两个卷积 autoencoder 同时进行卷积重构正确的输入数据，并在幂空间中获得数据表示的最短距离。在推理阶段，输入数据的幂空间表示距离原点的距离可以作为异常分数。我们使用多modalmacro生物图像分类 dataset 进行实验，并证明了我们的方法在多modal数据中获得更好的结果，而且比单modal方法更好。此外，我们还研究了不同的输入图像大小的效果，以及最近提出的特征多样化规范化的影响。我们发现这些规范化可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="BoIR-Box-Supervised-Instance-Representation-for-Multi-Person-Pose-Estimation"><a href="#BoIR-Box-Supervised-Instance-Representation-for-Multi-Person-Pose-Estimation" class="headerlink" title="BoIR: Box-Supervised Instance Representation for Multi-Person Pose Estimation"></a>BoIR: Box-Supervised Instance Representation for Multi-Person Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14072">http://arxiv.org/abs/2309.14072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uyoung-jeong/BoIR">https://github.com/uyoung-jeong/BoIR</a></li>
<li>paper_authors: Uyoung Jeong, Seungryul Baek, Hyung Jin Chang, Kwang In Kim</li>
<li>for: 这篇论文是为了提出一种解决多人 pose estimation 下 instances 分解问题的方法，提高了scene中人体pose estimation的性能。</li>
<li>methods: 该方法使用了 bounding box-level instance representation learning，同时解决了人体实例检测、实例分解和实例关键点匹配问题。它还使用了多任务学习，包括底层关键点估计、 bounding box 回归和对比式实例嵌入学习，无需在推理过程中添加额外计算成本。</li>
<li>results: 该方法在 CrowdPose 和 OCHuman 等数据集上达到了领先的性能水平，比如 COCO val (0.8 AP)、COCO test-dev (0.5 AP) 和 CrowdPose (4.9 AP) 等。<details>
<summary>Abstract</summary>
Single-stage multi-person human pose estimation (MPPE) methods have shown great performance improvements, but existing methods fail to disentangle features by individual instances under crowded scenes. In this paper, we propose a bounding box-level instance representation learning called BoIR, which simultaneously solves instance detection, instance disentanglement, and instance-keypoint association problems. Our new instance embedding loss provides a learning signal on the entire area of the image with bounding box annotations, achieving globally consistent and disentangled instance representation. Our method exploits multi-task learning of bottom-up keypoint estimation, bounding box regression, and contrastive instance embedding learning, without additional computational cost during inference. BoIR is effective for crowded scenes, outperforming state-of-the-art on COCO val (0.8 AP), COCO test-dev (0.5 AP), CrowdPose (4.9 AP), and OCHuman (3.5 AP). Code will be available at https://github.com/uyoung-jeong/BoIR
</details>
<details>
<summary>摘要</summary>
单stage多人人体 pose 估计（MPPE）方法已经达到了非常高的性能水平，但现有方法在拥挤场景下无法分离特征。在这篇论文中，我们提出了一种名为BoIR的 bounding box 级别实体表示学习方法，该方法同时解决实体检测、实体分离和实体关键点匹配问题。我们的新的实体嵌入损失提供了图像整体的学习信号，实现了全局一致的和分离的实体表示。我们的方法通过 bottom-up 关键点估计、 bounding box 回归和对比实体嵌入学习 Multi-task learning，不需要额外计算成本在推理过程中。BoIR 在拥挤场景下表现出色，与状态流行的 COCO val (0.8 AP)、COCO test-dev (0.5 AP)、CrowdPose (4.9 AP) 和 OCHuman (3.5 AP) 等测试集上表现出色。代码将在 GitHub 上提供，请参考 https://github.com/uyoung-jeong/BoIR。
</details></li>
</ul>
<hr>
<h2 id="Soft-Mixture-Denoising-Beyond-the-Expressive-Bottleneck-of-Diffusion-Models"><a href="#Soft-Mixture-Denoising-Beyond-the-Expressive-Bottleneck-of-Diffusion-Models" class="headerlink" title="Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models"></a>Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14068">http://arxiv.org/abs/2309.14068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangming Li, Boris van Breugel, Mihaela van der Schaar</li>
<li>for: 这 paper 是 investigate  diffusion models 的表现和假设的，特别是在 backward denoising 方面。</li>
<li>methods: 这 paper 使用 current diffusion models 和 soft mixture denoising (SMD) 方法进行研究。</li>
<li>results: 这 paper 发现 current diffusion models 在 backward denoising 方面存在 expressive bottleneck 和 unbounded errors，而 SMD 方法可以有效地解决这些问题，并在实际应用中表现出优于 diffusion models。<details>
<summary>Abstract</summary>
Because diffusion models have shown impressive performances in a number of tasks, such as image synthesis, there is a trend in recent works to prove (with certain assumptions) that these models have strong approximation capabilities. In this paper, we show that current diffusion models actually have an expressive bottleneck in backward denoising and some assumption made by existing theoretical guarantees is too strong. Based on this finding, we prove that diffusion models have unbounded errors in both local and global denoising. In light of our theoretical studies, we introduce soft mixture denoising (SMD), an expressive and efficient model for backward denoising. SMD not only permits diffusion models to well approximate any Gaussian mixture distributions in theory, but also is simple and efficient for implementation. Our experiments on multiple image datasets show that SMD significantly improves different types of diffusion models (e.g., DDPM), espeically in the situation of few backward iterations.
</details>
<details>
<summary>摘要</summary>
因为扩散模型在多种任务中表现出色，如图像生成等，所以有一些研究者在最近的工作中尝试证明（假设一些）这些模型具有强的近似能力。在这篇论文中，我们发现现有的扩散模型实际上在回卷降噪中存在表达力瓶颈，而现有的理论保证假设太强。基于这个发现，我们证明了扩散模型在本地和全局降噪中有无限大的错误。视我们的理论研究，我们引入了软组合降噪（SMD）模型，这是一种表达力强大且实用的回卷降噪模型。SMD不仅允许扩散模型在理论上能够很好地近似任何高斯混合分布，而且是实用的并简单的实现。我们在多个图像 dataset 上进行了实验，发现SMD可以大幅提高不同类型的扩散模型（如 DDPM），特别是在几个后向迭代中。
</details></li>
</ul>
<hr>
<h2 id="AsymFormer-Asymmetrical-Cross-Modal-Representation-Learning-for-Mobile-Platform-Real-Time-RGB-D-Semantic-Segmentation"><a href="#AsymFormer-Asymmetrical-Cross-Modal-Representation-Learning-for-Mobile-Platform-Real-Time-RGB-D-Semantic-Segmentation" class="headerlink" title="AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation"></a>AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14065">http://arxiv.org/abs/2309.14065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Fourier7754/AsymFormer">https://github.com/Fourier7754/AsymFormer</a></li>
<li>paper_authors: Siqi Du, Weixi Wang, Renzhong Guo, Shengjun Tang</li>
<li>for: 实现RGB-D semantic segmentation的高效精准算法，以提高 робо类智能系统的可靠性和效率。</li>
<li>methods: 提出了一种新的非对称网络AsymFormer，通过优化计算资源分布和非对称背bone来实现多模态特征的有效融合。还利用了特征选择和多模态自相似特征EXTRACTION来提高网络精度，而无需增加参数数量，保证实时执行于机器人平台。</li>
<li>results: 在NYUv2和SUNRGBD datasets上进行了测试，AsymFormer与52.0% mIoU在NYUv2和49.1% mIoU在SUNRGBD达到了竞争性的结果。此外，AsymFormer在RTX3090上实现了79 FPS的推理速度，在混合精度量化后达到了65 FPS的推理速度。这些结果表明AsymFormer可以在RGB-D semantic segmentation中寻求高精度和高效率的平衡。<details>
<summary>Abstract</summary>
In the realm of robotic intelligence, achieving efficient and precise RGB-D semantic segmentation is a key cornerstone. State-of-the-art multimodal semantic segmentation methods, primarily rooted in symmetrical skeleton networks, find it challenging to harmonize computational efficiency and precision. In this work, we propose AsymFormer, a novel network for real-time RGB-D semantic segmentation, which targets the minimization of superfluous parameters by optimizing the distribution of computational resources and introduces an asymmetrical backbone to allow for the effective fusion of multimodal features. Furthermore, we explore techniques to bolster network accuracy by redefining feature selection and extracting multi-modal self-similarity features without a substantial increase in the parameter count, thereby ensuring real-time execution on robotic platforms. Additionally, a Local Attention-Guided Feature Selection (LAFS) module is used to selectively fuse features from different modalities by leveraging their dependencies. Subsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding (CMA) module is introduced to further extract cross-modal representations. This method is evaluated on NYUv2 and SUNRGBD datasets, with AsymFormer demonstrating competitive results with 52.0% mIoU on NYUv2 and 49.1% mIoU on SUNRGBD. Notably, AsymFormer achieves an inference speed of 65 FPS and after implementing mixed precision quantization, it attains an impressive inference speed of 79 FPS on RTX3090. This significantly outperforms existing multi-modal methods, thereby demonstrating that AsymFormer can strike a balance between high accuracy and efficiency for RGB-D semantic segmentation.
</details>
<details>
<summary>摘要</summary>
在机器人智能领域，实现高效精准的 RGB-D semantic segmentation 是一个关键的核心。现有的多Modal semantic segmentation 方法，主要基于对称骨网络，很难实现计算效率和准确之间的平衡。在这个工作中，我们提出了 AsymFormer，一种新的网络，旨在最小化无用参数，通过分布计算资源的优化，并引入非对称脊梁，以有效地融合多Modal 特征。此外，我们还 explore 了提高网络准确性的技术，包括重新定义特征选择和提取多Modal 自相似特征，而无需增加参数数量，以确保实时执行于机器人平台。另外，我们还使用 Local Attention-Guided Feature Selection (LAFS) 模块， selecing 不同模式的特征，并使用 Cross-Modal Attention-Guided Feature Correlation Embedding (CMA) 模块，进一步提取交Modal 表示。这种方法在 NYUv2 和 SUNRGBD 数据集上进行评估，AsymFormer Displaying competitive results with 52.0% mIoU on NYUv2 and 49.1% mIoU on SUNRGBD。含义的是，AsymFormer 在实时执行中能够平衡高准确率和高效率，为 RGB-D semantic segmentation 提供了一个可靠的解决方案。
</details></li>
</ul>
<hr>
<h2 id="FeCAM-Exploiting-the-Heterogeneity-of-Class-Distributions-in-Exemplar-Free-Continual-Learning"><a href="#FeCAM-Exploiting-the-Heterogeneity-of-Class-Distributions-in-Exemplar-Free-Continual-Learning" class="headerlink" title="FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning"></a>FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14062">http://arxiv.org/abs/2309.14062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dipamgoswami/fecam">https://github.com/dipamgoswami/fecam</a></li>
<li>paper_authors: Dipam Goswami, Yuyang Liu, Bartłomiej Twardowski, Joost van de Weijer</li>
<li>for: 这篇研究探讨了类别增执行（Continual Learning）中的概念增执行（Class-Incremental Learning），尤其是在没有回溯的情况下，因为这会受到严重的遗传问题的影响。</li>
<li>methods: 这篇研究使用了 prototype 网络，它们生成了新的类别标本，并使用封锁的特征提取器来分类特征基于 Euclidian 距离。</li>
<li>results: 研究发现，在统计学上分布的分类是成功的，但是在学习非站势数据时，Euclidian 距离是不理想的，并且特征分布是多样的。因此，这篇研究提出了使用不规律 Mahalanobis 距离来解决这个挑战。此外，这篇研究还证明了模型特征相关性比之前的对应样本从正常分布中采样和训练线性分类器的方法更好。相比于现有的方法，这篇研究的方法可以在多个shot CIL 设定下进行普遍化，并且在领域增执行设定下也能够取得顶尖的结果。<details>
<summary>Abstract</summary>
Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is better than previous attempts at sampling features from normal distributions and training a linear classifier. Unlike existing methods, our approach generalizes to both many- and few-shot CIL settings, as well as to domain-incremental settings. Interestingly, without updating the backbone network, our method obtains state-of-the-art results on several standard continual learning benchmarks. Code is available at https://github.com/dipamgoswami/FeCAM.
</details>
<details>
<summary>摘要</summary>
例子空间自适应学习（CIL）具有一些挑战，因为它禁止在前一个任务中重新训练数据，从而导致忘记现象。最近的途径是通过冻结特征提取器来逐步学习类别器，这些途径在这篇论文中得到了广泛的关注。我们在这篇论文中探讨了示例网络，它们通过冻结特征提取器生成新的类 prototype，并基于Euclidean距离来类别特征。我们在类别分布分析中表明，基于Euclidean距离的分类是适用于共同训练的特征。然而，当学习非站点数据时，我们发现Euclidean距离是不优的，特征分布是不均匀的。为了解决这个挑战，我们再次考虑了不规则的Mahalanobis距离。此外，我们验证了模型特征 covariance 关系的模型化是比前一些尝试在normal分布上随机抽取特征并训练线性分类器更好。与现有方法不同的是，我们的方法可以在多少shot CIL设定下和领域增量学习设定下普遍适用，并且不需要更新后台网络，我们的方法在多个标准的不断学习标准benchmark上获得了状态机器人的结果。代码可以在https://github.com/dipamgoswami/FeCAM中找到。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Semantic-Segmentation-by-Knowledge-Graph-Inference"><a href="#Weakly-Supervised-Semantic-Segmentation-by-Knowledge-Graph-Inference" class="headerlink" title="Weakly Supervised Semantic Segmentation by Knowledge Graph Inference"></a>Weakly Supervised Semantic Segmentation by Knowledge Graph Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14057">http://arxiv.org/abs/2309.14057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jia-zhang666/grm_layer">https://github.com/jia-zhang666/grm_layer</a></li>
<li>paper_authors: Jia Zhang, Bo Peng, Xi Wu</li>
<li>for: 提高弱监督semantic segmentation（WSSS）的性能，特别是在Convolutional Neural Networks（CNNs）上。</li>
<li>methods: 提出了一种基于图reasoning的方法，通过同时提高多类划分网络和 segmentation network的两个阶段，提高WSSS的整体性能。在多类划分网络阶段，外部知识被 интеGRATED，并与GCNs结合，以全局理解图像中的inter-class依赖关系。在 segmentation network阶段，提出了一种Graph Reasoning Mapping（GRM）模块，使用文本数据库来挖掘图像区域中的semantic coherence，以提高特征表示。</li>
<li>results: 使用图像级别的supervision，在PASCAL VOC 2012和MS-COCO datasets上实现了WSSS的状态oa-level性能。经验表明，提出的图reasoning方法有效地提高了WSSS的性能。<details>
<summary>Abstract</summary>
Currently, existing efforts in Weakly Supervised Semantic Segmentation (WSSS) based on Convolutional Neural Networks (CNNs) have predominantly focused on enhancing the multi-label classification network stage, with limited attention given to the equally important downstream segmentation network. Furthermore, CNN-based local convolutions lack the ability to model the extensive inter-category dependencies. Therefore, this paper introduces a graph reasoning-based approach to enhance WSSS. The aim is to improve WSSS holistically by simultaneously enhancing both the multi-label classification and segmentation network stages. In the multi-label classification network segment, external knowledge is integrated, coupled with GCNs, to globally reason about inter-class dependencies. This encourages the network to uncover features in non-salient regions of images, thereby refining the completeness of generated pseudo-labels. In the segmentation network segment, the proposed Graph Reasoning Mapping (GRM) module is employed to leverage knowledge obtained from textual databases, facilitating contextual reasoning for class representation within image regions. This GRM module enhances feature representation in high-level semantics of the segmentation network's local convolutions, while dynamically learning semantic coherence for individual samples. Using solely image-level supervision, we have achieved state-of-the-art performance in WSSS on the PASCAL VOC 2012 and MS-COCO datasets. Extensive experimentation on both the multi-label classification and segmentation network stages underscores the effectiveness of the proposed graph reasoning approach for advancing WSSS.
</details>
<details>
<summary>摘要</summary>
现有的弱监督Semantic Segmentation（WSSS）基于Convolutional Neural Networks（CNNs）的努力主要集中在增强多标签分类网络阶段，忽略了下游分类网络的 equally important 部分。此外，CNN-based 本地卷积缺乏模型图像中的广泛交互类关系。因此，本文提出了基于图 reasoning的方法，以提高 WSSS。目标是通过同时提高多标签分类和分类网络两个阶段来改进 WSSS。在多标签分类网络段，外部知识被集成，与GCNs相结合，以全局理解图像中的交互类关系。这使得网络可以在非关键区域找到特征，从而提高生成的 Pseudo-labels 的完整性。在分类网络段，我们提出的 Graph Reasoning Mapping（GRM）模块，使用文本数据库中的知识，以便在图像区域内进行 Contextual Reasoning 。这个 GRM 模块可以增强分类网络的高级 semantics 表示，同时动态学习个体样本的 semantic coherence。通过仅使用图像水平监督，我们在 PASCAL VOC 2012 和 MS-COCO 数据集上实现了 WSSS 的最新状态。广泛的实验表明，提出的图 reasoning 方法有效地提高 WSSS。
</details></li>
</ul>
<hr>
<h2 id="Single-Image-Test-Time-Adaptation-for-Segmentation"><a href="#Single-Image-Test-Time-Adaptation-for-Segmentation" class="headerlink" title="Single Image Test-Time Adaptation for Segmentation"></a>Single Image Test-Time Adaptation for Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14052">http://arxiv.org/abs/2309.14052</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klarajanouskova/SITTA-Segmentation">https://github.com/klarajanouskova/SITTA-Segmentation</a></li>
<li>paper_authors: Klara Janouskova, Tamir Shor, Chaim Baskin, Jiri Matas</li>
<li>for: 这项研究旨在提高深度神经网络对领域变化的Robustness，并在图像分类或分割任务上实现这一目标。</li>
<li>methods: 这项研究使用Test-Time Adaptation（TTA）方法，并且特点在于使用自动生成的mask来进行验证。</li>
<li>results: 研究发现，通过在测试时间使用自动生成的mask来优化自我超vised损失，可以提高图像分割模型的Robustness。在不同的条件下，这些改进可以提高模型的性能，相比之下，不使用这些改进，提高的性能仅为1.7%和2.16%。<details>
<summary>Abstract</summary>
Test-Time Adaptation (TTA) methods improve the robustness of deep neural networks to domain shift on a variety of tasks such as image classification or segmentation. This work explores adapting segmentation models to a single unlabelled image with no other data available at test-time. In particular, this work focuses on adaptation by optimizing self-supervised losses at test-time. Multiple baselines based on different principles are evaluated under diverse conditions and a novel adversarial training is introduced for adaptation with mask refinement. Our additions to the baselines result in a 3.51 and 3.28 % increase over non-adapted baselines, without these improvements, the increase would be 1.7 and 2.16 % only.
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）方法可以提高深度神经网络对频率差shift的Robustness在各种任务上，如图像分类或分割。这项工作探讨在没有其他数据可用的情况下，使用单个无标注图像进行适应。具体来说，这项工作关注在测试时优化自我指导损失来进行适应。我们评估了基于不同原则的多个基准，并引入了一种新的对适应进行干扰训练。我们的改进对基准而言，导致了3.51%和3.28%的提高，如果没有这些改进，则只有1.7%和2.16%的提高。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Fairness-Biases-in-Deep-Learning-Based-Brain-MRI-Reconstruction"><a href="#Unveiling-Fairness-Biases-in-Deep-Learning-Based-Brain-MRI-Reconstruction" class="headerlink" title="Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction"></a>Unveiling Fairness Biases in Deep Learning-Based Brain MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14392">http://arxiv.org/abs/2309.14392</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ydu0117/reconfairness">https://github.com/ydu0117/reconfairness</a></li>
<li>paper_authors: Yuning Du, Yuyang Xue, Rohan Dharmakumar, Sotirios A. Tsaftaris</li>
<li>for: 这个研究旨在检查深度学习（DL）重建的脑磁共振成像中是否存在不公正性问题，以及如何通过不同的方法来解决这个问题。</li>
<li>methods: 这个研究使用的方法包括U-Net架构来重建图像，并对基eline Empirical Risk Minimisation（ERM）和重新均衡策略进行实现，以检测和解决不公正性问题。</li>
<li>results: 研究发现，DL重建模型存在男女和年龄 subgroup的性别偏见，但不是由数据不均衡和训练歧视引起的。这些发现可以帮助我们更好地理解深度学习图像重建中的不公正性问题，并为医疗AI应用中的公平性提供更多的参考。<details>
<summary>Abstract</summary>
Deep learning (DL) reconstruction particularly of MRI has led to improvements in image fidelity and reduction of acquisition time. In neuroimaging, DL methods can reconstruct high-quality images from undersampled data. However, it is essential to consider fairness in DL algorithms, particularly in terms of demographic characteristics. This study presents the first fairness analysis in a DL-based brain MRI reconstruction model. The model utilises the U-Net architecture for image reconstruction and explores the presence and sources of unfairness by implementing baseline Empirical Risk Minimisation (ERM) and rebalancing strategies. Model performance is evaluated using image reconstruction metrics. Our findings reveal statistically significant performance biases between the gender and age subgroups. Surprisingly, data imbalance and training discrimination are not the main sources of bias. This analysis provides insights of fairness in DL-based image reconstruction and aims to improve equity in medical AI applications.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）重建特别是MRI的场景下，已经导致图像准确性的提高和数据采样时间的减少。在神经成像中，DL方法可以从不充分的数据中重建高质量图像。然而，是必须考虑公平性在DL算法中，特别是在人口特征方面。本研究提供了首次DL基于脑MRI重建模型的公平分析。该模型采用U-Net架构 для图像重建，并实施基eline Empirical Risk Minimization（ERM）和重新填充策略来探讨不公正的存在和来源。模型的性能被评估使用图像重建指标。我们的发现表明男女和年龄子集之间存在 statistically significant的性能偏好。意外地，数据不均衡和训练歧视并不是主要的偏好来源。这种分析提供了DL基于图像重建中的公平性的视角，并旨在提高医疗AI应用中的公平性。
</details></li>
</ul>
<hr>
<h2 id="Hashing-Neural-Video-Decomposition-with-Multiplicative-Residuals-in-Space-Time"><a href="#Hashing-Neural-Video-Decomposition-with-Multiplicative-Residuals-in-Space-Time" class="headerlink" title="Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time"></a>Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14022">http://arxiv.org/abs/2309.14022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Hung Chan, Cheng-Yang Yuan, Cheng Sun, Hwann-Tzong Chen</li>
<li>for: 这个研究旨在提供一种基于层次编辑的视频分解方法，以便在视频中进行快速、高质量的编辑。</li>
<li>methods: 该方法使用神经网络模型将输入视频分解成多个层次表示，每个层包括一个2D текстура地图、原始视频的遮盾、和表示视频中的空间时间变化的乘法差异。该方法可以快速地在单个GPU上学习神经表示，并在实时渲染编辑结果。</li>
<li>results: 该方法可以在不同视频中生成高质量的编辑效果，并且可以在实时进行编辑。同时，该方法还提出了一种基于特征跟踪的评价指标，以对视频编辑效果进行 объектив评估。<details>
<summary>Abstract</summary>
We present a video decomposition method that facilitates layer-based editing of videos with spatiotemporally varying lighting and motion effects. Our neural model decomposes an input video into multiple layered representations, each comprising a 2D texture map, a mask for the original video, and a multiplicative residual characterizing the spatiotemporal variations in lighting conditions. A single edit on the texture maps can be propagated to the corresponding locations in the entire video frames while preserving other contents' consistencies. Our method efficiently learns the layer-based neural representations of a 1080p video in 25s per frame via coordinate hashing and allows real-time rendering of the edited result at 71 fps on a single GPU. Qualitatively, we run our method on various videos to show its effectiveness in generating high-quality editing effects. Quantitatively, we propose to adopt feature-tracking evaluation metrics for objectively assessing the consistency of video editing. Project page: https://lightbulb12294.github.io/hashing-nvd/
</details>
<details>
<summary>摘要</summary>
我们提出了一种视频分解方法，该方法使得可以对视频进行层次编辑，包括空间时间变化的灯光效果。我们的神经网络模型将输入视频分解成多个层次表示，每个层包括一个2D текстура地图、原始视频的遮盾和表示空间时间变化的乘法差异。通过修改Texture Maps可以在整个视频帧中快速传播修改，而保持其他内容的一致性。我们的方法可以快速学习视频层次神经表示，并在单个GPU上实现实时渲染修改后的结果，每帧25s。我们对多个视频进行了质量评估，并提出了一种基于特征跟踪的评价指标来客观评估视频编辑的一致性。项目页面：https://lightbulb12294.github.io/hashing-nvd/Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Variational-Inference-for-Scalable-3D-Object-centric-Learning"><a href="#Variational-Inference-for-Scalable-3D-Object-centric-Learning" class="headerlink" title="Variational Inference for Scalable 3D Object-centric Learning"></a>Variational Inference for Scalable 3D Object-centric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14010">http://arxiv.org/abs/2309.14010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Wang, Kee Siong Ng, Miaomiao Liu</li>
<li>for: 学习3D场景中对象-центric的无监督表示学习，以适应更大的场景。</li>
<li>methods: 分别学习对象姿态和外观表示，并将对象表示升级到视图不变的形式，以保持对象身份。使用渐进变型推断过程可以处理顺序输入和在线更新对象热度分布。</li>
<li>results: 实验表明，我们提出的方法可以在synthetic和实际数据集上INFER和维护3D场景中对象-центric的表示，并比前一代模型表现更好。<details>
<summary>Abstract</summary>
We tackle the task of scalable unsupervised object-centric representation learning on 3D scenes. Existing approaches to object-centric representation learning show limitations in generalizing to larger scenes as their learning processes rely on a fixed global coordinate system. In contrast, we propose to learn view-invariant 3D object representations in localized object coordinate systems. To this end, we estimate the object pose and appearance representation separately and explicitly map object representations across views while maintaining object identities. We adopt an amortized variational inference pipeline that can process sequential input and scalably update object latent distributions online. To handle large-scale scenes with a varying number of objects, we further introduce a Cognitive Map that allows the registration and query of objects on a per-scene global map to achieve scalable representation learning. We explore the object-centric neural radiance field (NeRF) as our 3D scene representation, which is jointly modeled within our unsupervised object-centric learning framework. Experimental results on synthetic and real datasets show that our proposed method can infer and maintain object-centric representations of 3D scenes and outperforms previous models.
</details>
<details>
<summary>摘要</summary>
我们面临三维场景上可扩展无监督物体中心表示学习的挑战。现有的物体中心表示学习方法在扩大到更大的场景时存在限制，因为它们的学习过程基于固定的全球坐标系。与此相反，我们提议通过分离出视图不变的3D物体表示和维护物体标识来学习视图不变的3D物体表示。我们采用了一个缓存梯度推导管道，可以处理顺序输入并在线更新物体热度分布。为了处理大规模场景中的变化数量物体，我们还引入了认知地图，允许在每个场景全球地图上注册和查询物体，以实现可扩展的表示学习。我们采用物体中心场景灯光场（NeRF）作为我们的三维场景表示，并将其与我们的无监督物体中心学习框架集成。实验结果表明，我们的提议方法可以推断和维护三维场景中的物体中心表示，并在现实数据集上超越先前的模型。
</details></li>
</ul>
<hr>
<h2 id="Better-Generalization-of-White-Matter-Tract-Segmentation-to-Arbitrary-Datasets-with-Scaled-Residual-Bootstrap"><a href="#Better-Generalization-of-White-Matter-Tract-Segmentation-to-Arbitrary-Datasets-with-Scaled-Residual-Bootstrap" class="headerlink" title="Better Generalization of White Matter Tract Segmentation to Arbitrary Datasets with Scaled Residual Bootstrap"></a>Better Generalization of White Matter Tract Segmentation to Arbitrary Datasets with Scaled Residual Bootstrap</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13980">http://arxiv.org/abs/2309.13980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wan Liu, Chuyang Ye</li>
<li>for: 提高 diffusion magnetic resonance imaging（dMRI）white matter（WM）轨迹分割的泛化性能。</li>
<li>methods: 使用降噪 bootstrap 缩放策略对培oki数据进行增强，以适应不同测试数据集的分布误差。</li>
<li>results: 对两个 dMRI 数据集进行实验，结果表明，提出的方法可以在不同的设置下提高 WM 轨迹分割的泛化性能。<details>
<summary>Abstract</summary>
White matter (WM) tract segmentation is a crucial step for brain connectivity studies. It is performed on diffusion magnetic resonance imaging (dMRI), and deep neural networks (DNNs) have achieved promising segmentation accuracy. Existing DNN-based methods use an annotated dataset for model training. However, the performance of the trained model on a different test dataset may not be optimal due to distribution shift, and it is desirable to design WM tract segmentation approaches that allow better generalization of the segmentation model to arbitrary test datasets. In this work, we propose a WM tract segmentation approach that improves the generalization with scaled residual bootstrap. The difference between dMRI scans in training and test datasets is most noticeably caused by the different numbers of diffusion gradients and noise levels. Since both of them lead to different signal-to-noise ratios (SNRs) between the training and test data, we propose to augment the training scans by adjusting the noise magnitude and develop an adapted residual bootstrap strategy for the augmentation. To validate the proposed approach, two dMRI datasets were used, and the experimental results show that our method consistently improved the generalization of WM tract segmentation under various settings.
</details>
<details>
<summary>摘要</summary>
白 matter（WM）轨迹分 segmentation 是脑连接研究的关键步骤。它通过 diffusion magnetic resonance imaging（dMRI）进行，并使用深度神经网络（DNNs）获得了有前途的分 segmentation 精度。现有的 DNN-based 方法通常使用标注数据集进行模型训练。然而，训练模型在不同的测试数据集上的性能可能不是最佳，因为分布转移，而且您想要设计 WM tract segmentation 方法，可以更好地将分 segmentation 模型应用于任何测试数据集。在这项工作中，我们提出了一种 WM tract segmentation 方法，可以提高分 segmentation 模型的泛化性。我们通过扩大 residual bootstrap 来实现这一点。测试和训练数据集之间的主要差异在于增强和噪声水平的不同，这两个因素都会导致测试和训练数据集之间的信号噪声比（SNR）的不同。因此，我们提议在训练数据集上进行噪声调整，并开发了适应的 residual bootstrap 策略。为验证我们的方法，我们使用了两个 dMRI 数据集，并实验结果表明，我们的方法可以在不同的设置下提高 WM tract segmentation 的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Semantic-Image-Editing-with-Style-Codes"><a href="#Diverse-Semantic-Image-Editing-with-Style-Codes" class="headerlink" title="Diverse Semantic Image Editing with Style Codes"></a>Diverse Semantic Image Editing with Style Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13975">http://arxiv.org/abs/2309.13975</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hakansivuk/divsem">https://github.com/hakansivuk/divsem</a></li>
<li>paper_authors: Hakan Sivuk, Aysegul Dundar<br>for: 这个论文的目的是提出一种基于semantic map的Semantic Image Editing方法，可以填充杀死图像中的 pixels，同时保持图像的内在逻辑和外在逻辑的一致性。methods: 该方法使用了一种新的Style Encoding机制，可以区分可见和部分可见对象的风格编码，从而提高了图像的一致性和多样性。results: 对比之前的条件图像生成和Semantic Image Editing算法，该方法在评价指标上具有显著的改进，并且可以提供更多的多样化结果。<details>
<summary>Abstract</summary>
Semantic image editing requires inpainting pixels following a semantic map. It is a challenging task since this inpainting requires both harmony with the context and strict compliance with the semantic maps. The majority of the previous methods proposed for this task try to encode the whole information from erased images. However, when an object is added to a scene such as a car, its style cannot be encoded from the context alone. On the other hand, the models that can output diverse generations struggle to output images that have seamless boundaries between the generated and unerased parts. Additionally, previous methods do not have a mechanism to encode the styles of visible and partially visible objects differently for better performance. In this work, we propose a framework that can encode visible and partially visible objects with a novel mechanism to achieve consistency in the style encoding and final generations. We extensively compare with previous conditional image generation and semantic image editing algorithms. Our extensive experiments show that our method significantly improves over the state-of-the-art. Our method not only achieves better quantitative results but also provides diverse results. Please refer to the project web page for the released code and demo: https://github.com/hakansivuk/DivSem.
</details>
<details>
<summary>摘要</summary>
semantic image editing 需要根据semantic map进行像素填充。这是一项复杂的任务，因为这种填充需要与上下文相协调，并且必须严格遵循semantic map。大多数之前的方法都尝试从抹除图像中提取整个信息。然而，当一个 objet 如车加入场景时，其样式不能从上下文中提取。相反，模型们可以输出多个生成物，但是这些生成物往往与不抹除部分之间没有滑块。此外，之前的方法没有机制来对可见和部分可见对象的样式进行不同的编码，以提高性能。在这种情况下，我们提出了一个框架，可以对可见和部分可见对象进行novel机制来实现样式编码的一致性和最终生成的一致性。我们与前期的条件图像生成和semantic image editing算法进行了广泛的比较。我们的广泛实验表明，我们的方法在状态前的性能上显著提高。我们不仅获得了更好的量化结果，还提供了更多的多样性。请参考项目网页获取代码和示例：https://github.com/hakansivuk/DivSem。
</details></li>
</ul>
<hr>
<h2 id="Egocentric-RGB-Depth-Action-Recognition-in-Industry-Like-Settings"><a href="#Egocentric-RGB-Depth-Action-Recognition-in-Industry-Like-Settings" class="headerlink" title="Egocentric RGB+Depth Action Recognition in Industry-Like Settings"></a>Egocentric RGB+Depth Action Recognition in Industry-Like Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13962">http://arxiv.org/abs/2309.13962</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jkini/Meccano">https://github.com/jkini/Meccano</a></li>
<li>paper_authors: Jyoti Kini, Sarah Fleischer, Ishan Dave, Mubarak Shah</li>
<li>for: 本研究旨在提高 egocentric 视角下的人机交互行为识别率，通过利用RGB和深度模式。</li>
<li>methods: 我们采用3D Video SWIN Transformer来有效地编码RGB和深度模式，并提出了一种基于exponentially decaying variant of the focal loss modulating factor的训练策略，以及late fusion来结合两种模式的预测结果。</li>
<li>results: 我们在MECCANO dataset上进行了广泛的评估，并在多模式人体动作识别挑战中获得了优秀成绩，其中包括在ICIAP 2023 多模式人体动作识别比赛中获得第一名。<details>
<summary>Abstract</summary>
Action recognition from an egocentric viewpoint is a crucial perception task in robotics and enables a wide range of human-robot interactions. While most computer vision approaches prioritize the RGB camera, the Depth modality - which can further amplify the subtleties of actions from an egocentric perspective - remains underexplored. Our work focuses on recognizing actions from egocentric RGB and Depth modalities in an industry-like environment. To study this problem, we consider the recent MECCANO dataset, which provides a wide range of assembling actions. Our framework is based on the 3D Video SWIN Transformer to encode both RGB and Depth modalities effectively. To address the inherent skewness in real-world multimodal action occurrences, we propose a training strategy using an exponentially decaying variant of the focal loss modulating factor. Additionally, to leverage the information in both RGB and Depth modalities, we opt for late fusion to combine the predictions from each modality. We thoroughly evaluate our method on the action recognition task of the MECCANO dataset, and it significantly outperforms the prior work. Notably, our method also secured first place at the multimodal action recognition challenge at ICIAP 2023.
</details>
<details>
<summary>摘要</summary>
egocentric 视角下的行为识别是机器人学中的一项重要感知任务，它允许机器人与人类进行广泛的互动。而大多数计算机视觉方法强调RGB摄像头，但深度modalidad - 可以进一步强调 egocentric 视角下的动作细节 - 仍然未得到充分的利用。我们的工作是 Egocentric RGB 和深度modalities 上的动作识别在行业环境中进行研究。为了研究这个问题，我们使用了最近的MECCANO dataset，该集合提供了许多精心搭建的动作。我们的框架基于3D Video SWIN Transformer来有效地编码RGB和深度modalities。为了解决实际世界中多Modal 动作的自然偏见，我们提出了一种使用加速式衰减的焦点损失模块化因子来进行训练策略。此外，我们选择了将RGB和深度modalities 的预测结果进行融合，以利用每个模式中的信息。我们对MECCANO dataset上的动作识别任务进行了严格的评估，并证明了我们的方法在此任务上明显超越了先前的工作。值得注意的是，我们的方法还在ICIAP 2023 多模态动作识别挑战中获得了第一名。
</details></li>
</ul>
<hr>
<h2 id="In-Domain-GAN-Inversion-for-Faithful-Reconstruction-and-Editability"><a href="#In-Domain-GAN-Inversion-for-Faithful-Reconstruction-and-Editability" class="headerlink" title="In-Domain GAN Inversion for Faithful Reconstruction and Editability"></a>In-Domain GAN Inversion for Faithful Reconstruction and Editability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13956">http://arxiv.org/abs/2309.13956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiapeng Zhu, Yujun Shen, Yinghao Xu, Deli Zhao, Qifeng Chen, Bolei Zhou</li>
<li>for: 这篇论文的目的是提出一种内部类型对应（In-Domain GAN Inversion），将已经预训的GAN模型中的内存空间对应到原始影像，以便实现各种影像修改应用程序，无需重新训练。</li>
<li>methods: 这篇论文使用了域导向encoder和域调整优化器，将内存空间对应到原始影像，并进行了广泛的分析，包括预测器结构、起始对应点和对应参数空间的影响，以探索修改性和重建质量之间的贡献。</li>
<li>results: 这篇论文发现，内部类型对应可以将GAN模型中学习的知识应用到实际的影像修改上，并且可以让GAN模型在不需要重新训练的情况下，实现各种影像修改应用程序。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) have significantly advanced image synthesis through mapping randomly sampled latent codes to high-fidelity synthesized images. However, applying well-trained GANs to real image editing remains challenging. A common solution is to find an approximate latent code that can adequately recover the input image to edit, which is also known as GAN inversion. To invert a GAN model, prior works typically focus on reconstructing the target image at the pixel level, yet few studies are conducted on whether the inverted result can well support manipulation at the semantic level. This work fills in this gap by proposing in-domain GAN inversion, which consists of a domain-guided encoder and a domain-regularized optimizer, to regularize the inverted code in the native latent space of the pre-trained GAN model. In this way, we manage to sufficiently reuse the knowledge learned by GANs for image reconstruction, facilitating a wide range of editing applications without any retraining. We further make comprehensive analyses on the effects of the encoder structure, the starting inversion point, as well as the inversion parameter space, and observe the trade-off between the reconstruction quality and the editing property. Such a trade-off sheds light on how a GAN model represents an image with various semantics encoded in the learned latent distribution. Code, models, and demo are available at the project page: https://genforce.github.io/idinvert/.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Speed-Co-Augmentation-for-Unsupervised-Audio-Visual-Pre-training"><a href="#Speed-Co-Augmentation-for-Unsupervised-Audio-Visual-Pre-training" class="headerlink" title="Speed Co-Augmentation for Unsupervised Audio-Visual Pre-training"></a>Speed Co-Augmentation for Unsupervised Audio-Visual Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13942">http://arxiv.org/abs/2309.13942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiangliu Wang, Jianbo Jiao, Yibing Song, Stephen James, Zhan Tong, Chongjian Ge, Pieter Abbeel, Yun-hui Liu</li>
<li>for: 本研究旨在提高无监督音视频预训练。</li>
<li>methods: 我们提议一种新的速度协调增强方法，该方法随机改变音频和视频数据的播放速率。</li>
<li>results: 实验结果表明，我们提议的方法可以明显提高学习的表示。<details>
<summary>Abstract</summary>
This work aims to improve unsupervised audio-visual pre-training. Inspired by the efficacy of data augmentation in visual contrastive learning, we propose a novel speed co-augmentation method that randomly changes the playback speeds of both audio and video data. Despite its simplicity, the speed co-augmentation method possesses two compelling attributes: (1) it increases the diversity of audio-visual pairs and doubles the size of negative pairs, resulting in a significant enhancement in the learned representations, and (2) it changes the strict correlation between audio-visual pairs but introduces a partial relationship between the augmented pairs, which is modeled by our proposed SoftInfoNCE loss to further boost the performance. Experimental results show that the proposed method significantly improves the learned representations when compared to vanilla audio-visual contrastive learning.
</details>
<details>
<summary>摘要</summary>
这项工作目的是提高无监督音视频预训练。受到视觉对冲学习的数据扩充效果启发，我们提议一种新的速度合并方法，该方法随机改变音频和视频数据的播放速度。尽管简单，这种速度合并方法具有两个吸引人的特点：（1）它增加了音视频对的多样性，同时将负对的数据量增加一倍，从而对学习的表示进行了显著提升，和（2）它改变了音视频对的紧密相关性，但是引入了增强对的关系，这种关系由我们提议的SoftInfoNCE损失函数来模型，以进一步提高表示的性能。实验结果表明，我们的方法在无监督音视频对比学习中显著提高了学习的表示。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Recurrent-Grouping-Attention-Network-for-Video-Super-Resolution"><a href="#A-Lightweight-Recurrent-Grouping-Attention-Network-for-Video-Super-Resolution" class="headerlink" title="A Lightweight Recurrent Grouping Attention Network for Video Super-Resolution"></a>A Lightweight Recurrent Grouping Attention Network for Video Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13940">http://arxiv.org/abs/2309.13940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/karlygzhu/rgan">https://github.com/karlygzhu/rgan</a></li>
<li>paper_authors: Yonggui Zhu, Guofang Li</li>
<li>for: 提高视频超分解（VSR）模型的效率和可扩展性。</li>
<li>methods: 提出了一种轻量级循环分组注意力网络，利用前向特征提取模块和后向特征提取模块从两个方向收集时间信息，并提出了一种新的分组机制以高效地收集参照帧和其邻域帧的空间时间信息。</li>
<li>results: 实验表明，我们的模型在多个数据集上达到了当今主流模型的最佳性能。<details>
<summary>Abstract</summary>
Effective aggregation of temporal information of consecutive frames is the core of achieving video super-resolution. Many scholars have utilized structures such as sliding windows and recurrent to gather spatio-temporal information of frames. However, although the performance of the constructed VSR models is improving, the size of the models is also increasing, exacerbating the demand on the equipment. Thus, to reduce the stress on the device, we propose a novel lightweight recurrent grouping attention network. The parameters of this model are only 0.878M, which is much lower than the current mainstream model for studying video super-resolution. We design forward feature extraction module and backward feature extraction module to collect temporal information between consecutive frames from two directions. Moreover, a new grouping mechanism is proposed to efficiently collect spatio-temporal information of the reference frame and its neighboring frames. The attention supplementation module is presented to further enhance the information gathering range of the model. The feature reconstruction module aims to aggregate information from different directions to reconstruct high-resolution features. Experiments demonstrate that our model achieves state-of-the-art performance on multiple datasets.
</details>
<details>
<summary>摘要</summary>
“有效地聚合 consecutiverame 的时间信息是视频超分辨的核心。许多学者们使用 slide window 和 recurrent 结构收集 frame 的空间时间信息。although the constructed VSR models are improving, the size of the models is also increasing, exacerbating the demand on the equipment. Therefore, to reduce the stress on the device, we propose a novel lightweight recurrent grouping attention network. The parameters of this model are only 0.878M, which is much lower than the current mainstream model for studying video super-resolution.我们设计了 forward feature extraction module 和 backward feature extraction module，用于从两个方向收集 consecutive frames 的时间信息。此外，我们还提出了一种新的 grouping mechanism，用于高效地收集 reference frame 和其邻近 frame 的空间时间信息。另外，我们还提出了一种 attention supplementation module，用于进一步增强模型的信息收集范围。最后，我们设计了一个 feature reconstruction module，用于将信息从不同的方向聚合到高分辨特征上。实验表明，我们的模型在多个 dataset 上达到了状态监测性能。”
</details></li>
</ul>
<hr>
<h2 id="Recursive-Counterfactual-Deconfounding-for-Object-Recognition"><a href="#Recursive-Counterfactual-Deconfounding-for-Object-Recognition" class="headerlink" title="Recursive Counterfactual Deconfounding for Object Recognition"></a>Recursive Counterfactual Deconfounding for Object Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13924">http://arxiv.org/abs/2309.13924</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayin Sun, Hong Wang, Qiulei Dong</li>
<li>for: 提高图像识别 tasks 的准确率和泛化能力，特别是对于闭SET和开SET scenario。</li>
<li>methods: 基于counterfactual分析的Recursive Counterfactual Deconfounding（RCD）模型，包括对图像特征、模型预测和干扰因素之间的关系建立和更新。</li>
<li>results: 在closed-set和open-set scenario的识别任务中，提出的RCD模型与11个基eline模型相比，在大多数情况下表现出色，并且可以进一步提高图像识别的准确率和泛化能力。<details>
<summary>Abstract</summary>
Image recognition is a classic and common task in the computer vision field, which has been widely applied in the past decade. Most existing methods in literature aim to learn discriminative features from labeled images for classification, however, they generally neglect confounders that infiltrate into the learned features, resulting in low performances for discriminating test images. To address this problem, we propose a Recursive Counterfactual Deconfounding model for object recognition in both closed-set and open-set scenarios based on counterfactual analysis, called RCD. The proposed model consists of a factual graph and a counterfactual graph, where the relationships among image features, model predictions, and confounders are built and updated recursively for learning more discriminative features. It performs in a recursive manner so that subtler counterfactual features could be learned and eliminated progressively, and both the discriminability and generalization of the proposed model could be improved accordingly. In addition, a negative correlation constraint is designed for alleviating the negative effects of the counterfactual features further at the model training stage. Extensive experimental results on both closed-set recognition task and open-set recognition task demonstrate that the proposed RCD model performs better than 11 state-of-the-art baselines significantly in most cases.
</details>
<details>
<summary>摘要</summary>
RCD 模型包括一个事实图和一个对事实图的对照图，其中包含了图像特征、模型预测和干扰因素之间的关系，通过 recursively 更新和学习更加细致的特征，以提高模型的抗混淆性和泛化能力。此外，为了进一步缓解对对照特征的负面影响，我们在模型训练阶段设计了一种负 correlated 约束。在 closed-set 识别任务和 open-set 识别任务中，我们进行了广泛的实验，结果显示，相比于 11 个基eline，RCD 模型在大多数情况下能够显著提高对象识别的性能。
</details></li>
</ul>
<hr>
<h2 id="Subspace-Aware-Feature-Reconstruction-for-Unsupervised-Anomaly-Localization"><a href="#Subspace-Aware-Feature-Reconstruction-for-Unsupervised-Anomaly-Localization" class="headerlink" title="Subspace-Aware Feature Reconstruction for Unsupervised Anomaly Localization"></a>Subspace-Aware Feature Reconstruction for Unsupervised Anomaly Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13904">http://arxiv.org/abs/2309.13904</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katsuya Hotta, Chao Zhang, Yoshihiro Hagihara, Takuya Akashi</li>
<li>for: 这篇论文旨在提出一个新的 anomaly localization 方法，实现适应 feature approximation。</li>
<li>methods: 方法基于 self-expressive model，learn low-dimensional subspaces，并通过这些 subspaces 重建 feature representation。</li>
<li>results: 实验结果显示，这篇论文的方法可以与现有的 state-of-the-art 方法相比，实现适应 feature approximation，并且只需要少量的 samples。<details>
<summary>Abstract</summary>
Unsupervised anomaly localization, which plays a critical role in industrial manufacturing, is to identify anomalous regions that deviate from patterns established exclusively from nominal samples. Recent mainstream methods focus on approximating the target feature distribution by leveraging embeddings from ImageNet models. However, a common issue in many anomaly localization methods is the lack of adaptability of the feature approximations to specific targets. Consequently, their ability to effectively identify anomalous regions relies significantly on the data coverage provided by the finite resources in a memory bank. In this paper, we propose a novel subspace-aware feature reconstruction framework for anomaly localization. To achieve adaptive feature approximation, our proposed method involves the reconstruction of the feature representation through the self-expressive model designed to learn low-dimensional subspaces. Importantly, the sparsity of the subspace representation contributes to covering feature patterns from the same subspace with fewer resources, leading to a reduction in the memory bank. Extensive experiments across three industrial benchmark datasets demonstrate that our approach achieves competitive anomaly localization performance compared to state-of-the-art methods by adaptively reconstructing target features with a small number of samples.
</details>
<details>
<summary>摘要</summary>
不监督异常定位，在工业生产中扮演关键的角色，是要将异常区域与基于nominal样本所建立的模式进行比较。现今主流方法通常是通过利用ImageNet模型生成的嵌入来近似目标特征分布。然而，许多异常定位方法中的共同问题是特征近似的灵活性不够，这使得它们在特定目标上效果地识别异常区域的能力受到数据库中的finite资源的限制。在本文中，我们提出了一种新的子空间意识激发特征重建框架，用于异常定位。我们的提议方法包括通过自我表达模型学习低维度子空间，以达到适应性的特征近似。重要的是，子空间表示的稀疏性使得从同一个子空间中覆盖特征模式需要 fewer 资源，从而降低数据库。我们在三个工业标准 datasets上进行了广泛的实验，结果表明，我们的方法可以与当前状态OF-the-art方法竞争地实现异常定位，只需要一小数量的样本。
</details></li>
</ul>
<hr>
<h2 id="Bitstream-Corrupted-Video-Recovery-A-Novel-Benchmark-Dataset-and-Method"><a href="#Bitstream-Corrupted-Video-Recovery-A-Novel-Benchmark-Dataset-and-Method" class="headerlink" title="Bitstream-Corrupted Video Recovery: A Novel Benchmark Dataset and Method"></a>Bitstream-Corrupted Video Recovery: A Novel Benchmark Dataset and Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13890">http://arxiv.org/abs/2309.13890</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liutighe/bscv-dataset">https://github.com/liutighe/bscv-dataset</a></li>
<li>paper_authors: Tianyi Liu, Kejun Wu, Yi Wang, Wenyang Liu, Kim-Hui Yap, Lap-Pui Chau</li>
<li>for: 提供了一个大规模的bitstream-corrupted video（BSCV）benchmark，用于解决实世界中的视频损坏问题。</li>
<li>methods: 使用了一个三个参数的视频损坏模型，并提供了一个可替换的video recovery框架，用于评估现有的视频填写方法。</li>
<li>results: 透过评估现有的视频填写方法，发现了这些方法在解决实世界中的视频损坏问题上的限制，并证明了我们的框架在解决这个问题上的优势。<details>
<summary>Abstract</summary>
The past decade has witnessed great strides in video recovery by specialist technologies, like video inpainting, completion, and error concealment. However, they typically simulate the missing content by manual-designed error masks, thus failing to fill in the realistic video loss in video communication (e.g., telepresence, live streaming, and internet video) and multimedia forensics. To address this, we introduce the bitstream-corrupted video (BSCV) benchmark, the first benchmark dataset with more than 28,000 video clips, which can be used for bitstream-corrupted video recovery in the real world. The BSCV is a collection of 1) a proposed three-parameter corruption model for video bitstream, 2) a large-scale dataset containing rich error patterns, multiple corruption levels, and flexible dataset branches, and 3) a plug-and-play module in video recovery framework that serves as a benchmark. We evaluate state-of-the-art video inpainting methods on the BSCV dataset, demonstrating existing approaches' limitations and our framework's advantages in solving the bitstream-corrupted video recovery problem. The benchmark and dataset are released at https://github.com/LIUTIGHE/BSCV-Dataset.
</details>
<details>
<summary>摘要</summary>
过去一代，视频恢复技术得到了大幅度的进步，如视频填充、完善和错误隐藏。然而，这些技术通常通过手动设计的错误面积来模拟缺失内容，因此无法填充实际的视频损害在视频通信（如电子投票、直播和互联网视频）和多媒体证明中。为解决这一问题，我们介绍了 bitstream-corrupted video（BSCV） benchmark，这是世界上第一个以上28,000个视频剪辑为基础的损害视频恢复 benchmark。BSCV包括以下三个组成部分：1）一种提议的三参数损害模型 для视频比特流；2）一个大规模的数据集，包含丰富的错误特征、多个损害水平和灵活的数据支线；3）一个在视频恢复框架中的插件模块，作为 benchmark。我们对现有视频填充方法进行了BSCV dataset上的评估，并证明了我们的框架在解决损害视频恢复问题中的优势。BSCV dataset和 benchmark将于https://github.com/LIUTIGHE/BSCV-Dataset上发布。
</details></li>
</ul>
<hr>
<h2 id="Skip-Connected-Neural-Networks-with-Layout-Graphs-for-Floor-Plan-Auto-Generation"><a href="#Skip-Connected-Neural-Networks-with-Layout-Graphs-for-Floor-Plan-Auto-Generation" class="headerlink" title="Skip-Connected Neural Networks with Layout Graphs for Floor Plan Auto-Generation"></a>Skip-Connected Neural Networks with Layout Graphs for Floor Plan Auto-Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13881">http://arxiv.org/abs/2309.13881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuntaeJ/SkipNet-FloorPlanGen">https://github.com/yuntaeJ/SkipNet-FloorPlanGen</a></li>
<li>paper_authors: Yuntae Jeon, Dai Quoc Tran, Seunghee Park</li>
<li>for: automated and efficient floor plan designs</li>
<li>methods: skip-connected neural networks integrated with layout graphs</li>
<li>results: 93.9 mIoU score in the 1st CVAAD workshop challenge<details>
<summary>Abstract</summary>
With the advent of AI and computer vision techniques, the quest for automated and efficient floor plan designs has gained momentum. This paper presents a novel approach using skip-connected neural networks integrated with layout graphs. The skip-connected layers capture multi-scale floor plan information, and the encoder-decoder networks with GNN facilitate pixel-level probability-based generation. Validated on the MSD dataset, our approach achieved a 93.9 mIoU score in the 1st CVAAD workshop challenge. Code and pre-trained models are publicly available at https://github.com/yuntaeJ/SkipNet-FloorPlanGe.
</details>
<details>
<summary>摘要</summary>
With the advent of AI and computer vision techniques, the quest for automated and efficient floor plan designs has gained momentum. This paper presents a novel approach using skip-connected neural networks integrated with layout graphs. The skip-connected layers capture multi-scale floor plan information, and the encoder-decoder networks with GNN facilitate pixel-level probability-based generation. Validated on the MSD dataset, our approach achieved a 93.9 mIoU score in the 1st CVAAD workshop challenge. Code and pre-trained models are publicly available at https://github.com/yuntaeJ/SkipNet-FloorPlanGe.Here's the translation in Traditional Chinese:随着人工智能和计算机见识技术的发展，自动化和高效的地图设计问题得到了很多关注。这篇论文提出了一种使用跳接连接神经网络与格局图Integrated的新方法。跳接层 capture多値标高图信息，并且与对应的encoder-decoder网络和GNN结合，实现像素级概率基于生成。在MSD dataset上验证，我们的方法实现了93.9 mIoU分数在1st CVAAD工作坊挑战中。代码和预训模型公开可以在https://github.com/yuntaeJ/SkipNet-FloorPlanGe中找到。
</details></li>
</ul>
<hr>
<h2 id="Attention-and-Pooling-based-Sigmoid-Colon-Segmentation-in-3D-CT-images"><a href="#Attention-and-Pooling-based-Sigmoid-Colon-Segmentation-in-3D-CT-images" class="headerlink" title="Attention and Pooling based Sigmoid Colon Segmentation in 3D CT images"></a>Attention and Pooling based Sigmoid Colon Segmentation in 3D CT images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13872">http://arxiv.org/abs/2309.13872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Akizur Rahman, Sonit Singh, Kuruparan Shanmugalingam, Sankaran Iyer, Alan Blair, Praveen Ravindran, Arcot Sowmya</li>
<li>for: 该研究旨在开发一种基于修改的3D U-Net体系结构的深度学习模型，用于从计算机 Tomography（CT）图像中 segments the sigmoid colon。</li>
<li>methods: 该研究使用了Pyramid pooling（PyP）和通道空间压缩和刺激（csSE）等技术来改进模型性能。</li>
<li>results: 结果表明，使用PyP和csSE技术可以提高分割精度，并且 ensemble方法可以提高分割精度。最终，该研究结果表明，基于修改的3D U-Net体系结构是有效的用于 segments the sigmoid colon在CT图像中。<details>
<summary>Abstract</summary>
Segmentation of the sigmoid colon is a crucial aspect of treating diverticulitis. It enables accurate identification and localisation of inflammation, which in turn helps healthcare professionals make informed decisions about the most appropriate treatment options. This research presents a novel deep learning architecture for segmenting the sigmoid colon from Computed Tomography (CT) images using a modified 3D U-Net architecture. Several variations of the 3D U-Net model with modified hyper-parameters were examined in this study. Pyramid pooling (PyP) and channel-spatial Squeeze and Excitation (csSE) were also used to improve the model performance. The networks were trained using manually annotated sigmoid colon. A five-fold cross-validation procedure was used on a test dataset to evaluate the network's performance. As indicated by the maximum Dice similarity coefficient (DSC) of 56.92+/-1.42%, the application of PyP and csSE techniques improves segmentation precision. We explored ensemble methods including averaging, weighted averaging, majority voting, and max ensemble. The results show that average and majority voting approaches with a threshold value of 0.5 and consistent weight distribution among the top three models produced comparable and optimal results with DSC of 88.11+/-3.52%. The results indicate that the application of a modified 3D U-Net architecture is effective for segmenting the sigmoid colon in Computed Tomography (CT) images. In addition, the study highlights the potential benefits of integrating ensemble methods to improve segmentation precision.
</details>
<details>
<summary>摘要</summary>
Segmentation of the sigmoid colon is a crucial aspect of treating diverticulitis. It enables accurate identification and localization of inflammation, which in turn helps healthcare professionals make informed decisions about the most appropriate treatment options. This research presents a novel deep learning architecture for segmenting the sigmoid colon from Computed Tomography (CT) images using a modified 3D U-Net architecture. Several variations of the 3D U-Net model with modified hyper-parameters were examined in this study. Pyramid pooling (PyP) and channel-spatial Squeeze and Excitation (csSE) were also used to improve the model performance. The networks were trained using manually annotated sigmoid colon. A five-fold cross-validation procedure was used on a test dataset to evaluate the network's performance. As indicated by the maximum Dice similarity coefficient (DSC) of 56.92+/-1.42%, the application of PyP and csSE techniques improves segmentation precision. We explored ensemble methods including averaging, weighted averaging, majority voting, and max ensemble. The results show that average and majority voting approaches with a threshold value of 0.5 and consistent weight distribution among the top three models produced comparable and optimal results with DSC of 88.11+/-3.52%. The results indicate that the application of a modified 3D U-Net architecture is effective for segmenting the sigmoid colon in Computed Tomography (CT) images. In addition, the study highlights the potential benefits of integrating ensemble methods to improve segmentation precision.Here's the text in Traditional Chinese:分 segmentation of the sigmoid colon 是 diverticulitis 的一个重要方面，可以精确地识别和localization of inflammation，从而帮助医疗专业人员做出最适当的治疗选择。本研究提出了一个基于 modified 3D U-Net 架构的深度学习模型，用于 Computed Tomography (CT) 影像中的sigmoid colon 分 segmentation。本研究中评估了多种 modified 3D U-Net 模型的不同参数，并使用 Pyramid pooling (PyP) 和 channel-spatial Squeeze and Excitation (csSE) 技术来改善模型性能。模型被训练使用手动标注的sigmoid colon。使用五fold cross-validation 方法进行评估，结果显示，使用 PyP 和 csSE 技术可以提高分 segmentation 精度。我们explored ensemble methods，包括 averaging、weighted averaging、majority voting 和 max ensemble，并发现，使用average 和 majority voting 方法，并设置阈值为 0.5，可以获得最佳结果，DSC 为 88.11+/-3.52%。结果显示，使用 modified 3D U-Net 架构可以有效地分 segmentation sigmoid colon 在 Computed Tomography (CT) 影像中。此外，研究也显示了 ensemble methods 的潜在优化效果。
</details></li>
</ul>
<hr>
<h2 id="On-Calibration-of-Modern-Quantized-Efficient-Neural-Networks"><a href="#On-Calibration-of-Modern-Quantized-Efficient-Neural-Networks" class="headerlink" title="On Calibration of Modern Quantized Efficient Neural Networks"></a>On Calibration of Modern Quantized Efficient Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13866">http://arxiv.org/abs/2309.13866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joey Kuang, Alexander Wong</li>
<li>for: 这个论文探讨了几种 Architecture 和两个 Dataset 上的 Calibration 性能，以及这些 Calibration 性能与精度之间的关系。</li>
<li>methods: 该论文使用了 ShuffleNetv2、GhostNet-VGG 和 MobileOne 三种 Architecture，以及 CIFAR-100 和 PathMNIST 两个 Dataset。它们使用了不同的精度来评估 Calibration 性能。</li>
<li>results: 研究发现，Calibration 性能与精度之间存在相互关系，尤其是在 4 位 activation  режиmé下。GhostNet-VGG 被发现为最为鲁减的 Architecture，能够在低精度下保持比较好的 Calibration 性能。另外，温度 scaling 可以改善 Calibration 错误，但也有一些限制。<details>
<summary>Abstract</summary>
We explore calibration properties at various precisions for three architectures: ShuffleNetv2, GhostNet-VGG, and MobileOne; and two datasets: CIFAR-100 and PathMNIST. The quality of calibration is observed to track the quantization quality; it is well-documented that performance worsens with lower precision, and we observe a similar correlation with poorer calibration. This becomes especially egregious at 4-bit activation regime. GhostNet-VGG is shown to be the most robust to overall performance drop at lower precision. We find that temperature scaling can improve calibration error for quantized networks, with some caveats. We hope that these preliminary insights can lead to more opportunities for explainable and reliable EdgeML.
</details>
<details>
<summary>摘要</summary>
我们研究了不同精度下的准确性质量，对三种架构：ShuffleNetv2、GhostNet-VGG和MobileOne，以及两个 dataset：CIFAR-100和PathMNIST。我们发现，准确性质量与量化质量之间存在直接的相关性，即性能随着精度下降而变差，我们在4比特活动 режиimen中发现了类似的关系。 GhostNet-VGG 显示为低精度下的最高抗性。我们发现了温度Scaling可以改善量化网络的准确性错误，但有一些限制。我们希望这些初步发现可以带来更多的可靠和可解释的EdgeML。
</details></li>
</ul>
<hr>
<h2 id="SuPerPM-A-Large-Deformation-Robust-Surgical-Perception-Framework-Based-on-Deep-Point-Matching-Learned-from-Physical-Constrained-Simulation-Data"><a href="#SuPerPM-A-Large-Deformation-Robust-Surgical-Perception-Framework-Based-on-Deep-Point-Matching-Learned-from-Physical-Constrained-Simulation-Data" class="headerlink" title="SuPerPM: A Large Deformation-Robust Surgical Perception Framework Based on Deep Point Matching Learned from Physical Constrained Simulation Data"></a>SuPerPM: A Large Deformation-Robust Surgical Perception Framework Based on Deep Point Matching Learned from Physical Constrained Simulation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13863">http://arxiv.org/abs/2309.13863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shan Lin, Albert J. Miao, Ali Alabiad, Fei Liu, Kaiyuan Wang, Jingpei Lu, Florian Richter, Michael C. Yip</li>
<li>for: 实现更好的骨盘识别和重建，减少对大幅弯曲的肿瘤进行追踪和重建的误差。</li>
<li>methods: 使用学习型非静态点云匹配来进行数据汇合，以应对大幅弯曲。</li>
<li>results: 在复杂的骨盘运动中，得到了superior的表现，比前一代医疗场景追踪算法更好。<details>
<summary>Abstract</summary>
Manipulation of tissue with surgical tools often results in large deformations that current methods in tracking and reconstructing algorithms have not effectively addressed. A major source of tracking errors during large deformations stems from wrong data association between observed sensor measurements with previously tracked scene. To mitigate this issue, we present a surgical perception framework, SuPerPM, that leverages learning-based non-rigid point cloud matching for data association, thus accommodating larger deformations. The learning models typically require training data with ground truth point cloud correspondences, which is challenging or even impractical to collect in surgical environments. Thus, for tuning the learning model, we gather endoscopic data of soft tissue being manipulated by a surgical robot and then establish correspondences between point clouds at different time points to serve as ground truth. This was achieved by employing a position-based dynamics (PBD) simulation to ensure that the correspondences adhered to physical constraints. The proposed framework is demonstrated on several challenging surgical datasets that are characterized by large deformations, achieving superior performance over state-of-the-art surgical scene tracking algorithms.
</details>
<details>
<summary>摘要</summary>
人体组织的 manipulate 使用手术工具经常会导致大幅变形，现有的跟踪和重建算法并未有效地处理这些变形。主要的跟踪错误源于 incorrect data association between observed sensor measurements with previously tracked scene。为解决这个问题，我们提出了一个手术认知框架，SuPerPM，该框架利用学习基于非RIGID点云匹配来实现数据关联，因此可以满足更大的变形。学习模型通常需要训练数据包含真实的点云对应关系，但在手术环境中收集这些数据是困难或者实际上不可能的。因此，我们为训练学习模型，收集了endoscopic数据 soft tissue being manipulated by a surgical robot，并在不同时间点之间建立了点云对应关系，以供真实的参照。我们使用了位置基于动力学（PBD）模拟来确保这些对应关系遵循物理约束。我们提出的框架在一些具有大幅变形的手术数据集上进行了评测，实现了与当前最佳手术场景跟踪算法相比的更高性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-on-Video-Object-Segmentation-with-Hard-Region-Discovery"><a href="#Adversarial-Attacks-on-Video-Object-Segmentation-with-Hard-Region-Discovery" class="headerlink" title="Adversarial Attacks on Video Object Segmentation with Hard Region Discovery"></a>Adversarial Attacks on Video Object Segmentation with Hard Region Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13857">http://arxiv.org/abs/2309.13857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Li, Yu Zhang, Li Yuan, Jian Zhao, Xianghua Xu, Xiaoqin Zhang</li>
<li>for: 本研究旨在提高视频对象 segmentation 模型的安全性，对抗 adversarial examples 的攻击。</li>
<li>methods: 本研究提出了一种基于 first-frame attack 的对象agnostic adversary，通过探索 easily confused region 来生成具有更强的攻击力的干扰。</li>
<li>results: 实验结果表明，我们的攻击器可以很大程度下降了多种 state-of-the-art video object segmentation 模型的性能。<details>
<summary>Abstract</summary>
Video object segmentation has been applied to various computer vision tasks, such as video editing, autonomous driving, and human-robot interaction. However, the methods based on deep neural networks are vulnerable to adversarial examples, which are the inputs attacked by almost human-imperceptible perturbations, and the adversary (i.e., attacker) will fool the segmentation model to make incorrect pixel-level predictions. This will rise the security issues in highly-demanding tasks because small perturbations to the input video will result in potential attack risks. Though adversarial examples have been extensively used for classification, it is rarely studied in video object segmentation. Existing related methods in computer vision either require prior knowledge of categories or cannot be directly applied due to the special design for certain tasks, failing to consider the pixel-wise region attack. Hence, this work develops an object-agnostic adversary that has adversarial impacts on VOS by first-frame attacking via hard region discovery. Particularly, the gradients from the segmentation model are exploited to discover the easily confused region, in which it is difficult to identify the pixel-wise objects from the background in a frame. This provides a hardness map that helps to generate perturbations with a stronger adversarial power for attacking the first frame. Empirical studies on three benchmarks indicate that our attacker significantly degrades the performance of several state-of-the-art video object segmentation models.
</details>
<details>
<summary>摘要</summary>
“视频对象分割（VOS）在计算机视觉任务中得到应用，如视频编辑、自动驾驶和人机交互。然而，基于深度神经网络的方法容易受到恶意示例（adversarial examples）的攻击，这些攻击者（i.e., 攻击者）会使用 almost human-imperceptible 的干扰，让 VOS 模型进行错误的像素级预测。这会导致高度需求任务中的安全问题。虽然恶意示例在分类领域已经广泛研究，但在 VOS 领域 rarely 被研究。现有的计算机视觉相关方法 Either require prior knowledge of categories 或者不可直接应用，因为它们特定的设计不适用于 VOS。因此，这项工作开发了一种对 VOS 具有恶意影响的对象agnostic adversary。具体来说，从 VOS 模型的梯度来发现容易困惑的区域，这个区域中的像素Difficult to identify from the background in a frame.这提供了一个 hardness map，帮助生成具有更强的恶意力的攻击。实验表明，我们的攻击者可以 Significantly degrade 多种 state-of-the-art VOS 模型的性能。”
</details></li>
</ul>
<hr>
<h2 id="DISeR-Designing-Imaging-Systems-with-Reinforcement-Learning"><a href="#DISeR-Designing-Imaging-Systems-with-Reinforcement-Learning" class="headerlink" title="DISeR: Designing Imaging Systems with Reinforcement Learning"></a>DISeR: Designing Imaging Systems with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13851">http://arxiv.org/abs/2309.13851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tzofi Klinghoffer, Kushagra Tiwary, Nikhil Behari, Bhavya Agrawalla, Ramesh Raskar</li>
<li>for: 本研究旨在自动化图像系统设计，提高图像系统的性能和可靠性。</li>
<li>methods: 本研究使用语言学模型和强化学习来自动搜索图像系统的组件，包括摄像头、光源、光学元件和感知模型。</li>
<li>results: 研究示出，通过自动搜索图像系统的组件，可以实现更高的任务性能和更好的可靠性。实验结果表明，与业界标准相比，我们的方法可以提供更高的深度估计和更好的摄像头配置。<details>
<summary>Abstract</summary>
Imaging systems consist of cameras to encode visual information about the world and perception models to interpret this encoding. Cameras contain (1) illumination sources, (2) optical elements, and (3) sensors, while perception models use (4) algorithms. Directly searching over all combinations of these four building blocks to design an imaging system is challenging due to the size of the search space. Moreover, cameras and perception models are often designed independently, leading to sub-optimal task performance. In this paper, we formulate these four building blocks of imaging systems as a context-free grammar (CFG), which can be automatically searched over with a learned camera designer to jointly optimize the imaging system with task-specific perception models. By transforming the CFG to a state-action space, we then show how the camera designer can be implemented with reinforcement learning to intelligently search over the combinatorial space of possible imaging system configurations. We demonstrate our approach on two tasks, depth estimation and camera rig design for autonomous vehicles, showing that our method yields rigs that outperform industry-wide standards. We believe that our proposed approach is an important step towards automating imaging system design.
</details>
<details>
<summary>摘要</summary>
In this paper, we use a context-free grammar (CFG) to formulate these four building blocks of imaging systems. The CFG can be automatically searched over with a learned camera designer to jointly optimize the imaging system with task-specific perception models. By transforming the CFG to a state-action space, we can implement the camera designer with reinforcement learning to intelligently search over the combinatorial space of possible imaging system configurations.We demonstrate our approach on two tasks, depth estimation and camera rig design for autonomous vehicles. Our method yields rigs that outperform industry-wide standards. We believe that our proposed approach is an important step towards automating imaging system design.
</details></li>
</ul>
<hr>
<h2 id="Tuning-Multi-mode-Token-level-Prompt-Alignment-across-Modalities"><a href="#Tuning-Multi-mode-Token-level-Prompt-Alignment-across-Modalities" class="headerlink" title="Tuning Multi-mode Token-level Prompt Alignment across Modalities"></a>Tuning Multi-mode Token-level Prompt Alignment across Modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13847">http://arxiv.org/abs/2309.13847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wds2014/ALIGN">https://github.com/wds2014/ALIGN</a></li>
<li>paper_authors: Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo Chen, Hanwang Zhang</li>
<li>for: 提高视觉概念理解的开放世界视觉模型表现。</li>
<li>methods: 使用多模式Token级别调整框架，利用最优运输来学习和协调多模式modalities的Prompt tokens。</li>
<li>results: 在各种图像识别benchmark上显示出优于常见方法的总体化和几个shot能力，并且Qualitative分析表明学习的Prompt tokens能够捕捉多种视觉概念。<details>
<summary>Abstract</summary>
Advancements in prompt tuning of vision-language models have underscored their potential in enhancing open-world visual concept comprehension. However, prior works only primarily focus on single-mode (only one prompt for each modality) and holistic level (image or sentence) semantic alignment, which fails to capture the sample diversity, leading to sub-optimal prompt discovery. To address the limitation, we propose a multi-mode token-level tuning framework that leverages the optimal transportation to learn and align a set of prompt tokens across modalities. Specifically, we rely on two essential factors: 1) multi-mode prompts discovery, which guarantees diverse semantic representations, and 2) token-level alignment, which helps explore fine-grained similarity. Consequently, the similarity can be calculated as a hierarchical transportation problem between the modality-specific sets. Extensive experiments on popular image recognition benchmarks show the superior generalization and few-shot abilities of our approach. The qualitative analysis demonstrates that the learned prompt tokens have the ability to capture diverse visual concepts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Multi-mode prompts discovery, which guarantees diverse semantic representations.2. Token-level alignment, which helps explore fine-grained similarity.Consequently, the similarity can be calculated as a hierarchical transportation problem between the modality-specific sets. Extensive experiments on popular image recognition benchmarks show the superior generalization and few-shot abilities of our approach. The qualitative analysis demonstrates that the learned prompt tokens have the ability to capture diverse visual concepts.Note: “vision-language models” should be translated as “视觉语言模型” in Simplified Chinese.</details></li>
</ol>
<hr>
<h2 id="Traj-LO-In-Defense-of-LiDAR-Only-Odometry-Using-an-Effective-Continuous-Time-Trajectory"><a href="#Traj-LO-In-Defense-of-LiDAR-Only-Odometry-Using-an-Effective-Continuous-Time-Trajectory" class="headerlink" title="Traj-LO: In Defense of LiDAR-Only Odometry Using an Effective Continuous-Time Trajectory"></a>Traj-LO: In Defense of LiDAR-Only Odometry Using an Effective Continuous-Time Trajectory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13842">http://arxiv.org/abs/2309.13842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kevin2431/traj-lo">https://github.com/kevin2431/traj-lo</a></li>
<li>paper_authors: Xin Zheng, Jianke Zhu</li>
<li>for: 这个论文旨在探讨LiDAR仅凭借LiDAR点云数据实现的ODometry问题，而不是通过附加惯性仪器来提高精度。</li>
<li>methods: 该方法首先将LiDAR测量点视为流动的流点，然后将LiDAR运动 parameterized为简单 yet effective的连续时间曲线。最后，我们的Traj-LO方法尝试通过紧密结合LiDAR点云的 геометри信息和运动约束来重建 LiDAR的空间时间准确的运动。</li>
<li>results: 对于不同类型的LiDAR和多个LiDAR系统，我们的方法表现出了可靠和有效的性能，甚至在kinematic状态超过IMU测量范围的情况下也能够取得良好的结果。我们的实现已经在GitHub上公开。<details>
<summary>Abstract</summary>
LiDAR Odometry is an essential component in many robotic applications. Unlike the mainstreamed approaches that focus on improving the accuracy by the additional inertial sensors, this letter explores the capability of LiDAR-only odometry through a continuous-time perspective. Firstly, the measurements of LiDAR are regarded as streaming points continuously captured at high frequency. Secondly, the LiDAR movement is parameterized by a simple yet effective continuous-time trajectory. Therefore, our proposed Traj-LO approach tries to recover the spatial-temporal consistent movement of LiDAR by tightly coupling the geometric information from LiDAR points and kinematic constraints from trajectory smoothness. This framework is generalized for different kinds of LiDAR as well as multi-LiDAR systems. Extensive experiments on the public datasets demonstrate the robustness and effectiveness of our proposed LiDAR-only approach, even in scenarios where the kinematic state exceeds the IMU's measuring range. Our implementation is open-sourced on GitHub.
</details>
<details>
<summary>摘要</summary>
雷达探测器（LiDAR）是许多 робо械应用中的一个关键组件。与主流方法不同，这封信函数通过不断增强附加的惯性传感器来提高精度。而我们的方法则是通过持续时间的视角来探讨LiDAR只的定位。首先，我们视为LiDAR测量点是高频Capture的流动点。其次，我们将LiDAR的移动参数化为简单 yet effective的持续时间曲线。因此，我们提出的Traj-LO方法尝试通过与LiDAR点的几何信息和运动稳定性的束缚来恢复LiDAR的空间时间准确的运动。这种框架适用于不同类型的LiDAR以及多个LiDAR系统。我们的实现在GitHub上公开。Extensive experiments on public datasets have demonstrated the robustness and effectiveness of our proposed LiDAR-only approach, even in scenarios where the kinematic state exceeds the IMU's measuring range.
</details></li>
</ul>
<hr>
<h2 id="Fill-the-K-Space-and-Refine-the-Image-Prompting-for-Dynamic-and-Multi-Contrast-MRI-Reconstruction"><a href="#Fill-the-K-Space-and-Refine-the-Image-Prompting-for-Dynamic-and-Multi-Contrast-MRI-Reconstruction" class="headerlink" title="Fill the K-Space and Refine the Image: Prompting for Dynamic and Multi-Contrast MRI Reconstruction"></a>Fill the K-Space and Refine the Image: Prompting for Dynamic and Multi-Contrast MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13839">http://arxiv.org/abs/2309.13839</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellopipu/promptmr">https://github.com/hellopipu/promptmr</a></li>
<li>paper_authors: Bingyu Xin, Meng Ye, Leon Axel, Dimitris N. Metaxas</li>
<li>for: 提高动态或多contrast MRI重建的精度和效率，以及扩展现有的MRI重建模型到不同的输入类型和参数 Conditioning.</li>
<li>methods: 提出了一种两 stage重建管道，首先利用物理学基础重建缺失的k-空间数据，然后通过一种基于提示的学习方法（PromptMR）来进行多视图、多对比度、邻近类型和加速因子的all-in-one重建。</li>
<li>results: 对比 précédente estado del arte的加速MRI重建方法，提出的方法得到了显著的提高，并且可以更好地适应不同的输入类型和参数 Conditioning.<details>
<summary>Abstract</summary>
The key to dynamic or multi-contrast magnetic resonance imaging (MRI) reconstruction lies in exploring inter-frame or inter-contrast information. Currently, the unrolled model, an approach combining iterative MRI reconstruction steps with learnable neural network layers, stands as the best-performing method for MRI reconstruction. However, there are two main limitations to overcome: firstly, the unrolled model structure and GPU memory constraints restrict the capacity of each denoising block in the network, impeding the effective extraction of detailed features for reconstruction; secondly, the existing model lacks the flexibility to adapt to variations in the input, such as different contrasts, resolutions or views, necessitating the training of separate models for each input type, which is inefficient and may lead to insufficient reconstruction. In this paper, we propose a two-stage MRI reconstruction pipeline to address these limitations. The first stage involves filling the missing k-space data, which we approach as a physics-based reconstruction problem. We first propose a simple yet efficient baseline model, which utilizes adjacent frames/contrasts and channel attention to capture the inherent inter-frame/-contrast correlation. Then, we extend the baseline model to a prompt-based learning approach, PromptMR, for all-in-one MRI reconstruction from different views, contrasts, adjacent types, and acceleration factors. The second stage is to refine the reconstruction from the first stage, which we treat as a general video restoration problem to further fuse features from neighboring frames/contrasts in the image domain. Extensive experiments show that our proposed method significantly outperforms previous state-of-the-art accelerated MRI reconstruction methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>针对动态或多比特磁共振成像（MRI）重建，关键在于挖掘 между帧或比特信息。目前，最佳性能的方法是折叠模型，它将iterative MRI重建步骤与可学习神经网络层组合起来。然而，需要突破两个主要限制：首先，折叠模型的结构和GPU内存限制限制每个除噪块在网络中的容量，阻碍细节特征的有效提取;其次，现有模型缺乏适应输入的灵活性，需要对不同的输入，如不同的比特、分辨率、视野或视角，进行分别训练，这是不fficient和可能导致重建不足。在这篇论文中，我们提出了一个两个阶段的MRI重建管道，用于解决这些限制。第一阶段是填充缺失的k空间数据，我们对此采用物理基础的重建方法。我们首先提出了一个简单 yet efficient的基准模型，该模型利用邻近帧/比特和通道注意力 capture the inherent inter-frame/-contrast correlation。然后，我们将基准模型扩展到PromptMR，用于从不同的视角、比特、邻近类型和加速因子中进行一起的MRI重建。第二阶段是对第一阶段的重建进行进一步的纠正，我们将其视为一个通用视频恢复问题，以更好地融合邻近帧/比特中的特征。经过广泛的实验，我们发现我们的提议方法在前一个状态的加速MRI重建方法上显著超越。
</details></li>
</ul>
<hr>
<h2 id="IBVC-Interpolation-driven-B-frame-Video-Compression"><a href="#IBVC-Interpolation-driven-B-frame-Video-Compression" class="headerlink" title="IBVC: Interpolation-driven B-frame Video Compression"></a>IBVC: Interpolation-driven B-frame Video Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13835">http://arxiv.org/abs/2309.13835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiqin Liu, Chenming Xu, Chao Yao, Weisi Lin, Yao Zhao</li>
<li>For: The paper aims to improve B-frame video compression by addressing inaccurate quantized motions and inefficient motion compensation in previous learned approaches.* Methods: The proposed method, Interpolation-driven B-frame Video Compression (IBVC), involves two major operations: video frame interpolation and artifact reduction compression. It uses a bit-rate free MEMC based on interpolation and a residual guided masking encoder to adaptively select meaningful contexts with interpolated multi-scale dependencies.* Results: The experimental results on B-frame coding demonstrate that IBVC has significant improvements compared to relevant state-of-the-art methods, and can save bit rates compared with the random access (RA) configuration of H.266 (VTM).Here are the three points in Simplified Chinese:* For: 提高B帧视频压缩，解决前一些学习方法中的不准确量化运动和不fficient的运动补做。* Methods: 提议的方法是Interpolation-driven B-frame Video Compression (IBVC)，它包括两个主要操作：视频 interpolating和artefact reduction compression。它使用免费的MEMC based on interpolation，并使用 residual guided masking encoder来选择有用的上下文。* Results: 实验结果表明，IBVC在B帧编码方面有显著的改进，并可以比Random Access（RA）配置的H.266（VTM）节省比特率。<details>
<summary>Abstract</summary>
Learned B-frame video compression aims to adopt bi-directional motion estimation and motion compensation (MEMC) coding for middle frame reconstruction. However, previous learned approaches often directly extend neural P-frame codecs to B-frame relying on bi-directional optical-flow estimation or video frame interpolation. They suffer from inaccurate quantized motions and inefficient motion compensation. To address these issues, we propose a simple yet effective structure called Interpolation-driven B-frame Video Compression (IBVC). Our approach only involves two major operations: video frame interpolation and artifact reduction compression. IBVC introduces a bit-rate free MEMC based on interpolation, which avoids optical-flow quantization and additional compression distortions. Later, to reduce duplicate bit-rate consumption and focus on unaligned artifacts, a residual guided masking encoder is deployed to adaptively select the meaningful contexts with interpolated multi-scale dependencies. In addition, a conditional spatio-temporal decoder is proposed to eliminate location errors and artifacts instead of using MEMC coding in other methods. The experimental results on B-frame coding demonstrate that IBVC has significant improvements compared to the relevant state-of-the-art methods. Meanwhile, our approach can save bit rates compared with the random access (RA) configuration of H.266 (VTM). The code will be available at https://github.com/ruhig6/IBVC.
</details>
<details>
<summary>摘要</summary>
学习B帧视频压缩targets采用双向运动估计和运动补做(MEMC)编码来重建中间帧。然而，以前的学习方法通常直接将神经网络P帧编码器扩展到B帧，基于双向光流估计或视频帧 interpolación。它们受到不准确的量化运动和不fficient的运动补做的影响。为了解决这些问题，我们提出了一种简单 yet effective的结构，即 interpolación-driven B帧视频压缩(IBVC)。我们的方法只有两个主要操作：视频帧 interpolación和噪声压缩。IBVC引入了一种免费的MEMC基于 interpolación，这可以避免光流量化和额外压缩损害。后来，为了减少重复的比特率消耗和重点关注不同依赖度的噪声，我们提出了一种适应性的masking编码器，以便选择 interpolated多尺度依赖关系中的有意义上下文。此外，我们还提出了一种conditional spatio-temporal解码器，以消除Location errors和噪声而不是使用MEMC编码。实验结果表明，IBVC与相关的状态 искусternal methods相比有显著改善。同时，我们的方法可以与H.266（VTM）中的随机访问（RA）配置相比节省比特率。代码将在https://github.com/ruhig6/IBVC上提供。
</details></li>
</ul>
<hr>
<h2 id="PARTICLE-Part-Discovery-and-Contrastive-Learning-for-Fine-grained-Recognition"><a href="#PARTICLE-Part-Discovery-and-Contrastive-Learning-for-Fine-grained-Recognition" class="headerlink" title="PARTICLE: Part Discovery and Contrastive Learning for Fine-grained Recognition"></a>PARTICLE: Part Discovery and Contrastive Learning for Fine-grained Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13822">http://arxiv.org/abs/2309.13822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvl-umass/PARTICLE">https://github.com/cvl-umass/PARTICLE</a></li>
<li>paper_authors: Oindrila Saha, Subhransu Maji</li>
<li>for: 这些方法是用于提高细化分类和分割任务的自助学习方法。</li>
<li>methods: 这些方法包括instance-discriminative contrastive学习和part-centric equivariance和不变性目标。</li>
<li>results: 这些方法可以提高图像分类和分割任务的性能，例如在Linear-evaluation scheme中，使用DetCon自助学习方法训练ResNet50在ImageNet上的分类精度从35.4%提高到42.0%在Caltech-UCSD Birds上，从35.5%提高到44.1%在FGVC Aircraft上，从29.7%提高到37.4%在Stanford Cars上。<details>
<summary>Abstract</summary>
We develop techniques for refining representations for fine-grained classification and segmentation tasks in a self-supervised manner. We find that fine-tuning methods based on instance-discriminative contrastive learning are not as effective, and posit that recognizing part-specific variations is crucial for fine-grained categorization. We present an iterative learning approach that incorporates part-centric equivariance and invariance objectives. First, pixel representations are clustered to discover parts. We analyze the representations from convolutional and vision transformer networks that are best suited for this task. Then, a part-centric learning step aggregates and contrasts representations of parts within an image. We show that this improves the performance on image classification and part segmentation tasks across datasets. For example, under a linear-evaluation scheme, the classification accuracy of a ResNet50 trained on ImageNet using DetCon, a self-supervised learning approach, improves from 35.4% to 42.0% on the Caltech-UCSD Birds, from 35.5% to 44.1% on the FGVC Aircraft, and from 29.7% to 37.4% on the Stanford Cars. We also observe significant gains in few-shot part segmentation tasks using the proposed technique, while instance-discriminative learning was not as effective. Smaller, yet consistent, improvements are also observed for stronger networks based on transformers.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)我们开发了一种基于自我指导的方法，用于精细分类和分割任务中的表示更新。我们发现，基于实例异同学习的精化方法并不那么有效，而recognizing特定部分的变化是精度分类的关键。我们提出了一种循环学习方法，其中包括部分准确性和不变性目标。首先，我们使用图像中的像素表示进行聚合，以便发现特定部分。然后，我们分析了图像中的卷积和视力转换网络，以确定最适合这种任务的表示。接着，我们在图像中聚合和对比部分表示，以提高图像分类和分割任务的性能。我们发现，这种方法在多个数据集上都有显著提高，而instance-discriminative学习方法不太有效。此外，我们还发现，对于更强大的网络，基于转换器的方法也会得到更小 yet consistent的改进。
</details></li>
</ul>
<hr>
<h2 id="MMA-Net-Multiple-Morphology-Aware-Network-for-Automated-Cobb-Angle-Measurement"><a href="#MMA-Net-Multiple-Morphology-Aware-Network-for-Automated-Cobb-Angle-Measurement" class="headerlink" title="MMA-Net: Multiple Morphology-Aware Network for Automated Cobb Angle Measurement"></a>MMA-Net: Multiple Morphology-Aware Network for Automated Cobb Angle Measurement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13817">http://arxiv.org/abs/2309.13817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengxuan Qiu, Jie Yang, Jiankun Wang</li>
<li>for: 提高骨盘畸形诊断和评估中的自动定角度测量精度。</li>
<li>methods: 利用多种骨盘 morphology 作为注意力信息，并将 segmentation 结果与原始 X-ray 图像 concatenate 作为 regression 模块进行精度的定角度测量。</li>
<li>results: 在 AASCE 挑战数据集上测试，SMAPE 为 7.28%，MAE 为 3.18{\deg}, 与其他竞争方法相比表现出色。<details>
<summary>Abstract</summary>
Scoliosis diagnosis and assessment depend largely on the measurement of the Cobb angle in spine X-ray images. With the emergence of deep learning techniques that employ landmark detection, tilt prediction, and spine segmentation, automated Cobb angle measurement has become increasingly popular. However, these methods encounter difficulties such as high noise sensitivity, intricate computational procedures, and exclusive reliance on a single type of morphological information. In this paper, we introduce the Multiple Morphology-Aware Network (MMA-Net), a novel framework that improves Cobb angle measurement accuracy by integrating multiple spine morphology as attention information. In the MMA-Net, we first feed spine X-ray images into the segmentation network to produce multiple morphological information (spine region, centerline, and boundary) and then concatenate the original X-ray image with the resulting segmentation maps as input for the regression module to perform precise Cobb angle measurement. Furthermore, we devise joint loss functions for our segmentation and regression network training, respectively. We evaluate our method on the AASCE challenge dataset and achieve superior performance with the SMAPE of 7.28% and the MAE of 3.18{\deg}, indicating a strong competitiveness compared to other outstanding methods. Consequently, we can offer clinicians automated, efficient, and reliable Cobb angle measurement.
</details>
<details>
<summary>摘要</summary>
诊断和评估斯科利病（Scoliosis）几乎完全依赖在脊梁X射线图像中测量Cobb角度。随着深度学习技术的出现，使用landmark检测、倾斜预测和脊梁分 segmentation的自动化Cobb角度测量方法在现场上变得越来越受欢迎。然而，这些方法受到高噪音敏感、复杂计算过程和单一类型形态信息的限制。在这篇论文中，我们介绍了多种形态意识网络（MMA-Net），一种新的框架，可以提高Cobb角度测量精度。在MMA-Net中，我们首先将脊梁X射线图像传递给分 segmentation网络，以生成多种形态信息（脊梁区域、中心线和边界），然后将原始X射线图像和生成的分 segmentation图像作为输入传递给 regression模块进行精确Cobb角度测量。此外，我们定义了joint损失函数用于我们的分 segmentation和回归网络训练。我们在AASCE挑战数据集上评估了我们的方法，并实现了优于其他突出的方法的性能，SMAPE值为7.28%和MAE值为3.18°，这表明我们的方法具有强大的竞争力。因此，我们可以为临床医生提供自动化、高效和可靠的Cobb角度测量。
</details></li>
</ul>
<hr>
<h2 id="DVI-SLAM-A-Dual-Visual-Inertial-SLAM-Network"><a href="#DVI-SLAM-A-Dual-Visual-Inertial-SLAM-Network" class="headerlink" title="DVI-SLAM: A Dual Visual Inertial SLAM Network"></a>DVI-SLAM: A Dual Visual Inertial SLAM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13814">http://arxiv.org/abs/2309.13814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiongfeng Peng, Zhihua Liu, Weiming Li, Ping Tan, SoonYong Cho, Qiang Wang</li>
<li>for: This paper aims to improve visual simultaneous localization and mapping (SLAM) methods by better integrating visual information and inertial measurement unit (IMU) data.</li>
<li>methods: The proposed method uses a novel deep SLAM network with dual visual factors, which integrates both photometric and re-projection factors into an end-to-end differentiable structure through a multi-factor data association module.</li>
<li>results: The proposed method significantly outperforms state-of-the-art methods on several public datasets, including TartanAir, EuRoC, and ETH3D-SLAM. Specifically, the absolute trajectory error was reduced by 45.3% and 36.2% for monocular and stereo configurations on the EuRoC dataset, respectively.<details>
<summary>Abstract</summary>
Recent deep learning based visual simultaneous localization and mapping (SLAM) methods have made significant progress. However, how to make full use of visual information as well as better integrate with inertial measurement unit (IMU) in visual SLAM has potential research value. This paper proposes a novel deep SLAM network with dual visual factors. The basic idea is to integrate both photometric factor and re-projection factor into the end-to-end differentiable structure through multi-factor data association module. We show that the proposed network dynamically learns and adjusts the confidence maps of both visual factors and it can be further extended to include the IMU factors as well. Extensive experiments validate that our proposed method significantly outperforms the state-of-the-art methods on several public datasets, including TartanAir, EuRoC and ETH3D-SLAM. Specifically, when dynamically fusing the three factors together, the absolute trajectory error for both monocular and stereo configurations on EuRoC dataset has reduced by 45.3% and 36.2% respectively.
</details>
<details>
<summary>摘要</summary>
现代深度学习基于视觉同时定位地图（SLAM）方法在最近几年中已经做出了很大的进步。然而，如何更好地利用视觉信息并更好地与惯性测量单元（IMU）在视觉SLAM中进行集成，这是有研究价值的。本文提出了一种新的深度SLAM网络，具有双视觉因素。基本思想是通过多因素数据匹配模块将 photometric 因素和重投影因素集成到端到端可微结构中。我们显示了我们提出的网络可以在运动中学习和调整两个视觉因素的信任地图，并且可以进一步包括 IMU 因素。广泛的实验证明了我们的提出方法在多个公共数据集上具有显著的优势，包括 TartanAir、EuRoC 和 ETH3D-SLAM。具体来说，在动态混合三个因素时，EuRoC 数据集上的绝对轨迹错误量降低了45.3%和36.2%分别 для 单镜和 стерео 配置。
</details></li>
</ul>
<hr>
<h2 id="Boundary-Aware-Proposal-Generation-Method-for-Temporal-Action-Localization"><a href="#Boundary-Aware-Proposal-Generation-Method-for-Temporal-Action-Localization" class="headerlink" title="Boundary-Aware Proposal Generation Method for Temporal Action Localization"></a>Boundary-Aware Proposal Generation Method for Temporal Action Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13810">http://arxiv.org/abs/2309.13810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Chunyan Feng, Jiahui Yang, Zheng Li, Caili Guo<br>for: 本文旨在提出一种基于界限感知的暂时动作地理化（TAL）方法，以便在未处理视频中找到动作类别和时间边界。methods: 本文提出的Boundary-Aware Proposal Generation（BAPG）方法，通过强调界限感知来改善TAL的准确性。BAPG不依赖现有的TAL网络架构，可以与主流TAL模型进行插件式应用。results: 对于THUMOS14和ActivityNet-1.3 dataset的广泛实验表明，BAPG可以显著提高TAL的性能。<details>
<summary>Abstract</summary>
The goal of Temporal Action Localization (TAL) is to find the categories and temporal boundaries of actions in an untrimmed video. Most TAL methods rely heavily on action recognition models that are sensitive to action labels rather than temporal boundaries. More importantly, few works consider the background frames that are similar to action frames in pixels but dissimilar in semantics, which also leads to inaccurate temporal boundaries. To address the challenge above, we propose a Boundary-Aware Proposal Generation (BAPG) method with contrastive learning. Specifically, we define the above background frames as hard negative samples. Contrastive learning with hard negative mining is introduced to improve the discrimination of BAPG. BAPG is independent of the existing TAL network architecture, so it can be applied plug-and-play to mainstream TAL models. Extensive experimental results on THUMOS14 and ActivityNet-1.3 demonstrate that BAPG can significantly improve the performance of TAL.
</details>
<details>
<summary>摘要</summary>
文本内容：目的是Temporal Action Localization（TAL）找到视频中的分类和时间边界。大多数TAL方法都依赖于动作识别模型，而这些模型更关注动作标签而非时间边界。更重要的是，有些工作不考虑视频中的背景帧，这些帧与动作帧相似在像素级别，但是在semantics方面不同，这也导致了不准确的时间边界。为解决这个挑战，我们提出了Boundary-Aware Proposal Generation（BAPG）方法，该方法使用了对比学习。我们定义了上述背景帧为hard negative samples。对比学习可以提高BAPG的推iscrimination。BAPG与主流TAL网络架构独立，因此可以直接应用于主流TAL模型。我们在THUMOS14和ActivityNet-1.3上进行了广泛的实验，结果表明BAPG可以显著提高TAL的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/cs.CV_2023_09_25/" data-id="clpztdniq00kees881py1hmom" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/cs.AI_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T12:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/cs.AI_2023_09_25/">cs.AI - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Integrating-Higher-Order-Dynamics-and-Roadway-Compliance-into-Constrained-ILQR-based-Trajectory-Planning-for-Autonomous-Vehicles"><a href="#Integrating-Higher-Order-Dynamics-and-Roadway-Compliance-into-Constrained-ILQR-based-Trajectory-Planning-for-Autonomous-Vehicles" class="headerlink" title="Integrating Higher-Order Dynamics and Roadway-Compliance into Constrained ILQR-based Trajectory Planning for Autonomous Vehicles"></a>Integrating Higher-Order Dynamics and Roadway-Compliance into Constrained ILQR-based Trajectory Planning for Autonomous Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14566">http://arxiv.org/abs/2309.14566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanxiang Li, Jiaqiao Zhang, Sheng Zhu, Dongjian Tang, Donghao Xu</li>
<li>for: 本研究旨在提出一种基于CILQR优化算法的在道路上的自动驾驶汽车路径规划方法，以提高安全性和舒适性。</li>
<li>methods: 本研究使用了CILQR优化算法，并增加了更高阶的约束和成本，以确保路径规划是可控的。此外，本研究还考虑了道路规则遵从性，以确保车辆遵循路径规划的约束。</li>
<li>results:  simulation和实际驾驶场景 validate了本研究的方法，显示了改进的安全性和舒适性。<details>
<summary>Abstract</summary>
This paper addresses the advancements in on-road trajectory planning for Autonomous Passenger Vehicles (APV). Trajectory planning aims to produce a globally optimal route for APVs, considering various factors such as vehicle dynamics, constraints, and detected obstacles. Traditional techniques involve a combination of sampling methods followed by optimization algorithms, where the former ensures global awareness and the latter refines for local optima. Notably, the Constrained Iterative Linear Quadratic Regulator (CILQR) optimization algorithm has recently emerged, adapted for APV systems, emphasizing improved safety and comfort. However, existing implementations utilizing the vehicle bicycle kinematic model may not guarantee controllable trajectories. We augment this model by incorporating higher-order terms, including the first and second-order derivatives of curvature and longitudinal jerk. This inclusion facilitates a richer representation in our cost and constraint design. We also address roadway compliance, emphasizing adherence to lane boundaries and directions, which past work often overlooked. Lastly, we adopt a relaxed logarithmic barrier function to address the CILQR's dependency on feasible initial trajectories. The proposed methodology is then validated through simulation and real-world experiment driving scenes in real time.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we augment the model by incorporating higher-order terms, including the first and second-order derivatives of curvature and longitudinal jerk. This allows for a more detailed representation in our cost and constraint design. Additionally, we emphasize adherence to lane boundaries and directions, which past work often overlooked. To address the CILQR's dependency on feasible initial trajectories, we adopt a relaxed logarithmic barrier function.The proposed methodology is then validated through simulation and real-world experiment driving scenes in real time. This paper's contributions include a more accurate and comprehensive vehicle model, improved roadway compliance, and a relaxed logarithmic barrier function to address the CILQR's dependency on feasible initial trajectories. These advancements lead to more controllable and safe trajectories for APVs.
</details></li>
</ul>
<hr>
<h2 id="Generative-Escher-Meshes"><a href="#Generative-Escher-Meshes" class="headerlink" title="Generative Escher Meshes"></a>Generative Escher Meshes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14564">http://arxiv.org/abs/2309.14564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noam Aigerman, Thibault Groueix</li>
<li>For:  This paper proposes a fully-automatic, text-guided generative method for producing periodic, repeating, tile-able 2D art, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher.* Methods: The method uses an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group, and modifies the laplacian used in a 2D mesh-mapping technique - Orbifold Tutte Embedding - to achieve all possible tiling configurations for a chosen planar symmetry group. The method also leverages a trained image diffusion model to define a loss on the resulting image, thereby updating the mesh’s parameters based on its appearance matching the text prompt.* Results: The paper shows that the method is able to produce plausible, appealing results, with non-trivial tiles, for a variety of different periodic tiling patterns.<details>
<summary>Abstract</summary>
This paper proposes a fully-automatic, text-guided generative method for producing periodic, repeating, tile-able 2D art, such as the one seen on floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the standard concept of a seamless texture, i.e., square images that are seamless when tiled, our method generates non-square tilings which comprise solely of repeating copies of the same object. It achieves this by optimizing both geometry and color of a 2D mesh, in order to generate a non-square tile in the shape and appearance of the desired object, with close to no additional background details. We enable geometric optimization of tilings by our key technical contribution: an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group. Namely, we prove that modifying the laplacian used in a 2D mesh-mapping technique - Orbifold Tutte Embedding - can achieve all possible tiling configurations for a chosen planar symmetry group. We thus consider both the mesh's tile-shape and its texture as optimizable parameters, rendering the textured mesh via a differentiable renderer. We leverage a trained image diffusion model to define a loss on the resulting image, thereby updating the mesh's parameters based on its appearance matching the text prompt. We show our method is able to produce plausible, appealing results, with non-trivial tiles, for a variety of different periodic tiling patterns.
</details>
<details>
<summary>摘要</summary>
Our key technical contribution is an unconstrained, differentiable parameterization of the space of all possible tileable shapes for a given symmetry group. We modify the laplacian used in a 2D mesh-mapping technique called Orbifold Tutte Embedding to achieve all possible tiling configurations for a chosen planar symmetry group. This allows us to optimize both the mesh's tile shape and its texture as parameters, which are then rendered using a differentiable renderer.We use a trained image diffusion model to define a loss on the resulting image, which is used to update the mesh's parameters based on its appearance matching the text prompt. Our method is able to produce plausible and appealing results with non-trivial tiles for a variety of different periodic tiling patterns.
</details></li>
</ul>
<hr>
<h2 id="Training-free-Linear-Image-Inversion-via-Flows"><a href="#Training-free-Linear-Image-Inversion-via-Flows" class="headerlink" title="Training-free Linear Image Inversion via Flows"></a>Training-free Linear Image Inversion via Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04432">http://arxiv.org/abs/2310.04432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashwini Pokle, Matthew J. Muckley, Ricky T. Q. Chen, Brian Karrer</li>
<li>for:  Linear image inversion without training</li>
<li>methods: 使用预训练的生成模型，采用流程匹配模型，使用理论支持的质量补做方法，大幅减少手动参数调整。</li>
<li>results: 在高维数据集上（ImageNet-64&#x2F;128和AFHQ-256），无需特定问题调整，我们的流程基于匹配方法对图像反向问题进行了有效的解决。<details>
<summary>Abstract</summary>
Training-free linear inversion involves the use of a pretrained generative model and -- through appropriate modifications to the generation process -- solving inverse problems without any finetuning of the generative model. While recent prior methods have explored the use of diffusion models, they still require the manual tuning of many hyperparameters for different inverse problems. In this work, we propose a training-free method for image inversion using pretrained flow models, leveraging the simplicity and efficiency of Flow Matching models, using theoretically-justified weighting schemes and thereby significantly reducing the amount of manual tuning. In particular, we draw inspiration from two main sources: adopting prior gradient correction methods to the flow regime, and a solver scheme based on conditional Optimal Transport paths. As pretrained diffusion models are widely accessible, we also show how to practically adapt diffusion models for our method. Empirically, our approach requires no problem-specific tuning across an extensive suite of noisy linear image inversion problems on high-dimensional datasets, ImageNet-64/128 and AFHQ-256, and we observe that our flow-based method for image inversion significantly improves upon closely-related diffusion-based linear inversion methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用预训练的生成模型进行无需训练的线性逆转，通过对生成过程进行相应的修改，可以解决逆转问题无需生成模型的负载。Recent prior方法已经探索了使用扩散模型，但仍然需要手动调整许多超参数 для不同的逆转问题。在这种工作中，我们提出了一种无需训练的图像逆转方法使用预训练的流模型，利用流模型的简单性和高效性，并使用理论上正确的权重分配方案，以降低手动调整的数量。特别是，我们从两个主要的来源中突破想法：在流程中采用先前的梯度修正方法，以及基于条件最优运输路径的解决方案。由于预训练的扩散模型广泛可用，我们还展示了如何实际地适应 diffusion 模型。在实验中，我们发现我们的流基于方法可以在高维度的数据集上进行无需具体问题调整的图像逆转，并且与相似的扩散基于线性逆转方法相比，我们的流基于方法可以获得显著的改进。>>
</details></li>
</ul>
<hr>
<h2 id="Disinformation-Detection-An-Evolving-Challenge-in-the-Age-of-LLMs"><a href="#Disinformation-Detection-An-Evolving-Challenge-in-the-Age-of-LLMs" class="headerlink" title="Disinformation Detection: An Evolving Challenge in the Age of LLMs"></a>Disinformation Detection: An Evolving Challenge in the Age of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15847">http://arxiv.org/abs/2309.15847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohan Jiang, Zhen Tan, Ayushi Nirmal, Huan Liu</li>
<li>for: 本研究旨在探讨利用大型语言模型（LLMs）生成的假信息攻击性的威胁，以及如何通过利用LLMs自身来建立可靠的防御机制。</li>
<li>methods: 本研究采用了现有的假信息检测技术，以及利用LLMs自身来生成检测假信息的模型。</li>
<li>results: 研究发现，现有的假信息检测技术对LLMs生成的假信息有限的检测能力，而利用LLMs自身来生成检测假信息的模型则表现更高效。<details>
<summary>Abstract</summary>
The advent of generative Large Language Models (LLMs) such as ChatGPT has catalyzed transformative advancements across multiple domains. However, alongside these advancements, they have also introduced potential threats. One critical concern is the misuse of LLMs by disinformation spreaders, leveraging these models to generate highly persuasive yet misleading content that challenges the disinformation detection system. This work aims to address this issue by answering three research questions: (1) To what extent can the current disinformation detection technique reliably detect LLM-generated disinformation? (2) If traditional techniques prove less effective, can LLMs themself be exploited to serve as a robust defense against advanced disinformation? and, (3) Should both these strategies falter, what novel approaches can be proposed to counter this burgeoning threat effectively? A holistic exploration for the formation and detection of disinformation is conducted to foster this line of research.
</details>
<details>
<summary>摘要</summary>
LLMs的出现已经导致多个领域的进步，但同时也引入了潜在的威胁。一个重要的问题是利用LLMs散布假信息，使用这些模型生成高度感染的假信息，这会挑战假信息检测系统。本研究的目的是回答以下三个研究问题：（1）现有的假信息检测技术能够有效地检测LLM生成的假信息吗？（2）如果传统技术不够有效，可以利用LLM们自己作为防止高级假信息的强大防御手段吗？以及（3）如果这两种策略失败，可以提出新的方法来有效地对抗这种快速发展的威胁。通过探讨假信息的形成和检测，本研究旨在推动这一领域的研究。
</details></li>
</ul>
<hr>
<h2 id="Art-or-Artifice-Large-Language-Models-and-the-False-Promise-of-Creativity"><a href="#Art-or-Artifice-Large-Language-Models-and-the-False-Promise-of-Creativity" class="headerlink" title="Art or Artifice? Large Language Models and the False Promise of Creativity"></a>Art or Artifice? Large Language Models and the False Promise of Creativity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14556">http://arxiv.org/abs/2309.14556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, Chien-Sheng Wu</li>
<li>for: 评估大语言模型（LLM）的创作能力</li>
<li>methods: 使用Consensual Assessment Technique和Torrance Test of Creative Writing评估创作性</li>
<li>results: LLM生成的故事通过TTCW测试失败率较高，并且使用LLM作为评估器时与专业作者的评估结果没有正面相关性。<details>
<summary>Abstract</summary>
Researchers have argued that large language models (LLMs) exhibit high-quality writing capabilities from blogs to stories. However, evaluating objectively the creativity of a piece of writing is challenging. Inspired by the Torrance Test of Creative Thinking (TTCT), which measures creativity as a process, we use the Consensual Assessment Technique [3] and propose the Torrance Test of Creative Writing (TTCW) to evaluate creativity as a product. TTCW consists of 14 binary tests organized into the original dimensions of Fluency, Flexibility, Originality, and Elaboration. We recruit 10 creative writers and implement a human assessment of 48 stories written either by professional authors or LLMs using TTCW. Our analysis shows that LLM-generated stories pass 3-10X less TTCW tests than stories written by professionals. In addition, we explore the use of LLMs as assessors to automate the TTCW evaluation, revealing that none of the LLMs positively correlate with the expert assessments.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:研究人员认为大语言模型（LLM）具有高质量的写作能力，从博客到故事。然而，评估创作文章的创新性是具有挑战性的。受某种创新思维测试（TTCT）的启发，我们使用了共识评估技术 [3]，并提出了杜鲁门创作写作测试（TTCW），以评估创作作品的质量。TTCW包括14个二进制测试，涵盖了原始维度的流畅、灵活性、原创性和发展。我们邀请了10名创作作家，并对由专业作家或 LLM 写作的48篇故事进行人类评估使用 TTCW。我们的分析显示，LLM 生成的故事通过 TTCW 测试的数量比专业作家的故事少得多，3-10 倍。此外，我们还探讨了使用 LLM 作为评估者，以自动化 TTCW 评估，结果显示，没有任何 LLM 与专业评估相关。
</details></li>
</ul>
<hr>
<h2 id="Tactile-Estimation-of-Extrinsic-Contact-Patch-for-Stable-Placement"><a href="#Tactile-Estimation-of-Extrinsic-Contact-Patch-for-Stable-Placement" class="headerlink" title="Tactile Estimation of Extrinsic Contact Patch for Stable Placement"></a>Tactile Estimation of Extrinsic Contact Patch for Stable Placement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14552">http://arxiv.org/abs/2309.14552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kei Ota, Devesh K. Jha, Krishna Murthy Jatavallabhula, Asako Kanezaki, Joshua B. Tenenbaum</li>
<li>for: 这个论文是为了研究机器人如何具备细化的操作技能，特别是在堆叠复杂形状物体时。</li>
<li>methods: 该论文使用了反馈技能来帮助机器人学习堆叠复杂形状物体。机器人通过感受到物体与环境之间的轻微接触来理解物体的稳定性。</li>
<li>results: 研究结果表明，通过对物体与环境之间的轻微接触来估算物体的稳定性是可能的。此外，该方法还可以估算物体在释放 grasp 时的稳定性。实验结果表明，该方法可以在不同的物体对象中实现精准的堆叠。<details>
<summary>Abstract</summary>
Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.
</details>
<details>
<summary>摘要</summary>
<<sys: language="zh-Hans">精准感受接触互动是机器人细致 manipulate 技能的关键。在这篇论文中，我们提出了机器人学习排序复杂形状物体的方法。为了设计这种系统，机器人需要能够根据非常轻微的接触互动理解物体的稳定性。我们的结果表明，可以通过触感读数在物体和其环境之间的接触形成时计算物体的稳定性。特别是，我们可以通过力和感觉观察来估计握持物体和环境之间的接触面积，以估计物体在释放时的稳定性。我们的方法在各种普遍的板球游戏中使用了不同的对象。</sys>>Note that Simplified Chinese is used in the translation, as it is the more widely used standard for Chinese writing in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Algorithmic-Collusion-or-Competition-the-Role-of-Platforms’-Recommender-Systems"><a href="#Algorithmic-Collusion-or-Competition-the-Role-of-Platforms’-Recommender-Systems" class="headerlink" title="Algorithmic Collusion or Competition: the Role of Platforms’ Recommender Systems"></a>Algorithmic Collusion or Competition: the Role of Platforms’ Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14548">http://arxiv.org/abs/2309.14548</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingchen Xu, Stephanie Lee, Yong Tan</li>
<li>For: This paper examines how recommendation algorithms used by e-commerce platforms can impact the competitive dynamics of AI-based pricing algorithms.* Methods: The paper uses a repeated game framework to model the interactions between sellers and the platform’s recommender system, and conducts experiments to observe price dynamics and determine the final equilibrium.* Results: The paper finds that a profit-based recommender system can intensify algorithmic collusion among sellers, while a demand-based recommender system can foster price competition and result in lower prices. The results are robust in various market scenarios.<details>
<summary>Abstract</summary>
Recent academic research has extensively examined algorithmic collusion resulting from the utilization of artificial intelligence (AI)-based dynamic pricing algorithms. Nevertheless, e-commerce platforms employ recommendation algorithms to allocate exposure to various products, and this important aspect has been largely overlooked in previous studies on algorithmic collusion. Our study bridges this important gap in the literature and examines how recommendation algorithms can determine the competitive or collusive dynamics of AI-based pricing algorithms. Specifically, two commonly deployed recommendation algorithms are examined: (i) a recommender system that aims to maximize the sellers' total profit (profit-based recommender system) and (ii) a recommender system that aims to maximize the demand for products sold on the platform (demand-based recommender system). We construct a repeated game framework that incorporates both pricing algorithms adopted by sellers and the platform's recommender system. Subsequently, we conduct experiments to observe price dynamics and ascertain the final equilibrium. Experimental results reveal that a profit-based recommender system intensifies algorithmic collusion among sellers due to its congruence with sellers' profit-maximizing objectives. Conversely, a demand-based recommender system fosters price competition among sellers and results in a lower price, owing to its misalignment with sellers' goals. Extended analyses suggest the robustness of our findings in various market scenarios. Overall, we highlight the importance of platforms' recommender systems in delineating the competitive structure of the digital marketplace, providing important insights for market participants and corresponding policymakers.
</details>
<details>
<summary>摘要</summary>
现代学术研究已经广泛研究了基于人工智能（AI）的动态价格算法的算法协作。然而，电商平台使用推荐算法来分配产品的曝光，这一重要方面在过去的研究中受到了广泛的忽略。我们的研究填补了这一重要的研究漏洞，并研究了推荐算法如何影响AI基于价格算法的竞争或协作动态。 Specifically,我们研究了两种通常部署的推荐算法：（i）一个目标 Maximize sellers' total profit的推荐系统（profit-based recommender system），和（ii）一个目标 Maximize the demand for products sold on the platform的推荐系统（demand-based recommender system）。我们建立了一个重复游戏框架，该框架包括采用的价格算法和平台的推荐系统。然后，我们进行实验，观察价格动态并确定最终平衡。实验结果表明，一个基于利润的推荐系统会使算法协作增强，因为它与卖家的利润最大化目标相匹配。相反，一个基于需求的推荐系统会促进价格竞争，导致价格下降，因为它与卖家的目标不一致。 extended 分析表明我们的结论在不同的市场情况下具有坚实性。总的来说，我们强调了平台的推荐系统在数字市场的竞争结构中发挥重要作用，为市场参与者和相关政策制定者提供重要的洞察。
</details></li>
</ul>
<hr>
<h2 id="Effect-of-roundabout-design-on-the-behavior-of-road-users-A-case-study-of-roundabouts-with-application-of-Unsupervised-Machine-Learning"><a href="#Effect-of-roundabout-design-on-the-behavior-of-road-users-A-case-study-of-roundabouts-with-application-of-Unsupervised-Machine-Learning" class="headerlink" title="Effect of roundabout design on the behavior of road users: A case study of roundabouts with application of Unsupervised Machine Learning"></a>Effect of roundabout design on the behavior of road users: A case study of roundabouts with application of Unsupervised Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14540">http://arxiv.org/abs/2309.14540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tasnim M. Dwekat, Ayda A. Almsre, Huthaifa I. Ashqar<br>for: 这个研究的目的是评估缓冲器的性能并研究人行道用户在互动缓冲器时的行为。methods: 该研究使用了观察和分类 drivers的行为，以及预测道路用户在缓冲器交叉点的行为。results: 研究发现，缓冲器可以减少拐弯口口的速度，入口速度和相应的影响速度取决于道路用户的评级。此外，车辆的速度在过缓冲器时更适合于汽车和卡车的速度。此外，缓冲器具有两种内在特点，首先，由于汽车的小尺寸和缓冲器所有部分都可见，因此从所有方向进入缓冲器时，所有 drivers 都需要减速，从而增加了他们在穿过缓冲器时的反应时间，降低了事故的风险。其次，由于缓冲器内部的流速更少， drivers 只需要左看（在右侧交通），从而更容易过缓冲器。<details>
<summary>Abstract</summary>
This research aims to evaluate the performance of the rotors and study the behavior of the human driver in interacting with the rotors. In recent years, rotors have been increasingly used between countries due to their safety, capacity, and environmental advantages, and because they provide safe and fluid flows of vehicles for transit and integration. It turns out that roundabouts can significantly reduce speed at twisting intersections, entry speed and the resulting effect on speed depends on the rating of road users. In our research, (bus, car, truck) drivers were given special attention and their behavior was categorized into (conservative, normal, aggressive). Anticipating and recognizing driver behavior is an important challenge. Therefore, the aim of this research is to study the effect of roundabouts on these classifiers and to develop a method for predicting the behavior of road users at roundabout intersections. Safety is primarily due to two inherent features of the rotor. First, by comparing the data collected and processed in order to classify and evaluate drivers' behavior, and comparing the speeds of the drivers (bus, car and truck), the speed of motorists at crossing the roundabout was more fit than that of buses and trucks. We looked because the car is smaller and all parts of the rotor are visible to it. So drivers coming from all directions have to slow down, giving them more time to react and mitigating the consequences in the event of an accident. Second, with fewer conflicting flows (and points of conflict), drivers only need to look to their left (in right-hand traffic) for other vehicles, making their job of crossing the roundabout easier as there is less need to split attention between different directions.
</details>
<details>
<summary>摘要</summary>
Safety is a primary concern, and rotors have two inherent features that contribute to safety. First, the speed of motorists crossing the roundabout is more controlled compared to buses and trucks, as the smaller car size allows for better visibility of all parts of the rotor. This requires drivers to slow down, giving them more time to react and reducing the risk of accidents. Second, with fewer conflicting flows and points of conflict, drivers only need to look to their left (in right-hand traffic) for other vehicles, making it easier to cross the roundabout and reducing the need to split attention between different directions.In our research, we collected and processed data to classify and evaluate driver behavior, and compared the speeds of buses, cars, and trucks. We found that the speed of motorists crossing the roundabout was more controlled than that of buses and trucks, as the car's smaller size allows for better visibility of all parts of the rotor. Overall, the design of rotors provides a safer and more efficient way to manage traffic flow, and our research aims to further understand and improve the performance of these intersections.
</details></li>
</ul>
<hr>
<h2 id="Watch-Your-Language-Large-Language-Models-and-Content-Moderation"><a href="#Watch-Your-Language-Large-Language-Models-and-Content-Moderation" class="headerlink" title="Watch Your Language: Large Language Models and Content Moderation"></a>Watch Your Language: Large Language Models and Content Moderation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14517">http://arxiv.org/abs/2309.14517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deepak Kumar, Yousef AbuHashem, Zakir Durumeric</li>
<li>for: 这个论文旨在研究大型自然语言模型（LLM）在内容审核任务中的表现。</li>
<li>methods: 论文使用了现代商业化的GPT-3、GPT-3.5和GPT-4大型自然语言模型，对两种常见的内容审核任务进行评估：规则基础的社区审核和价值评估。</li>
<li>results: 论文发现，LLMs可以有效地进行许多社区的规则基础审核， median accuracy 达到 64%， median precision 达到 83%。而对恶意内容检测，LLMs 表现明显 луч于现有的商业化恶意类别化器。但是，论文发现，在恶意检测任务上，Recent 附加的模型大小增加只有微scopic 的提升，表明 LLMs 在这种任务上可能已经达到性能杯顶。<details>
<summary>Abstract</summary>
Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs perform in content moderation settings. In this work, we evaluate a suite of modern, commercial LLMs (GPT-3, GPT-3.5, GPT-4) on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we construct 95 LLM moderation-engines prompted with rules from 95 Reddit subcommunities and find that LLMs can be effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. For toxicity detection, we find that LLMs significantly outperform existing commercially available toxicity classifiers. However, we also find that recent increases in model size add only marginal benefit to toxicity detection, suggesting a potential performance plateau for LLMs on toxicity detection tasks. We conclude by outlining avenues for future work in studying LLMs and content moderation.
</details>
<details>
<summary>摘要</summary>
For rule-based community moderation, we created 95 LLM moderation engines using rules from 95 Reddit subcommunities. We found that LLMs can effectively moderate content for many communities, achieving a median accuracy of 64% and a median precision of 83%.For toxicity detection, we found that LLMs significantly outperformed existing commercial toxicity classifiers. However, we also found that increasing the size of the model only provided marginal benefits for toxicity detection, suggesting a potential performance plateau for LLMs on this task.Based on our findings, we outline potential avenues for future research on LLMs and content moderation.
</details></li>
</ul>
<hr>
<h2 id="Interaction-Aware-Decision-Making-for-Autonomous-Vehicles-in-Forced-Merging-Scenario-Leveraging-Social-Psychology-Factors"><a href="#Interaction-Aware-Decision-Making-for-Autonomous-Vehicles-in-Forced-Merging-Scenario-Leveraging-Social-Psychology-Factors" class="headerlink" title="Interaction-Aware Decision-Making for Autonomous Vehicles in Forced Merging Scenario Leveraging Social Psychology Factors"></a>Interaction-Aware Decision-Making for Autonomous Vehicles in Forced Merging Scenario Leveraging Social Psychology Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14497">http://arxiv.org/abs/2309.14497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Li, Kaiwen Liu, H. Eric Tseng, Anouck Girard, Ilya Kolmanovsky</li>
<li>for: 本研究旨在帮助自动驾驶车辆在复杂的交通场景中成功完成其驾驶任务，特别是在高速公路强制汇聚场景中。</li>
<li>methods: 本研究使用了社会行为模型，该模型考虑了交互的 drivers 的社会行为和个人目标。基于这个模型，我们开发了一种退火策略控制的决策策略，可以在线估计其他司机的意图，并在不确定的意图下预测附近车辆的行为。</li>
<li>results: 我们通过对比game理论控制器和实际交通数据进行了对比，证明了我们的决策策略的有效性。<details>
<summary>Abstract</summary>
Understanding the intention of vehicles in the surrounding traffic is crucial for an autonomous vehicle to successfully accomplish its driving tasks in complex traffic scenarios such as highway forced merging. In this paper, we consider a behavioral model that incorporates both social behaviors and personal objectives of the interacting drivers. Leveraging this model, we develop a receding-horizon control-based decision-making strategy, that estimates online the other drivers' intentions using Bayesian filtering and incorporates predictions of nearby vehicles' behaviors under uncertain intentions. The effectiveness of the proposed decision-making strategy is demonstrated and evaluated based on simulation studies in comparison with a game theoretic controller and a real-world traffic dataset.
</details>
<details>
<summary>摘要</summary>
理解周围交通中车辆的意图是自动驾驶车辆在复杂交通场景中成功完成驾驶任务的关键。在这篇论文中，我们考虑了一种行为模型，该模型包括交互驾驶员的社会行为和个人目标。利用这种模型，我们开发了一种往复控制基于决策策略，该策略在线上估计其他驾驶员的意图使用 bayesian 筛选，并在不确定意图下预测附近车辆的行为。我们通过模拟研究和与游戏理论控制器进行比较，证明了提议的决策策略的有效性。
</details></li>
</ul>
<hr>
<h2 id="Era-Splitting-–-Invariant-Learning-for-Decision-Trees"><a href="#Era-Splitting-–-Invariant-Learning-for-Decision-Trees" class="headerlink" title="Era Splitting – Invariant Learning for Decision Trees"></a>Era Splitting – Invariant Learning for Decision Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14496">http://arxiv.org/abs/2309.14496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jefferythewind/era-splitting-notebook-examples">https://github.com/jefferythewind/era-splitting-notebook-examples</a></li>
<li>paper_authors: Timothy DeLise</li>
<li>For: The paper is written to address the issue of out-of-distribution (OOD) generalization in decision tree models, specifically random forest and gradient-boosting decision trees.* Methods: The paper proposes two new splitting criteria for decision trees that incorporate era-wise information into the splitting process, allowing the models to find split points that are optimal across all disjoint eras in the data.* Results: The paper describes unique experiments to showcase the benefits of the new splitting criteria, which improve metrics in the authors’ experiments out-of-sample. The new criteria are incorporated into a state-of-the-art gradient boosted decision tree model in the Scikit-Learn code base, which is made freely available.Here are the three key points in Simplified Chinese text:* For: 本研究是为了解决决策树模型中的外部数据泛化问题，特别是随机森林和梯度拟合决策树模型。* Methods: 本研究提出了两种新的分割 criterion，用于决策树模型中的分割过程中，以便在不同的时间和地点上进行数据分割。* Results: 本研究通过一系列唯一的实验，展示了新分割 criterion 的优势，可以在尝试样本中提高 metric 的表现。新 criterion 被 integrate 到 Scikit-Learn 代码库中的一个状态最佳的梯度拟合决策树模型中，并且免费释出。<details>
<summary>Abstract</summary>
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optimal over the entire data set pooled together, which is the default setting. We describe the new splitting criteria in detail and develop unique experiments to showcase the benefits of these new criteria, which improve metrics in our experiments out-of-sample. The new criteria are incorporated into the a state-of-the-art gradient boosted decision tree model in the Scikit-Learn code base, which is made freely available.
</details>
<details>
<summary>摘要</summary>
The new splitting criteria use era-wise information associated with each data point to find split points that are optimal across all disjoint eras in the data, rather than optimal over the entire data set pooled together. We describe the new splitting criteria in detail and conduct unique experiments to demonstrate their benefits, which improve metrics out-of-sample.We have incorporated the new splitting criteria into a state-of-the-art gradient boosted decision tree model in the Scikit-Learn code base and made it freely available. This research provides a new approach to addressing distributional shifts in machine learning and improving the generalization of tree-based models.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Deep-Learning-Technique-for-Morphology-Preserved-Fetal-ECG-Extraction-from-Mother-ECG-using-1D-CycleGAN"><a href="#A-Novel-Deep-Learning-Technique-for-Morphology-Preserved-Fetal-ECG-Extraction-from-Mother-ECG-using-1D-CycleGAN" class="headerlink" title="A Novel Deep Learning Technique for Morphology Preserved Fetal ECG Extraction from Mother ECG using 1D-CycleGAN"></a>A Novel Deep Learning Technique for Morphology Preserved Fetal ECG Extraction from Mother ECG using 1D-CycleGAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03759">http://arxiv.org/abs/2310.03759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Promit Basak, A. H. M Nazmus Sakib, Muhammad E. H. Chowdhury, Nasser Al-Emadi, Huseyin Cagatay Yalcin, Shona Pedersen, Sakib Mahmud, Serkan Kiranyaz, Somaya Al-Maadeed<br>for: 这个研究的目的是监测胎儿心脏的电压信号，以实现胎儿心脏疾病的早期诊断和后续照护。methods: 这个研究使用了1D CycleGAN来重建胎儿心脏电压信号，并且进行了广泛的预处理和适当的框架，以维持信号的结构。results: 这个研究的结果显示，使用1D CycleGAN重建胎儿心脏电压信号的方法可以获得高精度的胎儿心脏疾病诊断和胎儿心脏功能监测。这个方法可以实现胎儿心脏疾病的早期诊断和后续照护，并且与现有的相关技术相比，具有较高的精度和可靠性。<details>
<summary>Abstract</summary>
Monitoring the electrical pulse of fetal heart through a non-invasive fetal electrocardiogram (fECG) can easily detect abnormalities in the developing heart to significantly reduce the infant mortality rate and post-natal complications. Due to the overlapping of maternal and fetal R-peaks, the low amplitude of the fECG, systematic and ambient noises, typical signal extraction methods, such as adaptive filters, independent component analysis, empirical mode decomposition, etc., are unable to produce satisfactory fECG. While some techniques can produce accurate QRS waves, they often ignore other important aspects of the ECG. Our approach, which is based on 1D CycleGAN, can reconstruct the fECG signal from the mECG signal while maintaining the morphology due to extensive preprocessing and appropriate framework. The performance of our solution was evaluated by combining two available datasets from Physionet, "Abdominal and Direct Fetal ECG Database" and "Fetal electrocardiograms, direct and abdominal with reference heartbeat annotations", where it achieved an average PCC and Spectral-Correlation score of 88.4% and 89.4%, respectively. It detects the fQRS of the signal with accuracy, precision, recall and F1 score of 92.6%, 97.6%, 94.8% and 96.4%, respectively. It can also accurately produce the estimation of fetal heart rate and R-R interval with an error of 0.25% and 0.27%, respectively. The main contribution of our work is that, unlike similar studies, it can retain the morphology of the ECG signal with high fidelity. The accuracy of our solution for fetal heart rate and R-R interval length is comparable to existing state-of-the-art techniques. This makes it a highly effective tool for early diagnosis of fetal heart diseases and regular health checkups of the fetus.
</details>
<details>
<summary>摘要</summary>
监测胎儿心脏电压通过非侵入式胎儿电cardiogram (fECG) 可以轻松地检测胎儿心脏发育异常，从而减少新生儿死亡率和哺乳期后的合并症状。由于胎母和胎儿的R峰重叠，低强度fECG，系统性和 ambient 噪声，传统的信号提取方法，如适应过滤、独立 componenets 分析、empirical mode decomposition 等，通常无法生成满意的fECG。虽然一些技术可以生成准确的QRS波，但它们通常忽略了其他重要的ECG方面。我们的方法基于1D CycleGAN，可以从mECG信号中重建fECG信号，同时保持信号的形态，因为我们进行了广泛的预处理和适当的框架。我们的解决方案的性能得到了两个可用的Physionet数据集("Abdominal and Direct Fetal ECG Database"和"Fetal electrocardiograms, direct and abdominal with reference heartbeat annotations")的评估，其中获得了88.4%和89.4%的PCC和spectral-correlation分数。它可以准确地检测信号中的fQRS，并具有92.6%、97.6%、94.8%和96.4%的准确率、精度、回归率和F1分数。它还可以准确地计算胎儿心率和R-R间隔的误差，分别为0.25%和0.27%。我们的工作的主要贡献在于，与其他相似的研究不同，可以保持ECG信号的形态高度准确。我们的解决方案的准确率和R-R间隔长度与现有的状态 искусственный智能技术相当。这使得它成为了诊断胎心疾病的高效工具，以及哺乳期后胎心健康检查的重要工具。
</details></li>
</ul>
<hr>
<h2 id="When-Automated-Assessment-Meets-Automated-Content-Generation-Examining-Text-Quality-in-the-Era-of-GPTs"><a href="#When-Automated-Assessment-Meets-Automated-Content-Generation-Examining-Text-Quality-in-the-Era-of-GPTs" class="headerlink" title="When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs"></a>When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14488">http://arxiv.org/abs/2309.14488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nd-hal/automated-ml-scoring-versus-generation">https://github.com/nd-hal/automated-ml-scoring-versus-generation</a></li>
<li>paper_authors: Marialena Bevilacqua, Kezia Oketch, Ruiyang Qin, Will Stamey, Xinyuan Zhang, Yi Gan, Kai Yang, Ahmed Abbasi</li>
<li>for: 这个论文主要研究了机器学习（ML）模型在文本数据评分中的应用和发展。</li>
<li>methods: 该论文使用了多种机器学习模型，包括文本生成大语言模型（GPTs）和卷积神经网络（CNN&#x2F;RNN），对人类生成的文本和GPT生成的文本进行评分和评估。</li>
<li>results: 研究发现，使用 transformer 预训练语言模型（PLMs）可以更准确地评分人类生成的文本质量，而 traditional deep learning 和特征基于的 ML 模型则更倾向于评分人类文本较高。此外，研究还发现，transformer PLMs 具有更强的泛化能力，可以更好地处理 GPT 生成的文本。<details>
<summary>Abstract</summary>
The use of machine learning (ML) models to assess and score textual data has become increasingly pervasive in an array of contexts including natural language processing, information retrieval, search and recommendation, and credibility assessment of online content. A significant disruption at the intersection of ML and text are text-generating large-language models such as generative pre-trained transformers (GPTs). We empirically assess the differences in how ML-based scoring models trained on human content assess the quality of content generated by humans versus GPTs. To do so, we propose an analysis framework that encompasses essay scoring ML-models, human and ML-generated essays, and a statistical model that parsimoniously considers the impact of type of respondent, prompt genre, and the ML model used for assessment model. A rich testbed is utilized that encompasses 18,460 human-generated and GPT-based essays. Results of our benchmark analysis reveal that transformer pretrained language models (PLMs) more accurately score human essay quality as compared to CNN/RNN and feature-based ML methods. Interestingly, we find that the transformer PLMs tend to score GPT-generated text 10-15\% higher on average, relative to human-authored documents. Conversely, traditional deep learning and feature-based ML models score human text considerably higher. Further analysis reveals that although the transformer PLMs are exclusively fine-tuned on human text, they more prominently attend to certain tokens appearing only in GPT-generated text, possibly due to familiarity/overlap in pre-training. Our framework and results have implications for text classification settings where automated scoring of text is likely to be disrupted by generative AI.
</details>
<details>
<summary>摘要</summary>
使用机器学习（ML）模型评分文本数据已经在多种场景中广泛应用，包括自然语言处理、信息检索、搜索和推荐、以及在线内容可靠性评分。一种 significante 的干预在机器学习和文本之间的交叉处是文本生成大语言模型（GPTs）。我们employs一种分析框架，覆盖了文本评分ML模型、人类和GPTs生成的文本，以及一个简单的统计模型，考虑了评分模型的类型、提示类型和评分模型的影响。我们使用了一个丰富的测试环境，包括18,460个人类生成和GPTs生成的文本。我们的基准分析结果显示，transformer预训练语言模型（PLMs）在评分人类文本质量方面更为准确，比起CNN/RNN和特征基于ML方法。另外，我们发现transformer PLMs对GPTs生成的文本进行评分，相对于人类写作的文本，提高了10-15%的平均分。然而，传统的深度学习和特征基于ML方法对人类文本进行评分，显著高于。进一步的分析表明，although transformer PLMs是仅仅特征基于人类文本进行finetune，它们更加强调在GPTs生成的文本中出现的某些字符，可能是因为预训练中的熟悉/重叠。我们的框架和结果对于文本分类设置，其中自动评分文本可能受到生成AI的干扰有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Ensemble-and-Transfer-Learning-For-An-End-To-End-Auto-Colorized-Image-Detection-Model"><a href="#Incorporating-Ensemble-and-Transfer-Learning-For-An-End-To-End-Auto-Colorized-Image-Detection-Model" class="headerlink" title="Incorporating Ensemble and Transfer Learning For An End-To-End Auto-Colorized Image Detection Model"></a>Incorporating Ensemble and Transfer Learning For An End-To-End Auto-Colorized Image Detection Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14478">http://arxiv.org/abs/2309.14478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Samir Ragab, Shereen Aly Taie, Howida Youssry Abdelnaby</li>
<li>for: 这个论文旨在提出一种新的图像色调检测方法，用于分辨天然颜色图像和计算机色调图像。</li>
<li>methods: 该方法结合了传输学习和集成学习的优点，并使用预训练的VGG16和Resnet50树脊，以及Mobile Net v2或Efficientnet特征向量。</li>
<li>results: 该模型在分类性能和泛化能力方面表现出色，准确率在94.55%到99.13%之间，偏差总错误率很低。与现有状态的先进模型相比，该模型表现出了更高的分类性能和泛化能力。<details>
<summary>Abstract</summary>
Image colorization is the process of colorizing grayscale images or recoloring an already-color image. This image manipulation can be used for grayscale satellite, medical and historical images making them more expressive. With the help of the increasing computation power of deep learning techniques, the colorization algorithms results are becoming more realistic in such a way that human eyes cannot differentiate between natural and colorized images. However, this poses a potential security concern, as forged or illegally manipulated images can be used illegally. There is a growing need for effective detection methods to distinguish between natural color and computer-colorized images. This paper presents a novel approach that combines the advantages of transfer and ensemble learning approaches to help reduce training time and resource requirements while proposing a model to classify natural color and computer-colorized images. The proposed model uses pre-trained branches VGG16 and Resnet50, along with Mobile Net v2 or Efficientnet feature vectors. The proposed model showed promising results, with accuracy ranging from 94.55% to 99.13% and very low Half Total Error Rate values. The proposed model outperformed existing state-of-the-art models regarding classification performance and generalization capabilities.
</details>
<details>
<summary>摘要</summary>
Image colorization是将灰度图像或已经颜色化的图像中的颜色更改的过程。这种图像修改可以用于灰度卫星图像、医疗图像和历史图像等，使其更加表达力。随着深度学习技术的计算能力的提高，图像色化算法的结果变得越来越真实，以至于人眼无法分辨天然颜色和计算机颜色化图像之间的差异。然而，这也带来了安全性问题，因为假或非法修改的图像可以用于违法活动。随着需求的增长，有效地检测天然颜色和计算机颜色化图像的方法变得越来越重要。本文提出了一种新的方法，该方法结合了传输学习和集成学习的优点，以减少训练时间和资源需求，并提出了一种用于分类天然颜色和计算机颜色化图像的模型。该模型使用预训练分支VGG16和Resnet50，以及Mobile Net v2或Efficientnet特征向量。该模型的实验结果表现出色，准确率在94.55%至99.13%之间，并且半总错误率很低。该模型超越了现有的状态对模型，以 regards to classification performance和总体能力。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Double-Q-Learning-for-Continuous-Reinforcement-Learning"><a href="#Adapting-Double-Q-Learning-for-Continuous-Reinforcement-Learning" class="headerlink" title="Adapting Double Q-Learning for Continuous Reinforcement Learning"></a>Adapting Double Q-Learning for Continuous Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14471">http://arxiv.org/abs/2309.14471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arsenii Kuznetsov</li>
<li>for: 这个论文主要针对off-policy reinforcement learning中的偏高偏估问题，提出了一种新的偏估控制方法。</li>
<li>methods: 该方法基于一个混合策略，每个策略组件由两个分立的网络评估，从而消除了基于偏估的假设。</li>
<li>results: 该方法在一些MuJoCo环境中达到了near-SOTA的result，显示了其可行性和有效性。<details>
<summary>Abstract</summary>
Majority of off-policy reinforcement learning algorithms use overestimation bias control techniques. Most of these techniques rooted in heuristics, primarily addressing the consequences of overestimation rather than its fundamental origins. In this work we present a novel approach to the bias correction, similar in spirit to Double Q-Learning. We propose using a policy in form of a mixture with two components. Each policy component is maximized and assessed by separate networks, which removes any basis for the overestimation bias. Our approach shows promising near-SOTA results on a small set of MuJoCo environments.
</details>
<details>
<summary>摘要</summary>
大多数Off-policy reinforcement learning算法使用过估偏调技术。这些技术基于规则，主要是解决过估的后果而不是其基本原因。在这项工作中，我们提出了一种新的偏调修正方法，类似于Double Q-Learning。我们提议使用一个策略组合，其中每个策略组件是由两个分开的网络评估和最大化。这种方法可以消除任何基于过估偏调的基础。我们的方法在一些MuJoCo环境上显示了有优的近SOTA结果。
</details></li>
</ul>
<hr>
<h2 id="DefGoalNet-Contextual-Goal-Learning-from-Demonstrations-For-Deformable-Object-Manipulation"><a href="#DefGoalNet-Contextual-Goal-Learning-from-Demonstrations-For-Deformable-Object-Manipulation" class="headerlink" title="DefGoalNet: Contextual Goal Learning from Demonstrations For Deformable Object Manipulation"></a>DefGoalNet: Contextual Goal Learning from Demonstrations For Deformable Object Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14463">http://arxiv.org/abs/2309.14463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bao Thach, Tanner Watts, Shing-Hei Ho, Tucker Hermans, Alan Kuntz</li>
<li>for: 解决控制柔体物体到目标形状的问题，即shape servoing问题。</li>
<li>methods: 开发了一种基于神经网络的 DefGoalNet，可以直接从人类示范中学习柔体物体目标形状。</li>
<li>results: 在 simulate 和physical robot 上进行了多种任务测试，包括手术压缩任务，并达到了高达90%的成功率，表明该方法可以有效地解决shape servoing问题， bringing deformable object manipulation closer to practical, real-world applications.<details>
<summary>Abstract</summary>
Shape servoing, a robotic task dedicated to controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. An issue arises, however, with the reliance on the specification of a goal shape. This goal has been obtained either by a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. In this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. We demonstrate our method's effectiveness on various robotic tasks, both in simulation and on a physical robot. Notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. These results mark a substantial advancement in enabling shape servoing methods to bring deformable object manipulation closer to practical, real-world applications.
</details>
<details>
<summary>摘要</summary>
shape servoing, a robotic task focused on controlling objects to desired goal shapes, is a promising approach to deformable object manipulation. however, a challenge arises with the reliance on the specification of a goal shape. this goal has been obtained either through a laborious domain knowledge engineering process or by manually manipulating the object into the desired shape and capturing the goal shape at that specific moment, both of which are impractical in various robotic applications. in this paper, we solve this problem by developing a novel neural network DefGoalNet, which learns deformable object goal shapes directly from a small number of human demonstrations. we demonstrate our method's effectiveness on various robotic tasks, both in simulation and on a physical robot. notably, in the surgical retraction task, even when trained with as few as 10 demonstrations, our method achieves a median success percentage of nearly 90%. these results mark a substantial advancement in enabling shape servoing methods to bring deformable object manipulation closer to practical, real-world applications.
</details></li>
</ul>
<hr>
<h2 id="Online-Active-Learning-For-Sound-Event-Detection"><a href="#Online-Active-Learning-For-Sound-Event-Detection" class="headerlink" title="Online Active Learning For Sound Event Detection"></a>Online Active Learning For Sound Event Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14460">http://arxiv.org/abs/2309.14460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Lindsey, Ankit Shah, Francis Kubala, Richard M. Stern</li>
<li>for: 这篇论文是为了提高Sound Event Detection（SED）中的监督学习效率而写的。</li>
<li>methods: 这篇论文使用了线上活动学习（OAL）来减少监督学习需要的时间和努力。它还使用了新的损失函数来解决现有OAL方法中的问题，例如气流分布的变化和数据漂移。</li>
<li>results: 实验结果显示，使用OAL可以将SED监督学习的时间和努力缩减到SONYC dataset中的一半，并且新的方法可以成功地解决现有OAL方法中的问题。<details>
<summary>Abstract</summary>
Data collection and annotation is a laborious, time-consuming prerequisite for supervised machine learning tasks. Online Active Learning (OAL) is a paradigm that addresses this issue by simultaneously minimizing the amount of annotation required to train a classifier and adapting to changes in the data over the duration of the data collection process. Prior work has indicated that fluctuating class distributions and data drift are still common problems for OAL. This work presents new loss functions that address these challenges when OAL is applied to Sound Event Detection (SED). Experimental results from the SONYC dataset and two Voice-Type Discrimination (VTD) corpora indicate that OAL can reduce the time and effort required to train SED classifiers by a factor of 5 for SONYC, and that the new methods presented here successfully resolve issues present in existing OAL methods.
</details>
<details>
<summary>摘要</summary>
数据收集和注释是超级vised机器学习任务的必要前置条件，但它们是时间consuming和劳动密集的。在线活动学习（OAL）是一种方法，它同时减少了训练分类器所需的注释量和适应数据的变化过程中的数据风险。前一个研究表示，在OAL中仍然存在涨落分布和数据漂移的问题。这个工作提出了新的损失函数，以解决这些问题在音频事件检测（SED）领域中。实验结果来自SONYC数据集和两个语音类型识别（VTD） corpora，表明OAL可以将SED分类器训练所需的时间和劳动量减少到SONYC数据集的5倍，并且新的方法在现有OAL方法中成功解决了问题。
</details></li>
</ul>
<hr>
<h2 id="Self-Recovery-Prompting-Promptable-General-Purpose-Service-Robot-System-with-Foundation-Models-and-Self-Recovery"><a href="#Self-Recovery-Prompting-Promptable-General-Purpose-Service-Robot-System-with-Foundation-Models-and-Self-Recovery" class="headerlink" title="Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery"></a>Self-Recovery Prompting: Promptable General Purpose Service Robot System with Foundation Models and Self-Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14425">http://arxiv.org/abs/2309.14425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mimo Shirasaka, Tatsuya Matsushima, Soshi Tsunashima, Yuya Ikeda, Aoi Horo, So Ikoma, Chikaha Tsuji, Hikaru Wada, Tsunekazu Omija, Dai Komukai, Yutaka Matsuo Yusuke Iwasawa</li>
<li>for: 本研究旨在开发一个可以执行多种任务的通用服务机器人（GPSR），需要一个高度普适和适应任务和环境的系统。</li>
<li>methods: 我们首先基于多个基础模型开发了一个高级GPSR系统，并通过让每个模型提示来使其普适和适应。</li>
<li>results: 我们发现在更实际的GPSR应用场景中存在三种类型的失败情况：缺乏信息、错误的规划生成和执行失败。我们则提出了自适应提示管道，以探索必要的信息并修改提示来恢复失败。我们的实验证明，具有自适应机制的系统可以完成任务并解决多种失败情况。<details>
<summary>Abstract</summary>
A general-purpose service robot (GPSR), which can execute diverse tasks in various environments, requires a system with high generalizability and adaptability to tasks and environments. In this paper, we first developed a top-level GPSR system for worldwide competition (RoboCup@Home 2023) based on multiple foundation models. This system is both generalizable to variations and adaptive by prompting each model. Then, by analyzing the performance of the developed system, we found three types of failure in more realistic GPSR application settings: insufficient information, incorrect plan generation, and plan execution failure. We then propose the self-recovery prompting pipeline, which explores the necessary information and modifies its prompts to recover from failure. We experimentally confirm that the system with the self-recovery mechanism can accomplish tasks by resolving various failure cases. Supplementary videos are available at https://sites.google.com/view/srgpsr .
</details>
<details>
<summary>摘要</summary>
一种通用服务机器人（GPSR），能够执行多种任务在多种环境中，需要一个高度通用和适应性的系统。在这篇论文中，我们首先基于多个基础模型开发了一个全面的GPSR系统，并通过提示每个模型来使其通用和适应性更高。然后，通过分析系统的性能，我们发现在更实际的GPSR应用场景中存在三种失败类型：不充分的信息、错误的计划生成和计划执行失败。我们提出了自动恢复提示管道，以探索所需的信息并修改提示来解决失败。我们通过实验证明，具有自动恢复机制的系统可以成功完成任务，并解决多种失败情况。补充视频可以在https://sites.google.com/view/srgpsr 中找到。
</details></li>
</ul>
<hr>
<h2 id="Extreme-Parkour-with-Legged-Robots"><a href="#Extreme-Parkour-with-Legged-Robots" class="headerlink" title="Extreme Parkour with Legged Robots"></a>Extreme Parkour with Legged Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14341">http://arxiv.org/abs/2309.14341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuxin Cheng, Kexin Shi, Ananye Agarwal, Deepak Pathak</li>
<li>for: 本研究旨在开发一种小型低成本机器人，以便它可以通过困难的满足某些难以控制的环境。</li>
<li>methods: 该研究使用一种单一的前视频摄像头和深度学习算法，以便从摄像头图像直接生成高精度控制行为。</li>
<li>results: 研究结果显示，该机器人可以通过跳高障碍物、跨越差距、悬停和跑过倾斜的坡道等动作，并可以在新的障碍物环境中进行普适化。<details>
<summary>Abstract</summary>
Humans can perform parkour by traversing obstacles in a highly dynamic fashion requiring precise eye-muscle coordination and movement. Getting robots to do the same task requires overcoming similar challenges. Classically, this is done by independently engineering perception, actuation, and control systems to very low tolerances. This restricts them to tightly controlled settings such as a predetermined obstacle course in labs. In contrast, humans are able to learn parkour through practice without significantly changing their underlying biology. In this paper, we take a similar approach to developing robot parkour on a small low-cost robot with imprecise actuation and a single front-facing depth camera for perception which is low-frequency, jittery, and prone to artifacts. We show how a single neural net policy operating directly from a camera image, trained in simulation with large-scale RL, can overcome imprecise sensing and actuation to output highly precise control behavior end-to-end. We show our robot can perform a high jump on obstacles 2x its height, long jump across gaps 2x its length, do a handstand and run across tilted ramps, and generalize to novel obstacle courses with different physical properties. Parkour videos at https://extreme-parkour.github.io/
</details>
<details>
<summary>摘要</summary>
人类可以通过穿梭障碍物来完成公园OUR，需要精准的眼睛肌肉协调和运动。为了让机器人做同样的任务，需要超越类似的挑战。传统上，这是通过独立地工程感知、行动和控制系统来实现的，这会限制它们只能在严格控制的室内预先设定的赛跑课程中运行。与此相反，人类可以通过练习而不是改变基本生物结构来学习公园OUR。在这篇论文中，我们采用类似的方法，使用一个小型低成本机器人，具有不精准的运动和单个前方深度摄像头来感知，它的摄像头图像是低频、颤动和噪声易产生的。我们证明了一个单一神经网络策略，直接从摄像头图像中获取控制行为，通过大规模RL在模拟中训练，可以超越不精准的感知和运动，并输出高精度的控制行为。我们的机器人可以跳高障碍物2倍其高度，跳跃差2倍其长度，执行手stand和跑在倾斜的滚动道上，并可以通过不同物理特性的新障碍课程进行扩展。有关公园OUR视频，请参考https://extreme-parkour.github.io/
</details></li>
</ul>
<hr>
<h2 id="Joint-Audio-and-Speech-Understanding"><a href="#Joint-Audio-and-Speech-Understanding" class="headerlink" title="Joint Audio and Speech Understanding"></a>Joint Audio and Speech Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14405">http://arxiv.org/abs/2309.14405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YuanGongND/ltu">https://github.com/YuanGongND/ltu</a></li>
<li>paper_authors: Yuan Gong, Alexander H. Liu, Hongyin Luo, Leonid Karlinsky, James Glass</li>
<li>for: 这篇论文旨在构建一个基于机器学习的听说识别和理解模型，以便更好地理解人类听说信号中的语音和非语音声音。</li>
<li>methods: 该模型基于 integrate Whisper 和 LLaMA 两个模块，分别用于听说识别和理解语音和非语音声音。</li>
<li>results: 模型可以同时识别和理解语音和非语音声音，包括语音和非语音声音的识别、语音特征提取、语音识别和语音理解等任务。<details>
<summary>Abstract</summary>
Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper as a perception module and LLaMA as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.
</details>
<details>
<summary>摘要</summary>
人类在听到各种各样的声音信号周围，包括语音和非语音声音。认识和理解语音和非语音声音事件，以及对它们之间的关系的深入理解，是人类的基本认知能力。我们现在第一次建立了一个机器学习模型，即LTU-AS，它具有类似于人类听觉的概念性和高级逻辑能力。具体来说，通过将Whisper作为感知模块和LLaMA作为逻辑模块相结合，LTU-AS可以同时认识和共同理解说话文本、语音非语言特征和非语音声音事件——大致上来说，听到 audio 信号中的一切可见事物。
</details></li>
</ul>
<hr>
<h2 id="UnitedHuman-Harnessing-Multi-Source-Data-for-High-Resolution-Human-Generation"><a href="#UnitedHuman-Harnessing-Multi-Source-Data-for-High-Resolution-Human-Generation" class="headerlink" title="UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation"></a>UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14335">http://arxiv.org/abs/2309.14335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/unitedhuman/unitedhuman">https://github.com/unitedhuman/unitedhuman</a></li>
<li>paper_authors: Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Wayne Wu, Ziwei Liu</li>
<li>for: 提高人像生成质量</li>
<li>methods: 多源数据集合并学习高分辨率人像生成模型</li>
<li>results: 与单一数据集学习的模型相比，通过jointly learning from multi-source data achieve superior quality in human image generation.<details>
<summary>Abstract</summary>
Human generation has achieved significant progress. Nonetheless, existing methods still struggle to synthesize specific regions such as faces and hands. We argue that the main reason is rooted in the training data. A holistic human dataset inevitably has insufficient and low-resolution information on local parts. Therefore, we propose to use multi-source datasets with various resolution images to jointly learn a high-resolution human generative model. However, multi-source data inherently a) contains different parts that do not spatially align into a coherent human, and b) comes with different scales. To tackle these challenges, we propose an end-to-end framework, UnitedHuman, that empowers continuous GAN with the ability to effectively utilize multi-source data for high-resolution human generation. Specifically, 1) we design a Multi-Source Spatial Transformer that spatially aligns multi-source images to full-body space with a human parametric model. 2) Next, a continuous GAN is proposed with global-structural guidance and CutMix consistency. Patches from different datasets are then sampled and transformed to supervise the training of this scale-invariant generative model. Extensive experiments demonstrate that our model jointly learned from multi-source data achieves superior quality than those learned from a holistic dataset.
</details>
<details>
<summary>摘要</summary>
人类生成技术已经取得了 significiant 的进步，然而现有的方法仍然困难将特定的区域如面部和手臂等生成出来。我们认为这主要的原因在于训练数据。总体的人类数据集缺乏和低分辨率的地方部分信息，因此我们提议使用多源数据集，包括不同分辨率的图像，并将其集成到一个高分辨率的人类生成模型中。然而，多源数据集具有以下两个挑战：一是不同的部分不能够在一个准确的人类空间中匹配，二是不同的数据集来源的图像尺寸不同。为了解决这些挑战，我们提出了一个综合框架，名为 UnitedHuman，它使得 kontinuous GAN 能够有效地利用多源数据来生成高分辨率的人类图像。具体来说，我们设计了一个 Multi-Source Spatial Transformer，它将多源图像转换到全身人类空间中，并使用人类参数模型来进行匹配。然后，我们提出了一个 kontinuous GAN，它具有全STRUCTURE 导向和 CutMix 一致性。不同的数据集中的小块被随机选择并转换，以supervise  kontinuous GAN 的训练。我们的实验表明，我们从多源数据集中 JOINTLY 学习的模型可以超过来自整体数据集的模型。
</details></li>
</ul>
<hr>
<h2 id="LinGCN-Structural-Linearized-Graph-Convolutional-Network-for-Homomorphically-Encrypted-Inference"><a href="#LinGCN-Structural-Linearized-Graph-Convolutional-Network-for-Homomorphically-Encrypted-Inference" class="headerlink" title="LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference"></a>LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14331">http://arxiv.org/abs/2309.14331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harveyp123/lingcn-neurips23">https://github.com/harveyp123/lingcn-neurips23</a></li>
<li>paper_authors: Hongwu Peng, Ran Ran, Yukui Luo, Jiahui Zhao, Shaoyi Huang, Kiran Thorat, Tong Geng, Chenghong Wang, Xiaolin Xu, Wujie Wen, Caiwen Ding</li>
<li>for: 这个论文是为了提高Graph Convolution Network (GCN)模型的安全性和可扩展性而写的。</li>
<li>methods: 这个论文使用了Homomorphic Encryption (HE)技术来保护客户端数据，并且提出了一个名为LinGCN的框架，用于实现GCN模型的加密运算。LinGCN使用了分别为node-wise non-linear location selection和compact node-wise polynomial replacement policy两个关键元素，以提高GCN模型的性能和可扩展性。</li>
<li>results: 这个论文的实验结果显示，LinGCN在NTU-XVIEW skeleton joint dataset上具有较高的延迟速度、准确率和可扩展性，相比CryptoGCN等其他解决方案。具体来说，LinGCN在GCN模型的加密运算中实现了14.2倍的延迟速度提升，保持了75%的准确率，并且降低了 multiplication depth。<details>
<summary>Abstract</summary>
The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a model with minimized multiplication depth. (2) A compact node-wise polynomial replacement policy with a second-order trainable activation function, steered towards superior convergence by a two-level distillation approach from an all-ReLU based teacher model. (3) an enhanced HE solution that enables finer-grained operator fusion for node-wise activation functions, further reducing multiplication level consumption in HE-based inference. Our experiments on the NTU-XVIEW skeleton joint dataset reveal that LinGCN excels in latency, accuracy, and scalability for homomorphically encrypted inference, outperforming solutions such as CryptoGCN. Remarkably, LinGCN achieves a 14.2x latency speedup relative to CryptoGCN, while preserving an inference accuracy of 75% and notably reducing multiplication depth.
</details>
<details>
<summary>摘要</summary>
Graph Convolutional Network (GCN) 模型的发展已经革命化了许多应用程序，超过了人类表现在个人健康监测和金融系统等领域。但是在云端部署GCNs时，隐私问题引起了关注，因为可能会发生对客户数据的敌意攻击。为解决安全问题，使用了同源加密（HE）来保护敏感客户数据。然而，HE引入了实际应用中的重要计算开销。为了解决这些挑战，我们提出了LinGCN框架，用于降低 multiplication depth并优化HE基于GCN的推理性能。LinGCN框架包括三个关键元素：1. 可微分结构线性化算法，并且通过一个参数化的整数指示函数，与模型参数进行共训练，以达到优化目标。这种策略可以实现精细化节点级非线性位置选择，从而降低 multiplication depth。2. 一种减少 multiplication depth的紧凑型节点值替换策略，通过一个二次可训练的活化函数，由一个两级液态灵感法推导，从一个所有ReLU基于教师模型中学习。3. 一种可以进一步减少HE基于节点活化函数的 multiplication level 的加强HE解决方案。我们的实验表明，LinGCN在NTU-XVIEW骨架联合数据集上表现出了较高的延迟、准确率和扩展性，与CRYPTOGCN相比，LinGCN可以实现14.2倍的延迟速度提升，保持75%的推理精度，同时减少 multiplication depth。
</details></li>
</ul>
<hr>
<h2 id="Innovative-Digital-Storytelling-with-AIGC-Exploration-and-Discussion-of-Recent-Advances"><a href="#Innovative-Digital-Storytelling-with-AIGC-Exploration-and-Discussion-of-Recent-Advances" class="headerlink" title="Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances"></a>Innovative Digital Storytelling with AIGC: Exploration and Discussion of Recent Advances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14329">http://arxiv.org/abs/2309.14329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongzhang Gu, Hui Li, Changyue Su, Wayne Wu</li>
<li>for: 这个研究的目的是提高人们对将AI生成内容（AIGC）与数字故事创作的结合现状、局限性和挑战的认识。</li>
<li>methods: 这篇论文使用了现有的AIGC技术和数字故事创作工具，并通过实验和专家采访来研究AIGC与数字故事创作的整体效果和挑战。</li>
<li>results: 研究发现，虽然AIGC可以快速生成图片、音频和音效，但是在复杂的人物动画、表情和声音效果方面，人类仍然无法被代表。此外，AIGC与数字故事创作的结合还存在许多挑战和限制，如人工创作的灵活性和艺术感受的缺失。<details>
<summary>Abstract</summary>
Digital storytelling, as an art form, has struggled with cost-quality balance. The emergence of AI-generated Content (AIGC) is considered as a potential solution for efficient digital storytelling production. However, the specific form, effects, and impacts of this fusion remain unclear, leaving the boundaries of AIGC combined with storytelling undefined. This work explores the current integration state of AIGC and digital storytelling, investigates the artistic value of their fusion in a sample project, and addresses common issues through interviews. Through our study, we conclude that AIGC, while proficient in image creation, voiceover production, and music composition, falls short of replacing humans due to the irreplaceable elements of human creativity and aesthetic sensibilities at present, especially in complex character animations, facial expressions, and sound effects. The research objective is to increase public awareness of the current state, limitations, and challenges arising from combining AIGC and digital storytelling.
</details>
<details>
<summary>摘要</summary>
“数字storytelling”作为艺术形式，困惑于成本质量平衡。人工智能生成内容（AIGC）的出现被视为可能解决高效数字storytelling生产的问题。然而，这两者的结合的具体形式、效果和影响仍然不清楚，“数字storytelling”与AIGC的界限未定。本研究探讨了AIGC与数字storytelling的当前整合状况，研究了这两者艺术价值的融合效果，并通过采访 Addressing common issues. Through our study, we conclude that AIGC, while proficient in image creation, voiceover production, and music composition, falls short of replacing humans due to the irreplaceable elements of human creativity and aesthetic sensibilities at present, especially in complex character animations, facial expressions, and sound effects. The research objective is to increase public awareness of the current state, limitations, and challenges arising from combining AIGC and digital storytelling.
</details></li>
</ul>
<hr>
<h2 id="Physics-of-Language-Models-Part-3-2-Knowledge-Manipulation"><a href="#Physics-of-Language-Models-Part-3-2-Knowledge-Manipulation" class="headerlink" title="Physics of Language Models: Part 3.2, Knowledge Manipulation"></a>Physics of Language Models: Part 3.2, Knowledge Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14402">http://arxiv.org/abs/2309.14402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyuan Allen-Zhu, Yuanzhi Li<br>for:This paper explores the ability of language models to manipulate stored knowledge during inference, specifically focusing on four manipulation types: retrieval, classification, comparison, and inverse search.methods:The authors use pre-trained language models like GPT2&#x2F;3&#x2F;4 and employ Chain of Thoughts (CoTs) during both training and inference to improve performance on simple classification and comparison tasks.results:The paper finds that language models struggle with simple classification and comparison tasks unless CoTs are employed, and they perform poorly in inverse knowledge search, even with adequate instruct fine-tuning. The primary contribution of the paper is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses in language models.Here is the text in Simplified Chinese:for:这篇论文探讨了语言模型在推理过程中是否可以有效地把已经存储的知识 manipulate。methods:作者使用了预训练的语言模型如GPT2&#x2F;3&#x2F;4，并在推理过程中使用链条思维（CoTs）来提高简单的分类和比较任务的性能。results:论文发现，语言模型在简单的分类和比较任务上需要使用CoTs才能够有效地完成，而且它们在反向知识搜索任务中表现不佳，即使有足够的指导 fine-tuning。Primary contribution是一个控制实验的synthetic数据集，以confirm这些语言模型内在的劣势。<details>
<summary>Abstract</summary>
Language models can store vast amounts of factual knowledge, but their ability to use this knowledge for logical reasoning remains questionable. This paper explores a language model's ability to manipulate its stored knowledge during inference. We focus on four manipulation types: retrieval (e.g., "What is person A's attribute X"), classification (e.g., "Is A's attribute X even or odd?"), comparison (e.g., "Is A greater than B in attribute X?") and inverse search (e.g., "Which person's attribute X equals T?")   We observe that pre-trained language models like GPT2/3/4 excel in knowledge retrieval but struggle with simple classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. They also perform poorly in inverse knowledge search, irrespective of the prompts. Our primary contribution is a synthetic dataset for a controlled experiment that confirms these inherent weaknesses: a language model cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored and fully extractable in the models, and despite adequate instruct fine-tuning.
</details>
<details>
<summary>摘要</summary>
语言模型可以存储庞大的 фактические知识，但它们在逻辑推理方面的能力仍然存在问题。这篇论文探讨了语言模型在推理过程中如何 manipulate 存储的知识。我们关注了四种推理方法：提取（例如，"人A的特征X是什么？）、分类（例如，"A的特征X是偶数或奇数？）、比较（例如，"A是B在特征X方面大吗？）以及反向搜索（例如，"谁的特征X等于T？）。我们发现，预训练的语言模型如GPT2/3/4在知识提取方面表现出色，但在简单的分类或比较任务中，除非使用链接思维（CoTs），否则表现不佳。它们还在反向知识搜索方面表现不佳，不管提示是什么。我们的主要贡献是一个控制性的 synthetic 数据集，用于确认这些内在的弱点：语言模型不能效率地从预训练数据中提取知识，即使这些知识完全可以在模型中提取并且受到了充分的训练精化。
</details></li>
</ul>
<hr>
<h2 id="Physics-of-Language-Models-Part-3-1-Knowledge-Storage-and-Extraction"><a href="#Physics-of-Language-Models-Part-3-1-Knowledge-Storage-and-Extraction" class="headerlink" title="Physics of Language Models: Part 3.1, Knowledge Storage and Extraction"></a>Physics of Language Models: Part 3.1, Knowledge Storage and Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14316">http://arxiv.org/abs/2309.14316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyuan Allen Zhu, Yuanzhi Li</li>
<li>for: 本研究探讨了大语言模型是否真正从知识源中提取信息，或者只是通过在训练中遇到类似的问题来回答问题。</li>
<li>methods: 本研究使用控制的半人工生物графи信息来进行深入研究这个问题。</li>
<li>results: 研究发现，模型的知识提取能力与不同多样性度指标的训练数据相关。通过近似线性探测，发现模型在隐藏嵌入名词的位置或者在训练文本中的其他符号的嵌入位置上线性编码知识属性强相关。<details>
<summary>Abstract</summary>
Large language models can store extensive world knowledge, often extractable through question-answering (e.g., "What is Abraham Lincoln's birthday?"). However, it's unclear whether the model answers questions based on exposure to exact/similar questions during training, or if it genuinely extracts knowledge from the source (e.g., Wikipedia biographies).   In this paper, we conduct an in-depth study of this problem using a controlled set of semi-synthetic biography data. We uncover a relationship between the model's knowledge extraction ability and different diversity measures of the training data. We conduct (nearly) linear probing, revealing a strong correlation between this relationship and whether the model (nearly) linearly encodes the knowledge attributes at the hidden embedding of the entity names, or across the embeddings of other tokens in the training text.
</details>
<details>
<summary>摘要</summary>
大型语言模型可以储存广泛的世界知识，通常通过问答（例如，“亚伯拉罕林肯的生日是什么？）来抽出知识。然而，是否 modelo 回答问题基于训练时期所曝露的具体/相似问题，或是它实际提取知识从源（例如，Wikipedia 传记），这是一个未知的问题。在这篇论文中，我们透过一个控制的半人工生物agraph 数据集进行了深入的研究。我们发现了知识提取能力和不同多样性度量的训练数据之间的关系。我们进行了（近乎）直线探索，发现这个关系和模型（近乎）直线将知识属性嵌入到实体名称的隐藏嵌入中，或者在训练文本中的其他 tokens 的嵌入中。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Different-Explanations-for-Image-Classifiers"><a href="#Multiple-Different-Explanations-for-Image-Classifiers" class="headerlink" title="Multiple Different Explanations for Image Classifiers"></a>Multiple Different Explanations for Image Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14309">http://arxiv.org/abs/2309.14309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hana Chockler, David A. Kelly, Daniel Kroening</li>
<li>for: 提供多个预测结果的算法和工具，以帮助理解黑盒图像分类器的行为。</li>
<li>methods: 基于 causal 理论，使用原理导向的方法计算多个预测结果。</li>
<li>results: 在 ImageNet-mini benchmark 上，REX 算法可以对 7 倍更多的图像进行多个预测结果计算，与之前的工作具有显著的提升。<details>
<summary>Abstract</summary>
Existing explanation tools for image classifiers usually give only one single explanation for an image. For many images, however, both humans and image classifiers accept more than one explanation for the image label. Thus, restricting the number of explanations to just one severely limits the insight into the behavior of the classifier. In this paper, we describe an algorithm and a tool, REX, for computing multiple explanations of the output of a black-box image classifier for a given image. Our algorithm uses a principled approach based on causal theory. We analyse its theoretical complexity and provide experimental results showing that REX finds multiple explanations on 7 times more images than the previous work on the ImageNet-mini benchmark.
</details>
<details>
<summary>摘要</summary>
现有的图像分类器解释工具通常只给出一个图像的解释。然而，许多图像都可以由人类和图像分类器接受多个解释。因此，只给出一个解释将限制我们对分类器的行为的理解。在这篇论文中，我们描述了一种算法和工具，即REX，用于计算一个黑板图像分类器的输出对某图像的多个解释。我们的算法基于 causal theory，我们分析了其理论复杂性，并提供了实验结果，显示REX在ImageNet-mini benchmark上可以对7个图像计算多个解释。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-Class-Activation-Maps-for-Visualization-Explainability"><a href="#Overview-of-Class-Activation-Maps-for-Visualization-Explainability" class="headerlink" title="Overview of Class Activation Maps for Visualization Explainability"></a>Overview of Class Activation Maps for Visualization Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14304">http://arxiv.org/abs/2309.14304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anh Pham Thi Minh</li>
<li>for: 本研究旨在概述过去几年内Class Activation Map（CAM）方法的演进，以及评价CAM的精度和可读性。</li>
<li>methods: 本研究使用了多种方法来评价CAM的精度和可读性，包括使用不同的评价指标和附加技术来提高CAM的精度和可读性。</li>
<li>results: 本研究发现了一些CAM方法的缺点和限制，并提出了未来研究的可能性，以提高CAM的可读性和精度。<details>
<summary>Abstract</summary>
Recent research in deep learning methodology has led to a variety of complex modelling techniques in computer vision (CV) that reach or even outperform human performance. Although these black-box deep learning models have obtained astounding results, they are limited in their interpretability and transparency which are critical to take learning machines to the next step to include them in sensitive decision-support systems involving human supervision. Hence, the development of explainable techniques for computer vision (XCV) has recently attracted increasing attention. In the realm of XCV, Class Activation Maps (CAMs) have become widely recognized and utilized for enhancing interpretability and insights into the decision-making process of deep learning models. This work presents a comprehensive overview of the evolution of Class Activation Map methods over time. It also explores the metrics used for evaluating CAMs and introduces auxiliary techniques to improve the saliency of these methods. The overview concludes by proposing potential avenues for future research in this evolving field.
</details>
<details>
<summary>摘要</summary>
Within the realm of XCV, Class Activation Maps (CAMs) have become widely recognized and utilized for enhancing interpretability and insights into the decision-making process of deep learning models. This overview provides a comprehensive review of the evolution of CAM methods over time, explores the metrics used for evaluating CAMs, and introduces auxiliary techniques to improve the saliency of these methods. The overview concludes by proposing potential avenues for future research in this rapidly evolving field.Translation notes:* "black-box" refers to the lack of transparency and interpretability of deep learning models.* "sensitive decision-support systems" refers to systems that require human supervision and involve critical decision-making.* "XCV" stands for "explainable computer vision".* "CAMs" stands for "Class Activation Maps".
</details></li>
</ul>
<hr>
<h2 id="Identifying-the-Risks-of-LM-Agents-with-an-LM-Emulated-Sandbox"><a href="#Identifying-the-Risks-of-LM-Agents-with-an-LM-Emulated-Sandbox" class="headerlink" title="Identifying the Risks of LM Agents with an LM-Emulated Sandbox"></a>Identifying the Risks of LM Agents with an LM-Emulated Sandbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15817">http://arxiv.org/abs/2309.15817</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ryoungj/toolemu">https://github.com/ryoungj/toolemu</a></li>
<li>paper_authors: Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, Tatsunori Hashimoto<br>for:The paper aims to address the challenges of identifying risks in Language Model (LM) agents and tools, such as leaking private data or causing financial losses, by introducing a framework called ToolEmu and an automatic safety evaluator.methods:The ToolEmu framework uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios without manual instantiation. The evaluator examines agent failures and quantifies associated risks.results:The tool emulator and evaluator were tested through human evaluation, and the results showed that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. The paper also provides a quantitative risk analysis of current LM agents and identifies numerous failures with potentially severe outcomes, highlighting the need to develop safer LM agents for real-world deployment.Here’s the simplified Chinese text:for: 这篇论文目标是解决语言模型（LM）代理和工具中的风险识别问题，如泄露private数据或者导致金融损失，通过引入工具模拟器（ToolEmu）和自动安全评估器。methods: 工具模拟器使用LM来模拟工具执行，无需手动实例化，可以测试LM代理在多种工具和enario下。自动安全评估器对代理失败进行评估并评估相关风险。results: 通过人工评估，工具模拟器和自动安全评估器的测试结果显示，68.8%的失败是真实的世界代理失败。论文还提供了当前LM代理的量化风险分析，并发现了许多可能导致严重后果的失败，高亮了需要开发更安全的LM代理 для实际应用。<details>
<summary>Abstract</summary>
Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.
</details>
<details>
<summary>摘要</summary>
近期语言模型（LM）代理和工具的应用，如ChatGPT插件，提供了一个富有可能性的功能集，但也扩大了潜在风险的范围 - 如泄露私人数据或导致财务损失。识别这些风险是劳动密集的，需要实施工具，手动设置测试enario的环境，并找到危险的场景。随着工具和代理的复杂度的增加，测试这些代理的成本将在不断增加，使得找到高度投资、长尾风险变得越来越困难。为解决这些挑战，我们引入 ToolEmu：一个基于LM的工具抽象框架，可以在多种工具和enario下测试LM代理，无需手动实例化。同时，我们开发了基于LM的自动安全评估工具，可以对代理失败进行评估，并评估相关风险。我们通过人工评估测试了ToolEmu和评估工具，发现68.8%的失败是真实世界中的代理失败。使用我们精心准备的初始准 benchmark，包括36个高度投资工具和144个测试场景，我们提供了一个量化风险分析，并发现当前LM代理中存在许多失败，其中23.9%的时间，最安全的LM代理也会出现这些失败。这表明需要为实际部署开发更安全的LM代理。
</details></li>
</ul>
<hr>
<h2 id="NAS-NeRF-Generative-Neural-Architecture-Search-for-Neural-Radiance-Fields"><a href="#NAS-NeRF-Generative-Neural-Architecture-Search-for-Neural-Radiance-Fields" class="headerlink" title="NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields"></a>NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14293">http://arxiv.org/abs/2309.14293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeejith Nair, Yuhao Chen, Mohammad Javad Shafiee, Alexander Wong</li>
<li>for: 该文章的目的是提出一种基于神经网络搜索的NeRF架构优化策略，以实现在不同场景下达到高品质synthesis的同时控制计算复杂性。</li>
<li>methods: 该方法使用神经网络搜索策略来生成适应不同场景的NeRF架构，并通过约束目标synthesis质量指标和预算来引导搜索。</li>
<li>results: 实验结果表明，提议的NAS-NeRF可以生成比基eline NeRF更加小巧、快速和低耗的NeRF架构，而且在不同场景下都可以保持高品质synthesis。<details>
<summary>Abstract</summary>
Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but their high computational complexity limits deployability. While existing neural-based solutions strive for efficiency, they use one-size-fits-all architectures regardless of scene complexity. The same architecture may be unnecessarily large for simple scenes but insufficient for complex ones. Thus, there is a need to dynamically optimize the neural network component of NeRFs to achieve a balance between computational complexity and specific targets for synthesis quality. We introduce NAS-NeRF, a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. Experiments on the Blender synthetic dataset show the proposed NAS-NeRF can generate architectures up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$ faster on a GPU than baseline NeRFs, without suffering a drop in SSIM. Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to 23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made publicly available at https://saeejithnair.github.io/NAS-NeRF.
</details>
<details>
<summary>摘要</summary>
神经震荡场（NeRF）可以实现高质量的新视图合成，但是它们的计算复杂性限制了它们的部署。现有的神经网络解决方案尽量减少计算复杂性，但是它们使用一个适用于所有场景的 Architecture，无论场景的复杂性如何。这种 Architecture 可能是对简单场景来说过大，对复杂场景来说则不够。因此，有一个需要 dynamically optimize NeRF 的神经网络组件，以达到计算复杂性和特定目标的平衡。我们介绍了 NAS-NeRF，一种生成神经架构搜索策略，可以生成适合场景的 Compact 和 Scene-Specialized NeRF 架构，并且可以根据目标合成质量指标和预算来导引搜索。我们的方法可以在 Blender synthetic 数据集上实现，并且可以生成与基eline NeRF 相比，5.74倍小、4.19倍 fewer FLOPs、1.93倍快于 GPU 上的 NeRF 架构，而无需做出 SSIM 下降。此外，我们还证明了 NAS-NeRF 可以生成与基eline NeRF 相比，23倍小、22倍 fewer FLOPs、4.7倍快于 GPU 上的 NeRF 架构，只有5.3%的 average SSIM 下降。我们的源代码也公开发布在 <https://saeejithnair.github.io/NAS-NeRF>。
</details></li>
</ul>
<hr>
<h2 id="Perception-and-Energy-aware-Motion-Planning-for-UAV-using-Learning-based-Model-under-Heteroscedastic-Uncertainty"><a href="#Perception-and-Energy-aware-Motion-Planning-for-UAV-using-Learning-based-Model-under-Heteroscedastic-Uncertainty" class="headerlink" title="Perception-and-Energy-aware Motion Planning for UAV using Learning-based Model under Heteroscedastic Uncertainty"></a>Perception-and-Energy-aware Motion Planning for UAV using Learning-based Model under Heteroscedastic Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14272">http://arxiv.org/abs/2309.14272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/rei08/perception-energy-planner">https://gitlab.com/rei08/perception-energy-planner</a></li>
<li>paper_authors: Reiya Takemura, Genya Ishigami</li>
<li>for: 本研究旨在帮助无人航空器（UAV）在Global Navigation Satellite Systems（GNSS） denied环境中能够能量高效、可靠地飞行。</li>
<li>methods: 该研究提出了一种基于感知和能源的动作规划算法，用于解决UAV在GNSS denied环境中的轨迹规划问题。该算法优化了一个权重因子，包括UAV的总能量消耗和LiDAR探测器 mounted on UAV 的感知质量。在线导航之前，高精度模拟器从实际飞行数据中学习了UAV的能量消耗和LiDAR测量的不确定性，以便在线导航时能够更好地估算这些参数。</li>
<li>results: 对比实际环境中的 fotorealistic 环境，实验结果表明，提出的算法可以在不确定性下进行权衡，以提高UAV的能效性和感知质量。开源代码可以在 <a target="_blank" rel="noopener" href="https://gitlab.com/ReI08/perception-energy-planner">https://gitlab.com/ReI08/perception-energy-planner</a> 上下载。<details>
<summary>Abstract</summary>
Global navigation satellite systems (GNSS) denied environments/conditions require unmanned aerial vehicles (UAVs) to energy-efficiently and reliably fly. To this end, this study presents perception-and-energy-aware motion planning for UAVs in GNSS-denied environments. The proposed planner solves the trajectory planning problem by optimizing a cost function consisting of two indices: the total energy consumption of a UAV and the perception quality of light detection and ranging (LiDAR) sensor mounted on the UAV. Before online navigation, a high-fidelity simulator acquires a flight dataset to learn energy consumption for the UAV and heteroscedastic uncertainty associated with LiDAR measurements, both as functions of the horizontal velocity of the UAV. The learned models enable the online planner to estimate energy consumption and perception quality, reducing UAV battery usage and localization errors. Simulation experiments in a photorealistic environment confirm that the proposed planner can address the trade-off between energy efficiency and perception quality under heteroscedastic uncertainty. The open-source code is released at https://gitlab.com/ReI08/perception-energy-planner.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unsupervised-correspondence-with-combined-geometric-learning-and-imaging-for-radiotherapy-applications"><a href="#Unsupervised-correspondence-with-combined-geometric-learning-and-imaging-for-radiotherapy-applications" class="headerlink" title="Unsupervised correspondence with combined geometric learning and imaging for radiotherapy applications"></a>Unsupervised correspondence with combined geometric learning and imaging for radiotherapy applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14269">http://arxiv.org/abs/2309.14269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rrr-uom-projects/unsup-rt-corr-net">https://github.com/rrr-uom-projects/unsup-rt-corr-net</a></li>
<li>paper_authors: Edward G. A. Henderson, Marcel van Herk, Andrew F. Green, Eliana M. Vasquez Osorio</li>
<li>For: The paper aims to develop a model for accurately identifying corresponding points between organ segmentations of different patients for radiotherapy applications.* Methods: The model uses a combination of 3D shape information and imaging information to estimate correspondences and perform interpolation. The model was trained with head and neck organ segmentations from planning CT scans, and two approaches were used to incorporate imaging information: extracting features directly from image patches and including the mean square error between patches as part of the loss function.* Results: The correspondence and interpolation performance were evaluated using several metrics, including geodesic error, chamfer distance, and conformal distortion. The best performing model configuration incorporated imaging information as part of the loss function, which produced more anatomically plausible correspondences. The model outperformed a baseline non-rigid registration approach and the original model with direct inclusion of image features.Here is the same information in Simplified Chinese:* For: 这篇论文的目标是为了准确地标注不同患者的器官分割中的对应点，以便在放疗应用中使用。* Methods: 该模型使用了3D形状信息和成像信息来估算对应点并进行插值。模型通过使用规划CT扫描图像的头颈器官分割来进行训练，并采用了两种方法来包含成像信息：直接从图像块中提取特征，以及将图像块之间的平均方差作为损失函数的一部分。* Results: 对应点和插值性能被评估使用了几种指标，包括 геодезиック误差、斜截距离和几何扭曲误差。最佳配置中包含了成像信息作为损失函数的一部分，生成了更 Plausible的对应点。模型比非rigid registration方法和原始模型 WITH direct inclusion of image features更好。<details>
<summary>Abstract</summary>
The aim of this study was to develop a model to accurately identify corresponding points between organ segmentations of different patients for radiotherapy applications. A model for simultaneous correspondence and interpolation estimation in 3D shapes was trained with head and neck organ segmentations from planning CT scans. We then extended the original model to incorporate imaging information using two approaches: 1) extracting features directly from image patches, and 2) including the mean square error between patches as part of the loss function. The correspondence and interpolation performance were evaluated using the geodesic error, chamfer distance and conformal distortion metrics, as well as distances between anatomical landmarks. Each of the models produced significantly better correspondences than the baseline non-rigid registration approach. The original model performed similarly to the model with direct inclusion of image features. The best performing model configuration incorporated imaging information as part of the loss function which produced more anatomically plausible correspondences. We will use the best performing model to identify corresponding anatomical points on organs to improve spatial normalisation, an important step in outcome modelling, or as an initialisation for anatomically informed registrations. All our code is publicly available at https://github.com/rrr-uom-projects/Unsup-RT-Corr-Net
</details>
<details>
<summary>摘要</summary>
“本研究的目标是开发一种能够准确标注不同病人器官分割的模型，以便在放射治疗应用中进行空间Normalization。我们使用了头和 neck器官分割的规划CT扫描图进行模型训练。然后，我们将原始模型扩展以包括影像信息，使用两种方法：1）直接从图像块中提取特征，2）在损失函数中包括图像块的平均方差。对于每个模型，我们评估了各种维度的比较，包括地odesic error、Chamfer distance和conformal distortion metric，以及器官标志点之间的距离。每个模型都生成了较好的对应关系，比基eline非rigid registration方法更好。原始模型和直接包括图像特征的模型的性能相似。最佳配置是将影像信息作为损失函数的一部分来，生成更符合解剖学的对应关系。我们将使用最佳配置来标注器官之间的对应点，以提高结果模型中的空间Normalization，或者作为初始化 для解剖学指导的registrations。我们的代码都公开在https://github.com/rrr-uom-projects/Unsup-RT-Corr-Net上”
</details></li>
</ul>
<hr>
<h2 id="Date-Driven-Approach-for-Identifying-State-of-Hemodialysis-Fistulas-Entropy-Complexity-and-Formal-Concept-Analysis"><a href="#Date-Driven-Approach-for-Identifying-State-of-Hemodialysis-Fistulas-Entropy-Complexity-and-Formal-Concept-Analysis" class="headerlink" title="Date-Driven Approach for Identifying State of Hemodialysis Fistulas: Entropy-Complexity and Formal Concept Analysis"></a>Date-Driven Approach for Identifying State of Hemodialysis Fistulas: Entropy-Complexity and Formal Concept Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14399">http://arxiv.org/abs/2309.14399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilii A. Gromov, E. I. Zvorykina, Yurii N. Beschastnov, Majid Sohrabi</li>
<li>for: 这种研究旨在为诊断病理性尿道采用数学方法进行分类。</li>
<li>methods: 这种方法基于laminar blood flow的假设，认为正常功能的尿道会出现单普液流，而病理性尿道会出现湍流。这种方法包括在 entropy-complexity 平面上将时序列映射，并与已知集合进行比较，以及使用正式概念分析构建 concepts-objects 图。</li>
<li>results: 这两种方法具有高效性，可以准确地确定尿道的状态。<details>
<summary>Abstract</summary>
The paper explores mathematical methods that differentiate regular and chaotic time series, specifically for identifying pathological fistulas. It proposes a noise-resistant method for classifying responding rows of normally and pathologically functioning fistulas. This approach is grounded in the hypothesis that laminar blood flow signifies normal function, while turbulent flow indicates pathology. The study explores two distinct methods for distinguishing chaotic from regular time series. The first method involves mapping the time series onto the entropy-complexity plane and subsequently comparing it to established clusters. The second method, introduced by the authors, constructs a concepts-objects graph using formal concept analysis. Both of these methods exhibit high efficiency in determining the state of the fistula.
</details>
<details>
<summary>摘要</summary>
文章研究了用数学方法分辨常规和异常时间序列，特别是用于识别病理性尿道。它提出了一种对应行的响应类型进行分类的听力抗噪方法，这种方法基于假设：在正常情况下，液体血流表示正常功能，而在异常情况下，液体血流表示疾病。文章研究了两种方法来分辨混乱和常规时间序列：首先，将时间序列映射到复杂度-自 entropy 平面，然后与已知的集群进行比较；其次，通过正式概念分析构建一个概念物件图。两种方法都能高效地确定尿道的状态。
</details></li>
</ul>
<hr>
<h2 id="OmniEvent-A-Comprehensive-Fair-and-Easy-to-Use-Toolkit-for-Event-Understanding"><a href="#OmniEvent-A-Comprehensive-Fair-and-Easy-to-Use-Toolkit-for-Event-Understanding" class="headerlink" title="OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding"></a>OmniEvent: A Comprehensive, Fair, and Easy-to-Use Toolkit for Event Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14258">http://arxiv.org/abs/2309.14258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-keg/omnievent">https://github.com/thu-keg/omnievent</a></li>
<li>paper_authors: Hao Peng, Xiaozhi Wang, Feng Yao, Zimu Wang, Chuzhao Zhu, Kaisheng Zeng, Lei Hou, Juanzi Li</li>
<li>for: 本研究开发了一个全面的事件理解工具kit OmniEvent，用于解决文本中事件检测、事件Argument提取和事件关系提取等复杂的信息提取任务。</li>
<li>methods: OmniEvent支持主流的模型化方法，并处理了Peng et al. (2023)所报告的隐藏评估坑，以确保公平的比较。</li>
<li>results: OmniEvent提供了一个可直接用于生产环境的Web服务，并提供了可修改的模块化框架，以便用户根据需要进行自定义模型评估。<details>
<summary>Abstract</summary>
Event understanding aims at understanding the content and relationship of events within texts, which covers multiple complicated information extraction tasks: event detection, event argument extraction, and event relation extraction. To facilitate related research and application, we present an event understanding toolkit OmniEvent, which features three desiderata: (1) Comprehensive. OmniEvent supports mainstream modeling paradigms of all the event understanding tasks and the processing of 15 widely-used English and Chinese datasets. (2) Fair. OmniEvent carefully handles the inconspicuous evaluation pitfalls reported in Peng et al. (2023), which ensures fair comparisons between different models. (3) Easy-to-use. OmniEvent is designed to be easily used by users with varying needs. We provide off-the-shelf models that can be directly deployed as web services. The modular framework also enables users to easily implement and evaluate new event understanding models with OmniEvent. The toolkit (https://github.com/THU-KEG/OmniEvent) is publicly released along with the demonstration website and video (https://omnievent.xlore.cn/).
</details>
<details>
<summary>摘要</summary>
Event理解目标是理解文本中的事件内容和关系，包括多种复杂信息提取任务：事件检测、事件参数提取和事件关系提取。为推动相关研究和应用，我们提供了一套事件理解工具包 OmniEvent，具有以下三个目标：1. 全面。OmniEvent支持主流模型化思路所有的事件理解任务，并处理15种常用的英文和中文数据集。2. 公平。OmniEvent通过彻底处理报告在Peng et al. (2023)中报道的隐藏评估坑，以确保比较不同模型的公平。3. 易用。OmniEvent设计便于用户们根据需要使用。我们提供了直接可以部署为网服务的准备好的模型，框架也允许用户轻松实现和评估新的事件理解模型。工具kit（https://github.com/THU-KEG/OmniEvent）公开发布，并提供了示例网站和视频（https://omnievent.xlore.cn/）。
</details></li>
</ul>
<hr>
<h2 id="Prediction-Model-For-Wordle-Game-Results-With-High-Robustness"><a href="#Prediction-Model-For-Wordle-Game-Results-With-High-Robustness" class="headerlink" title="Prediction Model For Wordle Game Results With High Robustness"></a>Prediction Model For Wordle Game Results With High Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14250">http://arxiv.org/abs/2309.14250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zeniSoida/pl1">https://github.com/zeniSoida/pl1</a></li>
<li>paper_authors: Jiaqi Weng, Chunlin Feng</li>
<li>for: 本研究用数据分析和机器学习方法研究Wordle游戏的动态。</li>
<li>methods: 我们使用ARIMAX模型和反射神经网络模型来预测词语的难度，并使用K-means归一化来分类词语的数值。</li>
<li>results: 我们的研究发现，在2023年3月1日，约有12,884个结果将被提交，词语”尴尬”的平均尝试次数为4.8，属于最难的分数 cluster。此外，我们还研究了玩家的忠诚度和他们是否做日常挑战的比例。我们的模型经过了严格的敏感分析和验证，确认其稳定性。总的来说，本研究提供了基于日期或给定的五个字词语的Wordle游戏预测框架，结果已经提交给纽约时报游戏编辑。<details>
<summary>Abstract</summary>
In this study, we delve into the dynamics of Wordle using data analysis and machine learning. Our analysis initially focused on the correlation between the date and the number of submitted results. Due to initial popularity bias, we modeled stable data using an ARIMAX model with coefficient values of 9, 0, 2, and weekdays/weekends as the exogenous variable. We found no significant relationship between word attributes and hard mode results.   To predict word difficulty, we employed a Backpropagation Neural Network, overcoming overfitting via feature engineering. We also used K-means clustering, optimized at five clusters, to categorize word difficulty numerically. Our findings indicate that on March 1st, 2023, around 12,884 results will be submitted and the word "eerie" averages 4.8 attempts, falling into the hardest difficulty cluster.   We further examined the percentage of loyal players and their propensity to undertake daily challenges. Our models underwent rigorous sensitivity analyses, including ADF, ACF, PACF tests, and cross-validation, confirming their robustness. Overall, our study provides a predictive framework for Wordle gameplay based on date or a given five-letter word. Results have been summarized and submitted to the Puzzle Editor of the New York Times.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们使用数据分析和机器学习来研究Wordle的动态。我们首先查看了日期和提交结果之间的相关性。由于初始的受欢迎偏见，我们使用ARIMAX模型，其含有9, 0, 2的系数值和星期天/星期六作为外生变量。我们发现没有显著的词属性和困难模式之间的关系。  为预测词难度，我们使用反射神经网络，并通过特征工程来避免过拟合。我们还使用K-means聚类算法，并优化为五个分类。我们发现，在2023年3月1日，约有12,884个结果将被提交，并且词“幽默”的平均尝试次数为4.8，属于最难的分类。  我们进一步研究了忠诚玩家的百分比和他们的日常挑战的倾向。我们的模型经过了严格的敏感分析，包括ADF、ACF、PACF测试和批处理，以确认其可靠性。总的来说，我们的研究提供了基于日期或给定的五个字的Wordle游戏玩法预测框架。结果已经总结并提交给纽约时报游戏编辑。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Internet-Communication-Through-LLMs-How-Close-Are-We"><a href="#Rethinking-Internet-Communication-Through-LLMs-How-Close-Are-We" class="headerlink" title="Rethinking Internet Communication Through LLMs: How Close Are We?"></a>Rethinking Internet Communication Through LLMs: How Close Are We?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14247">http://arxiv.org/abs/2309.14247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sifat Ut Taki, Spyridon Mastorakis</li>
<li>for: 重新思考互联网上用户之间的交流方式，以便更好地捕捉用户对另一端通信频道的认知。</li>
<li>methods: 提出使用大语言模型（LLM）代表用户之间的交流，并提出了实现这种通信架构的方法。</li>
<li>results: 对现有技术的可行性进行了 reality check，并讨论了未来研究的挑战和有趣的方向。<details>
<summary>Abstract</summary>
In this paper, we rethink the way that communication among users over the Internet, one of the fundamental outcomes of the Internet evolution, takes place. Instead of users communicating directly over the Internet, we explore an architecture that enables users to communicate with (query) Large Language Models (LLMs) that capture the cognition of users on the other end of the communication channel. We present an architecture to achieve such LLM-based communication and we perform a reality check to assess how close we are today to realizing such a communication architecture from a technical point of view. Finally, we discuss several research challenges and identify interesting directions for future research.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们重新思考互联网上用户之间的通信方式，这是互联网进化的一个基本结果。而不是直接通过互联网进行用户之间的通信，我们研究了一种使用大自然语言模型（LLM）来捕捉用户对另一端通信频道的认知。我们提出了实现这种 LLM-based 通信架构的方案，并进行了技术实现的现实性检查。最后，我们讨论了一些研究挑战和未来研究的有趣方向。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-data-efficiency-in-reinforcement-learning-a-novel-imagination-mechanism-based-on-mesh-information-propagation"><a href="#Enhancing-data-efficiency-in-reinforcement-learning-a-novel-imagination-mechanism-based-on-mesh-information-propagation" class="headerlink" title="Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation"></a>Enhancing data efficiency in reinforcement learning: a novel imagination mechanism based on mesh information propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14243">http://arxiv.org/abs/2309.14243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ouazusakou/imagination_mechanism">https://github.com/ouazusakou/imagination_mechanism</a></li>
<li>paper_authors: Zihang Wang, Maowei Jiang</li>
<li>for: 提高深度学习强化学习（RL）算法的数据效率，特别是面临高维状态空间和大规模问题时。</li>
<li>methods: 引入一种人类类似的想象机制（Imagination Mechanism，IM），用于在不同话题间共享样本信息，从而提高RL算法的学习效率。</li>
<li>results: 在四种主流SOTA RL算法（SAC、PPO、DDPG和DQN）上，IM可以带来显著的提高，最终导致不同任务上的表现均得到提高。<details>
<summary>Abstract</summary>
Reinforcement learning(RL) algorithms face the challenge of limited data efficiency, particularly when dealing with high-dimensional state spaces and large-scale problems. Most of RL methods often rely solely on state transition information within the same episode when updating the agent's Critic, which can lead to low data efficiency and sub-optimal training time consumption. Inspired by human-like analogical reasoning abilities, we introduce a novel mesh information propagation mechanism, termed the 'Imagination Mechanism (IM)', designed to significantly enhance the data efficiency of RL algorithms. Specifically, IM enables information generated by a single sample to be effectively broadcasted to different states across episodes, instead of simply transmitting in the same episode. This capability enhances the model's comprehension of state interdependencies and facilitates more efficient learning of limited sample information. To promote versatility, we extend the IM to function as a plug-and-play module that can be seamlessly and fluidly integrated into other widely adopted RL algorithms. Our experiments demonstrate that IM consistently boosts four mainstream SOTA RL algorithms, such as SAC, PPO, DDPG, and DQN, by a considerable margin, ultimately leading to superior performance than before across various tasks. For access to our code and data, please visit https://github.com/OuAzusaKou/imagination_mechanism
</details>
<details>
<summary>摘要</summary>
利用人类类似的想象能力，我们引入了一种新的网格信息传递机制，称为“想象机制”（IM），以提高深度学习束缚学习（RL）算法的数据效率。Specifically，IM使得单个样本生成的信息可以有效地在不同话数据集中传递，而不是仅在同一话数据集中传递。这种能力提高模型对状态之间的相互关系的理解，并且使得学习有限样本信息更加高效。为了推广可用性，我们将IM作为一个可插入式和可靠地Integrate into other widely adopted RL algorithms。我们的实验表明，IM可以持续地提高四种主流SOTA RL算法，如SAC、PPO、DDPG和DQN，并 ultimately leading to superior performance across various tasks. For access to our code and data, please visit https://github.com/OuAzusaKou/imagination_mechanism。
</details></li>
</ul>
<hr>
<h2 id="Seeing-and-hearing-what-has-not-been-said-A-multimodal-client-behavior-classifier-in-Motivational-Interviewing-with-interpretable-fusion"><a href="#Seeing-and-hearing-what-has-not-been-said-A-multimodal-client-behavior-classifier-in-Motivational-Interviewing-with-interpretable-fusion" class="headerlink" title="Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion"></a>Seeing and hearing what has not been said; A multimodal client behavior classifier in Motivational Interviewing with interpretable fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14398">http://arxiv.org/abs/2309.14398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucie Galland, Catherine Pelachaud, Florian Pecune</li>
<li>for: 评估动机听讲的质量，以便提高治疗效果。</li>
<li>methods: 利用多Modal特征，如文本、语音、脸部表情和身体表情，建立一个准确地分类客户话语的模型。</li>
<li>results: 通过对AnnoMI数据集进行注解，收集多Modal信息，并确定最重要的Modalities在决策过程中的扮演。<details>
<summary>Abstract</summary>
Motivational Interviewing (MI) is an approach to therapy that emphasizes collaboration and encourages behavioral change. To evaluate the quality of an MI conversation, client utterances can be classified using the MISC code as either change talk, sustain talk, or follow/neutral talk. The proportion of change talk in a MI conversation is positively correlated with therapy outcomes, making accurate classification of client utterances essential. In this paper, we present a classifier that accurately distinguishes between the three MISC classes (change talk, sustain talk, and follow/neutral talk) leveraging multimodal features such as text, prosody, facial expressivity, and body expressivity. To train our model, we perform annotations on the publicly available AnnoMI dataset to collect multimodal information, including text, audio, facial expressivity, and body expressivity. Furthermore, we identify the most important modalities in the decision-making process, providing valuable insights into the interplay of different modalities during a MI conversation.
</details>
<details>
<summary>摘要</summary>
《动机导向会议》（MI）是一种帮助客户改变行为的医疗方法。为评估MI会议质量，客户的语言可以被分类为变化语言、维持语言或跟随/中立语言。变化语言的比例和治疗效果正相关，因此正确地分类客户语言非常重要。在这篇论文中，我们提出了一种精准地分类客户语言的分类器，利用多Modal特征，如文本、 просодия、 facial expressivity 和 body expressivity。为了训练我们的模型，我们对公共可用的 AnnoMI 数据集进行了标注，以收集多Modal信息，包括文本、音频、 facial expressivity 和 body expressivity。此外，我们还确定了决策过程中最重要的Modalities，提供了有价值的发现，描述了不同Modalities在MI会议中的协作。
</details></li>
</ul>
<hr>
<h2 id="MoDem-V2-Visuo-Motor-World-Models-for-Real-World-Robot-Manipulation"><a href="#MoDem-V2-Visuo-Motor-World-Models-for-Real-World-Robot-Manipulation" class="headerlink" title="MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation"></a>MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14236">http://arxiv.org/abs/2309.14236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Lancaster, Nicklas Hansen, Aravind Rajeswaran, Vikash Kumar<br>for:* 本研究旨在开发一个能够在无instrumented real-world environments中学习 contact-rich manipulation的系统，以提高现代机器人系统的可靠性和安全性。methods:* 该系统基于latest algorithmic advancements in model-based reinforcement learning (MBRL), demo-bootstrapping, and effective exploration，并使用 visual pixels directly for learning。results:* 该系统能够在实际世界中学习 contact-rich dexterous manipulation skills，并在四个复杂的visuo-motor manipulation问题中进行了 empirical demonstration。这是首个直接在实际世界中学习的 demonstration-augmented visual MBRL系统。<details>
<summary>Abstract</summary>
Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based reinforcement learning (MBRL), demo-bootstrapping, and effective exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills directly in the real world. We identify key ingredients for leveraging demonstrations in model learning while respecting real-world safety considerations -- exploration centering, agency handover, and actor-critic ensembles. We empirically demonstrate the contribution of these ingredients in four complex visuo-motor manipulation problems in both simulation and the real world. To the best of our knowledge, our work presents the first successful system for demonstration-augmented visual MBRL trained directly in the real world. Visit https://sites.google.com/view/modem-v2 for videos and more details.
</details>
<details>
<summary>摘要</summary>
роботизированные системы, которые стремятся работать в неинструментированных реальных средах, должны воспринимать мир непосредственно с помощью наборного зрения. Системы обучения на основе зрения стремятся отказаться от необходимости в инструментированной среде, построив implicit понимание мира на основе raw пикселей, но навигация высокомерных пространств по solely sparse visual reward signals значительно увеличивает вызов эксплорации. Поэтому применение таких систем ограничено симулированными или инженерными средами, так как исследуние агента в реальном мире без руководства эксплицитных state estimation и dense reward может привести к небезопасному поведению и фатальным ошибкам.В этом исследовании мы изолируем корни этих ограничений, чтобы разработать систему, называемую MoDem-V2, которая может научиться контактным манипуляциям непосредственно в неинструментированном реальном мире. На основе последних достижений в области алгоритмов моделируемого обучения (MBRL), демо-ботстроппинга и эффективного исследувания, MoDem-V2 может приобрести контактные манипулятивные навыки в реальном мире. Мы идентифицируем ключевые ингредиенты для использования демонстраций в модельном обучении, уважая considertions безопасности реального мира -- exploration centering, handover agency и actor-critic ensembles. Мы экспериментально подтверждаем вклад этих ингредиентов в решении четырех сложных визуометрических манипулятивных задач в обеих симуляции и реальном мире. По нашему знанию, наша работа представляет первую успешную систему demonstration-augmented visual MBRL, обученную непосредственно в реальном мире. Посетите https://sites.google.com/view/modem-v2 для видео и дополнительные детали.
</details></li>
</ul>
<hr>
<h2 id="Stackelberg-Driver-Model-for-Continual-Policy-Improvement-in-Scenario-Based-Closed-Loop-Autonomous-Driving"><a href="#Stackelberg-Driver-Model-for-Continual-Policy-Improvement-in-Scenario-Based-Closed-Loop-Autonomous-Driving" class="headerlink" title="Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving"></a>Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14235">http://arxiv.org/abs/2309.14235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BlueCat-de/SDM">https://github.com/BlueCat-de/SDM</a></li>
<li>paper_authors: Haoyi Niu, Qimao Chen, Yingyue Li, Jianming Hu</li>
<li>for: 本研究旨在提高自动驾驶车辆（AV）的性能和可靠性，通过针对极其罕见 yet critical corner cases 的优化。</li>
<li>methods: 该研究使用 adversarial generation 方法生成安全关键的驾驶场景，并通过 Stackelberg Driver Model（SDM）模型准确描述了车辆之间的层次结构，以实现 AV 的不断改进。</li>
<li>results: 实验表明，该算法在高维场景中表现出色，比基准方法更高效，导致了 AV 的显著进步，同时 continually generating progressively challenging scenarios。<details>
<summary>Abstract</summary>
The deployment of autonomous vehicles (AVs) has faced hurdles due to the dominance of rare but critical corner cases within the long-tail distribution of driving scenarios, which negatively affects their overall performance. To address this challenge, adversarial generation methods have emerged as a class of efficient approaches to synthesize safety-critical scenarios for AV testing. However, these generated scenarios are often underutilized for AV training, resulting in the potential for continual AV policy improvement remaining untapped, along with a deficiency in the closed-loop design needed to achieve it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately characterize the hierarchical nature of vehicle interaction dynamics, facilitating iterative improvement by engaging background vehicles (BVs) and AV in a sequential game-like interaction paradigm. With AV acting as the leader and BVs as followers, this leader-follower modeling ensures that AV would consistently refine its policy, always taking into account the additional information that BVs play the best response to challenge AV. Extensive experiments have shown that our algorithm exhibits superior performance compared to several baselines especially in higher dimensional scenarios, leading to substantial advancements in AV capabilities while continually generating progressively challenging scenarios. Code is available at https://github.com/BlueCat-de/SDM.
</details>
<details>
<summary>摘要</summary>
自带驱动自动车 (AV) 的部署面临了由罕见而重要的角度案例所带来的阻碍，这些角度案例会影响 AV 的总性表现。为解决这个挑战， adversarial 生成方法在 AV 测试中出现了，这些方法可以生成安全关键的驾驶enario。然而，这些生成的场景通常不被用于 AV 训练，导致 AV 政策的持续改进 remained untapped，以及closed-loop 设计的缺失。因此，我们将 Stackelberg 驾驶器模型 (SDM) 改进，以便准确地描述车辆交互动力学的层次结构，从而实现了 iterative 改进，让 AV 在 background 车辆 (BV) 的支持下，通过 sequential 交互模式来精细调整其策略，并且总是考虑 BV 的最佳回应，以挑战 AV。我们的算法在高维度enario中表现出色，比如基eline 特别出色，从而实现了 AV 的重要进步，同时 continually 生成进一步挑战 AV 的场景。代码可以在 <https://github.com/BlueCat-de/SDM> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Combined-sizing-and-layout-optimization-of-truss-structures-via-update-Monte-Carlo-tree-search-UMCTS-algorithm"><a href="#Combined-sizing-and-layout-optimization-of-truss-structures-via-update-Monte-Carlo-tree-search-UMCTS-algorithm" class="headerlink" title="Combined sizing and layout optimization of truss structures via update Monte Carlo tree search (UMCTS) algorithm"></a>Combined sizing and layout optimization of truss structures via update Monte Carlo tree search (UMCTS) algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14231">http://arxiv.org/abs/2309.14231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fu-Yao Ko, Katsuyuki Suzuki, Kazuo Yonekura</li>
<li>for: 本研究的主要目标是找到螺栓结构的最佳设计，同时考虑尺寸和布局变量。</li>
<li>methods: 本研究使用了一种强化学习方法，名为更新 Monte Carlo 搜索（UMCTS），用于解决螺栓结构的尺寸和布局优化问题。</li>
<li>results: 研究表明，使用 UMCTS 方法可以减少计算时间，并且稳定地实现较好的解决方案，比传统方法更好。<details>
<summary>Abstract</summary>
The main concern of this study is to find the optimal design of truss structures considering sizing and layout variables simultaneously. As compared to purely sizing optimization problems, this problem is more challenging since the two types of variables involved are fundamentally different in nature. In this paper, a reinforcement learning method combining the update process and Monte Carlo tree search called the update Monte Carlo tree search (UMCTS) for sizing optimization problems is applied to solve combined sizing and layout optimization for truss structures. This study proposes a novel update process for nodal coordinates with two features. (1) The allowed range of each coordinate varies in each round. (2) Accelerators for the number of entries in the allowed range and iteration numbers are introduced to reduce the computation time. Furthermore, nodal coordinates and member areas are determined at the same time with only one search tree in each round. The validation and efficiency of the UMCTS are tested on benchmark problems of planar and spatial trusses with discrete sizing variables and continuous layout variables. It is shown that the CPU time of the UMCTS is two times faster than the branch and bound method. The numerical results demonstrate that the proposed method stably achieves a better solution than other traditional methods.
</details>
<details>
<summary>摘要</summary>
本研究的主要担忧是查找螺栓结构的最优设计，同时考虑大小和布局变量。与纯粹的大小优化问题相比，这个问题更加具有挑战性，因为这两种变量的本质不同。在这篇论文中，我们应用了一种combined reinforcement learning method，called update Monte Carlo tree search (UMCTS)，解决螺栓结构的大小和布局优化问题。我们提出了一种新的更新过程，其中每个坐标的允许范围在每一轮都不同，同时还引入了加速器来减少计算时间。此外，每一轮都只需要一个搜索树来确定节点坐标和部件面积。我们对标准问题进行验证和效率测试，结果显示，UMCTS的计算时间比branch and bound方法快两倍。数值结果表明，我们提出的方法可稳定地实现更好的解决方案，比传统方法更好。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Sensing-in-Traffic-Optimization-Advanced-Deep-Reinforcement-Learning-Techniques"><a href="#Implicit-Sensing-in-Traffic-Optimization-Advanced-Deep-Reinforcement-Learning-Techniques" class="headerlink" title="Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques"></a>Implicit Sensing in Traffic Optimization: Advanced Deep Reinforcement Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14395">http://arxiv.org/abs/2309.14395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuel Figetakis, Yahuza Bello, Ahmed Refaey, Lei Lei, Medhat Moussa<br>for: 这个论文的目的是解决高速公路突然出现堵塞的问题，使用自动驾驶车辆（AV）的感知器来做出智能决策，以避免因堵塞而导致的延迟。methods: 这个论文使用了深度强化学习（DRL）技术，基于Markov决策过程（MDP）模型，训练RL代理人以适应实际驾驶情况。具体来说，使用了SUMO仿真器和OPENAI GYM评估工具来评估提议模型的性能。results: 结果表明，使用{\epsilon}-抽象策略训练DQN代理人后，其性能明显超过使用Boltzmann策略训练的DQN代理人。<details>
<summary>Abstract</summary>
A sudden roadblock on highways due to many reasons such as road maintenance, accidents, and car repair is a common situation we encounter almost daily. Autonomous Vehicles (AVs) equipped with sensors that can acquire vehicle dynamics such as speed, acceleration, and location can make intelligent decisions to change lanes before reaching a roadblock. A number of literature studies have examined car-following models and lane-changing models. However, only a few studies proposed an integrated car-following and lane-changing model, which has the potential to model practical driving maneuvers. Hence, in this paper, we present an integrated car-following and lane-changing decision-control system based on Deep Reinforcement Learning (DRL) to address this issue. Specifically, we consider a scenario where sudden construction work will be carried out along a highway. We model the scenario as a Markov Decision Process (MDP) and employ the well-known DQN algorithm to train the RL agent to make the appropriate decision accordingly (i.e., either stay in the same lane or change lanes). To overcome the delay and computational requirement of DRL algorithms, we adopt an MEC-assisted architecture where the RL agents are trained on MEC servers. We utilize the highly reputable SUMO simulator and OPENAI GYM to evaluate the performance of the proposed model under two policies; {\epsilon}-greedy policy and Boltzmann policy. The results unequivocally demonstrate that the DQN agent trained using the {\epsilon}-greedy policy significantly outperforms the one trained with the Boltzmann policy.
</details>
<details>
<summary>摘要</summary>
高速公路上突然出现堵塞，常见的情况之一，可能是道路维护、事故或车辆维修等多种原因。自动驾驶车（AV）配备感知器可以获取车辆动态状态，如速度、加速度和位置，可以做出智能决策，以避免堵塞。许多文献研究了车辆随驾模型和车道变更模型，但只有一些研究提出了集成车辆随驾和车道变更模型，这种模型具有实际驾驶行为的潜在优势。因此，在这篇论文中，我们提出了基于深度强化学习（DRL）的集成车辆随驾和车道变更决策控制系统，以解决这个问题。具体来说，我们考虑了高速公路上突然进行建设工程的情况。我们将这种情况模型为马克夫满度决策过程（MDP），并使用了知名的DQN算法来训练RL代理人进行适当决策（即留在同一个车道或变更车道）。为了解决DRL算法的延迟和计算资源的问题，我们采用了MEC助け的架构，其中RL代理人在MEC服务器上进行训练。我们使用了非常可靠的SUMO仿真器和OPENAI GYM来评估提出的模型的性能，并对两种策略进行评估：{\epsilon}-抽象策略和博尔ツ曼策略。结果明确表明，使用{\epsilon}-抽象策略训练的DQN代理人明显超越使用博尔ツ曼策略训练的DQN代理人。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Noises-in-Diffusion-Model-for-Semi-Supervised-Multi-Domain-Translation"><a href="#Multiple-Noises-in-Diffusion-Model-for-Semi-Supervised-Multi-Domain-Translation" class="headerlink" title="Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation"></a>Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14394">http://arxiv.org/abs/2309.14394</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tsiry Mayet, Simon Bernard, Clement Chatelain, Romain Herault</li>
<li>for: 这 paper 的目的是提出一种多个频道的域转换方法，用于在半指导下进行多个域之间的域转换。</li>
<li>methods: 这 paper 使用了一种名为 Multi-Domain Diffusion (MDD) 的噪声扩散框架，它不需要定义输入和输出域，可以在任意的域分配中进行域转换（如 $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, 等等），而不需要额外训练每个域配置的模型。</li>
<li>results: 这 paper 在一个多个域的Synthetic image translation dataset上进行了实验，并得到了一些有趣的结果。<details>
<summary>Abstract</summary>
Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the training task from a simple reconstruction task to a domain translation task, where the model relies on less noisy domains to reconstruct more noisy domains. We present results on a multi-domain (with more than two domains) synthetic image translation dataset with challenging semantic domain inversion.
</details>
<details>
<summary>摘要</summary>
域到域翻译（Domain-to-domain translation）是生成目标域样本，给定源域的条件。现有的方法都是针对固定的输入和输出域，即只能处理特定的配置（例如 $D_1\to D_2$ 或 $D_2\to D_1$）。这篇论文提出了多域扩散（Multi-Domain Diffusion，MDD），一种基于半supervised的域扩散框架。与先前的方法不同，MDD不需要定义输入和输出域，可以在一个集合（例如 $(D_1, D_2)\to D_3$，$D_2\to (D_1, D_3)$，$D_3\to D_1$ 等）中进行翻译，无需为每个域配置单独训练模型。MDD的关键思想是利用扩散模型的噪声表示，每个域都有一个噪声水平，这使得缺失的域可以自然地被噪声表示。这将训练任务从一个简单的重建任务变为域翻译任务，其中模型通过更加净化的域来重建更加噪声的域。我们在多域（包括更多于两个域）的Synthetic image翻译dataset上进行了实验，并取得了具有挑战性的semantic domain inversion的结果。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Machine-Learning-Algorithms-with-Adaptive-Sampling"><a href="#Accelerating-Machine-Learning-Algorithms-with-Adaptive-Sampling" class="headerlink" title="Accelerating Machine Learning Algorithms with Adaptive Sampling"></a>Accelerating Machine Learning Algorithms with Adaptive Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14221">http://arxiv.org/abs/2309.14221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mo Tiwari</li>
<li>for: 提高大规模数据处理中机器学习算法的效率。</li>
<li>methods: 使用Randomized counterparts instead of computationally intensive subroutines to improve computational efficiency.</li>
<li>results: 几乎没有质量下降，但可以大幅提高计算效率。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
The era of huge data necessitates highly efficient machine learning algorithms. Many common machine learning algorithms, however, rely on computationally intensive subroutines that are prohibitively expensive on large datasets. Oftentimes, existing techniques subsample the data or use other methods to improve computational efficiency, at the expense of incurring some approximation error. This thesis demonstrates that it is often sufficient, instead, to substitute computationally intensive subroutines with a special kind of randomized counterparts that results in almost no degradation in quality.
</details>
<details>
<summary>摘要</summary>
era of big data 需要非常高效的机器学习算法。然而，许多常见的机器学习算法却依赖于计算昂贵的子routine，对大量数据来说是不可接受的。有时候，现有的技术会采用采样或其他方法来提高计算效率，但这会导致一定的近似错误。这个论文示出，可以在代之前 substitute computationally intensive subroutines with a special kind of randomized counterparts，而不会导致质量下降。
</details></li>
</ul>
<hr>
<h2 id="MemDA-Forecasting-Urban-Time-Series-with-Memory-based-Drift-Adaptation"><a href="#MemDA-Forecasting-Urban-Time-Series-with-Memory-based-Drift-Adaptation" class="headerlink" title="MemDA: Forecasting Urban Time Series with Memory-based Drift Adaptation"></a>MemDA: Forecasting Urban Time Series with Memory-based Drift Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14216">http://arxiv.org/abs/2309.14216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepkashiwa20/Urban_Concept_Drift">https://github.com/deepkashiwa20/Urban_Concept_Drift</a></li>
<li>paper_authors: Zekun Cai, Renhe Jiang, Xinyu Yang, Zhaonan Wang, Diansheng Guo, Hiroki Kobayashi, Xuan Song, Ryosuke Shibasaki</li>
<li>for: 本研究旨在解决城市时间序列预测中的概念漂移问题，以提高城市智能化的可持续发展。</li>
<li>methods: 本研究提出了一种新的城市时间序列预测模型，该模型通过考虑数据周期性并在预测过程中进行协调调整，以适应概念漂移。</li>
<li>results: 实验结果表明，本研究的设计在实际数据上显著超越了现有方法，并且可以通过减少预测模型对数据分布变化的敏感性，提高模型的可重用性和泛化能力。<details>
<summary>Abstract</summary>
Urban time series data forecasting featuring significant contributions to sustainable development is widely studied as an essential task of the smart city. However, with the dramatic and rapid changes in the world environment, the assumption that data obey Independent Identically Distribution is undermined by the subsequent changes in data distribution, known as concept drift, leading to weak replicability and transferability of the model over unseen data. To address the issue, previous approaches typically retrain the model, forcing it to fit the most recent observed data. However, retraining is problematic in that it leads to model lag, consumption of resources, and model re-invalidation, causing the drift problem to be not well solved in realistic scenarios. In this study, we propose a new urban time series prediction model for the concept drift problem, which encodes the drift by considering the periodicity in the data and makes on-the-fly adjustments to the model based on the drift using a meta-dynamic network. Experiments on real-world datasets show that our design significantly outperforms state-of-the-art methods and can be well generalized to existing prediction backbones by reducing their sensitivity to distribution changes.
</details>
<details>
<summary>摘要</summary>
城市时序数据预测 featuring 重要贡献于可持续发展是智能城市广泛研究的必要任务。然而，随着世界环境的剧变和快速变化，假设数据遵循独立同分布（ID）的假设被后续数据分布变化所推翻，导致模型的弱复现和传输性，从而使得随变问题在实际场景中并不得到好的解决。在本研究中，我们提出了一种新的城市时序预测模型，该模型通过考虑数据中的周期性来编码随变，并在随变过程中进行实时调整，使用元动态网络。实验表明，我们的设计在实际数据集上显著超越了现有方法，并且可以将现有预测基础结构降低其对分布变化的敏感性。
</details></li>
</ul>
<hr>
<h2 id="Continual-Driving-Policy-Optimization-with-Closed-Loop-Individualized-Curricula"><a href="#Continual-Driving-Policy-Optimization-with-Closed-Loop-Individualized-Curricula" class="headerlink" title="Continual Driving Policy Optimization with Closed-Loop Individualized Curricula"></a>Continual Driving Policy Optimization with Closed-Loop Individualized Curricula</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14209">http://arxiv.org/abs/2309.14209</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YizhouXu-THU/CLIC">https://github.com/YizhouXu-THU/CLIC</a></li>
<li>paper_authors: Haoyi Niu, Yizhou Xu, Xingjian Jiang, Jianming Hu<br>for: This paper aims to improve the safety of autonomous vehicles (AVs) by developing a continual driving policy optimization framework called Closed-Loop Individualized Curricula (CLIC).methods: The CLIC framework uses a collision prediction task to estimate the chance of AV failures in pre-collected scenarios, and then tailors individualized curricula for downstream training based on these failure probabilities.results: The experimental results show that CLIC surpasses other curriculum-based training strategies in managing risky scenarios while maintaining proficiency in handling simpler cases, demonstrating the effectiveness of the CLIC framework in improving the safety of AVs.Here is the answer in Simplified Chinese text:for: 这篇论文目的是提高自动驾驶车辆（AV）的安全性，通过开发一种循环驾驶政策优化框架——封闭循环个性化课程（CLIC）。methods: CLIC框架使用碰撞预测任务来估计AV失败的可能性，然后基于这些失败概率而tailor个性化课程 для下游训练。results: 实验结果表明，CLIC超过了其他课程基本训练策略，在管理危险场景方面达到了显著改进，而且仍能保持处理简单场景的能力，这表明CLIC框架可以有效地提高AV的安全性。<details>
<summary>Abstract</summary>
The safety of autonomous vehicles (AV) has been a long-standing top concern, stemming from the absence of rare and safety-critical scenarios in the long-tail naturalistic driving distribution. To tackle this challenge, a surge of research in scenario-based autonomous driving has emerged, with a focus on generating high-risk driving scenarios and applying them to conduct safety-critical testing of AV models. However, limited work has been explored on the reuse of these extensive scenarios to iteratively improve AV models. Moreover, it remains intractable and challenging to filter through gigantic scenario libraries collected from other AV models with distinct behaviors, attempting to extract transferable information for current AV improvement. Therefore, we develop a continual driving policy optimization framework featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into a set of standardized sub-modules for flexible implementation choices: AV Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a collision prediction task, where it estimates the chance of AV failures in these scenarios at each iteration. Subsequently, by re-sampling from historical scenarios based on these failure probabilities, CLIC tailors individualized curricula for downstream training, aligning them with the evaluated capability of AV. Accordingly, CLIC not only maximizes the utilization of the vast pre-collected scenario library for closed-loop driving policy optimization but also facilitates AV improvement by individualizing its training with more challenging cases out of those poorly organized scenarios. Experimental results clearly indicate that CLIC surpasses other curriculum-based training strategies, showing substantial improvement in managing risky scenarios, while still maintaining proficiency in handling simpler cases.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆（AV）的安全性问题一直是长期的主要担忧，这是因为自然驾驶驾驶分布中罕见的危险和安全关键场景的缺失。为解决这个挑战，自动驾驶场景研究有了很大的干预，关注生成高风险驾驶场景，并应用其进行安全检测自动驾驶模型。然而，有限的研究是关于重复这些广泛的场景来进一步改进AV模型。此外，从其他AV模型的巨大场景库中挑选有用信息是困难和挑战的。因此，我们开发了一个基于closed-loop个性化课程（CLIC）的驱动策略优化框架，它可以分解为以下几个标准化子模块：AV评估、场景选择和AV培训。在CLIC中，AV评估被设置为预测AV失败的概率任务，每轮评估AV在这些场景中的失败概率，然后根据这些概率重新采样历史场景，为下游培训生成个性化课程，使AV的培训更加个性化，与评估其能力相匹配。因此，CLIC不仅可以最大化已收集的历史场景库的利用，同时也可以通过个性化培训，提高AV在危险场景中的管理能力，而不会妨碍其在简单场景中的运作。实验结果表明，CLIC超越了其他课程基本培训策略，在管理危险场景方面显示了明显的改进，而且仍能保持简单场景中的运作效率。
</details></li>
</ul>
<hr>
<h2 id="Framework-based-on-complex-networks-to-model-and-mine-patient-pathways"><a href="#Framework-based-on-complex-networks-to-model-and-mine-patient-pathways" class="headerlink" title="Framework based on complex networks to model and mine patient pathways"></a>Framework based on complex networks to model and mine patient pathways</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14208">http://arxiv.org/abs/2309.14208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caroline-rosa/framework_patient_pathways">https://github.com/caroline-rosa/framework_patient_pathways</a></li>
<li>paper_authors: Caroline de Oliveira Costa Souza Rosa, Márcia Ito, Alex Borges Vieira, Klaus Wehmuth, Antônio Tadeu Azevedo Gomes</li>
<li>for: 这个研究旨在自动发现患者群体的医疗系统历史记录，以提高医疗质量和效率。</li>
<li>methods: 该研究提出了一个框架，包括多方面图模型、基于时间的不同程度衡量方法和基于传统中心度指标的挖掘方法。</li>
<li>results: 研究在孕综和糖尿病两个例子中证明了该框架的有用性，可以找到相似路径集合、简洁表示路径和按照多个视角显示最重要的 Pattern。<details>
<summary>Abstract</summary>
The automatic discovery of a model to represent the history of encounters of a group of patients with the healthcare system -- the so-called "pathway of patients" -- is a new field of research that supports clinical and organisational decisions to improve the quality and efficiency of the treatment provided. The pathways of patients with chronic conditions tend to vary significantly from one person to another, have repetitive tasks, and demand the analysis of multiple perspectives (interventions, diagnoses, medical specialities, among others) influencing the results. Therefore, modelling and mining those pathways is still a challenging task. In this work, we propose a framework comprising: (i) a pathway model based on a multi-aspect graph, (ii) a novel dissimilarity measurement to compare pathways taking the elapsed time into account, and (iii) a mining method based on traditional centrality measures to discover the most relevant steps of the pathways. We evaluated the framework using the study cases of pregnancy and diabetes, which revealed its usefulness in finding clusters of similar pathways, representing them in an easy-to-interpret way, and highlighting the most significant patterns according to multiple perspectives.
</details>
<details>
<summary>摘要</summary>
自动发现患者群体对医疗系统的互动历史模型 -- 称之为"患者路径" -- 是一个新的研究领域，用于支持临床和组织决策，以提高治疗质量和效率。患者的路径通常在不同人群中有很大差异，具有重复的任务和多个视角（如 intervenciones、诊断、医学专业等）的影响。因此，模型和挖掘这些路径仍然是一项挑战。在这项工作中，我们提出了以下框架：（i）基于多方面图的路径模型，（ii）基于时间因素的不同度量来比较路径，以及（iii）基于传统中心度量来挖掘路径中最重要的步骤。我们使用了孕期和糖尿病两个案例进行评估，发现该框架可以快速找到相似路径集，将其易于理解地表示出来，并高亮多个视角中的重要特征。
</details></li>
</ul>
<hr>
<h2 id="LLMCarbon-Modeling-the-end-to-end-Carbon-Footprint-of-Large-Language-Models"><a href="#LLMCarbon-Modeling-the-end-to-end-Carbon-Footprint-of-Large-Language-Models" class="headerlink" title="LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models"></a>LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14393">http://arxiv.org/abs/2309.14393</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sotarokaneda/mlcarbon">https://github.com/sotarokaneda/mlcarbon</a></li>
<li>paper_authors: Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Parteek Sharma, Fan Chen, Lei Jiang</li>
<li>for: 这研究旨在提供一个能够精准计算大语言模型（LLM）训练过程中的碳脚印，包括操作和嵌入碳脚印，以及新的 нейрон网络设计的碳脚印预测模型。</li>
<li>methods: 该研究使用了一种名为\textit{LLMCarbon}的端到端碳脚印预测模型，可以对 dense 和 mixture-of-experts（MoE） LLMs 进行碳脚印预测。与之前的研究mlco2相比，\textit{LLMCarbon} 能够更好地预测不同 LLMs 的碳脚印。</li>
<li>results: 对于不同的 LLMs，\textit{LLMCarbon} 能够提供更高的预测精度，并且可以模型出操作和嵌入碳脚印，以及新的 нейрон网络设计的碳脚印。<details>
<summary>Abstract</summary>
The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{LLMCarbon}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly enhances the accuracy of carbon footprint estimations for various LLMs.
</details>
<details>
<summary>摘要</summary>
Large language models (LLMs) 的碳 hoofprint 是一个重要的问题，包括训练、推理、实验和存储过程中的碳排放，包括运行和嵌入碳排放。一个重要的方面是在新的 LLM 出现之前已经准确地估算其碳影响，这主要取决于 GPU 使用情况。现有的研究已经报告了 LLM 训练的碳排放，但只有一个工具，mlco2，可以在物理训练之前预测新的神经网络的碳排放。然而，mlco2 有多个严重的限制。它无法扩展到 dense 或 mixture-of-experts (MoE) LLMs，忽略了关键的建筑 Parameters，围绕 GPU 进行固定的注意力，并不能模拟嵌入碳排放。为了解决这些缺陷，我们介绍了 \textit{LLMCarbon}，一个针对 dense 和 MoE LLMs 的碳排放预测模型。与 mlco2 相比，LLMCarbon 可以对不同类型的 LLMs 提供更高精度的碳排放估算。
</details></li>
</ul>
<hr>
<h2 id="Species196-A-One-Million-Semi-supervised-Dataset-for-Fine-grained-Species-Recognition"><a href="#Species196-A-One-Million-Semi-supervised-Dataset-for-Fine-grained-Species-Recognition" class="headerlink" title="Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition"></a>Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14183">http://arxiv.org/abs/2309.14183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Species-Dataset/species-dataset.github.io">https://github.com/Species-Dataset/species-dataset.github.io</a></li>
<li>paper_authors: Wei He, Kai Han, Ying Nie, Chengcheng Wang, Yunhe Wang</li>
<li>for: 本研究旨在提供大规模的 semi-supervised 数据集，用于驱逐物种识别领域的深度学习基础模型开发。</li>
<li>methods: 本研究使用 semi-supervised 学习方法，包括 Species196-L 和 Species196-U 两个数据集，以及四种 эксперименталь设定：超级vised 学习、semi-supervised 学习、自我supervised 预训练和 zero-shot 推理。</li>
<li>results: 本研究通过对 Species196 数据集的 represntative 方法进行实证研究，以评估这些方法在驱逐物种识别领域的表现。<details>
<summary>Abstract</summary>
The development of foundation vision models has pushed the general visual recognition to a high level, but cannot well address the fine-grained recognition in specialized domain such as invasive species classification. Identifying and managing invasive species has strong social and ecological value. Currently, most invasive species datasets are limited in scale and cover a narrow range of species, which restricts the development of deep-learning based invasion biometrics systems. To fill the gap of this area, we introduced Species196, a large-scale semi-supervised dataset of 196-category invasive species. It collects over 19K images with expert-level accurate annotations Species196-L, and 1.2M unlabeled images of invasive species Species196-U. The dataset provides four experimental settings for benchmarking the existing models and algorithms, namely, supervised learning, semi-supervised learning, self-supervised pretraining and zero-shot inference ability of large multi-modal models. To facilitate future research on these four learning paradigms, we conduct an empirical study of the representative methods on the introduced dataset. The dataset is publicly available at https://species-dataset.github.io/.
</details>
<details>
<summary>摘要</summary>
开发基础视觉模型已经提高了普通视识能力到高水平，但无法良好地解决特殊领域的细腻识别。识别和管理入侵物种有着强烈的社会和生态价值。目前，大多数入侵物种数据集都具有有限的规模和局部的种类覆盖率，这限制了深入学习基于入侵生物ometrics系统的发展。为了填补这一领域的空白，我们引入了Species196数据集，这是一个大规模的半指导式数据集，收集了196类入侵物种的19K多张图像，其中Expert-level准确标注 Species196-L，以及1.2万张不标注的入侵物种图像 Species196-U。该数据集提供了四种实验设置，用于测试现有模型和算法的性能，即：指导学习、半指导学习、自动预训练和大多模式模型的零码推理能力。为了促进未来关于这四种学习方法的研究，我们进行了 Species196 数据集上的实验研究。该数据集公开可用于 <https://species-dataset.github.io/>。
</details></li>
</ul>
<hr>
<h2 id="Q-Bench-A-Benchmark-for-General-Purpose-Foundation-Models-on-Low-level-Vision"><a href="#Q-Bench-A-Benchmark-for-General-Purpose-Foundation-Models-on-Low-level-Vision" class="headerlink" title="Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision"></a>Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14181">http://arxiv.org/abs/2309.14181</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Q-Future/Q-Bench">https://github.com/Q-Future/Q-Bench</a></li>
<li>paper_authors: Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin</li>
<li>for: 这个论文是为了评估多Modal Large Language Models (MLLMs)在低级别视觉理解和描述能力方面的能力而写的。</li>
<li>methods: 这个论文使用了以下方法：constructed LLVisionQA dataset，proposed LLDescribe dataset，和一种基于GPT的比较管道来评估MLLMs的描述能力。</li>
<li>results: 这个论文的结果表明MLLMs具有初步的低级别视觉能力，但这些能力还是不稳定和不准确的，需要进一步的提升。<details>
<summary>Abstract</summary>
The rapid evolution of Multi-modality Large Language Models (MLLMs) has catalyzed a shift in computer vision from specialized models to general-purpose foundation models. Nevertheless, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a holistic benchmark crafted to systematically evaluate potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment. a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions. b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: https://vqassessment.github.io/Q-Bench.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>多Modal Large Language Models（MLLMs）的快速EVOLUTION catalyzed a shift from specialized models to general-purpose foundation models in computer vision. However, there is still an inadequacy in assessing the abilities of MLLMs on low-level visual perception and understanding. To address this gap, we present Q-Bench, a comprehensive benchmark crafted to systematically evaluate the potential abilities of MLLMs on three realms: low-level visual perception, low-level visual description, and overall visual quality assessment.a) To evaluate the low-level perception ability, we construct the LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped with a human-asked question focusing on its low-level attributes. We then measure the correctness of MLLMs on answering these questions.b) To examine the description ability of MLLMs on low-level information, we propose the LLDescribe dataset consisting of long expert-labelled golden low-level text descriptions on 499 images, and a GPT-involved comparison pipeline between outputs of MLLMs and the golden descriptions.c) Besides these two tasks, we further measure their visual quality assessment ability to align with human opinion scores. Specifically, we design a softmax-based strategy that enables MLLMs to predict quantifiable quality scores, and evaluate them on various existing image quality assessment (IQA) datasets. Our evaluation across the three abilities confirms that MLLMs possess preliminary low-level visual skills. However, these skills are still unstable and relatively imprecise, indicating the need for specific enhancements on MLLMs towards these abilities. We hope that our benchmark can encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs. Project Page: <https://vqassessment.github.io/Q-Bench>.
</details></li>
</ul>
<hr>
<h2 id="Data-Upcycling-Knowledge-Distillation-for-Image-Super-Resolution"><a href="#Data-Upcycling-Knowledge-Distillation-for-Image-Super-Resolution" class="headerlink" title="Data Upcycling Knowledge Distillation for Image Super-Resolution"></a>Data Upcycling Knowledge Distillation for Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14162">http://arxiv.org/abs/2309.14162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhang, Wei Li, Simiao Li, Jie Hu, Hanting Chen, Hailing Wang, Zhijun Tu, Wenjia Wang, Bingyi Jing, Yunhe Wang</li>
<li>for: 这个论文旨在提出一种基于有效数据利用的知识储存抽象（DUKD）方法，以提高单个图像超分解（SISR）学习模型的表现。</li>
<li>methods: 该方法利用了两种有效的图像缩放操作和可逆数据增强操作，通过引入标签一致常数化来加强知识储存抽象的效果。</li>
<li>results: 对于多个 benchmark 测试，DUKD 方法可以明显超过基eline方法，例如PSNR 指标提高0.5dB，并且减少了 RCAN 模型的参数量，但是其表现与 RCAN 教师模型相当。<details>
<summary>Abstract</summary>
Knowledge distillation (KD) emerges as a challenging yet promising technique for compressing deep learning models, characterized by the transmission of extensive learning representations from proficient and computationally intensive teacher models to compact student models. However, only a handful of studies have endeavored to compress the models for single image super-resolution (SISR) through KD, with their effects on student model enhancement remaining marginal. In this paper, we put forth an approach from the perspective of efficient data utilization, namely, the Data Upcycling Knowledge Distillation (DUKD) which facilitates the student model by the prior knowledge teacher provided via upcycled in-domain data derived from their inputs. This upcycling process is realized through two efficient image zooming operations and invertible data augmentations which introduce the label consistency regularization to the field of KD for SISR and substantially boosts student model's generalization. The DUKD, due to its versatility, can be applied across a broad spectrum of teacher-student architectures. Comprehensive experiments across diverse benchmarks demonstrate that our proposed DUKD method significantly outperforms previous art, exemplified by an increase of up to 0.5dB in PSNR over baselines methods, and a 67% parameters reduced RCAN model's performance remaining on par with that of the RCAN teacher model.
</details>
<details>
<summary>摘要</summary>
知识储备（KD）技术为深度学习模型压缩，涉及教师模型传递丰富的学习表示，以提高学生模型的表达能力。然而，只有一些研究利用KD技术进行单张图像超分辨（SISR）压缩，其影响于学生模型的提高仍然较有限。本文提出了一种基于有效数据利用的方法，即数据升级知识储备（DUKD），通过教师模型提供的先前知识，对学生模型进行升级。这个升级过程通过两种高效的图像缩放操作和可逆数据增强来实现，并在KD领域中引入标签一致化规则。DUKD方法因其灵活性，可以应用于多种教师-学生架构。实验结果表明，我们提出的DUKD方法在多个标准benchmark上达到了 significanly更高的PSNR水平，相比基eline方法，提高了67%的参数量，而RCAN教师模型的性能仍然与RCAN教师模型相当。
</details></li>
</ul>
<hr>
<h2 id="SPIRT-A-Fault-Tolerant-and-Reliable-Peer-to-Peer-Serverless-ML-Training-Architecture"><a href="#SPIRT-A-Fault-Tolerant-and-Reliable-Peer-to-Peer-Serverless-ML-Training-Architecture" class="headerlink" title="SPIRT: A Fault-Tolerant and Reliable Peer-to-Peer Serverless ML Training Architecture"></a>SPIRT: A Fault-Tolerant and Reliable Peer-to-Peer Serverless ML Training Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14148">http://arxiv.org/abs/2309.14148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amine Barrak, Mayssa Jaziri, Ranim Trabelsi, Fehmi Jaafar, Fabio Petrillo</li>
<li>for: 这篇论文旨在探讨 Parametric Serverless Distributed Machine Learning (PSDML) 技术，尤其是在 P2P 分布式学习环境中。</li>
<li>methods: 这篇论文提出了一种基于 RedisAI 的 P2P 分布式学习架构，名为 SPIRT，以实现 fault-tolerant、可靠和安全的分布式机器学习训练。</li>
<li>results: SPIRT 架构可以减少模型更新和梯度平均所需时间的82%，并且具有抗坏 peer 和新 peer 集成的能力，同时保证了分布式机器学习任务的安全性。<details>
<summary>Abstract</summary>
The advent of serverless computing has ushered in notable advancements in distributed machine learning, particularly within parameter server-based architectures. Yet, the integration of serverless features within peer-to-peer (P2P) distributed networks remains largely uncharted. In this paper, we introduce SPIRT, a fault-tolerant, reliable, and secure serverless P2P ML training architecture. designed to bridge this existing gap.   Capitalizing on the inherent robustness and reliability innate to P2P systems, SPIRT employs RedisAI for in-database operations, leading to an 82\% reduction in the time required for model updates and gradient averaging across a variety of models and batch sizes. This architecture showcases resilience against peer failures and adeptly manages the integration of new peers, thereby highlighting its fault-tolerant characteristics and scalability. Furthermore, SPIRT ensures secure communication between peers, enhancing the reliability of distributed machine learning tasks. Even in the face of Byzantine attacks, the system's robust aggregation algorithms maintain high levels of accuracy. These findings illuminate the promising potential of serverless architectures in P2P distributed machine learning, offering a significant stride towards the development of more efficient, scalable, and resilient applications.
</details>
<details>
<summary>摘要</summary>
来自服务器无法 computing的启示，导致分布式机器学习中的分布式机器学习架构得到了重要的进步，特别是在基于参数服务器的架构中。然而，在对等（P2P）分布式网络中 интеGRATION of serverless特性仍然largely unexplored。在这篇论文中，我们引入SPIRT，一个可靠、可靠性和安全的服务器无法分布式机器学习训练架构。通过利用P2P系统的自然强大和可靠性，SPIRT使用RedisAI进行库操作，从而实现82%的模型更新和梯度平均时间优化。这个架构展示了对 peer 失败的抗衰变和新 peer 的适应能力，彰显其可靠性和可扩展性。此外，SPIRT确保了peer之间的安全通信，进一步提高了分布式机器学习任务的可靠性。甚至在面对拜尼黑攻击时，系统的坚固的总和算法可以保持高水平的准确性。这些发现探讨了服务器无法架构在P2P分布式机器学习中的应用前景，提供了一个重要的进步。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Impact-of-Serverless-Computing-on-Peer-To-Peer-Training-Machine-Learning"><a href="#Exploring-the-Impact-of-Serverless-Computing-on-Peer-To-Peer-Training-Machine-Learning" class="headerlink" title="Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning"></a>Exploring the Impact of Serverless Computing on Peer To Peer Training Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14139">http://arxiv.org/abs/2309.14139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aminebarrak/peertopeerserverless">https://github.com/aminebarrak/peertopeerserverless</a></li>
<li>paper_authors: Amine Barrak, Ranim Trabelsi, Fehmi Jaafar, Fabio Petrillo</li>
<li>for: 该论文主要旨在提出一种基于服务器レス计算和分布式训练的新架构，以提高分布式训练的可扩展性和容错性。</li>
<li>methods: 该论文使用了分布式 gradient computation 技术，并提出了一种基于 serverless computing 的高效并发分布式训练方法。</li>
<li>results: 研究发现，与传统分布式训练方法相比，该方法可以提高分布式训练的 gradient computation 时间，最高可达 97.34% 的提升。然而，在资源约束下，服务器レス架构可能带来更高的成本，最高达 5.4 倍于实例基础架构。<details>
<summary>Abstract</summary>
The increasing demand for computational power in big data and machine learning has driven the development of distributed training methodologies. Among these, peer-to-peer (P2P) networks provide advantages such as enhanced scalability and fault tolerance. However, they also encounter challenges related to resource consumption, costs, and communication overhead as the number of participating peers grows. In this paper, we introduce a novel architecture that combines serverless computing with P2P networks for distributed training and present a method for efficient parallel gradient computation under resource constraints.   Our findings show a significant enhancement in gradient computation time, with up to a 97.34\% improvement compared to conventional P2P distributed training methods. As for costs, our examination confirmed that the serverless architecture could incur higher expenses, reaching up to 5.4 times more than instance-based architectures. It is essential to consider that these higher costs are associated with marked improvements in computation time, particularly under resource-constrained scenarios. Despite the cost-time trade-off, the serverless approach still holds promise due to its pay-as-you-go model. Utilizing dynamic resource allocation, it enables faster training times and optimized resource utilization, making it a promising candidate for a wide range of machine learning applications.
</details>
<details>
<summary>摘要</summary>
随着大数据和机器学习的需求增长，分布式训练方法得到了广泛应用。在这些方法中，点对点（P2P）网络具有提高可扩展性和fault tolerance的优势。然而，随着参与者的增加，P2P网络也面临资源占用、成本和通信开销的挑战。在这篇论文中，我们介绍了一种新的架构，即无服务器计算与P2P网络的结合，用于分布式训练。我们还提出了一种高效的并发梯度计算方法，以适应资源限制的情况。我们的研究表明，在资源限制情况下，使用无服务器架构可以提高梯度计算时间，最多达97.34%。相比传统的P2P分布式训练方法。虽然无服务器架构可能会增加成本，但是这些成本与计算时间之间的trade-off很明显。尤其是在资源受限的情况下，无服务器架构仍然保持了优势。通过动态资源分配，它可以减少训练时间并优化资源利用，使其成为许多机器学习应用的优选。
</details></li>
</ul>
<hr>
<h2 id="Small-Objects-Matters-in-Weakly-supervised-Semantic-Segmentation"><a href="#Small-Objects-Matters-in-Weakly-supervised-Semantic-Segmentation" class="headerlink" title="Small Objects Matters in Weakly-supervised Semantic Segmentation"></a>Small Objects Matters in Weakly-supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14117">http://arxiv.org/abs/2309.14117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheolhyun Mun, Sanghuk Lee, Youngjung Uh, Junsuk Choe, Hyeran Byun</li>
<li>for: 本研究旨在提供一种全面评估不同 объек 大小的 semantic segmentation 方法的评价指标，以及一个大小均衡的评估集，以便评估不同的 object size 下的 semantic segmentation 方法表现。</li>
<li>methods: 本研究提出了一种新的评价指标，以及一种大小均衡的 cross-entropy 损失函数，以及一种适当的训练策略，以解决现有的 semantic segmentation 方法在小对象上的表现不佳问题。</li>
<li>results: 对于十个基准方法在三个不同的 datasets 上进行了评估，研究发现现有的 semantic segmentation 方法在小对象上的表现不佳，而新提出的大小均衡 cross-entropy 损失函数和适当的训练策略可以改善现有的 semantic segmentation 方法表现。<details>
<summary>Abstract</summary>
Weakly-supervised semantic segmentation (WSSS) performs pixel-wise classification given only image-level labels for training. Despite the difficulty of this task, the research community has achieved promising results over the last five years. Still, current WSSS literature misses the detailed sense of how well the methods perform on different sizes of objects. Thus we propose a novel evaluation metric to provide a comprehensive assessment across different object sizes and collect a size-balanced evaluation set to complement PASCAL VOC. With these two gadgets, we reveal that the existing WSSS methods struggle in capturing small objects. Furthermore, we propose a size-balanced cross-entropy loss coupled with a proper training strategy. It generally improves existing WSSS methods as validated upon ten baselines on three different datasets.
</details>
<details>
<summary>摘要</summary>
弱监督semantic segmentation（WSSS）在给定图像级别标签的情况下进行像素级分类。虽然这是一项复杂的任务，但过去五年研究社区已经取得了可喜的成果。然而，现有WSSS литераature缺乏对不同物体大小的详细评估。因此，我们提出了一种新的评估度量，并收集了一个Size-balanced评估集，以完善PASCAL VOC。通过这两个工具，我们发现现有WSSS方法在捕捉小物体方面努力不足。此外，我们提出了一种Size-balancedcross-entropy损失函数，并与适当的训练策略相结合。它通常会改进现有WSSS方法，并在三个不同的dataset上验证了十个基elines。
</details></li>
</ul>
<hr>
<h2 id="Semi-Abstract-Value-Based-Argumentation-Framework"><a href="#Semi-Abstract-Value-Based-Argumentation-Framework" class="headerlink" title="Semi-Abstract Value-Based Argumentation Framework"></a>Semi-Abstract Value-Based Argumentation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14112">http://arxiv.org/abs/2309.14112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jovan Jeromela</li>
<li>for: 本论文主要研究的是abstract argumentation frameworks的扩展和应用。</li>
<li>methods: 本论文使用了value-based argumentation framework和semi-abstract argumentation framework两种扩展，它们增加了对Arguments的结构化表示。</li>
<li>results: 本论文提出了一种新的semi-abstract value-based argumentation framework，该框架可以将 Proposition associate with individual arguments映射到一组排序的值上，并通过新引入的攻击原则来使得隐式攻击变得明确。此外，本论文还使用了这两种框架来形ulate一个复杂的道德困境。<details>
<summary>Abstract</summary>
In his seminal paper, Phan Minh Dung (1995) proposed abstract argumentation framework, which models argumentation using directed graphs where structureless arguments are the nodes and attacks among the arguments are the edges. In the following years, many extensions of this framework were introduced. These extensions typically add a certain form of structure to the arguments. This thesis showcases two such extensions -- value-based argumentation framework by Trevor Bench-Capon (2002) and semi-abstract argumentation framework by Esther Anna Corsi and Christian Ferm\"uller (2017). The former introduces a mapping function that links individual arguments to a set of ordered values, enabling a distinction between objectively and subjectively acceptable arguments. The latter links claims of individual arguments to propositional formulae and then applies newly-introduced attack principles in order to make implicit attacks explicit and to enable a definition of a consequence relation that relies on neither the truth values nor the interpretations in the usual sense.   The contribution of this thesis is two-fold. Firstly, the new semi-abstract value-based argumentation framework is introduced. This framework maps propositional formulae associated with individual arguments to a set of ordered values. Secondly, a complex moral dilemma is formulated using the original and the value-based argumentation frameworks showcasing the expressivity of these formalisms.
</details>
<details>
<summary>摘要</summary>
Phan Minh Dung（1995）提出了抽象论证框架，该框架使用导航图模型了论证，其中无结构的论证是图节点，而论证之间的攻击是图边。后来，许多对这种框架的扩展都被引入。这些扩展通常增加了论证的某种结构。本论文介绍了两种这种扩展：基于值的论证框架（Trevor Bench-Capon，2002）和半抽象论证框架（Esther Anna Corsi和Christian Fermüller，2017）。前者引入了一个映射函数，该函数将 individuak 论证映射到一个排序的值集中，以便分辨 объекively 和 subjectively 可接受的论证。后者将各个论证的laims链接到 propositional 式中，然后应用新引入的攻击原则，以使隐式攻击显式化，并使得定义一种基于真值和解释的后果关系。本论文的贡献有两个方面。首先，本论文引入了一种新的半抽象值基论证框架，该框架将 propositional 式与值集相关联。其次，通过原始论证框架和值基论证框架，形ulated 一个复杂的道德困境示例，以示这两种形式主义的表达能力。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Overview-of-Named-Entity-Recognition-Models-Domain-Specific-Applications-and-Challenges"><a href="#Comprehensive-Overview-of-Named-Entity-Recognition-Models-Domain-Specific-Applications-and-Challenges" class="headerlink" title="Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges"></a>Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14084">http://arxiv.org/abs/2309.14084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kalyani Pakhale</li>
<li>for: 本文旨在探讨Named Entity Recognition（NER）技术的发展和应用，尤其是将传统rule-based策略与当今AI技术相结合，以提高NER的准确率和泛化能力。</li>
<li>methods: 本文涵盖了NER的基本概念、传统技术和当今AI技术的应用，包括BERT、LSTM和CNN等。特别是在领域化NER模型方面，本文强调了适应性的重要性，并提出了域специфи互调模型的概念。</li>
<li>results: 本文通过实践示例和数据分析，证明了NER技术在金融和生物医学等领域的应用，提高了自动化文本分类和结构化抽取的精度和效率。同时，本文还探讨了NER技术的未来发展和挑战，提出了一些未来研究的可能性和方向。<details>
<summary>Abstract</summary>
In the domain of Natural Language Processing (NLP), Named Entity Recognition (NER) stands out as a pivotal mechanism for extracting structured insights from unstructured text. This manuscript offers an exhaustive exploration into the evolving landscape of NER methodologies, blending foundational principles with contemporary AI advancements. Beginning with the rudimentary concepts of NER, the study spans a spectrum of techniques from traditional rule-based strategies to the contemporary marvels of transformer architectures, particularly highlighting integrations such as BERT with LSTM and CNN. The narrative accentuates domain-specific NER models, tailored for intricate areas like finance, legal, and healthcare, emphasizing their specialized adaptability. Additionally, the research delves into cutting-edge paradigms including reinforcement learning, innovative constructs like E-NER, and the interplay of Optical Character Recognition (OCR) in augmenting NER capabilities. Grounding its insights in practical realms, the paper sheds light on the indispensable role of NER in sectors like finance and biomedicine, addressing the unique challenges they present. The conclusion outlines open challenges and avenues, marking this work as a comprehensive guide for those delving into NER research and applications.
</details>
<details>
<summary>摘要</summary>
在自然语言处理（NLP）领域，命名实体识别（NER）作为提取结构化知识从未结构化文本中的重要机制，这篇论文对NER方法的发展进行了极其广泛的探讨，结合了基础原则和当代人工智能技术。这篇论文从传统的规则基础的斜笔概念开始，涵盖了从传统的字符串处理技术到当代的变换器架构，特别是BERT与LSTM和CNN的集成。研究着重点在各个领域中特化的NER模型，如金融、法律和医疗等，强调其特殊适应性。此外，研究还探讨了当前的前沿方法，如强化学习、创新的构造和E-NER，以及Optical Character Recognition（OCR）在NER能力的增强中的作用。以实际场景为基础，论文探讨了NER在金融和生物医学等领域的不可或缺的作用，解决这些领域所存在的特殊挑战。结尾，论文概述了目前的开放挑战和前瞻，用作NER研究和应用的全面指南。
</details></li>
</ul>
<hr>
<h2 id="ODE-based-Recurrent-Model-free-Reinforcement-Learning-for-POMDPs"><a href="#ODE-based-Recurrent-Model-free-Reinforcement-Learning-for-POMDPs" class="headerlink" title="ODE-based Recurrent Model-free Reinforcement Learning for POMDPs"></a>ODE-based Recurrent Model-free Reinforcement Learning for POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14078">http://arxiv.org/abs/2309.14078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanle Zhao, Duzhen Zhang, Liyuan Han, Tielin Zhang, Bo Xu</li>
<li>for: 解决部分可见（PO）环境中的不可见信息推理问题，提高agent的决策能力。</li>
<li>methods: 使用循环策略与紧凑上下文，基于上下文抽象学习（Context-based reinforcement learning）来提取历史转移中的不可见信息。</li>
<li>results: 通过结合ODEs和无约束RL框架，在POMDPs中解决部分可见控制和meta-RL任务，并在不规则观察数据上进行了实验验证。<details>
<summary>Abstract</summary>
Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularly-sampled time series.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Maximum-Likelihood-Estimation-of-Latent-Variable-Structural-Equation-Models-A-Neural-Network-Approach"><a href="#Maximum-Likelihood-Estimation-of-Latent-Variable-Structural-Equation-Models-A-Neural-Network-Approach" class="headerlink" title="Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach"></a>Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14073">http://arxiv.org/abs/2309.14073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehrzad Saremi</li>
<li>for: 这个论文是为了提出一种稳定的图形结构模型，可以在linearity和Gaussianity假设下保持稳定。</li>
<li>methods: 该论文使用了一种基于GPU的算法，用于计算最大 likelihood estimation 的这些模型。</li>
<li>results: 该论文表明，计算最大 likelihood estimation 的这些模型等价于训练一个神经网络。<details>
<summary>Abstract</summary>
We propose a graphical structure for structural equation models that is stable under marginalization under linearity and Gaussianity assumptions. We show that computing the maximum likelihood estimation of this model is equivalent to training a neural network. We implement a GPU-based algorithm that computes the maximum likelihood estimation of these models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种图解结构，用于结构方程模型，该结构在 Linearity 和 Gaussianity 假设下是稳定的。我们表明计算最大likelihood估计这种模型的过程与训练神经网络相同。我们实现了基于GPU的算法，用于计算这种模型的最大likelihood估计。Note: "Linearity" and "Gaussianity" are not exact translations of the English words, but they are commonly used terms in statistics and machine learning to refer to the assumptions of linearity and normality, respectively.
</details></li>
</ul>
<hr>
<h2 id="Adapt-then-Unlearn-Exploiting-Parameter-Space-Semantics-for-Unlearning-in-Generative-Adversarial-Networks"><a href="#Adapt-then-Unlearn-Exploiting-Parameter-Space-Semantics-for-Unlearning-in-Generative-Adversarial-Networks" class="headerlink" title="Adapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks"></a>Adapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14054">http://arxiv.org/abs/2309.14054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Piyush Tiwary, Atri Guha, Subhodip Panda, Prathosh A. P</li>
<li>for: 防止深度生成模型生成包含不良、袋陋或危险内容的输出。</li>
<li>methods: 基于用户提供的负例进行适应，然后使用排斥正则化训练已经适应模型，以快速忘记特定不良特征。</li>
<li>results: 验证了方法的有效性，能够快速、高效地忘记深度生成模型中不良特征，同时保持生成样本质量。<details>
<summary>Abstract</summary>
The increased attention to regulating the outputs of deep generative models, driven by growing concerns about privacy and regulatory compliance, has highlighted the need for effective control over these models. This necessity arises from instances where generative models produce outputs containing undesirable, offensive, or potentially harmful content. To tackle this challenge, the concept of machine unlearning has emerged, aiming to forget specific learned information or to erase the influence of undesired data subsets from a trained model. The objective of this work is to prevent the generation of outputs containing undesired features from a pre-trained GAN where the underlying training data set is inaccessible. Our approach is inspired by a crucial observation: the parameter space of GANs exhibits meaningful directions that can be leveraged to suppress specific undesired features. However, such directions usually result in the degradation of the quality of generated samples. Our proposed method, known as 'Adapt-then-Unlearn,' excels at unlearning such undesirable features while also maintaining the quality of generated samples. This method unfolds in two stages: in the initial stage, we adapt the pre-trained GAN using negative samples provided by the user, while in the subsequent stage, we focus on unlearning the undesired feature. During the latter phase, we train the pre-trained GAN using positive samples, incorporating a repulsion regularizer. This regularizer encourages the model's parameters to be away from the parameters associated with the adapted model from the first stage while also maintaining the quality of generated samples. To the best of our knowledge, our approach stands as first method addressing unlearning in GANs. We validate the effectiveness of our method through comprehensive experiments.
</details>
<details>
<summary>摘要</summary>
“随着深度生成模型的输出控制需求的增加，导致了关于隐私和合规遵循的担忧。这些担忧的来源是深度生成模型生成的内容中可能包含不适合、歧视或可能危害的内容。为了解决这个挑战，机器忘记（Machine Unlearning）的概念已经出现，旨在忘记特定学习的信息或从已训练的模型中除去不适合的数据子集。我们的目标是防止从已训练的GAN（生成推导网络）中生成包含不适合特征的出力。我们的方法是根据GAN的参数空间展现意义的方向来抑制不适合的特征。但是，这些方向通常会导致生成的样本质量下降。我们的提案方法，称为“Adapt-then-Unlearn”，能够忘记不适合的特征而保持生成的质量。这个方法分成两个阶段：在首先阶段，我们适应已训练的GAN使用用户提供的负面样本，而在后续阶段，我们专注于忘记不适合的特征。在这个阶段中，我们使用正常化器来训练已训练的GAN，并且将这些参数导向远离已适应的模型参数。我们的方法是首个对GAN进行忘记的方法。我们透过广泛的实验证明了我们的方法的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Revisiting-LARS-for-Large-Batch-Training-Generalization-of-Neural-Networks"><a href="#Revisiting-LARS-for-Large-Batch-Training-Generalization-of-Neural-Networks" class="headerlink" title="Revisiting LARS for Large Batch Training Generalization of Neural Networks"></a>Revisiting LARS for Large Batch Training Generalization of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14053">http://arxiv.org/abs/2309.14053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoi Do, Duong Nguyen, Hoa Nguyen, Long Tran-Thanh, Quoc-Viet Pham</li>
<li>for: 本文旨在研究Large Batch Learning（LBL）中的稳定性问题，特别是AI训练过程中捕捉到锐 minimum 的问题。</li>
<li>methods: 本文使用了LARS和LAMB两种广泛使用的技术，以及一种热启动策略。</li>
<li>results: 实验表明，TVLARS可以在不使用热启动策略的情况下实现稳定的训练，并且在使用热启动策略时可以与LARS和LAMB相比赢得竞争性的成绩。<details>
<summary>Abstract</summary>
LARS and LAMB have emerged as prominent techniques in Large Batch Learning (LBL), ensuring the stability of AI training. One of the primary challenges in LBL is convergence stability, where the AI agent usually gets trapped into the sharp minimizer. Addressing this challenge, a relatively recent technique, known as warm-up, has been employed. However, warm-up lacks a strong theoretical foundation, leaving the door open for further exploration of more efficacious algorithms. In light of this situation, we conduct empirical experiments to analyze the behaviors of the two most popular optimizers in the LARS family: LARS and LAMB, with and without a warm-up strategy. Our analyses give us a comprehension of the novel LARS, LAMB, and the necessity of a warm-up technique in LBL. Building upon these insights, we propose a novel algorithm called Time Varying LARS (TVLARS), which facilitates robust training in the initial phase without the need for warm-up. Experimental evaluation demonstrates that TVLARS achieves competitive results with LARS and LAMB when warm-up is utilized while surpassing their performance without the warm-up technique.
</details>
<details>
<summary>摘要</summary>
LARS和LAMB已成为大批学习（LBL）中显著的技术，确保训练稳定性。LBL的一个主要挑战是稳定性，AI代理通常会被拥堵在细小的最小值中。为解决这个挑战，一种相对较新的技术——暖身法——已经被采用。然而，暖身法没有强有力的理论基础，留下了进一步探索更有效的算法的门户。在这种情况下，我们进行了实验研究，分析了LARS和LAMB两个最受欢迎的优化器在LBL中的行为。我们的分析帮助我们更好地理解LARS、LAMB和暖身法的必要性，并在这些基础上提出了一种新的算法——时间变化LARS（TVLARS）。TVLARS可以在初始阶段实现稳定训练，不需要暖身法。实验评估表明，TVLARS在使用暖身法时与LARS和LAMB具有竞争性的性能，而无需暖身法时则超越它们。
</details></li>
</ul>
<hr>
<h2 id="An-automatic-selection-of-optimal-recurrent-neural-network-architecture-for-processes-dynamics-modelling-purposes"><a href="#An-automatic-selection-of-optimal-recurrent-neural-network-architecture-for-processes-dynamics-modelling-purposes" class="headerlink" title="An automatic selection of optimal recurrent neural network architecture for processes dynamics modelling purposes"></a>An automatic selection of optimal recurrent neural network architecture for processes dynamics modelling purposes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14037">http://arxiv.org/abs/2309.14037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krzysztof Laddach, Rafał Łangowski, Tomasz A. Rutkowski, Bartosz Puchalski</li>
<li>for: 这个论文是为了解决人工神经网络用于行为（黑盒）模型Selected动态过程的开发问题。</li>
<li>methods: 本研究包括四种原创的神经网络架构搜索算法，基于 известные优化技术如进化算法和梯度下降方法。</li>
<li>results: 在使用了扩展验证研究的数据，研究人员通过提出特殊化的进化操作来优化神经网络架构，实现了神经网络的尺寸和准确性之间的变换。<details>
<summary>Abstract</summary>
A problem related to the development of algorithms designed to find the structure of artificial neural network used for behavioural (black-box) modelling of selected dynamic processes has been addressed in this paper. The research has included four original proposals of algorithms dedicated to neural network architecture search. Algorithms have been based on well-known optimisation techniques such as evolutionary algorithms and gradient descent methods. In the presented research an artificial neural network of recurrent type has been used, whose architecture has been selected in an optimised way based on the above-mentioned algorithms. The optimality has been understood as achieving a trade-off between the size of the neural network and its accuracy in capturing the response of the mathematical model under which it has been learnt. During the optimisation, original specialised evolutionary operators have been proposed. The research involved an extended validation study based on data generated from a mathematical model of the fast processes occurring in a pressurised water nuclear reactor.
</details>
<details>
<summary>摘要</summary>
In the research, an artificial neural network of recurrent type is used, and its architecture is selected in an optimized way based on the above-mentioned algorithms. The optimality is understood as achieving a trade-off between the size of the neural network and its accuracy in capturing the response of the mathematical model under which it has been learned.During the optimization, original specialized evolutionary operators are proposed. The research involves an extended validation study based on data generated from a mathematical model of the fast processes occurring in a pressurized water nuclear reactor.Translated into Simplified Chinese:这篇论文关注了人工神经网络（ANN）用于Behavioral（黑盒）模型选择动态过程的开发算法问题。研究包括四种原创的算法提案，基于常见的优化技术 such as evolutionary algorithms和梯度下降方法。在研究中，使用了一个人工神经网络的回归类型，其架构通过上述算法进行优化。优化的目标是在神经网络的大小和学习模型的响应之间寻找一个平衡点。在优化过程中，提出了原创的特殊演化算法。研究还包括一个扩展验证研究，基于核电站压力水堆受控核反应的数学模型生成的数据。
</details></li>
</ul>
<hr>
<h2 id="DeepACO-Neural-enhanced-Ant-Systems-for-Combinatorial-Optimization"><a href="#DeepACO-Neural-enhanced-Ant-Systems-for-Combinatorial-Optimization" class="headerlink" title="DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization"></a>DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14032">http://arxiv.org/abs/2309.14032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/henry-yeh/DeepACO">https://github.com/henry-yeh/DeepACO</a></li>
<li>paper_authors: Haoran Ye, Jiarui Wang, Zhiguang Cao, Helan Liang, Yong Li</li>
<li>for: 本研究提出了一种基于深度学习的ACO框架，以自动化ACO算法中的规则设计。</li>
<li>methods: 该框架使用深度学习来强化ACO算法中的优化策略，并且只需一个神经网络和一组超参数来应用于多种具体问题。</li>
<li>results: 对八种具体问题进行测试，DeepACO consistently outperforms传统的ACO算法，并且在许多情况下比特定问题的方法更好或与其相当。<details>
<summary>Abstract</summary>
Ant Colony Optimization (ACO) is a meta-heuristic algorithm that has been successfully applied to various Combinatorial Optimization Problems (COPs). Traditionally, customizing ACO for a specific problem requires the expert design of knowledge-driven heuristics. In this paper, we propose DeepACO, a generic framework that leverages deep reinforcement learning to automate heuristic designs. DeepACO serves to strengthen the heuristic measures of existing ACO algorithms and dispense with laborious manual design in future ACO applications. As a neural-enhanced meta-heuristic, DeepACO consistently outperforms its ACO counterparts on eight COPs using a single neural model and a single set of hyperparameters. As a Neural Combinatorial Optimization method, DeepACO performs better than or on par with problem-specific methods on canonical routing problems. Our code is publicly available at https://github.com/henry-yeh/DeepACO.
</details>
<details>
<summary>摘要</summary>
《蟑螂群体优化（ACO）是一种元规则算法，已经成功应用于多种 combinatorial optimization problems（COPs）。传统上，为特定问题自定义 ACO 需要专家设计知识驱动的规则。在这篇论文中，我们提议了 DeepACO，一种通用框架，利用深度强化学习自动化规则设计。DeepACO 可以增强现有 ACO 算法的规则措施，并减少未来 ACO 应用中的劳动密集设计。作为一种神经元规则优化方法，DeepACO 在八种 COPs 上以单个神经网络和单个超参数表现出色，并且在 canonical routing problems 中表现更好或与专门方法一样。我们的代码公开在 GitHub 上，请参考 <https://github.com/henry-yeh/DeepACO>。》
</details></li>
</ul>
<hr>
<h2 id="Diffeomorphic-Transformations-for-Time-Series-Analysis-An-Efficient-Approach-to-Nonlinear-Warping"><a href="#Diffeomorphic-Transformations-for-Time-Series-Analysis-An-Efficient-Approach-to-Nonlinear-Warping" class="headerlink" title="Diffeomorphic Transformations for Time Series Analysis: An Efficient Approach to Nonlinear Warping"></a>Diffeomorphic Transformations for Time Series Analysis: An Efficient Approach to Nonlinear Warping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14029">http://arxiv.org/abs/2309.14029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iñigo Martinez</li>
<li>for: 这个论文主要针对的是如何处理时间序列数据，以及设计特有的时间序列相似性、分类和对应方法。</li>
<li>methods: 本论文提出了一些新的柔性对焦方法，使用参数化和对焦变换来扩展和改进传统的时间序列相似性计量方法。这些方法是可微的、可逆的、敏感度高且可以应对噪音和异常值。</li>
<li>results: 本论文的结果显示，这些新的柔性对焦方法可以实现高精度的时间序列相似性计量，并且可以与深度学习架构结合，以提高时间序列分类和对应的性能。此外，论文还提出了一些进一步的技术，例如增强的时间transformer网络、深度学习基于时间序列分类模型、可扩展的时间序列对焦分群算法和可扩展的时间序列对焦模型。<details>
<summary>Abstract</summary>
The proliferation and ubiquity of temporal data across many disciplines has sparked interest for similarity, classification and clustering methods specifically designed to handle time series data. A core issue when dealing with time series is determining their pairwise similarity, i.e., the degree to which a given time series resembles another. Traditional distance measures such as the Euclidean are not well-suited due to the time-dependent nature of the data. Elastic metrics such as dynamic time warping (DTW) offer a promising approach, but are limited by their computational complexity, non-differentiability and sensitivity to noise and outliers. This thesis proposes novel elastic alignment methods that use parametric \& diffeomorphic warping transformations as a means of overcoming the shortcomings of DTW-based metrics. The proposed method is differentiable \& invertible, well-suited for deep learning architectures, robust to noise and outliers, computationally efficient, and is expressive and flexible enough to capture complex patterns. Furthermore, a closed-form solution was developed for the gradient of these diffeomorphic transformations, which allows an efficient search in the parameter space, leading to better solutions at convergence. Leveraging the benefits of these closed-form diffeomorphic transformations, this thesis proposes a suite of advancements that include: (a) an enhanced temporal transformer network for time series alignment and averaging, (b) a deep-learning based time series classification model to simultaneously align and classify signals with high accuracy, (c) an incremental time series clustering algorithm that is warping-invariant, scalable and can operate under limited computational and time resources, and finally, (d) a normalizing flow model that enhances the flexibility of affine transformations in coupling and autoregressive layers.
</details>
<details>
<summary>摘要</summary>
“随着时间数据的普遍和多元化，对时间序列资料的相似性、分类和对应方法已经引起了广泛的关注。时间序列之间的相似度决定是一个核心问题，因为传统的距离度量如欧几何距离（Euclidean distance）不适合时间序列资料。弹性度量如动态时间截弯（DTW）提供了一个有前途的方法，但是它们受到计算复杂度、非断统和干扰和噪音的影响。本论文提出了一些新的弹性对称方法，使用参数和 diffeomorphic 截弯变换来超越 DTW 基础的缺陷。这些方法是可微和可逆的，适合深度学习架构，具有较高的计算效率，并且具有较好的抗干扰和噪音性。此外，这些方法还具有关于参数空间的关注解，可以实现更好的搜索和更高的精度。本论文提出了以下几个提升：（a）改进的时间序列变换网络，用于时间序列Alignment和平均（b）使用深度学习的时间序列分类模型，同时进行时间序列Alignment和分类，（c）可扩展的时间序列集群分析算法，可以在有限的计算和时间资源下进行扩展和可扩展，（d）使用流形变换来增强时间序列的弹性和自适应性。”
</details></li>
</ul>
<hr>
<h2 id="LORD-Low-Rank-Decomposition-Of-Monolingual-Code-LLMs-For-One-Shot-Compression"><a href="#LORD-Low-Rank-Decomposition-Of-Monolingual-Code-LLMs-For-One-Shot-Compression" class="headerlink" title="LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression"></a>LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14021">http://arxiv.org/abs/2309.14021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Kaushal, Tejas Vaidhya, Irina Rish</li>
<li>for: 这篇论文探讨了如何使用低阶分解（LoRD）来压缩大语言模型（LLM），以提高执行速度。</li>
<li>methods: 论文使用了低阶分解（LoRD）方法，将大量的Linear层分解为两个较小的Matrix，以减少模型的参数数量，并且保持了完全可微和所有参数可训练。</li>
<li>results: 论文的实验结果显示，使用LoRD压缩StarCoder 16B模型，可以将其变数数量从16B降至13.2B，而且只需要少于10分钟的时间，且没有Drop的情况下，对于推导速度有22.35%的提升。此外，LoRD模型可以与现有的高效缓存方法进行并行优化，以提高推导速度。<details>
<summary>Abstract</summary>
Low Rank Decomposition of matrix - splitting a large matrix into a product of two smaller matrix offers a means for compression that reduces the parameters of a model without sparsification, and hence delivering more speedup on modern hardware. Moreover, unlike quantization, the compressed linear layers remain fully differentiable and all the parameters trainable, while being able to leverage the existing highly efficient kernels over floating point matrices. We study the potential to compress Large Language Models (LLMs) for monolingual Code generation via Low Rank Decomposition (LoRD) and observe that ranks for the linear layers in these models can be reduced by upto 39.58% with less than 1% increase in perplexity. We then use Low Rank Decomposition (LoRD) to compress StarCoder 16B to 13.2B parameter with no drop and to 12.3B with minimal drop in HumanEval Pass@1 score, in less than 10 minutes on a single A100. The compressed models speeds up inference by up to 22.35% with just a single line of change in code over huggingface's implementation with pytorch backend. Low Rank Decomposition (LoRD) models remain compatible with state of the art near-lossless quantization method such as SpQR, which allows leveraging further compression gains of quantization. Lastly, QLoRA over Low Rank Decomposition (LoRD) model further reduces memory requirements by as much as 21.2% over vanilla QLoRA while offering similar gains from parameter efficient fine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new paradigm for LLM compression.
</details>
<details>
<summary>摘要</summary>
LOW Rank Decomposition of matrix - 将大Matrix split into two smaller matrices 提供了压缩方法，可以减少模型参数而不是简化，从而在现代硬件上提高速度。此外，与量化不同，压缩的线性层保持完全可导和所有参数可训练，同时可以利用浮点数矩阵的高效内核。我们研究了使用LOW Rank Decomposition (LoRD)压缩大型自然语言模型（LLMs），并发现可以将线性层的排名减少到39.58%，并且影响下降小于1%。然后，我们使用LoRD压缩StarCoder 16B 到 13.2B 参数，在单个 A100 上在 less than 10 分钟内完成，而无需Drop的情况下，带有 minimal Drop 的 HumanEval Pass@1 分数。压缩后的模型可以提高推理速度，达到22.35%的提升，只需要在代码中进行单行修改。LoRD 模型与现有的高效 near-lossless 量化方法相容，例如 SpQR，可以进一步减少压缩参数。最后，QLoRA over LoRD 模型可以减少内存需求，达到21.2%的减少，同时保持与参数高效 fine-tuning 的相同减少。我们的工作表明LOW Rank Decomposition (LoRD) 是一种有前途的新方法 для LLM 压缩。
</details></li>
</ul>
<hr>
<h2 id="Morphological-Computing-as-Logic-Underlying-Cognition-in-Human-Animal-and-Intelligent-Machine"><a href="#Morphological-Computing-as-Logic-Underlying-Cognition-in-Human-Animal-and-Intelligent-Machine" class="headerlink" title="Morphological Computing as Logic Underlying Cognition in Human, Animal, and Intelligent Machine"></a>Morphological Computing as Logic Underlying Cognition in Human, Animal, and Intelligent Machine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13979">http://arxiv.org/abs/2309.13979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gordana Dodig-Crnkovic</li>
<li>for: 本文探讨了自然主义传统下的逻辑、 epistemology 和科学之间的关系。</li>
<li>methods: 文章提出了一种连接逻辑、数学、物理、化学、生物和认知的方案，强调自然 proceses 中的缩减不变的、自组织的动力学。</li>
<li>results: 文章表明了生物体的逻辑存在于自然过程中，并且 humans, animals 和 artifactual agents 都具有内在的逻辑。人类中心的、基于自然语言的逻辑是生物体演化出来的复杂逻辑的 simplest form。因此, cognitive 逻辑来自物理、化学和生物逻辑的演化。在一个自组织的计算框架中，可以使用基于形态&#x2F;物理&#x2F;自然计算的创新计算框架来解释人类中心的逻辑的起源。extend Evolutionary Synthesis 是理解人类逻辑的起源和逻辑与信息处理&#x2F;计算 epistemology 之间的关系的关键。<details>
<summary>Abstract</summary>
This work examines the interconnections between logic, epistemology, and sciences within the Naturalist tradition. It presents a scheme that connects logic, mathematics, physics, chemistry, biology, and cognition, emphasizing scale-invariant, self-organizing dynamics across organizational tiers of nature. The inherent logic of agency exists in natural processes at various levels, under information exchanges. It applies to humans, animals, and artifactual agents. The common human-centric, natural language-based logic is an example of complex logic evolved by living organisms that already appears in the simplest form at the level of basal cognition of unicellular organisms. Thus, cognitive logic stems from the evolution of physical, chemical, and biological logic. In a computing nature framework with a self-organizing agency, innovative computational frameworks grounded in morphological/physical/natural computation can be used to explain the genesis of human-centered logic through the steps of naturalized logical processes at lower levels of organization. The Extended Evolutionary Synthesis of living agents is essential for understanding the emergence of human-level logic and the relationship between logic and information processing/computational epistemology. We conclude that more research is needed to elucidate the details of the mechanisms linking natural phenomena with the logic of agency in nature.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Detecting-Sexual-Content-at-the-Sentence-Level-in-First-Millennium-Latin-Texts"><a href="#Detecting-Sexual-Content-at-the-Sentence-Level-in-First-Millennium-Latin-Texts" class="headerlink" title="Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts"></a>Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14974">http://arxiv.org/abs/2309.14974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lascivaroma/seligator">https://github.com/lascivaroma/seligator</a></li>
<li>paper_authors: Thibault Clérice</li>
<li>for: 这个研究旨在使用深度学习方法对句子水平进行Semantic classification，以加速人文学科和语言学科中 tradicional和时间consuming的Corpus建设。</li>
<li>methods: 我们引入了一个新的Corpus，包括约2500句文本，从300BCE到900CE，涵盖性 semantics（医学、 эротика等）。我们评估了各种句子分类方法和不同的输入嵌入层，并显示它们都能够超越简单的符号based搜索。</li>
<li>results: 我们的结果表明，这种方法有效，具有高精度和真正的正确率（TPR），分别为70.60%和86.33% using HAN。我们也评估了数据集大小对模型性能的影响（420个 вместо 2013），并显示，虽然我们的模型性能下降，但仍然可以提供高准确率和TPR，甚至无需MLM。<details>
<summary>Abstract</summary>
In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models perform worse, they still offer a high enough precision and TPR, even without MLM, respectively 69% and 51%. Given the result, we provide an analysis of the attention mechanism as a supporting added value for humanists in order to produce more data.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提议使用深度学习方法进行含义分类，以加速人文科学和语言学领域的 корпу文建设，这是传统的时间消耗性任务。我们介绍了一个新的词库，包含约2500个句子，从300年前至900年前，涵盖性 semantics（医学、 эротиче、等）。我们评估了不同句子分类方法和输入嵌入层，发现它们都能够持续性地超越简单的token-based搜索。我们探索了idiololectal和sociolectic metadata嵌入（世纪、作者、类型的写作）的集成，但发现它会导致过拟合。我们的结果表明这种方法的有效性，卷积率分别为70.60%和86.33%使用HAN。我们评估了数据集大小对模型性能的影响（420个 вместо2013），发现，虽然我们的模型表现不佳，但它们仍然可以提供高准确率和TPR，甚至没有MLM，分别为69%和51%。 giventhe result，我们提供了关注机制的分析，作为支持的加值，以便人文科学家生产更多数据。
</details></li>
</ul>
<hr>
<h2 id="Audio-classification-with-Dilated-Convolution-with-Learnable-Spacings"><a href="#Audio-classification-with-Dilated-Convolution-with-Learnable-Spacings" class="headerlink" title="Audio classification with Dilated Convolution with Learnable Spacings"></a>Audio classification with Dilated Convolution with Learnable Spacings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13972">http://arxiv.org/abs/2309.13972</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k-h-ismail/dcls-audio">https://github.com/k-h-ismail/dcls-audio</a></li>
<li>paper_authors: Ismail Khalfaoui-Hassani, Timothée Masquelier, Thomas Pellegrini</li>
<li>for: 这个论文是关于音频标注的研究，使用了增宽 convolution 方法来提高音频分类的准确率。</li>
<li>methods: 这个论文使用了 learnable spacings 的增宽 convolution 方法（DCLS），将 DSC 层替换为 DCLS 层，以提高 AudioSet 分类 benchmark 的准确率。</li>
<li>results: 研究发现，使用 DCLS 方法可以在不增加参数数量和只增加低成本的情况下，提高音频分类的准确率。<details>
<summary>Abstract</summary>
Dilated convolution with learnable spacings (DCLS) is a recent convolution method in which the positions of the kernel elements are learned throughout training by backpropagation. Its interest has recently been demonstrated in computer vision (ImageNet classification and downstream tasks). Here we show that DCLS is also useful for audio tagging using the AudioSet classification benchmark. We took two state-of-the-art convolutional architectures using depthwise separable convolutions (DSC), ConvNeXt and ConvFormer, and a hybrid one using attention in addition, FastViT, and drop-in replaced all the DSC layers by DCLS ones. This significantly improved the mean average precision (mAP) with the three architectures without increasing the number of parameters and with only a low cost on the throughput. The method code is based on PyTorch and is available at https://github.com/K-H-Ismail/DCLS-Audio
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>最近的扩展 convolution 方法之一是learned spacings dilated convolution (DCLS)，它在训练过程中通过反传播学习kernel元素的位置。在计算机视觉中（ImageNet分类和下游任务），DCLS的利用得到了广泛的关注。在这篇文章中，我们表明DCLS也是有用的 для音频标注，我们使用了两个现代 convolutional 架构（DSC），ConvNeXt和ConvFormer，以及一个hybrid架构使用注意力，FastViT，并将所有DSC层换为DCLS层。这会显著提高mAP值，而无需增加参数数量和只增加低成本的通过put Throughput。代码基于PyTorch，可在https://github.com/K-H-Ismail/DCLS-Audio 上下载。
</details></li>
</ul>
<hr>
<h2 id="An-AI-Chatbot-for-Explaining-Deep-Reinforcement-Learning-Decisions-of-Service-oriented-Systems"><a href="#An-AI-Chatbot-for-Explaining-Deep-Reinforcement-Learning-Decisions-of-Service-oriented-Systems" class="headerlink" title="An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems"></a>An AI Chatbot for Explaining Deep Reinforcement Learning Decisions of Service-oriented Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14391">http://arxiv.org/abs/2309.14391</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/xrl2/chat4xai">https://gitlab.com/xrl2/chat4xai</a></li>
<li>paper_authors: Andreas Metzger, Jone Bartel, Jan Laufer</li>
<li>for: 本研究旨在帮助服务开发人员、服务提供者和服务用户更好地理解深度强化学习（Deep Reinforcement Learning，简称Deep RL）的决策过程，以便在服务系统中应用Deep RL。</li>
<li>methods: 本研究使用了现代人工智能对话系统技术和专门的提问工程来实现自然语言解释。相比于传统的软件基于对话系统，使用AI对话系统可以消除需要抽象出问题和答案的过程。</li>
<li>results: 本研究通过使用OpenAI的ChatGPT API实现了Chat4XAI，并评估了其解释的准确性和稳定性，结果表明，使用自然语言解释可以提高服务开发人员、服务提供者和服务用户对Deep RL决策过程的理解，并且可以提高服务用户对服务的信任和接受度。<details>
<summary>Abstract</summary>
Deep Reinforcement Learning (Deep RL) is increasingly used to cope with the open-world assumption in service-oriented systems. Deep RL was successfully applied to problems such as dynamic service composition, job scheduling, and offloading, as well as service adaptation. While Deep RL offers many benefits, understanding the decision-making of Deep RL is challenging because its learned decision-making policy essentially appears as a black box. Yet, understanding the decision-making of Deep RL is key to help service developers perform debugging, support service providers to comply with relevant legal frameworks, and facilitate service users to build trust. We introduce Chat4XAI to facilitate the understanding of the decision-making of Deep RL by providing natural-language explanations. Compared with visual explanations, the reported benefits of natural-language explanations include better understandability for non-technical users, increased user acceptance and trust, as well as more efficient explanations. Chat4XAI leverages modern AI chatbot technology and dedicated prompt engineering. Compared to earlier work on natural-language explanations using classical software-based dialogue systems, using an AI chatbot eliminates the need for eliciting and defining potential questions and answers up-front. We prototypically realize Chat4XAI using OpenAI's ChatGPT API and evaluate the fidelity and stability of its explanations using an adaptive service exemplar.
</details>
<details>
<summary>摘要</summary>
深度强化学习（深度RL）在服务 ориентирован系统中得到广泛应用，以应对开放世界假设。深度RL在动态服务组合、作业调度和下载等问题上取得了成功，同时也应用于服务适应性。然而，深度RL的决策过程理解具有挑战，因为它的学习决策策略看起来就像黑盒子。然而，理解深度RL的决策过程是关键，以帮助服务开发人员进行调试、支持服务提供者遵守相关法规，并促进服务用户建立信任。我们介绍了 Chat4XAI，用于促进深度RL 决策过程的理解，提供自然语言解释。与视觉解释相比，报告的优点包括更好的可读性 для非技术用户、更高的用户接受度和信任度，以及更高效的解释。 Chat4XAI 利用现代人工智能聊天机器人技术和专门的推荐工程。与先前的классиical软件基础的对话系统相比，使用 AI 聊天机器人解除了需要提取和定义 potential questions and answers 的需求。我们使用 OpenAI 的 ChatGPT API 实现 Chat4XAI，并评估其解释的准确性和稳定性使用适应服务示例。
</details></li>
</ul>
<hr>
<h2 id="May-I-Ask-a-Follow-up-Question-Understanding-the-Benefits-of-Conversations-in-Neural-Network-Explainability"><a href="#May-I-Ask-a-Follow-up-Question-Understanding-the-Benefits-of-Conversations-in-Neural-Network-Explainability" class="headerlink" title="May I Ask a Follow-up Question? Understanding the Benefits of Conversations in Neural Network Explainability"></a>May I Ask a Follow-up Question? Understanding the Benefits of Conversations in Neural Network Explainability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13965">http://arxiv.org/abs/2309.13965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Zhang, X. Jessie Yang, Boyang Li</li>
<li>for: 提高用户理解和信任AI模型的决策过程</li>
<li>methods: 使用自由形态对话提高用户理解和信任</li>
<li>results: 对话可以提高用户的理解、acceptance和信任，并促进人机合作<details>
<summary>Abstract</summary>
Research in explainable AI (XAI) aims to provide insights into the decision-making process of opaque AI models. To date, most XAI methods offer one-off and static explanations, which cannot cater to the diverse backgrounds and understanding levels of users. With this paper, we investigate if free-form conversations can enhance users' comprehension of static explanations, improve acceptance and trust in the explanation methods, and facilitate human-AI collaboration. Participants are presented with static explanations, followed by a conversation with a human expert regarding the explanations. We measure the effect of the conversation on participants' ability to choose, from three machine learning models, the most accurate one based on explanations and their self-reported comprehension, acceptance, and trust. Empirical results show that conversations significantly improve comprehension, acceptance, trust, and collaboration. Our findings highlight the importance of customized model explanations in the format of free-form conversations and provide insights for the future design of conversational explanations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Early-Churn-Prediction-from-Large-Scale-User-Product-Interaction-Time-Series"><a href="#Early-Churn-Prediction-from-Large-Scale-User-Product-Interaction-Time-Series" class="headerlink" title="Early Churn Prediction from Large Scale User-Product Interaction Time Series"></a>Early Churn Prediction from Large Scale User-Product Interaction Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14390">http://arxiv.org/abs/2309.14390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamik Bhattacharjee, Utkarsh Thukral, Nilesh Patil</li>
<li>For: The paper aims to predict user churn in business-to-customer scenarios, with a focus on fantasy sports, and to provide insights for businesses to formulate effective retention plans.* Methods: The paper uses historical data and combines user activity with deep neural networks for multivariate time series classification, demonstrating remarkable results for churn prediction in complex contexts.* Results: The paper achieves high accuracy in predicting customer churn likelihood, providing valuable insights for businesses to understand attrition trends and develop effective retention strategies.Here’s the simplified Chinese text for the three information points:* For: 这篇论文目标是预测商业到客户场景中的用户弃用，特别是在幻想体育中，以便为企业提供有价值的归属趋势和退休计划。* Methods: 该论文使用历史数据，将用户活动与深度神经网络结合，实现多变量时间序列分类，在复杂的商业到客户场景中达到了Remarkable的弃用预测结果。* Results: 该论文在预测客户弃用可能性方面实现了高精度，为企业提供有价值的归属趋势和退休计划。<details>
<summary>Abstract</summary>
User churn, characterized by customers ending their relationship with a business, has profound economic consequences across various Business-to-Customer scenarios. For numerous system-to-user actions, such as promotional discounts and retention campaigns, predicting potential churners stands as a primary objective. In volatile sectors like fantasy sports, unpredictable factors such as international sports events can influence even regular spending habits. Consequently, while transaction history and user-product interaction are valuable in predicting churn, they demand deep domain knowledge and intricate feature engineering. Additionally, feature development for churn prediction systems can be resource-intensive, particularly in production settings serving 200m+ users, where inference pipelines largely focus on feature engineering. This paper conducts an exhaustive study on predicting user churn using historical data. We aim to create a model forecasting customer churn likelihood, facilitating businesses in comprehending attrition trends and formulating effective retention plans. Our approach treats churn prediction as multivariate time series classification, demonstrating that combining user activity and deep neural networks yields remarkable results for churn prediction in complex business-to-customer contexts.
</details>
<details>
<summary>摘要</summary>
用户卷退，指客户与企业结束业务关系，对各种商业到客户场景产生深刻的经济影响。在多种系统到用户行为中，预测可能卷退者为primary objective。在投机领域如虚拟运动，国际运动赛事的不可预测因素可能对常规支付习惯产生影响。因此，对卷退预测系统的特征工程可能会占用资源，特别是在服务2000万用户以上的生产环境中，where inference pipelines largely focus on feature engineering。本文通过对历史数据进行广泛的研究，旨在创建一个预测用户卷退可能性的模型，帮助企业理解卷退趋势并制定有效的保留计划。我们的方法将卷退预测视为多变量时间系列分类，示出将用户活动和深度神经网络结合可以在复杂的商业到客户场景中实现remarkable的卷退预测结果。
</details></li>
</ul>
<hr>
<h2 id="VidChapters-7M-Video-Chapters-at-Scale"><a href="#VidChapters-7M-Video-Chapters-at-Scale" class="headerlink" title="VidChapters-7M: Video Chapters at Scale"></a>VidChapters-7M: Video Chapters at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13952">http://arxiv.org/abs/2309.13952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antoyang/VidChapters">https://github.com/antoyang/VidChapters</a></li>
<li>paper_authors: Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, Cordelia Schmid</li>
<li>for: 该论文旨在提供一个大规模的视频分章数据集，以便进行视频分章任务的研究。</li>
<li>methods: 该论文使用了自动抓取视频网站上的用户标注的分章信息，自动生成了817万个视频和7万个分章的数据集。</li>
<li>results: 该论文通过对这些数据进行分析，实现了三个任务：视频分章生成、视频分章grounding和 dense video captioning。 Results show that pretraining on VidChapters-7M transfers well to dense video captioning tasks, largely improving the state of the art on the YouCook2 and ViTT benchmarks.<details>
<summary>Abstract</summary>
Segmenting long videos into chapters enables users to quickly navigate to the information of their interest. This important topic has been understudied due to the lack of publicly released datasets. To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total. VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation. We introduce the following three tasks based on this data. First, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment. To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title. We benchmark both simple baselines and state-of-the-art video-language models for these three tasks. We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the YouCook2 and ViTT benchmarks. Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset. Our dataset, code, and models are publicly available at https://antoyang.github.io/vidchapters.html.
</details>
<details>
<summary>摘要</summary>
“将长片 видео分成章节可以让用户快速导航到他们所需的信息。这个重要主题一直未被充分研究，原因是公共释出的数据缺乏。为解决这个问题，我们提出了 VidChapters-7M  dataset，包含 817 万个用户分成的影片和 7 百万个章节。 VidChapters-7M 是通过自动抓取网络上的影片而实现的，无需任何额外的手动标注。我们提出了以下三个任务：影片章节生成任务，包括时间段分影片和生成每个段落的章节标题；以及两个这个任务的变化：影片章节生成基于预设边界，需要根据预设的影片段落标注生成章节标题，以及影片章节固定，需要根据章节标题进行时间位置local化。我们在这些三个任务上评估了基本的基础模型和现有的影词组言模型，并证明了这些模型在零shot和调整设定下具有优秀的表现。此外，我们的实验显示，下游性能与预训练数据的大小成正比。我们的 dataset、代码和模型都可以在 <https://antoyang.github.io/vidchapters.html> 获取。”
</details></li>
</ul>
<hr>
<h2 id="The-Time-Traveler’s-Guide-to-Semantic-Web-Research-Analyzing-Fictitious-Research-Themes-in-the-ESWC-“Next-20-Years”-Track"><a href="#The-Time-Traveler’s-Guide-to-Semantic-Web-Research-Analyzing-Fictitious-Research-Themes-in-the-ESWC-“Next-20-Years”-Track" class="headerlink" title="The Time Traveler’s Guide to Semantic Web Research: Analyzing Fictitious Research Themes in the ESWC “Next 20 Years” Track"></a>The Time Traveler’s Guide to Semantic Web Research: Analyzing Fictitious Research Themes in the ESWC “Next 20 Years” Track</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13939">http://arxiv.org/abs/2309.13939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irene Celino, Heiko Paulheim</li>
<li>for: The paper is written to explore the future research directions and themes of the Semantic Web community in the late 2040s and early 2050s.</li>
<li>methods: The paper uses fictitious research papers as a way to gather ideas from the community on potential future research themes and topics, and analyzes the research methods applied by the authors in these submissions.</li>
<li>results: The paper provides a survey of the “science fiction” papers submitted to the “Next 20 years” track of ESWC 2023, including the emerging research themes and topics, and investigates the most fictitious parts of the submissions.<details>
<summary>Abstract</summary>
What will Semantic Web research focus on in 20 years from now? We asked this question to the community and collected their visions in the "Next 20 years" track of ESWC 2023. We challenged the participants to submit "future" research papers, as if they were submitting to the 2043 edition of the conference. The submissions - entirely fictitious - were expected to be full scientific papers, with research questions, state of the art references, experimental results and future work, with the goal to get an idea of the research agenda for the late 2040s and early 2050s. We received ten submissions, eight of which were accepted for presentation at the conference, that mixed serious ideas of potential future research themes and discussion topics with some fun and irony.   In this paper, we intend to provide a survey of those "science fiction" papers, considering the emerging research themes and topics, analysing the research methods applied by the authors in these very special submissions, and investigating also the most fictitious parts (e.g., neologisms, fabricated references). Our goal is twofold: on the one hand, we investigate what this special track tells us about the Semantic Web community and, on the other hand, we aim at getting some insights on future research practices and directions.
</details>
<details>
<summary>摘要</summary>
在未来20年，semantic web研究将集中焦点在什么？我们问了社区，收集了他们的见解在“未来20年”track of ESWC 2023中。我们邀请 particiants to submitting“未来”研究论文，如果他们是在2043年版本的会议上提交的。提交的“未来”论文应包括研究问题、现场研究、实验结果和未来工作，以获得2040年代和2050年代的研究训练。我们收到了10篇提交，8篇被接受到会议上，其中有一些具有可能性的未来研究主题和讨论topic。在这篇文章中，我们将对这10篇“科幻”论文进行调查，探讨这些论文中的emerging research theme和topic，分析作者所应用的研究方法，以及一些虚构的部分（例如， neologisms和 fabricated references）。我们的目标是twofold：一方面，我们想要了解这个特别track的semantic web社区，另一方面，我们希望透过这些未来研究方法和方向获得一些预见。
</details></li>
</ul>
<hr>
<h2 id="SPOTS-Stable-Placement-of-Objects-with-Reasoning-in-Semi-Autonomous-Teleoperation-Systems"><a href="#SPOTS-Stable-Placement-of-Objects-with-Reasoning-in-Semi-Autonomous-Teleoperation-Systems" class="headerlink" title="SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems"></a>SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13937">http://arxiv.org/abs/2309.13937</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joonhyung-lee/spots">https://github.com/joonhyung-lee/spots</a></li>
<li>paper_authors: Joonhyung Lee, Sangbeom Park, Jeongeun Park, Kyungjae Lee, Sungjoon Choi</li>
<li>for: 本研究主要针对pick-and-place任务中的“place”任务，即在人工智能框架下将物品放置在合适的位置。</li>
<li>methods: 本研究提出一种结合实验驱动的物理稳定验证和大语言模型的 semantic reasoning 能力，以生成基于Contextual reasonableness和物理稳定性的物品放置候选者概率分布。</li>
<li>results: 对于两个实验环境和一个实际世界环境进行了广泛的评估，表明OUR方法可以大幅提高物品放置的物理可行性和上下文合理性，同时考虑用户首选。<details>
<summary>Abstract</summary>
Pick-and-place is one of the fundamental tasks in robotics research. However, the attention has been mostly focused on the ``pick'' task, leaving the ``place'' task relatively unexplored. In this paper, we address the problem of placing objects in the context of a teleoperation framework. Particularly, we focus on two aspects of the place task: stability robustness and contextual reasonableness of object placements. Our proposed method combines simulation-driven physical stability verification via real-to-sim and the semantic reasoning capability of large language models. In other words, given place context information (e.g., user preferences, object to place, and current scene information), our proposed method outputs a probability distribution over the possible placement candidates, considering the robustness and reasonableness of the place task. Our proposed method is extensively evaluated in two simulation and one real world environments and we show that our method can greatly increase the physical plausibility of the placement as well as contextual soundness while considering user preferences.
</details>
<details>
<summary>摘要</summary>
Pick-and-place 是 robotics 研究中的基本任务之一，但是它们的注意力主要集中在“捕获”任务上，剩下的“放置”任务则得到了更少的关注。在这篇论文中，我们对置物任务进行了研究，特别是在电子操作框架中。我们关注了放置物品的两个方面：稳定性和上下文合理性。我们提出的方法结合了实际驱动的物理稳定性验证和大语言模型的Semantic reasoning能力。具体来说，我们根据放置上下文信息（例如用户偏好、要放置的物品和当前场景信息）输出一个可能的放置候选者概率分布，考虑放置任务的稳定性和上下文合理性。我们的方法在三个 simulations 和一个真实世界环境中进行了广泛的评估，并显示了我们的方法可以大幅提高物理可能性以及上下文合理性。
</details></li>
</ul>
<hr>
<h2 id="Fairness-and-Bias-in-Algorithmic-Hiring"><a href="#Fairness-and-Bias-in-Algorithmic-Hiring" class="headerlink" title="Fairness and Bias in Algorithmic Hiring"></a>Fairness and Bias in Algorithmic Hiring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13933">http://arxiv.org/abs/2309.13933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Alessandro Fabris, Nina Baranowska, Matthew J. Dennis, Philipp Hacker, Jorge Saldivar, Frederik Zuiderveen Borgesius, Asia J. Biega</li>
<li>for: 这篇论文是为了探讨算法招聘技术在招聘过程中的应用和公平性问题。</li>
<li>methods: 本论文使用了多学科的方法，包括系统评估、偏见检测、数据分析等，以探讨算法招聘技术的优劣和应用场景。</li>
<li>results: 本论文结果表明，算法招聘技术可以减少招聘过程中的偏见和不公平，但是现有的数据和方法有限制，需要进一步的研究和开发以确保这些技术的公平性和可靠性。<details>
<summary>Abstract</summary>
Employers are adopting algorithmic hiring technology throughout the recruitment pipeline. Algorithmic fairness is especially applicable in this domain due to its high stakes and structural inequalities. Unfortunately, most work in this space provides partial treatment, often constrained by two competing narratives, optimistically focused on replacing biased recruiter decisions or pessimistically pointing to the automation of discrimination. Whether, and more importantly what types of, algorithmic hiring can be less biased and more beneficial to society than low-tech alternatives currently remains unanswered, to the detriment of trustworthiness. This multidisciplinary survey caters to practitioners and researchers with a balanced and integrated coverage of systems, biases, measures, mitigation strategies, datasets, and legal aspects of algorithmic hiring and fairness. Our work supports a contextualized understanding and governance of this technology by highlighting current opportunities and limitations, providing recommendations for future work to ensure shared benefits for all stakeholders.
</details>
<details>
<summary>摘要</summary>
雇主正在整个招聘过程中广泛采用算法招聘技术。算法公平特别适用于这个领域，因为它具有高的重要性和结构性不平等。然而，大多数工作在这个领域都提供了半路处理，经常受到两种竞争的观点所限制：一是乐观地关注代表人员偏见的替换，另一是悲观地指出自动化歧视。无论算法招聘能否更不偏袋更有利于社会，以及哪种类型的算法招聘可以更加不偏袋，目前仍未得到答案。这个多学科调查旨在为实践者和研究人员提供一个平衡和一致的涵盖系统、偏见、测量、缓减策略、数据集和法律方面的算法招聘公平问题的全面覆盖。我们的工作旨在支持Contextualized理解和管理这种技术，通过强调当前的机会和限制，提供未来工作的建议，以确保所有参与者共享利益。
</details></li>
</ul>
<hr>
<h2 id="UCF-Crime-Annotation-A-Benchmark-for-Surveillance-Video-and-Language-Understanding"><a href="#UCF-Crime-Annotation-A-Benchmark-for-Surveillance-Video-and-Language-Understanding" class="headerlink" title="UCF-Crime Annotation: A Benchmark for Surveillance Video-and-Language Understanding"></a>UCF-Crime Annotation: A Benchmark for Surveillance Video-and-Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13925">http://arxiv.org/abs/2309.13925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuange923/uca-dataset">https://github.com/xuange923/uca-dataset</a></li>
<li>paper_authors: Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Jian Jin, Zhenzhen Jiao</li>
<li>for: 本研究旨在提供一个新的多模态Surveillance视频数据集，以便进行多模态Surveillance视频分析。</li>
<li>methods: 我们使用了手动标注的实际世界Surveillance视频数据集UCF-Crime，并将其注解为细腻事件内容和时间。我们的新创建的数据集UCA（UCF-Crime Annotation）提供了一个新的benchmark для多模态Surveillance视频分析。</li>
<li>results: 我们在这个新创建的数据集上测试了当前主流的多模态任务模型，发现这些模型在多模态Surveillance视频场景下表现不佳，这 highlights the necessity of constructing this dataset。<details>
<summary>Abstract</summary>
Surveillance videos are an essential component of daily life with various critical applications, particularly in public security. However, current surveillance video tasks mainly focus on classifying and localizing anomalous events. Existing methods are limited to detecting and classifying the predefined events with unsatisfactory generalization ability and semantic understanding, although they have obtained considerable performance. To address this issue, we propose constructing the first multimodal surveillance video dataset by manually annotating the real-world surveillance dataset UCF-Crime with fine-grained event content and timing. Our newly annotated dataset, UCA (UCF-Crime Annotation), provides a novel benchmark for multimodal surveillance video analysis. It not only describes events in detailed descriptions but also provides precise temporal grounding of the events in 0.1-second intervals. UCA contains 20,822 sentences, with an average length of 23 words, and its annotated videos are as long as 102 hours. Furthermore, we benchmark the state-of-the-art models of multiple multimodal tasks on this newly created dataset, including temporal sentence grounding in videos, video captioning, and dense video captioning. Through our experiments, we found that mainstream models used in previously publicly available datasets perform poorly on multimodal surveillance video scenarios, which highlights the necessity of constructing this dataset. The link to our dataset and code is provided at: https://github.com/Xuange923/UCA-dataset.
</details>
<details>
<summary>摘要</summary>
侦查视频是我们日常生活中的一个重要组成部分，尤其在公共安全领域。然而，现有的侦查视频任务主要集中在异常事件的分类和地点化。现有的方法具有不满足的泛化能力和 semantics理解，尽管它们在性能方面已经取得了一定的进步。为解决这个问题，我们提议创建了首个多模态侦查视频数据集，通过手动标注实际世界的侦查视频数据集UCF-Crime，并将其注解为细化事件内容和时间。我们新创建的数据集，UCAC（UCF-Crime Annotation），不仅描述事件的内容，还提供精确的时间地标，每个事件在0.1秒间隔内进行标注。UCAC包含20822句话，平均长度为23个单词，其注解视频的长度为102小时。此外，我们在这个新创建的数据集上测试了当今主流的多模态任务模型，包括视频句子注释、视频句子注释和稠密视频句子注释。我们的实验结果显示，主流在多模态侦查视频场景下表现糟糕，这 highlights 了我们构建这个数据集的必要性。数据集和代码的链接可以在 GitHub 上找到：https://github.com/Xuange923/UCA-dataset。
</details></li>
</ul>
<hr>
<h2 id="A-comparison-of-controller-architectures-and-learning-mechanisms-for-arbitrary-robot-morphologies"><a href="#A-comparison-of-controller-architectures-and-learning-mechanisms-for-arbitrary-robot-morphologies" class="headerlink" title="A comparison of controller architectures and learning mechanisms for arbitrary robot morphologies"></a>A comparison of controller architectures and learning mechanisms for arbitrary robot morphologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13908">http://arxiv.org/abs/2309.13908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Luo, Jakub Tomczak, Karine Miras, Agoston E. Eiben</li>
<li>for: 这个论文的主要问题是：如果机器人的形态不知道先天，那么哪种控制器和学习方法应该使用？作者们的兴趣是基于模块化演化的机器人，但问题也适用于广泛的系统设计者，寻找可重用的解决方案。</li>
<li>methods: 作者们使用了三种控制器和学习方法的组合：一种基于动物 lokomootion 模型（中央 Pattern Generators，CPG）和一个进化算法学习者，另一种使用强化学习（RL）和一个神经网络控制器架构，以及一种 combining 的方法，其中控制器是神经网络，学习者是进化算法。</li>
<li>results: 作者们对一组模块化机器人进行了测试，并对三种组合的有效性、效率和稳定性进行了比较。结果显示，通常的 CPG 和 RL 方法被外围的 combining 组合所超越，这个组合更加稳定和高效。<details>
<summary>Abstract</summary>
The main question this paper addresses is: What combination of a robot controller and a learning method should be used, if the morphology of the learning robot is not known in advance? Our interest is rooted in the context of morphologically evolving modular robots, but the question is also relevant in general, for system designers interested in widely applicable solutions. We perform an experimental comparison of three controller-and-learner combinations: one approach where controllers are based on modelling animal locomotion (Central Pattern Generators, CPG) and the learner is an evolutionary algorithm, a completely different method using Reinforcement Learning (RL) with a neural network controller architecture, and a combination `in-between' where controllers are neural networks and the learner is an evolutionary algorithm. We apply these three combinations to a test suite of modular robots and compare their efficacy, efficiency, and robustness. Surprisingly, the usual CPG-based and RL-based options are outperformed by the in-between combination that is more robust and efficient than the other two setups.
</details>
<details>
<summary>摘要</summary>
本文探讨的主要问题是：在不知道机器人形态的情况下，应用哪种控制器和学习方法？我们的兴趣基于模块化 robots 的形态演化，但这个问题也适用于更广泛的系统设计师，寻找通用的解决方案。我们通过实验比较三种控制器和学习方法的组合：一种使用动物步态模型（中央 Pattern Generators，CPG）和进化算法学习者，一种完全不同的方法使用奖励学习（RL）和神经网络控制器架构，以及一种混合“中间”的方法，其中控制器是神经网络，学习者是进化算法。我们将这三种组合应用到一组模块 robots 上，并比较其效果、效率和稳定性。结果各种意外地发现，通常的 CPG-based 和 RL-based 选项被“中间”组合所超越，这种组合更加稳定和高效。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-the-Efficacy-of-an-LLM-Only-Approach-for-Image-based-Document-Question-Answering"><a href="#Analyzing-the-Efficacy-of-an-LLM-Only-Approach-for-Image-based-Document-Question-Answering" class="headerlink" title="Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering"></a>Analyzing the Efficacy of an LLM-Only Approach for Image-based Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14389">http://arxiv.org/abs/2309.14389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nidhi Hegde, Sujoy Paul, Gagan Madan, Gaurav Aggarwal</li>
<li>for: 这个论文的目的是研究文档问答模型中的两个关键组件：视觉编码器和大型自然语言模型（LLM），以及这两个组件之间的相对贡献。</li>
<li>methods: 这篇论文使用了一种叫做“LLM-只”的方法，即直接将文档图像中的文本信息序列化并传递给一个受训练的 LLM，以便不需要显式的视觉编码器。</li>
<li>results: 论文的结果表明，使用这种“LLM-只”方法可以在多种 datasets 上达到与或接近领先性水平的表现。<details>
<summary>Abstract</summary>
Recent document question answering models consist of two key components: the vision encoder, which captures layout and visual elements in images, and a Large Language Model (LLM) that helps contextualize questions to the image and supplements them with external world knowledge to generate accurate answers. However, the relative contributions of the vision encoder and the language model in these tasks remain unclear. This is especially interesting given the effectiveness of instruction-tuned LLMs, which exhibit remarkable adaptability to new tasks. To this end, we explore the following aspects in this work: (1) The efficacy of an LLM-only approach on document question answering tasks (2) strategies for serializing textual information within document images and feeding it directly to an instruction-tuned LLM, thus bypassing the need for an explicit vision encoder (3) thorough quantitative analysis on the feasibility of such an approach. Our comprehensive analysis encompasses six diverse benchmark datasets, utilizing LLMs of varying scales. Our findings reveal that a strategy exclusively reliant on the LLM yields results that are on par with or closely approach state-of-the-art performance across a range of datasets. We posit that this evaluation framework will serve as a guiding resource for selecting appropriate datasets for future research endeavors that emphasize the fundamental importance of layout and image content information.
</details>
<details>
<summary>摘要</summary>
现代文档问答模型通常包括两个关键组件：视觉编码器，用于捕捉图像中的布局和视觉元素，以及一个大语言模型（LLM），用于将问题与图像相关联并补充问题以外的知识来生成准确的答案。然而，视觉编码器和语言模型在这些任务中的相对贡献还不清楚。这 especially interesting， given the effectiveness of instruction-tuned LLMs，which exhibit remarkable adaptability to new tasks。为此，我们在这项工作中进行了以下三个方面的研究：1. 使用LLM-only方法解决文档问答任务的效果（2）， Serializing textual information within document images and feeding it directly to an instruction-tuned LLM, thus bypassing the need for an explicit vision encoder。我们的全面分析涵盖了六个多样化的 benchmarck 数据集，使用不同规模的LLM。我们的发现表明，一种仅依靠LLM的方法可以在多个数据集上达到或接近状态艺术性的表现。我们认为这种评价框架将成为未来研究着重于图像布局和内容信息的关键资源。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Robot-Morphology-Spaces-through-Breadth-First-Search-and-Random-Query"><a href="#Exploring-Robot-Morphology-Spaces-through-Breadth-First-Search-and-Random-Query" class="headerlink" title="Exploring Robot Morphology Spaces through Breadth-First Search and Random Query"></a>Exploring Robot Morphology Spaces through Breadth-First Search and Random Query</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14387">http://arxiv.org/abs/2309.14387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Luo</li>
<li>for: 这项研究旨在 Investigating the role of query mechanisms in the brain-body co-evolution of modular robots, and comparing the effectiveness of two different query mechanisms (BFS and Random Query) in evolving robot morphologies.</li>
<li>methods: 该研究使用了 CPPNs and robot controllers using tensors,以及两种不同的查询机制（BFS和随机查询），在两种演化框架（LAMARCK和达尔沃尼系统）中进行了对比性分析。</li>
<li>results: 研究发现，BFS 比 Random Query 更有效率地生成高性能的机器人体，并且在达尔沃尼系统中，BFS 导致机器人体的演化和性能具有更高的多样性和特征。<details>
<summary>Abstract</summary>
Evolutionary robotics offers a powerful framework for designing and evolving robot morphologies, particularly in the context of modular robots. However, the role of query mechanisms during the genotype-to-phenotype mapping process has been largely overlooked. This research addresses this gap by conducting a comparative analysis of query mechanisms in the brain-body co-evolution of modular robots. Using two different query mechanisms, Breadth-First Search (BFS) and Random Query, within the context of evolving robot morphologies using CPPNs and robot controllers using tensors, and testing them in two evolutionary frameworks, Lamarckian and Darwinian systems, this study investigates their influence on evolutionary outcomes and performance. The findings demonstrate the impact of the two query mechanisms on the evolution and performance of modular robot bodies, including morphological intelligence, diversity, and morphological traits. This study suggests that BFS is both more effective and efficient in producing highly performing robots. It also reveals that initially, robot diversity was higher with BFS compared to Random Query, but in the Lamarckian system, it declines faster, converging to superior designs, while in the Darwinian system, BFS led to higher end-process diversity.
</details>
<details>
<summary>摘要</summary>
生态进化机器人学提供了一个强大的框架 для设计和演化机器人体形，特别在模块化机器人中。然而，在基因型-到形态映射过程中 Query 机制的角色被大量遗弃。这种研究填补了这个遗弃，通过对 CPPN 和机器人控制器使用矩阵进行演化 robots 的脑体进行比较分析。使用 BFS 和随机 Query 两种不同的 Query 机制，在拉马克思主义和达尔文主义两种演化框架下测试它们，这种研究研究它们对演化结果和性能的影响。发现 BFS 比Random Query 更有效和高效地生成高性能机器人体形，并且发现在拉马克思主义系统中，BFS 初始时 robot 多样性比 Random Query 高，但随着演化，它快速下降，转化为优秀设计，而达尔文主义系统中，BFS 导致最终的多样性高于 Random Query。
</details></li>
</ul>
<hr>
<h2 id="Scene-Informer-Anchor-based-Occlusion-Inference-and-Trajectory-Prediction-in-Partially-Observable-Environments"><a href="#Scene-Informer-Anchor-based-Occlusion-Inference-and-Trajectory-Prediction-in-Partially-Observable-Environments" class="headerlink" title="Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments"></a>Scene Informer: Anchor-based Occlusion Inference and Trajectory Prediction in Partially Observable Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13893">http://arxiv.org/abs/2309.13893</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sisl/sceneinformer">https://github.com/sisl/sceneinformer</a></li>
<li>paper_authors: Bernard Lange, Jiachen Li, Mykel J. Kochenderfer</li>
<li>for: 本研究旨在提高自动驾驶汽车在部分可见环境中的导航能力，包括预测 observable agents 的未来运动和推断 occluded agents。</li>
<li>methods: 我们引入了 Scene Informer，一种统一的方法，可以同时预测 observable agents 的运动和推断 occluded agents。Scene Informer 使用 transformer 来聚合不同输入模式，并提供选择性查询 occlusions 可能与 AV 计划路径相交。</li>
<li>results: 我们的方法在 Waymo Open Motion Dataset 上的部分可见设置下表现出色，超过了现有方法在 occupancy 预测和 trajectory 预测方面。<details>
<summary>Abstract</summary>
Navigating complex and dynamic environments requires autonomous vehicles (AVs) to reason about both visible and occluded regions. This involves predicting the future motion of observed agents, inferring occluded ones, and modeling their interactions based on vectorized scene representations of the partially observable environment. However, prior work on occlusion inference and trajectory prediction have developed in isolation, with the former based on simplified rasterized methods and the latter assuming full environment observability. We introduce the Scene Informer, a unified approach for predicting both observed agent trajectories and inferring occlusions in a partially observable setting. It uses a transformer to aggregate various input modalities and facilitate selective queries on occlusions that might intersect with the AV's planned path. The framework estimates occupancy probabilities and likely trajectories for occlusions, as well as forecast motion for observed agents. We explore common observability assumptions in both domains and their performance impact. Our approach outperforms existing methods in both occupancy prediction and trajectory prediction in partially observable setting on the Waymo Open Motion Dataset.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:自适应环境需要自动驾驶车 (AV) 能够理解可见和遮盖的区域。这些区域包括预测可见的代理人的未来运动、推理遮盖的人和基于可见环境的场景表示的Vectorization。然而，先前的遮盖推断和轨迹预测工作都是分离的，前者基于简化的扫描方法，后者假设环境完全可见。我们介绍了Scene Informer，一种统一的方法，可以预测可见代理人的轨迹和预测遮盖物的存在。它使用 transformer 来聚合不同的输入模式，并且可以根据预测轨迹的可能性进行选择性的查询遮盖物。框架可以估算遮盖物的存在概率和可能的轨迹，以及可见代理人的预测运动。我们探讨了两个域的共同可见假设和其影响性。我们的方法在 partially observable 环境中的 Waymo Open Motion Dataset 上比过去的方法表现出色。
</details></li>
</ul>
<hr>
<h2 id="TouchUp-G-Improving-Feature-Representation-through-Graph-Centric-Finetuning"><a href="#TouchUp-G-Improving-Feature-Representation-through-Graph-Centric-Finetuning" class="headerlink" title="TouchUp-G: Improving Feature Representation through Graph-Centric Finetuning"></a>TouchUp-G: Improving Feature Representation through Graph-Centric Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13885">http://arxiv.org/abs/2309.13885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jing Zhu, Xiang Song, Vassilis N. Ioannidis, Danai Koutra, Christos Faloutsos</li>
<li>for: 提高下游图学任务中节点特征的质量，以提高图 neural network（GNN）的表现。</li>
<li>methods: 使用TOUCHUP-G方法，该方法是一种通用的、多Modal的、原则正的方法，可以提高任何下游图任务中节点特征的质量。</li>
<li>results: TOUCHUP-G方法可以在四个真实世界数据集上达到状态的最佳结果，这些数据集包括不同的任务和modal。<details>
<summary>Abstract</summary>
How can we enhance the node features acquired from Pretrained Models (PMs) to better suit downstream graph learning tasks? Graph Neural Networks (GNNs) have become the state-of-the-art approach for many high-impact, real-world graph applications. For feature-rich graphs, a prevalent practice involves utilizing a PM directly to generate features, without incorporating any domain adaptation techniques. Nevertheless, this practice is suboptimal because the node features extracted from PM are graph-agnostic and prevent GNNs from fully utilizing the potential correlations between the graph structure and node features, leading to a decline in GNNs performance. In this work, we seek to improve the node features obtained from a PM for downstream graph tasks and introduce TOUCHUP-G, which has several advantages. It is (a) General: applicable to any downstream graph task, including link prediction which is often employed in recommender systems; (b) Multi-modal: able to improve raw features of any modality (e.g. images, texts, audio); (c) Principled: it is closely related to a novel metric, feature homophily, which we propose to quantify the potential correlations between the graph structure and node features and we show that TOUCHUP-G can effectively shrink the discrepancy between the graph structure and node features; (d) Effective: achieving state-of-the-art results on four real-world datasets spanning different tasks and modalities.
</details>
<details>
<summary>摘要</summary>
如何增强从预训练模型（PM）获取的节点特征以更适合下游图学任务？图神经网络（GNNs）已成为许多高impact、实际世界图应用的州立艺术。对于具有丰富特征的图，一种常见做法是直接使用PM生成特征，不 incorporating任何领域适应技术。然而，这种做法是不优化的，因为PM中的节点特征是图无关的，不能让GNNs完全利用图结构和节点特征之间的潜在相关性，导致GNNs的性能下降。在这项工作中，我们想要改进从PM获取的节点特征，并引入了TOUCHUP-G，它具有以下优势：* 通用：适用于任何下游图任务，包括常用的链接预测任务（常用于推荐系统）。* 多模式：能够提高任何类型的原始特征（例如图像、文本、音频）。* 原则性：与我们提出的一种新的度量（特征同化度）有紧密关系，我们表明了TOUCHUP-G可以有效缩小图结构和节点特征之间的差异。* 有效：在四个实际数据集上达到了状态之最的结果，这些数据集来自不同的任务和模式。
</details></li>
</ul>
<hr>
<h2 id="PRiSM-Enhancing-Low-Resource-Document-Level-Relation-Extraction-with-Relation-Aware-Score-Calibration"><a href="#PRiSM-Enhancing-Low-Resource-Document-Level-Relation-Extraction-with-Relation-Aware-Score-Calibration" class="headerlink" title="PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration"></a>PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13869">http://arxiv.org/abs/2309.13869</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brightjade/prism">https://github.com/brightjade/prism</a></li>
<li>paper_authors: Minseok Choi, Hyesu Lim, Jaegul Choo</li>
<li>for:  DocRE是为了提取文档中所有实体对的关系而设计的。</li>
<li>methods: 我们使用了一种叫做PRiSM的方法，它可以根据关系Semantic信息来适应logits。</li>
<li>results: 我们在三个DocRE数据集上进行评估，结果表明，将现有模型与PRiSM结合可以提高表达度，而且在训练时使用的数据量只需要3%。同时，我们发现在训练过程中，PRiSM可以降低偏差错误的数量，达到36倍的提升。<details>
<summary>Abstract</summary>
Document-level relation extraction (DocRE) aims to extract relations of all entity pairs in a document. A key challenge in DocRE is the cost of annotating such data which requires intensive human effort. Thus, we investigate the case of DocRE in a low-resource setting, and we find that existing models trained on low data overestimate the NA ("no relation") label, causing limited performance. In this work, we approach the problem from a calibration perspective and propose PRiSM, which learns to adapt logits based on relation semantic information. We evaluate our method on three DocRE datasets and demonstrate that integrating existing models with PRiSM improves performance by as much as 26.38 F1 score, while the calibration error drops as much as 36 times when trained with about 3% of data. The code is publicly available at https://github.com/brightjade/PRiSM.
</details>
<details>
<summary>摘要</summary>
文档级关系提取（DocRE）目标是在文档中提取所有实体对的关系。一个主要挑战在DocRE中是获取数据 annotating 的成本，需要卷积的人工劳动。因此，我们在低资源设定下调查DocRE问题，发现现有模型在低数据上训练后过度估计NA("无关")标签，导致性能有限。在这种情况下，我们从抽象角度出发，提出了PRiSM，它学习基于关系semantic信息来调整logits。我们对三个DocRE数据集进行评估，并证明了将现有模型与PRiSM结合使用可以提高性能，最高提高26.38准确率，同时抽象错误下降了36倍，只需训练约3%的数据。代码可以在https://github.com/brightjade/PRiSM上获取。
</details></li>
</ul>
<hr>
<h2 id="Fast-HuBERT-An-Efficient-Training-Framework-for-Self-Supervised-Speech-Representation-Learning"><a href="#Fast-HuBERT-An-Efficient-Training-Framework-for-Self-Supervised-Speech-Representation-Learning" class="headerlink" title="Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning"></a>Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13860">http://arxiv.org/abs/2309.13860</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanghaha0908/fasthubert">https://github.com/yanghaha0908/fasthubert</a></li>
<li>paper_authors: Guanrou Yang, Ziyang Ma, Zhisheng Zheng, Yakun Song, Zhikang Niu, Xie Chen</li>
<li>for: 这篇论文主要是为了提高自动标注学习（SSL）方法在语音处理任务中的效率，并且测试这些方法在不同的下游任务中的表现。</li>
<li>methods: 本论文使用了HuBERT模型，并进行了多个效率优化，包括范例删除、批评迭代、条件更新、等。</li>
<li>results: 相比原始实现， Fast-HuBERT可以在1.1天内训练完成，并且无损性能，实现了5.2倍的速度提升。此外， authors 还 explore了两种已知技术，并证明了这些技术可以获得类似的改进。<details>
<summary>Abstract</summary>
Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)近年来，自动学习（Self-Supervised Learning，SSL）方法在语音处理任务中得到了 significative 进步。许多基于语音的 SSL 模型已经被开发出来，并在各种下游任务中表现出色，包括语音识别。然而，现有的语音基于 SSL 模型面临着计算成本的问题，这可能会限制它们的应用和学术研究的深度。为解决这个问题，我们首先分析了不同模块的计算成本在 HuBERT 预训练过程中，然后引入了一堆性能优化技术，称之为 Fast-HuBERT。我们的 Fast-HuBERT 可以在 1.1 天内使用 8 个 V100 GPU 在 Librispeech 960h 测试集上进行预训练，无需性能下降，相比原始实现，实现了 5.2 倍的速度提升。此外，我们还探索了两种已经广泛研究的技术，并在 Fast-HuBERT 中进行了详细的实验，并发现了一致的改进。
</details></li>
</ul>
<hr>
<h2 id="Can-neural-networks-count-digit-frequency"><a href="#Can-neural-networks-count-digit-frequency" class="headerlink" title="Can neural networks count digit frequency?"></a>Can neural networks count digit frequency?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04431">http://arxiv.org/abs/2310.04431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PadmakshKhandelwal/Can-neural-networks-count">https://github.com/PadmakshKhandelwal/Can-neural-networks-count</a></li>
<li>paper_authors: Padmaksh Khandelwal</li>
<li>for: 本研究旨在比较不同的古典机器学习模型和神经网络在识别每个数字的频率出现的问题上的性能。这有各种应用场景，如获取视频场景中目标对象的频率。</li>
<li>methods: 我们在这个问题上采用了一种混合的分类和回归任务，并且特意制作了自己的数据集来观察系统性的差异。我们使用不同的度量来评估每种方法的性能，并且在多个数据集上进行了评估。</li>
<li>results: 我们发现，决策树和Random Forest具有内在的偏见，导致它们无法泛化好。同时，神经网络在分类和回归两个任务上都明显超过了古典机器学习模型，尤其是在6位和10位数据集上。数据集和代码在github上公开。<details>
<summary>Abstract</summary>
In this research, we aim to compare the performance of different classical machine learning models and neural networks in identifying the frequency of occurrence of each digit in a given number. It has various applications in machine learning and computer vision, e.g. for obtaining the frequency of a target object in a visual scene. We considered this problem as a hybrid of classification and regression tasks. We carefully create our own datasets to observe systematic differences between different methods. We evaluate each of the methods using different metrics across multiple datasets.The metrics of performance used were the root mean squared error and mean absolute error for regression evaluation, and accuracy for classification performance evaluation. We observe that decision trees and random forests overfit to the dataset, due to their inherent bias, and are not able to generalize well. We also observe that the neural networks significantly outperform the classical machine learning models in terms of both the regression and classification metrics for both the 6-digit and 10-digit number datasets. Dataset and code are available on github.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们目标是比较不同的古典机器学习模型和神经网络在识别每个数字的频率出现的性能。它在机器学习和计算机视觉等领域有各种应用，例如获取视觉场景中目标对象的频率。我们将这个问题视为分类和回归任务的混合问题。我们仔细制作了自己的数据集，以观察不同方法之间的系统差异。我们对每种方法使用不同的指标进行评估，并在多个数据集上进行评估。我们发现决策树和随机森林因数据集的偏袋而过拟合，无法通过泛化而表现出色。我们还发现神经网络在分类和回归指标方面对6位和10位数据集都有显著的优势。数据集和代码可以在github上下载。
</details></li>
</ul>
<hr>
<h2 id="Sampling-Variational-Auto-Encoder-Ensemble-In-the-Quest-of-Explainable-Artificial-Intelligence"><a href="#Sampling-Variational-Auto-Encoder-Ensemble-In-the-Quest-of-Explainable-Artificial-Intelligence" class="headerlink" title="Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence"></a>Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14385">http://arxiv.org/abs/2309.14385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarit Maitra, Vivek Mishra, Pratima Verma, Manav Chopra, Priyanka Nath</li>
<li>for: 这篇论文的目的是提出一种新的可解释人工智能（XAI）框架，用于解释人工智能模型的输出。</li>
<li>methods: 这篇论文使用了一种新的混合架构，称为Sampling-Variational Auto Encoder-Ensemble Anomaly Detection（SVEAD），它将Variational Auto Encoder（VAE）与集成折衔和SHapley Additive exPlanations（SHAP）相结合，用于解决不平衡分类问题。</li>
<li>results: 研究发现，将VAE、集成折衔和SHAP结合使用可以不仅提高模型性能，还可以提供一个简单易于解释的框架。此外，研究还使用SHAP与排序重要性和个体条件预期结合，创造了一个强大的模型解释方法。这些发现对实际应用中的XAI具有重要的意义，可以增强人工智能应用的信任度。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) models have recently attracted a great deal of interest from a variety of application sectors. Despite significant developments in this area, there are still no standardized methods or approaches for understanding AI model outputs. A systematic and cohesive framework is also increasingly necessary to incorporate new techniques like discriminative and generative models to close the gap. This paper contributes to the discourse on XAI by presenting an empirical evaluation based on a novel framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble stacking and SHapley Additive exPlanations are used for imbalanced classification. The finding reveals that combining ensemble stacking, VAE, and SHAP can. not only lead to better model performance but also provide an easily explainable framework. This work has used SHAP combined with Permutation Importance and Individual Conditional Expectations to create a powerful interpretability of the model. The finding has an important implication in the real world, where the need for XAI is paramount to boost confidence in AI applications.
</details>
<details>
<summary>摘要</summary>
《可解释人工智能（XAI）模型在不同应用领域引起了很大的关注。虽然这一领域已经取得了很大的进步，但是还没有标准化的方法或方法来理解人工智能模型的输出。一个系统和一致的框架也在增加，以整合新技术如探测和生成模型，以填补这一空白。这篇论文对XAI进行了评估，通过提出了一个新的框架：采样-自适应变换器-ensemble异常检测（SVEAD）。这是一种混合体系，其中变换器与ensemble栈和SHapley Additive exPlanations（SHAP）结合使用，用于处理不均衡的分类。研究发现，将ensemble栈、变换器和SHAP结合使用，不仅可以提高模型性能，还可以提供一个简单易理解的框架。本研究使用SHAP与排序重要性和个体条件预期结合，创造了一个强大的模型解释能力。这种发现对现实中的XAI需求具有重要意义，以增加人工智能应用的信任度。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is done using a machine translation tool, and may not be perfect. Please note that the translation may not capture all the nuances and idiomatic expressions of the original text.
</details></li>
</ul>
<hr>
<h2 id="Prior-Bilinear-Based-Models-for-Knowledge-Graph-Completion"><a href="#Prior-Bilinear-Based-Models-for-Knowledge-Graph-Completion" class="headerlink" title="Prior Bilinear Based Models for Knowledge Graph Completion"></a>Prior Bilinear Based Models for Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13834">http://arxiv.org/abs/2309.13834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Li, Ruilin Luo, Jiaqi Sun, Jing Xiao, Yujiu Yang</li>
<li>for: 本文主要针对知识图（KG）完成任务，探讨bilinear模型忽略了先前属性的问题。</li>
<li>methods: 作者提出了一种解决方案called Unit Ball Bilinear Model (UniBi)，该模型不仅有理论上的优势，还提供了更好的解释性和性能，通过最小化无用学习的约束来减少不必要的学习。</li>
<li>results: 实验表明，UniBi模型能够capture先前属性，并且verify其解释性和性能。<details>
<summary>Abstract</summary>
Bilinear based models are powerful and widely used approaches for Knowledge Graphs Completion (KGC). Although bilinear based models have achieved significant advances, these studies mainly concentrate on posterior properties (based on evidence, e.g. symmetry pattern) while neglecting the prior properties. In this paper, we find a prior property named "the law of identity" that cannot be captured by bilinear based models, which hinders them from comprehensively modeling the characteristics of KGs. To address this issue, we introduce a solution called Unit Ball Bilinear Model (UniBi). This model not only achieves theoretical superiority but also offers enhanced interpretability and performance by minimizing ineffective learning through minimal constraints. Experiments demonstrate that UniBi models the prior property and verify its interpretability and performance.
</details>
<details>
<summary>摘要</summary>
bilinear基于模型在知识图完成（KGC）领域是非常强大和广泛使用的方法。虽然bilinear基于模型已经取得了显著的进步，但这些研究主要集中于后果性质（基于证据，例如对称模式）而忽略了先前性质。在这篇论文中，我们发现了一种先前性质名为“同一性法律”，这种法律不能被bilinear基于模型捕捉，这会限制它们完全模型知识图的特点。为解决这个问题，我们介绍了一种解决方案called Unit Ball Bilinear Model（UniBi）。这个模型不仅具有理论上的优越性，也提供了更好的解释性和性能，通过最小化不必要的学习来减少不必要的约束。实验表明，UniBi模型了先前性质并证明了其解释性和性能。
</details></li>
</ul>
<hr>
<h2 id="Dual-Feature-Augmentation-Network-for-Generalized-Zero-shot-Learning"><a href="#Dual-Feature-Augmentation-Network-for-Generalized-Zero-shot-Learning" class="headerlink" title="Dual Feature Augmentation Network for Generalized Zero-shot Learning"></a>Dual Feature Augmentation Network for Generalized Zero-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13833">http://arxiv.org/abs/2309.13833</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sion1/dfan">https://github.com/sion1/dfan</a></li>
<li>paper_authors: Lei Xiang, Yuan Zhou, Haoran Duan, Yang Long</li>
<li>for: 这篇论文主要针对零例学习（Zero-shot learning）问题，旨在无需训练样本就能够推断未经训练的类别。</li>
<li>methods: 该论文提出了一种新的DUAL Feature Augmentation Network（DFAN），包括两个特征增强模块，一个用于视觉特征，另一个用于语义特征。视觉特征增强模块通过学习特征特性，使用高 cosine 距离来强化特征表示。语义特征增强模块则通过提出了一个偏置学习器，捕捉数据集的偏移，使得预测值与实际值之间的差距更小。此外，我们还引入了两个预测器，以冲突地解决本地和全局特征之间的冲突。</li>
<li>results: 实验结果表明，我们的方法在三个 benchmark 上表现出了明显的进步，与现有方法相比，具有更高的准确率和更好的一致性。<details>
<summary>Abstract</summary>
Zero-shot learning (ZSL) aims to infer novel classes without training samples by transferring knowledge from seen classes. Existing embedding-based approaches for ZSL typically employ attention mechanisms to locate attributes on an image. However, these methods often ignore the complex entanglement among different attributes' visual features in the embedding space. Additionally, these methods employ a direct attribute prediction scheme for classification, which does not account for the diversity of attributes in images of the same category. To address these issues, we propose a novel Dual Feature Augmentation Network (DFAN), which comprises two feature augmentation modules, one for visual features and the other for semantic features. The visual feature augmentation module explicitly learns attribute features and employs cosine distance to separate them, thus enhancing attribute representation. In the semantic feature augmentation module, we propose a bias learner to capture the offset that bridges the gap between actual and predicted attribute values from a dataset's perspective. Furthermore, we introduce two predictors to reconcile the conflicts between local and global features. Experimental results on three benchmarks demonstrate the marked advancement of our method compared to state-of-the-art approaches. Our code is available at https://github.com/Sion1/DFAN.
</details>
<details>
<summary>摘要</summary>
为了解决这些问题，我们提出了一个新的对应网络（DFAN），其包括两个对应模组：一个用于可视特征，另一个用于 semantic 特征。可视特征增强模组会明确地学习属性特征，并使用做 Cosine 距离来分离它们，这样提高了属性表示。另一方面，semantic 增强模组中，我们提出了一个偏置学习器，以捕捉实际数据集的偏移，将预测的属性值与实际值匹配。此外，我们引入了两个预测器，以调解本地和全局特征之间的冲突。实验结果显示，我们的方法与现有的方法相比，在三个 benchmark 上有明显的进步。我们的代码可以在 GitHub 上获取：https://github.com/Sion1/DFAN。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Cognitive-Maps-and-Planning-in-Large-Language-Models-with-CogEval"><a href="#Evaluating-Cognitive-Maps-and-Planning-in-Large-Language-Models-with-CogEval" class="headerlink" title="Evaluating Cognitive Maps and Planning in Large Language Models with CogEval"></a>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15129">http://arxiv.org/abs/2309.15129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, Jonathan Larson</li>
<li>for: 这研究的目的是系统evaluate大语言模型（LLM）的认知能力。</li>
<li>methods: 该研究使用了一种基于认知科学的评估协议，称为CogEval，来评估 LLM 的认知能力。</li>
<li>results: 研究发现， LL M 在一些简单的规划任务上表现出 Competence，但在更复杂的规划任务上存在 Failure modes，包括hallucination 和循环。这些发现不支持 LL M 具有出色的规划能力。<details>
<summary>Abstract</summary>
Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.
</details>
<details>
<summary>摘要</summary>
近期有多个研究表明大语言模型（LLM）具有新的认知能力。然而，大多数研究仅仅基于启示，忽略训练集的杂乱，或者缺乏多个任务、控制条件、多个迭代和统计学robustness测试。我们在这里作出了两个主要贡献。首先，我们提议了一种认知科学途径 protocol for the systematic evaluation of cognitive capacities in Large Language Models，可以用于评估多种能力。其次，我们遵循这种协议来系统地评估 eight LLMs（OpenAI GPT-4、GPT-3.5-turbo-175B、davinci-003-175B、Google Bard、Cohere-xlarge-52.4B、Anthropic Claude-1-52B、LLaMA-13B和Alpaca-7B）的认知地图和规划能力。我们基于人类实验的任务提示，这些任务具有建立的验证有效性，且不存在在 LLM 训练集中。我们发现，虽然 LLMs 在一些规划任务中显示出一定的能力，但系统性评估表明，LLMs 在规划任务中存在明显的失败模式，包括hallucination 无效的轨迹和gets trapped in loops。这些发现不支持 LLMS 具有出 Box 的规划能力。这可能是因为 LLMS 不理解规划问题下的隐藏关系结构，并且无法基于这种结构推导目标导向的轨迹。我们的发现有很多应用和未来方向的意义。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Local-Robustness-of-High-Accuracy-Binary-Neural-Networks-for-Enhanced-Traffic-Sign-Recognition"><a href="#Benchmarking-Local-Robustness-of-High-Accuracy-Binary-Neural-Networks-for-Enhanced-Traffic-Sign-Recognition" class="headerlink" title="Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition"></a>Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03033">http://arxiv.org/abs/2310.03033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/christopherbrix/vnncomp2023_benchmarks">https://github.com/christopherbrix/vnncomp2023_benchmarks</a></li>
<li>paper_authors: Andreea Postovan, Mădălina Eraşcu</li>
<li>for: 本研究旨在提高自动驾驶系统中的道路标志识别精度，并且面临着实际中的挑战，如抗抗例和遮挡。</li>
<li>methods: 本研究使用了二进制神经网络（BNN）来构建精度高的道路标志识别模型，并且强调了模型的尺寸和计算资源的有效使用。</li>
<li>results: 本研究发现，使用BNN模型可以在实际中提高道路标志识别精度，但是存在一些地区的异常输出和错误结果。<details>
<summary>Abstract</summary>
Traffic signs play a critical role in road safety and traffic management for autonomous driving systems. Accurate traffic sign classification is essential but challenging due to real-world complexities like adversarial examples and occlusions. To address these issues, binary neural networks offer promise in constructing classifiers suitable for resource-constrained devices.   In our previous work, we proposed high-accuracy BNN models for traffic sign recognition, focusing on compact size for limited computation and energy resources. To evaluate their local robustness, this paper introduces a set of benchmark problems featuring layers that challenge state-of-the-art verification tools. These layers include binarized convolutions, max pooling, batch normalization, fully connected. The difficulty of the verification problem is given by the high number of network parameters (905k - 1.7 M), of the input dimension (2.7k-12k), and of the number of regions (43) as well by the fact that the neural networks are not sparse.   The proposed BNN models and local robustness properties can be checked at https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/traffic_signs_recognition.   The results of the 4th International Verification of Neural Networks Competition (VNN-COMP'23) revealed the fact that 4, out of 7, solvers can handle many of our benchmarks randomly selected (minimum is 6, maximum is 36, out of 45). Surprisingly, tools output also wrong results or missing counterexample (ranging from 1 to 4). Currently, our focus lies in exploring the possibility of achieving a greater count of solved instances by extending the allotted time (previously set at 8 minutes). Furthermore, we are intrigued by the reasons behind the erroneous outcomes provided by the tools for certain benchmarks.
</details>
<details>
<summary>摘要</summary>
traffic signs 对道路安全和交通管理具有关键作用，因此精准的交通标志分类是非常重要，但又是具有挑战性的。为了解决这些问题， binary neural networks（BNN）提供了一种可能性，它们可以在有限的计算和能源资源下构建高精度的分类器。在我们的前一项工作中，我们已经提出了高精度的BNN模型，专注于模型的紧凑性，以适应有限的计算资源。为了评估这些模型的本地稳定性，这篇论文引入了一组 benchmark 问题，这些问题挑战了当前的验证工具。这些问题包括缩进几何学层、最大池化层、批量常量层和归一化层，它们的难度来自于网络参数的大量（905k-1.7M）、输入维度的大量（2.7k-12k）和区域的数量（43）以及网络不是稀疏的性。可以在 <https://github.com/ChristopherBrix/vnncomp2023_benchmarks/tree/main/benchmarks/traffic_signs_recognition> 中查看我们的模型和本地稳定性性质。 competition 的结果表明，4个出 из 7个解决方案可以随机选择的 benchmark 中的许多（最小值是6，最大值是36，总共45）。尽管有些工具输出了错误的结果或缺失Counterexample（从1到4），但我们目前的注意力是探索可以通过延长时间（原先设置为8分钟）来提高解决的数量。此外，我们也对工具输出错误的原因产生了极大的兴趣。
</details></li>
</ul>
<hr>
<h2 id="Privacy-preserving-Linear-Computations-in-Spiking-Neural-P-Systems"><a href="#Privacy-preserving-Linear-Computations-in-Spiking-Neural-P-Systems" class="headerlink" title="Privacy-preserving Linear Computations in Spiking Neural P Systems"></a>Privacy-preserving Linear Computations in Spiking Neural P Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13803">http://arxiv.org/abs/2309.13803</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hieu9955/ggggg">https://github.com/hieu9955/ggggg</a></li>
<li>paper_authors: Mihail-Iulian Plesa, Marian Gheorghe, Florentin Ipate</li>
<li>for: 这个论文旨在提出一种基于生物神经元的启发式计算模型，以及这种模型在不同领域的应用，如正式验证、人工智能和加密等。</li>
<li>methods: 作者提出了一种基于SN P系统的隐私保护协议，允许客户端使用远程服务器来计算线性函数，而无需把函数参数和结果泄露给服务器。</li>
<li>results: 作者采用了SN P系统实现任意自然数上的线性函数，并评估了协议的安全性在“诚实但偷 curios”安全模型下。<details>
<summary>Abstract</summary>
Spiking Neural P systems are a class of membrane computing models inspired directly by biological neurons. Besides the theoretical progress made in this new computational model, there are also numerous applications of P systems in fields like formal verification, artificial intelligence, or cryptography. Motivated by all the use cases of SN P systems, in this paper, we present a new privacy-preserving protocol that enables a client to compute a linear function using an SN P system hosted on a remote server. Our protocol allows the client to use the server to evaluate functions of the form t_1k + t_2 without revealing t_1, t_2 or k and without the server knowing the result. We also present an SN P system to implement any linear function over natural numbers and some security considerations of our protocol in the honest-but-curious security model.
</details>
<details>
<summary>摘要</summary>
��Spiking Neural P Systems是一种基于生物神经元的计算模型。除了这种新的计算模型的理论进步之外，P系统还有很多应用于领域如正式验证、人工智能和加密等。在这篇论文中，我们提出了一种新的隐私保护协议，使得客户可以使用远程服务器上的SN P系统来计算函数形式为t_1k + t_2，而无需抛出t_1, t_2或k的报告，也无需服务器知道结果。我们还提出了一种实现任意自然数上的线性函数的SN P系统，以及一些安全考虑在诚实但叛逆安全模型中。
</details></li>
</ul>
<hr>
<h2 id="Can-LLM-Generated-Misinformation-Be-Detected"><a href="#Can-LLM-Generated-Misinformation-Be-Detected" class="headerlink" title="Can LLM-Generated Misinformation Be Detected?"></a>Can LLM-Generated Misinformation Be Detected?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13788">http://arxiv.org/abs/2309.13788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llm-misinformation/llm-misinformation">https://github.com/llm-misinformation/llm-misinformation</a></li>
<li>paper_authors: Canyu Chen, Kai Shu</li>
<li>for:  investigates whether LLM-generated misinformation can cause more harm than human-written misinformation</li>
<li>methods: builds a taxonomy of LLM-generated misinformation and categorizes potential real-world methods for generating misinformation with LLMs, and employs extensive empirical investigation to study the detection difficulty of LLM-generated misinformation</li>
<li>results: discovers that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, suggesting it can have more deceptive styles and potentially cause more harm<details>
<summary>Abstract</summary>
The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.
</details>
<details>
<summary>摘要</summary>
LLMs 的出现对社交媒体和网络安全带来了重大影响。然而， LLMS 如 ChatGPT 可能会被滥用来生成谣言，这对于在线安全和公众信任造成了严重的问题。我们提出了一个基本研究问题： LLMS 生成的谣言是人类写的谣言更可能导致更多的伤害吗？我们从检测困难性的角度来回答这个问题。我们首先构建了 LLMS 生成谣言的分类体系，然后将可能在实际情况下使用 LLMS 生成谣言的方法分类和验证。经过广泛的实验研究，我们发现 LLMS 生成的谣言比人类写的谣言更难以检测，它们可能具有更多的欺骗性和误导性，从而可能导致更多的伤害。我们还讨论了我们的发现对于在 LLMS 时代战击谣言的应用和对策的影响。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Benefit-of-Multimodal-Learning"><a href="#On-the-Computational-Benefit-of-Multimodal-Learning" class="headerlink" title="On the Computational Benefit of Multimodal Learning"></a>On the Computational Benefit of Multimodal Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13782">http://arxiv.org/abs/2309.13782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhou Lu</li>
<li>for: 本研究目的是调查多模态学习是否具有计算优势？</li>
<li>methods: 我们使用一种基于 intersecting two half-spaces 问题的新修改来实现多模态学习。</li>
<li>results: 我们发现，在某些条件下，多模态学习可以在计算上赶超单模态学习，具体来说是可以在幂时间内解决NP困难的学习任务。<details>
<summary>Abstract</summary>
Human perception inherently operates in a multimodal manner. Similarly, as machines interpret the empirical world, their learning processes ought to be multimodal. The recent, remarkable successes in empirical multimodal learning underscore the significance of understanding this paradigm. Yet, a solid theoretical foundation for multimodal learning has eluded the field for some time. While a recent study by Lu (2023) has shown the superior sample complexity of multimodal learning compared to its unimodal counterpart, another basic question remains: does multimodal learning also offer computational advantages over unimodal learning? This work initiates a study on the computational benefit of multimodal learning. We demonstrate that, under certain conditions, multimodal learning can outpace unimodal learning exponentially in terms of computation. Specifically, we present a learning task that is NP-hard for unimodal learning but is solvable in polynomial time by a multimodal algorithm. Our construction is based on a novel modification to the intersection of two half-spaces problem.
</details>
<details>
<summary>摘要</summary>
人类感知自然地进行多模态的处理。 Similarly，机器在观察现实世界时，其学习过程也应该是多模态的。当前， empirical multimodal learning的成功表明了这个思想的重要性。然而，领域中对多模态学习的基础理论还没有得到充分的解决。一些研究（如Lu 2023）已经表明了多模态学习的样本复杂性比单模态学习更低，但另一个基本问题仍然没有得到答案：多模态学习是否也提供了计算上的优势？本研究开始了对多模态学习的计算优势的研究。我们证明，在某些条件下，多模态学习可以在计算上赶超单模态学习，并且可以在计算时间方面呈指数增长。特别是，我们提出了一个NP困难的学习任务，但是通过多模态算法可以在 polynomial time 内解决。我们的构造基于一种新的两个半空间的交叉问题的修改。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Machine-Learning-for-ICU-Readmission-Prediction"><a href="#Explainable-Machine-Learning-for-ICU-Readmission-Prediction" class="headerlink" title="Explainable Machine Learning for ICU Readmission Prediction"></a>Explainable Machine Learning for ICU Readmission Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13781">http://arxiv.org/abs/2309.13781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex G. C. de Sá, Daniel Gould, Anna Fedyukova, Mitchell Nicholas, Lucy Dockrell, Calvin Fletcher, David Pilcher, Daniel Capurro, David B. Ascher, Khaled El-Khawas, Douglas E. V. Pires</li>
<li>For: The paper aims to develop a standardized and explainable machine learning pipeline to predict patient readmission in the intensive care unit (ICU) using a multicentric database.* Methods: The paper uses a machine learning approach with a Random Forest classification model to predict patient readmission, and validates the model on both monocentric and multicentric settings. The authors also provide explanations for the constructed models to derive insightful conclusions.* Results: The paper achieves predictive performance with an area under the receiver operating characteristic curve (AUC) up to 0.7, and demonstrates good calibration and consistency on validation sets. The authors also identify a set of variables related to vital signs, blood tests, demographics, and ICU-associated variables that are associated with patient readmission.<details>
<summary>Abstract</summary>
The intensive care unit (ICU) comprises a complex hospital environment, where decisions made by clinicians have a high level of risk for the patients' lives. A comprehensive care pathway must then be followed to reduce p complications. Uncertain, competing and unplanned aspects within this environment increase the difficulty in uniformly implementing the care pathway. Readmission contributes to this pathway's difficulty, occurring when patients are admitted again to the ICU in a short timeframe, resulting in high mortality rates and high resource utilisation. Several works have tried to predict readmission through patients' medical information. Although they have some level of success while predicting readmission, those works do not properly assess, characterise and understand readmission prediction. This work proposes a standardised and explainable machine learning pipeline to model patient readmission on a multicentric database (i.e., the eICU cohort with 166,355 patients, 200,859 admissions and 6,021 readmissions) while validating it on monocentric (i.e., the MIMIC IV cohort with 382,278 patients, 523,740 admissions and 5,984 readmissions) and multicentric settings. Our machine learning pipeline achieved predictive performance in terms of the area of the receiver operating characteristic curve (AUC) up to 0.7 with a Random Forest classification model, yielding an overall good calibration and consistency on validation sets. From explanations provided by the constructed models, we could also derive a set of insightful conclusions, primarily on variables related to vital signs and blood tests (e.g., albumin, blood urea nitrogen and hemoglobin levels), demographics (e.g., age, and admission height and weight), and ICU-associated variables (e.g., unit type). These insights provide an invaluable source of information during clinicians' decision-making while discharging ICU patients.
</details>
<details>
<summary>摘要</summary>
医院快速病区（ICU）是一个复杂的医疗环境，医生的决策对患者生命的风险很高。为了降低复杂性和风险，一个完整的护理路径必须采取。不确定、竞争和不计划的因素在这个环境中增加了困难，使得一致性地实施护理路径变得更加困难。重复入院是这个护理路径的一个主要障碍物，入院次数较多，导致高死亡率和资源利用率高。许多研究已经尝试预测重复入院，但这些研究并没有充分评估、描述和理解重复入院预测。本文提出了一个标准化和可解释的机器学习管道，用于在多中心数据库（i.e., eICU cohort）上预测患者重复入院，并在多中心和单中心设置上验证。我们的机器学习管道在预测重复入院方面达到了AUC0.7的水平，并在验证集上表现出了一致性和准确性。从构建的模型中提供的解释中，我们也得到了一些有价值的结论，主要关注于生命指标和血液测试（如蛋白质、尿氨酸和血红细胞含量）、人口学特征（如年龄和入院高度和重量）以及ICU相关变量（如单元类型）。这些结论可以提供医生决策时的很有价值的信息。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/cs.AI_2023_09_25/" data-id="clpztdnbs004les88f1ycde4l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/cs.CL_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T11:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/cs.CL_2023_09_25/">cs.CL - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Introducing-DictaLM-–-A-Large-Generative-Language-Model-for-Modern-Hebrew"><a href="#Introducing-DictaLM-–-A-Large-Generative-Language-Model-for-Modern-Hebrew" class="headerlink" title="Introducing DictaLM – A Large Generative Language Model for Modern Hebrew"></a>Introducing DictaLM – A Large Generative Language Model for Modern Hebrew</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14568">http://arxiv.org/abs/2309.14568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaltiel Shmidman, Avi Shmidman, Amir David Nissan Cohen, Moshe Koppel</li>
<li>for: 这篇论文是为了开发一个适用于现代希伯来语的大规模语言模型。</li>
<li>methods: 这篇论文使用了7亿个参数的模型，主要是在希伯来语中进行训练。作者还发布了基础模型和指导适应模型，并将其发布于Creative Commons许可证下。此外，作者还介绍了DictaLM-Rab基础模型，这是专门针对 rabbinic&#x2F;历史希伯来语的。</li>
<li>results: 这篇论文提出了一个初步的希伯来语大规模语言模型，可以用于多种希伯来语特定任务的细化调整，如教学、问答、情感分析等。这是一个对希伯来语NLP社区的一个初步探索。<details>
<summary>Abstract</summary>
We present DictaLM, a large-scale language model tailored for Modern Hebrew. Boasting 7B parameters, this model is predominantly trained on Hebrew-centric data. As a commitment to promoting research and development in the Hebrew language, we release both the foundation model and the instruct-tuned model under a Creative Commons license. Concurrently, we introduce DictaLM-Rab, another foundation model geared towards Rabbinic/Historical Hebrew. These foundation models serve as ideal starting points for fine-tuning various Hebrew-specific tasks, such as instruction, Q&A, sentiment analysis, and more. This release represents a preliminary step, offering an initial Hebrew LLM model for the Hebrew NLP community to experiment with.
</details>
<details>
<summary>摘要</summary>
我们介绍DictaLM，一种适用于现代希伯来语的大规模语言模型。这个模型拥有70亿参数，主要基于希伯来语数据进行训练。作为推广希伯来语研究和发展的承诺，我们在Creative Commons许可证下发布了基础模型和指导训练模型。同时，我们还引入DictaLM-Rab，另一个针对 rabbinic/历史希伯来语的基础模型。这些基础模型可以用于多种希伯来语特定任务的细化调整，如教学、问答、情感分析等。这次发布代表希伯来语NLPT社区的初步尝试，希望能够促进希伯来语语言模型的研究和发展。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Large-Multimodal-Models-with-Factually-Augmented-RLHF"><a href="#Aligning-Large-Multimodal-Models-with-Factually-Augmented-RLHF" class="headerlink" title="Aligning Large Multimodal Models with Factually Augmented RLHF"></a>Aligning Large Multimodal Models with Factually Augmented RLHF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14525">http://arxiv.org/abs/2309.14525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell</li>
<li>for: addressing the multimodal misalignment issue in large multimodal models (LMM)</li>
<li>methods: using reinforcement learning from human feedback (RLHF) to train a vision-language model to align with human annotations, and augmenting the reward model with additional factual information such as image captions and ground-truth multi-choice options</li>
<li>results: achieving a remarkable improvement of 94% on the LLaVA-Bench dataset and an improvement by 60% on MMHAL-BENCH over other baselines, with the first LMM trained with RLHF.Here are the three information in Simplified Chinese text:</li>
<li>for: 解决大量多模态模型（LMM）中的多模态误差问题</li>
<li>methods: 使用人类反馈学习（RLHF）来训练一个视觉语言模型，并将奖励模型增加了更多的事实信息，如图文描述和真实多选项</li>
<li>results: 在LLaVA-Bench数据集上达到了94%的表现水平，比前一个最佳方法提高了60%，并且开源了代码、模型和数据在<a target="_blank" rel="noopener" href="https://llava-rlhf.github.io./">https://llava-rlhf.github.io。</a><details>
<summary>Abstract</summary>
Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in "hallucination", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.
</details>
<details>
<summary>摘要</summary>
大型多modal模型（LMM）在modalities之间建立起来，但是这两个modalities之间的不一致可能导致"幻觉",生成不受多modal信息的支持的文本输出。为了解决多modal不一致问题，我们从文本领域中提取了人类反馈学习（RLHF），并将其应用到视觉语言对应中，请求人工标注员比较两个响应，并标出更加幻觉的一个，并训练视觉语言模型以 Maximize 模拟人类奖励。我们提出了一种新的对ignment算法called Factually Augmented RLHF，该算法将奖励模型中的奖励信息与更多的事实信息（如图像描述和真实多选项）相结合，以解决奖励黑客现象，并进一步提高性能。此外，我们还使用了之前已有的人类写的图像文本对应来提高我们模型的总能力。为了评估我们的方法在实际场景中的表现，我们开发了一个新的评估标准MMHAL-BENCH，强调对幻觉进行惩罚。作为首个RLHF模型，我们的方法在LLaVA-Bench数据集上达到了94%的性能水平，而前一个最佳方法只能达到87%的水平，在MMHAL-BENCH上与其他基eline相比，我们的方法提高了60%。我们将代码、模型和数据公开发布在https://llava-rlhf.github.io。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-Performance-on-Standardized-Testing-Exam-–-A-Proposed-Strategy-for-Learners"><a href="#ChatGPT-Performance-on-Standardized-Testing-Exam-–-A-Proposed-Strategy-for-Learners" class="headerlink" title="ChatGPT Performance on Standardized Testing Exam – A Proposed Strategy for Learners"></a>ChatGPT Performance on Standardized Testing Exam – A Proposed Strategy for Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14519">http://arxiv.org/abs/2309.14519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umer Farooq, Saira Anwar</li>
<li>for: 这研究探讨了ChatGPT在标准化测试准备中的问题解决能力，特点是关注GRE数学部分。先前的研究表明了ChatGPT在不同学科中的学习方法有很大的潜力。</li>
<li>methods: 我们通过对GRE数学部分100个随机选择的问题进行Quantitative评估来研究ChatGPT在不同内容领域中的问题解决能力。我们还使用t检验来检验修改问题提示对ChatGPT的准确率的影响。</li>
<li>results: 结果显示，对问题提示进行修改后，ChatGPT的准确率有 statistically significant 的提高（84%对修改后的问题，69%对原始数据）。研究还发现ChatGPT在某些问题上存在困难，并提供了修改问题提示的方法可以帮助学生准备标准测试 like GRE。<details>
<summary>Abstract</summary>
This study explores the problem solving capabilities of ChatGPT and its prospective applications in standardized test preparation, focusing on the GRE quantitative exam. Prior research has shown great potential for the utilization of ChatGPT for academic purposes in revolutionizing the approach to studying across various disciplines. We investigate how ChatGPT performs across various question types in the GRE quantitative domain, and how modifying question prompts impacts its accuracy. More specifically this study addressed two research questions: 1. How does ChatGPT perform in answering GRE-based quantitative questions across various content areas? 2. How does the accuracy of ChatGPT vary with modifying the question prompts? The dataset consisting of 100 randomly selected GRE quantitative questions was collected from the ETS official guide to GRE test preparation. We used quantitative evaluation to answer our first research question, and t-test to examine the statistical association between prompt modification and ChatGPT's accuracy. Results show a statistical improvement in the ChatGPT's accuracy after applying instruction priming and contextual prompts to the original questions. ChatGPT showed 84% accuracy with the modified prompts compared to 69% with the original data. The study discusses the areas where ChatGPT struggled with certain questions and how modifications can be helpful for preparing for standardized tests like GRE and provides future directions for prompt modifications.
</details>
<details>
<summary>摘要</summary>
To answer our first research question, we used quantitative evaluation to assess ChatGPT's performance on GRE-based quantitative questions across different content areas. We also used a t-test to examine the statistical association between prompt modification and ChatGPT's accuracy. Our results show that ChatGPT's accuracy improved statistically after we applied instruction priming and contextual prompts to the original questions. With the modified prompts, ChatGPT achieved 84% accuracy, compared to 69% with the original data.The study also discusses the areas where ChatGPT struggled with certain questions and how modifications can be helpful for preparing for standardized tests like GRE. We provide future directions for prompt modifications and highlight the potential of using ChatGPT for test preparation.Here is the translation in Simplified Chinese:这个研究探讨了ChatGPT的问题解决能力和其在标准化测试准备中的可能应用，特点是关注GRE数学部分。先前的研究表明了ChatGPT可以用于学术目的，可以革命化学习的方式。我们调查了ChatGPT如何在不同类型的GRE数学题目上表现，以及如何修改问题提示影响其准确率。为了回答我们的第一个研究问题，我们使用量化评估来评估ChatGPT在GRE数学题目上的表现，并使用t检验来检验修改提示和ChatGPT的准确率之间的统计关系。我们的结果显示，在我们应用了指导提示和文本提示后，ChatGPT的准确率有 statistically 的提高。与原始数据相比，ChatGPT在修改后的问题上达到了84%的准确率，与原始数据相比，这是69%的提高。研究还讨论了ChatGPT在某些问题上的困难之处，以及修改如何有助于为GRE和其他标准化测试准备。我们还提供了未来的提示修改方向，并强调了使用ChatGPT进行测试准备的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="DeepSpeed-Ulysses-System-Optimizations-for-Enabling-Training-of-Extreme-Long-Sequence-Transformer-Models"><a href="#DeepSpeed-Ulysses-System-Optimizations-for-Enabling-Training-of-Extreme-Long-Sequence-Transformer-Models" class="headerlink" title="DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"></a>DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14509">http://arxiv.org/abs/2309.14509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He</li>
<li>for: 本研究旨在提高大型语言模型（LLM）的训练效率，特别是对长序Transformer模型进行加速。</li>
<li>methods: 本文提出了一种新的方法——DeepSpeed-Ulysses，它可以高效地对长序LLM进行训练。这种方法通过分解输入数据的序列维度，并使用高效的所有到所有集成通信来计算注意力。</li>
<li>results: 实验表明，DeepSpeed-Ulysses可以比现有基eline方法快速2.5倍，并且可以在4倍长的序列长度上进行训练。<details>
<summary>Abstract</summary>
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.
</details>
<details>
<summary>摘要</summary>
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size, and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable, and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5 times faster with 4 times longer sequence length than the existing method SOTA baseline.
</details></li>
</ul>
<hr>
<h2 id="Classifying-token-frequencies-using-angular-Minkowski-p-distance"><a href="#Classifying-token-frequencies-using-angular-Minkowski-p-distance" class="headerlink" title="Classifying token frequencies using angular Minkowski $p$-distance"></a>Classifying token frequencies using angular Minkowski $p$-distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14495">http://arxiv.org/abs/2309.14495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Urs Lenz, Chris Cornelis</li>
<li>for: 本研究旨在探讨Angular Minkowski $p$-distance是一种代替cosine dissimilarity的异同度量，以及它在20-newsgroups dataset上的分类性能。</li>
<li>methods: 本研究使用了粗糙最近几个邻居和经典加权最近几个邻居来评估分类性能，并分析了$p$参数、数据维度$m$,邻居数$k$和权重的选择对分类性能的影响。</li>
<li>results: 研究发现，采用Angular Minkowski $p$-distance可以获得substantially higher的分类性能，特别是当$p$取得合适的值时。<details>
<summary>Abstract</summary>
Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by replacing Euclidean distance in the definition of cosine dissimilarity with other Minkowski $p$-distances. Cosine dissimilarity is frequently used with datasets containing token frequencies, and angular Minkowski $p$-distance may potentially be an even better choice for certain tasks. In a case study based on the 20-newsgroups dataset, we evaluate clasification performance for classical weighted nearest neighbours, as well as fuzzy rough nearest neighbours. In addition, we analyse the relationship between the hyperparameter $p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the choice of weights and the choice of classifier. We conclude that it is possible to obtain substantially higher classification performance with angular Minkowski $p$-distance with suitable values for $p$ than with classical cosine dissimilarity.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Explainable-and-Accurate-Natural-Language-Understanding-for-Voice-Assistants-and-Beyond"><a href="#Explainable-and-Accurate-Natural-Language-Understanding-for-Voice-Assistants-and-Beyond" class="headerlink" title="Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond"></a>Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14485">http://arxiv.org/abs/2309.14485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kalpa Gunaratna, Vijay Srinivasan, Hongxia Jin</li>
<li>for: joint NLU (Natural Language Understanding)  JOINT NLU是智能声助手上无可或缺的一环，它的目标是同时检测用户的意图和 slot filling。</li>
<li>methods: 使用各种技术提高准确率，并使模型自然易于理解和解释。</li>
<li>results: 对 JOINT NLU 模型进行自然易于理解和解释，不会影响准确率。同时，这种扩展可以在其他普通分类任务中使用。<details>
<summary>Abstract</summary>
Joint intent detection and slot filling, which is also termed as joint NLU (Natural Language Understanding) is invaluable for smart voice assistants. Recent advancements in this area have been heavily focusing on improving accuracy using various techniques. Explainability is undoubtedly an important aspect for deep learning-based models including joint NLU models. Without explainability, their decisions are opaque to the outside world and hence, have tendency to lack user trust. Therefore to bridge this gap, we transform the full joint NLU model to be `inherently' explainable at granular levels without compromising on accuracy. Further, as we enable the full joint NLU model explainable, we show that our extension can be successfully used in other general classification tasks. We demonstrate this using sentiment analysis and named entity recognition.
</details>
<details>
<summary>摘要</summary>
joint意图检测和插槽填充（joint NLU）对智能声音助手是非常重要的。近期的进展在这个领域主要集中在提高准确率上。解释性是深度学习模型，包括联合NLU模型的重要方面。没有解释性，这些模型的决策对外部世界来说是不透明的，因此容易lack user trust。因此，我们将全部联合NLU模型变换成“基本”的解释性模型，无需牺牲准确率。此外，我们证明了我们的扩展可以成功应用于其他通用分类任务中。我们通过 sentiment分析和名称实体识别来说明这一点。
</details></li>
</ul>
<hr>
<h2 id="DeepSpeed-VisualChat-Multi-Round-Multi-Image-Interleave-Chat-via-Multi-Modal-Causal-Attention"><a href="#DeepSpeed-VisualChat-Multi-Round-Multi-Image-Interleave-Chat-via-Multi-Modal-Causal-Attention" class="headerlink" title="DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention"></a>DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14327">http://arxiv.org/abs/2309.14327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/deepspeedexamples">https://github.com/microsoft/deepspeedexamples</a></li>
<li>paper_authors: Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He</li>
<li>for: 提高大型语言模型对交互对话的适应性和扩展性</li>
<li>methods: 引入多模态功能，包括创新的多模态 causal 注意机制和数据融合技术</li>
<li>results: 比对 existed 框架 superior 扩展性，可以承受大型语言模型的70亿参数大小Here’s the Chinese text in simplified format:</li>
<li>for: 提高大型语言模型对交互对话的适应性和扩展性</li>
<li>methods: 引入多模态功能，包括创新的多模态 causal 注意机制和数据融合技术</li>
<li>results: 比对 existed 框架 superior 扩展性，可以承受大型语言模型的70亿参数大小<details>
<summary>Abstract</summary>
Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B parameter language model size, representing a significant advancement in multi-modal language models and setting a solid foundation for future explorations.
</details>
<details>
<summary>摘要</summary>
大多数现有多模态模型受到其不能够有效地处理交错图像和文本输入的限制，在多图多轮对话中受到资源分配和数据可accessibility的限制，影响其适应性和扩展性。为解决这一问题，我们提出了DeepSpeed-VisualChat框架，旨在优化大型语言模型，通过多模态能力提高大型语言和视觉模型对交错输入的处理能力。我们的框架具有以下三个特点：1. 支持多轮多图对话的开源实现，以便实现无缝的多模态对话。2. 引入创新的多模态 causal attention机制，以提高模型对交错输入的处理能力。3. 通过使用现有数据集的混合技术，保证多轮多图对话中的无缝交互。与现有框架相比，DeepSpeed-VisualChat显示出比较出色的扩展性，可以 Handle up to 70B parameter language model size，代表着多模态语言模型的显著进步，并为未来的探索提供了坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Purpose-Text-Instruction-Guided-Voice-Conversion"><a href="#Towards-General-Purpose-Text-Instruction-Guided-Voice-Conversion" class="headerlink" title="Towards General-Purpose Text-Instruction-Guided Voice Conversion"></a>Towards General-Purpose Text-Instruction-Guided Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14324">http://arxiv.org/abs/2309.14324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/text-guided-vc/text-guided-vc.github.io">https://github.com/text-guided-vc/text-guided-vc.github.io</a></li>
<li>paper_authors: Chun-Yi Kuan, Chen An Li, Tsu-Yuan Hsu, Tse-Yang Lin, Ho-Lam Chung, Kai-Wei Chang, Shuo-yiin Chang, Hung-yi Lee</li>
<li>for: 这篇论文旨在描述一种新的语音转换（VC）模型，该模型根据文本指令（如“诠释慢速深音”或“说话带着幽默boyish音色”）进行指导。与传统方法不同的是，我们的模型可以根据文本指令来修改转换后的语音的谐音和情感信息，从而提供更多的灵活性和具体性。</li>
<li>methods: 该VC模型是一种基于神经网络编码语言模型的，它处理一个序列的精度码，并将其转换为转换后的语音序列。该模型使用文本指令作为样式提示，以修改源语音的不同方面。与之前的方法不同的是，我们的模型可以在端到端方式下处理不同方面的语音信息，而不需要分别使用多个编码器。</li>
<li>results: 实验表明，我们的模型能够很好地理解指令，并提供合理的转换结果。<details>
<summary>Abstract</summary>
This paper introduces a novel voice conversion (VC) model, guided by text instructions such as "articulate slowly with a deep tone" or "speak in a cheerful boyish voice". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Urdu-Poetry-Generated-by-Using-Deep-Learning-Techniques"><a href="#Urdu-Poetry-Generated-by-Using-Deep-Learning-Techniques" class="headerlink" title="Urdu Poetry Generated by Using Deep Learning Techniques"></a>Urdu Poetry Generated by Using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14233">http://arxiv.org/abs/2309.14233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Shoaib Farooq, Ali Abbas</li>
<li>For: 本研究提供了使用不同深度学习技术和算法生成的 Urdu 诗歌。* Methods: 该研究使用了 Long Short-term Memory Networks (LSTM) 和 Gated Recurrent Unit (GRU) 等深度学习模型，以及自然语言处理 (NLP) 技术来理解、分析和生成人类可以理解和使用的语言。* Results: 研究结果表明，使用这些技术可以生成具有高准确性的 Urdu 诗歌。<details>
<summary>Abstract</summary>
This study provides Urdu poetry generated using different deep-learning techniques and algorithms. The data was collected through the Rekhta website, containing 1341 text files with several couplets. The data on poetry was not from any specific genre or poet. Instead, it was a collection of mixed Urdu poems and Ghazals. Different deep learning techniques, such as the model applied Long Short-term Memory Networks (LSTM) and Gated Recurrent Unit (GRU), have been used. Natural Language Processing (NLP) may be used in machine learning to understand, analyze, and generate a language humans may use and understand. Much work has been done on generating poetry for different languages using different techniques. The collection and use of data were also different for different researchers. The primary purpose of this project is to provide a model that generates Urdu poems by using data completely, not by sampling data. Also, this may generate poems in pure Urdu, not Roman Urdu, as in the base paper. The results have shown good accuracy in the poems generated by the model.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Autonomous-Vehicles-an-overview-on-system-cyber-security-risks-issues-and-a-way-forward"><a href="#Autonomous-Vehicles-an-overview-on-system-cyber-security-risks-issues-and-a-way-forward" class="headerlink" title="Autonomous Vehicles an overview on system, cyber security, risks, issues, and a way forward"></a>Autonomous Vehicles an overview on system, cyber security, risks, issues, and a way forward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14213">http://arxiv.org/abs/2309.14213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Aminul Islam, Sarah Alqahtani<br>for:这篇论文主要是为了探讨自动驾驶车的基本组件和运行特点，以及它们如何在互联网的框架下集成。methods:该论文使用了感知器、人工智能标识系统、控制机制等技术，并将其与云计算服务器集成在一起。results:该论文探讨了自动驾驶车在交通预测和交通预测等领域的实践应用，以及它们对不同产业的自动化任务的影响。同时，它还探讨了自动驾驶车的安全性问题，包括伦理、环境、法律、职业和社会方面的风险。<details>
<summary>Abstract</summary>
This chapter explores the complex realm of autonomous cars, analyzing their fundamental components and operational characteristics. The initial phase of the discussion is elucidating the internal mechanics of these automobiles, encompassing the crucial involvement of sensors, artificial intelligence (AI) identification systems, control mechanisms, and their integration with cloud-based servers within the framework of the Internet of Things (IoT). It delves into practical implementations of autonomous cars, emphasizing their utilization in forecasting traffic patterns and transforming the dynamics of transportation. The text also explores the topic of Robotic Process Automation (RPA), illustrating the impact of autonomous cars on different businesses through the automation of tasks. The primary focus of this investigation lies in the realm of cybersecurity, specifically in the context of autonomous vehicles. A comprehensive analysis will be conducted to explore various risk management solutions aimed at protecting these vehicles from potential threats including ethical, environmental, legal, professional, and social dimensions, offering a comprehensive perspective on their societal implications. A strategic plan for addressing the challenges and proposing strategies for effectively traversing the complex terrain of autonomous car systems, cybersecurity, hazards, and other concerns are some resources for acquiring an understanding of the intricate realm of autonomous cars and their ramifications in contemporary society, supported by a comprehensive compilation of resources for additional investigation.   Keywords: RPA, Cyber Security, AV, Risk, Smart Cars
</details>
<details>
<summary>摘要</summary>
The primary focus of this investigation lies in the realm of cybersecurity, specifically in the context of autonomous vehicles. A comprehensive analysis will be conducted to explore various risk management solutions aimed at protecting these vehicles from potential threats, including ethical, environmental, legal, professional, and social dimensions. This will provide a comprehensive perspective on their societal implications.In addition, a strategic plan for addressing the challenges and proposing strategies for effectively traversing the complex terrain of autonomous car systems, cybersecurity, hazards, and other concerns will be presented. This will be supported by a comprehensive compilation of resources for additional investigation.Keywords: RPA, Cyber Security, AV, Risk, Smart Cars
</details></li>
</ul>
<hr>
<h2 id="Only-5-Attention-Is-All-You-Need-Efficient-Long-range-Document-level-Neural-Machine-Translation"><a href="#Only-5-Attention-Is-All-You-Need-Efficient-Long-range-Document-level-Neural-Machine-Translation" class="headerlink" title="Only 5% Attention Is All You Need: Efficient Long-range Document-level Neural Machine Translation"></a>Only 5% Attention Is All You Need: Efficient Long-range Document-level Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14174">http://arxiv.org/abs/2309.14174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Liu, Zewei Sun, Shanbo Cheng, Shujian Huang, Mingxuan Wang</li>
<li>for:  DocNMT for handling discourse phenomena in Machine Translation tasks, with the goal of improving efficiency while maintaining performance.</li>
<li>methods: The paper introduces a lightweight attention mechanism to select a small portion of tokens to be attended, reducing the computational cost of the attention module while maintaining performance.</li>
<li>results: The method achieves up to 95% sparsity (only 5% tokens attended) and saves 93% computation cost on the attention module compared to the original Transformer, while maintaining performance.<details>
<summary>Abstract</summary>
Document-level Neural Machine Translation (DocNMT) has been proven crucial for handling discourse phenomena by introducing document-level context information. One of the most important directions is to input the whole document directly to the standard Transformer model. In this case, efficiency becomes a critical concern due to the quadratic complexity of the attention module. Existing studies either focus on the encoder part, which cannot be deployed on sequence-to-sequence generation tasks, e.g., Machine Translation (MT), or suffer from a significant performance drop. In this work, we keep the translation performance while gaining 20\% speed up by introducing extra selection layer based on lightweight attention that selects a small portion of tokens to be attended. It takes advantage of the original attention to ensure performance and dimension reduction to accelerate inference. Experimental results show that our method could achieve up to 95\% sparsity (only 5\% tokens attended) approximately, and save 93\% computation cost on the attention module compared with the original Transformer, while maintaining the performance.
</details>
<details>
<summary>摘要</summary>
文档水平神经机器翻译（DocNMT）已经被证明是处理讨论现象的关键，通过引入文档级别的上下文信息。一个重要的方向是直接将整个文档输入到标准变换器模型中。在这种情况下，效率成为一个关键问题，因为变换器模型的注意模块的复杂度是二次的。现有的研究 either ocus 在encoder部分，无法在序列到序列生成任务中使用，例如机器翻译（MT），或者受到 significativ performance drop。在这种工作中，我们保持翻译性能，同时减少了20%的计算成本，通过引入附加的选择层，选择一小部分的Token进行注意。它利用原始注意来确保性能，并将维度减少以加速推理。实验结果表明，我们的方法可以达到约95%的稀疏性（只有5%的Token被注意），并将93%的计算成本减少在变换器模型中，同时保持性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-End-User-Development-for-IoT-A-Case-Study-on-Semantic-Parsing-of-Cooking-Recipes-for-Programming-Kitchen-Devices"><a href="#Towards-End-User-Development-for-IoT-A-Case-Study-on-Semantic-Parsing-of-Cooking-Recipes-for-Programming-Kitchen-Devices" class="headerlink" title="Towards End-User Development for IoT: A Case Study on Semantic Parsing of Cooking Recipes for Programming Kitchen Devices"></a>Towards End-User Development for IoT: A Case Study on Semantic Parsing of Cooking Recipes for Programming Kitchen Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14165">http://arxiv.org/abs/2309.14165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filipposventirozos/towards-end-user-development-for-iot">https://github.com/filipposventirozos/towards-end-user-development-for-iot</a></li>
<li>paper_authors: Filippos Ventirozos, Sarah Clinch, Riza Batista-Navarro</li>
<li>for: 支持烹饪recipe instructions的自然语言编程</li>
<li>methods: 使用 conditional random fields (CRF) 和神经网络模型进行语义分析</li>
<li>results: training semantic parsers based on annotations is feasible, but most natural-language instructions are incomplete and need to be transformed into formal meaning representation.Here’s the breakdown of each piece of information:</li>
<li>for: 支持烹饪recipe instructions的自然语言编程 (What the paper is written for)</li>
<li>methods: 使用 conditional random fields (CRF) 和神经网络模型进行语义分析 (What methods the paper uses)</li>
<li>results: training semantic parsers based on annotations is feasible, but most natural-language instructions are incomplete and need to be transformed into formal meaning representation. (What results the paper gets)<details>
<summary>Abstract</summary>
Semantic parsing of user-generated instructional text, in the way of enabling end-users to program the Internet of Things (IoT), is an underexplored area. In this study, we provide a unique annotated corpus which aims to support the transformation of cooking recipe instructions to machine-understandable commands for IoT devices in the kitchen. Each of these commands is a tuple capturing the semantics of an instruction involving a kitchen device in terms of "What", "Where", "Why" and "How". Based on this corpus, we developed machine learning-based sequence labelling methods, namely conditional random fields (CRF) and a neural network model, in order to parse recipe instructions and extract our tuples of interest from them. Our results show that while it is feasible to train semantic parsers based on our annotations, most natural-language instructions are incomplete, and thus transforming them into formal meaning representation, is not straightforward.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Semantic parsing of user-generated instructional text, in the way of enabling end-users to program the Internet of Things (IoT), is an underexplored area. In this study, we provide a unique annotated corpus which aims to support the transformation of cooking recipe instructions to machine-understandable commands for IoT devices in the kitchen. Each of these commands is a tuple capturing the semantics of an instruction involving a kitchen device in terms of "What", "Where", "Why" and "How". Based on this corpus, we developed machine learning-based sequence labelling methods, namely conditional random fields (CRF) and a neural network model, in order to parse recipe instructions and extract our tuples of interest from them. Our results show that while it is feasible to train semantic parsers based on our annotations, most natural-language instructions are incomplete, and thus transforming them into formal meaning representation, is not straightforward.中文翻译：用户生成的 instrucitonal text 的 semantics parsing，以实现终端用户对 Internet of Things (IoT) 的程式设定，是一个未得到充分探讨的领域。在这个研究中，我们提供了一个唯一的标注集，以支持将烹饪recipe的 instrucitons 转换为机器可理解的命令，并且每个命令都是一个捕捉烹饪 instruciton 的含义的 tuple，包括 "What"、"Where"、"Why" 和 "How"。基于这个标注集，我们开发了机器学习基于条件随机场 (CRF) 和神经网络模型，以解析 recipe instrucitons 并将我们的感兴趣 tuple 提取出来。我们的结果显示，可以对我们的标注集进行训练，但大多数自然语言 instrucitons 是不完整的，因此将它们转换为正式的意义表现，不是一个 straightforward 的任务。
</details></li>
</ul>
<hr>
<h2 id="Examining-Temporal-Bias-in-Abusive-Language-Detection"><a href="#Examining-Temporal-Bias-in-Abusive-Language-Detection" class="headerlink" title="Examining Temporal Bias in Abusive Language Detection"></a>Examining Temporal Bias in Abusive Language Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14146">http://arxiv.org/abs/2309.14146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mali Jin, Yida Mu, Diana Maynard, Kalina Bontcheva<br>for: This study aims to investigate the nature and impact of temporal bias in abusive language detection across various languages and explore mitigation methods.methods: The study evaluates the performance of models on abusive data sets from different time periods and presents an extensive linguistic analysis of these abusive data sets from a diachronic perspective.results: The results demonstrate that temporal bias is a significant challenge for abusive language detection, with models trained on historical data showing a significant drop in performance over time.<details>
<summary>Abstract</summary>
The use of abusive language online has become an increasingly pervasive problem that damages both individuals and society, with effects ranging from psychological harm right through to escalation to real-life violence and even death. Machine learning models have been developed to automatically detect abusive language, but these models can suffer from temporal bias, the phenomenon in which topics, language use or social norms change over time. This study aims to investigate the nature and impact of temporal bias in abusive language detection across various languages and explore mitigation methods. We evaluate the performance of models on abusive data sets from different time periods. Our results demonstrate that temporal bias is a significant challenge for abusive language detection, with models trained on historical data showing a significant drop in performance over time. We also present an extensive linguistic analysis of these abusive data sets from a diachronic perspective, aiming to explore the reasons for language evolution and performance decline. This study sheds light on the pervasive issue of temporal bias in abusive language detection across languages, offering crucial insights into language evolution and temporal bias mitigation.
</details>
<details>
<summary>摘要</summary>
互联网上的辱语问题日益普遍，对个人和社会造成心理副作用、实际暴力和even death的影响。机器学习模型已经开发出来自动检测辱语，但这些模型可能会受到时间偏见的影响，时间偏见是指语言、语言使用或社会规范随着时间的变化。本研究旨在研究辱语检测中的时间偏见问题，以及不同语言下的时间偏见的影响。我们对不同时间段的辱语数据集进行了评估，结果显示，时间偏见是辱语检测中的一大挑战，历史数据上训练的模型表现下降显著。此外，我们还进行了对这些辱语数据集的广泛语言分析，尝试探讨语言演化的原因和表现下降的原因。这项研究突出了辱语检测中时间偏见的问题，为语言演化和时间偏见缓解提供了关键的洞察。
</details></li>
</ul>
<hr>
<h2 id="On-the-Relation-between-Internal-Language-Model-and-Sequence-Discriminative-Training-for-Neural-Transducers"><a href="#On-the-Relation-between-Internal-Language-Model-and-Sequence-Discriminative-Training-for-Neural-Transducers" class="headerlink" title="On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers"></a>On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14130">http://arxiv.org/abs/2309.14130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Yang, Wei Zhou, Ralf Schlüter, Hermann Ney</li>
<li>for: 提高 RNN-Transducer 的表现，使用外部语言模型 (LM) 融合</li>
<li>methods: 使用序列推理训练，并对 ILM 进行减法</li>
<li>results: 序列推理训练和 ILM 减法在 Librispeech 上的各种实验中具有类似的表现，包括 MMI 和 MBR 等标准。减法对 ILM 的影响也变得较小。<details>
<summary>Abstract</summary>
Internal language model (ILM) subtraction has been widely applied to improve the performance of the RNN-Transducer with external language model (LM) fusion for speech recognition. In this work, we show that sequence discriminative training has a strong correlation with ILM subtraction from both theoretical and empirical points of view. Theoretically, we derive that the global optimum of maximum mutual information (MMI) training shares a similar formula as ILM subtraction. Empirically, we show that ILM subtraction and sequence discriminative training achieve similar performance across a wide range of experiments on Librispeech, including both MMI and minimum Bayes risk (MBR) criteria, as well as neural transducers and LMs of both full and limited context. The benefit of ILM subtraction also becomes much smaller after sequence discriminative training. We also provide an in-depth study to show that sequence discriminative training has a minimal effect on the commonly used zero-encoder ILM estimation, but a joint effect on both encoder and prediction + joint network for posterior probability reshaping including both ILM and blank suppression.
</details>
<details>
<summary>摘要</summary>
内部语言模型（ILM）减法广泛应用于改进RNN-Transducer的语音识别性能，在这项工作中，我们表明了序列推理训练与ILM减法之间存在强相关性。从理论上来看，我们得出了最大共识度（MMI）训练的全球最优点与ILM减法的相似公式。从实验来看，我们证明了ILM减法和序列推理训练在Librispeech上覆盖广泛的实验中具有相似的性能，包括MMI和最小 bayes风险（MBR） критериria，以及神经转移和LM的全文和有限文本上的性能。ILM减法的利益也变得很小之后进行序列推理训练。我们还进行了深入的研究，发现序列推理训练对于通常使用零编码ILM估计的影响非常小，但对于encoder和预测+联合网络进行 posterior probability重塑，包括ILM和空白抑制，有 JOINT 效果。
</details></li>
</ul>
<hr>
<h2 id="Wav2vec-based-Detection-and-Severity-Level-Classification-of-Dysarthria-from-Speech"><a href="#Wav2vec-based-Detection-and-Severity-Level-Classification-of-Dysarthria-from-Speech" class="headerlink" title="Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech"></a>Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14107">http://arxiv.org/abs/2309.14107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farhad Javanmardi, Saska Tirronen, Manila Kodali, Sudarsana Reddy Kadiri, Paavo Alku</li>
<li>for: 这个研究旨在使用自动检测和评估瘫疡症患者的语音信号，以便在医疗诊断中使用。</li>
<li>methods: 这个研究使用了预训练的wav2vec 2.0模型作为特征提取器，建立检测和评估瘫疡症语音的系统。</li>
<li>results: 实验结果显示，使用wav2vec 2.0模型的对应层 embeddings（第一层）可以实现最佳的检测性能，相比基准模型（spectrogram）的最高表现提高1.23%的精度。在研究的评估瘫疡症严重程度分类任务中，使用最终层 embeddings 可以实现10.62%的精度提高，相比基准特征（mel-frequency cepstral coefficients）。<details>
<summary>Abstract</summary>
Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).
</details>
<details>
<summary>摘要</summary>
自动检测和评估瘫疡程度可以将单词识别和瘫疡程度分类 directly from acoustic speech signals 用作医疗诊断工具。在这个工作中，预训练的 wav2vec 2.0 模型被研究作为特征提取器，以建立检测和瘫疡程度分类系统。实验使用了常用的 UA-speech 数据库。在检测实验中，结果显示，使用 wav2vec 模型的第一层嵌入得到最佳表现，对比基准特征（spectrogram）的最佳表现，获得了绝对提升1.23%的准确度。在研究的瘫疡程度分类任务中，结果显示，使用 wav2vec 模型的最终层嵌入得到最佳表现，与基准特征（mel-frequency cepstral coefficients）的最佳表现相比，获得了绝对提升10.62%的准确度。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Detection-of-Pathological-Voice-using-Glottal-Source-Features"><a href="#Analysis-and-Detection-of-Pathological-Voice-using-Glottal-Source-Features" class="headerlink" title="Analysis and Detection of Pathological Voice using Glottal Source Features"></a>Analysis and Detection of Pathological Voice using Glottal Source Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14080">http://arxiv.org/abs/2309.14080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Paavo Alku</li>
<li>for: 该研究旨在对声音疾病进行自动检测，以提供 объектив评估和早期 intervención 的 диагности方法。</li>
<li>methods: 该研究使用 quasi-closed phase (QCP) glottal inverse filtering 方法 estimate glottal flows，并使用 zero frequency filtering (ZFF) 方法计算 approximate glottal source signals，以及直接使用声音信号。此外，研究还提出了 derivate mel-frequency cepstral coefficients (MFCCs) from glottal source waveforms computed by QCP和ZFF，以具体地捕捉 glottal source 谱的变化。</li>
<li>results: 研究结果表明，glottal source 特征含有可以区分正常和疾病声音的信息。通过支持向量机 (SVM) 进行检测试验，发现 studied glottal source 特征可以与 convential MFCCs 和 perceptual linear prediction (PLP) 特征相比，达到了同等或更好的检测性能。此外，combine glottal source 特征与 convential MFCCs 和 PLP 特征可以获得最佳的检测性能，这表明这些特征之间存在辅助性的关系。<details>
<summary>Abstract</summary>
Automatic detection of voice pathology enables objective assessment and earlier intervention for the diagnosis. This study provides a systematic analysis of glottal source features and investigates their effectiveness in voice pathology detection. Glottal source features are extracted using glottal flows estimated with the quasi-closed phase (QCP) glottal inverse filtering method, using approximate glottal source signals computed with the zero frequency filtering (ZFF) method, and using acoustic voice signals directly. In addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from the glottal source waveforms computed by QCP and ZFF to effectively capture the variations in glottal source spectra of pathological voice. Experiments were carried out using two databases, the Hospital Universitario Principe de Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database. Analysis of features revealed that the glottal source contains information that discriminates normal and pathological voice. Pathology detection experiments were carried out using support vector machine (SVM). From the detection experiments it was observed that the performance achieved with the studied glottal source features is comparable or better than that of conventional MFCCs and perceptual linear prediction (PLP) features. The best detection performance was achieved when the glottal source features were combined with the conventional MFCCs and PLP features, which indicates the complementary nature of the features.
</details>
<details>
<summary>摘要</summary>
自动检测声道疾病可以提供对象评估和早期 intervención для诊断。本研究提供了声道疾病检测中频谱源特征的系统性分析，并investigates其效iveness。频谱源特征通过预计closed phase（QCP）glottal inverse filtering方法、零频率 filtering（ZFF）方法和直接使用声音信号来提取。此外，我们提议 derivation of mel-frequency cepstral coefficients（MFCCs）from the glottal source waveforms computed by QCP和ZFF，以有效捕捉声道源спектrum的变化。实验使用了两个数据库，大学医院主楼（HUPA）数据库和 saarbrucken voice disorders（SVD）数据库。特征分析表明，频谱源含有可以区分正常和疾病声音的信息。疾病检测实验使用支持向量机（SVM）。从检测实验中，我们发现，研究中的频谱源特征表现比或更好于 conventioml MFCCs和perceptual linear prediction（PLP）特征。最佳检测性能是在glottal source特征与conventioml MFCCs和PLP特征结合时得到的，这表明这些特征之间存在衔接关系。
</details></li>
</ul>
<hr>
<h2 id="Multiple-evolutionary-pressures-shape-identical-consonant-avoidance-in-the-world’s-languages"><a href="#Multiple-evolutionary-pressures-shape-identical-consonant-avoidance-in-the-world’s-languages" class="headerlink" title="Multiple evolutionary pressures shape identical consonant avoidance in the world’s languages"></a>Multiple evolutionary pressures shape identical consonant avoidance in the world’s languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14006">http://arxiv.org/abs/2309.14006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chundra A. Cathcart</li>
<li>for: 本研究探讨语言演化中同音字符的出现频率是否受限制，以及这些限制的来源。</li>
<li>methods: 研究者使用phylogenetic分析方法，对同义词的演化进行比较分析，探讨语言演化过程中同音字符的出现频率和word form mutation的影响。</li>
<li>results: 研究发现，同音字符在词形变化中更容易消失，而非出现。此外，同音字符的出现频率较低，word form mutation也更可能将同音字符消除。但同时发现，同音字符不会更容易消失。结论是，同音字符的出现频率受到语言演化的限制，但这些限制不来自于语言使用者的选择。<details>
<summary>Abstract</summary>
Languages disfavor word forms containing sequences of similar or identical consonants, due to the biomechanical and cognitive difficulties posed by patterns of this sort. However, the specific evolutionary processes responsible for this phenomenon are not fully understood. Words containing sequences of identical consonants may be more likely to arise than those without; processes of word form mutation may be more likely to remove than create sequences of identical consonants in word forms; finally, words containing identical consonants may die out more frequently than those without. Phylogenetic analyses of the evolution of homologous word forms indicate that words with identical consonants arise less frequently than those without, and processes which mutate word forms are more likely to remove sequences of identical consonants than introduce them. However, words with identical consonants do not die out more frequently than those without. Further analyses reveal that forms with identical consonants are replaced in basic meaning functions more frequently than words without. Taken together, results suggest that the under representation of sequences of identical consonants is overwhelmingly a byproduct of constraints on word form coinage, though processes related to word usage also serve to ensure that such patterns are infrequent in more salient vocabulary items. These findings clarify previously unknown aspects of processes of lexical evolution and competition that take place during language change, optimizing communicative systems.
</details>
<details>
<summary>摘要</summary>
语言偏远同辅音序列，因为这些模式带来生物机械和认知上的困难。然而，这种现象的具体演化过程仍未完全了解。words containing sequences of identical consonants may be more likely to arise than those without; processes of word form mutation may be more likely to remove than create sequences of identical consonants in word forms; finally, words containing identical consonants may die out more frequently than those without.phylogenetic analyses of the evolution of homologous word forms indicate that words with identical consonants arise less frequently than those without, and processes which mutate word forms are more likely to remove sequences of identical consonants than introduce them. however, words with identical consonants do not die out more frequently than those without. further analyses reveal that forms with identical consonants are replaced in basic meaning functions more frequently than words without. taken together, results suggest that the under representation of sequences of identical consonants is overwhelmingly a byproduct of constraints on word form coinage, though processes related to word usage also serve to ensure that such patterns are infrequent in more salient vocabulary items. these findings clarify previously unknown aspects of processes of lexical evolution and competition that take place during language change, optimizing communicative systems.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR"><a href="#Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR" class="headerlink" title="Connecting Speech Encoder and Large Language Model for ASR"></a>Connecting Speech Encoder and Large Language Model for ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13963">http://arxiv.org/abs/2309.13963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang</li>
<li>for: 本研究旨在比较三种常用的结构，包括完全连接层、多头交叉注意力和Q-Former，以实现自动语音识别（ASR）模型的集成。</li>
<li>methods: 研究使用了Whisper模型系列的语音编码器和Vicuna模型系列的不同模型大小的LLMs，并在LibriSpeech、Common Voice和GigaSpeech datasets上进行了实验。</li>
<li>results: Q-Former-based LLMs在不同数据集上显示了一致和显著的单词错误率（WER）减少，相比其他结构。此外，一种新的段级Q-Former也被提出，使LLMs可以识别长于编码器限制的语音段，带来17%的相对WER减少。<details>
<summary>Abstract</summary>
The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的印象力和多方面性在自动话语识别（ASR）中受到了越来越多的关注，有几个先锋性研究尝试建立了 integrate ASR 模型，通过与话语编码器连接。这篇文章发表了三种常用的结构，包括完全连接层、多头标注和Q-Former 的比较研究。研究使用了 Whisper 模型系列的话语编码器和 Vicuna 模型系列的不同模型大小的 LLM，并在 LibriSpeech、Common Voice 和 GigaSpeech  datasets 上进行实验。实验结果显示，使用 Q-Former 的 LLM 可以在不使用域内训练数据的情况下，实现了12% 的相对 palabier error rate（WER）降低，相比其他结构。此外，一个新的段级 Q-Former 被提议，允许 LLM 识别长度超过编码器限制的语音段，从而实现了17% 的相对 WER 降低。
</details></li>
</ul>
<hr>
<h2 id="Reproducing-Whisper-Style-Training-Using-an-Open-Source-Toolkit-and-Publicly-Available-Data"><a href="#Reproducing-Whisper-Style-Training-Using-an-Open-Source-Toolkit-and-Publicly-Available-Data" class="headerlink" title="Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data"></a>Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13876">http://arxiv.org/abs/2309.13876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/espnet/espnet">https://github.com/espnet/espnet</a></li>
<li>paper_authors: Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe</li>
<li>for: 该论文的目的是开发一个开源的听说模型，以便于研究人员可以在开源的工具kit和公共可用的数据上进行训练和改进。</li>
<li>methods: 该论文使用的方法是基于开源的工具kit和公共可用的数据进行听说模型的训练，并支持更多的翻译方向。</li>
<li>results: 该论文可以在零shot设置下实现良好的一致性和翻译性，并且可以在训练过程中提高效率和稳定性。<details>
<summary>Abstract</summary>
Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science.
</details>
<details>
<summary>摘要</summary>
<SYS>Translate the given text into Simplified Chinese.</SYS>预训术语模型在大量数据上已经取得了很大成功。OpenAI Whisper是一个多语言多任务模型，在680k小时的监督术语数据上训练。它在不同的术语识别和翻译 bencmarks 中进行了良好的泛化，甚至在零shot setup 下也能达到良好的性能。然而，整个模型开发管道（从数据收集到训练）没有公开 accessible，这使得研究人员很难进一步改进其性能和 Address training-related issues such as efficiency, robustness, fairness, and bias。这项工作提出了一个 Open Whisper-style Speech Model (OWSM)，该模型通过使用开源工具包和公开可用的数据来重现 Whisper-style 训练。OWSM 还支持更多的翻译方向，并且可以更高效地训练。我们将公开所有数据准备、训练、推理和评分脚本以及预训练模型和训练日志，以便推动开放科学。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/cs.CL_2023_09_25/" data-id="clpztdne600cees883wrl2yvo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/cs.LG_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T10:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/cs.LG_2023_09_25/">cs.LG - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Understanding-the-Structure-of-QM7b-and-QM9-Quantum-Mechanical-Datasets-Using-Unsupervised-Learning"><a href="#Understanding-the-Structure-of-QM7b-and-QM9-Quantum-Mechanical-Datasets-Using-Unsupervised-Learning" class="headerlink" title="Understanding the Structure of QM7b and QM9 Quantum Mechanical Datasets Using Unsupervised Learning"></a>Understanding the Structure of QM7b and QM9 Quantum Mechanical Datasets Using Unsupervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15130">http://arxiv.org/abs/2309.15130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julio J. Valdés, Alain B. Tchagang</li>
<li>for: 本研究探讨了量子机制数据集（QM7b、QM9）的内部结构，它们包含了数量级的有机分子，并通过电子性质来描述。了解这类数据的结构和特点对于预测分子组成是非常重要。</li>
<li>methods: 本研究使用了内在维度分析、聚类和异常检测方法来研究QM7b和QM9数据集。结果显示，QM7b数据集具有清晰定义的集群，与原子组成直接相关。QM9数据集则有一个外围区域主要由异常点组成，以及一个内部核心区域集中的线性对象。与分子大小有直接关系的关系存在于这两个数据集中。</li>
<li>results:  despite the structural differences between the two datasets, the predictability of variables of interest for inverse molecular design is high. This is exemplified with models estimating the number of atoms of the molecule from both the original properties and from lower dimensional embedding spaces.<details>
<summary>Abstract</summary>
This paper explores the internal structure of two quantum mechanics datasets (QM7b, QM9), composed of several thousands of organic molecules and described in terms of electronic properties. Understanding the structure and characteristics of this kind of data is important when predicting the atomic composition from the properties in inverse molecular designs. Intrinsic dimension analysis, clustering, and outlier detection methods were used in the study. They revealed that for both datasets the intrinsic dimensionality is several times smaller than the descriptive dimensions. The QM7b data is composed of well defined clusters related to atomic composition. The QM9 data consists of an outer region predominantly composed of outliers, and an inner core region that concentrates clustered, inliner objects. A significant relationship exists between the number of atoms in the molecule and its outlier/inner nature. Despite the structural differences, the predictability of variables of interest for inverse molecular design is high. This is exemplified with models estimating the number of atoms of the molecule from both the original properties, and from lower dimensional embedding spaces.
</details>
<details>
<summary>摘要</summary>
The QM7b dataset is composed of well-defined clusters related to atomic composition, while the QM9 dataset has an outer region primarily consisting of outliers and an inner core region with clustered, linear objects. There is a significant correlation between the number of atoms in the molecule and its outlier/inner nature. Despite the structural differences, the predictability of variables of interest for inverse molecular design is high, as demonstrated by models estimating the number of atoms of the molecule from both the original properties and lower-dimensional embedding spaces.
</details></li>
</ul>
<hr>
<h2 id="Towards-a-statistical-theory-of-data-selection-under-weak-supervision"><a href="#Towards-a-statistical-theory-of-data-selection-under-weak-supervision" class="headerlink" title="Towards a statistical theory of data selection under weak supervision"></a>Towards a statistical theory of data selection under weak supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14563">http://arxiv.org/abs/2309.14563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Germain Kolossov, Andrea Montanari, Pulkit Tandon</li>
<li>for: 选择一个小于原始样本大小的子样本，以优化数据预处理和机器学习计算复杂性。</li>
<li>methods: 使用代理模型预测样本标签，然后选择一个子样本，并使用这些标签进行模型训练。</li>
<li>results: 数据选择可以非常有效，在某些情况下甚至可以超过使用整个样本集来训练模型。另外，一些受欢迎的数据选择方法（如偏向重样本或影响函数基于的子样本选择）可能会很差。<details>
<summary>Abstract</summary>
Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{\boldsymbol x}_i\}_{i\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\{\boldsymbol x}_i\}_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.   By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selection methods (e.g. unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.
</details>
<details>
<summary>摘要</summary>
Given a sample size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step can reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{\mathbf{x}_i\}_{i\le N}$, and to be given access to a 'surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, denoted by $\{\mathbf{x}_i\}_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and use them to train a model via regularized empirical risk minimization.By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high-dimensional asymptotics, we show that:$(i)$ Data selection can be very effective, in particular beating training on the full sample in some cases;$(ii)$ Certain popular choices in data selection methods (e.g., unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.
</details></li>
</ul>
<hr>
<h2 id="Disruption-Detection-for-a-Cognitive-Digital-Supply-Chain-Twin-Using-Hybrid-Deep-Learning"><a href="#Disruption-Detection-for-a-Cognitive-Digital-Supply-Chain-Twin-Using-Hybrid-Deep-Learning" class="headerlink" title="Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning"></a>Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14557">http://arxiv.org/abs/2309.14557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Ashraf, Amr Eltawil, Islam Ali</li>
<li>For: The paper aims to provide an effective and efficient tool for mitigating the impact of disruptive events on global supply chains by introducing a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework.* Methods: The proposed approach uses a deep autoencoder neural network combined with a one-class support vector machine algorithm to detect disruptions in real-time. Long-short term memory neural network models are also developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect.* Results: The proposed approach can help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing negative impact of disruptive events based on real-time disruption detection data. The results demonstrate the trade-off between disruption detection model sensitivity, encountered delay in disruption detection, and false alarms.Here are the three key points in Simplified Chinese text:</li>
<li>for: 本文目的是提供一种有效和高效的工具，以减轻突发事件对全球供应链的影响。</li>
<li>methods: 该方法使用深度自适应神经网络与一类支持向量机算法检测干扰。此外，使用长短期记忆神经网络模型识别受影响的echelon，预测干扰影响的时间恢复。</li>
<li>results: 该方法可以帮助决策者和供应链实践者根据实时干扰检测数据进行合适的决策，以减轻突发事件的负面影响。结果表明干扰检测模型的敏感度、遇到延迟的干扰检测和假阳性的负面关系。这种方法在当前文献中很少被使用。<details>
<summary>Abstract</summary>
Purpose: Recent disruptive events, such as COVID-19 and Russia-Ukraine conflict, had a significant impact of global supply chains. Digital supply chain twins have been proposed in order to provide decision makers with an effective and efficient tool to mitigate disruption impact. Methods: This paper introduces a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework to enhance supply chain resilience. The proposed disruption detection module utilises a deep autoencoder neural network combined with a one-class support vector machine algorithm. In addition, long-short term memory neural network models are developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect. Results: The obtained information from the proposed approach will help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing negative impact of disruptive events based on real-time disruption detection data. The results demonstrate the trade-off between disruption detection model sensitivity, encountered delay in disruption detection, and false alarms. This approach has seldom been used in recent literature addressing this issue.
</details>
<details>
<summary>摘要</summary>
目的：latest disruptive events, such as COVID-19 and Russia-Ukraine conflict, have had a significant impact on global supply chains. Digital supply chain twins have been proposed to provide decision makers with an effective and efficient tool to mitigate the impact of disruptions.方法：this paper introduces a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework to enhance supply chain resilience. The proposed disruption detection module uses a deep autoencoder neural network combined with a one-class support vector machine algorithm. In addition, long-short term memory neural network models are developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect.结果：the obtained information from the proposed approach will help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing the negative impact of disruptive events based on real-time disruption detection data. The results demonstrate the trade-off between disruption detection model sensitivity, encountered delay in disruption detection, and false alarms. This approach has seldom been used in recent literature addressing this issue.Here's the translation in Traditional Chinese:目的：最近的干扰事件，如COVID-19和俄乌战争，对全球供应链造成了巨大的影响。数字供应链双生物被提议，以提供决策者具有更高效和更高效的工具，以mitigate干扰的影响。方法：本篇文章介绍了一种混合深度学习方法，用于干扰检测在认知数字供应链双生物框架中，以提高供应链可靠性。提议的干扰检测模组使用深度自动Encoder神经网络，与一类支持向量机器学习算法结合。此外，长期快速传统神经网络模型也被开发，以识别受到干扰的层次，并预测干扰影响的时间回复。结果：取得的信息将助决策者和供应链实践者做出适当的决策，以减少干扰事件的负面影响。结果显示出干扰检测模型的敏感度、遭遇延误的干扰检测时间和误干扰的负面影响之间的贸易。这种方法在最近的文献中 rarely 被使用以解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="Cluster-based-Method-for-Eavesdropping-Identification-and-Localization-in-Optical-Links"><a href="#Cluster-based-Method-for-Eavesdropping-Identification-and-Localization-in-Optical-Links" class="headerlink" title="Cluster-based Method for Eavesdropping Identification and Localization in Optical Links"></a>Cluster-based Method for Eavesdropping Identification and Localization in Optical Links</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14541">http://arxiv.org/abs/2309.14541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Song, Rui Lin, Andrea Sgambelluri, Filippo Cugini, Yajie Li, Jie Zhang, Paolo Monti</li>
<li>for: 检测和定位光线系统中的窃听事件</li>
<li>methods: 基于集群方法检测和定位窃听事件</li>
<li>results: 通过收集发送器端的光性能监测数据可以检测小功率损失引起的窃听事件，而通过利用线上监测数据可以有效地定位窃听事件。<details>
<summary>Abstract</summary>
We propose a cluster-based method to detect and locate eavesdropping events in optical line systems characterized by small power losses. Our findings indicate that detecting such subtle losses from eavesdropping can be accomplished solely through optical performance monitoring (OPM) data collected at the receiver. On the other hand, the localization of such events can be effectively achieved by leveraging in-line OPM data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于集群的方法，用于检测和定位光纤系统中的窃听事件。我们的发现表明，通过收集接收端的光性能监测（OPM）数据，可以寻查到这些微量损失。然而，通过利用线上OPM数据，可以有效地确定这些事件的位置。
</details></li>
</ul>
<hr>
<h2 id="Detach-ROCKET-Sequential-feature-selection-for-time-series-classification-with-random-convolutional-kernels"><a href="#Detach-ROCKET-Sequential-feature-selection-for-time-series-classification-with-random-convolutional-kernels" class="headerlink" title="Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels"></a>Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14518">http://arxiv.org/abs/2309.14518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gon-uri/detach_rocket">https://github.com/gon-uri/detach_rocket</a></li>
<li>paper_authors: Gonzalo Uribarri, Federico Barone, Alessio Ansuini, Erik Fransén</li>
<li>for: 这篇论文的目的是提出一种Sequential Feature Detachment（SFD）方法，用于时间序列分类（TSC）中删除无用的特征，提高模型的执行效率和泛化能力。</li>
<li>methods: 这篇论文使用了ROCKET模型和其变体，以及模型的权重来Estimate feature importance，并通过Sequential Feature Detachment（SFD）方法删除无用的特征。</li>
<li>results: 根据UCR archive的试验结果，SFD方法可以将时间序列分类模型的特征数量从原本的1000多个降至10%左右，同时提高模型的测试精度0.2%。此外， Detach-ROCKET方法可以对最大的 binary UCR 数据集进行最佳化，从而提高测试精度0.6%，同时删除98.9%的特征。<details>
<summary>Abstract</summary>
Time Series Classification (TSC) is essential in many fields, such as medicine, environmental science and finance, enabling tasks like disease diagnosis, anomaly detection, and stock price analysis. Machine learning models for TSC like Recurrent Neural Networks and InceptionTime, while successful in numerous applications, can face scalability limitations due to intensive computational requirements. To address this, efficient models such as ROCKET and its derivatives have emerged, simplifying training and achieving state-of-the-art performance by utilizing a large number of randomly generated features from time series data. However, due to their random nature, most of the generated features are redundant or non-informative, adding unnecessary computational load and compromising generalization. Here, we introduce Sequential Feature Detachment (SFD) as a method to identify and prune these non-essential features. SFD uses model coefficients to estimate feature importance and, unlike previous algorithms, can handle large feature sets without the need for complex hyperparameter tuning. Testing on the UCR archive demonstrates that SFD can produce models with $10\%$ of the original features while improving the accuracy $0.2\%$ on the test set. We also present an end-to-end procedure for determining an optimal balance between the number of features and model accuracy, called Detach-ROCKET. When applied to the largest binary UCR dataset, Detach-ROCKET is able to improve test accuracy by $0.6\%$ while reducing the number of features by $98.9\%$. Thus, our proposed procedure is not only lightweight to train and effective in reducing model size and enhancing generalization, but its significant reduction in feature count also paves the way for feature interpretation.
</details>
<details>
<summary>摘要</summary>
时序分类（TSC）在医学、环境科学和金融等领域具有重要意义，可以实现疾病诊断、异常检测和股票价格分析等任务。机器学习模型 для TSC，如循环神经网络和InceptionTime，虽然在多个应用中取得成功，但可能会面临扩展性限制，因为计算需求很高。为解决这问题，有效的模型如ROCKET和其 derivates出现了，使得训练更加简单，并在多个时序数据上实现了状态机器学习性能。然而，由于这些生成的特征 Random，大多数生成的特征都是 redundant或非指导的，这会增加计算负担并降低泛化性。在这种情况下，我们介绍了时序特征分离（SFD）方法，可以识别和剔除非关键的特征。SFD使用模型系数来估计特征重要性，与之前的算法不同之处在于可以处理大量特征集 ohne需要复杂的超参数调整。在UCRL архиivos上进行测试，SFD可以生成模型，其中90%的特征是非关键的，而测试集上的准确率提高0.2%。我们还提出了一种从头到尾的过程，可以确定最佳的特征数量和模型准确率之间的平衡，称为Detach-ROCKET。当应用于最大的二进制UCRL数据集时，Detach-ROCKET可以提高测试准确率0.6%，同时减少特征数量98.9%。因此，我们的提出的过程不仅轻量级训练，效果减小模型大小和提高泛化性，而且它的重要减少特征计数也为特征解释开辟了道路。
</details></li>
</ul>
<hr>
<h2 id="Zeroth-order-Riemannian-Averaging-Stochastic-Approximation-Algorithms"><a href="#Zeroth-order-Riemannian-Averaging-Stochastic-Approximation-Algorithms" class="headerlink" title="Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms"></a>Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14506">http://arxiv.org/abs/2309.14506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Li, Krishnakumar Balasubramanian, Shiqian Ma</li>
<li>for: 这个论文是为了研究在里曼尼抽象上的泛化随机搜索问题。</li>
<li>methods: 这个论文使用了 Zero-order Riemannian Averaging Stochastic Approximation（\texttt{Zo-RASA）} 算法，并使用了 RiemaNN 移动平均梯度估计器和一种新的 RiemaNN-Lyapunov 分析技术来进行转化分析。</li>
<li>results: 这个论文表明了 \texttt{Zo-RASA} 算法可以使用单个样本或常数级批处理在每个迭代中实现 $\epsilon$-近似首Order站点解。此外，论文还引入了一种新的几何条件，即 bounded second fundamental form，可以用于精度地 approximate parallel transport。<details>
<summary>Abstract</summary>
We present Zeroth-order Riemannian Averaging Stochastic Approximation (\texttt{Zo-RASA}) algorithms for stochastic optimization on Riemannian manifolds. We show that \texttt{Zo-RASA} achieves optimal sample complexities for generating $\epsilon$-approximation first-order stationary solutions using only one-sample or constant-order batches in each iteration. Our approach employs Riemannian moving-average stochastic gradient estimators, and a novel Riemannian-Lyapunov analysis technique for convergence analysis. We improve the algorithm's practicality by using retractions and vector transport, instead of exponential mappings and parallel transports, thereby reducing per-iteration complexity. Additionally, we introduce a novel geometric condition, satisfied by manifolds with bounded second fundamental form, which enables new error bounds for approximating parallel transport with vector transport.
</details>
<details>
<summary>摘要</summary>
我们提出了零预orde Riemannian Averaging Stochastic Approximation（\texttt{Zo-RASA）}算法，用于在里曼尼抽象上进行数学估计。我们证明了\texttt{Zo-RASA}可以在每个迭代中使用单一样本或常量组数据，实现 $\epsilon$-近似首先稳定解的生成。我们的方法使用里曼尼运动平均梯度估计器，并使用一种新的里曼尼- Lyapunov 分析技术进行对准性分析。我们通过使用抽像和向量运输，而不是对称对映和平行运输，将每次迭代的复杂性降低。此外，我们也提出了一个新的几何条件，这个条件是在具有受限第二funamental form的抽象上存在的，它允许我们给出新的错误上限，用于近似平行运输。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Aware-Deep-Learning-for-Particle-Accelerators"><a href="#Uncertainty-Aware-Deep-Learning-for-Particle-Accelerators" class="headerlink" title="Uncertainty Aware Deep Learning for Particle Accelerators"></a>Uncertainty Aware Deep Learning for Particle Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14502">http://arxiv.org/abs/2309.14502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kishansingh Rajput, Malachi Schram, Karthik Somayaji</li>
<li>for: 这篇论文是为了提出一种用Deep Gaussian Process Approximation（DGPA）方法进行误束预测和不确定性评估的方法。</li>
<li>methods: 这篇论文使用了Deep Gaussian Process Approximation（DGPA）方法，该方法可以捕捉复杂系统动态，但是需要考虑误束和不确定性。</li>
<li>results: 这篇论文在SNS加速器中进行了误束预测和不确定性评估，并提供了一个不确定性意识的模型。<details>
<summary>Abstract</summary>
Standard deep learning models for classification and regression applications are ideal for capturing complex system dynamics. However, their predictions can be arbitrarily inaccurate when the input samples are not similar to the training data. Implementation of distance aware uncertainty estimation can be used to detect these scenarios and provide a level of confidence associated with their predictions. In this paper, we present results from using Deep Gaussian Process Approximation (DGPA) methods for errant beam prediction at Spallation Neutron Source (SNS) accelerator (classification) and we provide an uncertainty aware surrogate model for the Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex (regression).
</details>
<details>
<summary>摘要</summary>
标准的深度学习模型可以很好地捕捉复杂系统的动态。但是，它们的预测结果可能无法实际地准确，尤其是对于与训练数据不同的输入数据。在这篇文章中，我们使用深度 Gaussian Process Approximation（DGPA）方法进行误偏照射预测（分类），并提供了不对称 uncertainty 意识模型（重回应）。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Potential-of-Deep-Learning-Models-for-Solar-Flare-Prediction-in-Near-Limb-Regions"><a href="#Unveiling-the-Potential-of-Deep-Learning-Models-for-Solar-Flare-Prediction-in-Near-Limb-Regions" class="headerlink" title="Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions"></a>Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14483">http://arxiv.org/abs/2309.14483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chetraj Pandey, Rafal A. Angryk, Berkay Aydin</li>
<li>for: 本研究旨在评估深度学习模型在预测solar flare的性能，使用每小时采样的全盘线条图像，特别是关注近似limb区域（beyond ±70°的太阳盘面）的许多次过looked flare事件。</li>
<li>methods: 我们使用了三种well-known deep learning architecture——AlexNet、VGG16和ResNet34进行了转移学习，并对三个模型进行了比较和评估，使用了真实技能统计（TSS）和Heidke技能分数（HSS），以及计算了回快分数，以理解预测敏感性在中心和近似limb区域中。</li>
<li>results: 我们的研究发现，使用AlexNet基于的模型在全体性能方面表现最高，其TSS和HSS分别为0.53和0.37；而进一步的空间分析回快分数显示，在近似limb事件中，VGG16和ResNet34基于的模型具有更高的预测敏感性。最佳结果是使用ResNet34基于的模型，其near-limb预测回快率为0.59（X-和M-class预测回快率分别为0.81和0.56）。<details>
<summary>Abstract</summary>
This study aims to evaluate the performance of deep learning models in predicting $\geq$M-class solar flares with a prediction window of 24 hours, using hourly sampled full-disk line-of-sight (LoS) magnetogram images, particularly focusing on the often overlooked flare events corresponding to the near-limb regions (beyond $\pm$70$^{\circ}$ of the solar disk). We trained three well-known deep learning architectures--AlexNet, VGG16, and ResNet34 using transfer learning and compared and evaluated the overall performance of our models using true skill statistics (TSS) and Heidke skill score (HSS) and computed recall scores to understand the prediction sensitivity in central and near-limb regions for both X- and M-class flares. The following points summarize the key findings of our study: (1) The highest overall performance was observed with the AlexNet-based model, which achieved an average TSS$\sim$0.53 and HSS$\sim$0.37; (2) Further, a spatial analysis of recall scores disclosed that for the near-limb events, the VGG16- and ResNet34-based models exhibited superior prediction sensitivity. The best results, however, were seen with the ResNet34-based model for the near-limb flares, where the average recall was approximately 0.59 (the recall for X- and M-class was 0.81 and 0.56 respectively) and (3) Our research findings demonstrate that our models are capable of discerning complex spatial patterns from full-disk magnetograms and exhibit skill in predicting solar flares, even in the vicinity of near-limb regions. This ability holds substantial importance for operational flare forecasting systems.
</details>
<details>
<summary>摘要</summary>
这项研究的目标是评估深度学习模型在预测24小时内的$\geq$M级太阳风暴事件的性能，使用每小时采样的全盘线性图像，特别是关注太阳盘面外(-70度以上)的快速风暴事件。我们使用了三种已知的深度学习架构——AlexNet、VGG16和ResNet34进行转移学习，并对三个模型进行比较和评估，使用真实技能统计（TSS）和海德ке技能分数（HSS），并计算了中心和近缘区域的回快率以了解预测敏感度。研究的主要发现包括：1. AlexNet基于的模型在整体性能方面表现最高，其TSS和HSS分别为0.53和0.37；2. 空间分析回快率表明，近缘区域内的风暴事件预测敏感度最高，VGG16和ResNet34基于的模型在近缘区域内表现出色，特别是ResNet34基于的模型，其平均回快率为0.59，X级和M级风暴事件的回快率分别为0.81和0.56；3. 这些研究发现表明，我们的模型可以从全盘线性图像中提取复杂的空间特征，并在靠近近缘区域的风暴事件预测中展现出能力。这种能力对于实际风暴预测系统具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="LogGPT-Log-Anomaly-Detection-via-GPT"><a href="#LogGPT-Log-Anomaly-Detection-via-GPT" class="headerlink" title="LogGPT: Log Anomaly Detection via GPT"></a>LogGPT: Log Anomaly Detection via GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14482">http://arxiv.org/abs/2309.14482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Han, Shuhan Yuan, Mohamed Trabelsi</li>
<li>for: 这篇研究旨在提出一个基于日志数据的系统异常检测方法，以确保计算机系统的安全性和可靠性。</li>
<li>methods: 本研究使用深度学习模型进行日志异常检测，具体来说是将日志序列变数为自然语言，然后运用深度序列模型，例如LSTM或Transformer，对日志序列中的正常模式进行语言模型化。</li>
<li>results: 实验结果显示，LogGPT在三个 datasets 上表现出色，较 existing state-of-the-art 方法有更高的检测精度。<details>
<summary>Abstract</summary>
Detecting system anomalies based on log data is important for ensuring the security and reliability of computer systems. Recently, deep learning models have been widely used for log anomaly detection. The core idea is to model the log sequences as natural language and adopt deep sequential models, such as LSTM or Transformer, to encode the normal patterns in log sequences via language modeling. However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection. To fill up the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly detection. LogGPT is first trained to predict the next log entry based on the preceding sequence. To further enhance the performance of LogGPT, we propose a novel reinforcement learning strategy to finetune the model specifically for the log anomaly detection task. The experimental results on three datasets show that LogGPT significantly outperforms existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
检测计算机系统中的异常 based on 日志数据是保持安全和可靠性的重要任务。最近，深度学习模型在日志异常检测中得到了广泛的应用。核心思想是模型日志序列为自然语言，采用深度序列模型，如LSTM或Transformer，来编码正常的日志序列模式 via 语言模型化。但是，语言模型化和异常检测之间存在一个差距，因为训练深度序列模型via语言模型化损失并不直接相关于异常检测。为填充这个差距，我们提出了 LogGPT，一种新的框架，它采用 GPT 进行日志异常检测。LogGPT 首先通过预测下一个日志条目基于前一个序列来训练。为了进一步提高 LogGPT 的性能，我们提出了一种新的强化学习策略，用于特定地 finetune 模型为日志异常检测任务。实验结果表明，LogGPT 与现有状态的方法相比，在三个数据集上显著地提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Skilog-A-Smart-Sensor-System-for-Performance-Analysis-and-Biofeedback-in-Ski-Jumping"><a href="#Skilog-A-Smart-Sensor-System-for-Performance-Analysis-and-Biofeedback-in-Ski-Jumping" class="headerlink" title="Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping"></a>Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14455">http://arxiv.org/abs/2309.14455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Schulthess, Thorir Mar Ingolfsson, Marc Nölke, Michele Magno, Luca Benini, Christoph Leitner</li>
<li>for: 这份研究是为了开发一个智能、 компакт、能效的无线感应器系统，用于现场进行 Ski jumping 的实时性能分析和生成反馈。</li>
<li>methods: 本研究使用了100Hz测量脚压的方法，并使用Machine Learning（ML）模型来实现实时的反馈。</li>
<li>results: 研究获得了92.7%的中心质量预测精度（脊梁偏移、中立位和腹股偏移），并在低功耗的RISC-V架构上进行了实时推导和反馈（0.0109ms&#x2F;推导）。<details>
<summary>Abstract</summary>
In ski jumping, low repetition rates of jumps limit the effectiveness of training. Thus, increasing learning rate within every single jump is key to success. A critical element of athlete training is motor learning, which has been shown to be accelerated by feedback methods. In particular, a fine-grained control of the center of gravity in the in-run is essential. This is because the actual takeoff occurs within a blink of an eye ($\sim$300ms), thus any unbalanced body posture during the in-run will affect flight. This paper presents a smart, compact, and energy-efficient wireless sensor system for real-time performance analysis and biofeedback during ski jumping. The system operates by gauging foot pressures at three distinct points on the insoles of the ski boot at 100Hz. Foot pressure data can either be directly sent to coaches to improve their feedback, or fed into a ML model to give athletes instantaneous in-action feedback using a vibration motor in the ski boot. In the biofeedback scenario, foot pressures act as input variables for an optimized XGBoost model. We achieve a high predictive accuracy of 92.7% for center of mass predictions (dorsal shift, neutral stand, ventral shift). Subsequently, we parallelized and fine-tuned our XGBoost model for a RISC-V based low power parallel processor (GAP9), based on the PULP architecture. We demonstrate real-time detection and feedback (0.0109ms/inference) using our on-chip deployment. The proposed smart system is unobtrusive with a slim form factor (13mm baseboard, 3.2mm antenna) and a lightweight build (26g). Power consumption analysis reveals that the system's energy-efficient design enables sustained operation over multiple days (up to 300 hours) without requiring recharge.
</details>
<details>
<summary>摘要</summary>
在跳台滑雪中，低重复率的跳跃限制了训练的效iveness。因此，在每次跳跃中提高学习率是关键到success。运动员训练中的核心元素是 дви作学习，已经证明可以通过反馈方法加速。特别是在具有细致控制中心重力的跑道上是关键。因为实际的起飞只需要几十毫秒（约300ms），所以任何不平衡的身体姿势会影响飞行。本文介绍了一种智能、卷积、能效的无线传感器系统，用于实时性表现分析和生物反馈 durante跳台滑雪。该系统通过在跳板底部的三个点检测脚压力，每秒100次获取数据。脚压力数据可以直接给教练提供反馈，或者通过一个机器学习模型给运动员实时反馈，使用跳板内置的振荡机。在生物反馈场景中，脚压力作为输入变量，用于优化的XGBoost模型。我们实现了中心质量预测的高预测精度（92.7%）。然后，我们将XGBoost模型并行化和优化，基于RISC-V架构的低功耗并行处理器（GAP9）。我们实现了实时探测和反馈（0.0109ms/推导），并在芯片上部署。提案的智能系统轻便，减少了跳板的尺寸（13mm基板、3.2mm天线）和重量（26g）。能源消耗分析表明，该系统的能效设计可以持续运行多天（最多300小时）无需充电。
</details></li>
</ul>
<hr>
<h2 id="Learning-dislocation-dynamics-mobility-laws-from-large-scale-MD-simulations"><a href="#Learning-dislocation-dynamics-mobility-laws-from-large-scale-MD-simulations" class="headerlink" title="Learning dislocation dynamics mobility laws from large-scale MD simulations"></a>Learning dislocation dynamics mobility laws from large-scale MD simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14450">http://arxiv.org/abs/2309.14450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Bertin, Vasily V. Bulatov, Fei Zhou</li>
<li>for: 研究金属塑性的 mesoscale 模型 - 粗化 atomistic 动力学中的扭轧动力学</li>
<li>methods: 使用 machine learning (ML) 框架，通过 graph neural networks (GNN) 模型来自动化扭轧动力学的开发</li>
<li>results: 在 BCC 钨中示出了准确地复制了真实 MD  simulations 中的压缩&#x2F;张力偏见，并且在低刺激速度下预测了流体压力， demonstrating 了方法的能力学习扭轧动力学的Physics。<details>
<summary>Abstract</summary>
The computational method of discrete dislocation dynamics (DDD), used as a coarse-grained model of true atomistic dynamics of lattice dislocations, has become of powerful tool to study metal plasticity arising from the collective behavior of dislocations. As a mesoscale approach, motion of dislocations in the DDD model is prescribed via the mobility law; a function which specifies how dislocation lines should respond to the driving force. However, the development of traditional hand-crafted mobility laws can be a cumbersome task and may involve detrimental simplifications. Here we introduce a machine-learning (ML) framework to streamline the development of data-driven mobility laws which are modeled as graph neural networks (GNN) trained on large-scale Molecular Dynamics (MD) simulations of crystal plasticity. We illustrate our approach on BCC tungsten and demonstrate that our GNN mobility implemented in large-scale DDD simulations accurately reproduces the challenging tension/compression asymmetry observed in ground-truth MD simulations while correctly predicting the flow stress at lower straining rate conditions unseen during training, thereby demonstrating the ability of our method to learn relevant dislocation physics. Our DDD+ML approach opens new promising avenues to improve fidelity of the DDD model and to incorporate more complex dislocation motion behaviors in an automated way, providing a faithful proxy for dislocation dynamics several orders of magnitude faster than ground-truth MD simulations.
</details>
<details>
<summary>摘要</summary>
计算方法的粗化扭变动力学（DDD）模型，作为真实原子动力学扭变动力学的粗化模型，已成为金属塑形力学的研究powerful工具。作为中规模方法，DDD模型中扭变线的运动是通过 mobilicity 法规定的，这是一个指定扭变线应对驱动力的函数。然而，开发传统手动设计 mobilicity 法可能是一项繁琐的任务，并且可能会带来不利的简化。在这里，我们引入机器学习（ML）框架，以数据驱动的方式开发出更加简单的 mobilicity 法。我们使用Graph Neural Networks（GNN）模型，在大规模的分子动力学（MD） simulation 中训练这些 mobilicity 法。我们在 BCC 钴中实现了我们的 GNN  mobilicity，并在大规模的 DDD  simulations 中证明了我们的方法可以准确地复制真实 MD  simulation 中的困难的压缩/扩展不均勋，并且可以正确地预测低剪力环境下的流体压缩强度。这表明我们的方法可以学习扭变物理学。我们的 DDD + ML 方法打开了新的可能性，以提高 DDD 模型的准确性，并自动地包含更加复杂的扭变动力学行为，提供一个 faithful 的扭变动力学代理，在训练中未达到的低剪力环境下可以正确地预测流体压缩强度。
</details></li>
</ul>
<hr>
<h2 id="On-the-expressivity-of-embedding-quantum-kernels"><a href="#On-the-expressivity-of-embedding-quantum-kernels" class="headerlink" title="On the expressivity of embedding quantum kernels"></a>On the expressivity of embedding quantum kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14419">http://arxiv.org/abs/2309.14419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elies Gil-Fuster, Jens Eisert, Vedran Dunjko</li>
<li>for: 这个论文的目的是研究量子机器学习和经典机器学习之间的自然连接，特别是在内核方法上。</li>
<li>methods: 这篇论文使用的方法包括量子特征状态的构造和嵌入量子内核。</li>
<li>results: 这篇论文的结果是证明任何量子内核都可以表示为量子特征状态的内积，并且提出了一些新的、未探索的量子内核家族，其中是否还有效的嵌入量子内核还需要进一步研究。<details>
<summary>Abstract</summary>
One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part, we formalize the question of universality of efficient embedding quantum kernels. For shift-invariant kernels, we use the technique of random Fourier features to show that they are universal within the broad class of all kernels which allow a variant of efficient Fourier sampling. We then extend this result to a new class of so-called composition kernels, which we show also contains projected quantum kernels introduced in recent works. After proving the universality of embedding quantum kernels for both shift-invariant and composition kernels, we identify the directions towards new, more exotic, and unexplored quantum kernel families, for which it still remains open whether they correspond to efficient embedding quantum kernels.
</details>
<details>
<summary>摘要</summary>
（一些）自然的量子机器学习和经典机器学习之间的连接在内核方法上已经得到了证明。内核方法 rely on 内核，它们是特征向量生活在大特征空间的内积。量子内核通常通过明确构建量子特征状态来评估，然后计算它们的内积，这被称为嵌入量子内核。由于经典内核通常不直接使用特征向量，我们所思考嵌入量子内核的表达能力如何。在这项工作中，我们提出了一个基本问题：可以所有的量子内核都表示为量子特征状态的内积吗？我们的第一个结果是正的：通过计算 universality，我们发现了对于任何内核函数，都存在一个对应的量子特征映射和嵌入量子内核。对于更操作性的问题，我们在第二部分中正式化了嵌入量子内核的 universality 问题。对于不变内核，我们使用随机傅里埃特性来证明它们是 universality 的，并将其扩展到一个新的 composition kernels 类型，这类型包括已知的 projected quantum kernels。在证明嵌入量子内核的 universality 之后，我们确定了新、更有趣、未探索的量子内核家族的方向，其中是否存在效果嵌入量子内核还未知。
</details></li>
</ul>
<hr>
<h2 id="Provable-advantages-of-kernel-based-quantum-learners-and-quantum-preprocessing-based-on-Grover’s-algorithm"><a href="#Provable-advantages-of-kernel-based-quantum-learners-and-quantum-preprocessing-based-on-Grover’s-algorithm" class="headerlink" title="Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover’s algorithm"></a>Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover’s algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14406">http://arxiv.org/abs/2309.14406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Till Muser, Elias Zapusek, Vasilis Belis, Florentin Reiter</li>
<li>for: 该研究目的是提高学习问题的计算效率，特别是应用量子计算机在支持向量机中的速度优势。</li>
<li>methods: 该研究使用了Shor的算法和Grover的算法来实现量子支持向量机的速度优势。</li>
<li>results: 研究发现，通过在支持向量机的kernel中使用量子计算机，可以获得 exponential 的速度优势。此外，通过将量子计算机与类传统的分类方法结合使用，可以进一步提高分类器的性能。<details>
<summary>Abstract</summary>
There is an ongoing effort to find quantum speedups for learning problems. Recently, [Y. Liu et al., Nat. Phys. $\textbf{17}$, 1013--1017 (2021)] have proven an exponential speedup for quantum support vector machines by leveraging the speedup of Shor's algorithm. We expand upon this result and identify a speedup utilizing Grover's algorithm in the kernel of a support vector machine. To show the practicality of the kernel structure we apply it to a problem related to pattern matching, providing a practical yet provable advantage. Moreover, we show that combining quantum computation in a preprocessing step with classical methods for classification further improves classifier performance.
</details>
<details>
<summary>摘要</summary>
有一个持续进行的努力是找到量子速度减少学习问题。最近，李宇等人（Nat. Phys. $\textbf{17}$, 1013--1017 (2021)）已经证明了量子支持向量机的加速，通过利用戈Vor的算法速度。我们在这个结果基础上进一步扩展，并证明了使用格罗弗尔算法在支持向量机的kernel中获得加速。为证明实用性，我们应用了这种结构到一个相关的模式匹配问题，并提供了实用却可证明的优势。此外，我们还证明了结合量子计算在预处理步骤中与经典方法结合，可以进一步提高分类器性能。
</details></li>
</ul>
<hr>
<h2 id="Tasks-Makyth-Models-Machine-Learning-Assisted-Surrogates-for-Tipping-Points"><a href="#Tasks-Makyth-Models-Machine-Learning-Assisted-Surrogates-for-Tipping-Points" class="headerlink" title="Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping Points"></a>Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping Points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14334">http://arxiv.org/abs/2309.14334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Fabiani, Nikolaos Evangelou, Tianqi Cui, Juan M. Bello-Rivas, Cristina P. Martin-Linares, Constantinos Siettos, Ioannis G. Kevrekidis</li>
<li>for: 这个论文的目的是提出一种基于机器学习的框架，用于探索复杂系统的突变点和罕见事件的可能性。</li>
<li>methods: 该论文使用了拟合多元space，神经网络，高斯过程和无方程多尺度模型来实现这个目的。</li>
<li>results: 该论文通过使用这些方法对高维时空数据进行压缩， constructions 缩写模型来描述不同的级别的 emergent 动力学，并且可以准确地预测突变点和罕见事件的可能性。<details>
<summary>Abstract</summary>
We present a machine learning (ML)-assisted framework bridging manifold learning, neural networks, Gaussian processes, and Equation-Free multiscale modeling, for (a) detecting tipping points in the emergent behavior of complex systems, and (b) characterizing probabilities of rare events (here, catastrophic shifts) near them. Our illustrative example is an event-driven, stochastic agent-based model (ABM) describing the mimetic behavior of traders in a simple financial market. Given high-dimensional spatiotemporal data -- generated by the stochastic ABM -- we construct reduced-order models for the emergent dynamics at different scales: (a) mesoscopic Integro-Partial Differential Equations (IPDEs); and (b) mean-field-type Stochastic Differential Equations (SDEs) embedded in a low-dimensional latent space, targeted to the neighborhood of the tipping point. We contrast the uses of the different models and the effort involved in learning them.
</details>
<details>
<summary>摘要</summary>
我们提出了一个基于机器学习（ML）的框架，它将拓扑学学习、神经网络、高斯过程和无方程多尺度模型绑定在一起，用于检测复杂系统的 emergent 行为中的跌宕点，以及在其附近的罕见事件的概率Characterization。我们的示例是一个事件驱动的随机 Agent-Based Model（ABM），描述了金融市场中的模拟行为。给出高维空间时间数据（由随机 ABM 生成），我们构建了不同级别的减少模型，用于描述不同级别的 emergent 动力学：（a） mesoscopic Integro-Partial Differential Equations（IPDEs）；和（b） Mean-field-type Stochastic Differential Equations（SDEs），其embedded在一个低维的隐藏空间中，targeted to the neighborhood of the tipping point。我们对不同模型的使用和学习努力进行了对比。
</details></li>
</ul>
<hr>
<h2 id="pLMFPPred-a-novel-approach-for-accurate-prediction-of-functional-peptides-integrating-embedding-from-pre-trained-protein-language-model-and-imbalanced-learning"><a href="#pLMFPPred-a-novel-approach-for-accurate-prediction-of-functional-peptides-integrating-embedding-from-pre-trained-protein-language-model-and-imbalanced-learning" class="headerlink" title="pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning"></a>pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14404">http://arxiv.org/abs/2309.14404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mnb66/plmfppred">https://github.com/mnb66/plmfppred</a></li>
<li>paper_authors: Zebin Ma, Yonglin Zou, Xiaobin Huang, Wenjin Yan, Hao Xu, Jiexin Yang, Ying Zhang, Jinqi Huang</li>
<li>for: 预测功能肽，即使用人工智能计算策略来快速从蛋白质序列集中鉴别出新的功能肽并确定其不同的功能。</li>
<li>methods: 使用蛋白语言模型基于的插入（ESM-2），开发了一种名为pLMFPPred（蛋白语言模型基于功能肽预测器）来预测功能肽和识别 токси肽。同时，使用SMOTE-TOMEK数据合成采样技术和Shapley值基于的特征选择技术来解决数据不均衡问题，降低计算成本。</li>
<li>results: 在一个验证的独立测试集上，pLMFPPred实现了精度、接收操作特征曲线值和F1值的0.974、0.99和0.974，分别。 comparative experiments show that pLMFPPred outperforms current methods for predicting functional peptides。实验结果表明，提案的方法（pLMFPPred）可以在预测功能肽方面提供更好的性能，并代表一种新的计算方法。<details>
<summary>Abstract</summary>
Functional peptides have the potential to treat a variety of diseases. Their good therapeutic efficacy and low toxicity make them ideal therapeutic agents. Artificial intelligence-based computational strategies can help quickly identify new functional peptides from collections of protein sequences and discover their different functions.Using protein language model-based embeddings (ESM-2), we developed a tool called pLMFPPred (Protein Language Model-based Functional Peptide Predictor) for predicting functional peptides and identifying toxic peptides. We also introduced SMOTE-TOMEK data synthesis sampling and Shapley value-based feature selection techniques to relieve data imbalance issues and reduce computational costs. On a validated independent test set, pLMFPPred achieved accuracy, Area under the curve - Receiver Operating Characteristics, and F1-Score values of 0.974, 0.99, and 0.974, respectively. Comparative experiments show that pLMFPPred outperforms current methods for predicting functional peptides.The experimental results suggest that the proposed method (pLMFPPred) can provide better performance in terms of Accuracy, Area under the curve - Receiver Operating Characteristics, and F1-Score than existing methods. pLMFPPred has achieved good performance in predicting functional peptides and represents a new computational method for predicting functional peptides.
</details>
<details>
<summary>摘要</summary>
功能蛋白质有很大的治疗潜力。它们的良好的治疗效果和低度的致病性使得它们成为理想的药物代用品。通过人工智能基于计算的方法，可以快速地从蛋白质序列集中获取新的功能蛋白质和其不同的功能。我们使用蛋白质语言模型基于嵌入（ESM-2），开发了一个名为pLMFPPred（蛋白质语言模型基于功能蛋白质预测器）的工具，用于预测功能蛋白质和识别毒蛋白质。我们还使用SMOTE-TOMEK数据合成抽样和Shapley值基于特征选择技术来解决数据均衡问题和降低计算成本。在一个验证的独立测试集上，pLMFPPred实现了精度、接收操作特征曲线值和F1分数的值为0.974、0.99和0.974，分别。相比之下，相关的方法实现了较差的效果。实验结果表明，提案的方法（pLMFPPred）可以在预测功能蛋白质方面提供更好的表现，并代表了一种新的计算方法。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Framework-for-Uniform-Signal-Recovery-in-Nonlinear-Generative-Compressed-Sensing"><a href="#A-Unified-Framework-for-Uniform-Signal-Recovery-in-Nonlinear-Generative-Compressed-Sensing" class="headerlink" title="A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing"></a>A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03758">http://arxiv.org/abs/2310.03758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junren Chen, Jonathan Scarlett, Michael K. Ng, Zhaoqiang Liu</li>
<li>For: The paper is written to study the problem of generative compressed sensing (GCS) with nonlinear measurements, and to provide uniform recovery guarantees for this problem.* Methods: The paper uses a unified framework that combines the observation model with the generative model to derive uniform recovery guarantees for nonlinear GCS. The framework accommodates GCS with 1-bit&#x2F;uniformly quantized observations and single index models as canonical examples.* Results: The paper shows that using a single realization of the sensing ensemble and generalized Lasso, all $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\ell_2$-error at most $\epsilon$ using roughly $\tilde{O}({k}&#x2F;{\epsilon^2})$ samples, with omitted logarithmic factors typically being dominated by $\log L$. This is almost coincident with existing non-uniform guarantees up to logarithmic factors, indicating that the uniformity comes with a very small cost.<details>
<summary>Abstract</summary>
In generative compressed sensing (GCS), we want to recover a signal $\mathbf{x}^* \in \mathbb{R}^n$ from $m$ measurements ($m\ll n$) using a generative prior $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$, where $G$ is typically an $L$-Lipschitz continuous generative model and $\mathbb{B}_2^k(r)$ represents the radius-$r$ $\ell_2$-ball in $\mathbb{R}^k$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $\mathbf{x}^*$ rather than for all $\mathbf{x}^*$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, {\em all} $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\ell_2$-error at most $\epsilon$ using roughly $\tilde{O}({k}/{\epsilon^2})$ samples, with omitted logarithmic factors typically being dominated by $\log L$. Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce the Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bounds for product processes whose index sets have low metric entropy. Experimental results are presented to corroborate our theory.
</details>
<details>
<summary>摘要</summary>
在生成式压缩感知（GCS）中，我们想要从 $m$ 测量 ($m \ll n$) 中还原一个信号 $\mathbf{x}^* \in \mathbb{R}^n$ 使用生成模型 $G$，其中 $G$ 是一个 Typically $L$-Lipschitz 连续的生成模型，$\mathbb{B}_2^k(r)$ 表示半径-$r$ $\ell_2$-球在 $\mathbb{R}^k$ 中。在非线性测量下，大多数先前结果是非均匀的，即它们在固定 $\mathbf{x}^*$ 上持有高概率而不是所有 $\mathbf{x}^*$ 上同时持有。在这篇论文中，我们构建了一个统一的框架，用于 derive 均匀的恢复保证，对于非线性 GCS，测量模型可能是不连续或未知的。我们的框架可以涵盖 GCS 与 1-bit/均匀量化观测和单index模型作为 kanonikus 例子。具体来说，使用单个感知ensemble和普通lasso，所有 $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ 可以在 $\ell_2$ 误差不超过 $\epsilon$ 的情况下，使用约 $\tilde{O}({k}/{\epsilon^2})$ 个样本进行恢复，忽略 logs 因子通常是由 $\log L$ 控制。这与现有的非均匀保证相差只有 logs 因子，因此均匀性的代价很低。在我们的技术贡献中，我们引入了 Lipschitz approximation 来处理不连续测量模型。我们还开发了一种集中不等式，用于生成产品过程中的指标集 whose 度量 entropy 较低。实验结果用于证明我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Futility-and-utility-of-a-few-ancillas-for-Pauli-channel-learning"><a href="#Futility-and-utility-of-a-few-ancillas-for-Pauli-channel-learning" class="headerlink" title="Futility and utility of a few ancillas for Pauli channel learning"></a>Futility and utility of a few ancillas for Pauli channel learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14326">http://arxiv.org/abs/2309.14326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sitan Chen, Weiyuan Gong</li>
<li>For: 本文 revisits one of the prototypical tasks for characterizing the structure of noise in quantum devices, estimating the eigenvalues of an $n$-qubit Pauli noise channel.* Methods: 本文使用 exponential lower bounds to show the limitations of algorithms for estimating the eigenvalues of the noise channel. These lower bounds hold even for the easier hypothesis testing problem of determining whether the underlying channel is completely depolarizing or has exactly one other nontrivial eigenvalue.* Results: 本文 gets the following results:	1. Any algorithm without quantum memory must make $\Omega(2^n&#x2F;\epsilon^2)$ measurements to estimate each eigenvalue within error $\epsilon$.	2. Any algorithm with $\le k$ ancilla qubits of quantum memory must make $\Omega(2^{(n-k)&#x2F;3})$ queries to the unknown channel.	3. With only $k&#x3D;2$ ancilla qubits of quantum memory, there is an algorithm that solves the hypothesis testing task with high probability using a single measurement.I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this paper we revisit one of the prototypical tasks for characterizing the structure of noise in quantum devices, estimating the eigenvalues of an $n$-qubit Pauli noise channel. Prior work (Chen et al., 2022) established exponential lower bounds for this task for algorithms with limited quantum memory. We first improve upon their lower bounds and show:   (1) Any algorithm without quantum memory must make $\Omega(2^n/\epsilon^2)$ measurements to estimate each eigenvalue within error $\epsilon$. This is tight and implies the randomized benchmarking protocol is optimal, resolving an open question of (Flammia and Wallman, 2020).   (2) Any algorithm with $\le k$ ancilla qubits of quantum memory must make $\Omega(2^{(n-k)/3})$ queries to the unknown channel. Crucially, unlike in (Chen et al., 2022), our bound holds even if arbitrary adaptive control and channel concatenation are allowed.   In fact these lower bounds, like those of (Chen et al., 2022), hold even for the easier hypothesis testing problem of determining whether the underlying channel is completely depolarizing or has exactly one other nontrivial eigenvalue. Surprisingly, we show that:   (3) With only $k=2$ ancilla qubits of quantum memory, there is an algorithm that solves this hypothesis testing task with high probability using a single measurement.   Note that (3) does not contradict (2) as the protocol concatenates exponentially many queries to the channel before the measurement. This result suggests a novel mechanism by which channel concatenation and $O(1)$ qubits of quantum memory could work in tandem to yield striking speedups for quantum process learning that are not possible for quantum state learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们回顾了一个杜理量化器设备噪声结构的典型任务，即估算 $n$- Quint Pauli 噪声通道的 eigenvalues。先前的工作（Chen et al., 2022）已经证明了这个任务的下界，我们首先提高了这个下界并证明：  1. 没有量子储存的算法必须做 $\Omega(2^n/\epsilon^2)$ 测量来估算每个含误值。这是最佳的和 Flammia 和 Wallman（2020）的问题的解。  2. 具有 $\le k$  ancilla qubits 的量子储存算法必须做 $\Omega(2^{(n-k)/3})$ 请求来 unknown 通道。这个下界不仅如 Chen et al.（2022）的下界，而且允许任意适应控制和通道 concatenation。  事实上，这些下界也适用于另一个更容易的假设测试问题，即判断 underlying 通道是完全depolarizing 或者有 exactly one 其他非特性含误值。我们证明：  3. 只有 $k=2$ ancilla qubits 的量子储存算法可以使用单个测量来高probability 解决这个假设测试问题。这个结果表明，通过 concatenating  exponentially many queries to the channel before the measurement, it is possible to achieve striking speedups for quantum process learning that are not possible for quantum state learning.
</details></li>
</ul>
<hr>
<h2 id="Small-scale-proxies-for-large-scale-Transformer-training-instabilities"><a href="#Small-scale-proxies-for-large-scale-Transformer-training-instabilities" class="headerlink" title="Small-scale proxies for large-scale Transformer training instabilities"></a>Small-scale proxies for large-scale Transformer training instabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14322">http://arxiv.org/abs/2309.14322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, Simon Kornblith</li>
<li>for: 这个论文的目的是研究大型Transformer模型在大规模训练中出现的训练不稳定性的原因，以及这些不稳定性在小规模训练中的表现。</li>
<li>methods: 本文使用了两种源引起训练不稳定性的研究：在注意层中增长的logits（Dehghani et al., 2023）和输出logits与输出概率的分化（Chowdhery et al., 2022）。通过测量学习率和损失之间的关系，我们显示这些不稳定性也在小模型中出现，并且在高学习率训练中使用了先前在大规模训练中使用的缓解方法可以达到相似的损失值。</li>
<li>results: 本文研究了一些已知优化器和模型调整的影响，包括温存、Weight decay和$\mu$Param（Yang et al., 2022）。我们发现可以通过组合这些技术来训练小模型，以实现在不同学习率下的损失值之间的相似性。最后，我们研究了两种可以预测训练不稳定性的情况：模型活动和梯度 нор的扩展行为。<details>
<summary>Abstract</summary>
Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training stability and instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to which other known optimizer and model interventions influence the sensitivity of the final loss to changes in the learning rate. To this end, we study methods such as warm-up, weight decay, and the $\mu$Param (Yang et al., 2022), and combine techniques to train small models that achieve similar losses across orders of magnitude of learning rate variation. Finally, to conclude our exploration we study two cases where instabilities can be predicted before they emerge by examining the scaling behavior of model activation and gradient norms.
</details>
<details>
<summary>摘要</summary>
团队已经训练过大型Transformer模型时报告了训练不稳定的问题，而这些问题在相同的超参数下不会出现。虽然这些不稳定的原因具有科学兴趣，但是需要资源来调查。在这项工作中，我们寻找一种在小规模上重现和研究训练稳定性和不稳定性。我们首先关注以下两种训练不稳定的来源：在注意层中 logged 的生长（Dehghani et al., 2023）和输出 logits 与 log probability 的分化（Chowdhery et al., 2022）。通过测量学习率和损失之间的关系，我们表明这些不稳定也会在小型模型中出现，并且在这种情况下，已经在大规模上使用的 Mitigation 也是有效的。这使得我们想 investigate 其他知道的优化器和模型 intervención 对最终损失响应于学习率变化的敏感性。为此，我们研究了温存、重量 decay 和 $\mu$Param（Yang et al., 2022）等方法，并将这些方法结合使用来训练小模型，以实现在不同学习率下的相同损失水平。最后，我们研究了两种可以预测训练不稳定之前的情况：模型活动和梯度 norms 的扩散行为。
</details></li>
</ul>
<hr>
<h2 id="Lifelong-Robot-Learning-with-Human-Assisted-Language-Planners"><a href="#Lifelong-Robot-Learning-with-Human-Assisted-Language-Planners" class="headerlink" title="Lifelong Robot Learning with Human Assisted Language Planners"></a>Lifelong Robot Learning with Human Assisted Language Planners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14321">http://arxiv.org/abs/2309.14321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meenal Parakh, Alisha Fong, Anthony Simeonov, Tao Chen, Abhishek Gupta, Pulkit Agrawal</li>
<li>for: 这个论文是为了开发一种使用大型自然语言模型（LLM）来帮助机器人学习新的技能的方法。</li>
<li>methods: 论文使用了LLM来帮助机器人查询和学习新的技能，并且可以在数据和时间有效的情况下进行学习。</li>
<li>results: 研究人员通过实验和实际应用，证明了该方法可以帮助机器人在不同任务中快速学习和应用新的技能，并且可以在未来的任务中重用已经学习的技能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经被证明可以 acted like 观察者，将高水平的指令分解为可执行的指令序列。但现有的 LLM-based 观察者只能运行固定的技能。我们解决了这个极限，并提出了使用 LLM-based 观察者来查询新技能并教育机器人这些技能，在数据和时间效率下进行弹性物件抓取。我们的系统可以重复 newly acquired 技能，以便在未来任务中重复使用，这显示了开放世界和一生学习的潜力。我们在多个任务中进行了评估，并在网站上提供了详细的视频：https://sites.google.com/mit.edu/halp-robot-learning。
</details></li>
</ul>
<hr>
<h2 id="A-post-selection-algorithm-for-improving-dynamic-ensemble-selection-methods"><a href="#A-post-selection-algorithm-for-improving-dynamic-ensemble-selection-methods" class="headerlink" title="A post-selection algorithm for improving dynamic ensemble selection methods"></a>A post-selection algorithm for improving dynamic ensemble selection methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14307">http://arxiv.org/abs/2309.14307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prgc/ps-des">https://github.com/prgc/ps-des</a></li>
<li>paper_authors: Paulo R. G. Cordeiro, George D. C. Cavalcanti, Rafael M. O. Cruz</li>
<li>for: 这个研究的目的是为了选择最佳的多 кластер组件系统（MCS）方法，以提高精准度。</li>
<li>methods: 这个研究使用的方法是Post-Selection Dynamic Ensemble Selection（PS-DES）方法，它是一种在选择阶段选择最佳的组件方法。</li>
<li>results: 实验结果显示，使用精度作为选择组件方法的评估指标，PS-DES方法比单一的DES方法表现更好。Here’s the translation in English:</li>
<li>for: The purpose of this research is to select the best Multiple Classifier Systems (MCS) method to improve accuracy.</li>
<li>methods: The method used in this research is the Post-Selection Dynamic Ensemble Selection (PS-DES) method, which selects the best ensemble method in the selection phase.</li>
<li>results: Experimental results show that using accuracy as a metric to select the ensembles, PS-DES outperforms individual DES techniques.I hope that helps!<details>
<summary>Abstract</summary>
Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS) approach that aims to select an ensemble for each query sample during the selection phase. Even with the proposal of several DES approaches, no particular DES technique is the best choice for different problems. Thus, we hypothesize that selecting the best DES approach per query instance can lead to better accuracy. To evaluate this idea, we introduce the Post-Selection Dynamic Ensemble Selection (PS-DES) approach, a post-selection scheme that evaluates ensembles selected by several DES techniques using different metrics. Experimental results show that using accuracy as a metric to select the ensembles, PS-DES performs better than individual DES techniques. PS-DES source code is available in a GitHub repository
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS) approach that aims to select an ensemble for each query sample during the selection phase. Even with the proposal of several DES approaches, no particular DES technique is the best choice for different problems. Thus, we hypothesize that selecting the best DES approach per query instance can lead to better accuracy. To evaluate this idea, we introduce the Post-Selection Dynamic Ensemble Selection (PS-DES) approach, a post-selection scheme that evaluates ensembles selected by several DES techniques using different metrics. Experimental results show that using accuracy as a metric to select the ensembles, PS-DES performs better than individual DES techniques. PS-DES source code is available in a GitHub repository" into Simplified Chinese.Here's the translation:<<SYS>>多个类ifier系统（MCS）方法之一是动态ensemble选择（DES），它在选择阶段为每个查询样本选择一个ensemble。尽管已经提出了多种DES方法，但是没有一个特定的DES技术适合所有问题。因此，我们提出了在每个查询实例中选择最佳DES方法的想法，以提高准确率。为了评估这个想法，我们引入了后期选择的动态ensemble选择（PS-DES）方法，该方法使用不同的度量评估由多种DES技术选择的ensemble。实验结果显示，使用准确率作为度量选择ensemble时，PS-DES方法perform Better than个 DES技术。PS-DES源代码可以在GitHub存储库中找到。
</details></li>
</ul>
<hr>
<h2 id="Improved-Algorithms-for-Stochastic-Linear-Bandits-Using-Tail-Bounds-for-Martingale-Mixtures"><a href="#Improved-Algorithms-for-Stochastic-Linear-Bandits-Using-Tail-Bounds-for-Martingale-Mixtures" class="headerlink" title="Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures"></a>Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14298">http://arxiv.org/abs/2309.14298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters</li>
<li>for: 这个论文targets the stochastic linear bandit problem, and proposes improved algorithms with worst-case regret guarantees.</li>
<li>methods: 该论文使用了一种新的tail bound for adaptive martingale mixtures to construct confidence sequences, which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming.</li>
<li>results: 该论文提供了一种基于 confidence sequences的 linear bandit algorithm, which is guaranteed to achieve competitive worst-case regret.  Additionally, the authors show that their confidence sequences are tighter than competitors, both empirically and theoretically, and demonstrate improved performance in several hyperparameter tuning tasks.<details>
<summary>Abstract</summary>
We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used "optimism in the face of uncertainty" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confidence sequences give improved performance in several hyperparameter tuning tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了改进的算法，带有最坏情况的悔检保证，用于Stochastic Linear Bandit问题。通过“面对不确定性的optimism”原则，将随机bandit问题转化为建立不确定奖金函数的信任序列。算法的性能取决于信任序列的大小，小的信任序列对实际性能和悔检保证具有更好的效果。在这项工作中，我们使用了一种新的尾部 bound for adaptive martingale mixtures来构建信任序列，这些信任序列适用于随机bandits。这些信任序列使得可以通过几何编程进行高效的动作选择。我们证明了一个基于我们的信任序列的线性bandit算法能够实现竞争性最坏情况的悔检保证。我们还证明了我们的信任序列比竞争者更紧， tantoempirically和理论上。最后，我们示出了我们的紧密信任序列可以提高一些超参数调整任务的性能。
</details></li>
</ul>
<hr>
<h2 id="On-the-Non-Associativity-of-Analog-Computations"><a href="#On-the-Non-Associativity-of-Analog-Computations" class="headerlink" title="On the Non-Associativity of Analog Computations"></a>On the Non-Associativity of Analog Computations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14292">http://arxiv.org/abs/2309.14292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lisa Kuhn, Bernhard Klein, Holger Fröning</li>
<li>for: 这种研究旨在探讨分析计算中的缺失精度问题，以及这些问题对机器学习任务的影响。</li>
<li>methods: 该研究使用了一个简单的模型来示例出实际的分析处理器中的排序效应。</li>
<li>results: 结果表明，忽略排序可能会导致准确率下降substantially。<details>
<summary>Abstract</summary>
The energy efficiency of analog forms of computing makes it one of the most promising candidates to deploy resource-hungry machine learning tasks on resource-constrained system such as mobile or embedded devices. However, it is well known that for analog computations the safety net of discretization is missing, thus all analog computations are exposed to a variety of imperfections of corresponding implementations. Examples include non-linearities, saturation effect and various forms of noise. In this work, we observe that the ordering of input operands of an analog operation also has an impact on the output result, which essentially makes analog computations non-associative, even though the underlying operation might be mathematically associative. We conduct a simple test by creating a model of a real analog processor which captures such ordering effects. With this model we assess the importance of ordering by comparing the test accuracy of a neural network for keyword spotting, which is trained based either on an ordered model, on a non-ordered variant, and on real hardware. The results prove the existence of ordering effects as well as their high impact, as neglecting ordering results in substantial accuracy drops.
</details>
<details>
<summary>摘要</summary>
“Analog计算的能源效率使得它成为部署资源受限的移动或嵌入式设备上耗费资源的最佳候选人。然而，所有的analog计算都缺乏精度的保障，因此它们暴晒于实现中的各种不稳定性，如非线性、饱和效应和各种噪声。在这个工作中，我们发现了输入操作的顺序也对输出结果产生影响，从而使analog计算变得非关联的，即使其下面的运算可能是数学上关联的。我们创建了一个模型，用于捕捉这些顺序效应。通过这个模型，我们评估了顺序的重要性，并发现忽略顺序会导致准确性下降。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DECORAIT-–-DECentralized-Opt-in-out-Registry-for-AI-Training"><a href="#DECORAIT-–-DECentralized-Opt-in-out-Registry-for-AI-Training" class="headerlink" title="DECORAIT – DECentralized Opt-in&#x2F;out Registry for AI Training"></a>DECORAIT – DECentralized Opt-in&#x2F;out Registry for AI Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14400">http://arxiv.org/abs/2309.14400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kar Balan, Alex Black, Simon Jenni, Andrew Gilbert, Andy Parsons, John Collomosse</li>
<li>For: The paper aims to address the data governance challenge faced by content creators who want to share their work openly without sanctioning its use for training AI models, and to ensure fair recognition and reward for their contributions.* Methods: The paper proposes a decentralized registry called DECORAIT, which uses hierarchical clustering and a combination of on&#x2F;off-chain storage to trace the provenance of GenAI training data and determine training consent. The registry leverages distributed ledger technology (DLT) and visual fingerprinting, and is built on the emerging C2PA standard.* Results: The paper reports a prototype of DECORAIT, which demonstrates the feasibility of using a decentralized registry to trace the provenance of GenAI training data and ensure fair recognition and reward for content creators. The prototype combines the strengths of DLT and visual fingerprinting to create a secure, open registry that can be used to express consent and data ownership for GenAI.<details>
<summary>Abstract</summary>
We present DECORAIT; a decentralized registry through which content creators may assert their right to opt in or out of AI training as well as receive reward for their contributions. Generative AI (GenAI) enables images to be synthesized using AI models trained on vast amounts of data scraped from public sources. Model and content creators who may wish to share their work openly without sanctioning its use for training are thus presented with a data governance challenge. Further, establishing the provenance of GenAI training data is important to creatives to ensure fair recognition and reward for their such use. We report a prototype of DECORAIT, which explores hierarchical clustering and a combination of on/off-chain storage to create a scalable decentralized registry to trace the provenance of GenAI training data in order to determine training consent and reward creatives who contribute that data. DECORAIT combines distributed ledger technology (DLT) with visual fingerprinting, leveraging the emerging C2PA (Coalition for Content Provenance and Authenticity) standard to create a secure, open registry through which creatives may express consent and data ownership for GenAI.
</details>
<details>
<summary>摘要</summary>
我们介绍DECORAIT，一个去中心化的数据库，让内容创作者可以选择是否参与人工智能训练，并获得创作所获得的回馈。生成型人工智能（GenAI）可以使用基于大量公开资料的人工智能模型生成图像。如果模型和内容创作者想要公开分享他们的作品而不授权其用于训练，他们面临资料管理挑战。此外，确定GenAI训练资料的起源是重要的，以确保创作者获得公平的认可和奖励。我们报告DECORAIT的原型，它使用嵌入式数据和分支链技术（DLT），并与可识别的视觉指纹（Visual Fingerprinting）集成，以创建一个可靠、公开的数据库，让创作者表达同意和资料所有权 для GenAI。
</details></li>
</ul>
<hr>
<h2 id="Learning-Risk-Aware-Quadrupedal-Locomotion-using-Distributional-Reinforcement-Learning"><a href="#Learning-Risk-Aware-Quadrupedal-Locomotion-using-Distributional-Reinforcement-Learning" class="headerlink" title="Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning"></a>Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14246">http://arxiv.org/abs/2309.14246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Schneider, Jonas Frey, Takahiro Miki, Marco Hutter</li>
<li>for: 本研究旨在帮助机器人在危险环境中进行行动，以避免意外和减少风险。</li>
<li>methods: 本研究使用分布式再决策学习来考虑安全性，并将完整的值分布来衡量机器人与环境之间的uncertainty。</li>
<li>results: 本研究在 simulate 和 ANYmal quadruped robot上实现了 emergent 风险敏感的行动行为，并且可以通过控制一个参数来调整机器人的行为风格，从而实现风险敏感性。<details>
<summary>Abstract</summary>
Deployment in hazardous environments requires robots to understand the risks associated with their actions and movements to prevent accidents. Despite its importance, these risks are not explicitly modeled by currently deployed locomotion controllers for legged robots. In this work, we propose a risk sensitive locomotion training method employing distributional reinforcement learning to consider safety explicitly. Instead of relying on a value expectation, we estimate the complete value distribution to account for uncertainty in the robot's interaction with the environment. The value distribution is consumed by a risk metric to extract risk sensitive value estimates. These are integrated into Proximal Policy Optimization (PPO) to derive our method, Distributional Proximal Policy Optimization (DPPO). The risk preference, ranging from risk-averse to risk-seeking, can be controlled by a single parameter, which enables to adjust the robot's behavior dynamically. Importantly, our approach removes the need for additional reward function tuning to achieve risk sensitivity. We show emergent risk sensitive locomotion behavior in simulation and on the quadrupedal robot ANYmal.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对危险环境部署需要机器人理解其动作和移动的风险，以避免意外。现有的步行控制器并未显式地考虑这些风险。在这项工作中，我们提出一种带有安全考虑的步行训练方法，使用分布式再增强学习来考虑安全。而不是仅仅依靠值期望，我们估算整个值分布，以考虑机器人与环境的互动不确定性。这个值分布被消耗到风险度量来提取风险敏感的价值估计。这些估计被 integrate 到 proximal policy optimization（PPO）中，得到我们的方法：分布式 proximal policy optimization（DPPO）。风险偏好，从不敢风险到敢风险，可以通过一个参数控制，以 dynamically 调整机器人的行为。这种方法可以消除需要额外奖励函数调整以实现风险敏感。我们在 simulated 和 ANYmal 四足机器人上实现了 emergent 风险敏感的步行行为。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Abstain-From-Uninformative-Data"><a href="#Learning-to-Abstain-From-Uninformative-Data" class="headerlink" title="Learning to Abstain From Uninformative Data"></a>Learning to Abstain From Uninformative Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14240">http://arxiv.org/abs/2309.14240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikai Zhang, Songzhu Zheng, Mina Dalirrooyfard, Pengxiang Wu, Anderson Schneider, Anant Raj, Yuriy Nevmyvaka, Chao Chen</li>
<li>for: 本研究探讨了在高噪音比例下学习和决策的问题，如金融或医疗领域。</li>
<li>methods: 我们提出了一种基于选择学习理论的损失函数，以及一种迭代算法，可以同时优化预测器和选择器，并在多种场景中评估其实验性能。</li>
<li>results: 我们的方法可以在具有高噪音比例的数据上提供有效的学习和决策，并且可以在训练和测试阶段处理不相关的数据。<details>
<summary>Abstract</summary>
Learning and decision-making in domains with naturally high noise-to-signal ratio, such as Finance or Healthcare, is often challenging, while the stakes are very high. In this paper, we study the problem of learning and acting under a general noisy generative process. In this problem, the data distribution has a significant proportion of uninformative samples with high noise in the label, while part of the data contains useful information represented by low label noise. This dichotomy is present during both training and inference, which requires the proper handling of uninformative data during both training and testing. We propose a novel approach to learning under these conditions via a loss inspired by the selective learning theory. By minimizing this loss, the model is guaranteed to make a near-optimal decision by distinguishing informative data from uninformative data and making predictions. We build upon the strength of our theoretical guarantees by describing an iterative algorithm, which jointly optimizes both a predictor and a selector, and evaluates its empirical performance in a variety of settings.
</details>
<details>
<summary>摘要</summary>
学习和决策在具有自然高噪声比例的领域，如金融或医疗，经常是一项挑战，而且风险很高。在这篇论文中，我们研究学习和行动在一个通用的噪声生成过程下的问题。在这个问题中，数据分布中有一定的无用样本，具有高噪声的标签，而其中一部分数据具有低噪声的有用信息。这种分化存在于训练和测试阶段，需要正确处理无用数据。我们提出了一种基于选择学习理论的新方法，通过最小化这种损失函数，使模型能够做出最佳决策。我们在理论保证的基础上描述了一种迭代算法，它同时优化一个预测器和一个选择器，并评估其实际性能。
</details></li>
</ul>
<hr>
<h2 id="Predicting-environment-effects-on-breast-cancer-by-implementing-machine-learning"><a href="#Predicting-environment-effects-on-breast-cancer-by-implementing-machine-learning" class="headerlink" title="Predicting environment effects on breast cancer by implementing machine learning"></a>Predicting environment effects on breast cancer by implementing machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14397">http://arxiv.org/abs/2309.14397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Shoaib Farooq, Mehreen Ilyas</li>
<li>for: 本研究旨在探讨环境因素对乳腺癌的发生和进程中的作用，以及这些因素对乳腺癌预后的影响。</li>
<li>methods: 本研究使用了机器学习算法，包括逻辑回归、随机森林、KNN算法、Support Vector Machine和Extra Tree Classifier，以表达预测。</li>
<li>results: 研究发现，Random Forest算法的准确率为0.91%，ROC曲线为0.901%，表示这些机器学习算法在乳腺癌存活分析中具有良好的准确性，这些技术可能成为乳腺癌预后预测的新选择。<details>
<summary>Abstract</summary>
The biggest Breast cancer is increasingly a major factor in female fatalities, overtaking heart disease. While genetic factors are important in the growth of breast cancer, new research indicates that environmental factors also play a substantial role in its occurrence and progression. The literature on the various environmental factors that may affect breast cancer risk, incidence, and outcomes is thoroughly reviewed in this study report. The study starts by looking at how lifestyle decisions, such as eating habits, exercise routines, and alcohol consumption, may affect hormonal imbalances and inflammation, two important factors driving the development of breast cancer. Additionally, it explores the part played by environmental contaminants such pesticides, endocrine-disrupting chemicals (EDCs), and industrial emissions, all of which have been linked to a higher risk of developing breast cancer due to their interference with hormone signaling and DNA damage. Algorithms for machine learning are used to express predictions. Logistic Regression, Random Forest, KNN Algorithm, SVC and extra tree classifier. Metrics including the confusion matrix correlation coefficient, F1-score, Precision, Recall, and ROC curve were used to evaluate the models. The best accuracy among all the classifiers is Random Forest with 0.91% accuracy and ROC curve 0.901% of Logistic Regression. The accuracy of the multiple algorithms for machine learning utilized in this research was good, which is important and indicates that these techniques could serve as replacement forecasting techniques in breast cancer survival analysis, notably in the Asia region.
</details>
<details>
<summary>摘要</summary>
最大的乳癌是在女性死亡中日益占据主导地位，超越心血管疾病。虽然遗传因素在乳癌增长中扮演重要角色，但新研究表明环境因素也在乳癌发生和进程中扮演了重要角色。本研究报告 thorougly  reviewed the literature on various environmental factors that may affect breast cancer risk, incidence, and outcomes. The study begins by examining how lifestyle decisions, such as dietary habits, exercise routines, and alcohol consumption, may affect hormonal imbalances and inflammation, two key factors driving the development of breast cancer. Additionally, it explores the role played by environmental pollutants such as pesticides, endocrine-disrupting chemicals (EDCs), and industrial emissions, all of which have been linked to a higher risk of developing breast cancer due to their interference with hormone signaling and DNA damage. The study used machine learning algorithms, including logistic regression, random forest, KNN algorithm, SVC, and extra tree classifier, to express predictions. Metrics including confusion matrix, correlation coefficient, F1-score, precision, recall, and ROC curve were used to evaluate the models. The best accuracy among all the classifiers was Random Forest with 0.91% accuracy and ROC curve 0.901% of logistic regression. The accuracy of the multiple machine learning algorithms used in this research was good, indicating that these techniques could serve as replacement forecasting techniques in breast cancer survival analysis, particularly in the Asia region.
</details></li>
</ul>
<hr>
<h2 id="Guess-Sketch-Language-Model-Guided-Transpilation"><a href="#Guess-Sketch-Language-Model-Guided-Transpilation" class="headerlink" title="Guess &amp; Sketch: Language Model Guided Transpilation"></a>Guess &amp; Sketch: Language Model Guided Transpilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14396">http://arxiv.org/abs/2309.14396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celine Lee, Abdulrahman Mahmoud, Michal Kurek, Simone Campanoni, David Brooks, Stephen Chong, Gu-Yeon Wei, Alexander M. Rush</li>
<li>for: 本研究旨在提高维护旧系统软件的效率，使用了学习型转换器来自动将 Assembly code 转换为其他编程语言。</li>
<li>methods: 本研究使用了一种名为 Guess &amp; Sketch 的 neurosymbolic 方法，它将 LM 和符号解决器结合在一起，以实现 Assembly code 的自动转换。</li>
<li>results: 根据实验结果，Guess &amp; Sketch 可以成功转换 57.6% 更多的 Assembly code 示例，比 GPT-4 和手动编写的转换器更高效。<details>
<summary>Abstract</summary>
Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.
</details>
<details>
<summary>摘要</summary>
维护遗传软件需要很多软件和系统工程时间。 Assembly 程式，它们需要低层次控制电脑机器状态，并没有变数名称，对人类分析而言特别困难。现有的传统程式翻译器可以保证正确性，但是它们是手工设计的源和目标程式码语言的。学习型翻译，即自动翻译程式码，提供了一个人工重新写程式码的替代方案。自动 симвоlic 程式翻译方法可以保证正确性，但是它们在更长的程式码中缺乏扩展性，因为搜索空间是指数增长的。它们的僵化规则系统也限制了它们的表达力，只能理解一个受限的程式空间。概率神经语言模型（LM）可以生成可能的输出，但是它们在交互时需要付出正确性的代价。在这个工作中，我们利用 LM 和符号方法的优点，在 assembly 程式中实现了学习型翻译。 assembly 程式可以被分成更短的非分支基本块，适合使用符号方法。 Guess & Sketch 首先从 LM 中提取对适合性和信任度的资讯，然后将其转交给符号方法以解决这个翻译任务的内涵相等性。我们在三个不同的 assembly 翻译任务上进行测试，发现 Guess & Sketch 成功翻译了 57.6% 更多的例子，比 GPT-4 和手工设计的翻译器更高。我们还提供了这个任务的训练和评估数据集。
</details></li>
</ul>
<hr>
<h2 id="Learning-Restricted-Boltzmann-Machines-with-greedy-quantum-search"><a href="#Learning-Restricted-Boltzmann-Machines-with-greedy-quantum-search" class="headerlink" title="Learning Restricted Boltzmann Machines with greedy quantum search"></a>Learning Restricted Boltzmann Machines with greedy quantum search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14196">http://arxiv.org/abs/2309.14196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liming Zhao, Aman Agrawal, Patrick Rebentrost</li>
<li>for: 扩展Restricted Boltzmann Machines（RBMs）的结构学习问题到量子计算领域，并提出相应的量子算法来解决这个问题。</li>
<li>methods: 使用量子算法来学习RBMs的结构，包括两种特定类型的RBMs：ferromagnetic RBMs和地方一致RBMs。</li>
<li>results: 对于这两种类型的RBMs，量子算法比类型的纯类型算法具有 polynomial 速度增长。<details>
<summary>Abstract</summary>
Restricted Boltzmann Machines (RBMs) are widely used probabilistic undirected graphical models with visible and latent nodes, playing an important role in statistics and machine learning. The task of structure learning for RBMs involves inferring the underlying graph by using samples from the visible nodes. Specifically, learning the two-hop neighbors of each visible node allows for the inference of the graph structure. Prior research has addressed the structure learning problem for specific classes of RBMs, namely ferromagnetic and locally consistent RBMs. In this paper, we extend the scope to the quantum computing domain and propose corresponding quantum algorithms for this problem. Our study demonstrates that the proposed quantum algorithms yield a polynomial speedup compared to the classical algorithms for learning the structure of these two classes of RBMs.
</details>
<details>
<summary>摘要</summary>
restrictive Boltzmann machines (RBMs) 是一种广泛使用的可能性图模型，具有可见节点和隐藏节点，在统计学和机器学习中扮演着重要角色。structure learning问题的解决方法是使用可见节点的样本来推断图结构。特别是，了解每个可见节点的两步邻居，可以推断出图结构。先前的研究已经对特定类型的 RBMs 进行了结构学习问题的研究， specifically ferromagnetic 和 locally consistent RBMs。在这篇论文中，我们将这个问题推广到量子计算领域，并提出相应的量子算法来解决这个问题。我们的研究表明，提议的量子算法与类icial算法相比，对于这两种类型的 RBMs 的结构学习问题，具有Polynomial Speedup。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Under-Restricted-User-Availability"><a href="#Federated-Learning-Under-Restricted-User-Availability" class="headerlink" title="Federated Learning Under Restricted User Availability"></a>Federated Learning Under Restricted User Availability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14176">http://arxiv.org/abs/2309.14176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Periklis Theodoropoulos, Konstantinos E. Nikolakakis, Dionysis Kalogerias</li>
<li>for: 这篇论文旨在提出一个可靠的联合学习框架，能够在不违反数据隐私的情况下进行联合模型训练。</li>
<li>methods: 本论文使用了一个可能随机的站台选择策略，称为随机存取模型（RAM），并提出了一个新的联合学习问题形ulation，可以有效捕捉和减少具有限制的数据参与的问题。</li>
<li>results: 实验结果显示，提出的方法可以与标准联合学习相比，在不同的设定下均表现出较好的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a decentralized machine learning framework that enables collaborative model training while respecting data privacy. In various applications, non-uniform availability or participation of users is unavoidable due to an adverse or stochastic environment, the latter often being uncontrollable during learning. Here, we posit a generic user selection mechanism implementing a possibly randomized, stationary selection policy, suggestively termed as a Random Access Model (RAM). We propose a new formulation of the FL problem which effectively captures and mitigates limited participation of data originating from infrequent, or restricted users, at the presence of a RAM. By employing the Conditional Value-at-Risk (CVaR) over the (unknown) RAM distribution, we extend the expected loss FL objective to a risk-aware objective, enabling the design of an efficient training algorithm that is completely oblivious to the RAM, and with essentially identical complexity as FedAvg. Our experiments on synthetic and benchmark datasets show that the proposed approach achieves significantly improved performance as compared with standard FL, under a variety of setups.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Designing-and-evaluating-an-online-reinforcement-learning-agent-for-physical-exercise-recommendations-in-N-of-1-trials"><a href="#Designing-and-evaluating-an-online-reinforcement-learning-agent-for-physical-exercise-recommendations-in-N-of-1-trials" class="headerlink" title="Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials"></a>Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14156">http://arxiv.org/abs/2309.14156</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hialab/reinforcement-learning-agents-in-n-of-1-trials">https://github.com/hialab/reinforcement-learning-agents-in-n-of-1-trials</a></li>
<li>paper_authors: Dominik Meier, Ipek Ensari, Stefan Konigorski</li>
<li>For: The paper is written to explore the feasibility and effectiveness of using an online reinforcement learning agent to implement personalized adaptive interventions in clinical settings.* Methods: The paper uses a novel study on physical exercise recommendations to reduce pain in endometriosis as an illustration, and describes the design of a contextual bandit recommendation agent. The agent is evaluated in simulation studies.* Results: The results show that adaptive interventions can add complexity to the design and implementation process, but have the potential to improve patients’ benefits even with limited observations. The approach is expected to be transferable to other interventions and clinical settings.Here is the information in Simplified Chinese text:* For: 本研究是为了探讨个性化适应性治疗在临床设置中的可行性和效果。* Methods: 本研究使用了一个新的物理运动推荐算法来降低悬股症的痛症，并对这种算法进行了评估。* Results: 结果显示个性化适应性治疗可能会增加设计和实施过程的复杂性，但是它们可以在有限的观察数据下提高患者的效果。这种方法预期可以在其他治疗和临床设置中传递应用。<details>
<summary>Abstract</summary>
Personalized adaptive interventions offer the opportunity to increase patient benefits, however, there are challenges in their planning and implementation. Once implemented, it is an important question whether personalized adaptive interventions are indeed clinically more effective compared to a fixed gold standard intervention. In this paper, we present an innovative N-of-1 trial study design testing whether implementing a personalized intervention by an online reinforcement learning agent is feasible and effective. Throughout, we use a new study on physical exercise recommendations to reduce pain in endometriosis for illustration. We describe the design of a contextual bandit recommendation agent and evaluate the agent in simulation studies. The results show that adaptive interventions add complexity to the design and implementation process, but have the potential to improve patients' benefits even if only few observations are available. In order to quantify the expected benefit, data from previous interventional studies is required. We expect our approach to be transferable to other interventions and clinical interventions.
</details>
<details>
<summary>摘要</summary>
个人化适应 intervención 可以提高病人的效果，但是规划和实施中存在挑战。如果实施了个人化适应 intervención，是否比静态黄金标准 intervención 更有效？在这篇论文中，我们介绍了一种新的 N-of-1 试验研究设计，用于测试个人化 intervención 是否可行和有效。我们使用了一项新的 физи exercise 推荐算法来减轻疼痛的研究，以 illustrate 我们的方法。我们描述了一种上下文 bandit 推荐代理的设计，并在模拟研究中评估了代理。结果显示，个人化 intervención 可以增加病人的效果，但是设计和实施过程可能会加入复杂性。为了量化预期的效果，需要对前一次的 intervenational 研究数据进行分析。我们预计我们的方法可以应用于其他 intervenational 和临床研究。
</details></li>
</ul>
<hr>
<h2 id="Extragradient-Type-Methods-for-Riemannian-Variational-Inequality-Problems"><a href="#Extragradient-Type-Methods-for-Riemannian-Variational-Inequality-Problems" class="headerlink" title="Extragradient Type Methods for Riemannian Variational Inequality Problems"></a>Extragradient Type Methods for Riemannian Variational Inequality Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14155">http://arxiv.org/abs/2309.14155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Hu, Guanghui Wang, Xi Wang, Andre Wibisono, Jacob Abernethy, Molei Tao<br>for: 这个论文主要研究的是偏微分方程问题（Monotone Riemannian Variational Inequality Problems，简称RVIPs）的最优化问题。methods: 这个论文提出了两种新的算法：Riemannian extragradient（REG）方法和Riemannian past extragradient（RPEG）方法，这两种方法都可以在几何扩展空间上实现最优化问题的解。results: 这个论文的结果表明，REG和RPEG方法的最后迭代都会收敛到RVIPs的解，并且这个收敛速率是$O\left(\frac{1}{\sqrt{T}\right)$。此外，这个论文还证明了REG和RPEG方法的平均迭代收敛速率是$O\left(\frac{1}{T}\right)$，这与欧几里得空间中的观察相一致。<details>
<summary>Abstract</summary>
Riemannian convex optimization and minimax optimization have recently drawn considerable attention. Their appeal lies in their capacity to adeptly manage the non-convexity of the objective function as well as constraints inherent in the feasible set in the Euclidean sense. In this work, we delve into monotone Riemannian Variational Inequality Problems (RVIPs), which encompass both Riemannian convex optimization and minimax optimization as particular cases. In the context of Euclidean space, it is established that the last-iterates of both the extragradient (EG) and past extragradient (PEG) methods converge to the solution of monotone variational inequality problems at a rate of $O\left(\frac{1}{\sqrt{T}\right)$ (Cai et al., 2022). However, analogous behavior on Riemannian manifolds remains an open question. To bridge this gap, we introduce the Riemannian extragradient (REG) and Riemannian past extragradient (RPEG) methods. We demonstrate that both exhibit $O\left(\frac{1}{\sqrt{T}\right)$ last-iterate convergence. Additionally, we show that the average-iterate convergence of both REG and RPEG is $O\left(\frac{1}{T}\right)$, aligning with observations in the Euclidean case (Mokhtari et al., 2020). These results are enabled by judiciously addressing the holonomy effect so that additional complications in Riemannian cases can be reduced and the Euclidean proof inspired by the performance estimation problem (PEP) technique or the sum-of-squares (SOS) technique can be applied again.
</details>
<details>
<summary>摘要</summary>
“里曼尼安 convex 优化和最小最大优化在最近吸引了广泛关注。它们的吸引力在于它们可以有效地处理非凸函数和约束的非凸性在欧几何上。在这篇文章中，我们深入研究幂等里曼尼变量不等式问题（RVIPs），它们包括里曼尼 convex 优化和最小最大优化为特殊情况。在欧几何空间中，已经证明了extragradient（EG）和过去extragradient（PEG）方法的最后迭代都会 converge到变量不等式问题的解的 $O\left(\frac{1}{\sqrt{T}\right)$ 速率（Cai et al., 2022）。然而，在里曼尼拓扑上的相似行为仍然是一个未解决的问题。为了桥接这个差距，我们引入里曼尼extragradient（REG）和里曼尼过去extragradient（RPEG）方法。我们证明了这两种方法的最后迭代都会 converge于 $O\left(\frac{1}{\sqrt{T}\right)$ 速率。此外，我们还证明了REG和RPEG的平均迭代速率为 $O\left(\frac{1}{T}\right)$，与欧几何空间中观察到的（Mokhtari et al., 2020）相一致。这些结果是通过谨慎地处理启动效应，使得里曼尼拓扑上的额外复杂性可以被减少，并且可以再次应用欧几何空间中的性能估计问题（PEP）技术或准则集（SOS）技术来实现。”
</details></li>
</ul>
<hr>
<h2 id="One-Class-Classification-for-Intrusion-Detection-on-Vehicular-Networks"><a href="#One-Class-Classification-for-Intrusion-Detection-on-Vehicular-Networks" class="headerlink" title="One-Class Classification for Intrusion Detection on Vehicular Networks"></a>One-Class Classification for Intrusion Detection on Vehicular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14134">http://arxiv.org/abs/2309.14134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake Guidry, Fahad Sohrab, Raju Gottumukkala, Satya Katragadda, Moncef Gabbouj</li>
<li>for: 防护 vehicular networks 中的 Controller Area Network bus 系统免受现代黑客攻击</li>
<li>methods: 使用机器学习方法进行检测和报告攻击</li>
<li>results: 试验了多种state-of-the-art一类分类方法对 Controller Area Network bus 流量中的射预攻击的效果，发现Subspace Support Vector Data Description 方法在normal operation 和被攻击时都能够最高效，Gmean 约85%<details>
<summary>Abstract</summary>
Controller Area Network bus systems within vehicular networks are not equipped with the tools necessary to ward off and protect themselves from modern cyber-security threats. Work has been done on using machine learning methods to detect and report these attacks, but common methods are not robust towards unknown attacks. These methods usually rely on there being a sufficient representation of attack data, which may not be available due to there either not being enough data present to adequately represent its distribution or the distribution itself is too diverse in nature for there to be a sufficient representation of it. With the use of one-class classification methods, this issue can be mitigated as only normal data is required to train a model for the detection of anomalous instances. Research has been done on the efficacy of these methods, most notably One-Class Support Vector Machine and Support Vector Data Description, but many new extensions of these works have been proposed and have yet to be tested for injection attacks in vehicular networks. In this paper, we investigate the performance of various state-of-the-art one-class classification methods for detecting injection attacks on Controller Area Network bus traffic. We investigate the effectiveness of these techniques on attacks launched on Controller Area Network buses from two different vehicles during normal operation and while being attacked. We observe that the Subspace Support Vector Data Description method outperformed all other tested methods with a Gmean of about 85%.
</details>
<details>
<summary>摘要</summary>
控制器网络攻击系统在交通网络中没有具备防御modern cyber安全攻击的工具。工作已经在使用机器学习方法检测和报告这些攻击，但通用方法不够鲜硬度Unknown攻击。这些方法通常需要充分的攻击数据来表征攻击的分布，但可能缺乏数据或者攻击分布太多样化，导致无法充分表征。使用一类分类方法可以解决这问题，只需要正常数据来训练模型来检测异常情况。研究表示一类支持向量数据描述法和一类支持向量分类器在检测插入攻击方面表现出色，但还没有在交通网络中进行测试。本文 investigate了一些当前顶尖一类分类方法在Controller Area Network总线上检测插入攻击的性能。我们对两辆不同的车辆在正常运行和遭受攻击时的Controller Area Network总线上的攻击进行了测试。我们发现Subspace Support Vector Data Description法的性能高于所有测试过的方法，Gmean约85%。
</details></li>
</ul>
<hr>
<h2 id="Driving-behavior-guided-battery-health-monitoring-for-electric-vehicles-using-machine-learning"><a href="#Driving-behavior-guided-battery-health-monitoring-for-electric-vehicles-using-machine-learning" class="headerlink" title="Driving behavior-guided battery health monitoring for electric vehicles using machine learning"></a>Driving behavior-guided battery health monitoring for electric vehicles using machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14125">http://arxiv.org/abs/2309.14125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nanhua Jiang, Jiawei Zhang, Weiran Jiang, Yao Ren, Jing Lin, Edwin Khoo, Ziyou Song</li>
<li>for: 提供了一种基于特征的机器学习管道，用于准确和可靠地监测电池健康状态。</li>
<li>methods: 使用了多种健康指标（HI）的合理选择和融合，以及考虑了实际驾驶行为。</li>
<li>results: 提供了一种能够考虑实际驾驶行为的功能特征选择和融合方法，以提高电池健康监测的准确性和实用性。<details>
<summary>Abstract</summary>
An accurate estimation of the state of health (SOH) of batteries is critical to ensuring the safe and reliable operation of electric vehicles (EVs). Feature-based machine learning methods have exhibited enormous potential for rapidly and precisely monitoring battery health status. However, simultaneously using various health indicators (HIs) may weaken estimation performance due to feature redundancy. Furthermore, ignoring real-world driving behaviors can lead to inaccurate estimation results as some features are rarely accessible in practical scenarios. To address these issues, we proposed a feature-based machine learning pipeline for reliable battery health monitoring, enabled by evaluating the acquisition probability of features under real-world driving conditions. We first summarized and analyzed various individual HIs with mechanism-related interpretations, which provide insightful guidance on how these features relate to battery degradation modes. Moreover, all features were carefully evaluated and screened based on estimation accuracy and correlation analysis on three public battery degradation datasets. Finally, the scenario-based feature fusion and acquisition probability-based practicality evaluation method construct a useful tool for feature extraction with consideration of driving behaviors. This work highlights the importance of balancing the performance and practicality of HIs during the development of feature-based battery health monitoring algorithms.
</details>
<details>
<summary>摘要</summary>
《 accurately estimating the state of health (SOH) of batteries is crucial for ensuring the safe and reliable operation of electric vehicles (EVs). feature-based machine learning methods have shown great potential for rapidly and precisely monitoring battery health status. however, using various health indicators (HIs) simultaneously may weaken estimation performance due to feature redundancy. furthermore, ignoring real-world driving behaviors can lead to inaccurate estimation results as some features are rarely accessible in practical scenarios. to address these issues, we proposed a feature-based machine learning pipeline for reliable battery health monitoring, enabled by evaluating the acquisition probability of features under real-world driving conditions. we first summarized and analyzed various individual HIs with mechanism-related interpretations, which provide insightful guidance on how these features relate to battery degradation modes. moreover, all features were carefully evaluated and screened based on estimation accuracy and correlation analysis on three public battery degradation datasets. finally, the scenario-based feature fusion and acquisition probability-based practicality evaluation method construct a useful tool for feature extraction with consideration of driving behaviors. this work highlights the importance of balancing the performance and practicality of HIs during the development of feature-based battery health monitoring algorithms.》Note: Please note that the translation is in Simplified Chinese, and the sentence structure and wording may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Solution-of-The-Stationary-Fokker-Plank-Equation-for-a-Class-of-Nonlinear-Dynamical-Systems-An-Evaluation-Study"><a href="#Physics-Informed-Solution-of-The-Stationary-Fokker-Plank-Equation-for-a-Class-of-Nonlinear-Dynamical-Systems-An-Evaluation-Study" class="headerlink" title="Physics-Informed Solution of The Stationary Fokker-Plank Equation for a Class of Nonlinear Dynamical Systems: An Evaluation Study"></a>Physics-Informed Solution of The Stationary Fokker-Plank Equation for a Class of Nonlinear Dynamical Systems: An Evaluation Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16725">http://arxiv.org/abs/2309.16725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussam Alhussein, Mohammed Khasawneh, Mohammed F. Daqaq<br>for:  This paper aims to present a data-free, physics-informed neural network (PINN) framework to solve the Fokker-Planck (FP) equation for a class of nonlinear stochastic dynamical systems.methods: The PINN framework uses a neural network to approximate the solution of the FP equation, without requiring any data from the system.results: The paper demonstrates the ability and accuracy of the PINN framework in predicting the probability density function (PDF) under the combined effect of additive and multiplicative noise, capturing P-bifurcations of the PDF, and effectively treating high-dimensional systems. The computational time associated with the PINN solution can be substantially reduced by using transfer learning.<details>
<summary>Abstract</summary>
The Fokker-Planck (FP) equation is a linear partial differential equation which governs the temporal and spatial evolution of the probability density function (PDF) associated with the response of stochastic dynamical systems. An exact analytical solution of the FP equation is only available for a limited subset of dynamical systems. Semi-analytical methods are available for larger, yet still a small subset of systems, while traditional computational methods; e.g. Finite Elements and Finite Difference require dividing the computational domain into a grid of discrete points, which incurs significant computational costs for high-dimensional systems. Physics-informed learning offers a potentially powerful alternative to traditional computational schemes. To evaluate its potential, we present a data-free, physics-informed neural network (PINN) framework to solve the FP equation for a class of nonlinear stochastic dynamical systems. In particular, through several examples concerning the stochastic response of the Duffing, Van der Pol, and the Duffing-Van der Pol oscillators, we assess the ability and accuracy of the PINN framework in $i)$ predicting the PDF under the combined effect of additive and multiplicative noise, $ii)$ capturing P-bifurcations of the PDF, and $iii)$ effectively treating high-dimensional systems. Through comparisons with Monte-Carlo simulations and the available literature, we show that PINN can effectively address all of the afore-described points. We also demonstrate that the computational time associated with the PINN solution can be substantially reduced by using transfer learning.
</details>
<details>
<summary>摘要</summary>
《福克-普朗克方程》是一个线性偏微分方程，其控制了杂态征函数（PDF）的时间和空间演化，该PDF与杂态动力系统的响应相关。唯一的精确分析解是仅适用于有限个动力系统中。半分析方法可以用于更大的子集，而传统计算方法，如finite element和finite difference，需要将计算Domain分成一个离散点的网格，这会带来高维系统的计算成本很高。物理学 Informed learning提供了一种可能有力的替代方案。为了评估其潜力，我们提出了一个数据自由、物理学 Informed neural network（PINN）框架，用于解决非线性杂态动力系统的福克-普朗克方程。具体来说，通过DUFFING、VAN der POL和DUFFING-VAN der POL振荡器的 einige examples，我们评估了PINN框架在下列方面的能力和准确性：1. 对于添加itive和乘数噪声的PDF预测。2. 捕捉PDF的P-分岔。3. 对高维系统的有效处理。通过与 Monte-Carlo 仿真和已有文献的比较，我们显示了PINN可以有效地解决上述问题。此外，我们还示出了使用传输学习可以将PINN解的计算时间显著减少。
</details></li>
</ul>
<hr>
<h2 id="MultiModN-Multimodal-Multi-Task-Interpretable-Modular-Networks"><a href="#MultiModN-Multimodal-Multi-Task-Interpretable-Modular-Networks" class="headerlink" title="MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks"></a>MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14118">http://arxiv.org/abs/2309.14118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/epfl-iglobalhealth/multimodn">https://github.com/epfl-iglobalhealth/multimodn</a></li>
<li>paper_authors: Vinitra Swamy, Malika Satayeva, Jibril Frej, Thierry Bossy, Thijs Vogels, Martin Jaggi, Tanja Käser, Mary-Anne Hartley</li>
<li>for: 这 paper 的目的是提出一种可靠、多任务、多模态的机器学习模型，能够在不同的模式下进行预测和检测。</li>
<li>methods: 这 paper 使用了一种名为 MultiModN 的多模态、模块化网络，通过序列化多种数据类型的Feature Space来提高预测性能和可解释性。</li>
<li>results:  experiments 表明，MultiModN 在多个 benchmark 数据集上表现出色，能够在不同的模式下进行预测和检测，而且在面临不够数据时并不会出现 catastrophic failure。<details>
<summary>Abstract</summary>
Predicting multiple real-world tasks in a single model often requires a particularly diverse feature space. Multimodal (MM) models aim to extract the synergistic predictive potential of multiple data types to create a shared feature space with aligned semantic meaning across inputs of drastically varying sizes (i.e. images, text, sound). Most current MM architectures fuse these representations in parallel, which not only limits their interpretability but also creates a dependency on modality availability. We present MultiModN, a multimodal, modular network that fuses latent representations in a sequence of any number, combination, or type of modality while providing granular real-time predictive feedback on any number or combination of predictive tasks. MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness. We perform four experiments on several benchmark MM datasets across 10 real-world tasks (predicting medical diagnoses, academic performance, and weather), and show that MultiModN's sequential MM fusion does not compromise performance compared with a baseline of parallel fusion. By simulating the challenging bias of missing not-at-random (MNAR), this work shows that, contrary to MultiModN, parallel fusion baselines erroneously learn MNAR and suffer catastrophic failure when faced with different patterns of MNAR at inference. To the best of our knowledge, this is the first inherently MNAR-resistant approach to MM modeling. In conclusion, MultiModN provides granular insights, robustness, and flexibility without compromising performance.
</details>
<details>
<summary>摘要</summary>
多任务多模式（MM）模型目的是抽取多种数据类型的共同预测潜力，创建具有不同大小和类型的输入数据中共同含义的共享特征空间。大多数当前的MM架构使用平行融合这些表示，不仅限制了它们的可解释性，还受到数据类型可用性的限制。我们介绍了MultiModN，一种多模态、模块化网络，可以在任意数量、组合或类型的模态中融合干ARN表示，并提供精细的实时预测反馈。MultiModN的可组合管道是设计可解释的，同时也是自然多任务和鲁棒于基本问题的偏见缺失。我们在多个MM数据集上进行了四个实验，测试了MultiModN在10个实际任务（预测医疾诊断、学术表现和天气）上的性能。结果显示，MultiModN的顺序MM融合不会Compromise performance相比基线Parallel融合。通过模拟偏见缺失（MNAR）的挑战，这个工作表明，与MultiModN不同，平行融合基elines会在不同的MNAR挑战时erroneously learn MNAR并遭受极端的失败。在我们知道的范围内，这是首个自然具有MNAR抗性的MM模型。因此，MultiModN提供了细化的洞察、鲁棒性和灵活性，无需牺牲性能。
</details></li>
</ul>
<hr>
<h2 id="HyperTrack-Neural-Combinatorics-for-High-Energy-Physics"><a href="#HyperTrack-Neural-Combinatorics-for-High-Energy-Physics" class="headerlink" title="HyperTrack: Neural Combinatorics for High Energy Physics"></a>HyperTrack: Neural Combinatorics for High Energy Physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14113">http://arxiv.org/abs/2309.14113</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mieskolainen/hypertrack">https://github.com/mieskolainen/hypertrack</a></li>
<li>paper_authors: Mikael Mieskolainen</li>
<li>for: 这个论文是为了解决高能物理中的 combinatorial inverse problems 而写的。</li>
<li>methods: 这个论文使用了一种新的深度学习驱动的 clustering 算法，该算法使用了空间时间非本地可学习图构建器、图神经网络和集成变换器。模型通过节点、边和对象层的损失函数进行训练，包括对比学习和元级超级视图。</li>
<li>results: 作者通过 partiicle tracking  simulations 表明了这种前导AI方法的有效性。代码可以在线获取。<details>
<summary>Abstract</summary>
Combinatorial inverse problems in high energy physics span enormous algorithmic challenges. This work presents a new deep learning driven clustering algorithm that utilizes a space-time non-local trainable graph constructor, a graph neural network, and a set transformer. The model is trained with loss functions at the graph node, edge and object level, including contrastive learning and meta-supervision. The algorithm can be applied to problems such as charged particle tracking, calorimetry, pile-up discrimination, jet physics, and beyond. We showcase the effectiveness of this cutting-edge AI approach through particle tracking simulations. The code is available online.
</details>
<details>
<summary>摘要</summary>
高能物理中的 combinatorial inverse problems 涉及到庞大的算法挑战。这项工作提出了一种新的深度学习驱动 clustering 算法，使用空间-时非本地可学习图构建器、图神经网络和集Transformer。该模型通过图节、边和对象层的损失函数进行训练，包括对比学习和超级监督。该算法可以应用于荷电粒子跟踪、calorimetry、堆积排除、jet物理和更多。我们通过粒子跟踪模拟显示了这种前沿人工智能方法的效果。代码可在线获取。
</details></li>
</ul>
<hr>
<h2 id="Affective-Game-Computing-A-Survey"><a href="#Affective-Game-Computing-A-Survey" class="headerlink" title="Affective Game Computing: A Survey"></a>Affective Game Computing: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14104">http://arxiv.org/abs/2309.14104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Georgios N. Yannakakis, David Melhart</li>
<li>for: 这篇论文探讨了现代情感计算原理、方法和工具在游戏领域的应用，即情感游戏计算。</li>
<li>methods: 论文通过四个核心phasis of the affective loop：游戏情感诱发、游戏情感感知、游戏情感检测和游戏情感适应来进行了评查。</li>
<li>results: 论文提供了情感游戏计算领域的一份综述，并在这一领域中进行了一系列分析和评估。<details>
<summary>Abstract</summary>
This paper surveys the current state of the art in affective computing principles, methods and tools as applied to games. We review this emerging field, namely affective game computing, through the lens of the four core phases of the affective loop: game affect elicitation, game affect sensing, game affect detection and game affect adaptation. In addition, we provide a taxonomy of terms, methods and approaches used across the four phases of the affective game loop and situate the field within this taxonomy. We continue with a comprehensive review of available affect data collection methods with regards to gaming interfaces, sensors, annotation protocols, and available corpora. The paper concludes with a discussion on the current limitations of affective game computing and our vision for the most promising future research directions in the field.
</details>
<details>
<summary>摘要</summary>
We then review available affect data collection methods for gaming interfaces, sensors, annotation protocols, and corpora. Finally, we discuss the current limitations of affective game computing and outline the most promising future research directions in the field.Here is the text in Simplified Chinese:这篇论文介绍了现代情感计算原则、方法和工具在游戏领域的应用。我们通过分析四个核心阶段的情感循环来评估这个新兴领域：游戏情感诱发、游戏情感感知、游戏情感检测和游戏情感适应。此外，我们提供了情感循环中不同阶段的术语、方法和approaches的分类，并将该领域置于这种分类中。接下来，我们进行了有关游戏界面、感知器、注释协议和可用数据集的情感数据收集方法的全面回顾。最后，我们讨论了情感游戏计算的当前局限性，并提出了未来研究的最有前途的方向。
</details></li>
</ul>
<hr>
<h2 id="Tracking-Control-for-a-Spherical-Pendulum-via-Curriculum-Reinforcement-Learning"><a href="#Tracking-Control-for-a-Spherical-Pendulum-via-Curriculum-Reinforcement-Learning" class="headerlink" title="Tracking Control for a Spherical Pendulum via Curriculum Reinforcement Learning"></a>Tracking Control for a Spherical Pendulum via Curriculum Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14096">http://arxiv.org/abs/2309.14096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Klink, Florian Wolf, Kai Ploeger, Jan Peters, Joni Pajarinen</li>
<li>for: 学习非平凡机器人控制法则，不需要手动设计规则。</li>
<li>methods: 使用最新的自动生成课程算法和大规模并行计算，通过改进的优化方案，以更好地识别非欧几何任务结构，从而更快速、更稳定地学习控制器。</li>
<li>results: 学习策略与优化基线相当，在真实系统上达到了类似于最优控制策略的性能。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) allows learning non-trivial robot control laws purely from data. However, many successful applications of RL have relied on ad-hoc regularizations, such as hand-crafted curricula, to regularize the learning performance. In this paper, we pair a recent algorithm for automatically building curricula with RL on massively parallelized simulations to learn a tracking controller for a spherical pendulum on a robotic arm via RL. Through an improved optimization scheme that better respects the non-Euclidean task structure, we allow the method to reliably generate curricula of trajectories to be tracked, resulting in faster and more robust learning compared to an RL baseline that does not exploit this form of structured learning. The learned policy matches the performance of an optimal control baseline on the real system, demonstrating the potential of curriculum RL to jointly learn state estimation and control for non-linear tracking tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Benefit-of-Optimal-Transport-for-Curriculum-Reinforcement-Learning"><a href="#On-the-Benefit-of-Optimal-Transport-for-Curriculum-Reinforcement-Learning" class="headerlink" title="On the Benefit of Optimal Transport for Curriculum Reinforcement Learning"></a>On the Benefit of Optimal Transport for Curriculum Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14091">http://arxiv.org/abs/2309.14091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Klink, Carlo D’Eramo, Jan Peters, Joni Pajarinen</li>
<li>for:  solves complex tasks by generating a tailored sequence of learning tasks</li>
<li>methods: uses interpolations between task distributions to generate curricula</li>
<li>results: improves upon existing CRL methods and achieves high performance in various tasks<details>
<summary>Abstract</summary>
Curriculum reinforcement learning (CRL) allows solving complex tasks by generating a tailored sequence of learning tasks, starting from easy ones and subsequently increasing their difficulty. Although the potential of curricula in RL has been clearly shown in various works, it is less clear how to generate them for a given learning environment, resulting in various methods aiming to automate this task. In this work, we focus on framing curricula as interpolations between task distributions, which has previously been shown to be a viable approach to CRL. Identifying key issues of existing methods, we frame the generation of a curriculum as a constrained optimal transport problem between task distributions. Benchmarks show that this way of curriculum generation can improve upon existing CRL methods, yielding high performance in various tasks with different characteristics.
</details>
<details>
<summary>摘要</summary>
使用简化中文翻译文本。</SYS>学习补充课程（CRL）可以解决复杂任务，通过生成适应性较高的学习任务序列，从易于学习的任务开始，然后逐渐增加Difficulty。虽然CRL的潜力已经在不同的研究中得到了证明，但是如何为给定的学习环境生成课程，还是一个不够清楚的问题，因此有多种方法试图自动化这个任务。在这个工作中，我们将关注将课程框架为 interpolations between task distributions，这种方法在过去已经被证明是CRL的可能的方法。从exististing方法的角度，我们将生成课程的问题定义为constrained optimal transport problem between task distributions。 benchmark表明，这种方法可以超越现有的CRL方法，在不同的任务特点下实现高性能。
</details></li>
</ul>
<hr>
<h2 id="BiSinger-Bilingual-Singing-Voice-Synthesis"><a href="#BiSinger-Bilingual-Singing-Voice-Synthesis" class="headerlink" title="BiSinger: Bilingual Singing Voice Synthesis"></a>BiSinger: Bilingual Singing Voice Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14089">http://arxiv.org/abs/2309.14089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huali Zhou, Yueqian Lin, Yao Shi, Peng Sun, Ming Li</li>
<li>for: 该研究旨在开拓多语言歌唱Synthetic Voice（SVS）领域，提出一种可以同时模拟英语和中文普通话的BiSinger系统。</li>
<li>methods: 该系统使用CMU字典和映射规则实现语言共享表示，并将单语言歌唱数据与开源歌唱voice转换技术相结合，生成双语歌唱声音。</li>
<li>results: 实验表明，BiSinger系统可以在英语和中文普通话之间进行自由融合，同时保持中文歌曲表现。音频样本可以在<a target="_blank" rel="noopener" href="https://bisinger-svs.github.io/">https://bisinger-svs.github.io</a>中找到。<details>
<summary>Abstract</summary>
Although Singing Voice Synthesis (SVS) has made great strides with Text-to-Speech (TTS) techniques, multilingual singing voice modeling remains relatively unexplored. This paper presents BiSinger, a bilingual pop SVS system for English and Chinese Mandarin. Current systems require separate models per language and cannot accurately represent both Chinese and English, hindering code-switch SVS. To address this gap, we design a shared representation between Chinese and English singing voices, achieved by using the CMU dictionary with mapping rules. We fuse monolingual singing datasets with open-source singing voice conversion techniques to generate bilingual singing voices while also exploring the potential use of bilingual speech data. Experiments affirm that our language-independent representation and incorporation of related datasets enable a single model with enhanced performance in English and code-switch SVS while maintaining Chinese song performance. Audio samples are available at https://bisinger-svs.github.io.
</details>
<details>
<summary>摘要</summary>
尽管Singing Voice Synthesis（SVS）已经在文本到语音（TTS）技术方面做出了很大的进步，但多语言歌声模型还尚未得到充分的探索。这篇论文介绍了BiSinger，一个拥有英语和中文普通话的双语PoP SVS系统。现有系统需要separate的模型来处理不同的语言，而这会导致不能准确地表示英语和中文，从而限制了码换SVS。为解决这个难题，我们设计了共享表示 между英语和中文歌声，通过使用CMU字典和映射规则来实现。我们将单语言歌声数据与开源的歌声voice转换技术相结合，以生成双语歌声，同时也在探索使用双语言言言言数据。实验证明了我们的语言独立表示和相关数据的 incorporation 使得单个模型在英语和码换SVS中具有提高的性能，同时保持中文歌曲表现。音频示例可以在https://bisinger-svs.github.io中找到。
</details></li>
</ul>
<hr>
<h2 id="REPA-Client-Clustering-without-Training-and-Data-Labels-for-Improved-Federated-Learning-in-Non-IID-Settings"><a href="#REPA-Client-Clustering-without-Training-and-Data-Labels-for-Improved-Federated-Learning-in-Non-IID-Settings" class="headerlink" title="REPA: Client Clustering without Training and Data Labels for Improved Federated Learning in Non-IID Settings"></a>REPA: Client Clustering without Training and Data Labels for Improved Federated Learning in Non-IID Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14088">http://arxiv.org/abs/2309.14088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Radovič, Veljko Pejović</li>
<li>for: 提高非独立并且同样分布的数据设置下的联合学习（Federated Learning，FL）性能，通过对客户端进行分组，以实现更好的数据分布匹配。</li>
<li>methods: 使用一种新的超级vised autoencoder-based方法，创建不需要本地训练和服务器数据暴露的客户端嵌入，以profile客户端的下游数据生成过程。</li>
<li>results: 对三个不同的数据集进行实验分析，显示REPA可以提供最佳模型性能，同时扩展联合学习的应用范围，覆盖之前未经考虑的用 caso。<details>
<summary>Abstract</summary>
Clustering clients into groups that exhibit relatively homogeneous data distributions represents one of the major means of improving the performance of federated learning (FL) in non-independent and identically distributed (non-IID) data settings. Yet, the applicability of current state-of-the-art approaches remains limited as these approaches cluster clients based on information, such as the evolution of local model parameters, that is only obtainable through actual on-client training. On the other hand, there is a need to make FL models available to clients who are not able to perform the training themselves, as they do not have the processing capabilities required for training, or simply want to use the model without participating in the training. Furthermore, the existing alternative approaches that avert the training still require that individual clients have a sufficient amount of labeled data upon which the clustering is based, essentially assuming that each client is a data annotator. In this paper, we present REPA, an approach to client clustering in non-IID FL settings that requires neither training nor labeled data collection. REPA uses a novel supervised autoencoder-based method to create embeddings that profile a client's underlying data-generating processes without exposing the data to the server and without requiring local training. Our experimental analysis over three different datasets demonstrates that REPA delivers state-of-the-art model performance while expanding the applicability of cluster-based FL to previously uncovered use cases.
</details>
<details>
<summary>摘要</summary>
clustering 客户端到组合显示相对同质数据分布的组合是非独立和同样分布（非-IID）数据设置中提高联邦学习（FL）性能的一种主要方法。然而，现有的现状之前方法的应用范围仍然受限，因为这些方法基于本地模型参数的演化获取信息来分组客户端。在另一方面，有一个需要使联邦学习模型可用于无法进行训练的客户端，因为他们没有训练所需的处理能力或者只想使用模型而不参与训练。此外，现有的备用方法都需要每个客户端都具备足够量的标注数据，即每个客户端都是一名数据注释者。在这篇论文中，我们提出了一种不需要训练也不需要标注数据的客户端分组方法，称为REPA。REPA使用一种新的监督式自动encoder方法来创建嵌入，这些嵌入 profiling客户端的下游数据生成过程，无需服务器暴露数据，也无需本地训练。我们在三个不同的数据集上进行了实验分析，结果表明，REPA可以提供状态之前的模型性能，同时扩大了基于分组的联邦学习的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Diversify-and-Conquer-Bandits-and-Diversity-for-an-Enhanced-E-commerce-Homepage-Experience"><a href="#Diversify-and-Conquer-Bandits-and-Diversity-for-an-Enhanced-E-commerce-Homepage-Experience" class="headerlink" title="Diversify and Conquer: Bandits and Diversity for an Enhanced E-commerce Homepage Experience"></a>Diversify and Conquer: Bandits and Diversity for an Enhanced E-commerce Homepage Experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14046">http://arxiv.org/abs/2309.14046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangeet Jaiswal, Korah T Malayil, Saif Jawaid, Sreekanth Vempati</li>
<li>For: 本研究旨在提高电子商务平台上的推荐广告和产品的效果，具体来说是通过vertical widget reordering来个性化推荐widget。* Methods: 本研究使用了contextual multi-arm bandit问题和增强层来实现个性化推荐。* Results: 通过在Myntra proprietary数据上进行线上和线下A&#x2F;B测试，研究发现该方法可以提高推荐的效果。<details>
<summary>Abstract</summary>
In the realm of e-commerce, popular platforms utilize widgets to recommend advertisements and products to their users. However, the prevalence of mobile device usage on these platforms introduces a unique challenge due to the limited screen real estate available. Consequently, the positioning of relevant widgets becomes pivotal in capturing and maintaining customer engagement. Given the restricted screen size of mobile devices, widgets placed at the top of the interface are more prominently displayed and thus attract greater user attention. Conversely, widgets positioned further down the page require users to scroll, resulting in reduced visibility and subsequent lower impression rates. Therefore it becomes imperative to place relevant widgets on top. However, selecting relevant widgets to display is a challenging task as the widgets can be heterogeneous, widgets can be introduced or removed at any given time from the platform. In this work, we model the vertical widget reordering as a contextual multi-arm bandit problem with delayed batch feedback. The objective is to rank the vertical widgets in a personalized manner. We present a two-stage ranking framework that combines contextual bandits with a diversity layer to improve the overall ranking. We demonstrate its effectiveness through offline and online A/B results, conducted on proprietary data from Myntra, a major fashion e-commerce platform in India.
</details>
<details>
<summary>摘要</summary>
在电商领域中，流行的平台通过Widget来推荐广告和产品给他们的用户。然而，由于移动设备的使用，导致了屏幕空间的限制，这意味着Widget的位置变得非常重要，以确保维持用户的兴趣。由于移动设备的屏幕尺寸有限，位于页面顶部的Widget更加抢耳，因此吸引更多的用户注意力。相反，位于页面底部的Widget需要用户滚动，这会导致它们的可见性减少，并最终导致吸引率下降。因此，需要将相关的Widget放在顶部。然而，选择需要显示的Widget是一项困难的任务，因为Widget可以是不同的，并且可以在任何时候从平台中引入或删除。在这种情况下，我们模型了垂直Widget重新排序为Contextual多臂抽象问题。我们的目标是个性化排序垂直Widget。我们提出了一个两个阶段的排名框架， combinatesContextual bandits with a diversity layer，以提高总体排名的效果。我们通过在Myntra，一家主要的印度电商平台上进行的实验和在线A/B测试，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Imitation-Learning-for-Stochastic-Environments"><a href="#Hierarchical-Imitation-Learning-for-Stochastic-Environments" class="headerlink" title="Hierarchical Imitation Learning for Stochastic Environments"></a>Hierarchical Imitation Learning for Stochastic Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14003">http://arxiv.org/abs/2309.14003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Igl, Punit Shah, Paul Mougin, Sirish Srinivasan, Tarun Gupta, Brandyn White, Kyriacos Shiarlis, Shimon Whiteson</li>
<li>for: 该论文旨在提高自适应学习中代理人的行为模型，以便在训练数据中生成完整的行为分布。</li>
<li>methods: 该论文提出了Robust Type Conditioning（RTC）方法，通过对于随机类型的 adversarial 训练来消除环境噪声导致的分布变换。</li>
<li>results: 实验结果表明，RTC 方法可以提高代理人的行为模型的分布实际性，同时保持或改善任务性能，对于两个领域的大规模实验结果都表现出色。<details>
<summary>Abstract</summary>
Many applications of imitation learning require the agent to generate the full distribution of behaviour observed in the training data. For example, to evaluate the safety of autonomous vehicles in simulation, accurate and diverse behaviour models of other road users are paramount. Existing methods that improve this distributional realism typically rely on hierarchical policies. These condition the policy on types such as goals or personas that give rise to multi-modal behaviour. However, such methods are often inappropriate for stochastic environments where the agent must also react to external factors: because agent types are inferred from the observed future trajectory during training, these environments require that the contributions of internal and external factors to the agent behaviour are disentangled and only internal factors, i.e., those under the agent's control, are encoded in the type. Encoding future information about external factors leads to inappropriate agent reactions during testing, when the future is unknown and types must be drawn independently from the actual future. We formalize this challenge as distribution shift in the conditional distribution of agent types under environmental stochasticity. We propose Robust Type Conditioning (RTC), which eliminates this shift with adversarial training under randomly sampled types. Experiments on two domains, including the large-scale Waymo Open Motion Dataset, show improved distributional realism while maintaining or improving task performance compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)许多学习借风溯行情景中需要智能机器人生成训练数据中全部行为分布的完整分布。例如，评估自动驾驶车辆在模拟环境中的安全性，需要准确且多样化的其他路用者行为模型。现有的方法通常通过层次政策来提高这种分布真实性，这些政策根据目标或人格类型来生成多模样式的行为。然而，这些方法在随机环境中不适用，因为在训练过程中探测到的未来轨迹会影响智能机器人的行为，因此需要在类型编码中分离内部和外部因素的贡献。编码未来环境信息会导致测试时的不合适机器人反应，因为未来是未知的，类型必须从实际未来中独立地采样。我们将这种挑战称为环境随机性导致的类型 conditional distribution 的分布偏移。我们提议Robust Type Conditioning（RTC），通过对随机类型进行对抗训练来消除这种偏移。实验结果表明，RTC在两个领域中提高了分布真实性，同时保持或提高了任务性能，相比于现有的基elines。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Mixtures-of-Discrete-Product-Distributions-in-Near-Optimal-Sample-and-Time-Complexity"><a href="#Identification-of-Mixtures-of-Discrete-Product-Distributions-in-Near-Optimal-Sample-and-Time-Complexity" class="headerlink" title="Identification of Mixtures of Discrete Product Distributions in Near-Optimal Sample and Time Complexity"></a>Identification of Mixtures of Discrete Product Distributions in Near-Optimal Sample and Time Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13993">http://arxiv.org/abs/2309.13993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer L. Gordon, Erik Jahn, Bijan Mazaheri, Yuval Rabani, Leonard J. Schulman</li>
<li>for: 本研究旨在从统计数据中识别一个混合型杂度分布 $X_1,\ldots,X_n$，其中每个变量 $X_i$ 是一个独立的离散随机变量。</li>
<li>methods: 本研究使用了一种类似于 robust tensor decomposition 的方法，并利用了一种新的约束矩阵的condition number bounding方法，称为 Hadamard extensions。</li>
<li>results: 本研究显示，对于任何 $n\geq 2k-1$，可以在 $(1&#x2F;\zeta)^{O(k)}$ 的样本复杂度和时间复杂度下识别混合型杂度分布 $X_1,\ldots,X_n$。此外，我们还扩展了 $e^{\Omega(k)}$ 的下界，使其与我们的上界相匹配，并且这种下界适用于各种不同的 $\zeta$。<details>
<summary>Abstract</summary>
We consider the problem of identifying, from statistics, a distribution of discrete random variables $X_1,\ldots,X_n$ that is a mixture of $k$ product distributions. The best previous sample complexity for $n \in O(k)$ was $(1/\zeta)^{O(k^2 \log k)}$ (under a mild separation assumption parameterized by $\zeta$). The best known lower bound was $\exp(\Omega(k))$. It is known that $n\geq 2k-1$ is necessary and sufficient for identification. We show, for any $n\geq 2k-1$, how to achieve sample complexity and run-time complexity $(1/\zeta)^{O(k)}$. We also extend the known lower bound of $e^{\Omega(k)}$ to match our upper bound across a broad range of $\zeta$. Our results are obtained by combining (a) a classic method for robust tensor decomposition, (b) a novel way of bounding the condition number of key matrices called Hadamard extensions, by studying their action only on flattened rank-1 tensors.
</details>
<details>
<summary>摘要</summary>
我们考虑一个统计方面的问题，即从分布统计中识别 $X_1,\ldots,X_n$ 是一个由 $k$ 个产品分布组成的混合分布。最好的前一个样本复杂性为 $(1/\zeta)^{O(k^2 \log k)}$（受到 $\zeta$ 的宽度假设），而最佳知识下界为 $\exp(\Omega(k))$。我们知道 $n\geq 2k-1$ 是必要和充分的条件。我们示出，对于任何 $n\geq 2k-1$，可以实现样本复杂性和运行时间复杂性 $(1/\zeta)^{O(k)}$。我们还扩展了知识下界，使其与我们的上界在广泛的 $\zeta$ 范围内匹配。我们的结果来自于将（a）纯粹的稳定矩阵分解方法与（b）一种约束矩阵的condition数 bounds的新方法结合在一起，通过研究这些矩阵在扁平 rank-1 张量上的行为来获得 Hadamard 扩展。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective"><a href="#A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective" class="headerlink" title="A Novel Approach for Effective Multi-View Clustering with Information-Theoretic Perspective"></a>A Novel Approach for Effective Multi-View Clustering with Information-Theoretic Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13989">http://arxiv.org/abs/2309.13989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gzcch/A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective-SUMVC">https://github.com/gzcch/A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective-SUMVC</a></li>
<li>paper_authors: Chenhang Cui, Yazhou Ren, Jingyu Pu, Jiawei Li, Xiaorong Pu, Tianyi Wu, Yutao Shi, Lifang He</li>
<li>for: 提高多视图数据 clustering 性能 using various data sources.</li>
<li>methods: 使用 variational analysis 生成一致信息，并提出一种 suficient representation lower bound 来增强一致信息和减少视图中的无用信息.</li>
<li>results: 在理论分析和多个多视图数据集上，SUMVC 方法表现出优于传统方法，提供了一种新的多视图数据分析的视角.<details>
<summary>Abstract</summary>
Multi-view clustering (MVC) is a popular technique for improving clustering performance using various data sources. However, existing methods primarily focus on acquiring consistent information while often neglecting the issue of redundancy across multiple views. This study presents a new approach called Sufficient Multi-View Clustering (SUMVC) that examines the multi-view clustering framework from an information-theoretic standpoint. Our proposed method consists of two parts. Firstly, we develop a simple and reliable multi-view clustering method SCMVC (simple consistent multi-view clustering) that employs variational analysis to generate consistent information. Secondly, we propose a sufficient representation lower bound to enhance consistent information and minimise unnecessary information among views. The proposed SUMVC method offers a promising solution to the problem of multi-view clustering and provides a new perspective for analyzing multi-view data.   To verify the effectiveness of our model, we conducted a theoretical analysis based on the Bayes Error Rate, and experiments on multiple multi-view datasets demonstrate the superior performance of SUMVC.
</details>
<details>
<summary>摘要</summary>
多视图划分（MVC）是一种广泛使用的技术，以提高划分性能使用多种数据源。然而，现有方法主要强调获取一致信息，经常忽略多视图之间的重复性问题。本研究提出了一种新的方法，即足够多视图划分（SUMVC），它从信息论角度探讨多视图划分框架。我们的提议方法包括两部分：首先，我们开发了一种简单可靠的多视图划分方法SCMVC（简单一致多视图划分），使用变量分析生成一致信息。其次，我们提出了一种足够表示下界，以增强一致信息并最小化多视图之间的无用信息。提出的 SUMVC 方法提供了多视图划分问题的有效解决方案，并提供了新的多视图数据分析的视角。为证明我们的模型的有效性，我们基于 bayes 错误率进行了理论分析，并在多个多视图数据集上进行了实验，展示了 SUMVC 的超越性。
</details></li>
</ul>
<hr>
<h2 id="Physics-Driven-ML-Based-Modelling-for-Correcting-Inverse-Estimation"><a href="#Physics-Driven-ML-Based-Modelling-for-Correcting-Inverse-Estimation" class="headerlink" title="Physics-Driven ML-Based Modelling for Correcting Inverse Estimation"></a>Physics-Driven ML-Based Modelling for Correcting Inverse Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13985">http://arxiv.org/abs/2309.13985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiyuan Kang, Tingting Mu, Panos Liatsis, Dimitrios C. Kyritsis</li>
<li>for: 避免机器学习估算失败，尤其在科学和工程领域（SAE）中，以避免严重的后果，如飞机引擎设计。这项工作关注于探测和修复机器学习估算时的失败状态，通过使用模拟和基于物理法律的性能指标来做到这一点。</li>
<li>methods: 使用模拟和性能指标 guid by physical laws， flag 机器学习估算失败并提出一种新的方法GEESE，通过优化来实现低误差和高效率。GEESE的关键设计包括（1）一种混合模拟错误模型，以减少模拟成本和启用基于错误反馈的梯度循环，以及（2）两种生成模型，用于模拟候选状态的演示和探索行为。所有三个模型均为神经网络。</li>
<li>results: GEESE在三个真实的 SAE 逆问题上进行测试，与一些现有的优化&#x2F;搜索方法进行比较。结果表明，GEESE最少失败次数，并且通常需要物理评估更少次。<details>
<summary>Abstract</summary>
When deploying machine learning estimators in science and engineering (SAE) domains, it is critical to avoid failed estimations that can have disastrous consequences, e.g., in aero engine design. This work focuses on detecting and correcting failed state estimations before adopting them in SAE inverse problems, by utilizing simulations and performance metrics guided by physical laws. We suggest to flag a machine learning estimation when its physical model error exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it through optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE include (1) a hybrid surrogate error model to provide fast error estimations to reduce simulation cost and to enable gradient based backpropagation of error feedback, and (2) two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviours. All three models are constructed as neural networks. GEESE is tested on three real-world SAE inverse problems and compared to a number of state-of-the-art optimization/search approaches. Results show that it fails the least number of times in terms of finding a feasible state correction, and requires physical evaluations less frequently in general.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A hybrid surrogate error model to provide fast error estimations and reduce simulation cost, enabling gradient-based backpropagation of error feedback.2. Two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviors. All three models are constructed as neural networks.GEESE is tested on three real-world SAE inverse problems and compared to several state-of-the-art optimization&#x2F;search approaches. Results show that it fails the least number of times in terms of finding a feasible state correction, and requires physical evaluations less frequently in general.</details></li>
</ol>
<hr>
<h2 id="Newton-Method-based-Subspace-Support-Vector-Data-Description"><a href="#Newton-Method-based-Subspace-Support-Vector-Data-Description" class="headerlink" title="Newton Method-based Subspace Support Vector Data Description"></a>Newton Method-based Subspace Support Vector Data Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13960">http://arxiv.org/abs/2309.13960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahad Sohrab, Firas Laakom, Moncef Gabbouj</li>
<li>for: 本文提出了一种基于新トン方法的S-SVDD优化方法，以优化一类分类中的数据映射和描述。</li>
<li>methods: 本文使用了Newton方法来优化数据映射和描述，以提高一类分类中的子空间学习。</li>
<li>results: 实验结果表明，提出的优化策略比 gradient-based S-SVDD 在大多数情况下表现更好。<details>
<summary>Abstract</summary>
In this paper, we present an adaptation of Newton's method for the optimization of Subspace Support Vector Data Description (S-SVDD). The objective of S-SVDD is to map the original data to a subspace optimized for one-class classification, and the iterative optimization process of data mapping and description in S-SVDD relies on gradient descent. However, gradient descent only utilizes first-order information, which may lead to suboptimal results. To address this limitation, we leverage Newton's method to enhance data mapping and data description for an improved optimization of subspace learning-based one-class classification. By incorporating this auxiliary information, Newton's method offers a more efficient strategy for subspace learning in one-class classification as compared to gradient-based optimization. The paper discusses the limitations of gradient descent and the advantages of using Newton's method in subspace learning for one-class classification tasks. We provide both linear and nonlinear formulations of Newton's method-based optimization for S-SVDD. In our experiments, we explored both the minimization and maximization strategies of the objective. The results demonstrate that the proposed optimization strategy outperforms the gradient-based S-SVDD in most cases.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了新顿方法在SVDD（Subspace Support Vector Data Description）优化中的应用。S-SVDD的目标是将原始数据映射到优化一类分类的子空间，而S-SVDD的迭代优化过程中的数据映射和描述逻辑依赖于梯度下降。然而，梯度下降只使用一阶信息，可能会导致优化结果不佳。为了解决这种限制，我们利用新顿方法来增强数据映射和描述，从而提高子空间学习基于一类分类的优化。通过利用这些辅助信息，新顿方法在子空间学习中提供了更高效的优化策略，比梯度下降更有效。文章讨论了梯度下降的局限性和新顿方法在子空间学习中的优势，并提供了线性和非线性形式的新顿方法基于优化方法。我们在实验中探索了最小化和最大化目标的两种策略。结果表明，我们提出的优化策略在大多数情况下超越了梯度下降基于S-SVDD的优化。
</details></li>
</ul>
<hr>
<h2 id="Beam-Enumeration-Probabilistic-Explainability-For-Sample-Efficient-Self-conditioned-Molecular-Design"><a href="#Beam-Enumeration-Probabilistic-Explainability-For-Sample-Efficient-Self-conditioned-Molecular-Design" class="headerlink" title="Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design"></a>Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13957">http://arxiv.org/abs/2309.13957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/schwallergroup/augmented_memory">https://github.com/schwallergroup/augmented_memory</a></li>
<li>paper_authors: Jeff Guo, Philippe Schwaller</li>
<li>for:  This paper aims to improve the explainability and sample efficiency of generative molecular design.</li>
<li>methods: The paper proposes Beam Enumeration, a method that exhaustively enumerates the most probable sub-sequences from language-based molecular generative models and extracts meaningful molecular substructures.</li>
<li>results: The proposed method improves the performance of the recently reported Augmented Memory algorithm, achieving better sample efficiency and generating more high-reward molecules.Here’s the Chinese translation:</li>
<li>for: 本研究旨在提高分子设计的可解释性和样本效率。</li>
<li>methods: 本文提出的方法是Beam Enumeration，它可以对语言基础的分子生成模型中的最可能的子序列进行广泛的枚举，并将分子子结构提取出来。</li>
<li>results: 提议的方法可以提高最近报道的Augmented Memory算法的性能，实现更好的样本效率和产生更多的高质量分子。<details>
<summary>Abstract</summary>
Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration is the first method to jointly address explainability and sample efficiency for molecular design.
</details>
<details>
<summary>摘要</summary>
生成分子设计已经从证明阶段积极应用到实际应用阶段，这是在最近几年的论文中 reporting 实验验证。关键的挑战是解释性和样本效率，这些挑战可以增强生成设计，直接优化昂贵的高精度观测器和提供可行的专业意见。我们提出了 Beam Enumeration，将 exhaustively enumerate 最有可能的子序列从语言基于的分子生成模型，并证明分子结构可以被提取。当与循环学习搭配时，提取的分子结构会具有意义，提供解释性和提高样本效率的自我条件生成。Beam Enumeration 适用于任何语言基于的分子生成模型，并且进一步提高 Augmented Memory 算法的性能，该算法已经在 Practical Molecular Optimization 问题上实现新的顶峰性，具体是 Sample Efficiency。联合算法可以更快地生成更高的奖励分子， givent a fixed oracle budget。Beam Enumeration 是第一个同时解释性和样本效率的分子设计方法。Simplified Chinese:生成分子设计已经从证明阶段普及到实际应用阶段，这是最近几年的论文中 reporting 实验验证。关键的挑战是解释性和样本效率，这些挑战可以增强生成设计，直接优化昂贵的高精度观测器和提供可行的专业意见。我们提出了 Beam Enumeration，将 exhaustively enumerate 最有可能的子序列从语言基于的分子生成模型，并证明分子结构可以被提取。当与循环学习搭配时，提取的分子结构会具有意义，提供解释性和提高样本效率的自我条件生成。Beam Enumeration 适用于任何语言基于的分子生成模型，并且进一步提高 Augmented Memory 算法的性能。这种算法已经在 Practical Molecular Optimization 问题上实现新的顶峰性，具体是 Sample Efficiency。联合算法可以更快地生成更高的奖励分子， givent a fixed oracle budget。Beam Enumeration 是第一个同时解释性和样本效率的分子设计方法。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-the-Heat-Transfer-Control-of-Pulsating-Impinging-Jets"><a href="#Deep-Reinforcement-Learning-for-the-Heat-Transfer-Control-of-Pulsating-Impinging-Jets" class="headerlink" title="Deep Reinforcement Learning for the Heat Transfer Control of Pulsating Impinging Jets"></a>Deep Reinforcement Learning for the Heat Transfer Control of Pulsating Impinging Jets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13955">http://arxiv.org/abs/2309.13955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajad Salavatidezfouli, Giovanni Stabile, Gianluigi Rozza</li>
<li>for: 这个研究探讨了深度强化学习（DRL）在热控制中的应用可能性，通过 Computational Fluid Dynamics 进行研究。</li>
<li>methods: 研究使用了 vanilla Deep Q-Network（DQN）方法进行热控制，并对不同的 DRL 变体进行了比较。</li>
<li>results: 结果表明，soft Double 和 Duel DQN 在所有变体中表现最好，具有高效学习和动作优先级能力。soft Double DQN 超过 hard Double DQN。此外，soft Double 和 Duel 能够在控制周期内维持温度在所需的阈值内超过 98% 的时间。这些发现表明 DRL 在热控制系统中具有扎实的潜力。<details>
<summary>Abstract</summary>
This research study explores the applicability of Deep Reinforcement Learning (DRL) for thermal control based on Computational Fluid Dynamics. To accomplish that, the forced convection on a hot plate prone to a pulsating cooling jet with variable velocity has been investigated. We begin with evaluating the efficiency and viability of a vanilla Deep Q-Network (DQN) method for thermal control. Subsequently, a comprehensive comparison between different variants of DRL is conducted. Soft Double and Duel DQN achieved better thermal control performance among all the variants due to their efficient learning and action prioritization capabilities. Results demonstrate that the soft Double DQN outperforms the hard Double DQN. Moreover, soft Double and Duel can maintain the temperature in the desired threshold for more than 98% of the control cycle. These findings demonstrate the promising potential of DRL in effectively addressing thermal control systems.
</details>
<details>
<summary>摘要</summary>
这个研究项目探讨了深度强化学习（DRL）在计算流体动力学中的应用性。为了实现这一目标，我们 investigate了一种受到脉冲冷风的热板，其中冷风速度是可变的。我们首先评估了普通的深度Q网络（DQN）方法的效率和可行性。然后，我们进行了不同变体的DRL比较。 results indicate that soft Double DQN和Duel DQN在所有变体中表现最佳，它们具有高效学习和动作优先级能力。此外，soft Double DQN超过了hard Double DQN。此外，soft Double和Duel可以在控制ecycle中维持温度在所需的阈值上超过98%的时间。这些发现表明DRL在thermal控制系统中具有扎实的潜力。
</details></li>
</ul>
<hr>
<h2 id="Local-and-Global-Trend-Bayesian-Exponential-Smoothing-Models"><a href="#Local-and-Global-Trend-Bayesian-Exponential-Smoothing-Models" class="headerlink" title="Local and Global Trend Bayesian Exponential Smoothing Models"></a>Local and Global Trend Bayesian Exponential Smoothing Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13950">http://arxiv.org/abs/2309.13950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Slawek Smyl, Christoph Bergmeir, Alexander Dokumentov, Erwin Wibowo, Daniel Schmidt</li>
<li>for: 本研究旨在探讨一种基于加法和乘法均摊满足的季节性和非季节性时间序列模型，以满足快速增长、波动性时间序列的需求。</li>
<li>methods: 本研究使用现代抽象贝叶斯适应技术来开发这种模型，并在M3竞赛数据集上应用。</li>
<li>results: 比较其他竞赛算法和参照值，本研究在M3竞赛数据集上得到了最佳的结果，从而在Literature中实现了最佳单variate方法的result。<details>
<summary>Abstract</summary>
This paper describes a family of seasonal and non-seasonal time series models that can be viewed as generalisations of additive and multiplicative exponential smoothing models. Their development is motivated by fast-growing, volatile time series, and facilitated by state-of-the-art Bayesian fitting techniques. When applied to the M3 competition data set, they outperform the best algorithms in the competition as well as other benchmarks, thus achieving to the best of our knowledge the best results of univariate methods on this dataset in the literature.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Characterising-User-Transfer-Amid-Industrial-Resource-Variation-A-Bayesian-Nonparametric-Approach"><a href="#Characterising-User-Transfer-Amid-Industrial-Resource-Variation-A-Bayesian-Nonparametric-Approach" class="headerlink" title="Characterising User Transfer Amid Industrial Resource Variation: A Bayesian Nonparametric Approach"></a>Characterising User Transfer Amid Industrial Resource Variation: A Bayesian Nonparametric Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13949">http://arxiv.org/abs/2309.13949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongxu Lei, Xiaotian Lin, Xinghu Yu, Zhan Li, Weichao Sun, Jianbin Qiu, Songlin Zhuang, Huijun Gao</li>
<li>for: 本研究旨在提高资源管理策略的发展，通过准确描述用户负载传递的 macro 级模式。</li>
<li>methods: 本研究提出了一种可解释的 hierarchical Bayesian nonparametric 模型 CLUSTER，可自动确定用户群和资源变化对用户传递的影响。</li>
<li>results: 实验结果表明，CLUSTER 模型能够准确预测用户传递响应资源变化，并且能够Quantify uncertainty for reliable decision-making。此外，CLUSTER 模型能够独立地函数于个人可 identificable 信息，保护用户隐私。<details>
<summary>Abstract</summary>
In a multitude of industrial fields, a key objective entails optimising resource management whilst satisfying user requirements. Resource management by industrial practitioners can result in a passive transfer of user loads across resource providers, a phenomenon whose accurate characterisation is both challenging and crucial. This research reveals the existence of user clusters, which capture macro-level user transfer patterns amid resource variation. We then propose CLUSTER, an interpretable hierarchical Bayesian nonparametric model capable of automating cluster identification, and thereby predicting user transfer in response to resource variation. Furthermore, CLUSTER facilitates uncertainty quantification for further reliable decision-making. Our method enables privacy protection by functioning independently of personally identifiable information. Experiments with simulated and real-world data from the communications industry reveal a pronounced alignment between prediction results and empirical observations across a spectrum of resource management scenarios. This research establishes a solid groundwork for advancing resource management strategy development.
</details>
<details>
<summary>摘要</summary>
在多种工业领域中，一个关键目标是优化资源管理，同时满足用户需求。但是资源管理的实践可能导致用户负载的投递式传输，这种现象的准确描述是非常困难和重要。这项研究发现用户群，这些群体捕捉了资源变化下的用户传输模式。我们提议CLUSTER，一种可解释性强的树状贝叶拟合模型，能够自动确定用户群和用户传输响应资源变化。此外，CLUSTER还可以对不确定性进行评估，以便更加可靠地做出决策。我们的方法可以保护用户隐私，不需要个人可识别信息。实验表明，CLUSTER在通信业中的仿真数据和实际数据上具有杰出的一致性，在资源管理方案的多种场景中具有广泛的应用前景。这项研究为资源管理策略的发展提供了坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Provable-Training-for-Graph-Contrastive-Learning"><a href="#Provable-Training-for-Graph-Contrastive-Learning" class="headerlink" title="Provable Training for Graph Contrastive Learning"></a>Provable Training for Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13944">http://arxiv.org/abs/2309.13944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/voidharuhi/pot-gcl">https://github.com/voidharuhi/pot-gcl</a></li>
<li>paper_authors: Yue Yu, Xiao Wang, Mengmei Zhang, Nian Liu, Chuan Shi</li>
<li>for: 本研究旨在解决 Graph Contrastive Learning (GCL) 训练中存在的不均衡问题，提高 GCL 的性能和可靠性。</li>
<li>methods: 本研究使用了实验证明 GCL 训练是不均衡的，并提出了一个名为 “node compactness” 的度量来衡量每个节点是否遵循 GCL 原理。此外，本研究还提出了一种名为 PrOvable Training (POT) 的训练方法，通过在 GCL 训练中添加 regularization 来增强 GCL 的性能。</li>
<li>results: 通过对多个 benchmark 进行了广泛的实验，本研究发现 POT 可以一直提高 GCL 的性能，并且可以作为一个可靠的插件使用。<details>
<summary>Abstract</summary>
Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL? To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the form of node compactness theoretically through bound propagation, which can be integrated into binary cross-entropy as a regularization. To this end, we propose the PrOvable Training (POT) for GCL, which regularizes the training of GCL to encode node embeddings that follows the GCL principle better. Through extensive experiments on various benchmarks, POT consistently improves the existing GCL approaches, serving as a friendly plugin.
</details>
<details>
<summary>摘要</summary>
graph contrastive learning (GCL) 已经成为无标签学习节点嵌入的受欢迎训练方法。 despite the well-established key principle of maximizing the similarity between positive node pairs while minimizing it between negative node pairs, some fundamental problems are still unclear. considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? or are there some nodes more likely to be untrained across graph augmentations and violate the principle? how to distinguish these nodes and further guide the training of GCL?to answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. to address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. we further derive the form of node compactness theoretically through bound propagation, which can be integrated into binary cross-entropy as a regularization. to this end, we propose the PrOvable Training (POT) for GCL, which regularizes the training of GCL to encode node embeddings that follows the GCL principle better. through extensive experiments on various benchmarks, POT consistently improves the existing GCL approaches, serving as a friendly plugin.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Classification-Systems-Against-Soft-Labels-with-Fuzzy-Precision-and-Recall"><a href="#Evaluating-Classification-Systems-Against-Soft-Labels-with-Fuzzy-Precision-and-Recall" class="headerlink" title="Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall"></a>Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13938">http://arxiv.org/abs/2309.13938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manu Harju, Annamaria Mesaros</li>
<li>for: 本研究旨在提出一种新的精度和准确率计算方法，以便在使用非二进制参考标签时评估音 Event detection系统的性能。</li>
<li>methods: 本研究使用了Kullback-Leibler divergence来衡量系统是否能够准确地遵循数据，并提出了一种基于非二进制参考标签的精度和准确率计算方法。</li>
<li>results: 研究发现，使用提议的计算方法可以准确地评估音 Event detection系统的性能，并且可以避免因数据二进制化而导致的错误解释。<details>
<summary>Abstract</summary>
Classification systems are normally trained by minimizing the cross-entropy between system outputs and reference labels, which makes the Kullback-Leibler divergence a natural choice for measuring how closely the system can follow the data. Precision and recall provide another perspective for measuring the performance of a classification system. Non-binary references can arise from various sources, and it is often beneficial to use the soft labels for training instead of the binarized data. However, the existing definitions for precision and recall require binary reference labels, and binarizing the data can cause erroneous interpretations. We present a novel method to calculate precision, recall and F-score without quantizing the data. The proposed metrics extend the well established metrics as the definitions coincide when used with binary labels. To understand the behavior of the metrics we show simple example cases and an evaluation of different sound event detection models trained on real data with soft labels.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Classification systems are normally trained by minimizing the cross-entropy between system outputs and reference labels, which makes the Kullback-Leibler divergence a natural choice for measuring how closely the system can follow the data. Precision and recall provide another perspective for measuring the performance of a classification system. Non-binary references can arise from various sources, and it is often beneficial to use the soft labels for training instead of the binarized data. However, the existing definitions for precision and recall require binary reference labels, and binarizing the data can cause erroneous interpretations. We present a novel method to calculate precision, recall and F-score without quantizing the data. The proposed metrics extend the well established metrics as the definitions coincide when used with binary labels. To understand the behavior of the metrics we show simple example cases and an evaluation of different sound event detection models trained on real data with soft labels." into Simplified Chinese.翻译文本为Simplified Chinese：通常，分类系统通过最小化系统输出与参考标签之间的cross-entropy来训练，这使得庒啄-利卜征函数成为衡量系统如何准确地跟踪数据的自然选择。精度和回归提供了另一种视角来衡量分类系统的性能。非二进制参考可以从多种来源 arise，并且在训练时使用软标签可以是有利的。然而，现有的精度和回归定义都需要二进制参考标签，并且将数据二进制化可能会导致错误的解释。我们提出了一种新的方法来计算精度、回归和F-score，而无需将数据二进制化。我们的 metric 扩展了现有的 metric，因为在使用二进制标签时，定义协调。为了理解metric的行为，我们给出了简单的例子cases和使用实际数据和软标签训练不同的音Event检测模型的评估。
</details></li>
</ul>
<hr>
<h2 id="SAMN-A-Sample-Attention-Memory-Network-Combining-SVM-and-NN-in-One-Architecture"><a href="#SAMN-A-Sample-Attention-Memory-Network-Combining-SVM-and-NN-in-One-Architecture" class="headerlink" title="SAMN: A Sample Attention Memory Network Combining SVM and NN in One Architecture"></a>SAMN: A Sample Attention Memory Network Combining SVM and NN in One Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13930">http://arxiv.org/abs/2309.13930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiaoling Yang, Linkai Luo, Haoyu Zhang, Hong Peng, Ziyang Chen</li>
<li>for: This paper aims to combine Support Vector Machines (SVM) and Neural Networks (NN) to create a more powerful function for multi-classification tasks.</li>
<li>methods: The proposed method, called Sample Attention Memory Network (SAMN), incorporates a sample attention module, class prototypes, and a memory block into NN to effectively combine SVM and NN.</li>
<li>results: Extensive experiments show that SAMN achieves better classification performance than single SVM or single NN with similar parameter sizes, as well as the previous best model for combining SVM and NN.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是将支持向量机(SVM)和神经网络(NN)结合起来，以创造更强大的多类分类器。</li>
<li>methods: 提议的方法是将样本注意力模块、类型评估模块和记忆块 incorporated into NN，以实现SVM和NN的有效结合。</li>
<li>results: 广泛的实验表明，SAMN比单个 SVM 或单个 NN 相同参数大小下的性能更好，以及之前最佳结合 SVM 和 NN 的模型。<details>
<summary>Abstract</summary>
Support vector machine (SVM) and neural networks (NN) have strong complementarity. SVM focuses on the inner operation among samples while NN focuses on the operation among the features within samples. Thus, it is promising and attractive to combine SVM and NN, as it may provide a more powerful function than SVM or NN alone. However, current work on combining them lacks true integration. To address this, we propose a sample attention memory network (SAMN) that effectively combines SVM and NN by incorporating sample attention module, class prototypes, and memory block to NN. SVM can be viewed as a sample attention machine. It allows us to add a sample attention module to NN to implement the main function of SVM. Class prototypes are representatives of all classes, which can be viewed as alternatives to support vectors. The memory block is used for the storage and update of class prototypes. Class prototypes and memory block effectively reduce the computational cost of sample attention and make SAMN suitable for multi-classification tasks. Extensive experiments show that SAMN achieves better classification performance than single SVM or single NN with similar parameter sizes, as well as the previous best model for combining SVM and NN. The sample attention mechanism is a flexible module that can be easily deepened and incorporated into neural networks that require it.
</details>
<details>
<summary>摘要</summary>
支持向量机(SVM)和神经网络(NN)具有强大的补做性。SVM关注样本之间的内部运算，而NN关注样本中特征之间的运算。因此，将SVM和NN结合起来可能提供一个更强大的函数，而不需要增加参数量。然而，现有的SVM和NN结合方法缺乏真正的集成。为此，我们提议一种叫做样本注意力储存网络(SAMN)，它有效地结合了SVM和NN。SVM可以看作是一种样本注意机器。它允许我们将样本注意模块添加到NN中，以实现SVM的主要功能。类型范例是所有类型的代表，它们可以看作是支持向量的替代品。储存块用于存储和更新类型范例。类型范例和储存块可以有效减少样本注意的计算成本，使SAMN适用于多类分类任务。广泛的实验表明，SAMN在类比单独使用SVM或NN时，达到了更好的分类性能，同时也比前一个最佳结合SVM和NN的模型更好。样本注意机制是一种灵活的模块，可以轻松地深入 incorporated into neural networks 中，当需要时。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Label-Selection-is-a-Decision-Problem"><a href="#Pseudo-Label-Selection-is-a-Decision-Problem" class="headerlink" title="Pseudo Label Selection is a Decision Problem"></a>Pseudo Label Selection is a Decision Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13926">http://arxiv.org/abs/2309.13926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aditya-rathi/Credit-Score-">https://github.com/aditya-rathi/Credit-Score-</a></li>
<li>paper_authors: Julian Rodemann</li>
<li>for: 这个论文的目的是提出一种基于决策理论的pseudo-label选择（PLS）方法，以解决confirmation bias问题。</li>
<li>methods: 这个方法基于一种新的选择 criterion，即 Pseudo posterior predictive的分析性approximation，这个分析性approximation是基于 Bayes-optimality 的。</li>
<li>results: 在模拟和实际数据上，BPLS方法在面临过拟合和confirmation bias问题时表现出优于传统的 PLS 方法。此外，这个方法还可以使得 PLS 更加鲁棒地对待模型假设。<details>
<summary>Abstract</summary>
Pseudo-Labeling is a simple and effective approach to semi-supervised learning. It requires criteria that guide the selection of pseudo-labeled data. The latter have been shown to crucially affect pseudo-labeling's generalization performance. Several such criteria exist and were proven to work reasonably well in practice. However, their performance often depends on the initial model fit on labeled data. Early overfitting can be propagated to the final model by choosing instances with overconfident but wrong predictions, often called confirmation bias. In two recent works, we demonstrate that pseudo-label selection (PLS) can be naturally embedded into decision theory. This paves the way for BPLS, a Bayesian framework for PLS that mitigates the issue of confirmation bias. At its heart is a novel selection criterion: an analytical approximation of the posterior predictive of pseudo-samples and labeled data. We derive this selection criterion by proving Bayes-optimality of this "pseudo posterior predictive". We empirically assess BPLS for generalized linear, non-parametric generalized additive models and Bayesian neural networks on simulated and real-world data. When faced with data prone to overfitting and thus a high chance of confirmation bias, BPLS outperforms traditional PLS methods. The decision-theoretic embedding further allows us to render PLS more robust towards the involved modeling assumptions. To achieve this goal, we introduce a multi-objective utility function. We demonstrate that the latter can be constructed to account for different sources of uncertainty and explore three examples: model selection, accumulation of errors and covariate shift.
</details>
<details>
<summary>摘要</summary>
假标注是一种简单而有效的 semi-supervised learning 方法。它需要一些导向选择假标注数据的标准。这些标准有助于减少假标注的泛化性能。然而，它们的性能通常取决于初始模型适应 labels 数据。早期过度适应可能会传递到最终模型，通常被称为 confirmation bias。在两篇最近的论文中，我们展示了 pseudo-label 选择（PLS）可以自然地被嵌入到决策理论中。这种方法可以减轻 confirmation bias 的问题。PLS 的核心是一个新的选择标准：一种 Analytical 预测 pseudo-samples 和标注数据的 posterior 预测。我们 derive 这个选择标准通过证明 Bayes-优化这个 "pseudo posterior predictive"。我们在 simulated 和实际数据上进行了 Empirical 评估，发现在面临数据泛化和高度可能性 confirmation bias 的情况下，BPLS 比传统的 PLS 方法更好。决策理论的嵌入还使得 PLS 更加抗性待命模型假设。为了实现这一目标，我们引入了一个多目标价值函数。我们示例了这个价值函数可以考虑不同的不确定性来源，并 explore 三个例子：模型选择、积累错误和变量转移。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Neural-Policy-Mirror-Descent-for-Policy-Optimization-on-Low-Dimensional-Manifolds"><a href="#Sample-Complexity-of-Neural-Policy-Mirror-Descent-for-Policy-Optimization-on-Low-Dimensional-Manifolds" class="headerlink" title="Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds"></a>Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13915">http://arxiv.org/abs/2309.13915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghao Xu, Xiang Ji, Minshuo Chen, Mengdi Wang, Tuo Zhao</li>
<li>for: 本文研究了使用神经网络的策略算法在强化学习中解决高维策略优化问题。</li>
<li>methods: 本文使用了神经网络作为策略和价值函数的函数近似器，并研究了NPMD算法的样本复杂性。</li>
<li>results: 研究发现，NPMD算法可以在高维策略优化问题中减轻维度着色问题，并可以在有限样本下找到$\epsilon$-优的策略，其样本复杂性为$\widetilde{O}(\epsilon^{-{\frac{d}{\alpha}-2})$。<details>
<summary>Abstract</summary>
Policy-based algorithms equipped with deep neural networks have achieved great success in solving high-dimensional policy optimization problems in reinforcement learning. However, current analyses cannot explain why they are resistant to the curse of dimensionality. In this work, we study the sample complexity of the neural policy mirror descent (NPMD) algorithm with convolutional neural networks (CNN) as function approximators. Motivated by the empirical observation that many high-dimensional environments have state spaces possessing low-dimensional structures, such as those taking images as states, we consider the state space to be a $d$-dimensional manifold embedded in the $D$-dimensional Euclidean space with intrinsic dimension $d\ll D$. We show that in each iteration of NPMD, both the value function and the policy can be well approximated by CNNs. The approximation errors are controlled by the size of the networks, and the smoothness of the previous networks can be inherited. As a result, by properly choosing the network size and hyperparameters, NPMD can find an $\epsilon$-optimal policy with $\widetilde{O}(\epsilon^{-\frac{d}{\alpha}-2})$ samples in expectation, where $\alpha\in(0,1]$ indicates the smoothness of environment. Compared to previous work, our result exhibits that NPMD can leverage the low-dimensional structure of state space to escape from the curse of dimensionality, providing an explanation for the efficacy of deep policy-based algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Matrix-Factorization-in-Tropical-and-Mixed-Tropical-Linear-Algebras"><a href="#Matrix-Factorization-in-Tropical-and-Mixed-Tropical-Linear-Algebras" class="headerlink" title="Matrix Factorization in Tropical and Mixed Tropical-Linear Algebras"></a>Matrix Factorization in Tropical and Mixed Tropical-Linear Algebras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13914">http://arxiv.org/abs/2309.13914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Kordonis, Emmanouil Theodosis, George Retsinas, Petros Maragos</li>
<li>for:  Matrix Factorization (MF) 为机器学习和数据探索中的应用，包括共同推荐系统、维度缩减、数据可视化和社群探测。</li>
<li>methods: 我们使用тропікалgebra和几何学来研究两个问题，包括 Tropical Matrix Factorization (TMF) 和一个新的组合矩阵分解问题。</li>
<li>results: 我们提出了一个改进的TMF算法，可以避免许多地方最佳解；此外，我们还提出了一个新的组合矩阵分解方法，具有与多个用户学习 utility 函数的 interessante解释。我们还呈现了一些数据，证明我们的方法的有效性，并实现了一个推荐系统的应用，获得了显著的结果。<details>
<summary>Abstract</summary>
Matrix Factorization (MF) has found numerous applications in Machine Learning and Data Mining, including collaborative filtering recommendation systems, dimensionality reduction, data visualization, and community detection. Motivated by the recent successes of tropical algebra and geometry in machine learning, we investigate two problems involving matrix factorization over the tropical algebra. For the first problem, Tropical Matrix Factorization (TMF), which has been studied already in the literature, we propose an improved algorithm that avoids many of the local optima. The second formulation considers the approximate decomposition of a given matrix into the product of three matrices where a usual matrix product is followed by a tropical product. This formulation has a very interesting interpretation in terms of the learning of the utility functions of multiple users. We also present numerical results illustrating the effectiveness of the proposed algorithms, as well as an application to recommendation systems with promising results.
</details>
<details>
<summary>摘要</summary>
矩阵因式（MF）在机器学习和数据挖掘中找到了许多应用，包括共享推荐系统、维度减少、数据可视化和社区检测。受推荐系统的最近成功而受欢迎的泛洋算术和几何，我们 investigate two 矩阵因式问题，其中一个是已经在文献中研究的极地矩阵因式（TMF），我们提出了一种改进的算法，可以避免许多地方最佳点。第二个形式是对给定矩阵的approximate decompositions into the product of three matrices，其中一个是 usual matrix product followed by a tropical product。这个形式有非常有趣的学习多个用户的实用函数的解释。我们还present numerical results demonstrating the effectiveness of the proposed algorithms, as well as an application to recommendation systems with promising results。
</details></li>
</ul>
<hr>
<h2 id="Follow-ups-Also-Matter-Improving-Contextual-Bandits-via-Post-serving-Contexts"><a href="#Follow-ups-Also-Matter-Improving-Contextual-Bandits-via-Post-serving-Contexts" class="headerlink" title="Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts"></a>Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13896">http://arxiv.org/abs/2309.13896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqi Wang, Ziyu Ye, Zhe Feng, Ashwinkumar Badanidiyuru, Haifeng Xu</li>
<li>for: 提高在线学习效率，解决 contectual bandit problem 中 valuable 的后服务上下文信息不可见的问题。</li>
<li>methods: 提出了一种新的contextual bandit problem 模型，利用后服务上下文信息进行学习，并设计了一种新的算法poLinUCB，可以在标准假设下实现紧凑的征逐 regret。</li>
<li>results: 对 synthetic 和实际数据进行了广泛的实验测试，证明了利用后服务上下文信息可以提高学习效率，以及poLinUCB 算法的综合性和可靠性。<details>
<summary>Abstract</summary>
Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which valuable additional context can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok also observe valuable follow-up information pertinent to the user's reward after recommendation (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate noise in data. Such robustification is necessary for tackling our problem, and we believe it could also be of general interest. Extensive empirical tests on both synthetic and real-world datasets demonstrate the significant benefit of utilizing post-serving contexts as well as the superior performance of our algorithm over the state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
通常的上下文抽剂问题假设所有相关的上下文都可以在算法选择器之前观察。这种模型虽然有用，但在处理不可见上下文的问题时，它经常失足。例如，内容推荐平台 like Youtube、Instagram 和 Tiktok 可以在推荐后观察有价值的用户奖励信息（例如用户停留时间、用户播放速度等）。为了在这些应用中提高在线学习效率，我们研究了一种新的上下文抽剂问题，即 post-serving 上下文，并设计了一个新的算法 poLinUCB。我们的技术证明基于一种更加稳健和泛化的 Elliptical Potential Lemma (EPL)，可以承受数据噪声。这种稳健性是我们问题的必要条件，并且我们认为这也可能对总体有利。我们的实验表明，利用 post-serving 上下文和我们的算法的优秀性，可以在实验室和实际数据上达到显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Graph-Representation-Learning-Towards-Patents-Network-Analysis"><a href="#Graph-Representation-Learning-Towards-Patents-Network-Analysis" class="headerlink" title="Graph Representation Learning Towards Patents Network Analysis"></a>Graph Representation Learning Towards Patents Network Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13888">http://arxiv.org/abs/2309.13888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Mohammad Heydari, Babak Teimourpour</li>
<li>for: 本研究使用graph representation learning方法分析了伊朗官方公报中的专利数据，以找出相似性和新领域。</li>
<li>methods: 研究使用自然语言处理和实体解析技术提取了专利记录中的关键实体，然后将其转换为伊朗专利图граffe。</li>
<li>results: 研究结果显示，通过使用Graph representation learning和文本挖掘技术，可以实现专利资料分析和探索新领域，并且可以预防重复申请专利、熟悉相似和相连的发明、了解法律实体支持专利和研究人员在特定领域的知识。<details>
<summary>Abstract</summary>
Patent analysis has recently been recognized as a powerful technique for large companies worldwide to lend them insight into the age of competition among various industries. This technique is considered a shortcut for developing countries since it can significantly accelerate their technology development. Therefore, as an inevitable process, patent analysis can be utilized to monitor rival companies and diverse industries. This research employed a graph representation learning approach to create, analyze, and find similarities in the patent data registered in the Iranian Official Gazette. The patent records were scrapped and wrangled through the Iranian Official Gazette portal. Afterward, the key entities were extracted from the scrapped patents dataset to create the Iranian patents graph from scratch based on novel natural language processing and entity resolution techniques. Finally, thanks to the utilization of novel graph algorithms and text mining methods, we identified new areas of industry and research from Iranian patent data, which can be used extensively to prevent duplicate patents, familiarity with similar and connected inventions, Awareness of legal entities supporting patents and knowledge of researchers and linked stakeholders in a particular research field.
</details>
<details>
<summary>摘要</summary>
具有广泛应用前景的专利分析技术已经在全球范围内被大型公司广泛应用，以帮助这些公司更好地了解不同行业的竞争情况。这种技术被视为发展中国家的短cut，因为它可以快速加速技术发展。因此，通过监测竞争对手和多个行业的专利分析，这种技术可以帮助公司更好地了解自己的市场环境。本研究采用了图表学习方法来创建、分析和找出专利数据库中的相似性。我们从伊朗官方报纸网站上抓取了专利笔记，然后使用新的自然语言处理和实体解决技术提取了关键实体。最后，我们通过使用新的图算法和文本挖掘技术，在伊朗专利数据中找到了新的行业和研究领域，这些领域可以用于避免重复专利、熟悉相似和相连的发明、了解法定机构支持专利、了解研究人员和相关的投资者在特定研究领域的知识。
</details></li>
</ul>
<hr>
<h2 id="Can-Class-Priors-Help-Single-Positive-Multi-Label-Learning"><a href="#Can-Class-Priors-Help-Single-Positive-Multi-Label-Learning" class="headerlink" title="Can Class-Priors Help Single-Positive Multi-Label Learning?"></a>Can Class-Priors Help Single-Positive Multi-Label Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13886">http://arxiv.org/abs/2309.13886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biao Liu, Jie Wang, Ning Xu, Xin Geng</li>
<li>for:  solves the problem of single-positive multi-label learning (SPMLL) with class-prior differences in real-world scenarios.</li>
<li>methods:  proposes a novel framework called Class-pRiors Induced Single-Positive multi-label learning, which includes a class-priors estimator and an unbiased risk estimator for classification.</li>
<li>results:  experiments on ten MLL benchmark datasets demonstrate the effectiveness and superiority of the proposed method over existing SPMLL approaches.<details>
<summary>Abstract</summary>
Single-positive multi-label learning (SPMLL) is a typical weakly supervised multi-label learning problem, where each training example is annotated with only one positive label. Existing SPMLL methods typically assign pseudo-labels to unannotated labels with the assumption that prior probabilities of all classes are identical. However, the class-prior of each category may differ significantly in real-world scenarios, which makes the predictive model not perform as well as expected due to the unrealistic assumption on real-world application. To alleviate this issue, a novel framework named {\proposed}, i.e., Class-pRiors Induced Single-Positive multi-label learning, is proposed. Specifically, a class-priors estimator is introduced, which could estimate the class-priors that are theoretically guaranteed to converge to the ground-truth class-priors. In addition, based on the estimated class-priors, an unbiased risk estimator for classification is derived, and the corresponding risk minimizer could be guaranteed to approximately converge to the optimal risk minimizer on fully supervised data. Experimental results on ten MLL benchmark datasets demonstrate the effectiveness and superiority of our method over existing SPMLL approaches.
</details>
<details>
<summary>摘要</summary>
单正向多标签学习（SPMLL）是一种 Typical weakly supervised multi-label learning 问题，每个训练示例只有一个正确标签。现有的 SPMLL 方法通常将 pseudo-labels 赋给未标记的标签，假设所有类别的先验概率相同。然而，实际应用中每个类别的类别先验可能很大不同，这会使 predictive 模型不能如预期那样表现，因为这是不真实的假设。为解决这个问题，我们提出了一个新的框架，即 Class-pRiors Induced Single-Positive multi-label learning，简称为 \proposed。 Specifically, 我们引入了一个类别先验估计器，可以估算类别先验，并且这些估计器 theoretically guaranteed to converge to the ground-truth class-priors。此外，基于估计器，我们 derive 了一个不偏的风险估计器 для 分类，并且可以 guarantee that the corresponding risk minimizer could approximately converge to the optimal risk minimizer on fully supervised data。实验结果表明，我们的方法在十个 MLL  benchmark 数据集上表现出色，比现有的 SPMLL 方法更有效。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Treatment-Effects-Under-Heterogeneous-Interference"><a href="#Estimating-Treatment-Effects-Under-Heterogeneous-Interference" class="headerlink" title="Estimating Treatment Effects Under Heterogeneous Interference"></a>Estimating Treatment Effects Under Heterogeneous Interference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13884">http://arxiv.org/abs/2309.13884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxf208/hinite">https://github.com/linxf208/hinite</a></li>
<li>paper_authors: Xiaofeng Lin, Guoxi Zhang, Xiaotian Lu, Han Bao, Koh Takeuchi, Hisashi Kashima</li>
<li>for: The paper is written for estimating individual treatment effects (ITEs) in the presence of interference, specifically in online applications where units are associated and interference can be heterogeneous.</li>
<li>methods: The paper proposes a novel approach to model heterogeneous interference by developing a new architecture that aggregates information from diverse neighbors, using graph neural networks, a mechanism to aggregate information from different views, and attention mechanisms.</li>
<li>results: The proposed method significantly outperforms existing methods for ITE estimation in experiments on multiple datasets with heterogeneous interference, confirming the importance of modeling heterogeneous interference.<details>
<summary>Abstract</summary>
Treatment effect estimation can assist in effective decision-making in e-commerce, medicine, and education. One popular application of this estimation lies in the prediction of the impact of a treatment (e.g., a promotion) on an outcome (e.g., sales) of a particular unit (e.g., an item), known as the individual treatment effect (ITE). In many online applications, the outcome of a unit can be affected by the treatments of other units, as units are often associated, which is referred to as interference. For example, on an online shopping website, sales of an item will be influenced by an advertisement of its co-purchased item. Prior studies have attempted to model interference to estimate the ITE accurately, but they often assume a homogeneous interference, i.e., relationships between units only have a single view. However, in real-world applications, interference may be heterogeneous, with multi-view relationships. For instance, the sale of an item is usually affected by the treatment of its co-purchased and co-viewed items. We hypothesize that ITE estimation will be inaccurate if this heterogeneous interference is not properly modeled. Therefore, we propose a novel approach to model heterogeneous interference by developing a new architecture to aggregate information from diverse neighbors. Our proposed method contains graph neural networks that aggregate same-view information, a mechanism that aggregates information from different views, and attention mechanisms. In our experiments on multiple datasets with heterogeneous interference, the proposed method significantly outperforms existing methods for ITE estimation, confirming the importance of modeling heterogeneous interference.
</details>
<details>
<summary>摘要</summary>
干预效果估计可以帮助在电商、医疗和教育等领域进行有效的决策。一种受欢迎的应用之一是估计干预（例如推广）对单元（例如商品）的结果（例如销售）的影响，known as 个体干预效果（ITE）。在许多在线应用程序中，单元的结果可能受到其他单元的干预，这是因为单元经常相关，这被称为干扰。例如，在一个在线购物网站上，一个商品的销售会受到其推广的影响。先前的研究尝试了模型干扰，以便准确地估计ITE，但它们通常假设了同质干扰，即单元之间只有一种视角。然而，在实际应用中，干扰可能是多质，即单元之间有多种视角。例如，一个商品的销售通常受到其推广和浏览的影响。我们认为，如果不正确地模型多质干扰，ITE估计就会不准确。因此，我们提出了一种新的方法，用于模型多质干扰。我们的提议方法包括图 neural networks 来聚合同视角信息，一种机制来聚合不同视角信息，以及注意机制。在我们对多个数据集上进行的实验中，我们的提议方法在存在多质干扰的情况下显著超过了现有的ITE估计方法，确认了模型多质干扰的重要性。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Conditional-Expectation-Model-for-Efficient-and-Robust-Target-Speech-Extraction"><a href="#Diffusion-Conditional-Expectation-Model-for-Efficient-and-Robust-Target-Speech-Extraction" class="headerlink" title="Diffusion Conditional Expectation Model for Efficient and Robust Target Speech Extraction"></a>Diffusion Conditional Expectation Model for Efficient and Robust Target Speech Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13874">http://arxiv.org/abs/2309.13874</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vivian556123/dcem">https://github.com/vivian556123/dcem</a></li>
<li>paper_authors: Leying Zhang, Yao Qian, Linfeng Yu, Heming Wang, Xinkai Wang, Hemin Yang, Long Zhou, Shujie Liu, Yanmin Qian, Michael Zeng</li>
<li>for: 本文旨在提出一种高效的生成方法，用于 Target Speech Extraction (TSE)。</li>
<li>methods: 本文使用了Diffusion Conditional Expectation Model (DCEM)，可以处理多 speaker和单 speaker场景，并且可以在各种噪音和清晰Condition下进行处理。</li>
<li>results:  comparing with传统方法，本文的方法在非侵入和侵入 metric 中表现出色，并且具有高效的推理速度和对未看到任务的稳定性。Audio例子可以在线上预览（<a target="_blank" rel="noopener" href="https://vivian556123.github.io/dcem%EF%BC%89%E3%80%82">https://vivian556123.github.io/dcem）。</a><details>
<summary>Abstract</summary>
Target Speech Extraction (TSE) is a crucial task in speech processing that focuses on isolating the clean speech of a specific speaker from complex mixtures. While discriminative methods are commonly used for TSE, they can introduce distortion in terms of speech perception quality. On the other hand, generative approaches, particularly diffusion-based methods, can enhance speech quality perceptually but suffer from slower inference speed. We propose an efficient generative approach named Diffusion Conditional Expectation Model (DCEM) for TSE. It can handle multi- and single-speaker scenarios in both noisy and clean conditions. Additionally, we introduce Regenerate-DCEM (R-DCEM) that can regenerate and optimize speech quality based on pre-processed speech from a discriminative model. Our method outperforms conventional methods in terms of both intrusive and non-intrusive metrics and demonstrates notable strengths in inference efficiency and robustness to unseen tasks. Audio examples are available online (https://vivian556123.github.io/dcem).
</details>
<details>
<summary>摘要</summary>
target speech extraction (tse)是speech processing中关键的任务，它的目标是从复杂的混合中分离出清晰的speaker的speech。although discriminative methods are commonly used for tse, they can introduce distortion in terms of speech perception quality. on the other hand, generative approaches, particularly diffusion-based methods, can enhance speech quality perceptually but suffer from slower inference speed. we propose an efficient generative approach named diffusion conditional expectation model (dcem) for tse. it can handle multi- and single-speaker scenarios in both noisy and clean conditions. additionally, we introduce regenerate-dcem (r-dcem) that can regenerate and optimize speech quality based on pre-processed speech from a discriminative model. our method outperforms conventional methods in terms of both intrusive and non-intrusive metrics and demonstrates notable strengths in inference efficiency and robustness to unseen tasks. audio examples are available online (https://vivian556123.github.io/dcem).Here's the translation breakdown:* target speech extraction (tse) = 目标语音采样 (tse)* speech processing = 语音处理* discriminative methods = 分类方法* generative approaches = 生成方法* diffusion-based methods = 扩散基于方法* Diffusion Conditional Expectation Model (DCEM) = 扩散 conditional expectation model (DCEM)* Regenerate-DCEM (R-DCEM) = 重新生成-DCEM (R-DCEM)* pre-processed speech = 预处理的语音* inference efficiency = 推理效率* robustness to unseen tasks = 对未经见任务的Robustness* audio examples = 音频示例Please note that the translation is done in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Statistical-Perspective-of-Top-K-Sparse-Softmax-Gating-Mixture-of-Experts"><a href="#Statistical-Perspective-of-Top-K-Sparse-Softmax-Gating-Mixture-of-Experts" class="headerlink" title="Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts"></a>Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13850">http://arxiv.org/abs/2309.13850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Nguyen, Pedram Akbarian, Fanqi Yan, Nhat Ho</li>
<li>for: 这篇论文主要针对的问题是解释权重补做的顶层-$K$ sparse softmax权重函数对输入空间的分割和深度学习模型的性能的影响。</li>
<li>methods: 作者使用了 Gaussian mixture of experts 来设计一个简单的模型，并通过定义新的损失函数来捕捉输入空间不同区域的行为。</li>
<li>results: 研究发现，当知道真实的专家数 $k_{\ast}$ 时，随着样本大小增加，权重补做的散度和参数估计的速度都是 Parametric。但是，当真实模型超出了 $k_{\ast}$ 的情况下，选择从顶层-$K$ sparse softmax权重函数中的专家数量必须大于某些 Voronoi 细胞与真实参数之间的总体 cardinality，以确保权重补做的散度估计 converge。此外，虽然散度估计的速度仍然是 Parametric，但参数估计速度受到权重补做和专家函数之间的内在交互的影响，导致很慢。<details>
<summary>Abstract</summary>
Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\ast}$ becomes unknown and the true model is over-specified by a Gaussian mixture of $k$ experts where $k > k_{\ast}$, our findings suggest that the number of experts selected from the top-K sparse softmax gating function must exceed the total cardinality of a certain number of Voronoi cells associated with the true parameters to guarantee the convergence of the density estimation. Moreover, while the density estimation rate remains parametric under this setting, the parameter estimation rates become substantially slow due to an intrinsic interaction between the softmax gating and expert functions.
</details>
<details>
<summary>摘要</summary>
Top-K 稀疏软max权重混合专家已经广泛应用于扩大深度学习架构而无需增加计算成本。尽管在实际应用中它非常受欢迎，但是其理论理解仍然是一个开放的问题。主要挑战在于 top-K 稀疏软max权重混合函数的结构，该函数将输入空间分成多个区域，每个区域具有不同的行为。通过关注 Gaussian mixture of experts，我们建立了关于 top-K 稀疏软max权重混合函数对输入空间的影响的理论结果。我们的结论基于定义新的损失函数来捕捉不同区域的输入行为。当真实的专家数量 $k_{\ast}$ 知道时，我们证明随样本大小的散度和参数估计的渐近率都是参数的。然而，当 $k_{\ast}$ 不知道，真实的模型被过度规定为 Gaussian mixture of $k$ 专家，其中 $k > k_{\ast}$，我们发现，为保证散度估计的渐近，从 top-K 稀疏软max权重混合函数中选择的专家数量必须大于真实参数的总 cardinality。此外，虽然散度估计率保持参数的，但参数估计率因软max权重和专家函数之间的内在互动而变得非常慢。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Adversarial-Samples-against-Ensemble-Learning-based-Windows-PE-Malware-Detectors"><a href="#On-the-Effectiveness-of-Adversarial-Samples-against-Ensemble-Learning-based-Windows-PE-Malware-Detectors" class="headerlink" title="On the Effectiveness of Adversarial Samples against Ensemble Learning-based Windows PE Malware Detectors"></a>On the Effectiveness of Adversarial Samples against Ensemble Learning-based Windows PE Malware Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13841">http://arxiv.org/abs/2309.13841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trong-Nghia To, Danh Le Kim, Do Thi Thu Hien, Nghi Hoang Khoa, Hien Do Hoang, Phan The Duy, Van-Hau Pham<br>for:这个研究的目的是提出一个组合GAN和RL模型的变异系统，以抵消基于集成学习的检测器。methods:研究使用了GAN和RL模型，包括MalGAN和Deep Q-network anti-malware Engines Attacking Framework (DQEAF)。results:实验结果显示，100%的选择的异常样品保留了可执行档案的格式，而在执行可能性和黑客性方面也有一定的成功。<details>
<summary>Abstract</summary>
Recently, there has been a growing focus and interest in applying machine learning (ML) to the field of cybersecurity, particularly in malware detection and prevention. Several research works on malware analysis have been proposed, offering promising results for both academic and practical applications. In these works, the use of Generative Adversarial Networks (GANs) or Reinforcement Learning (RL) can aid malware creators in crafting metamorphic malware that evades antivirus software. In this study, we propose a mutation system to counteract ensemble learning-based detectors by combining GANs and an RL model, overcoming the limitations of the MalGAN model. Our proposed FeaGAN model is built based on MalGAN by incorporating an RL model called the Deep Q-network anti-malware Engines Attacking Framework (DQEAF). The RL model addresses three key challenges in performing adversarial attacks on Windows Portable Executable malware, including format preservation, executability preservation, and maliciousness preservation. In the FeaGAN model, ensemble learning is utilized to enhance the malware detector's evasion ability, with the generated adversarial patterns. The experimental results demonstrate that 100\% of the selected mutant samples preserve the format of executable files, while certain successes in both executability preservation and maliciousness preservation are achieved, reaching a stable success rate.
</details>
<details>
<summary>摘要</summary>
近些年来，机器学习（ML）在计算机安全领域的应用得到了越来越多的关注和兴趣，特别是在恶意软件检测和防范方面。一些关于恶意软件分析的研究工作已经提出，其中使用生成对抗网络（GANs）或强化学习（RL）可以帮助恶意软件创作者制作形态变化的恶意软件，从而躲避反恶意软件检测。在本研究中，我们提出了一种突变系统，用于对集成学习基于检测器的攻击者进行对抗。我们的提出的FeaGAN模型基于MalGAN模型，通过加入一个RL模型called Deep Q-network anti-malware Engines Attacking Framework（DQEAF），解决了对Windows Portable Executable恶意软件的三大挑战，包括格式保留、可执行性保留和害意保留。在FeaGAN模型中， ensemble learning被使用来增强恶意软件检测器的逃脱能力，通过生成的对抗模式。实验结果表明，100%的选择的突变样本保留了可执行文件的格式，而在可执行性和害意方面也有一定的成功，达到了稳定的成功率。
</details></li>
</ul>
<hr>
<h2 id="Penalized-Principal-Component-Analysis-using-Nesterov-Smoothing"><a href="#Penalized-Principal-Component-Analysis-using-Nesterov-Smoothing" class="headerlink" title="Penalized Principal Component Analysis using Nesterov Smoothing"></a>Penalized Principal Component Analysis using Nesterov Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13838">http://arxiv.org/abs/2309.13838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca M. Hurwitz, Georg Hahn</li>
<li>for: 本文使用权重约束最小化方法（PEP）来缩短高维数据中的维度，并添加L1偏导函数约束。</li>
<li>methods: 本文提出了一种使用馈积抑制（Nesterov smoothing）来计算LASSO-类L1偏导函数的分布式优化方法，并使用已有的单值分解（SVD）结果来计算高阶特征向量。</li>
<li>results: 使用1000个基因计划数据集，我们实验ally示出了使用我们提议的精炼PEP可以提高数值稳定性并获得有意义的特征向量。我们还 investigate了对传统PCA的约束最小化方法的比较。<details>
<summary>Abstract</summary>
Principal components computed via PCA (principal component analysis) are traditionally used to reduce dimensionality in genomic data or to correct for population stratification. In this paper, we explore the penalized eigenvalue problem (PEP) which reformulates the computation of the first eigenvector as an optimization problem and adds an L1 penalty constraint. The contribution of our article is threefold. First, we extend PEP by applying Nesterov smoothing to the original LASSO-type L1 penalty. This allows one to compute analytical gradients which enable faster and more efficient minimization of the objective function associated with the optimization problem. Second, we demonstrate how higher order eigenvectors can be calculated with PEP using established results from singular value decomposition (SVD). Third, using data from the 1000 Genome Project dataset, we empirically demonstrate that our proposed smoothed PEP allows one to increase numerical stability and obtain meaningful eigenvectors. We further investigate the utility of the penalized eigenvector approach over traditional PCA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用主成分分析（PCA）计算主成分是传统地用于降维度的 genomic 数据或是对人口分布进行修正。在这篇论文中，我们探讨了增加 penalty 的 eigenvalue 问题（PEP），它将计算第一个 eigenvector 转换为优化问题，并添加 L1 罚项限制。我们的贡献有三个方面：首先，我们扩展了 PEP 方法，通过应用 Nesterov 缓和法来计算 analytical 导数，从而更快地和更有效地解决优化问题中关联的目标函数。第二，我们使用已有的 singular value decomposition（SVD）结果来计算高级别的 eigenvectors。第三，使用 1000 Genome Project 数据集，我们实际示出了我们提议的平滑 PEP 可以增加数值稳定性并获得有意义的 eigenvectors。我们进一步调查了使用增加 penalty 的 eigenvector 方法与传统 PCA 方法的优劣。
</details></li>
</ul>
<hr>
<h2 id="Backorder-Prediction-in-Inventory-Management-Classification-Techniques-and-Cost-Considerations"><a href="#Backorder-Prediction-in-Inventory-Management-Classification-Techniques-and-Cost-Considerations" class="headerlink" title="Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations"></a>Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13837">http://arxiv.org/abs/2309.13837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarit Maitra, Sukanya Kundu</li>
<li>for: 预测库存缺失(backorder)管理</li>
<li>methods: 使用多种分类技术，包括平衡携带分类器、柔logic、变分自适应网络-生成对抗网络、多层感知器等，对不同的数据集进行评估，并考虑财务因素和缺失成本。</li>
<li>results: 结果表明，结合模型方法，包括集成技术和VAE，可以有效地处理不均衡数据集，提高预测精度，减少假阳性和假阴性，并增加可 interpretability。<details>
<summary>Abstract</summary>
This article introduces an advanced analytical approach for predicting backorders in inventory management. Backorder refers to an order that cannot be immediately fulfilled due to stock depletion. Multiple classification techniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational Autoencoder - Generative Adversarial Networks, and Multi-layer Perceptron classifiers, are assessed in this work using performance evaluation metrics such as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function and misclassification costs, considering the financial implications and costs associated with inventory management and backorder handling. The study suggests that a combination of modeling approaches, including ensemble techniques and VAE, can effectively address imbalanced datasets in inventory management, emphasizing interpretability and reducing false positives and false negatives. This research contributes to the advancement of predictive analytics and offers valuable insights for future investigations in backorder forecasting and inventory control optimization for decision-making.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇文章介绍了一种高级分析方法，用于预测库存欠货（backorder）管理中的库存异常情况。这种方法包括多种分类技术，如均衡搅拌分类器、杂化逻辑、变量自动编码-生成敌对网络、多层感知器等，并使用表现评估指标，如ROC-AUC和PR-AUC来评估其表现。此外，这种研究还考虑了财务因素和误分类成本，包括库存管理和欠货处理中的财务影响和成本。研究表明，结合不同的模型方法，包括ensemble技术和VAE，可以有效地处理库存管理中的偏度数据，提高预测精度和减少false阳和false降。这项研究对预测分析领域的发展做出了贡献，并为未来的库存预测和库存控制优化做出了有价值的着想。
</details></li>
</ul>
<hr>
<h2 id="NSOTree-Neural-Survival-Oblique-Tree"><a href="#NSOTree-Neural-Survival-Oblique-Tree" class="headerlink" title="NSOTree: Neural Survival Oblique Tree"></a>NSOTree: Neural Survival Oblique Tree</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13825">http://arxiv.org/abs/2309.13825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xs018/NSOTree">https://github.com/xs018/NSOTree</a></li>
<li>paper_authors: Xiaotong Sun, Peijie Qiu</li>
<li>for: 这篇论文探讨了Survival分析领域中的时间至事件（Time-to-Event）数据，以及将深度学习方法应用到这个领域中，以提高表现和可解性。</li>
<li>methods: 这篇论文提出了一个名为Neural Survival Oblique Tree（NSOTree）的新方法，它结合了深度学习和树型方法，以维持可解性和表现力。NSOTree 基于 ReLU 网络，并且可以与现有的存生模型集成在一起，以便应用。</li>
<li>results: 论文的评估结果显示，NSOTree 能够在实际数据上实现高性能和可解性，并且在存生领域中提供了一个可靠的方法。<details>
<summary>Abstract</summary>
Survival analysis is a statistical method employed to scrutinize the duration until a specific event of interest transpires, known as time-to-event information characterized by censorship. Recently, deep learning-based methods have dominated this field due to their representational capacity and state-of-the-art performance. However, the black-box nature of the deep neural network hinders its interpretability, which is desired in real-world survival applications but has been largely neglected by previous works. In contrast, conventional tree-based methods are advantageous with respect to interpretability, while consistently grappling with an inability to approximate the global optima due to greedy expansion. In this paper, we leverage the strengths of both neural networks and tree-based methods, capitalizing on their ability to approximate intricate functions while maintaining interpretability. To this end, we propose a Neural Survival Oblique Tree (NSOTree) for survival analysis. Specifically, the NSOTree was derived from the ReLU network and can be easily incorporated into existing survival models in a plug-and-play fashion. Evaluations on both simulated and real survival datasets demonstrated the effectiveness of the proposed method in terms of performance and interpretability.
</details>
<details>
<summary>摘要</summary>
生存分析是一种统计方法，用于检查一个特定事件的发生时间，称为时间至事件信息，受到限制。最近，深度学习基于方法在这一领域占据主导地位，因为它们具有表达能力和现代性。然而，深度神经网络的黑盒特性阻碍了其解释性，这在实际生存应用中是极其重要的，但之前的工作却忽略了这一点。相比之下，传统的树状方法具有解释性的优势，但它们难以近似全局最优解。在这篇论文中，我们利用神经网络和树状方法的优点，同时维持解释性。为此，我们提出了神经生存斜树（NSOTree）方法。具体来说，NSOTree是基于ReLU网络的，可以轻松地与现有的生存模型集成。我们对实际和 simulations 数据进行了评估，并证明了我们提出的方法在性能和解释性两个方面具有效果。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-large-collections-of-time-series-feature-based-methods"><a href="#Forecasting-large-collections-of-time-series-feature-based-methods" class="headerlink" title="Forecasting large collections of time series: feature-based methods"></a>Forecasting large collections of time series: feature-based methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13807">http://arxiv.org/abs/2309.13807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lixixibj/forecasting-with-time-series-imaging">https://github.com/lixixibj/forecasting-with-time-series-imaging</a></li>
<li>paper_authors: Li Li, Feng Li, Yanfei Kang</li>
<li>for: 这篇论文主要针对 econometrics 和其他预测领域中的复杂实际问题，即时间序列数据的复杂性使得单一模型不能涵盖所有数据生成过程。</li>
<li>methods: 这篇论文介绍了基于时间序列特征的两种方法来预测大量时间序列数据，即特征基于的模型选择和特征基于的模型组合。</li>
<li>results: 论文详细介绍了现场 откры源软件实现的状态 искусственного预测方法，包括基于时间序列特征的模型选择和模型组合。<details>
<summary>Abstract</summary>
In economics and many other forecasting domains, the real world problems are too complex for a single model that assumes a specific data generation process. The forecasting performance of different methods changes depending on the nature of the time series. When forecasting large collections of time series, two lines of approaches have been developed using time series features, namely feature-based model selection and feature-based model combination. This chapter discusses the state-of-the-art feature-based methods, with reference to open-source software implementations.
</details>
<details>
<summary>摘要</summary>
在经济和许多其他预测领域中，现实世界问题太复杂，不可以单独采用一个模型，假设特定的数据生成过程。预测不同时序系列的表现，不同方法的预测性能会有所不同。当预测大量时序系列时，有两条方向的方法得到发展，一是基于时序特征的模型选择，二是基于时序特征的模型组合。本章介绍了当前最佳实践的特征基于方法，参考开源软件实现。
</details></li>
</ul>
<hr>
<h2 id="Projected-Randomized-Smoothing-for-Certified-Adversarial-Robustness"><a href="#Projected-Randomized-Smoothing-for-Certified-Adversarial-Robustness" class="headerlink" title="Projected Randomized Smoothing for Certified Adversarial Robustness"></a>Projected Randomized Smoothing for Certified Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13794">http://arxiv.org/abs/2309.13794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spfrommer/projected_randomized_smoothing">https://github.com/spfrommer/projected_randomized_smoothing</a></li>
<li>paper_authors: Samuel Pfrommer, Brendon G. Anderson, Somayeh Sojoudi</li>
<li>for: 提供可证明 robustness 的分类器设计</li>
<li>methods: 使用随机填充方法在低维投影空间中进行随机缓和，并 characterize 缓和后的证明区域</li>
<li>results: 对 CIFAR-10 和 SVHN 进行实验，表明我们的方法可以提供 tractable 的下界，并且在证明区域内捕捉到普通扰动的perturbationsHere is the same information in Simplified Chinese:</li>
<li>for: 设计可证明 robustness 的分类器</li>
<li>methods: 使用随机填充方法在低维投影空间中进行随机缓和，并 characterize 缓和后的证明区域</li>
<li>results: 对 CIFAR-10 和 SVHN 进行实验，表明我们的方法可以提供 tractable 的下界，并且在证明区域内捕捉到普通扰动的perturbations<details>
<summary>Abstract</summary>
Randomized smoothing is the current state-of-the-art method for producing provably robust classifiers. While randomized smoothing typically yields robust $\ell_2$-ball certificates, recent research has generalized provable robustness to different norm balls as well as anisotropic regions. This work considers a classifier architecture that first projects onto a low-dimensional approximation of the data manifold and then applies a standard classifier. By performing randomized smoothing in the low-dimensional projected space, we characterize the certified region of our smoothed composite classifier back in the high-dimensional input space and prove a tractable lower bound on its volume. We show experimentally on CIFAR-10 and SVHN that classifiers without the initial projection are vulnerable to perturbations that are normal to the data manifold and yet are captured by the certified regions of our method. We compare the volume of our certified regions against various baselines and show that our method improves on the state-of-the-art by many orders of magnitude.
</details>
<details>
<summary>摘要</summary>
随机缓和是当前状态艺术方法，生成可证明抗干扰的分类器。通常情况下，随机缓和会生成 $\ell_2$-球证明，但最近的研究已经推广了不同 norm 球证明以及不规则区域。这个工作考虑一种分类器架构，首先将数据投影到低维度的数据投影空间，然后应用标准分类器。通过在低维度投影空间中进行随机缓和，我们Characterize了我们熔化 composite 分类器的证明区域，并证明了可读取的下界。我们在 CIFAR-10 和 SVHN 上进行实验，并证明了不包含初始投影的分类器容易受到数据投影方向的攻击，但是我们的方法可以捕捉这些攻击。我们比较了我们的证明区域的体积与各种基准值，并证明了我们的方法在状态艺术中提高了多个阶段的质量。
</details></li>
</ul>
<hr>
<h2 id="ReMasker-Imputing-Tabular-Data-with-Masked-Autoencoding"><a href="#ReMasker-Imputing-Tabular-Data-with-Masked-Autoencoding" class="headerlink" title="ReMasker: Imputing Tabular Data with Masked Autoencoding"></a>ReMasker: Imputing Tabular Data with Masked Autoencoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13793">http://arxiv.org/abs/2309.13793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tydusky/remasker">https://github.com/tydusky/remasker</a></li>
<li>paper_authors: Tianyu Du, Luca Melis, Ting Wang</li>
<li>for: 这个论文是为了推算缺失数据的 tabular 数据填充方法。</li>
<li>methods: 这个方法是基于 masked autoencoding 框架的扩展，其中 besides 缺失数据（即自然缺失），还随机 “重新mask” 一些值，通过优化 autoencoder 来重建这些重新mask 的值，并将模型应用于预测缺失数据。</li>
<li>results: 与优秀的方法进行比较，我们在多种缺失设定下进行了广泛的评估，并显示了 ReMasker 在缺失率不同的情况下的性能都在或超过了现有方法，而且其性能优势通常随缺失数据的比例增长。此外，我们还进行了理论准确性的探讨，并证明 ReMasker 通常学习缺失数据不变的表示。<details>
<summary>Abstract</summary>
We present ReMasker, a new method of imputing missing values in tabular data by extending the masked autoencoding framework. Compared with prior work, ReMasker is both simple -- besides the missing values (i.e., naturally masked), we randomly ``re-mask'' another set of values, optimize the autoencoder by reconstructing this re-masked set, and apply the trained model to predict the missing values; and effective -- with extensive evaluation on benchmark datasets, we show that ReMasker performs on par with or outperforms state-of-the-art methods in terms of both imputation fidelity and utility under various missingness settings, while its performance advantage often increases with the ratio of missing data. We further explore theoretical justification for its effectiveness, showing that ReMasker tends to learn missingness-invariant representations of tabular data. Our findings indicate that masked modeling represents a promising direction for further research on tabular data imputation. The code is publicly available.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法 called ReMasker，用于填充缺失数据的表格数据中的缺失值。与之前的工作相比，ReMasker 更简单，只有缺失值（即自然缺失）之外，我们随机“重新覆盖”另一个集合的值，然后优化自动编码器，重建这个重新覆盖集合，并使用训练模型预测缺失值。与此同时，我们还进行了广泛的评估，发现 ReMasker 在不同的缺失设定下，与或超过现有方法的稳定性和实用性。此外，我们还进行了理论 justify 其效果，发现 ReMasker 倾向于学习缺失性 invariable 的表格数据表示。我们的发现表明，masked modeling 是一个有前途的研究方向。代码公开 available。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Free-Statistical-Dispersion-Control-for-Societal-Applications"><a href="#Distribution-Free-Statistical-Dispersion-Control-for-Societal-Applications" class="headerlink" title="Distribution-Free Statistical Dispersion Control for Societal Applications"></a>Distribution-Free Statistical Dispersion Control for Societal Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13786">http://arxiv.org/abs/2309.13786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhun Deng, Thomas P. Zollo, Jake C. Snell, Toniann Pitassi, Richard Zemel</li>
<li>for: 这个论文主要是为了提供有关机器学习模型性能的证明，以确保机器学习模型在实际应用中的性能是否符合预期。</li>
<li>methods: 这篇论文使用了一种简单 yet 灵活的框架，可以处理更加复杂的统计函数。这种框架使用了分布自由的方法，以控制不同人群的统计分布。</li>
<li>results: 该论文通过实验表明，该方法可以在恶意评论检测、医疗影像和电影推荐等领域中提供精确的统计保证。<details>
<summary>Abstract</summary>
Explicit finite-sample statistical guarantees on model performance are an important ingredient in responsible machine learning. Previous work has focused mainly on bounding either the expected loss of a predictor or the probability that an individual prediction will incur a loss value in a specified range. However, for many high-stakes applications, it is crucial to understand and control the dispersion of a loss distribution, or the extent to which different members of a population experience unequal effects of algorithmic decisions. We initiate the study of distribution-free control of statistical dispersion measures with societal implications and propose a simple yet flexible framework that allows us to handle a much richer class of statistical functionals beyond previous work. Our methods are verified through experiments in toxic comment detection, medical imaging, and film recommendation.
</details>
<details>
<summary>摘要</summary>
具有具体 finite-sample 统计保证的机器学习是责任感知的重要组成部分。先前的工作主要关注在预测器的预期损失下的约束或者预测结果会在指定范围内带来损失值的概率上。然而，许多高度投资应用中，控制统计分布的偏差是关键，也就是不同人群受到机器决策的不同影响程度。我们开始研究不含统计分布的控制方法，并提出了简单 yet flexible 的框架，可以处理更加复杂的统计函数。我们的方法通过对毒评排除、医疗成像和电影推荐进行实验验证。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-For-Reduced-Popularity-Bias-In-Multi-Territory-Video-Recommendations"><a href="#Multi-Task-Learning-For-Reduced-Popularity-Bias-In-Multi-Territory-Video-Recommendations" class="headerlink" title="Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations"></a>Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03148">http://arxiv.org/abs/2310.03148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phanideep Gampa, Farnoosh Javadi, Belhassen Bayar, Ainur Yessenalina</li>
<li>for: 提高多区域个性化推荐系统中 item 的准确率，解决 globally prevalent item 的偏袋问题。</li>
<li>methods: 使用多任务学习 (MTL) 技术，并采用适应性增 sampling 方法来减少 popularity bias。</li>
<li>results: 通过实验，我们 demonstarte 了我们的框架在多区域比基eline 表现出较好的效果，PR-AUC 指标中的增幅可达 $65.27%$。<details>
<summary>Abstract</summary>
Various data imbalances that naturally arise in a multi-territory personalized recommender system can lead to a significant item bias for globally prevalent items. A locally popular item can be overshadowed by a globally prevalent item. Moreover, users' viewership patterns/statistics can drastically change from one geographic location to another which may suggest to learn specific user embeddings. In this paper, we propose a multi-task learning (MTL) technique, along with an adaptive upsampling method to reduce popularity bias in multi-territory recommendations. Our proposed framework is designed to enrich training examples with active users representation through upsampling, and capable of learning geographic-based user embeddings by leveraging MTL. Through experiments, we demonstrate the effectiveness of our framework in multiple territories compared to a baseline not incorporating our proposed techniques.~Noticeably, we show improved relative gain of up to $65.27\%$ in PR-AUC metric. A case study is presented to demonstrate the advantages of our methods in attenuating the popularity bias of global items.
</details>
<details>
<summary>摘要</summary>
不同地区的用户偏好会自然出现在多地区个性化推荐系统中，导致全球热销商品的 item bias。一个地区热销商品可能被全球热销商品所掩蔽。此外，用户的视频浏览习惯可能在不同的地理位置发生重大变化，这可能建议学习特定用户嵌入。在这篇论文中，我们提出了一种多任务学习（MTL）技术，以及适应填充方法，以减少多地区推荐中的流行度偏好。我们的提议框架通过填充活跃用户表示来增强训练示例，并能够通过 MTL 学习地域基于用户嵌入。通过实验，我们证明了我们的框架在多地区比基eline不 incorporating 我们的提议技术时表现更高的效果。特别是，我们显示了改进的相对增长率达到 $65.27\%$ 的 PR-AUC 指标。一个案例研究表明了我们的方法在减少全球商品的流行度偏好中的优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/cs.LG_2023_09_25/" data-id="clpztdnlc00sges88fv6jhlr7" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/eess.SP_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T08:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/eess.SP_2023_09_25/">eess.SP - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-a-Novel-Ultrasound-System-Based-on-Low-Frequency-Feature-Extraction-From-a-Fully-Printed-Flexible-Transducer"><a href="#Towards-a-Novel-Ultrasound-System-Based-on-Low-Frequency-Feature-Extraction-From-a-Fully-Printed-Flexible-Transducer" class="headerlink" title="Towards a Novel Ultrasound System Based on Low-Frequency Feature Extraction From a Fully-Printed Flexible Transducer"></a>Towards a Novel Ultrasound System Based on Low-Frequency Feature Extraction From a Fully-Printed Flexible Transducer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14569">http://arxiv.org/abs/2309.14569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Giordano, Kirill Keller, Francesco Greco, Luca Benini, Michele Magno, Christoph Leitner</li>
<li>for: 这个研究是为了开发一个可靠、便宜、携带式的数码声带测量系统，用于不侵入性地、连续地监测生命 Parameters。</li>
<li>methods: 这个研究使用了一个全新的印刷式、无铅、数码声带感应器，可以实现较好的适材化性。实验室设置使用了一个模拟人体血液流的流体模型和一个模拟心跳的拍脉机，以验证方法。</li>
<li>results: 研究结果显示，这个新型数码声带感应器可以实现高精度的血液流速度测量，并且可以实现低功耗和低带宽的处理。在不同的心跳rhythm下，测量结果皆具有误差不超过0.05Hz（3bpm）。此外，实验室设置显示，这个方法可以实现6倍以上的讯号宽度减少，从12.5MHz降至2MHz。<details>
<summary>Abstract</summary>
Ultrasound is a key technology in healthcare, and it is being explored for non-invasive, wearable, continuous monitoring of vital signs. However, its widespread adoption in this scenario is still hindered by the size, complexity, and power consumption of current devices. Moreover, such an application demands adaptability to human anatomy, which is hard to achieve with current transducer technology. This paper presents a novel ultrasound system prototype based on a fully printed, lead-free, and flexible polymer ultrasound transducer, whose bending radius promises good adaptability to the human anatomy. Our application scenario focuses on continuous blood flow monitoring. We implemented a hardware envelope filter to efficiently transpose high-frequency ultrasound signals to a lower-frequency spectrum. This reduces computational and power demands with little to no degradation in the task proposed for this work. We validated our method on a setup that mimics human blood flow by using a flow phantom and a peristaltic pump simulating 3 different heartbeat rhythms: 60, 90 and 120 beats per minute. Our ultrasound setup reconstructs peristaltic pump frequencies with errors of less than 0.05 Hz (3 bpm) from the set pump frequency, both for the raw echo and the enveloped echo. The analog pre-processing showed a promising reduction of signal bandwidth of more than 6x: pulse-echo signals of transducers excited at 12.5 MHz were reduced to about 2 MHz. Thus, allowing consumer MCUs to acquire and elaborate signals within mW-power range in an inexpensive fashion.
</details>
<details>
<summary>摘要</summary>
“ultrasound是现代医疗技术中关键的一种，正在探索无侵入、可穿戴、不间断监测生命体指标的应用场景。然而，现有设备的大小、复杂度和功耗仍然阻碍了广泛的应用。此外，这种应用场景需要适应人体解剖结构，这是现有探音器技术很难实现。这篇论文提出了一种新的探音系统原型，基于完全印刷、无铅、 flexible полимер探音器。这种探音器的弯曲半径 promise good适应人体解剖结构。我们的应用场景是无间断血流监测。我们实施了硬件滤波器，以有效地将高频探音信号转换为低频spectrum。这 reduces computational和功耗占用，几乎不会影响我们所提出的任务。我们验证了我们的方法，使用一个模拟人血流的流体phantom和一个模拟心跳的剧热泵。我们的ultrasound设备可以准确地重construct peristaltic pump frequencies， errors of less than 0.05 Hz（3 bpm）from the set pump frequency， both for the raw echo and the enveloped echo。analog pre-processing showed a promising reduction of signal bandwidth of more than 6x：pulse-echo signals of transducers excited at 12.5 MHz were reduced to about 2 MHz。因此，allowing consumer MCUs to acquire and elaborate signals within mW-power range in an inexpensive fashion。”
</details></li>
</ul>
<hr>
<h2 id="Secret-Message-Transmission-by-Echoing-Encrypted-Probes-–-STEEP"><a href="#Secret-Message-Transmission-by-Echoing-Encrypted-Probes-–-STEEP" class="headerlink" title="Secret-Message Transmission by Echoing Encrypted Probes – STEEP"></a>Secret-Message Transmission by Echoing Encrypted Probes – STEEP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14529">http://arxiv.org/abs/2309.14529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingbo Hua</li>
<li>for: 这篇论文研究了Maurer、Ahlswede和Csiszar（MAC）为秘密键容量的下限和上限的性质，并基于这些下限的约束，提出了一种名为“秘密信息传输通过响应加密探测”（STEEP）的方案。</li>
<li>methods: 该方案包括两个阶段：在第一阶段，Alice通过探测频道发送随机探测信号给Bob；在第二阶段，Bob根据预测的探测信号发送一个加密的版本给Alice，但是这个加密版本是使用一个秘密来加密的。</li>
<li>results: 如果恶作剂Eve无法获得Alice在第一阶段发送的准确探测信号，那么STEEP可以在返回频道上保证从Bob到Alice的秘密率大于0，即使恶作剂在探测频道上的频率 stronger than Bob的。 STEEP适用于物理层和Upper层在连接网络中。<details>
<summary>Abstract</summary>
This paper examines the properties of the lower and upper bounds established by Maurer, Ahlswede and Csiszar (MAC) for secret-key capacity in the case of channel probing over single-input and single-output (SISO) channels. Inspired by the insights into MAC's bounds, a scheme called secret-message transmission by echoing encrypted probes (STEEP) is proposed. STEEP consists of two phases: in phase 1, Alice sends random probes over a probing channel to Bob; in phase 2, Bob echoes back an estimated version of the probes, but encrypted by a secret, over a high-quality return channel. Provided that Eve is unable to obtain the exact probes transmitted by Alice in phase 1, STEEP guarantees a positive secrecy rate from Bob to Alice over the return channel even if Eve's channel strength during channel probing is stronger than Bob's. STEEP is applicable to both physical layer and upper layers in connected networks.
</details>
<details>
<summary>摘要</summary>
STEEP consists of two phases:1. In phase 1, Alice sends random probes over a probing channel to Bob.2. In phase 2, Bob echoes back an estimated version of the probes, but encrypted by a secret, over a high-quality return channel.Assuming that Eve is unable to obtain the exact probes transmitted by Alice in phase 1, STEEP guarantees a positive secrecy rate from Bob to Alice over the return channel, even if Eve's channel strength during channel probing is stronger than Bob's.STEEP is applicable to both physical layer and upper layers in connected networks.
</details></li>
</ul>
<hr>
<h2 id="Heart-rate-measurement-using-the-built-in-triaxial-accelerometer-from-a-commercial-digital-writing-device"><a href="#Heart-rate-measurement-using-the-built-in-triaxial-accelerometer-from-a-commercial-digital-writing-device" class="headerlink" title="Heart rate measurement using the built-in triaxial accelerometer from a commercial digital writing device"></a>Heart rate measurement using the built-in triaxial accelerometer from a commercial digital writing device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14308">http://arxiv.org/abs/2309.14308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julie Payette, Fabrice Vaussenat, Sylvain G. Cloutier</li>
<li>for: 这个研究用于比较智能笔的内置加速度仪和标准ECG仪器中的心率数据是否准确。</li>
<li>methods: 这个研究使用了智能笔equipped with sensors（STABILO的DigiPen）的内置加速度仪和标准ECG仪器来收集数据。数据处理使用了Butterworth滤波器减少噪声。</li>
<li>results: 研究发现，智能笔的内置加速度仪可以准确地预测心率，与标准ECG数据的相关性高于0.99。<details>
<summary>Abstract</summary>
Wearable devices are on the rise. Smart watches and phones, fitness trackers or smart textiles now provide unprecedented access to our own personal data. As such, wearable devices can enable health monitoring without disrupting our daily routines. In clinical settings, electrocardiograms (ECGs) and photoplethysmographies (PPGs) are used to monitor the heart's and respiratory behaviors. In more practical settings, accelerometers can be used to estimate the heartrate when they are attached to the chest. They can also help filter out some noise in ECG signal from movement. In this work, we compare the heart rate data extracted from the built-in accelerometer of a commercial smart pen equipped with sensors (STABILO's DigiPen), with a standard ECG monitor readouts. We demonstrate that it is possible to accurately predict the heart rate from the smart pencil. The data collection is done with eight volunteers, writing the alphabet continuously for five minutes. The signal is processed with a Butterworth filter to cut off noise. We achieve a mean-squared error (MSE) better than 6.685x10$^{-3}$ comparing the DigiPen's computed ${\Delta}$t (time between pulses) with the reference ECG data. The peaks' timestamps for both signals all maintain a correlation higher than 0.99. All computed heart rates from the pen accurately correlate with the reference ECG signals.
</details>
<details>
<summary>摘要</summary>
“智能装置在不断增长。智能手表和手机、健身器或智能纺织物现在提供了前所未有的个人数据访问权。因此，智能装置可以不间断地健康监测，不会影响我们的日常生活。在临床设置下，电喷呈（ECG）和光谱呈（PPG）用于监测心脏和呼吸的行为。在更实际的设置下，加速计可以用于估算心率，当它们附加到胸部时。它们还可以帮助滤除一些运动所引起的噪声在ECG信号中。在这个工作中，我们比较了 comercial smart pen 内建的加速计和标准 ECG 监测器的数据。我们示示了可以准确地预测心率从 smart pen 中提取的数据。数据采集使用八名志愿者，在五分钟内连续写字母。信号处理使用Butterworth滤波器剪辑噪声。我们实现了比较于 6.685 x 10$^{-3}$ 的mean-squared error（MSE），比较 commercial smart pen 计算的 $\Delta$t（心脏бит间隔）与参考 ECG 数据。两个信号的峰时间戳都保持了高于 0.99 的相关性。所有从 pen 中计算的心率都准确地与参考 ECG 信号相关。”
</details></li>
</ul>
<hr>
<h2 id="Joint-RIS-Phase-Profile-Design-and-Power-Allocation-for-Parameter-Estimation-in-Presence-of-Eavesdropping"><a href="#Joint-RIS-Phase-Profile-Design-and-Power-Allocation-for-Parameter-Estimation-in-Presence-of-Eavesdropping" class="headerlink" title="Joint RIS Phase Profile Design and Power Allocation for Parameter Estimation in Presence of Eavesdropping"></a>Joint RIS Phase Profile Design and Power Allocation for Parameter Estimation in Presence of Eavesdropping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14280">http://arxiv.org/abs/2309.14280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Mehdipour Abadi, Ayda Nodel Hokmabadi, Sinan Gezici</li>
<li>for: 本文主要研究了在具有各种各样的环境和干扰的情况下，通过重新配置智能表面（RIS），实现安全传输一个权重矢量参数的精准性。</li>
<li>methods: 本文提出了一种基于Fisher信息矩阵追踪（FIM）的估计精度度量，并使用其关于接收器和侦测器的关键性能指标，以便在RIS环境中实现优化传输精度。</li>
<li>results: 本文通过 alternating 优化和semidefinite relaxation 等方法，解决了RIS级别和发射器级别的优化问题，并通过实验证明了干扰的影响和RIS级别的选择对传输精度的影响。<details>
<summary>Abstract</summary>
We consider secure transmission of a deterministic complex-valued parameter vector from a transmitter to an intended receiver in the presence of an eavesdropper in a reconfigurable intelligent surface (RIS)-integrated environment. We aim to jointly optimize the RIS phase profile and the power allocation matrix at the transmitter to enhance the estimation accuracy at the intended receiver while limiting that at the eavesdropper. We utilize the trace of the Fisher information matrix (FIM), equivalently, the average Fisher information, as the estimation accuracy metric, and obtain its closed form expression for the intended receiver and the eavesdropper. Accordingly, the joint RIS phase profile and power allocation problem is formulated, and it is solved via alternating optimization. When the power allocation matrix is fixed during alternating optimization, the optimal RIS phase profile design problem is formulated as a non-convex problem and it is solved via semidefinite relaxation and rank reduction. When the RIS phase profile is fixed, a linear programming formulation is obtained for optimal power allocation. Via simulations, the effects of RIS phase design and power allocation are illustrated individually and jointly. Moreover, extensions are provided by considering the presence of line of sight paths in the environment and the availability of RIS elements with adjustable magnitudes.
</details>
<details>
<summary>摘要</summary>
我们考虑了一个报文加密传输的幂等复数参数向量从发送器到目标接收器的安全传输，在扩展智能表面（RIS）集成环境中。我们希望同时优化RIS相位特征和发送器的功率分配矩阵，以提高接收器的估计精度，同时限制侦测器的估计精度。我们使用追踪 Fisher信息矩阵（FIM）的跟踪，即平均 Fisher信息，作为估计精度度量，并得到其闭合形式表达。根据此，我们提出了共同优化RIS相位特征和功率分配问题，并通过 alternate 优化解决。当功率分配矩阵在 alternate 优化过程中固定时，则RIS相位特征设计问题变为非对称问题，并通过半definite  relaksation和级数减少解决。当RIS相位特征固定时，则发送器的功率分配问题可以转化为线性程序。通过实验，我们证明了RIS相位设计和功率分配在个体和共同优化下的效果。此外，我们还提供了考虑线性路径的存在和RIS元素的调整级别的扩展。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Three-Layer-Hybrid-Reconfigurable-Intelligent-Surface-for-6G-Wireless-Communication-Trade-offs-and-Performance"><a href="#Adaptive-Three-Layer-Hybrid-Reconfigurable-Intelligent-Surface-for-6G-Wireless-Communication-Trade-offs-and-Performance" class="headerlink" title="Adaptive Three Layer Hybrid Reconfigurable Intelligent Surface for 6G Wireless Communication: Trade-offs and Performance"></a>Adaptive Three Layer Hybrid Reconfigurable Intelligent Surface for 6G Wireless Communication: Trade-offs and Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14087">http://arxiv.org/abs/2309.14087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashed Hasan Ratul, Muhammad Iqbal, Tabinda Ashraf, Jen-Yi Pan, Yi-Han Wang, Shao-Yu Lien</li>
<li>for: 本研究旨在提供一种三层混合式RIS助け的配置方案，可以响应active和passive RIS的变化，同时具有静止或无功状态。</li>
<li>methods: 本研究使用了hybrid RIS-assisted配置，包括三层混合式RIS，以适应变化的发射功率和无线链路质量。</li>
<li>results:  simulations表明，该三层混合式RIS-assisted配置比单独使用passive或active RIS-assisted技术更有优势。<details>
<summary>Abstract</summary>
A potential candidate technology for the development of future 6G networks has been recognized as Reconfigurable Intelligent Surface (RIS). However, due to the variation in radio link quality, traditional passive RISs only accomplish a minimal signal gain in situations with strong direct links between user equipment (UE) and base station (BS). In order to get over this fundamental restriction of smaller gain, the idea of active RISs might be a suitable solution. In contrast to current passive RIS, which simply reflects and directs signals without any additional amplification, active RISs have the ability to enhance reflected signals by the incorporation of amplifiers inside its elements. However, with additional amplifiers, apart from the relatively complex attributes of RIS-assisted arrangements, the additional energy consumption of such technologies is often disregarded. So, there might be a tradeoff between the additional energy consumption for the RIS technologies and the overall gain acquired by deploying this potential advancement. The objective of this work is to provide a primary idea of a three-layer hybrid RIS-assisted configuration that is responsive to both active and passive RIS, as well as an additional dormant or inactive state. The single RIS structure should be capable of adjusting its overall configuration in response to fluctuations in transmit power and radio link quality. Furthermore, our fabricated passive RIS-assisted structure verifies a portion of the proposed idea, with simulations highlighting its advantages over standalone passive or active RIS-assisted technologies.
</details>
<details>
<summary>摘要</summary>
sixth generation 网络（6G）的发展中，一种潜在的技术是可配置智能表面（Reconfigurable Intelligent Surface，RIS）。然而，由于无线链路质量的变化，传统的静止RIS只能实现最小的信号增强，尤其在用户设备（UE）和基站（BS）之间的强直接链路情况下。为了突破这种基本限制，可能适用的解决方案是活动RIS。与现有的静止RIS相比，活动RIS可以通过内置扩增器提高反射信号的强度。然而，随着这些技术的增加，除了RIS-assisted的复杂性外，额外的能源消耗也常被忽视。因此，可能存在一种负担增加和增加的负担之间的权衡。本工作的目标是提供一种三层混合RIS-assisted配置，可以响应活动和静止RIS，以及额外的休眠或不活跃状态。单一RIS结构应该能够根据发射功率和无线链路质量的变化进行调整。此外，我们制造的静止RIS-assisted结构的实验证明了一部分的提案的优势，而且模拟结果表明，与独立的静止或活动RIS-assisted技术相比，这种三层混合配置具有更高的优势。
</details></li>
</ul>
<hr>
<h2 id="Single-Antenna-Jammers-in-MIMO-OFDM-Can-Resemble-Multi-Antenna-Jammers"><a href="#Single-Antenna-Jammers-in-MIMO-OFDM-Can-Resemble-Multi-Antenna-Jammers" class="headerlink" title="Single-Antenna Jammers in MIMO-OFDM Can Resemble Multi-Antenna Jammers"></a>Single-Antenna Jammers in MIMO-OFDM Can Resemble Multi-Antenna Jammers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14059">http://arxiv.org/abs/2309.14059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iip-group/ofdm-jammer">https://github.com/iip-group/ofdm-jammer</a></li>
<li>paper_authors: Gian Marti, Christoph Studer</li>
<li>for: 这篇论文研究了多输入多输出（MIMO）无线系统中频率平坦渠道下单天线干扰器对接收器的影响。</li>
<li>methods: 论文使用了线性空间滤波来消除干扰器所引起的干扰。</li>
<li>results: 研究发现，当干扰器不遵守OFDM协议，它会在多个子帧上引起干扰，而不是单一的一维空间。这意味着在MIMO-OFDM系统中，单天线干扰器可以类比为L天线干扰器。<details>
<summary>Abstract</summary>
In multiple-input multiple-output (MIMO) wireless systems with frequency-flat channels, a single-antenna jammer causes receive interference that is confined to a one-dimensional subspace. Such a jammer can thus be nulled using linear spatial filtering at the cost of one degree of freedom. Frequency-selective channels are often transformed into multiple frequency-flat subcarriers with orthogonal frequency-division multiplexing (OFDM). We show that when a single-antenna jammer violates the OFDM protocol by not sending a cyclic prefix, the interference received on each subcarrier by a multi-antenna receiver is, in general, not confined to a subspace of dimension one (as a single-antenna jammer in a frequency-flat scenario would be), but of dimension L, where L is the jammer's number of channel taps. In MIMO-OFDM systems, a single-antenna jammer can therefore resemble an L-antenna jammer. Simulations corroborate our theoretical results. These findings imply that mitigating jammers with large delay spread through linear spatial filtering is infeasible. We discuss some (im)possibilities for the way forward.
</details>
<details>
<summary>摘要</summary>
在多输入多输出（MIMO）无线系统中，频率平坦渠道上的单天线妨碍器会导致接收干扰，这种干扰将被限制在一维空间中。这种妨碍器可以使用线性空间滤波来纠正，但是需要一个自由度。频率选择性渠道通常会被转换为多个平坦频分谱下的多个子帧，使用多载波分多plexing（OFDM）。我们表明，当妨碍器不遵循OFDM协议，并不发送循环前fix，则干扰接收到每个子帧的多天线接收器是，在总体来说，不再局限于一维空间中（如单天线妨碍器在频率平坦场景中会），而是局限于维度L，其中L是妨碍器的通道扩散的数量。在MIMO-OFDM系统中，单天线妨碍器可以类比于L天线妨碍器。实验证明了我们的理论结果。这些发现表明，通过线性空间滤波来mitigate妨碍器的影响是不可能的。我们讨论了一些（不）可能的前进方向。
</details></li>
</ul>
<hr>
<h2 id="Beam-Squint-Assisted-User-Localization-in-Near-Field-Integrated-Sensing-and-Communications-Systems"><a href="#Beam-Squint-Assisted-User-Localization-in-Near-Field-Integrated-Sensing-and-Communications-Systems" class="headerlink" title="Beam Squint Assisted User Localization in Near-Field Integrated Sensing and Communications Systems"></a>Beam Squint Assisted User Localization in Near-Field Integrated Sensing and Communications Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14012">http://arxiv.org/abs/2309.14012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongliang Luo, Feifei Gao, Wanmai Yuan, Shun Zhang</li>
<li>for: 这篇论文旨在提出一种基于true-time-delay lines（TTDs）的近场通信系统中的用户定位方法，用于解决宽频MIMO系统中的辐束偏移问题。</li>
<li>methods: 该方法利用TTDs控制近场辐束偏移的轨迹，并通过不同子载波束的扫描来实现用户定位。</li>
<li>results:  simulations show that the proposed method can effectively reduce the beam sweeping overhead and achieve high accuracy user localization.<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC) has been regarded as a key technology for 6G wireless communications, in which large-scale multiple input and multiple output (MIMO) array with higher and wider frequency bands will be adopted. However, recent studies show that the beam squint phenomenon can not be ignored in wideband MIMO system, which generally deteriorates the communications performance. In this paper, we find that with the aid of true-time-delay lines (TTDs), the range and trajectory of the beam squint in near-field communications systems can be freely controlled, and hence it is possible to reversely utilize the beam squint for user localization. We derive the trajectory equation for near-field beam squint points and design a way to control such trajectory. With the proposed design, beamforming from different subcarriers would purposely point to different angles and different distances, such that users from different positions would receive the maximum power at different subcarriers. Hence, one can simply localize multiple users from the beam squint effect in frequency domain, and thus reduce the beam sweeping overhead as compared to the conventional time domain beam search based approach. Furthermore, we utilize the phase difference of the maximum power subcarriers received by the user at different frequencies in several times beam sweeping to obtain a more accurate distance estimation result, ultimately realizing high accuracy and low beam sweeping overhead user localization. Simulation results demonstrate the effectiveness of the proposed schemes.
</details>
<details>
<summary>摘要</summary>
integrated sensing and communication (ISAC) 被认为是 sixth generation wireless communication (6G) 中的关键技术，其中大规模的多输入多输出 (MIMO) 阵列将在更高频率范围内使用。然而，最近的研究表明，在宽频MIMO系统中，扫描干扰（beam squint）现象无法忽略。在这篇论文中，我们发现，通过使用真实时延线（TTD），在近距离通信系统中扫描干扰的范围和轨迹可以自由控制，因此可以利用扫描干扰进行用户位置localization。我们 derive了近距离扫描干扰点的轨迹方程，并设计了控制这种轨迹的方法。与我们的设计相比，通过时域扫描来实现用户位置的方法可以减少扫描干扰的过程。此外，我们利用不同频率下接收到用户的最大功率Subcarrier的相位差来获取更加准确的距离估计结果，从而实现高精度低扫描干扰的用户位置定位。实验结果表明我们的方案的有效性。
</details></li>
</ul>
<hr>
<h2 id="Carrier-Aggregation-Enabled-Integrated-Sensing-and-Communication-Signal-Design-and-Processing"><a href="#Carrier-Aggregation-Enabled-Integrated-Sensing-and-Communication-Signal-Design-and-Processing" class="headerlink" title="Carrier Aggregation Enabled Integrated Sensing and Communication Signal Design and Processing"></a>Carrier Aggregation Enabled Integrated Sensing and Communication Signal Design and Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14008">http://arxiv.org/abs/2309.14008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Wei, Haotian Liu, Xinyi Yang, Wangjun Jiang, Huici Wu, Xingwang Li, Zhiyong Feng</li>
<li>for: 本研究旨在提高未来移动通信系统中智能应用（如互联网联盟（IoV）和扩展现实（XR））的数据传输率和探测精度，通过 интегрирован的感知和通信（ISAC）技术。</li>
<li>methods: 本研究提议使用加载组合（CA）技术将高频和低频频率带width拼接成一个信号，以提高探测性能。此外，本研究还提出了基于压缩感知（CS）的ISAC信号处理算法，并使用快速融合减少阈值算法（FISTA）解决重新配置减少问题。</li>
<li>results: 实验结果表明，CA技术可以有效提高距离和速度估计的准确性。<details>
<summary>Abstract</summary>
The future mobile communication systems will support intelligent applications such as Internet of Vehicles (IoV) and Extended Reality (XR). Integrated Sensing and Communication (ISAC) is regarded as one of the key technologies satisfying the high data rate communication and highly accurate sensing for these intelligent applications in future mobile communication systems. With the explosive growth of wireless devices and services, the shortage of spectrum resources leads to the fragmentation of available frequency bands for ISAC systems, which degrades sensing performance. Facing the above challenges, this paper proposes a Carrier Aggregation (CA)-based ISAC signal aggregating high and low-frequency bands to improve the sensing performance, where the CA-based ISAC signal can use four different aggregated pilot structures for sensing. Then, an ISAC signal processing algorithm with Compressed Sensing (CS) is proposed and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is used to solve the reconfiguration convex optimization problem. Finally, the Cram'er-Rao Lower Bounds (CRLBs) are derived for the CA-based ISAC signal. Simulation results show that CA efficiently improves the accuracy of range and velocity estimation.
</details>
<details>
<summary>摘要</summary>
将来的移动通信系统将支持智能应用程序，如网络化交通 (IoV) 和增强现实 (XR)。集成感知通信 (ISAC) 被认为是未来移动通信系统支持高速数据传输和高精度感知的关键技术。随着无线设备和服务的快速增长，可用频率带的缺乏导致 ISAC 系统的分配频率带产生干扰，从而降低感知性能。面对这些挑战，本文提出了基于搅合 (CA) 的 ISAC 信号搅合高频和低频频率带以提高感知性能。然后，一种基于 CS 的 ISAC 信号处理算法和快速融合缩放算法 (FISTA) 被提出，以解决重配置减少问题。最后，CA 基于 ISAC 信号的 Cram'er-Rao Lower Bounds (CRLBs) 被 derivation。实验结果表明，CA 可以有效提高范围和速度估计的准确性。
</details></li>
</ul>
<hr>
<h2 id="Near-field-Hybrid-Beamforming-for-Terahertz-band-Integrated-Sensing-and-Communications"><a href="#Near-field-Hybrid-Beamforming-for-Terahertz-band-Integrated-Sensing-and-Communications" class="headerlink" title="Near-field Hybrid Beamforming for Terahertz-band Integrated Sensing and Communications"></a>Near-field Hybrid Beamforming for Terahertz-band Integrated Sensing and Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13984">http://arxiv.org/abs/2309.13984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet M. Elbir, Abdulkadir Celik, Ahmed M. Eltawil</li>
<li>for: 这篇论文旨在探讨 sixth generation 无线网络中的 Terahertz (THz) 频段通信和集成感知通信 (ISAC) 两大方面。</li>
<li>methods: 作者提出了一种 alternating optimization 技术来解决 near-field THz-ISAC enario 中的质量降低问题。</li>
<li>results: 作者通过数值仿真显示了该方法可以在不添加增加硬件Components的情况下实现atisfactory的spectral efficiency表现，并准确地估算near-field beamformers，有效地 mitigate near-field beam-squint。<details>
<summary>Abstract</summary>
Terahertz (THz) band communications and integrated sensing and communications (ISAC) are two main facets of the sixth generation wireless networks. In order to compensate the severe attenuation, the THz wireless systems employ large arrays, wherein the near-field beam-squint severely degrades the beamforming accuracy. Contrary to prior works that examine only either narrowband ISAC beamforming or far-field models, we introduce an alternating optimization technique for hybrid beamforming design in near-field THz-ISAC scenario. We also propose an efficient approach to compensate near-field beam-squint via baseband beamformers. Via numerical simulations, we show that the proposed approach achieves satisfactory spectral efficiency performance while accurately estimating the near-field beamformers and mitigating the beam-squint without additional hardware components.
</details>
<details>
<summary>摘要</summary>
六代无线网络中的tera响（THz）频段通信和集成感知通信（ISAC）是两个主要方面。为了抵消严重强化，THz无线系统使用大型阵列，其中靠近场区域的 beam-squint 严重降低了射频形成精度。与先前的工作只研究了窄频段ISAC射频形成或远场模型，我们引入了交替优化技术为混合射频形成设计。我们还提出了一种有效的方法来资料near-field beam-squint via baseband射频former。通过数值仿真，我们表明了我们的方法可以实现满意的spectral efficiency性能，准确地估计near-field射频former和mitigate beam-squint，无需额外硬件组件。
</details></li>
</ul>
<hr>
<h2 id="Track-before-detect-Algorithm-based-on-Cost-reference-Particle-Filter-Bank-for-Weak-Target-Detection"><a href="#Track-before-detect-Algorithm-based-on-Cost-reference-Particle-Filter-Bank-for-Weak-Target-Detection" class="headerlink" title="Track-before-detect Algorithm based on Cost-reference Particle Filter Bank for Weak Target Detection"></a>Track-before-detect Algorithm based on Cost-reference Particle Filter Bank for Weak Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13922">http://arxiv.org/abs/2309.13922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Lu, Guojie Peng, Weichuan Zhang, Changming Sun</li>
<li>for: 这个论文是为了解决在雷达、声纳等应用中检测弱目标的问题而写的。</li>
<li>methods: 该论文提出了一种基于改进的 particel Filter 的 track-before-detect（TBD）算法，即 cost-reference particle filter bank（CRPFB）。该算法将目标检测转化为两层假设测试问题。</li>
<li>results: 对于非线性频率调制（NLFM）信号检测和跟踪实验， simulate 结果表明，提出的 TBD 算法比现有的 TBD 算法在检测、跟踪和时间效率方面表现更好。<details>
<summary>Abstract</summary>
Detecting weak target is an important and challenging problem in many applications such as radar, sonar etc. However, conventional detection methods are often ineffective in this case because of low signal-to-noise ratio (SNR). This paper presents a track-before-detect (TBD) algorithm based on an improved particle filter, i.e. cost-reference particle filter bank (CRPFB), which turns the problem of target detection to the problem of two-layer hypothesis testing. The first layer is implemented by CRPFB for state estimation of possible target. CRPFB has entirely parallel structure, consisting amounts of cost-reference particle filters with different hypothesized prior information. The second layer is to compare a test metric with a given threshold, which is constructed from the output of the first layer and fits GEV distribution. The performance of our proposed TBD algorithm and the existed TBD algorithms are compared according to the experiments on nonlinear frequency modulated (NLFM) signal detection and tracking. Simulation results show that the proposed TBD algorithm has better performance than the state-of-the-arts in detection, tracking, and time efficiency.
</details>
<details>
<summary>摘要</summary>
检测弱目标是许多应用中的一个重要和挑战性问题，如雷达和SONAR等。然而，传统的检测方法经常无法处理这种情况，因为信号噪声比（SNR）过低。这篇论文提出了基于改进的粒子筛算法（cost-reference particle filter bank，CRPFB）的track-before-detect（TBD）算法，将目标检测转化为两层假设测试问题。第一层由CRPFB实现的状态估计可能的目标，CRPFB具有完全平行结构，包括不同假设先验信息的多个成本参照粒子筛。第二层是比较测试指标与给定的阈值，该阈值由第一层的输出和适应GEV分布构建。对于非线性频率模ulation（NLFM）信号检测和跟踪的实验结果显示，提议的TBD算法比现有的TBD算法在检测、跟踪和时间效率方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="Online-Resource-Allocation-for-Semantic-Aware-Edge-Computing-Systems"><a href="#Online-Resource-Allocation-for-Semantic-Aware-Edge-Computing-Systems" class="headerlink" title="Online Resource Allocation for Semantic-Aware Edge Computing Systems"></a>Online Resource Allocation for Semantic-Aware Edge Computing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13917">http://arxiv.org/abs/2309.13917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Cang, Ming Chen, Zhaohui Yang, Yuntao Hu, Yinlu Wang, Chongwen Huang, Zhaoyang Zhang</li>
<li>for: 这篇论文旨在提出一个基于 semantics 的联合通信和计算资源分配框架，以减轻 MEC 系统中讯息的传输负担。</li>
<li>methods: 这篇论文使用了 Lyapunov 优化、封页数据分析和继承数据分析等方法，将联合通信和计算资源分配问题转化为一个可解决的长期问题。</li>
<li>results:  simulations 显示，提出的算法可以与无 semantics 的分配方法相比，节省至多 41.8% 的能源。<details>
<summary>Abstract</summary>
In this paper, we propose a semantic-aware joint communication and computation resource allocation framework for MEC systems. In the considered system, random tasks arrive at each terminal device (TD), which needs to be computed locally or offloaded to the MEC server. To further release the transmission burden, each TD sends the small-size extracted semantic information of tasks to the server instead of the original large-size raw data. An optimization problem of joint semanticaware division factor, communication and computation resource management is formulated. The problem aims to minimize the energy consumption of the whole system, while satisfying longterm delay and processing rate constraints. To solve this problem, an online low-complexity algorithm is proposed. In particular, Lyapunov optimization is utilized to decompose the original coupled long-term problem into a series of decoupled deterministic problems without requiring the realizations of future task arrivals and channel gains. Then, the block coordinate descent method and successive convex approximation algorithm are adopted to solve the current time slot deterministic problem by observing the current system states. Moreover, the closed-form optimal solution of each optimization variable is provided. Simulation results show that the proposed algorithm yields up to 41.8% energy reduction compared to its counterpart without semantic-aware allocation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于 semantics 的共享计算和通信资源分配框架 для MEC 系统。在考虑的系统中，Random tasks 会随机到each terminal device（TD），需要本地计算或者卸载到 MEC 服务器。为了进一步减轻传输负担，each TD 将送出小型抽象信息（semantic information）到服务器，而不是原始大型原始数据。我们建立了一个优化问题，该问题的目标是最小化整体系统的能耗，同时满足长期延迟和处理率约束。为解决这个问题，我们提出了一种在线低复杂度算法。具体来说，我们利用了 Lyapunov 优化来将原来的 Coupled 长期问题分解成一系列的解耦的决定问题，无需考虑未来任务的到达和通道增益的实现。然后，我们采用了块坐标 descend 方法和Successive Convex Approximation 算法来解决当前时间槽的决定问题，并且提供了每个优化变量的关闭式最优解。实验结果显示，我们的算法可以提供 Up to 41.8% 的能源减少，相比无semantic-aware分配的对照方案。
</details></li>
</ul>
<hr>
<h2 id="NoncovANM-Gridless-DOA-Estimation-for-LPDF-System"><a href="#NoncovANM-Gridless-DOA-Estimation-for-LPDF-System" class="headerlink" title="NoncovANM: Gridless DOA Estimation for LPDF System"></a>NoncovANM: Gridless DOA Estimation for LPDF System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13902">http://arxiv.org/abs/2309.13902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangying Zhao, Peng Chen, Zhenxin Cao, Xianbin Wang</li>
<li>for: 提高低成本irection finding系统的精度和效率</li>
<li>methods: 利用智能可编程表面（IRS）和低成本探测chnology，采用一个完全可用的接收通道，并利用atomic norm minimization（ANM）问题来优化direction finding表现</li>
<li>results: 提出了一种基于非对称�C-ANM算法的方法，可以快速和高效地优化direction finding表现，并且在实验中比较高效和精度高于比较方法Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the accuracy and efficiency of low-cost passive direction finding (LPDF) systems.</li>
<li>methods: The proposed method utilizes an intelligent reconfigurable surface (IRS)-aided LPDF system, which only requires one fully functional receiving channel. The method exploits the sparsity of targets in the spatial domain by formulating an atomic norm minimization (ANM) problem to estimate the direction of arrival (DOA). To solve the ANM problem, a novel nonconvex-based ANM (NC-ANM) method is proposed, which uses gradient threshold iteration to avoid falling into saddle points. The theoretical analysis for the convergence of the NC-ANM method is also provided.</li>
<li>results: The proposed method outperforms compared methods in DOA estimation with lower computational complexity in the LPDF system, as shown by simulation results.<details>
<summary>Abstract</summary>
Direction of arrival (DOA) estimation is an important research in the area of array signal processing, and has been studied for decades. High resolution DOA estimation requires large array aperture, which leads to the increase of hardware cost. Besides, high accuracy DOA estimation methods usually have high computational complexity. In this paper, the problem of decreasing the hardware cost and algorithm complexity is addressed. First, considering the ability of flexible controlling the electromagnetic waves and low-cost, an intelligent reconfigurable surface (IRS)-aided low-cost passive direction finding (LPDF) system is developed, where only one fully functional receiving channel is adopted. Then, the sparsity of targets direction in the spatial domain is exploited by formulating an atomic norm minimization (ANM) problem to estimate the DOA. Traditionally, solving ANM problem is complex and cannot be realized efficiently. Hence, a novel nonconvex-based ANM (NC-ANM) method is proposed by gradient threshold iteration, where a perturbation is introduced to avoid falling into saddle points. The theoretical analysis for the convergence of the NC-ANM method is also given. Moreover, the corresponding Cram\'er-Rao lower bound (CRLB) in the LPDF system is derived, and taken as the referred bound of the DOA estimation. Simulation results show that the proposed method outperforms the compared methods in the DOA estimation with lower computational complexity in the LPDF system.
</details>
<details>
<summary>摘要</summary>
irection of arrival (DOA) 估计是阵列信号处理领域的重要研究，已经在数十年来被研究。高分辨率DOA估计需要大型阵列尺寸，这会导致硬件成本的增加。此外，高精度DOA估计方法通常具有高计算复杂性。在这篇论文中，解决降低硬件成本和算法复杂性的问题。首先，根据可控电磁波的能力和低成本，一种智能可重配置表面（IRS）帮助的低成本通过探测（LPDF）系统被开发，只有一个完全可用的接收通道。然后，通过利用目标方向在空间领域的稀畴性，提出一个原子范数最小化（ANM）问题来估计DOA。传统上，解决ANM问题是复杂的，无法有效实现。因此，一种新的非对称-基于ANM（NC-ANM）方法被提出，通过梯度阈值迭代来解决。另外，对NC-ANM方法的理论分析也给出。此外，对LPDF系统中DOA估计的Cramér-Rao下界（CRLB）也被 derive，作为DOA估计的参照 bound。实验结果表明，提出的方法在LPDF系统中的DOA估计中具有较低的计算复杂性和更高的精度，并且超过相关比较方法。
</details></li>
</ul>
<hr>
<h2 id="DNN-DANM-A-High-Accuracy-Two-Dimensional-DOA-Estimation-Method-Using-Practical-RIS"><a href="#DNN-DANM-A-High-Accuracy-Two-Dimensional-DOA-Estimation-Method-Using-Practical-RIS" class="headerlink" title="DNN-DANM: A High-Accuracy Two-Dimensional DOA Estimation Method Using Practical RIS"></a>DNN-DANM: A High-Accuracy Two-Dimensional DOA Estimation Method Using Practical RIS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13856">http://arxiv.org/abs/2309.13856</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenpengseu/dnn-danm">https://github.com/chenpengseu/dnn-danm</a></li>
<li>paper_authors: Zhimin Chen, Peng Chen, Le Zheng, Yudong Zhang</li>
<li>for: 这个论文研究了在实际的快速知识Surface (RIS) 系统中，使用深度学习和分解原理来实现两个方向的到来角度估计 (DOA) 问题。</li>
<li>methods: 该论文提出了一种 combining 深度学习 (DNN) 和分解原理 (DANM) 的新方法，用于解决 DOA 估计问题。此外，还提出了一种低计算复杂度的 semi-definite programming (SDP) 方法来解决原子化最小化问题。</li>
<li>results: 该论文通过实验和原型 validate 了该方法在实际 RIS 系统中的性能，并证明了它在两个维度 DOA 估计中具有较低的复杂度和较高的准确率。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) or intelligent reflecting surface (IRS) has been an attractive technology for future wireless communication and sensing systems. However, in the practical RIS, the mutual coupling effect among RIS elements, the reflection phase shift, and amplitude errors will degrade the RIS performance significantly. This paper investigates the two-dimensional direction-of-arrival (DOA) estimation problem in the scenario using a practical RIS. After formulating the system model with the mutual coupling effect and the reflection phase/amplitude errors of the RIS, a novel DNNDANM method is proposed for the DOA estimation by combining the deep neural network (DNN) and the decoupling atomic norm minimization (DANM). The DNN step reconstructs the received signal from the one with RIS impairments, and the DANM step exploits the signal sparsity in the two-dimensional spatial domain. Additionally, a semi-definite programming (SDP) method with low computational complexity is proposed to solve the atomic minimization problem. Finally, both simulation and prototype are carried out to show estimation performance, and the proposed method outperforms the existing methods in the two-dimensional DOA estimation with low complexity in the scenario with practical RIS.
</details>
<details>
<summary>摘要</summary>
现代化的智能反射表（RIS）或智能镜面（IRS）技术在未来无线通信和探测系统中具有吸引力。然而，在实际应用中的RIS中，元件之间的共振效应、反射阶段偏移和干扰错误会对RIS性能产生负面影响。本文研究使用实际RIS场景下的两个维度方向来估计（DOA）问题。经过制定系统模型，包括RIS中的共振效应和反射阶段偏移/干扰错误，我们提出了一种基于深度神经网络（DNN）和解决原子范数最小化（DANM）的新的DNNDANM方法。DNN步骤重建接收信号，而DANM步骤利用信号在两个维度空间中的稀疏性。此外，我们还提出了一种具有低计算复杂性的半definiteProgramming（SDP）方法来解决原子最小化问题。最后，我们通过实验和prototype来证明我们的方法在两个维度DOA估计中具有低复杂性和高性能，并且超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="On-the-Energy-Efficiency-of-THz-NOMA-enhanced-UAV-Cooperative-Network-with-SWIPT"><a href="#On-the-Energy-Efficiency-of-THz-NOMA-enhanced-UAV-Cooperative-Network-with-SWIPT" class="headerlink" title="On the Energy Efficiency of THz-NOMA enhanced UAV Cooperative Network with SWIPT"></a>On the Energy Efficiency of THz-NOMA enhanced UAV Cooperative Network with SWIPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13836">http://arxiv.org/abs/2309.13836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jalal Jalali, Ata Khalili, Hina Tabassum, Rafael Berkvens, Jeroen Famaey, Walid Saad</li>
<li>for: 该论文寻求最大化无线能效 (EE)，在同时进行无线信息传输和能量传输的无人机飞行器协同网络中，采用teraHertz（THz）频率。</li>
<li>methods: 论文使用了非对称多访问（NOMA）功率分配系数、SWIPT功率拆分（PS）比率和无人机轨迹优化，以最大化EE。</li>
<li>results: 论文通过分解为两个阶段优化问题，使用替代优化方法，并通过比较基准点来证明提议的资源分配算法的效果。<details>
<summary>Abstract</summary>
This paper considers the energy efficiency (EE) maximization of a simultaneous wireless information and power transfer (SWIPT)-assisted unmanned aerial vehicles (UAV) cooperative network operating at TeraHertz (THz) frequencies. The source performs SWIPT enabling the UAV to receive both power and information while also transmitting the information to a designated destination node. Subsequently, the UAV utilizes the harvested energy to relay the data to the intended destination node effectively. Specifically, we maximize EE by optimizing the non-orthogonal multiple access (NOMA) power allocation coefficients, SWIPT power splitting (PS) ratio, and UAV trajectory. The main problem is broken down into a two-stage optimization problem and solved using an alternating optimization approach. In the first stage, optimization of the PS ratio and trajectory is performed by employing successive convex approximation using a lower bound on the exponential factor in the THz channel model. In the second phase, the NOMA power coefficients are optimized using a quadratic transform approach. Numerical results demonstrate the effectiveness of our proposed resource allocation algorithm compared to the baselines where there is no trajectory optimization or no NOMA power or PS optimization.
</details>
<details>
<summary>摘要</summary>
The problem is broken down into a two-stage optimization problem and solved using an alternating optimization approach. In the first stage, the PS ratio and trajectory are optimized using successive convex approximation with a lower bound on the exponential factor in the THz channel model. In the second stage, the NOMA power coefficients are optimized using a quadratic transform approach.Numerical results show that the proposed resource allocation algorithm outperforms baseline scenarios without trajectory optimization or NOMA power or PS optimization.
</details></li>
</ul>
<hr>
<h2 id="Study-of-Robust-Adaptive-Beamforming-Algorithms-Based-on-Power-Method-Processing-and-Spatial-Spectrum-Matching"><a href="#Study-of-Robust-Adaptive-Beamforming-Algorithms-Based-on-Power-Method-Processing-and-Spatial-Spectrum-Matching" class="headerlink" title="Study of Robust Adaptive Beamforming Algorithms Based on Power Method Processing and Spatial Spectrum Matching"></a>Study of Robust Adaptive Beamforming Algorithms Based on Power Method Processing and Spatial Spectrum Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13785">http://arxiv.org/abs/2309.13785</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Mohammadzadeh, V. H. Nascimento, R. C. de Lamare, O. Kukrer</li>
<li>for: 提高Robust adaptive beamforming（RAB）在covariance矩阵重建错误存在时的性能。</li>
<li>methods: 提议一种基于interference-plus-noise covariance（INC）矩阵重建的efficient RAB技术，包括根据力量方法估计干扰的电力和方向 вектор，然后使用空间匹配处理重建所需的信号-Plus-噪声矩阵。最后，排除噪声组件以保留所需的信号矩阵。</li>
<li>results: 对比已有方法，提议的方法可以更好地适应covariance矩阵重建错误的情况，并且可以提高RAB的性能。<details>
<summary>Abstract</summary>
Robust adaptive beamforming (RAB) based on interference-plus-noise covariance (INC) matrix reconstruction can experience performance degradation when model mismatch errors exist, particularly when the input signal-to-noise ratio (SNR) is large. In this work, we devise an efficient RAB technique for dealing with covariance matrix reconstruction issues. The proposed method involves INC matrix reconstruction using an idea in which the power and the steering vector of the interferences are estimated based on the power method. Furthermore, spatial match processing is computed to reconstruct the desired signal-plus-noise covariance matrix. Then, the noise components are excluded to retain the desired signal (DS) covariance matrix. A key feature of the proposed technique is to avoid eigenvalue decomposition of the INC matrix to obtain the dominant power of the interference-plus-noise region. Moreover, the INC reconstruction is carried out according to the definition of the theoretical INC matrix. Simulation results are shown and discussed to verify the effectiveness of the proposed method against existing approaches.
</details>
<details>
<summary>摘要</summary>
《robust适应 beamforming（RAB）基于干扰＋噪声 covariance（INC）矩阵重建可以遇到性能下降问题，特别是当输入信号响应比（SNR）较大时。在这种工作中，我们设计了一种高效的 RAB 技术来处理 INC 矩阵重建问题。该方法包括使用力量和扫描向量来估算干扰的 INC 矩阵，然后通过空间匹配处理来重建 желатель信号＋噪声 covariance 矩阵。最后，噪声成分被排除，保留 желатель信号 covariance 矩阵。本方法的一个关键特点是不需要对 INC 矩阵进行特征值分解，以获得干扰＋噪声区域的主要功率。此外，INC 重建遵循了理论 INC 矩阵的定义。实验结果表明，提议的方法比既有方法更高效。》Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/eess.SP_2023_09_25/" data-id="clpztdnui01f2es88gyo88jnv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/41/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/40/">40</a><a class="page-number" href="/page/41/">41</a><span class="page-number current">42</span><a class="page-number" href="/page/43/">43</a><a class="page-number" href="/page/44/">44</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/43/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
