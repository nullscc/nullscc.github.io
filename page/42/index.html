
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/42/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/cs.CL_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T11:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/cs.CL_2023_09_25/">cs.CL - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Introducing-DictaLM-–-A-Large-Generative-Language-Model-for-Modern-Hebrew"><a href="#Introducing-DictaLM-–-A-Large-Generative-Language-Model-for-Modern-Hebrew" class="headerlink" title="Introducing DictaLM – A Large Generative Language Model for Modern Hebrew"></a>Introducing DictaLM – A Large Generative Language Model for Modern Hebrew</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14568">http://arxiv.org/abs/2309.14568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaltiel Shmidman, Avi Shmidman, Amir David Nissan Cohen, Moshe Koppel</li>
<li>for: 这篇论文是为了开发一个适用于现代希伯来语的大规模语言模型。</li>
<li>methods: 这篇论文使用了7亿个参数的模型，主要是在希伯来语中进行训练。作者还发布了基础模型和指导适应模型，并将其发布于Creative Commons许可证下。此外，作者还介绍了DictaLM-Rab基础模型，这是专门针对 rabbinic&#x2F;历史希伯来语的。</li>
<li>results: 这篇论文提出了一个初步的希伯来语大规模语言模型，可以用于多种希伯来语特定任务的细化调整，如教学、问答、情感分析等。这是一个对希伯来语NLP社区的一个初步探索。<details>
<summary>Abstract</summary>
We present DictaLM, a large-scale language model tailored for Modern Hebrew. Boasting 7B parameters, this model is predominantly trained on Hebrew-centric data. As a commitment to promoting research and development in the Hebrew language, we release both the foundation model and the instruct-tuned model under a Creative Commons license. Concurrently, we introduce DictaLM-Rab, another foundation model geared towards Rabbinic/Historical Hebrew. These foundation models serve as ideal starting points for fine-tuning various Hebrew-specific tasks, such as instruction, Q&A, sentiment analysis, and more. This release represents a preliminary step, offering an initial Hebrew LLM model for the Hebrew NLP community to experiment with.
</details>
<details>
<summary>摘要</summary>
我们介绍DictaLM，一种适用于现代希伯来语的大规模语言模型。这个模型拥有70亿参数，主要基于希伯来语数据进行训练。作为推广希伯来语研究和发展的承诺，我们在Creative Commons许可证下发布了基础模型和指导训练模型。同时，我们还引入DictaLM-Rab，另一个针对 rabbinic/历史希伯来语的基础模型。这些基础模型可以用于多种希伯来语特定任务的细化调整，如教学、问答、情感分析等。这次发布代表希伯来语NLPT社区的初步尝试，希望能够促进希伯来语语言模型的研究和发展。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Large-Multimodal-Models-with-Factually-Augmented-RLHF"><a href="#Aligning-Large-Multimodal-Models-with-Factually-Augmented-RLHF" class="headerlink" title="Aligning Large Multimodal Models with Factually Augmented RLHF"></a>Aligning Large Multimodal Models with Factually Augmented RLHF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14525">http://arxiv.org/abs/2309.14525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell</li>
<li>for: addressing the multimodal misalignment issue in large multimodal models (LMM)</li>
<li>methods: using reinforcement learning from human feedback (RLHF) to train a vision-language model to align with human annotations, and augmenting the reward model with additional factual information such as image captions and ground-truth multi-choice options</li>
<li>results: achieving a remarkable improvement of 94% on the LLaVA-Bench dataset and an improvement by 60% on MMHAL-BENCH over other baselines, with the first LMM trained with RLHF.Here are the three information in Simplified Chinese text:</li>
<li>for: 解决大量多模态模型（LMM）中的多模态误差问题</li>
<li>methods: 使用人类反馈学习（RLHF）来训练一个视觉语言模型，并将奖励模型增加了更多的事实信息，如图文描述和真实多选项</li>
<li>results: 在LLaVA-Bench数据集上达到了94%的表现水平，比前一个最佳方法提高了60%，并且开源了代码、模型和数据在<a target="_blank" rel="noopener" href="https://llava-rlhf.github.io./">https://llava-rlhf.github.io。</a><details>
<summary>Abstract</summary>
Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in "hallucination", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.
</details>
<details>
<summary>摘要</summary>
大型多modal模型（LMM）在modalities之间建立起来，但是这两个modalities之间的不一致可能导致"幻觉",生成不受多modal信息的支持的文本输出。为了解决多modal不一致问题，我们从文本领域中提取了人类反馈学习（RLHF），并将其应用到视觉语言对应中，请求人工标注员比较两个响应，并标出更加幻觉的一个，并训练视觉语言模型以 Maximize 模拟人类奖励。我们提出了一种新的对ignment算法called Factually Augmented RLHF，该算法将奖励模型中的奖励信息与更多的事实信息（如图像描述和真实多选项）相结合，以解决奖励黑客现象，并进一步提高性能。此外，我们还使用了之前已有的人类写的图像文本对应来提高我们模型的总能力。为了评估我们的方法在实际场景中的表现，我们开发了一个新的评估标准MMHAL-BENCH，强调对幻觉进行惩罚。作为首个RLHF模型，我们的方法在LLaVA-Bench数据集上达到了94%的性能水平，而前一个最佳方法只能达到87%的水平，在MMHAL-BENCH上与其他基eline相比，我们的方法提高了60%。我们将代码、模型和数据公开发布在https://llava-rlhf.github.io。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-Performance-on-Standardized-Testing-Exam-–-A-Proposed-Strategy-for-Learners"><a href="#ChatGPT-Performance-on-Standardized-Testing-Exam-–-A-Proposed-Strategy-for-Learners" class="headerlink" title="ChatGPT Performance on Standardized Testing Exam – A Proposed Strategy for Learners"></a>ChatGPT Performance on Standardized Testing Exam – A Proposed Strategy for Learners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14519">http://arxiv.org/abs/2309.14519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Umer Farooq, Saira Anwar</li>
<li>for: 这研究探讨了ChatGPT在标准化测试准备中的问题解决能力，特点是关注GRE数学部分。先前的研究表明了ChatGPT在不同学科中的学习方法有很大的潜力。</li>
<li>methods: 我们通过对GRE数学部分100个随机选择的问题进行Quantitative评估来研究ChatGPT在不同内容领域中的问题解决能力。我们还使用t检验来检验修改问题提示对ChatGPT的准确率的影响。</li>
<li>results: 结果显示，对问题提示进行修改后，ChatGPT的准确率有 statistically significant 的提高（84%对修改后的问题，69%对原始数据）。研究还发现ChatGPT在某些问题上存在困难，并提供了修改问题提示的方法可以帮助学生准备标准测试 like GRE。<details>
<summary>Abstract</summary>
This study explores the problem solving capabilities of ChatGPT and its prospective applications in standardized test preparation, focusing on the GRE quantitative exam. Prior research has shown great potential for the utilization of ChatGPT for academic purposes in revolutionizing the approach to studying across various disciplines. We investigate how ChatGPT performs across various question types in the GRE quantitative domain, and how modifying question prompts impacts its accuracy. More specifically this study addressed two research questions: 1. How does ChatGPT perform in answering GRE-based quantitative questions across various content areas? 2. How does the accuracy of ChatGPT vary with modifying the question prompts? The dataset consisting of 100 randomly selected GRE quantitative questions was collected from the ETS official guide to GRE test preparation. We used quantitative evaluation to answer our first research question, and t-test to examine the statistical association between prompt modification and ChatGPT's accuracy. Results show a statistical improvement in the ChatGPT's accuracy after applying instruction priming and contextual prompts to the original questions. ChatGPT showed 84% accuracy with the modified prompts compared to 69% with the original data. The study discusses the areas where ChatGPT struggled with certain questions and how modifications can be helpful for preparing for standardized tests like GRE and provides future directions for prompt modifications.
</details>
<details>
<summary>摘要</summary>
To answer our first research question, we used quantitative evaluation to assess ChatGPT's performance on GRE-based quantitative questions across different content areas. We also used a t-test to examine the statistical association between prompt modification and ChatGPT's accuracy. Our results show that ChatGPT's accuracy improved statistically after we applied instruction priming and contextual prompts to the original questions. With the modified prompts, ChatGPT achieved 84% accuracy, compared to 69% with the original data.The study also discusses the areas where ChatGPT struggled with certain questions and how modifications can be helpful for preparing for standardized tests like GRE. We provide future directions for prompt modifications and highlight the potential of using ChatGPT for test preparation.Here is the translation in Simplified Chinese:这个研究探讨了ChatGPT的问题解决能力和其在标准化测试准备中的可能应用，特点是关注GRE数学部分。先前的研究表明了ChatGPT可以用于学术目的，可以革命化学习的方式。我们调查了ChatGPT如何在不同类型的GRE数学题目上表现，以及如何修改问题提示影响其准确率。为了回答我们的第一个研究问题，我们使用量化评估来评估ChatGPT在GRE数学题目上的表现，并使用t检验来检验修改提示和ChatGPT的准确率之间的统计关系。我们的结果显示，在我们应用了指导提示和文本提示后，ChatGPT的准确率有 statistically 的提高。与原始数据相比，ChatGPT在修改后的问题上达到了84%的准确率，与原始数据相比，这是69%的提高。研究还讨论了ChatGPT在某些问题上的困难之处，以及修改如何有助于为GRE和其他标准化测试准备。我们还提供了未来的提示修改方向，并强调了使用ChatGPT进行测试准备的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="DeepSpeed-Ulysses-System-Optimizations-for-Enabling-Training-of-Extreme-Long-Sequence-Transformer-Models"><a href="#DeepSpeed-Ulysses-System-Optimizations-for-Enabling-Training-of-Extreme-Long-Sequence-Transformer-Models" class="headerlink" title="DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"></a>DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14509">http://arxiv.org/abs/2309.14509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He</li>
<li>for: 本研究旨在提高大型语言模型（LLM）的训练效率，特别是对长序Transformer模型进行加速。</li>
<li>methods: 本文提出了一种新的方法——DeepSpeed-Ulysses，它可以高效地对长序LLM进行训练。这种方法通过分解输入数据的序列维度，并使用高效的所有到所有集成通信来计算注意力。</li>
<li>results: 实验表明，DeepSpeed-Ulysses可以比现有基eline方法快速2.5倍，并且可以在4倍长的序列长度上进行训练。<details>
<summary>Abstract</summary>
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.
</details>
<details>
<summary>摘要</summary>
Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size, and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable, and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5 times faster with 4 times longer sequence length than the existing method SOTA baseline.
</details></li>
</ul>
<hr>
<h2 id="Classifying-token-frequencies-using-angular-Minkowski-p-distance"><a href="#Classifying-token-frequencies-using-angular-Minkowski-p-distance" class="headerlink" title="Classifying token frequencies using angular Minkowski $p$-distance"></a>Classifying token frequencies using angular Minkowski $p$-distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14495">http://arxiv.org/abs/2309.14495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Urs Lenz, Chris Cornelis</li>
<li>for: 本研究旨在探讨Angular Minkowski $p$-distance是一种代替cosine dissimilarity的异同度量，以及它在20-newsgroups dataset上的分类性能。</li>
<li>methods: 本研究使用了粗糙最近几个邻居和经典加权最近几个邻居来评估分类性能，并分析了$p$参数、数据维度$m$,邻居数$k$和权重的选择对分类性能的影响。</li>
<li>results: 研究发现，采用Angular Minkowski $p$-distance可以获得substantially higher的分类性能，特别是当$p$取得合适的值时。<details>
<summary>Abstract</summary>
Angular Minkowski $p$-distance is a dissimilarity measure that is obtained by replacing Euclidean distance in the definition of cosine dissimilarity with other Minkowski $p$-distances. Cosine dissimilarity is frequently used with datasets containing token frequencies, and angular Minkowski $p$-distance may potentially be an even better choice for certain tasks. In a case study based on the 20-newsgroups dataset, we evaluate clasification performance for classical weighted nearest neighbours, as well as fuzzy rough nearest neighbours. In addition, we analyse the relationship between the hyperparameter $p$, the dimensionality $m$ of the dataset, the number of neighbours $k$, the choice of weights and the choice of classifier. We conclude that it is possible to obtain substantially higher classification performance with angular Minkowski $p$-distance with suitable values for $p$ than with classical cosine dissimilarity.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Explainable-and-Accurate-Natural-Language-Understanding-for-Voice-Assistants-and-Beyond"><a href="#Explainable-and-Accurate-Natural-Language-Understanding-for-Voice-Assistants-and-Beyond" class="headerlink" title="Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond"></a>Explainable and Accurate Natural Language Understanding for Voice Assistants and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14485">http://arxiv.org/abs/2309.14485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kalpa Gunaratna, Vijay Srinivasan, Hongxia Jin</li>
<li>for: joint NLU (Natural Language Understanding)  JOINT NLU是智能声助手上无可或缺的一环，它的目标是同时检测用户的意图和 slot filling。</li>
<li>methods: 使用各种技术提高准确率，并使模型自然易于理解和解释。</li>
<li>results: 对 JOINT NLU 模型进行自然易于理解和解释，不会影响准确率。同时，这种扩展可以在其他普通分类任务中使用。<details>
<summary>Abstract</summary>
Joint intent detection and slot filling, which is also termed as joint NLU (Natural Language Understanding) is invaluable for smart voice assistants. Recent advancements in this area have been heavily focusing on improving accuracy using various techniques. Explainability is undoubtedly an important aspect for deep learning-based models including joint NLU models. Without explainability, their decisions are opaque to the outside world and hence, have tendency to lack user trust. Therefore to bridge this gap, we transform the full joint NLU model to be `inherently' explainable at granular levels without compromising on accuracy. Further, as we enable the full joint NLU model explainable, we show that our extension can be successfully used in other general classification tasks. We demonstrate this using sentiment analysis and named entity recognition.
</details>
<details>
<summary>摘要</summary>
joint意图检测和插槽填充（joint NLU）对智能声音助手是非常重要的。近期的进展在这个领域主要集中在提高准确率上。解释性是深度学习模型，包括联合NLU模型的重要方面。没有解释性，这些模型的决策对外部世界来说是不透明的，因此容易lack user trust。因此，我们将全部联合NLU模型变换成“基本”的解释性模型，无需牺牲准确率。此外，我们证明了我们的扩展可以成功应用于其他通用分类任务中。我们通过 sentiment分析和名称实体识别来说明这一点。
</details></li>
</ul>
<hr>
<h2 id="DeepSpeed-VisualChat-Multi-Round-Multi-Image-Interleave-Chat-via-Multi-Modal-Causal-Attention"><a href="#DeepSpeed-VisualChat-Multi-Round-Multi-Image-Interleave-Chat-via-Multi-Modal-Causal-Attention" class="headerlink" title="DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention"></a>DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14327">http://arxiv.org/abs/2309.14327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/deepspeedexamples">https://github.com/microsoft/deepspeedexamples</a></li>
<li>paper_authors: Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He</li>
<li>for: 提高大型语言模型对交互对话的适应性和扩展性</li>
<li>methods: 引入多模态功能，包括创新的多模态 causal 注意机制和数据融合技术</li>
<li>results: 比对 existed 框架 superior 扩展性，可以承受大型语言模型的70亿参数大小Here’s the Chinese text in simplified format:</li>
<li>for: 提高大型语言模型对交互对话的适应性和扩展性</li>
<li>methods: 引入多模态功能，包括创新的多模态 causal 注意机制和数据融合技术</li>
<li>results: 比对 existed 框架 superior 扩展性，可以承受大型语言模型的70亿参数大小<details>
<summary>Abstract</summary>
Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B parameter language model size, representing a significant advancement in multi-modal language models and setting a solid foundation for future explorations.
</details>
<details>
<summary>摘要</summary>
大多数现有多模态模型受到其不能够有效地处理交错图像和文本输入的限制，在多图多轮对话中受到资源分配和数据可accessibility的限制，影响其适应性和扩展性。为解决这一问题，我们提出了DeepSpeed-VisualChat框架，旨在优化大型语言模型，通过多模态能力提高大型语言和视觉模型对交错输入的处理能力。我们的框架具有以下三个特点：1. 支持多轮多图对话的开源实现，以便实现无缝的多模态对话。2. 引入创新的多模态 causal attention机制，以提高模型对交错输入的处理能力。3. 通过使用现有数据集的混合技术，保证多轮多图对话中的无缝交互。与现有框架相比，DeepSpeed-VisualChat显示出比较出色的扩展性，可以 Handle up to 70B parameter language model size，代表着多模态语言模型的显著进步，并为未来的探索提供了坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Towards-General-Purpose-Text-Instruction-Guided-Voice-Conversion"><a href="#Towards-General-Purpose-Text-Instruction-Guided-Voice-Conversion" class="headerlink" title="Towards General-Purpose Text-Instruction-Guided Voice Conversion"></a>Towards General-Purpose Text-Instruction-Guided Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14324">http://arxiv.org/abs/2309.14324</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/text-guided-vc/text-guided-vc.github.io">https://github.com/text-guided-vc/text-guided-vc.github.io</a></li>
<li>paper_authors: Chun-Yi Kuan, Chen An Li, Tsu-Yuan Hsu, Tse-Yang Lin, Ho-Lam Chung, Kai-Wei Chang, Shuo-yiin Chang, Hung-yi Lee</li>
<li>for: 这篇论文旨在描述一种新的语音转换（VC）模型，该模型根据文本指令（如“诠释慢速深音”或“说话带着幽默boyish音色”）进行指导。与传统方法不同的是，我们的模型可以根据文本指令来修改转换后的语音的谐音和情感信息，从而提供更多的灵活性和具体性。</li>
<li>methods: 该VC模型是一种基于神经网络编码语言模型的，它处理一个序列的精度码，并将其转换为转换后的语音序列。该模型使用文本指令作为样式提示，以修改源语音的不同方面。与之前的方法不同的是，我们的模型可以在端到端方式下处理不同方面的语音信息，而不需要分别使用多个编码器。</li>
<li>results: 实验表明，我们的模型能够很好地理解指令，并提供合理的转换结果。<details>
<summary>Abstract</summary>
This paper introduces a novel voice conversion (VC) model, guided by text instructions such as "articulate slowly with a deep tone" or "speak in a cheerful boyish voice". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Urdu-Poetry-Generated-by-Using-Deep-Learning-Techniques"><a href="#Urdu-Poetry-Generated-by-Using-Deep-Learning-Techniques" class="headerlink" title="Urdu Poetry Generated by Using Deep Learning Techniques"></a>Urdu Poetry Generated by Using Deep Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14233">http://arxiv.org/abs/2309.14233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Shoaib Farooq, Ali Abbas</li>
<li>For: 本研究提供了使用不同深度学习技术和算法生成的 Urdu 诗歌。* Methods: 该研究使用了 Long Short-term Memory Networks (LSTM) 和 Gated Recurrent Unit (GRU) 等深度学习模型，以及自然语言处理 (NLP) 技术来理解、分析和生成人类可以理解和使用的语言。* Results: 研究结果表明，使用这些技术可以生成具有高准确性的 Urdu 诗歌。<details>
<summary>Abstract</summary>
This study provides Urdu poetry generated using different deep-learning techniques and algorithms. The data was collected through the Rekhta website, containing 1341 text files with several couplets. The data on poetry was not from any specific genre or poet. Instead, it was a collection of mixed Urdu poems and Ghazals. Different deep learning techniques, such as the model applied Long Short-term Memory Networks (LSTM) and Gated Recurrent Unit (GRU), have been used. Natural Language Processing (NLP) may be used in machine learning to understand, analyze, and generate a language humans may use and understand. Much work has been done on generating poetry for different languages using different techniques. The collection and use of data were also different for different researchers. The primary purpose of this project is to provide a model that generates Urdu poems by using data completely, not by sampling data. Also, this may generate poems in pure Urdu, not Roman Urdu, as in the base paper. The results have shown good accuracy in the poems generated by the model.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Autonomous-Vehicles-an-overview-on-system-cyber-security-risks-issues-and-a-way-forward"><a href="#Autonomous-Vehicles-an-overview-on-system-cyber-security-risks-issues-and-a-way-forward" class="headerlink" title="Autonomous Vehicles an overview on system, cyber security, risks, issues, and a way forward"></a>Autonomous Vehicles an overview on system, cyber security, risks, issues, and a way forward</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14213">http://arxiv.org/abs/2309.14213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Aminul Islam, Sarah Alqahtani<br>for:这篇论文主要是为了探讨自动驾驶车的基本组件和运行特点，以及它们如何在互联网的框架下集成。methods:该论文使用了感知器、人工智能标识系统、控制机制等技术，并将其与云计算服务器集成在一起。results:该论文探讨了自动驾驶车在交通预测和交通预测等领域的实践应用，以及它们对不同产业的自动化任务的影响。同时，它还探讨了自动驾驶车的安全性问题，包括伦理、环境、法律、职业和社会方面的风险。<details>
<summary>Abstract</summary>
This chapter explores the complex realm of autonomous cars, analyzing their fundamental components and operational characteristics. The initial phase of the discussion is elucidating the internal mechanics of these automobiles, encompassing the crucial involvement of sensors, artificial intelligence (AI) identification systems, control mechanisms, and their integration with cloud-based servers within the framework of the Internet of Things (IoT). It delves into practical implementations of autonomous cars, emphasizing their utilization in forecasting traffic patterns and transforming the dynamics of transportation. The text also explores the topic of Robotic Process Automation (RPA), illustrating the impact of autonomous cars on different businesses through the automation of tasks. The primary focus of this investigation lies in the realm of cybersecurity, specifically in the context of autonomous vehicles. A comprehensive analysis will be conducted to explore various risk management solutions aimed at protecting these vehicles from potential threats including ethical, environmental, legal, professional, and social dimensions, offering a comprehensive perspective on their societal implications. A strategic plan for addressing the challenges and proposing strategies for effectively traversing the complex terrain of autonomous car systems, cybersecurity, hazards, and other concerns are some resources for acquiring an understanding of the intricate realm of autonomous cars and their ramifications in contemporary society, supported by a comprehensive compilation of resources for additional investigation.   Keywords: RPA, Cyber Security, AV, Risk, Smart Cars
</details>
<details>
<summary>摘要</summary>
The primary focus of this investigation lies in the realm of cybersecurity, specifically in the context of autonomous vehicles. A comprehensive analysis will be conducted to explore various risk management solutions aimed at protecting these vehicles from potential threats, including ethical, environmental, legal, professional, and social dimensions. This will provide a comprehensive perspective on their societal implications.In addition, a strategic plan for addressing the challenges and proposing strategies for effectively traversing the complex terrain of autonomous car systems, cybersecurity, hazards, and other concerns will be presented. This will be supported by a comprehensive compilation of resources for additional investigation.Keywords: RPA, Cyber Security, AV, Risk, Smart Cars
</details></li>
</ul>
<hr>
<h2 id="Only-5-Attention-Is-All-You-Need-Efficient-Long-range-Document-level-Neural-Machine-Translation"><a href="#Only-5-Attention-Is-All-You-Need-Efficient-Long-range-Document-level-Neural-Machine-Translation" class="headerlink" title="Only 5% Attention Is All You Need: Efficient Long-range Document-level Neural Machine Translation"></a>Only 5% Attention Is All You Need: Efficient Long-range Document-level Neural Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14174">http://arxiv.org/abs/2309.14174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Liu, Zewei Sun, Shanbo Cheng, Shujian Huang, Mingxuan Wang</li>
<li>for:  DocNMT for handling discourse phenomena in Machine Translation tasks, with the goal of improving efficiency while maintaining performance.</li>
<li>methods: The paper introduces a lightweight attention mechanism to select a small portion of tokens to be attended, reducing the computational cost of the attention module while maintaining performance.</li>
<li>results: The method achieves up to 95% sparsity (only 5% tokens attended) and saves 93% computation cost on the attention module compared to the original Transformer, while maintaining performance.<details>
<summary>Abstract</summary>
Document-level Neural Machine Translation (DocNMT) has been proven crucial for handling discourse phenomena by introducing document-level context information. One of the most important directions is to input the whole document directly to the standard Transformer model. In this case, efficiency becomes a critical concern due to the quadratic complexity of the attention module. Existing studies either focus on the encoder part, which cannot be deployed on sequence-to-sequence generation tasks, e.g., Machine Translation (MT), or suffer from a significant performance drop. In this work, we keep the translation performance while gaining 20\% speed up by introducing extra selection layer based on lightweight attention that selects a small portion of tokens to be attended. It takes advantage of the original attention to ensure performance and dimension reduction to accelerate inference. Experimental results show that our method could achieve up to 95\% sparsity (only 5\% tokens attended) approximately, and save 93\% computation cost on the attention module compared with the original Transformer, while maintaining the performance.
</details>
<details>
<summary>摘要</summary>
文档水平神经机器翻译（DocNMT）已经被证明是处理讨论现象的关键，通过引入文档级别的上下文信息。一个重要的方向是直接将整个文档输入到标准变换器模型中。在这种情况下，效率成为一个关键问题，因为变换器模型的注意模块的复杂度是二次的。现有的研究 either ocus 在encoder部分，无法在序列到序列生成任务中使用，例如机器翻译（MT），或者受到 significativ performance drop。在这种工作中，我们保持翻译性能，同时减少了20%的计算成本，通过引入附加的选择层，选择一小部分的Token进行注意。它利用原始注意来确保性能，并将维度减少以加速推理。实验结果表明，我们的方法可以达到约95%的稀疏性（只有5%的Token被注意），并将93%的计算成本减少在变换器模型中，同时保持性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-End-User-Development-for-IoT-A-Case-Study-on-Semantic-Parsing-of-Cooking-Recipes-for-Programming-Kitchen-Devices"><a href="#Towards-End-User-Development-for-IoT-A-Case-Study-on-Semantic-Parsing-of-Cooking-Recipes-for-Programming-Kitchen-Devices" class="headerlink" title="Towards End-User Development for IoT: A Case Study on Semantic Parsing of Cooking Recipes for Programming Kitchen Devices"></a>Towards End-User Development for IoT: A Case Study on Semantic Parsing of Cooking Recipes for Programming Kitchen Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14165">http://arxiv.org/abs/2309.14165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filipposventirozos/towards-end-user-development-for-iot">https://github.com/filipposventirozos/towards-end-user-development-for-iot</a></li>
<li>paper_authors: Filippos Ventirozos, Sarah Clinch, Riza Batista-Navarro</li>
<li>for: 支持烹饪recipe instructions的自然语言编程</li>
<li>methods: 使用 conditional random fields (CRF) 和神经网络模型进行语义分析</li>
<li>results: training semantic parsers based on annotations is feasible, but most natural-language instructions are incomplete and need to be transformed into formal meaning representation.Here’s the breakdown of each piece of information:</li>
<li>for: 支持烹饪recipe instructions的自然语言编程 (What the paper is written for)</li>
<li>methods: 使用 conditional random fields (CRF) 和神经网络模型进行语义分析 (What methods the paper uses)</li>
<li>results: training semantic parsers based on annotations is feasible, but most natural-language instructions are incomplete and need to be transformed into formal meaning representation. (What results the paper gets)<details>
<summary>Abstract</summary>
Semantic parsing of user-generated instructional text, in the way of enabling end-users to program the Internet of Things (IoT), is an underexplored area. In this study, we provide a unique annotated corpus which aims to support the transformation of cooking recipe instructions to machine-understandable commands for IoT devices in the kitchen. Each of these commands is a tuple capturing the semantics of an instruction involving a kitchen device in terms of "What", "Where", "Why" and "How". Based on this corpus, we developed machine learning-based sequence labelling methods, namely conditional random fields (CRF) and a neural network model, in order to parse recipe instructions and extract our tuples of interest from them. Our results show that while it is feasible to train semantic parsers based on our annotations, most natural-language instructions are incomplete, and thus transforming them into formal meaning representation, is not straightforward.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Semantic parsing of user-generated instructional text, in the way of enabling end-users to program the Internet of Things (IoT), is an underexplored area. In this study, we provide a unique annotated corpus which aims to support the transformation of cooking recipe instructions to machine-understandable commands for IoT devices in the kitchen. Each of these commands is a tuple capturing the semantics of an instruction involving a kitchen device in terms of "What", "Where", "Why" and "How". Based on this corpus, we developed machine learning-based sequence labelling methods, namely conditional random fields (CRF) and a neural network model, in order to parse recipe instructions and extract our tuples of interest from them. Our results show that while it is feasible to train semantic parsers based on our annotations, most natural-language instructions are incomplete, and thus transforming them into formal meaning representation, is not straightforward.中文翻译：用户生成的 instrucitonal text 的 semantics parsing，以实现终端用户对 Internet of Things (IoT) 的程式设定，是一个未得到充分探讨的领域。在这个研究中，我们提供了一个唯一的标注集，以支持将烹饪recipe的 instrucitons 转换为机器可理解的命令，并且每个命令都是一个捕捉烹饪 instruciton 的含义的 tuple，包括 "What"、"Where"、"Why" 和 "How"。基于这个标注集，我们开发了机器学习基于条件随机场 (CRF) 和神经网络模型，以解析 recipe instrucitons 并将我们的感兴趣 tuple 提取出来。我们的结果显示，可以对我们的标注集进行训练，但大多数自然语言 instrucitons 是不完整的，因此将它们转换为正式的意义表现，不是一个 straightforward 的任务。
</details></li>
</ul>
<hr>
<h2 id="Examining-Temporal-Bias-in-Abusive-Language-Detection"><a href="#Examining-Temporal-Bias-in-Abusive-Language-Detection" class="headerlink" title="Examining Temporal Bias in Abusive Language Detection"></a>Examining Temporal Bias in Abusive Language Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14146">http://arxiv.org/abs/2309.14146</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mali Jin, Yida Mu, Diana Maynard, Kalina Bontcheva<br>for: This study aims to investigate the nature and impact of temporal bias in abusive language detection across various languages and explore mitigation methods.methods: The study evaluates the performance of models on abusive data sets from different time periods and presents an extensive linguistic analysis of these abusive data sets from a diachronic perspective.results: The results demonstrate that temporal bias is a significant challenge for abusive language detection, with models trained on historical data showing a significant drop in performance over time.<details>
<summary>Abstract</summary>
The use of abusive language online has become an increasingly pervasive problem that damages both individuals and society, with effects ranging from psychological harm right through to escalation to real-life violence and even death. Machine learning models have been developed to automatically detect abusive language, but these models can suffer from temporal bias, the phenomenon in which topics, language use or social norms change over time. This study aims to investigate the nature and impact of temporal bias in abusive language detection across various languages and explore mitigation methods. We evaluate the performance of models on abusive data sets from different time periods. Our results demonstrate that temporal bias is a significant challenge for abusive language detection, with models trained on historical data showing a significant drop in performance over time. We also present an extensive linguistic analysis of these abusive data sets from a diachronic perspective, aiming to explore the reasons for language evolution and performance decline. This study sheds light on the pervasive issue of temporal bias in abusive language detection across languages, offering crucial insights into language evolution and temporal bias mitigation.
</details>
<details>
<summary>摘要</summary>
互联网上的辱语问题日益普遍，对个人和社会造成心理副作用、实际暴力和even death的影响。机器学习模型已经开发出来自动检测辱语，但这些模型可能会受到时间偏见的影响，时间偏见是指语言、语言使用或社会规范随着时间的变化。本研究旨在研究辱语检测中的时间偏见问题，以及不同语言下的时间偏见的影响。我们对不同时间段的辱语数据集进行了评估，结果显示，时间偏见是辱语检测中的一大挑战，历史数据上训练的模型表现下降显著。此外，我们还进行了对这些辱语数据集的广泛语言分析，尝试探讨语言演化的原因和表现下降的原因。这项研究突出了辱语检测中时间偏见的问题，为语言演化和时间偏见缓解提供了关键的洞察。
</details></li>
</ul>
<hr>
<h2 id="On-the-Relation-between-Internal-Language-Model-and-Sequence-Discriminative-Training-for-Neural-Transducers"><a href="#On-the-Relation-between-Internal-Language-Model-and-Sequence-Discriminative-Training-for-Neural-Transducers" class="headerlink" title="On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers"></a>On the Relation between Internal Language Model and Sequence Discriminative Training for Neural Transducers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14130">http://arxiv.org/abs/2309.14130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijian Yang, Wei Zhou, Ralf Schlüter, Hermann Ney</li>
<li>for: 提高 RNN-Transducer 的表现，使用外部语言模型 (LM) 融合</li>
<li>methods: 使用序列推理训练，并对 ILM 进行减法</li>
<li>results: 序列推理训练和 ILM 减法在 Librispeech 上的各种实验中具有类似的表现，包括 MMI 和 MBR 等标准。减法对 ILM 的影响也变得较小。<details>
<summary>Abstract</summary>
Internal language model (ILM) subtraction has been widely applied to improve the performance of the RNN-Transducer with external language model (LM) fusion for speech recognition. In this work, we show that sequence discriminative training has a strong correlation with ILM subtraction from both theoretical and empirical points of view. Theoretically, we derive that the global optimum of maximum mutual information (MMI) training shares a similar formula as ILM subtraction. Empirically, we show that ILM subtraction and sequence discriminative training achieve similar performance across a wide range of experiments on Librispeech, including both MMI and minimum Bayes risk (MBR) criteria, as well as neural transducers and LMs of both full and limited context. The benefit of ILM subtraction also becomes much smaller after sequence discriminative training. We also provide an in-depth study to show that sequence discriminative training has a minimal effect on the commonly used zero-encoder ILM estimation, but a joint effect on both encoder and prediction + joint network for posterior probability reshaping including both ILM and blank suppression.
</details>
<details>
<summary>摘要</summary>
内部语言模型（ILM）减法广泛应用于改进RNN-Transducer的语音识别性能，在这项工作中，我们表明了序列推理训练与ILM减法之间存在强相关性。从理论上来看，我们得出了最大共识度（MMI）训练的全球最优点与ILM减法的相似公式。从实验来看，我们证明了ILM减法和序列推理训练在Librispeech上覆盖广泛的实验中具有相似的性能，包括MMI和最小 bayes风险（MBR） критериria，以及神经转移和LM的全文和有限文本上的性能。ILM减法的利益也变得很小之后进行序列推理训练。我们还进行了深入的研究，发现序列推理训练对于通常使用零编码ILM估计的影响非常小，但对于encoder和预测+联合网络进行 posterior probability重塑，包括ILM和空白抑制，有 JOINT 效果。
</details></li>
</ul>
<hr>
<h2 id="Wav2vec-based-Detection-and-Severity-Level-Classification-of-Dysarthria-from-Speech"><a href="#Wav2vec-based-Detection-and-Severity-Level-Classification-of-Dysarthria-from-Speech" class="headerlink" title="Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech"></a>Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14107">http://arxiv.org/abs/2309.14107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farhad Javanmardi, Saska Tirronen, Manila Kodali, Sudarsana Reddy Kadiri, Paavo Alku</li>
<li>for: 这个研究旨在使用自动检测和评估瘫疡症患者的语音信号，以便在医疗诊断中使用。</li>
<li>methods: 这个研究使用了预训练的wav2vec 2.0模型作为特征提取器，建立检测和评估瘫疡症语音的系统。</li>
<li>results: 实验结果显示，使用wav2vec 2.0模型的对应层 embeddings（第一层）可以实现最佳的检测性能，相比基准模型（spectrogram）的最高表现提高1.23%的精度。在研究的评估瘫疡症严重程度分类任务中，使用最终层 embeddings 可以实现10.62%的精度提高，相比基准特征（mel-frequency cepstral coefficients）。<details>
<summary>Abstract</summary>
Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).
</details>
<details>
<summary>摘要</summary>
自动检测和评估瘫疡程度可以将单词识别和瘫疡程度分类 directly from acoustic speech signals 用作医疗诊断工具。在这个工作中，预训练的 wav2vec 2.0 模型被研究作为特征提取器，以建立检测和瘫疡程度分类系统。实验使用了常用的 UA-speech 数据库。在检测实验中，结果显示，使用 wav2vec 模型的第一层嵌入得到最佳表现，对比基准特征（spectrogram）的最佳表现，获得了绝对提升1.23%的准确度。在研究的瘫疡程度分类任务中，结果显示，使用 wav2vec 模型的最终层嵌入得到最佳表现，与基准特征（mel-frequency cepstral coefficients）的最佳表现相比，获得了绝对提升10.62%的准确度。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Detection-of-Pathological-Voice-using-Glottal-Source-Features"><a href="#Analysis-and-Detection-of-Pathological-Voice-using-Glottal-Source-Features" class="headerlink" title="Analysis and Detection of Pathological Voice using Glottal Source Features"></a>Analysis and Detection of Pathological Voice using Glottal Source Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14080">http://arxiv.org/abs/2309.14080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Paavo Alku</li>
<li>for: 该研究旨在对声音疾病进行自动检测，以提供 объектив评估和早期 intervención 的 диагности方法。</li>
<li>methods: 该研究使用 quasi-closed phase (QCP) glottal inverse filtering 方法 estimate glottal flows，并使用 zero frequency filtering (ZFF) 方法计算 approximate glottal source signals，以及直接使用声音信号。此外，研究还提出了 derivate mel-frequency cepstral coefficients (MFCCs) from glottal source waveforms computed by QCP和ZFF，以具体地捕捉 glottal source 谱的变化。</li>
<li>results: 研究结果表明，glottal source 特征含有可以区分正常和疾病声音的信息。通过支持向量机 (SVM) 进行检测试验，发现 studied glottal source 特征可以与 convential MFCCs 和 perceptual linear prediction (PLP) 特征相比，达到了同等或更好的检测性能。此外，combine glottal source 特征与 convential MFCCs 和 PLP 特征可以获得最佳的检测性能，这表明这些特征之间存在辅助性的关系。<details>
<summary>Abstract</summary>
Automatic detection of voice pathology enables objective assessment and earlier intervention for the diagnosis. This study provides a systematic analysis of glottal source features and investigates their effectiveness in voice pathology detection. Glottal source features are extracted using glottal flows estimated with the quasi-closed phase (QCP) glottal inverse filtering method, using approximate glottal source signals computed with the zero frequency filtering (ZFF) method, and using acoustic voice signals directly. In addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from the glottal source waveforms computed by QCP and ZFF to effectively capture the variations in glottal source spectra of pathological voice. Experiments were carried out using two databases, the Hospital Universitario Principe de Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database. Analysis of features revealed that the glottal source contains information that discriminates normal and pathological voice. Pathology detection experiments were carried out using support vector machine (SVM). From the detection experiments it was observed that the performance achieved with the studied glottal source features is comparable or better than that of conventional MFCCs and perceptual linear prediction (PLP) features. The best detection performance was achieved when the glottal source features were combined with the conventional MFCCs and PLP features, which indicates the complementary nature of the features.
</details>
<details>
<summary>摘要</summary>
自动检测声道疾病可以提供对象评估和早期 intervención для诊断。本研究提供了声道疾病检测中频谱源特征的系统性分析，并investigates其效iveness。频谱源特征通过预计closed phase（QCP）glottal inverse filtering方法、零频率 filtering（ZFF）方法和直接使用声音信号来提取。此外，我们提议 derivation of mel-frequency cepstral coefficients（MFCCs）from the glottal source waveforms computed by QCP和ZFF，以有效捕捉声道源спектrum的变化。实验使用了两个数据库，大学医院主楼（HUPA）数据库和 saarbrucken voice disorders（SVD）数据库。特征分析表明，频谱源含有可以区分正常和疾病声音的信息。疾病检测实验使用支持向量机（SVM）。从检测实验中，我们发现，研究中的频谱源特征表现比或更好于 conventioml MFCCs和perceptual linear prediction（PLP）特征。最佳检测性能是在glottal source特征与conventioml MFCCs和PLP特征结合时得到的，这表明这些特征之间存在衔接关系。
</details></li>
</ul>
<hr>
<h2 id="Multiple-evolutionary-pressures-shape-identical-consonant-avoidance-in-the-world’s-languages"><a href="#Multiple-evolutionary-pressures-shape-identical-consonant-avoidance-in-the-world’s-languages" class="headerlink" title="Multiple evolutionary pressures shape identical consonant avoidance in the world’s languages"></a>Multiple evolutionary pressures shape identical consonant avoidance in the world’s languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14006">http://arxiv.org/abs/2309.14006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chundra A. Cathcart</li>
<li>for: 本研究探讨语言演化中同音字符的出现频率是否受限制，以及这些限制的来源。</li>
<li>methods: 研究者使用phylogenetic分析方法，对同义词的演化进行比较分析，探讨语言演化过程中同音字符的出现频率和word form mutation的影响。</li>
<li>results: 研究发现，同音字符在词形变化中更容易消失，而非出现。此外，同音字符的出现频率较低，word form mutation也更可能将同音字符消除。但同时发现，同音字符不会更容易消失。结论是，同音字符的出现频率受到语言演化的限制，但这些限制不来自于语言使用者的选择。<details>
<summary>Abstract</summary>
Languages disfavor word forms containing sequences of similar or identical consonants, due to the biomechanical and cognitive difficulties posed by patterns of this sort. However, the specific evolutionary processes responsible for this phenomenon are not fully understood. Words containing sequences of identical consonants may be more likely to arise than those without; processes of word form mutation may be more likely to remove than create sequences of identical consonants in word forms; finally, words containing identical consonants may die out more frequently than those without. Phylogenetic analyses of the evolution of homologous word forms indicate that words with identical consonants arise less frequently than those without, and processes which mutate word forms are more likely to remove sequences of identical consonants than introduce them. However, words with identical consonants do not die out more frequently than those without. Further analyses reveal that forms with identical consonants are replaced in basic meaning functions more frequently than words without. Taken together, results suggest that the under representation of sequences of identical consonants is overwhelmingly a byproduct of constraints on word form coinage, though processes related to word usage also serve to ensure that such patterns are infrequent in more salient vocabulary items. These findings clarify previously unknown aspects of processes of lexical evolution and competition that take place during language change, optimizing communicative systems.
</details>
<details>
<summary>摘要</summary>
语言偏远同辅音序列，因为这些模式带来生物机械和认知上的困难。然而，这种现象的具体演化过程仍未完全了解。words containing sequences of identical consonants may be more likely to arise than those without; processes of word form mutation may be more likely to remove than create sequences of identical consonants in word forms; finally, words containing identical consonants may die out more frequently than those without.phylogenetic analyses of the evolution of homologous word forms indicate that words with identical consonants arise less frequently than those without, and processes which mutate word forms are more likely to remove sequences of identical consonants than introduce them. however, words with identical consonants do not die out more frequently than those without. further analyses reveal that forms with identical consonants are replaced in basic meaning functions more frequently than words without. taken together, results suggest that the under representation of sequences of identical consonants is overwhelmingly a byproduct of constraints on word form coinage, though processes related to word usage also serve to ensure that such patterns are infrequent in more salient vocabulary items. these findings clarify previously unknown aspects of processes of lexical evolution and competition that take place during language change, optimizing communicative systems.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR"><a href="#Connecting-Speech-Encoder-and-Large-Language-Model-for-ASR" class="headerlink" title="Connecting Speech Encoder and Large Language Model for ASR"></a>Connecting Speech Encoder and Large Language Model for ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13963">http://arxiv.org/abs/2309.13963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang</li>
<li>for: 本研究旨在比较三种常用的结构，包括完全连接层、多头交叉注意力和Q-Former，以实现自动语音识别（ASR）模型的集成。</li>
<li>methods: 研究使用了Whisper模型系列的语音编码器和Vicuna模型系列的不同模型大小的LLMs，并在LibriSpeech、Common Voice和GigaSpeech datasets上进行了实验。</li>
<li>results: Q-Former-based LLMs在不同数据集上显示了一致和显著的单词错误率（WER）减少，相比其他结构。此外，一种新的段级Q-Former也被提出，使LLMs可以识别长于编码器限制的语音段，带来17%的相对WER减少。<details>
<summary>Abstract</summary>
The impressive capability and versatility of large language models (LLMs) have aroused increasing attention in automatic speech recognition (ASR), with several pioneering studies attempting to build integrated ASR models by connecting a speech encoder with an LLM. This paper presents a comparative study of three commonly used structures as connectors, including fully connected layers, multi-head cross-attention, and Q-Former. Speech encoders from the Whisper model series as well as LLMs from the Vicuna model series with different model sizes were studied. Experiments were performed on the commonly used LibriSpeech, Common Voice, and GigaSpeech datasets, where the LLMs with Q-Formers demonstrated consistent and considerable word error rate (WER) reductions over LLMs with other connector structures. Q-Former-based LLMs can generalise well to out-of-domain datasets, where 12% relative WER reductions over the Whisper baseline ASR model were achieved on the Eval2000 test set without using any in-domain training data from Switchboard. Moreover, a novel segment-level Q-Former is proposed to enable LLMs to recognise speech segments with a duration exceeding the limitation of the encoders, which results in 17% relative WER reductions over other connector structures on 90-second-long speech data.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的印象力和多方面性在自动话语识别（ASR）中受到了越来越多的关注，有几个先锋性研究尝试建立了 integrate ASR 模型，通过与话语编码器连接。这篇文章发表了三种常用的结构，包括完全连接层、多头标注和Q-Former 的比较研究。研究使用了 Whisper 模型系列的话语编码器和 Vicuna 模型系列的不同模型大小的 LLM，并在 LibriSpeech、Common Voice 和 GigaSpeech  datasets 上进行实验。实验结果显示，使用 Q-Former 的 LLM 可以在不使用域内训练数据的情况下，实现了12% 的相对 palabier error rate（WER）降低，相比其他结构。此外，一个新的段级 Q-Former 被提议，允许 LLM 识别长度超过编码器限制的语音段，从而实现了17% 的相对 WER 降低。
</details></li>
</ul>
<hr>
<h2 id="Reproducing-Whisper-Style-Training-Using-an-Open-Source-Toolkit-and-Publicly-Available-Data"><a href="#Reproducing-Whisper-Style-Training-Using-an-Open-Source-Toolkit-and-Publicly-Available-Data" class="headerlink" title="Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data"></a>Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13876">http://arxiv.org/abs/2309.13876</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/espnet/espnet">https://github.com/espnet/espnet</a></li>
<li>paper_authors: Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe</li>
<li>for: 该论文的目的是开发一个开源的听说模型，以便于研究人员可以在开源的工具kit和公共可用的数据上进行训练和改进。</li>
<li>methods: 该论文使用的方法是基于开源的工具kit和公共可用的数据进行听说模型的训练，并支持更多的翻译方向。</li>
<li>results: 该论文可以在零shot设置下实现良好的一致性和翻译性，并且可以在训练过程中提高效率和稳定性。<details>
<summary>Abstract</summary>
Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisper-style training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pre-trained models and training logs to promote open science.
</details>
<details>
<summary>摘要</summary>
<SYS>Translate the given text into Simplified Chinese.</SYS>预训术语模型在大量数据上已经取得了很大成功。OpenAI Whisper是一个多语言多任务模型，在680k小时的监督术语数据上训练。它在不同的术语识别和翻译 bencmarks 中进行了良好的泛化，甚至在零shot setup 下也能达到良好的性能。然而，整个模型开发管道（从数据收集到训练）没有公开 accessible，这使得研究人员很难进一步改进其性能和 Address training-related issues such as efficiency, robustness, fairness, and bias。这项工作提出了一个 Open Whisper-style Speech Model (OWSM)，该模型通过使用开源工具包和公开可用的数据来重现 Whisper-style 训练。OWSM 还支持更多的翻译方向，并且可以更高效地训练。我们将公开所有数据准备、训练、推理和评分脚本以及预训练模型和训练日志，以便推动开放科学。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/cs.CL_2023_09_25/" data-id="clp88dbtq00cgob88dxldgyf5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/cs.LG_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T10:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/cs.LG_2023_09_25/">cs.LG - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Understanding-the-Structure-of-QM7b-and-QM9-Quantum-Mechanical-Datasets-Using-Unsupervised-Learning"><a href="#Understanding-the-Structure-of-QM7b-and-QM9-Quantum-Mechanical-Datasets-Using-Unsupervised-Learning" class="headerlink" title="Understanding the Structure of QM7b and QM9 Quantum Mechanical Datasets Using Unsupervised Learning"></a>Understanding the Structure of QM7b and QM9 Quantum Mechanical Datasets Using Unsupervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15130">http://arxiv.org/abs/2309.15130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julio J. Valdés, Alain B. Tchagang</li>
<li>for: 本研究探讨了量子机制数据集（QM7b、QM9）的内部结构，它们包含了数量级的有机分子，并通过电子性质来描述。了解这类数据的结构和特点对于预测分子组成是非常重要。</li>
<li>methods: 本研究使用了内在维度分析、聚类和异常检测方法来研究QM7b和QM9数据集。结果显示，QM7b数据集具有清晰定义的集群，与原子组成直接相关。QM9数据集则有一个外围区域主要由异常点组成，以及一个内部核心区域集中的线性对象。与分子大小有直接关系的关系存在于这两个数据集中。</li>
<li>results:  despite the structural differences between the two datasets, the predictability of variables of interest for inverse molecular design is high. This is exemplified with models estimating the number of atoms of the molecule from both the original properties and from lower dimensional embedding spaces.<details>
<summary>Abstract</summary>
This paper explores the internal structure of two quantum mechanics datasets (QM7b, QM9), composed of several thousands of organic molecules and described in terms of electronic properties. Understanding the structure and characteristics of this kind of data is important when predicting the atomic composition from the properties in inverse molecular designs. Intrinsic dimension analysis, clustering, and outlier detection methods were used in the study. They revealed that for both datasets the intrinsic dimensionality is several times smaller than the descriptive dimensions. The QM7b data is composed of well defined clusters related to atomic composition. The QM9 data consists of an outer region predominantly composed of outliers, and an inner core region that concentrates clustered, inliner objects. A significant relationship exists between the number of atoms in the molecule and its outlier/inner nature. Despite the structural differences, the predictability of variables of interest for inverse molecular design is high. This is exemplified with models estimating the number of atoms of the molecule from both the original properties, and from lower dimensional embedding spaces.
</details>
<details>
<summary>摘要</summary>
The QM7b dataset is composed of well-defined clusters related to atomic composition, while the QM9 dataset has an outer region primarily consisting of outliers and an inner core region with clustered, linear objects. There is a significant correlation between the number of atoms in the molecule and its outlier/inner nature. Despite the structural differences, the predictability of variables of interest for inverse molecular design is high, as demonstrated by models estimating the number of atoms of the molecule from both the original properties and lower-dimensional embedding spaces.
</details></li>
</ul>
<hr>
<h2 id="Towards-a-statistical-theory-of-data-selection-under-weak-supervision"><a href="#Towards-a-statistical-theory-of-data-selection-under-weak-supervision" class="headerlink" title="Towards a statistical theory of data selection under weak supervision"></a>Towards a statistical theory of data selection under weak supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14563">http://arxiv.org/abs/2309.14563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Germain Kolossov, Andrea Montanari, Pulkit Tandon</li>
<li>for: 选择一个小于原始样本大小的子样本，以优化数据预处理和机器学习计算复杂性。</li>
<li>methods: 使用代理模型预测样本标签，然后选择一个子样本，并使用这些标签进行模型训练。</li>
<li>results: 数据选择可以非常有效，在某些情况下甚至可以超过使用整个样本集来训练模型。另外，一些受欢迎的数据选择方法（如偏向重样本或影响函数基于的子样本选择）可能会很差。<details>
<summary>Abstract</summary>
Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{\boldsymbol x}_i\}_{i\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\{\boldsymbol x}_i\}_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.   By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selection methods (e.g. unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.
</details>
<details>
<summary>摘要</summary>
Given a sample size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step can reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{\mathbf{x}_i\}_{i\le N}$, and to be given access to a 'surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, denoted by $\{\mathbf{x}_i\}_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and use them to train a model via regularized empirical risk minimization.By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high-dimensional asymptotics, we show that:$(i)$ Data selection can be very effective, in particular beating training on the full sample in some cases;$(ii)$ Certain popular choices in data selection methods (e.g., unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.
</details></li>
</ul>
<hr>
<h2 id="Disruption-Detection-for-a-Cognitive-Digital-Supply-Chain-Twin-Using-Hybrid-Deep-Learning"><a href="#Disruption-Detection-for-a-Cognitive-Digital-Supply-Chain-Twin-Using-Hybrid-Deep-Learning" class="headerlink" title="Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning"></a>Disruption Detection for a Cognitive Digital Supply Chain Twin Using Hybrid Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14557">http://arxiv.org/abs/2309.14557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Ashraf, Amr Eltawil, Islam Ali</li>
<li>For: The paper aims to provide an effective and efficient tool for mitigating the impact of disruptive events on global supply chains by introducing a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework.* Methods: The proposed approach uses a deep autoencoder neural network combined with a one-class support vector machine algorithm to detect disruptions in real-time. Long-short term memory neural network models are also developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect.* Results: The proposed approach can help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing negative impact of disruptive events based on real-time disruption detection data. The results demonstrate the trade-off between disruption detection model sensitivity, encountered delay in disruption detection, and false alarms.Here are the three key points in Simplified Chinese text:</li>
<li>for: 本文目的是提供一种有效和高效的工具，以减轻突发事件对全球供应链的影响。</li>
<li>methods: 该方法使用深度自适应神经网络与一类支持向量机算法检测干扰。此外，使用长短期记忆神经网络模型识别受影响的echelon，预测干扰影响的时间恢复。</li>
<li>results: 该方法可以帮助决策者和供应链实践者根据实时干扰检测数据进行合适的决策，以减轻突发事件的负面影响。结果表明干扰检测模型的敏感度、遇到延迟的干扰检测和假阳性的负面关系。这种方法在当前文献中很少被使用。<details>
<summary>Abstract</summary>
Purpose: Recent disruptive events, such as COVID-19 and Russia-Ukraine conflict, had a significant impact of global supply chains. Digital supply chain twins have been proposed in order to provide decision makers with an effective and efficient tool to mitigate disruption impact. Methods: This paper introduces a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework to enhance supply chain resilience. The proposed disruption detection module utilises a deep autoencoder neural network combined with a one-class support vector machine algorithm. In addition, long-short term memory neural network models are developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect. Results: The obtained information from the proposed approach will help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing negative impact of disruptive events based on real-time disruption detection data. The results demonstrate the trade-off between disruption detection model sensitivity, encountered delay in disruption detection, and false alarms. This approach has seldom been used in recent literature addressing this issue.
</details>
<details>
<summary>摘要</summary>
目的：latest disruptive events, such as COVID-19 and Russia-Ukraine conflict, have had a significant impact on global supply chains. Digital supply chain twins have been proposed to provide decision makers with an effective and efficient tool to mitigate the impact of disruptions.方法：this paper introduces a hybrid deep learning approach for disruption detection within a cognitive digital supply chain twin framework to enhance supply chain resilience. The proposed disruption detection module uses a deep autoencoder neural network combined with a one-class support vector machine algorithm. In addition, long-short term memory neural network models are developed to identify the disrupted echelon and predict time-to-recovery from the disruption effect.结果：the obtained information from the proposed approach will help decision-makers and supply chain practitioners make appropriate decisions aiming at minimizing the negative impact of disruptive events based on real-time disruption detection data. The results demonstrate the trade-off between disruption detection model sensitivity, encountered delay in disruption detection, and false alarms. This approach has seldom been used in recent literature addressing this issue.Here's the translation in Traditional Chinese:目的：最近的干扰事件，如COVID-19和俄乌战争，对全球供应链造成了巨大的影响。数字供应链双生物被提议，以提供决策者具有更高效和更高效的工具，以mitigate干扰的影响。方法：本篇文章介绍了一种混合深度学习方法，用于干扰检测在认知数字供应链双生物框架中，以提高供应链可靠性。提议的干扰检测模组使用深度自动Encoder神经网络，与一类支持向量机器学习算法结合。此外，长期快速传统神经网络模型也被开发，以识别受到干扰的层次，并预测干扰影响的时间回复。结果：取得的信息将助决策者和供应链实践者做出适当的决策，以减少干扰事件的负面影响。结果显示出干扰检测模型的敏感度、遭遇延误的干扰检测时间和误干扰的负面影响之间的贸易。这种方法在最近的文献中 rarely 被使用以解决这个问题。
</details></li>
</ul>
<hr>
<h2 id="Cluster-based-Method-for-Eavesdropping-Identification-and-Localization-in-Optical-Links"><a href="#Cluster-based-Method-for-Eavesdropping-Identification-and-Localization-in-Optical-Links" class="headerlink" title="Cluster-based Method for Eavesdropping Identification and Localization in Optical Links"></a>Cluster-based Method for Eavesdropping Identification and Localization in Optical Links</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14541">http://arxiv.org/abs/2309.14541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Song, Rui Lin, Andrea Sgambelluri, Filippo Cugini, Yajie Li, Jie Zhang, Paolo Monti</li>
<li>for: 检测和定位光线系统中的窃听事件</li>
<li>methods: 基于集群方法检测和定位窃听事件</li>
<li>results: 通过收集发送器端的光性能监测数据可以检测小功率损失引起的窃听事件，而通过利用线上监测数据可以有效地定位窃听事件。<details>
<summary>Abstract</summary>
We propose a cluster-based method to detect and locate eavesdropping events in optical line systems characterized by small power losses. Our findings indicate that detecting such subtle losses from eavesdropping can be accomplished solely through optical performance monitoring (OPM) data collected at the receiver. On the other hand, the localization of such events can be effectively achieved by leveraging in-line OPM data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于集群的方法，用于检测和定位光纤系统中的窃听事件。我们的发现表明，通过收集接收端的光性能监测（OPM）数据，可以寻查到这些微量损失。然而，通过利用线上OPM数据，可以有效地确定这些事件的位置。
</details></li>
</ul>
<hr>
<h2 id="Detach-ROCKET-Sequential-feature-selection-for-time-series-classification-with-random-convolutional-kernels"><a href="#Detach-ROCKET-Sequential-feature-selection-for-time-series-classification-with-random-convolutional-kernels" class="headerlink" title="Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels"></a>Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14518">http://arxiv.org/abs/2309.14518</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gon-uri/detach_rocket">https://github.com/gon-uri/detach_rocket</a></li>
<li>paper_authors: Gonzalo Uribarri, Federico Barone, Alessio Ansuini, Erik Fransén</li>
<li>for: 这篇论文的目的是提出一种Sequential Feature Detachment（SFD）方法，用于时间序列分类（TSC）中删除无用的特征，提高模型的执行效率和泛化能力。</li>
<li>methods: 这篇论文使用了ROCKET模型和其变体，以及模型的权重来Estimate feature importance，并通过Sequential Feature Detachment（SFD）方法删除无用的特征。</li>
<li>results: 根据UCR archive的试验结果，SFD方法可以将时间序列分类模型的特征数量从原本的1000多个降至10%左右，同时提高模型的测试精度0.2%。此外， Detach-ROCKET方法可以对最大的 binary UCR 数据集进行最佳化，从而提高测试精度0.6%，同时删除98.9%的特征。<details>
<summary>Abstract</summary>
Time Series Classification (TSC) is essential in many fields, such as medicine, environmental science and finance, enabling tasks like disease diagnosis, anomaly detection, and stock price analysis. Machine learning models for TSC like Recurrent Neural Networks and InceptionTime, while successful in numerous applications, can face scalability limitations due to intensive computational requirements. To address this, efficient models such as ROCKET and its derivatives have emerged, simplifying training and achieving state-of-the-art performance by utilizing a large number of randomly generated features from time series data. However, due to their random nature, most of the generated features are redundant or non-informative, adding unnecessary computational load and compromising generalization. Here, we introduce Sequential Feature Detachment (SFD) as a method to identify and prune these non-essential features. SFD uses model coefficients to estimate feature importance and, unlike previous algorithms, can handle large feature sets without the need for complex hyperparameter tuning. Testing on the UCR archive demonstrates that SFD can produce models with $10\%$ of the original features while improving the accuracy $0.2\%$ on the test set. We also present an end-to-end procedure for determining an optimal balance between the number of features and model accuracy, called Detach-ROCKET. When applied to the largest binary UCR dataset, Detach-ROCKET is able to improve test accuracy by $0.6\%$ while reducing the number of features by $98.9\%$. Thus, our proposed procedure is not only lightweight to train and effective in reducing model size and enhancing generalization, but its significant reduction in feature count also paves the way for feature interpretation.
</details>
<details>
<summary>摘要</summary>
时序分类（TSC）在医学、环境科学和金融等领域具有重要意义，可以实现疾病诊断、异常检测和股票价格分析等任务。机器学习模型 для TSC，如循环神经网络和InceptionTime，虽然在多个应用中取得成功，但可能会面临扩展性限制，因为计算需求很高。为解决这问题，有效的模型如ROCKET和其 derivates出现了，使得训练更加简单，并在多个时序数据上实现了状态机器学习性能。然而，由于这些生成的特征 Random，大多数生成的特征都是 redundant或非指导的，这会增加计算负担并降低泛化性。在这种情况下，我们介绍了时序特征分离（SFD）方法，可以识别和剔除非关键的特征。SFD使用模型系数来估计特征重要性，与之前的算法不同之处在于可以处理大量特征集 ohne需要复杂的超参数调整。在UCRL архиivos上进行测试，SFD可以生成模型，其中90%的特征是非关键的，而测试集上的准确率提高0.2%。我们还提出了一种从头到尾的过程，可以确定最佳的特征数量和模型准确率之间的平衡，称为Detach-ROCKET。当应用于最大的二进制UCRL数据集时，Detach-ROCKET可以提高测试准确率0.6%，同时减少特征数量98.9%。因此，我们的提出的过程不仅轻量级训练，效果减小模型大小和提高泛化性，而且它的重要减少特征计数也为特征解释开辟了道路。
</details></li>
</ul>
<hr>
<h2 id="Zeroth-order-Riemannian-Averaging-Stochastic-Approximation-Algorithms"><a href="#Zeroth-order-Riemannian-Averaging-Stochastic-Approximation-Algorithms" class="headerlink" title="Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms"></a>Zeroth-order Riemannian Averaging Stochastic Approximation Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14506">http://arxiv.org/abs/2309.14506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxiang Li, Krishnakumar Balasubramanian, Shiqian Ma</li>
<li>for: 这个论文是为了研究在里曼尼抽象上的泛化随机搜索问题。</li>
<li>methods: 这个论文使用了 Zero-order Riemannian Averaging Stochastic Approximation（\texttt{Zo-RASA）} 算法，并使用了 RiemaNN 移动平均梯度估计器和一种新的 RiemaNN-Lyapunov 分析技术来进行转化分析。</li>
<li>results: 这个论文表明了 \texttt{Zo-RASA} 算法可以使用单个样本或常数级批处理在每个迭代中实现 $\epsilon$-近似首Order站点解。此外，论文还引入了一种新的几何条件，即 bounded second fundamental form，可以用于精度地 approximate parallel transport。<details>
<summary>Abstract</summary>
We present Zeroth-order Riemannian Averaging Stochastic Approximation (\texttt{Zo-RASA}) algorithms for stochastic optimization on Riemannian manifolds. We show that \texttt{Zo-RASA} achieves optimal sample complexities for generating $\epsilon$-approximation first-order stationary solutions using only one-sample or constant-order batches in each iteration. Our approach employs Riemannian moving-average stochastic gradient estimators, and a novel Riemannian-Lyapunov analysis technique for convergence analysis. We improve the algorithm's practicality by using retractions and vector transport, instead of exponential mappings and parallel transports, thereby reducing per-iteration complexity. Additionally, we introduce a novel geometric condition, satisfied by manifolds with bounded second fundamental form, which enables new error bounds for approximating parallel transport with vector transport.
</details>
<details>
<summary>摘要</summary>
我们提出了零预orde Riemannian Averaging Stochastic Approximation（\texttt{Zo-RASA）}算法，用于在里曼尼抽象上进行数学估计。我们证明了\texttt{Zo-RASA}可以在每个迭代中使用单一样本或常量组数据，实现 $\epsilon$-近似首先稳定解的生成。我们的方法使用里曼尼运动平均梯度估计器，并使用一种新的里曼尼- Lyapunov 分析技术进行对准性分析。我们通过使用抽像和向量运输，而不是对称对映和平行运输，将每次迭代的复杂性降低。此外，我们也提出了一个新的几何条件，这个条件是在具有受限第二funamental form的抽象上存在的，它允许我们给出新的错误上限，用于近似平行运输。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Aware-Deep-Learning-for-Particle-Accelerators"><a href="#Uncertainty-Aware-Deep-Learning-for-Particle-Accelerators" class="headerlink" title="Uncertainty Aware Deep Learning for Particle Accelerators"></a>Uncertainty Aware Deep Learning for Particle Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14502">http://arxiv.org/abs/2309.14502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kishansingh Rajput, Malachi Schram, Karthik Somayaji</li>
<li>for: 这篇论文是为了提出一种用Deep Gaussian Process Approximation（DGPA）方法进行误束预测和不确定性评估的方法。</li>
<li>methods: 这篇论文使用了Deep Gaussian Process Approximation（DGPA）方法，该方法可以捕捉复杂系统动态，但是需要考虑误束和不确定性。</li>
<li>results: 这篇论文在SNS加速器中进行了误束预测和不确定性评估，并提供了一个不确定性意识的模型。<details>
<summary>Abstract</summary>
Standard deep learning models for classification and regression applications are ideal for capturing complex system dynamics. However, their predictions can be arbitrarily inaccurate when the input samples are not similar to the training data. Implementation of distance aware uncertainty estimation can be used to detect these scenarios and provide a level of confidence associated with their predictions. In this paper, we present results from using Deep Gaussian Process Approximation (DGPA) methods for errant beam prediction at Spallation Neutron Source (SNS) accelerator (classification) and we provide an uncertainty aware surrogate model for the Fermi National Accelerator Lab (FNAL) Booster Accelerator Complex (regression).
</details>
<details>
<summary>摘要</summary>
标准的深度学习模型可以很好地捕捉复杂系统的动态。但是，它们的预测结果可能无法实际地准确，尤其是对于与训练数据不同的输入数据。在这篇文章中，我们使用深度 Gaussian Process Approximation（DGPA）方法进行误偏照射预测（分类），并提供了不对称 uncertainty 意识模型（重回应）。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Potential-of-Deep-Learning-Models-for-Solar-Flare-Prediction-in-Near-Limb-Regions"><a href="#Unveiling-the-Potential-of-Deep-Learning-Models-for-Solar-Flare-Prediction-in-Near-Limb-Regions" class="headerlink" title="Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions"></a>Unveiling the Potential of Deep Learning Models for Solar Flare Prediction in Near-Limb Regions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14483">http://arxiv.org/abs/2309.14483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chetraj Pandey, Rafal A. Angryk, Berkay Aydin</li>
<li>for: 本研究旨在评估深度学习模型在预测solar flare的性能，使用每小时采样的全盘线条图像，特别是关注近似limb区域（beyond ±70°的太阳盘面）的许多次过looked flare事件。</li>
<li>methods: 我们使用了三种well-known deep learning architecture——AlexNet、VGG16和ResNet34进行了转移学习，并对三个模型进行了比较和评估，使用了真实技能统计（TSS）和Heidke技能分数（HSS），以及计算了回快分数，以理解预测敏感性在中心和近似limb区域中。</li>
<li>results: 我们的研究发现，使用AlexNet基于的模型在全体性能方面表现最高，其TSS和HSS分别为0.53和0.37；而进一步的空间分析回快分数显示，在近似limb事件中，VGG16和ResNet34基于的模型具有更高的预测敏感性。最佳结果是使用ResNet34基于的模型，其near-limb预测回快率为0.59（X-和M-class预测回快率分别为0.81和0.56）。<details>
<summary>Abstract</summary>
This study aims to evaluate the performance of deep learning models in predicting $\geq$M-class solar flares with a prediction window of 24 hours, using hourly sampled full-disk line-of-sight (LoS) magnetogram images, particularly focusing on the often overlooked flare events corresponding to the near-limb regions (beyond $\pm$70$^{\circ}$ of the solar disk). We trained three well-known deep learning architectures--AlexNet, VGG16, and ResNet34 using transfer learning and compared and evaluated the overall performance of our models using true skill statistics (TSS) and Heidke skill score (HSS) and computed recall scores to understand the prediction sensitivity in central and near-limb regions for both X- and M-class flares. The following points summarize the key findings of our study: (1) The highest overall performance was observed with the AlexNet-based model, which achieved an average TSS$\sim$0.53 and HSS$\sim$0.37; (2) Further, a spatial analysis of recall scores disclosed that for the near-limb events, the VGG16- and ResNet34-based models exhibited superior prediction sensitivity. The best results, however, were seen with the ResNet34-based model for the near-limb flares, where the average recall was approximately 0.59 (the recall for X- and M-class was 0.81 and 0.56 respectively) and (3) Our research findings demonstrate that our models are capable of discerning complex spatial patterns from full-disk magnetograms and exhibit skill in predicting solar flares, even in the vicinity of near-limb regions. This ability holds substantial importance for operational flare forecasting systems.
</details>
<details>
<summary>摘要</summary>
这项研究的目标是评估深度学习模型在预测24小时内的$\geq$M级太阳风暴事件的性能，使用每小时采样的全盘线性图像，特别是关注太阳盘面外(-70度以上)的快速风暴事件。我们使用了三种已知的深度学习架构——AlexNet、VGG16和ResNet34进行转移学习，并对三个模型进行比较和评估，使用真实技能统计（TSS）和海德ке技能分数（HSS），并计算了中心和近缘区域的回快率以了解预测敏感度。研究的主要发现包括：1. AlexNet基于的模型在整体性能方面表现最高，其TSS和HSS分别为0.53和0.37；2. 空间分析回快率表明，近缘区域内的风暴事件预测敏感度最高，VGG16和ResNet34基于的模型在近缘区域内表现出色，特别是ResNet34基于的模型，其平均回快率为0.59，X级和M级风暴事件的回快率分别为0.81和0.56；3. 这些研究发现表明，我们的模型可以从全盘线性图像中提取复杂的空间特征，并在靠近近缘区域的风暴事件预测中展现出能力。这种能力对于实际风暴预测系统具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="LogGPT-Log-Anomaly-Detection-via-GPT"><a href="#LogGPT-Log-Anomaly-Detection-via-GPT" class="headerlink" title="LogGPT: Log Anomaly Detection via GPT"></a>LogGPT: Log Anomaly Detection via GPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14482">http://arxiv.org/abs/2309.14482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiao Han, Shuhan Yuan, Mohamed Trabelsi</li>
<li>for: 这篇研究旨在提出一个基于日志数据的系统异常检测方法，以确保计算机系统的安全性和可靠性。</li>
<li>methods: 本研究使用深度学习模型进行日志异常检测，具体来说是将日志序列变数为自然语言，然后运用深度序列模型，例如LSTM或Transformer，对日志序列中的正常模式进行语言模型化。</li>
<li>results: 实验结果显示，LogGPT在三个 datasets 上表现出色，较 existing state-of-the-art 方法有更高的检测精度。<details>
<summary>Abstract</summary>
Detecting system anomalies based on log data is important for ensuring the security and reliability of computer systems. Recently, deep learning models have been widely used for log anomaly detection. The core idea is to model the log sequences as natural language and adopt deep sequential models, such as LSTM or Transformer, to encode the normal patterns in log sequences via language modeling. However, there is a gap between language modeling and anomaly detection as the objective of training a sequential model via a language modeling loss is not directly related to anomaly detection. To fill up the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly detection. LogGPT is first trained to predict the next log entry based on the preceding sequence. To further enhance the performance of LogGPT, we propose a novel reinforcement learning strategy to finetune the model specifically for the log anomaly detection task. The experimental results on three datasets show that LogGPT significantly outperforms existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
检测计算机系统中的异常 based on 日志数据是保持安全和可靠性的重要任务。最近，深度学习模型在日志异常检测中得到了广泛的应用。核心思想是模型日志序列为自然语言，采用深度序列模型，如LSTM或Transformer，来编码正常的日志序列模式 via 语言模型化。但是，语言模型化和异常检测之间存在一个差距，因为训练深度序列模型via语言模型化损失并不直接相关于异常检测。为填充这个差距，我们提出了 LogGPT，一种新的框架，它采用 GPT 进行日志异常检测。LogGPT 首先通过预测下一个日志条目基于前一个序列来训练。为了进一步提高 LogGPT 的性能，我们提出了一种新的强化学习策略，用于特定地 finetune 模型为日志异常检测任务。实验结果表明，LogGPT 与现有状态的方法相比，在三个数据集上显著地提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Skilog-A-Smart-Sensor-System-for-Performance-Analysis-and-Biofeedback-in-Ski-Jumping"><a href="#Skilog-A-Smart-Sensor-System-for-Performance-Analysis-and-Biofeedback-in-Ski-Jumping" class="headerlink" title="Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping"></a>Skilog: A Smart Sensor System for Performance Analysis and Biofeedback in Ski Jumping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14455">http://arxiv.org/abs/2309.14455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Schulthess, Thorir Mar Ingolfsson, Marc Nölke, Michele Magno, Luca Benini, Christoph Leitner</li>
<li>for: 这份研究是为了开发一个智能、 компакт、能效的无线感应器系统，用于现场进行 Ski jumping 的实时性能分析和生成反馈。</li>
<li>methods: 本研究使用了100Hz测量脚压的方法，并使用Machine Learning（ML）模型来实现实时的反馈。</li>
<li>results: 研究获得了92.7%的中心质量预测精度（脊梁偏移、中立位和腹股偏移），并在低功耗的RISC-V架构上进行了实时推导和反馈（0.0109ms&#x2F;推导）。<details>
<summary>Abstract</summary>
In ski jumping, low repetition rates of jumps limit the effectiveness of training. Thus, increasing learning rate within every single jump is key to success. A critical element of athlete training is motor learning, which has been shown to be accelerated by feedback methods. In particular, a fine-grained control of the center of gravity in the in-run is essential. This is because the actual takeoff occurs within a blink of an eye ($\sim$300ms), thus any unbalanced body posture during the in-run will affect flight. This paper presents a smart, compact, and energy-efficient wireless sensor system for real-time performance analysis and biofeedback during ski jumping. The system operates by gauging foot pressures at three distinct points on the insoles of the ski boot at 100Hz. Foot pressure data can either be directly sent to coaches to improve their feedback, or fed into a ML model to give athletes instantaneous in-action feedback using a vibration motor in the ski boot. In the biofeedback scenario, foot pressures act as input variables for an optimized XGBoost model. We achieve a high predictive accuracy of 92.7% for center of mass predictions (dorsal shift, neutral stand, ventral shift). Subsequently, we parallelized and fine-tuned our XGBoost model for a RISC-V based low power parallel processor (GAP9), based on the PULP architecture. We demonstrate real-time detection and feedback (0.0109ms/inference) using our on-chip deployment. The proposed smart system is unobtrusive with a slim form factor (13mm baseboard, 3.2mm antenna) and a lightweight build (26g). Power consumption analysis reveals that the system's energy-efficient design enables sustained operation over multiple days (up to 300 hours) without requiring recharge.
</details>
<details>
<summary>摘要</summary>
在跳台滑雪中，低重复率的跳跃限制了训练的效iveness。因此，在每次跳跃中提高学习率是关键到success。运动员训练中的核心元素是 дви作学习，已经证明可以通过反馈方法加速。特别是在具有细致控制中心重力的跑道上是关键。因为实际的起飞只需要几十毫秒（约300ms），所以任何不平衡的身体姿势会影响飞行。本文介绍了一种智能、卷积、能效的无线传感器系统，用于实时性表现分析和生物反馈 durante跳台滑雪。该系统通过在跳板底部的三个点检测脚压力，每秒100次获取数据。脚压力数据可以直接给教练提供反馈，或者通过一个机器学习模型给运动员实时反馈，使用跳板内置的振荡机。在生物反馈场景中，脚压力作为输入变量，用于优化的XGBoost模型。我们实现了中心质量预测的高预测精度（92.7%）。然后，我们将XGBoost模型并行化和优化，基于RISC-V架构的低功耗并行处理器（GAP9）。我们实现了实时探测和反馈（0.0109ms/推导），并在芯片上部署。提案的智能系统轻便，减少了跳板的尺寸（13mm基板、3.2mm天线）和重量（26g）。能源消耗分析表明，该系统的能效设计可以持续运行多天（最多300小时）无需充电。
</details></li>
</ul>
<hr>
<h2 id="Learning-dislocation-dynamics-mobility-laws-from-large-scale-MD-simulations"><a href="#Learning-dislocation-dynamics-mobility-laws-from-large-scale-MD-simulations" class="headerlink" title="Learning dislocation dynamics mobility laws from large-scale MD simulations"></a>Learning dislocation dynamics mobility laws from large-scale MD simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14450">http://arxiv.org/abs/2309.14450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Bertin, Vasily V. Bulatov, Fei Zhou</li>
<li>for: 研究金属塑性的 mesoscale 模型 - 粗化 atomistic 动力学中的扭轧动力学</li>
<li>methods: 使用 machine learning (ML) 框架，通过 graph neural networks (GNN) 模型来自动化扭轧动力学的开发</li>
<li>results: 在 BCC 钨中示出了准确地复制了真实 MD  simulations 中的压缩&#x2F;张力偏见，并且在低刺激速度下预测了流体压力， demonstrating 了方法的能力学习扭轧动力学的Physics。<details>
<summary>Abstract</summary>
The computational method of discrete dislocation dynamics (DDD), used as a coarse-grained model of true atomistic dynamics of lattice dislocations, has become of powerful tool to study metal plasticity arising from the collective behavior of dislocations. As a mesoscale approach, motion of dislocations in the DDD model is prescribed via the mobility law; a function which specifies how dislocation lines should respond to the driving force. However, the development of traditional hand-crafted mobility laws can be a cumbersome task and may involve detrimental simplifications. Here we introduce a machine-learning (ML) framework to streamline the development of data-driven mobility laws which are modeled as graph neural networks (GNN) trained on large-scale Molecular Dynamics (MD) simulations of crystal plasticity. We illustrate our approach on BCC tungsten and demonstrate that our GNN mobility implemented in large-scale DDD simulations accurately reproduces the challenging tension/compression asymmetry observed in ground-truth MD simulations while correctly predicting the flow stress at lower straining rate conditions unseen during training, thereby demonstrating the ability of our method to learn relevant dislocation physics. Our DDD+ML approach opens new promising avenues to improve fidelity of the DDD model and to incorporate more complex dislocation motion behaviors in an automated way, providing a faithful proxy for dislocation dynamics several orders of magnitude faster than ground-truth MD simulations.
</details>
<details>
<summary>摘要</summary>
计算方法的粗化扭变动力学（DDD）模型，作为真实原子动力学扭变动力学的粗化模型，已成为金属塑形力学的研究powerful工具。作为中规模方法，DDD模型中扭变线的运动是通过 mobilicity 法规定的，这是一个指定扭变线应对驱动力的函数。然而，开发传统手动设计 mobilicity 法可能是一项繁琐的任务，并且可能会带来不利的简化。在这里，我们引入机器学习（ML）框架，以数据驱动的方式开发出更加简单的 mobilicity 法。我们使用Graph Neural Networks（GNN）模型，在大规模的分子动力学（MD） simulation 中训练这些 mobilicity 法。我们在 BCC 钴中实现了我们的 GNN  mobilicity，并在大规模的 DDD  simulations 中证明了我们的方法可以准确地复制真实 MD  simulation 中的困难的压缩/扩展不均勋，并且可以正确地预测低剪力环境下的流体压缩强度。这表明我们的方法可以学习扭变物理学。我们的 DDD + ML 方法打开了新的可能性，以提高 DDD 模型的准确性，并自动地包含更加复杂的扭变动力学行为，提供一个 faithful 的扭变动力学代理，在训练中未达到的低剪力环境下可以正确地预测流体压缩强度。
</details></li>
</ul>
<hr>
<h2 id="On-the-expressivity-of-embedding-quantum-kernels"><a href="#On-the-expressivity-of-embedding-quantum-kernels" class="headerlink" title="On the expressivity of embedding quantum kernels"></a>On the expressivity of embedding quantum kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14419">http://arxiv.org/abs/2309.14419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elies Gil-Fuster, Jens Eisert, Vedran Dunjko</li>
<li>for: 这个论文的目的是研究量子机器学习和经典机器学习之间的自然连接，特别是在内核方法上。</li>
<li>methods: 这篇论文使用的方法包括量子特征状态的构造和嵌入量子内核。</li>
<li>results: 这篇论文的结果是证明任何量子内核都可以表示为量子特征状态的内积，并且提出了一些新的、未探索的量子内核家族，其中是否还有效的嵌入量子内核还需要进一步研究。<details>
<summary>Abstract</summary>
One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part, we formalize the question of universality of efficient embedding quantum kernels. For shift-invariant kernels, we use the technique of random Fourier features to show that they are universal within the broad class of all kernels which allow a variant of efficient Fourier sampling. We then extend this result to a new class of so-called composition kernels, which we show also contains projected quantum kernels introduced in recent works. After proving the universality of embedding quantum kernels for both shift-invariant and composition kernels, we identify the directions towards new, more exotic, and unexplored quantum kernel families, for which it still remains open whether they correspond to efficient embedding quantum kernels.
</details>
<details>
<summary>摘要</summary>
（一些）自然的量子机器学习和经典机器学习之间的连接在内核方法上已经得到了证明。内核方法 rely on 内核，它们是特征向量生活在大特征空间的内积。量子内核通常通过明确构建量子特征状态来评估，然后计算它们的内积，这被称为嵌入量子内核。由于经典内核通常不直接使用特征向量，我们所思考嵌入量子内核的表达能力如何。在这项工作中，我们提出了一个基本问题：可以所有的量子内核都表示为量子特征状态的内积吗？我们的第一个结果是正的：通过计算 universality，我们发现了对于任何内核函数，都存在一个对应的量子特征映射和嵌入量子内核。对于更操作性的问题，我们在第二部分中正式化了嵌入量子内核的 universality 问题。对于不变内核，我们使用随机傅里埃特性来证明它们是 universality 的，并将其扩展到一个新的 composition kernels 类型，这类型包括已知的 projected quantum kernels。在证明嵌入量子内核的 universality 之后，我们确定了新、更有趣、未探索的量子内核家族的方向，其中是否存在效果嵌入量子内核还未知。
</details></li>
</ul>
<hr>
<h2 id="Provable-advantages-of-kernel-based-quantum-learners-and-quantum-preprocessing-based-on-Grover’s-algorithm"><a href="#Provable-advantages-of-kernel-based-quantum-learners-and-quantum-preprocessing-based-on-Grover’s-algorithm" class="headerlink" title="Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover’s algorithm"></a>Provable advantages of kernel-based quantum learners and quantum preprocessing based on Grover’s algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14406">http://arxiv.org/abs/2309.14406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Till Muser, Elias Zapusek, Vasilis Belis, Florentin Reiter</li>
<li>for: 该研究目的是提高学习问题的计算效率，特别是应用量子计算机在支持向量机中的速度优势。</li>
<li>methods: 该研究使用了Shor的算法和Grover的算法来实现量子支持向量机的速度优势。</li>
<li>results: 研究发现，通过在支持向量机的kernel中使用量子计算机，可以获得 exponential 的速度优势。此外，通过将量子计算机与类传统的分类方法结合使用，可以进一步提高分类器的性能。<details>
<summary>Abstract</summary>
There is an ongoing effort to find quantum speedups for learning problems. Recently, [Y. Liu et al., Nat. Phys. $\textbf{17}$, 1013--1017 (2021)] have proven an exponential speedup for quantum support vector machines by leveraging the speedup of Shor's algorithm. We expand upon this result and identify a speedup utilizing Grover's algorithm in the kernel of a support vector machine. To show the practicality of the kernel structure we apply it to a problem related to pattern matching, providing a practical yet provable advantage. Moreover, we show that combining quantum computation in a preprocessing step with classical methods for classification further improves classifier performance.
</details>
<details>
<summary>摘要</summary>
有一个持续进行的努力是找到量子速度减少学习问题。最近，李宇等人（Nat. Phys. $\textbf{17}$, 1013--1017 (2021)）已经证明了量子支持向量机的加速，通过利用戈Vor的算法速度。我们在这个结果基础上进一步扩展，并证明了使用格罗弗尔算法在支持向量机的kernel中获得加速。为证明实用性，我们应用了这种结构到一个相关的模式匹配问题，并提供了实用却可证明的优势。此外，我们还证明了结合量子计算在预处理步骤中与经典方法结合，可以进一步提高分类器性能。
</details></li>
</ul>
<hr>
<h2 id="Tasks-Makyth-Models-Machine-Learning-Assisted-Surrogates-for-Tipping-Points"><a href="#Tasks-Makyth-Models-Machine-Learning-Assisted-Surrogates-for-Tipping-Points" class="headerlink" title="Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping Points"></a>Tasks Makyth Models: Machine Learning Assisted Surrogates for Tipping Points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14334">http://arxiv.org/abs/2309.14334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Fabiani, Nikolaos Evangelou, Tianqi Cui, Juan M. Bello-Rivas, Cristina P. Martin-Linares, Constantinos Siettos, Ioannis G. Kevrekidis</li>
<li>for: 这个论文的目的是提出一种基于机器学习的框架，用于探索复杂系统的突变点和罕见事件的可能性。</li>
<li>methods: 该论文使用了拟合多元space，神经网络，高斯过程和无方程多尺度模型来实现这个目的。</li>
<li>results: 该论文通过使用这些方法对高维时空数据进行压缩， constructions 缩写模型来描述不同的级别的 emergent 动力学，并且可以准确地预测突变点和罕见事件的可能性。<details>
<summary>Abstract</summary>
We present a machine learning (ML)-assisted framework bridging manifold learning, neural networks, Gaussian processes, and Equation-Free multiscale modeling, for (a) detecting tipping points in the emergent behavior of complex systems, and (b) characterizing probabilities of rare events (here, catastrophic shifts) near them. Our illustrative example is an event-driven, stochastic agent-based model (ABM) describing the mimetic behavior of traders in a simple financial market. Given high-dimensional spatiotemporal data -- generated by the stochastic ABM -- we construct reduced-order models for the emergent dynamics at different scales: (a) mesoscopic Integro-Partial Differential Equations (IPDEs); and (b) mean-field-type Stochastic Differential Equations (SDEs) embedded in a low-dimensional latent space, targeted to the neighborhood of the tipping point. We contrast the uses of the different models and the effort involved in learning them.
</details>
<details>
<summary>摘要</summary>
我们提出了一个基于机器学习（ML）的框架，它将拓扑学学习、神经网络、高斯过程和无方程多尺度模型绑定在一起，用于检测复杂系统的 emergent 行为中的跌宕点，以及在其附近的罕见事件的概率Characterization。我们的示例是一个事件驱动的随机 Agent-Based Model（ABM），描述了金融市场中的模拟行为。给出高维空间时间数据（由随机 ABM 生成），我们构建了不同级别的减少模型，用于描述不同级别的 emergent 动力学：（a） mesoscopic Integro-Partial Differential Equations（IPDEs）；和（b） Mean-field-type Stochastic Differential Equations（SDEs），其embedded在一个低维的隐藏空间中，targeted to the neighborhood of the tipping point。我们对不同模型的使用和学习努力进行了对比。
</details></li>
</ul>
<hr>
<h2 id="pLMFPPred-a-novel-approach-for-accurate-prediction-of-functional-peptides-integrating-embedding-from-pre-trained-protein-language-model-and-imbalanced-learning"><a href="#pLMFPPred-a-novel-approach-for-accurate-prediction-of-functional-peptides-integrating-embedding-from-pre-trained-protein-language-model-and-imbalanced-learning" class="headerlink" title="pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning"></a>pLMFPPred: a novel approach for accurate prediction of functional peptides integrating embedding from pre-trained protein language model and imbalanced learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14404">http://arxiv.org/abs/2309.14404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mnb66/plmfppred">https://github.com/mnb66/plmfppred</a></li>
<li>paper_authors: Zebin Ma, Yonglin Zou, Xiaobin Huang, Wenjin Yan, Hao Xu, Jiexin Yang, Ying Zhang, Jinqi Huang</li>
<li>for: 预测功能肽，即使用人工智能计算策略来快速从蛋白质序列集中鉴别出新的功能肽并确定其不同的功能。</li>
<li>methods: 使用蛋白语言模型基于的插入（ESM-2），开发了一种名为pLMFPPred（蛋白语言模型基于功能肽预测器）来预测功能肽和识别 токси肽。同时，使用SMOTE-TOMEK数据合成采样技术和Shapley值基于的特征选择技术来解决数据不均衡问题，降低计算成本。</li>
<li>results: 在一个验证的独立测试集上，pLMFPPred实现了精度、接收操作特征曲线值和F1值的0.974、0.99和0.974，分别。 comparative experiments show that pLMFPPred outperforms current methods for predicting functional peptides。实验结果表明，提案的方法（pLMFPPred）可以在预测功能肽方面提供更好的性能，并代表一种新的计算方法。<details>
<summary>Abstract</summary>
Functional peptides have the potential to treat a variety of diseases. Their good therapeutic efficacy and low toxicity make them ideal therapeutic agents. Artificial intelligence-based computational strategies can help quickly identify new functional peptides from collections of protein sequences and discover their different functions.Using protein language model-based embeddings (ESM-2), we developed a tool called pLMFPPred (Protein Language Model-based Functional Peptide Predictor) for predicting functional peptides and identifying toxic peptides. We also introduced SMOTE-TOMEK data synthesis sampling and Shapley value-based feature selection techniques to relieve data imbalance issues and reduce computational costs. On a validated independent test set, pLMFPPred achieved accuracy, Area under the curve - Receiver Operating Characteristics, and F1-Score values of 0.974, 0.99, and 0.974, respectively. Comparative experiments show that pLMFPPred outperforms current methods for predicting functional peptides.The experimental results suggest that the proposed method (pLMFPPred) can provide better performance in terms of Accuracy, Area under the curve - Receiver Operating Characteristics, and F1-Score than existing methods. pLMFPPred has achieved good performance in predicting functional peptides and represents a new computational method for predicting functional peptides.
</details>
<details>
<summary>摘要</summary>
功能蛋白质有很大的治疗潜力。它们的良好的治疗效果和低度的致病性使得它们成为理想的药物代用品。通过人工智能基于计算的方法，可以快速地从蛋白质序列集中获取新的功能蛋白质和其不同的功能。我们使用蛋白质语言模型基于嵌入（ESM-2），开发了一个名为pLMFPPred（蛋白质语言模型基于功能蛋白质预测器）的工具，用于预测功能蛋白质和识别毒蛋白质。我们还使用SMOTE-TOMEK数据合成抽样和Shapley值基于特征选择技术来解决数据均衡问题和降低计算成本。在一个验证的独立测试集上，pLMFPPred实现了精度、接收操作特征曲线值和F1分数的值为0.974、0.99和0.974，分别。相比之下，相关的方法实现了较差的效果。实验结果表明，提案的方法（pLMFPPred）可以在预测功能蛋白质方面提供更好的表现，并代表了一种新的计算方法。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Framework-for-Uniform-Signal-Recovery-in-Nonlinear-Generative-Compressed-Sensing"><a href="#A-Unified-Framework-for-Uniform-Signal-Recovery-in-Nonlinear-Generative-Compressed-Sensing" class="headerlink" title="A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing"></a>A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03758">http://arxiv.org/abs/2310.03758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junren Chen, Jonathan Scarlett, Michael K. Ng, Zhaoqiang Liu</li>
<li>For: The paper is written to study the problem of generative compressed sensing (GCS) with nonlinear measurements, and to provide uniform recovery guarantees for this problem.* Methods: The paper uses a unified framework that combines the observation model with the generative model to derive uniform recovery guarantees for nonlinear GCS. The framework accommodates GCS with 1-bit&#x2F;uniformly quantized observations and single index models as canonical examples.* Results: The paper shows that using a single realization of the sensing ensemble and generalized Lasso, all $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\ell_2$-error at most $\epsilon$ using roughly $\tilde{O}({k}&#x2F;{\epsilon^2})$ samples, with omitted logarithmic factors typically being dominated by $\log L$. This is almost coincident with existing non-uniform guarantees up to logarithmic factors, indicating that the uniformity comes with a very small cost.<details>
<summary>Abstract</summary>
In generative compressed sensing (GCS), we want to recover a signal $\mathbf{x}^* \in \mathbb{R}^n$ from $m$ measurements ($m\ll n$) using a generative prior $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$, where $G$ is typically an $L$-Lipschitz continuous generative model and $\mathbb{B}_2^k(r)$ represents the radius-$r$ $\ell_2$-ball in $\mathbb{R}^k$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $\mathbf{x}^*$ rather than for all $\mathbf{x}^*$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, {\em all} $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ can be recovered up to an $\ell_2$-error at most $\epsilon$ using roughly $\tilde{O}({k}/{\epsilon^2})$ samples, with omitted logarithmic factors typically being dominated by $\log L$. Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce the Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bounds for product processes whose index sets have low metric entropy. Experimental results are presented to corroborate our theory.
</details>
<details>
<summary>摘要</summary>
在生成式压缩感知（GCS）中，我们想要从 $m$ 测量 ($m \ll n$) 中还原一个信号 $\mathbf{x}^* \in \mathbb{R}^n$ 使用生成模型 $G$，其中 $G$ 是一个 Typically $L$-Lipschitz 连续的生成模型，$\mathbb{B}_2^k(r)$ 表示半径-$r$ $\ell_2$-球在 $\mathbb{R}^k$ 中。在非线性测量下，大多数先前结果是非均匀的，即它们在固定 $\mathbf{x}^*$ 上持有高概率而不是所有 $\mathbf{x}^*$ 上同时持有。在这篇论文中，我们构建了一个统一的框架，用于 derive 均匀的恢复保证，对于非线性 GCS，测量模型可能是不连续或未知的。我们的框架可以涵盖 GCS 与 1-bit/均匀量化观测和单index模型作为 kanonikus 例子。具体来说，使用单个感知ensemble和普通lasso，所有 $\mathbf{x}^*\in G(\mathbb{B}_2^k(r))$ 可以在 $\ell_2$ 误差不超过 $\epsilon$ 的情况下，使用约 $\tilde{O}({k}/{\epsilon^2})$ 个样本进行恢复，忽略 logs 因子通常是由 $\log L$ 控制。这与现有的非均匀保证相差只有 logs 因子，因此均匀性的代价很低。在我们的技术贡献中，我们引入了 Lipschitz approximation 来处理不连续测量模型。我们还开发了一种集中不等式，用于生成产品过程中的指标集 whose 度量 entropy 较低。实验结果用于证明我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Futility-and-utility-of-a-few-ancillas-for-Pauli-channel-learning"><a href="#Futility-and-utility-of-a-few-ancillas-for-Pauli-channel-learning" class="headerlink" title="Futility and utility of a few ancillas for Pauli channel learning"></a>Futility and utility of a few ancillas for Pauli channel learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14326">http://arxiv.org/abs/2309.14326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sitan Chen, Weiyuan Gong</li>
<li>For: 本文 revisits one of the prototypical tasks for characterizing the structure of noise in quantum devices, estimating the eigenvalues of an $n$-qubit Pauli noise channel.* Methods: 本文使用 exponential lower bounds to show the limitations of algorithms for estimating the eigenvalues of the noise channel. These lower bounds hold even for the easier hypothesis testing problem of determining whether the underlying channel is completely depolarizing or has exactly one other nontrivial eigenvalue.* Results: 本文 gets the following results:	1. Any algorithm without quantum memory must make $\Omega(2^n&#x2F;\epsilon^2)$ measurements to estimate each eigenvalue within error $\epsilon$.	2. Any algorithm with $\le k$ ancilla qubits of quantum memory must make $\Omega(2^{(n-k)&#x2F;3})$ queries to the unknown channel.	3. With only $k&#x3D;2$ ancilla qubits of quantum memory, there is an algorithm that solves the hypothesis testing task with high probability using a single measurement.I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this paper we revisit one of the prototypical tasks for characterizing the structure of noise in quantum devices, estimating the eigenvalues of an $n$-qubit Pauli noise channel. Prior work (Chen et al., 2022) established exponential lower bounds for this task for algorithms with limited quantum memory. We first improve upon their lower bounds and show:   (1) Any algorithm without quantum memory must make $\Omega(2^n/\epsilon^2)$ measurements to estimate each eigenvalue within error $\epsilon$. This is tight and implies the randomized benchmarking protocol is optimal, resolving an open question of (Flammia and Wallman, 2020).   (2) Any algorithm with $\le k$ ancilla qubits of quantum memory must make $\Omega(2^{(n-k)/3})$ queries to the unknown channel. Crucially, unlike in (Chen et al., 2022), our bound holds even if arbitrary adaptive control and channel concatenation are allowed.   In fact these lower bounds, like those of (Chen et al., 2022), hold even for the easier hypothesis testing problem of determining whether the underlying channel is completely depolarizing or has exactly one other nontrivial eigenvalue. Surprisingly, we show that:   (3) With only $k=2$ ancilla qubits of quantum memory, there is an algorithm that solves this hypothesis testing task with high probability using a single measurement.   Note that (3) does not contradict (2) as the protocol concatenates exponentially many queries to the channel before the measurement. This result suggests a novel mechanism by which channel concatenation and $O(1)$ qubits of quantum memory could work in tandem to yield striking speedups for quantum process learning that are not possible for quantum state learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们回顾了一个杜理量化器设备噪声结构的典型任务，即估算 $n$- Quint Pauli 噪声通道的 eigenvalues。先前的工作（Chen et al., 2022）已经证明了这个任务的下界，我们首先提高了这个下界并证明：  1. 没有量子储存的算法必须做 $\Omega(2^n/\epsilon^2)$ 测量来估算每个含误值。这是最佳的和 Flammia 和 Wallman（2020）的问题的解。  2. 具有 $\le k$  ancilla qubits 的量子储存算法必须做 $\Omega(2^{(n-k)/3})$ 请求来 unknown 通道。这个下界不仅如 Chen et al.（2022）的下界，而且允许任意适应控制和通道 concatenation。  事实上，这些下界也适用于另一个更容易的假设测试问题，即判断 underlying 通道是完全depolarizing 或者有 exactly one 其他非特性含误值。我们证明：  3. 只有 $k=2$ ancilla qubits 的量子储存算法可以使用单个测量来高probability 解决这个假设测试问题。这个结果表明，通过 concatenating  exponentially many queries to the channel before the measurement, it is possible to achieve striking speedups for quantum process learning that are not possible for quantum state learning.
</details></li>
</ul>
<hr>
<h2 id="Small-scale-proxies-for-large-scale-Transformer-training-instabilities"><a href="#Small-scale-proxies-for-large-scale-Transformer-training-instabilities" class="headerlink" title="Small-scale proxies for large-scale Transformer training instabilities"></a>Small-scale proxies for large-scale Transformer training instabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14322">http://arxiv.org/abs/2309.14322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, Simon Kornblith</li>
<li>for: 这个论文的目的是研究大型Transformer模型在大规模训练中出现的训练不稳定性的原因，以及这些不稳定性在小规模训练中的表现。</li>
<li>methods: 本文使用了两种源引起训练不稳定性的研究：在注意层中增长的logits（Dehghani et al., 2023）和输出logits与输出概率的分化（Chowdhery et al., 2022）。通过测量学习率和损失之间的关系，我们显示这些不稳定性也在小模型中出现，并且在高学习率训练中使用了先前在大规模训练中使用的缓解方法可以达到相似的损失值。</li>
<li>results: 本文研究了一些已知优化器和模型调整的影响，包括温存、Weight decay和$\mu$Param（Yang et al., 2022）。我们发现可以通过组合这些技术来训练小模型，以实现在不同学习率下的损失值之间的相似性。最后，我们研究了两种可以预测训练不稳定性的情况：模型活动和梯度 нор的扩展行为。<details>
<summary>Abstract</summary>
Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training stability and instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to which other known optimizer and model interventions influence the sensitivity of the final loss to changes in the learning rate. To this end, we study methods such as warm-up, weight decay, and the $\mu$Param (Yang et al., 2022), and combine techniques to train small models that achieve similar losses across orders of magnitude of learning rate variation. Finally, to conclude our exploration we study two cases where instabilities can be predicted before they emerge by examining the scaling behavior of model activation and gradient norms.
</details>
<details>
<summary>摘要</summary>
团队已经训练过大型Transformer模型时报告了训练不稳定的问题，而这些问题在相同的超参数下不会出现。虽然这些不稳定的原因具有科学兴趣，但是需要资源来调查。在这项工作中，我们寻找一种在小规模上重现和研究训练稳定性和不稳定性。我们首先关注以下两种训练不稳定的来源：在注意层中 logged 的生长（Dehghani et al., 2023）和输出 logits 与 log probability 的分化（Chowdhery et al., 2022）。通过测量学习率和损失之间的关系，我们表明这些不稳定也会在小型模型中出现，并且在这种情况下，已经在大规模上使用的 Mitigation 也是有效的。这使得我们想 investigate 其他知道的优化器和模型 intervención 对最终损失响应于学习率变化的敏感性。为此，我们研究了温存、重量 decay 和 $\mu$Param（Yang et al., 2022）等方法，并将这些方法结合使用来训练小模型，以实现在不同学习率下的相同损失水平。最后，我们研究了两种可以预测训练不稳定之前的情况：模型活动和梯度 norms 的扩散行为。
</details></li>
</ul>
<hr>
<h2 id="Lifelong-Robot-Learning-with-Human-Assisted-Language-Planners"><a href="#Lifelong-Robot-Learning-with-Human-Assisted-Language-Planners" class="headerlink" title="Lifelong Robot Learning with Human Assisted Language Planners"></a>Lifelong Robot Learning with Human Assisted Language Planners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14321">http://arxiv.org/abs/2309.14321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meenal Parakh, Alisha Fong, Anthony Simeonov, Tao Chen, Abhishek Gupta, Pulkit Agrawal</li>
<li>for: 这个论文是为了开发一种使用大型自然语言模型（LLM）来帮助机器人学习新的技能的方法。</li>
<li>methods: 论文使用了LLM来帮助机器人查询和学习新的技能，并且可以在数据和时间有效的情况下进行学习。</li>
<li>results: 研究人员通过实验和实际应用，证明了该方法可以帮助机器人在不同任务中快速学习和应用新的技能，并且可以在未来的任务中重用已经学习的技能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have been shown to act like planners that can decompose high-level instructions into a sequence of executable instructions. However, current LLM-based planners are only able to operate with a fixed set of skills. We overcome this critical limitation and present a method for using LLM-based planners to query new skills and teach robots these skills in a data and time-efficient manner for rigid object manipulation. Our system can re-use newly acquired skills for future tasks, demonstrating the potential of open world and lifelong learning. We evaluate the proposed framework on multiple tasks in simulation and the real world. Videos are available at: https://sites.google.com/mit.edu/halp-robot-learning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经被证明可以 acted like 观察者，将高水平的指令分解为可执行的指令序列。但现有的 LLM-based 观察者只能运行固定的技能。我们解决了这个极限，并提出了使用 LLM-based 观察者来查询新技能并教育机器人这些技能，在数据和时间效率下进行弹性物件抓取。我们的系统可以重复 newly acquired 技能，以便在未来任务中重复使用，这显示了开放世界和一生学习的潜力。我们在多个任务中进行了评估，并在网站上提供了详细的视频：https://sites.google.com/mit.edu/halp-robot-learning。
</details></li>
</ul>
<hr>
<h2 id="A-post-selection-algorithm-for-improving-dynamic-ensemble-selection-methods"><a href="#A-post-selection-algorithm-for-improving-dynamic-ensemble-selection-methods" class="headerlink" title="A post-selection algorithm for improving dynamic ensemble selection methods"></a>A post-selection algorithm for improving dynamic ensemble selection methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14307">http://arxiv.org/abs/2309.14307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prgc/ps-des">https://github.com/prgc/ps-des</a></li>
<li>paper_authors: Paulo R. G. Cordeiro, George D. C. Cavalcanti, Rafael M. O. Cruz</li>
<li>for: 这个研究的目的是为了选择最佳的多 кластер组件系统（MCS）方法，以提高精准度。</li>
<li>methods: 这个研究使用的方法是Post-Selection Dynamic Ensemble Selection（PS-DES）方法，它是一种在选择阶段选择最佳的组件方法。</li>
<li>results: 实验结果显示，使用精度作为选择组件方法的评估指标，PS-DES方法比单一的DES方法表现更好。Here’s the translation in English:</li>
<li>for: The purpose of this research is to select the best Multiple Classifier Systems (MCS) method to improve accuracy.</li>
<li>methods: The method used in this research is the Post-Selection Dynamic Ensemble Selection (PS-DES) method, which selects the best ensemble method in the selection phase.</li>
<li>results: Experimental results show that using accuracy as a metric to select the ensembles, PS-DES outperforms individual DES techniques.I hope that helps!<details>
<summary>Abstract</summary>
Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS) approach that aims to select an ensemble for each query sample during the selection phase. Even with the proposal of several DES approaches, no particular DES technique is the best choice for different problems. Thus, we hypothesize that selecting the best DES approach per query instance can lead to better accuracy. To evaluate this idea, we introduce the Post-Selection Dynamic Ensemble Selection (PS-DES) approach, a post-selection scheme that evaluates ensembles selected by several DES techniques using different metrics. Experimental results show that using accuracy as a metric to select the ensembles, PS-DES performs better than individual DES techniques. PS-DES source code is available in a GitHub repository
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Dynamic Ensemble Selection (DES) is a Multiple Classifier Systems (MCS) approach that aims to select an ensemble for each query sample during the selection phase. Even with the proposal of several DES approaches, no particular DES technique is the best choice for different problems. Thus, we hypothesize that selecting the best DES approach per query instance can lead to better accuracy. To evaluate this idea, we introduce the Post-Selection Dynamic Ensemble Selection (PS-DES) approach, a post-selection scheme that evaluates ensembles selected by several DES techniques using different metrics. Experimental results show that using accuracy as a metric to select the ensembles, PS-DES performs better than individual DES techniques. PS-DES source code is available in a GitHub repository" into Simplified Chinese.Here's the translation:<<SYS>>多个类ifier系统（MCS）方法之一是动态ensemble选择（DES），它在选择阶段为每个查询样本选择一个ensemble。尽管已经提出了多种DES方法，但是没有一个特定的DES技术适合所有问题。因此，我们提出了在每个查询实例中选择最佳DES方法的想法，以提高准确率。为了评估这个想法，我们引入了后期选择的动态ensemble选择（PS-DES）方法，该方法使用不同的度量评估由多种DES技术选择的ensemble。实验结果显示，使用准确率作为度量选择ensemble时，PS-DES方法perform Better than个 DES技术。PS-DES源代码可以在GitHub存储库中找到。
</details></li>
</ul>
<hr>
<h2 id="Improved-Algorithms-for-Stochastic-Linear-Bandits-Using-Tail-Bounds-for-Martingale-Mixtures"><a href="#Improved-Algorithms-for-Stochastic-Linear-Bandits-Using-Tail-Bounds-for-Martingale-Mixtures" class="headerlink" title="Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures"></a>Improved Algorithms for Stochastic Linear Bandits Using Tail Bounds for Martingale Mixtures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14298">http://arxiv.org/abs/2309.14298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamish Flynn, David Reeb, Melih Kandemir, Jan Peters</li>
<li>for: 这个论文targets the stochastic linear bandit problem, and proposes improved algorithms with worst-case regret guarantees.</li>
<li>methods: 该论文使用了一种新的tail bound for adaptive martingale mixtures to construct confidence sequences, which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming.</li>
<li>results: 该论文提供了一种基于 confidence sequences的 linear bandit algorithm, which is guaranteed to achieve competitive worst-case regret.  Additionally, the authors show that their confidence sequences are tighter than competitors, both empirically and theoretically, and demonstrate improved performance in several hyperparameter tuning tasks.<details>
<summary>Abstract</summary>
We present improved algorithms with worst-case regret guarantees for the stochastic linear bandit problem. The widely used "optimism in the face of uncertainty" principle reduces a stochastic bandit problem to the construction of a confidence sequence for the unknown reward function. The performance of the resulting bandit algorithm depends on the size of the confidence sequence, with smaller confidence sets yielding better empirical performance and stronger regret guarantees. In this work, we use a novel tail bound for adaptive martingale mixtures to construct confidence sequences which are suitable for stochastic bandits. These confidence sequences allow for efficient action selection via convex programming. We prove that a linear bandit algorithm based on our confidence sequences is guaranteed to achieve competitive worst-case regret. We show that our confidence sequences are tighter than competitors, both empirically and theoretically. Finally, we demonstrate that our tighter confidence sequences give improved performance in several hyperparameter tuning tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了改进的算法，带有最坏情况的悔检保证，用于Stochastic Linear Bandit问题。通过“面对不确定性的optimism”原则，将随机bandit问题转化为建立不确定奖金函数的信任序列。算法的性能取决于信任序列的大小，小的信任序列对实际性能和悔检保证具有更好的效果。在这项工作中，我们使用了一种新的尾部 bound for adaptive martingale mixtures来构建信任序列，这些信任序列适用于随机bandits。这些信任序列使得可以通过几何编程进行高效的动作选择。我们证明了一个基于我们的信任序列的线性bandit算法能够实现竞争性最坏情况的悔检保证。我们还证明了我们的信任序列比竞争者更紧， tantoempirically和理论上。最后，我们示出了我们的紧密信任序列可以提高一些超参数调整任务的性能。
</details></li>
</ul>
<hr>
<h2 id="On-the-Non-Associativity-of-Analog-Computations"><a href="#On-the-Non-Associativity-of-Analog-Computations" class="headerlink" title="On the Non-Associativity of Analog Computations"></a>On the Non-Associativity of Analog Computations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14292">http://arxiv.org/abs/2309.14292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lisa Kuhn, Bernhard Klein, Holger Fröning</li>
<li>for: 这种研究旨在探讨分析计算中的缺失精度问题，以及这些问题对机器学习任务的影响。</li>
<li>methods: 该研究使用了一个简单的模型来示例出实际的分析处理器中的排序效应。</li>
<li>results: 结果表明，忽略排序可能会导致准确率下降substantially。<details>
<summary>Abstract</summary>
The energy efficiency of analog forms of computing makes it one of the most promising candidates to deploy resource-hungry machine learning tasks on resource-constrained system such as mobile or embedded devices. However, it is well known that for analog computations the safety net of discretization is missing, thus all analog computations are exposed to a variety of imperfections of corresponding implementations. Examples include non-linearities, saturation effect and various forms of noise. In this work, we observe that the ordering of input operands of an analog operation also has an impact on the output result, which essentially makes analog computations non-associative, even though the underlying operation might be mathematically associative. We conduct a simple test by creating a model of a real analog processor which captures such ordering effects. With this model we assess the importance of ordering by comparing the test accuracy of a neural network for keyword spotting, which is trained based either on an ordered model, on a non-ordered variant, and on real hardware. The results prove the existence of ordering effects as well as their high impact, as neglecting ordering results in substantial accuracy drops.
</details>
<details>
<summary>摘要</summary>
“Analog计算的能源效率使得它成为部署资源受限的移动或嵌入式设备上耗费资源的最佳候选人。然而，所有的analog计算都缺乏精度的保障，因此它们暴晒于实现中的各种不稳定性，如非线性、饱和效应和各种噪声。在这个工作中，我们发现了输入操作的顺序也对输出结果产生影响，从而使analog计算变得非关联的，即使其下面的运算可能是数学上关联的。我们创建了一个模型，用于捕捉这些顺序效应。通过这个模型，我们评估了顺序的重要性，并发现忽略顺序会导致准确性下降。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DECORAIT-–-DECentralized-Opt-in-out-Registry-for-AI-Training"><a href="#DECORAIT-–-DECentralized-Opt-in-out-Registry-for-AI-Training" class="headerlink" title="DECORAIT – DECentralized Opt-in&#x2F;out Registry for AI Training"></a>DECORAIT – DECentralized Opt-in&#x2F;out Registry for AI Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14400">http://arxiv.org/abs/2309.14400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kar Balan, Alex Black, Simon Jenni, Andrew Gilbert, Andy Parsons, John Collomosse</li>
<li>For: The paper aims to address the data governance challenge faced by content creators who want to share their work openly without sanctioning its use for training AI models, and to ensure fair recognition and reward for their contributions.* Methods: The paper proposes a decentralized registry called DECORAIT, which uses hierarchical clustering and a combination of on&#x2F;off-chain storage to trace the provenance of GenAI training data and determine training consent. The registry leverages distributed ledger technology (DLT) and visual fingerprinting, and is built on the emerging C2PA standard.* Results: The paper reports a prototype of DECORAIT, which demonstrates the feasibility of using a decentralized registry to trace the provenance of GenAI training data and ensure fair recognition and reward for content creators. The prototype combines the strengths of DLT and visual fingerprinting to create a secure, open registry that can be used to express consent and data ownership for GenAI.<details>
<summary>Abstract</summary>
We present DECORAIT; a decentralized registry through which content creators may assert their right to opt in or out of AI training as well as receive reward for their contributions. Generative AI (GenAI) enables images to be synthesized using AI models trained on vast amounts of data scraped from public sources. Model and content creators who may wish to share their work openly without sanctioning its use for training are thus presented with a data governance challenge. Further, establishing the provenance of GenAI training data is important to creatives to ensure fair recognition and reward for their such use. We report a prototype of DECORAIT, which explores hierarchical clustering and a combination of on/off-chain storage to create a scalable decentralized registry to trace the provenance of GenAI training data in order to determine training consent and reward creatives who contribute that data. DECORAIT combines distributed ledger technology (DLT) with visual fingerprinting, leveraging the emerging C2PA (Coalition for Content Provenance and Authenticity) standard to create a secure, open registry through which creatives may express consent and data ownership for GenAI.
</details>
<details>
<summary>摘要</summary>
我们介绍DECORAIT，一个去中心化的数据库，让内容创作者可以选择是否参与人工智能训练，并获得创作所获得的回馈。生成型人工智能（GenAI）可以使用基于大量公开资料的人工智能模型生成图像。如果模型和内容创作者想要公开分享他们的作品而不授权其用于训练，他们面临资料管理挑战。此外，确定GenAI训练资料的起源是重要的，以确保创作者获得公平的认可和奖励。我们报告DECORAIT的原型，它使用嵌入式数据和分支链技术（DLT），并与可识别的视觉指纹（Visual Fingerprinting）集成，以创建一个可靠、公开的数据库，让创作者表达同意和资料所有权 для GenAI。
</details></li>
</ul>
<hr>
<h2 id="Learning-Risk-Aware-Quadrupedal-Locomotion-using-Distributional-Reinforcement-Learning"><a href="#Learning-Risk-Aware-Quadrupedal-Locomotion-using-Distributional-Reinforcement-Learning" class="headerlink" title="Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning"></a>Learning Risk-Aware Quadrupedal Locomotion using Distributional Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14246">http://arxiv.org/abs/2309.14246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Schneider, Jonas Frey, Takahiro Miki, Marco Hutter</li>
<li>for: 本研究旨在帮助机器人在危险环境中进行行动，以避免意外和减少风险。</li>
<li>methods: 本研究使用分布式再决策学习来考虑安全性，并将完整的值分布来衡量机器人与环境之间的uncertainty。</li>
<li>results: 本研究在 simulate 和 ANYmal quadruped robot上实现了 emergent 风险敏感的行动行为，并且可以通过控制一个参数来调整机器人的行为风格，从而实现风险敏感性。<details>
<summary>Abstract</summary>
Deployment in hazardous environments requires robots to understand the risks associated with their actions and movements to prevent accidents. Despite its importance, these risks are not explicitly modeled by currently deployed locomotion controllers for legged robots. In this work, we propose a risk sensitive locomotion training method employing distributional reinforcement learning to consider safety explicitly. Instead of relying on a value expectation, we estimate the complete value distribution to account for uncertainty in the robot's interaction with the environment. The value distribution is consumed by a risk metric to extract risk sensitive value estimates. These are integrated into Proximal Policy Optimization (PPO) to derive our method, Distributional Proximal Policy Optimization (DPPO). The risk preference, ranging from risk-averse to risk-seeking, can be controlled by a single parameter, which enables to adjust the robot's behavior dynamically. Importantly, our approach removes the need for additional reward function tuning to achieve risk sensitivity. We show emergent risk sensitive locomotion behavior in simulation and on the quadrupedal robot ANYmal.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对危险环境部署需要机器人理解其动作和移动的风险，以避免意外。现有的步行控制器并未显式地考虑这些风险。在这项工作中，我们提出一种带有安全考虑的步行训练方法，使用分布式再增强学习来考虑安全。而不是仅仅依靠值期望，我们估算整个值分布，以考虑机器人与环境的互动不确定性。这个值分布被消耗到风险度量来提取风险敏感的价值估计。这些估计被 integrate 到 proximal policy optimization（PPO）中，得到我们的方法：分布式 proximal policy optimization（DPPO）。风险偏好，从不敢风险到敢风险，可以通过一个参数控制，以 dynamically 调整机器人的行为。这种方法可以消除需要额外奖励函数调整以实现风险敏感。我们在 simulated 和 ANYmal 四足机器人上实现了 emergent 风险敏感的步行行为。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Abstain-From-Uninformative-Data"><a href="#Learning-to-Abstain-From-Uninformative-Data" class="headerlink" title="Learning to Abstain From Uninformative Data"></a>Learning to Abstain From Uninformative Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14240">http://arxiv.org/abs/2309.14240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikai Zhang, Songzhu Zheng, Mina Dalirrooyfard, Pengxiang Wu, Anderson Schneider, Anant Raj, Yuriy Nevmyvaka, Chao Chen</li>
<li>for: 本研究探讨了在高噪音比例下学习和决策的问题，如金融或医疗领域。</li>
<li>methods: 我们提出了一种基于选择学习理论的损失函数，以及一种迭代算法，可以同时优化预测器和选择器，并在多种场景中评估其实验性能。</li>
<li>results: 我们的方法可以在具有高噪音比例的数据上提供有效的学习和决策，并且可以在训练和测试阶段处理不相关的数据。<details>
<summary>Abstract</summary>
Learning and decision-making in domains with naturally high noise-to-signal ratio, such as Finance or Healthcare, is often challenging, while the stakes are very high. In this paper, we study the problem of learning and acting under a general noisy generative process. In this problem, the data distribution has a significant proportion of uninformative samples with high noise in the label, while part of the data contains useful information represented by low label noise. This dichotomy is present during both training and inference, which requires the proper handling of uninformative data during both training and testing. We propose a novel approach to learning under these conditions via a loss inspired by the selective learning theory. By minimizing this loss, the model is guaranteed to make a near-optimal decision by distinguishing informative data from uninformative data and making predictions. We build upon the strength of our theoretical guarantees by describing an iterative algorithm, which jointly optimizes both a predictor and a selector, and evaluates its empirical performance in a variety of settings.
</details>
<details>
<summary>摘要</summary>
学习和决策在具有自然高噪声比例的领域，如金融或医疗，经常是一项挑战，而且风险很高。在这篇论文中，我们研究学习和行动在一个通用的噪声生成过程下的问题。在这个问题中，数据分布中有一定的无用样本，具有高噪声的标签，而其中一部分数据具有低噪声的有用信息。这种分化存在于训练和测试阶段，需要正确处理无用数据。我们提出了一种基于选择学习理论的新方法，通过最小化这种损失函数，使模型能够做出最佳决策。我们在理论保证的基础上描述了一种迭代算法，它同时优化一个预测器和一个选择器，并评估其实际性能。
</details></li>
</ul>
<hr>
<h2 id="Predicting-environment-effects-on-breast-cancer-by-implementing-machine-learning"><a href="#Predicting-environment-effects-on-breast-cancer-by-implementing-machine-learning" class="headerlink" title="Predicting environment effects on breast cancer by implementing machine learning"></a>Predicting environment effects on breast cancer by implementing machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14397">http://arxiv.org/abs/2309.14397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Shoaib Farooq, Mehreen Ilyas</li>
<li>for: 本研究旨在探讨环境因素对乳腺癌的发生和进程中的作用，以及这些因素对乳腺癌预后的影响。</li>
<li>methods: 本研究使用了机器学习算法，包括逻辑回归、随机森林、KNN算法、Support Vector Machine和Extra Tree Classifier，以表达预测。</li>
<li>results: 研究发现，Random Forest算法的准确率为0.91%，ROC曲线为0.901%，表示这些机器学习算法在乳腺癌存活分析中具有良好的准确性，这些技术可能成为乳腺癌预后预测的新选择。<details>
<summary>Abstract</summary>
The biggest Breast cancer is increasingly a major factor in female fatalities, overtaking heart disease. While genetic factors are important in the growth of breast cancer, new research indicates that environmental factors also play a substantial role in its occurrence and progression. The literature on the various environmental factors that may affect breast cancer risk, incidence, and outcomes is thoroughly reviewed in this study report. The study starts by looking at how lifestyle decisions, such as eating habits, exercise routines, and alcohol consumption, may affect hormonal imbalances and inflammation, two important factors driving the development of breast cancer. Additionally, it explores the part played by environmental contaminants such pesticides, endocrine-disrupting chemicals (EDCs), and industrial emissions, all of which have been linked to a higher risk of developing breast cancer due to their interference with hormone signaling and DNA damage. Algorithms for machine learning are used to express predictions. Logistic Regression, Random Forest, KNN Algorithm, SVC and extra tree classifier. Metrics including the confusion matrix correlation coefficient, F1-score, Precision, Recall, and ROC curve were used to evaluate the models. The best accuracy among all the classifiers is Random Forest with 0.91% accuracy and ROC curve 0.901% of Logistic Regression. The accuracy of the multiple algorithms for machine learning utilized in this research was good, which is important and indicates that these techniques could serve as replacement forecasting techniques in breast cancer survival analysis, notably in the Asia region.
</details>
<details>
<summary>摘要</summary>
最大的乳癌是在女性死亡中日益占据主导地位，超越心血管疾病。虽然遗传因素在乳癌增长中扮演重要角色，但新研究表明环境因素也在乳癌发生和进程中扮演了重要角色。本研究报告 thorougly  reviewed the literature on various environmental factors that may affect breast cancer risk, incidence, and outcomes. The study begins by examining how lifestyle decisions, such as dietary habits, exercise routines, and alcohol consumption, may affect hormonal imbalances and inflammation, two key factors driving the development of breast cancer. Additionally, it explores the role played by environmental pollutants such as pesticides, endocrine-disrupting chemicals (EDCs), and industrial emissions, all of which have been linked to a higher risk of developing breast cancer due to their interference with hormone signaling and DNA damage. The study used machine learning algorithms, including logistic regression, random forest, KNN algorithm, SVC, and extra tree classifier, to express predictions. Metrics including confusion matrix, correlation coefficient, F1-score, precision, recall, and ROC curve were used to evaluate the models. The best accuracy among all the classifiers was Random Forest with 0.91% accuracy and ROC curve 0.901% of logistic regression. The accuracy of the multiple machine learning algorithms used in this research was good, indicating that these techniques could serve as replacement forecasting techniques in breast cancer survival analysis, particularly in the Asia region.
</details></li>
</ul>
<hr>
<h2 id="Guess-Sketch-Language-Model-Guided-Transpilation"><a href="#Guess-Sketch-Language-Model-Guided-Transpilation" class="headerlink" title="Guess &amp; Sketch: Language Model Guided Transpilation"></a>Guess &amp; Sketch: Language Model Guided Transpilation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14396">http://arxiv.org/abs/2309.14396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celine Lee, Abdulrahman Mahmoud, Michal Kurek, Simone Campanoni, David Brooks, Stephen Chong, Gu-Yeon Wei, Alexander M. Rush</li>
<li>for: 本研究旨在提高维护旧系统软件的效率，使用了学习型转换器来自动将 Assembly code 转换为其他编程语言。</li>
<li>methods: 本研究使用了一种名为 Guess &amp; Sketch 的 neurosymbolic 方法，它将 LM 和符号解决器结合在一起，以实现 Assembly code 的自动转换。</li>
<li>results: 根据实验结果，Guess &amp; Sketch 可以成功转换 57.6% 更多的 Assembly code 示例，比 GPT-4 和手动编写的转换器更高效。<details>
<summary>Abstract</summary>
Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. Automated symbolic program translation approaches guarantee correctness but struggle to scale to longer programs due to the exponentially large search space. Their rigid rule-based systems also limit their expressivity, so they can only reason about a reduced space of programs. Probabilistic neural language models (LMs) produce plausible outputs for every input, but do so at the cost of guaranteed correctness. In this work, we leverage the strengths of LMs and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Assembly code is an appropriate setting for a neurosymbolic approach, since assembly code can be divided into shorter non-branching basic blocks amenable to the use of symbolic methods. Guess & Sketch extracts alignment and confidence information from features of the LM then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess & Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6% more examples than GPT-4 and 39.6% more examples than an engineered transpiler. We also share a training and evaluation dataset for this task.
</details>
<details>
<summary>摘要</summary>
维护遗传软件需要很多软件和系统工程时间。 Assembly 程式，它们需要低层次控制电脑机器状态，并没有变数名称，对人类分析而言特别困难。现有的传统程式翻译器可以保证正确性，但是它们是手工设计的源和目标程式码语言的。学习型翻译，即自动翻译程式码，提供了一个人工重新写程式码的替代方案。自动 симвоlic 程式翻译方法可以保证正确性，但是它们在更长的程式码中缺乏扩展性，因为搜索空间是指数增长的。它们的僵化规则系统也限制了它们的表达力，只能理解一个受限的程式空间。概率神经语言模型（LM）可以生成可能的输出，但是它们在交互时需要付出正确性的代价。在这个工作中，我们利用 LM 和符号方法的优点，在 assembly 程式中实现了学习型翻译。 assembly 程式可以被分成更短的非分支基本块，适合使用符号方法。 Guess & Sketch 首先从 LM 中提取对适合性和信任度的资讯，然后将其转交给符号方法以解决这个翻译任务的内涵相等性。我们在三个不同的 assembly 翻译任务上进行测试，发现 Guess & Sketch 成功翻译了 57.6% 更多的例子，比 GPT-4 和手工设计的翻译器更高。我们还提供了这个任务的训练和评估数据集。
</details></li>
</ul>
<hr>
<h2 id="Learning-Restricted-Boltzmann-Machines-with-greedy-quantum-search"><a href="#Learning-Restricted-Boltzmann-Machines-with-greedy-quantum-search" class="headerlink" title="Learning Restricted Boltzmann Machines with greedy quantum search"></a>Learning Restricted Boltzmann Machines with greedy quantum search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14196">http://arxiv.org/abs/2309.14196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liming Zhao, Aman Agrawal, Patrick Rebentrost</li>
<li>for: 扩展Restricted Boltzmann Machines（RBMs）的结构学习问题到量子计算领域，并提出相应的量子算法来解决这个问题。</li>
<li>methods: 使用量子算法来学习RBMs的结构，包括两种特定类型的RBMs：ferromagnetic RBMs和地方一致RBMs。</li>
<li>results: 对于这两种类型的RBMs，量子算法比类型的纯类型算法具有 polynomial 速度增长。<details>
<summary>Abstract</summary>
Restricted Boltzmann Machines (RBMs) are widely used probabilistic undirected graphical models with visible and latent nodes, playing an important role in statistics and machine learning. The task of structure learning for RBMs involves inferring the underlying graph by using samples from the visible nodes. Specifically, learning the two-hop neighbors of each visible node allows for the inference of the graph structure. Prior research has addressed the structure learning problem for specific classes of RBMs, namely ferromagnetic and locally consistent RBMs. In this paper, we extend the scope to the quantum computing domain and propose corresponding quantum algorithms for this problem. Our study demonstrates that the proposed quantum algorithms yield a polynomial speedup compared to the classical algorithms for learning the structure of these two classes of RBMs.
</details>
<details>
<summary>摘要</summary>
restrictive Boltzmann machines (RBMs) 是一种广泛使用的可能性图模型，具有可见节点和隐藏节点，在统计学和机器学习中扮演着重要角色。structure learning问题的解决方法是使用可见节点的样本来推断图结构。特别是，了解每个可见节点的两步邻居，可以推断出图结构。先前的研究已经对特定类型的 RBMs 进行了结构学习问题的研究， specifically ferromagnetic 和 locally consistent RBMs。在这篇论文中，我们将这个问题推广到量子计算领域，并提出相应的量子算法来解决这个问题。我们的研究表明，提议的量子算法与类icial算法相比，对于这两种类型的 RBMs 的结构学习问题，具有Polynomial Speedup。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-Under-Restricted-User-Availability"><a href="#Federated-Learning-Under-Restricted-User-Availability" class="headerlink" title="Federated Learning Under Restricted User Availability"></a>Federated Learning Under Restricted User Availability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14176">http://arxiv.org/abs/2309.14176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Periklis Theodoropoulos, Konstantinos E. Nikolakakis, Dionysis Kalogerias</li>
<li>for: 这篇论文旨在提出一个可靠的联合学习框架，能够在不违反数据隐私的情况下进行联合模型训练。</li>
<li>methods: 本论文使用了一个可能随机的站台选择策略，称为随机存取模型（RAM），并提出了一个新的联合学习问题形ulation，可以有效捕捉和减少具有限制的数据参与的问题。</li>
<li>results: 实验结果显示，提出的方法可以与标准联合学习相比，在不同的设定下均表现出较好的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a decentralized machine learning framework that enables collaborative model training while respecting data privacy. In various applications, non-uniform availability or participation of users is unavoidable due to an adverse or stochastic environment, the latter often being uncontrollable during learning. Here, we posit a generic user selection mechanism implementing a possibly randomized, stationary selection policy, suggestively termed as a Random Access Model (RAM). We propose a new formulation of the FL problem which effectively captures and mitigates limited participation of data originating from infrequent, or restricted users, at the presence of a RAM. By employing the Conditional Value-at-Risk (CVaR) over the (unknown) RAM distribution, we extend the expected loss FL objective to a risk-aware objective, enabling the design of an efficient training algorithm that is completely oblivious to the RAM, and with essentially identical complexity as FedAvg. Our experiments on synthetic and benchmark datasets show that the proposed approach achieves significantly improved performance as compared with standard FL, under a variety of setups.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Designing-and-evaluating-an-online-reinforcement-learning-agent-for-physical-exercise-recommendations-in-N-of-1-trials"><a href="#Designing-and-evaluating-an-online-reinforcement-learning-agent-for-physical-exercise-recommendations-in-N-of-1-trials" class="headerlink" title="Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials"></a>Designing and evaluating an online reinforcement learning agent for physical exercise recommendations in N-of-1 trials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14156">http://arxiv.org/abs/2309.14156</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hialab/reinforcement-learning-agents-in-n-of-1-trials">https://github.com/hialab/reinforcement-learning-agents-in-n-of-1-trials</a></li>
<li>paper_authors: Dominik Meier, Ipek Ensari, Stefan Konigorski</li>
<li>For: The paper is written to explore the feasibility and effectiveness of using an online reinforcement learning agent to implement personalized adaptive interventions in clinical settings.* Methods: The paper uses a novel study on physical exercise recommendations to reduce pain in endometriosis as an illustration, and describes the design of a contextual bandit recommendation agent. The agent is evaluated in simulation studies.* Results: The results show that adaptive interventions can add complexity to the design and implementation process, but have the potential to improve patients’ benefits even with limited observations. The approach is expected to be transferable to other interventions and clinical settings.Here is the information in Simplified Chinese text:* For: 本研究是为了探讨个性化适应性治疗在临床设置中的可行性和效果。* Methods: 本研究使用了一个新的物理运动推荐算法来降低悬股症的痛症，并对这种算法进行了评估。* Results: 结果显示个性化适应性治疗可能会增加设计和实施过程的复杂性，但是它们可以在有限的观察数据下提高患者的效果。这种方法预期可以在其他治疗和临床设置中传递应用。<details>
<summary>Abstract</summary>
Personalized adaptive interventions offer the opportunity to increase patient benefits, however, there are challenges in their planning and implementation. Once implemented, it is an important question whether personalized adaptive interventions are indeed clinically more effective compared to a fixed gold standard intervention. In this paper, we present an innovative N-of-1 trial study design testing whether implementing a personalized intervention by an online reinforcement learning agent is feasible and effective. Throughout, we use a new study on physical exercise recommendations to reduce pain in endometriosis for illustration. We describe the design of a contextual bandit recommendation agent and evaluate the agent in simulation studies. The results show that adaptive interventions add complexity to the design and implementation process, but have the potential to improve patients' benefits even if only few observations are available. In order to quantify the expected benefit, data from previous interventional studies is required. We expect our approach to be transferable to other interventions and clinical interventions.
</details>
<details>
<summary>摘要</summary>
个人化适应 intervención 可以提高病人的效果，但是规划和实施中存在挑战。如果实施了个人化适应 intervención，是否比静态黄金标准 intervención 更有效？在这篇论文中，我们介绍了一种新的 N-of-1 试验研究设计，用于测试个人化 intervención 是否可行和有效。我们使用了一项新的 физи exercise 推荐算法来减轻疼痛的研究，以 illustrate 我们的方法。我们描述了一种上下文 bandit 推荐代理的设计，并在模拟研究中评估了代理。结果显示，个人化 intervención 可以增加病人的效果，但是设计和实施过程可能会加入复杂性。为了量化预期的效果，需要对前一次的 intervenational 研究数据进行分析。我们预计我们的方法可以应用于其他 intervenational 和临床研究。
</details></li>
</ul>
<hr>
<h2 id="Extragradient-Type-Methods-for-Riemannian-Variational-Inequality-Problems"><a href="#Extragradient-Type-Methods-for-Riemannian-Variational-Inequality-Problems" class="headerlink" title="Extragradient Type Methods for Riemannian Variational Inequality Problems"></a>Extragradient Type Methods for Riemannian Variational Inequality Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14155">http://arxiv.org/abs/2309.14155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Hu, Guanghui Wang, Xi Wang, Andre Wibisono, Jacob Abernethy, Molei Tao<br>for: 这个论文主要研究的是偏微分方程问题（Monotone Riemannian Variational Inequality Problems，简称RVIPs）的最优化问题。methods: 这个论文提出了两种新的算法：Riemannian extragradient（REG）方法和Riemannian past extragradient（RPEG）方法，这两种方法都可以在几何扩展空间上实现最优化问题的解。results: 这个论文的结果表明，REG和RPEG方法的最后迭代都会收敛到RVIPs的解，并且这个收敛速率是$O\left(\frac{1}{\sqrt{T}\right)$。此外，这个论文还证明了REG和RPEG方法的平均迭代收敛速率是$O\left(\frac{1}{T}\right)$，这与欧几里得空间中的观察相一致。<details>
<summary>Abstract</summary>
Riemannian convex optimization and minimax optimization have recently drawn considerable attention. Their appeal lies in their capacity to adeptly manage the non-convexity of the objective function as well as constraints inherent in the feasible set in the Euclidean sense. In this work, we delve into monotone Riemannian Variational Inequality Problems (RVIPs), which encompass both Riemannian convex optimization and minimax optimization as particular cases. In the context of Euclidean space, it is established that the last-iterates of both the extragradient (EG) and past extragradient (PEG) methods converge to the solution of monotone variational inequality problems at a rate of $O\left(\frac{1}{\sqrt{T}\right)$ (Cai et al., 2022). However, analogous behavior on Riemannian manifolds remains an open question. To bridge this gap, we introduce the Riemannian extragradient (REG) and Riemannian past extragradient (RPEG) methods. We demonstrate that both exhibit $O\left(\frac{1}{\sqrt{T}\right)$ last-iterate convergence. Additionally, we show that the average-iterate convergence of both REG and RPEG is $O\left(\frac{1}{T}\right)$, aligning with observations in the Euclidean case (Mokhtari et al., 2020). These results are enabled by judiciously addressing the holonomy effect so that additional complications in Riemannian cases can be reduced and the Euclidean proof inspired by the performance estimation problem (PEP) technique or the sum-of-squares (SOS) technique can be applied again.
</details>
<details>
<summary>摘要</summary>
“里曼尼安 convex 优化和最小最大优化在最近吸引了广泛关注。它们的吸引力在于它们可以有效地处理非凸函数和约束的非凸性在欧几何上。在这篇文章中，我们深入研究幂等里曼尼变量不等式问题（RVIPs），它们包括里曼尼 convex 优化和最小最大优化为特殊情况。在欧几何空间中，已经证明了extragradient（EG）和过去extragradient（PEG）方法的最后迭代都会 converge到变量不等式问题的解的 $O\left(\frac{1}{\sqrt{T}\right)$ 速率（Cai et al., 2022）。然而，在里曼尼拓扑上的相似行为仍然是一个未解决的问题。为了桥接这个差距，我们引入里曼尼extragradient（REG）和里曼尼过去extragradient（RPEG）方法。我们证明了这两种方法的最后迭代都会 converge于 $O\left(\frac{1}{\sqrt{T}\right)$ 速率。此外，我们还证明了REG和RPEG的平均迭代速率为 $O\left(\frac{1}{T}\right)$，与欧几何空间中观察到的（Mokhtari et al., 2020）相一致。这些结果是通过谨慎地处理启动效应，使得里曼尼拓扑上的额外复杂性可以被减少，并且可以再次应用欧几何空间中的性能估计问题（PEP）技术或准则集（SOS）技术来实现。”
</details></li>
</ul>
<hr>
<h2 id="One-Class-Classification-for-Intrusion-Detection-on-Vehicular-Networks"><a href="#One-Class-Classification-for-Intrusion-Detection-on-Vehicular-Networks" class="headerlink" title="One-Class Classification for Intrusion Detection on Vehicular Networks"></a>One-Class Classification for Intrusion Detection on Vehicular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14134">http://arxiv.org/abs/2309.14134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jake Guidry, Fahad Sohrab, Raju Gottumukkala, Satya Katragadda, Moncef Gabbouj</li>
<li>for: 防护 vehicular networks 中的 Controller Area Network bus 系统免受现代黑客攻击</li>
<li>methods: 使用机器学习方法进行检测和报告攻击</li>
<li>results: 试验了多种state-of-the-art一类分类方法对 Controller Area Network bus 流量中的射预攻击的效果，发现Subspace Support Vector Data Description 方法在normal operation 和被攻击时都能够最高效，Gmean 约85%<details>
<summary>Abstract</summary>
Controller Area Network bus systems within vehicular networks are not equipped with the tools necessary to ward off and protect themselves from modern cyber-security threats. Work has been done on using machine learning methods to detect and report these attacks, but common methods are not robust towards unknown attacks. These methods usually rely on there being a sufficient representation of attack data, which may not be available due to there either not being enough data present to adequately represent its distribution or the distribution itself is too diverse in nature for there to be a sufficient representation of it. With the use of one-class classification methods, this issue can be mitigated as only normal data is required to train a model for the detection of anomalous instances. Research has been done on the efficacy of these methods, most notably One-Class Support Vector Machine and Support Vector Data Description, but many new extensions of these works have been proposed and have yet to be tested for injection attacks in vehicular networks. In this paper, we investigate the performance of various state-of-the-art one-class classification methods for detecting injection attacks on Controller Area Network bus traffic. We investigate the effectiveness of these techniques on attacks launched on Controller Area Network buses from two different vehicles during normal operation and while being attacked. We observe that the Subspace Support Vector Data Description method outperformed all other tested methods with a Gmean of about 85%.
</details>
<details>
<summary>摘要</summary>
控制器网络攻击系统在交通网络中没有具备防御modern cyber安全攻击的工具。工作已经在使用机器学习方法检测和报告这些攻击，但通用方法不够鲜硬度Unknown攻击。这些方法通常需要充分的攻击数据来表征攻击的分布，但可能缺乏数据或者攻击分布太多样化，导致无法充分表征。使用一类分类方法可以解决这问题，只需要正常数据来训练模型来检测异常情况。研究表示一类支持向量数据描述法和一类支持向量分类器在检测插入攻击方面表现出色，但还没有在交通网络中进行测试。本文 investigate了一些当前顶尖一类分类方法在Controller Area Network总线上检测插入攻击的性能。我们对两辆不同的车辆在正常运行和遭受攻击时的Controller Area Network总线上的攻击进行了测试。我们发现Subspace Support Vector Data Description法的性能高于所有测试过的方法，Gmean约85%。
</details></li>
</ul>
<hr>
<h2 id="Driving-behavior-guided-battery-health-monitoring-for-electric-vehicles-using-machine-learning"><a href="#Driving-behavior-guided-battery-health-monitoring-for-electric-vehicles-using-machine-learning" class="headerlink" title="Driving behavior-guided battery health monitoring for electric vehicles using machine learning"></a>Driving behavior-guided battery health monitoring for electric vehicles using machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14125">http://arxiv.org/abs/2309.14125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nanhua Jiang, Jiawei Zhang, Weiran Jiang, Yao Ren, Jing Lin, Edwin Khoo, Ziyou Song</li>
<li>for: 提供了一种基于特征的机器学习管道，用于准确和可靠地监测电池健康状态。</li>
<li>methods: 使用了多种健康指标（HI）的合理选择和融合，以及考虑了实际驾驶行为。</li>
<li>results: 提供了一种能够考虑实际驾驶行为的功能特征选择和融合方法，以提高电池健康监测的准确性和实用性。<details>
<summary>Abstract</summary>
An accurate estimation of the state of health (SOH) of batteries is critical to ensuring the safe and reliable operation of electric vehicles (EVs). Feature-based machine learning methods have exhibited enormous potential for rapidly and precisely monitoring battery health status. However, simultaneously using various health indicators (HIs) may weaken estimation performance due to feature redundancy. Furthermore, ignoring real-world driving behaviors can lead to inaccurate estimation results as some features are rarely accessible in practical scenarios. To address these issues, we proposed a feature-based machine learning pipeline for reliable battery health monitoring, enabled by evaluating the acquisition probability of features under real-world driving conditions. We first summarized and analyzed various individual HIs with mechanism-related interpretations, which provide insightful guidance on how these features relate to battery degradation modes. Moreover, all features were carefully evaluated and screened based on estimation accuracy and correlation analysis on three public battery degradation datasets. Finally, the scenario-based feature fusion and acquisition probability-based practicality evaluation method construct a useful tool for feature extraction with consideration of driving behaviors. This work highlights the importance of balancing the performance and practicality of HIs during the development of feature-based battery health monitoring algorithms.
</details>
<details>
<summary>摘要</summary>
《 accurately estimating the state of health (SOH) of batteries is crucial for ensuring the safe and reliable operation of electric vehicles (EVs). feature-based machine learning methods have shown great potential for rapidly and precisely monitoring battery health status. however, using various health indicators (HIs) simultaneously may weaken estimation performance due to feature redundancy. furthermore, ignoring real-world driving behaviors can lead to inaccurate estimation results as some features are rarely accessible in practical scenarios. to address these issues, we proposed a feature-based machine learning pipeline for reliable battery health monitoring, enabled by evaluating the acquisition probability of features under real-world driving conditions. we first summarized and analyzed various individual HIs with mechanism-related interpretations, which provide insightful guidance on how these features relate to battery degradation modes. moreover, all features were carefully evaluated and screened based on estimation accuracy and correlation analysis on three public battery degradation datasets. finally, the scenario-based feature fusion and acquisition probability-based practicality evaluation method construct a useful tool for feature extraction with consideration of driving behaviors. this work highlights the importance of balancing the performance and practicality of HIs during the development of feature-based battery health monitoring algorithms.》Note: Please note that the translation is in Simplified Chinese, and the sentence structure and wording may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Solution-of-The-Stationary-Fokker-Plank-Equation-for-a-Class-of-Nonlinear-Dynamical-Systems-An-Evaluation-Study"><a href="#Physics-Informed-Solution-of-The-Stationary-Fokker-Plank-Equation-for-a-Class-of-Nonlinear-Dynamical-Systems-An-Evaluation-Study" class="headerlink" title="Physics-Informed Solution of The Stationary Fokker-Plank Equation for a Class of Nonlinear Dynamical Systems: An Evaluation Study"></a>Physics-Informed Solution of The Stationary Fokker-Plank Equation for a Class of Nonlinear Dynamical Systems: An Evaluation Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.16725">http://arxiv.org/abs/2309.16725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussam Alhussein, Mohammed Khasawneh, Mohammed F. Daqaq<br>for:  This paper aims to present a data-free, physics-informed neural network (PINN) framework to solve the Fokker-Planck (FP) equation for a class of nonlinear stochastic dynamical systems.methods: The PINN framework uses a neural network to approximate the solution of the FP equation, without requiring any data from the system.results: The paper demonstrates the ability and accuracy of the PINN framework in predicting the probability density function (PDF) under the combined effect of additive and multiplicative noise, capturing P-bifurcations of the PDF, and effectively treating high-dimensional systems. The computational time associated with the PINN solution can be substantially reduced by using transfer learning.<details>
<summary>Abstract</summary>
The Fokker-Planck (FP) equation is a linear partial differential equation which governs the temporal and spatial evolution of the probability density function (PDF) associated with the response of stochastic dynamical systems. An exact analytical solution of the FP equation is only available for a limited subset of dynamical systems. Semi-analytical methods are available for larger, yet still a small subset of systems, while traditional computational methods; e.g. Finite Elements and Finite Difference require dividing the computational domain into a grid of discrete points, which incurs significant computational costs for high-dimensional systems. Physics-informed learning offers a potentially powerful alternative to traditional computational schemes. To evaluate its potential, we present a data-free, physics-informed neural network (PINN) framework to solve the FP equation for a class of nonlinear stochastic dynamical systems. In particular, through several examples concerning the stochastic response of the Duffing, Van der Pol, and the Duffing-Van der Pol oscillators, we assess the ability and accuracy of the PINN framework in $i)$ predicting the PDF under the combined effect of additive and multiplicative noise, $ii)$ capturing P-bifurcations of the PDF, and $iii)$ effectively treating high-dimensional systems. Through comparisons with Monte-Carlo simulations and the available literature, we show that PINN can effectively address all of the afore-described points. We also demonstrate that the computational time associated with the PINN solution can be substantially reduced by using transfer learning.
</details>
<details>
<summary>摘要</summary>
《福克-普朗克方程》是一个线性偏微分方程，其控制了杂态征函数（PDF）的时间和空间演化，该PDF与杂态动力系统的响应相关。唯一的精确分析解是仅适用于有限个动力系统中。半分析方法可以用于更大的子集，而传统计算方法，如finite element和finite difference，需要将计算Domain分成一个离散点的网格，这会带来高维系统的计算成本很高。物理学 Informed learning提供了一种可能有力的替代方案。为了评估其潜力，我们提出了一个数据自由、物理学 Informed neural network（PINN）框架，用于解决非线性杂态动力系统的福克-普朗克方程。具体来说，通过DUFFING、VAN der POL和DUFFING-VAN der POL振荡器的 einige examples，我们评估了PINN框架在下列方面的能力和准确性：1. 对于添加itive和乘数噪声的PDF预测。2. 捕捉PDF的P-分岔。3. 对高维系统的有效处理。通过与 Monte-Carlo 仿真和已有文献的比较，我们显示了PINN可以有效地解决上述问题。此外，我们还示出了使用传输学习可以将PINN解的计算时间显著减少。
</details></li>
</ul>
<hr>
<h2 id="MultiModN-Multimodal-Multi-Task-Interpretable-Modular-Networks"><a href="#MultiModN-Multimodal-Multi-Task-Interpretable-Modular-Networks" class="headerlink" title="MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks"></a>MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14118">http://arxiv.org/abs/2309.14118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/epfl-iglobalhealth/multimodn">https://github.com/epfl-iglobalhealth/multimodn</a></li>
<li>paper_authors: Vinitra Swamy, Malika Satayeva, Jibril Frej, Thierry Bossy, Thijs Vogels, Martin Jaggi, Tanja Käser, Mary-Anne Hartley</li>
<li>for: 这 paper 的目的是提出一种可靠、多任务、多模态的机器学习模型，能够在不同的模式下进行预测和检测。</li>
<li>methods: 这 paper 使用了一种名为 MultiModN 的多模态、模块化网络，通过序列化多种数据类型的Feature Space来提高预测性能和可解释性。</li>
<li>results:  experiments 表明，MultiModN 在多个 benchmark 数据集上表现出色，能够在不同的模式下进行预测和检测，而且在面临不够数据时并不会出现 catastrophic failure。<details>
<summary>Abstract</summary>
Predicting multiple real-world tasks in a single model often requires a particularly diverse feature space. Multimodal (MM) models aim to extract the synergistic predictive potential of multiple data types to create a shared feature space with aligned semantic meaning across inputs of drastically varying sizes (i.e. images, text, sound). Most current MM architectures fuse these representations in parallel, which not only limits their interpretability but also creates a dependency on modality availability. We present MultiModN, a multimodal, modular network that fuses latent representations in a sequence of any number, combination, or type of modality while providing granular real-time predictive feedback on any number or combination of predictive tasks. MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness. We perform four experiments on several benchmark MM datasets across 10 real-world tasks (predicting medical diagnoses, academic performance, and weather), and show that MultiModN's sequential MM fusion does not compromise performance compared with a baseline of parallel fusion. By simulating the challenging bias of missing not-at-random (MNAR), this work shows that, contrary to MultiModN, parallel fusion baselines erroneously learn MNAR and suffer catastrophic failure when faced with different patterns of MNAR at inference. To the best of our knowledge, this is the first inherently MNAR-resistant approach to MM modeling. In conclusion, MultiModN provides granular insights, robustness, and flexibility without compromising performance.
</details>
<details>
<summary>摘要</summary>
多任务多模式（MM）模型目的是抽取多种数据类型的共同预测潜力，创建具有不同大小和类型的输入数据中共同含义的共享特征空间。大多数当前的MM架构使用平行融合这些表示，不仅限制了它们的可解释性，还受到数据类型可用性的限制。我们介绍了MultiModN，一种多模态、模块化网络，可以在任意数量、组合或类型的模态中融合干ARN表示，并提供精细的实时预测反馈。MultiModN的可组合管道是设计可解释的，同时也是自然多任务和鲁棒于基本问题的偏见缺失。我们在多个MM数据集上进行了四个实验，测试了MultiModN在10个实际任务（预测医疾诊断、学术表现和天气）上的性能。结果显示，MultiModN的顺序MM融合不会Compromise performance相比基线Parallel融合。通过模拟偏见缺失（MNAR）的挑战，这个工作表明，与MultiModN不同，平行融合基elines会在不同的MNAR挑战时erroneously learn MNAR并遭受极端的失败。在我们知道的范围内，这是首个自然具有MNAR抗性的MM模型。因此，MultiModN提供了细化的洞察、鲁棒性和灵活性，无需牺牲性能。
</details></li>
</ul>
<hr>
<h2 id="HyperTrack-Neural-Combinatorics-for-High-Energy-Physics"><a href="#HyperTrack-Neural-Combinatorics-for-High-Energy-Physics" class="headerlink" title="HyperTrack: Neural Combinatorics for High Energy Physics"></a>HyperTrack: Neural Combinatorics for High Energy Physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14113">http://arxiv.org/abs/2309.14113</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mieskolainen/hypertrack">https://github.com/mieskolainen/hypertrack</a></li>
<li>paper_authors: Mikael Mieskolainen</li>
<li>for: 这个论文是为了解决高能物理中的 combinatorial inverse problems 而写的。</li>
<li>methods: 这个论文使用了一种新的深度学习驱动的 clustering 算法，该算法使用了空间时间非本地可学习图构建器、图神经网络和集成变换器。模型通过节点、边和对象层的损失函数进行训练，包括对比学习和元级超级视图。</li>
<li>results: 作者通过 partiicle tracking  simulations 表明了这种前导AI方法的有效性。代码可以在线获取。<details>
<summary>Abstract</summary>
Combinatorial inverse problems in high energy physics span enormous algorithmic challenges. This work presents a new deep learning driven clustering algorithm that utilizes a space-time non-local trainable graph constructor, a graph neural network, and a set transformer. The model is trained with loss functions at the graph node, edge and object level, including contrastive learning and meta-supervision. The algorithm can be applied to problems such as charged particle tracking, calorimetry, pile-up discrimination, jet physics, and beyond. We showcase the effectiveness of this cutting-edge AI approach through particle tracking simulations. The code is available online.
</details>
<details>
<summary>摘要</summary>
高能物理中的 combinatorial inverse problems 涉及到庞大的算法挑战。这项工作提出了一种新的深度学习驱动 clustering 算法，使用空间-时非本地可学习图构建器、图神经网络和集Transformer。该模型通过图节、边和对象层的损失函数进行训练，包括对比学习和超级监督。该算法可以应用于荷电粒子跟踪、calorimetry、堆积排除、jet物理和更多。我们通过粒子跟踪模拟显示了这种前沿人工智能方法的效果。代码可在线获取。
</details></li>
</ul>
<hr>
<h2 id="Affective-Game-Computing-A-Survey"><a href="#Affective-Game-Computing-A-Survey" class="headerlink" title="Affective Game Computing: A Survey"></a>Affective Game Computing: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14104">http://arxiv.org/abs/2309.14104</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Georgios N. Yannakakis, David Melhart</li>
<li>for: 这篇论文探讨了现代情感计算原理、方法和工具在游戏领域的应用，即情感游戏计算。</li>
<li>methods: 论文通过四个核心phasis of the affective loop：游戏情感诱发、游戏情感感知、游戏情感检测和游戏情感适应来进行了评查。</li>
<li>results: 论文提供了情感游戏计算领域的一份综述，并在这一领域中进行了一系列分析和评估。<details>
<summary>Abstract</summary>
This paper surveys the current state of the art in affective computing principles, methods and tools as applied to games. We review this emerging field, namely affective game computing, through the lens of the four core phases of the affective loop: game affect elicitation, game affect sensing, game affect detection and game affect adaptation. In addition, we provide a taxonomy of terms, methods and approaches used across the four phases of the affective game loop and situate the field within this taxonomy. We continue with a comprehensive review of available affect data collection methods with regards to gaming interfaces, sensors, annotation protocols, and available corpora. The paper concludes with a discussion on the current limitations of affective game computing and our vision for the most promising future research directions in the field.
</details>
<details>
<summary>摘要</summary>
We then review available affect data collection methods for gaming interfaces, sensors, annotation protocols, and corpora. Finally, we discuss the current limitations of affective game computing and outline the most promising future research directions in the field.Here is the text in Simplified Chinese:这篇论文介绍了现代情感计算原则、方法和工具在游戏领域的应用。我们通过分析四个核心阶段的情感循环来评估这个新兴领域：游戏情感诱发、游戏情感感知、游戏情感检测和游戏情感适应。此外，我们提供了情感循环中不同阶段的术语、方法和approaches的分类，并将该领域置于这种分类中。接下来，我们进行了有关游戏界面、感知器、注释协议和可用数据集的情感数据收集方法的全面回顾。最后，我们讨论了情感游戏计算的当前局限性，并提出了未来研究的最有前途的方向。
</details></li>
</ul>
<hr>
<h2 id="Tracking-Control-for-a-Spherical-Pendulum-via-Curriculum-Reinforcement-Learning"><a href="#Tracking-Control-for-a-Spherical-Pendulum-via-Curriculum-Reinforcement-Learning" class="headerlink" title="Tracking Control for a Spherical Pendulum via Curriculum Reinforcement Learning"></a>Tracking Control for a Spherical Pendulum via Curriculum Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14096">http://arxiv.org/abs/2309.14096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Klink, Florian Wolf, Kai Ploeger, Jan Peters, Joni Pajarinen</li>
<li>for: 学习非平凡机器人控制法则，不需要手动设计规则。</li>
<li>methods: 使用最新的自动生成课程算法和大规模并行计算，通过改进的优化方案，以更好地识别非欧几何任务结构，从而更快速、更稳定地学习控制器。</li>
<li>results: 学习策略与优化基线相当，在真实系统上达到了类似于最优控制策略的性能。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) allows learning non-trivial robot control laws purely from data. However, many successful applications of RL have relied on ad-hoc regularizations, such as hand-crafted curricula, to regularize the learning performance. In this paper, we pair a recent algorithm for automatically building curricula with RL on massively parallelized simulations to learn a tracking controller for a spherical pendulum on a robotic arm via RL. Through an improved optimization scheme that better respects the non-Euclidean task structure, we allow the method to reliably generate curricula of trajectories to be tracked, resulting in faster and more robust learning compared to an RL baseline that does not exploit this form of structured learning. The learned policy matches the performance of an optimal control baseline on the real system, demonstrating the potential of curriculum RL to jointly learn state estimation and control for non-linear tracking tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Benefit-of-Optimal-Transport-for-Curriculum-Reinforcement-Learning"><a href="#On-the-Benefit-of-Optimal-Transport-for-Curriculum-Reinforcement-Learning" class="headerlink" title="On the Benefit of Optimal Transport for Curriculum Reinforcement Learning"></a>On the Benefit of Optimal Transport for Curriculum Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14091">http://arxiv.org/abs/2309.14091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Klink, Carlo D’Eramo, Jan Peters, Joni Pajarinen</li>
<li>for:  solves complex tasks by generating a tailored sequence of learning tasks</li>
<li>methods: uses interpolations between task distributions to generate curricula</li>
<li>results: improves upon existing CRL methods and achieves high performance in various tasks<details>
<summary>Abstract</summary>
Curriculum reinforcement learning (CRL) allows solving complex tasks by generating a tailored sequence of learning tasks, starting from easy ones and subsequently increasing their difficulty. Although the potential of curricula in RL has been clearly shown in various works, it is less clear how to generate them for a given learning environment, resulting in various methods aiming to automate this task. In this work, we focus on framing curricula as interpolations between task distributions, which has previously been shown to be a viable approach to CRL. Identifying key issues of existing methods, we frame the generation of a curriculum as a constrained optimal transport problem between task distributions. Benchmarks show that this way of curriculum generation can improve upon existing CRL methods, yielding high performance in various tasks with different characteristics.
</details>
<details>
<summary>摘要</summary>
使用简化中文翻译文本。</SYS>学习补充课程（CRL）可以解决复杂任务，通过生成适应性较高的学习任务序列，从易于学习的任务开始，然后逐渐增加Difficulty。虽然CRL的潜力已经在不同的研究中得到了证明，但是如何为给定的学习环境生成课程，还是一个不够清楚的问题，因此有多种方法试图自动化这个任务。在这个工作中，我们将关注将课程框架为 interpolations between task distributions，这种方法在过去已经被证明是CRL的可能的方法。从exististing方法的角度，我们将生成课程的问题定义为constrained optimal transport problem between task distributions。 benchmark表明，这种方法可以超越现有的CRL方法，在不同的任务特点下实现高性能。
</details></li>
</ul>
<hr>
<h2 id="BiSinger-Bilingual-Singing-Voice-Synthesis"><a href="#BiSinger-Bilingual-Singing-Voice-Synthesis" class="headerlink" title="BiSinger: Bilingual Singing Voice Synthesis"></a>BiSinger: Bilingual Singing Voice Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14089">http://arxiv.org/abs/2309.14089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huali Zhou, Yueqian Lin, Yao Shi, Peng Sun, Ming Li</li>
<li>for: 该研究旨在开拓多语言歌唱Synthetic Voice（SVS）领域，提出一种可以同时模拟英语和中文普通话的BiSinger系统。</li>
<li>methods: 该系统使用CMU字典和映射规则实现语言共享表示，并将单语言歌唱数据与开源歌唱voice转换技术相结合，生成双语歌唱声音。</li>
<li>results: 实验表明，BiSinger系统可以在英语和中文普通话之间进行自由融合，同时保持中文歌曲表现。音频样本可以在<a target="_blank" rel="noopener" href="https://bisinger-svs.github.io/">https://bisinger-svs.github.io</a>中找到。<details>
<summary>Abstract</summary>
Although Singing Voice Synthesis (SVS) has made great strides with Text-to-Speech (TTS) techniques, multilingual singing voice modeling remains relatively unexplored. This paper presents BiSinger, a bilingual pop SVS system for English and Chinese Mandarin. Current systems require separate models per language and cannot accurately represent both Chinese and English, hindering code-switch SVS. To address this gap, we design a shared representation between Chinese and English singing voices, achieved by using the CMU dictionary with mapping rules. We fuse monolingual singing datasets with open-source singing voice conversion techniques to generate bilingual singing voices while also exploring the potential use of bilingual speech data. Experiments affirm that our language-independent representation and incorporation of related datasets enable a single model with enhanced performance in English and code-switch SVS while maintaining Chinese song performance. Audio samples are available at https://bisinger-svs.github.io.
</details>
<details>
<summary>摘要</summary>
尽管Singing Voice Synthesis（SVS）已经在文本到语音（TTS）技术方面做出了很大的进步，但多语言歌声模型还尚未得到充分的探索。这篇论文介绍了BiSinger，一个拥有英语和中文普通话的双语PoP SVS系统。现有系统需要separate的模型来处理不同的语言，而这会导致不能准确地表示英语和中文，从而限制了码换SVS。为解决这个难题，我们设计了共享表示 между英语和中文歌声，通过使用CMU字典和映射规则来实现。我们将单语言歌声数据与开源的歌声voice转换技术相结合，以生成双语歌声，同时也在探索使用双语言言言言数据。实验证明了我们的语言独立表示和相关数据的 incorporation 使得单个模型在英语和码换SVS中具有提高的性能，同时保持中文歌曲表现。音频示例可以在https://bisinger-svs.github.io中找到。
</details></li>
</ul>
<hr>
<h2 id="REPA-Client-Clustering-without-Training-and-Data-Labels-for-Improved-Federated-Learning-in-Non-IID-Settings"><a href="#REPA-Client-Clustering-without-Training-and-Data-Labels-for-Improved-Federated-Learning-in-Non-IID-Settings" class="headerlink" title="REPA: Client Clustering without Training and Data Labels for Improved Federated Learning in Non-IID Settings"></a>REPA: Client Clustering without Training and Data Labels for Improved Federated Learning in Non-IID Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14088">http://arxiv.org/abs/2309.14088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Radovič, Veljko Pejović</li>
<li>for: 提高非独立并且同样分布的数据设置下的联合学习（Federated Learning，FL）性能，通过对客户端进行分组，以实现更好的数据分布匹配。</li>
<li>methods: 使用一种新的超级vised autoencoder-based方法，创建不需要本地训练和服务器数据暴露的客户端嵌入，以profile客户端的下游数据生成过程。</li>
<li>results: 对三个不同的数据集进行实验分析，显示REPA可以提供最佳模型性能，同时扩展联合学习的应用范围，覆盖之前未经考虑的用 caso。<details>
<summary>Abstract</summary>
Clustering clients into groups that exhibit relatively homogeneous data distributions represents one of the major means of improving the performance of federated learning (FL) in non-independent and identically distributed (non-IID) data settings. Yet, the applicability of current state-of-the-art approaches remains limited as these approaches cluster clients based on information, such as the evolution of local model parameters, that is only obtainable through actual on-client training. On the other hand, there is a need to make FL models available to clients who are not able to perform the training themselves, as they do not have the processing capabilities required for training, or simply want to use the model without participating in the training. Furthermore, the existing alternative approaches that avert the training still require that individual clients have a sufficient amount of labeled data upon which the clustering is based, essentially assuming that each client is a data annotator. In this paper, we present REPA, an approach to client clustering in non-IID FL settings that requires neither training nor labeled data collection. REPA uses a novel supervised autoencoder-based method to create embeddings that profile a client's underlying data-generating processes without exposing the data to the server and without requiring local training. Our experimental analysis over three different datasets demonstrates that REPA delivers state-of-the-art model performance while expanding the applicability of cluster-based FL to previously uncovered use cases.
</details>
<details>
<summary>摘要</summary>
clustering 客户端到组合显示相对同质数据分布的组合是非独立和同样分布（非-IID）数据设置中提高联邦学习（FL）性能的一种主要方法。然而，现有的现状之前方法的应用范围仍然受限，因为这些方法基于本地模型参数的演化获取信息来分组客户端。在另一方面，有一个需要使联邦学习模型可用于无法进行训练的客户端，因为他们没有训练所需的处理能力或者只想使用模型而不参与训练。此外，现有的备用方法都需要每个客户端都具备足够量的标注数据，即每个客户端都是一名数据注释者。在这篇论文中，我们提出了一种不需要训练也不需要标注数据的客户端分组方法，称为REPA。REPA使用一种新的监督式自动encoder方法来创建嵌入，这些嵌入 profiling客户端的下游数据生成过程，无需服务器暴露数据，也无需本地训练。我们在三个不同的数据集上进行了实验分析，结果表明，REPA可以提供状态之前的模型性能，同时扩大了基于分组的联邦学习的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Diversify-and-Conquer-Bandits-and-Diversity-for-an-Enhanced-E-commerce-Homepage-Experience"><a href="#Diversify-and-Conquer-Bandits-and-Diversity-for-an-Enhanced-E-commerce-Homepage-Experience" class="headerlink" title="Diversify and Conquer: Bandits and Diversity for an Enhanced E-commerce Homepage Experience"></a>Diversify and Conquer: Bandits and Diversity for an Enhanced E-commerce Homepage Experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14046">http://arxiv.org/abs/2309.14046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangeet Jaiswal, Korah T Malayil, Saif Jawaid, Sreekanth Vempati</li>
<li>For: 本研究旨在提高电子商务平台上的推荐广告和产品的效果，具体来说是通过vertical widget reordering来个性化推荐widget。* Methods: 本研究使用了contextual multi-arm bandit问题和增强层来实现个性化推荐。* Results: 通过在Myntra proprietary数据上进行线上和线下A&#x2F;B测试，研究发现该方法可以提高推荐的效果。<details>
<summary>Abstract</summary>
In the realm of e-commerce, popular platforms utilize widgets to recommend advertisements and products to their users. However, the prevalence of mobile device usage on these platforms introduces a unique challenge due to the limited screen real estate available. Consequently, the positioning of relevant widgets becomes pivotal in capturing and maintaining customer engagement. Given the restricted screen size of mobile devices, widgets placed at the top of the interface are more prominently displayed and thus attract greater user attention. Conversely, widgets positioned further down the page require users to scroll, resulting in reduced visibility and subsequent lower impression rates. Therefore it becomes imperative to place relevant widgets on top. However, selecting relevant widgets to display is a challenging task as the widgets can be heterogeneous, widgets can be introduced or removed at any given time from the platform. In this work, we model the vertical widget reordering as a contextual multi-arm bandit problem with delayed batch feedback. The objective is to rank the vertical widgets in a personalized manner. We present a two-stage ranking framework that combines contextual bandits with a diversity layer to improve the overall ranking. We demonstrate its effectiveness through offline and online A/B results, conducted on proprietary data from Myntra, a major fashion e-commerce platform in India.
</details>
<details>
<summary>摘要</summary>
在电商领域中，流行的平台通过Widget来推荐广告和产品给他们的用户。然而，由于移动设备的使用，导致了屏幕空间的限制，这意味着Widget的位置变得非常重要，以确保维持用户的兴趣。由于移动设备的屏幕尺寸有限，位于页面顶部的Widget更加抢耳，因此吸引更多的用户注意力。相反，位于页面底部的Widget需要用户滚动，这会导致它们的可见性减少，并最终导致吸引率下降。因此，需要将相关的Widget放在顶部。然而，选择需要显示的Widget是一项困难的任务，因为Widget可以是不同的，并且可以在任何时候从平台中引入或删除。在这种情况下，我们模型了垂直Widget重新排序为Contextual多臂抽象问题。我们的目标是个性化排序垂直Widget。我们提出了一个两个阶段的排名框架， combinatesContextual bandits with a diversity layer，以提高总体排名的效果。我们通过在Myntra，一家主要的印度电商平台上进行的实验和在线A/B测试，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Imitation-Learning-for-Stochastic-Environments"><a href="#Hierarchical-Imitation-Learning-for-Stochastic-Environments" class="headerlink" title="Hierarchical Imitation Learning for Stochastic Environments"></a>Hierarchical Imitation Learning for Stochastic Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14003">http://arxiv.org/abs/2309.14003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Igl, Punit Shah, Paul Mougin, Sirish Srinivasan, Tarun Gupta, Brandyn White, Kyriacos Shiarlis, Shimon Whiteson</li>
<li>for: 该论文旨在提高自适应学习中代理人的行为模型，以便在训练数据中生成完整的行为分布。</li>
<li>methods: 该论文提出了Robust Type Conditioning（RTC）方法，通过对于随机类型的 adversarial 训练来消除环境噪声导致的分布变换。</li>
<li>results: 实验结果表明，RTC 方法可以提高代理人的行为模型的分布实际性，同时保持或改善任务性能，对于两个领域的大规模实验结果都表现出色。<details>
<summary>Abstract</summary>
Many applications of imitation learning require the agent to generate the full distribution of behaviour observed in the training data. For example, to evaluate the safety of autonomous vehicles in simulation, accurate and diverse behaviour models of other road users are paramount. Existing methods that improve this distributional realism typically rely on hierarchical policies. These condition the policy on types such as goals or personas that give rise to multi-modal behaviour. However, such methods are often inappropriate for stochastic environments where the agent must also react to external factors: because agent types are inferred from the observed future trajectory during training, these environments require that the contributions of internal and external factors to the agent behaviour are disentangled and only internal factors, i.e., those under the agent's control, are encoded in the type. Encoding future information about external factors leads to inappropriate agent reactions during testing, when the future is unknown and types must be drawn independently from the actual future. We formalize this challenge as distribution shift in the conditional distribution of agent types under environmental stochasticity. We propose Robust Type Conditioning (RTC), which eliminates this shift with adversarial training under randomly sampled types. Experiments on two domains, including the large-scale Waymo Open Motion Dataset, show improved distributional realism while maintaining or improving task performance compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)许多学习借风溯行情景中需要智能机器人生成训练数据中全部行为分布的完整分布。例如，评估自动驾驶车辆在模拟环境中的安全性，需要准确且多样化的其他路用者行为模型。现有的方法通常通过层次政策来提高这种分布真实性，这些政策根据目标或人格类型来生成多模样式的行为。然而，这些方法在随机环境中不适用，因为在训练过程中探测到的未来轨迹会影响智能机器人的行为，因此需要在类型编码中分离内部和外部因素的贡献。编码未来环境信息会导致测试时的不合适机器人反应，因为未来是未知的，类型必须从实际未来中独立地采样。我们将这种挑战称为环境随机性导致的类型 conditional distribution 的分布偏移。我们提议Robust Type Conditioning（RTC），通过对随机类型进行对抗训练来消除这种偏移。实验结果表明，RTC在两个领域中提高了分布真实性，同时保持或提高了任务性能，相比于现有的基elines。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Mixtures-of-Discrete-Product-Distributions-in-Near-Optimal-Sample-and-Time-Complexity"><a href="#Identification-of-Mixtures-of-Discrete-Product-Distributions-in-Near-Optimal-Sample-and-Time-Complexity" class="headerlink" title="Identification of Mixtures of Discrete Product Distributions in Near-Optimal Sample and Time Complexity"></a>Identification of Mixtures of Discrete Product Distributions in Near-Optimal Sample and Time Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13993">http://arxiv.org/abs/2309.13993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer L. Gordon, Erik Jahn, Bijan Mazaheri, Yuval Rabani, Leonard J. Schulman</li>
<li>for: 本研究旨在从统计数据中识别一个混合型杂度分布 $X_1,\ldots,X_n$，其中每个变量 $X_i$ 是一个独立的离散随机变量。</li>
<li>methods: 本研究使用了一种类似于 robust tensor decomposition 的方法，并利用了一种新的约束矩阵的condition number bounding方法，称为 Hadamard extensions。</li>
<li>results: 本研究显示，对于任何 $n\geq 2k-1$，可以在 $(1&#x2F;\zeta)^{O(k)}$ 的样本复杂度和时间复杂度下识别混合型杂度分布 $X_1,\ldots,X_n$。此外，我们还扩展了 $e^{\Omega(k)}$ 的下界，使其与我们的上界相匹配，并且这种下界适用于各种不同的 $\zeta$。<details>
<summary>Abstract</summary>
We consider the problem of identifying, from statistics, a distribution of discrete random variables $X_1,\ldots,X_n$ that is a mixture of $k$ product distributions. The best previous sample complexity for $n \in O(k)$ was $(1/\zeta)^{O(k^2 \log k)}$ (under a mild separation assumption parameterized by $\zeta$). The best known lower bound was $\exp(\Omega(k))$. It is known that $n\geq 2k-1$ is necessary and sufficient for identification. We show, for any $n\geq 2k-1$, how to achieve sample complexity and run-time complexity $(1/\zeta)^{O(k)}$. We also extend the known lower bound of $e^{\Omega(k)}$ to match our upper bound across a broad range of $\zeta$. Our results are obtained by combining (a) a classic method for robust tensor decomposition, (b) a novel way of bounding the condition number of key matrices called Hadamard extensions, by studying their action only on flattened rank-1 tensors.
</details>
<details>
<summary>摘要</summary>
我们考虑一个统计方面的问题，即从分布统计中识别 $X_1,\ldots,X_n$ 是一个由 $k$ 个产品分布组成的混合分布。最好的前一个样本复杂性为 $(1/\zeta)^{O(k^2 \log k)}$（受到 $\zeta$ 的宽度假设），而最佳知识下界为 $\exp(\Omega(k))$。我们知道 $n\geq 2k-1$ 是必要和充分的条件。我们示出，对于任何 $n\geq 2k-1$，可以实现样本复杂性和运行时间复杂性 $(1/\zeta)^{O(k)}$。我们还扩展了知识下界，使其与我们的上界在广泛的 $\zeta$ 范围内匹配。我们的结果来自于将（a）纯粹的稳定矩阵分解方法与（b）一种约束矩阵的condition数 bounds的新方法结合在一起，通过研究这些矩阵在扁平 rank-1 张量上的行为来获得 Hadamard 扩展。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective"><a href="#A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective" class="headerlink" title="A Novel Approach for Effective Multi-View Clustering with Information-Theoretic Perspective"></a>A Novel Approach for Effective Multi-View Clustering with Information-Theoretic Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13989">http://arxiv.org/abs/2309.13989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gzcch/A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective-SUMVC">https://github.com/gzcch/A-Novel-Approach-for-Effective-Multi-View-Clustering-with-Information-Theoretic-Perspective-SUMVC</a></li>
<li>paper_authors: Chenhang Cui, Yazhou Ren, Jingyu Pu, Jiawei Li, Xiaorong Pu, Tianyi Wu, Yutao Shi, Lifang He</li>
<li>for: 提高多视图数据 clustering 性能 using various data sources.</li>
<li>methods: 使用 variational analysis 生成一致信息，并提出一种 suficient representation lower bound 来增强一致信息和减少视图中的无用信息.</li>
<li>results: 在理论分析和多个多视图数据集上，SUMVC 方法表现出优于传统方法，提供了一种新的多视图数据分析的视角.<details>
<summary>Abstract</summary>
Multi-view clustering (MVC) is a popular technique for improving clustering performance using various data sources. However, existing methods primarily focus on acquiring consistent information while often neglecting the issue of redundancy across multiple views. This study presents a new approach called Sufficient Multi-View Clustering (SUMVC) that examines the multi-view clustering framework from an information-theoretic standpoint. Our proposed method consists of two parts. Firstly, we develop a simple and reliable multi-view clustering method SCMVC (simple consistent multi-view clustering) that employs variational analysis to generate consistent information. Secondly, we propose a sufficient representation lower bound to enhance consistent information and minimise unnecessary information among views. The proposed SUMVC method offers a promising solution to the problem of multi-view clustering and provides a new perspective for analyzing multi-view data.   To verify the effectiveness of our model, we conducted a theoretical analysis based on the Bayes Error Rate, and experiments on multiple multi-view datasets demonstrate the superior performance of SUMVC.
</details>
<details>
<summary>摘要</summary>
多视图划分（MVC）是一种广泛使用的技术，以提高划分性能使用多种数据源。然而，现有方法主要强调获取一致信息，经常忽略多视图之间的重复性问题。本研究提出了一种新的方法，即足够多视图划分（SUMVC），它从信息论角度探讨多视图划分框架。我们的提议方法包括两部分：首先，我们开发了一种简单可靠的多视图划分方法SCMVC（简单一致多视图划分），使用变量分析生成一致信息。其次，我们提出了一种足够表示下界，以增强一致信息并最小化多视图之间的无用信息。提出的 SUMVC 方法提供了多视图划分问题的有效解决方案，并提供了新的多视图数据分析的视角。为证明我们的模型的有效性，我们基于 bayes 错误率进行了理论分析，并在多个多视图数据集上进行了实验，展示了 SUMVC 的超越性。
</details></li>
</ul>
<hr>
<h2 id="Physics-Driven-ML-Based-Modelling-for-Correcting-Inverse-Estimation"><a href="#Physics-Driven-ML-Based-Modelling-for-Correcting-Inverse-Estimation" class="headerlink" title="Physics-Driven ML-Based Modelling for Correcting Inverse Estimation"></a>Physics-Driven ML-Based Modelling for Correcting Inverse Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13985">http://arxiv.org/abs/2309.13985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruiyuan Kang, Tingting Mu, Panos Liatsis, Dimitrios C. Kyritsis</li>
<li>for: 避免机器学习估算失败，尤其在科学和工程领域（SAE）中，以避免严重的后果，如飞机引擎设计。这项工作关注于探测和修复机器学习估算时的失败状态，通过使用模拟和基于物理法律的性能指标来做到这一点。</li>
<li>methods: 使用模拟和性能指标 guid by physical laws， flag 机器学习估算失败并提出一种新的方法GEESE，通过优化来实现低误差和高效率。GEESE的关键设计包括（1）一种混合模拟错误模型，以减少模拟成本和启用基于错误反馈的梯度循环，以及（2）两种生成模型，用于模拟候选状态的演示和探索行为。所有三个模型均为神经网络。</li>
<li>results: GEESE在三个真实的 SAE 逆问题上进行测试，与一些现有的优化&#x2F;搜索方法进行比较。结果表明，GEESE最少失败次数，并且通常需要物理评估更少次。<details>
<summary>Abstract</summary>
When deploying machine learning estimators in science and engineering (SAE) domains, it is critical to avoid failed estimations that can have disastrous consequences, e.g., in aero engine design. This work focuses on detecting and correcting failed state estimations before adopting them in SAE inverse problems, by utilizing simulations and performance metrics guided by physical laws. We suggest to flag a machine learning estimation when its physical model error exceeds a feasible threshold, and propose a novel approach, GEESE, to correct it through optimization, aiming at delivering both low error and high efficiency. The key designs of GEESE include (1) a hybrid surrogate error model to provide fast error estimations to reduce simulation cost and to enable gradient based backpropagation of error feedback, and (2) two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviours. All three models are constructed as neural networks. GEESE is tested on three real-world SAE inverse problems and compared to a number of state-of-the-art optimization/search approaches. Results show that it fails the least number of times in terms of finding a feasible state correction, and requires physical evaluations less frequently in general.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>A hybrid surrogate error model to provide fast error estimations and reduce simulation cost, enabling gradient-based backpropagation of error feedback.2. Two generative models to approximate the probability distributions of the candidate states for simulating the exploitation and exploration behaviors. All three models are constructed as neural networks.GEESE is tested on three real-world SAE inverse problems and compared to several state-of-the-art optimization&#x2F;search approaches. Results show that it fails the least number of times in terms of finding a feasible state correction, and requires physical evaluations less frequently in general.</details></li>
</ol>
<hr>
<h2 id="Newton-Method-based-Subspace-Support-Vector-Data-Description"><a href="#Newton-Method-based-Subspace-Support-Vector-Data-Description" class="headerlink" title="Newton Method-based Subspace Support Vector Data Description"></a>Newton Method-based Subspace Support Vector Data Description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13960">http://arxiv.org/abs/2309.13960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahad Sohrab, Firas Laakom, Moncef Gabbouj</li>
<li>for: 本文提出了一种基于新トン方法的S-SVDD优化方法，以优化一类分类中的数据映射和描述。</li>
<li>methods: 本文使用了Newton方法来优化数据映射和描述，以提高一类分类中的子空间学习。</li>
<li>results: 实验结果表明，提出的优化策略比 gradient-based S-SVDD 在大多数情况下表现更好。<details>
<summary>Abstract</summary>
In this paper, we present an adaptation of Newton's method for the optimization of Subspace Support Vector Data Description (S-SVDD). The objective of S-SVDD is to map the original data to a subspace optimized for one-class classification, and the iterative optimization process of data mapping and description in S-SVDD relies on gradient descent. However, gradient descent only utilizes first-order information, which may lead to suboptimal results. To address this limitation, we leverage Newton's method to enhance data mapping and data description for an improved optimization of subspace learning-based one-class classification. By incorporating this auxiliary information, Newton's method offers a more efficient strategy for subspace learning in one-class classification as compared to gradient-based optimization. The paper discusses the limitations of gradient descent and the advantages of using Newton's method in subspace learning for one-class classification tasks. We provide both linear and nonlinear formulations of Newton's method-based optimization for S-SVDD. In our experiments, we explored both the minimization and maximization strategies of the objective. The results demonstrate that the proposed optimization strategy outperforms the gradient-based S-SVDD in most cases.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了新顿方法在SVDD（Subspace Support Vector Data Description）优化中的应用。S-SVDD的目标是将原始数据映射到优化一类分类的子空间，而S-SVDD的迭代优化过程中的数据映射和描述逻辑依赖于梯度下降。然而，梯度下降只使用一阶信息，可能会导致优化结果不佳。为了解决这种限制，我们利用新顿方法来增强数据映射和描述，从而提高子空间学习基于一类分类的优化。通过利用这些辅助信息，新顿方法在子空间学习中提供了更高效的优化策略，比梯度下降更有效。文章讨论了梯度下降的局限性和新顿方法在子空间学习中的优势，并提供了线性和非线性形式的新顿方法基于优化方法。我们在实验中探索了最小化和最大化目标的两种策略。结果表明，我们提出的优化策略在大多数情况下超越了梯度下降基于S-SVDD的优化。
</details></li>
</ul>
<hr>
<h2 id="Beam-Enumeration-Probabilistic-Explainability-For-Sample-Efficient-Self-conditioned-Molecular-Design"><a href="#Beam-Enumeration-Probabilistic-Explainability-For-Sample-Efficient-Self-conditioned-Molecular-Design" class="headerlink" title="Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design"></a>Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13957">http://arxiv.org/abs/2309.13957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/schwallergroup/augmented_memory">https://github.com/schwallergroup/augmented_memory</a></li>
<li>paper_authors: Jeff Guo, Philippe Schwaller</li>
<li>for:  This paper aims to improve the explainability and sample efficiency of generative molecular design.</li>
<li>methods: The paper proposes Beam Enumeration, a method that exhaustively enumerates the most probable sub-sequences from language-based molecular generative models and extracts meaningful molecular substructures.</li>
<li>results: The proposed method improves the performance of the recently reported Augmented Memory algorithm, achieving better sample efficiency and generating more high-reward molecules.Here’s the Chinese translation:</li>
<li>for: 本研究旨在提高分子设计的可解释性和样本效率。</li>
<li>methods: 本文提出的方法是Beam Enumeration，它可以对语言基础的分子生成模型中的最可能的子序列进行广泛的枚举，并将分子子结构提取出来。</li>
<li>results: 提议的方法可以提高最近报道的Augmented Memory算法的性能，实现更好的样本效率和产生更多的高质量分子。<details>
<summary>Abstract</summary>
Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration is the first method to jointly address explainability and sample efficiency for molecular design.
</details>
<details>
<summary>摘要</summary>
生成分子设计已经从证明阶段积极应用到实际应用阶段，这是在最近几年的论文中 reporting 实验验证。关键的挑战是解释性和样本效率，这些挑战可以增强生成设计，直接优化昂贵的高精度观测器和提供可行的专业意见。我们提出了 Beam Enumeration，将 exhaustively enumerate 最有可能的子序列从语言基于的分子生成模型，并证明分子结构可以被提取。当与循环学习搭配时，提取的分子结构会具有意义，提供解释性和提高样本效率的自我条件生成。Beam Enumeration 适用于任何语言基于的分子生成模型，并且进一步提高 Augmented Memory 算法的性能，该算法已经在 Practical Molecular Optimization 问题上实现新的顶峰性，具体是 Sample Efficiency。联合算法可以更快地生成更高的奖励分子， givent a fixed oracle budget。Beam Enumeration 是第一个同时解释性和样本效率的分子设计方法。Simplified Chinese:生成分子设计已经从证明阶段普及到实际应用阶段，这是最近几年的论文中 reporting 实验验证。关键的挑战是解释性和样本效率，这些挑战可以增强生成设计，直接优化昂贵的高精度观测器和提供可行的专业意见。我们提出了 Beam Enumeration，将 exhaustively enumerate 最有可能的子序列从语言基于的分子生成模型，并证明分子结构可以被提取。当与循环学习搭配时，提取的分子结构会具有意义，提供解释性和提高样本效率的自我条件生成。Beam Enumeration 适用于任何语言基于的分子生成模型，并且进一步提高 Augmented Memory 算法的性能。这种算法已经在 Practical Molecular Optimization 问题上实现新的顶峰性，具体是 Sample Efficiency。联合算法可以更快地生成更高的奖励分子， givent a fixed oracle budget。Beam Enumeration 是第一个同时解释性和样本效率的分子设计方法。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-the-Heat-Transfer-Control-of-Pulsating-Impinging-Jets"><a href="#Deep-Reinforcement-Learning-for-the-Heat-Transfer-Control-of-Pulsating-Impinging-Jets" class="headerlink" title="Deep Reinforcement Learning for the Heat Transfer Control of Pulsating Impinging Jets"></a>Deep Reinforcement Learning for the Heat Transfer Control of Pulsating Impinging Jets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13955">http://arxiv.org/abs/2309.13955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajad Salavatidezfouli, Giovanni Stabile, Gianluigi Rozza</li>
<li>for: 这个研究探讨了深度强化学习（DRL）在热控制中的应用可能性，通过 Computational Fluid Dynamics 进行研究。</li>
<li>methods: 研究使用了 vanilla Deep Q-Network（DQN）方法进行热控制，并对不同的 DRL 变体进行了比较。</li>
<li>results: 结果表明，soft Double 和 Duel DQN 在所有变体中表现最好，具有高效学习和动作优先级能力。soft Double DQN 超过 hard Double DQN。此外，soft Double 和 Duel 能够在控制周期内维持温度在所需的阈值内超过 98% 的时间。这些发现表明 DRL 在热控制系统中具有扎实的潜力。<details>
<summary>Abstract</summary>
This research study explores the applicability of Deep Reinforcement Learning (DRL) for thermal control based on Computational Fluid Dynamics. To accomplish that, the forced convection on a hot plate prone to a pulsating cooling jet with variable velocity has been investigated. We begin with evaluating the efficiency and viability of a vanilla Deep Q-Network (DQN) method for thermal control. Subsequently, a comprehensive comparison between different variants of DRL is conducted. Soft Double and Duel DQN achieved better thermal control performance among all the variants due to their efficient learning and action prioritization capabilities. Results demonstrate that the soft Double DQN outperforms the hard Double DQN. Moreover, soft Double and Duel can maintain the temperature in the desired threshold for more than 98% of the control cycle. These findings demonstrate the promising potential of DRL in effectively addressing thermal control systems.
</details>
<details>
<summary>摘要</summary>
这个研究项目探讨了深度强化学习（DRL）在计算流体动力学中的应用性。为了实现这一目标，我们 investigate了一种受到脉冲冷风的热板，其中冷风速度是可变的。我们首先评估了普通的深度Q网络（DQN）方法的效率和可行性。然后，我们进行了不同变体的DRL比较。 results indicate that soft Double DQN和Duel DQN在所有变体中表现最佳，它们具有高效学习和动作优先级能力。此外，soft Double DQN超过了hard Double DQN。此外，soft Double和Duel可以在控制ecycle中维持温度在所需的阈值上超过98%的时间。这些发现表明DRL在thermal控制系统中具有扎实的潜力。
</details></li>
</ul>
<hr>
<h2 id="Local-and-Global-Trend-Bayesian-Exponential-Smoothing-Models"><a href="#Local-and-Global-Trend-Bayesian-Exponential-Smoothing-Models" class="headerlink" title="Local and Global Trend Bayesian Exponential Smoothing Models"></a>Local and Global Trend Bayesian Exponential Smoothing Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13950">http://arxiv.org/abs/2309.13950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Slawek Smyl, Christoph Bergmeir, Alexander Dokumentov, Erwin Wibowo, Daniel Schmidt</li>
<li>for: 本研究旨在探讨一种基于加法和乘法均摊满足的季节性和非季节性时间序列模型，以满足快速增长、波动性时间序列的需求。</li>
<li>methods: 本研究使用现代抽象贝叶斯适应技术来开发这种模型，并在M3竞赛数据集上应用。</li>
<li>results: 比较其他竞赛算法和参照值，本研究在M3竞赛数据集上得到了最佳的结果，从而在Literature中实现了最佳单variate方法的result。<details>
<summary>Abstract</summary>
This paper describes a family of seasonal and non-seasonal time series models that can be viewed as generalisations of additive and multiplicative exponential smoothing models. Their development is motivated by fast-growing, volatile time series, and facilitated by state-of-the-art Bayesian fitting techniques. When applied to the M3 competition data set, they outperform the best algorithms in the competition as well as other benchmarks, thus achieving to the best of our knowledge the best results of univariate methods on this dataset in the literature.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Characterising-User-Transfer-Amid-Industrial-Resource-Variation-A-Bayesian-Nonparametric-Approach"><a href="#Characterising-User-Transfer-Amid-Industrial-Resource-Variation-A-Bayesian-Nonparametric-Approach" class="headerlink" title="Characterising User Transfer Amid Industrial Resource Variation: A Bayesian Nonparametric Approach"></a>Characterising User Transfer Amid Industrial Resource Variation: A Bayesian Nonparametric Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13949">http://arxiv.org/abs/2309.13949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongxu Lei, Xiaotian Lin, Xinghu Yu, Zhan Li, Weichao Sun, Jianbin Qiu, Songlin Zhuang, Huijun Gao</li>
<li>for: 本研究旨在提高资源管理策略的发展，通过准确描述用户负载传递的 macro 级模式。</li>
<li>methods: 本研究提出了一种可解释的 hierarchical Bayesian nonparametric 模型 CLUSTER，可自动确定用户群和资源变化对用户传递的影响。</li>
<li>results: 实验结果表明，CLUSTER 模型能够准确预测用户传递响应资源变化，并且能够Quantify uncertainty for reliable decision-making。此外，CLUSTER 模型能够独立地函数于个人可 identificable 信息，保护用户隐私。<details>
<summary>Abstract</summary>
In a multitude of industrial fields, a key objective entails optimising resource management whilst satisfying user requirements. Resource management by industrial practitioners can result in a passive transfer of user loads across resource providers, a phenomenon whose accurate characterisation is both challenging and crucial. This research reveals the existence of user clusters, which capture macro-level user transfer patterns amid resource variation. We then propose CLUSTER, an interpretable hierarchical Bayesian nonparametric model capable of automating cluster identification, and thereby predicting user transfer in response to resource variation. Furthermore, CLUSTER facilitates uncertainty quantification for further reliable decision-making. Our method enables privacy protection by functioning independently of personally identifiable information. Experiments with simulated and real-world data from the communications industry reveal a pronounced alignment between prediction results and empirical observations across a spectrum of resource management scenarios. This research establishes a solid groundwork for advancing resource management strategy development.
</details>
<details>
<summary>摘要</summary>
在多种工业领域中，一个关键目标是优化资源管理，同时满足用户需求。但是资源管理的实践可能导致用户负载的投递式传输，这种现象的准确描述是非常困难和重要。这项研究发现用户群，这些群体捕捉了资源变化下的用户传输模式。我们提议CLUSTER，一种可解释性强的树状贝叶拟合模型，能够自动确定用户群和用户传输响应资源变化。此外，CLUSTER还可以对不确定性进行评估，以便更加可靠地做出决策。我们的方法可以保护用户隐私，不需要个人可识别信息。实验表明，CLUSTER在通信业中的仿真数据和实际数据上具有杰出的一致性，在资源管理方案的多种场景中具有广泛的应用前景。这项研究为资源管理策略的发展提供了坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="Provable-Training-for-Graph-Contrastive-Learning"><a href="#Provable-Training-for-Graph-Contrastive-Learning" class="headerlink" title="Provable Training for Graph Contrastive Learning"></a>Provable Training for Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13944">http://arxiv.org/abs/2309.13944</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/voidharuhi/pot-gcl">https://github.com/voidharuhi/pot-gcl</a></li>
<li>paper_authors: Yue Yu, Xiao Wang, Mengmei Zhang, Nian Liu, Chuan Shi</li>
<li>for: 本研究旨在解决 Graph Contrastive Learning (GCL) 训练中存在的不均衡问题，提高 GCL 的性能和可靠性。</li>
<li>methods: 本研究使用了实验证明 GCL 训练是不均衡的，并提出了一个名为 “node compactness” 的度量来衡量每个节点是否遵循 GCL 原理。此外，本研究还提出了一种名为 PrOvable Training (POT) 的训练方法，通过在 GCL 训练中添加 regularization 来增强 GCL 的性能。</li>
<li>results: 通过对多个 benchmark 进行了广泛的实验，本研究发现 POT 可以一直提高 GCL 的性能，并且可以作为一个可靠的插件使用。<details>
<summary>Abstract</summary>
Graph Contrastive Learning (GCL) has emerged as a popular training approach for learning node embeddings from augmented graphs without labels. Despite the key principle that maximizing the similarity between positive node pairs while minimizing it between negative node pairs is well established, some fundamental problems are still unclear. Considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? Or are there some nodes more likely to be untrained across graph augmentations and violate the principle? How to distinguish these nodes and further guide the training of GCL? To answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. To address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. We further derive the form of node compactness theoretically through bound propagation, which can be integrated into binary cross-entropy as a regularization. To this end, we propose the PrOvable Training (POT) for GCL, which regularizes the training of GCL to encode node embeddings that follows the GCL principle better. Through extensive experiments on various benchmarks, POT consistently improves the existing GCL approaches, serving as a friendly plugin.
</details>
<details>
<summary>摘要</summary>
graph contrastive learning (GCL) 已经成为无标签学习节点嵌入的受欢迎训练方法。 despite the well-established key principle of maximizing the similarity between positive node pairs while minimizing it between negative node pairs, some fundamental problems are still unclear. considering the complex graph structure, are some nodes consistently well-trained and following this principle even with different graph augmentations? or are there some nodes more likely to be untrained across graph augmentations and violate the principle? how to distinguish these nodes and further guide the training of GCL?to answer these questions, we first present experimental evidence showing that the training of GCL is indeed imbalanced across all nodes. to address this problem, we propose the metric "node compactness", which is the lower bound of how a node follows the GCL principle related to the range of augmentations. we further derive the form of node compactness theoretically through bound propagation, which can be integrated into binary cross-entropy as a regularization. to this end, we propose the PrOvable Training (POT) for GCL, which regularizes the training of GCL to encode node embeddings that follows the GCL principle better. through extensive experiments on various benchmarks, POT consistently improves the existing GCL approaches, serving as a friendly plugin.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Classification-Systems-Against-Soft-Labels-with-Fuzzy-Precision-and-Recall"><a href="#Evaluating-Classification-Systems-Against-Soft-Labels-with-Fuzzy-Precision-and-Recall" class="headerlink" title="Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall"></a>Evaluating Classification Systems Against Soft Labels with Fuzzy Precision and Recall</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13938">http://arxiv.org/abs/2309.13938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manu Harju, Annamaria Mesaros</li>
<li>for: 本研究旨在提出一种新的精度和准确率计算方法，以便在使用非二进制参考标签时评估音 Event detection系统的性能。</li>
<li>methods: 本研究使用了Kullback-Leibler divergence来衡量系统是否能够准确地遵循数据，并提出了一种基于非二进制参考标签的精度和准确率计算方法。</li>
<li>results: 研究发现，使用提议的计算方法可以准确地评估音 Event detection系统的性能，并且可以避免因数据二进制化而导致的错误解释。<details>
<summary>Abstract</summary>
Classification systems are normally trained by minimizing the cross-entropy between system outputs and reference labels, which makes the Kullback-Leibler divergence a natural choice for measuring how closely the system can follow the data. Precision and recall provide another perspective for measuring the performance of a classification system. Non-binary references can arise from various sources, and it is often beneficial to use the soft labels for training instead of the binarized data. However, the existing definitions for precision and recall require binary reference labels, and binarizing the data can cause erroneous interpretations. We present a novel method to calculate precision, recall and F-score without quantizing the data. The proposed metrics extend the well established metrics as the definitions coincide when used with binary labels. To understand the behavior of the metrics we show simple example cases and an evaluation of different sound event detection models trained on real data with soft labels.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Classification systems are normally trained by minimizing the cross-entropy between system outputs and reference labels, which makes the Kullback-Leibler divergence a natural choice for measuring how closely the system can follow the data. Precision and recall provide another perspective for measuring the performance of a classification system. Non-binary references can arise from various sources, and it is often beneficial to use the soft labels for training instead of the binarized data. However, the existing definitions for precision and recall require binary reference labels, and binarizing the data can cause erroneous interpretations. We present a novel method to calculate precision, recall and F-score without quantizing the data. The proposed metrics extend the well established metrics as the definitions coincide when used with binary labels. To understand the behavior of the metrics we show simple example cases and an evaluation of different sound event detection models trained on real data with soft labels." into Simplified Chinese.翻译文本为Simplified Chinese：通常，分类系统通过最小化系统输出与参考标签之间的cross-entropy来训练，这使得庒啄-利卜征函数成为衡量系统如何准确地跟踪数据的自然选择。精度和回归提供了另一种视角来衡量分类系统的性能。非二进制参考可以从多种来源 arise，并且在训练时使用软标签可以是有利的。然而，现有的精度和回归定义都需要二进制参考标签，并且将数据二进制化可能会导致错误的解释。我们提出了一种新的方法来计算精度、回归和F-score，而无需将数据二进制化。我们的 metric 扩展了现有的 metric，因为在使用二进制标签时，定义协调。为了理解metric的行为，我们给出了简单的例子cases和使用实际数据和软标签训练不同的音Event检测模型的评估。
</details></li>
</ul>
<hr>
<h2 id="SAMN-A-Sample-Attention-Memory-Network-Combining-SVM-and-NN-in-One-Architecture"><a href="#SAMN-A-Sample-Attention-Memory-Network-Combining-SVM-and-NN-in-One-Architecture" class="headerlink" title="SAMN: A Sample Attention Memory Network Combining SVM and NN in One Architecture"></a>SAMN: A Sample Attention Memory Network Combining SVM and NN in One Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13930">http://arxiv.org/abs/2309.13930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiaoling Yang, Linkai Luo, Haoyu Zhang, Hong Peng, Ziyang Chen</li>
<li>for: This paper aims to combine Support Vector Machines (SVM) and Neural Networks (NN) to create a more powerful function for multi-classification tasks.</li>
<li>methods: The proposed method, called Sample Attention Memory Network (SAMN), incorporates a sample attention module, class prototypes, and a memory block into NN to effectively combine SVM and NN.</li>
<li>results: Extensive experiments show that SAMN achieves better classification performance than single SVM or single NN with similar parameter sizes, as well as the previous best model for combining SVM and NN.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是将支持向量机(SVM)和神经网络(NN)结合起来，以创造更强大的多类分类器。</li>
<li>methods: 提议的方法是将样本注意力模块、类型评估模块和记忆块 incorporated into NN，以实现SVM和NN的有效结合。</li>
<li>results: 广泛的实验表明，SAMN比单个 SVM 或单个 NN 相同参数大小下的性能更好，以及之前最佳结合 SVM 和 NN 的模型。<details>
<summary>Abstract</summary>
Support vector machine (SVM) and neural networks (NN) have strong complementarity. SVM focuses on the inner operation among samples while NN focuses on the operation among the features within samples. Thus, it is promising and attractive to combine SVM and NN, as it may provide a more powerful function than SVM or NN alone. However, current work on combining them lacks true integration. To address this, we propose a sample attention memory network (SAMN) that effectively combines SVM and NN by incorporating sample attention module, class prototypes, and memory block to NN. SVM can be viewed as a sample attention machine. It allows us to add a sample attention module to NN to implement the main function of SVM. Class prototypes are representatives of all classes, which can be viewed as alternatives to support vectors. The memory block is used for the storage and update of class prototypes. Class prototypes and memory block effectively reduce the computational cost of sample attention and make SAMN suitable for multi-classification tasks. Extensive experiments show that SAMN achieves better classification performance than single SVM or single NN with similar parameter sizes, as well as the previous best model for combining SVM and NN. The sample attention mechanism is a flexible module that can be easily deepened and incorporated into neural networks that require it.
</details>
<details>
<summary>摘要</summary>
支持向量机(SVM)和神经网络(NN)具有强大的补做性。SVM关注样本之间的内部运算，而NN关注样本中特征之间的运算。因此，将SVM和NN结合起来可能提供一个更强大的函数，而不需要增加参数量。然而，现有的SVM和NN结合方法缺乏真正的集成。为此，我们提议一种叫做样本注意力储存网络(SAMN)，它有效地结合了SVM和NN。SVM可以看作是一种样本注意机器。它允许我们将样本注意模块添加到NN中，以实现SVM的主要功能。类型范例是所有类型的代表，它们可以看作是支持向量的替代品。储存块用于存储和更新类型范例。类型范例和储存块可以有效减少样本注意的计算成本，使SAMN适用于多类分类任务。广泛的实验表明，SAMN在类比单独使用SVM或NN时，达到了更好的分类性能，同时也比前一个最佳结合SVM和NN的模型更好。样本注意机制是一种灵活的模块，可以轻松地深入 incorporated into neural networks 中，当需要时。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Label-Selection-is-a-Decision-Problem"><a href="#Pseudo-Label-Selection-is-a-Decision-Problem" class="headerlink" title="Pseudo Label Selection is a Decision Problem"></a>Pseudo Label Selection is a Decision Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13926">http://arxiv.org/abs/2309.13926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aditya-rathi/Credit-Score-">https://github.com/aditya-rathi/Credit-Score-</a></li>
<li>paper_authors: Julian Rodemann</li>
<li>for: 这个论文的目的是提出一种基于决策理论的pseudo-label选择（PLS）方法，以解决confirmation bias问题。</li>
<li>methods: 这个方法基于一种新的选择 criterion，即 Pseudo posterior predictive的分析性approximation，这个分析性approximation是基于 Bayes-optimality 的。</li>
<li>results: 在模拟和实际数据上，BPLS方法在面临过拟合和confirmation bias问题时表现出优于传统的 PLS 方法。此外，这个方法还可以使得 PLS 更加鲁棒地对待模型假设。<details>
<summary>Abstract</summary>
Pseudo-Labeling is a simple and effective approach to semi-supervised learning. It requires criteria that guide the selection of pseudo-labeled data. The latter have been shown to crucially affect pseudo-labeling's generalization performance. Several such criteria exist and were proven to work reasonably well in practice. However, their performance often depends on the initial model fit on labeled data. Early overfitting can be propagated to the final model by choosing instances with overconfident but wrong predictions, often called confirmation bias. In two recent works, we demonstrate that pseudo-label selection (PLS) can be naturally embedded into decision theory. This paves the way for BPLS, a Bayesian framework for PLS that mitigates the issue of confirmation bias. At its heart is a novel selection criterion: an analytical approximation of the posterior predictive of pseudo-samples and labeled data. We derive this selection criterion by proving Bayes-optimality of this "pseudo posterior predictive". We empirically assess BPLS for generalized linear, non-parametric generalized additive models and Bayesian neural networks on simulated and real-world data. When faced with data prone to overfitting and thus a high chance of confirmation bias, BPLS outperforms traditional PLS methods. The decision-theoretic embedding further allows us to render PLS more robust towards the involved modeling assumptions. To achieve this goal, we introduce a multi-objective utility function. We demonstrate that the latter can be constructed to account for different sources of uncertainty and explore three examples: model selection, accumulation of errors and covariate shift.
</details>
<details>
<summary>摘要</summary>
假标注是一种简单而有效的 semi-supervised learning 方法。它需要一些导向选择假标注数据的标准。这些标准有助于减少假标注的泛化性能。然而，它们的性能通常取决于初始模型适应 labels 数据。早期过度适应可能会传递到最终模型，通常被称为 confirmation bias。在两篇最近的论文中，我们展示了 pseudo-label 选择（PLS）可以自然地被嵌入到决策理论中。这种方法可以减轻 confirmation bias 的问题。PLS 的核心是一个新的选择标准：一种 Analytical 预测 pseudo-samples 和标注数据的 posterior 预测。我们 derive 这个选择标准通过证明 Bayes-优化这个 "pseudo posterior predictive"。我们在 simulated 和实际数据上进行了 Empirical 评估，发现在面临数据泛化和高度可能性 confirmation bias 的情况下，BPLS 比传统的 PLS 方法更好。决策理论的嵌入还使得 PLS 更加抗性待命模型假设。为了实现这一目标，我们引入了一个多目标价值函数。我们示例了这个价值函数可以考虑不同的不确定性来源，并 explore 三个例子：模型选择、积累错误和变量转移。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Neural-Policy-Mirror-Descent-for-Policy-Optimization-on-Low-Dimensional-Manifolds"><a href="#Sample-Complexity-of-Neural-Policy-Mirror-Descent-for-Policy-Optimization-on-Low-Dimensional-Manifolds" class="headerlink" title="Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds"></a>Sample Complexity of Neural Policy Mirror Descent for Policy Optimization on Low-Dimensional Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13915">http://arxiv.org/abs/2309.13915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghao Xu, Xiang Ji, Minshuo Chen, Mengdi Wang, Tuo Zhao</li>
<li>for: 本文研究了使用神经网络的策略算法在强化学习中解决高维策略优化问题。</li>
<li>methods: 本文使用了神经网络作为策略和价值函数的函数近似器，并研究了NPMD算法的样本复杂性。</li>
<li>results: 研究发现，NPMD算法可以在高维策略优化问题中减轻维度着色问题，并可以在有限样本下找到$\epsilon$-优的策略，其样本复杂性为$\widetilde{O}(\epsilon^{-{\frac{d}{\alpha}-2})$。<details>
<summary>Abstract</summary>
Policy-based algorithms equipped with deep neural networks have achieved great success in solving high-dimensional policy optimization problems in reinforcement learning. However, current analyses cannot explain why they are resistant to the curse of dimensionality. In this work, we study the sample complexity of the neural policy mirror descent (NPMD) algorithm with convolutional neural networks (CNN) as function approximators. Motivated by the empirical observation that many high-dimensional environments have state spaces possessing low-dimensional structures, such as those taking images as states, we consider the state space to be a $d$-dimensional manifold embedded in the $D$-dimensional Euclidean space with intrinsic dimension $d\ll D$. We show that in each iteration of NPMD, both the value function and the policy can be well approximated by CNNs. The approximation errors are controlled by the size of the networks, and the smoothness of the previous networks can be inherited. As a result, by properly choosing the network size and hyperparameters, NPMD can find an $\epsilon$-optimal policy with $\widetilde{O}(\epsilon^{-\frac{d}{\alpha}-2})$ samples in expectation, where $\alpha\in(0,1]$ indicates the smoothness of environment. Compared to previous work, our result exhibits that NPMD can leverage the low-dimensional structure of state space to escape from the curse of dimensionality, providing an explanation for the efficacy of deep policy-based algorithms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Matrix-Factorization-in-Tropical-and-Mixed-Tropical-Linear-Algebras"><a href="#Matrix-Factorization-in-Tropical-and-Mixed-Tropical-Linear-Algebras" class="headerlink" title="Matrix Factorization in Tropical and Mixed Tropical-Linear Algebras"></a>Matrix Factorization in Tropical and Mixed Tropical-Linear Algebras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13914">http://arxiv.org/abs/2309.13914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Kordonis, Emmanouil Theodosis, George Retsinas, Petros Maragos</li>
<li>for:  Matrix Factorization (MF) 为机器学习和数据探索中的应用，包括共同推荐系统、维度缩减、数据可视化和社群探测。</li>
<li>methods: 我们使用тропікалgebra和几何学来研究两个问题，包括 Tropical Matrix Factorization (TMF) 和一个新的组合矩阵分解问题。</li>
<li>results: 我们提出了一个改进的TMF算法，可以避免许多地方最佳解；此外，我们还提出了一个新的组合矩阵分解方法，具有与多个用户学习 utility 函数的 interessante解释。我们还呈现了一些数据，证明我们的方法的有效性，并实现了一个推荐系统的应用，获得了显著的结果。<details>
<summary>Abstract</summary>
Matrix Factorization (MF) has found numerous applications in Machine Learning and Data Mining, including collaborative filtering recommendation systems, dimensionality reduction, data visualization, and community detection. Motivated by the recent successes of tropical algebra and geometry in machine learning, we investigate two problems involving matrix factorization over the tropical algebra. For the first problem, Tropical Matrix Factorization (TMF), which has been studied already in the literature, we propose an improved algorithm that avoids many of the local optima. The second formulation considers the approximate decomposition of a given matrix into the product of three matrices where a usual matrix product is followed by a tropical product. This formulation has a very interesting interpretation in terms of the learning of the utility functions of multiple users. We also present numerical results illustrating the effectiveness of the proposed algorithms, as well as an application to recommendation systems with promising results.
</details>
<details>
<summary>摘要</summary>
矩阵因式（MF）在机器学习和数据挖掘中找到了许多应用，包括共享推荐系统、维度减少、数据可视化和社区检测。受推荐系统的最近成功而受欢迎的泛洋算术和几何，我们 investigate two 矩阵因式问题，其中一个是已经在文献中研究的极地矩阵因式（TMF），我们提出了一种改进的算法，可以避免许多地方最佳点。第二个形式是对给定矩阵的approximate decompositions into the product of three matrices，其中一个是 usual matrix product followed by a tropical product。这个形式有非常有趣的学习多个用户的实用函数的解释。我们还present numerical results demonstrating the effectiveness of the proposed algorithms, as well as an application to recommendation systems with promising results。
</details></li>
</ul>
<hr>
<h2 id="Follow-ups-Also-Matter-Improving-Contextual-Bandits-via-Post-serving-Contexts"><a href="#Follow-ups-Also-Matter-Improving-Contextual-Bandits-via-Post-serving-Contexts" class="headerlink" title="Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts"></a>Follow-ups Also Matter: Improving Contextual Bandits via Post-serving Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13896">http://arxiv.org/abs/2309.13896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoqi Wang, Ziyu Ye, Zhe Feng, Ashwinkumar Badanidiyuru, Haifeng Xu</li>
<li>for: 提高在线学习效率，解决 contectual bandit problem 中 valuable 的后服务上下文信息不可见的问题。</li>
<li>methods: 提出了一种新的contextual bandit problem 模型，利用后服务上下文信息进行学习，并设计了一种新的算法poLinUCB，可以在标准假设下实现紧凑的征逐 regret。</li>
<li>results: 对 synthetic 和实际数据进行了广泛的实验测试，证明了利用后服务上下文信息可以提高学习效率，以及poLinUCB 算法的综合性和可靠性。<details>
<summary>Abstract</summary>
Standard contextual bandit problem assumes that all the relevant contexts are observed before the algorithm chooses an arm. This modeling paradigm, while useful, often falls short when dealing with problems in which valuable additional context can be observed after arm selection. For example, content recommendation platforms like Youtube, Instagram, Tiktok also observe valuable follow-up information pertinent to the user's reward after recommendation (e.g., how long the user stayed, what is the user's watch speed, etc.). To improve online learning efficiency in these applications, we study a novel contextual bandit problem with post-serving contexts and design a new algorithm, poLinUCB, that achieves tight regret under standard assumptions. Core to our technical proof is a robustified and generalized version of the well-known Elliptical Potential Lemma (EPL), which can accommodate noise in data. Such robustification is necessary for tackling our problem, and we believe it could also be of general interest. Extensive empirical tests on both synthetic and real-world datasets demonstrate the significant benefit of utilizing post-serving contexts as well as the superior performance of our algorithm over the state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
通常的上下文抽剂问题假设所有相关的上下文都可以在算法选择器之前观察。这种模型虽然有用，但在处理不可见上下文的问题时，它经常失足。例如，内容推荐平台 like Youtube、Instagram 和 Tiktok 可以在推荐后观察有价值的用户奖励信息（例如用户停留时间、用户播放速度等）。为了在这些应用中提高在线学习效率，我们研究了一种新的上下文抽剂问题，即 post-serving 上下文，并设计了一个新的算法 poLinUCB。我们的技术证明基于一种更加稳健和泛化的 Elliptical Potential Lemma (EPL)，可以承受数据噪声。这种稳健性是我们问题的必要条件，并且我们认为这也可能对总体有利。我们的实验表明，利用 post-serving 上下文和我们的算法的优秀性，可以在实验室和实际数据上达到显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Graph-Representation-Learning-Towards-Patents-Network-Analysis"><a href="#Graph-Representation-Learning-Towards-Patents-Network-Analysis" class="headerlink" title="Graph Representation Learning Towards Patents Network Analysis"></a>Graph Representation Learning Towards Patents Network Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13888">http://arxiv.org/abs/2309.13888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Mohammad Heydari, Babak Teimourpour</li>
<li>for: 本研究使用graph representation learning方法分析了伊朗官方公报中的专利数据，以找出相似性和新领域。</li>
<li>methods: 研究使用自然语言处理和实体解析技术提取了专利记录中的关键实体，然后将其转换为伊朗专利图граffe。</li>
<li>results: 研究结果显示，通过使用Graph representation learning和文本挖掘技术，可以实现专利资料分析和探索新领域，并且可以预防重复申请专利、熟悉相似和相连的发明、了解法律实体支持专利和研究人员在特定领域的知识。<details>
<summary>Abstract</summary>
Patent analysis has recently been recognized as a powerful technique for large companies worldwide to lend them insight into the age of competition among various industries. This technique is considered a shortcut for developing countries since it can significantly accelerate their technology development. Therefore, as an inevitable process, patent analysis can be utilized to monitor rival companies and diverse industries. This research employed a graph representation learning approach to create, analyze, and find similarities in the patent data registered in the Iranian Official Gazette. The patent records were scrapped and wrangled through the Iranian Official Gazette portal. Afterward, the key entities were extracted from the scrapped patents dataset to create the Iranian patents graph from scratch based on novel natural language processing and entity resolution techniques. Finally, thanks to the utilization of novel graph algorithms and text mining methods, we identified new areas of industry and research from Iranian patent data, which can be used extensively to prevent duplicate patents, familiarity with similar and connected inventions, Awareness of legal entities supporting patents and knowledge of researchers and linked stakeholders in a particular research field.
</details>
<details>
<summary>摘要</summary>
具有广泛应用前景的专利分析技术已经在全球范围内被大型公司广泛应用，以帮助这些公司更好地了解不同行业的竞争情况。这种技术被视为发展中国家的短cut，因为它可以快速加速技术发展。因此，通过监测竞争对手和多个行业的专利分析，这种技术可以帮助公司更好地了解自己的市场环境。本研究采用了图表学习方法来创建、分析和找出专利数据库中的相似性。我们从伊朗官方报纸网站上抓取了专利笔记，然后使用新的自然语言处理和实体解决技术提取了关键实体。最后，我们通过使用新的图算法和文本挖掘技术，在伊朗专利数据中找到了新的行业和研究领域，这些领域可以用于避免重复专利、熟悉相似和相连的发明、了解法定机构支持专利、了解研究人员和相关的投资者在特定研究领域的知识。
</details></li>
</ul>
<hr>
<h2 id="Can-Class-Priors-Help-Single-Positive-Multi-Label-Learning"><a href="#Can-Class-Priors-Help-Single-Positive-Multi-Label-Learning" class="headerlink" title="Can Class-Priors Help Single-Positive Multi-Label Learning?"></a>Can Class-Priors Help Single-Positive Multi-Label Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13886">http://arxiv.org/abs/2309.13886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biao Liu, Jie Wang, Ning Xu, Xin Geng</li>
<li>for:  solves the problem of single-positive multi-label learning (SPMLL) with class-prior differences in real-world scenarios.</li>
<li>methods:  proposes a novel framework called Class-pRiors Induced Single-Positive multi-label learning, which includes a class-priors estimator and an unbiased risk estimator for classification.</li>
<li>results:  experiments on ten MLL benchmark datasets demonstrate the effectiveness and superiority of the proposed method over existing SPMLL approaches.<details>
<summary>Abstract</summary>
Single-positive multi-label learning (SPMLL) is a typical weakly supervised multi-label learning problem, where each training example is annotated with only one positive label. Existing SPMLL methods typically assign pseudo-labels to unannotated labels with the assumption that prior probabilities of all classes are identical. However, the class-prior of each category may differ significantly in real-world scenarios, which makes the predictive model not perform as well as expected due to the unrealistic assumption on real-world application. To alleviate this issue, a novel framework named {\proposed}, i.e., Class-pRiors Induced Single-Positive multi-label learning, is proposed. Specifically, a class-priors estimator is introduced, which could estimate the class-priors that are theoretically guaranteed to converge to the ground-truth class-priors. In addition, based on the estimated class-priors, an unbiased risk estimator for classification is derived, and the corresponding risk minimizer could be guaranteed to approximately converge to the optimal risk minimizer on fully supervised data. Experimental results on ten MLL benchmark datasets demonstrate the effectiveness and superiority of our method over existing SPMLL approaches.
</details>
<details>
<summary>摘要</summary>
单正向多标签学习（SPMLL）是一种 Typical weakly supervised multi-label learning 问题，每个训练示例只有一个正确标签。现有的 SPMLL 方法通常将 pseudo-labels 赋给未标记的标签，假设所有类别的先验概率相同。然而，实际应用中每个类别的类别先验可能很大不同，这会使 predictive 模型不能如预期那样表现，因为这是不真实的假设。为解决这个问题，我们提出了一个新的框架，即 Class-pRiors Induced Single-Positive multi-label learning，简称为 \proposed。 Specifically, 我们引入了一个类别先验估计器，可以估算类别先验，并且这些估计器 theoretically guaranteed to converge to the ground-truth class-priors。此外，基于估计器，我们 derive 了一个不偏的风险估计器 для 分类，并且可以 guarantee that the corresponding risk minimizer could approximately converge to the optimal risk minimizer on fully supervised data。实验结果表明，我们的方法在十个 MLL  benchmark 数据集上表现出色，比现有的 SPMLL 方法更有效。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Treatment-Effects-Under-Heterogeneous-Interference"><a href="#Estimating-Treatment-Effects-Under-Heterogeneous-Interference" class="headerlink" title="Estimating Treatment Effects Under Heterogeneous Interference"></a>Estimating Treatment Effects Under Heterogeneous Interference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13884">http://arxiv.org/abs/2309.13884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/linxf208/hinite">https://github.com/linxf208/hinite</a></li>
<li>paper_authors: Xiaofeng Lin, Guoxi Zhang, Xiaotian Lu, Han Bao, Koh Takeuchi, Hisashi Kashima</li>
<li>for: The paper is written for estimating individual treatment effects (ITEs) in the presence of interference, specifically in online applications where units are associated and interference can be heterogeneous.</li>
<li>methods: The paper proposes a novel approach to model heterogeneous interference by developing a new architecture that aggregates information from diverse neighbors, using graph neural networks, a mechanism to aggregate information from different views, and attention mechanisms.</li>
<li>results: The proposed method significantly outperforms existing methods for ITE estimation in experiments on multiple datasets with heterogeneous interference, confirming the importance of modeling heterogeneous interference.<details>
<summary>Abstract</summary>
Treatment effect estimation can assist in effective decision-making in e-commerce, medicine, and education. One popular application of this estimation lies in the prediction of the impact of a treatment (e.g., a promotion) on an outcome (e.g., sales) of a particular unit (e.g., an item), known as the individual treatment effect (ITE). In many online applications, the outcome of a unit can be affected by the treatments of other units, as units are often associated, which is referred to as interference. For example, on an online shopping website, sales of an item will be influenced by an advertisement of its co-purchased item. Prior studies have attempted to model interference to estimate the ITE accurately, but they often assume a homogeneous interference, i.e., relationships between units only have a single view. However, in real-world applications, interference may be heterogeneous, with multi-view relationships. For instance, the sale of an item is usually affected by the treatment of its co-purchased and co-viewed items. We hypothesize that ITE estimation will be inaccurate if this heterogeneous interference is not properly modeled. Therefore, we propose a novel approach to model heterogeneous interference by developing a new architecture to aggregate information from diverse neighbors. Our proposed method contains graph neural networks that aggregate same-view information, a mechanism that aggregates information from different views, and attention mechanisms. In our experiments on multiple datasets with heterogeneous interference, the proposed method significantly outperforms existing methods for ITE estimation, confirming the importance of modeling heterogeneous interference.
</details>
<details>
<summary>摘要</summary>
干预效果估计可以帮助在电商、医疗和教育等领域进行有效的决策。一种受欢迎的应用之一是估计干预（例如推广）对单元（例如商品）的结果（例如销售）的影响，known as 个体干预效果（ITE）。在许多在线应用程序中，单元的结果可能受到其他单元的干预，这是因为单元经常相关，这被称为干扰。例如，在一个在线购物网站上，一个商品的销售会受到其推广的影响。先前的研究尝试了模型干扰，以便准确地估计ITE，但它们通常假设了同质干扰，即单元之间只有一种视角。然而，在实际应用中，干扰可能是多质，即单元之间有多种视角。例如，一个商品的销售通常受到其推广和浏览的影响。我们认为，如果不正确地模型多质干扰，ITE估计就会不准确。因此，我们提出了一种新的方法，用于模型多质干扰。我们的提议方法包括图 neural networks 来聚合同视角信息，一种机制来聚合不同视角信息，以及注意机制。在我们对多个数据集上进行的实验中，我们的提议方法在存在多质干扰的情况下显著超过了现有的ITE估计方法，确认了模型多质干扰的重要性。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Conditional-Expectation-Model-for-Efficient-and-Robust-Target-Speech-Extraction"><a href="#Diffusion-Conditional-Expectation-Model-for-Efficient-and-Robust-Target-Speech-Extraction" class="headerlink" title="Diffusion Conditional Expectation Model for Efficient and Robust Target Speech Extraction"></a>Diffusion Conditional Expectation Model for Efficient and Robust Target Speech Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13874">http://arxiv.org/abs/2309.13874</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vivian556123/dcem">https://github.com/vivian556123/dcem</a></li>
<li>paper_authors: Leying Zhang, Yao Qian, Linfeng Yu, Heming Wang, Xinkai Wang, Hemin Yang, Long Zhou, Shujie Liu, Yanmin Qian, Michael Zeng</li>
<li>for: 本文旨在提出一种高效的生成方法，用于 Target Speech Extraction (TSE)。</li>
<li>methods: 本文使用了Diffusion Conditional Expectation Model (DCEM)，可以处理多 speaker和单 speaker场景，并且可以在各种噪音和清晰Condition下进行处理。</li>
<li>results:  comparing with传统方法，本文的方法在非侵入和侵入 metric 中表现出色，并且具有高效的推理速度和对未看到任务的稳定性。Audio例子可以在线上预览（<a target="_blank" rel="noopener" href="https://vivian556123.github.io/dcem%EF%BC%89%E3%80%82">https://vivian556123.github.io/dcem）。</a><details>
<summary>Abstract</summary>
Target Speech Extraction (TSE) is a crucial task in speech processing that focuses on isolating the clean speech of a specific speaker from complex mixtures. While discriminative methods are commonly used for TSE, they can introduce distortion in terms of speech perception quality. On the other hand, generative approaches, particularly diffusion-based methods, can enhance speech quality perceptually but suffer from slower inference speed. We propose an efficient generative approach named Diffusion Conditional Expectation Model (DCEM) for TSE. It can handle multi- and single-speaker scenarios in both noisy and clean conditions. Additionally, we introduce Regenerate-DCEM (R-DCEM) that can regenerate and optimize speech quality based on pre-processed speech from a discriminative model. Our method outperforms conventional methods in terms of both intrusive and non-intrusive metrics and demonstrates notable strengths in inference efficiency and robustness to unseen tasks. Audio examples are available online (https://vivian556123.github.io/dcem).
</details>
<details>
<summary>摘要</summary>
target speech extraction (tse)是speech processing中关键的任务，它的目标是从复杂的混合中分离出清晰的speaker的speech。although discriminative methods are commonly used for tse, they can introduce distortion in terms of speech perception quality. on the other hand, generative approaches, particularly diffusion-based methods, can enhance speech quality perceptually but suffer from slower inference speed. we propose an efficient generative approach named diffusion conditional expectation model (dcem) for tse. it can handle multi- and single-speaker scenarios in both noisy and clean conditions. additionally, we introduce regenerate-dcem (r-dcem) that can regenerate and optimize speech quality based on pre-processed speech from a discriminative model. our method outperforms conventional methods in terms of both intrusive and non-intrusive metrics and demonstrates notable strengths in inference efficiency and robustness to unseen tasks. audio examples are available online (https://vivian556123.github.io/dcem).Here's the translation breakdown:* target speech extraction (tse) = 目标语音采样 (tse)* speech processing = 语音处理* discriminative methods = 分类方法* generative approaches = 生成方法* diffusion-based methods = 扩散基于方法* Diffusion Conditional Expectation Model (DCEM) = 扩散 conditional expectation model (DCEM)* Regenerate-DCEM (R-DCEM) = 重新生成-DCEM (R-DCEM)* pre-processed speech = 预处理的语音* inference efficiency = 推理效率* robustness to unseen tasks = 对未经见任务的Robustness* audio examples = 音频示例Please note that the translation is done in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Statistical-Perspective-of-Top-K-Sparse-Softmax-Gating-Mixture-of-Experts"><a href="#Statistical-Perspective-of-Top-K-Sparse-Softmax-Gating-Mixture-of-Experts" class="headerlink" title="Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts"></a>Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13850">http://arxiv.org/abs/2309.13850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huy Nguyen, Pedram Akbarian, Fanqi Yan, Nhat Ho</li>
<li>for: 这篇论文主要针对的问题是解释权重补做的顶层-$K$ sparse softmax权重函数对输入空间的分割和深度学习模型的性能的影响。</li>
<li>methods: 作者使用了 Gaussian mixture of experts 来设计一个简单的模型，并通过定义新的损失函数来捕捉输入空间不同区域的行为。</li>
<li>results: 研究发现，当知道真实的专家数 $k_{\ast}$ 时，随着样本大小增加，权重补做的散度和参数估计的速度都是 Parametric。但是，当真实模型超出了 $k_{\ast}$ 的情况下，选择从顶层-$K$ sparse softmax权重函数中的专家数量必须大于某些 Voronoi 细胞与真实参数之间的总体 cardinality，以确保权重补做的散度估计 converge。此外，虽然散度估计的速度仍然是 Parametric，但参数估计速度受到权重补做和专家函数之间的内在交互的影响，导致很慢。<details>
<summary>Abstract</summary>
Top-K sparse softmax gating mixture of experts has been widely used for scaling up massive deep-learning architectures without increasing the computational cost. Despite its popularity in real-world applications, the theoretical understanding of that gating function has remained an open problem. The main challenge comes from the structure of the top-K sparse softmax gating function, which partitions the input space into multiple regions with distinct behaviors. By focusing on a Gaussian mixture of experts, we establish theoretical results on the effects of the top-K sparse softmax gating function on both density and parameter estimations. Our results hinge upon defining novel loss functions among parameters to capture different behaviors of the input regions. When the true number of experts $k_{\ast}$ is known, we demonstrate that the convergence rates of density and parameter estimations are both parametric on the sample size. However, when $k_{\ast}$ becomes unknown and the true model is over-specified by a Gaussian mixture of $k$ experts where $k > k_{\ast}$, our findings suggest that the number of experts selected from the top-K sparse softmax gating function must exceed the total cardinality of a certain number of Voronoi cells associated with the true parameters to guarantee the convergence of the density estimation. Moreover, while the density estimation rate remains parametric under this setting, the parameter estimation rates become substantially slow due to an intrinsic interaction between the softmax gating and expert functions.
</details>
<details>
<summary>摘要</summary>
Top-K 稀疏软max权重混合专家已经广泛应用于扩大深度学习架构而无需增加计算成本。尽管在实际应用中它非常受欢迎，但是其理论理解仍然是一个开放的问题。主要挑战在于 top-K 稀疏软max权重混合函数的结构，该函数将输入空间分成多个区域，每个区域具有不同的行为。通过关注 Gaussian mixture of experts，我们建立了关于 top-K 稀疏软max权重混合函数对输入空间的影响的理论结果。我们的结论基于定义新的损失函数来捕捉不同区域的输入行为。当真实的专家数量 $k_{\ast}$ 知道时，我们证明随样本大小的散度和参数估计的渐近率都是参数的。然而，当 $k_{\ast}$ 不知道，真实的模型被过度规定为 Gaussian mixture of $k$ 专家，其中 $k > k_{\ast}$，我们发现，为保证散度估计的渐近，从 top-K 稀疏软max权重混合函数中选择的专家数量必须大于真实参数的总 cardinality。此外，虽然散度估计率保持参数的，但参数估计率因软max权重和专家函数之间的内在互动而变得非常慢。
</details></li>
</ul>
<hr>
<h2 id="On-the-Effectiveness-of-Adversarial-Samples-against-Ensemble-Learning-based-Windows-PE-Malware-Detectors"><a href="#On-the-Effectiveness-of-Adversarial-Samples-against-Ensemble-Learning-based-Windows-PE-Malware-Detectors" class="headerlink" title="On the Effectiveness of Adversarial Samples against Ensemble Learning-based Windows PE Malware Detectors"></a>On the Effectiveness of Adversarial Samples against Ensemble Learning-based Windows PE Malware Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13841">http://arxiv.org/abs/2309.13841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trong-Nghia To, Danh Le Kim, Do Thi Thu Hien, Nghi Hoang Khoa, Hien Do Hoang, Phan The Duy, Van-Hau Pham<br>for:这个研究的目的是提出一个组合GAN和RL模型的变异系统，以抵消基于集成学习的检测器。methods:研究使用了GAN和RL模型，包括MalGAN和Deep Q-network anti-malware Engines Attacking Framework (DQEAF)。results:实验结果显示，100%的选择的异常样品保留了可执行档案的格式，而在执行可能性和黑客性方面也有一定的成功。<details>
<summary>Abstract</summary>
Recently, there has been a growing focus and interest in applying machine learning (ML) to the field of cybersecurity, particularly in malware detection and prevention. Several research works on malware analysis have been proposed, offering promising results for both academic and practical applications. In these works, the use of Generative Adversarial Networks (GANs) or Reinforcement Learning (RL) can aid malware creators in crafting metamorphic malware that evades antivirus software. In this study, we propose a mutation system to counteract ensemble learning-based detectors by combining GANs and an RL model, overcoming the limitations of the MalGAN model. Our proposed FeaGAN model is built based on MalGAN by incorporating an RL model called the Deep Q-network anti-malware Engines Attacking Framework (DQEAF). The RL model addresses three key challenges in performing adversarial attacks on Windows Portable Executable malware, including format preservation, executability preservation, and maliciousness preservation. In the FeaGAN model, ensemble learning is utilized to enhance the malware detector's evasion ability, with the generated adversarial patterns. The experimental results demonstrate that 100\% of the selected mutant samples preserve the format of executable files, while certain successes in both executability preservation and maliciousness preservation are achieved, reaching a stable success rate.
</details>
<details>
<summary>摘要</summary>
近些年来，机器学习（ML）在计算机安全领域的应用得到了越来越多的关注和兴趣，特别是在恶意软件检测和防范方面。一些关于恶意软件分析的研究工作已经提出，其中使用生成对抗网络（GANs）或强化学习（RL）可以帮助恶意软件创作者制作形态变化的恶意软件，从而躲避反恶意软件检测。在本研究中，我们提出了一种突变系统，用于对集成学习基于检测器的攻击者进行对抗。我们的提出的FeaGAN模型基于MalGAN模型，通过加入一个RL模型called Deep Q-network anti-malware Engines Attacking Framework（DQEAF），解决了对Windows Portable Executable恶意软件的三大挑战，包括格式保留、可执行性保留和害意保留。在FeaGAN模型中， ensemble learning被使用来增强恶意软件检测器的逃脱能力，通过生成的对抗模式。实验结果表明，100%的选择的突变样本保留了可执行文件的格式，而在可执行性和害意方面也有一定的成功，达到了稳定的成功率。
</details></li>
</ul>
<hr>
<h2 id="Penalized-Principal-Component-Analysis-using-Nesterov-Smoothing"><a href="#Penalized-Principal-Component-Analysis-using-Nesterov-Smoothing" class="headerlink" title="Penalized Principal Component Analysis using Nesterov Smoothing"></a>Penalized Principal Component Analysis using Nesterov Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13838">http://arxiv.org/abs/2309.13838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca M. Hurwitz, Georg Hahn</li>
<li>for: 本文使用权重约束最小化方法（PEP）来缩短高维数据中的维度，并添加L1偏导函数约束。</li>
<li>methods: 本文提出了一种使用馈积抑制（Nesterov smoothing）来计算LASSO-类L1偏导函数的分布式优化方法，并使用已有的单值分解（SVD）结果来计算高阶特征向量。</li>
<li>results: 使用1000个基因计划数据集，我们实验ally示出了使用我们提议的精炼PEP可以提高数值稳定性并获得有意义的特征向量。我们还 investigate了对传统PCA的约束最小化方法的比较。<details>
<summary>Abstract</summary>
Principal components computed via PCA (principal component analysis) are traditionally used to reduce dimensionality in genomic data or to correct for population stratification. In this paper, we explore the penalized eigenvalue problem (PEP) which reformulates the computation of the first eigenvector as an optimization problem and adds an L1 penalty constraint. The contribution of our article is threefold. First, we extend PEP by applying Nesterov smoothing to the original LASSO-type L1 penalty. This allows one to compute analytical gradients which enable faster and more efficient minimization of the objective function associated with the optimization problem. Second, we demonstrate how higher order eigenvectors can be calculated with PEP using established results from singular value decomposition (SVD). Third, using data from the 1000 Genome Project dataset, we empirically demonstrate that our proposed smoothed PEP allows one to increase numerical stability and obtain meaningful eigenvectors. We further investigate the utility of the penalized eigenvector approach over traditional PCA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用主成分分析（PCA）计算主成分是传统地用于降维度的 genomic 数据或是对人口分布进行修正。在这篇论文中，我们探讨了增加 penalty 的 eigenvalue 问题（PEP），它将计算第一个 eigenvector 转换为优化问题，并添加 L1 罚项限制。我们的贡献有三个方面：首先，我们扩展了 PEP 方法，通过应用 Nesterov 缓和法来计算 analytical 导数，从而更快地和更有效地解决优化问题中关联的目标函数。第二，我们使用已有的 singular value decomposition（SVD）结果来计算高级别的 eigenvectors。第三，使用 1000 Genome Project 数据集，我们实际示出了我们提议的平滑 PEP 可以增加数值稳定性并获得有意义的 eigenvectors。我们进一步调查了使用增加 penalty 的 eigenvector 方法与传统 PCA 方法的优劣。
</details></li>
</ul>
<hr>
<h2 id="Backorder-Prediction-in-Inventory-Management-Classification-Techniques-and-Cost-Considerations"><a href="#Backorder-Prediction-in-Inventory-Management-Classification-Techniques-and-Cost-Considerations" class="headerlink" title="Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations"></a>Backorder Prediction in Inventory Management: Classification Techniques and Cost Considerations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13837">http://arxiv.org/abs/2309.13837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarit Maitra, Sukanya Kundu</li>
<li>for: 预测库存缺失(backorder)管理</li>
<li>methods: 使用多种分类技术，包括平衡携带分类器、柔logic、变分自适应网络-生成对抗网络、多层感知器等，对不同的数据集进行评估，并考虑财务因素和缺失成本。</li>
<li>results: 结果表明，结合模型方法，包括集成技术和VAE，可以有效地处理不均衡数据集，提高预测精度，减少假阳性和假阴性，并增加可 interpretability。<details>
<summary>Abstract</summary>
This article introduces an advanced analytical approach for predicting backorders in inventory management. Backorder refers to an order that cannot be immediately fulfilled due to stock depletion. Multiple classification techniques, including Balanced Bagging Classifiers, Fuzzy Logic, Variational Autoencoder - Generative Adversarial Networks, and Multi-layer Perceptron classifiers, are assessed in this work using performance evaluation metrics such as ROC-AUC and PR-AUC. Moreover, this work incorporates a profit function and misclassification costs, considering the financial implications and costs associated with inventory management and backorder handling. The study suggests that a combination of modeling approaches, including ensemble techniques and VAE, can effectively address imbalanced datasets in inventory management, emphasizing interpretability and reducing false positives and false negatives. This research contributes to the advancement of predictive analytics and offers valuable insights for future investigations in backorder forecasting and inventory control optimization for decision-making.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇文章介绍了一种高级分析方法，用于预测库存欠货（backorder）管理中的库存异常情况。这种方法包括多种分类技术，如均衡搅拌分类器、杂化逻辑、变量自动编码-生成敌对网络、多层感知器等，并使用表现评估指标，如ROC-AUC和PR-AUC来评估其表现。此外，这种研究还考虑了财务因素和误分类成本，包括库存管理和欠货处理中的财务影响和成本。研究表明，结合不同的模型方法，包括ensemble技术和VAE，可以有效地处理库存管理中的偏度数据，提高预测精度和减少false阳和false降。这项研究对预测分析领域的发展做出了贡献，并为未来的库存预测和库存控制优化做出了有价值的着想。
</details></li>
</ul>
<hr>
<h2 id="NSOTree-Neural-Survival-Oblique-Tree"><a href="#NSOTree-Neural-Survival-Oblique-Tree" class="headerlink" title="NSOTree: Neural Survival Oblique Tree"></a>NSOTree: Neural Survival Oblique Tree</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13825">http://arxiv.org/abs/2309.13825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xs018/NSOTree">https://github.com/xs018/NSOTree</a></li>
<li>paper_authors: Xiaotong Sun, Peijie Qiu</li>
<li>for: 这篇论文探讨了Survival分析领域中的时间至事件（Time-to-Event）数据，以及将深度学习方法应用到这个领域中，以提高表现和可解性。</li>
<li>methods: 这篇论文提出了一个名为Neural Survival Oblique Tree（NSOTree）的新方法，它结合了深度学习和树型方法，以维持可解性和表现力。NSOTree 基于 ReLU 网络，并且可以与现有的存生模型集成在一起，以便应用。</li>
<li>results: 论文的评估结果显示，NSOTree 能够在实际数据上实现高性能和可解性，并且在存生领域中提供了一个可靠的方法。<details>
<summary>Abstract</summary>
Survival analysis is a statistical method employed to scrutinize the duration until a specific event of interest transpires, known as time-to-event information characterized by censorship. Recently, deep learning-based methods have dominated this field due to their representational capacity and state-of-the-art performance. However, the black-box nature of the deep neural network hinders its interpretability, which is desired in real-world survival applications but has been largely neglected by previous works. In contrast, conventional tree-based methods are advantageous with respect to interpretability, while consistently grappling with an inability to approximate the global optima due to greedy expansion. In this paper, we leverage the strengths of both neural networks and tree-based methods, capitalizing on their ability to approximate intricate functions while maintaining interpretability. To this end, we propose a Neural Survival Oblique Tree (NSOTree) for survival analysis. Specifically, the NSOTree was derived from the ReLU network and can be easily incorporated into existing survival models in a plug-and-play fashion. Evaluations on both simulated and real survival datasets demonstrated the effectiveness of the proposed method in terms of performance and interpretability.
</details>
<details>
<summary>摘要</summary>
生存分析是一种统计方法，用于检查一个特定事件的发生时间，称为时间至事件信息，受到限制。最近，深度学习基于方法在这一领域占据主导地位，因为它们具有表达能力和现代性。然而，深度神经网络的黑盒特性阻碍了其解释性，这在实际生存应用中是极其重要的，但之前的工作却忽略了这一点。相比之下，传统的树状方法具有解释性的优势，但它们难以近似全局最优解。在这篇论文中，我们利用神经网络和树状方法的优点，同时维持解释性。为此，我们提出了神经生存斜树（NSOTree）方法。具体来说，NSOTree是基于ReLU网络的，可以轻松地与现有的生存模型集成。我们对实际和 simulations 数据进行了评估，并证明了我们提出的方法在性能和解释性两个方面具有效果。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-large-collections-of-time-series-feature-based-methods"><a href="#Forecasting-large-collections-of-time-series-feature-based-methods" class="headerlink" title="Forecasting large collections of time series: feature-based methods"></a>Forecasting large collections of time series: feature-based methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13807">http://arxiv.org/abs/2309.13807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lixixibj/forecasting-with-time-series-imaging">https://github.com/lixixibj/forecasting-with-time-series-imaging</a></li>
<li>paper_authors: Li Li, Feng Li, Yanfei Kang</li>
<li>for: 这篇论文主要针对 econometrics 和其他预测领域中的复杂实际问题，即时间序列数据的复杂性使得单一模型不能涵盖所有数据生成过程。</li>
<li>methods: 这篇论文介绍了基于时间序列特征的两种方法来预测大量时间序列数据，即特征基于的模型选择和特征基于的模型组合。</li>
<li>results: 论文详细介绍了现场 откры源软件实现的状态 искусственного预测方法，包括基于时间序列特征的模型选择和模型组合。<details>
<summary>Abstract</summary>
In economics and many other forecasting domains, the real world problems are too complex for a single model that assumes a specific data generation process. The forecasting performance of different methods changes depending on the nature of the time series. When forecasting large collections of time series, two lines of approaches have been developed using time series features, namely feature-based model selection and feature-based model combination. This chapter discusses the state-of-the-art feature-based methods, with reference to open-source software implementations.
</details>
<details>
<summary>摘要</summary>
在经济和许多其他预测领域中，现实世界问题太复杂，不可以单独采用一个模型，假设特定的数据生成过程。预测不同时序系列的表现，不同方法的预测性能会有所不同。当预测大量时序系列时，有两条方向的方法得到发展，一是基于时序特征的模型选择，二是基于时序特征的模型组合。本章介绍了当前最佳实践的特征基于方法，参考开源软件实现。
</details></li>
</ul>
<hr>
<h2 id="Projected-Randomized-Smoothing-for-Certified-Adversarial-Robustness"><a href="#Projected-Randomized-Smoothing-for-Certified-Adversarial-Robustness" class="headerlink" title="Projected Randomized Smoothing for Certified Adversarial Robustness"></a>Projected Randomized Smoothing for Certified Adversarial Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13794">http://arxiv.org/abs/2309.13794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spfrommer/projected_randomized_smoothing">https://github.com/spfrommer/projected_randomized_smoothing</a></li>
<li>paper_authors: Samuel Pfrommer, Brendon G. Anderson, Somayeh Sojoudi</li>
<li>for: 提供可证明 robustness 的分类器设计</li>
<li>methods: 使用随机填充方法在低维投影空间中进行随机缓和，并 characterize 缓和后的证明区域</li>
<li>results: 对 CIFAR-10 和 SVHN 进行实验，表明我们的方法可以提供 tractable 的下界，并且在证明区域内捕捉到普通扰动的perturbationsHere is the same information in Simplified Chinese:</li>
<li>for: 设计可证明 robustness 的分类器</li>
<li>methods: 使用随机填充方法在低维投影空间中进行随机缓和，并 characterize 缓和后的证明区域</li>
<li>results: 对 CIFAR-10 和 SVHN 进行实验，表明我们的方法可以提供 tractable 的下界，并且在证明区域内捕捉到普通扰动的perturbations<details>
<summary>Abstract</summary>
Randomized smoothing is the current state-of-the-art method for producing provably robust classifiers. While randomized smoothing typically yields robust $\ell_2$-ball certificates, recent research has generalized provable robustness to different norm balls as well as anisotropic regions. This work considers a classifier architecture that first projects onto a low-dimensional approximation of the data manifold and then applies a standard classifier. By performing randomized smoothing in the low-dimensional projected space, we characterize the certified region of our smoothed composite classifier back in the high-dimensional input space and prove a tractable lower bound on its volume. We show experimentally on CIFAR-10 and SVHN that classifiers without the initial projection are vulnerable to perturbations that are normal to the data manifold and yet are captured by the certified regions of our method. We compare the volume of our certified regions against various baselines and show that our method improves on the state-of-the-art by many orders of magnitude.
</details>
<details>
<summary>摘要</summary>
随机缓和是当前状态艺术方法，生成可证明抗干扰的分类器。通常情况下，随机缓和会生成 $\ell_2$-球证明，但最近的研究已经推广了不同 norm 球证明以及不规则区域。这个工作考虑一种分类器架构，首先将数据投影到低维度的数据投影空间，然后应用标准分类器。通过在低维度投影空间中进行随机缓和，我们Characterize了我们熔化 composite 分类器的证明区域，并证明了可读取的下界。我们在 CIFAR-10 和 SVHN 上进行实验，并证明了不包含初始投影的分类器容易受到数据投影方向的攻击，但是我们的方法可以捕捉这些攻击。我们比较了我们的证明区域的体积与各种基准值，并证明了我们的方法在状态艺术中提高了多个阶段的质量。
</details></li>
</ul>
<hr>
<h2 id="ReMasker-Imputing-Tabular-Data-with-Masked-Autoencoding"><a href="#ReMasker-Imputing-Tabular-Data-with-Masked-Autoencoding" class="headerlink" title="ReMasker: Imputing Tabular Data with Masked Autoencoding"></a>ReMasker: Imputing Tabular Data with Masked Autoencoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13793">http://arxiv.org/abs/2309.13793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tydusky/remasker">https://github.com/tydusky/remasker</a></li>
<li>paper_authors: Tianyu Du, Luca Melis, Ting Wang</li>
<li>for: 这个论文是为了推算缺失数据的 tabular 数据填充方法。</li>
<li>methods: 这个方法是基于 masked autoencoding 框架的扩展，其中 besides 缺失数据（即自然缺失），还随机 “重新mask” 一些值，通过优化 autoencoder 来重建这些重新mask 的值，并将模型应用于预测缺失数据。</li>
<li>results: 与优秀的方法进行比较，我们在多种缺失设定下进行了广泛的评估，并显示了 ReMasker 在缺失率不同的情况下的性能都在或超过了现有方法，而且其性能优势通常随缺失数据的比例增长。此外，我们还进行了理论准确性的探讨，并证明 ReMasker 通常学习缺失数据不变的表示。<details>
<summary>Abstract</summary>
We present ReMasker, a new method of imputing missing values in tabular data by extending the masked autoencoding framework. Compared with prior work, ReMasker is both simple -- besides the missing values (i.e., naturally masked), we randomly ``re-mask'' another set of values, optimize the autoencoder by reconstructing this re-masked set, and apply the trained model to predict the missing values; and effective -- with extensive evaluation on benchmark datasets, we show that ReMasker performs on par with or outperforms state-of-the-art methods in terms of both imputation fidelity and utility under various missingness settings, while its performance advantage often increases with the ratio of missing data. We further explore theoretical justification for its effectiveness, showing that ReMasker tends to learn missingness-invariant representations of tabular data. Our findings indicate that masked modeling represents a promising direction for further research on tabular data imputation. The code is publicly available.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法 called ReMasker，用于填充缺失数据的表格数据中的缺失值。与之前的工作相比，ReMasker 更简单，只有缺失值（即自然缺失）之外，我们随机“重新覆盖”另一个集合的值，然后优化自动编码器，重建这个重新覆盖集合，并使用训练模型预测缺失值。与此同时，我们还进行了广泛的评估，发现 ReMasker 在不同的缺失设定下，与或超过现有方法的稳定性和实用性。此外，我们还进行了理论 justify 其效果，发现 ReMasker 倾向于学习缺失性 invariable 的表格数据表示。我们的发现表明，masked modeling 是一个有前途的研究方向。代码公开 available。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Free-Statistical-Dispersion-Control-for-Societal-Applications"><a href="#Distribution-Free-Statistical-Dispersion-Control-for-Societal-Applications" class="headerlink" title="Distribution-Free Statistical Dispersion Control for Societal Applications"></a>Distribution-Free Statistical Dispersion Control for Societal Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13786">http://arxiv.org/abs/2309.13786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhun Deng, Thomas P. Zollo, Jake C. Snell, Toniann Pitassi, Richard Zemel</li>
<li>for: 这个论文主要是为了提供有关机器学习模型性能的证明，以确保机器学习模型在实际应用中的性能是否符合预期。</li>
<li>methods: 这篇论文使用了一种简单 yet 灵活的框架，可以处理更加复杂的统计函数。这种框架使用了分布自由的方法，以控制不同人群的统计分布。</li>
<li>results: 该论文通过实验表明，该方法可以在恶意评论检测、医疗影像和电影推荐等领域中提供精确的统计保证。<details>
<summary>Abstract</summary>
Explicit finite-sample statistical guarantees on model performance are an important ingredient in responsible machine learning. Previous work has focused mainly on bounding either the expected loss of a predictor or the probability that an individual prediction will incur a loss value in a specified range. However, for many high-stakes applications, it is crucial to understand and control the dispersion of a loss distribution, or the extent to which different members of a population experience unequal effects of algorithmic decisions. We initiate the study of distribution-free control of statistical dispersion measures with societal implications and propose a simple yet flexible framework that allows us to handle a much richer class of statistical functionals beyond previous work. Our methods are verified through experiments in toxic comment detection, medical imaging, and film recommendation.
</details>
<details>
<summary>摘要</summary>
具有具体 finite-sample 统计保证的机器学习是责任感知的重要组成部分。先前的工作主要关注在预测器的预期损失下的约束或者预测结果会在指定范围内带来损失值的概率上。然而，许多高度投资应用中，控制统计分布的偏差是关键，也就是不同人群受到机器决策的不同影响程度。我们开始研究不含统计分布的控制方法，并提出了简单 yet flexible 的框架，可以处理更加复杂的统计函数。我们的方法通过对毒评排除、医疗成像和电影推荐进行实验验证。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-For-Reduced-Popularity-Bias-In-Multi-Territory-Video-Recommendations"><a href="#Multi-Task-Learning-For-Reduced-Popularity-Bias-In-Multi-Territory-Video-Recommendations" class="headerlink" title="Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations"></a>Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03148">http://arxiv.org/abs/2310.03148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phanideep Gampa, Farnoosh Javadi, Belhassen Bayar, Ainur Yessenalina</li>
<li>for: 提高多区域个性化推荐系统中 item 的准确率，解决 globally prevalent item 的偏袋问题。</li>
<li>methods: 使用多任务学习 (MTL) 技术，并采用适应性增 sampling 方法来减少 popularity bias。</li>
<li>results: 通过实验，我们 demonstarte 了我们的框架在多区域比基eline 表现出较好的效果，PR-AUC 指标中的增幅可达 $65.27%$。<details>
<summary>Abstract</summary>
Various data imbalances that naturally arise in a multi-territory personalized recommender system can lead to a significant item bias for globally prevalent items. A locally popular item can be overshadowed by a globally prevalent item. Moreover, users' viewership patterns/statistics can drastically change from one geographic location to another which may suggest to learn specific user embeddings. In this paper, we propose a multi-task learning (MTL) technique, along with an adaptive upsampling method to reduce popularity bias in multi-territory recommendations. Our proposed framework is designed to enrich training examples with active users representation through upsampling, and capable of learning geographic-based user embeddings by leveraging MTL. Through experiments, we demonstrate the effectiveness of our framework in multiple territories compared to a baseline not incorporating our proposed techniques.~Noticeably, we show improved relative gain of up to $65.27\%$ in PR-AUC metric. A case study is presented to demonstrate the advantages of our methods in attenuating the popularity bias of global items.
</details>
<details>
<summary>摘要</summary>
不同地区的用户偏好会自然出现在多地区个性化推荐系统中，导致全球热销商品的 item bias。一个地区热销商品可能被全球热销商品所掩蔽。此外，用户的视频浏览习惯可能在不同的地理位置发生重大变化，这可能建议学习特定用户嵌入。在这篇论文中，我们提出了一种多任务学习（MTL）技术，以及适应填充方法，以减少多地区推荐中的流行度偏好。我们的提议框架通过填充活跃用户表示来增强训练示例，并能够通过 MTL 学习地域基于用户嵌入。通过实验，我们证明了我们的框架在多地区比基eline不 incorporating 我们的提议技术时表现更高的效果。特别是，我们显示了改进的相对增长率达到 $65.27\%$ 的 PR-AUC 指标。一个案例研究表明了我们的方法在减少全球商品的流行度偏好中的优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/cs.LG_2023_09_25/" data-id="clp88dbyu00seob8873s1g4ab" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_25" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/25/eess.SP_2023_09_25/" class="article-date">
  <time datetime="2023-09-25T08:00:00.000Z" itemprop="datePublished">2023-09-25</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/25/eess.SP_2023_09_25/">eess.SP - 2023-09-25</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Towards-a-Novel-Ultrasound-System-Based-on-Low-Frequency-Feature-Extraction-From-a-Fully-Printed-Flexible-Transducer"><a href="#Towards-a-Novel-Ultrasound-System-Based-on-Low-Frequency-Feature-Extraction-From-a-Fully-Printed-Flexible-Transducer" class="headerlink" title="Towards a Novel Ultrasound System Based on Low-Frequency Feature Extraction From a Fully-Printed Flexible Transducer"></a>Towards a Novel Ultrasound System Based on Low-Frequency Feature Extraction From a Fully-Printed Flexible Transducer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14569">http://arxiv.org/abs/2309.14569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Giordano, Kirill Keller, Francesco Greco, Luca Benini, Michele Magno, Christoph Leitner</li>
<li>for: 这个研究是为了开发一个可靠、便宜、携带式的数码声带测量系统，用于不侵入性地、连续地监测生命 Parameters。</li>
<li>methods: 这个研究使用了一个全新的印刷式、无铅、数码声带感应器，可以实现较好的适材化性。实验室设置使用了一个模拟人体血液流的流体模型和一个模拟心跳的拍脉机，以验证方法。</li>
<li>results: 研究结果显示，这个新型数码声带感应器可以实现高精度的血液流速度测量，并且可以实现低功耗和低带宽的处理。在不同的心跳rhythm下，测量结果皆具有误差不超过0.05Hz（3bpm）。此外，实验室设置显示，这个方法可以实现6倍以上的讯号宽度减少，从12.5MHz降至2MHz。<details>
<summary>Abstract</summary>
Ultrasound is a key technology in healthcare, and it is being explored for non-invasive, wearable, continuous monitoring of vital signs. However, its widespread adoption in this scenario is still hindered by the size, complexity, and power consumption of current devices. Moreover, such an application demands adaptability to human anatomy, which is hard to achieve with current transducer technology. This paper presents a novel ultrasound system prototype based on a fully printed, lead-free, and flexible polymer ultrasound transducer, whose bending radius promises good adaptability to the human anatomy. Our application scenario focuses on continuous blood flow monitoring. We implemented a hardware envelope filter to efficiently transpose high-frequency ultrasound signals to a lower-frequency spectrum. This reduces computational and power demands with little to no degradation in the task proposed for this work. We validated our method on a setup that mimics human blood flow by using a flow phantom and a peristaltic pump simulating 3 different heartbeat rhythms: 60, 90 and 120 beats per minute. Our ultrasound setup reconstructs peristaltic pump frequencies with errors of less than 0.05 Hz (3 bpm) from the set pump frequency, both for the raw echo and the enveloped echo. The analog pre-processing showed a promising reduction of signal bandwidth of more than 6x: pulse-echo signals of transducers excited at 12.5 MHz were reduced to about 2 MHz. Thus, allowing consumer MCUs to acquire and elaborate signals within mW-power range in an inexpensive fashion.
</details>
<details>
<summary>摘要</summary>
“ultrasound是现代医疗技术中关键的一种，正在探索无侵入、可穿戴、不间断监测生命体指标的应用场景。然而，现有设备的大小、复杂度和功耗仍然阻碍了广泛的应用。此外，这种应用场景需要适应人体解剖结构，这是现有探音器技术很难实现。这篇论文提出了一种新的探音系统原型，基于完全印刷、无铅、 flexible полимер探音器。这种探音器的弯曲半径 promise good适应人体解剖结构。我们的应用场景是无间断血流监测。我们实施了硬件滤波器，以有效地将高频探音信号转换为低频spectrum。这 reduces computational和功耗占用，几乎不会影响我们所提出的任务。我们验证了我们的方法，使用一个模拟人血流的流体phantom和一个模拟心跳的剧热泵。我们的ultrasound设备可以准确地重construct peristaltic pump frequencies， errors of less than 0.05 Hz（3 bpm）from the set pump frequency， both for the raw echo and the enveloped echo。analog pre-processing showed a promising reduction of signal bandwidth of more than 6x：pulse-echo signals of transducers excited at 12.5 MHz were reduced to about 2 MHz。因此，allowing consumer MCUs to acquire and elaborate signals within mW-power range in an inexpensive fashion。”
</details></li>
</ul>
<hr>
<h2 id="Secret-Message-Transmission-by-Echoing-Encrypted-Probes-–-STEEP"><a href="#Secret-Message-Transmission-by-Echoing-Encrypted-Probes-–-STEEP" class="headerlink" title="Secret-Message Transmission by Echoing Encrypted Probes – STEEP"></a>Secret-Message Transmission by Echoing Encrypted Probes – STEEP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14529">http://arxiv.org/abs/2309.14529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingbo Hua</li>
<li>for: 这篇论文研究了Maurer、Ahlswede和Csiszar（MAC）为秘密键容量的下限和上限的性质，并基于这些下限的约束，提出了一种名为“秘密信息传输通过响应加密探测”（STEEP）的方案。</li>
<li>methods: 该方案包括两个阶段：在第一阶段，Alice通过探测频道发送随机探测信号给Bob；在第二阶段，Bob根据预测的探测信号发送一个加密的版本给Alice，但是这个加密版本是使用一个秘密来加密的。</li>
<li>results: 如果恶作剂Eve无法获得Alice在第一阶段发送的准确探测信号，那么STEEP可以在返回频道上保证从Bob到Alice的秘密率大于0，即使恶作剂在探测频道上的频率 stronger than Bob的。 STEEP适用于物理层和Upper层在连接网络中。<details>
<summary>Abstract</summary>
This paper examines the properties of the lower and upper bounds established by Maurer, Ahlswede and Csiszar (MAC) for secret-key capacity in the case of channel probing over single-input and single-output (SISO) channels. Inspired by the insights into MAC's bounds, a scheme called secret-message transmission by echoing encrypted probes (STEEP) is proposed. STEEP consists of two phases: in phase 1, Alice sends random probes over a probing channel to Bob; in phase 2, Bob echoes back an estimated version of the probes, but encrypted by a secret, over a high-quality return channel. Provided that Eve is unable to obtain the exact probes transmitted by Alice in phase 1, STEEP guarantees a positive secrecy rate from Bob to Alice over the return channel even if Eve's channel strength during channel probing is stronger than Bob's. STEEP is applicable to both physical layer and upper layers in connected networks.
</details>
<details>
<summary>摘要</summary>
STEEP consists of two phases:1. In phase 1, Alice sends random probes over a probing channel to Bob.2. In phase 2, Bob echoes back an estimated version of the probes, but encrypted by a secret, over a high-quality return channel.Assuming that Eve is unable to obtain the exact probes transmitted by Alice in phase 1, STEEP guarantees a positive secrecy rate from Bob to Alice over the return channel, even if Eve's channel strength during channel probing is stronger than Bob's.STEEP is applicable to both physical layer and upper layers in connected networks.
</details></li>
</ul>
<hr>
<h2 id="Heart-rate-measurement-using-the-built-in-triaxial-accelerometer-from-a-commercial-digital-writing-device"><a href="#Heart-rate-measurement-using-the-built-in-triaxial-accelerometer-from-a-commercial-digital-writing-device" class="headerlink" title="Heart rate measurement using the built-in triaxial accelerometer from a commercial digital writing device"></a>Heart rate measurement using the built-in triaxial accelerometer from a commercial digital writing device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14308">http://arxiv.org/abs/2309.14308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julie Payette, Fabrice Vaussenat, Sylvain G. Cloutier</li>
<li>for: 这个研究用于比较智能笔的内置加速度仪和标准ECG仪器中的心率数据是否准确。</li>
<li>methods: 这个研究使用了智能笔equipped with sensors（STABILO的DigiPen）的内置加速度仪和标准ECG仪器来收集数据。数据处理使用了Butterworth滤波器减少噪声。</li>
<li>results: 研究发现，智能笔的内置加速度仪可以准确地预测心率，与标准ECG数据的相关性高于0.99。<details>
<summary>Abstract</summary>
Wearable devices are on the rise. Smart watches and phones, fitness trackers or smart textiles now provide unprecedented access to our own personal data. As such, wearable devices can enable health monitoring without disrupting our daily routines. In clinical settings, electrocardiograms (ECGs) and photoplethysmographies (PPGs) are used to monitor the heart's and respiratory behaviors. In more practical settings, accelerometers can be used to estimate the heartrate when they are attached to the chest. They can also help filter out some noise in ECG signal from movement. In this work, we compare the heart rate data extracted from the built-in accelerometer of a commercial smart pen equipped with sensors (STABILO's DigiPen), with a standard ECG monitor readouts. We demonstrate that it is possible to accurately predict the heart rate from the smart pencil. The data collection is done with eight volunteers, writing the alphabet continuously for five minutes. The signal is processed with a Butterworth filter to cut off noise. We achieve a mean-squared error (MSE) better than 6.685x10$^{-3}$ comparing the DigiPen's computed ${\Delta}$t (time between pulses) with the reference ECG data. The peaks' timestamps for both signals all maintain a correlation higher than 0.99. All computed heart rates from the pen accurately correlate with the reference ECG signals.
</details>
<details>
<summary>摘要</summary>
“智能装置在不断增长。智能手表和手机、健身器或智能纺织物现在提供了前所未有的个人数据访问权。因此，智能装置可以不间断地健康监测，不会影响我们的日常生活。在临床设置下，电喷呈（ECG）和光谱呈（PPG）用于监测心脏和呼吸的行为。在更实际的设置下，加速计可以用于估算心率，当它们附加到胸部时。它们还可以帮助滤除一些运动所引起的噪声在ECG信号中。在这个工作中，我们比较了 comercial smart pen 内建的加速计和标准 ECG 监测器的数据。我们示示了可以准确地预测心率从 smart pen 中提取的数据。数据采集使用八名志愿者，在五分钟内连续写字母。信号处理使用Butterworth滤波器剪辑噪声。我们实现了比较于 6.685 x 10$^{-3}$ 的mean-squared error（MSE），比较 commercial smart pen 计算的 $\Delta$t（心脏бит间隔）与参考 ECG 数据。两个信号的峰时间戳都保持了高于 0.99 的相关性。所有从 pen 中计算的心率都准确地与参考 ECG 信号相关。”
</details></li>
</ul>
<hr>
<h2 id="Joint-RIS-Phase-Profile-Design-and-Power-Allocation-for-Parameter-Estimation-in-Presence-of-Eavesdropping"><a href="#Joint-RIS-Phase-Profile-Design-and-Power-Allocation-for-Parameter-Estimation-in-Presence-of-Eavesdropping" class="headerlink" title="Joint RIS Phase Profile Design and Power Allocation for Parameter Estimation in Presence of Eavesdropping"></a>Joint RIS Phase Profile Design and Power Allocation for Parameter Estimation in Presence of Eavesdropping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14280">http://arxiv.org/abs/2309.14280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Mehdipour Abadi, Ayda Nodel Hokmabadi, Sinan Gezici</li>
<li>for: 本文主要研究了在具有各种各样的环境和干扰的情况下，通过重新配置智能表面（RIS），实现安全传输一个权重矢量参数的精准性。</li>
<li>methods: 本文提出了一种基于Fisher信息矩阵追踪（FIM）的估计精度度量，并使用其关于接收器和侦测器的关键性能指标，以便在RIS环境中实现优化传输精度。</li>
<li>results: 本文通过 alternating 优化和semidefinite relaxation 等方法，解决了RIS级别和发射器级别的优化问题，并通过实验证明了干扰的影响和RIS级别的选择对传输精度的影响。<details>
<summary>Abstract</summary>
We consider secure transmission of a deterministic complex-valued parameter vector from a transmitter to an intended receiver in the presence of an eavesdropper in a reconfigurable intelligent surface (RIS)-integrated environment. We aim to jointly optimize the RIS phase profile and the power allocation matrix at the transmitter to enhance the estimation accuracy at the intended receiver while limiting that at the eavesdropper. We utilize the trace of the Fisher information matrix (FIM), equivalently, the average Fisher information, as the estimation accuracy metric, and obtain its closed form expression for the intended receiver and the eavesdropper. Accordingly, the joint RIS phase profile and power allocation problem is formulated, and it is solved via alternating optimization. When the power allocation matrix is fixed during alternating optimization, the optimal RIS phase profile design problem is formulated as a non-convex problem and it is solved via semidefinite relaxation and rank reduction. When the RIS phase profile is fixed, a linear programming formulation is obtained for optimal power allocation. Via simulations, the effects of RIS phase design and power allocation are illustrated individually and jointly. Moreover, extensions are provided by considering the presence of line of sight paths in the environment and the availability of RIS elements with adjustable magnitudes.
</details>
<details>
<summary>摘要</summary>
我们考虑了一个报文加密传输的幂等复数参数向量从发送器到目标接收器的安全传输，在扩展智能表面（RIS）集成环境中。我们希望同时优化RIS相位特征和发送器的功率分配矩阵，以提高接收器的估计精度，同时限制侦测器的估计精度。我们使用追踪 Fisher信息矩阵（FIM）的跟踪，即平均 Fisher信息，作为估计精度度量，并得到其闭合形式表达。根据此，我们提出了共同优化RIS相位特征和功率分配问题，并通过 alternate 优化解决。当功率分配矩阵在 alternate 优化过程中固定时，则RIS相位特征设计问题变为非对称问题，并通过半definite  relaksation和级数减少解决。当RIS相位特征固定时，则发送器的功率分配问题可以转化为线性程序。通过实验，我们证明了RIS相位设计和功率分配在个体和共同优化下的效果。此外，我们还提供了考虑线性路径的存在和RIS元素的调整级别的扩展。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Three-Layer-Hybrid-Reconfigurable-Intelligent-Surface-for-6G-Wireless-Communication-Trade-offs-and-Performance"><a href="#Adaptive-Three-Layer-Hybrid-Reconfigurable-Intelligent-Surface-for-6G-Wireless-Communication-Trade-offs-and-Performance" class="headerlink" title="Adaptive Three Layer Hybrid Reconfigurable Intelligent Surface for 6G Wireless Communication: Trade-offs and Performance"></a>Adaptive Three Layer Hybrid Reconfigurable Intelligent Surface for 6G Wireless Communication: Trade-offs and Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14087">http://arxiv.org/abs/2309.14087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rashed Hasan Ratul, Muhammad Iqbal, Tabinda Ashraf, Jen-Yi Pan, Yi-Han Wang, Shao-Yu Lien</li>
<li>for: 本研究旨在提供一种三层混合式RIS助け的配置方案，可以响应active和passive RIS的变化，同时具有静止或无功状态。</li>
<li>methods: 本研究使用了hybrid RIS-assisted配置，包括三层混合式RIS，以适应变化的发射功率和无线链路质量。</li>
<li>results:  simulations表明，该三层混合式RIS-assisted配置比单独使用passive或active RIS-assisted技术更有优势。<details>
<summary>Abstract</summary>
A potential candidate technology for the development of future 6G networks has been recognized as Reconfigurable Intelligent Surface (RIS). However, due to the variation in radio link quality, traditional passive RISs only accomplish a minimal signal gain in situations with strong direct links between user equipment (UE) and base station (BS). In order to get over this fundamental restriction of smaller gain, the idea of active RISs might be a suitable solution. In contrast to current passive RIS, which simply reflects and directs signals without any additional amplification, active RISs have the ability to enhance reflected signals by the incorporation of amplifiers inside its elements. However, with additional amplifiers, apart from the relatively complex attributes of RIS-assisted arrangements, the additional energy consumption of such technologies is often disregarded. So, there might be a tradeoff between the additional energy consumption for the RIS technologies and the overall gain acquired by deploying this potential advancement. The objective of this work is to provide a primary idea of a three-layer hybrid RIS-assisted configuration that is responsive to both active and passive RIS, as well as an additional dormant or inactive state. The single RIS structure should be capable of adjusting its overall configuration in response to fluctuations in transmit power and radio link quality. Furthermore, our fabricated passive RIS-assisted structure verifies a portion of the proposed idea, with simulations highlighting its advantages over standalone passive or active RIS-assisted technologies.
</details>
<details>
<summary>摘要</summary>
sixth generation 网络（6G）的发展中，一种潜在的技术是可配置智能表面（Reconfigurable Intelligent Surface，RIS）。然而，由于无线链路质量的变化，传统的静止RIS只能实现最小的信号增强，尤其在用户设备（UE）和基站（BS）之间的强直接链路情况下。为了突破这种基本限制，可能适用的解决方案是活动RIS。与现有的静止RIS相比，活动RIS可以通过内置扩增器提高反射信号的强度。然而，随着这些技术的增加，除了RIS-assisted的复杂性外，额外的能源消耗也常被忽视。因此，可能存在一种负担增加和增加的负担之间的权衡。本工作的目标是提供一种三层混合RIS-assisted配置，可以响应活动和静止RIS，以及额外的休眠或不活跃状态。单一RIS结构应该能够根据发射功率和无线链路质量的变化进行调整。此外，我们制造的静止RIS-assisted结构的实验证明了一部分的提案的优势，而且模拟结果表明，与独立的静止或活动RIS-assisted技术相比，这种三层混合配置具有更高的优势。
</details></li>
</ul>
<hr>
<h2 id="Single-Antenna-Jammers-in-MIMO-OFDM-Can-Resemble-Multi-Antenna-Jammers"><a href="#Single-Antenna-Jammers-in-MIMO-OFDM-Can-Resemble-Multi-Antenna-Jammers" class="headerlink" title="Single-Antenna Jammers in MIMO-OFDM Can Resemble Multi-Antenna Jammers"></a>Single-Antenna Jammers in MIMO-OFDM Can Resemble Multi-Antenna Jammers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14059">http://arxiv.org/abs/2309.14059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iip-group/ofdm-jammer">https://github.com/iip-group/ofdm-jammer</a></li>
<li>paper_authors: Gian Marti, Christoph Studer</li>
<li>for: 这篇论文研究了多输入多输出（MIMO）无线系统中频率平坦渠道下单天线干扰器对接收器的影响。</li>
<li>methods: 论文使用了线性空间滤波来消除干扰器所引起的干扰。</li>
<li>results: 研究发现，当干扰器不遵守OFDM协议，它会在多个子帧上引起干扰，而不是单一的一维空间。这意味着在MIMO-OFDM系统中，单天线干扰器可以类比为L天线干扰器。<details>
<summary>Abstract</summary>
In multiple-input multiple-output (MIMO) wireless systems with frequency-flat channels, a single-antenna jammer causes receive interference that is confined to a one-dimensional subspace. Such a jammer can thus be nulled using linear spatial filtering at the cost of one degree of freedom. Frequency-selective channels are often transformed into multiple frequency-flat subcarriers with orthogonal frequency-division multiplexing (OFDM). We show that when a single-antenna jammer violates the OFDM protocol by not sending a cyclic prefix, the interference received on each subcarrier by a multi-antenna receiver is, in general, not confined to a subspace of dimension one (as a single-antenna jammer in a frequency-flat scenario would be), but of dimension L, where L is the jammer's number of channel taps. In MIMO-OFDM systems, a single-antenna jammer can therefore resemble an L-antenna jammer. Simulations corroborate our theoretical results. These findings imply that mitigating jammers with large delay spread through linear spatial filtering is infeasible. We discuss some (im)possibilities for the way forward.
</details>
<details>
<summary>摘要</summary>
在多输入多输出（MIMO）无线系统中，频率平坦渠道上的单天线妨碍器会导致接收干扰，这种干扰将被限制在一维空间中。这种妨碍器可以使用线性空间滤波来纠正，但是需要一个自由度。频率选择性渠道通常会被转换为多个平坦频分谱下的多个子帧，使用多载波分多plexing（OFDM）。我们表明，当妨碍器不遵循OFDM协议，并不发送循环前fix，则干扰接收到每个子帧的多天线接收器是，在总体来说，不再局限于一维空间中（如单天线妨碍器在频率平坦场景中会），而是局限于维度L，其中L是妨碍器的通道扩散的数量。在MIMO-OFDM系统中，单天线妨碍器可以类比于L天线妨碍器。实验证明了我们的理论结果。这些发现表明，通过线性空间滤波来mitigate妨碍器的影响是不可能的。我们讨论了一些（不）可能的前进方向。
</details></li>
</ul>
<hr>
<h2 id="Beam-Squint-Assisted-User-Localization-in-Near-Field-Integrated-Sensing-and-Communications-Systems"><a href="#Beam-Squint-Assisted-User-Localization-in-Near-Field-Integrated-Sensing-and-Communications-Systems" class="headerlink" title="Beam Squint Assisted User Localization in Near-Field Integrated Sensing and Communications Systems"></a>Beam Squint Assisted User Localization in Near-Field Integrated Sensing and Communications Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14012">http://arxiv.org/abs/2309.14012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongliang Luo, Feifei Gao, Wanmai Yuan, Shun Zhang</li>
<li>for: 这篇论文旨在提出一种基于true-time-delay lines（TTDs）的近场通信系统中的用户定位方法，用于解决宽频MIMO系统中的辐束偏移问题。</li>
<li>methods: 该方法利用TTDs控制近场辐束偏移的轨迹，并通过不同子载波束的扫描来实现用户定位。</li>
<li>results:  simulations show that the proposed method can effectively reduce the beam sweeping overhead and achieve high accuracy user localization.<details>
<summary>Abstract</summary>
Integrated sensing and communication (ISAC) has been regarded as a key technology for 6G wireless communications, in which large-scale multiple input and multiple output (MIMO) array with higher and wider frequency bands will be adopted. However, recent studies show that the beam squint phenomenon can not be ignored in wideband MIMO system, which generally deteriorates the communications performance. In this paper, we find that with the aid of true-time-delay lines (TTDs), the range and trajectory of the beam squint in near-field communications systems can be freely controlled, and hence it is possible to reversely utilize the beam squint for user localization. We derive the trajectory equation for near-field beam squint points and design a way to control such trajectory. With the proposed design, beamforming from different subcarriers would purposely point to different angles and different distances, such that users from different positions would receive the maximum power at different subcarriers. Hence, one can simply localize multiple users from the beam squint effect in frequency domain, and thus reduce the beam sweeping overhead as compared to the conventional time domain beam search based approach. Furthermore, we utilize the phase difference of the maximum power subcarriers received by the user at different frequencies in several times beam sweeping to obtain a more accurate distance estimation result, ultimately realizing high accuracy and low beam sweeping overhead user localization. Simulation results demonstrate the effectiveness of the proposed schemes.
</details>
<details>
<summary>摘要</summary>
integrated sensing and communication (ISAC) 被认为是 sixth generation wireless communication (6G) 中的关键技术，其中大规模的多输入多输出 (MIMO) 阵列将在更高频率范围内使用。然而，最近的研究表明，在宽频MIMO系统中，扫描干扰（beam squint）现象无法忽略。在这篇论文中，我们发现，通过使用真实时延线（TTD），在近距离通信系统中扫描干扰的范围和轨迹可以自由控制，因此可以利用扫描干扰进行用户位置localization。我们 derive了近距离扫描干扰点的轨迹方程，并设计了控制这种轨迹的方法。与我们的设计相比，通过时域扫描来实现用户位置的方法可以减少扫描干扰的过程。此外，我们利用不同频率下接收到用户的最大功率Subcarrier的相位差来获取更加准确的距离估计结果，从而实现高精度低扫描干扰的用户位置定位。实验结果表明我们的方案的有效性。
</details></li>
</ul>
<hr>
<h2 id="Carrier-Aggregation-Enabled-Integrated-Sensing-and-Communication-Signal-Design-and-Processing"><a href="#Carrier-Aggregation-Enabled-Integrated-Sensing-and-Communication-Signal-Design-and-Processing" class="headerlink" title="Carrier Aggregation Enabled Integrated Sensing and Communication Signal Design and Processing"></a>Carrier Aggregation Enabled Integrated Sensing and Communication Signal Design and Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14008">http://arxiv.org/abs/2309.14008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Wei, Haotian Liu, Xinyi Yang, Wangjun Jiang, Huici Wu, Xingwang Li, Zhiyong Feng</li>
<li>for: 本研究旨在提高未来移动通信系统中智能应用（如互联网联盟（IoV）和扩展现实（XR））的数据传输率和探测精度，通过 интегрирован的感知和通信（ISAC）技术。</li>
<li>methods: 本研究提议使用加载组合（CA）技术将高频和低频频率带width拼接成一个信号，以提高探测性能。此外，本研究还提出了基于压缩感知（CS）的ISAC信号处理算法，并使用快速融合减少阈值算法（FISTA）解决重新配置减少问题。</li>
<li>results: 实验结果表明，CA技术可以有效提高距离和速度估计的准确性。<details>
<summary>Abstract</summary>
The future mobile communication systems will support intelligent applications such as Internet of Vehicles (IoV) and Extended Reality (XR). Integrated Sensing and Communication (ISAC) is regarded as one of the key technologies satisfying the high data rate communication and highly accurate sensing for these intelligent applications in future mobile communication systems. With the explosive growth of wireless devices and services, the shortage of spectrum resources leads to the fragmentation of available frequency bands for ISAC systems, which degrades sensing performance. Facing the above challenges, this paper proposes a Carrier Aggregation (CA)-based ISAC signal aggregating high and low-frequency bands to improve the sensing performance, where the CA-based ISAC signal can use four different aggregated pilot structures for sensing. Then, an ISAC signal processing algorithm with Compressed Sensing (CS) is proposed and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is used to solve the reconfiguration convex optimization problem. Finally, the Cram'er-Rao Lower Bounds (CRLBs) are derived for the CA-based ISAC signal. Simulation results show that CA efficiently improves the accuracy of range and velocity estimation.
</details>
<details>
<summary>摘要</summary>
将来的移动通信系统将支持智能应用程序，如网络化交通 (IoV) 和增强现实 (XR)。集成感知通信 (ISAC) 被认为是未来移动通信系统支持高速数据传输和高精度感知的关键技术。随着无线设备和服务的快速增长，可用频率带的缺乏导致 ISAC 系统的分配频率带产生干扰，从而降低感知性能。面对这些挑战，本文提出了基于搅合 (CA) 的 ISAC 信号搅合高频和低频频率带以提高感知性能。然后，一种基于 CS 的 ISAC 信号处理算法和快速融合缩放算法 (FISTA) 被提出，以解决重配置减少问题。最后，CA 基于 ISAC 信号的 Cram'er-Rao Lower Bounds (CRLBs) 被 derivation。实验结果表明，CA 可以有效提高范围和速度估计的准确性。
</details></li>
</ul>
<hr>
<h2 id="Near-field-Hybrid-Beamforming-for-Terahertz-band-Integrated-Sensing-and-Communications"><a href="#Near-field-Hybrid-Beamforming-for-Terahertz-band-Integrated-Sensing-and-Communications" class="headerlink" title="Near-field Hybrid Beamforming for Terahertz-band Integrated Sensing and Communications"></a>Near-field Hybrid Beamforming for Terahertz-band Integrated Sensing and Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13984">http://arxiv.org/abs/2309.13984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet M. Elbir, Abdulkadir Celik, Ahmed M. Eltawil</li>
<li>for: 这篇论文旨在探讨 sixth generation 无线网络中的 Terahertz (THz) 频段通信和集成感知通信 (ISAC) 两大方面。</li>
<li>methods: 作者提出了一种 alternating optimization 技术来解决 near-field THz-ISAC enario 中的质量降低问题。</li>
<li>results: 作者通过数值仿真显示了该方法可以在不添加增加硬件Components的情况下实现atisfactory的spectral efficiency表现，并准确地估算near-field beamformers，有效地 mitigate near-field beam-squint。<details>
<summary>Abstract</summary>
Terahertz (THz) band communications and integrated sensing and communications (ISAC) are two main facets of the sixth generation wireless networks. In order to compensate the severe attenuation, the THz wireless systems employ large arrays, wherein the near-field beam-squint severely degrades the beamforming accuracy. Contrary to prior works that examine only either narrowband ISAC beamforming or far-field models, we introduce an alternating optimization technique for hybrid beamforming design in near-field THz-ISAC scenario. We also propose an efficient approach to compensate near-field beam-squint via baseband beamformers. Via numerical simulations, we show that the proposed approach achieves satisfactory spectral efficiency performance while accurately estimating the near-field beamformers and mitigating the beam-squint without additional hardware components.
</details>
<details>
<summary>摘要</summary>
六代无线网络中的tera响（THz）频段通信和集成感知通信（ISAC）是两个主要方面。为了抵消严重强化，THz无线系统使用大型阵列，其中靠近场区域的 beam-squint 严重降低了射频形成精度。与先前的工作只研究了窄频段ISAC射频形成或远场模型，我们引入了交替优化技术为混合射频形成设计。我们还提出了一种有效的方法来资料near-field beam-squint via baseband射频former。通过数值仿真，我们表明了我们的方法可以实现满意的spectral efficiency性能，准确地估计near-field射频former和mitigate beam-squint，无需额外硬件组件。
</details></li>
</ul>
<hr>
<h2 id="Track-before-detect-Algorithm-based-on-Cost-reference-Particle-Filter-Bank-for-Weak-Target-Detection"><a href="#Track-before-detect-Algorithm-based-on-Cost-reference-Particle-Filter-Bank-for-Weak-Target-Detection" class="headerlink" title="Track-before-detect Algorithm based on Cost-reference Particle Filter Bank for Weak Target Detection"></a>Track-before-detect Algorithm based on Cost-reference Particle Filter Bank for Weak Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13922">http://arxiv.org/abs/2309.13922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Lu, Guojie Peng, Weichuan Zhang, Changming Sun</li>
<li>for: 这个论文是为了解决在雷达、声纳等应用中检测弱目标的问题而写的。</li>
<li>methods: 该论文提出了一种基于改进的 particel Filter 的 track-before-detect（TBD）算法，即 cost-reference particle filter bank（CRPFB）。该算法将目标检测转化为两层假设测试问题。</li>
<li>results: 对于非线性频率调制（NLFM）信号检测和跟踪实验， simulate 结果表明，提出的 TBD 算法比现有的 TBD 算法在检测、跟踪和时间效率方面表现更好。<details>
<summary>Abstract</summary>
Detecting weak target is an important and challenging problem in many applications such as radar, sonar etc. However, conventional detection methods are often ineffective in this case because of low signal-to-noise ratio (SNR). This paper presents a track-before-detect (TBD) algorithm based on an improved particle filter, i.e. cost-reference particle filter bank (CRPFB), which turns the problem of target detection to the problem of two-layer hypothesis testing. The first layer is implemented by CRPFB for state estimation of possible target. CRPFB has entirely parallel structure, consisting amounts of cost-reference particle filters with different hypothesized prior information. The second layer is to compare a test metric with a given threshold, which is constructed from the output of the first layer and fits GEV distribution. The performance of our proposed TBD algorithm and the existed TBD algorithms are compared according to the experiments on nonlinear frequency modulated (NLFM) signal detection and tracking. Simulation results show that the proposed TBD algorithm has better performance than the state-of-the-arts in detection, tracking, and time efficiency.
</details>
<details>
<summary>摘要</summary>
检测弱目标是许多应用中的一个重要和挑战性问题，如雷达和SONAR等。然而，传统的检测方法经常无法处理这种情况，因为信号噪声比（SNR）过低。这篇论文提出了基于改进的粒子筛算法（cost-reference particle filter bank，CRPFB）的track-before-detect（TBD）算法，将目标检测转化为两层假设测试问题。第一层由CRPFB实现的状态估计可能的目标，CRPFB具有完全平行结构，包括不同假设先验信息的多个成本参照粒子筛。第二层是比较测试指标与给定的阈值，该阈值由第一层的输出和适应GEV分布构建。对于非线性频率模ulation（NLFM）信号检测和跟踪的实验结果显示，提议的TBD算法比现有的TBD算法在检测、跟踪和时间效率方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="Online-Resource-Allocation-for-Semantic-Aware-Edge-Computing-Systems"><a href="#Online-Resource-Allocation-for-Semantic-Aware-Edge-Computing-Systems" class="headerlink" title="Online Resource Allocation for Semantic-Aware Edge Computing Systems"></a>Online Resource Allocation for Semantic-Aware Edge Computing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13917">http://arxiv.org/abs/2309.13917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Cang, Ming Chen, Zhaohui Yang, Yuntao Hu, Yinlu Wang, Chongwen Huang, Zhaoyang Zhang</li>
<li>for: 这篇论文旨在提出一个基于 semantics 的联合通信和计算资源分配框架，以减轻 MEC 系统中讯息的传输负担。</li>
<li>methods: 这篇论文使用了 Lyapunov 优化、封页数据分析和继承数据分析等方法，将联合通信和计算资源分配问题转化为一个可解决的长期问题。</li>
<li>results:  simulations 显示，提出的算法可以与无 semantics 的分配方法相比，节省至多 41.8% 的能源。<details>
<summary>Abstract</summary>
In this paper, we propose a semantic-aware joint communication and computation resource allocation framework for MEC systems. In the considered system, random tasks arrive at each terminal device (TD), which needs to be computed locally or offloaded to the MEC server. To further release the transmission burden, each TD sends the small-size extracted semantic information of tasks to the server instead of the original large-size raw data. An optimization problem of joint semanticaware division factor, communication and computation resource management is formulated. The problem aims to minimize the energy consumption of the whole system, while satisfying longterm delay and processing rate constraints. To solve this problem, an online low-complexity algorithm is proposed. In particular, Lyapunov optimization is utilized to decompose the original coupled long-term problem into a series of decoupled deterministic problems without requiring the realizations of future task arrivals and channel gains. Then, the block coordinate descent method and successive convex approximation algorithm are adopted to solve the current time slot deterministic problem by observing the current system states. Moreover, the closed-form optimal solution of each optimization variable is provided. Simulation results show that the proposed algorithm yields up to 41.8% energy reduction compared to its counterpart without semantic-aware allocation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于 semantics 的共享计算和通信资源分配框架 для MEC 系统。在考虑的系统中，Random tasks 会随机到each terminal device（TD），需要本地计算或者卸载到 MEC 服务器。为了进一步减轻传输负担，each TD 将送出小型抽象信息（semantic information）到服务器，而不是原始大型原始数据。我们建立了一个优化问题，该问题的目标是最小化整体系统的能耗，同时满足长期延迟和处理率约束。为解决这个问题，我们提出了一种在线低复杂度算法。具体来说，我们利用了 Lyapunov 优化来将原来的 Coupled 长期问题分解成一系列的解耦的决定问题，无需考虑未来任务的到达和通道增益的实现。然后，我们采用了块坐标 descend 方法和Successive Convex Approximation 算法来解决当前时间槽的决定问题，并且提供了每个优化变量的关闭式最优解。实验结果显示，我们的算法可以提供 Up to 41.8% 的能源减少，相比无semantic-aware分配的对照方案。
</details></li>
</ul>
<hr>
<h2 id="NoncovANM-Gridless-DOA-Estimation-for-LPDF-System"><a href="#NoncovANM-Gridless-DOA-Estimation-for-LPDF-System" class="headerlink" title="NoncovANM: Gridless DOA Estimation for LPDF System"></a>NoncovANM: Gridless DOA Estimation for LPDF System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13902">http://arxiv.org/abs/2309.13902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangying Zhao, Peng Chen, Zhenxin Cao, Xianbin Wang</li>
<li>for: 提高低成本irection finding系统的精度和效率</li>
<li>methods: 利用智能可编程表面（IRS）和低成本探测chnology，采用一个完全可用的接收通道，并利用atomic norm minimization（ANM）问题来优化direction finding表现</li>
<li>results: 提出了一种基于非对称�C-ANM算法的方法，可以快速和高效地优化direction finding表现，并且在实验中比较高效和精度高于比较方法Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the accuracy and efficiency of low-cost passive direction finding (LPDF) systems.</li>
<li>methods: The proposed method utilizes an intelligent reconfigurable surface (IRS)-aided LPDF system, which only requires one fully functional receiving channel. The method exploits the sparsity of targets in the spatial domain by formulating an atomic norm minimization (ANM) problem to estimate the direction of arrival (DOA). To solve the ANM problem, a novel nonconvex-based ANM (NC-ANM) method is proposed, which uses gradient threshold iteration to avoid falling into saddle points. The theoretical analysis for the convergence of the NC-ANM method is also provided.</li>
<li>results: The proposed method outperforms compared methods in DOA estimation with lower computational complexity in the LPDF system, as shown by simulation results.<details>
<summary>Abstract</summary>
Direction of arrival (DOA) estimation is an important research in the area of array signal processing, and has been studied for decades. High resolution DOA estimation requires large array aperture, which leads to the increase of hardware cost. Besides, high accuracy DOA estimation methods usually have high computational complexity. In this paper, the problem of decreasing the hardware cost and algorithm complexity is addressed. First, considering the ability of flexible controlling the electromagnetic waves and low-cost, an intelligent reconfigurable surface (IRS)-aided low-cost passive direction finding (LPDF) system is developed, where only one fully functional receiving channel is adopted. Then, the sparsity of targets direction in the spatial domain is exploited by formulating an atomic norm minimization (ANM) problem to estimate the DOA. Traditionally, solving ANM problem is complex and cannot be realized efficiently. Hence, a novel nonconvex-based ANM (NC-ANM) method is proposed by gradient threshold iteration, where a perturbation is introduced to avoid falling into saddle points. The theoretical analysis for the convergence of the NC-ANM method is also given. Moreover, the corresponding Cram\'er-Rao lower bound (CRLB) in the LPDF system is derived, and taken as the referred bound of the DOA estimation. Simulation results show that the proposed method outperforms the compared methods in the DOA estimation with lower computational complexity in the LPDF system.
</details>
<details>
<summary>摘要</summary>
irection of arrival (DOA) 估计是阵列信号处理领域的重要研究，已经在数十年来被研究。高分辨率DOA估计需要大型阵列尺寸，这会导致硬件成本的增加。此外，高精度DOA估计方法通常具有高计算复杂性。在这篇论文中，解决降低硬件成本和算法复杂性的问题。首先，根据可控电磁波的能力和低成本，一种智能可重配置表面（IRS）帮助的低成本通过探测（LPDF）系统被开发，只有一个完全可用的接收通道。然后，通过利用目标方向在空间领域的稀畴性，提出一个原子范数最小化（ANM）问题来估计DOA。传统上，解决ANM问题是复杂的，无法有效实现。因此，一种新的非对称-基于ANM（NC-ANM）方法被提出，通过梯度阈值迭代来解决。另外，对NC-ANM方法的理论分析也给出。此外，对LPDF系统中DOA估计的Cramér-Rao下界（CRLB）也被 derive，作为DOA估计的参照 bound。实验结果表明，提出的方法在LPDF系统中的DOA估计中具有较低的计算复杂性和更高的精度，并且超过相关比较方法。
</details></li>
</ul>
<hr>
<h2 id="DNN-DANM-A-High-Accuracy-Two-Dimensional-DOA-Estimation-Method-Using-Practical-RIS"><a href="#DNN-DANM-A-High-Accuracy-Two-Dimensional-DOA-Estimation-Method-Using-Practical-RIS" class="headerlink" title="DNN-DANM: A High-Accuracy Two-Dimensional DOA Estimation Method Using Practical RIS"></a>DNN-DANM: A High-Accuracy Two-Dimensional DOA Estimation Method Using Practical RIS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13856">http://arxiv.org/abs/2309.13856</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenpengseu/dnn-danm">https://github.com/chenpengseu/dnn-danm</a></li>
<li>paper_authors: Zhimin Chen, Peng Chen, Le Zheng, Yudong Zhang</li>
<li>for: 这个论文研究了在实际的快速知识Surface (RIS) 系统中，使用深度学习和分解原理来实现两个方向的到来角度估计 (DOA) 问题。</li>
<li>methods: 该论文提出了一种 combining 深度学习 (DNN) 和分解原理 (DANM) 的新方法，用于解决 DOA 估计问题。此外，还提出了一种低计算复杂度的 semi-definite programming (SDP) 方法来解决原子化最小化问题。</li>
<li>results: 该论文通过实验和原型 validate 了该方法在实际 RIS 系统中的性能，并证明了它在两个维度 DOA 估计中具有较低的复杂度和较高的准确率。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) or intelligent reflecting surface (IRS) has been an attractive technology for future wireless communication and sensing systems. However, in the practical RIS, the mutual coupling effect among RIS elements, the reflection phase shift, and amplitude errors will degrade the RIS performance significantly. This paper investigates the two-dimensional direction-of-arrival (DOA) estimation problem in the scenario using a practical RIS. After formulating the system model with the mutual coupling effect and the reflection phase/amplitude errors of the RIS, a novel DNNDANM method is proposed for the DOA estimation by combining the deep neural network (DNN) and the decoupling atomic norm minimization (DANM). The DNN step reconstructs the received signal from the one with RIS impairments, and the DANM step exploits the signal sparsity in the two-dimensional spatial domain. Additionally, a semi-definite programming (SDP) method with low computational complexity is proposed to solve the atomic minimization problem. Finally, both simulation and prototype are carried out to show estimation performance, and the proposed method outperforms the existing methods in the two-dimensional DOA estimation with low complexity in the scenario with practical RIS.
</details>
<details>
<summary>摘要</summary>
现代化的智能反射表（RIS）或智能镜面（IRS）技术在未来无线通信和探测系统中具有吸引力。然而，在实际应用中的RIS中，元件之间的共振效应、反射阶段偏移和干扰错误会对RIS性能产生负面影响。本文研究使用实际RIS场景下的两个维度方向来估计（DOA）问题。经过制定系统模型，包括RIS中的共振效应和反射阶段偏移/干扰错误，我们提出了一种基于深度神经网络（DNN）和解决原子范数最小化（DANM）的新的DNNDANM方法。DNN步骤重建接收信号，而DANM步骤利用信号在两个维度空间中的稀疏性。此外，我们还提出了一种具有低计算复杂性的半definiteProgramming（SDP）方法来解决原子最小化问题。最后，我们通过实验和prototype来证明我们的方法在两个维度DOA估计中具有低复杂性和高性能，并且超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="On-the-Energy-Efficiency-of-THz-NOMA-enhanced-UAV-Cooperative-Network-with-SWIPT"><a href="#On-the-Energy-Efficiency-of-THz-NOMA-enhanced-UAV-Cooperative-Network-with-SWIPT" class="headerlink" title="On the Energy Efficiency of THz-NOMA enhanced UAV Cooperative Network with SWIPT"></a>On the Energy Efficiency of THz-NOMA enhanced UAV Cooperative Network with SWIPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13836">http://arxiv.org/abs/2309.13836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jalal Jalali, Ata Khalili, Hina Tabassum, Rafael Berkvens, Jeroen Famaey, Walid Saad</li>
<li>for: 该论文寻求最大化无线能效 (EE)，在同时进行无线信息传输和能量传输的无人机飞行器协同网络中，采用teraHertz（THz）频率。</li>
<li>methods: 论文使用了非对称多访问（NOMA）功率分配系数、SWIPT功率拆分（PS）比率和无人机轨迹优化，以最大化EE。</li>
<li>results: 论文通过分解为两个阶段优化问题，使用替代优化方法，并通过比较基准点来证明提议的资源分配算法的效果。<details>
<summary>Abstract</summary>
This paper considers the energy efficiency (EE) maximization of a simultaneous wireless information and power transfer (SWIPT)-assisted unmanned aerial vehicles (UAV) cooperative network operating at TeraHertz (THz) frequencies. The source performs SWIPT enabling the UAV to receive both power and information while also transmitting the information to a designated destination node. Subsequently, the UAV utilizes the harvested energy to relay the data to the intended destination node effectively. Specifically, we maximize EE by optimizing the non-orthogonal multiple access (NOMA) power allocation coefficients, SWIPT power splitting (PS) ratio, and UAV trajectory. The main problem is broken down into a two-stage optimization problem and solved using an alternating optimization approach. In the first stage, optimization of the PS ratio and trajectory is performed by employing successive convex approximation using a lower bound on the exponential factor in the THz channel model. In the second phase, the NOMA power coefficients are optimized using a quadratic transform approach. Numerical results demonstrate the effectiveness of our proposed resource allocation algorithm compared to the baselines where there is no trajectory optimization or no NOMA power or PS optimization.
</details>
<details>
<summary>摘要</summary>
The problem is broken down into a two-stage optimization problem and solved using an alternating optimization approach. In the first stage, the PS ratio and trajectory are optimized using successive convex approximation with a lower bound on the exponential factor in the THz channel model. In the second stage, the NOMA power coefficients are optimized using a quadratic transform approach.Numerical results show that the proposed resource allocation algorithm outperforms baseline scenarios without trajectory optimization or NOMA power or PS optimization.
</details></li>
</ul>
<hr>
<h2 id="Study-of-Robust-Adaptive-Beamforming-Algorithms-Based-on-Power-Method-Processing-and-Spatial-Spectrum-Matching"><a href="#Study-of-Robust-Adaptive-Beamforming-Algorithms-Based-on-Power-Method-Processing-and-Spatial-Spectrum-Matching" class="headerlink" title="Study of Robust Adaptive Beamforming Algorithms Based on Power Method Processing and Spatial Spectrum Matching"></a>Study of Robust Adaptive Beamforming Algorithms Based on Power Method Processing and Spatial Spectrum Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13785">http://arxiv.org/abs/2309.13785</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Mohammadzadeh, V. H. Nascimento, R. C. de Lamare, O. Kukrer</li>
<li>for: 提高Robust adaptive beamforming（RAB）在covariance矩阵重建错误存在时的性能。</li>
<li>methods: 提议一种基于interference-plus-noise covariance（INC）矩阵重建的efficient RAB技术，包括根据力量方法估计干扰的电力和方向 вектор，然后使用空间匹配处理重建所需的信号-Plus-噪声矩阵。最后，排除噪声组件以保留所需的信号矩阵。</li>
<li>results: 对比已有方法，提议的方法可以更好地适应covariance矩阵重建错误的情况，并且可以提高RAB的性能。<details>
<summary>Abstract</summary>
Robust adaptive beamforming (RAB) based on interference-plus-noise covariance (INC) matrix reconstruction can experience performance degradation when model mismatch errors exist, particularly when the input signal-to-noise ratio (SNR) is large. In this work, we devise an efficient RAB technique for dealing with covariance matrix reconstruction issues. The proposed method involves INC matrix reconstruction using an idea in which the power and the steering vector of the interferences are estimated based on the power method. Furthermore, spatial match processing is computed to reconstruct the desired signal-plus-noise covariance matrix. Then, the noise components are excluded to retain the desired signal (DS) covariance matrix. A key feature of the proposed technique is to avoid eigenvalue decomposition of the INC matrix to obtain the dominant power of the interference-plus-noise region. Moreover, the INC reconstruction is carried out according to the definition of the theoretical INC matrix. Simulation results are shown and discussed to verify the effectiveness of the proposed method against existing approaches.
</details>
<details>
<summary>摘要</summary>
《robust适应 beamforming（RAB）基于干扰＋噪声 covariance（INC）矩阵重建可以遇到性能下降问题，特别是当输入信号响应比（SNR）较大时。在这种工作中，我们设计了一种高效的 RAB 技术来处理 INC 矩阵重建问题。该方法包括使用力量和扫描向量来估算干扰的 INC 矩阵，然后通过空间匹配处理来重建 желатель信号＋噪声 covariance 矩阵。最后，噪声成分被排除，保留 желатель信号 covariance 矩阵。本方法的一个关键特点是不需要对 INC 矩阵进行特征值分解，以获得干扰＋噪声区域的主要功率。此外，INC 重建遵循了理论 INC 矩阵的定义。实验结果表明，提议的方法比既有方法更高效。》Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/25/eess.SP_2023_09_25/" data-id="clp88dc7f01epob889zqp2nvg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/cs.SD_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T15:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/24/cs.SD_2023_09_24/">cs.SD - 2023-09-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Cross-modal-Alignment-with-Optimal-Transport-for-CTC-based-ASR"><a href="#Cross-modal-Alignment-with-Optimal-Transport-for-CTC-based-ASR" class="headerlink" title="Cross-modal Alignment with Optimal Transport for CTC-based ASR"></a>Cross-modal Alignment with Optimal Transport for CTC-based ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13650">http://arxiv.org/abs/2309.13650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai</li>
<li>for: 提高 CTCAASR 系统的准确率，使其能够更好地利用语言模型（LM）中的语言知识。</li>
<li>methods: 使用 optimal transport（OT）算法实现语音特征与文本特征之间的交叉模式对应，从而让语音特征编码上下文 dependent 语言特征。</li>
<li>results: 在 AISHELL-1 数据集上，我们的系统达到了 3.96% 和 4.27% 字符错误率（CER），对比基eline 系统而言，相对提高了 28.39% 和 29.42%。<details>
<summary>Abstract</summary>
Temporal connectionist temporal classification (CTC)-based automatic speech recognition (ASR) is one of the most successful end to end (E2E) ASR frameworks. However, due to the token independence assumption in decoding, an external language model (LM) is required which destroys its fast parallel decoding property. Several studies have been proposed to transfer linguistic knowledge from a pretrained LM (PLM) to the CTC based ASR. Since the PLM is built from text while the acoustic model is trained with speech, a cross-modal alignment is required in order to transfer the context dependent linguistic knowledge from the PLM to acoustic encoding. In this study, we propose a novel cross-modal alignment algorithm based on optimal transport (OT). In the alignment process, a transport coupling matrix is obtained using OT, which is then utilized to transform a latent acoustic representation for matching the context-dependent linguistic features encoded by the PLM. Based on the alignment, the latent acoustic feature is forced to encode context dependent linguistic information. We integrate this latent acoustic feature to build conformer encoder-based CTC ASR system. On the AISHELL-1 data corpus, our system achieved 3.96% and 4.27% character error rate (CER) for dev and test sets, respectively, which corresponds to relative improvements of 28.39% and 29.42% compared to the baseline conformer CTC ASR system without cross-modal knowledge transfer.
</details>
<details>
<summary>摘要</summary>
temporal connectionist temporal classification（CTC）基于自动语音识别（ASR）系统是最成功的端到端（E2E）ASR框架之一。然而，由于decode进程中的令符独立假设，需要一个外部语言模型（LM），这样会消除它的快速并行解码性。多个研究已经提出将语言知识从预训练语言模型（PLM）传递到CTC基于ASR系统。由于PLM是由文本建立的，而语音模型则是通过语音训练的，因此需要在语音编码和PLM中的语言知识之间进行交叉模式对齐。在本研究中，我们提出了一种基于最优运输（OT）的交叉模式对齐算法。在对齐过程中，使用OT获得了交叉运输矩阵，然后将其用于将 latent acoustic representation 变换为与语言模型（LM）中的上下文依赖的语言特征匹配。根据对齐，latent acoustic feature 被迫编码上下文依赖的语言信息。我们将这个latent acoustic feature 集成到基于CTC的ASR系统中，并在AISHELL-1数据集上进行测试。测试结果表明，我们的系统在dev和test集上的字符错误率（CER）分别为3.96%和4.27%，相对于基eline conformer CTC ASR系统而言，升幅分别为28.39%和29.42%。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Black-Box-Speaker-Verification-Model-Adaptation-with-Reprogramming-and-Backend-Learning"><a href="#Efficient-Black-Box-Speaker-Verification-Model-Adaptation-with-Reprogramming-and-Backend-Learning" class="headerlink" title="Efficient Black-Box Speaker Verification Model Adaptation with Reprogramming and Backend Learning"></a>Efficient Black-Box Speaker Verification Model Adaptation with Reprogramming and Backend Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13605">http://arxiv.org/abs/2309.13605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyu Li, Tan Lee</li>
<li>for: 这篇论文的目的是提出一种基于深度神经网络的话语识别系统中的领域匹配问题的解决方案，并且透过对模型的数据类型进行修改，以提高SV系统的性能。</li>
<li>methods: 这篇论文使用了一种基于对模型的数据类型进行修改的方法，即利用对模型的预设值进行修改，以实现领域匹配。这种方法通过估计模型的参数 gradients，将模型视为黑盒模型，并使用两层背景学习模组进行最终的适应。</li>
<li>results: 实验结果显示，这种方法可以在语言匹配情况下，对SV系统进行领域匹配，并且使用了 much less computation cost，实现了与完全调整的模型相似的性能。<details>
<summary>Abstract</summary>
The development of deep neural networks (DNN) has significantly enhanced the performance of speaker verification (SV) systems in recent years. However, a critical issue that persists when applying DNN-based SV systems in practical applications is domain mismatch. To mitigate the performance degradation caused by the mismatch, domain adaptation becomes necessary. This paper introduces an approach to adapt DNN-based SV models by manipulating the learnable model inputs, inspired by the concept of adversarial reprogramming. The pre-trained SV model remains fixed and functions solely in the forward process, resembling a black-box model. A lightweight network is utilized to estimate the gradients for the learnable parameters at the input, which bypasses the gradient backpropagation through the black-box model. The reprogrammed output is processed by a two-layer backend learning module as the final adapted speaker embedding. The number of parameters involved in the gradient calculation is small in our design. With few additional parameters, the proposed method achieves both memory and parameter efficiency. The experiments are conducted in language mismatch scenarios. Using much less computation cost, the proposed method obtains close or superior performance to the fully finetuned models in our experiments, which demonstrates its effectiveness.
</details>
<details>
<summary>摘要</summary>
Deep neural networks (DNN) 的发展有助于提高 speaker verification (SV) 系统的性能，但是在实际应用中，域名匹配问题仍然是一个主要的问题。为了解决这个问题，我们需要进行域名适应。这篇文章介绍了一种将 DNN-based SV 模型适应到域名不同的方法，通过修改可学习的模型输入，以及基于反对抗整形的概念。先前训练的 SV 模型保持不变，只参与前向处理，类似于黑盒模型。我们使用轻量级网络计算输入中的梯度，以便更新可学习参数。最终，我们使用两层后端学习模块来处理整形后的输出，并生成最终的适应的 speaker 嵌入。我们的设计具有少量参数，同时具有内存和参数效率。我们的实验结果表明，使用许多更少的计算成本，我们的方法可以在语言匹配场景中实现与完全训练模型相当或更好的性能。
</details></li>
</ul>
<hr>
<h2 id="The-second-multi-channel-multi-party-meeting-transcription-challenge-M2MeT-2-0-A-benchmark-for-speaker-attributed-ASR"><a href="#The-second-multi-channel-multi-party-meeting-transcription-challenge-M2MeT-2-0-A-benchmark-for-speaker-attributed-ASR" class="headerlink" title="The second multi-channel multi-party meeting transcription challenge (M2MeT) 2.0): A benchmark for speaker-attributed ASR"></a>The second multi-channel multi-party meeting transcription challenge (M2MeT) 2.0): A benchmark for speaker-attributed ASR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13573">http://arxiv.org/abs/2309.13573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Liang, Mohan Shi, Fan Yu, Yangze Li, Shiliang Zhang, Zhihao Du, Qian Chen, Lei Xie, Yanmin Qian, Jian Wu, Zhuo Chen, Kong Aik Lee, Zhijie Yan, Hui Bu</li>
<li>for: 本文主要探讨了一个实际场景中的“谁说了什么， WHEN”问题，即speaker-attributed ASR (SA-ASR)问题。</li>
<li>methods: 本文使用了两个子track：固定训练条件子track和开放训练条件子track。固定训练条件子track限制了训练数据的使用，但允许参与者使用任何开源预训练模型。开放训练条件子track则允许参与者使用所有可用数据和模型。</li>
<li>results: 本文公布了一个新的10小时测试集，用于排名挑战。本文还提供了参与者提交系统的结果和分析，作为SA-ASR领域的现状标准。<details>
<summary>Abstract</summary>
With the success of the first Multi-channel Multi-party Meeting Transcription challenge (M2MeT), the second M2MeT challenge (M2MeT 2.0) held in ASRU2023 particularly aims to tackle the complex task of \emph{speaker-attributed ASR (SA-ASR)}, which directly addresses the practical and challenging problem of ``who spoke what at when" at typical meeting scenario. We particularly established two sub-tracks. The fixed training condition sub-track, where the training data is constrained to predetermined datasets, but participants can use any open-source pre-trained model. The open training condition sub-track, which allows for the use of all available data and models without limitation. In addition, we release a new 10-hour test set for challenge ranking. This paper provides an overview of the dataset, track settings, results, and analysis of submitted systems, as a benchmark to show the current state of speaker-attributed ASR.
</details>
<details>
<summary>摘要</summary>
在M2MeT挑战的成功之后，M2MeT 2.0挑战在ASRU2023中进一步挑战了复杂的 speaker-attributed ASR（SA-ASR）任务，直接面临typical会议场景中的“谁说了什么，何时”问题。我们设置了两个子轨道。固定培训条件子轨道，团队可以使用预先训练的任何开源模型，但是团队必须使用 predetermined datasets 进行培训。开放培训条件子轨道，允许使用所有可用的数据和模型。此外，我们发布了一个新的10小时测试集，用于挑战排名。本文提供了数据集、轨道设置、结果和分析 submitted系统的概述，作为SA-ASR现状的标准 referential。
</details></li>
</ul>
<hr>
<h2 id="Coco-Nut-Corpus-of-Japanese-Utterance-and-Voice-Characteristics-Description-for-Prompt-based-Control"><a href="#Coco-Nut-Corpus-of-Japanese-Utterance-and-Voice-Characteristics-Description-for-Prompt-based-Control" class="headerlink" title="Coco-Nut: Corpus of Japanese Utterance and Voice Characteristics Description for Prompt-based Control"></a>Coco-Nut: Corpus of Japanese Utterance and Voice Characteristics Description for Prompt-based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13509">http://arxiv.org/abs/2309.13509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aya Watanabe, Shinnosuke Takamichi, Yuki Saito, Wataru Nakata, Detai Xin, Hiroshi Saruwatari</li>
<li>for: 研究控制语音特征的多目的语音合成。</li>
<li>methods: 使用文本conditioned生成，如文本-图像生成，以实现直觉和复杂的语音特征控制。</li>
<li>results: 开发了一个新的语音 corpus，包括多样化的日本语音样本，以及相应的文本转录和自由形式语音特征描述。<details>
<summary>Abstract</summary>
In text-to-speech, controlling voice characteristics is important in achieving various-purpose speech synthesis. Considering the success of text-conditioned generation, such as text-to-image, free-form text instruction should be useful for intuitive and complicated control of voice characteristics. A sufficiently large corpus of high-quality and diverse voice samples with corresponding free-form descriptions can advance such control research. However, neither an open corpus nor a scalable method is currently available. To this end, we develop Coco-Nut, a new corpus including diverse Japanese utterances, along with text transcriptions and free-form voice characteristics descriptions. Our methodology to construct this corpus consists of 1) automatic collection of voice-related audio data from the Internet, 2) quality assurance, and 3) manual annotation using crowdsourcing. Additionally, we benchmark our corpus on the prompt embedding model trained by contrastive speech-text learning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>在文本到语音Synthesizer中，控制声音特征是关键以实现多种目标 speech synthesis。考虑到文本条件生成的成功，如文本到图像，自由形文本指令可以为Intuitive和复杂的声音控制提供便利。一个具有充分覆盖和多样性的声音样本库可以提高这种控制研究。然而，目前并没有公开的库 nor可扩展的方法。为此，我们开发了Coco-Nut，一个新的声音库，包括日本语音样本，以及文本转录和自由形声音特征描述。我们的方法包括：1. 自动从互联网上收集声音相关的音频数据2. 质量控制3. 使用人工投票来手动标注此外，我们对这个库进行了基于对比Speech-text学习的唤起式模型的测试。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/cs.SD_2023_09_24/" data-id="clp88dc1g00zpob88dajae2rq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/cs.CV_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T13:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/24/cs.CV_2023_09_24/">cs.CV - 2023-09-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Diffeomorphic-Multi-Resolution-Deep-Learning-Registration-for-Applications-in-Breast-MRI"><a href="#Diffeomorphic-Multi-Resolution-Deep-Learning-Registration-for-Applications-in-Breast-MRI" class="headerlink" title="Diffeomorphic Multi-Resolution Deep Learning Registration for Applications in Breast MRI"></a>Diffeomorphic Multi-Resolution Deep Learning Registration for Applications in Breast MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13777">http://arxiv.org/abs/2309.13777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew G. French, Gonzalo D. Maso Talou, Thiranja P. Babarenda Gamage, Martyn P. Nash, Poul M. Nielsen, Anthony J. Doyle, Juan Eugenio Iglesias, Yaël Balbastre, Sean I. Young</li>
<li>for: 静脉成像规划中的精准注册可以提高乳腺癌治疗中肿瘤的定位。</li>
<li>methods: 本文提出了一种learning-based注册方法，该方法遵循 diffeomorphic 约束，并且在静脉成像中提供了优秀的注册结果。</li>
<li>results: 本文的实验结果表明，该注册方法可以提供高质量的注册结果，同时也遵循 diffeomorphic 约束。<details>
<summary>Abstract</summary>
In breast surgical planning, accurate registration of MR images across patient positions has the potential to improve the localisation of tumours during breast cancer treatment. While learning-based registration methods have recently become the state-of-the-art approach for most medical image registration tasks, these methods have yet to make inroads into breast image registration due to certain difficulties-the lack of rich texture information in breast MR images and the need for the deformations to be diffeomophic. In this work, we propose learning strategies for breast MR image registration that are amenable to diffeomorphic constraints, together with early experimental results from in-silico and in-vivo experiments. One key contribution of this work is a registration network which produces superior registration outcomes for breast images in addition to providing diffeomorphic guarantees.
</details>
<details>
<summary>摘要</summary>
医学影像识别是一个重要的领域，它可以帮助医生更好地识别和治疗癌症。在乳腺癌治疗中，精准地将MR图像注册到患者的不同位置中有可能提高肿瘤的定位。然而，学习基本的注册方法在乳腺影像注册中尚未得到广泛应用，因为乳腺MR图像的纹理信息缺乏，并且需要的变换是 diffeomophic。在这种情况下，我们提出了一些学习策略，可以考虑到 diffeomorphic 约束，并且在实验中获得了出色的注册结果。我们的一个关键贡献是一种注册网络，可以生成高质量的注册结果，同时也提供 diffeomorphic  garanties。
</details></li>
</ul>
<hr>
<h2 id="Motion-Segmentation-from-a-Moving-Monocular-Camera"><a href="#Motion-Segmentation-from-a-Moving-Monocular-Camera" class="headerlink" title="Motion Segmentation from a Moving Monocular Camera"></a>Motion Segmentation from a Moving Monocular Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13772">http://arxiv.org/abs/2309.13772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Huang, John Zelek</li>
<li>for: 能够减少视觉SLAM或SFM中的运动物体识别，以便建立地图。</li>
<li>methods: synergistically fusing two popular branches of monocular motion segmentation approaches：point trajectory based和optical flow based methods。</li>
<li>results: 在KT3DMoSeg dataset上达到了状态计算机科学技术的表现水平，能够处理复杂的运动和场景结构。<details>
<summary>Abstract</summary>
Identifying and segmenting moving objects from a moving monocular camera is difficult when there is unknown camera motion, different types of object motions and complex scene structures. To tackle these challenges, we take advantage of two popular branches of monocular motion segmentation approaches: point trajectory based and optical flow based methods, by synergistically fusing these two highly complementary motion cues at object level. By doing this, we are able to model various complex object motions in different scene structures at once, which has not been achieved by existing methods. We first obtain object-specific point trajectories and optical flow mask for each common object in the video, by leveraging the recent foundational models in object recognition, segmentation and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the whole video using epipolar geometry and the motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices and obtain the final clustering. Our method shows state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. Being able to identify moving objects allows us to remove them for map building when using visual SLAM or SFM.
</details>
<details>
<summary>摘要</summary>
Difficulties in identifying and segmenting moving objects from a moving monocular camera include unknown camera motion, diverse object motions, and complex scene structures. To address these challenges, we synergistically fuse two popular monocular motion segmentation approaches: point trajectory-based and optical flow-based methods, at the object level. This enables us to model various complex object motions in different scene structures simultaneously, which has not been achieved by existing methods.We first obtain object-specific point trajectories and optical flow masks for each common object in the video by leveraging recent foundational models in object recognition, segmentation, and tracking. We then construct two robust affinity matrices representing the pairwise object motion affinities throughout the entire video using epipolar geometry and motion information provided by optical flow. Finally, co-regularized multi-view spectral clustering is used to fuse the two affinity matrices, resulting in the final clustering. Our method achieves state-of-the-art performance on the KT3DMoSeg dataset, which contains complex motions and scene structures. By identifying moving objects, we can remove them for map building when using visual SLAM or SFM.
</details></li>
</ul>
<hr>
<h2 id="Devil-in-the-Number-Towards-Robust-Multi-modality-Data-Filter"><a href="#Devil-in-the-Number-Towards-Robust-Multi-modality-Data-Filter" class="headerlink" title="Devil in the Number: Towards Robust Multi-modality Data Filter"></a>Devil in the Number: Towards Robust Multi-modality Data Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13770">http://arxiv.org/abs/2309.13770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichen Xu, Zihan Xu, Wenhao Chai, Zhonghan Zhao, Enxin Song, Gaoang Wang</li>
<li>For: 这个研究的目的是为了提高CLIP的表现和降低训练成本，通过适当的筛选方法来筛选多modal资料集。* Methods: 这个研究使用了CLIP score筛选器和文本检测方法来筛选资料。在分析资料集时，我们发现了大量的重复信息，例如数字，在文本内容中。我们进行了实验，发现这些重复元素对CLIP scores有着内在的影响。* Results: 我们的文本基于CLIP筛选器在DataComp中的“小规模”频道上比顶尖方法表现出色，实现了3.6%的性能提升。实验还显示了我们提议的文本填充筛选器比原始CLIP score筛选器在选择顶尖40%的资料时表现更好。此外，我们的研究还发现了数字对CLIP和其处理的影响，具有价值的指导意义，包括语言重写技术。<details>
<summary>Abstract</summary>
In order to appropriately filter multi-modality data sets on a web-scale, it becomes crucial to employ suitable filtering methods to boost performance and reduce training costs. For instance, LAION papers employs the CLIP score filter to select data with CLIP scores surpassing a certain threshold. On the other hand, T-MARS achieves high-quality data filtering by detecting and masking text within images and then filtering by CLIP score. Through analyzing the dataset, we observe a significant proportion of redundant information, such as numbers, present in the textual content. Our experiments on a subset of the data unveil the profound impact of these redundant elements on the CLIP scores. A logical approach would involve reevaluating the CLIP scores after eliminating these influences. Experimentally, our text-based CLIP filter outperforms the top-ranked method on the ``small scale" of DataComp (a data filtering benchmark) on ImageNet distribution shifts, achieving a 3.6% performance improvement. The results also demonstrate that our proposed text-masked filter outperforms the original CLIP score filter when selecting the top 40% of the data. The impact of numbers on CLIP and their handling provide valuable insights for improving the effectiveness of CLIP training, including language rewrite techniques.
</details>
<details>
<summary>摘要</summary>
We observe a significant amount of redundant information, such as numbers, in the textual content of the dataset. Our experiments on a subset of the data reveal that these redundant elements have a profound impact on the CLIP scores. A logical approach would be to reevaluate the CLIP scores after eliminating these influences.Experimentally, our text-based CLIP filter outperforms the top-ranked method on the "small scale" of DataComp (a data filtering benchmark) on ImageNet distribution shifts, achieving a 3.6% performance improvement. The results also show that our proposed text-masked filter outperforms the original CLIP score filter when selecting the top 40% of the data. The impact of numbers on CLIP and their handling provide valuable insights for improving the effectiveness of CLIP training, including language rewrite techniques.
</details></li>
</ul>
<hr>
<h2 id="Combining-Two-Adversarial-Attacks-Against-Person-Re-Identification-Systems"><a href="#Combining-Two-Adversarial-Attacks-Against-Person-Re-Identification-Systems" class="headerlink" title="Combining Two Adversarial Attacks Against Person Re-Identification Systems"></a>Combining Two Adversarial Attacks Against Person Re-Identification Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13763">http://arxiv.org/abs/2309.13763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo de O. Andrade, Igor Garcia Ballhausen Sampaio, Joris Guérin, José Viterbo</li>
<li>for: 这个研究是针对人员识别系统（Re-ID）的安全性进行研究，尤其是运用深度神经网络来实现人员识别。</li>
<li>methods: 本研究使用了两种攻击方法：P-FGSM和Deep Mis-Ranking，并且将其应用到两个受测Re-ID模型：IDE（ResNet-50）和AlignedReID。</li>
<li>results: 研究结果显示，这些攻击方法可以对Re-ID模型造成较大的影响，其中AlignedReID在CUHK03 dataset上的 Rank-10 指数下降了3.36%。此外，研究者还尝试使用Dropout进行防护。<details>
<summary>Abstract</summary>
The field of Person Re-Identification (Re-ID) has received much attention recently, driven by the progress of deep neural networks, especially for image classification. The problem of Re-ID consists in identifying individuals through images captured by surveillance cameras in different scenarios. Governments and companies are investing a lot of time and money in Re-ID systems for use in public safety and identifying missing persons. However, several challenges remain for successfully implementing Re-ID, such as occlusions and light reflections in people's images. In this work, we focus on adversarial attacks on Re-ID systems, which can be a critical threat to the performance of these systems. In particular, we explore the combination of adversarial attacks against Re-ID models, trying to strengthen the decrease in the classification results. We conduct our experiments on three datasets: DukeMTMC-ReID, Market-1501, and CUHK03. We combine the use of two types of adversarial attacks, P-FGSM and Deep Mis-Ranking, applied to two popular Re-ID models: IDE (ResNet-50) and AlignedReID. The best result demonstrates a decrease of 3.36% in the Rank-10 metric for AlignedReID applied to CUHK03. We also try to use Dropout during the inference as a defense method.
</details>
<details>
<summary>摘要</summary>
人员重复识别（Re-ID）领域在最近几年内受到了广泛关注，启发于深度神经网络的进步，特别是图像分类。Re-ID问题的核心是通过不同场景的安全摄像头捕捉到人员的图像，并在不同的环境下进行人员识别。政府和公司在公共安全和失踪人员问题上投入了大量时间和资金，以实现Re-ID系统的应用。然而，Re-ID实施还存在一些挑战，如人像中的遮挡和反射光。在这种情况下，我们将关注Re-ID系统中的对抗攻击，这可能会对系统的性能产生重要的威胁。我们在三个数据集上进行了实验：DukeMTMC-ReID、Market-1501和CUHK03。我们将两种对抗攻击相结合：P-FGSM和Deep Mis-Ranking，并将其应用于两种流行的Re-ID模型：IDE（ResNet-50）和AlignedReID。最佳结果表明，对CUHK03数据集应用AlignedReID模型，P-FGSM和Deep Mis-Ranking的组合可以导致rank-10指标下的下降为3.36%。我们还尝试了在推理过程中使用Dropout作为防御方法。
</details></li>
</ul>
<hr>
<h2 id="Look-Ma-no-code-fine-tuning-nnU-Net-for-the-AutoPET-II-challenge-by-only-adjusting-its-JSON-plans"><a href="#Look-Ma-no-code-fine-tuning-nnU-Net-for-the-AutoPET-II-challenge-by-only-adjusting-its-JSON-plans" class="headerlink" title="Look Ma, no code: fine tuning nnU-Net for the AutoPET II challenge by only adjusting its JSON plans"></a>Look Ma, no code: fine tuning nnU-Net for the AutoPET II challenge by only adjusting its JSON plans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13747">http://arxiv.org/abs/2309.13747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabian Isensee, Klaus H. Maier-Hein</li>
<li>for: 提高 AutoPET II 挑战中 nnU-Net 的性能</li>
<li>methods: 通过 modifying nnU-Net 的 ‘nnUNetPlans.json’ 文件，switch to UNet with residual encoder，增加 batch size 和 patch size，以提高模型的性能</li>
<li>results: 比自动配置的 nnU-Net 基eline（5-fold cross-validation Dice score of 65.14 vs 33.28）substantially outperform，但是需要更多的计算资源来训练模型。最终提交ensemble两个最有前途的配置。当提交时，我们的方法在预测集上排名第一。<details>
<summary>Abstract</summary>
We participate in the AutoPET II challenge by modifying nnU-Net only through its easy to understand and modify 'nnUNetPlans.json' file. By switching to a UNet with residual encoder, increasing the batch size and increasing the patch size we obtain a configuration that substantially outperforms the automatically configured nnU-Net baseline (5-fold cross-validation Dice score of 65.14 vs 33.28) at the expense of increased compute requirements for model training. Our final submission ensembles the two most promising configurations. At the time of submission our method ranks first on the preliminary test set.
</details>
<details>
<summary>摘要</summary>
我们参加了AutoPET II挑战，只通过nnUNetPlans.json文件进行 modify nnU-Net。通过更改残差编码器，增加批处理大小和增加补做大小，我们获得了与自动配置的nnU-Net基线（5次交叉验证精度分数为65.14 vs 33.28）的性能显著提高，但是需要更高的计算资源来训练模型。我们最终提交的结果是两种最有前途的配置的ensemble。在提交时，我们的方法在预测集上排名第一。Note: "nnUNetPlans.json" is a JSON file that contains the architecture of the nnU-Net model, and it is "easy to understand and modify" as mentioned in the text.
</details></li>
</ul>
<hr>
<h2 id="DROP-Dynamics-Responses-from-Human-Motion-Prior-and-Projective-Dynamics"><a href="#DROP-Dynamics-Responses-from-Human-Motion-Prior-and-Projective-Dynamics" class="headerlink" title="DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics"></a>DROP: Dynamics Responses from Human Motion Prior and Projective Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13742">http://arxiv.org/abs/2309.13742</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifeng Jiang, Jungdam Won, Yuting Ye, C. Karen Liu</li>
<li>for: 这篇论文旨在实现人类动作的生成和跟踪，以满足计算机视觉、运动和医疗等领域的需求。</li>
<li>methods: 该论文提出了一种名为DROP的新框架，它利用生成式动作优先逻辑和投影动力来模型人类动作的响应。</li>
<li>results: 经过广泛的评估，DROP模型在不同的动作任务和物理干扰下表现出了可scalability和多样性的特点。<details>
<summary>Abstract</summary>
Synthesizing realistic human movements, dynamically responsive to the environment, is a long-standing objective in character animation, with applications in computer vision, sports, and healthcare, for motion prediction and data augmentation. Recent kinematics-based generative motion models offer impressive scalability in modeling extensive motion data, albeit without an interface to reason about and interact with physics. While simulator-in-the-loop learning approaches enable highly physically realistic behaviors, the challenges in training often affect scalability and adoption. We introduce DROP, a novel framework for modeling Dynamics Responses of humans using generative mOtion prior and Projective dynamics. DROP can be viewed as a highly stable, minimalist physics-based human simulator that interfaces with a kinematics-based generative motion prior. Utilizing projective dynamics, DROP allows flexible and simple integration of the learned motion prior as one of the projective energies, seamlessly incorporating control provided by the motion prior with Newtonian dynamics. Serving as a model-agnostic plug-in, DROP enables us to fully leverage recent advances in generative motion models for physics-based motion synthesis. We conduct extensive evaluations of our model across different motion tasks and various physical perturbations, demonstrating the scalability and diversity of responses.
</details>
<details>
<summary>摘要</summary>
实现人类动作的实惠真实、对环境 dynamically responsive 是动画人物的长期目标，应用于电脑感知、运动和医疗等领域，如动作预测和数据增强。现有的运动基础的生成动作模型可以实现广泛的动作数据模型，但是没有与物理相互作用的界面。而使用模拟器-在-the-loop 学习方法可以实现高度的物理真实行为，但是训练问题往往会影响数据量和采纳。我们介绍了 DROP，一个新的框架，用于模型人类动作的 Dynamics Responses，使用生成动作假设和投影动力学。 DROP 可以被视为一个高度稳定、最小化的物理基础的人类模拟器，与生成动作假设的投影动力学相互作用。通过将学习的动作假设作为投影能量的一部分，DROP 允许flexible和简单地整合已学习的动作假设和新频率动力学。作为一个模型无关的插件，DROP 允许我们充分利用最近的生成动作模型，以便实现物理基础的动作合成。我们在不同的动作任务和各种物理损害中进行了广泛的评估，证明了 DROP 的普遍性和多样性。
</details></li>
</ul>
<hr>
<h2 id="MOSAIC-Multi-Object-Segmented-Arbitrary-Stylization-Using-CLIP"><a href="#MOSAIC-Multi-Object-Segmented-Arbitrary-Stylization-Using-CLIP" class="headerlink" title="MOSAIC: Multi-Object Segmented Arbitrary Stylization Using CLIP"></a>MOSAIC: Multi-Object Segmented Arbitrary Stylization Using CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13716">http://arxiv.org/abs/2309.13716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prajwal Ganugula, Y S S S Santosh Kumar, N K Sagar Reddy, Prabhath Chellingi, Avinash Thakur, Neeraj Kasera, C Shyam Anand</li>
<li>for: 这篇论文的目的是提出一种基于文本提示的多对象分割自由风格化方法，以提高风格化图像的控制精度和扩展性。</li>
<li>methods: 该方法使用了视transformer架构进行文本基于分割和风格化模块，可以针对不同的对象进行精细的风格化控制。</li>
<li>results: 该方法可以生成高质量的风格化图像，并且可以在不同的对象类上进行扩展性测试，而且可以在不同的风格转换中保持图像的可读性。<details>
<summary>Abstract</summary>
Style transfer driven by text prompts paved a new path for creatively stylizing the images without collecting an actual style image. Despite having promising results, with text-driven stylization, the user has no control over the stylization. If a user wants to create an artistic image, the user requires fine control over the stylization of various entities individually in the content image, which is not addressed by the current state-of-the-art approaches. On the other hand, diffusion style transfer methods also suffer from the same issue because the regional stylization control over the stylized output is ineffective. To address this problem, We propose a new method Multi-Object Segmented Arbitrary Stylization Using CLIP (MOSAIC), that can apply styles to different objects in the image based on the context extracted from the input prompt. Text-based segmentation and stylization modules which are based on vision transformer architecture, were used to segment and stylize the objects. Our method can extend to any arbitrary objects, styles and produce high-quality images compared to the current state of art methods. To our knowledge, this is the first attempt to perform text-guided arbitrary object-wise stylization. We demonstrate the effectiveness of our approach through qualitative and quantitative analysis, showing that it can generate visually appealing stylized images with enhanced control over stylization and the ability to generalize to unseen object classes.
</details>
<details>
<summary>摘要</summary>
文本驱动的样式传递开创了一条新的创作图像样式化路径，而不需要实际收集样式图像。尽管有promising结果，文本驱动样式化方法还有一个问题：用户无法控制样式化的精度。如果用户想创造艺术图像，用户需要精准地控制图像中的多种实体的样式化。现有的approach都无法解决这个问题。另一方面，扩散样式传递方法也有同样的问题，因为对彩色输出的区域样式控制是无效的。为解决这个问题，我们提出了一种新的方法：多对象分割自由样式传递使用CLIP（MOSAIC）。我们使用了基于视力转换器架构的文本基于分割和样式化模块，可以将不同的对象在图像中应用不同的样式。我们的方法可以扩展到任意对象、样式和生成高质量图像，比现状态的方法更高效。我们知道，这是文本引导自由对象样式传递的首次尝试。我们通过质量和量化分析，证明我们的方法可以生成美观的样式化图像，并且可以增强样式化的控制和泛化到未看过的对象类型。
</details></li>
</ul>
<hr>
<h2 id="Sound-Print-Generalised-Face-Presentation-Attack-Detection-using-Deep-Representation-of-Sound-Echoes"><a href="#Sound-Print-Generalised-Face-Presentation-Attack-Detection-using-Deep-Representation-of-Sound-Echoes" class="headerlink" title="Sound-Print: Generalised Face Presentation Attack Detection using Deep Representation of Sound Echoes"></a>Sound-Print: Generalised Face Presentation Attack Detection using Deep Representation of Sound Echoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13704">http://arxiv.org/abs/2309.13704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raghavendra Ramachandra, Jag Mohan Singh, Sushma Venkatesh</li>
<li>for: 这篇论文主要目的是提出一种基于阴投信号的声学回音攻击探测方法，以实现智能手机上的面部识别系统中的安全性。</li>
<li>methods: 本论文使用的方法包括对于声学回音的分析和模elling，并提出一种基于宽频脉冲的传输信号，以提高信号与噪音的比例。</li>
<li>results: 实验结果显示，提出的方法可以妥善地探测不同类型的面部攻击，包括印刷攻击、显示攻击和塑胶面伪攻击。<details>
<summary>Abstract</summary>
Facial biometrics are widely deployed in smartphone-based applications because of their usability and increased verification accuracy in unconstrained scenarios. The evolving applications of smartphone-based facial recognition have also increased Presentation Attacks (PAs), where an attacker can present a Presentation Attack Instrument (PAI) to maliciously gain access to the application. Because the materials used to generate PAI are not deterministic, the detection of unknown presentation attacks is challenging. In this paper, we present an acoustic echo-based face Presentation Attack Detection (PAD) on a smartphone in which the PAs are detected based on the reflection profiles of the transmitted signal. We propose a novel transmission signal based on the wide pulse that allows us to model the background noise before transmitting the signal and increase the Signal-to-Noise Ratio (SNR). The received signal reflections were processed to remove background noise and accurately represent reflection characteristics. The reflection profiles of the bona fide and PAs are different owing to the different reflection characteristics of the human skin and artefact materials. Extensive experiments are presented using the newly collected Acoustic Sound Echo Dataset (ASED) with 4807 samples captured from bona fide and four different types of PAIs, including print (two types), display, and silicone face-mask attacks. The obtained results indicate the robustness of the proposed method for detecting unknown face presentation attacks.
</details>
<details>
<summary>摘要</summary>
“人脸生物特征在智能手机应用中广泛应用，因为它们的使用性和无限制场景中的验证精度提高。随着智能手机上的人脸识别应用的发展，也增加了演示攻击（PA），其中攻击者可以使用演示攻击工具（PAI）来恶意获取应用程序。由于攻击工具的材料不决定性，检测未知的演示攻击是困难的。在这篇论文中，我们提出了基于声学回音的人脸演示攻击检测（PAD）方法，在智能手机上进行。我们提出了一种基于宽PULSE的新的传输信号，使得我们可以在传输信号之前模拟背景噪声，提高信号噪声比（SNR）。接收到的声学回音后，我们对噪声进行了处理，以便准确地表示回音特征。人脸和攻击工具的反射特征不同，因为人脸和 artifact材料的反射特征不同。我们在新收集的声学回音数据集（ASED）上进行了广泛的实验，该数据集包含4807个样本，其中有4种不同类型的PAI，包括印刷（两种）、显示和塑料面具攻击。获得的结果表明，我们提出的方法对于检测未知的人脸演示攻击具有坚定的Robustness。”
</details></li>
</ul>
<hr>
<h2 id="Video-Adverse-Weather-Component-Suppression-Network-via-Weather-Messenger-and-Adversarial-Backpropagation"><a href="#Video-Adverse-Weather-Component-Suppression-Network-via-Weather-Messenger-and-Adversarial-Backpropagation" class="headerlink" title="Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation"></a>Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13700">http://arxiv.org/abs/2309.13700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/scott-yjyang/ViWS-Net">https://github.com/scott-yjyang/ViWS-Net</a></li>
<li>paper_authors: Yijun Yang, Angelica I. Aviles-Rivero, Huazhu Fu, Ye Liu, Weiming Wang, Lei Zhu</li>
<li>for:  Restoring videos degraded by any weather condition</li>
<li>methods:  Video adverse-weather-component suppression network (ViWS-Net), including a weather-agnostic video transformer encoder, long short-term temporal modeling mechanism, weather discriminator, and messenger-driven video transformer decoder</li>
<li>results:  Outperforms current state-of-the-art methods in restoring videos degraded by any weather condition, on benchmark datasets and real-world weather videos.<details>
<summary>Abstract</summary>
Although convolutional neural networks (CNNs) have been proposed to remove adverse weather conditions in single images using a single set of pre-trained weights, they fail to restore weather videos due to the absence of temporal information. Furthermore, existing methods for removing adverse weather conditions (e.g., rain, fog, and snow) from videos can only handle one type of adverse weather. In this work, we propose the first framework for restoring videos from all adverse weather conditions by developing a video adverse-weather-component suppression network (ViWS-Net). To achieve this, we first devise a weather-agnostic video transformer encoder with multiple transformer stages. Moreover, we design a long short-term temporal modeling mechanism for weather messenger to early fuse input adjacent video frames and learn weather-specific information. We further introduce a weather discriminator with gradient reversion, to maintain the weather-invariant common information and suppress the weather-specific information in pixel features, by adversarially predicting weather types. Finally, we develop a messenger-driven video transformer decoder to retrieve the residual weather-specific feature, which is spatiotemporally aggregated with hierarchical pixel features and refined to predict the clean target frame of input videos. Experimental results, on benchmark datasets and real-world weather videos, demonstrate that our ViWS-Net outperforms current state-of-the-art methods in terms of restoring videos degraded by any weather condition.
</details>
<details>
<summary>摘要</summary>
尽管卷积神经网络（CNNs）已经提议用单个预训练 веса来去除单个图像中的不良天气情况，但它们无法恢复天气视频，因为缺乏时间信息。此外，现有的天气视频修复方法只能处理一种类型的不良天气。在这种情况下，我们提出了第一个可以恢复所有不良天气视频的框架，即视频不良天气组件抑制网络（ViWS-Net）。以实现这一目标，我们首先设计了不同天气情况下的视频转换器编码器，其中包括多个转换器阶段。此外，我们还设计了一种长期短期模型来早期融合输入视频帧的邻近信息，以学习天气特定的信息。此外，我们还引入了一种天气预测器，以便对输入视频帧的像素特征进行拟合，并且通过对天气类型进行反向推导，以维护天气不变的通用信息，并抑制天气特定的信息。最后，我们开发了一种天气驱动的视频转换器解码器，以恢复输入视频中的剩余天气特定特征，并将其空间时间聚合和归一化，以预测输入视频的干净目标帧。实验结果，在标准测试集和实际天气视频上，表明我们的ViWS-Net可以超越当前状态的方法，在任何天气条件下恢复受损的视频。
</details></li>
</ul>
<hr>
<h2 id="Causal-DFQ-Causality-Guided-Data-free-Network-Quantization"><a href="#Causal-DFQ-Causality-Guided-Data-free-Network-Quantization" class="headerlink" title="Causal-DFQ: Causality Guided Data-free Network Quantization"></a>Causal-DFQ: Causality Guided Data-free Network Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13682">http://arxiv.org/abs/2309.13682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/42shawn/causal-dfq">https://github.com/42shawn/causal-dfq</a></li>
<li>paper_authors: Yuzhang Shang, Bingxin Xu, Gaowen Liu, Ramana Kompella, Yan Yan</li>
<li>For: 这个研究的目的是为了解决在实际应用中无法提供训练数据的情况下，深度神经网络协商过程中的问题。* Methods: 这个研究使用了 causal reasoning 来建立 causal 图模型，并提出了一个基于 causality 的 data-free network quantization 方法（Causal-DFQ），以消除依赖于数据的限制。* Results: 实验结果显示，Causal-DFQ 能够将深度神经网络协商到更小的网络，并且可以在不需要训练数据的情况下保持比较高的预测性能。<details>
<summary>Abstract</summary>
Model quantization, which aims to compress deep neural networks and accelerate inference speed, has greatly facilitated the development of cumbersome models on mobile and edge devices. There is a common assumption in quantization methods from prior works that training data is available. In practice, however, this assumption cannot always be fulfilled due to reasons of privacy and security, rendering these methods inapplicable in real-life situations. Thus, data-free network quantization has recently received significant attention in neural network compression. Causal reasoning provides an intuitive way to model causal relationships to eliminate data-driven correlations, making causality an essential component of analyzing data-free problems. However, causal formulations of data-free quantization are inadequate in the literature. To bridge this gap, we construct a causal graph to model the data generation and discrepancy reduction between the pre-trained and quantized models. Inspired by the causal understanding, we propose the Causality-guided Data-free Network Quantization method, Causal-DFQ, to eliminate the reliance on data via approaching an equilibrium of causality-driven intervened distributions. Specifically, we design a content-style-decoupled generator, synthesizing images conditioned on the relevant and irrelevant factors; then we propose a discrepancy reduction loss to align the intervened distributions of the pre-trained and quantized models. It is worth noting that our work is the first attempt towards introducing causality to data-free quantization problem. Extensive experiments demonstrate the efficacy of Causal-DFQ. The code is available at https://github.com/42Shawn/Causal-DFQ.
</details>
<details>
<summary>摘要</summary>
模型减量，用于压缩深度神经网络并加速推理速度，已经大大便化了移动和边缘设备上的模型开发。但是，现实中的假设是所有训练数据都可用，而在实际应用中，这个假设不一定成立，因为隐私和安全问题。因此，无数据网络减量在神经网络压缩中收到了重要注意。 causal reasoning提供了一种直观的方式来模型 causal 关系，以消除数据驱动的相关性，使 causality 成为分析无数据问题的关键组成部分。然而， literature 中关于无数据网络减量的 causal 表述不充分。为了bridging这个差距，我们构建了 causal 图来模型数据生成和差异减少 между 预训练和减量模型。 inspirited  by causal 理解，我们提出了 causality-guided 无数据网络减量方法（Causal-DFQ），以消除数据的依赖性。具体来说，我们设计了内容-风格-分解的生成器，通过conditioning 图像的相关和 irrelevant 因素来生成图像。然后，我们提出了干扰分布的减少损失，以将预训练和减量模型之间的 intervened 分布接近。值得注意的是，我们的工作是无数据网络减量问题中首次引入 causality 的尝试。extensive  experiments 表明了 Causal-DFQ 的有效性。代码可以在 https://github.com/42Shawn/Causal-DFQ 上获取。
</details></li>
</ul>
<hr>
<h2 id="BdSpell-A-YOLO-based-Real-time-Finger-Spelling-System-for-Bangla-Sign-Language"><a href="#BdSpell-A-YOLO-based-Real-time-Finger-Spelling-System-for-Bangla-Sign-Language" class="headerlink" title="BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign Language"></a>BdSpell: A YOLO-based Real-time Finger Spelling System for Bangla Sign Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13676">http://arxiv.org/abs/2309.13676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naimul Haque, Meraj Serker, Tariq Bin Bashar</li>
<li>for: 提高孟加拉手语（BdSL）解释的可用性和包容性，增进孟加拉手语社区中的语言平等。</li>
<li>methods: 基于YOLOv5架构的实时手势识别系统，采用特定规则和数字类作为触发器，高效生成隐藏和复合字符，消减用户的压力。</li>
<li>results: 实现字符识别时间优化为1.32秒，准确率达98%，YOLOv5模型在9147张图像上显示出极高的平均精度报告率（mAP）为96.4%。<details>
<summary>Abstract</summary>
In the domain of Bangla Sign Language (BdSL) interpretation, prior approaches often imposed a burden on users, requiring them to spell words without hidden characters, which were subsequently corrected using Bangla grammar rules due to the missing classes in BdSL36 dataset. However, this method posed a challenge in accurately guessing the incorrect spelling of words. To address this limitation, we propose a novel real-time finger spelling system based on the YOLOv5 architecture. Our system employs specified rules and numerical classes as triggers to efficiently generate hidden and compound characters, eliminating the necessity for additional classes and significantly enhancing user convenience. Notably, our approach achieves character spelling in an impressive 1.32 seconds with a remarkable accuracy rate of 98\%. Furthermore, our YOLOv5 model, trained on 9147 images, demonstrates an exceptional mean Average Precision (mAP) of 96.4\%. These advancements represent a substantial progression in augmenting BdSL interpretation, promising increased inclusivity and accessibility for the linguistic minority. This innovative framework, characterized by compatibility with existing YOLO versions, stands as a transformative milestone in enhancing communication modalities and linguistic equity within the Bangla Sign Language community.
</details>
<details>
<summary>摘要</summary>
在孟加拉手语（BdSL）解释领域，先前的方法经常对用户带来压力，需要他们在无隐藏字符的情况下寻找字符，然后根据孟加拉语法规则进行修正，由于在BdSL36数据集中缺失的类型。但这种方法难以准确地猜测错误的拼写。为解决这个限制，我们提出了一种新的实时手写系统，基于YOLOv5架构。我们的系统采用了特定的规则和数字类作为触发器，以高效地生成隐藏和复合字符，从而消除了额外的类和增加了用户的便利。特别是，我们的方法在1.32秒内完成字符拼写，并达到了98%的精度。此外，我们的YOLOv5模型，在9147张图像上训练，显示了极高的平均精度（mAP）96.4%。这些进步表明了在增强孟加拉手语解释方面的重要突破，这将为孟加拉手语社区提供更多的包容性和可用性。这种革命性的框架，具有与现有YOLO版本兼容的特点，代表了孟加拉手语解释领域的巨大进步，并将在语言平等和通信模式方面产生深远的影响。
</details></li>
</ul>
<hr>
<h2 id="Joint-inversion-of-Time-Lapse-Surface-Gravity-and-Seismic-Data-for-Monitoring-of-3D-CO-2-Plumes-via-Deep-Learning"><a href="#Joint-inversion-of-Time-Lapse-Surface-Gravity-and-Seismic-Data-for-Monitoring-of-3D-CO-2-Plumes-via-Deep-Learning" class="headerlink" title="Joint inversion of Time-Lapse Surface Gravity and Seismic Data for Monitoring of 3D CO$_2$ Plumes via Deep Learning"></a>Joint inversion of Time-Lapse Surface Gravity and Seismic Data for Monitoring of 3D CO$_2$ Plumes via Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04430">http://arxiv.org/abs/2310.04430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Celaya, Mauricio Araya-Polo</li>
<li>for: 预测地下CO2涡，作为监测CO2储存部署的辅助工具。</li>
<li>methods: 基于深度学习的3D结合时间差表地重力和地震数据重建地下密度和速度模型。</li>
<li>results: 与深度学习基于重力只和地震只的拟合模型相比，joint匹配模型得到了改善的密度和速度重建、准确的分割和高的R-squared系数。这些结果表明深度学习基于联合拟合是有效的CO2储存监测工具。<details>
<summary>Abstract</summary>
We introduce a fully 3D, deep learning-based approach for the joint inversion of time-lapse surface gravity and seismic data for reconstructing subsurface density and velocity models. The target application of this proposed inversion approach is the prediction of subsurface CO2 plumes as a complementary tool for monitoring CO2 sequestration deployments. Our joint inversion technique outperforms deep learning-based gravity-only and seismic-only inversion models, achieving improved density and velocity reconstruction, accurate segmentation, and higher R-squared coefficients. These results indicate that deep learning-based joint inversion is an effective tool for CO$_2$ storage monitoring. Future work will focus on validating our approach with larger datasets, simulations with other geological storage sites, and ultimately field data.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种完全三维、深度学习基于的方法，用于同时逆合时间序列表面重力和地震数据，以重建地下密度和速度模型。我们的目标应用是预测地下CO2泵，作为监测CO2储存部署的辅助工具。我们的联合逆合模型在密度和速度重建、准确分割和高R-平方 coefficient方面表现出色，这表明深度学习基于的联合逆合是有效的CO$_2$储存监测工具。未来工作将集中于验证我们的方法，使用更大的数据集、其他地质储存站的 simulate 和最终场景数据。
</details></li>
</ul>
<hr>
<h2 id="OneSeg-Self-learning-and-One-shot-Learning-based-Single-slice-Annotation-for-3D-Medical-Image-Segmentation"><a href="#OneSeg-Self-learning-and-One-shot-Learning-based-Single-slice-Annotation-for-3D-Medical-Image-Segmentation" class="headerlink" title="OneSeg: Self-learning and One-shot Learning based Single-slice Annotation for 3D Medical Image Segmentation"></a>OneSeg: Self-learning and One-shot Learning based Single-slice Annotation for 3D Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13671">http://arxiv.org/abs/2309.13671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixuan Wu, Bo Zheng, Jintai Chen, Danny Z. Chen, Jian Wu</li>
<li>for: 提高医疗图像分割精度，减少数据标注努力。</li>
<li>methods: 提议一种自学习和一键学习基于构建，只需要标注一个3D图像的一 slice，以提高3D医疗图像分割精度。</li>
<li>results: 比对完全监督方法，新方法可以达到相似的性能，仅需要0.1%的数据标注，并且在多个异常测试集上进行了广泛的实验验证。<details>
<summary>Abstract</summary>
As deep learning methods continue to improve medical image segmentation performance, data annotation is still a big bottleneck due to the labor-intensive and time-consuming burden on medical experts, especially for 3D images. To significantly reduce annotation efforts while attaining competitive segmentation accuracy, we propose a self-learning and one-shot learning based framework for 3D medical image segmentation by annotating only one slice of each 3D image. Our approach takes two steps: (1) self-learning of a reconstruction network to learn semantic correspondence among 2D slices within 3D images, and (2) representative selection of single slices for one-shot manual annotation and propagating the annotated data with the well-trained reconstruction network. Extensive experiments verify that our new framework achieves comparable performance with less than 1% annotated data compared with fully supervised methods and generalizes well on several out-of-distribution testing sets.
</details>
<details>
<summary>摘要</summary>
随着深度学习方法在医疗影像分割性能的提高，数据注释仍然是一个大的瓶颈，因为医疗专家需要投入大量的劳动和时间来进行注释，特别是 для 3D 影像。为了减少注释努力而获得竞争性的分割精度，我们提出了一个自学习和一次学习基于框架，只需要注释每个 3D 影像中的一个平面。我们的方法包括两步：（1）自学习一个重建网络，以学习 2D 影像内 3D 影像中的semantic相关性，以及（2）选择单个平面进行一次手动注释，并使用已经训练好的重建网络将注释数据传播到其他影像中。我们的新方法在多个out-of-distribution测试集上进行了广泛的实验，并证明了它可以与完全监督方法相比，并且在不同的测试集上具有良好的一致性。
</details></li>
</ul>
<hr>
<h2 id="Adaptation-of-the-super-resolution-SOTA-for-Art-Restoration-in-camera-capture-images"><a href="#Adaptation-of-the-super-resolution-SOTA-for-Art-Restoration-in-camera-capture-images" class="headerlink" title="Adaptation of the super resolution SOTA for Art Restoration in camera capture images"></a>Adaptation of the super resolution SOTA for Art Restoration in camera capture images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13655">http://arxiv.org/abs/2309.13655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naagar/art_restoration_dm">https://github.com/naagar/art_restoration_dm</a></li>
<li>paper_authors: Sandeep Nagar, Abhinaba Bala, Sai Amrit Patnaik<br>for: 这项研究旨在开发一个基于计算机视觉模型的自动化艺术修复方法，以提高和重建受损艺术作品的视觉质量，保留原始特点和瑰宝。methods: 该研究采用了基于扩散模型（DM）的图像超分辨率技术，并对其进行了微调，以适应不同类型的受损，包括噪声、模糊、scratches、淡化等。results: 研究结果显示，通过微调一个超分辨率模型，可以处理多种受损类型，并且可以提高和重建受损艺术作品的视觉质量，而不需要专业知识和较长的时间。代码链接：<a target="_blank" rel="noopener" href="https://github.com/Naagar/art_restoration_DM%E3%80%82">https://github.com/Naagar/art_restoration_DM。</a><details>
<summary>Abstract</summary>
Preserving cultural heritage is of paramount importance. In the domain of art restoration, developing a computer vision model capable of effectively restoring deteriorated images of art pieces was difficult, but now we have a good computer vision state-of-art. Traditional restoration methods are often time-consuming and require extensive expertise. The aim of this work is to design an automated solution based on computer vision models that can enhance and reconstruct degraded artworks, improving their visual quality while preserving their original characteristics and artifacts. The model should handle a diverse range of deterioration types, including but not limited to noise, blur, scratches, fading, and other common forms of degradation. We adapt the current state-of-art for the image super-resolution based on the Diffusion Model (DM) and fine-tune it for Image art restoration. Our results show that instead of fine-tunning multiple different models for different kinds of degradation, fine-tuning one super-resolution. We train it on multiple datasets to make it robust. code link: https://github.com/Naagar/art_restoration_DM
</details>
<details>
<summary>摘要</summary>
保护文化遗产对于我们非常重要。在艺术修复领域，开发一个可以有效地恢复褪色的艺术作品图像的计算机视觉模型是一项具有挑战性的任务，但现在我们已经有了一个非常出色的计算机视觉状态。传统的修复方法通常是时间consuming且需要广泛的专业知识。我们的目标是设计一个自动化的解决方案，基于计算机视觉模型，可以提高褪色的艺术作品图像的视觉质量，同时保持原始特征和痕迹。我们采用当前状态的扩充模型（DM），并进行了精细调整，以适应不同类型的褪色，包括噪声、模糊、擦抹、淡化和其他常见的褪色形式。我们的结果表明，不同于先前的多个模型的微调，我们可以通过微调一个超解析模型来实现图像修复。我们在多个数据集上训练这个模型，以使其具有坚固性。更多信息请参考：https://github.com/Naagar/art_restoration_DM。
</details></li>
</ul>
<hr>
<h2 id="ILNet-Low-level-Matters-for-Salient-Infrared-Small-Target-Detection"><a href="#ILNet-Low-level-Matters-for-Salient-Infrared-Small-Target-Detection" class="headerlink" title="ILNet: Low-level Matters for Salient Infrared Small Target Detection"></a>ILNet: Low-level Matters for Salient Infrared Small Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13646">http://arxiv.org/abs/2309.13646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/li-haoqing/ilnet">https://github.com/li-haoqing/ilnet</a></li>
<li>paper_authors: Haoqing Li, Jinfu Yang, Runshi Wang, Yifei Xu</li>
<li>for: 该文章目标是提出一种基于干扰低级网络（ILNet）的干扰小目标检测方法，以提高干扰小目标特征的表示能力。</li>
<li>methods: 该方法使用了一种新的轻量级特征融合模块（IPOF），将低级信息更加注重地融合到深层网络中，以提高干扰小目标的检测性能。此外，还使用了一种动态一维度聚合层（DODA）来动态调整低维度信息的聚合方式。此外，该方法还使用了 Representative Block（RB）来动态分配深层和浅层网络的权重。</li>
<li>results: 实验结果表明，提出的 ILNet 方法在NUAA-SIRST 数据集上取得了78.22% nIoU 和 1.33e-6 Fa 的最佳性能，并在 IRSTD-1K 数据集上取得了68.91% nIoU 和 3.23e-6 Fa 的最佳性能。此外，ILNet 还能够在数据量增加时获得更大的提升。<details>
<summary>Abstract</summary>
Infrared small target detection is a technique for finding small targets from infrared clutter background. Due to the dearth of high-level semantic information, small infrared target features are weakened in the deep layers of the CNN, which underachieves the CNN's representation ability. To address the above problem, in this paper, we propose an infrared low-level network (ILNet) that considers infrared small targets as salient areas with little semantic information. Unlike other SOTA methods, ILNet pays greater attention to low-level information instead of treating them equally. A new lightweight feature fusion module, named Interactive Polarized Orthogonal Fusion module (IPOF), is proposed, which integrates more important low-level features from the shallow layers into the deep layers. A Dynamic One-Dimensional Aggregation layers (DODA) are inserted into the IPOF, to dynamically adjust the aggregation of low dimensional information according to the number of input channels. In addition, the idea of ensemble learning is used to design a Representative Block (RB) to dynamically allocate weights for shallow and deep layers. Experimental results on the challenging NUAA-SIRST (78.22% nIoU and 1.33e-6 Fa) and IRSTD-1K (68.91% nIoU and 3.23e-6 Fa) dataset demonstrate that the proposed ILNet can get better performances than other SOTA methods. Moreover, ILNet can obtain a greater improvement with the increasement of data volume. Training code are available at https://github.com/Li-Haoqing/ILNet.
</details>
<details>
<summary>摘要</summary>
infrared小目标检测是一种技术，用于从抖抖辐射背景中检测小目标。由于高级 semantic信息的缺乏，小抖抖辐射目标特征在深层神经网络中弱化，这会导致神经网络的表征能力受到限制。为解决上述问题，本文提出了一种infrared低级网络（ILNet），它视小抖抖辐射目标为有少量semantic信息的突出区域。不同于其他SOTA方法，ILNet更加注重低级信息，而不是对其进行平等处理。为了更好地捕捉低级信息，我们提出了一种新的轻量级特征融合模块（IPOF），该模块将深层神经网络中的重要低级特征与浅层神经网络中的低级特征进行有效的融合。此外，我们还使用了 Representative Block（RB）来动态分配深浅层神经网络中的权重。实验结果表明，提出的ILNet可以在NUAA-SIRST（78.22% nIoU和1.33e-6 Fa）和IRSTD-1K（68.91% nIoU和3.23e-6 Fa） dataset上达到SOTA的性能。此外，ILNet可以随着数据量的增加而获得更大的改进。训练代码可以在https://github.com/Li-Haoqing/ILNet中找到。
</details></li>
</ul>
<hr>
<h2 id="Changes-Aware-Transformer-Learning-Generalized-Changes-Representation"><a href="#Changes-Aware-Transformer-Learning-Generalized-Changes-Representation" class="headerlink" title="Changes-Aware Transformer: Learning Generalized Changes Representation"></a>Changes-Aware Transformer: Learning Generalized Changes Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13619">http://arxiv.org/abs/2309.13619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Wang, Licheng Jiao, Jie Chen, Shuyuan Yang, Fang Liu</li>
<li>for: 本研究旨在提高 Change Detection (CD) 任务中的变化检测精度，通过学习多种变化的总体表示，并提出一种Changes-Aware Transformer (CAT) 来修正差异特征。</li>
<li>methods: 本研究使用了一种novel的 Changes-Aware Transformer (CAT) 来修正差异特征，CAT 通过栅格cosine cross-attention层和自我注意层来实现这一目的。</li>
<li>results: 实验结果表明，我们的方法可以在 remote sensing CD 数据集和街景 CD 数据集上达到状态之 arts 性能，并且具有良好的普适性。<details>
<summary>Abstract</summary>
Difference features obtained by comparing the images of two periods play an indispensable role in the change detection (CD) task. However, a pair of bi-temporal images can exhibit diverse changes, which may cause various difference features. Identifying changed pixels with differ difference features to be the same category is thus a challenge for CD. Most nowadays' methods acquire distinctive difference features in implicit ways like enhancing image representation or supervision information. Nevertheless, informative image features only guarantee object semantics are modeled and can not guarantee that changed pixels have similar semantics in the difference feature space and are distinct from those unchanged ones. In this work, the generalized representation of various changes is learned straightforwardly in the difference feature space, and a novel Changes-Aware Transformer (CAT) for refining difference features is proposed. This generalized representation can perceive which pixels are changed and which are unchanged and further guide the update of pixels' difference features. CAT effectively accomplishes this refinement process through the stacked cosine cross-attention layer and self-attention layer. After refinement, the changed pixels in the difference feature space are closer to each other, which facilitates change detection. In addition, CAT is compatible with various backbone networks and existing CD methods. Experiments on remote sensing CD data set and street scene CD data set show that our method achieves state-of-the-art performance and has excellent generalization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT diferenciales características obtenidas por comparar las imágenes de dos períodos juegan un papel fundamental en la tarea de detección de cambios (CD). Sin embargo, una pareja de imágenes bi-temporales puede exhibir cambios diversificados, lo que puede causar diferentes diferenciales características. Identificar pixels cambiados con diferenciales características similares es un desafío para la CD. La mayoría de los métodos actuales adquieren características diferenciales distintivas de manera implícita, como la mejora de la representación de la imagen o la información de supervisión. Sin embargo, las características de la imagen informativas solo garantizan que las semánticas de los objetos se modelen y no garantizan que los pixels cambiados tengan semánticas similares en el espacio de características de diferencia y se distingan de los pixels no cambiados. En este trabajo, se aprende una representación generalizada de los cambios en el espacio de características de diferencia y se propone un Novel Changes-Aware Transformer (CAT) para refinar las características de diferencia. Esta representación generalizada puede percibir qué pixels están cambiados y qué pixels no están cambiados y guiar el update de las características de diferencia de los pixels. CAT efectúa este proceso de refinamiento mediante capas de atención cruzada cosínica y de atención a sí misma. Después de la refinement, los pixels cambiados en el espacio de características de diferencia están más cercanos entre sí, lo que facilita la detección de cambios. Además, CAT es compatible con redes de soporte existentes y métodos de CD. Los experimentos en los conjuntos de datos de CD de áreas remotas y escenas de la calle muestran que nuestro método logra un rendimiento estatal de arte y tiene una excelente generalización.Note: The text is translated using the Google Translate API, and the translation may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="VisionKG-Unleashing-the-Power-of-Visual-Datasets-via-Knowledge-Graph"><a href="#VisionKG-Unleashing-the-Power-of-Visual-Datasets-via-Knowledge-Graph" class="headerlink" title="VisionKG: Unleashing the Power of Visual Datasets via Knowledge Graph"></a>VisionKG: Unleashing the Power of Visual Datasets via Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13610">http://arxiv.org/abs/2309.13610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jicheng Yuan, Anh Le-Tuan, Manh Nguyen-Duc, Trung-Kien Tran, Manfred Hauswirth, Danh Le-Phuoc</li>
<li>for: 提供一个全面的 computer vision 数据资源，实现跨多个源、任务和分类的Visual dataset集成。</li>
<li>methods: 使用知识 graphs和Semantic Web技术来整合、组织和管理多种形式的Visual dataset，提供简单的存取和查询服务，并具有扩展性和可扩展性。</li>
<li>results: 组建了一个名为 Vision Knowledge Graph（VisionKG）的资源，它可以实现跨多个源、任务和分类的Visual dataset集成，并提供了多种数据 Retrieval 和探索服务。<details>
<summary>Abstract</summary>
The availability of vast amounts of visual data with heterogeneous features is a key factor for developing, testing, and benchmarking of new computer vision (CV) algorithms and architectures. Most visual datasets are created and curated for specific tasks or with limited image data distribution for very specific situations, and there is no unified approach to manage and access them across diverse sources, tasks, and taxonomies. This not only creates unnecessary overheads when building robust visual recognition systems, but also introduces biases into learning systems and limits the capabilities of data-centric AI. To address these problems, we propose the Vision Knowledge Graph (VisionKG), a novel resource that interlinks, organizes and manages visual datasets via knowledge graphs and Semantic Web technologies. It can serve as a unified framework facilitating simple access and querying of state-of-the-art visual datasets, regardless of their heterogeneous formats and taxonomies. One of the key differences between our approach and existing methods is that ours is knowledge-based rather than metadatabased. It enhances the enrichment of the semantics at both image and instance levels and offers various data retrieval and exploratory services via SPARQL. VisionKG currently contains 519 million RDF triples that describe approximately 40 million entities, and are accessible at https://vision.semkg.org and through APIs. With the integration of 30 datasets and four popular CV tasks, we demonstrate its usefulness across various scenarios when working with CV pipelines.
</details>
<details>
<summary>摘要</summary>
“现代计算机视觉（CV）算法和架构的开发、测试和评估中，庞大量的视觉数据的可用性是关键因素。大多数视觉数据集是为特定任务或有限的图像数据分布而创建和维护的，而且没有一种统一的方法来管理和访问它们。这不仅会增加建立可靠的视觉识别系统的开发成本，而且会引入偏见到学习系统中和限制数据驱动AI的能力。为解决这些问题，我们提议了视觉知识图（VisionKG），一种新的资源，通过知识图和Semantic Web技术来集成、组织和管理视觉数据集。它可以作为一个统一的框架，方便访问和查询多种不同的视觉任务和数据集，无论它们的格式和分类如何。我们的方法与现有方法的主要区别在于，我们的方法是基于知识图而不是基于元数据的。它可以增强图像和实例层次的 semantics，并提供了多种数据检索和探索服务via SPARQL。VisionKG目前包含519亿个RDF三元组，描述约40亿个实体，可以在https://vision.semkg.org和通过API访问。我们通过将30个数据集和4种常见CV任务集成到VisionKG中，证明了它在不同的场景中对CV管道的有用性。”
</details></li>
</ul>
<hr>
<h2 id="Vulnerabilities-in-Video-Quality-Assessment-Models-The-Challenge-of-Adversarial-Attacks"><a href="#Vulnerabilities-in-Video-Quality-Assessment-Models-The-Challenge-of-Adversarial-Attacks" class="headerlink" title="Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks"></a>Vulnerabilities in Video Quality Assessment Models: The Challenge of Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13609">http://arxiv.org/abs/2309.13609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gzhu-dvl/attackvqa">https://github.com/gzhu-dvl/attackvqa</a></li>
<li>paper_authors: Ao-Xiang Zhang, Yu Ran, Weixuan Tang, Yuan-Gen Wang</li>
<li>for: This paper focuses on evaluating the robustness of No-Reference Video Quality Assessment (NR-VQA) models against adversarial attacks, and proposing a patch-based random search method for black-box attacks.</li>
<li>methods: The paper uses Convolutional Neural Networks (CNNs) and Transformers as the base models for NR-VQA, and proposes a novel loss function called Score-Reversed Boundary Loss to evaluate the robustness of these models against adversarial attacks.</li>
<li>results: The paper presents the results of evaluating the robustness of NR-VQA models against adversarial attacks using the proposed Score-Reversed Boundary Loss, and shows that the proposed method can effectively launch both white-box and black-box attacks in an imperceptible manner.Here is the simplified Chinese text for the three information points:</li>
<li>for: 这篇论文关注NR-VQA模型对针对攻击的Robustness评估，并提出了一种基于随机搜索的黑盒攻击方法。</li>
<li>methods: 该论文使用Convolutional Neural Networks (CNNs)和Transformers作为NR-VQA模型的基础模型，并提出了一种新的损失函数called Score-Reversed Boundary Loss来评估NR-VQA模型对攻击的Robustness。</li>
<li>results: 该论文通过使用提出的Score-Reversed Boundary Loss来评估NR-VQA模型对攻击的Robustness，并显示了该方法可以效果地发起白盒和黑盒攻击，并且在无人知情的情况下进行。<details>
<summary>Abstract</summary>
No-Reference Video Quality Assessment (NR-VQA) plays an essential role in improving the viewing experience of end-users. Driven by deep learning, recent NR-VQA models based on Convolutional Neural Networks (CNNs) and Transformers have achieved outstanding performance. To build a reliable and practical assessment system, it is of great necessity to evaluate their robustness. However, such issue has received little attention in the academic community. In this paper, we make the first attempt to evaluate the robustness of NR-VQA models against adversarial attacks, and propose a patch-based random search method for black-box attack. Specifically, considering both the attack effect on quality score and the visual quality of adversarial video, the attack problem is formulated as misleading the estimated quality score under the constraint of just-noticeable difference (JND). Built upon such formulation, a novel loss function called Score-Reversed Boundary Loss is designed to push the adversarial video's estimated quality score far away from its ground-truth score towards a specific boundary, and the JND constraint is modeled as a strict $L_2$ and $L_\infty$ norm restriction. By this means, both white-box and black-box attacks can be launched in an effective and imperceptible manner. The source code is available at https://github.com/GZHU-DVL/AttackVQA.
</details>
<details>
<summary>摘要</summary>
“无参考视频质量评估（NR-VQA）在提高用户视频观看体验中扮演着关键性的角色。驱动深度学习，最新的NR-VQA模型基于卷积神经网络（CNNs）和变换器（Transformers）已经实现了出色的表现。为建立可靠和实用的评估系统，必须评估其可靠性。然而，这一问题在学术界得到了少量的关注。本文是首次评估NR-VQA模型对抗攻击的尝试，并提出了一种黑盒攻击方法基于补丁随机搜索。具体来说，我们认为攻击问题应该是让估计的质量分数受到攻击，同时保证视频质量的变化在可以快速感知的范围内。针对这一问题，我们提出了一种新的损失函数 called Score-Reversed Boundary Loss，它可以让攻击者通过控制估计质量分数的变化，使得攻击者可以在无法察觉的情况下发动白盒和黑盒攻击。源代码可以在https://github.com/GZHU-DVL/AttackVQA上获取。”
</details></li>
</ul>
<hr>
<h2 id="FaceAtt-Enhancing-Image-Captioning-with-Facial-Attributes-for-Portrait-Images"><a href="#FaceAtt-Enhancing-Image-Captioning-with-Facial-Attributes-for-Portrait-Images" class="headerlink" title="FaceAtt: Enhancing Image Captioning with Facial Attributes for Portrait Images"></a>FaceAtt: Enhancing Image Captioning with Facial Attributes for Portrait Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13601">http://arxiv.org/abs/2309.13601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naimul Haque, Iffat Labiba, Sadia Akter</li>
<li>for: This paper focuses on developing a novel approach to attribute-focused image captioning that accurately depicts facial attributes within images.</li>
<li>methods: The FaceAtt model uses deep learning techniques and annotated attributes of portraits as supplementary prior knowledge to improve caption quality.</li>
<li>results: The FaceAtt model yields a subtle yet discernible enhancement in resulting caption scores, demonstrating the effectiveness of incorporating additional attribute vectors during training.Here’s the simplified Chinese text for the three key points:</li>
<li>for: 这篇论文关注开发一种基于人脸特征的图像描述模型，以准确描述图像中的人脸特征。</li>
<li>methods:  FaceAtt模型使用深度学习技术和人脸特征注释作为辅助知识，以提高描述质量。</li>
<li>results: FaceAtt模型在训练时使用人脸特征注释可以提供微妙 yet 可识别的提升，表明注释的添加可以提高模型的表现。<details>
<summary>Abstract</summary>
Automated image caption generation is a critical area of research that enhances accessibility and understanding of visual content for diverse audiences. In this study, we propose the FaceAtt model, a novel approach to attribute-focused image captioning that emphasizes the accurate depiction of facial attributes within images. FaceAtt automatically detects and describes a wide range of attributes, including emotions, expressions, pointed noses, fair skin tones, hair textures, attractiveness, and approximate age ranges. Leveraging deep learning techniques, we explore the impact of different image feature extraction methods on caption quality and evaluate our model's performance using metrics such as BLEU and METEOR. Our FaceAtt model leverages annotated attributes of portraits as supplementary prior knowledge for our portrait images before captioning. This innovative addition yields a subtle yet discernible enhancement in the resulting scores, exemplifying the potency of incorporating additional attribute vectors during training. Furthermore, our research contributes to the broader discourse on ethical considerations in automated captioning. This study sets the stage for future research in refining attribute-focused captioning techniques, with a focus on enhancing linguistic coherence, addressing biases, and accommodating diverse user needs.
</details>
<details>
<summary>摘要</summary>
自动生成图像标签是一个关键的研究领域，它提高了视觉内容的可访问性和理解，便于不同的用户群体。在这项研究中，我们提出了FaceAtt模型，一种新的图像标签生成方法，强调在图像中准确描述人脸特征。FaceAtt自动检测和描述了各种特征，包括情感、表情、短脚、白肤肤、头发Texture、吸引力和年龄范围。我们利用深度学习技术，研究不同的图像特征提取方法对标签质量的影响，并使用BLEU和METEOR等 метри来评估我们的FaceAtt模型。我们的FaceAtt模型利用了人脸图像的注解特征作为额外知识来进行预处理，这种创新的添加带来了微妙 yet 可见的提高，表明了在训练时添加特征向量的力量。此外，我们的研究对自动标签技术的伦理考虑进行贡献。这项研究为未来更进一步的增强特征强调标签技术做出了平台，包括提高语言一致性、消除偏见和满足多样化用户需求。
</details></li>
</ul>
<hr>
<h2 id="Multi-Dimensional-Hyena-for-Spatial-Inductive-Bias"><a href="#Multi-Dimensional-Hyena-for-Spatial-Inductive-Bias" class="headerlink" title="Multi-Dimensional Hyena for Spatial Inductive Bias"></a>Multi-Dimensional Hyena for Spatial Inductive Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13600">http://arxiv.org/abs/2309.13600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Itamar Zimerman, Lior Wolf</li>
<li>for: 这个论文是为了提出一种数据效率的视觉变换器，不需要自注意。它使用了一种新的多轴泛化方法，基于最近的Hyena层。</li>
<li>methods: 这个论文使用了一种新的泛化方法，即Hyena N-D层，以提高视觉变换器的性能。它还提出了多种不同的方法来实现这种泛化，并从实际和理论上进行了详细的分析。</li>
<li>results: 实验结果显示，Hyena N-D层能够提高多种视觉变换器架构的性能，如ViT、Swin和DeiT等。此外，在小数据集 régime中，Hyena-based ViT比特有些文献中提出的特定设计来解决这个问题的ViT变种更好。最后， authors表明了一种hybrid方法，将Hyena N-D层用于前几层，然后使用传统注意力层，能够持续提高不同的视觉变换器架构的性能。<details>
<summary>Abstract</summary>
In recent years, Vision Transformers have attracted increasing interest from computer vision researchers. However, the advantage of these transformers over CNNs is only fully manifested when trained over a large dataset, mainly due to the reduced inductive bias towards spatial locality within the transformer's self-attention mechanism. In this work, we present a data-efficient vision transformer that does not rely on self-attention. Instead, it employs a novel generalization to multiple axes of the very recent Hyena layer. We propose several alternative approaches for obtaining this generalization and delve into their unique distinctions and considerations from both empirical and theoretical perspectives.   Our empirical findings indicate that the proposed Hyena N-D layer boosts the performance of various Vision Transformer architectures, such as ViT, Swin, and DeiT across multiple datasets. Furthermore, in the small dataset regime, our Hyena-based ViT is favorable to ViT variants from the recent literature that are specifically designed for solving the same challenge, i.e., working with small datasets or incorporating image-specific inductive bias into the self-attention mechanism. Finally, we show that a hybrid approach that is based on Hyena N-D for the first layers in ViT, followed by layers that incorporate conventional attention, consistently boosts the performance of various vision transformer architectures.
</details>
<details>
<summary>摘要</summary>
Recently, Vision Transformers have gained increasing attention from computer vision researchers. However, the advantage of these transformers over Convolutional Neural Networks (CNNs) is only fully manifested when trained on a large dataset, due to the reduced inductive bias towards spatial locality within the transformer's self-attention mechanism. In this work, we propose a data-efficient vision transformer that does not rely on self-attention. Instead, it employs a novel generalization to multiple axes of the very recent Hyena layer. We present several alternative approaches for obtaining this generalization and discuss their unique distinctions and considerations from both empirical and theoretical perspectives.Our empirical findings indicate that the proposed Hyena N-D layer enhances the performance of various Vision Transformer architectures, such as ViT, Swin, and DeiT, across multiple datasets. Furthermore, in the small dataset regime, our Hyena-based ViT outperforms ViT variants from the recent literature that are specifically designed for solving the same challenge, i.e., working with small datasets or incorporating image-specific inductive bias into the self-attention mechanism. Finally, we show that a hybrid approach that is based on Hyena N-D for the first layers in ViT, followed by layers that incorporate conventional attention, consistently boosts the performance of various vision transformer architectures.
</details></li>
</ul>
<hr>
<h2 id="On-the-Posterior-Distribution-in-Denoising-Application-to-Uncertainty-Quantification"><a href="#On-the-Posterior-Distribution-in-Denoising-Application-to-Uncertainty-Quantification" class="headerlink" title="On the Posterior Distribution in Denoising: Application to Uncertainty Quantification"></a>On the Posterior Distribution in Denoising: Application to Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13598">http://arxiv.org/abs/2309.13598</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HilaManor/GaussianDenoisingPosterior">https://github.com/HilaManor/GaussianDenoisingPosterior</a></li>
<li>paper_authors: Hila Manor, Tomer Michaeli</li>
<li>for: 这篇论文主要针对的是降噪方法的应用，包括低级图像感知器的降噪、以及基于 Tweedie 公式的score-based生成模型。</li>
<li>methods: 该论文使用 Gaussian denoising 的 posterior distribution 链接到数据分布的 posterior mean，并 derivates 出高阶中心差的关系。</li>
<li>results: 该论文可以快速和减少内存占用来计算 posterior distribution 的主要方向和高阶中心差，不需要训练或精度调整降噪器。<details>
<summary>Abstract</summary>
Denoisers play a central role in many applications, from noise suppression in low-grade imaging sensors, to empowering score-based generative models. The latter category of methods makes use of Tweedie's formula, which links the posterior mean in Gaussian denoising (i.e., the minimum MSE denoiser) with the score of the data distribution. Here, we derive a fundamental relation between the higher-order central moments of the posterior distribution, and the higher-order derivatives of the posterior mean. We harness this result for uncertainty quantification of pre-trained denoisers. Particularly, we show how to efficiently compute the principal components of the posterior distribution for any desired region of an image, as well as to approximate the full marginal distribution along those (or any other) one-dimensional directions. Our method is fast and memory efficient, as it does not explicitly compute or store the high-order moment tensors and it requires no training or fine tuning of the denoiser. Code and examples are available on the project's webpage in https://hilamanor.github.io/GaussianDenoisingPosterior/
</details>
<details>
<summary>摘要</summary>
纹理恢复器在许多应用中扮演着中心角色，从噪声消除低级图像感知器到激发Score-based生成模型。后者使用Tweedie的公式，将 posterior mean在 Gaussian denoising 中相应的负面积最小化。我们 derivate 出 posterior distribution 的高级中心均值和 posterior mean 的高级导数之间的基本关系。我们利用这个结果进行uncertainty quantification of pre-trained denoisers。特别是，我们可以快速计算 posterior distribution 的主要Components在任意区域中，以及任意一个方向的全级分布。我们的方法快速，内存占用少，因为它不需要直接计算或存储高级 moment tensor，也不需要训练或微调denoiser。 codes 和示例可以在https://hilamanor.github.io/GaussianDenoisingPosterior/ 的项目网站上找到。
</details></li>
</ul>
<hr>
<h2 id="Advancements-in-3D-Lane-Detection-Using-LiDAR-Point-Clouds-From-Data-Collection-to-Model-Development"><a href="#Advancements-in-3D-Lane-Detection-Using-LiDAR-Point-Clouds-From-Data-Collection-to-Model-Development" class="headerlink" title="Advancements in 3D Lane Detection Using LiDAR Point Clouds: From Data Collection to Model Development"></a>Advancements in 3D Lane Detection Using LiDAR Point Clouds: From Data Collection to Model Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13596">http://arxiv.org/abs/2309.13596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runkai Zhao, Yuwen Heng, Yuanda Gao, Shilei Liu, Heng Wang, Changhao Yao, Jiawen Chen, Weidong Cai</li>
<li>for: 本研究旨在提高自动驾驶系统（ADAS）的车辆感知和决策能力，通过利用学习基于的技术。</li>
<li>methods: 本研究使用了LiDAR数据集，并设计了一个简单 yet effective的自动标注管线，以生成更加精细的车道标注。</li>
<li>results: 实验结果显示，LiLaDet模型在K-Lane数据集和LiSV-3DLane数据集上的3D车道检测任务中表现出色，超过了现有的摄像头和LiDAR基于的方法。<details>
<summary>Abstract</summary>
Advanced Driver-Assistance Systems (ADAS) have successfully integrated learning-based techniques into vehicle perception and decision-making. However, their application in 3D lane detection for effective driving environment perception is hindered by the lack of comprehensive LiDAR datasets. The sparse nature of LiDAR point cloud data prevents an efficient manual annotation process. To solve this problem, we present LiSV-3DLane, a large-scale 3D lane dataset that comprises 20k frames of surround-view LiDAR point clouds with enriched semantic annotation. Unlike existing datasets confined to a frontal perspective, LiSV-3DLane provides a full 360-degree spatial panorama around the ego vehicle, capturing complex lane patterns in both urban and highway environments. We leverage the geometric traits of lane lines and the intrinsic spatial attributes of LiDAR data to design a simple yet effective automatic annotation pipeline for generating finer lane labels. To propel future research, we propose a novel LiDAR-based 3D lane detection model, LiLaDet, incorporating the spatial geometry learning of the LiDAR point cloud into Bird's Eye View (BEV) based lane identification. Experimental results indicate that LiLaDet outperforms existing camera- and LiDAR-based approaches in the 3D lane detection task on the K-Lane dataset and our LiSV-3DLane.
</details>
<details>
<summary>摘要</summary>
高级驾驶辅助系统（ADAS）已成功地将学习基于的技术 integrate 到车辆的感知和决策中。然而，它们在3D车道检测中为有效的驾驶环境感知受到了LiDAR数据的缺乏全面的障碍。LiDAR点云数据的稀疏性阻碍了人工注释的效率。为解决这个问题，我们提出了LiSV-3DLane，一个大规模的3D车道数据集，包含20000帧的周围视野LiDAR点云数据，并且具有增强的semantic注释。与现有的前视角所限定的数据集不同，LiSV-3DLane提供了360度的全景视图，捕捉了城市和高速公路环境中复杂的车道模式。我们利用LiDAR数据的几何特征和点云数据的内在空间属性，设计了一个简单 yet effective的自动注释管道，以生成更细的车道标签。为未来的研究提供动力，我们提出了一种基于LiDAR的3D车道检测模型LiLaDet，该模型将LiDAR点云中的空间几何学学习 integrate 到基于bird's eye view（BEV）的车道标识中。实验结果表明，LiLaDet在K-Lane数据集和我们的LiSV-3DLane上的3D车道检测任务中表现出色，比摄像头和LiDAR基的方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Encoder-Decoder-Architectures-for-Biplanar-X-ray-to-3D-Shape-Reconstruction"><a href="#Benchmarking-Encoder-Decoder-Architectures-for-Biplanar-X-ray-to-3D-Shape-Reconstruction" class="headerlink" title="Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction"></a>Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Shape Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13587">http://arxiv.org/abs/2309.13587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahesh Shakya, Bishesh Khanal</li>
<li>for: 这些论文的目的是为了evaluate多种深度学习模型在2D-3D骨形状重建方面的性能，以便在临床应用中进行评估和选择最佳模型。</li>
<li>methods: 这些论文使用的方法包括多种深度学习模型，以及Automatic clinical parameter and landmark extraction methods。</li>
<li>results: 这些论文的结果表明，关注全域空间关系的注意力机制方法在所有骨性质和数据集上表现较好，但是在临床相关的 subgroup中表现可能会被过度估计，肋骨比 femur、hip 和脊梁更加困难重建，并且 dice score 改进不总是导致自动计算临床相关参数的改进。<details>
<summary>Abstract</summary>
Various deep learning models have been proposed for 3D bone shape reconstruction from two orthogonal (biplanar) X-ray images. However, it is unclear how these models compare against each other since they are evaluated on different anatomy, cohort and (often privately held) datasets. Moreover, the impact of the commonly optimized image-based segmentation metrics such as dice score on the estimation of clinical parameters relevant in 2D-3D bone shape reconstruction is not well known. To move closer toward clinical translation, we propose a benchmarking framework that evaluates tasks relevant to real-world clinical scenarios, including reconstruction of fractured bones, bones with implants, robustness to population shift, and error in estimating clinical parameters. Our open-source platform provides reference implementations of 8 models (many of whose implementations were not publicly available), APIs to easily collect and preprocess 6 public datasets, and the implementation of automatic clinical parameter and landmark extraction methods. We present an extensive evaluation of 8 2D-3D models on equal footing using 6 public datasets comprising images for four different anatomies. Our results show that attention-based methods that capture global spatial relationships tend to perform better across all anatomies and datasets; performance on clinically relevant subgroups may be overestimated without disaggregated reporting; ribs are substantially more difficult to reconstruct compared to femur, hip and spine; and the dice score improvement does not always bring a corresponding improvement in the automatic estimation of clinically relevant parameters.
</details>
<details>
<summary>摘要</summary>
各种深度学习模型已经提议用于从两个mutually orthogonal（biplanar）X射线图像中重建3D骨形状。然而，它们之间的比较很难，因为它们在不同的解剖学、人群和（常常是私人拥有）数据集上进行评估。此外，通常优化的图像基于分割指标如 dice score 对2D-3D骨形状重建中的临床参数的影响不够了解。为了更近地到临床翻译，我们提出了一个 benchmarking 框架，评估了实际临床情景中的任务，包括骨折重建、骨嵌入、人口变化的Robustness和临床参数的错误。我们的开源平台提供了8个模型的参考实现（许多实现没有公开）、6个公共数据集的自动化采集和处理API，以及自动提取临床参数和标记的实现。我们对8个2D-3D模型进行了平等评估，使用6个公共数据集，包括4种不同的解剖学。我们的结果显示： attention-based 方法， capture 全局空间关系，在所有解剖学和数据集上表现较好; 不分解的报告可能会过分估计临床重要 subgroup; 肋骨重建相比股骨、股骨和脊梁更加困难; 并 dice score 改进不总是导致自动计算临床参数的改进。
</details></li>
</ul>
<hr>
<h2 id="Solving-Low-Dose-CT-Reconstruction-via-GAN-with-Local-Coherence"><a href="#Solving-Low-Dose-CT-Reconstruction-via-GAN-with-Local-Coherence" class="headerlink" title="Solving Low-Dose CT Reconstruction via GAN with Local Coherence"></a>Solving Low-Dose CT Reconstruction via GAN with Local Coherence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13584">http://arxiv.org/abs/2309.13584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lwjie595/GANLC">https://github.com/lwjie595/GANLC</a></li>
<li>paper_authors: Wenjie Liu</li>
<li>for: 用于诊断人体内部器官病变的计算Tomography（CT）成为医学影像领域的基本话题之一，低剂CT的使用被广泛采用，因此其重建方法得到了广泛的研究。</li>
<li>methods: 我们提出了一种基于生成对抗网络（GANs）的新方法，该方法可以利用运动场进行优化，从而提高重建图像的地方协调性和稳定性。</li>
<li>results: 我们对实验数据进行评估，结果表明，我们的提议方法可以与现有的状态对抗方法相比，显著提高重建图像的精度和稳定性。<details>
<summary>Abstract</summary>
The Computed Tomography (CT) for diagnosis of lesions in human internal organs is one of the most fundamental topics in medical imaging. Low-dose CT, which offers reduced radiation exposure, is preferred over standard-dose CT, and therefore its reconstruction approaches have been extensively studied. However, current low-dose CT reconstruction techniques mainly rely on model-based methods or deep-learning-based techniques, which often ignore the coherence and smoothness for sequential CT slices. To address this issue, we propose a novel approach using generative adversarial networks (GANs) with enhanced local coherence. The proposed method can capture the local coherence of adjacent images by optical flow, which yields significant improvements in the precision and stability of the constructed images. We evaluate our proposed method on real datasets and the experimental results suggest that it can outperform existing state-of-the-art reconstruction approaches significantly.
</details>
<details>
<summary>摘要</summary>
computed tomography (CT) 用于人体内部肿瘤诊断是医学影像领域的基本话题之一。低剂量 CT 比标准剂量 CT 更受欢迎，因此其重建方法得到了广泛的研究。然而，现有的低剂量 CT 重建技术主要基于模型基本方法或深度学习基本方法，这些方法经常忽略邻域 CT slice 的协调性和平滑性。为解决这个问题，我们提出了一种使用生成对抗网络 (GANs) 增强本地协调性的新方法。该方法可以通过光流来捕捉邻域图像的本地协调性，从而实现显著提高重建图像的精度和稳定性。我们在实际数据集上测试了我们的提议方法，实验结果表明，它可以与现有的状态空间重建方法相比，显著提高重建图像的质量。
</details></li>
</ul>
<hr>
<h2 id="A-SAM-based-Solution-for-Hierarchical-Panoptic-Segmentation-of-Crops-and-Weeds-Competition"><a href="#A-SAM-based-Solution-for-Hierarchical-Panoptic-Segmentation-of-Crops-and-Weeds-Competition" class="headerlink" title="A SAM-based Solution for Hierarchical Panoptic Segmentation of Crops and Weeds Competition"></a>A SAM-based Solution for Hierarchical Panoptic Segmentation of Crops and Weeds Competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13578">http://arxiv.org/abs/2309.13578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoa Dang Nguyen, Thanh-Hai Phung, Hoang-Giang Cao</li>
<li>for: 这个论文旨在探讨农业领域的高级计算机视觉技术——泛型分割，以提高农业作物和杂草的识别和分类。</li>
<li>methods: 该论文提出了一种combines Segment AnyThing Model (SAM)和对象检测模型的方法，以实现高级分割任务。 specifically, 该方法 integrate了两种对象检测模型的特点，namely DINO和YOLO-v8。</li>
<li>results: 该论文的best-performing模型在竞赛中的PQ+分数为81.33。<details>
<summary>Abstract</summary>
Panoptic segmentation in agriculture is an advanced computer vision technique that provides a comprehensive understanding of field composition. It facilitates various tasks such as crop and weed segmentation, plant panoptic segmentation, and leaf instance segmentation, all aimed at addressing challenges in agriculture. Exploring the application of panoptic segmentation in agriculture, the 8th Workshop on Computer Vision in Plant Phenotyping and Agriculture (CVPPA) hosted the challenge of hierarchical panoptic segmentation of crops and weeds using the PhenoBench dataset. To tackle the tasks presented in this competition, we propose an approach that combines the effectiveness of the Segment AnyThing Model (SAM) for instance segmentation with prompt input from object detection models. Specifically, we integrated two notable approaches in object detection, namely DINO and YOLO-v8. Our best-performing model achieved a PQ+ score of 81.33 based on the evaluation metrics of the competition.
</details>
<details>
<summary>摘要</summary>
“对农业中的涵盖分割技术（panoptic segmentation）进行了进一步的探索，以获得农田场景的全面理解。这技术可以帮助农业面临的问题，例如作物和杂草分类、植物涵盖分类以及叶子实例分类。为了探索这些应用，CVPPA年会（8th Workshop on Computer Vision in Plant Phenotyping and Agriculture）举办了一个挑战，即使用PhenoBench数据集进行阶层涵盖分类。我们提出了一个结合SAM模型（Segment AnyThing Model）的实例分类方法，并与物件探测模型（DINO和YOLO-v8）进行了统合。我们的最佳模型在竞赛中的PQ+分数为81.33。”Note: "PQ+ score" is a combination of precision, recall, and F1-score, which is a common evaluation metric for segmentation tasks.
</details></li>
</ul>
<hr>
<h2 id="Matrix-Completion-Informed-Deep-Unfolded-Equilibrium-Models-for-Self-Supervised-k-Space-Interpolation-in-MRI"><a href="#Matrix-Completion-Informed-Deep-Unfolded-Equilibrium-Models-for-Self-Supervised-k-Space-Interpolation-in-MRI" class="headerlink" title="Matrix Completion-Informed Deep Unfolded Equilibrium Models for Self-Supervised k-Space Interpolation in MRI"></a>Matrix Completion-Informed Deep Unfolded Equilibrium Models for Self-Supervised k-Space Interpolation in MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13571">http://arxiv.org/abs/2309.13571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Luo, Huayu Wang, Taofeng Xie, Qiyu Jin, Guoqing Chen, Zhuo-Xu Cui, Dong Liang</li>
<li>for: 提高MRI图像的速度和质量，不需要完整的标签数据</li>
<li>methods: 利用深度学习模型，同时保留常规模型的理论保证</li>
<li>results: 提出一种自适应深度学习方法，可以在不具备完整标签数据的情况下，实现MRI图像的加速和提高Here is the full text in Simplified Chinese:</li>
<li>for: 本研究旨在提高MRI图像的速度和质量，不需要完整的标签数据。</li>
<li>methods: 我们提出了一种利用深度学习模型的自适应方法，同时保留常规模型的理论保证。</li>
<li>results: 我们的方法可以在不具备完整标签数据的情况下，实现MRI图像的加速和提高，并且超过了现有的自适应方法和传统正则化方法的性能。<details>
<summary>Abstract</summary>
Recently, regularization model-driven deep learning (DL) has gained significant attention due to its ability to leverage the potent representational capabilities of DL while retaining the theoretical guarantees of regularization models. However, most of these methods are tailored for supervised learning scenarios that necessitate fully sampled labels, which can pose challenges in practical MRI applications. To tackle this challenge, we propose a self-supervised DL approach for accelerated MRI that is theoretically guaranteed and does not rely on fully sampled labels. Specifically, we achieve neural network structure regularization by exploiting the inherent structural low-rankness of the $k$-space data. Simultaneously, we constrain the network structure to resemble a nonexpansive mapping, ensuring the network's convergence to a fixed point. Thanks to this well-defined network structure, this fixed point can completely reconstruct the missing $k$-space data based on matrix completion theory, even in situations where full-sampled labels are unavailable. Experiments validate the effectiveness of our proposed method and demonstrate its superiority over existing self-supervised approaches and traditional regularization methods, achieving performance comparable to that of supervised learning methods in certain scenarios.
</details>
<details>
<summary>摘要</summary>
We achieve neural network structure regularization by exploiting the inherent low-rankness of the $k$-space data. Simultaneously, we constrain the network structure to be nonexpansive, ensuring the network's convergence to a fixed point. Thanks to this well-defined network structure, this fixed point can completely reconstruct the missing $k$-space data based on matrix completion theory, even when full-sampled labels are unavailable.Experiments demonstrate the effectiveness of our proposed method and its superiority over existing self-supervised approaches and traditional regularization methods. In certain scenarios, our method achieves performance comparable to that of supervised learning methods.
</details></li>
</ul>
<hr>
<h2 id="Robust-Digital-Twin-Localization-via-An-RGBD-based-Transformer-Network-and-A-Comprehensive-Evaluation-on-a-Mobile-Dataset"><a href="#Robust-Digital-Twin-Localization-via-An-RGBD-based-Transformer-Network-and-A-Comprehensive-Evaluation-on-a-Mobile-Dataset" class="headerlink" title="Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset"></a>Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13570">http://arxiv.org/abs/2309.13570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/augcog/dttd2">https://github.com/augcog/dttd2</a></li>
<li>paper_authors: Zixun Huang, Keling Yao, Seth Z. Zhao, Chuanyu Pan, Tianjian Xu, Weiyu Feng, Allen Y. Yang</li>
<li>for: 本研究旨在探讨数字双技术在3D物体跟踪和地理位置确定方面的潜在作用，并提出一种基于变换器的6DoF姿态估计器，以实现在真实世界噪声数据下的最佳准确性。</li>
<li>methods: 本研究使用变换器来实现6DoF姿态估计器，并通过对现有 литературы的全面验证，提出了一个新的RGBD数据集called Digital Twin Tracking Dataset v2 (DTTD2)，以适应iPhone感知器数据。</li>
<li>results: 经过广泛的实验和深入分析，本研究证明了我们的方法在面临深度数据错误时仍然能够表现出优于现有基elines的性能。<details>
<summary>Abstract</summary>
The potential of digital-twin technology, involving the creation of precise digital replicas of physical objects, to reshape AR experiences in 3D object tracking and localization scenarios is significant. However, enabling robust 3D object tracking in dynamic mobile AR environments remains a formidable challenge. These scenarios often require a more robust pose estimator capable of handling the inherent sensor-level measurement noise. In this paper, recognizing the challenges of comprehensive solutions in existing literature, we propose a transformer-based 6DoF pose estimator designed to achieve state-of-the-art accuracy under real-world noisy data. To systematically validate the new solution's performance against the prior art, we also introduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2), which is focused on digital-twin object tracking scenarios. Expanded from an existing DTTD v1 (DTTD1), the new dataset adds digital-twin data captured using a cutting-edge mobile RGBD sensor suite on Apple iPhone 14 Pro, expanding the applicability of our approach to iPhone sensor data. Through extensive experimentation and in-depth analysis, we illustrate the effectiveness of our methods under significant depth data errors, surpassing the performance of existing baselines. Code and dataset are made publicly available at: https://github.com/augcog/DTTD2
</details>
<details>
<summary>摘要</summary>
“数字双身技术的潜在可能性，即创建精确的数字对象复制，对于3D对象跟踪和本地化场景的AR经验进行重塑，是非常 significannot。然而，在动态 mobil AR 环境中实现Robust 3D对象跟踪仍然是一大挑战。这些场景通常需要一个更加Robust的 pose estimator，可以处理潜在的 sensor-level 测量噪音。在这篇论文中，我们认为现有Literature中的全面解决方案存在挑战，因此我们提出了一种基于 transformer 的 6DoF pose estimator，可以在实际世界噪音数据下实现 state-of-the-art 精度。为了系统地验证我们的新解决方案的性能，我们还发布了一个名为 Digital Twin Tracking Dataset v2 (DTTD2) 的新数据集，该数据集专注于数字双身对象跟踪场景。DTTD2 是基于 DTTD1 的扩展，新增了使用高级 mobil RGBD 感知器 suite 在 Apple iPhone 14 Pro 上 captured 的数字双身数据，使我们的方法可以应用于 iPhone 感知器数据。通过广泛的实验和深入分析，我们证明了我们的方法在重大深度数据错误下可以实现更高的性能，超过现有的基准值。Code 和数据集在 GitHub 上公开，请参考：https://github.com/augcog/DTTD2。”
</details></li>
</ul>
<hr>
<h2 id="Multivariate-Prototype-Representation-for-Domain-Generalized-Incremental-Learning"><a href="#Multivariate-Prototype-Representation-for-Domain-Generalized-Incremental-Learning" class="headerlink" title="Multivariate Prototype Representation for Domain-Generalized Incremental Learning"></a>Multivariate Prototype Representation for Domain-Generalized Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13563">http://arxiv.org/abs/2309.13563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Peng, Piotr Koniusz, Kaiyu Guo, Brian C. Lovell, Peyman Moghadam</li>
<li>for: 这种研究旨在解决深度学习模型在新类样本微调时发生的灾难性忘记问题，以及这种问题在不同领域数据上进行测试时的域shift问题。</li>
<li>methods: 我们提出了一种Domain-Generalized Class-Incremental Learning（DGCIL）方法，该方法能够保持老类，适应新类，并可以在未看过的领域上进行可靠的分类。我们的损失函数保持分类boundary，并且降低每个类的域特定信息。无需保存老示例，我们使用知识传播和估计老类prototype偏移来进行逐步训练。我们的prototype表示基于多变量正态分布，其中的均值和协方差是随着模型特征的变化而不断地适应老类。为了保持老类的表示，我们采用Cholesky分解来采样pseudo-特征。相比之前的pseudo-特征采样策略，我们的方法能够更好地捕捉变异semantic信息。</li>
<li>results: 我们在多个benchmark上进行了实验，并证明了我们的方法的主张。<details>
<summary>Abstract</summary>
Deep learning models suffer from catastrophic forgetting when being fine-tuned with samples of new classes. This issue becomes even more pronounced when faced with the domain shift between training and testing data. In this paper, we study the critical and less explored Domain-Generalized Class-Incremental Learning (DGCIL). We design a DGCIL approach that remembers old classes, adapts to new classes, and can classify reliably objects from unseen domains. Specifically, our loss formulation maintains classification boundaries and suppresses the domain-specific information of each class. With no old exemplars stored, we use knowledge distillation and estimate old class prototype drift as incremental training advances. Our prototype representations are based on multivariate Normal distributions whose means and covariances are constantly adapted to changing model features to represent old classes well by adapting to the feature space drift. For old classes, we sample pseudo-features from the adapted Normal distributions with the help of Cholesky decomposition. In contrast to previous pseudo-feature sampling strategies that rely solely on average mean prototypes, our method excels at capturing varying semantic information. Experiments on several benchmarks validate our claims.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LOGICSEG-Parsing-Visual-Semantics-with-Neural-Logic-Learning-and-Reasoning"><a href="#LOGICSEG-Parsing-Visual-Semantics-with-Neural-Logic-Learning-and-Reasoning" class="headerlink" title="LOGICSEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning"></a>LOGICSEG: Parsing Visual Semantics with Neural Logic Learning and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13556">http://arxiv.org/abs/2309.13556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liulei Li, Wenguan Wang, Yi Yang</li>
<li>for: 填充高性能semantic segmentation模型的潜在空白，使得模型能够更好地理解视觉世界的结构和抽象。</li>
<li>methods: 利用神经 inductive 学习和逻辑推理，将数据和符号知识结合在一起，从而实现视 semantic 解析。</li>
<li>results: 在四个 dataset 上进行了广泛的实验，证明了 LOGICSEG 的效果和通用性。<details>
<summary>Abstract</summary>
Current high-performance semantic segmentation models are purely data-driven sub-symbolic approaches and blind to the structured nature of the visual world. This is in stark contrast to human cognition which abstracts visual perceptions at multiple levels and conducts symbolic reasoning with such structured abstraction. To fill these fundamental gaps, we devise LOGICSEG, a holistic visual semantic parser that integrates neural inductive learning and logic reasoning with both rich data and symbolic knowledge. In particular, the semantic concepts of interest are structured as a hierarchy, from which a set of constraints are derived for describing the symbolic relations and formalized as first-order logic rules. After fuzzy logic-based continuous relaxation, logical formulae are grounded onto data and neural computational graphs, hence enabling logic-induced network training. During inference, logical constraints are packaged into an iterative process and injected into the network in a form of several matrix multiplications, so as to achieve hierarchy-coherent prediction with logic reasoning. These designs together make LOGICSEG a general and compact neural-logic machine that is readily integrated into existing segmentation models. Extensive experiments over four datasets with various segmentation models and backbones verify the effectiveness and generality of LOGICSEG. We believe this study opens a new avenue for visual semantic parsing.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:当前高性能semantic segmentation模型都是纯数据驱动的sub-symbolic方法，而这与人类认知的抽象Visual perception at multiple levels and symbolic reasoning with structured abstraction is in stark contrast. To fill these fundamental gaps, we propose LOGICSEG, a comprehensive visual semantic parser that combines neural inductive learning and logic reasoning with both rich data and symbolic knowledge. Specifically, the semantic concepts of interest are structured as a hierarchy, from which a set of constraints are derived for describing the symbolic relations and formalized as first-order logic rules. After fuzzy logic-based continuous relaxation, logical formulae are grounded onto data and neural computational graphs, thereby enabling logic-induced network training. During inference, logical constraints are packaged into an iterative process and injected into the network in the form of several matrix multiplications, thereby achieving hierarchy-coherent prediction with logic reasoning. These designs together make LOGICSEG a versatile and compact neural-logic machine that can be seamlessly integrated into existing segmentation models. Experimental results over four datasets with various segmentation models and backbones demonstrate the effectiveness and generality of LOGICSEG. We believe this study opens a new avenue for visual semantic parsing.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Dice-Focal-Loss-trained-3D-Residual-UNet-for-Automated-Lesion-Segmentation-in-Whole-Body-FDG-PET-CT-Images"><a href="#Generalized-Dice-Focal-Loss-trained-3D-Residual-UNet-for-Automated-Lesion-Segmentation-in-Whole-Body-FDG-PET-CT-Images" class="headerlink" title="Generalized Dice Focal Loss trained 3D Residual UNet for Automated Lesion Segmentation in Whole-Body FDG PET&#x2F;CT Images"></a>Generalized Dice Focal Loss trained 3D Residual UNet for Automated Lesion Segmentation in Whole-Body FDG PET&#x2F;CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13553">http://arxiv.org/abs/2309.13553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ahxmeds/autosegnet">https://github.com/ahxmeds/autosegnet</a></li>
<li>paper_authors: Shadab Ahamed, Arman Rahmim</li>
<li>For: The paper is written for developing a comprehensive PET&#x2F;CT lesion segmentation model for routine quantitative image analysis.* Methods: The paper uses a 3D Residual UNet with Generalized Dice Focal Loss function on the AutoPET challenge 2023 training dataset, and develops the model in a 5-fold cross-validation setting with ensemble learning.* Results: The average ensemble achieved a Dice similarity coefficient (DSC) of 0.5417, false-positive volume (FPV) of 0.8261 ml, and false negative volume (FNV) of 0.2538 ml, while the weighted-average ensemble achieved similar results.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了开发一个 Routine 量化图像分析中的 PET&#x2F;CT 癌症分割模型。* Methods: 这篇论文使用了 3D Residual UNet 与 Generalized Dice Focal Loss 函数在 AutoPET 挑战 2023 训练集上进行了训练，并使用了 5-fold 交叉验证设置和 ensemble 学习。* Results: 平均ensemble 达到了 Dice 相似度系数 (DSC) 为 0.5417，false-positive volume (FPV) 为 0.8261 ml，false negative volume (FNV) 为 0.2538 ml，而 weighted-average ensemble 也达到了类似的结果。<details>
<summary>Abstract</summary>
Automated segmentation of cancerous lesions in PET/CT images is a vital initial task for quantitative analysis. However, it is often challenging to train deep learning-based segmentation methods to high degree of accuracy due to the diversity of lesions in terms of their shapes, sizes, and radiotracer uptake levels. These lesions can be found in various parts of the body, often close to healthy organs that also show significant uptake. Consequently, developing a comprehensive PET/CT lesion segmentation model is a demanding endeavor for routine quantitative image analysis. In this work, we train a 3D Residual UNet using Generalized Dice Focal Loss function on the AutoPET challenge 2023 training dataset. We develop our models in a 5-fold cross-validation setting and ensemble the five models via average and weighted-average ensembling. On the preliminary test phase, the average ensemble achieved a Dice similarity coefficient (DSC), false-positive volume (FPV) and false negative volume (FNV) of 0.5417, 0.8261 ml, and 0.2538 ml, respectively, while the weighted-average ensemble achieved 0.5417, 0.8186 ml, and 0.2538 ml, respectively. Our algorithm can be accessed via this link: https://github.com/ahxmeds/autosegnet.
</details>
<details>
<summary>摘要</summary>
自动 segmentation of cancerous lesions in PET/CT images 是一项非常重要的初始任务，用于量化分析。然而，由于肿瘤的多样性，包括形状、大小和辐射追踪水平，因此往往具有很高的学习难度。这些肿瘤可以在体内各个部位找到， часто靠近健康的器官，这些器官也会显示出明显的辐射吸收。因此，开发一个全面的 PET/CT 肿瘤 segmentation 模型是一项复杂的任务，用于日常量化图像分析。在这个工作中，我们使用 Generalized Dice Focal Loss 函数来训练一个 3D Residual UNet 模型。我们在 5-fold 跨Validation  Setting 中进行了模型开发，并使用 average 和 weighted-average  ensemble。在预liminary test阶段，average ensemble 达到了 Dice similarity coefficient (DSC)、false-positive volume (FPV) 和 false negative volume (FNV) 的值为 0.5417，0.8261 ml 和 0.2538 ml，分别。而 weighted-average ensemble 达到了 0.5417，0.8186 ml 和 0.2538 ml，分别。我们的算法可以通过以下链接访问：https://github.com/ahxmeds/autosegnet。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Robot-3D-Perception-in-Urban-Environments-The-UT-Campus-Object-Dataset"><a href="#Towards-Robust-Robot-3D-Perception-in-Urban-Environments-The-UT-Campus-Object-Dataset" class="headerlink" title="Towards Robust Robot 3D Perception in Urban Environments: The UT Campus Object Dataset"></a>Towards Robust Robot 3D Perception in Urban Environments: The UT Campus Object Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13549">http://arxiv.org/abs/2309.13549</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ut-amrl/coda-models">https://github.com/ut-amrl/coda-models</a></li>
<li>paper_authors: Arthur Zhang, Chaitanya Eranki, Christina Zhang, Ji-Hwan Park, Raymond Hong, Pranav Kalyani, Lochana Kalyanaraman, Arsh Gamare, Arnav Bagad, Maria Esteva, Joydeep Biswas</li>
<li>for: 这个论文是为了提供一个大学校园环境下的自主 Navigation 的数据集，用于 Egocentric 3D 识别和规划。</li>
<li>methods: 该论文使用了多 modal 感知器，包括 3D 点云和颜色视频，以及 RGB-D 视频和 IMU 传感器，并提供了大量的Annotation。</li>
<li>results: 该论文的实验结果表明，使用 CODa 数据集可以提高urban 环境中 3D  объек检测性能，并且 sensor-specific 细化调整和预训练可以进一步提高检测精度。<details>
<summary>Abstract</summary>
We introduce the UT Campus Object Dataset (CODa), a mobile robot egocentric perception dataset collected on the University of Texas Austin Campus. Our dataset contains 8.5 hours of multimodal sensor data: synchronized 3D point clouds and stereo RGB video from a 128-channel 3D LiDAR and two 1.25MP RGB cameras at 10 fps; RGB-D videos from an additional 0.5MP sensor at 7 fps, and a 9-DOF IMU sensor at 40 Hz. We provide 58 minutes of ground-truth annotations containing 1.3 million 3D bounding boxes with instance IDs for 53 semantic classes, 5000 frames of 3D semantic annotations for urban terrain, and pseudo-ground truth localization. We repeatedly traverse identical geographic locations for a wide range of indoor and outdoor areas, weather conditions, and times of the day. Using CODa, we empirically demonstrate that: 1) 3D object detection performance in urban settings is significantly higher when trained using CODa compared to existing datasets even when employing state-of-the-art domain adaptation approaches, 2) sensor-specific fine-tuning improves 3D object detection accuracy and 3) pretraining on CODa improves cross-dataset 3D object detection performance in urban settings compared to pretraining on AV datasets. Using our dataset and annotations, we release benchmarks for 3D object detection and 3D semantic segmentation using established metrics. In the future, the CODa benchmark will include additional tasks like unsupervised object discovery and re-identification. We publicly release CODa on the Texas Data Repository, pre-trained models, dataset development package, and interactive dataset viewer on our website at https://amrl.cs.utexas.edu/coda. We expect CODa to be a valuable dataset for research in egocentric 3D perception and planning for autonomous navigation in urban environments.
</details>
<details>
<summary>摘要</summary>
我们介绍UT кампус物件Dataset（CODa），是一个移动机器人自我观察 Dataset，在德州大学奥斯汀分校范围内收集到的8.5小时多modal感应数据。我们的数据包括同步3D点云和stereoRGB影像，来自128通道3D LiDAR和两个1.25MPRGB摄像头，每秒10帧;RGB-D影像从额外0.5MP感应器，每秒7帧，以及9DOF IMU感应器，每秒40Hz。我们提供58分钟的真实标注，包括1.3百万个3D bounding box，每个物体都有实体ID，分配到53个semantic class中;5000帧3D实体标注，用于城市地形的处理;以及假的地理位置标注。我们在同一个地理位置上重复探索了各种室内和室外区域，天气状况和时间。使用CODa，我们经过实验证明：1）在城市设置中，使用CODa进行训练后，3D物体检测性能高于现有数据集，即使使用现有的领域适应方法;2）感应器特定的精确调整可以提高3D物体检测精度;3）使用CODa进行预训可以在城市设置中提高交叉数据集3D物体检测性能。我们在我们的网站上公开了CODa，包括预训模型、数据开发套件和互动数据检视器，可以在https://amrl.cs.utexas.edu/coda 中找到。我们预期CODa将成为城市自主navigation egocentric 3D视察和规划的重要数据集。
</details></li>
</ul>
<hr>
<h2 id="DFRD-Data-Free-Robustness-Distillation-for-Heterogeneous-Federated-Learning"><a href="#DFRD-Data-Free-Robustness-Distillation-for-Heterogeneous-Federated-Learning" class="headerlink" title="DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning"></a>DFRD: Data-Free Robustness Distillation for Heterogeneous Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13546">http://arxiv.org/abs/2309.13546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kangyang Luo, Shuai Wang, Yexuan Fu, Xiang Li, Yunshi Lan, Ming Gao</li>
<li>for: 提出了一种隐私保护的分布式学习方法（DFRD），可以在数据不同和模型不同的场景下培养一个稳定和有效的全局模型。</li>
<li>methods: 在服务器端使用一个条件生成器来估算本地模型上传的训练空间，并系统地调查其训练的准确度、传输性和多样性。</li>
<li>results: 通过实验证明，DFRD在多个图像分类任务上比最佳参考模型具有显著的性能提升。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a privacy-constrained decentralized machine learning paradigm in which clients enable collaborative training without compromising private data. However, how to learn a robust global model in the data-heterogeneous and model-heterogeneous FL scenarios is challenging. To address it, we resort to data-free knowledge distillation to propose a new FL method (namely DFRD). DFRD equips a conditional generator on the server to approximate the training space of the local models uploaded by clients, and systematically investigates its training in terms of fidelity, transferability} and diversity. To overcome the catastrophic forgetting of the global model caused by the distribution shifts of the generator across communication rounds, we maintain an exponential moving average copy of the generator on the server. Additionally, we propose dynamic weighting and label sampling to accurately extract knowledge from local models. Finally, our extensive experiments on various image classification tasks illustrate that DFRD achieves significant performance gains compared to SOTA baselines.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种遵循 privacy 的分布式机器学习模式，在Client端实现协同训练而无需披露私人数据。然而，在数据不同和模型不同的 FL 场景中，学习 Robust 的全球模型是一个挑战。为此，我们通过不使用数据的知识热化来提出一种新的 FL 方法（namely DFRD）。DFRD 在服务器端安装一个Conditional generator，用于模拟客户端上传的本地模型的训练空间，并系统地研究其训练的准确性、传递性和多样性。为了解决由生成器在交流周期中的分布转移所引起的全球模型的忘却性，我们在服务器端维护一个指数移动平均的生成器复制。此外，我们提出了动态权重和标签采样，以准确地提取本地模型中的知识。最后，我们在不同的图像分类任务上进行了广泛的实验，结果显示，DFRD 与当前的标准基eline相比， achieved 显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Evaluation-of-Transfer-Learning-for-Classification-of-Brain-Tumor-Using-MRI"><a href="#Comparative-Evaluation-of-Transfer-Learning-for-Classification-of-Brain-Tumor-Using-MRI" class="headerlink" title="Comparative Evaluation of Transfer Learning for Classification of Brain Tumor Using MRI"></a>Comparative Evaluation of Transfer Learning for Classification of Brain Tumor Using MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.02270">http://arxiv.org/abs/2310.02270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abu Kaisar Mohammad Masum, Nusrat Badhon, S. M. Saiful Islam Badhon, Nushrat Jahan Ria, Sheikh Abujar, Muntaser Mansur Syed, Naveed Mahmud</li>
<li>for: 这项研究旨在利用计算机助成诊断技术，尤其是机器学习和深度学习，以分类三种脑肿瘤。</li>
<li>methods: 我们使用了四种转移学习技术来分类脑肿瘤，并在一个标准数据集上进行测试，包括3064个MRI图像，表示三种脑肿瘤。</li>
<li>results: 我们发现，使用ResNet-50模型可以达到99.06%的准确率，超过其他模型。我们还证明了如何在均衡数据集上提高准确率，而无需使用扩展方法。<details>
<summary>Abstract</summary>
Abnormal growth of cells in the brain and its surrounding tissues is known as a brain tumor. There are two types, one is benign (non-cancerous) and another is malignant (cancerous) which may cause death. The radiologists' ability to diagnose malignancies is greatly aided by magnetic resonance imaging (MRI). Brain cancer diagnosis has been considerably expedited by the field of computer-assisted diagnostics, especially in machine learning and deep learning. In our study, we categorize three different kinds of brain tumors using four transfer learning techniques. Our models were tested on a benchmark dataset of $3064$ MRI pictures representing three different forms of brain cancer. Notably, ResNet-50 outperformed other models with a remarkable accuracy of $99.06\%$. We stress the significance of a balanced dataset for improving accuracy without the use of augmentation methods. Additionally, we experimentally demonstrate our method and compare with other classification algorithms on the CE-MRI dataset using evaluations like F1-score, AUC, precision and recall.
</details>
<details>
<summary>摘要</summary>
异常组织增长在脑和周围组织中 known as 脑肿瘤。这有两种，一种是非恶性（非癌细胞），另一种是恶性（癌细胞），可能导致死亡。医学影像识别异常性的能力得到了巨大的助益，特别是在电磁共振成像（MRI）和电脑协助诊断领域。在我们的研究中，我们分类了三种不同的脑肿瘤，使用四种转移学习技术。我们的模型在一个底本数据集上进行测试，包括3064幅 MRI 照片，代表三种不同的脑癌。值得注意的是，ResNet-50 的准确率达到了99.06%，在其他模型中具有卓越的表现。我们强调了统计数据的平衡性，以提高准确性，而不需使用增强方法。此外，我们实验性地评估了我们的方法，并与其他分类算法进行比较，使用评估指标如 F1 分数、AUC、精度和 recall。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Domain-Generalization-for-Object-Detection-via-Language-Guided-Feature-Alignment"><a href="#Semi-Supervised-Domain-Generalization-for-Object-Detection-via-Language-Guided-Feature-Alignment" class="headerlink" title="Semi-Supervised Domain Generalization for Object Detection via Language-Guided Feature Alignment"></a>Semi-Supervised Domain Generalization for Object Detection via Language-Guided Feature Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13525">http://arxiv.org/abs/2309.13525</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sinamalakouti/CDDMSL">https://github.com/sinamalakouti/CDDMSL</a></li>
<li>paper_authors: Sina Malakouti, Adriana Kovashka</li>
<li>for: 这篇论文旨在解决半有 labels 的领域泛化（Domain Generalization，DG）和领域转换（Domain Adaptation，DA）问题，并且将vision-language预训应用于这个问题。</li>
<li>methods: 这篇论文使用了一种新的 Cross-Domain Descriptive Multi-Scale Learning（CDDMSL）方法，它通过将图像描述在语言空间中进行对领域特有特征的对应，以实现图像描述的协调。</li>
<li>results:  compared to existing methods, CDDMSL 在 DG 和 DA 环境中都有着重要的进步，实现了11.7%和7.5%的改善。<details>
<summary>Abstract</summary>
Existing domain adaptation (DA) and generalization (DG) methods in object detection enforce feature alignment in the visual space but face challenges like object appearance variability and scene complexity, which make it difficult to distinguish between objects and achieve accurate detection. In this paper, we are the first to address the problem of semi-supervised domain generalization by exploring vision-language pre-training and enforcing feature alignment through the language space. We employ a novel Cross-Domain Descriptive Multi-Scale Learning (CDDMSL) aiming to maximize the agreement between descriptions of an image presented with different domain-specific characteristics in the embedding space. CDDMSL significantly outperforms existing methods, achieving 11.7% and 7.5% improvement in DG and DA settings, respectively. Comprehensive analysis and ablation studies confirm the effectiveness of our method, positioning CDDMSL as a promising approach for domain generalization in object detection tasks.
</details>
<details>
<summary>摘要</summary>
现有的领域适应（DA）和通用化（DG）方法在物体检测中强制视觉空间中的特征对齐，但面临对象外观多样性和场景复杂性等挑战，这使得分辨对象并不容易，精度检测也不高。在这篇论文中，我们是首次解决半supervised领域通用化问题，通过探索视觉语言预训练和在语言空间强制特征对齐。我们提出了一种新的跨领域描述多Scale学习（CDDMSL），旨在 maximize图像的描述在嵌入空间中的一致性。CDDMSL与现有方法相比，显著提高了11.7%和7.5%的提升率，分别在DA和DG设置下。广泛的分析和缺省研究证明了我们的方法的有效性，positioning CDDMSL为领域通用化在物体检测任务中的可靠方法。
</details></li>
</ul>
<hr>
<h2 id="LiDAR-UDA-Self-ensembling-Through-Time-for-Unsupervised-LiDAR-Domain-Adaptation"><a href="#LiDAR-UDA-Self-ensembling-Through-Time-for-Unsupervised-LiDAR-Domain-Adaptation" class="headerlink" title="LiDAR-UDA: Self-ensembling Through Time for Unsupervised LiDAR Domain Adaptation"></a>LiDAR-UDA: Self-ensembling Through Time for Unsupervised LiDAR Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13523">http://arxiv.org/abs/2309.13523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirreza Shaban, JoonHo Lee, Sanghun Jung, Xiangyun Meng, Byron Boots</li>
<li>for: 这个研究是为了提出一个基于自适应领域对应（UDA）的 LiDAR 分类方法，以应对不同 LiDAR 感应器配置所带来的领域差异。</li>
<li>methods: 这个方法使用了两个技术来降低感应器差异和提高pseudo标签质量：1）LiDAR 焦点抽样，实现不同 LiDAR 扫描模式的模拟；2）跨帧聚合，利用 consecutive 帧的时间一致性来生成更可靠的pseudo标签。</li>
<li>results: 这个方法在多个公开 LiDAR 数据集上进行评估，与现有的方法相比，获得了更高的平均 mIoU 分量 ($3.9%$) 。<details>
<summary>Abstract</summary>
We introduce LiDAR-UDA, a novel two-stage self-training-based Unsupervised Domain Adaptation (UDA) method for LiDAR segmentation. Existing self-training methods use a model trained on labeled source data to generate pseudo labels for target data and refine the predictions via fine-tuning the network on the pseudo labels. These methods suffer from domain shifts caused by different LiDAR sensor configurations in the source and target domains. We propose two techniques to reduce sensor discrepancy and improve pseudo label quality: 1) LiDAR beam subsampling, which simulates different LiDAR scanning patterns by randomly dropping beams; 2) cross-frame ensembling, which exploits temporal consistency of consecutive frames to generate more reliable pseudo labels. Our method is simple, generalizable, and does not incur any extra inference cost. We evaluate our method on several public LiDAR datasets and show that it outperforms the state-of-the-art methods by more than $3.9\%$ mIoU on average for all scenarios. Code will be available at https://github.com/JHLee0513/LiDARUDA.
</details>
<details>
<summary>摘要</summary>
我们介绍了LiDAR-UDA，一种新的两阶段自我训练基于无监督领域适应（UDA）方法，用于LiDAR分割。现有的自我训练方法使用一个基于源数据的模型来生成目标数据的假标签，然后通过调整网络来提高预测。这些方法受到源和目标领域之间的频率差引起的频率差问题。我们提出了两种技术来减少探测器差异并提高假标签质量：1）LiDAR扫描方式抽样，可以模拟不同的LiDAR扫描方式，通过随机删除探测器来实现；2）同帧集成，可以利用连续帧的时间一致性来生成更可靠的假标签。我们的方法简单、普适，无需额外的推理成本。我们在一些公共LiDAR数据集上评估了我们的方法，并证明它在所有场景上超过了state-of-the-art方法的$3.9\%$ mIoU平均提升。代码将在https://github.com/JHLee0513/LiDARUDA上提供。
</details></li>
</ul>
<hr>
<h2 id="InSpaceType-Reconsider-Space-Type-in-Indoor-Monocular-Depth-Estimation"><a href="#InSpaceType-Reconsider-Space-Type-in-Indoor-Monocular-Depth-Estimation" class="headerlink" title="InSpaceType: Reconsider Space Type in Indoor Monocular Depth Estimation"></a>InSpaceType: Reconsider Space Type in Indoor Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13516">http://arxiv.org/abs/2309.13516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cho-Ying Wu, Quankai Gao, Chin-Cheng Hsu, Te-Lin Wu, Jing-Wen Chen, Ulrich Neumann</li>
<li>for: 本研究旨在探讨indoor monocular depth estimation方法在实际场景中的稳定性和泛化性，特别是在不同的空间类型下的表现。</li>
<li>methods: 本研究使用了11种最新的方法进行比较，并发现这些方法在不同的空间类型下存在明显的表现偏好。</li>
<li>results: 研究发现，现有的方法在不同的空间类型下存在明显的性能差异，表明这些方法存在偏好，而且在某些空间类型下表现非常差。<details>
<summary>Abstract</summary>
Indoor monocular depth estimation has attracted increasing research interest. Most previous works have been focusing on methodology, primarily experimenting with NYU-Depth-V2 (NYUv2) Dataset, and only concentrated on the overall performance over the test set. However, little is known regarding robustness and generalization when it comes to applying monocular depth estimation methods to real-world scenarios where highly varying and diverse functional \textit{space types} are present such as library or kitchen. A study for performance breakdown into space types is essential to realize a pretrained model's performance variance. To facilitate our investigation for robustness and address limitations of previous works, we collect InSpaceType, a high-quality and high-resolution RGBD dataset for general indoor environments. We benchmark 11 recent methods on InSpaceType and find they severely suffer from performance imbalance concerning space types, which reveals their underlying bias. We extend our analysis to 4 other datasets, 3 mitigation approaches, and the ability to generalize to unseen space types. Our work marks the first in-depth investigation of performance imbalance across space types for indoor monocular depth estimation, drawing attention to potential safety concerns for model deployment without considering space types, and further shedding light on potential ways to improve robustness. See \url{https://depthcomputation.github.io/DepthPublic} for data.
</details>
<details>
<summary>摘要</summary>
内部单目深度估计已经吸引了越来越多的研究兴趣。大多数前一些工作都是在方法ologies上进行了尝试，主要使用NYU-Depth-V2（NYUv2）数据集，并且只是对测试集的总性性能进行了评估。然而，对于实际世界场景中的应用，尚不甚了解单目深度估计方法的稳定性和泛化性。为了实现预训练模型的性能变化，我们需要进行空间类型的性能剖析。为了促进我们的调查和解决前一些工作的局限性，我们收集了InSpaceType，一个高质量、高分辨率的RGBD数据集，用于普遍的内部环境。我们对InSpaceType进行了11种最近的方法的测试，发现它们在不同的空间类型上表现出了严重的性能偏好。我们还扩展了我们的分析至4个其他数据集、3种缓解方法和无seen空间类型的能力。我们的工作是内部单目深度估计中首次对空间类型的性能偏好进行了深入的调查，这引起了关注在没有考虑空间类型的情况下部署模型可能存在的安全风险，以及如何提高模型的稳定性。参考链接：<https://depthcomputation.github.io/DepthPublic>。
</details></li>
</ul>
<hr>
<h2 id="Rewrite-Caption-Semantics-Bridging-Semantic-Gaps-for-Language-Supervised-Semantic-Segmentation"><a href="#Rewrite-Caption-Semantics-Bridging-Semantic-Gaps-for-Language-Supervised-Semantic-Segmentation" class="headerlink" title="Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation"></a>Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13505">http://arxiv.org/abs/2309.13505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xing0047/rewrite">https://github.com/xing0047/rewrite</a></li>
<li>paper_authors: Yun Xing, Jian Kang, Aoran Xiao, Jiahao Nie, Shao Ling, Shijian Lu</li>
<li>for: 增强语言授 зада务下的semantic segmentation的能力，使得图像可以通过文本描述进行空间localization。</li>
<li>methods: 利用CLIP来补做缺失的semantics，建立一个概念库，并通过群集导航 sampling来选择相关的概念，然后将其 feed into pre-training。</li>
<li>results: 在8个 segmentation benchmark上进行了广泛的实验，表明CoCu可以减轻语言授 зада务下的semantic gap，大幅提高语言授 зада务下的semantic segmentation的性能。<details>
<summary>Abstract</summary>
Vision-Language Pre-training has demonstrated its remarkable zero-shot recognition ability and potential to learn generalizable visual representations from language supervision. Taking a step ahead, language-supervised semantic segmentation enables spatial localization of textual inputs by learning pixel grouping solely from image-text pairs. Nevertheless, the state-of-the-art suffers from clear semantic gaps between visual and textual modality: plenty of visual concepts appeared in images are missing in their paired captions. Such semantic misalignment circulates in pre-training, leading to inferior zero-shot performance in dense predictions due to insufficient visual concepts captured in textual representations. To close such semantic gap, we propose Concept Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing semantics. For each image-text pair, we establish a concept archive that maintains potential visually-matched concepts with our proposed vision-driven expansion and text-to-vision-guided ranking. Relevant concepts can thus be identified via cluster-guided sampling and fed into pre-training, thereby bridging the gap between visual and textual semantics. Extensive experiments over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb zero-shot transfer performance and greatly boosts language-supervised segmentation baseline by a large margin, suggesting the value of bridging semantic gap in pre-training data.
</details>
<details>
<summary>摘要</summary>
“视言预训示出了无需示例数据的惊人识别能力和可能学习通用的视觉表示。尝试一步前进，语言指导的semantic segmentation可以将文本输入的空间局部化，通过从图像和文本对的学习像素组合。然而，当前的状态艺术受到清晰的Semantic Gap问题困扰，即图像中的许多视觉概念没有在其关联的文本中出现。这种semantic misalignment在预训练中循环，导致零例预测中的稠密预测性能下降，因为预训练中的文本表示中缺失的视觉概念。为了填充这种semantic gap，我们提出了Concept Curation（CoCu）管线，它利用CLIP来补偿缺失的semantics。对每个图像和文本对，我们建立了一个concept archive，该archive保存了可能与图像匹配的视觉概念，我们提出的视力驱动扩展和文本驱动的排名。通过群组指导采样，可以从concept archive中提取相关的概念，并将其传递给预训练，从而bridging视觉和文本semantic之间的 gap。我们对8种 segmentation benchmark进行了广泛的实验，结果表明CoCu可以 achieve superb zero-shot transfer performance，并大幅提高语言指导 segmentation baseline，这表明bridging semantic gap在预训练数据中的价值。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/cs.CV_2023_09_24/" data-id="clp88dbw600k4ob88bkwfgc54" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/cs.AI_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T12:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/24/cs.AI_2023_09_24/">cs.AI - 2023-09-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GHN-QAT-Training-Graph-Hypernetworks-to-Predict-Quantization-Robust-Parameters-of-Unseen-Limited-Precision-Neural-Networks"><a href="#GHN-QAT-Training-Graph-Hypernetworks-to-Predict-Quantization-Robust-Parameters-of-Unseen-Limited-Precision-Neural-Networks" class="headerlink" title="GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust Parameters of Unseen Limited Precision Neural Networks"></a>GHN-QAT: Training Graph Hypernetworks to Predict Quantization-Robust Parameters of Unseen Limited Precision Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13773">http://arxiv.org/abs/2309.13773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stone Yun, Alexander Wong</li>
<li>for: 这 paper 的目的是研究 Graph Hypernetworks (GHN) 可以预测 CNN 架构中不同参数的值，并且在预测过程中减少了大量的优化迭代。</li>
<li>methods: 这 paper 使用 GHN 预测 CNN 架构中的参数，并且对预测结果进行了量化化。</li>
<li>results: 这 paper 的结果表明，通过在量化 aware 训练中使用 GHN 预测参数，可以提高量化后 CNN 的准确率，并且在一些情况下可以达到随机 initialization 的水平。<details>
<summary>Abstract</summary>
Graph Hypernetworks (GHN) can predict the parameters of varying unseen CNN architectures with surprisingly good accuracy at a fraction of the cost of iterative optimization. Following these successes, preliminary research has explored the use of GHNs to predict quantization-robust parameters for 8-bit and 4-bit quantized CNNs. However, this early work leveraged full-precision float32 training and only quantized for testing. We explore the impact of quantization-aware training and/or other quantization-based training strategies on quantized robustness and performance of GHN predicted parameters for low-precision CNNs. We show that quantization-aware training can significantly improve quantized accuracy for GHN predicted parameters of 4-bit quantized CNNs and even lead to greater-than-random accuracy for 2-bit quantized CNNs. These promising results open the door for future explorations such as investigating the use of GHN predicted parameters as initialization for further quantized training of individual CNNs, further exploration of "extreme bitwidth" quantization, and mixed precision quantization schemes.
</details>
<details>
<summary>摘要</summary>
格子嵌入网络（GHN）可以预测未seen convolutional neural network（CNN）的参数， surprisingly good accuracy at a fraction of the cost of iterative optimization. Following these successes, preliminary research has explored the use of GHNs to predict quantization-robust parameters for 8-bit and 4-bit quantized CNNs. However, this early work leveraged full-precision float32 training and only quantized for testing. We explore the impact of quantization-aware training and/or other quantization-based training strategies on quantized robustness and performance of GHN predicted parameters for low-precision CNNs. We show that quantization-aware training can significantly improve quantized accuracy for GHN predicted parameters of 4-bit quantized CNNs and even lead to greater-than-random accuracy for 2-bit quantized CNNs. These promising results open the door for future explorations such as investigating the use of GHN predicted parameters as initialization for further quantized training of individual CNNs, further exploration of "extreme bitwidth" quantization, and mixed precision quantization schemes.Here's the text with the Chinese characters and English translation:格子嵌入网络（GHN）可以预测未seen convolutional neural network（CNN）的参数， surprisingly good accuracy at a fraction of the cost of iterative optimization.following these successes, preliminary research has explored the use of GHNs to predict quantization-robust parameters for 8-bit and 4-bit quantized CNNs.However, this early work leveraged full-precision float32 training and only quantized for testing.We explore the impact of quantization-aware training and/or other quantization-based training strategies on quantized robustness and performance of GHN predicted parameters for low-precision CNNs.We show that quantization-aware training can significantly improve quantized accuracy for GHN predicted parameters of 4-bit quantized CNNs and even lead to greater-than-random accuracy for 2-bit quantized CNNs.These promising results open the door for future explorations such as investigating the use of GHN predicted parameters as initialization for further quantized training of individual CNNs, further exploration of "extreme bitwidth" quantization, and mixed precision quantization schemes.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Based-Connector-Detection-for-Robotized-Assembly-of-Automotive-Wire-Harnesses"><a href="#Deep-Learning-Based-Connector-Detection-for-Robotized-Assembly-of-Automotive-Wire-Harnesses" class="headerlink" title="Deep Learning-Based Connector Detection for Robotized Assembly of Automotive Wire Harnesses"></a>Deep Learning-Based Connector Detection for Robotized Assembly of Automotive Wire Harnesses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13746">http://arxiv.org/abs/2309.13746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wang, Björn Johansson</li>
<li>for: 本研究旨在提高自动化汽车电子零部件的质量，通过深度学习方法探测汽车电缆套件中的连接器。</li>
<li>methods: 本研究使用了两种对象检测模型，一种是两stage模型，另一种是一stage模型，以 trains和评估数据集来检测汽车电缆套件中的连接器。</li>
<li>results: 实验结果表明，深度学习方法可以有效检测汽车电缆套件中的连接器，但连接器外部设计有限制。<details>
<summary>Abstract</summary>
The shift towards electrification and autonomous driving in the automotive industry results in more and more automotive wire harnesses being installed in modern automobiles, which stresses the great significance of guaranteeing the quality of automotive wire harness assembly. The mating of connectors is essential in the final assembly of automotive wire harnesses due to the importance of connectors on wire harness connection and signal transmission. However, the current manual operation of mating connectors leads to severe problems regarding assembly quality and ergonomics, where the robotized assembly has been considered, and different vision-based solutions have been proposed to facilitate a better perception of the robot control system on connectors. Nonetheless, there has been a lack of deep learning-based solutions for detecting automotive wire harness connectors in previous literature. This paper presents a deep learning-based connector detection for robotized automotive wire harness assembly. A dataset of twenty automotive wire harness connectors was created to train and evaluate a two-stage and a one-stage object detection model, respectively. The experiment results indicate the effectiveness of deep learning-based connector detection for automotive wire harness assembly but are limited by the design of the exteriors of connectors.
</details>
<details>
<summary>摘要</summary>
随着汽车业的电动化和自动驾驶技术的发展，现代汽车中的电动线套件越来越多，因此保证汽车电动线套件的质量变得非常重要。连接器的匹配在汽车电动线套件的最终组装中是非常重要的，因为连接器对电动线套件的连接和信号传输具有非常重要的作用。然而，现有的手动操作匹配连接器会导致组装质量和人机工程学习的严重问题，而Robotized assembly受到了考虑，不同的视觉基于解决方案也被提出，但在过去的文献中没有深入学习基于解决方案。本文提出了深入学习基于的汽车电动线套件连接器检测方法，并创建了20个汽车电动线套件连接器的数据集来训练和评估两个阶段和一个阶段对象检测模型。实验结果表明深入学习基于的连接器检测方法在汽车电动线套件组装中是有效的，但由于连接器的外部设计，其限制了检测的精度。
</details></li>
</ul>
<hr>
<h2 id="Computer-Vision-Technology-for-Robotized-Wire-Harness-Assembly"><a href="#Computer-Vision-Technology-for-Robotized-Wire-Harness-Assembly" class="headerlink" title="Computer Vision Technology for Robotized Wire Harness Assembly"></a>Computer Vision Technology for Robotized Wire Harness Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13745">http://arxiv.org/abs/2309.13745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wang, Omkar Salunkhe, Walter Quadrini, Dan Lämkull, Fredrik Ore, Björn Johansson, Johan Stahre</li>
<li>for: 本研究旨在提高汽车电子系统的绝缘电缆组装质量、效率和人机交互性，满足现代汽车电子系统的需求。</li>
<li>methods: 本研究使用计算机视觉技术来自动化绝缘电缆组装，以提高抗压缩性和抗摩擦性，并且可以在实际生产环境中实现自动化组装。</li>
<li>results: 本研究发现，计算机视觉技术可以帮助机器人更好地识别和操纵绝缘电缆，提高自动化组装的精度和效率。但是，还有一些研究 gap 需要进一步研究，以便在实际生产环境中实现更加实用的机器人自动化组装。<details>
<summary>Abstract</summary>
Wire harnesses are essential hardware for electronic systems in modern automotive vehicles. With a shift in the automotive industry towards electrification and autonomous driving, more and more automotive electronics are responsible for energy transmission and safety-critical functions such as maneuvering, driver assistance, and safety system. This paradigm shift places more demand on automotive wiring harnesses from the safety perspective and stresses the greater importance of high-quality wire harness assembly in vehicles. However, most of the current operations of wire harness assembly are still performed manually by skilled workers, and some of the manual processes are problematic from different perspectives, such as quality control and ergonomics. There is also a persistent demand in the industry to increase competitiveness and gain market share. Hence, assuring assembly quality while improving ergonomics and optimizing labor costs is desired. Robotized assembly, accomplished by robots or in human-robot collaboration, is a key enabler for fulfilling the increasingly demanding quality and safety as it enables more replicable, transparent, and comprehensible processes than completely manual operations. However, robotized assembly of wire harnesses is challenging in real environments due to the flexibility of the deformable objects, though many preliminary automation solutions have been proposed under simplified industrial configurations. Previous research efforts have proposed the use of computer vision technology to facilitate robotized automation of wire harness assembly, enabling the robots to better perceive and manipulate the flexible wire harness. This article presents an overview on computer vision technology proposed for robotized wire harness assembly and derives research gaps that require further study to facilitate a more practical robotized assembly of wire harness.
</details>
<details>
<summary>摘要</summary>
电子系统在现代汽车中的重要硬件是电缆集成。随着汽车工业向电气化和自动驾驶转变，电缆集成的重要性日益增加，它们不仅承担了能量传输，还承担了安全关键功能，如行驶助手、驾驶员助手和安全系统。这种平台转移增加了电缆集成的安全要求，同时也增加了对高质量电缆组装的需求。然而，大多数现有的电缆组装过程仍然是手动完成的，有些手动过程存在质量控制和人体工程学问题。此外，业界也有强烈的竞争和市场份额增长的需求。因此，保证组装质量的同时，改善人体工程学和优化劳动成本是需要的。 robotized assembly，通过机器人或人机合作，是实现提高质量和安全性的关键。然而，在真实环境中，机器人化电缆组装具有较大的挑战，主要是因为电缆是可变形的物体。虽然有许多先前的自动化解决方案在 simplifies 的工业配置下得到了应用，但是在真实环境中，这些解决方案很难实现。以前的研究努力已经提出了利用计算机视觉技术来实现机器人化电缆组装，使机器人可以更好地感知和操纵 flexible 的电缆。本文提供了计算机视觉技术在机器人化电缆组装中的概述，并确定了需要进一步研究的研究漏洞，以便更好地实现实用的机器人化电缆组装。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Literature-Review-of-Computer-Vision-Applications-in-Robotized-Wire-Harness-Assembly"><a href="#A-Systematic-Literature-Review-of-Computer-Vision-Applications-in-Robotized-Wire-Harness-Assembly" class="headerlink" title="A Systematic Literature Review of Computer Vision Applications in Robotized Wire Harness Assembly"></a>A Systematic Literature Review of Computer Vision Applications in Robotized Wire Harness Assembly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13744">http://arxiv.org/abs/2309.13744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wang, Omkar Salunkhe, Walter Quadrini, Björn Johansson, Dan Lämkull, Fredrik Ore, Mélanie Despeisse, Luca Fumagalli, Johan Stahre</li>
<li>for: 这篇论文探讨了计算机视觉技术在机器人化电缆组装中的应用，挑战现有研究所出现的挑战，并提出未来研究的机遇以促进实用的机器人化电缆组装。</li>
<li>methods: 该论文采用了系统性的文献综述方法，检索了目前关于计算机视觉在机器人化电缆组装中的应用研究。</li>
<li>results: 该论文总结了现有研究中的挑战和未来研究的机遇，以促进实用的机器人化电缆组装。<details>
<summary>Abstract</summary>
This article presents a systematic literature review on computer vision applications that have been proposed for robotized wire harness assembly, derives challenges from existing studies, and identifies opportunities for future research to promote a more practical robotized assembly of wire harnesses.
</details>
<details>
<summary>摘要</summary>
这篇文章提出了一项系统性文献复查，探讨了计算机视觉技术在机器人化电缆组装中的应用，从现有研究中提取了挑战，并标识了未来研究的机遇，以促进更实用的机器人化电缆组装。Here's a breakdown of the translation:* "这篇文章" (zhè běn wén zhāng) - This article* "提出了一项" (tí shū le yī jiāng) - Proposes a systematic review* "系统性文献复查" (xì tǒng xìng běn bǎo) - Systematic literature review* "探讨了计算机视觉技术" (tàng shuō le jì shù zhì yè jì) - Explores computer vision technology* "在机器人化电缆组装中" (zhī zhì hóu diàn zhè bù zào) - In robotized wire harness assembly* "提取了挑战" (tí qū le bào zhèng) - Identifies challenges* "并标识了未来研究的机遇" (yuè yì le wèi lǎi yán jí de jī hǎng) - And identifies opportunities for future research* "以促进更实用的机器人化电缆组装" (yǐn jí yī jì zhèng zhì de jī zhì hóu diàn zhè bù zào) - To promote more practical robotized assembly of wire harnesses.
</details></li>
</ul>
<hr>
<h2 id="Use-of-Large-Language-Models-for-Stance-Classification"><a href="#Use-of-Large-Language-Models-for-Stance-Classification" class="headerlink" title="Use of Large Language Models for Stance Classification"></a>Use of Large Language Models for Stance Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13734">http://arxiv.org/abs/2309.13734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iain J. Cruickshank, Lynnette Hui Xian Ng</li>
<li>for: 本研究旨在探讨大型自然语言模型（LLM）在立场分类任务中的表现，以减少人工标注的使用。</li>
<li>methods: 我们使用四种不同的提问方案与LLM进行比较，以确定它们在不同的数据集中的精度。</li>
<li>results: 我们发现，虽然LLM可以与指导模型匹配或者超越它们的结果，但全局的精度并不是准确的。这表明LLM在立场分类方面还有一定的改进空间。然而，通过使用LLM，我们可以实现无监督的立场检测，从而降低人工标注的需求，并拓宽语言之间的应用范围。<details>
<summary>Abstract</summary>
Stance detection, the task of predicting an author's viewpoint towards a subject of interest, has long been a focal point of research. Current stance detection methods predominantly rely on manual annotation of sentences, followed by training a supervised machine learning model. This manual annotation process, however, imposes limitations on the model's ability to fully comprehend the stances in the sentence and hampers its potential to generalize across different contexts. In this study, we investigate the use of Large Language Models (LLMs) for the task of stance classification, with an absolute minimum use of human labels. We scrutinize four distinct types of prompting schemes combined with LLMs, comparing their accuracies with manual stance determination. Our study reveals that while LLMs can match or sometimes even exceed the benchmark results in each dataset, their overall accuracy is not definitively better than what can be produced by supervised models. This suggests potential areas for improvement in the stance classification for LLMs. The application of LLMs, however, opens up promising avenues for unsupervised stance detection, thereby curtailing the need for manual collection and annotation of stances. This not only streamlines the process but also paves the way for expanding stance detection capabilities across languages. Through this paper, we shed light on the stance classification abilities of LLMs, thereby contributing valuable insights that can guide future advancements in this domain.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>>作者视点推断任务，长期是研究的焦点。当前的作者视点推断方法主要依靠手动标注句子，然后训练一个超级vised机器学习模型。然而，这个手动标注过程限制了模型对句子中作者视点的全面理解，使得其在不同上下文中的泛化能力受到限制。在本研究中，我们调查使用大型自然语言模型（LLM）进行作者视点分类任务，具有最小的人工标注使用。我们比较了四种不同的激励方案与LLMs的精度，并与手动决定作者视点的结果进行比较。我们的研究发现，虽然LLMs可以与或超过每个数据集的标准结果，但总的来说，它们的精度不是definitive更好于supervised模型。这表明了LLMs的作者视点分类方面可能存在改进的potential。不过，通过LLMs的应用，可以实现不需要手动收集和标注作者视点的不超级vised推断，这不仅简化了过程，还为推断语言的扩展开辟了道路。通过这篇论文，我们为LLMs的作者视点分类能力提供了有价值的反馈，以帮助未来在这个领域的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Arabic-Sentiment-Analysis-with-Noisy-Deep-Explainable-Model"><a href="#Arabic-Sentiment-Analysis-with-Noisy-Deep-Explainable-Model" class="headerlink" title="Arabic Sentiment Analysis with Noisy Deep Explainable Model"></a>Arabic Sentiment Analysis with Noisy Deep Explainable Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13731">http://arxiv.org/abs/2309.13731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Atabuzzaman, Md Shajalal, Maksuda Bilkis Baby, Alexander Boden</li>
<li>for: 本研究旨在提出一种可解释的情感分类框架，以解决现有的阿拉伯语情感分类模型中的黑盒问题。</li>
<li>methods: 该框架基于加入噪声层的Bi-Directional Long Short-Term Memory（BiLSTM）和Convolutional Neural Networks（CNN）-BiLSTM模型，可以解释特定预测的原因。</li>
<li>results: 实验结果表明，在公共 benchmark 阿拉伯语情感分类数据集上，加入噪声层可以改善阿拉伯语情感分类的性能，并且我们的方法比一些已知的状态作准方法表现更好。此外，引入的解释性噪声层可以使模型更透明和可负责任，有助于普及AI enabled系统。<details>
<summary>Abstract</summary>
Sentiment Analysis (SA) is an indispensable task for many real-world applications. Compared to limited resourced languages (i.e., Arabic, Bengali), most of the research on SA are conducted for high resourced languages (i.e., English, Chinese). Moreover, the reasons behind any prediction of the Arabic sentiment analysis methods exploiting advanced artificial intelligence (AI)-based approaches are like black-box - quite difficult to understand. This paper proposes an explainable sentiment classification framework for the Arabic language by introducing a noise layer on Bi-Directional Long Short-Term Memory (BiLSTM) and Convolutional Neural Networks (CNN)-BiLSTM models that overcome over-fitting problem. The proposed framework can explain specific predictions by training a local surrogate explainable model to understand why a particular sentiment (positive or negative) is being predicted. We carried out experiments on public benchmark Arabic SA datasets. The results concluded that adding noise layers improves the performance in sentiment analysis for the Arabic language by reducing overfitting and our method outperformed some known state-of-the-art methods. In addition, the introduced explainability with noise layer could make the model more transparent and accountable and hence help adopting AI-enabled system in practice.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-using-Cough-for-Respiratory-Disease-Diagnosis-by-leveraging-Artificial-Intelligence-A-Survey"><a href="#Towards-using-Cough-for-Respiratory-Disease-Diagnosis-by-leveraging-Artificial-Intelligence-A-Survey" class="headerlink" title="Towards using Cough for Respiratory Disease Diagnosis by leveraging Artificial Intelligence: A Survey"></a>Towards using Cough for Respiratory Disease Diagnosis by leveraging Artificial Intelligence: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14383">http://arxiv.org/abs/2309.14383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneeqa Ijaz, Muhammad Nabeel, Usama Masood, Tahir Mahmood, Mydah Sajid Hashmi, Iryna Posokhova, Ali Rizwan, Ali Imran</li>
<li>For: The paper is written for medical experts and AI scientists to analyze the decisive role of AI&#x2F;ML in detecting and diagnosing respiratory diseases based on cough acoustics.* Methods: The paper uses a comprehensive review of the literature on cough-based AI algorithms to demonstrate the significance of AI&#x2F;ML in detecting the onset of specific respiratory diseases. The authors also investigate the mechanism of cough and the latent cough features of respiratory modalities, and analyze customized cough monitoring applications and their AI-powered recognition algorithms.* Results: The paper provides a detailed list of significant features for cough data-driven ML&#x2F;DL detection and preliminary diagnosis frameworks, and discusses challenges and future research directions to develop practical, robust, and ubiquitous solutions for respiratory disease prediction.Here is the format you requested:* For: 论文是为医疗专家和人工智能科学家分析AI&#x2F;ML在抑制呼吸疾病中的重要作用。* Methods: 论文使用综述文献来展示呼吸学AI算法在诊断特定呼吸疾病的开头的重要性。文章还研究呼吸机制和呼吸模式的潜在特征，以及个性化呼吸监测应用程序和其AI驱动的识别算法。* Results: 论文提供了呼吸数据驱动ML&#x2F;DL检测和初步诊断框架中的重要特征列表，并讨论了实用、 Robust、和通用解决方案的挑战和未来研究方向。<details>
<summary>Abstract</summary>
Cough acoustics contain multitudes of vital information about pathomorphological alterations in the respiratory system. Reliable and accurate detection of cough events by investigating the underlying cough latent features and disease diagnosis can play an indispensable role in revitalizing the healthcare practices. The recent application of Artificial Intelligence (AI) and advances of ubiquitous computing for respiratory disease prediction has created an auspicious trend and myriad of future possibilities in the medical domain. In particular, there is an expeditiously emerging trend of Machine learning (ML) and Deep Learning (DL)-based diagnostic algorithms exploiting cough signatures. The enormous body of literature on cough-based AI algorithms demonstrate that these models can play a significant role for detecting the onset of a specific respiratory disease. However, it is pertinent to collect the information from all relevant studies in an exhaustive manner for the medical experts and AI scientists to analyze the decisive role of AI/ML. This survey offers a comprehensive overview of the cough data-driven ML/DL detection and preliminary diagnosis frameworks, along with a detailed list of significant features. We investigate the mechanism that causes cough and the latent cough features of the respiratory modalities. We also analyze the customized cough monitoring application, and their AI-powered recognition algorithms. Challenges and prospective future research directions to develop practical, robust, and ubiquitous solutions are also discussed in detail.
</details>
<details>
<summary>摘要</summary>
咳嗽学包含多种重要信息，可以帮助诊断呼吸系统的疾病变化。通过检测咳嗽特征来进行精准的疾病诊断，可以在医疗实践中发挥关键作用。现在，人工智能（AI）和 ubique computing 在呼吸疾病预测方面的应用正在迅速发展，这在医学领域创造了一种潜在的未来可能性。尤其是在机器学习（ML）和深度学习（DL）方面，已经出现了一种以咳嗽特征为基础的诊断算法的迅速增长趋势。但是，为了全面了解这些研究的结果，需要对所有相关的研究进行总结，以便医学专家和 AI 科学家进行分析。本调查概述了基于咳嗽数据的 ML/DL 检测和先期诊断框架，以及相关的重要特征。我们研究咳嗽的机制和呼吸Modalities 中的潜在特征。我们还分析了自定义咳嗽监测应用程序，以及它们的 AI 驱动的识别算法。挑战和未来研究方向也在详细地讨论。
</details></li>
</ul>
<hr>
<h2 id="Agree-To-Disagree"><a href="#Agree-To-Disagree" class="headerlink" title="Agree To Disagree"></a>Agree To Disagree</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14382">http://arxiv.org/abs/2309.14382</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mpagli/Agree-to-Disagree">https://github.com/mpagli/Agree-to-Disagree</a></li>
<li>paper_authors: Abhinav Raghuvanshi, Siddhesh Pawar, Anirudh Mittal</li>
<li>for: 这篇论文是为了提供一种自动解析和概括长文档中重要信息的机器学习方法。</li>
<li>methods: 该方法使用机器学习算法对长文档进行自动解析和概括，以提供用户友好的摘要。</li>
<li>results: 该方法可以帮助用户快速理解长文档中的重要信息，从而减少用户对各种服务协议和软件使用协议的审核时间。<details>
<summary>Abstract</summary>
How frequently do individuals thoroughly review terms and conditions before proceeding to register for a service, install software, or access a website? The majority of internet users do not engage in this practice. This trend is not surprising, given that terms and conditions typically consist of lengthy documents replete with intricate legal terminology and convoluted sentences. In this paper, we introduce a Machine Learning-powered approach designed to automatically parse and summarize critical information in a user-friendly manner. This technology focuses on distilling the pertinent details that users should contemplate before committing to an agreement.
</details>
<details>
<summary>摘要</summary>
有多少人在注册服务、安装软件或访问网站之前， thorougly review terms and conditions？大多数互联网用户不这样做。这种趋势并不奇怪，因为条款和条件通常是长长的文档，拥有复杂的法律术语和句子结构。在这篇论文中，我们介绍了一种基于机器学习的方法，可以自动解析和概括重要信息，以便用户在决定时更好地了解。这种技术将关键信息简化，以便用户更好地理解。
</details></li>
</ul>
<hr>
<h2 id="ORLA-Mobile-Manipulator-Based-Object-Rearrangement-with-Lazy-A"><a href="#ORLA-Mobile-Manipulator-Based-Object-Rearrangement-with-Lazy-A" class="headerlink" title="ORLA*: Mobile Manipulator-Based Object Rearrangement with Lazy A*"></a>ORLA*: Mobile Manipulator-Based Object Rearrangement with Lazy A*</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13707">http://arxiv.org/abs/2309.13707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gaokai15/ORLA-Star">https://github.com/gaokai15/ORLA-Star</a></li>
<li>paper_authors: Kai Gao, Yan Ding, Shiqi Zhang, Jingjin Yu</li>
<li>for: 这个论文主要针对的是移动搅拌器（如搅拌桌或吃卤桌）中的物体重新排序问题，即如何选择合适的物体重新排序策略以实现最佳的物体重新排序结果。</li>
<li>methods: 该论文提出了一种名为ORLA<em>的算法，该算法利用延迟评估（lazy evaluation）技术，搜索一个高质量的物体捕获和放置顺序，考虑了机器人手部和机器人基础的运动。同时，ORLA</em>还支持多层次重新排序任务，使用机器学习来保证物体堆积稳定。</li>
<li>results: 通过对大量的 simulate和减少研究，authors confirm了ORLA*的效果，能够提供高质量的重新排序解决方案，并且可以达到全球最佳性。<details>
<summary>Abstract</summary>
Effectively performing object rearrangement is an essential skill for mobile manipulators, e.g., setting up a dinner table or organizing a desk. A key challenge in such problems is deciding an appropriate manipulation order for objects to effectively untangle dependencies between objects while considering the necessary motions for realizing the manipulations (e.g., pick and place). To our knowledge, computing time-optimal multi-object rearrangement solutions for mobile manipulators remains a largely untapped research direction. In this research, we propose ORLA*, which leverages delayed (lazy) evaluation in searching for a high-quality object pick and place sequence that considers both end-effector and mobile robot base travel. ORLA* also supports multi-layered rearrangement tasks considering pile stability using machine learning. Employing an optimal solver for finding temporary locations for displacing objects, ORLA* can achieve global optimality. Through extensive simulation and ablation study, we confirm the effectiveness of ORLA* delivering quality solutions for challenging rearrangement instances. Supplementary materials are available at: https://gaokai15.github.io/ORLA-Star/
</details>
<details>
<summary>摘要</summary>
通过有效地重新排序物品，移动抓取机器人可以具备更高效的操作能力，例如设置晚餐桌或整理办公桌面。一个主要挑战在这些问题中是决定合适的物品重新排序顺序，以便有效地解决物品之间的依赖关系，同时考虑必要的动作（如找取和放置）。根据我们所知，计算时间最优的多物品重新排序解决方案仍然是移动抓取机器人研究的一个未探讨的方向。在这个研究中，我们提出了ORLA*，它利用延迟（懒散）评估来搜索高质量的物品找取和放置顺序，考虑了执行器和移动机器人基础体的必要运动。ORLA*还支持多层次重新排序任务，使用机器学习来考虑积累稳定性。通过优质的临时解决方案找取物品的位置，ORLA*可以 дости到全球优化。通过广泛的 simulations和减少研究，我们证明了ORLA*在具有挑战性的重新排序任务中的效果。补充材料可以在以下链接中找到：https://gaokai15.github.io/ORLA-Star/
</details></li>
</ul>
<hr>
<h2 id="A-Neural-Guided-Dynamic-Symbolic-Network-for-Exploring-Mathematical-Expressions-from-Data"><a href="#A-Neural-Guided-Dynamic-Symbolic-Network-for-Exploring-Mathematical-Expressions-from-Data" class="headerlink" title="A Neural-Guided Dynamic Symbolic Network for Exploring Mathematical Expressions from Data"></a>A Neural-Guided Dynamic Symbolic Network for Exploring Mathematical Expressions from Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13705">http://arxiv.org/abs/2309.13705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqiang Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Yanjie Li</li>
<li>for: 本研究的目的是提出一种新的神经网络引导的动态符号网络方法（DySymNet），用于实现数据探索的符号回归问题。</li>
<li>methods: 本方法使用一种新的网络结构，并通过优化这些结构来找到更适合数据的表达。这种方法不仅能够处理高维问题，还能够优化常数。</li>
<li>results: 根据广泛的数值实验表示，DySymNet方法可以达到现有方法的最佳性能水平，并且在噪音较高的情况下保持稳定性。<details>
<summary>Abstract</summary>
Symbolic regression (SR) is a powerful technique for discovering the underlying mathematical expressions from observed data. Inspired by the success of deep learning, recent efforts have focused on two categories for SR methods. One is using a neural network or genetic programming to search the expression tree directly. Although this has shown promising results, the large search space poses difficulties in learning constant factors and processing high-dimensional problems. Another approach is leveraging a transformer-based model training on synthetic data and offers advantages in inference speed. However, this method is limited to fixed small numbers of dimensions and may encounter inference problems when given data is out-of-distribution compared to the synthetic data. In this work, we propose DySymNet, a novel neural-guided Dynamic Symbolic Network for SR. Instead of searching for expressions within a large search space, we explore DySymNet with various structures and optimize them to identify expressions that better-fitting the data. With a topology structure like neural networks, DySymNet not only tackles the challenge of high-dimensional problems but also proves effective in optimizing constants. Based on extensive numerical experiments using low-dimensional public standard benchmarks and the well-known SRBench with more variables, our method achieves state-of-the-art performance in terms of fitting accuracy and robustness to noise.
</details>
<details>
<summary>摘要</summary>
Symbolic regression (SR) 是一种强大的技术，用于从观察数据中发现下面的数学表达。随着深度学习的成功， latest efforts have focused on two categories of SR methods. One is to use a neural network or genetic programming to search the expression tree directly. Although this has shown promising results, the large search space poses difficulties in learning constant factors and processing high-dimensional problems. Another approach is to leverage a transformer-based model training on synthetic data, which offers advantages in inference speed. However, this method is limited to fixed small numbers of dimensions and may encounter inference problems when given data is out-of-distribution compared to the synthetic data.在这个工作中，我们提出了 DySymNet，一种新的神经网络引导的动态 симвоlic Network for SR. Instead of searching for expressions within a large search space, we explore DySymNet with various structures and optimize them to identify expressions that better-fitting the data. With a topology structure like neural networks, DySymNet not only tackles the challenge of high-dimensional problems but also proves effective in optimizing constants. Based on extensive numerical experiments using low-dimensional public standard benchmarks and the well-known SRBench with more variables, our method achieves state-of-the-art performance in terms of fitting accuracy and robustness to noise.
</details></li>
</ul>
<hr>
<h2 id="Skill-Check-Some-Considerations-on-the-Evaluation-of-Gamemastering-Models-for-Role-playing-Games"><a href="#Skill-Check-Some-Considerations-on-the-Evaluation-of-Gamemastering-Models-for-Role-playing-Games" class="headerlink" title="Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games"></a>Skill Check: Some Considerations on the Evaluation of Gamemastering Models for Role-playing Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13702">http://arxiv.org/abs/2309.13702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sgongora27/skill-check-gm-tests">https://github.com/sgongora27/skill-check-gm-tests</a></li>
<li>paper_authors: Santiago Góngora, Luis Chiruzzo, Gonzalo Méndez, Pablo Gervás</li>
<li>for: 这篇论文是关于用Interactive Storytelling和自然语言处理方法模型游戏主持人（GM）的。</li>
<li>methods: 这篇论文使用了三个测试类划分来评估这些对话系统，并用它们测试了ChatGPT、Bard和OpenAssistant三个简单的GM。</li>
<li>results: 根据测试结果，这三个对话系统在不同的情况下都能够表现出不同的能力和缺点。<details>
<summary>Abstract</summary>
In role-playing games a Game Master (GM) is the player in charge of the game, who must design the challenges the players face and narrate the outcomes of their actions. In this work we discuss some challenges to model GMs from an Interactive Storytelling and Natural Language Processing perspective. Following those challenges we propose three test categories to evaluate such dialogue systems, and we use them to test ChatGPT, Bard and OpenAssistant as out-of-the-box GMs.
</details>
<details>
<summary>摘要</summary>
在角色扮演游戏中，游戏主持人（GM）是游戏中的主要玩家，负责设计玩家面临的挑战和描述玩家行动的结果。在这项工作中，我们讨论了对GM的模型化从互动故事与自然语言处理的角度来面临一些挑战。随后，我们提出了三个测试类别来评估这些对话系统，并使用它们测试ChatGPT、Bard和OpenAssistant作为直接GM。
</details></li>
</ul>
<hr>
<h2 id="ALLURE-Auditing-and-Improving-LLM-based-Evaluation-of-Text-using-Iterative-In-Context-Learning"><a href="#ALLURE-Auditing-and-Improving-LLM-based-Evaluation-of-Text-using-Iterative-In-Context-Learning" class="headerlink" title="ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning"></a>ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13701">http://arxiv.org/abs/2309.13701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, Ida Momennejad</li>
<li>for:  This paper aims to improve the ability of large language models (LLMs) to evaluate text by auditing and refining their performance.</li>
<li>methods: The authors introduce a systematic approach called ALLURE, which involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator. The evaluator leverages in-context learning (ICL) to enhance and improve the robust evaluation of text by LLMs.</li>
<li>results: The authors demonstrate the effectiveness of ALLURE in improving the performance of the evaluator LLM, reducing reliance on human annotators in the evaluation process. They anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and productivity.<details>
<summary>Abstract</summary>
From grading papers to summarizing medical documents, large language models (LLMs) are evermore used for evaluation of text generated by humans and AI alike. However, despite their extensive utility, LLMs exhibit distinct failure modes, necessitating a thorough audit and improvement of their text evaluation capabilities. Here we introduce ALLURE, a systematic approach to Auditing Large Language Models Understanding and Reasoning Errors. ALLURE involves comparing LLM-generated evaluations with annotated data, and iteratively incorporating instances of significant deviation into the evaluator, which leverages in-context learning (ICL) to enhance and improve robust evaluation of text by LLMs. Through this iterative process, we refine the performance of the evaluator LLM, ultimately reducing reliance on human annotators in the evaluation process. We anticipate ALLURE to serve diverse applications of LLMs in various domains related to evaluation of textual data, such as medical summarization, education, and and productivity.
</details>
<details>
<summary>摘要</summary>
从分发纸到摘要医疗文档，大型自然语言模型（LLM）在评估人类和AI生成的文本中越来越广泛使用。然而，尽管它们的应用非常广泛，LLM仍然会出现不同的失败模式，因此需要进行系统性的审核和改进。在这篇文章中，我们介绍了ALLURE，一个系统性的方法来审核和改进LLM的文本评估能力。ALLURE通过比较LLM生成的评估和标注数据进行比较，并逐步包含具有重要差异的例子进入评估器中，以利用内容学习（ICL）来提高和改进LLM评估文本的能力。透过这个迭代过程，我们可以提高评估器LLM的性能，最终减少人类标注员在评估过程中的依赖。我们预计ALLURE将能够应用于各种领域中的LLM应用，例如医疗摘要、教育和生产力。
</details></li>
</ul>
<hr>
<h2 id="Smart-OMVI-Obfuscated-Malware-Variant-Identification-using-a-novel-dataset"><a href="#Smart-OMVI-Obfuscated-Malware-Variant-Identification-using-a-novel-dataset" class="headerlink" title="Smart OMVI: Obfuscated Malware Variant Identification using a novel dataset"></a>Smart OMVI: Obfuscated Malware Variant Identification using a novel dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10670">http://arxiv.org/abs/2310.10670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Suleman Qamar</li>
<li>for: 这个论文是为了提供一个更真实和代表性的病毒分析环境，以evaluate病毒分析技术的效果。</li>
<li>methods: 这个论文使用了多种传统机器学习算法，包括但不限于支持向量机(SVM)、随机森林(RF)和极大梯度提升(XGBOOST)等。</li>
<li>results: XGBOOST算法在这些算法中表现最佳，具有82%的准确率、88%的精度、80%的回归率和83%的F1分数。<details>
<summary>Abstract</summary>
Cybersecurity has become a significant issue in the digital era as a result of the growth in everyday computer use. Cybercriminals now engage in more than virus distribution and computer hacking. Cyberwarfare has developed as a result because it has become a threat to a nation's survival. Malware analysis serves as the first line of defence against an attack and is a significant component of cybercrime. Every day, malware attacks target a large number of computer users, businesses, and governmental agencies, causing billions of dollars in losses. Malware may evade multiple AV software with a very minor, cunning tweak made by its designers, despite the fact that security experts have a variety of tools at their disposal to identify it. To address this challenge, a new dataset called the Obfuscated Malware Dataset (OMD) has been developed. This dataset comprises 40 distinct malware families having 21924 samples, and it incorporates obfuscation techniques that mimic the strategies employed by malware creators to make their malware variations different from the original samples. The purpose of this dataset is to provide a more realistic and representative environment for evaluating the effectiveness of malware analysis techniques. Different conventional machine learning algorithms including but not limited to Support Vector Machine (SVM), Random Forrest (RF), Extreme Gradient Boosting (XGBOOST) etc are applied and contrasted. The results demonstrated that XGBoost outperformed the other algorithms, achieving an accuracy of f 82%, precision of 88%, recall of 80%, and an F1-Score of 83%.
</details>
<details>
<summary>摘要</summary>
在数字时代，cybersecurity已成为一项重要的问题，归功于日常计算机使用的增长。现在，黑客不仅限于散发病毒和黑客行为，而且开发了cyberwarfare，这成为了国家存亡的威胁。针对这种挑战，一个新的数据集called the Obfuscated Malware Dataset (OMD)已经开发出来。这个数据集包含40种不同的黑客家族，共21924个样本，并包含了黑客创造者们使用的混淆技术来使其黑客变体与原始样本不同。该数据集的目的是为了提供更加现实和代表的环境，以评估黑客分析技术的效果。在这个数据集上，不同的传统机器学习算法，包括但不限于支持向量机 (SVM)、Random Forrest (RF) 和极限梯度提升 (XGBOOST) 等，被应用并比较。结果表明，XGBOOST在这些算法中表现出了最高的效果，具有82%的准确率、88%的精度、80%的回归率和83%的F1得分。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Image-to-Image-Translation"><a href="#Deep-Reinforcement-Learning-for-Image-to-Image-Translation" class="headerlink" title="Deep Reinforcement Learning for Image-to-Image Translation"></a>Deep Reinforcement Learning for Image-to-Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13672">http://arxiv.org/abs/2309.13672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Algolzw/SPAC-Deformable-Registration">https://github.com/Algolzw/SPAC-Deformable-Registration</a></li>
<li>paper_authors: Xin Wang, Ziwei Luo, Jing Hu, Chengming Feng, Shu Hu, Bin Zhu, Xi Wu, Siwei Lyu</li>
<li>for: 本研究旨在提出一种基于深度学习和强化学习的图像转换方法，以解决现有的图像转换方法在某些任务上存在困难和过拟合的问题。</li>
<li>methods: 本研究使用了深度学习和强化学习的方法，特别是在一个步骤基础上，通过简单的决策进程来逐步转换源图像到目标图像。此外，本研究还提出了一种新的元策略，可以在标准的actor-critic模型中处理高维连续状态和动作空间，并且可以使得actor生成更加可追踪的高维动作。</li>
<li>results: 实验结果表明，提出的RL-I2IT方法在面临高维连续动作空间问题时表现高效和稳定，并且可以在多个图像转换任务上达到高度的性能。<details>
<summary>Abstract</summary>
Most existing Image-to-Image Translation (I2IT) methods generate images in a single run of a deep learning (DL) model. However, designing such a single-step model is always challenging, requiring a huge number of parameters and easily falling into bad global minimums and overfitting. In this work, we reformulate I2IT as a step-wise decision-making problem via deep reinforcement learning (DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The key feature in the RL-I2IT framework is to decompose a monolithic learning process into small steps with a lightweight model to progressively transform a source image successively to a target image. Considering that it is challenging to handle high dimensional continuous state and action spaces in the conventional RL framework, we introduce meta policy with a new concept Plan to the standard Actor-Critic model, which is of a lower dimension than the original image and can facilitate the actor to generate a tractable high dimensional action. In the RL-I2IT framework, we also employ a task-specific auxiliary learning strategy to stabilize the training process and improve the performance of the corresponding task. Experiments on several I2IT tasks demonstrate the effectiveness and robustness of the proposed method when facing high-dimensional continuous action space problems.
</details>
<details>
<summary>摘要</summary>
大多数现有的图像到图像翻译（I2IT）方法都是通过深度学习（DL）模型在单次训练中生成图像。然而，设计这种单步模型总是困难，需要很多参数，容易落入坏的全局最优点和过拟合。在这种工作中，我们将I2IT重新划为一个步骤性决策问题，并提出了一个新的框架——RL-I2IT。RL-I2IT框架的关键特征在于将绘制学习过程中的庞大学习过程拆分成小步骤，使用轻量级模型逐步将源图像转换成目标图像。由于传统RL框架中高维连续状态和动作空间的处理是困难的，我们引入了一种新的概念——计划，并将其添加到标准actor-critic模型中。在RL-I2IT框架中，我们还使用了一种任务特有的辅助学习策略，以稳定训练过程并提高相应任务的性能。在几个I2IT任务上进行了实验，我们发现提议的方法在面临高维连续动作空间问题时表现得非常有效和稳定。
</details></li>
</ul>
<hr>
<h2 id="Survey-of-Social-Bias-in-Vision-Language-Models"><a href="#Survey-of-Social-Bias-in-Vision-Language-Models" class="headerlink" title="Survey of Social Bias in Vision-Language Models"></a>Survey of Social Bias in Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14381">http://arxiv.org/abs/2309.14381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, Pascale Fung</li>
<li>for: 这篇论文旨在探讨预训练模型中存在的社会偏见问题，以及如何在多模态场景中减少这些偏见。</li>
<li>methods: 该论文采用了文献综述的方法，检查了不同领域中预训练模型中的社会偏见问题，并提出了一些应对方法。</li>
<li>results: 该论文发现了预训练模型在不同领域中的社会偏见问题，并提出了一些可能的解决方案，以帮助研究人员在多模态场景中开发更公正的人工智能模型。<details>
<summary>Abstract</summary>
In recent years, the rapid advancement of machine learning (ML) models, particularly transformer-based pre-trained models, has revolutionized Natural Language Processing (NLP) and Computer Vision (CV) fields. However, researchers have discovered that these models can inadvertently capture and reinforce social biases present in their training datasets, leading to potential social harms, such as uneven resource allocation and unfair representation of specific social groups. Addressing these biases and ensuring fairness in artificial intelligence (AI) systems has become a critical concern in the ML community.   The recent introduction of pre-trained vision-and-language (VL) models in the emerging multimodal field demands attention to the potential social biases present in these models as well. Although VL models are susceptible to social bias, there is a limited understanding compared to the extensive discussions on bias in NLP and CV. This survey aims to provide researchers with a high-level insight into the similarities and differences of social bias studies in pre-trained models across NLP, CV, and VL. By examining these perspectives, the survey aims to offer valuable guidelines on how to approach and mitigate social bias in both unimodal and multimodal settings. The findings and recommendations presented here can benefit the ML community, fostering the development of fairer and non-biased AI models in various applications and research endeavors.
</details>
<details>
<summary>摘要</summary>
近年来，机器学习（ML）模型的快速发展，特别是基于转换器的预训练模型，对自然语言处理（NLP）和计算机视觉（CV）领域产生了革命性的变革。然而，研究人员发现，这些模型可能会不意imento capture和激发社会偏见，从而导致社会不公正和特定社会群体的不公正代表。解决这些偏见并确保人工智能（AI）系统的公正性已成为ML社区的关键问题。随着emerging multimodal领域中的视觉语言（VL）模型的出现，需要对这些模型中的社会偏见进行关注。虽然VL模型受到社会偏见的影响，但相比NLP和CV领域，对于VL模型的社会偏见还有很 limited的理解。本调查旨在为研究人员提供高级别的社会偏见研究在预训练模型中的类似和不同之处，以及NLP、CV和VL领域中社会偏见的研究方法和措施。通过对这些观点进行分析，本调查期望为ML社区提供有价值的指南，以帮助开发更公正、不偏见的AI模型，并在不同的应用和研究领域中做出贡献。
</details></li>
</ul>
<hr>
<h2 id="VoiceLDM-Text-to-Speech-with-Environmental-Context"><a href="#VoiceLDM-Text-to-Speech-with-Environmental-Context" class="headerlink" title="VoiceLDM: Text-to-Speech with Environmental Context"></a>VoiceLDM: Text-to-Speech with Environmental Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13664">http://arxiv.org/abs/2309.13664</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/glory20h/VoiceLDM">https://github.com/glory20h/VoiceLDM</a></li>
<li>paper_authors: Yeonghyeon Lee, Inmo Yeon, Juhan Nam, Joon Son Chung</li>
<li>for: 这个论文旨在生成准确地遵循两个自然语言文本提示：描述提示和内容提示。描述提示提供环境上下文信息，而内容提示则传达语言内容。</li>
<li>methods: 作者采用基于潜在扩散模型的文本到音频（TTA）模型，并将其扩展以接受额外的内容提示作为条件输入。通过使用预训练的对比语言-音频预训练（CLAP）和Whisper，作者在大量实际音频数据上进行了训练。此外，作者还使用了无束分类器自由指导来进一步提高VoiceLDM的可控性。</li>
<li>results: 实验结果表明，VoiceLDM可以生成准确地遵循两个输入条件的音频，甚至在AudioCaps测试集上超越原始音频的语音可解度。此外，作者还探索了TTS和零shot TTA的能力，并证明VoiceLDM可以达到竞争力的结果。<details>
<summary>Abstract</summary>
This paper presents VoiceLDM, a model designed to produce audio that accurately follows two distinct natural language text prompts: the description prompt and the content prompt. The former provides information about the overall environmental context of the audio, while the latter conveys the linguistic content. To achieve this, we adopt a text-to-audio (TTA) model based on latent diffusion models and extend its functionality to incorporate an additional content prompt as a conditional input. By utilizing pretrained contrastive language-audio pretraining (CLAP) and Whisper, VoiceLDM is trained on large amounts of real-world audio without manual annotations or transcriptions. Additionally, we employ dual classifier-free guidance to further enhance the controllability of VoiceLDM. Experimental results demonstrate that VoiceLDM is capable of generating plausible audio that aligns well with both input conditions, even surpassing the speech intelligibility of the ground truth audio on the AudioCaps test set. Furthermore, we explore the text-to-speech (TTS) and zero-shot text-to-audio capabilities of VoiceLDM and show that it achieves competitive results. Demos and code are available at https://voiceldm.github.io.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Machine-assisted-mixed-methods-augmenting-humanities-and-social-sciences-with-artificial-intelligence"><a href="#Machine-assisted-mixed-methods-augmenting-humanities-and-social-sciences-with-artificial-intelligence" class="headerlink" title="Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence"></a>Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14379">http://arxiv.org/abs/2309.14379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreskarjus/machineassistedmixedmethods">https://github.com/andreskarjus/machineassistedmixedmethods</a></li>
<li>paper_authors: Andres Karjus</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）在人文社科领域中的应用潜力，以帮助论文作者在数据分析 tasks 上增强和自动化人工劳动。</li>
<li>methods: 该论文提出了一种系统的混合方法 Framework，包括机器可观测和人类专家的协同分析、数据量化和可重复性的考虑。16个机器助手实践案例被用作证明。</li>
<li>results: 该论文的结果表明，在大多数情况下，LLM可以成功地执行许多质量分析任务，包括语言和дискурス分析、 lexical semantic change detection、采访分析、历史事件 causa inference 和文本挖掘、政治立场探测、文本和想法再利用、文学和电影类型组合、社交网络推理和自动 lexicography。此外，论文还发现，在使用LLM时，需要考虑人类专家的知识和经验，以确保结果的准确性和可靠性。<details>
<summary>Abstract</summary>
The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution proposes a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. 16 machine-assisted case studies are showcased as proof of concept. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference and text mining, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. In contrast to the focus on English in the emerging LLM applicability literature, many examples here deal with scenarios involving smaller languages and historical texts prone to digitization distortions. In all but the most difficult tasks requiring expert knowledge, generative LLMs can demonstrably serve as viable research instruments. LLM (and human) annotations may contain errors and variation, but the agreement rate can and should be accounted for in subsequent statistical modeling; a bootstrapping approach is discussed. The replications among the case studies illustrate how tasks previously requiring potentially months of team effort and complex computational pipelines, can now be accomplished by an LLM-assisted scholar in a fraction of the time. Importantly, this approach is not intended to replace, but to augment researcher knowledge and skills. With these opportunities in sight, qualitative expertise and the ability to pose insightful questions have arguably never been more critical.
</details>
<details>
<summary>摘要</summary>
LLMS 的增长 capacities 提供了无 precedent 的机会，以扩大人文社科领域的数据分析，通过机器执行和人工协助，自动化和加强质量分析任务，提高研究效率和准确性。本贡献提出了一种系统性的混合方法框架，结合人类专家知识和机器可扩展性，并强调透明度和复制性。这些案例中的 16 个机器助手案例作为证明。任务包括语言和 Diskourse 分析、lexical  semantics 变化检测、采访分析、历史事件 causality 推断和文本挖掘、政治立场推断、文本和意义 reuse、文学和电影种类作品 genre 组合、社交网络推断、自动 lexicography、缺失 metadata 扩充和多媒体视觉文化分析。与英语emerging LLMS 应用性文献中的焦点不同，这些例子中的大多数例子 involve 小语言和历史文献，这些文献可能会受到数字化改变的影响。除了最复杂的任务需要专家知识外，LLM 可以成功地服务为可靠的研究工具。LLM 和人类注解可能会包含错误和变化，但协调率可以并被考虑在后续统计模型中。这种方法不是替换研究者知识和技能，而是增强它们。这些机遇在视野中，专业知识和能够提出有价值的问题的能力 arguably  nunca 这样重要。
</details></li>
</ul>
<hr>
<h2 id="Embers-of-Autoregression-Understanding-Large-Language-Models-Through-the-Problem-They-are-Trained-to-Solve"><a href="#Embers-of-Autoregression-Understanding-Large-Language-Models-Through-the-Problem-They-are-Trained-to-Solve" class="headerlink" title="Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve"></a>Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13638">http://arxiv.org/abs/2309.13638</a></li>
<li>repo_url: None</li>
<li>paper_authors: R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, Thomas L. Griffiths</li>
<li>for: 本研究旨在理解大语言模型（LLM）的优劣点，并推广其应用。</li>
<li>methods: 本研究使用了teleological approach，即认为LLM在解决下一个单词预测任务时的压力，并预测LLM会采取什么策略。</li>
<li>results: 研究发现，LLM的准确率受任务执行概率、目标输出概率和输入提供概率的影响。在 deterministic 环境中，LLM 的准确率高于低概率情况下。此外，研究还发现了一些奇异的失败模式，如 GPT-4 在解码简单密码时的准确率为 51%，但只有 13% 在低概率情况下。这些结果表明，AI 专家应该在低概率情况下使用 LLB 时需要谨慎。<details>
<summary>Abstract</summary>
The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that in order to develop a holistic understanding of these systems we need to consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. This approach - which we call the teleological approach - leads us to identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. We predict that LLMs will achieve higher accuracy when these probabilities are high than when they are low - even in deterministic settings where probability should not matter. To test our predictions, we evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes. For instance, GPT-4's accuracy at decoding a simple cipher is 51% when the output is a high-probability word sequence but only 13% when it is low-probability. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system - one that has been shaped by its own particular set of pressures.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLM）的广泛采用，我们必须认可它们的优势和局限性。我们认为，为了发展它们的整体理解，我们需要考虑它们被训练的问题：以互联网文本为基础的下一个词预测。通过认真对待这些任务的压力，我们可以预测LLM会采取什么策略，从而对它们的成功和失败进行预测。我们称这种方法为“teleological approach”。我们认为，LLM的准确率受以下三个因素的影响：任务执行概率、目标输出概率和输入提供的概率。我们预测，当这些概率高时，LLM的准确率也将高；而当它们低时，准确率则将低，即使在deterministic Setting中，概率应该没有影响。为测试我们的预测，我们评估了两个LLM（GPT-3.5和GPT-4）在11个任务上的表现，并发现了robust的证据，证明了我们的假设。在许多情况下，实验发现了意外的失败模式。例如，GPT-4在解码简单密码的任务中的准确率为51%，但只有13% когда输出是低概率的word sequence。这些结果表明，AI实践者应该小心使用LLM在低概率情况下。更广泛地说，我们 concludeThat we should not evaluate LLMs as if they were humans, but rather as a distinct type of system that has been shaped by its own unique set of pressures.
</details></li>
</ul>
<hr>
<h2 id="Development-of-an-intelligent-system-for-the-detection-of-corona-virus-using-artificial-neural-network"><a href="#Development-of-an-intelligent-system-for-the-detection-of-corona-virus-using-artificial-neural-network" class="headerlink" title="Development of an intelligent system for the detection of corona virus using artificial neural network"></a>Development of an intelligent system for the detection of corona virus using artificial neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13636">http://arxiv.org/abs/2309.13636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nwafor Emmanuel O, Ngozi Maryrose Umeh, Ikechukwu Ekene Onyenwe</li>
<li>for: 本研究目的是开发一个人工神经网络检测新冠肺炎的智能系统。</li>
<li>methods: 本研究使用了文献综述和683组高烧 Body temperature数据（&gt;&#x3D; 38℃），从尼日利亚埃努古大学医院搜集到，用于训练人工神经网络检测模型。</li>
<li>results: 模型的评估结果显示，混淆矩阵、回归和方差平方误差（MSE）都是0.967，准确率是97%，这些结果显示新检测系统是可靠且高效。<details>
<summary>Abstract</summary>
This paper presents the development of an intelligent system for the detection of coronavirus using artificial neural network. This was done after series of literature review which indicated that high fever accounts for 87.9% of the COVID-19 symptoms. 683 temperature data of COVID-19 patients at >= 38C^o were collected from Colliery hospital Enugu, Nigeria and used to train an artificial neural network detective model for the detection of COVID-19. The reference model generated was used converted into Verilog codes using Hardware Description Language (HDL) and then burn into a Field Programming Gate Array (FPGA) controller using FPGA tool in Matlab. The performance of the model when evaluated using confusion matrix, regression and means square error (MSE) showed that the regression value is 0.967; the accuracy is 97% and then MSE is 0.00100Mu. These results all implied that the new detection system for is reliable and very effective for the detection of COVID-19.
</details>
<details>
<summary>摘要</summary>
本文介绍了一种人工神经网络系统的开发，用于检测新型冠状病毒（COVID-19）。这种系统是基于文献评审结果，表明高热会质量上占87.9%的COVID-19症状。我们收集了来自尼日利亚埃努古采矿医院的683例COVID-19患者体温大于或等于38℃的数据，并使用人工神经网络探测模型进行训练。模型生成的参考模型被转化为Verilog代码使用硬件描述语言（HDL），然后使用MATLAB中的FPGA工具烧录到场程控制器中。模型的性能测试结果表明，准确率为97%，回归值为0.967，平均方差为0.00100Mu。这些结果表明新检测系统具有可靠性和高效性，适用于COVID-19检测。
</details></li>
</ul>
<hr>
<h2 id="PanopticNDT-Efficient-and-Robust-Panoptic-Mapping"><a href="#PanopticNDT-Efficient-and-Robust-Panoptic-Mapping" class="headerlink" title="PanopticNDT: Efficient and Robust Panoptic Mapping"></a>PanopticNDT: Efficient and Robust Panoptic Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13635">http://arxiv.org/abs/2309.13635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tui-nicr/panoptic-mapping">https://github.com/tui-nicr/panoptic-mapping</a></li>
<li>paper_authors: Daniel Seichter, Benedict Stephan, Söhnke Benedikt Fischedick, Steffen Müller, Leonard Rabes, Horst-Michael Gross</li>
<li>for: 本研究旨在提供高精度3D精细地图，以便移动机器人在室内环境中自动操作。</li>
<li>methods: 本文提出了一种基于占用normal distribution transform（NDT）地图的有效和可靠的精细地图方法，名为PanopticNDT。</li>
<li>results: 对于公共可用的Hypersim和ScanNetV2数据集，our approach可以在移动机器人上实现高级别的精细地图，并且在实时精细地图中表达精细信息。此外，我们还证明了PanopticNDT在实际应用中的可行性。<details>
<summary>Abstract</summary>
As the application scenarios of mobile robots are getting more complex and challenging, scene understanding becomes increasingly crucial. A mobile robot that is supposed to operate autonomously in indoor environments must have precise knowledge about what objects are present, where they are, what their spatial extent is, and how they can be reached; i.e., information about free space is also crucial. Panoptic mapping is a powerful instrument providing such information. However, building 3D panoptic maps with high spatial resolution is challenging on mobile robots, given their limited computing capabilities. In this paper, we propose PanopticNDT - an efficient and robust panoptic mapping approach based on occupancy normal distribution transform (NDT) mapping. We evaluate our approach on the publicly available datasets Hypersim and ScanNetV2. The results reveal that our approach can represent panoptic information at a higher level of detail than other state-of-the-art approaches while enabling real-time panoptic mapping on mobile robots. Finally, we prove the real-world applicability of PanopticNDT with qualitative results in a domestic application.
</details>
<details>
<summary>摘要</summary>
Note:* "application scenarios" is translated as "应用场景" (yìng yìng jīng xìng)* "mobile robots" is translated as "移动机器人" (í mouth jī hū rén)* "scene understanding" is translated as "场景理解" (chǎng jǐng lǐ jiě)* "panoptic mapping" is translated as "批量地图" (pīn liàng dì tú)* "occupancy normal distribution transform" is translated as "占据正态分布变换" (zhāng yù zhèng tài fāng zhāng biàn huà)* "real-time panoptic mapping" is translated as "实时批量地图" (shí shí pīn liàng dì tú)* "domestic application" is translated as "家庭应用" (jiā tíng yìng yòu)
</details></li>
</ul>
<hr>
<h2 id="EvalLM-Interactive-Evaluation-of-Large-Language-Model-Prompts-on-User-Defined-Criteria"><a href="#EvalLM-Interactive-Evaluation-of-Large-Language-Model-Prompts-on-User-Defined-Criteria" class="headerlink" title="EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria"></a>EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13633">http://arxiv.org/abs/2309.13633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim</li>
<li>for: 本研究旨在帮助开发人员通过使用大语言模型（LLM）创造新的生成应用程序，并通过多次修改提示来优化这些应用程序。</li>
<li>methods: 本研究使用了大语言模型（LLM）来评估提示的多个输出，以 помочь开发人员评估Context-specific和主观标准。</li>
<li>results: 对比手动评估，使用EvalLM系统可以帮助开发人员更快速地COMPOSE更多样化的提示，并且需要59% fewer revisions来达到满意的提示。<details>
<summary>Abstract</summary>
By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system's LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond prompts, our work can be extended to augment model evaluation and alignment in specific application contexts.
</details>
<details>
<summary>摘要</summary>
通过简单地编写提示，开发者可以快速探索新的生成应用程序，使用大型自然语言模型（LLM）。但是，要将原型转化为产品，开发者需要不断修改提示，以评估输出的弱点。我们的研究发现，开发者在评估输出时投入了大量的时间和劳动，以评估Context-specific和主观的标准。我们提出了EvalLM，一个互动式系统，可以通过用户定义的标准来评估多个输出，并提供LLM-based评估器的反馈。通过自然语言描述标准，用户可以使用系统来评估输出的excel和不足，并根据评估器的反馈进行改进。我们的比较研究显示，EvalLM，相比于手动评估，帮助参与者编写更多样的标准，评估twice as many outputs，并在59% fewer revisions中得到满意的提示。此外，我们的工作可以扩展到增强特定应用场景中的模型评估和对齐。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-channel-EEG-Data-Analysis-for-Poor-Neuro-prognostication-in-Comatose-Patients-with-Self-and-Cross-channel-Attention-Mechanism"><a href="#A-Multi-channel-EEG-Data-Analysis-for-Poor-Neuro-prognostication-in-Comatose-Patients-with-Self-and-Cross-channel-Attention-Mechanism" class="headerlink" title="A Multi-channel EEG Data Analysis for Poor Neuro-prognostication in Comatose Patients with Self and Cross-channel Attention Mechanism"></a>A Multi-channel EEG Data Analysis for Poor Neuro-prognostication in Comatose Patients with Self and Cross-channel Attention Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03756">http://arxiv.org/abs/2310.03756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hemin Ali Qadir, Naimahmed Nesaragi, Per Steiner Halvorsen, Ilangko Balasingham</li>
<li>for: 这个研究旨在利用双极电enzephalogram（EEG）记录来有效预测中枢神经系统疾病的不良结果。</li>
<li>methods: 该研究采用了混合深度学习方法，包括特征编码器、学习位编码、 context网络、注意机制和回归和分类块，以优化一个目标函数，即高特异性（true positive rate，TPR）和降低假阳性（&lt;0.05）。</li>
<li>results: 该研究的提出的框架，OUS IVS，在隐藏验证数据上验证后，得分为0.57。<details>
<summary>Abstract</summary>
This work investigates the predictive potential of bipolar electroencephalogram (EEG) recordings towards efficient prediction of poor neurological outcomes. A retrospective design using a hybrid deep learning approach is utilized to optimize an objective function aiming for high specificity, i.e., true positive rate (TPR) with reduced false positives (< 0.05). A multi-channel EEG array of 18 bipolar channel pairs from a randomly selected 5-minute segment in an hour is kept. In order to determine the outcome prediction, a combination of a feature encoder with 1-D convolutional layers, learnable position encoding, a context network with attention mechanisms, and finally, a regressor and classifier blocks are used. The feature encoder extricates local temporal and spatial features, while the following position encoding and attention mechanisms attempt to capture global temporal dependencies. Results: The proposed framework by our team, OUS IVS, when validated on the challenge hidden validation data, exhibited a score of 0.57.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了使用双极电энце法记录（EEG）的预测潜在性，以提高不良神经学结果的预测精度。我们采用了混合深度学习方法，以优化一个目标函数，即高准确率（TPR），同时减少假阳性（<0.05）。我们使用的EEG数据包括18对双极通道，从一个随机选择的1小时内的5分钟段中选择。为了确定结果预测，我们使用了特征编码器、学习位编码、 Context网络和注意机制、以及最后的回归和分类块。特征编码器提取了本地时间和空间特征，而后续的位编码和注意机制尝试了捕捉全局时间相关性。结果：我们团队的提案方框，OUS IVS，在挑战隐藏验证数据上验证时达到了0.57分的得分。
</details></li>
</ul>
<hr>
<h2 id="GraphAdapter-Tuning-Vision-Language-Models-With-Dual-Knowledge-Graph"><a href="#GraphAdapter-Tuning-Vision-Language-Models-With-Dual-Knowledge-Graph" class="headerlink" title="GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph"></a>GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13625">http://arxiv.org/abs/2309.13625</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lixinustc/graphadapter">https://github.com/lixinustc/graphadapter</a></li>
<li>paper_authors: Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang</li>
<li>for: 提高vision-language模型（VLM）在低数据 régime下的表现，通过引入一些额外参数来挖掘任务特定的知识。</li>
<li>methods: 提出一种效果的 adapter-style tuning策略，名为GraphAdapter，它通过显式地模型两 modalities的结构知识来进一步提高文本特化器的表现。</li>
<li>results: 对11个标准 benchmark dataset进行了广泛的实验，并证明了 GraphAdapter 在前一个 adapter-based 方法之上具有显著的优势。<details>
<summary>Abstract</summary>
Adapter-style efficient transfer learning (ETL) has shown excellent performance in the tuning of vision-language models (VLMs) under the low-data regime, where only a few additional parameters are introduced to excavate the task-specific knowledge based on the general and powerful representation of VLMs. However, most adapter-style works face two limitations: (i) modeling task-specific knowledge with a single modality only; and (ii) overlooking the exploitation of the inter-class relationships in downstream tasks, thereby leading to sub-optimal solutions. To mitigate that, we propose an effective adapter-style tuning strategy, dubbed GraphAdapter, which performs the textual adapter by explicitly modeling the dual-modality structure knowledge (i.e., the correlation of different semantics/classes in textual and visual modalities) with a dual knowledge graph. In particular, the dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph, where the nodes and edges represent the semantics/classes and their correlations in two modalities, respectively. This enables the textual feature of each prompt to leverage the task-specific structure knowledge from both textual and visual modalities, yielding a more effective classifier for downstream tasks. Extensive experimental results on 11 benchmark datasets reveal that our GraphAdapter significantly outperforms previous adapter-based methods. The code will be released at https://github.com/lixinustc/GraphAdapter
</details>
<details>
<summary>摘要</summary>
adapter-style 高效传输学习（ETL）在视力语模型（VLM）的调整下表现出色，特别是在低数据条件下，只需要引入一些附加参数来挖掘任务特定知识基于通用和强大的 VLM 表示。然而，大多数 adapter-style 工作面临两个限制：（i）只使用单一模式来odel任务特定知识；（ii）忽略下游任务中间类关系的利用，导致优化解决方案。为了缓解这些问题，我们提出了一种有效的 adapter-style 调整策略，名为图像 adapter，它通过显式地模型两种模式之间的 dual-modality 结构知识（即文本和视觉模式之间的各种 semantics/classes 的相关性），使得文本特征可以从两种模式中获得任务特定的结构知识，从而更有效地进行下游任务。具体来说，我们建立了两个子图，即文本知识子图和视觉知识子图，其中节点和边表示两种模式中的 semantics/classes 和 их相关性。这使得文本特征可以从两种模式中获得任务特定的结构知识，从而更有效地进行下游任务。我们的 GraphAdapter 在 11 个 benchmark 数据集上进行了广泛的实验，结果显示，我们的 GraphAdapter 明显超越了前一代 adapter-based 方法。代码将在 https://github.com/lixinustc/GraphAdapter 上发布。
</details></li>
</ul>
<hr>
<h2 id="PRIS-Practical-robust-invertible-network-for-image-steganography"><a href="#PRIS-Practical-robust-invertible-network-for-image-steganography" class="headerlink" title="PRIS: Practical robust invertible network for image steganography"></a>PRIS: Practical robust invertible network for image steganography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13620">http://arxiv.org/abs/2309.13620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanghangai/pris">https://github.com/yanghangai/pris</a></li>
<li>paper_authors: Hang Yang, Yitian Xu, Xuhua Liu, Xiaodong Ma<br>for:PRIS is designed to improve the robustness of image steganography against distortion such as Gaussian noise and lossy compression.methods:PRIS uses invertible neural networks and two enhance modules before and after the extraction process, with a 3-step training strategy. It also considers rounding error, which is typically ignored by other methods, and proposes a gradient approximation function (GAF) to overcome the undifferentiable issue of rounding distortion.results:Experimental results show that PRIS outperforms state-of-the-art robust image steganography methods in both robustness and practicability.Here is the simplified Chinese text for the three key points:for:PRIS 是为了提高图像隐藏技术的鲁棒性，对容器图像受到的扰动（如 Gaussian 噪声和产生损失）进行鲁棒性测试。methods:PRIS 使用 invertible 神经网络，并在提取过程中添加了两个增强模块，使用三步训练策略。它还考虑了round error，通常被其他方法忽略，并提出了一种Gradient Approximation Function（GAF）来超越折射扰动的不可导性问题。results:实验结果表明，PRIS 在鲁棒性和实用性两个方面都超越了当前的图像隐藏方法。<details>
<summary>Abstract</summary>
Image steganography is a technique of hiding secret information inside another image, so that the secret is not visible to human eyes and can be recovered when needed. Most of the existing image steganography methods have low hiding robustness when the container images affected by distortion. Such as Gaussian noise and lossy compression. This paper proposed PRIS to improve the robustness of image steganography, it based on invertible neural networks, and put two enhance modules before and after the extraction process with a 3-step training strategy. Moreover, rounding error is considered which is always ignored by existing methods, but actually it is unavoidable in practical. A gradient approximation function (GAF) is also proposed to overcome the undifferentiable issue of rounding distortion. Experimental results show that our PRIS outperforms the state-of-the-art robust image steganography method in both robustness and practicability. Codes are available at https://github.com/yanghangAI/PRIS, demonstration of our model in practical at http://yanghang.site/hide/.
</details>
<details>
<summary>摘要</summary>
Image 隐藏技术是一种将秘密信息隐藏在另一个图像中，以便当需要时可以恢复。现有的大多数图像隐藏方法具有低的隐藏稳定性，容易受到扰动的影响。这篇论文提出了PRIS，用于提高图像隐藏的稳定性，基于可逆神经网络，并在提取过程前后加入了两个增强模块，采用3步训练策略。此外，我们还考虑了很多现实中常被忽略的圆拟误差问题，并提出了一种梯度近似函数（GAF）来解决圆拟误差问题。实验结果表明，我们的PRIS在稳定性和实用性两个方面都高于当前最佳的图像隐藏方法。代码可以在https://github.com/yanghangAI/PRIS找到，实验演示在http://yanghang.site/hide/.
</details></li>
</ul>
<hr>
<h2 id="Boosting-Offline-Reinforcement-Learning-for-Autonomous-Driving-with-Hierarchical-Latent-Skills"><a href="#Boosting-Offline-Reinforcement-Learning-for-Autonomous-Driving-with-Hierarchical-Latent-Skills" class="headerlink" title="Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills"></a>Boosting Offline Reinforcement Learning for Autonomous Driving with Hierarchical Latent Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13614">http://arxiv.org/abs/2309.13614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zenan Li, Fan Nie, Qiao Sun, Fang Da, Hang Zhao</li>
<li>for: 本文是为了解决learning-based vehicle planning中的长期规划挑战。</li>
<li>methods: 我们使用了variational autoencoder（VAE）来学习从Offline示例中的练习。为了解决VAEs的后验塌缩，我们提出了一种两极Sequence Encoder，可以捕捉练习中的细致驾驶技能的 discrete 和连续变化。</li>
<li>results: 我们在CARLA上进行了广泛的试验，并证明了我们的模型可以在新的enario中比较强的表现。此外，我们还提供了更多的视觉化和实验，以证明学习的策略的可读性和传递性。<details>
<summary>Abstract</summary>
Learning-based vehicle planning is receiving increasing attention with the emergence of diverse driving simulators and large-scale driving datasets. While offline reinforcement learning (RL) is well suited for these safety-critical tasks, it still struggles to plan over extended periods. In this work, we present a skill-based framework that enhances offline RL to overcome the long-horizon vehicle planning challenge. Specifically, we design a variational autoencoder (VAE) to learn skills from offline demonstrations. To mitigate posterior collapse of common VAEs, we introduce a two-branch sequence encoder to capture both discrete options and continuous variations of the complex driving skills. The final policy treats learned skills as actions and can be trained by any off-the-shelf offline RL algorithms. This facilitates a shift in focus from per-step actions to temporally extended skills, thereby enabling long-term reasoning into the future. Extensive results on CARLA prove that our model consistently outperforms strong baselines at both training and new scenarios. Additional visualizations and experiments demonstrate the interpretability and transferability of extracted skills.
</details>
<details>
<summary>摘要</summary>
学习基于的自动驾驶规划正在随着多种驾驶 simulator 和大规模驾驶数据的出现而得到越来越多的注意。虽然线上 reinforcement learning (RL) 适用于这些安全关键任务，但它仍然很难计划长期。在这项工作中，我们提出了一个基于技能的框架，以增强线上 RL 以抵消长期自动驾驶规划挑战。 Specifically，我们设计了一个变量自动编码器 (VAE)，以从线上示范中学习技能。为了解决常见 VAE 的后退问题，我们引入了两个分支序列编码器，以捕捉细致的驾驶技能的分类选择和连续变化。最终策略将学习的技能作为动作，可以通过任何准备好的线上 RL 算法进行训练。这使得我们的模型可以强调长期的规划，而不是每步的动作，从而使得在未来中进行长期预测。我们在 CARLA 上进行了广泛的实验，并证明了我们的模型在训练和新的enario 中 consistently 超过了强的基elines。此外，我们还提供了可读性和传输性的图像和实验，以确认提取的技能的可读性和可传输性。
</details></li>
</ul>
<hr>
<h2 id="A-Text-Classification-Based-Approach-for-Evaluating-and-Enhancing-the-Machine-Interpretability-of-Building-Codes"><a href="#A-Text-Classification-Based-Approach-for-Evaluating-and-Enhancing-the-Machine-Interpretability-of-Building-Codes" class="headerlink" title="A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes"></a>A Text Classification-Based Approach for Evaluating and Enhancing the Machine Interpretability of Building Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14374">http://arxiv.org/abs/2309.14374</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skydustz/text-classification-based-approach-for-evaluating-and-enhancing-machine-interpretability-of-building">https://github.com/skydustz/text-classification-based-approach-for-evaluating-and-enhancing-machine-interpretability-of-building</a></li>
<li>paper_authors: Zhe Zheng, Yu-Cheng Zhou, Ke-Yin Chen, Xin-Zheng Lu, Zhong-Tian She, Jia-Rui Lin</li>
<li>for: 本研究旨在提出一种自动评估和提高建筑法规机器可读性的方法，以便将建筑法规转换成计算机处理可能的格式。</li>
<li>methods: 本研究使用了一种基于域专属语言模型和传输学习技术的高效文本分类模型，并提出了一种用于评估建筑法规机器可读性的量化评价方法。</li>
<li>results: 实验表明，提出的文本分类算法在比较建筑法规中的表现更高，提高了F1-score从72.16%到93.60%，同时也提高了下游自动规则解释方法的性能。<details>
<summary>Abstract</summary>
Interpreting regulatory documents or building codes into computer-processable formats is essential for the intelligent design and construction of buildings and infrastructures. Although automated rule interpretation (ARI) methods have been investigated for years, most of them highly depend on the early and manual filtering of interpretable clauses from a building code. While few of them considered machine interpretability, which represents the potential to be transformed into a computer-processable format, from both clause- and document-level. Therefore, this research aims to propose a novel approach to automatically evaluate and enhance the machine interpretability of single clause and building codes. First, a few categories are introduced to classify each clause in a building code considering the requirements for rule interpretation, and a dataset is developed for model training. Then, an efficient text classification model is developed based on a pretrained domain-specific language model and transfer learning techniques. Finally, a quantitative evaluation method is proposed to assess the overall interpretability of building codes. Experiments show that the proposed text classification algorithm outperforms the existing CNN- or RNN-based methods, improving the F1-score from 72.16% to 93.60%. It is also illustrated that the proposed classification method can enhance downstream ARI methods with an improvement of 4%. Furthermore, analyzing the results of more than 150 building codes in China showed that their average interpretability is 34.40%, which implies that it is still hard to fully transform the entire regulatory document into computer-processable formats. It is also argued that the interpretability of building codes should be further improved both from the human side and the machine side.
</details>
<details>
<summary>摘要</summary>
“理解法规文档或基础设计文档的自动转换为电脑处理可能是建筑和基础设施设计中的重要因素。 although automated rule interpretation (ARI) 方法已经在多年来进行研究，大多数它们仅仅依赖早期的手动筛选可解释的条款，而几乎没有考虑过机器可读性，这代表了可以转换为电脑处理格式的潜力。因此，本研究的目的是提出一种新的方法来自动评估和提高建筑法规的机器可读性。首先，我们引入了一些分类建议，以评估每个条款的需求，然后发展了一个可读性训练 datasets。接着，我们开发了一个高效的文本分类模型，基于预训练的专业语言模型和转移学习技术。最后，我们提出了一个量化评估方法，以评估建筑法规的全面可读性。实验结果显示，我们的文本分类算法在 CNN 和 RNN 基础上进行训练后，对 F1 分数进行了提高，从 72.16% 提高至 93.60%。此外，我们还发现，使用我们的分类方法可以对下游 ARI 方法进行改进，提高了 4%。此外，遍历了中国逾 150 份建筑法规，我们发现其平均可读性为 34.40%，这 implies that it is still difficult to fully transform the entire regulatory document into computer-processable formats。此外，我们还认为，建筑法规的可读性应该在人类和机器两方面进行进一步改进。”
</details></li>
</ul>
<hr>
<h2 id="MM-NeRF-Multimodal-Guided-3D-Multi-Style-Transfer-of-Neural-Radiance-Field"><a href="#MM-NeRF-Multimodal-Guided-3D-Multi-Style-Transfer-of-Neural-Radiance-Field" class="headerlink" title="MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance Field"></a>MM-NeRF: Multimodal-Guided 3D Multi-Style Transfer of Neural Radiance Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13607">http://arxiv.org/abs/2309.13607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijiang Yang, Zhongwei Qiu, Chang Xu, Dongmei Fu</li>
<li>for: 本研究旨在实现高质量的3D多样化风格传输，使用神经辐射场（NeRF）来获取3D场景的高级别描述。</li>
<li>methods: 本研究提出了一种新的多Modal-guided 3D Multi-style transfer of NeRF（MM-NeRF），它可以实现高质量的3D多样化风格传输，并且可以根据多modal导向来指导风格传输。</li>
<li>results: 实验结果表明，MM-NeRF可以实现高质量的3D多样化风格传输，同时保持多视图一致性和多modal风格引导的semantic一致性。<details>
<summary>Abstract</summary>
3D style transfer aims to render stylized novel views of 3D scenes with the specified style, which requires high-quality rendering and keeping multi-view consistency. Benefiting from the ability of 3D representation from Neural Radiance Field (NeRF), existing methods learn the stylized NeRF by giving a reference style from an image. However, they suffer the challenges of high-quality stylization with texture details for multi-style transfer and stylization with multimodal guidance. In this paper, we reveal that the same objects in 3D scenes show various states (color tone, details, etc.) from different views after stylization since previous methods optimized by single-view image-based style loss functions, leading NeRF to tend to smooth texture details, further resulting in low-quality rendering. To tackle these problems, we propose a novel Multimodal-guided 3D Multi-style transfer of NeRF, termed MM-NeRF, which achieves high-quality 3D multi-style rendering with texture details and can be driven by multimodal-style guidance. First, MM-NeRF adopts a unified framework to project multimodal guidance into CLIP space and extracts multimodal style features to guide the multi-style stylization. To relieve the problem of lacking details, we propose a novel Multi-Head Learning Scheme (MLS), in which each style head predicts the parameters of the color head of NeRF. MLS decomposes the learning difficulty caused by the inconsistency of multi-style transfer and improves the quality of stylization. In addition, the MLS can generalize pre-trained MM-NeRF to any new styles by adding heads with small training costs (a few minutes). Extensive experiments on three real-world 3D scene datasets show that MM-NeRF achieves high-quality 3D multi-style stylization with multimodal guidance, keeps multi-view consistency, and keeps semantic consistency of multimodal style guidance. Codes will be released later.
</details>
<details>
<summary>摘要</summary>
三维样式传输目标是将三维场景渲染为指定的样式，需要高质量的渲染和保持多视图一致性。基于神经辐射场（NeRF）的存在，现有方法学习带有样式的NeRF，但它们面临高质量颜色细节的多样化颜色传输和多Modal导航颜色细节的渲染问题。在这篇论文中，我们发现在使用多视图颜色导航后，同一个三维对象在场景中会显示不同的颜色、细节等状态。这是因为前一代方法通过单视图图像基于风格损失优化NeRF，导致NeRF倾向于平滑Texture细节，从而导致低质量渲染。为解决这些问题，我们提出了一种新的多模态指导三维多样式传输NeRF（MM-NeRF），可以实现高质量三维多样式渲染，并且可以通过多模式导航颜色细节。首先，MM-NeRF采用一种统一框架，将多模态指导 проек到CLIP空间中，并提取多模式风格特征来引导多样式风格化。为解决缺乏细节的问题，我们提出了一种新的多头学习方案（MLS），每个风格头预测NeRF的颜色头的参数。MLS分解了多样式传输中学习的困难，并提高了风格化质量。此外，MLS可以将预训练MM-NeRF扩展到新的风格，只需要训练一些小时。广泛的实验表明，MM-NeRF可以实现高质量三维多样式渲染，保持多视图一致性，并保持多模式颜色导航的semantic一致性。代码将在未来发布。
</details></li>
</ul>
<hr>
<h2 id="Distribution-Aware-Continual-Test-Time-Adaptation-for-Semantic-Segmentation"><a href="#Distribution-Aware-Continual-Test-Time-Adaptation-for-Semantic-Segmentation" class="headerlink" title="Distribution-Aware Continual Test Time Adaptation for Semantic Segmentation"></a>Distribution-Aware Continual Test Time Adaptation for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13604">http://arxiv.org/abs/2309.13604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Ni, Senqiao Yang, Jiaming Liu, Xiaoqi Li, Wenyu Jiao, Ran Xu, Zehui Chen, Yi Liu, Shanghang Zhang<br>for: This paper proposes a distribution-aware tuning (DAT) method for efficient and practical continual test-time adaptation (CTTA) in semantic segmentation tasks.methods: The DAT method adaptively selects and updates two small groups of trainable parameters based on data distribution during the continual adaptation process, including domain-specific parameters (DSP) and task-relevant parameters (TRP).results: The proposed method achieves promising performance compared to previous state-of-the-art methods on two widely-used semantic segmentation CTTA benchmarks, demonstrating its effectiveness in mitigating the challenges of error accumulation and catastrophic forgetting.<details>
<summary>Abstract</summary>
Since autonomous driving systems usually face dynamic and ever-changing environments, continual test-time adaptation (CTTA) has been proposed as a strategy for transferring deployed models to continually changing target domains. However, the pursuit of long-term adaptation often introduces catastrophic forgetting and error accumulation problems, which impede the practical implementation of CTTA in the real world. Recently, existing CTTA methods mainly focus on utilizing a majority of parameters to fit target domain knowledge through self-training. Unfortunately, these approaches often amplify the challenge of error accumulation due to noisy pseudo-labels, and pose practical limitations stemming from the heavy computational costs associated with entire model updates. In this paper, we propose a distribution-aware tuning (DAT) method to make the semantic segmentation CTTA efficient and practical in real-world applications. DAT adaptively selects and updates two small groups of trainable parameters based on data distribution during the continual adaptation process, including domain-specific parameters (DSP) and task-relevant parameters (TRP). Specifically, DSP exhibits sensitivity to outputs with substantial distribution shifts, effectively mitigating the problem of error accumulation. In contrast, TRP are allocated to positions that are responsive to outputs with minor distribution shifts, which are fine-tuned to avoid the catastrophic forgetting problem. In addition, since CTTA is a temporal task, we introduce the Parameter Accumulation Update (PAU) strategy to collect the updated DSP and TRP in target domain sequences. We conduct extensive experiments on two widely-used semantic segmentation CTTA benchmarks, achieving promising performance compared to previous state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
自适应驾驶系统通常面临动态和不断变化的环境，因此提出了持续测试时适应（CTTA）作为将部署模型转移到不断变化的目标领域的策略。然而，追求长期适应通常会导致慢速忘记和错误积累问题，这些问题限制了CTTA在实际应用中的实施。现有的CTTA方法主要通过使用大量参数来适应目标领域知识进行自学习。然而，这些方法通常会增加 pseudo-标签 noise 的挑战，并且由于整个模型更新的重要计算成本，它们在实际应用中存在限制。在本文中，我们提出了分布意识调整（DAT）方法，以使得 semantic segmentation CTTA 在实际应用中变得有效和实用。DAT 在 continual adaptation 过程中适应ively 选择和更新两个小组trainable parameter，包括域pecific parameter（DSP）和任务相关 parameter（TRP）。具体来说，DSP 在输出具有显著分布差异时表现敏感，因此可以有效 mitigate 错误积累问题。相反，TRP 被分配到输出具有小分布差异的位置，并在避免慢速忘记问题的同时进行微调。此外，由于 CTTA 是一个时间任务，我们提出了 Parameter Accumulation Update（PAU）策略，用于在目标领域序列中收集更新的 DSP 和 TRP。我们在两个广泛使用的 semantic segmentation CTTA  bencmarks 上进行了广泛的实验，并 achieved 比前一个状态的方法更好的性能。
</details></li>
</ul>
<hr>
<h2 id="From-Cluster-Assumption-to-Graph-Convolution-Graph-based-Semi-Supervised-Learning-Revisited"><a href="#From-Cluster-Assumption-to-Graph-Convolution-Graph-based-Semi-Supervised-Learning-Revisited" class="headerlink" title="From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised Learning Revisited"></a>From Cluster Assumption to Graph Convolution: Graph-based Semi-Supervised Learning Revisited</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13599">http://arxiv.org/abs/2309.13599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Wang, Hongming Ding, Li Pan, Jianhua Li, Zhiguo Gong, Philip S. Yu</li>
<li>for: 本文研究 graph-based semi-supervised learning (GSSL) 的关系，并提出三种graph convolution方法来提高GSSL的性能。</li>
<li>methods: 本文使用了一种统一优化框架来探讨 traditional GSSL 方法和 graph convolutional networks (GCNs) 之间的关系。三种提议的graph convolution方法包括：1) supervised方法 OGC，使用标签来引导图 convolution 过程；2) 无标签方法 GGC，希望在图 convolution 过程中保持图结构信息；3) 多尺度版本 GGCM，将 GGC 应用到不同的尺度上。</li>
<li>results: 经过广泛的实验，本文证明了我们提出的三种方法都能够提高 GSSL 的性能。<details>
<summary>Abstract</summary>
Graph-based semi-supervised learning (GSSL) has long been a hot research topic. Traditional methods are generally shallow learners, based on the cluster assumption. Recently, graph convolutional networks (GCNs) have become the predominant techniques for their promising performance. In this paper, we theoretically discuss the relationship between these two types of methods in a unified optimization framework. One of the most intriguing findings is that, unlike traditional ones, typical GCNs may not jointly consider the graph structure and label information at each layer. Motivated by this, we further propose three simple but powerful graph convolution methods. The first is a supervised method OGC which guides the graph convolution process with labels. The others are two unsupervised methods: GGC and its multi-scale version GGCM, both aiming to preserve the graph structure information during the convolution process. Finally, we conduct extensive experiments to show the effectiveness of our methods.
</details>
<details>
<summary>摘要</summary>
Traditional GSSL methods are usually shallow learners, based on the cluster assumption. Recently, graph convolutional networks (GCNs) have become the predominant techniques for their promising performance. In this paper, we theoretically discuss the relationship between these two types of methods in a unified optimization framework. One of the most intriguing findings is that, unlike traditional ones, typical GCNs may not jointly consider the graph structure and label information at each layer. Motivated by this, we further propose three simple but powerful graph convolution methods. The first is a supervised method OGC, which guides the graph convolution process with labels. The others are two unsupervised methods: GGC and its multi-scale version GGCM, both aiming to preserve the graph structure information during the convolution process. Finally, we conduct extensive experiments to show the effectiveness of our methods.Here's the text with some notes on the translation:* "GSSL" is translated as "图像基于 semi-supervised learning" (tú xiàng bǐ yǐjīng xiǎng yù yì)* "traditional methods" is translated as "传统方法" (chuán chéng fāng fa)* "GCNs" is translated as "图aelastic networks" (tú yì xiǎng wǎng)* "unified optimization framework" is translated as "统一优化框架" (tǒng yī yǎo jì kōng jī)* "shallow learners" is translated as "浅学习" (shallow learners)* "cluster assumption" is translated as "团结假设" (cluster assumption)* "graph structure" is translated as "图 структура" (graph structure)* "label information" is translated as "标签信息" (label information)* "supervised method" is translated as "指导方法" (supervised method)* "unsupervised methods" is translated as "无指导方法" (unsupervised methods)* "GGC" is translated as "图structural preserved方法" (GGC)* "GGCM" is translated as "多级图structural preserved方法" (GGCM)Please note that the translation is done in a way that is consistent with the conventions of Simplified Chinese, and some of the terms used may not be exactly the same as the original English text.
</details></li>
</ul>
<hr>
<h2 id="Seeing-Is-Not-Always-Believing-Invisible-Collision-Attack-and-Defence-on-Pre-Trained-Models"><a href="#Seeing-Is-Not-Always-Believing-Invisible-Collision-Attack-and-Defence-on-Pre-Trained-Models" class="headerlink" title="Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models"></a>Seeing Is Not Always Believing: Invisible Collision Attack and Defence on Pre-Trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13579">http://arxiv.org/abs/2309.13579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous10240/framework">https://github.com/anonymous10240/framework</a></li>
<li>paper_authors: Minghang Deng, Zhong Zhang, Junming Shao</li>
<li>For: This paper proposes a novel framework for an invisible attack on large-scale pre-trained models (PTMs) like BERT and GPT, which can be used to manipulate the predictions of the models without being detected.* Methods: The proposed attack leverages the MD5 chosen-prefix collision to generate two equal-size models with the same MD5 checksum, which are then deployed on public websites to induce victims to download the poisoned model.* Results: The paper demonstrates the effectiveness and stealthiness of the proposed attack and defensive method on different models and data sets, and provides a theoretical justification for its feasibility.<details>
<summary>Abstract</summary>
Large-scale pre-trained models (PTMs) such as BERT and GPT have achieved great success in diverse fields. The typical paradigm is to pre-train a big deep learning model on large-scale data sets, and then fine-tune the model on small task-specific data sets for downstream tasks. Although PTMs have rapidly progressed with wide real-world applications, they also pose significant risks of potential attacks. Existing backdoor attacks or data poisoning methods often build up the assumption that the attacker invades the computers of victims or accesses the target data, which is challenging in real-world scenarios. In this paper, we propose a novel framework for an invisible attack on PTMs with enhanced MD5 collision. The key idea is to generate two equal-size models with the same MD5 checksum by leveraging the MD5 chosen-prefix collision. Afterwards, the two ``same" models will be deployed on public websites to induce victims to download the poisoned model. Unlike conventional attacks on deep learning models, this new attack is flexible, covert, and model-independent. Additionally, we propose a simple defensive strategy for recognizing the MD5 chosen-prefix collision and provide a theoretical justification for its feasibility. We extensively validate the effectiveness and stealthiness of our proposed attack and defensive method on different models and data sets.
</details>
<details>
<summary>摘要</summary>
大规模预训练模型（PTM）如BERT和GPT在多个领域取得了很大成功。典型的假设是先预训大深度学习模型在大规模数据集上，然后在小任务特定数据集上细化模型以进行下游任务。although PTMs have rapidly progressed with wide real-world applications, they also pose significant risks of potential attacks. 现有的后门攻击或数据毒液方法通常假设攻击者可以入侵受害者的计算机或访问目标数据，这是现实世界中的挑战。在这篇论文中，我们提出了一种新的隐形攻击方法，通过提高MD5撞击的方式来实现。关键思想是通过MD5选择前缀撞击来生成两个相同大小的模型，并将这两个“相同”的模型部署到公共网站上，以引诱受害者下载毒化模型。与传统的深度学习模型攻击方法不同，这种新的攻击方法更加灵活、隐蔽和模型独立。此外，我们还提出了一种简单的防御策略，可以识别MD5选择前缀撞击，并提供了理论上的可行性。我们在不同的模型和数据集上进行了广泛验证和证明了攻击和防御方法的效果和隐蔽性。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Weight-Fixing-Large-scale-training-of-neural-network-weight-uncertainties-for-quantization"><a href="#Probabilistic-Weight-Fixing-Large-scale-training-of-neural-network-weight-uncertainties-for-quantization" class="headerlink" title="Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization"></a>Probabilistic Weight Fixing: Large-scale training of neural network weight uncertainties for quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13575">http://arxiv.org/abs/2309.13575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/subiawaud/PWFN">https://github.com/subiawaud/PWFN</a></li>
<li>paper_authors: Christopher Subia-Waud, Srinandan Dasmahapatra</li>
<li>for: 降低大神经网络的执行时间和能耗，通过尝试将权重限制到一个有限的值集。</li>
<li>methods: 使用 Bayesian neural networks (BNNs) 和一种简化的轻量级 relaxation 来确定权重可以被移动到哪些中心和多少，基于它们的具体位置特有的学习不确定性分布。</li>
<li>results: 比前方法更高的压缩率和更高的准确率，特别是在使用 DeiT-Tiny 模型和 transformer 模型时。在 ImageNet 上，我们的方法可以将 5000 万个权重压缩到 296 个唯一值上，并且与前方法的 top-1 准确率相比提高 1.6%。<details>
<summary>Abstract</summary>
Weight-sharing quantization has emerged as a technique to reduce energy expenditure during inference in large neural networks by constraining their weights to a limited set of values. However, existing methods for weight-sharing quantization often make assumptions about the treatment of weights based on value alone that neglect the unique role weight position plays. This paper proposes a probabilistic framework based on Bayesian neural networks (BNNs) and a variational relaxation to identify which weights can be moved to which cluster centre and to what degree based on their individual position-specific learned uncertainty distributions. We introduce a new initialisation setting and a regularisation term which allow for the training of BNNs under complex dataset-model combinations. By leveraging the flexibility of weight values captured through a probability distribution, we enhance noise resilience and downstream compressibility. Our iterative clustering procedure demonstrates superior compressibility and higher accuracy compared to state-of-the-art methods on both ResNet models and the more complex transformer-based architectures. In particular, our method outperforms the state-of-the-art quantization method top-1 accuracy by 1.6% on ImageNet using DeiT-Tiny, with its 5 million+ weights now represented by only 296 unique values.
</details>
<details>
<summary>摘要</summary>
大型神经网络中的权重共享量化技术可以降低推理过程中的能耗。然而，现有的权重共享量化方法通常假设权重值的处理方法是基于价值alone neglects 权重位置的特殊作用。这篇论文提出了基于 Bayesian neural networks（BNNs）的概率框架和一种可relaxation的方法，用于确定权重可以被移动到哪些集中心和多少基于它们的具体位置特定学习不确定分布。我们提出了一种新的初始化设定和一种正则化项，allowing for the training of BNNs under complex dataset-model combinations。通过利用权重值 captured through a probability distribution 的灵活性，我们提高了雷达鲁抗性和下游压缩性。我们的迭代归一化过程比前式-of-the-art方法更高的压缩率和更高的准确率，特别是在使用 DeiT-Tiny 模型和更复杂的 transformer-based 架构时。在这些模型中，我们的方法可以将 5000万+ 个权重表示为只 296 个唯一的值，与state-of-the-art 方法的 top-1 准确率相比，提高了 1.6%。
</details></li>
</ul>
<hr>
<h2 id="Keeping-in-Time-Adding-Temporal-Context-to-Sentiment-Analysis-Models"><a href="#Keeping-in-Time-Adding-Temporal-Context-to-Sentiment-Analysis-Models" class="headerlink" title="Keeping in Time: Adding Temporal Context to Sentiment Analysis Models"></a>Keeping in Time: Adding Temporal Context to Sentiment Analysis Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13562">http://arxiv.org/abs/2309.13562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dean Ninalga</li>
<li>for: 提高和保持 sentiment analysis 模型的性能 across shorter and longer time periods.</li>
<li>methods: 使用日期前缀的文本输入，并使用自我标签法将无标签数据用于学习学生模型。 使用一种新的日期格式化策略来扩大自我标签过程。</li>
<li>results: 在 LongEval-Classification 评估集上实现了减少性能下降的最好 Result （RPD） (-0.0656)，并达到了总分 0.6923，位列第二名。<details>
<summary>Abstract</summary>
This paper presents a state-of-the-art solution to the LongEval CLEF 2023 Lab Task 2: LongEval-Classification. The goal of this task is to improve and preserve the performance of sentiment analysis models across shorter and longer time periods. Our framework feeds date-prefixed textual inputs to a pre-trained language model, where the timestamp is included in the text. We show date-prefixed samples better conditions model outputs on the temporal context of the respective texts. Moreover, we further boost performance by performing self-labeling on unlabeled data to train a student model. We augment the self-labeling process using a novel augmentation strategy leveraging the date-prefixed formatting of our samples. We demonstrate concrete performance gains on the LongEval-Classification evaluation set over non-augmented self-labeling. Our framework achieves a 2nd place ranking with an overall score of 0.6923 and reports the best Relative Performance Drop (RPD) of -0.0656 over the short evaluation set.
</details>
<details>
<summary>摘要</summary>
To further boost performance, we perform self-labeling on unlabeled data to train a student model. We augment the self-labeling process using a novel augmentation strategy that leverages the date-prefixed formatting of our samples. Our approach achieves concrete performance gains on the LongEval-Classification evaluation set compared to non-augmented self-labeling.Our framework achieved a 2nd place ranking with an overall score of 0.6923 and reported the best Relative Performance Drop (RPD) of -0.0656 over the short evaluation set.
</details></li>
</ul>
<hr>
<h2 id="Cordyceps-LT-EDI-Patching-Language-Specific-Homophobia-Transphobia-Classifiers-with-a-Multilingual-Understanding"><a href="#Cordyceps-LT-EDI-Patching-Language-Specific-Homophobia-Transphobia-Classifiers-with-a-Multilingual-Understanding" class="headerlink" title="Cordyceps@LT-EDI: Patching Language-Specific Homophobia&#x2F;Transphobia Classifiers with a Multilingual Understanding"></a>Cordyceps@LT-EDI: Patching Language-Specific Homophobia&#x2F;Transphobia Classifiers with a Multilingual Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13561">http://arxiv.org/abs/2309.13561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dean Ninalga</li>
<li>For: 本研究旨在探讨识别社交媒体评论中的恐同和恐 транс人辱骂语言的方法，以优化识别率和准确率。* Methods: 本研究采用了多语言（M-L）和语言特定（L-S）方法的结合，通过简单的权重 interpolating 来融合两种方法，以优化识别率和准确率。* Results: 本研究在 task A 的 ‘Shared Task on Homophobia&#x2F;Transphobia Detection in social media comments’ 数据集上实现了最佳结果，在五种语言中取得了三个语言的最佳结果，并在马拉雅邦语文本上 achieve 0.997 的macro F1 分数。<details>
<summary>Abstract</summary>
Detecting transphobia, homophobia, and various other forms of hate speech is difficult. Signals can vary depending on factors such as language, culture, geographical region, and the particular online platform. Here, we present a joint multilingual (M-L) and language-specific (L-S) approach to homophobia and transphobic hate speech detection (HSD). M-L models are needed to catch words, phrases, and concepts that are less common or missing in a particular language and subsequently overlooked by L-S models. Nonetheless, L-S models are better situated to understand the cultural and linguistic context of the users who typically write in a particular language. Here we construct a simple and successful way to merge the M-L and L-S approaches through simple weight interpolation in such a way that is interpretable and data-driven. We demonstrate our system on task A of the 'Shared Task on Homophobia/Transphobia Detection in social media comments' dataset for homophobia and transphobic HSD. Our system achieves the best results in three of five languages and achieves a 0.997 macro average F1-score on Malayalam texts.
</details>
<details>
<summary>摘要</summary>
检测transphobia、homophobia和其他形式的仇恨言语困难。信号可以因语言、文化、地区和在线平台而异常。我们介绍了一种联合多语言（M-L）和语言特定（L-S）方法来检测同性恋和变性人仇恨言语检测（HSD）。M-L模型可以捕捉语言中不常见或缺失的词汇和短语，并被L-S模型所过look。然而，L-S模型更好地理解用户 Typically write in a particular language的文化和语言背景。我们构建了一种简单有效的方法来融合M-L和L-S方法，通过简单的权重 interpolating 的方式，以便可以解释和数据驱动。我们在task A of the 'Shared Task on Homophobia/Transphobia Detection in social media comments' dataset上展示了我们的系统，并在五种语言中获得了最佳结果，并在马拉雅拉姆语文本上达到了0.997macro average F1-score。
</details></li>
</ul>
<hr>
<h2 id="Decoding-Radiologists-Intense-Focus-for-Accurate-CXR-Diagnoses-A-Controllable-and-Interpretable-AI-System"><a href="#Decoding-Radiologists-Intense-Focus-for-Accurate-CXR-Diagnoses-A-Controllable-and-Interpretable-AI-System" class="headerlink" title="Decoding Radiologists Intense Focus for Accurate CXR Diagnoses: A Controllable and Interpretable AI System"></a>Decoding Radiologists Intense Focus for Accurate CXR Diagnoses: A Controllable and Interpretable AI System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13550">http://arxiv.org/abs/2309.13550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trong Thang Pham, Jacob Brecheisen, Anh Nguyen, Hien Nguyen, Ngan Le</li>
<li>for: 这个论文目标是提出一种可控制可解释的护肺X光诊断管道，以帮助理解肺科医生在诊断过程中的认知过程。</li>
<li>methods: 该方法使用视Language模型，可以准确地控制诊断过程，并且可以排除不重要的特征。</li>
<li>results: 经过广泛的实验，表明该方法可以准确地 Classification tasks，只需使用护肺X光的一部分。<details>
<summary>Abstract</summary>
In the field of chest X-ray (CXR) diagnosis, existing works often focus solely on determining where a radiologist looks, typically through tasks such as detection, segmentation, or classification. However, these approaches are often designed as black-box models, lacking interpretability. In this paper, we introduce a novel and unified controllable interpretable pipeline for decoding the intense focus of radiologists in CXR diagnosis. Our approach addresses three key questions: where a radiologist looks, how long they focus on specific areas, and what findings they diagnose. By capturing the intensity of the radiologist's gaze, we provide a unified solution that offers insights into the cognitive process underlying radiological interpretation. Unlike current methods that rely on black-box machine learning models, which can be prone to extracting erroneous information from the entire input image during the diagnosis process, we tackle this issue by effectively masking out irrelevant information. Our approach leverages a vision-language model, allowing for precise control over the interpretation process while ensuring the exclusion of irrelevant features. To train our model, we utilize an eye gaze dataset to extract anatomical gaze information and generate ground truth heatmaps. Through extensive experimentation, we demonstrate the efficacy of our method. We showcase that the attention heatmaps, designed to mimic radiologists' focus, encode sufficient and relevant information, enabling accurate classification tasks using only a portion of CXR.
</details>
<details>
<summary>摘要</summary>
在胸部X射影（CXR）诊断领域，现有的工作通常围绕确定诊断人员的注意力点进行设计，通常通过检测、分割或分类等任务来完成。然而，这些方法经常设计成黑盒模型，缺乏可解释性。在这篇论文中，我们提出了一种新的可控可解释的扫描策略，用于解码诊断人员在CXR诊断过程中的焦点。我们的方法解决了三个关键问题：诊断人员注意力点在哪里、如何长时间关注特定区域，以及他们诊断了什么。我们通过捕捉诊断人员的眼动信息，提供了一种统一的解决方案，可以帮助理解诊断过程中的认知过程。不同于现有的黑盒机器学习模型，这些模型可能会从整个输入图像中提取错误信息，我们通过有效地遮盖无关信息来解决这个问题。我们的方法利用了视觉语言模型，可以准确控制解释过程，同时确保排除无关特征。为了训练我们的模型，我们使用了眼动数据集来提取 анатомиче gaze 信息，生成标准的热图。通过广泛的实验，我们证明了我们的方法的有效性。我们显示了注意力热图，设计用于模拟诊断人员的注意力，含有足够和相关的信息，可以使用CXR中的一部分进行准确的分类任务。
</details></li>
</ul>
<hr>
<h2 id="Related-Rhythms-Recommendation-System-To-Discover-Music-You-May-Like"><a href="#Related-Rhythms-Recommendation-System-To-Discover-Music-You-May-Like" class="headerlink" title="Related Rhythms: Recommendation System To Discover Music You May Like"></a>Related Rhythms: Recommendation System To Discover Music You May Like</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13544">http://arxiv.org/abs/2309.13544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rahul Singh, Pranav Kanuparthi</li>
<li>for: 这 paper 的目的是提出一个分布式机器学习（ML）管道，用于从 Million Songs Dataset（MSD）中提取类似于输入subset的歌曲。</li>
<li>methods: 该 paper 使用的方法包括使用分布式 ML 管道，以便在 MSD 上进行音频轨道分析和推荐。</li>
<li>results: 该 paper 的结果显示，使用分布式 ML 管道可以提供高效的推荐系统，并且可以在 MSD 上进行大规模的音频轨道分析和推荐。<details>
<summary>Abstract</summary>
Machine Learning models are being utilized extensively to drive recommender systems, which is a widely explored topic today. This is especially true of the music industry, where we are witnessing a surge in growth. Besides a large chunk of active users, these systems are fueled by massive amounts of data. These large-scale systems yield applications that aim to provide a better user experience and to keep customers actively engaged. In this paper, a distributed Machine Learning (ML) pipeline is delineated, which is capable of taking a subset of songs as input and producing a new subset of songs identified as being similar to the inputted subset. The publicly accessible Million Songs Dataset (MSD) enables researchers to develop and explore reasonably efficient systems for audio track analysis and recommendations, without having to access a commercialized music platform. The objective of the proposed application is to leverage an ML system trained to optimally recommend songs that a user might like.
</details>
<details>
<summary>摘要</summary>
机器学习模型在推荐系统方面得到广泛应用，特别是在音乐行业，目前在快速发展。这主要归功于大量数据的支持以及高效的机器学习算法。这些大规模系统的应用旨在提供更好的用户体验，并保持用户高度参与。在这篇论文中，我们提出了一个分布式机器学习（ML）管道，可以将输入subset of songs中的一部分作为输入，并生成与输入相似的新subset of songs。可以通过公共可访问的Million Songs Dataset（MSD），让研究人员开发和探索reasonably efficient的音频轨道分析和推荐系统，不需要访问商业化音乐平台。我们的目标是使用ML系统来优化推荐用户可能喜欢的歌曲。
</details></li>
</ul>
<hr>
<h2 id="Human-Transcription-Quality-Improvement"><a href="#Human-Transcription-Quality-Improvement" class="headerlink" title="Human Transcription Quality Improvement"></a>Human Transcription Quality Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14372">http://arxiv.org/abs/2309.14372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GenerateAI/LibriCrowd">https://github.com/GenerateAI/LibriCrowd</a></li>
<li>paper_authors: Jian Gao, Hanbo Sun, Cheng Cao, Zheng Du</li>
<li>for: 提高自动语音识别（ASR）系统的训练数据质量。</li>
<li>methods: 提出一种可靠的训练数据收集方法，包括对标注阶段进行信任度估计基于的重新处理，以及后置标注阶段的自动单词错误 corrections。</li>
<li>results: 实验显示，对100小时英语语音标注的Transcription WER减少了超过50%。进一步研究表明，错误的转录影响ASR模型性能强相关。改进转录质量提供了10%以上相对WER减少。发布了数据集和代码，为研究社区提供利益。<details>
<summary>Abstract</summary>
High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community.
</details>
<details>
<summary>摘要</summary>
高品质转录数据是自动语音识别（ASR）系统训练的关键。然而，现有的行业级数据采集管道对研究人员来说太costly，而且大众办理的转录质量低。在这篇论文中，我们提出一种可靠的方法来采集语音转录。我们提出了两种机制来提高转录质量：在标注阶段基于信息估计的重新处理，以及在后置阶段自动单词错误更正。我们采集并发布了LibriCrowd - 100小时英语语音转录的大规模众生采集数据集。实验表明，转录WER（识别错误率）下降了超过50%。我们进一步调查了转录错误对ASR模型性能的影响，发现了强相关性。高品质转录改善提供了10%以上相对WER降幅。我们发布数据集和代码，以便研究人员享受。
</details></li>
</ul>
<hr>
<h2 id="Speech-enhancement-with-frequency-domain-auto-regressive-modeling"><a href="#Speech-enhancement-with-frequency-domain-auto-regressive-modeling" class="headerlink" title="Speech enhancement with frequency domain auto-regressive modeling"></a>Speech enhancement with frequency domain auto-regressive modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13537">http://arxiv.org/abs/2309.13537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurenjan Purushothaman, Debottam Dutta, Rohit Kumar, Sriram Ganapathy</li>
<li>for: 提高在远场实际场景中的speech质量和自动语音识别（ASR）性能</li>
<li>methods: 使用AR模型进行束子域speech信号的干扰分解，并使用 dual path long short term memory（DPLSTM）模型进行束子域束子域信号的增强</li>
<li>results: 在REVERB挑战数据集和VOiCES数据集上，与基准系统相比，jointly learns speech dereverberation network和E2E ASR模型可以获得显著性能提高（相对于基准系统的平均相对提高率为10-24%），并且通过主观听测试得到了提高的音频质量。<details>
<summary>Abstract</summary>
Speech applications in far-field real world settings often deal with signals that are corrupted by reverberation. The task of dereverberation constitutes an important step to improve the audible quality and to reduce the error rates in applications like automatic speech recognition (ASR). We propose a unified framework of speech dereverberation for improving the speech quality and the ASR performance using the approach of envelope-carrier decomposition provided by an autoregressive (AR) model. The AR model is applied in the frequency domain of the sub-band speech signals to separate the envelope and carrier parts. A novel neural architecture based on dual path long short term memory (DPLSTM) model is proposed, which jointly enhances the sub-band envelope and carrier components. The dereverberated envelope-carrier signals are modulated and the sub-band signals are synthesized to reconstruct the audio signal back. The DPLSTM model for dereverberation of envelope and carrier components also allows the joint learning of the network weights for the down stream ASR task. In the ASR tasks on the REVERB challenge dataset as well as on the VOiCES dataset, we illustrate that the joint learning of speech dereverberation network and the E2E ASR model yields significant performance improvements over the baseline ASR system trained on log-mel spectrogram as well as other benchmarks for dereverberation (average relative improvements of 10-24% over the baseline system). The speech quality improvements, evaluated using subjective listening tests, further highlight the improved quality of the reconstructed audio.
</details>
<details>
<summary>摘要</summary>
讲话应用程序在远场实际场景中经常会遇到受泛音损害的信号。去泛音是提高语音质量和降低自动语音识别（ASR）错误率的重要步骤。我们提出一个统一框架，用于提高语音质量和ASR性能，基于autoregressive（AR）模型的振荡分解。在各个子带语音信号的频域中，AR模型用于分离振荡和载波部分。我们提出了一种基于双路长短期记忆（DPLSTM）模型的新型神经网络架构，可以同时提高子带振荡和载波组件。去泛音后，振荡和载波组件被修改，并将子带信号重新 sinthezied 以重构音频信号。我们在REVERB挑战数据集和VOiCES数据集上进行了ASR任务，并证明了将批量学习网络参数与下游ASR任务相结合可以获得显著性能提升（相对于基准系统，平均提升10-24%）。此外，通过主观听测试，我们还证明了去泛音后的重构音频质量的提高。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Reachability-Estimation-for-Safe-Reinforcement-Learning"><a href="#Iterative-Reachability-Estimation-for-Safe-Reinforcement-Learning" class="headerlink" title="Iterative Reachability Estimation for Safe Reinforcement Learning"></a>Iterative Reachability Estimation for Safe Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13528">http://arxiv.org/abs/2309.13528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, Sicun Gao</li>
<li>for: 本研究旨在提供一个新的安全性权限执行学习（RL）框架，以确保RL在实际应用中的安全性。</li>
<li>methods: 本研究提出了一种新的 reachability estimation 函数，用于在涉及到不确定环境的通用情况下进行安全性权限执行学习。</li>
<li>results: 研究人员通过对一系列安全RL环境进行实验，证明了他们的算法可以在 reward performance 和安全性两个方面提供改进。<details>
<summary>Abstract</summary>
Ensuring safety is important for the practical deployment of reinforcement learning (RL). Various challenges must be addressed, such as handling stochasticity in the environments, providing rigorous guarantees of persistent state-wise safety satisfaction, and avoiding overly conservative behaviors that sacrifice performance. We propose a new framework, Reachability Estimation for Safe Policy Optimization (RESPO), for safety-constrained RL in general stochastic settings. In the feasible set where there exist violation-free policies, we optimize for rewards while maintaining persistent safety. Outside this feasible set, our optimization produces the safest behavior by guaranteeing entrance into the feasible set whenever possible with the least cumulative discounted violations. We introduce a class of algorithms using our novel reachability estimation function to optimize in our proposed framework and in similar frameworks such as those concurrently handling multiple hard and soft constraints. We theoretically establish that our algorithms almost surely converge to locally optimal policies of our safe optimization framework. We evaluate the proposed methods on a diverse suite of safe RL environments from Safety Gym, PyBullet, and MuJoCo, and show the benefits in improving both reward performance and safety compared with state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
保证安全是RL实践中非常重要的一点。various challenges需要被解决，例如在环境中处理随机性，提供坚实的状态级别安全满足保证，并避免过度保守的行为，这会损害性能。我们提出了一个新的框架，即Reachability Estimation for Safe Policy Optimization（RESPO），用于安全限制RL在一般随机环境中。在可行集（feasible set）中，我们优化奖励，同时保持持续安全。外部可行集，我们的优化生成最安全的行为， garantizesthat entrance into the feasible set whenever possible with the least cumulative discounted violations。我们引入了一类使用我们的新的达性估计函数来优化的算法，并在我们的框架和类似框架（如同时处理多个硬 soft constraints）中进行优化。我们证明了我们的算法在我们的安全优化框架中幂等 converges to locally optimal policies。我们对一个包含了安全RL环境的多样化集合进行评估，并显示了与现有基准点相比，提高了奖励性能和安全性。
</details></li>
</ul>
<hr>
<h2 id="Global-correlated-3D-decoupling-Transformer-for-Clothed-Avatar-Reconstruction"><a href="#Global-correlated-3D-decoupling-Transformer-for-Clothed-Avatar-Reconstruction" class="headerlink" title="Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction"></a>Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13524">http://arxiv.org/abs/2309.13524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/river-zhang/gta">https://github.com/river-zhang/gta</a></li>
<li>paper_authors: Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, Yi Yang</li>
<li>for:  reconstruction of 3D clothed human avatars from single images</li>
<li>methods: transformer-based architecture with global-correlated image features and 3D-decoupling decoder with cross-attention and learnable embeddings</li>
<li>results: outperforms state-of-the-art approaches in both geometry and texture reconstruction, with high robustness to challenging poses and loose clothing, and produces higher-resolution textures.Here’s the simplified Chinese text:</li>
<li>for: 用单张图像重建 clothed 人物模型</li>
<li>methods: 使用变换器建筑，利用全球相关的图像特征，并使用交叉注意力和学习嵌入来解耦三个平面特征</li>
<li>results: 在 CAPE 和 THuman2.0 数据集上表现出色，与现有方法相比，在几何和文本重建方面具有更高的精度和更高的纹理质量，并且具有更高的可靠性和更高的分辨率.<details>
<summary>Abstract</summary>
Reconstructing 3D clothed human avatars from single images is a challenging task, especially when encountering complex poses and loose clothing. Current methods exhibit limitations in performance, largely attributable to their dependence on insufficient 2D image features and inconsistent query methods. Owing to this, we present the Global-correlated 3D-decoupling Transformer for clothed Avatar reconstruction (GTA), a novel transformer-based architecture that reconstructs clothed human avatars from monocular images. Our approach leverages transformer architectures by utilizing a Vision Transformer model as an encoder for capturing global-correlated image features. Subsequently, our innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane features, using learnable embeddings as queries for cross-plane generation. To effectively enhance feature fusion with the tri-plane 3D feature and human body prior, we propose a hybrid prior fusion strategy combining spatial and prior-enhanced queries, leveraging the benefits of spatial localization and human body prior knowledge. Comprehensive experiments on CAPE and THuman2.0 datasets illustrate that our method outperforms state-of-the-art approaches in both geometry and texture reconstruction, exhibiting high robustness to challenging poses and loose clothing, and producing higher-resolution textures. Codes will be available at https://github.com/River-Zhang/GTA.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过单个图像重建三维人物模拟是一项具有挑战性的任务，尤其是当遇到复杂的姿势和裤子时。现有方法具有不足的二维图像特征和不一致的查询方法，导致表现有限。为此，我们提出了全球相关的3D分解变换器（GTA），一种基于变换器架构的新建模，用于从单个图像中重建裤装人物模拟。我们的方法利用变换器模型作为编码器，以捕捉全球相关的图像特征。然后，我们的创新的3D分解解码器使用交叉注意力来分解三平面特征，并使用学习的嵌入作为交叉平面生成的查询。为了有效地增强特征融合三平面3D特征和人体先天知识，我们提议一种混合的先天知识融合策略，将空间和先天知识增强的查询混合使用，利用空间本地化和人体先天知识的优点。通过对CAPE和THuman2.0数据集进行广泛的实验，我们的方法在几何学和纹理重建方面超越了当前状态艺术，展现出高稳定性和高分辨率，并能够有效地处理复杂的姿势和裤子。代码将在https://github.com/River-Zhang/GTA上提供。
</details></li>
</ul>
<hr>
<h2 id="Cordyceps-LT-EDI-Depression-Detection-with-Reddit-and-Self-training"><a href="#Cordyceps-LT-EDI-Depression-Detection-with-Reddit-and-Self-training" class="headerlink" title="Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training"></a>Cordyceps@LT-EDI: Depression Detection with Reddit and Self-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01418">http://arxiv.org/abs/2310.01418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dean Ninalga</li>
<li>for: 抑郁症是肇导病种之一，而且很普遍。研究发现过度社交媒体用户与抑郁症、ADHD等精神疾病存在相关性。鉴于这样一大量的人群，那么有很多可能未diagnosed的用户和他们创建的帖子。本文提出了一种抑郁严重程度检测系统，使用半指导学习技术预测用户是否经历严重、中度或低度（非诊断）抑郁。</li>
<li>methods: 我们使用一个训练好的模型来分类大量未标注的社交媒体帖子，然后使用生成的标签来训练更强大的分类器。</li>
<li>results: 我们在LT-EDI@RANLP 2023 shared task上展示了我们的框架，其中我们的框架在检测抑郁症的严重程度方面 ranks 3rd 总的。<details>
<summary>Abstract</summary>
Depression is debilitating, and not uncommon. Indeed, studies of excessive social media users show correlations with depression, ADHD, and other mental health concerns. Given that there is a large number of people with excessive social media usage, then there is a significant population of potentially undiagnosed users and posts that they create. In this paper, we propose a depression severity detection system using a semi-supervised learning technique to predict if a post is from a user who is experiencing severe, moderate, or low (non-diagnostic) levels of depression. Namely, we use a trained model to classify a large number of unlabelled social media posts from Reddit, then use these generated labels to train a more powerful classifier. We demonstrate our framework on Detecting Signs of Depression from Social Media Text - LT-EDI@RANLP 2023 shared task, where our framework ranks 3rd overall.
</details>
<details>
<summary>摘要</summary>
抑郁是毁伤性的，并不是罕见的。实际上，研究过度社交媒体用户表明了抑郁、ADHD和其他心理健康问题之间的相关性。 giventhat there is a large number of people with excessive social media usage, then there is a significant population of potentially undiagnosed users and posts that they create. 在这篇论文中，我们提出了一种抑郁严重程度检测系统，使用半指导学习技术来预测用户是否经历严重、中等或低（非诊断）度的抑郁。具体来说，我们使用一个训练好的模型来分类一大量的未标注社交媒体帖子，然后使用这些生成的标签来训练更强大的分类器。我们在LT-EDI@RANLP 2023共享任务上示出了我们的框架，其中我们的框架在总体排名第三。
</details></li>
</ul>
<hr>
<h2 id="Object-Classification-Model-Using-Ensemble-Learning-with-Gray-Level-Co-Occurrence-Matrix-and-Histogram-Extraction"><a href="#Object-Classification-Model-Using-Ensemble-Learning-with-Gray-Level-Co-Occurrence-Matrix-and-Histogram-Extraction" class="headerlink" title="Object Classification Model Using Ensemble Learning with Gray-Level Co-Occurrence Matrix and Histogram Extraction"></a>Object Classification Model Using Ensemble Learning with Gray-Level Co-Occurrence Matrix and Histogram Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13512">http://arxiv.org/abs/2309.13512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florentina Tatrin Kurniati, Daniel HF Manongga, Eko Sediyono, Sri Yulianto Joko Prasetyo, Roy Rudolf Huizen</li>
<li>for: 本研究旨在开发一种精准的物体分类方法，以便更好地识别和 отличать不同物体。</li>
<li>methods: 本研究使用了投票方法和组合分类器，其中包括Random Forest、K-NN、决策树、SVM和Naive Bayes等分类方法。</li>
<li>results: 测试结果表明，投票方法和组合分类器均取得了很好的结果，其中 ensemble voting 的准确率为92.4%，精度为78.6%，回归率为95.2%，F1-score为86.1%；组合分类器的准确率为99.3%，精度为97.6%，回归率为100%，F1-score为98.8%。根据测试结果，可以确定使用投票方法和组合分类器可以提高物体分类精度。<details>
<summary>Abstract</summary>
In the field of object classification, identification based on object variations is a challenge in itself. Variations include shape, size, color, and texture, these can cause problems in recognizing and distinguishing objects accurately. The purpose of this research is to develop a classification method so that objects can be accurately identified. The proposed classification model uses Voting and Combined Classifier, with Random Forest, K-NN, Decision Tree, SVM, and Naive Bayes classification methods. The test results show that the voting method and Combined Classifier obtain quite good results with each of them, ensemble voting with an accuracy value of 92.4%, 78.6% precision, 95.2% recall, and 86.1% F1-score. While the combined classifier with an accuracy value of 99.3%, a precision of 97.6%, a recall of 100%, and a 98.8% F1-score. Based on the test results, it can be concluded that the use of the Combined Classifier and voting methods is proven to increase the accuracy value. The contribution of this research increases the effectiveness of the Ensemble Learning method, especially the voting ensemble method and the Combined Classifier in increasing the accuracy of object classification in image processing.
</details>
<details>
<summary>摘要</summary>
在物体分类领域，基于物体变化的标识是一项挑战。这些变化包括形状、大小、颜色和文化，这些变化可能会导致对物体的识别和分类准确性受到影响。本研究的目的是开发一种精准的分类方法，以便更好地识别物体。提议的分类模型使用投票和组合分类器，其中包括随机森林、K-NN、决策树、支持向量机和愚蠢树分类方法。测试结果显示，投票方法和组合分类器各自取得了非常好的结果，其中投票方法的准确率为92.4%，命中率为78.6%，召回率为95.2%和准确率为86.1%。而组合分类器的准确率为99.3%，命中率为97.6%，召回率为100%和准确率为98.8%。根据测试结果，可以结论出，使用投票和组合分类器方法可以提高准确率。本研究的贡献是提高 ensemble learning 方法的效果，特别是投票ensemble方法和组合分类器在物体分类中的准确率。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-based-Context-Modeling-and-Reasoning-with-LLMs-A-Tutorial"><a href="#Natural-Language-based-Context-Modeling-and-Reasoning-with-LLMs-A-Tutorial" class="headerlink" title="Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial"></a>Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15074">http://arxiv.org/abs/2309.15074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyi Xiong, Jiang Bian, Sijia Yang, Xiaofei Zhang, Linghe Kong, Daqing Zhang</li>
<li>for: 这个研究是为了探讨大语言模型（LLM）在Context-aware computing中的应用，以及如何使用自然语言来建模上下文和进行上下文理解。</li>
<li>methods: 这个研究使用了各种人工智能技术，如 Ontology 和 OWL，来建模上下文和进行上下文理解。它还使用了自然语言处理技术，如 ChatGPT 和 GPT-4，来模拟用户的请求和上下文。</li>
<li>results: 研究人员在两个案例中证明了 LLMCaC 的可行性，包括在帮助生活中使用移动 z-arm 和规划旅行的上下文意识应用。<details>
<summary>Abstract</summary>
Large language models (LLMs) have become phenomenally surging, since 2018--two decades after introducing context-awareness into computing systems. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing (LCaC). In the LCaC paradigm, users' requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users' request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases--(1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在2018年以来，已经迅速增长，约二十年后引入了计算系统中的上下文意识。通过考虑设备、用户和社会的情况，上下文意识计算已经启动了一系列创新应用，例如协助生活、位置基于的社交网络服务等。为了识别上下文和根据此作出决策，人工智能技术，如 Ontology 和 OWL，已经被采用来表示上下文建模和推理。随着 LL M 的崛起和其改善的自然语言理解和推理能力，现在可以使用自然语言来建模上下文并通过与 LL M 交互，如 ChatGPT 和 GPT-4，进行上下文推理。在这个教程中，我们示例了使用文本、提示和自动代理（AutoAgent）来帮助 LL M 进行上下文建模和推理，不需要模型调整。我们组织和介绍相关领域的工作，并统称这个计算模式为 LLM-驱动的上下文意识计算（LCaC）。在 LCaC 模型中，用户的请求、感应器读取数据和 Command 到 actuator 是 supposed 为文本表示。当 AutoAgent 使用文本提示模型上下文时，LLM 将进行上下文推理，生成动作计划，并对 AutoAgent 回应。AutoAgent 接着根据动作计划进行行动，以实现上下文意识。为证明概念，我们使用了两个示例：在公寓内运作一个移动的 z-臂来协助生活，以及在上下文意识和个性化的方式规划旅行。
</details></li>
</ul>
<hr>
<h2 id="Guided-Cooperation-in-Hierarchical-Reinforcement-Learning-via-Model-based-Rollout"><a href="#Guided-Cooperation-in-Hierarchical-Reinforcement-Learning-via-Model-based-Rollout" class="headerlink" title="Guided Cooperation in Hierarchical Reinforcement Learning via Model-based Rollout"></a>Guided Cooperation in Hierarchical Reinforcement Learning via Model-based Rollout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13508">http://arxiv.org/abs/2309.13508</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haoranwang-tj/gcmr_aclg_official">https://github.com/haoranwang-tj/gcmr_aclg_official</a></li>
<li>paper_authors: Haoran Wang, Yaoru Sun, Fang Wang, Yeming Chen</li>
<li>for: 这个论文的目的是提出一种goal-conditioned层次强化学习（HRL）框架，以便在复杂的长期强化学习任务中实现有效的探索。</li>
<li>methods: 这个论文使用了一种名为Guided Cooperation via Model-based Rollout（GCMR）的方法，该方法通过估算前向动力学来促进层次协作。此外，论文还使用了一种一步滚动计划来进一步促进层次协作。</li>
<li>results: 实验结果表明，将GCMR框架与ACLG（一种分离变体的HIGL）结合使用，可以比基eline和之前的状态 искусственный风险（SOTA）层次强化学习算法更加稳定和可靠地改进政策。<details>
<summary>Abstract</summary>
Goal-conditioned hierarchical reinforcement learning (HRL) presents a promising approach for enabling effective exploration in complex long-horizon reinforcement learning (RL) tasks via temporal abstraction. Yet, most goal-conditioned HRL algorithms focused on the subgoal discovery, regardless of inter-level coupling. In essence, for hierarchical systems, the increased inter-level communication and coordination can induce more stable and robust policy improvement. Here, we present a goal-conditioned HRL framework with Guided Cooperation via Model-based Rollout (GCMR), which estimates forward dynamics to promote inter-level cooperation. The GCMR alleviates the state-transition error within off-policy correction through a model-based rollout, further improving the sample efficiency. Meanwhile, to avoid being disrupted by these corrected but possibly unseen or faraway goals, lower-level Q-function gradients are constrained using a gradient penalty with a model-inferred upper bound, leading to a more stable behavioral policy. Besides, we propose a one-step rollout-based planning to further facilitate inter-level cooperation, where the higher-level Q-function is used to guide the lower-level policy by estimating the value of future states so that global task information is transmitted downwards to avoid local pitfalls. Experimental results demonstrate that incorporating the proposed GCMR framework with ACLG, a disentangled variant of HIGL, yields more stable and robust policy improvement than baselines and substantially outperforms previous state-of-the-art (SOTA) HRL algorithms in both hard-exploration problems and robotic control.
</details>
<details>
<summary>摘要</summary>
目标受控层次学习（HRL）提供了一种有效的探索方法，用于复杂的长期回归学习（RL）任务。然而，大多数目标受控HRL算法都专注于发现子目标，忽略了层次之间的交互。实际上，在层次系统中，增加层次之间的通信和协调可以提高稳定和可靠的策略改进。我们提出了一种目标受控HRL框架，称为指导合作via模型基于滚动（GCMR），该框架利用前向动力学预测来促进层次之间的合作。GCMR通过模型基于滚动来减少状态转移错误，从而提高样本效率。此外，我们还提出了一种一步滚动规划方法，以便更好地协调层次之间的行为策略。在这种方法中，高层Q函数用于指导低层策略，并且通过估算未来状态的值来传递全局任务信息下来，以避免地方坑拥。实验结果表明，将我们提出的GCMR框架与ACLG（一种分离的HIGL变体）结合使用，可以获得更稳定和可靠的策略改进，并在硬探索问题和机器人控制方面实现substantially outperform前一个状态的艺术algorithm。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/cs.AI_2023_09_24/" data-id="clp88dbou004fob88d6hhgcmf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/cs.CL_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T11:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/24/cs.CL_2023_09_24/">cs.CL - 2023-09-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Text-Classification-A-Perspective-of-Deep-Learning-Methods"><a href="#Text-Classification-A-Perspective-of-Deep-Learning-Methods" class="headerlink" title="Text Classification: A Perspective of Deep Learning Methods"></a>Text Classification: A Perspective of Deep Learning Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13761">http://arxiv.org/abs/2309.13761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brijkishorsoni1210/Car-logo-classification">https://github.com/brijkishorsoni1210/Car-logo-classification</a></li>
<li>paper_authors: Zhongwei Wan</li>
<li>for: 本文旨在探讨深度学习方法在文本分类任务中的应用，以提高文本分类的准确率和效率。</li>
<li>methods: 本文 introduce了深度学习基于文本分类算法，包括特征提取、特征缩放和评价策略等重要步骤。</li>
<li>results: 文中对多种深度学习文本分类方法进行比较和总结，以便选择合适的方法 для实际应用。<details>
<summary>Abstract</summary>
In recent years, with the rapid development of information on the Internet, the number of complex texts and documents has increased exponentially, which requires a deeper understanding of deep learning methods in order to accurately classify texts using deep learning techniques, and thus deep learning methods have become increasingly important in text classification. Text classification is a class of tasks that automatically classifies a set of documents into multiple predefined categories based on their content and subject matter. Thus, the main goal of text classification is to enable users to extract information from textual resources and process processes such as retrieval, classification, and machine learning techniques together in order to classify different categories. Many new techniques of deep learning have already achieved excellent results in natural language processing. The success of these learning algorithms relies on their ability to understand complex models and non-linear relationships in data. However, finding the right structure, architecture, and techniques for text classification is a challenge for researchers. This paper introduces deep learning-based text classification algorithms, including important steps required for text classification tasks such as feature extraction, feature reduction, and evaluation strategies and methods. At the end of the article, different deep learning text classification methods are compared and summarized.
</details>
<details>
<summary>摘要</summary>
Recently, with the rapid development of information on the internet, the number of complex texts and documents has increased exponentially, requiring a deeper understanding of deep learning methods to accurately classify texts using deep learning techniques. As a result, deep learning methods have become increasingly important in text classification. Text classification is a type of task that automatically classifies a set of documents into multiple predefined categories based on their content and subject matter. Therefore, the main goal of text classification is to enable users to extract information from textual resources and perform processes such as retrieval, classification, and machine learning techniques together to classify different categories. Many new techniques of deep learning have already achieved excellent results in natural language processing. The success of these learning algorithms relies on their ability to understand complex models and non-linear relationships in data. However, finding the right structure, architecture, and techniques for text classification is a challenge for researchers. This paper introduces deep learning-based text classification algorithms, including important steps required for text classification tasks such as feature extraction, feature reduction, and evaluation strategies and methods. At the end of the article, different deep learning text classification methods are compared and summarized.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese writing systems. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Does-the-“most-sinfully-decadent-cake-ever”-taste-good-Answering-Yes-No-Questions-from-Figurative-Contexts"><a href="#Does-the-“most-sinfully-decadent-cake-ever”-taste-good-Answering-Yes-No-Questions-from-Figurative-Contexts" class="headerlink" title="Does the “most sinfully decadent cake ever” taste good? Answering Yes&#x2F;No Questions from Figurative Contexts"></a>Does the “most sinfully decadent cake ever” taste good? Answering Yes&#x2F;No Questions from Figurative Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13748">http://arxiv.org/abs/2309.13748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geetanjali Rakshit, Jeffrey Flanigan</li>
<li>for:  investigate the robustness of Question Answering (QA) models on figurative text</li>
<li>methods:  use yes&#x2F;no questions with figurative and non-figurative contexts to test the models’ ability to understand figurative language</li>
<li>results:  state-of-the-art BERT-based QA models perform poorly on figurative contexts, but models like GPT-3 and ChatGPT can handle them better, and further performance gains can be achieved by automatically simplifying the figurative contexts.<details>
<summary>Abstract</summary>
Figurative language is commonplace in natural language, and while making communication memorable and creative, can be difficult to understand. In this work, we investigate the robustness of Question Answering (QA) models on figurative text. Yes/no questions, in particular, are a useful probe of figurative language understanding capabilities of large language models. We propose FigurativeQA, a set of 1000 yes/no questions with figurative and non-figurative contexts, extracted from the domains of restaurant and product reviews. We show that state-of-the-art BERT-based QA models exhibit an average performance drop of up to 15\% points when answering questions from figurative contexts, as compared to non-figurative ones. While models like GPT-3 and ChatGPT are better at handling figurative texts, we show that further performance gains can be achieved by automatically simplifying the figurative contexts into their non-figurative (literal) counterparts. We find that the best overall model is ChatGPT with chain-of-thought prompting to generate non-figurative contexts. Our work provides a promising direction for building more robust QA models with figurative language understanding capabilities.
</details>
<details>
<summary>摘要</summary>
通用语言中的比喻语言非常普遍，它可以使交流更加生动、创新，但同时也可以使得理解变得更加困难。在这项工作中，我们研究了问答模型对比喻文本的Robustness。特别是yes/no问题，是 figural语言理解能力的一种有用的检验。我们提出了一个名为FigurativeQA的1000个yes/no问题的集合，其中包括了餐厅和产品评论中的figural和非 figural上下文。我们发现，当问答模型回答figural上下文中的问题时，其性能会下降15%左右，相比于非 figural上下文。虽然模型如GPT-3和ChatGPT能够更好地处理figural语言，但我们发现可以通过自动将figural上下文简化成非 figural（literal）上下文来提高性能。我们发现最佳的模型是ChatGPT加chain-of-thought提示，可以生成非 figural上下文。我们的工作提供了构建更加Robust的问答模型的可能方向。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Relations-Classification-using-Imbalanced-Predictions-Adaptation"><a href="#Multiple-Relations-Classification-using-Imbalanced-Predictions-Adaptation" class="headerlink" title="Multiple Relations Classification using Imbalanced Predictions Adaptation"></a>Multiple Relations Classification using Imbalanced Predictions Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13718">http://arxiv.org/abs/2309.13718</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sa5r/mrca">https://github.com/sa5r/mrca</a></li>
<li>paper_authors: Sakher Khalil Alqaaidi, Elika Bozorgi, Krzysztof J. Kochut</li>
<li>for: 这个论文主要用于关系分类任务中处理多个关系的问题。</li>
<li>methods: 该模型使用自定义输出架构和采用额外输入特征来解决不均匀预测问题。</li>
<li>results: 对于一些常用的数据集，模型表现出了显著的改善，尤其是在处理不均匀预测的情况下。<details>
<summary>Abstract</summary>
The relation classification task assigns the proper semantic relation to a pair of subject and object entities; the task plays a crucial role in various text mining applications, such as knowledge graph construction and entities interaction discovery in biomedical text. Current relation classification models employ additional procedures to identify multiple relations in a single sentence. Furthermore, they overlook the imbalanced predictions pattern. The pattern arises from the presence of a few valid relations that need positive labeling in a relatively large predefined relations set. We propose a multiple relations classification model that tackles these issues through a customized output architecture and by exploiting additional input features. Our findings suggest that handling the imbalanced predictions leads to significant improvements, even on a modest training design. The results demonstrate superiority performance on benchmark datasets commonly used in relation classification. To the best of our knowledge, this work is the first that recognizes the imbalanced predictions within the relation classification task.
</details>
<details>
<summary>摘要</summary>
“关系分类任务是将对象和主题实体对应的Semantic关系分类为正确的类别，这个任务在文本挖掘应用中扮演着关键角色，如知识图构建和生物医学文本中实体互动发现。现有关系分类模型采用多种方法来识别单句中的多个关系，但它们忽略了不均匀预测模式。这种模式来自于一些有效的关系，它们需要在大量预定的关系集中得到正面标注。我们提出了一种多关系分类模型，通过自定义输出架构和采用额外输入特征来解决这些问题。我们的发现表明，处理不均匀预测可以取得显著改善，即使在较小的训练设计下。结果表明我们的模型在常用的 benchmark 数据集上显示出了优秀的表现，并且根据我们所知，这是第一个认可关系分类任务中的不均匀预测问题的研究。”
</details></li>
</ul>
<hr>
<h2 id="MentaLLaMA-Interpretable-Mental-Health-Analysis-on-Social-Media-with-Large-Language-Models"><a href="#MentaLLaMA-Interpretable-Mental-Health-Analysis-on-Social-Media-with-Large-Language-Models" class="headerlink" title="MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models"></a>MentaLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13567">http://arxiv.org/abs/2309.13567</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevekgyang/mentallama">https://github.com/stevekgyang/mentallama</a></li>
<li>paper_authors: Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Sophia Ananiadou, Jimin Huang</li>
<li>for: 这个论文的目的是为了提供一个可解释的心理健康分析方法，使用大语言模型来提供详细的解释，并在社交媒体上进行自动心理健康分析。</li>
<li>methods: 这个论文使用了ChatGPT来生成可解释的回答，并使用专家写的少量提示来提高模型的性能。</li>
<li>results: 研究结果显示，MentalLLaMA可以与状态艺术方法匹配，并且生成高质量的解释。<details>
<summary>Abstract</summary>
With the development of web technology, social media texts are becoming a rich source for automatic mental health analysis. As traditional discriminative methods bear the problem of low interpretability, the recent large language models have been explored for interpretable mental health analysis on social media, which aims to provide detailed explanations along with predictions. The results show that ChatGPT can generate approaching-human explanations for its correct classifications. However, LLMs still achieve unsatisfactory classification performance in a zero-shot/few-shot manner. Domain-specific finetuning is an effective solution, but faces 2 challenges: 1) lack of high-quality training data. 2) no open-source LLMs for interpretable mental health analysis were released to lower the finetuning cost. To alleviate these problems, we build the first multi-task and multi-source interpretable mental health instruction (IMHI) dataset on social media, with 105K data samples. The raw social media data are collected from 10 existing sources covering 8 mental health analysis tasks. We use expert-written few-shot prompts and collected labels to prompt ChatGPT and obtain explanations from its responses. To ensure the reliability of the explanations, we perform strict automatic and human evaluations on the correctness, consistency, and quality of generated data. Based on the IMHI dataset and LLaMA2 foundation models, we train MentalLLaMA, the first open-source LLM series for interpretable mental health analysis with instruction-following capability. We also evaluate the performance of MentalLLaMA on the IMHI evaluation benchmark with 10 test sets, where their correctness for making predictions and the quality of explanations are examined. The results show that MentalLLaMA approaches state-of-the-art discriminative methods in correctness and generates high-quality explanations.
</details>
<details>
<summary>摘要</summary>
随着网络技术的发展，社交媒体文本正在成为自动心理健康分析的丰富来源。传统的排除性方法具有低可解释性问题，而最近的大语言模型在社交媒体上进行可解释心理健康分析，旨在提供详细的解释以及预测。结果显示，ChatGPT可以生成接近人类解释的正确分类结果。然而，LLMs仍在零容量/几容量情况下实现不满足的分类性能。预处理特定领域的训练是有效的解决方案，但面临两个挑战：1）缺乏高质量训练数据。2）没有开源LLMs для可解释心理健康分析，以降低训练成本。为解决这些问题，我们建立了首个多任务多源可解释心理健康指令（IMHI）数据集，包括105W个数据样本。 raw社交媒体数据由10个现有源收集，覆盖8个心理健康分析任务。我们使用专家写的少量示例和收集的标签来提取ChatGPT的回答，并对其生成的数据进行严格的自动和人类评估，以确保数据的正确性、一致性和质量。基于IMHI数据集和LLaMA2基础模型，我们训练了心理LLaMA，首个开源LLM系列 для可解释心理健康分析，并实现了指令遵循能力。我们还对IMHI评估标准测试集进行了10个测试集的性能评估，其中正确性和生成的解释质量均进行了评估。结果显示，心理LLaMA与状态艺术方法相当，并生成高质量的解释。
</details></li>
</ul>
<hr>
<h2 id="Substituting-Data-Annotation-with-Balanced-Updates-and-Collective-Loss-in-Multi-label-Text-Classification"><a href="#Substituting-Data-Annotation-with-Balanced-Updates-and-Collective-Loss-in-Multi-label-Text-Classification" class="headerlink" title="Substituting Data Annotation with Balanced Updates and Collective Loss in Multi-label Text Classification"></a>Substituting Data Annotation with Balanced Updates and Collective Loss in Multi-label Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13543">http://arxiv.org/abs/2309.13543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muberra Ozmen, Joseph Cotnareanu, Mark Coates</li>
<li>for: 这篇论文的目的是解决多标签文本分类（MLTC）任务，并且在没有足够标签数据的情况下进行分类。</li>
<li>methods: 本篇论文使用了自然语言推理来将输入文本转换为初步的标签可能性分布，然后使用标签描述来计算一个标签依赖关系表，最后使用讯息传递法更新初步的标签可能性分布，使用一个集体损失函数来注入预期的标签频率和预期的多标签卡дина优化。</li>
<li>results: 实验结果显示，提案的框架在具有仅具有几个标签的低监控情况下可以获得有效的性能，并且相比使用预训语言模型时，提案的方法可以提高性能 BY 70%。<details>
<summary>Abstract</summary>
Multi-label text classification (MLTC) is the task of assigning multiple labels to a given text, and has a wide range of application domains. Most existing approaches require an enormous amount of annotated data to learn a classifier and/or a set of well-defined constraints on the label space structure, such as hierarchical relations which may be complicated to provide as the number of labels increases. In this paper, we study the MLTC problem in annotation-free and scarce-annotation settings in which the magnitude of available supervision signals is linear to the number of labels. Our method follows three steps, (1) mapping input text into a set of preliminary label likelihoods by natural language inference using a pre-trained language model, (2) calculating a signed label dependency graph by label descriptions, and (3) updating the preliminary label likelihoods with message passing along the label dependency graph, driven with a collective loss function that injects the information of expected label frequency and average multi-label cardinality of predictions. The experiments show that the proposed framework achieves effective performance under low supervision settings with almost imperceptible computational and memory overheads added to the usage of pre-trained language model outperforming its initial performance by 70\% in terms of example-based F1 score.
</details>
<details>
<summary>摘要</summary>
多标签文本分类（MLTC）是将多个标签分配给一个文本的任务，具有广泛的应用领域。大多数现有方法需要巨大量的注释数据来学习一个分类器和/或一组定义的约束，例如层次关系，这些约束可能会变得复杂，特别是当标签数量增加时。在这篇论文中，我们研究了在无注释和缺乏注释的设置下进行MLTC问题。我们的方法包括以下三步：1. 将输入文本映射到一组初步的标签可能性，使用一个预训练的自然语言模型进行自然语言推理。2. 计算一个签名标签依赖图，使用标签描述来计算。3. 更新初步的标签可能性，使用消息传递算法在标签依赖图上进行更新，驱动一个集体损失函数，该函数注入预期的标签频率和预测多个标签 cardinality的信息。实验表明，我们的框架在低级注释设置下达到了有效性，并且增加了非常小的计算和存储开销，相对于使用预训练自然语言模型的使用，提高了70%的例子基于F1分数。
</details></li>
</ul>
<hr>
<h2 id="The-Study-of-Perceptual-Training-of-Chinese-Mandarin-Tones-for-Monolingual-Speakers-of-English-Using-Adaptive-Computer-Based-Training-Software"><a href="#The-Study-of-Perceptual-Training-of-Chinese-Mandarin-Tones-for-Monolingual-Speakers-of-English-Using-Adaptive-Computer-Based-Training-Software" class="headerlink" title="The Study of Perceptual Training of Chinese Mandarin Tones for Monolingual Speakers of English Using Adaptive Computer Based Training Software"></a>The Study of Perceptual Training of Chinese Mandarin Tones for Monolingual Speakers of English Using Adaptive Computer Based Training Software</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13513">http://arxiv.org/abs/2309.13513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuke Wang</li>
<li>for: 这个研究探讨了一种新的声调训练技术，可能对第二语言学习和声调训练产生积极影响。</li>
<li>methods: 该研究使用了一种新的声调训练技术，该技术基于语音识别和生成技术，可以帮助学生更好地学习和理解声调。</li>
<li>results: 研究发现，使用该新技术可以提高学生对声调的识别和生成能力，并且可以帮助学生更好地理解和使用声调。<details>
<summary>Abstract</summary>
The study explored a new technique of phonetic tone training, which may have a positive impact on second language learning and tone training.
</details>
<details>
<summary>摘要</summary>
研究探讨了一种新的声音训练技巧，这种技巧可能对第二语言学习和声音训练产生积极影响。Here's a breakdown of the translation:研究 (study)探讨 (explored)一种 (a new)声音 (phonetic)训练 (training)技巧 (technique)可能 (may)对 (positive impact on)第二语言 (second language)学习 (learning)和 (and)声音 (tone)训练 (training)产生 (have a positive impact)
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/cs.CL_2023_09_24/" data-id="clp88dbto00caob8822l19iqd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/cs.LG_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T10:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/24/cs.LG_2023_09_24/">cs.LG - 2023-09-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Design-Principles-of-Robust-Multi-Armed-Bandit-Framework-in-Video-Recommendations"><a href="#Design-Principles-of-Robust-Multi-Armed-Bandit-Framework-in-Video-Recommendations" class="headerlink" title="Design Principles of Robust Multi-Armed Bandit Framework in Video Recommendations"></a>Design Principles of Robust Multi-Armed Bandit Framework in Video Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.01419">http://arxiv.org/abs/2310.01419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Belhassen Bayar, Phanideep Gampa, Ainur Yessenalina, Zhen Wen</li>
<li>for: 提出了一种新的多臂弓箭推荐系统设计原则，以抵御时变metadata信号的影响，避免item杀死和数据稀缺导致的弓箭模型异常。</li>
<li>methods: 提出了三种设计原则，包括：一、使用时变metadata信号进行适应；二、避免item杀死和数据稀缺导致弓箭模型异常；三、避免弓箭模型 weights 频繁变化。</li>
<li>results: 通过系列实验，证明了提出的设计原则的优势，包括：在ROC-AUC和PR-AUC中提高了相对增量达到11.88%和44.85%，并在推荐特定受欢迎和不受欢迎标题时保持了公平性。<details>
<summary>Abstract</summary>
Current multi-armed bandit approaches in recommender systems (RS) have focused more on devising effective exploration techniques, while not adequately addressing common exploitation challenges related to distributional changes and item cannibalization. Little work exists to guide the design of robust bandit frameworks that can address these frequent challenges in RS. In this paper, we propose a new design principles to (i) make bandit models robust to time-variant metadata signals, (ii) less prone to item cannibalization, and (iii) prevent their weights fluctuating due to data sparsity. Through a series of experiments, we systematically examine the influence of several important bandit design choices. We demonstrate the advantage of our proposed design principles at making bandit models robust to dynamic behavioral changes through in-depth analyses. Noticeably, we show improved relative gain compared to a baseline bandit model not incorporating our design choices of up to $11.88\%$ and $44.85\%$, respectively in ROC-AUC and PR-AUC. Case studies about fairness in recommending specific popular and unpopular titles are presented, to demonstrate the robustness of our proposed design at addressing popularity biases.
</details>
<details>
<summary>摘要</summary>
当前多臂罂缸方法在推荐系统（RS）中更多地关注了发展有效探索技术，而不够注意常见的利用探索挑战，如分布变化和物品吃掉。现有的工作不够引导设计Robust罂缸框架，以解决这些常见挑战。在这篇论文中，我们提出了一些新的设计原则，以使罂缸模型更加Robust于时变元数据信号， menos可害性和数据稀缺性。通过一系列实验，我们系统地检验了一些重要的罂缸设计选择的影响。我们示出了我们提出的设计原则的优势，使罂缸模型更加Robust于动态行为变化，并提高了相对增量比例，分别为11.88%和44.85%。我们还对推荐特定受欢迎和不受欢迎标题的公平性进行了案例研究，以示出我们的设计方法能够解决受欢迎性偏见。
</details></li>
</ul>
<hr>
<h2 id="The-Rashomon-Importance-Distribution-Getting-RID-of-Unstable-Single-Model-based-Variable-Importance"><a href="#The-Rashomon-Importance-Distribution-Getting-RID-of-Unstable-Single-Model-based-Variable-Importance" class="headerlink" title="The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance"></a>The Rashomon Importance Distribution: Getting RID of Unstable, Single Model-based Variable Importance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13775">http://arxiv.org/abs/2309.13775</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jdonnelly36/Rashomon_Importance_Distribution">https://github.com/jdonnelly36/Rashomon_Importance_Distribution</a></li>
<li>paper_authors: Jon Donnelly, Srikar Katta, Cynthia Rudin, Edward P. Browne</li>
<li>for: 这paper的目的是提出一种新的变量重要性评估框架，以便在不同的模型和数据集中Quantifying variable importance，并且可以针对不同的数据分布进行稳定的评估。</li>
<li>methods: 这paper使用了一种新的变量重要性评估方法，它可以考虑所有可能的解释，并且可以在不同的数据分布下保持稳定性。这个方法可以与大多数现有的模型类型和全局变量重要性指标集成。</li>
<li>results: 实验表明，这paper的方法可以在复杂的模拟场景中成功地评估变量重要性，并且可以准确地估计变量重要性的真实排名。此外，这paper还提供了理论保证和 finite sample error rates的分析，以及一个实际案例研究，以证明这paper的方法在实际应用中的效用。<details>
<summary>Abstract</summary>
Quantifying variable importance is essential for answering high-stakes questions in fields like genetics, public policy, and medicine. Current methods generally calculate variable importance for a given model trained on a given dataset. However, for a given dataset, there may be many models that explain the target outcome equally well; without accounting for all possible explanations, different researchers may arrive at many conflicting yet equally valid conclusions given the same data. Additionally, even when accounting for all possible explanations for a given dataset, these insights may not generalize because not all good explanations are stable across reasonable data perturbations. We propose a new variable importance framework that quantifies the importance of a variable across the set of all good models and is stable across the data distribution. Our framework is extremely flexible and can be integrated with most existing model classes and global variable importance metrics. We demonstrate through experiments that our framework recovers variable importance rankings for complex simulation setups where other methods fail. Further, we show that our framework accurately estimates the true importance of a variable for the underlying data distribution. We provide theoretical guarantees on the consistency and finite sample error rates for our estimator. Finally, we demonstrate its utility with a real-world case study exploring which genes are important for predicting HIV load in persons with HIV, highlighting an important gene that has not previously been studied in connection with HIV. Code is available here.
</details>
<details>
<summary>摘要</summary>
量化变量重要性是解决高负荷问题的关键在遗传学、公共政策和医学等领域。现有的方法通常计算变量重要性为给定模型和给定数据集中的。然而，对于给定数据集，可能有多个模型都能够准确地预测目标结果，而不同的研究人员可能因数据而得出不同的、尚未得到证明的结论。此外，即使考虑所有可能的解释，这些发现也可能不会普适化，因为不同的数据变换可能会导致不同的优秀模型。我们提出了一个新的变量重要性框架，可以评估变量重要性在所有好的模型中，并且在数据分布下是稳定的。我们的框架非常灵活，可以与大多数现有的模型类型和全局变量重要性度量结合使用。我们通过实验表明，我们的框架可以在复杂的模拟设置中成功地重新分配变量重要性。此外，我们证明了我们的框架可以准确地估计变量重要性的真实值，并提供了理论保证变量重要性的一致性和 finite sample error rate。最后，我们通过一个实际的案例研究，探讨了抑制HIV荷重的关键基因，并发现了一个没有在HIV相关研究中受到过关注的重要基因。代码可以在这里找到。
</details></li>
</ul>
<hr>
<h2 id="Improving-Robustness-of-Deep-Convolutional-Neural-Networks-via-Multiresolution-Learning"><a href="#Improving-Robustness-of-Deep-Convolutional-Neural-Networks-via-Multiresolution-Learning" class="headerlink" title="Improving Robustness of Deep Convolutional Neural Networks via Multiresolution Learning"></a>Improving Robustness of Deep Convolutional Neural Networks via Multiresolution Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13752">http://arxiv.org/abs/2309.13752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyan Zhou, Yao Liang</li>
<li>for: 提高深度学习模型的鲁棒性，包括对1D信号和2D图像预测问题的鲁棒性。</li>
<li>methods: 使用多分辨率学习，并证明多分辨率学习可以显著提高深度学习模型的鲁棒性，包括随机噪声和敌意攻击的鲁棒性。</li>
<li>results: our results show that multiresolution learning can significantly improve the robustness of DNN models for both 1D signal and 2D signal (image) prediction problems, and that this improvement can be achieved with small training dataset size and without sacrificing standard accuracy.<details>
<summary>Abstract</summary>
The current learning process of deep learning, regardless of any deep neural network (DNN) architecture and/or learning algorithm used, is essentially a single resolution training. We explore multiresolution learning and show that multiresolution learning can significantly improve robustness of DNN models for both 1D signal and 2D signal (image) prediction problems. We demonstrate this improvement in terms of both noise and adversarial robustness as well as with small training dataset size. Our results also suggest that it may not be necessary to trade standard accuracy for robustness with multiresolution learning, which is, interestingly, contrary to the observation obtained from the traditional single resolution learning setting.
</details>
<details>
<summary>摘要</summary>
当前深度学习的学习过程，无论使用哪种深度神经网络（DNN）架构和学习算法，都是单分辨率训练。我们研究多分辨率学习，并证明多分辨率学习可以显著提高深度神经网络模型对1D信号和2D图像预测问题的鲁棒性。我们通过对噪声和攻击性诊断的改进来证明这一点，同时也发现了训练集大小的影响。我们的结果还表明，在多分辨率学习Setting中，可能不需要在标准准确率和鲁棒性之间进行权衡，这与传统单分辨率学习Setting中所获得的观察相反。
</details></li>
</ul>
<hr>
<h2 id="Generative-Residual-Diffusion-Modeling-for-Km-scale-Atmospheric-Downscaling"><a href="#Generative-Residual-Diffusion-Modeling-for-Km-scale-Atmospheric-Downscaling" class="headerlink" title="Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling"></a>Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15214">http://arxiv.org/abs/2309.15214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morteza Mardani, Noah Brenowitz, Yair Cohen, Jaideep Pathak, Chieh-Yu Chen, Cheng-Chin Liu, Arash Vahdat, Karthik Kashinath, Jan Kautz, Mike Pritchard</li>
<li>for: 这篇论文旨在提供一种可靠且cost-effective的物理隐患预测方法，以取代现有的 expensive km-scale数值 simulations。</li>
<li>methods: 这篇论文使用了一种叫做 ResDiff 的 two-step方法，其中第一步使用了一个（UNet）回归模型预测 Mean，第二步使用了一个扩散模型预测 Residual。这个方法可以对不同的物理过程和不同的尺度进行适当的调整。</li>
<li>results: 这篇论文的结果显示了 ResDiff 方法在 bulk RMSE 和 CRPS  scores 方面表现出了鼓舞人的能力。它还可以实现准确地预测风暴中的重要力学特征，例如降水和风速的分布。case studies 也显示了 ResDiff 方法在不同的气候现象中的适当运作。<details>
<summary>Abstract</summary>
The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phenomena reveal appropriate multivariate relationships reminiscent of learnt physics. This includes the sharp wind and temperature variations that co-locate with intense rainfall in a cold front, and the extreme winds and rainfall bands that surround the eyewall of typhoons. Some evidence of simultaneous bias correction is found. A first attempt at downscaling directly from an operational global forecast model successfully retains many of these benefits. The implication is that a new era of fully end-to-end, global-to-regional machine learning weather prediction is likely near at hand.
</details>
<details>
<summary>摘要</summary>
现代物理危机预测技术需要使用高resolution数值 simulate，这些 simulate 通常需要很多的计算资源和高resolution的全球输入数据。在这篇文章中，我们提出了一种cost-effective的km级下采 diffusion模型，作为一种alternative。这个模型在台湾地区高resolution天气模型上训练，并使用ERA5分析数据进行条件。为了 Addressing downscaling uncertainties, we employ a two-step approach（ResDiff），其中一个（UNet）回归预报mean，而另一个是diffusion模型预报差异。ResDiff exhibits encouraging skill in bulk RMSE和CRPS分数。预测的spectrum和分布从ResDiff faithful recover了重要的power law关系，这些关系控制了wind和rain extrema的formation。case studies of coherent weather phenomena reveal appropriate multivariate relationships reminiscent of learnt physics, such as the sharp wind and temperature variations that co-locate with intense rainfall in a cold front, and the extreme winds and rainfall bands that surround the eyewall of typhoons。有些证据表明同时进行偏差修正。我们首次尝试了直接从运行的全球预测模型下采，成功保留了大多数的优点。这表明一个全新的end-to-end, global-to-regional机器学习天气预测时代可能即将到来。
</details></li>
</ul>
<hr>
<h2 id="Geometry-of-Linear-Neural-Networks-Equivariance-and-Invariance-under-Permutation-Groups"><a href="#Geometry-of-Linear-Neural-Networks-Equivariance-and-Invariance-under-Permutation-Groups" class="headerlink" title="Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups"></a>Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13736">http://arxiv.org/abs/2309.13736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kathlén Kohn, Anna-Laura Sattelberger, Vahid Shahverdi</li>
<li>for:  investigate the subvariety of functions that are equivariant or invariant under the action of a permutation group.</li>
<li>methods: explicit description of their dimension, degree, Euclidean distance degree, and singularities.</li>
<li>results: fully characterize invariance for arbitrary permutation groups, and equivariance for cyclic groups, and prove that all invariant linear functions can be learned by linear autoencoders.Here’s the same information in Traditional Chinese:</li>
<li>for: 研究对应 Permutation group 的函数子集，包括对称函数和对称函数。</li>
<li>methods: 提供对应函数的精确描述，包括次数、度量、欧几何级数和缺陷。</li>
<li>results: 完全描述任意 Permutation group 的对称性，以及循环群的对称性，并证明所有对称函数可以通过线性自动化学习。<details>
<summary>Abstract</summary>
The set of functions parameterized by a linear fully-connected neural network is a determinantal variety. We investigate the subvariety of functions that are equivariant or invariant under the action of a permutation group. Examples of such group actions are translations or $90^\circ$ rotations on images. For such equivariant or invariant subvarieties, we provide an explicit description of their dimension, their degree as well as their Euclidean distance degree, and their singularities. We fully characterize invariance for arbitrary permutation groups, and equivariance for cyclic groups. We draw conclusions for the parameterization and the design of equivariant and invariant linear networks, such as a weight sharing property, and we prove that all invariant linear functions can be learned by linear autoencoders.
</details>
<details>
<summary>摘要</summary>
Set of functions parameterized by linear fully-connected neural network is a determinantal variety. We investigate subvariety of functions that are equivariant or invariant under action of permutation group. Examples of such group actions include translations or $90^\circ$ rotations on images. For such equivariant or invariant subvarieties, we provide explicit description of their dimension, degree, Euclidean distance degree, and singularities. We fully characterize invariance for arbitrary permutation groups and equivariance for cyclic groups. We draw conclusions for parameterization and design of equivariant and invariant linear networks, including weight sharing property, and prove that all invariant linear functions can be learned by linear autoencoders.
</details></li>
</ul>
<hr>
<h2 id="Towards-Tuning-Free-Minimum-Volume-Nonnegative-Matrix-Factorization"><a href="#Towards-Tuning-Free-Minimum-Volume-Nonnegative-Matrix-Factorization" class="headerlink" title="Towards Tuning-Free Minimum-Volume Nonnegative Matrix Factorization"></a>Towards Tuning-Free Minimum-Volume Nonnegative Matrix Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13733">http://arxiv.org/abs/2309.13733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc Toan Nguyen, Eric C. Chi</li>
<li>for: 这篇论文主要是为了探讨缺量矩阵因子分解（NMF）在数据矩阵中发现隐藏结构的方法。</li>
<li>methods: 这篇论文提出了一种基于最小体积的NMF方法，可以在噪声存在的情况下可靠地还原缺量矩阵。</li>
<li>results: 这篇论文提出了一种不需要选择医学参数的NMF方法，并且提供了一种基于主化最小化的逐步算法来实现。 employing this method, the authors show that the optimal choice of the tuning parameter is insensitive to the noise level in the data.<details>
<summary>Abstract</summary>
Nonnegative Matrix Factorization (NMF) is a versatile and powerful tool for discovering latent structures in data matrices, with many variations proposed in the literature. Recently, Leplat et al.\@ (2019) introduced a minimum-volume NMF for the identifiable recovery of rank-deficient matrices in the presence of noise. The performance of their formulation, however, requires the selection of a tuning parameter whose optimal value depends on the unknown noise level. In this work, we propose an alternative formulation of minimum-volume NMF inspired by the square-root lasso and its tuning-free properties. Our formulation also requires the selection of a tuning parameter, but its optimal value does not depend on the noise level. To fit our NMF model, we propose a majorization-minimization (MM) algorithm that comes with global convergence guarantees. We show empirically that the optimal choice of our tuning parameter is insensitive to the noise level in the data.
</details>
<details>
<summary>摘要</summary>
非负矩阵分解（NMF）是一种多变性强大的工具，用于找到数据矩阵中隐藏的结构，文献中有多种提案。最近，Leplat等人（2019）提出了一种可 identificable 的 minimum-volume NMF，用于在噪声存在的情况下 recuperate 缺rank 矩阵。然而，其表现需要选择一个调整参数，该参数的优化值取决于未知的噪声水平。在这篇文章中，我们提出了一种基于平方减法和其调整参数不виси的 minimum-volume NMF 形式化。我们的形式化也需要选择一个调整参数，但该参数的优化值不取决于噪声水平。为了适应我们的 NMF 模型，我们提出了一种majorization-minimization（MM）算法，该算法来with global convergence guarantees。我们通过实验表明，我们的调整参数的优化值对噪声水平的影响不大。
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-networks-with-ReLU-leaky-ReLU-and-softplus-activation-provably-overcome-the-curse-of-dimensionality-for-Kolmogorov-partial-differential-equations-with-Lipschitz-nonlinearities-in-the-L-p-sense"><a href="#Deep-neural-networks-with-ReLU-leaky-ReLU-and-softplus-activation-provably-overcome-the-curse-of-dimensionality-for-Kolmogorov-partial-differential-equations-with-Lipschitz-nonlinearities-in-the-L-p-sense" class="headerlink" title="Deep neural networks with ReLU, leaky ReLU, and softplus activation provably overcome the curse of dimensionality for Kolmogorov partial differential equations with Lipschitz nonlinearities in the $L^p$-sense"></a>Deep neural networks with ReLU, leaky ReLU, and softplus activation provably overcome the curse of dimensionality for Kolmogorov partial differential equations with Lipschitz nonlinearities in the $L^p$-sense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13722">http://arxiv.org/abs/2309.13722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Ackermann, Arnulf Jentzen, Thomas Kruse, Benno Kuckuck, Joshua Lee Padgett</li>
<li>for: 这些深度学习方法用于近似高维partial differential equations（PDEs）的批处。</li>
<li>methods: 这些方法使用的是深度神经网络（DNNs），并且使用了ReLU激活函数。</li>
<li>results: 这些方法可以在PDEs中超越几何约束（COD），即计算量只增长为 polynomial 函数，而不是几何函数。这些方法还可以在$L^p$ norm下与高维PDE解决方法进行比较。<details>
<summary>Abstract</summary>
Recently, several deep learning (DL) methods for approximating high-dimensional partial differential equations (PDEs) have been proposed. The interest that these methods have generated in the literature is in large part due to simulations which appear to demonstrate that such DL methods have the capacity to overcome the curse of dimensionality (COD) for PDEs in the sense that the number of computational operations they require to achieve a certain approximation accuracy $\varepsilon\in(0,\infty)$ grows at most polynomially in the PDE dimension $d\in\mathbb N$ and the reciprocal of $\varepsilon$. While there is thus far no mathematical result that proves that one of such methods is indeed capable of overcoming the COD, there are now a number of rigorous results in the literature that show that deep neural networks (DNNs) have the expressive power to approximate PDE solutions without the COD in the sense that the number of parameters used to describe the approximating DNN grows at most polynomially in both the PDE dimension $d\in\mathbb N$ and the reciprocal of the approximation accuracy $\varepsilon>0$. Roughly speaking, in the literature it is has been proved for every $T>0$ that solutions $u_d\colon [0,T]\times\mathbb R^d\to \mathbb R$, $d\in\mathbb N$, of semilinear heat PDEs with Lipschitz continuous nonlinearities can be approximated by DNNs with ReLU activation at the terminal time in the $L^2$-sense without the COD provided that the initial value functions $\mathbb R^d\ni x\mapsto u_d(0,x)\in\mathbb R$, $d\in\mathbb N$, can be approximated by ReLU DNNs without the COD. It is the key contribution of this work to generalize this result by establishing this statement in the $L^p$-sense with $p\in(0,\infty)$ and by allowing the activation function to be more general covering the ReLU, the leaky ReLU, and the softplus activation functions as special cases.
</details>
<details>
<summary>摘要</summary>
近些时候，一些深度学习（DL）方法用于近似高维partial differential equations（PDEs）已经被提出。这些方法在文献中引起了广泛的关注，主要是因为这些方法可以在computational operations上减少高维维度的影响，即“掌数之咎”（COD）。虽然没有现有的数学结论证明其中一种方法可以完全超越COD，但现在有一些文献证明了深度神经网络（DNNs）具有表达力可以在PDE维度$d\in\mathbb N$和reciprocal of approximation accuracy $\varepsilon>0$之间 polynomially增长。粗略地说，在文献中已经证明了，对于任意$T>0$，solutions $u_d\colon [0,T]\times\mathbb R^d\to \mathbb R$, $d\in\mathbb N$, of semilinear heat PDEs with Lipschitz continuous nonlinearities可以通过DNNs with ReLU activation在终点时间 уровнем$L^2$上无COD的方式进行approximation， provided that the initial value functions $\mathbb R^d\ni x\mapsto u_d(0,x)\in\mathbb R$, $d\in\mathbb N$,可以通过ReLU DNNs without COD进行approximation。这是本研究的关键贡献，是通过将这个结论推广到$L^p$ norm中($p\in(0,\infty)$)，并允许activation function可以是更加一般的，涵盖ReLU、泄漏ReLU和softplus activation function的特殊情况。
</details></li>
</ul>
<hr>
<h2 id="Federated-Deep-Multi-View-Clustering-with-Global-Self-Supervision"><a href="#Federated-Deep-Multi-View-Clustering-with-Global-Self-Supervision" class="headerlink" title="Federated Deep Multi-View Clustering with Global Self-Supervision"></a>Federated Deep Multi-View Clustering with Global Self-Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13697">http://arxiv.org/abs/2309.13697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyue Chen, Jie Xu, Yazhou Ren, Xiaorong Pu, Ce Zhu, Xiaofeng Zhu, Zhifeng Hao, Lifang He</li>
<li>for: 本研究旨在Addressing the challenges of incomplete multi-view data in distributed environments, where label information is unknown and data privacy must be preserved.</li>
<li>methods: 我们提出了一种novel federated deep multi-view clustering方法，包括sample alignment和data extension技术，以探索多个视图中的 complementary cluster结构。在服务器环境中，我们提出了一种global prototype和global pseudo-label的分布方式，以帮助客户端学习自我supervised信息。在客户端环境中，多个客户端使用全球自我supervised信息和深度自适应神经网络来学习视图特定的归一化分类结果和嵌入特征，并将其上传到服务器进行自我supervised信息的修正。</li>
<li>results: 我们的广泛实验结果表明，我们提出的方法可以有效地解决多视图数据的不完整性和隐私担忧问题，并且表现出色。<details>
<summary>Abstract</summary>
Federated multi-view clustering has the potential to learn a global clustering model from data distributed across multiple devices. In this setting, label information is unknown and data privacy must be preserved, leading to two major challenges. First, views on different clients often have feature heterogeneity, and mining their complementary cluster information is not trivial. Second, the storage and usage of data from multiple clients in a distributed environment can lead to incompleteness of multi-view data. To address these challenges, we propose a novel federated deep multi-view clustering method that can mine complementary cluster structures from multiple clients, while dealing with data incompleteness and privacy concerns. Specifically, in the server environment, we propose sample alignment and data extension techniques to explore the complementary cluster structures of multiple views. The server then distributes global prototypes and global pseudo-labels to each client as global self-supervised information. In the client environment, multiple clients use the global self-supervised information and deep autoencoders to learn view-specific cluster assignments and embedded features, which are then uploaded to the server for refining the global self-supervised information. Finally, the results of our extensive experiments demonstrate that our proposed method exhibits superior performance in addressing the challenges of incomplete multi-view data in distributed environments.
</details>
<details>
<summary>摘要</summary>
“联合多视角聚类”有可能从多个设备上的数据学习全球聚类模型。在这个设定下，标签信息未知，且需保持数据隐私，导致两个主要挑战。首先，不同客户的视野常有特征差异，采集其辅助聚类结构不单简。其次，在分布式环境中存储和使用多个客户的数据可能会导致多视角数据的不完整性。为解决这些挑战，我们提出了一个新的联合深度多视角聚类方法。在服务器环境中，我们提出了样本Alignment和数据扩展技术来探索多个视野之间的辅助聚类结构。服务器随后将全球原型和全球伪标给每个客户作为全球自我超级信息。在客户环境中，每个客户使用全球自我超级信息和深度自适应器来学习视野特定的聚类分配和嵌入特征，然后将结果上传到服务器进行改进全球自我超级信息。最后，我们的广泛实验结果显示，我们的提议方法在实际中处理多视角数据的不完整性时表现出色。”
</details></li>
</ul>
<hr>
<h2 id="Performance-Evaluation-of-Equal-Weight-Portfolio-and-Optimum-Risk-Portfolio-on-Indian-Stocks"><a href="#Performance-Evaluation-of-Equal-Weight-Portfolio-and-Optimum-Risk-Portfolio-on-Indian-Stocks" class="headerlink" title="Performance Evaluation of Equal-Weight Portfolio and Optimum Risk Portfolio on Indian Stocks"></a>Performance Evaluation of Equal-Weight Portfolio and Optimum Risk Portfolio on Indian Stocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13696">http://arxiv.org/abs/2309.13696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhiraj Sen, Jaydip Sen<br>for: 这个论文目的是为了设计一个最佳投资组合，使得投资组合的返回和风险得到优化。methods: 这篇论文使用了三种方法来设计投资组合，包括最小风险方法、最大返回方法和等权分配方法。results: 根据实际股票市场数据，这篇论文发现了三个投资组合，每个组合包括10家公司，可以最大化返回和最小化风险。这些组合的性能被评估于2022年1月1日至2022年12月31日的股票价格数据上，并与市场数据进行比较。<details>
<summary>Abstract</summary>
Designing an optimum portfolio for allocating suitable weights to its constituent assets so that the return and risk associated with the portfolio are optimized is a computationally hard problem. The seminal work of Markowitz that attempted to solve the problem by estimating the future returns of the stocks is found to perform sub-optimally on real-world stock market data. This is because the estimation task becomes extremely challenging due to the stochastic and volatile nature of stock prices. This work illustrates three approaches to portfolio design minimizing the risk, optimizing the risk, and assigning equal weights to the stocks of a portfolio. Thirteen critical sectors listed on the National Stock Exchange (NSE) of India are first chosen. Three portfolios are designed following the above approaches choosing the top ten stocks from each sector based on their free-float market capitalization. The portfolios are designed using the historical prices of the stocks from Jan 1, 2017, to Dec 31, 2022. The portfolios are evaluated on the stock price data from Jan 1, 2022, to Dec 31, 2022. The performances of the portfolios are compared, and the portfolio yielding the higher return for each sector is identified.
</details>
<details>
<summary>摘要</summary>
设计最佳投资组合，以优化投资组合的回报和风险，是一个计算复杂的问题。markowitz的基础工作，尝试通过估算未来股票回报来解决问题，发现在实际股市数据上表现下相对较差。这是因为估算任务在股票价格的随机和波动性下变得极其困难。本文介绍了三种方法来设计投资组合，即最小化风险、最大化回报和均衡分配股票。选择了13个关键领域的上市公司（NSE）在印度股市。根据每个领域的自由悬挂市值，选择了每个领域的前十名股票。使用历史股票价格从2017年1月1日到2022年12月31日，设计了三个投资组合。对于2022年1月1日到2022年12月31日的股票价格，评估了投资组合的表现。对每个领域，比较了投资组合的表现，并标识出每个领域的最高回报投资组合。
</details></li>
</ul>
<hr>
<h2 id="Regularization-and-Optimal-Multiclass-Learning"><a href="#Regularization-and-Optimal-Multiclass-Learning" class="headerlink" title="Regularization and Optimal Multiclass Learning"></a>Regularization and Optimal Multiclass Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13692">http://arxiv.org/abs/2309.13692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng</li>
<li>for:  This paper is written to study the role of regularization in multiclass learning with arbitrary label sets, and to introduce optimal learning algorithms that incorporate regularization using one-inclusion graphs (OIGs).</li>
<li>methods: The paper uses OIGs to exhibit optimal learning algorithms that relax structural risk minimization on two dimensions: allowing the regularization function to be “local” to datapoints, and using an unsupervised learning stage to learn this regularizer at the outset. The paper also introduces a combinatorial sequence called the Hall complexity, which is the first to characterize a problem’s transductive error rate exactly.</li>
<li>results: The paper shows that the introduced optimal learner relaxes structural risk minimization on two dimensions and uses an unsupervised learning stage to learn a regularizer at the outset. The paper also demonstrates that an agnostic version of the Hall complexity characterizes error rates exactly, and exhibits an optimal learner using maximum entropy programs.<details>
<summary>Abstract</summary>
The quintessential learning algorithm of empirical risk minimization (ERM) is known to fail in various settings for which uniform convergence does not characterize learning. It is therefore unsurprising that the practice of machine learning is rife with considerably richer algorithmic techniques for successfully controlling model capacity. Nevertheless, no such technique or principle has broken away from the pack to characterize optimal learning in these more general settings.   The purpose of this work is to characterize the role of regularization in perhaps the simplest setting for which ERM fails: multiclass learning with arbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimal learning algorithms that dovetail with tried-and-true algorithmic principles: Occam's Razor as embodied by structural risk minimization (SRM), the principle of maximum entropy, and Bayesian reasoning. Most notably, we introduce an optimal learner which relaxes structural risk minimization on two dimensions: it allows the regularization function to be "local" to datapoints, and uses an unsupervised learning stage to learn this regularizer at the outset. We justify these relaxations by showing that they are necessary: removing either dimension fails to yield a near-optimal learner. We also extract from OIGs a combinatorial sequence we term the Hall complexity, which is the first to characterize a problem's transductive error rate exactly.   Lastly, we introduce a generalization of OIGs and the transductive learning setting to the agnostic case, where we show that optimal orientations of Hamming graphs -- judged using nodes' outdegrees minus a system of node-dependent credits -- characterize optimal learners exactly. We demonstrate that an agnostic version of the Hall complexity again characterizes error rates exactly, and exhibit an optimal learner using maximum entropy programs.
</details>
<details>
<summary>摘要</summary>
《 Quintessential 学习算法的实际风险最小化（ERM）在各种设置下失败，因此 machine learning 实践中的许多更加复杂的算法技巧成功地控制模型容量。然而，没有任何技巧或原则能够在更一般的设置下Characterize 优化学习。》本文的目的是在多类学习中使用一 inclusion 图（OIGs）来Characterize 识别器的角色，并使用structural risk minimization（SRM）、最大Entropy 原则和 Bayesian 思维来定义优化学习算法。我们介绍了一个优化学习算法，它在两个维度上relax 了SRM：允许正则化函数在数据点上本地化，并在无监督学习阶段使用一个不supervised 学习来学习这个正则化器。我们证明了这些relaxation 是必要的， otherwise 不能得到近似优化学习算法。我们还从 OIGs 中提取了一个 combinatorial sequence，我们称之为 Hall complexity，它可以 exactly Characterize 问题的推导性错误率。 finally，我们将 OIGs 和推导学习设定扩展到agnostic 情况下，并证明在这种情况下，optimal  orientations of Hamming graphs（judged by nodes' outdegrees minus a system of node-dependent credits）Characterize 优化学习算法 exactly。我们还证明了agnostic 版本的 Hall complexity 可以 exactly Characterize 错误率，并展示了一个使用最大Entropy 程序的优化学习算法。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Large-Batch-Training-via-Gradient-Signal-to-Noise-Ratio-GSNR"><a href="#Accelerating-Large-Batch-Training-via-Gradient-Signal-to-Noise-Ratio-GSNR" class="headerlink" title="Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)"></a>Accelerating Large Batch Training via Gradient Signal to Noise Ratio (GSNR)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13681">http://arxiv.org/abs/2309.13681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guo-qing Jiang, Jinlong Liu, Zixiang Ding, Lin Guo, Wei Lin</li>
<li>for: 这paper的目的是提高大批量（LB）训练的通过率，但是训练LB任务经常遇到大的泛化差和下降最终精度，限制了扩大批量大小。</li>
<li>methods: 这paper提出了基于偏差信号噪声比（GSNR）的偏差减少技术（VRGD），并应用于流行的优化器 such as SGD&#x2F;Adam&#x2F;LARS&#x2F;LAMB。 authors还进行了关于整体趋势的分析和一般化分析，以解释它的快速训练动态和更小的泛化差。</li>
<li>results: 实验表明，VRGD可以加速训练（1-2倍），缩小泛化差和提高最终精度。 authors推进BERT预训练的批量大小到128k&#x2F;64k和DLRM到512k，而无需影响精度。 ImageNet Top-1准确率在96k上提高了0.52pp，比LARS更高。 总的来说，这paper的研究可以大幅减少BERT和ImageNet训练中的泛化差。<details>
<summary>Abstract</summary>
As models for nature language processing (NLP), computer vision (CV) and recommendation systems (RS) require surging computation, a large number of GPUs/TPUs are paralleled as a large batch (LB) to improve training throughput. However, training such LB tasks often meets large generalization gap and downgrades final precision, which limits enlarging the batch size. In this work, we develop the variance reduced gradient descent technique (VRGD) based on the gradient signal to noise ratio (GSNR) and apply it onto popular optimizers such as SGD/Adam/LARS/LAMB. We carry out a theoretical analysis of convergence rate to explain its fast training dynamics, and a generalization analysis to demonstrate its smaller generalization gap on LB training. Comprehensive experiments demonstrate that VRGD can accelerate training ($1\sim 2 \times$), narrow generalization gap and improve final accuracy. We push the batch size limit of BERT pretraining up to 128k/64k and DLRM to 512k without noticeable accuracy loss. We improve ImageNet Top-1 accuracy at 96k by $0.52pp$ than LARS. The generalization gap of BERT and ImageNet training is significantly reduce by over $65\%$.
</details>
<details>
<summary>摘要</summary>
为了提高自然语言处理（NLP）、计算机视觉（CV）和推荐系统（RS）的训练效率，常常使用大量的GPUs/TPUs并行为大批（LB）训练。然而，这些LB任务的训练经常会遇到大的泛化差异和下降最终精度，从而限制扩大批处理的大小。在这种情况下，我们开发了基于梯度信号噪声比（GSNR）的减少梯度下降技术（VRGD），并应用于流行的优化器如SGD/Adam/LAMB/LARS。我们进行了理论分析的速度和稳定性，以及通用的泛化分析，以证明它的快速训练特性和更小的泛化差异。实验表明，VRGD可以提高训练速度（1-2倍），缩小泛化差异和提高最终精度。我们把BERT预训练的批处理大小提高到128k/64k，而DLRM的批处理大小提高到512k，无需注意到精度下降。在ImageNet顶层1任务中，我们提高了LARS的性能，比原来的LARS提高了0.52pp。总的来说，我们通过减少BERT和ImageNet训练中的泛化差异，将其降低了65%以上。
</details></li>
</ul>
<hr>
<h2 id="Topology-Agnostic-Detection-of-Temporal-Money-Laundering-Flows-in-Billion-Scale-Transactions"><a href="#Topology-Agnostic-Detection-of-Temporal-Money-Laundering-Flows-in-Billion-Scale-Transactions" class="headerlink" title="Topology-Agnostic Detection of Temporal Money Laundering Flows in Billion-Scale Transactions"></a>Topology-Agnostic Detection of Temporal Money Laundering Flows in Billion-Scale Transactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13662">http://arxiv.org/abs/2309.13662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haseeb Tariq, Marwan Hassani</li>
<li>for: 这篇论文是为了探讨针对财务洗钱措施的检测系统弱点，并利用多个银行账户、层次化和转移 transactions 来隐藏财富的来源和流动。</li>
<li>methods: 该论文提出了一种名为 FaSTMAN 的框架，采用域特定约束适应而建立了一个时间图示 sequential transactions，并使用二阶 Graph 表示法来评估边的重要性。</li>
<li>results: 对于一个包含多个大 european 银行交易的数据集，该框架表现出了明显的高效性和实用性，在比较两种现有的探测恶意流动交易方法时。<details>
<summary>Abstract</summary>
Money launderers exploit the weaknesses in detection systems by purposefully placing their ill-gotten money into multiple accounts, at different banks. That money is then layered and moved around among mule accounts to obscure the origin and the flow of transactions. Consequently, the money is integrated into the financial system without raising suspicion. Path finding algorithms that aim at tracking suspicious flows of money usually struggle with scale and complexity. Existing community detection techniques also fail to properly capture the time-dependent relationships. This is particularly evident when performing analytics over massive transaction graphs. We propose a framework (called FaSTMAN), adapted for domain-specific constraints, to efficiently construct a temporal graph of sequential transactions. The framework includes a weighting method, using 2nd order graph representation, to quantify the significance of the edges. This method enables us to distribute complex queries on smaller and densely connected networks of flows. Finally, based on those queries, we can effectively identify networks of suspicious flows. We extensively evaluate the scalability and the effectiveness of our framework against two state-of-the-art solutions for detecting suspicious flows of transactions. For a dataset of over 1 Billion transactions from multiple large European banks, the results show a clear superiority of our framework both in efficiency and usefulness.
</details>
<details>
<summary>摘要</summary>
贩卖洗钱者利用检测系统的弱点，故意将黑钱分布到多个帐户，不同银行的帐户中。然后将这笔钱层层转移，以隐藏起点和转移流动的关系。因此，黑钱能够融入金融系统，无需引起怀疑。跟踪款流的算法通常在规模和复杂性方面遇到困难。现有社区检测技术也无法正确捕捉时间关系。特别是在处理庞大交易图时，这些技术的表现很差。我们提出了一个名为FaSTMAN的框架，适应域pecific约束，以生成Sequential Transactions的 temporal graph。该框架包括一种Edge重量方法，使用二次graph表示法，以衡量边的重要性。这种方法允许我们将复杂的查询分配到更小的、紧密连接的网络上。最后，基于这些查询，我们可以有效地认定涉嫌的款流网络。我们对两个国际领先的检测涉嫌款流解决方案进行了广泛的评估，并对一个包含多个大European银行的交易数据集进行了广泛的测试。结果表明，我们的框架在效率和有用性方面具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Fantastic-Generalization-Measures-are-Nowhere-to-be-Found"><a href="#Fantastic-Generalization-Measures-are-Nowhere-to-be-Found" class="headerlink" title="Fantastic Generalization Measures are Nowhere to be Found"></a>Fantastic Generalization Measures are Nowhere to be Found</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13658">http://arxiv.org/abs/2309.13658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Gastpar, Ido Nachum, Jonathan Shafer, Thomas Weinberger</li>
<li>for: 这个论文旨在探讨神经网络在过参数 Setting 下的泛化能力，以及相关的一般化 bound 的可能性。</li>
<li>methods: 作者使用了一些常见的一般化 bound 类型，包括依赖于训练集和输出学习算法的 bound，以及依赖于训练集和学习算法的稳定 bound。</li>
<li>results: 作者通过数学分析和实验研究发现，在过参数 Setting 下，无论使用哪种一般化 bound，都无法保证一般化能力的准确性。此外，如果学习算法在某些分布上具有良好的准确率，那么一般化 bound 就无法 uniformly 紧张。因此，作者结论认为，在过参数 Setting 下，一般化 bound 无法是紧张的，除非有适当的Assumption  sobre 人口分布。<details>
<summary>Abstract</summary>
Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, none of these bounds are tight. For instance, in their paper ``Fantastic Generalization Measures and Where to Find Them'', Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them imply guarantees that can explain the remarkable performance of neural networks. This raises the question of whether tight generalization bounds are at all possible. We consider two types of generalization bounds common in the literature: (1) bounds that depend on the training set and the output of the learning algorithm. There are multiple bounds of this type in the literature (e.g., norm-based and margin-based bounds), but we prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that depend on the training set and on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm's performance and the bound's tightness. Namely, if the algorithm achieves good accuracy on certain distributions in the overparameterized setting, then no generalization bound can be tight for it. We conclude that generalization bounds in the overparameterized setting cannot be tight without suitable assumptions on the population distribution.
</details>
<details>
<summary>摘要</summary>
很多通用 bound 在文献中被提出，以解释神经网络在过参数化设置下的泛化能力。然而，这些 bound 都不是紧张的。例如，在他们的 paper "Fantastic Generalization Measures and Where to Find Them" 中， Jiang et al. (2020)  examine over a dozen generalization bound, and show empirically that none of them can provide guarantees that can explain the remarkable performance of neural networks.这引起了是否存在紧张的 generalization bound 的问题。我们考虑了文献中两种常见的 generalization bound：1. 依赖于训练集和学习算法的 bound。文献中有多种这类 bound（例如，norm-based和margin-based bound），但我们证明了在过参数化设置下，无法得到一个 uniformly 紧张的 bound。2. 依赖于训练集和学习算法的 bound。例如，stability bound。我们显示出在某些分布下，如果学习算法 achieves 良好的准确率，那么不可能有一个紧张的 bound。我们结论是，在过参数化设置下，generalization bound 不可能是紧张的，除非有适当的人口分布假设。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Model-for-Data-Redundancy-in-the-Feature-Domain"><a href="#A-Probabilistic-Model-for-Data-Redundancy-in-the-Feature-Domain" class="headerlink" title="A Probabilistic Model for Data Redundancy in the Feature Domain"></a>A Probabilistic Model for Data Redundancy in the Feature Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13657">http://arxiv.org/abs/2309.13657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ghurumuruhan Ganesan</li>
<li>for: 这个论文是用概率模型来估算大量数据中具有低相关性和低多相关性的特征数量。</li>
<li>methods: 这个论文使用概率方法来获得相同顺序的上下限，用于估算具有低相关性和低多相关性的特征集的大小。</li>
<li>results: 论文提供了一种用于估算大量数据中具有低相关性和低多相关性的特征集的方法，并证明了一个关于互助约束集的辅助结果，这结果是独立有价值的。<details>
<summary>Abstract</summary>
In this paper, we use a probabilistic model to estimate the number of uncorrelated features in a large dataset. Our model allows for both pairwise feature correlation (collinearity) and interdependency of multiple features (multicollinearity) and we use the probabilistic method to obtain upper and lower bounds of the same order, for the size of a feature set that exhibits low collinearity and low multicollinearity. We also prove an auxiliary result regarding mutually good constrained sets that is of independent interest.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们使用一种 probabilistic 模型来估计大数据集中具有低相关性的特征数量。我们的模型允许对特征之间的对比关系（杂相关）以及多个特征之间的相互关系（多相关），并使用 probabilistic 方法获取同样的订正范围，以便确定具有低相关性和低多相关性的特征集的大小。我们还证明了一个有益的副结果，即具有互助约束的特征集是独立有价值的。
</details></li>
</ul>
<hr>
<h2 id="REWAFL-Residual-Energy-and-Wireless-Aware-Participant-Selection-for-Efficient-Federated-Learning-over-Mobile-Devices"><a href="#REWAFL-Residual-Energy-and-Wireless-Aware-Participant-Selection-for-Efficient-Federated-Learning-over-Mobile-Devices" class="headerlink" title="REWAFL: Residual Energy and Wireless Aware Participant Selection for Efficient Federated Learning over Mobile Devices"></a>REWAFL: Residual Energy and Wireless Aware Participant Selection for Efficient Federated Learning over Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13643">http://arxiv.org/abs/2309.13643</a></li>
<li>repo_url: None</li>
<li>paper_authors: Y. Li, X. Qin, J. Geng, R. Chen, Y. Hou, Y. Gong, M. Pan, P. Zhang</li>
<li>for: 本文旨在提高 federated learning（FL）训练的速度和效率，并且解决移动设备的剩余能量和无线传输速率的影响。</li>
<li>methods: 本文提出了一种基于剩余能量和无线传输速率的PS设计方法，其中引入了一个新的PS价值函数，该价值函数同时考虑了全局FL训练价值和本地能量价值。此外，本文还提出了一种基于REWAFL的剩余能量和无线传输速率aware的本地计算策略。</li>
<li>results: 实验结果表明，REWAFL可以提高训练精度和效率，同时避免移动设备”耗尽电池”的问题。<details>
<summary>Abstract</summary>
Participant selection (PS) helps to accelerate federated learning (FL) convergence, which is essential for the practical deployment of FL over mobile devices. While most existing PS approaches focus on improving training accuracy and efficiency rather than residual energy of mobile devices, which fundamentally determines whether the selected devices can participate. Meanwhile, the impacts of mobile devices' heterogeneous wireless transmission rates on PS and FL training efficiency are largely ignored. Moreover, PS causes the staleness issue. Prior research exploits isolated functions to force long-neglected devices to participate, which is decoupled from original PS designs. In this paper, we propose a residual energy and wireless aware PS design for efficient FL training over mobile devices (REWAFL). REW AFL introduces a novel PS utility function that jointly considers global FL training utilities and local energy utility, which integrates energy consumption and residual battery energy of candidate mobile devices. Under the proposed PS utility function framework, REW AFL further presents a residual energy and wireless aware local computing policy. Besides, REWAFL buries the staleness solution into its utility function and local computing policy. The experimental results show that REW AFL is effective in improving training accuracy and efficiency, while avoiding "flat battery" of mobile devices.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a residual energy and wireless aware PS design for efficient FL training over mobile devices (REWAFL). The proposed PS utility function jointly considers global FL training utilities and local energy utility, which integrates energy consumption and residual battery energy of candidate mobile devices. Under the proposed PS utility function framework, REW AFL further presents a residual energy and wireless aware local computing policy. Moreover, REWAFL buries the staleness solution into its utility function and local computing policy.The experimental results show that REWAFL is effective in improving training accuracy and efficiency while avoiding "flat battery" of mobile devices.
</details></li>
</ul>
<hr>
<h2 id="Crack-Net-Prediction-of-Crack-Propagation-in-Composites"><a href="#Crack-Net-Prediction-of-Crack-Propagation-in-Composites" class="headerlink" title="Crack-Net: Prediction of Crack Propagation in Composites"></a>Crack-Net: Prediction of Crack Propagation in Composites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13626">http://arxiv.org/abs/2309.13626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Xu, Wei Fan, Ambrose C. Taylor, Dongxiao Zhang, Lecheng Ruan, Rundong Shi</li>
<li>for: 这篇论文的目的是来提供一个基于深度学习的材料分子破坏预测模型，以便在结构应用中提高材料性能和微struktural设计。</li>
<li>methods: 这篇论文使用了一个名为Crack-Net的深度学习框架，这个框架可以模拟材料的破坏过程，并且可以考虑不同的微struktural设计。</li>
<li>results: 这篇论文的结果显示，Crack-Net可以高度准确地预测材料的破坏模式和压缩曲线，并且可以处理更复杂的微struktural设计。<details>
<summary>Abstract</summary>
Computational solid mechanics has become an indispensable approach in engineering, and numerical investigation of fracture in composites is essential as composites are widely used in structural applications. Crack evolution in composites is the bridge to elucidate the relationship between the microstructure and fracture performance, but crack-based finite element methods are computationally expensive and time-consuming, limiting their application in computation-intensive scenarios. Here we propose a deep learning framework called Crack-Net, which incorporates the relationship between crack evolution and stress response to predict the fracture process in composites. Trained on a high-precision fracture development dataset generated using the phase field method, Crack-Net demonstrates a remarkable capability to accurately forecast the long-term evolution of crack growth patterns and the stress-strain curve for a given composite design. The Crack-Net captures the essential principle of crack growth, which enables it to handle more complex microstructures such as binary co-continuous structures. Moreover, transfer learning is adopted to further improve the generalization ability of Crack-Net for composite materials with reinforcements of different strengths. The proposed Crack-Net holds great promise for practical applications in engineering and materials science, in which accurate and efficient fracture prediction is crucial for optimizing material performance and microstructural design.
</details>
<details>
<summary>摘要</summary>
computation solid mechanics 已成为工程领域必备的方法，数字调查对于复合材料的裂解是必要的，因为复合材料广泛应用于结构应用。 裂解进程中的裂解演化在复合材料中是关键，但是基于裂解的finite element方法 computationally expensive 和时间consuming，这限制了它们在 computation-intensive enario 中的应用。 我们提出了一个深度学习框架，叫做Crack-Net，它包含了裂解演化和压力应答之间的关系，以预测复合材料的裂解过程。 Crack-Net 在一个高精度的裂解发展数据集上训练，该数据集使用阶段场方法生成。 Crack-Net 能够准确预测复合材料的长期裂解趋势和压力-弹簧曲线。 Crack-Net 捕捉了裂解的基本原理，因此可以处理更复杂的微结构，例如二元共晶结构。 此外，我们采用了传输学习来进一步提高 Crack-Net 对于不同强度的增强材料的泛化能力。 我们提出的 Crack-Net 具有实际应用的潜在价值，在工程和材料科学中，准确和高效地预测裂解是关键的，以便优化材料性能和微结构设计。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Enhanced-Autoregressive-Feature-Transformation-Gradient-steered-Search-in-Continuous-Space-for-Postfix-Expressions"><a href="#Reinforcement-Enhanced-Autoregressive-Feature-Transformation-Gradient-steered-Search-in-Continuous-Space-for-Postfix-Expressions" class="headerlink" title="Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions"></a>Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13618">http://arxiv.org/abs/2309.13618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjie Wang, Meng Xiao, Min Wu, Pengfei Wang, Yuanchun Zhou, Yanjie Fu</li>
<li>for: This paper aims to improve the efficiency and effectiveness of feature transformation for machine learning tasks by reformulating the discrete search space into a continuous optimization task.</li>
<li>methods: The proposed method includes four steps: (1) reinforcement-enhanced data preparation, (2) feature transformation operation sequence embedding, (3) gradient-steered optimal embedding search, and (4) transformation operation sequence reconstruction.</li>
<li>results: The proposed method is expected to fundamentally fill the gap between efficiency and stability&#x2F;robustness in feature transformation, and to provide a more effective and efficient way to optimize feature transformation for machine learning tasks.<details>
<summary>Abstract</summary>
Feature transformation aims to generate new pattern-discriminative feature space from original features to improve downstream machine learning (ML) task performances. However, the discrete search space for the optimal feature explosively grows on the basis of combinations of features and operations from low-order forms to high-order forms. Existing methods, such as exhaustive search, expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy, suffer from large search space. Overly emphasizing efficiency in algorithm design usually sacrifices stability or robustness. To fundamentally fill this gap, we reformulate discrete feature transformation as a continuous space optimization task and develop an embedding-optimization-reconstruction framework. This framework includes four steps: 1) reinforcement-enhanced data preparation, aiming to prepare high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding, intending to encapsulate the knowledge of prepared training data within a continuous space; 3) gradient-steered optimal embedding search, dedicating to uncover potentially superior embeddings within the learned space; 4) transformation operation sequence reconstruction, striving to reproduce the feature transformation solution to pinpoint the optimal feature space.
</details>
<details>
<summary>摘要</summary>
<<SYS>>功能转换targets于生成新的特征分布，以提高下游机器学习（ML）任务表现。然而，原始特征的逐渐扩展的搜索空间会急剧增长，从低阶形式到高阶形式。现有的方法，如枚举搜索、减少扩展、进化算法、强化学习和迭代蜂巢，都受到搜索空间的限制。强调效率在算法设计中通常会牺牲稳定性或可靠性。为了彻底填补这个差距，我们将离散特征转换重新定义为连续空间优化任务，并开发一个嵌入优化重建框架。这个框架包括以下四个步骤：1. 增强驱动数据准备，目的是为特征转换精度训练数据做准备;2. 特征转换操作序列嵌入，旨在将准备好的训练数据中的知识嵌入到连续空间中;3. 梯度导航优化搜索，旨在在学习空间中找到可能更高质量的嵌入;4. 特征转换操作序列重建，努力将特征转换解决方案复制到特定的特征空间，以确定最佳特征空间。
</details></li>
</ul>
<hr>
<h2 id="DPA-WNO-A-gray-box-model-for-a-class-of-stochastic-mechanics-problem"><a href="#DPA-WNO-A-gray-box-model-for-a-class-of-stochastic-mechanics-problem" class="headerlink" title="DPA-WNO: A gray box model for a class of stochastic mechanics problem"></a>DPA-WNO: A gray box model for a class of stochastic mechanics problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.15128">http://arxiv.org/abs/2309.15128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tushar, Souvik Chakraborty</li>
<li>for: 解决数据驱动模型缺乏解释性、占据大量数据和不能泛化问题，提出了数据物理融合方法，并提出了一种新的可微分物理增强波лет神经网络操作器（DPA-WNO），将数据驱动模型和物理解决方法融合在一起，以便利用数据驱动模型学习 FROM 数据，同时保留物理解决方法的解释性和泛化能力。</li>
<li>methods: 提出的DPA-WNO结合了可微分物理解决方法和波лет神经网络操作器，使得该方法可以利用数据驱动模型学习 FROM 数据，同时保留物理解决方法的解释性和泛化能力。</li>
<li>results: 对四个不同领域的科学和工程中的时间不确定性量化和可靠性分析问题进行了解决，并得到了有趣的结果，表明该方法可以有效地解决数据驱动模型中的缺乏解释性、占据大量数据和不能泛化等问题。<details>
<summary>Abstract</summary>
The well-known governing physics in science and engineering is often based on certain assumptions and approximations. Therefore, analyses and designs carried out based on these equations are also approximate. The emergence of data-driven models has, to a certain degree, addressed this challenge; however, the purely data-driven models often (a) lack interpretability, (b) are data-hungry, and (c) do not generalize beyond the training window. Operator learning has recently been proposed as a potential alternative to address the aforementioned challenges; however, the challenges are still persistent. We here argue that one of the possible solutions resides in data-physics fusion, where the data-driven model is used to correct/identify the missing physics. To that end, we propose a novel Differentiable Physics Augmented Wavelet Neural Operator (DPA-WNO). The proposed DPA-WNO blends a differentiable physics solver with the Wavelet Neural Operator (WNO), where the role of WNO is to model the missing physics. This empowers the proposed framework to exploit the capability of WNO to learn from data while retaining the interpretability and generalizability associated with physics-based solvers. We illustrate the applicability of the proposed approach in solving time-dependent uncertainty quantification problems due to randomness in the initial condition. Four benchmark uncertainty quantification and reliability analysis examples from various fields of science and engineering are solved using the proposed approach. The results presented illustrate interesting features of the proposed approach.
</details>
<details>
<summary>摘要</summary>
科学和工程中常见的管理物理是基于某些假设和简化的。因此，基于这些方程的分析和设计也是有误差的。数据驱动模型的出现有所解决了这个挑战，但是纯数据驱动模型常有两个缺点：一是不可解释性，二是吃掉数据。运维学学习已经被提议为可能的解决方案之一，但是这些挑战仍然存在。我们认为一种可能的解决方案在数据物理融合中，其中数据驱动模型用于 corrections/identification of missing physics。为此，我们提出了一种新的可微分物理增强波let神经网络算法（DPA-WNO）。我们的提议的DPA-WNO将一个可微分物理解决器与波let神经网络（WNO）融合在一起，WNO用于模拟缺失的物理。这使得我们的框架能够利用WNO从数据中学习，同时保持与物理基础模型相关的可解释性和泛化性。我们通过解决时间依赖不确定性量化和可靠性分析问题来证明提议的方法的可行性。我们在不同的科学和工程领域中使用提议的方法解决了四个标准不确定性量化和可靠性分析问题的例子。结果表明了我们的方法的有趣特点。
</details></li>
</ul>
<hr>
<h2 id="Self-Tuning-Hamiltonian-Monte-Carlo-for-Accelerated-Sampling"><a href="#Self-Tuning-Hamiltonian-Monte-Carlo-for-Accelerated-Sampling" class="headerlink" title="Self-Tuning Hamiltonian Monte Carlo for Accelerated Sampling"></a>Self-Tuning Hamiltonian Monte Carlo for Accelerated Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13593">http://arxiv.org/abs/2309.13593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henrik Christiansen, Federico Errica, Francesco Alesiani</li>
<li>for: 本研究旨在自动调整哈密顿 Monte Carlo 方法的参数，以便快速探索参数空间。</li>
<li>methods: 本研究使用了完全可微分的设置和反射传播来优化参数。 furthermore, an attention-like loss is defined to allow for the gradient-driven learning of the distribution of integration steps.</li>
<li>results: 我们在一维振荡子和艾莫对蛋白质中进行了实验，发现我们的损失和自参数的散度之间存在良好的对映，从而获得了快速参数的调整。<details>
<summary>Abstract</summary>
The performance of Hamiltonian Monte Carlo crucially depends on its parameters, in particular the integration timestep and the number of integration steps. We present an adaptive general-purpose framework to automatically tune these parameters based on a loss function which promotes the fast exploration of phase-space. For this, we make use of a fully-differentiable set-up and use backpropagation for optimization. An attention-like loss is defined which allows for the gradient driven learning of the distribution of integration steps. We also highlight the importance of jittering for a smooth loss-surface. Our approach is demonstrated for the one-dimensional harmonic oscillator and alanine dipeptide, a small protein common as a test-case for simulation methods. We find a good correspondence between our loss and the autocorrelation times, resulting in well-tuned parameters for Hamiltonian Monte Carlo.
</details>
<details>
<summary>摘要</summary>
Hamiltonian Monte Carlo 的性能取决于它的参数，特别是 интеграル时步和integration step的数量。我们提出了一种自适应通用框架，通过损函数来促进快速探索phaspace的分布。我们利用了完全导数的设置，并使用反射进行优化。我们定义了一种注意力类损函数，允许通过梯度驱动学习的integration step的分布。我们 также强调了在损函数Surface上的缓冲作用。我们的方法在一维振荡体和 Alanine dipeptide 上进行了示例，发现我们的损函数和自相关时间之间存在良好的匹配，从而获得了良好地调整的 Hamiltonian Monte Carlo 参数。
</details></li>
</ul>
<hr>
<h2 id="Robust-Distributed-Learning-Tight-Error-Bounds-and-Breakdown-Point-under-Data-Heterogeneity"><a href="#Robust-Distributed-Learning-Tight-Error-Bounds-and-Breakdown-Point-under-Data-Heterogeneity" class="headerlink" title="Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity"></a>Robust Distributed Learning: Tight Error Bounds and Breakdown Point under Data Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13591">http://arxiv.org/abs/2309.13591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Allouah, Rachid Guerraoui, Nirupam Gupta, Rafaël Pinot, Geovani Rizk</li>
<li>for: 本研究旨在探讨鲁棒分布式学习算法的理论基础，以抵御邪恶机器学习模型。</li>
<li>methods: 本文使用(G,B)-梯度不同性模型来研究分布式学习下数据不均衡的情况，并提出了一种更加实际的不同性模型。</li>
<li>results: 本研究显示了现有理论下的学习误差下界不适用于实际场景中的数据不均衡情况，而且提出了一种新的下界。此外，我们还提出了一种robust变种的分布式梯度下降算法，并通过实验 validate our分析。<details>
<summary>Abstract</summary>
The theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. Under data heterogeneity however, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely (G,B)-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction 1/2. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysis reduces the gap between theory and practice.
</details>
<details>
<summary>摘要</summary>
Theory underlying robust distributed learning algorithms, designed to resist adversarial machines, matches empirical observations when data is homogeneous. However, under data heterogeneity, which is the norm in practical scenarios, established lower bounds on the learning error are essentially vacuous and greatly mismatch empirical observations. This is because the heterogeneity model considered is too restrictive and does not cover basic learning tasks such as least-squares regression. We consider in this paper a more realistic heterogeneity model, namely (G,B)-gradient dissimilarity, and show that it covers a larger class of learning problems than existing theory. Notably, we show that the breakdown point under heterogeneity is lower than the classical fraction 1/2. We also prove a new lower bound on the learning error of any distributed learning algorithm. We derive a matching upper bound for a robust variant of distributed gradient descent, and empirically show that our analysis reduces the gap between theory and practice.Note: The translation is done using a machine translation tool, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Neural-Network-Code-for-2D-Transient-Problems-PINN-2DT-Compatible-with-Google-Colab"><a href="#Physics-Informed-Neural-Network-Code-for-2D-Transient-Problems-PINN-2DT-Compatible-with-Google-Colab" class="headerlink" title="Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab"></a>Physics Informed Neural Network Code for 2D Transient Problems (PINN-2DT) Compatible with Google Colab</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03755">http://arxiv.org/abs/2310.03755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paweł Maczuga, Maciej Skoczeń, Przemysław Rożnawski, Filip Tłuszcz, Marcin Szubert, Marcin Łoś, Witold Dzwinel, Keshav Pingali, Maciej Paszyński</li>
<li>For: The paper presents an open-source Physics Informed Neural Network (PINN) environment for simulating transient phenomena on two-dimensional rectangular domains.* Methods: The PINN environment uses a neural network to solve time-dependent partial differential equations (PDEs) and supports various boundary conditions, including Neumann and Dirichlet conditions. It also allows for customization of the number of layers and neurons per layer, as well as for arbitrary activation functions.* Results: The PINN environment provides a simple interface for defining the residual loss, boundary condition, and initial loss, together with their weights. It also includes a library of problems, such as non-stationary heat transfer, wave equation modeling a tsunami, atmospheric simulations including thermal inversion, and tumor growth simulations.<details>
<summary>Abstract</summary>
We present an open-source Physics Informed Neural Network environment for simulations of transient phenomena on two-dimensional rectangular domains, with the following features: (1) it is compatible with Google Colab which allows automatic execution on cloud environment; (2) it supports two dimensional time-dependent PDEs; (3) it provides simple interface for definition of the residual loss, boundary condition and initial loss, together with their weights; (4) it support Neumann and Dirichlet boundary conditions; (5) it allows for customizing the number of layers and neurons per layer, as well as for arbitrary activation function; (6) the learning rate and number of epochs are available as parameters; (7) it automatically differentiates PINN with respect to spatial and temporal variables; (8) it provides routines for plotting the convergence (with running average), initial conditions learnt, 2D and 3D snapshots from the simulation and movies (9) it includes a library of problems: (a) non-stationary heat transfer; (b) wave equation modeling a tsunami; (c) atmospheric simulations including thermal inversion; (d) tumor growth simulations.
</details>
<details>
<summary>摘要</summary>
我们提供一个开源的物理学 Informed Neural Network 环境，用于二维矩形领域上的脉冲现象模拟，其特点如下：1. 兼容 Google Colab，可以在云环境自动执行;2. 支持二维时间依赖的偏微分方程;3. 提供简单的接口 для定义剩余损失、边界条件和初始损失，以及其权重;4. 支持内壁和 Dirichlet 边界条件;5. 允许自定义层数和神经元数，以及任意活动函数;6. 学习率和迭代次数作为参数;7. 自动 differentiate PINN 对于空间和时间变量;8. 提供折线Plot 的初始条件、2D和3D 快照和电影等;9. 包含一个库，包括：a. 非站点热传输;b. 泪滤波方程模拟潮汐;c. 大气模拟，包括热层倒挪;d. 肿瘤增长模拟。
</details></li>
</ul>
<hr>
<h2 id="Graph-enhanced-Optimizers-for-Structure-aware-Recommendation-Embedding-Evolution"><a href="#Graph-enhanced-Optimizers-for-Structure-aware-Recommendation-Embedding-Evolution" class="headerlink" title="Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution"></a>Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03032">http://arxiv.org/abs/2310.03032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Xu, Jun Wang, Jianyong Wang, Wei Zhang</li>
<li>for: 提高现代推荐系统中的嵌入性能，即虚拟实体的虚拟表示和后续决策模型的基础。</li>
<li>methods: 提出了一种新的嵌入更新机制——结构意识嵌入演化（SEvo），使相关节点在每步中进行相似演化。与传统的图神经网络（GNN）不同，SEvo可以直接将图结构信息注入嵌入，无需较大的计算开销。</li>
<li>results: SEvo可以提高推荐系统性能，并且可以轻松地与现有优化器结合使用。具体来说，SEvo可以在不同的模型和数据集上保持稳定的性能提升。<details>
<summary>Abstract</summary>
Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively utilize graph structure information beyond explicit GNN modules.
</details>
<details>
<summary>摘要</summary>
嵌入具有重要作用在现代推荐系统中，因为它们是虚拟世界实体的虚拟表示和后续决策模型的基础。在这篇论文中，我们提出了一种新的嵌入更新机制，即结构意识 embedding 演化（SEvo），以促进相关节点在每步中进行相似演化。与传统的 GNN（图 neural network）不同，SEvo 能够直接将图结构信息注入嵌入中，在训练中减少计算开销。我们还对 SEvo 的收敛性和可能的变体进行了理论分析，以证明其设计的有效性。此外，SEvo 可以轻松地与现有优化器结合使用，以实现最佳性能。例如，SEvo 加强的 AdamW  WITH moment estimate correction 在不同的模型和数据集上都显示了稳定的改进表现，这表明了在图结构信息上超出 Explicit GNN 模块的新技术路径。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Unlimited-Staleness-in-Federated-Learning-with-Intertwined-Data-and-Device-Heterogeneities"><a href="#Tackling-the-Unlimited-Staleness-in-Federated-Learning-with-Intertwined-Data-and-Device-Heterogeneities" class="headerlink" title="Tackling the Unlimited Staleness in Federated Learning with Intertwined Data and Device Heterogeneities"></a>Tackling the Unlimited Staleness in Federated Learning with Intertwined Data and Device Heterogeneities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13536">http://arxiv.org/abs/2309.13536</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pittisl/fl-with-intertwined-heterogeneity">https://github.com/pittisl/fl-with-intertwined-heterogeneity</a></li>
<li>paper_authors: Haoming Wang, Wei Gao</li>
<li>for: 本研究旨在提高 Federated Learning (FL) 的效率，解决数据和设备不同性的问题。</li>
<li>methods: 本研究提出了一种新的 FL 框架，利用梯度反转技术将停滞的客户端模型更新转化为非停滞的模型更新。</li>
<li>results: 实验结果表明，在面临无限停滞的情况下，本研究的方法可以提高训练模型准确率达到 20%，并提高 FL 训练进度达到 35%。<details>
<summary>Abstract</summary>
The efficiency of Federated Learning (FL) is often affected by both data and device heterogeneities. Data heterogeneity is defined as the heterogeneity of data distributions on different clients. Device heterogeneity is defined as the clients' variant latencies in uploading their local model updates due to heterogeneous conditions of local hardware resources, and causes the problem of staleness when being addressed by asynchronous FL. Traditional schemes of tackling the impact of staleness consider data and device heterogeneities as two separate and independent aspects in FL, but this assumption is unrealistic in many practical FL scenarios where data and device heterogeneities are intertwined. In these cases, traditional schemes of weighted aggregation in FL have been proved to be ineffective, and a better approach is to convert a stale model update into a non-stale one. In this paper, we present a new FL framework that leverages the gradient inversion technique for such conversion, hence efficiently tackling unlimited staleness in clients' model updates. Our basic idea is to use gradient inversion to get estimations of clients' local training data from their uploaded stale model updates, and use these estimations to compute non-stale client model updates. In this way, we address the problem of possible data quality drop when using gradient inversion, while still preserving the clients' local data privacy. We compared our approach with the existing FL strategies on mainstream datasets and models, and experiment results demonstrate that when tackling unlimited staleness, our approach can significantly improve the trained model accuracy by up to 20% and speed up the FL training progress by up to 35%.
</details>
<details>
<summary>摘要</summary>
受到数据和设备不同性的影响，联合学习（FL）的效率 часто受到数据和设备不同性的影响。数据不同性指的是客户端上的数据分布不同。设备不同性指的是客户端上的具有不同的本地硬件资源，导致异步FL Addressing staleness的问题。传统的FL方案将数据和设备不同性视为独立的两个方面，但这是在实际FL场景中不切实际的。在这些场景下，传统的权重汇集方法在FL中证明不效果，而一种更好的方法是将异步模型更新转换成非异步模型更新。在本文中，我们提出了一个新的FL框架，利用梯度反转技术来实现此类转换，从而高效地解决客户端模型更新中的无限异步问题。我们的基本思想是使用梯度反转获取客户端上传的异步模型更新中的本地训练数据估计，并使用这些估计来计算非异步客户端模型更新。这种方法可以解决使用梯度反转可能导致数据质量下降的问题，同时仍保持客户端本地数据隐私。我们与主流数据集和模型进行比较，实验结果表明，在面临无限异步情况下，我们的方法可以提高训练模型准确率达20%，并提高FL训练进度达35%。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Modeling-of-an-Unsaturated-Bentonite-Buffer-Model-Test-Under-High-Temperatures-Using-an-Enhanced-Axisymmetric-Reproducing-Kernel-Particle-Method"><a href="#Data-Driven-Modeling-of-an-Unsaturated-Bentonite-Buffer-Model-Test-Under-High-Temperatures-Using-an-Enhanced-Axisymmetric-Reproducing-Kernel-Particle-Method" class="headerlink" title="Data-Driven Modeling of an Unsaturated Bentonite Buffer Model Test Under High Temperatures Using an Enhanced Axisymmetric Reproducing Kernel Particle Method"></a>Data-Driven Modeling of an Unsaturated Bentonite Buffer Model Test Under High Temperatures Using an Enhanced Axisymmetric Reproducing Kernel Particle Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13519">http://arxiv.org/abs/2309.13519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonghyuk Baek, Yanran Wang, Xiaolong He, Yu Lu, John S. McCartney, J. S. Chen</li>
<li>for: 研究高级核废物深层地ологиRepository中焊铁粉抑融解行为下的高温环境下的焊铁粉 buffer 的THM行为。</li>
<li>methods: 使用深度神经网络(DNN)来模拟焊铁粉的水含量曲线，并将其 integrate into a Reproducing Kernel Particle Method (RKPM) 进行THM simulations。</li>
<li>results: 通过模拟一个焊铁粉层在中央加热的 tank-scale 实验，提出了一种新的抽象基函数，以便更好地模拟焊铁粉的THM行为。<details>
<summary>Abstract</summary>
In deep geological repositories for high level nuclear waste with close canister spacings, bentonite buffers can experience temperatures higher than 100 {\deg}C. In this range of extreme temperatures, phenomenological constitutive laws face limitations in capturing the thermo-hydro-mechanical (THM) behavior of the bentonite, since the pre-defined functional constitutive laws often lack generality and flexibility to capture a wide range of complex coupling phenomena as well as the effects of stress state and path dependency. In this work, a deep neural network (DNN)-based soil-water retention curve (SWRC) of bentonite is introduced and integrated into a Reproducing Kernel Particle Method (RKPM) for conducting THM simulations of the bentonite buffer. The DNN-SWRC model incorporates temperature as an additional input variable, allowing it to learn the relationship between suction and degree of saturation under the general non-isothermal condition, which is difficult to represent using a phenomenological SWRC. For effective modeling of the tank-scale test, new axisymmetric Reproducing Kernel basis functions enriched with singular Dirichlet enforcement representing heater placement and an effective convective heat transfer coefficient representing thin-layer composite tank construction are developed. The proposed method is demonstrated through the modeling of a tank-scale experiment involving a cylindrical layer of MX-80 bentonite exposed to central heating.
</details>
<details>
<summary>摘要</summary>
高度地储存核电废弃物的深层地储Repository中，бенто纳缓冲可能会面临高温（超过100℃），这个范围内的温度范围可能会导致现象学的定量关系不够捕捉潮湿-热-机械（THM）行为，因为现象学的定量关系通常缺乏普遍性和灵活性，无法捕捉各种复杂的交互效应以及压力状态和路径依赖的影响。在这种情况下，一种深度神经网络（DNN）基于的泥土水吸辊曲线（SWRC）模型被引入，并与基于 reproduce kernel particle method（RKPM）的THM模拟方法相结合。DNN-SWRC模型包含温度作为输入变量，以便学习在一般非同温度条件下湿度和吸附之间的关系，这是现象学SWRC难以表示的。为了有效地模拟储存试验，新的轴对称 reproduce kernel基函数，包括热器设置和热传递系数，被开发出来。该方法在一个筒形储存试验中，涉及一层MX-80泥土，在中央加热情况下进行模拟。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/cs.LG_2023_09_24/" data-id="clp88dbyp00s2ob88afeefaf9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/eess.IV_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T09:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/24/eess.IV_2023_09_24/">eess.IV - 2023-09-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Autopet-Challenge-2023-nnUNet-based-whole-body-3D-PET-CT-Tumour-Segmentation"><a href="#Autopet-Challenge-2023-nnUNet-based-whole-body-3D-PET-CT-Tumour-Segmentation" class="headerlink" title="Autopet Challenge 2023: nnUNet-based whole-body 3D PET-CT Tumour Segmentation"></a>Autopet Challenge 2023: nnUNet-based whole-body 3D PET-CT Tumour Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13675">http://arxiv.org/abs/2309.13675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anissa Alloula, Daniel R McGowan, Bartłomiej W. Papież</li>
<li>For: 这个论文的目的是用nnUNet进行全身PET-CT扫描中的肿瘤分 segmentation，并对不同的训练和后处理策略进行调查。* Methods: 这个论文使用的方法是nnUNet，并进行了不同的训练和后处理策略的调查。* Results: 这个论文的最佳模型在内部测试集上获得了69%的Dice分数和6.27 mL的假正和5.78 mL的假负量。<details>
<summary>Abstract</summary>
Fluorodeoxyglucose Positron Emission Tomography (FDG-PET) combined with Computed Tomography (CT) scans are critical in oncology to the identification of solid tumours and the monitoring of their progression. However, precise and consistent lesion segmentation remains challenging, as manual segmentation is time-consuming and subject to intra- and inter-observer variability. Despite their promise, automated segmentation methods often struggle with false positive segmentation of regions of healthy metabolic activity, particularly when presented with such a complex range of tumours across the whole body. In this paper, we explore the application of the nnUNet to tumour segmentation of whole-body PET-CT scans and conduct different experiments on optimal training and post-processing strategies. Our best model obtains a Dice score of 69\% and a false negative and false positive volume of 6.27 and 5.78 mL respectively, on our internal test set. This model is submitted as part of the autoPET 2023 challenge. Our code is available at: https://github.com/anissa218/autopet\_nnunet
</details>
<details>
<summary>摘要</summary>
fluorodeoxyglucose positron emission tomography（FDG-PET）与计算机扫描（CT）扫描结合是肿瘤诊断和肿瘤进展评估中非常重要。然而，准确和一致性的肿瘤分割仍然是一项挑战，因为手动分割时间费时且存在内外观察者差异。尽管自动分割方法在承诺的表现不佳，特别是在面临整个身体的复杂肿瘤时，容易出现健康代谢活动的假阳性分割。在这篇论文中，我们探讨使用nnuNet进行肿瘤分割的整体PET-CT扫描，并进行了不同的训练和后处理策略的试验。我们的最佳模型在我们的内部测试集上得到了69%的Dice分数和6.27和5.78 mL的假阴性和假正面量。这个模型已经被提交到autoPET 2023挑战中。我们的代码可以在以下链接中找到：https://github.com/anissa218/autopet_nnunet。
</details></li>
</ul>
<hr>
<h2 id="Sparsity-regularized-coded-ptychography-for-robust-and-efficient-lensless-microscopy-on-a-chip"><a href="#Sparsity-regularized-coded-ptychography-for-robust-and-efficient-lensless-microscopy-on-a-chip" class="headerlink" title="Sparsity-regularized coded ptychography for robust and efficient lensless microscopy on a chip"></a>Sparsity-regularized coded ptychography for robust and efficient lensless microscopy on a chip</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13611">http://arxiv.org/abs/2309.13611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ninghe Liu, Qianhao Zhao, Guoan Zheng</li>
<li>for: 提高ptychographic imaging的快速性和高分辨率</li>
<li>methods: 使用稀烈约束来减少测量频率，并通过梯度下降法进行优化</li>
<li>results: 能够生成高精度的重建图像，只需要八个Intensity测量值，并且可以在各种光学设备上实验 validate<details>
<summary>Abstract</summary>
In ptychographic imaging, the trade-off between the number of acquisitions and the resultant imaging quality presents a complex optimization problem. Increasing the number of acquisitions typically yields reconstructions with higher spatial resolution and finer details. Conversely, a reduction in measurement frequency often compromises the quality of the reconstructed images, manifesting as increased noise and coarser details. To address this challenge, we employ sparsity priors to reformulate the ptychographic reconstruction task as a total variation regularized optimization problem. We introduce a new computational framework, termed the ptychographic proximal total-variation (PPTV) solver, designed to integrate into existing ptychography settings without necessitating hardware modifications. Through comprehensive numerical simulations, we validate that PPTV-driven coded ptychography is capable of producing highly accurate reconstructions with a minimal set of eight intensity measurements. Convergence analysis further substantiates the robustness, stability, and computational feasibility of the proposed PPTV algorithm. Experimental results obtained from optical setups unequivocally demonstrate that the PPTV algorithm facilitates high-throughput, high-resolution imaging while significantly reducing the measurement burden. These findings indicate that the PPTV algorithm has the potential to substantially mitigate the resource-intensive requirements traditionally associated with high-quality ptychographic imaging, thereby offering a pathway toward the development of more compact and efficient ptychographic microscopy systems.
</details>
<details>
<summary>摘要</summary>
在ptychographic imaging中，数据量和图像质量之间的交换存在一个复杂的优化问题。增加数据量通常会提高图像的空间分辨率和细节，而减少测量频率则可能会丑化图像的重建效果，表现为增加杂变和粗化细节。为解决这个挑战，我们利用简约约束来修改ptychographic重建任务，将其转化为一个total variation regularized优化问题。我们提出了一种新的计算框架，名为ptychographic proximal total-variation（PPTV）解决方案，可以无需修改现有的ptychography设备。通过广泛的数字实验，我们验证了PPTV驱动的coded ptychography可以生成高精度的重建图像，只需要八个Intensity测量。对于PPTV算法的收敛分析，我们进一步证明了其稳定性、计算可行性和robustness。实验结果表明，PPTV算法可以提高高速、高分辨率的图像重建，同时减少测量负担。这些发现表明，PPTV算法有可能大幅减少传统ptychographic imaging中的资源占用，从而开 up a new pathway towards the development of more compact and efficient ptychographic microscopy systems。
</details></li>
</ul>
<hr>
<h2 id="MediViSTA-SAM-Zero-shot-Medical-Video-Analysis-with-Spatio-temporal-SAM-Adaptation"><a href="#MediViSTA-SAM-Zero-shot-Medical-Video-Analysis-with-Spatio-temporal-SAM-Adaptation" class="headerlink" title="MediViSTA-SAM: Zero-shot Medical Video Analysis with Spatio-temporal SAM Adaptation"></a>MediViSTA-SAM: Zero-shot Medical Video Analysis with Spatio-temporal SAM Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13539">http://arxiv.org/abs/2309.13539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sekeun Kim, Kyungsang Kim, Jiang Hu, Cheng Chen, Zhiliang Lyu, Ren Hui, Sunghwan Kim, Zhengliang Liu, Aoxiao Zhong, Xiang Li, Tianming Liu, Quanzheng Li</li>
<li>for: 这篇研究旨在适应医疗影像分类任务中使用Segmentation Anything Model (SAM)。</li>
<li>methods: 这篇研究引入了一种名为MediViSTA-SAM的新方法，它是一种适应医疗影像分类的Video Segmentation方法，使用了类似框架的对应运算，以及多尺度融合。</li>
<li>results: 实验结果显示，MediViSTA-SAM可以实现高准确性和有效性在医疗影像分类任务中。<details>
<summary>Abstract</summary>
In recent years, the Segmentation Anything Model (SAM) has attracted considerable attention as a foundational model well-known for its robust generalization capabilities across various downstream tasks. However, SAM does not exhibit satisfactory performance in the realm of medical image analysis. In this study, we introduce the first study on adapting SAM on video segmentation, called MediViSTA-SAM, a novel approach designed for medical video segmentation. Given video data, MediViSTA, spatio-temporal adapter captures long and short range temporal attention with cross-frame attention mechanism effectively constraining it to consider the immediately preceding video frame as a reference, while also considering spatial information effectively. Additionally, it incorporates multi-scale fusion by employing a U-shaped encoder and a modified mask decoder to handle objects of varying sizes. To evaluate our approach, extensive experiments were conducted using state-of-the-art (SOTA) methods, assessing its generalization abilities on multi-vendor in-house echocardiography datasets. The results highlight the accuracy and effectiveness of our network in medical video segmentation.
</details>
<details>
<summary>摘要</summary>
Recently, the Segmentation Anything Model (SAM) has gained significant attention as a foundational model known for its robust generalization capabilities across various downstream tasks. However, SAM does not exhibit satisfactory performance in the field of medical image analysis. In this study, we introduce the first study on adapting SAM for video segmentation, called MediViSTA-SAM, a novel approach designed for medical video segmentation. Given video data, MediViSTA, a spatio-temporal adapter, captures long and short range temporal attention with a cross-frame attention mechanism, effectively constraining it to consider the immediately preceding video frame as a reference while also considering spatial information effectively. Additionally, it incorporates multi-scale fusion by employing a U-shaped encoder and a modified mask decoder to handle objects of varying sizes. To evaluate our approach, extensive experiments were conducted using state-of-the-art (SOTA) methods, assessing its generalization abilities on multi-vendor in-house echocardiography datasets. The results highlight the accuracy and effectiveness of our network in medical video segmentation.Here's the word-for-word translation of the text into Simplified Chinese:近年来，Segmentation Anything Model（SAM）已经吸引了较大的关注，作为许多下游任务的基础模型，其robust generalization能力在各种领域得到了证明。然而，SAM在医学影像分析领域表现不 satisfactory。在这种研究中，我们介绍了首个采用SAM进行视频分 segmentation的研究，称为MediViSTA-SAM，这是一种专门为医学视频分 segmentation设计的新方法。给定视频数据，MediViSTA使用空间temporal adapter， capture long和short range temporal attention，通过跨帧注意力机制，有效地将其限制为考虑 immediately preceding video frame作为参考，同时也考虑空间信息。此外，它还 incorporates multi-scale fusion，通过使用U型编码器和修改的mask decoder来处理各种大小的对象。为了评估我们的方法，我们进行了广泛的实验，使用现有的state-of-the-art方法，评估我们的网络在医学视频分 segmentation领域的普适性。结果表明，我们的网络在医学视频分 segmentation中具有高度的准确性和有效性。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-workflow-for-accelerated-industrial-X-ray-Computed-Tomography"><a href="#Deep-learning-based-workflow-for-accelerated-industrial-X-ray-Computed-Tomography" class="headerlink" title="Deep learning based workflow for accelerated industrial X-ray Computed Tomography"></a>Deep learning based workflow for accelerated industrial X-ray Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.14371">http://arxiv.org/abs/2309.14371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Obaidullah Rahman, Singanallur V. Venkatakrishnan, Luke Scime, Paul Brackman, Curtis Frederick, Ryan Dehoff, Vincent Paquit, Amirkoushyar Ziabari</li>
<li>for: 用于高精度非 destruktive characterization of additively-manufactured metal components</li>
<li>methods: 使用两个神经网络来获得快速加速的重建</li>
<li>results: 可以准确地检测瑕疵和损害，并且可以robustly generalizes across several alloys和不同的缺乏数据情况<details>
<summary>Abstract</summary>
X-ray computed tomography (XCT) is an important tool for high-resolution non-destructive characterization of additively-manufactured metal components. XCT reconstructions of metal components may have beam hardening artifacts such as cupping and streaking which makes reliable detection of flaws and defects challenging. Furthermore, traditional workflows based on using analytic reconstruction algorithms require a large number of projections for accurate characterization - leading to longer measurement times and hindering the adoption of XCT for in-line inspections. In this paper, we introduce a new workflow based on the use of two neural networks to obtain high-quality accelerated reconstructions from sparse-view XCT scans of single material metal parts. The first network, implemented using fully-connected layers, helps reduce the impact of BH in the projection data without the need of any calibration or knowledge of the component material. The second network, a convolutional neural network, maps a low-quality analytic 3D reconstruction to a high-quality reconstruction. Using experimental data, we demonstrate that our method robustly generalizes across several alloys, and for a range of sparsity levels without any need for retraining the networks thereby enabling accurate and fast industrial XCT inspections.
</details>
<details>
<summary>摘要</summary>
X射 Computed Tomography (XCT) 是一种重要的不破坏性高分辨材料成型件的测量工具。 XCT 重建结果可能受到材料硬化的影响，导致识别瑕疵和缺陷困难。此外，传统的工作流程基于使用分析重建算法，需要较多的投射来进行准确的测量 - 导致测量时间长，阻碍 XCT 在生产线上的应用。在这篇论文中，我们介绍了一种新的工作流程，基于使用两个神经网络来从稀疏视图 XCT 扫描数据中获得高质量加速重建。首先，我们使用全连接层实现的第一个神经网络，帮助减少投射数据中的硬化效应，无需任何准备或组合物质知识。其次，我们使用卷积神经网络将低质量的分析3D重建映射到高质量的重建。使用实验数据，我们表明了我们的方法可靠地在不同的合金和稀疏程度上进行泛化，无需任何重新训练神经网络，以便快速和准确地进行工业 XCT 检测。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/eess.IV_2023_09_24/" data-id="clp88dc5s01auob887ewl6m7r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_24" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/24/eess.SP_2023_09_24/" class="article-date">
  <time datetime="2023-09-24T08:00:00.000Z" itemprop="datePublished">2023-09-24</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/24/eess.SP_2023_09_24/">eess.SP - 2023-09-24</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Non-Uniform-Sampling-Reconstruction-for-Symmetrical-NMR-Spectroscopy-by-Exploiting-Inherent-Symmetry"><a href="#Non-Uniform-Sampling-Reconstruction-for-Symmetrical-NMR-Spectroscopy-by-Exploiting-Inherent-Symmetry" class="headerlink" title="Non-Uniform Sampling Reconstruction for Symmetrical NMR Spectroscopy by Exploiting Inherent Symmetry"></a>Non-Uniform Sampling Reconstruction for Symmetrical NMR Spectroscopy by Exploiting Inherent Symmetry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13660">http://arxiv.org/abs/2309.13660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enping Lin, Ze Fang, Yuqing Huang, Yu Yang, Zhong Chen</li>
<li>For: The paper is written for researchers and scientists who use NMR spectroscopy to study biological macromolecules, specifically those who use multidimensional NMR spectroscopy and non-uniform sampling (NUS) techniques.* Methods: The paper proposes a new sampling schedule called SCPG (Symmetrical Copy Poisson Gap) and uses compressed sensing (CS) methods for reconstruction. The authors theoretically prove that the symmetrical constraint in SCPG is equivalent to sparsity, which improves the accuracy of NUS reconstruction.* Results: The authors show that the proposed SCPG sampling schedule outperforms state-of-the-art 2D Woven PG in NUS reconstruction for symmetrical NMR spectroscopy, both in simulated and experimental data.Here are the three points in Simplified Chinese text:</li>
<li>for: 本文是为研究生物 macromolecules 的研究人员和科学家编写的，尤其是使用多维度 NMR  спектроскопия和非均匀抽样 (NUS) 技术。</li>
<li>methods: 本文提出了一种新的抽样时间表 called SCPG (Symmetrical Copy Poisson Gap)，并使用压缩感知 (CS) 方法进行重建。作者理论上证明 SCPG 中的对称约束等效地实现了简约性。</li>
<li>results: 作者表明，SCPG 抽样时间表在对 symmetrical NMR  спектроскопия的 NUS 重建中比 state-of-the-art 2D Woven PG 高效， both in 模拟和实验数据中。<details>
<summary>Abstract</summary>
Symmetrical NMR spectroscopy constitutes a vital branch of multidimensional NMR spectroscopy, providing a powerful tool for the structural elucidation of biological macromolecules. Non-Uniform Sampling (NUS) serves as an effective strategy for averting the prohibitive acquisition time of multidimensional NMR spectroscopy by only sampling a few points according to NUS sampling schedules and reconstructing missing points via algorithms. However, current sampling schedules are unable to maintain the accurate recovery of cross peaks that are weak but important. In this work, we propose a novel sampling schedule termed as SCPG (Symmetrical Copy Poisson Gap) and employ CS (Compressed Sensing) methods for reconstruction. We theoretically prove that the symmetrical constraint, apart from sparsity, is implicitly implemented when SCPG is combined with CS methods. The simulated and experimental data substantiate the advantage of SCPG over state-of-the-art 2D Woven PG in the NUS reconstruction of symmetrical NMR spectroscopy.
</details>
<details>
<summary>摘要</summary>
同对称NMR光谱学是生物大分子结构解析的重要分支，具有强大的工具。非均匀抽样（NUS）是一种有效的策略，以减少多维度NMR光谱学的质量点扩展时间。然而，目前的抽样计划无法确保强度较弱但重要的交叉峰织入的精确重建。在这个工作中，我们提出一个新的抽样计划，称为SCPG（对称复复点差隔），并使用CS（压缩感知）方法进行重建。我们 teorically证明，在SCPG与CS方法的结合下，还会隐式地实现对称限制。实验和资料 validate SCPG的优势，比顶部的2D维织PG在NUS重建中。
</details></li>
</ul>
<hr>
<h2 id="6G-Positioning-and-Sensing-Through-the-Lens-of-Sustainability-Inclusiveness-and-Trustworthiness"><a href="#6G-Positioning-and-Sensing-Through-the-Lens-of-Sustainability-Inclusiveness-and-Trustworthiness" class="headerlink" title="6G Positioning and Sensing Through the Lens of Sustainability, Inclusiveness, and Trustworthiness"></a>6G Positioning and Sensing Through the Lens of Sustainability, Inclusiveness, and Trustworthiness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13602">http://arxiv.org/abs/2309.13602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henk Wymeersch, Hui Chen, Hao Guo, Musa Furkan Keskin, Bahare M. Khorsandi, Mohammad H. Moghaddam, Alejandro Ramirez, Kim Schindhelm, Athanasios Stavridis, Tommy Svensson, Vijaya Yajnanarayana</li>
<li>for: 本研究旨在探讨6G技术如何实现可持续、包容和可信worthiness的价值观念，并与传统的性能指标之间的关系。</li>
<li>methods: 本研究采用了文献综述和理论分析的方法，探讨6G技术的可持续性、包容性和可信worthiness的实现方式，以及这些价值观念与传统的性能指标之间的关系。</li>
<li>results: 本研究发现，6G技术可以通过增强位置和感知的集成来提高通信性能，同时也可以实现包容性和可信worthiness的价值观念。然而，这些价值观念与传统的性能指标之间存在融合关系，需要在设计和实现6G技术时进行综合考虑。<details>
<summary>Abstract</summary>
6G promises a paradigm shift in which positioning and sensing are inherently integrated, enhancing not only the communication performance but also enabling location- and context-aware services. Historically, positioning and sensing have been viewed through the lens of cost and performance trade-offs, implying an escalated demand for resources, such as radio, physical, and computational resources, for improved performance. However, 6G goes beyond this traditional perspective to encompass a set of broader values, namely sustainability, inclusiveness, and trustworthiness. This paper aims to: (i) shed light on these important value indicators and their relationship with the conventional key performance indicators, and (ii) unveil the dual nature of 6G in relation to these key value indicators (i.e., ensuring operation according to the values and enabling services that affect the values).
</details>
<details>
<summary>摘要</summary>
6G 承诺一种 Paradigm shift， Positioning 和 Sensing 被内置地集成，不仅提高了通信性能，还启用了 Location-和 Context-aware 服务。历史上，Positioning 和 Sensing 通常被视为成本和性能之间的贸易OFF，这意味着需要更多的 radio、物理和计算资源来提高性能。然而，6G 超越了传统的视角，涵盖更广泛的价值观念，包括可持续性、包容性和信任性。本文的目标是：（i）探讨这些重要的价值指标与传统的关键性能指标之间的关系，（ii）揭示 6G 对这些价值指标的双重性质（即，根据价值来运行并提供影响价值的服务）。
</details></li>
</ul>
<hr>
<h2 id="Identification-of-Ghost-Targets-for-Automotive-Radar-in-the-Presence-of-Multipath"><a href="#Identification-of-Ghost-Targets-for-Automotive-Radar-in-the-Presence-of-Multipath" class="headerlink" title="Identification of Ghost Targets for Automotive Radar in the Presence of Multipath"></a>Identification of Ghost Targets for Automotive Radar in the Presence of Multipath</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13585">http://arxiv.org/abs/2309.13585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Le Zheng, Jiamin Long, Marco Lops, Fan Liu, Xueyao Hu<br>for:The paper is written for detecting the presence of ghosts in automotive radar systems due to multipath.methods:The paper uses a composite hypothesis testing approach based on the Generalized Likelihood Ratio Test (GLRT) philosophy, combined with a sparsity-enforced Compressed Sensing (CS) approach and Levenberg-Marquardt (LM) optimization to estimate the angular parameters in the continuous domain.results:The paper provides an extensive performance analysis to validate the proposed solution for detecting ghosts in automotive radar systems.<details>
<summary>Abstract</summary>
Colocated multiple-input multiple-output (MIMO) technology has been widely used in automotive radars as it provides accurate angular estimation of the objects with relatively small number of transmitting and receiving antennas. Since the Direction Of Departure (DOD) and the Direction Of Arrival (DOA) of line-of-sight targets coincide, MIMO signal processing allows forming a larger virtual array for angle finding. However, multiple paths impinging the receiver is a major limiting factor, in that radar signals may bounce off obstacles, creating echoes for which the DOD does not equal the DOA. Thus, in complex scenarios with multiple scatterers, the direct paths of the intended targets may be corrupted by indirect paths from other objects, which leads to inaccurate angle estimation or ghost targets. In this paper, we focus on detecting the presence of ghosts due to multipath by regarding it as the problem of deciding between a composite hypothesis, ${\cal H}_0$ say, that the observations only contain an unknown number of direct paths sharing the same (unknown) DOD's and DOA's, and a composite alternative, ${\cal H}_1$ say, that the observations also contain an unknown number of indirect paths, for which DOD's and DOA's do not coincide. We exploit the Generalized Likelihood Ratio Test (GLRT) philosophy to determine the detector structure, wherein the unknown parameters are replaced by carefully designed estimators. The angles of both the active direct paths and of the multi-paths are indeed estimated through a sparsity-enforced Compressed Sensing (CS) approach with Levenberg-Marquardt (LM) optimization to estimate the angular parameters in the continuous domain. An extensive performance analysis is finally offered in order to validate the proposed solution.
</details>
<details>
<summary>摘要</summary>
协同多输入多出口（MIMO）技术在汽车雷达中广泛应用，因为它可以准确地估算目标物的方向，只需使用相对较少的发射和接收天线。由于发射和接收方向的DOD和DOA相同，MIMO信号处理可以组成较大的虚拟数组，以便角度测量。但是，多路射雷达信号可以受到障碍物的反射，导致信号返回不同的方向，从而导致DOD不等于DOA。因此，在复杂的多散体场景下，直接目标的直接路径可能会受到其他 объек的 indirect 路径的扰动，从而导致角度估算不准确或鬼目标。在这篇论文中，我们关注在多射场景中 Ghost 的探测，即在雷达信号中检测到不同的 DOD 和 DOA 的射频信号是否来自于直接或间接的多射。我们采用 Generalized Likelihood Ratio Test（GLRT）哲学来确定探测结构，其中未知参数被换成精心设计的估计器。雷达信号中的直接路径和多射路径的角度都是通过一种减少维度的 Compressed Sensing（CS）方法和 Levenberg-Marquardt（LM）优化来估计的。 finally，我们提供了广泛的性能分析，以验证我们的提案的可行性。
</details></li>
</ul>
<hr>
<h2 id="Sparsity-Based-Channel-Estimation-Exploiting-Deep-Unrolling-for-Downlink-Massive-MIMO"><a href="#Sparsity-Based-Channel-Estimation-Exploiting-Deep-Unrolling-for-Downlink-Massive-MIMO" class="headerlink" title="Sparsity-Based Channel Estimation Exploiting Deep Unrolling for Downlink Massive MIMO"></a>Sparsity-Based Channel Estimation Exploiting Deep Unrolling for Downlink Massive MIMO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.13545">http://arxiv.org/abs/2309.13545</a></li>
<li>repo_url: None</li>
<li>paper_authors: An Chen, Wenbo Xu, Liyang Lu, Yue Wang</li>
<li>for: 提高5G无线通信系统中大量多输入多出力（MIMO）的spectrum和能量效率，避免过多的射频过头增加频率占用。</li>
<li>methods: 通过抽象学模型驱动的压缩感知（CS）和数据驱动的深度卷积技术相结合，实现hybrid通道估计方案，包括粗略估计部分和精度修正部分，分别利用多普勒频率域和时域频率域的频率稀热性来大幅减少射频过头。</li>
<li>results: 理论结果表明，提案的方案可以减少射频过头量化频率域和时域频率域的频率稀热性，以实现低射频过头的多输入多出力通道估计，同时保证估计精度。实验结果表明，对于5G FDD巨量MIMO系统，提案的方案可以减少射频过头量化80%以上，而且估计精度与传统CS方案相当。<details>
<summary>Abstract</summary>
Massive multiple-input multiple-output (MIMO) enjoys great advantage in 5G wireless communication systems owing to its spectrum and energy efficiency. However, hundreds of antennas require large volumes of pilot overhead to guarantee reliable channel estimation in FDD massive MIMO system. Compressive sensing (CS) has been applied for channel estimation by exploiting the inherent sparse structure of massive MIMO channel but suffer from high complexity. To overcome this challenge, this paper develops a hybrid channel estimation scheme by integrating the model-driven CS and data-driven deep unrolling technique. The proposed scheme consists of a coarse estimation part and a fine correction part to respectively exploit the inter- and intraframe sparsities of channels to greatly reduce the pilot overhead. Theoretical result is provided to indicate the convergence of the fine correction and coarse estimation net. Simulation results are provided to verify that our scheme can estimate MIMO channels with low pilot overhead while guaranteeing estimation accuracy with relatively low complexity.
</details>
<details>
<summary>摘要</summary>
大量多输入多输出（MIMO）在5G无线通信系统中具有优异的优势，主要是在频率和能量方面。然而，数百个天线需要大量的射频过头来保证可靠的通道估计在FDD大量MIMO系统中。压缩感知（CS）已经应用于通道估计中，利用大量MIMO通道的自然稀畴结构。然而，它受到高复杂性的挑战。为了解决这个挑战，本文提出了一种混合模型驱动CS和数据驱动深层卷积技术的混合通道估计方案。该方案包括粗略估计部分和精度修正部分，分别利用通道之间和通道内部的稀畴性来大幅减少射频过头。我们提供了理论结果，证明了精度修正和粗略估计网的共振。实验结果表明，我们的方案可以在低射频过头下Estimation MIMO通道的精度，而且与相对较低的复杂性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/24/eess.SP_2023_09_24/" data-id="clp88dc7h01evob889ehvhxli" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/41/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/40/">40</a><a class="page-number" href="/page/41/">41</a><span class="page-number current">42</span><a class="page-number" href="/page/43/">43</a><a class="page-number" href="/page/44/">44</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/43/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
