
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/56/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.CV_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T13:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.CV_2023_09_05/">cs.CV - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Compressing-Vision-Transformers-for-Low-Resource-Visual-Learning"><a href="#Compressing-Vision-Transformers-for-Low-Resource-Visual-Learning" class="headerlink" title="Compressing Vision Transformers for Low-Resource Visual Learning"></a>Compressing Vision Transformers for Low-Resource Visual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02617">http://arxiv.org/abs/2309.02617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chensy7/efficient-vit">https://github.com/chensy7/efficient-vit</a></li>
<li>paper_authors: Eric Youn, Sai Mitheran J, Sanjana Prabhu, Siyuan Chen</li>
<li>for: 这个研究的目的是将视Transformer（ViT）和其变体带到边缘环境中，以提高视觉学习的可行性和效率。</li>
<li>methods: 我们使用了各种实用的模型压缩技术，包括传承、剪裁和量化，以降低ViT的模型大小和 Compute重量，并且保持与顶尖ViT的几乎相同的准确性。</li>
<li>results: 我们的实现可以在NVIDIA Jetson Nano（4GB）上实现快速的视觉 trasformer 推断，并且与顶尖ViT的准确性几乎相同，具体来说，我们在ImageNet题库上的Top-1准确性为85.3%，比顶尖ViT-B的86.3%高出0.6%。<details>
<summary>Abstract</summary>
Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. As a result, their deployment on mobile and edge scenarios is limited. In our work, we aim to take a step toward bringing vision transformers to the edge by utilizing popular model compression techniques such as distillation, pruning, and quantization.   Our chosen application environment is an unmanned aerial vehicle (UAV) that is battery-powered and memory-constrained, carrying a single-board computer on the scale of an NVIDIA Jetson Nano with 4GB of RAM. On the other hand, the UAV requires high accuracy close to that of state-of-the-art ViTs to ensure safe object avoidance in autonomous navigation, or correct localization of humans in search-and-rescue. Inference latency should also be minimized given the application requirements. Hence, our target is to enable rapid inference of a vision transformer on an NVIDIA Jetson Nano (4GB) with minimal accuracy loss. This allows us to deploy ViTs on resource-constrained devices, opening up new possibilities in surveillance, environmental monitoring, etc. Our implementation is made available at https://github.com/chensy7/efficient-vit.
</details>
<details>
<summary>摘要</summary>
“视力变换器”（ViT）和其变体在视觉学领导板卡上提供了状态机器的精度，包括图像分类、物体检测和 semantics 分割，通过不同部分的视觉输入注意力和捕捉长距离的空间相关性。但这些模型很大， computation 沉重。例如，最近提出的 ViT-B 模型有 86M 参数，使其在资源有限的设备上无法进行部署。因此，我们的目标是通过使用流行的模型压缩技术，如熔炼、剪辑和量化，将视力变换器带到边缘。我们选择的应用环境是无人飞行器（UAV），它是电池电池和内存有限制的，搭载了一款基于 NVIDIA Jetson Nano 的单板计算机，具有 4GB 的 RAM。然而，UAV 需要高精度，接近状态机器精度，以确保自主导航中的物体避免和人类的correct 当地化。推理延迟应该被最小化，因为应用要求。因此，我们的目标是在 NVIDIA Jetson Nano （4GB）上快速推理一个视力变换器，并尽可能减少精度损失。这样，我们就能够将视力变换器部署到资源有限的设备上，开放出新的可能性，例如监测、环境监测等。我们的实现可以在 GitHub 上找到：https://github.com/chensy7/efficient-vit。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Pretraining-Improves-Performance-and-Inference-Efficiency-in-Multiple-Lung-Ultrasound-Interpretation-Tasks"><a href="#Self-Supervised-Pretraining-Improves-Performance-and-Inference-Efficiency-in-Multiple-Lung-Ultrasound-Interpretation-Tasks" class="headerlink" title="Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks"></a>Self-Supervised Pretraining Improves Performance and Inference Efficiency in Multiple Lung Ultrasound Interpretation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02596">http://arxiv.org/abs/2309.02596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake VanBerlo, Brian Li, Jesse Hoey, Alexander Wong</li>
<li>for: 这个研究是 investigate whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis.</li>
<li>methods: 这个研究使用了自我监督预训练，并在三个肺超声分类任务上进行了细化。</li>
<li>results: 研究结果表明，使用自我监督预训练可以提高肺超声分类任务的平均总成功率，并且可以降低总计算时间。<details>
<summary>Abstract</summary>
In this study, we investigated whether self-supervised pretraining could produce a neural network feature extractor applicable to multiple classification tasks in B-mode lung ultrasound analysis. When fine-tuning on three lung ultrasound tasks, pretrained models resulted in an improvement of the average across-task area under the receiver operating curve (AUC) by 0.032 and 0.061 on local and external test sets respectively. Compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models. When training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification. Overall, the results indicate that self-supervised pretraining is useful for producing initial weights for lung ultrasound classifiers.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了自我监督预训练是否可以生成应用于多个分类任务的脑神经网络特征提取器。当细化到三个肺超声分类任务时，预训练模型可以提高平均 across-task 接受器操作曲线（AUC）的值 by 0.032 and 0.061 on local and external test sets respectively。compact nonlinear classifiers trained on features outputted by a single pretrained model did not improve performance across all tasks; however, they did reduce inference time by 49% compared to serial execution of separate fine-tuned models。when training using 1% of the available labels, pretrained models consistently outperformed fully supervised models, with a maximum observed test AUC increase of 0.396 for the task of view classification。总的来说，结果表明自我监督预训练是肺超声分类器的初始 веса的生成的有用。
</details></li>
</ul>
<hr>
<h2 id="Anatomy-Driven-Pathology-Detection-on-Chest-X-rays"><a href="#Anatomy-Driven-Pathology-Detection-on-Chest-X-rays" class="headerlink" title="Anatomy-Driven Pathology Detection on Chest X-rays"></a>Anatomy-Driven Pathology Detection on Chest X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02578">http://arxiv.org/abs/2309.02578</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/philip-mueller/adpd">https://github.com/philip-mueller/adpd</a></li>
<li>paper_authors: Philip Müller, Felix Meissen, Johannes Brandt, Georgios Kaissis, Daniel Rueckert</li>
<li>for:  automatic interpretation of medical scans, such as chest X-rays, and providing a high level of explainability to support radiologists in making informed decisions.</li>
<li>methods:  uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies, and studies two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels.</li>
<li>results:  outperforms weakly supervised methods and fully supervised detection with limited training samples, and the MIL approach is competitive with both baseline approaches, demonstrating the potential of the proposed approach.Here’s the text in Simplified Chinese:</li>
<li>for:  automatic化医疗影像解读，如胸部X线影像，并提供高水准的解释以支持 radiologists 做出 informed 的决策。</li>
<li>methods: 使用易于注释的 bounding boxes 的 anatomical regions 作为疾病的代理，并研究两种训练方法： supervised 训练使用 anatomy-level 疾病标签，以及 multiple instance learning (MIL)  with image-level 疾病标签。</li>
<li>results:  outperforms weakly supervised methods 和仅有限的训练样本的充分 supervised detection，并且 MIL 方法与两种基eline approaches 相当，因此 demonstrates 了提案的方法的潜力。<details>
<summary>Abstract</summary>
Pathology detection and delineation enables the automatic interpretation of medical scans such as chest X-rays while providing a high level of explainability to support radiologists in making informed decisions. However, annotating pathology bounding boxes is a time-consuming task such that large public datasets for this purpose are scarce. Current approaches thus use weakly supervised object detection to learn the (rough) localization of pathologies from image-level annotations, which is however limited in performance due to the lack of bounding box supervision. We therefore propose anatomy-driven pathology detection (ADPD), which uses easy-to-annotate bounding boxes of anatomical regions as proxies for pathologies. We study two training approaches: supervised training using anatomy-level pathology labels and multiple instance learning (MIL) with image-level pathology labels. Our results show that our anatomy-level training approach outperforms weakly supervised methods and fully supervised detection with limited training samples, and our MIL approach is competitive with both baseline approaches, therefore demonstrating the potential of our approach.
</details>
<details>
<summary>摘要</summary>
医学成像检测和定义可以自动解释医疗成像，如胸部X射线扫描，并提供高水平的解释，以支持 radiologist 作出 Informed Decision。但是，标注疾病 bounding box 是一个时间消耗大的任务，因此大型公共数据集 для此目的罕见。现有的方法因此使用弱型对象检测来学习 (粗略) 疾病的 localization，但是性能有限因缺少 bounding box 监督。我们因此提出了 anatomy-driven pathology detection (ADPD)，它使用容易标注的 anatomical region bounding box 作为疾病的代理。我们研究了两种训练方法：以 anatomy-level 疾病标签进行supervised 训练和多个实例学习 (MIL) 使用 image-level 疾病标签。我们的结果表明，我们的 anatomy-level 训练方法高于弱型方法和有限训练样本的完全监督检测，而我们的 MIL 方法与两种基线方法竞争，因此证明了我们的方法的潜在性。
</details></li>
</ul>
<hr>
<h2 id="Emphysema-Subtyping-on-Thoracic-Computed-Tomography-Scans-using-Deep-Neural-Networks"><a href="#Emphysema-Subtyping-on-Thoracic-Computed-Tomography-Scans-using-Deep-Neural-Networks" class="headerlink" title="Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks"></a>Emphysema Subtyping on Thoracic Computed Tomography Scans using Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02576">http://arxiv.org/abs/2309.02576</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diagnijmegen/bodyct-dram-emph-subtype">https://github.com/diagnijmegen/bodyct-dram-emph-subtype</a></li>
<li>paper_authors: Weiyi Xie, Colin Jacobs, Jean-Paul Charbonnier, Dirk Jan Slebos, Bram van Ginneken<br>for: 这份研究目的是为了自动识别emphysema的亚型和严重程度，以便更好地管理COPD疾病和研究疾病多样性。methods: 这种方法使用了深度学习的方法来自动应用Fleischner Society的视觉分分数系统来分类emphysema的亚型和严重程度。results: 这种方法可以在9650名COPD病人中取得52%的预测精度，比之前发表的方法的45%预测精度高。此外，这种方法可以生成高分辨率的地方化活化图，可以visualizing the network predictions，同时可以计算每个肺部中emphysema的涉及率。此外，这种方法还可以超过中心lobular emphysema的预测能力，可以包括paraseptal emphysema的亚型。<details>
<summary>Abstract</summary>
Accurate identification of emphysema subtypes and severity is crucial for effective management of COPD and the study of disease heterogeneity. Manual analysis of emphysema subtypes and severity is laborious and subjective. To address this challenge, we present a deep learning-based approach for automating the Fleischner Society's visual score system for emphysema subtyping and severity analysis. We trained and evaluated our algorithm using 9650 subjects from the COPDGene study. Our algorithm achieved the predictive accuracy at 52\%, outperforming a previously published method's accuracy of 45\%. In addition, the agreement between the predicted scores of our method and the visual scores was good, where the previous method obtained only moderate agreement. Our approach employs a regression training strategy to generate categorical labels while simultaneously producing high-resolution localized activation maps for visualizing the network predictions. By leveraging these dense activation maps, our method possesses the capability to compute the percentage of emphysema involvement per lung in addition to categorical severity scores. Furthermore, the proposed method extends its predictive capabilities beyond centrilobular emphysema to include paraseptal emphysema subtypes.
</details>
<details>
<summary>摘要</summary>
正确地识别肺血液性病变的亚型和严重程度是肺部疾病管理和疾病多样性研究中的重要课题。手动分类肺血液性病变的亚型和严重程度是劳动ious和主观的。为解决这个挑战，我们提出了一个基于深度学习的方法，用于自动化肺血液性病变的Fleischner社会可视分数系统。我们在9650名COPD病人中训练和评估了我们的算法，其预测精度为52%，比前一方法的精度高出17个百分点。此外，我们的方法可以生成高分辨率的局部活化地图，用于视觉化网络预测结果，并且可以计算每个肺部中肺血液性病变的百分比参数。此外，我们的方法可以进一步扩展预测的能力，以包括肺部分 septal emphysema 亚型。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-Kidney-Layer-Segmentation-on-Whole-Slide-Imaging-using-Convolutional-Neural-Networks-and-Transformers"><a href="#Evaluation-Kidney-Layer-Segmentation-on-Whole-Slide-Imaging-using-Convolutional-Neural-Networks-and-Transformers" class="headerlink" title="Evaluation Kidney Layer Segmentation on Whole Slide Imaging using Convolutional Neural Networks and Transformers"></a>Evaluation Kidney Layer Segmentation on Whole Slide Imaging using Convolutional Neural Networks and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02563">http://arxiv.org/abs/2309.02563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhao Liu, Chenyang Qi, Shunxing Bao, Quan Liu, Ruining Deng, Yu Wang, Shilin Zhao, Haichun Yang, Yuankai Huo</li>
<li>for: automated image analysis in renal pathology</li>
<li>methods: deep learning-based approaches (CNN and Transformer segmentation)</li>
<li>results: Transformer models generally outperform CNN-based models, with a decent Mean Intersection over Union (mIoU) index and the ability to enable quantitative evaluation of renal cortical structures.Here’s the full text in Simplified Chinese:</li>
<li>for: 这些深度学习方法的应用在肾脏病理学中的自动图像分析中发挥了重要的作用。</li>
<li>methods: 这些方法包括深度学习网络（CNN）和转换器分割方法（Transformer segmentation），包括Swin-Unet、医疗转换器、TransUNet、U-Net、PSPNet和DeepLabv3+。</li>
<li>results: 我们的方法的实验结果表明，转换器模型通常比CNN模型性能更高，并且可以提供肾脏层结构的量化评估。<details>
<summary>Abstract</summary>
The segmentation of kidney layer structures, including cortex, outer stripe, inner stripe, and inner medulla within human kidney whole slide images (WSI) plays an essential role in automated image analysis in renal pathology. However, the current manual segmentation process proves labor-intensive and infeasible for handling the extensive digital pathology images encountered at a large scale. In response, the realm of digital renal pathology has seen the emergence of deep learning-based methodologies. However, very few, if any, deep learning based approaches have been applied to kidney layer structure segmentation. Addressing this gap, this paper assesses the feasibility of performing deep learning based approaches on kidney layer structure segmetnation. This study employs the representative convolutional neural network (CNN) and Transformer segmentation approaches, including Swin-Unet, Medical-Transformer, TransUNet, U-Net, PSPNet, and DeepLabv3+. We quantitatively evaluated six prevalent deep learning models on renal cortex layer segmentation using mice kidney WSIs. The empirical results stemming from our approach exhibit compelling advancements, as evidenced by a decent Mean Intersection over Union (mIoU) index. The results demonstrate that Transformer models generally outperform CNN-based models. By enabling a quantitative evaluation of renal cortical structures, deep learning approaches are promising to empower these medical professionals to make more informed kidney layer segmentation.
</details>
<details>
<summary>摘要</summary>
人类脏器Layer结构分割在人类脏器整片图像（WSI）中扮演了重要的作用，包括肾脏层、外带层、内带层和内脏层。然而，现有的手动分割过程具有劳动密集和不可靠的缺点，不适合处理大规模的数字 PATHOLOGY 图像。面对这个问题，数字肾脏 PATHOLOGY 领域已经出现了深度学习基本的方法。然而，很少有任何深度学习基本的方法应用于肾脏层结构分割。为了解决这个漏洞，本文评估了深度学习基本的方法在肾脏层结构分割中的可能性。本研究采用了代表性的卷积神经网络（CNN）和Transformer分割方法，包括Swin-Unet、医疗Transformer、TransUNet、U-Net、PSPNet和DeepLabv3+。我们对六种流行的深度学习模型进行了数据的评估，并对mouse肾脏WSIs进行了量化评估。结果表明，Transformer模型在肾脏层分割中通常表现出色，并且对比于CNN基本模型具有更高的性能。这些结果表明，通过使用深度学习方法，医生和医疗工程师可以更加准确地分割肾脏层，从而提高诊断和治疗的效果。
</details></li>
</ul>
<hr>
<h2 id="Domain-Adaptation-for-Efficiently-Fine-tuning-Vision-Transformer-with-Encrypted-Images"><a href="#Domain-Adaptation-for-Efficiently-Fine-tuning-Vision-Transformer-with-Encrypted-Images" class="headerlink" title="Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images"></a>Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02556">http://arxiv.org/abs/2309.02556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teru Nagamori, Sayaka Shiota, Hitoshi Kiya</li>
<li>for: 这个论文应用于 privacy-preserving learning、access control 和 adversarial defenses 等应用。</li>
<li>methods: 本文提出一种基于 vision transformer（ViT）的域 adapted 方法，不会对模型的精度造成衰退。</li>
<li>results: 在实验中，我们确认了提案的方法可以防止精度衰退，即使使用加密的图像，使用 CIFAR-10 和 CIFAR-100 数据集。<details>
<summary>Abstract</summary>
In recent years, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Accordingly, in this paper, we propose a novel method for fine-tuning models with transformed images under the use of the vision transformer (ViT). The proposed domain adaptation method does not cause the accuracy degradation of models, and it is carried out on the basis of the embedding structure of ViT. In experiments, we confirmed that the proposed method prevents accuracy degradation even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.
</details>
<details>
<summary>摘要</summary>
Recently, deep neural networks (DNNs) trained with transformed data have been applied to various applications such as privacy-preserving learning, access control, and adversarial defenses. However, the use of transformed data decreases the performance of models. Therefore, in this paper, we propose a novel method for fine-tuning models with transformed images based on the vision transformer (ViT). Our proposed domain adaptation method does not degrade the accuracy of models and is based on the embedding structure of ViT. In experiments, we confirmed that the proposed method maintains accuracy even when using encrypted images with the CIFAR-10 and CIFAR-100 datasets.Note: The translation is in Simplified Chinese, which is one of the two standardized Chinese writing systems. The other is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-the-Impact-of-Self-Supervised-Pretraining-for-Diagnostic-Tasks-with-Radiological-Images"><a href="#A-Survey-of-the-Impact-of-Self-Supervised-Pretraining-for-Diagnostic-Tasks-with-Radiological-Images" class="headerlink" title="A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images"></a>A Survey of the Impact of Self-Supervised Pretraining for Diagnostic Tasks with Radiological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02555">http://arxiv.org/abs/2309.02555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blake VanBerlo, Jesse Hoey, Alexander Wong</li>
<li>for: 这个论文旨在探讨自动预训练在医学影像识别和分割任务中的效果，并比较自动预训练和完全监督学习的性能。</li>
<li>methods: 这些研究使用了不同的自动预训练方法，包括contrastive learning、self-supervised learning、semi-supervised learning等。</li>
<li>results: 研究发现，自动预训练通常能够提高下游任务性能，特别是当无标例大量多于标例时。此外，自动预训练还能够减少数据量和计算成本。<details>
<summary>Abstract</summary>
Self-supervised pretraining has been observed to be effective at improving feature representations for transfer learning, leveraging large amounts of unlabelled data. This review summarizes recent research into its usage in X-ray, computed tomography, magnetic resonance, and ultrasound imaging, concentrating on studies that compare self-supervised pretraining to fully supervised learning for diagnostic tasks such as classification and segmentation. The most pertinent finding is that self-supervised pretraining generally improves downstream task performance compared to full supervision, most prominently when unlabelled examples greatly outnumber labelled examples. Based on the aggregate evidence, recommendations are provided for practitioners considering using self-supervised learning. Motivated by limitations identified in current research, directions and practices for future study are suggested, such as integrating clinical knowledge with theoretically justified self-supervised learning methods, evaluating on public datasets, growing the modest body of evidence for ultrasound, and characterizing the impact of self-supervised pretraining on generalization.
</details>
<details>
<summary>摘要</summary>
自我超视教学在提高特征表示方面的效果已经被观察到，通过利用大量未标注数据进行学习。本文总结了最近关于这一点的研究，专注于对比自我超视学习和完全超视学习在靶体表示分类和分割任务中的表现。研究发现，自我超视学习通常会提高下游任务性能，尤其是当未标注示例大大超过标注示例时。根据总体证据，提供了实践者考虑使用自我超视学习的建议。受到现有研究的限制所启发，未来研究的方向和实践被建议，如结合临床知识与理论上正确的自我超视学习方法，评估在公共数据集上，扩大有限的证据库，以及Characterizing自我超视预训练对泛化的影响。
</details></li>
</ul>
<hr>
<h2 id="A-skeletonization-algorithm-for-gradient-based-optimization"><a href="#A-skeletonization-algorithm-for-gradient-based-optimization" class="headerlink" title="A skeletonization algorithm for gradient-based optimization"></a>A skeletonization algorithm for gradient-based optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02527">http://arxiv.org/abs/2309.02527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/martinmenten/skeletonization-for-gradient-based-optimization">https://github.com/martinmenten/skeletonization-for-gradient-based-optimization</a></li>
<li>paper_authors: Martin J. Menten, Johannes C. Paetzold, Veronika A. Zimmer, Suprosanna Shit, Ivan Ezhov, Robbie Holland, Monika Probst, Julia A. Schnabel, Daniel Rueckert</li>
<li>for: This paper aims to propose a three-dimensional skeletonization algorithm that is compatible with gradient-based optimization and preserves the object’s topology.</li>
<li>methods: The proposed method is based on matrix additions and multiplications, convolutional operations, basic non-linear functions, and sampling from a uniform probability distribution, which makes it easy to implement in any major deep learning library.</li>
<li>results: The authors demonstrate the advantages of their skeletonization algorithm compared to non-differentiable, morphological, and neural-network-based baselines through benchmarking experiments. They also integrate the algorithm with two medical image processing applications that use gradient-based optimization, including deep-learning-based blood vessel segmentation and multimodal registration of the mandible in computed tomography and magnetic resonance images.<details>
<summary>Abstract</summary>
The skeleton of a digital image is a compact representation of its topology, geometry, and scale. It has utility in many computer vision applications, such as image description, segmentation, and registration. However, skeletonization has only seen limited use in contemporary deep learning solutions. Most existing skeletonization algorithms are not differentiable, making it impossible to integrate them with gradient-based optimization. Compatible algorithms based on morphological operations and neural networks have been proposed, but their results often deviate from the geometry and topology of the true medial axis. This work introduces the first three-dimensional skeletonization algorithm that is both compatible with gradient-based optimization and preserves an object's topology. Our method is exclusively based on matrix additions and multiplications, convolutional operations, basic non-linear functions, and sampling from a uniform probability distribution, allowing it to be easily implemented in any major deep learning library. In benchmarking experiments, we prove the advantages of our skeletonization algorithm compared to non-differentiable, morphological, and neural-network-based baselines. Finally, we demonstrate the utility of our algorithm by integrating it with two medical image processing applications that use gradient-based optimization: deep-learning-based blood vessel segmentation, and multimodal registration of the mandible in computed tomography and magnetic resonance images.
</details>
<details>
<summary>摘要</summary>
“骨架”是一个数位影像的简洁表示，包括其顺序结构、几何和比例。它在计算机视觉应用中具有广泛的用途，如影像描述、分割和注册。然而，骨架化仅在当今的深度学习解决方案中具有有限的应用。大多数现有的骨架化算法不可微分，使得它们与梯度基本的优化不能集成。此外，基于 morphological 操作和神经网络的兼容算法也已经提出，但它们的结果通常与真实的中间轴几何和顺序结构存在差异。本文提出了第一个可微分的三维骨架化算法，可以保持物体的顺序结构和几何。我们的方法基于矩阵添加和乘法、卷积操作、基本非线性函数和随机抽样，可以轻松地在任何主流深度学习库中实现。在 benchmarking 实验中，我们证明了我们的骨架化算法与非可微分、 morphological 和神经网络基础的参考模型相比有益。最后，我们通过将我们的算法与两个医学影像处理应用程序集成，即深度学习基于血管分割和多Modal 融合注册，证明了我们的算法的实用性。
</details></li>
</ul>
<hr>
<h2 id="GO-SLAM-Global-Optimization-for-Consistent-3D-Instant-Reconstruction"><a href="#GO-SLAM-Global-Optimization-for-Consistent-3D-Instant-Reconstruction" class="headerlink" title="GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction"></a>GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02436">http://arxiv.org/abs/2309.02436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/youmi-zym/go-slam">https://github.com/youmi-zym/go-slam</a></li>
<li>paper_authors: Youmin Zhang, Fabio Tosi, Stefano Mattoccia, Matteo Poggi</li>
<li>for: 这篇论文主要用于提出一种基于深度学习的高精度视觉SLAM框架，以实时globally optimize pose estimation和3D reconstruction。</li>
<li>methods: 该框架使用深度学习来实现 pose estimation，并且通过高效的循环关闭和在线全束调整来优化每帧的位姿估计。同时，它在运行时对 implicit和连续表示进行了修正，以确保全局一致性。</li>
<li>results: 对于多种 sintetic和实际 dataset，GO-SLAM 能够在tracking robustness和3D reconstruction accuracy方面超越现有方法。此外，GO-SLAM 可以与 monocular、stereo 和 RGB-D输入一起运行。<details>
<summary>Abstract</summary>
Neural implicit representations have recently demonstrated compelling results on dense Simultaneous Localization And Mapping (SLAM) but suffer from the accumulation of errors in camera tracking and distortion in the reconstruction. Purposely, we present GO-SLAM, a deep-learning-based dense visual SLAM framework globally optimizing poses and 3D reconstruction in real-time. Robust pose estimation is at its core, supported by efficient loop closing and online full bundle adjustment, which optimize per frame by utilizing the learned global geometry of the complete history of input frames. Simultaneously, we update the implicit and continuous surface representation on-the-fly to ensure global consistency of 3D reconstruction. Results on various synthetic and real-world datasets demonstrate that GO-SLAM outperforms state-of-the-art approaches at tracking robustness and reconstruction accuracy. Furthermore, GO-SLAM is versatile and can run with monocular, stereo, and RGB-D input.
</details>
<details>
<summary>摘要</summary>
neural implicit representations 在最近的 dense Simultaneous Localization And Mapping (SLAM) 中表现出了吸引人的结果，但是它们受到相机跟踪的积累错误和重建的扭曲影响。为了解决这些问题，我们提出了 GO-SLAM，一种基于深度学习的 dense visual SLAM 框架，在实时中全球优化姿态和3D重建。姿态估计是其核心，得益于高效的循环关闭和在线全束补做，每帧都可以利用学习的全局几何结构来优化。同时，我们在实时更新了几何和连续表示，以确保3D重建的全球一致性。实验结果表明，GO-SLAM 在跟踪稳定性和重建精度方面超越了当前的方法。此外，GO-SLAM 可以与单目、双目和 RGB-D 输入运行。
</details></li>
</ul>
<hr>
<h2 id="ReliTalk-Relightable-Talking-Portrait-Generation-from-a-Single-Video"><a href="#ReliTalk-Relightable-Talking-Portrait-Generation-from-a-Single-Video" class="headerlink" title="ReliTalk: Relightable Talking Portrait Generation from a Single Video"></a>ReliTalk: Relightable Talking Portrait Generation from a Single Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02434">http://arxiv.org/abs/2309.02434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arthur-qiu/ReliTalk">https://github.com/arthur-qiu/ReliTalk</a></li>
<li>paper_authors: Haonan Qiu, Zhaoxi Chen, Yuming Jiang, Hang Zhou, Xiangyu Fan, Lei Yang, Wayne Wu, Ziwei Liu</li>
<li>for: 生成带有声音的人物肖像图像从单视视频中</li>
<li>methods: 提出了一种基于音频特征的人脸 нормаль学习方法，并使用这些 нормаль来进行反射环境的分解</li>
<li>results: 在实验中证明了该方法的超越性，能够在单视视频中生成高质量的带有声音的人物肖像图像，并且可以适应不同的背景和照明条件<details>
<summary>Abstract</summary>
Recent years have witnessed great progress in creating vivid audio-driven portraits from monocular videos. However, how to seamlessly adapt the created video avatars to other scenarios with different backgrounds and lighting conditions remains unsolved. On the other hand, existing relighting studies mostly rely on dynamically lighted or multi-view data, which are too expensive for creating video portraits. To bridge this gap, we propose ReliTalk, a novel framework for relightable audio-driven talking portrait generation from monocular videos. Our key insight is to decompose the portrait's reflectance from implicitly learned audio-driven facial normals and images. Specifically, we involve 3D facial priors derived from audio features to predict delicate normal maps through implicit functions. These initially predicted normals then take a crucial part in reflectance decomposition by dynamically estimating the lighting condition of the given video. Moreover, the stereoscopic face representation is refined using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video. Extensive experiments validate the superiority of our proposed framework on both real and synthetic datasets. Our code is released in https://github.com/arthur-qiu/ReliTalk.
</details>
<details>
<summary>摘要</summary>
Our key insight is to decompose the portrait's reflectance from implicitly learned audio-driven facial normals and images. Specifically, we use 3D facial priors derived from audio features to predict delicate normal maps through implicit functions. These initially predicted normals then play a crucial role in reflectance decomposition by dynamically estimating the lighting condition of the given video. Additionally, we refine the stereoscopic face representation using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video.Extensive experiments demonstrate the superiority of our proposed framework on both real and synthetic datasets. Our code is available at https://github.com/arthur-qiu/ReliTalk.
</details></li>
</ul>
<hr>
<h2 id="EgoPCA-A-New-Framework-for-Egocentric-Hand-Object-Interaction-Understanding"><a href="#EgoPCA-A-New-Framework-for-Egocentric-Hand-Object-Interaction-Understanding" class="headerlink" title="EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding"></a>EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02423">http://arxiv.org/abs/2309.02423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Xu, Yong-Lu Li, Zhemin Huang, Michael Xu Liu, Cewu Lu, Yu-Wing Tai, Chi-Keung Tang</li>
<li>for: 本研究是为了提高 Egocentric Hand-Object Interaction (Ego-HOI) 识别的性能，并解决现有研究基于第三人称视频动作识别的域外差问题。</li>
<li>methods: 本研究提出了一个新的框架，称为 Probing, Curation and Adaption (EgoPCA)，用于适应 Ego-HOI 识别。该框架包括了全面的预训练集、平衡测试集以及一个新的基线。</li>
<li>results: 根据本研究的结果，新的 EgoPCA 框架可以在 Ego-HOI 标准测试集上达到状态之最的性能。此外，本研究还提出了一些新的机制和设置，以进一步推动 Ego-HOI 研究的发展。<details>
<summary>Abstract</summary>
With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI), large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed. However, most current research is built on resources derived from third-person video action recognition. This inherent domain gap between first- and third-person action videos, which have not been adequately addressed before, makes current Ego-HOI suboptimal. This paper rethinks and proposes a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy. With our new framework, we not only achieve state-of-the-art performance on Ego-HOI benchmarks but also build several new and effective mechanisms and settings to advance further research. We believe our data and the findings will pave a new way for Ego-HOI understanding. Code and data are available at https://mvig-rhos.com/ego_pca
</details>
<details>
<summary>摘要</summary>
traditional Chinese:随着 Egocentric Hand-Object Interaction (Ego-HOI) 的注意度增加，大规模的数据集如 Ego4D 和 EPIC-KITCHENS 已经被提议。然而，现今大多数研究都基于第三人称视频动作识别资源，这种内在的领域差异使当前的 Ego-HOI 产生较差的性能。这篇论文重新思考并提出了一个新的框架，以提高 Ego-HOI 识别的基础设施，称为 EgoPCA。我们提供了完整的预训练集、平衡测试集和新的基线，并提供了一种训练-微调策略。我们不仅实现了 Ego-HOI 标准 benchmarcks 上的状态最佳性能，还构建了一些新有效的机制和设置，以推进更进一步的研究。我们认为我们的数据和发现将为 Ego-HOI 的理解开拓新的道路。代码和数据可以在 <https://mvig-rhos.com/ego_pca> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Doppelgangers-Learning-to-Disambiguate-Images-of-Similar-Structures"><a href="#Doppelgangers-Learning-to-Disambiguate-Images-of-Similar-Structures" class="headerlink" title="Doppelgangers: Learning to Disambiguate Images of Similar Structures"></a>Doppelgangers: Learning to Disambiguate Images of Similar Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02420">http://arxiv.org/abs/2309.02420</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RuojinCai/Doppelgangers">https://github.com/RuojinCai/Doppelgangers</a></li>
<li>paper_authors: Ruojin Cai, Joseph Tung, Qianqian Wang, Hadar Averbuch-Elor, Bharath Hariharan, Noah Snavely</li>
<li>for: 本文针对的是解决视觉歧义问题，即判断两个visually相似的图像是否描绘同一个3D表面（例如同一个或对称的建筑物的不同面）。</li>
<li>methods: 我们提出了一种基于学习的方法，将这个问题转化为图像对的二分类问题。我们还提出了一种新的数据集Doppelgangers，包含visually相似的图像对，并设计了一种网络结构，使用local keypoints和匹配的空间分布来输入，以更好地利用本地和全局cue。</li>
<li>results: 我们的方法可以在困难的情况下分辨出illusory匹配，并可以与SfM管道集成，以生成正确、不同歧义的3D重建结果。参考我们项目页面（<a target="_blank" rel="noopener" href="http://doppelgangers-3d.github.io)可以获得我们的代码、数据集和更多结果./">http://doppelgangers-3d.github.io）可以获得我们的代码、数据集和更多结果。</a><details>
<summary>Abstract</summary>
We consider the visual disambiguation task of determining whether a pair of visually similar images depict the same or distinct 3D surfaces (e.g., the same or opposite sides of a symmetric building). Illusory image matches, where two images observe distinct but visually similar 3D surfaces, can be challenging for humans to differentiate, and can also lead 3D reconstruction algorithms to produce erroneous results. We propose a learning-based approach to visual disambiguation, formulating it as a binary classification task on image pairs. To that end, we introduce a new dataset for this problem, Doppelgangers, which includes image pairs of similar structures with ground truth labels. We also design a network architecture that takes the spatial distribution of local keypoints and matches as input, allowing for better reasoning about both local and global cues. Our evaluation shows that our method can distinguish illusory matches in difficult cases, and can be integrated into SfM pipelines to produce correct, disambiguated 3D reconstructions. See our project page for our code, datasets, and more results: http://doppelgangers-3d.github.io/.
</details>
<details>
<summary>摘要</summary>
我们考虑了视觉异常分辨任务，即确定两个视觉相似图像是否描述同一个或不同的3D表面（例如，同一面或对面的对称建筑物）。假设图像的假寻常匹配，其中两个图像可能描述不同的3D表面，但视觉上看起来很相似。这种情况可能会使人类困难于分辨，同时也可能导致3D重建算法生成错误的结果。我们提出了一种学习基于的方法来解决这个问题，将其 форulate为图像对的二分类任务。为此，我们开发了一个新的数据集，即Doppelgangers，其包括了类似结构的图像对，以及真实的标签。我们还设计了一种网络架构，该架构可以接受图像对的空间分布的本地关键点和匹配的输入，以便更好地理解本地和全局cue。我们的评估结果表明，我们的方法可以在困难的情况下分辨假寻常匹配，并可以与SfM管道集成，以生成正确、异常分辨的3D重建结果。请参考我们的项目页面获取我们的代码、数据集和更多结果：http://doppelgangers-3d.github.io/.
</details></li>
</ul>
<hr>
<h2 id="Generating-Realistic-Images-from-In-the-wild-Sounds"><a href="#Generating-Realistic-Images-from-In-the-wild-Sounds" class="headerlink" title="Generating Realistic Images from In-the-wild Sounds"></a>Generating Realistic Images from In-the-wild Sounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02405">http://arxiv.org/abs/2309.02405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/etilelab/Generating-Realistic-Images-from-In-the-wild-Sounds">https://github.com/etilelab/Generating-Realistic-Images-from-In-the-wild-Sounds</a></li>
<li>paper_authors: Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim</li>
<li>for: 本研究旨在生成野生声音中的图像，因为现有的数据集缺乏声音和图像的对应对。</li>
<li>methods: 本研究使用了音频描述、音频注意力和句子注意力来表达声音的 richttraits，并使用了 CLIPscore 和 AudioCLIP 进行直接声音优化， finally 使用了扩散模型生成图像。</li>
<li>results: 实验结果显示，本模型能够生成高质量的图像从野生声音中，并在野外音频数据集上超越基elines 的 both 量化和质量评估。<details>
<summary>Abstract</summary>
Representing wild sounds as images is an important but challenging task due to the lack of paired datasets between sound and images and the significant differences in the characteristics of these two modalities. Previous studies have focused on generating images from sound in limited categories or music. In this paper, we propose a novel approach to generate images from in-the-wild sounds. First, we convert sound into text using audio captioning. Second, we propose audio attention and sentence attention to represent the rich characteristics of sound and visualize the sound. Lastly, we propose a direct sound optimization with CLIPscore and AudioCLIP and generate images with a diffusion-based model. In experiments, it shows that our model is able to generate high quality images from wild sounds and outperforms baselines in both quantitative and qualitative evaluations on wild audio datasets.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将野声表示为图像是一项重要但具有挑战性的任务，主要因为声音和图像之间没有匹配的数据集和这两种模式之间存在重要的差异。先前的研究主要集中在限定的类别或音乐中生成图像。在这篇论文中，我们提出一种生成声音中的图像的新方法。首先，我们将声音转换为文本使用音频描述。其次，我们提出了听音注意力和句子注意力来表示声音的丰富特征和视觉化声音。最后，我们提出了直接声音优化CLIPscore和AudioCLIP，并使用扩散模型生成图像。在实验中，我们发现我们的模型能够生成高质量的图像从野声，并在野声数据集上超过基线在量和质量评估中表现出色。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Voice-Morphing-Two-Identities-in-One-Voice"><a href="#Voice-Morphing-Two-Identities-in-One-Voice" class="headerlink" title="Voice Morphing: Two Identities in One Voice"></a>Voice Morphing: Two Identities in One Voice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02404">http://arxiv.org/abs/2309.02404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Sushanta K. Pani, Anurag Chowdhury, Morgan Sandler, Arun Ross</li>
<li>for: 本研究探讨了一种基于语音特征的 morph 攻击，即 Voice Identity Morphing (VIM)，可以Synthesize speech samples that impersonate the voice characteristics of a pair of individuals。</li>
<li>methods: 研究人员使用了 ECAPA-TDNN 和 x-vector 两种常见的 speaker recognition system，并通过对 Librispeech 数据集进行实验，发现这两种系统都具有高达80%的成功率，但同时也存在1%的假阳性率。</li>
<li>results: 研究人员通过实验发现，使用 VIM 可以在 speaker recognition 系统中实现高达80%的成功率，但同时也存在1%的假阳性率。<details>
<summary>Abstract</summary>
In a biometric system, each biometric sample or template is typically associated with a single identity. However, recent research has demonstrated the possibility of generating "morph" biometric samples that can successfully match more than a single identity. Morph attacks are now recognized as a potential security threat to biometric systems. However, most morph attacks have been studied on biometric modalities operating in the image domain, such as face, fingerprint, and iris. In this preliminary work, we introduce Voice Identity Morphing (VIM) - a voice-based morph attack that can synthesize speech samples that impersonate the voice characteristics of a pair of individuals. Our experiments evaluate the vulnerabilities of two popular speaker recognition systems, ECAPA-TDNN and x-vector, to VIM, with a success rate (MMPMR) of over 80% at a false match rate of 1% on the Librispeech dataset.
</details>
<details>
<summary>摘要</summary>
在生物特征识别系统中，每个生物特征样本或模板通常与单一身份相关。然而，最近的研究已经证明可以生成"变形"生物特征样本，可以成功匹配多个身份。这种"变形攻击"被视为生物特征识别系统的安全威胁。然而，大多数变形攻击都在生物特征模式 operating in the image domain, such as face, fingerprint, and iris 中进行研究。在这项初步工作中，我们引入了语音特征变形（VIM） - 一种基于语音的变形攻击，可以生成具有两个人之间语音特征的演示样本。我们的实验发现，使用 ECAPA-TDNN 和 x-vector 两种流行的 speaker recognition 系统都受到 VIM 攻击的威胁，false match rate 为 1%，在 Librispeech 数据集上达到了80% 的成功率。
</details></li>
</ul>
<hr>
<h2 id="Prototype-based-Dataset-Comparison"><a href="#Prototype-based-Dataset-Comparison" class="headerlink" title="Prototype-based Dataset Comparison"></a>Prototype-based Dataset Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02401">http://arxiv.org/abs/2309.02401</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nanne/protosim">https://github.com/nanne/protosim</a></li>
<li>paper_authors: Nanne van Noord</li>
<li>for: 这篇论文旨在推广 dataset inspection 的思想，通过对多个 dataset 进行比较，从而超越单个 dataset 中最为显著的视觉概念的限制。</li>
<li>methods: 作者提出了一种基于自动学习的模块，可以在多个 dataset 之间进行比较，从而发现不同 dataset 中的视觉概念。该模块通过不supervised learning来学习概念水平的聚合体，并在两个案例研究中证明了其效果。</li>
<li>results: 作者的研究表明，通过对多个 dataset 进行比较，可以扩展 dataset inspection 的范畴，并且可以发现更多的视觉概念。这些发现可以帮助更多的研究人员在 dataset inspection 中做出更多的发现。<details>
<summary>Abstract</summary>
Dataset summarisation is a fruitful approach to dataset inspection. However, when applied to a single dataset the discovery of visual concepts is restricted to those most prominent. We argue that a comparative approach can expand upon this paradigm to enable richer forms of dataset inspection that go beyond the most prominent concepts. To enable dataset comparison we present a module that learns concept-level prototypes across datasets. We leverage self-supervised learning to discover these prototypes without supervision, and we demonstrate the benefits of our approach in two case-studies. Our findings show that dataset comparison extends dataset inspection and we hope to encourage more works in this direction. Code and usage instructions available at https://github.com/Nanne/ProtoSim
</details>
<details>
<summary>摘要</summary>
dataset 概述是一种有济于数据集检查的方法。然而，当应用于单个数据集时，发现视觉概念的限制只能是最显著的。我们认为 Comparative approach 可以扩展这个 парадиг，以便进行更加丰富的数据集检查，超过最显著的概念。为实现数据集比较，我们提出了一个学习概念级别的原型模块。我们利用无监督学习来发现这些原型，并在两个案例研究中证明了我们的方法的效iveness。我们发现，数据集比较可以扩展数据集检查，并希望更多的研究在这个方向上。 Code 和使用说明可以在 <https://github.com/Nanne/ProtoSim> 获取。
</details></li>
</ul>
<hr>
<h2 id="STEP-–-Towards-Structured-Scene-Text-Spotting"><a href="#STEP-–-Towards-Structured-Scene-Text-Spotting" class="headerlink" title="STEP – Towards Structured Scene-Text Spotting"></a>STEP – Towards Structured Scene-Text Spotting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02356">http://arxiv.org/abs/2309.02356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergi Garcia-Bordils, Dimosthenis Karatzas, Marçal Rusiñol</li>
<li>for:  scene-text OCR系统的结构化文本检测任务，用于根据用户提供的正则表达式动态控制场景文本检测和识别。</li>
<li>methods: 我们提出了Structured TExt sPotter（STEP）模型，利用提供的文本结构来导航OCR过程。STEP可以处理包含空格的正则表达式，并不受WORD水平精度的限制。</li>
<li>results: 我们的方法可以在各种实际阅读场景中提供准确的零shot结构化文本检测，并且只基于公开available数据进行训练。我们还 introduce了一个新的挑战性测试集，包含多种场景中的Out-of-vocabulary结构化文本，例如价格、日期、序列号、车牌等。我们的方法在所有测试场景中都能提供专业化的OCR性能。<details>
<summary>Abstract</summary>
We introduce the structured scene-text spotting task, which requires a scene-text OCR system to spot text in the wild according to a query regular expression. Contrary to generic scene text OCR, structured scene-text spotting seeks to dynamically condition both scene text detection and recognition on user-provided regular expressions. To tackle this task, we propose the Structured TExt sPotter (STEP), a model that exploits the provided text structure to guide the OCR process. STEP is able to deal with regular expressions that contain spaces and it is not bound to detection at the word-level granularity. Our approach enables accurate zero-shot structured text spotting in a wide variety of real-world reading scenarios and is solely trained on publicly available data. To demonstrate the effectiveness of our approach, we introduce a new challenging test dataset that contains several types of out-of-vocabulary structured text, reflecting important reading applications of fields such as prices, dates, serial numbers, license plates etc. We demonstrate that STEP can provide specialised OCR performance on demand in all tested scenarios.
</details>
<details>
<summary>摘要</summary>
我们介绍了结构化场景文本搜寻任务，这需要一个场景文本OCR系统在用户提供的规律表达中搜寻文本。不同于通用场景文本OCR，结构化场景文本搜寻需要在用户提供的规律下动态地控制场景文本检测和识别。为解决这个任务，我们提出了结构化文本搜寻器（STEP），这个模型利用提供的文本结构来引导OCR процес。STEP能够处理包含空格的规律表达，并不受限制于字词水平的检测。我们的方法可以在实际阅读场景中提供精确的零基eline文本搜寻，并仅从公开available的数据进行训练。为证明我们的方法的有效性，我们引入了一个新的挑战性测试数据集，这个数据集包含了多种出版 vocabulary 的结构化文本，例如价格、日期、序号、车牌号码等。我们展示了 STEP 在所有测试场景中提供特殊化 OCR 性能。
</details></li>
</ul>
<hr>
<h2 id="Generating-Infinite-Resolution-Texture-using-GANs-with-Patch-by-Patch-Paradigm"><a href="#Generating-Infinite-Resolution-Texture-using-GANs-with-Patch-by-Patch-Paradigm" class="headerlink" title="Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm"></a>Generating Infinite-Resolution Texture using GANs with Patch-by-Patch Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02340">http://arxiv.org/abs/2309.02340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai4netzero/infinite_texture_gans">https://github.com/ai4netzero/infinite_texture_gans</a></li>
<li>paper_authors: Alhasan Abdellatif, Ahmed H. Elsheikh</li>
<li>for: 生成无穷限分辨率的纹理图像</li>
<li>methods: 基于patch-by-patch paradigm的GANs方法</li>
<li>results: 比现有方法更加可扩展和灵活，可以生成任意大小的纹理图像，同时保持视觉准确性和多样性。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel approach for generating texture images of infinite resolutions using Generative Adversarial Networks (GANs) based on a patch-by-patch paradigm. Existing texture synthesis techniques often rely on generating a large-scale texture using a one-forward pass to the generating model, this limits the scalability and flexibility of the generated images. In contrast, the proposed approach trains GANs models on a single texture image to generate relatively small patches that are locally correlated and can be seamlessly concatenated to form a larger image while using a constant GPU memory footprint. Our method learns the local texture structure and is able to generate arbitrary-size textures, while also maintaining coherence and diversity. The proposed method relies on local padding in the generator to ensure consistency between patches and utilizes spatial stochastic modulation to allow for local variations and diversity within the large-scale image. Experimental results demonstrate superior scalability compared to existing approaches while maintaining visual coherence of generated textures.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的方法，使用生成抗对抗网络（GANs）来生成无限分辨率的текxture图像，基于一种patch-by-patch的方法。现有的 texture合成技术通常是通过一次forward pass来生成一个大规模的 texture，这限制了生成图像的可扩展性和灵活性。相比之下，我们的方法是在一个单个 texture 图像上训练 GANs 模型，以生成相对较小的 patches，这些 patches 是地方相关的，可以在一定的 GPU 内存占用下进行 concatenation，以生成一个更大的图像。我们的方法学习了地方 texture 结构，能够生成任意大小的 texture，同时保持了视觉准确性和多样性。我们的方法利用了 generator 中的本地补充来确保 patches 之间的一致性，并使用空间随机变化来允许本地变化和多样性在大规模图像中。实验结果表明，我们的方法比现有的方法具有更高的可扩展性，同时保持了生成图像的视觉准确性。
</details></li>
</ul>
<hr>
<h2 id="DEEPBEAS3D-Deep-Learning-and-B-Spline-Explicit-Active-Surfaces"><a href="#DEEPBEAS3D-Deep-Learning-and-B-Spline-Explicit-Active-Surfaces" class="headerlink" title="DEEPBEAS3D: Deep Learning and B-Spline Explicit Active Surfaces"></a>DEEPBEAS3D: Deep Learning and B-Spline Explicit Active Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02335">http://arxiv.org/abs/2309.02335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helena Williams, João Pedrosa, Muhammad Asad, Laura Cattani, Tom Vercauteren, Jan Deprest, Jan D’hooge</li>
<li>for: 提高 automatic segmentation 方法的robustness，以便直接应用于临床。</li>
<li>methods: 使用 B-spline explicit active surface (BEAS)  Ensures 3D segmentation 是 Smooth 而具有 анатомиче可信度，同时允许用户精确地编辑 3D 表面。</li>
<li>results: 与 clinical 工具 4D View VOCAL 相比，提出的框架具有更低的 NASA-TLX 指数（30% 减少）和用户时间（70% 减少，p&lt;0.00001）。<details>
<summary>Abstract</summary>
Deep learning-based automatic segmentation methods have become state-of-the-art. However, they are often not robust enough for direct clinical application, as domain shifts between training and testing data affect their performance. Failure in automatic segmentation can cause sub-optimal results that require correction. To address these problems, we propose a novel 3D extension of an interactive segmentation framework that represents a segmentation from a convolutional neural network (CNN) as a B-spline explicit active surface (BEAS). BEAS ensures segmentations are smooth in 3D space, increasing anatomical plausibility, while allowing the user to precisely edit the 3D surface. We apply this framework to the task of 3D segmentation of the anal sphincter complex (AS) from transperineal ultrasound (TPUS) images, and compare it to the clinical tool used in the pelvic floor disorder clinic (4D View VOCAL, GE Healthcare; Zipf, Austria). Experimental results show that: 1) the proposed framework gives the user explicit control of the surface contour; 2) the perceived workload calculated via the NASA-TLX index was reduced by 30% compared to VOCAL; and 3) it required 7 0% (170 seconds) less user time than VOCAL (p< 0.00001)
</details>
<details>
<summary>摘要</summary>
深度学习自动分割方法已成为当前状态的惯性。然而，它们常不够鲁棒，对于直接临床应用而言。领域变化导致自动分割失败，从而导致了不优化的结果，需要更正。为解决这些问题，我们提出了一种新的3D扩展的互动分割框架。这个框架将convulsion neural network（CNN）的分割表示为B-spline显式活动表面（BEAS）。BEAS使得分割在3D空间是平滑的，提高了生物学可能性，同时允许用户精确地编辑3D表面。我们在分割Transperineal ultrasound（TPUS）图像中的下部缺陷复合（AS）任务上应用了这个框架，并与临床工具used in the pelvic floor disorder clinic（4D View VOCAL，GE Healthcare，Zipf，Austria）进行比较。实验结果显示：1）提案的框架给用户显式控制表面轮廓; 2）NASA-TLX指数计算的感知工作负荷比VOCAL减少30%；3）与VOCAL相比，用户时间减少70%（p<0.00001）。
</details></li>
</ul>
<hr>
<h2 id="TiAVox-Time-aware-Attenuation-Voxels-for-Sparse-view-4D-DSA-Reconstruction"><a href="#TiAVox-Time-aware-Attenuation-Voxels-for-Sparse-view-4D-DSA-Reconstruction" class="headerlink" title="TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction"></a>TiAVox: Time-aware Attenuation Voxels for Sparse-view 4D DSA Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02318">http://arxiv.org/abs/2309.02318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghong Zhou, Huangxuan Zhao, Jiemin Fang, Dongqiao Xiang, Lei Chen, Lingxia Wu, Feihong Wu, Wenyu Liu, Chuansheng Zheng, Xinggang Wang<br>for:  This paper aims to propose a novel approach for sparse-view 4D digital subtraction angiography (DSA) reconstruction, which can reduce the radiation dose while maintaining high-quality imaging results.methods:  The proposed approach, called Time-aware Attenuation Voxel (TiAVox), utilizes 4D attenuation voxel grids to model the attenuation properties of both spatial and temporal dimensions. It is optimized by minimizing discrepancies between the rendered images and sparse 2D DSA images, without relying on any neural network.results:  The proposed TiAVox approach achieved a 31.23 Peak Signal-to-Noise Ratio (PSNR) for novel view synthesis using only 30 views on a clinically sourced dataset, outperforming traditional Feldkamp-Davis-Kress methods which required 133 views. Additionally, TiAVox yielded a PSNR of 34.32 for novel view synthesis and 41.40 for 3D reconstruction on a synthetic dataset using merely 10 views.<details>
<summary>Abstract</summary>
Four-dimensional Digital Subtraction Angiography (4D DSA) plays a critical role in the diagnosis of many medical diseases, such as Arteriovenous Malformations (AVM) and Arteriovenous Fistulas (AVF). Despite its significant application value, the reconstruction of 4D DSA demands numerous views to effectively model the intricate vessels and radiocontrast flow, thereby implying a significant radiation dose. To address this high radiation issue, we propose a Time-aware Attenuation Voxel (TiAVox) approach for sparse-view 4D DSA reconstruction, which paves the way for high-quality 4D imaging. Additionally, 2D and 3D DSA imaging results can be generated from the reconstructed 4D DSA images. TiAVox introduces 4D attenuation voxel grids, which reflect attenuation properties from both spatial and temporal dimensions. It is optimized by minimizing discrepancies between the rendered images and sparse 2D DSA images. Without any neural network involved, TiAVox enjoys specific physical interpretability. The parameters of each learnable voxel represent the attenuation coefficients. We validated the TiAVox approach on both clinical and simulated datasets, achieving a 31.23 Peak Signal-to-Noise Ratio (PSNR) for novel view synthesis using only 30 views on the clinically sourced dataset, whereas traditional Feldkamp-Davis-Kress methods required 133 views. Similarly, with merely 10 views from the synthetic dataset, TiAVox yielded a PSNR of 34.32 for novel view synthesis and 41.40 for 3D reconstruction. We also executed ablation studies to corroborate the essential components of TiAVox. The code will be publically available.
</details>
<details>
<summary>摘要</summary>
四维数字抽取成像（4D DSA）在诊断医学疾病方面发挥重要作用，如arteriovenous malformation（AVM）和arteriovenous fistula（AVF）。尽管它具有重要应用价值，但4D DSA重建需要大量视图，以模拟复杂的血管和干扰物流动，从而导致高射线剂量。为解决这个高射线问题，我们提出了基于时间意识的减杂粒子（TiAVox）方法，这种方法可以在缺少视图情况下实现高质量4D成像。此外，2D和3D DSA成像结果也可以从重建的4D DSA图像中生成。TiAVox使用4D减杂粒子网格，该网格反映了空间和时间维度中的减杂特性。它通过最小化与渲染图像之间的差异来优化。不同于使用神经网络的方法，TiAVox具有特定的物理解释性。每个学习粒子的参数表示减杂系数。我们在临床和模拟数据集上验证了TiAVox方法，在使用30个视图时，对于新视图synthesis，TiAVox方法达到了31.23的峰值信号强度比率（PSNR），而传统的Feldkamp-Davis-Kress方法需要133个视图。同样，只使用10个视图从synthetic dataset，TiAVox方法可以达到34.32的PSNR для新视图synthesis和41.40的PSNR для3D重建。我们还进行了ablation研究，以证明TiAVox的关键组件。代码将公开。
</details></li>
</ul>
<hr>
<h2 id="CIEM-Contrastive-Instruction-Evaluation-Method-for-Better-Instruction-Tuning"><a href="#CIEM-Contrastive-Instruction-Evaluation-Method-for-Better-Instruction-Tuning" class="headerlink" title="CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning"></a>CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02301">http://arxiv.org/abs/2309.02301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyu Hu, Jiyuan Zhang, Minyi Zhao, Zhenbang Sun</li>
<li>for: 本研究旨在 Addressing the hallucination phenomenon in Large Vision-Language Models (LVLMs) by introducing a Contrastive Instruction Evaluation Method (CIEM) and a new instruction tuning method called Contrastive Instruction Tuning (CIT).</li>
<li>methods: 本研究使用了一个自动化管道，包括一个注解的图像文本数据集和一个Large Language Model (LLM)，生成了事实&#x2F;对比问题对的评估，以检测LVLMs中的幻觉现象。同时，基于CIEM，我们还提出了一种新的 instruction tuning 方法，即 CIT (Contrastive Instruction Tuning)，以自动生成高质量的事实&#x2F;对比问题对和相应的证明，以适应LVLMs中的幻觉现象。</li>
<li>results: 我们通过广泛的实验表明，CIEM 和 CIT 能够准确检测LVLMs中的幻觉现象，并且CIT-调教VLMs比CIEM和公共数据集更优。<details>
<summary>Abstract</summary>
Nowadays, the research on Large Vision-Language Models (LVLMs) has been significantly promoted thanks to the success of Large Language Models (LLM). Nevertheless, these Vision-Language Models (VLMs) are suffering from the drawback of hallucination -- due to insufficient understanding of vision and language modalities, VLMs may generate incorrect perception information when doing downstream applications, for example, captioning a non-existent entity. To address the hallucination phenomenon, on the one hand, we introduce a Contrastive Instruction Evaluation Method (CIEM), which is an automatic pipeline that leverages an annotated image-text dataset coupled with an LLM to generate factual/contrastive question-answer pairs for the evaluation of the hallucination of VLMs. On the other hand, based on CIEM, we further propose a new instruction tuning method called CIT (the abbreviation of Contrastive Instruction Tuning) to alleviate the hallucination of VLMs by automatically producing high-quality factual/contrastive question-answer pairs and corresponding justifications for model tuning. Through extensive experiments on CIEM and CIT, we pinpoint the hallucination issues commonly present in existing VLMs, the disability of the current instruction-tuning dataset to handle the hallucination phenomenon and the superiority of CIT-tuned VLMs over both CIEM and public datasets.
</details>
<details>
<summary>摘要</summary>
现在，大vision-language模型（LVLM）的研究得到了大语言模型（LLM）的成功，然而这些视力语言模型（VLM）却受到了一个缺点——因为不够理解视觉和语言模式，VLM可能在下游应用中生成错误的感知信息，例如captioning一个不存在的实体。为了解决这种幻觉现象，我们在一个手动管道中引入了一种对比 instruction evaluation方法（CIEM），这种方法利用了一个注解图像文本集和一个LLM来生成factual/对比问题对的评估。此外，基于CIEM，我们进一步提出了一种新的指令调整方法called CIT（对比指令调整），以解决VLM中的幻觉问题。通过广泛的CIEM和CIT实验，我们揭示了现有VLM中的幻觉问题，存在的指令调整数据集不能处理幻觉现象，以及CIT-调整VLM的superiority。
</details></li>
</ul>
<hr>
<h2 id="ATM-Action-Temporality-Modeling-for-Video-Question-Answering"><a href="#ATM-Action-Temporality-Modeling-for-Video-Question-Answering" class="headerlink" title="ATM: Action Temporality Modeling for Video Question Answering"></a>ATM: Action Temporality Modeling for Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02290">http://arxiv.org/abs/2309.02290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwen Chen, Jie Zhu, Yu Kong</li>
<li>for: 本研究旨在提高视频问答（VideoQA）中的 causal&#x2F;temporal 理解能力，因为现有方法在面对需要跨帧 temporality 理解的问题时表现不佳。</li>
<li>methods: 本研究提出了 Action Temporality Modeling (ATM) 方法，通过三种特点：（1）重新思考optical flow的表示方式，发现optical flow可以帮助捕捉长期 temporality 理解;（2）通过对视觉和文本模式的嵌入进行对比式学习，从而提高动作表示在视觉和文本模式中的性能;（3）在精度调整阶段避免使用混乱视频，以避免因为出现和运动的杂交关系而导致的假 positives。</li>
<li>results: 实验表明，ATM方法比前一些方法在多个 VideoQA 任务上表现更高的准确率，同时也能够更好地保持true temporality 理解能力。<details>
<summary>Abstract</summary>
Despite significant progress in video question answering (VideoQA), existing methods fall short of questions that require causal/temporal reasoning across frames. This can be attributed to imprecise motion representations. We introduce Action Temporality Modeling (ATM) for temporality reasoning via three-fold uniqueness: (1) rethinking the optical flow and realizing that optical flow is effective in capturing the long horizon temporality reasoning; (2) training the visual-text embedding by contrastive learning in an action-centric manner, leading to better action representations in both vision and text modalities; and (3) preventing the model from answering the question given the shuffled video in the fine-tuning stage, to avoid spurious correlation between appearance and motion and hence ensure faithful temporality reasoning. In the experiments, we show that ATM outperforms previous approaches in terms of the accuracy on multiple VideoQAs and exhibits better true temporality reasoning ability.
</details>
<details>
<summary>摘要</summary>
尽管现有的视频问答（VideoQA）技术已经取得了显著的进步，但现有的方法仍然无法解决需要时间/ causal 逻辑推理的问题。这可以归结于不精准的动作表示。我们提出了动作时间模型（ATM），通过三种独特性来进行时间逻辑推理：1. 重新思考光流，并发现光流能够 Capture 长远时间的时间逻辑推理;2. 通过对视觉和文本的嵌入进行对比学习，从而提高动作的表示在视觉和文本模式之间;3. 在练习阶段，防止模型根据混乱的视频回答问题，以避免因为外观和运动的偶推关系而导致的假设关系。在实验中，我们表明ATM在多个 VideoQA 上的准确率高于先前的方法，并且展现出更好的真实时间逻辑推理能力。
</details></li>
</ul>
<hr>
<h2 id="Haystack-A-Panoptic-Scene-Graph-Dataset-to-Evaluate-Rare-Predicate-Classes"><a href="#Haystack-A-Panoptic-Scene-Graph-Dataset-to-Evaluate-Rare-Predicate-Classes" class="headerlink" title="Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes"></a>Haystack: A Panoptic Scene Graph Dataset to Evaluate Rare Predicate Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02286">http://arxiv.org/abs/2309.02286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Lorenz, Florian Barthel, Daniel Kienzle, Rainer Lienhart</li>
<li>for: 本研究旨在构建一个新的SceneGraph dataset，以提高SceneGraph生成模型的预测性能，特别是对于罕见 predicate class。</li>
<li>methods: 该研究提出了一种模型协助的注释管道，以高效地找到图像中的罕见 predicate class。这种方法不同于现有的SceneGraph dataset，因为它包含Explicit negative annotations。</li>
<li>results: Haystack dataset可以轻松地与现有的SceneGraph dataset集成，并且可以帮助提高SceneGraph生成模型的预测性能，特别是对于罕见 predicate class。<details>
<summary>Abstract</summary>
Current scene graph datasets suffer from strong long-tail distributions of their predicate classes. Due to a very low number of some predicate classes in the test sets, no reliable metrics can be retrieved for the rarest classes. We construct a new panoptic scene graph dataset and a set of metrics that are designed as a benchmark for the predictive performance especially on rare predicate classes. To construct the new dataset, we propose a model-assisted annotation pipeline that efficiently finds rare predicate classes that are hidden in a large set of images like needles in a haystack.   Contrary to prior scene graph datasets, Haystack contains explicit negative annotations, i.e. annotations that a given relation does not have a certain predicate class. Negative annotations are helpful especially in the field of scene graph generation and open up a whole new set of possibilities to improve current scene graph generation models.   Haystack is 100% compatible with existing panoptic scene graph datasets and can easily be integrated with existing evaluation pipelines. Our dataset and code can be found here: https://lorjul.github.io/haystack/. It includes annotation files and simple to use scripts and utilities, to help with integrating our dataset in existing work.
</details>
<details>
<summary>摘要</summary>
To construct Haystack, we proposed a model-assisted annotation pipeline that efficiently finds rare predicate classes hidden in large sets of images, similar to finding needles in a haystack. This pipeline allows us to annotate rare predicate classes that were previously difficult or impossible to annotate.Haystack includes negative annotations, which are particularly useful in the field of scene graph generation. These negative annotations open up new possibilities for improving current scene graph generation models. Our dataset and code can be found at https://lorjul.github.io/haystack/, which includes annotation files and simple-to-use scripts and utilities to help integrate our dataset into existing work.
</details></li>
</ul>
<hr>
<h2 id="SAM-Deblur-Let-Segment-Anything-Boost-Image-Deblurring"><a href="#SAM-Deblur-Let-Segment-Anything-Boost-Image-Deblurring" class="headerlink" title="SAM-Deblur: Let Segment Anything Boost Image Deblurring"></a>SAM-Deblur: Let Segment Anything Boost Image Deblurring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02270">http://arxiv.org/abs/2309.02270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Li, Mingxuan Liu, Yating Zhang, Shu Chen, Haoxiang Li, Hong Chen, Zifei Dou</li>
<li>For: 该 paper 的目的是解决非均匀抖isser（non-uniform blurring）导致的图像恢复问题，使用 Segment Anything Model（SAM）的优先知识来提高恢复模型的通用性。* Methods: 该 paper 提出了一种框架，名为 SAM-Deblur，它将 SAM 的优先知识 integrate 到恢复任务中，并提出了一种面积融合（MAP）单元，用于融合 SAM 生成的分割区域，以提高模型的稳定性和普适性。* Results: 实验结果表明，通过 incorporating 我们的方法，可以提高 NAFNet 的 PSNR 值，具体如下：RealBlurJ 上提高 0.05，ReloBlur 上提高 0.96，REDs 上提高 7.03。<details>
<summary>Abstract</summary>
Image deblurring is a critical task in the field of image restoration, aiming to eliminate blurring artifacts. However, the challenge of addressing non-uniform blurring leads to an ill-posed problem, which limits the generalization performance of existing deblurring models. To solve the problem, we propose a framework SAM-Deblur, integrating prior knowledge from the Segment Anything Model (SAM) into the deblurring task for the first time. In particular, SAM-Deblur is divided into three stages. First, We preprocess the blurred images, obtain image masks via SAM, and propose a mask dropout method for training to enhance model robustness. Then, to fully leverage the structural priors generated by SAM, we propose a Mask Average Pooling (MAP) unit specifically designed to average SAM-generated segmented areas, serving as a plug-and-play component which can be seamlessly integrated into existing deblurring networks. Finally, we feed the fused features generated by the MAP Unit into the deblurring model to obtain a sharp image. Experimental results on the RealBlurJ, ReloBlur, and REDS datasets reveal that incorporating our methods improves NAFNet's PSNR by 0.05, 0.96, and 7.03, respectively. Code will be available at \href{https://github.com/HPLQAQ/SAM-Deblur}{SAM-Deblur}.
</details>
<details>
<summary>摘要</summary>
图像抖涂除是图像修复领域中的关键任务，旨在消除抖涂 artifacts。然而，非均匀抖涂的挑战导致一个不定性问题，限制了现有的抖涂除模型的泛化性能。为解决这问题，我们提出了一个框架SAM-Deblur，将Segment Anything Model（SAM）的先前知识integrated into the deblurring task。具体来说，SAM-Deblur分为三个阶段。首先，我们对抖涂图像进行预处理，通过SAM获得图像掩码，并提出了一种掩码抽样方法来提高模型Robustness。然后，我们提出了一种特殊的Mask Average Pooling（MAP）单元，用于平均SAM生成的分割区域，作为可插入到现有的抖涂除网络中的插件。最后，我们将MAP单元生成的融合特征 fed into the deblurring model，以获得锐化图像。实验结果表明，在RealBlurJ、ReloBlur和REDSDatasets上， incorporating our methods improve NAFNet's PSNR by 0.05, 0.96, and 7.03, respectively。代码将提供在 \href{https://github.com/HPLQAQ/SAM-Deblur}{SAM-Deblur}。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Chest-X-ray-Datasets-with-Non-Expert-Annotations"><a href="#Augmenting-Chest-X-ray-Datasets-with-Non-Expert-Annotations" class="headerlink" title="Augmenting Chest X-ray Datasets with Non-Expert Annotations"></a>Augmenting Chest X-ray Datasets with Non-Expert Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02244">http://arxiv.org/abs/2309.02244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cathrine Damgaard, Trine Naja Eriksen, Dovile Juodelyte, Veronika Cheplygina, Amelia Jiménez-Sánchez</li>
<li>for: 增加医疗影像分析中的机器学习算法的扩展，需要训练资料集的扩展。</li>
<li>methods: 使用自动标注抽象法从免费医疗报告中提取标注，以减少专家医生的 annotating 成本。</li>
<li>results: 通过将短cuts标注为管道，增加了两个公共可用的胸部X射像数据集的大小。使用非专家标注可以对医疗影像分析进行扩展。<details>
<summary>Abstract</summary>
The advancement of machine learning algorithms in medical image analysis requires the expansion of training datasets. A popular and cost-effective approach is automated annotation extraction from free-text medical reports, primarily due to the high costs associated with expert clinicians annotating chest X-ray images. However, it has been shown that the resulting datasets are susceptible to biases and shortcuts. Another strategy to increase the size of a dataset is crowdsourcing, a widely adopted practice in general computer vision with some success in medical image analysis. In a similar vein to crowdsourcing, we enhance two publicly available chest X-ray datasets by incorporating non-expert annotations. However, instead of using diagnostic labels, we annotate shortcuts in the form of tubes. We collect 3.5k chest drain annotations for CXR14, and 1k annotations for 4 different tube types in PadChest. We train a chest drain detector with the non-expert annotations that generalizes well to expert labels. Moreover, we compare our annotations to those provided by experts and show "moderate" to "almost perfect" agreement. Finally, we present a pathology agreement study to raise awareness about ground truth annotations. We make our annotations and code available.
</details>
<details>
<summary>摘要</summary>
“医学影像分析中的机器学习算法的进步需要训练数据集的扩展。一种受欢迎的和成本效益的方法是自动提取自自由文本医疗报告中的注释，主要是因为专业医生标注胸部X射线图像的成本很高。然而，已经证明了这些数据集具有偏见和短cuts。另一种增加数据集的方法是在大众筹资源，这是通用计算机视觉领域广泛采用的一种做法，在医学影像分析中也有一定的成功。我们在两个公共可用的胸部X射线数据集上进行了改进，并在医生标注中添加了非专业注释。我们收集了3500个胸部排液注释，并在PadChest上收集了4种不同的管道类型的1000个注释。我们使用非专业注释来训练胸部排液检测器，并发现其可以很好地泛化到专业标注。此外，我们比较了我们的注释和专业标注，并发现它们之间存在“中度”到“几乎完美”的一致。最后，我们进行了病理一致性研究，以提醒人们关于真实标注的重要性。我们将我们的注释和代码公开。”
</details></li>
</ul>
<hr>
<h2 id="Robustness-and-Generalizability-of-Deepfake-Detection-A-Study-with-Diffusion-Models"><a href="#Robustness-and-Generalizability-of-Deepfake-Detection-A-Study-with-Diffusion-Models" class="headerlink" title="Robustness and Generalizability of Deepfake Detection: A Study with Diffusion Models"></a>Robustness and Generalizability of Deepfake Detection: A Study with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02218">http://arxiv.org/abs/2309.02218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/OpenRL-Lab/DeepFakeFace">https://github.com/OpenRL-Lab/DeepFakeFace</a></li>
<li>paper_authors: Haixu Song, Shiyu Huang, Yinpeng Dong, Wei-Wei Tu</li>
<li>for: 本研究旨在帮助推广真实信息，减少深度模仿图像的散布。</li>
<li>methods: 研究采用了高级扩散模型生成虚假名人脸，并将其分享到线上平台上。用于训练和测试深度模仿检测算法的数据集是DFF集。</li>
<li>results: 研究发现，不同的深度模仿方法和图像变化，需要更好的深度模仿检测工具。DFF集和测试方法能够推动开发更有效的深度模仿检测算法。<details>
<summary>Abstract</summary>
The rise of deepfake images, especially of well-known personalities, poses a serious threat to the dissemination of authentic information. To tackle this, we present a thorough investigation into how deepfakes are produced and how they can be identified. The cornerstone of our research is a rich collection of artificial celebrity faces, titled DeepFakeFace (DFF). We crafted the DFF dataset using advanced diffusion models and have shared it with the community through online platforms. This data serves as a robust foundation to train and test algorithms designed to spot deepfakes. We carried out a thorough review of the DFF dataset and suggest two evaluation methods to gauge the strength and adaptability of deepfake recognition tools. The first method tests whether an algorithm trained on one type of fake images can recognize those produced by other methods. The second evaluates the algorithm's performance with imperfect images, like those that are blurry, of low quality, or compressed. Given varied results across deepfake methods and image changes, our findings stress the need for better deepfake detectors. Our DFF dataset and tests aim to boost the development of more effective tools against deepfakes.
</details>
<details>
<summary>摘要</summary>
“深圳技术”的出现，尤其是关于知名人物的深圳图像，对媒体传播Authentic信息提供了严重的威胁。为了解决这个问题，我们提供了一份深入探究深圳图像的生成和识别方法的研究报告。我们的研究的核心是一个名为“DeepFakeFace”（DFF）的人工知名人物脸部集合。我们使用了先进的扩散模型来制作了这个数据集，并通过在线平台分享给社区。这个数据集作为训练和测试深圳识别算法的基础，提供了一个robust的基础。我们对DFF数据集进行了住检查，并提出了两种评价方法来评估深圳识别算法的强度和适应性。第一种测试是用一种基于一种深圳方法训练的算法能否识别其他方法生成的深圳图像。第二种测试是用一种受到干扰、低质量或压缩等变化的图像来测试算法的性能。由于深圳方法和图像变化的多样性，我们的发现表明了深圳识别算法的需要更好。我们的DFF数据集和测试方法旨在推动对深圳图像的更好的识别工具的开发。
</details></li>
</ul>
<hr>
<h2 id="Advanced-Underwater-Image-Restoration-in-Complex-Illumination-Conditions"><a href="#Advanced-Underwater-Image-Restoration-in-Complex-Illumination-Conditions" class="headerlink" title="Advanced Underwater Image Restoration in Complex Illumination Conditions"></a>Advanced Underwater Image Restoration in Complex Illumination Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02217">http://arxiv.org/abs/2309.02217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Song, Mengkun She, Kevin Köser</li>
<li>for: 本研究旨在提高深水下摄影图像的修复效果，特别是在200米深度以下的潜水器拍摄场景， где自然光scarce和人工照明必须。</li>
<li>methods: 本研究使用了新的应earance变化约束，即对象或海底表面的变化，以估算照明场景。通过每个像素对相机视场中的光场的约束，每个voxel都可以保存一个信号因子和反射值，以便高效地修复摄影机-灯光平台上的图像。</li>
<li>results: 实验结果表明，本approach可以准确地修复摄影机-灯光平台上的图像，同时减轻照明和媒体效果的影响。此外，本approach可以轻松扩展到其他场景，如在空中拍摄或其他类似场景。<details>
<summary>Abstract</summary>
Underwater image restoration has been a challenging problem for decades since the advent of underwater photography. Most solutions focus on shallow water scenarios, where the scene is uniformly illuminated by the sunlight. However, the vast majority of uncharted underwater terrain is located beyond 200 meters depth where natural light is scarce and artificial illumination is needed. In such cases, light sources co-moving with the camera, dynamically change the scene appearance, which make shallow water restoration methods inadequate. In particular for multi-light source systems (composed of dozens of LEDs nowadays), calibrating each light is time-consuming, error-prone and tedious, and we observe that only the integrated illumination within the viewing volume of the camera is critical, rather than the individual light sources. The key idea of this paper is therefore to exploit the appearance changes of objects or the seafloor, when traversing the viewing frustum of the camera. Through new constraints assuming Lambertian surfaces, corresponding image pixels constrain the light field in front of the camera, and for each voxel a signal factor and a backscatter value are stored in a volumetric grid that can be used for very efficient image restoration of camera-light platforms, which facilitates consistently texturing large 3D models and maps that would otherwise be dominated by lighting and medium artifacts. To validate the effectiveness of our approach, we conducted extensive experiments on simulated and real-world datasets. The results of these experiments demonstrate the robustness of our approach in restoring the true albedo of objects, while mitigating the influence of lighting and medium effects. Furthermore, we demonstrate our approach can be readily extended to other scenarios, including in-air imaging with artificial illumination or other similar cases.
</details>
<details>
<summary>摘要</summary>
水下图像修复问题已经是数十年来的挑战，自 fotografías submarinas 的出现以来。大多数解决方案都专注于浅水enario， где场景由太阳照明均匀。然而，95%的未探索的水下地形都 locate在200米深度以下，其中自然光照明稀缺，需要人工照明。在这种情况下，相机 Move  along with light sources， dynamically change the scene appearance，使得浅水修复方法无法满足需求。特别是，现今的多光源系统（由多个LED组成），每个光源的准确耗时、容易出错和繁琐，而我们发现，只有相机前方照明的积合照明是关键的，而不是个别的光源。本文的关键想法是利用相机视图卷积中物体或海底的变化，来恢复图像。通过新的约束，对象或海底的镜像变化会帮助确定图像中的照明场景，并为每个 voxel 存储一个信号因子和反射值，可以高效地修复相机灯台上的图像，使得大型 3D 模型和地图可以一致地 текстури化，而不会受到照明和媒体效果的限制。为验证我们的方法的有效性，我们对 simulated 和实际数据进行了广泛的实验。实验结果表明，我们的方法可以准确地恢复物体的真实反射率，同时抑制照明和媒体效果的影响。此外，我们还证明了我们的方法可以轻松扩展到其他场景，包括在空中拍摄的人工照明或类似情况。
</details></li>
</ul>
<hr>
<h2 id="Continual-Cross-Dataset-Adaptation-in-Road-Surface-Classification"><a href="#Continual-Cross-Dataset-Adaptation-in-Road-Surface-Classification" class="headerlink" title="Continual Cross-Dataset Adaptation in Road Surface Classification"></a>Continual Cross-Dataset Adaptation in Road Surface Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02210">http://arxiv.org/abs/2309.02210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Cudrano, Matteo Bellusci, Giuseppe Macino, Matteo Matteucci</li>
<li>for: 这篇论文是为了解决自动驾驶车（AV）的道路表面分类问题，以便优化驾驶环境、提高安全性和实现进阶道路地图。</li>
<li>methods: 这篇论文使用了快速和有效的cross-dataset演化方法，以保持过去的知识并适应新数据，从而避免了忘记现象。</li>
<li>results: 实验结果显示，这种方法比Naive finetuning更有优势，可以实现性能与新 retraining 之间的几乎相同水平。<details>
<summary>Abstract</summary>
Accurate road surface classification is crucial for autonomous vehicles (AVs) to optimize driving conditions, enhance safety, and enable advanced road mapping. However, deep learning models for road surface classification suffer from poor generalization when tested on unseen datasets. To update these models with new information, also the original training dataset must be taken into account, in order to avoid catastrophic forgetting. This is, however, inefficient if not impossible, e.g., when the data is collected in streams or large amounts. To overcome this limitation and enable fast and efficient cross-dataset adaptation, we propose to employ continual learning finetuning methods designed to retain past knowledge while adapting to new data, thus effectively avoiding forgetting. Experimental results demonstrate the superiority of this approach over naive finetuning, achieving performance close to fresh retraining. While solving this known problem, we also provide a general description of how the same technique can be adopted in other AV scenarios. We highlight the potential computational and economic benefits that a continual-based adaptation can bring to the AV industry, while also reducing greenhouse emissions due to unnecessary joint retraining.
</details>
<details>
<summary>摘要</summary>
准确的路面类别化是自动驾驶车辆（AV）优化驾驶条件、提高安全性和实现高级路况映射的关键。然而，深度学习模型 для路面类别化受到不seen数据集的泛化问题带来挑战。为了更新这些模型，还需要考虑原始训练数据集，以避免恶性忘记。这是一个效率和可行性的限制，例如在流动数据集或大量数据集时。为了突破这个限制，我们提议使用 kontinual learning finetuning 方法，以保持过去知识而适应新数据，从而实现高效的跨数据集适应。实验结果表明，我们的方法与混合 retrained 方法具有类似的性能，而且具有更高的效率和可行性。此外，我们还描述了在其他 AV 场景中如何采用同样的技术，并指出了计算和经济上的优势，以及减少可能的绿色排放。
</details></li>
</ul>
<hr>
<h2 id="Delving-into-Ipsilateral-Mammogram-Assessment-under-Multi-View-Network"><a href="#Delving-into-Ipsilateral-Mammogram-Assessment-under-Multi-View-Network" class="headerlink" title="Delving into Ipsilateral Mammogram Assessment under Multi-View Network"></a>Delving into Ipsilateral Mammogram Assessment under Multi-View Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02197">http://arxiv.org/abs/2309.02197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thai Ngoc Toan Truong, Thanh-Huy Nguyen, Ba Thinh Lam, Vu Minh Duy Nguyen, Hong Phuc Nguyen</li>
<li>for: 这项研究旨在探讨多视图胸部X射图分析中的多种融合策略，包括平均和 concatenate 策略，以及不同个体和融合路径对模型学习行为的影响。</li>
<li>methods: 该研究使用了 Ipsilateral Multi-View Network，包括 Pre、Early、Middle、Last 和 Post Fusion 五种融合类型，并使用了 ResNet-18 网络。</li>
<li>results: 研究发现，中间融合方法是最佳均衡和有效的方法，可以提高深度学习模型在 VinDr-Mammo 数据集和 CMMD 数据集上的总体分类精度，并且在macro F1-Score上提高了 +2.06% (concatenate) 和 +5.29% (average)，以及 +2.03% (concatenate) 和 +3% (average)。<details>
<summary>Abstract</summary>
In many recent years, multi-view mammogram analysis has been focused widely on AI-based cancer assessment. In this work, we aim to explore diverse fusion strategies (average and concatenate) and examine the model's learning behavior with varying individuals and fusion pathways, involving Coarse Layer and Fine Layer. The Ipsilateral Multi-View Network, comprising five fusion types (Pre, Early, Middle, Last, and Post Fusion) in ResNet-18, is employed. Notably, the Middle Fusion emerges as the most balanced and effective approach, enhancing deep-learning models' generalization performance by +2.06% (concatenate) and +5.29% (average) in VinDr-Mammo dataset and +2.03% (concatenate) and +3% (average) in CMMD dataset on macro F1-Score. The paper emphasizes the crucial role of layer assignment in multi-view network extraction with various strategies.
</details>
<details>
<summary>摘要</summary>
多年来，多视图胸部X光分析已广泛关注人工智能基于癌病评估。在这项工作中，我们想要探索多种融合策略（平均和 concatenate），并研究模型在不同个体和融合路径上学习行为，包括粗层和细层。我们使用Ipsilateral Multi-View Network，其包括五种融合类型（Pre、Early、Middle、Last和Post Fusion）在ResNet-18中。值得注意的是，Middle Fusion表现为最 equilibrio和有效的方法，可以提高深度学习模型的泛化性能，在VinDr-Mammo数据集中提高了 macro F1-Score 的表现，分别提高了 +2.06%（ concatenate）和 +5.29%（average），在CMMD数据集中提高了 +2.03%（ concatenate）和 +3%（average）。文章强调了多视图网络EXTRACTION中不同策略的层分配的重要性。
</details></li>
</ul>
<hr>
<h2 id="High-resolution-3D-Maps-of-Left-Atrial-Displacements-using-an-Unsupervised-Image-Registration-Neural-Network"><a href="#High-resolution-3D-Maps-of-Left-Atrial-Displacements-using-an-Unsupervised-Image-Registration-Neural-Network" class="headerlink" title="High-resolution 3D Maps of Left Atrial Displacements using an Unsupervised Image Registration Neural Network"></a>High-resolution 3D Maps of Left Atrial Displacements using an Unsupervised Image Registration Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02179">http://arxiv.org/abs/2309.02179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoforos Galazis, Anil Anthony Bharath, Marta Varela</li>
<li>for: 这个研究旨在提供一种自动将左心室（LA）动态分割为不同阶段的工具，以便更好地了解室内动力学性质。</li>
<li>methods: 这个研究使用高级解剖磁共振成像（Cine MRI）技术，以获取高分辨率、全面覆盖的LA动态图像。然后，提出了一种自动将LA动态分割为不同阶段的工具，使用了扩展的距离函数和矩阵方法。</li>
<li>results: 研究发现，该工具能够准确地跟踪LA墙面在心动周期内的运动， Hausdorff距离平均值为2.51±1.3mm，Dice分数平均值为0.96±0.02。<details>
<summary>Abstract</summary>
Functional analysis of the left atrium (LA) plays an increasingly important role in the prognosis and diagnosis of cardiovascular diseases. Echocardiography-based measurements of LA dimensions and strains are useful biomarkers, but they provide an incomplete picture of atrial deformations. High-resolution dynamic magnetic resonance images (Cine MRI) offer the opportunity to examine LA motion and deformation in 3D, at higher spatial resolution and with full LA coverage. However, there are no dedicated tools to automatically characterise LA motion in 3D. Thus, we propose a tool that automatically segments the LA and extracts the displacement fields across the cardiac cycle. The pipeline is able to accurately track the LA wall across the cardiac cycle with an average Hausdorff distance of $2.51 \pm 1.3~mm$ and Dice score of $0.96 \pm 0.02$.
</details>
<details>
<summary>摘要</summary>
左心室功能分析在心血管疾病诊断和预后中发挥越来越重要的作用。使用echo响应测量左心室尺寸和弹性可以提供有用的生物标志物，但它们只提供了左心室弹性的部分图像。高分辨率动态磁共振成像（Cine MRI）可以让我们在三维空间中观察左心室的运动和弹性，并且具有完整的左心室覆盖。然而，目前没有专门的工具可以自动描述左心室的运动。因此，我们提出了一种工具，可以自动 segment左心室并提取征动过程中的挤压场。管道可以准确地跟踪左心室墙在征动过程中的挤压场，平均 Hausdorff 距离为2.51±1.3毫米，Dice 分数为0.96±0.02。
</details></li>
</ul>
<hr>
<h2 id="PCFGaze-Physics-Consistent-Feature-for-Appearance-based-Gaze-Estimation"><a href="#PCFGaze-Physics-Consistent-Feature-for-Appearance-based-Gaze-Estimation" class="headerlink" title="PCFGaze: Physics-Consistent Feature for Appearance-based Gaze Estimation"></a>PCFGaze: Physics-Consistent Feature for Appearance-based Gaze Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02165">http://arxiv.org/abs/2309.02165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwei Bao, Feng Lu</li>
<li>for: 本文试图解释如何将视线特征连接到物理上的视线定义。</li>
<li>methods: 本文分析了视线特征拟合空间，发现视线特征间的地odesic距离与样本之间的视线差异相关。基于这种发现，提出了物理相关特征（PCF），将视线特征与物理定义的视线连接。</li>
<li>results: 提出的PCFGAZE框架可以直接优化视线特征空间，无需额外训练数据，可以 Alleviate overfitting问题，并在不同预测器上提高跨预测器视线估计精度。<details>
<summary>Abstract</summary>
Although recent deep learning based gaze estimation approaches have achieved much improvement, we still know little about how gaze features are connected to the physics of gaze. In this paper, we try to answer this question by analyzing the gaze feature manifold. Our analysis revealed the insight that the geodesic distance between gaze features is consistent with the gaze differences between samples. According to this finding, we construct the Physics- Consistent Feature (PCF) in an analytical way, which connects gaze feature to the physical definition of gaze. We further propose the PCFGaze framework that directly optimizes gaze feature space by the guidance of PCF. Experimental results demonstrate that the proposed framework alleviates the overfitting problem and significantly improves cross-domain gaze estimation accuracy without extra training data. The insight of gaze feature has the potential to benefit other regression tasks with physical meanings.
</details>
<details>
<summary>摘要</summary>
尽管最近的深度学习基于眼动估算方法已经取得了大量进步，但我们对眼动特征与物理眼动之间的连接还知之 little。在这篇论文中，我们尝试回答这个问题，通过分析眼动特征抽象空间。我们的分析发现，在眼动特征空间中， closest geodesic distance 与眼动差异 between samples 相关。基于这一发现，我们构建了Physics-Consistent Feature (PCF)，将眼动特征连接到物理眼动的定义。我们进一步提出PCFGaze框架，通过PCF的指导，直接优化眼动特征空间。实验结果表明，我们的框架可以减少过拟合问题，在不同领域的眼动估算精度得到显著改善，无需额外的训练数据。我们的发现可能会对其他具有物理含义的回归任务产生影响。
</details></li>
</ul>
<hr>
<h2 id="The-Adversarial-Implications-of-Variable-Time-Inference"><a href="#The-Adversarial-Implications-of-Variable-Time-Inference" class="headerlink" title="The Adversarial Implications of Variable-Time Inference"></a>The Adversarial Implications of Variable-Time Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02159">http://arxiv.org/abs/2309.02159</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dudi709/Timing-Based-Attack">https://github.com/dudi709/Timing-Based-Attack</a></li>
<li>paper_authors: Dudi Biton, Aditi Misra, Efrat Levy, Jaidip Kotak, Ron Bitton, Roei Schuster, Nicolas Papernot, Yuval Elovici, Ben Nassi</li>
<li>For: The paper is written to demonstrate the ability to enhance decision-based attacks on machine learning models by exploiting a novel side channel in algorithmic timing.* Methods: The paper uses a technique called timing attack, which measures the execution time of the algorithm used to post-process the predictions of the ML model under attack.* Results: The paper demonstrates the ability to successfully evade object detection using adversarial examples and perform dataset inference by exploiting the timing leakage vulnerability inherent in the non-maximum suppression (NMS) algorithm. The adversarial examples exhibit superior perturbation quality compared to a decision-based attack.Here is the information in Simplified Chinese text:* For: 本文是为了展示如何使用一种新的边道攻击机器学习模型。* Methods: 本文使用了一种名为时间攻击的技术，该技术测量 ML 模型下的预测 outputs（例如，标签）的执行时间。* Results: 本文成功地逃脱了对象检测器的攻击，并完成了基于时间泄漏的数据推断。 adversarial examples 展现了比基于决策的攻击更好的杂化质量。<details>
<summary>Abstract</summary>
Machine learning (ML) models are known to be vulnerable to a number of attacks that target the integrity of their predictions or the privacy of their training data. To carry out these attacks, a black-box adversary must typically possess the ability to query the model and observe its outputs (e.g., labels). In this work, we demonstrate, for the first time, the ability to enhance such decision-based attacks. To accomplish this, we present an approach that exploits a novel side channel in which the adversary simply measures the execution time of the algorithm used to post-process the predictions of the ML model under attack. The leakage of inference-state elements into algorithmic timing side channels has never been studied before, and we have found that it can contain rich information that facilitates superior timing attacks that significantly outperform attacks based solely on label outputs. In a case study, we investigate leakage from the non-maximum suppression (NMS) algorithm, which plays a crucial role in the operation of object detectors. In our examination of the timing side-channel vulnerabilities associated with this algorithm, we identified the potential to enhance decision-based attacks. We demonstrate attacks against the YOLOv3 detector, leveraging the timing leakage to successfully evade object detection using adversarial examples, and perform dataset inference. Our experiments show that our adversarial examples exhibit superior perturbation quality compared to a decision-based attack. In addition, we present a new threat model in which dataset inference based solely on timing leakage is performed. To address the timing leakage vulnerability inherent in the NMS algorithm, we explore the potential and limitations of implementing constant-time inference passes as a mitigation strategy.
</details>
<details>
<summary>摘要</summary>
машинное обучение (ML) 模型已知容易受到一些攻击，这些攻击可能会影响模型预测的正确性或训练数据的隐私。为了进行这些攻击，黑盒式敌对者通常需要能够访问模型并观察其输出（例如，标签）。在这项工作中，我们显示了，对于第一次，能够增强这些决策基本攻击。我们提出了一种方法，利用一种新的侧途通道，即对 ML 模型下攻击的执行时间进行测量。我们发现，在执行时间方面的泄露包含有丰富的信息，可以提高基于决策的攻击，并且能够 significatively  exceed 基于标签输出的攻击。在一个案例研究中，我们investigated  leakage from the non-maximum suppression (NMS) algorithm，该算法在 объек检测器中扮演着关键的角色。我们发现，与 NMS 算法相关的时间泄露具有潜在的威胁，并且可以用于增强决策基本攻击。我们采用 YOLOv3 检测器进行攻击，通过利用时间泄露来逃脱物体检测，并进行数据集推理。我们的实验结果表明，我们的恶作剂示例具有较高的杂化质量，比基于决策的攻击更好。此外，我们还提出了一个新的威胁模型，在该模型中，攻击者 solely 基于时间泄露进行数据集推理。为了解决 NMS 算法中的时间泄露漏洞，我们探讨了可能的和限制的实现常量时间推理 passes 的缓解策略。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Light-Recognition-using-Convolutional-Neural-Networks-A-Survey"><a href="#Traffic-Light-Recognition-using-Convolutional-Neural-Networks-A-Survey" class="headerlink" title="Traffic Light Recognition using Convolutional Neural Networks: A Survey"></a>Traffic Light Recognition using Convolutional Neural Networks: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02158">http://arxiv.org/abs/2309.02158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Svetlana Pavlitska, Nico Lambing, Ashok Kumar Bangaru, J. Marius Zöllner</li>
<li>for: 本研究旨在提供一个涵盖汽车自动驾驶中实时交通信号识别的模型建立方法的综述。</li>
<li>methods: 本研究使用了 convolutional neural networks (CNNs) 进行交通信号识别方法的分析和检视。</li>
<li>results: 研究人员通过对 datasets 和 CNN 建模方法的分析，将交通信号识别方法分为三个主要群组：（1）特定任务特性补做的 generic object detectors 修改版本，（2）包含 rule-based 和 CNN 组件的多阶段方法，以及（3）专门为此任务设计的单阶段方法。<details>
<summary>Abstract</summary>
Real-time traffic light recognition is essential for autonomous driving. Yet, a cohesive overview of the underlying model architectures for this task is currently missing. In this work, we conduct a comprehensive survey and analysis of traffic light recognition methods that use convolutional neural networks (CNNs). We focus on two essential aspects: datasets and CNN architectures. Based on an underlying architecture, we cluster methods into three major groups: (1) modifications of generic object detectors which compensate for specific task characteristics, (2) multi-stage approaches involving both rule-based and CNN components, and (3) task-specific single-stage methods. We describe the most important works in each cluster, discuss the usage of the datasets, and identify research gaps.
</details>
<details>
<summary>摘要</summary>
现实时交通信号识别是自动驾驶的重要组成部分。然而，关于这个任务下的模型建立的总体概述却缺乏一个系统性的审查。在这项工作中，我们进行了全面的调研和分析，探讨了使用卷积神经网络（CNN）进行交通信号识别的方法。我们主要关注两个重要方面：数据集和CNN体系。基于基本体系，我们将方法分为三个主要群组：（1）特定任务特性补做的通用物体检测器修改版本，（2）包含Rule-based和CNN组件的多 stageapproaches，以及（3）专门为此任务设计的单 stage方法。我们描述了每个群组中最重要的工作，讨论了数据集的使用，并确定了研究漏洞。
</details></li>
</ul>
<hr>
<h2 id="S3C-Semi-Supervised-VQA-Natural-Language-Explanation-via-Self-Critical-Learning"><a href="#S3C-Semi-Supervised-VQA-Natural-Language-Explanation-via-Self-Critical-Learning" class="headerlink" title="S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning"></a>S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02155">http://arxiv.org/abs/2309.02155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Suo, Mengyang Sun, Weisong Liu, Yiqi Gao, Peng Wang, Yanning Zhang, Qi Wu</li>
<li>for: 这 paper 的目的是解释 VQA 模型的决策过程，以便更好地了解和让用户信任。</li>
<li>methods: 这 paper 使用了 Semi-Supervised VQA-NLE via Self-Critical Learning (S3C) 方法，通过回答奖励来评估候选的解释，从而提高了逻辑一致性 между答案和解释。</li>
<li>results: 这 paper 的方法在两个 VQA-NLE 数据集上达到了新的state-of-the-art性能，并且通过自动度量和人类评估都表明了方法的有效性。<details>
<summary>Abstract</summary>
VQA Natural Language Explanation (VQA-NLE) task aims to explain the decision-making process of VQA models in natural language. Unlike traditional attention or gradient analysis, free-text rationales can be easier to understand and gain users' trust. Existing methods mostly use post-hoc or self-rationalization models to obtain a plausible explanation. However, these frameworks are bottlenecked by the following challenges: 1) the reasoning process cannot be faithfully responded to and suffer from the problem of logical inconsistency. 2) Human-annotated explanations are expensive and time-consuming to collect. In this paper, we propose a new Semi-Supervised VQA-NLE via Self-Critical Learning (S3C), which evaluates the candidate explanations by answering rewards to improve the logical consistency between answers and rationales. With a semi-supervised learning framework, the S3C can benefit from a tremendous amount of samples without human-annotated explanations. A large number of automatic measures and human evaluations all show the effectiveness of our method. Meanwhile, the framework achieves a new state-of-the-art performance on the two VQA-NLE datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-Adaptation-for-Satellite-Borne-Hyperspectral-Cloud-Detection"><a href="#Domain-Adaptation-for-Satellite-Borne-Hyperspectral-Cloud-Detection" class="headerlink" title="Domain Adaptation for Satellite-Borne Hyperspectral Cloud Detection"></a>Domain Adaptation for Satellite-Borne Hyperspectral Cloud Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02150">http://arxiv.org/abs/2309.02150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Du, Anh-Dzung Doan, Yee Wei Law, Tat-Jun Chin</li>
<li>for: 本研究旨在解决遥感机器学习硬件加速器上部署云计算模型时遇到的领域差问题，以实现在新任务中使用新感器时的模型更新和提高。</li>
<li>methods: 本研究提出了新的领域适应任务，并开发了一种带宽效率的超vision学习领域适应算法。此外，本研究还提出了在遥感机器学习硬件加速器上实现测试时适应算法。</li>
<li>results: 本研究表明，通过使用新的领域适应任务和算法，可以在遥感机器学习硬件加速器上实现带宽效率的领域适应，以便在新任务中使用新感器时的模型更新和提高。<details>
<summary>Abstract</summary>
The advent of satellite-borne machine learning hardware accelerators has enabled the on-board processing of payload data using machine learning techniques such as convolutional neural networks (CNN). A notable example is using a CNN to detect the presence of clouds in hyperspectral data captured on Earth observation (EO) missions, whereby only clear sky data is downlinked to conserve bandwidth. However, prior to deployment, new missions that employ new sensors will not have enough representative datasets to train a CNN model, while a model trained solely on data from previous missions will underperform when deployed to process the data on the new missions. This underperformance stems from the domain gap, i.e., differences in the underlying distributions of the data generated by the different sensors in previous and future missions. In this paper, we address the domain gap problem in the context of on-board hyperspectral cloud detection. Our main contributions lie in formulating new domain adaptation tasks that are motivated by a concrete EO mission, developing a novel algorithm for bandwidth-efficient supervised domain adaptation, and demonstrating test-time adaptation algorithms on space deployable neural network accelerators. Our contributions enable minimal data transmission to be invoked (e.g., only 1% of the weights in ResNet50) to achieve domain adaptation, thereby allowing more sophisticated CNN models to be deployed and updated on satellites without being hampered by domain gap and bandwidth limitations.
</details>
<details>
<summary>摘要</summary>
卫星上的机器学习硬件加速器的出现，使得payload数据中使用机器学习技术，如 convolutional neural networks (CNN) 进行处理。一个典型的应用是使用 CNN 在 Earth observation (EO) 任务中检测地球表面上云的存在，从而只下载清晰天空数据，以保存带宽。然而，在新任务中使用新的探测器时，新任务不会有足够的表示性数据来训练 CNN 模型，而一个仅在前一任务中训练的模型在新任务中表现不佳，这是由于领域差距问题，即不同探测器生成的数据的下面分布之间的差异。在这篇论文中，我们在 Earth observation 中解决领域差距问题。我们的主要贡献在于提出了新的领域适应任务，开发了一种带宽有效的指导领域适应算法，并在空间部署可能的神经网络加速器上进行测试时适应算法。我们的贡献使得只需传输 minimal 的数据（例如，ResNet50 中的 1% 的参数）可以实现领域适应，从而允许更复杂的 CNN 模型在卫星上部署和更新，不受领域差距和带宽限制。
</details></li>
</ul>
<hr>
<h2 id="INCEPTNET-Precise-And-Early-Disease-Detection-Application-For-Medical-Images-Analyses"><a href="#INCEPTNET-Precise-And-Early-Disease-Detection-Application-For-Medical-Images-Analyses" class="headerlink" title="INCEPTNET: Precise And Early Disease Detection Application For Medical Images Analyses"></a>INCEPTNET: Precise And Early Disease Detection Application For Medical Images Analyses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02147">http://arxiv.org/abs/2309.02147</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AMiiR-S/Inceptnet_cancer_recognition">https://github.com/AMiiR-S/Inceptnet_cancer_recognition</a></li>
<li>paper_authors: Amirhossein Sajedi, Mohammad Javad Fadaeieslam</li>
<li>for: 本研究旨在提出一种新的深度神经网络（DNN），名为InceptNet，用于医疗图像处理，以提高疾病检测和医疗图像分 segmentation 的精度和性能。</li>
<li>methods: 本研究使用了Unet架构，并添加了多个并行的启发模块，以便快速地捕捉到医疗图像中的缩放区域。</li>
<li>results: 对四个referencedataset进行测试，包括血管 segmentation、肺肿尺segmentation、皮肤损伤 segmentation和乳腺癌细胞检测。结果表明，提案方法在图像中的小规模结构上表现出了更高的改进度。与前一代方法相比，提案方法的精度从0.9531、0.8900、0.9872和0.9881提高到0.9555、0.9510、0.9945和0.9945。<details>
<summary>Abstract</summary>
In view of the recent paradigm shift in deep AI based image processing methods, medical image processing has advanced considerably. In this study, we propose a novel deep neural network (DNN), entitled InceptNet, in the scope of medical image processing, for early disease detection and segmentation of medical images in order to enhance precision and performance. We also investigate the interaction of users with the InceptNet application to present a comprehensive application including the background processes, and foreground interactions with users. Fast InceptNet is shaped by the prominent Unet architecture, and it seizes the power of an Inception module to be fast and cost effective while aiming to approximate an optimal local sparse structure. Adding Inception modules with various parallel kernel sizes can improve the network's ability to capture the variations in the scaled regions of interest. To experiment, the model is tested on four benchmark datasets, including retina blood vessel segmentation, lung nodule segmentation, skin lesion segmentation, and breast cancer cell detection. The improvement was more significant on images with small scale structures. The proposed method improved the accuracy from 0.9531, 0.8900, 0.9872, and 0.9881 to 0.9555, 0.9510, 0.9945, and 0.9945 on the mentioned datasets, respectively, which show outperforming of the proposed method over the previous works. Furthermore, by exploring the procedure from start to end, individuals who have utilized a trial edition of InceptNet, in the form of a complete application, are presented with thirteen multiple choice questions in order to assess the proposed method. The outcomes are evaluated through the means of Human Computer Interaction.
</details>
<details>
<summary>摘要</summary>
因为深度AI技术的最近分布shift，医疗图像处理方法得到了显著提高。在这个研究中，我们提出了一种新的深度神经网络（DNN），即InceptNet，用于医疗图像处理领域的疾病检测和图像分割，以提高精度和性能。我们还investigated用户与InceptNet应用程序之间的交互，以提供全面的应用程序，包括背景进程和前景交互。快速InceptNet基于提前的Unet架构，并利用Inception模块以实现快速和经济的搅拌，同时尝试以最佳的本地稀疏结构来近似。通过添加不同的并行kernel大小的Inception模块，可以提高网络的捕捉缩放区域的变化能力。为了实验，我们测试了四个标准数据集，包括血液管Segmentation、肺肿Segmentation、皮肤病变Segmentation和乳腺癌细胞检测。提出的方法在这些数据集上提高了精度，从0.9531、0.8900、0.9872和0.9881提高到0.9555、0.9510、0.9945和0.9945，这表明提出的方法在先前的工作上出perform。此外，通过探索从开始到结束的过程，那些使用了InceptNet试用版的人员被提供了十三个多选题，以评估提出的方法。结果由人计算机交互评估。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Masked-3D-Diffusion-Model-for-Video-Outpainting"><a href="#Hierarchical-Masked-3D-Diffusion-Model-for-Video-Outpainting" class="headerlink" title="Hierarchical Masked 3D Diffusion Model for Video Outpainting"></a>Hierarchical Masked 3D Diffusion Model for Video Outpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02119">http://arxiv.org/abs/2309.02119</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanfanda/M3DDM">https://github.com/fanfanda/M3DDM</a></li>
<li>paper_authors: Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, Jianfeng Zhan</li>
<li>for: 这个论文的目的是提出一种基于掩码模型的3D扩散模型，用于视频填充。</li>
<li>methods: 这个方法使用掩码模型的技术进行训练，并使用多个引导帧连接多个视频帧的结果，以保证视频的时间一致性和避免邻帧的抖动。同时，该方法还使用全帧提取以提取视频中的全帧信息，并使用交叉注意力引导模型获取其他视频帧中的信息。</li>
<li>results: 该方法在视频填充任务中实现了state-of-the-art的结果。<details>
<summary>Abstract</summary>
Video outpainting aims to adequately complete missing areas at the edges of video frames. Compared to image outpainting, it presents an additional challenge as the model should maintain the temporal consistency of the filled area. In this paper, we introduce a masked 3D diffusion model for video outpainting. We use the technique of mask modeling to train the 3D diffusion model. This allows us to use multiple guide frames to connect the results of multiple video clip inferences, thus ensuring temporal consistency and reducing jitter between adjacent frames. Meanwhile, we extract the global frames of the video as prompts and guide the model to obtain information other than the current video clip using cross-attention. We also introduce a hybrid coarse-to-fine inference pipeline to alleviate the artifact accumulation problem. The existing coarse-to-fine pipeline only uses the infilling strategy, which brings degradation because the time interval of the sparse frames is too large. Our pipeline benefits from bidirectional learning of the mask modeling and thus can employ a hybrid strategy of infilling and interpolation when generating sparse frames. Experiments show that our method achieves state-of-the-art results in video outpainting tasks. More results are provided at our https://fanfanda.github.io/M3DDM/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>视频外绘目标是完整地填充视频帧边缘中的缺失区域。与图像外绘相比，它增加了一个额外挑战，即模型需要保持视频帧中填充的区域的时间一致性。在这篇论文中，我们介绍了一种masked 3D扩散模型 для视频外绘。我们使用模型的技术来训练3D扩散模型。这 позволяет我们使用多个引导帧连接多个视频帧的结果，以确保时间一致性，并减少相邻帧之间的振荡。同时，我们提取视频全帧作为提示，并使用交叉注意力导引模型获取除当前视频帧之外的信息。我们还提出了一种混合粗细调制pipeline来缓解artifact散布问题。现有的粗细调制pipeline只使用填充策略，这会导致质量下降，因为缺失帧的时间间隔太长。我们的管道可以利用面精模型的混合学习，因此可以采用混合策略，在生成缺失帧时使用填充和 interpolate 两种策略。实验显示，我们的方法实现了视频外绘任务的状态精算结果。更多结果请参考我们的 <https://fanfanda.github.io/M3DDM/>。
</details></li>
</ul>
<hr>
<h2 id="Towards-Diverse-and-Consistent-Typography-Generation"><a href="#Towards-Diverse-and-Consistent-Typography-Generation" class="headerlink" title="Towards Diverse and Consistent Typography Generation"></a>Towards Diverse and Consistent Typography Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02099">http://arxiv.org/abs/2309.02099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Wataru Shimoda, Daichi Haraguchi, Seiichi Uchida, Kota Yamaguchi</li>
<li>for: 这篇论文旨在实现多元化的 typography 设计，以增加设计文件中的多样性。</li>
<li>methods: 论文使用了一个精确的 attribute generation 模型，并建立了一个自适应的 sampling 方法，以生成具有与输入设计上下文相符的多样化 typography。</li>
<li>results: 实验结果显示，论文的模型成功将多样化的 typography 生成出来，并且保持了一致的 typography 结构。Is there anything else I can help with?<details>
<summary>Abstract</summary>
In this work, we consider the typography generation task that aims at producing diverse typographic styling for the given graphic document. We formulate typography generation as a fine-grained attribute generation for multiple text elements and build an autoregressive model to generate diverse typography that matches the input design context. We further propose a simple yet effective sampling approach that respects the consistency and distinction principle of typography so that generated examples share consistent typographic styling across text elements. Our empirical study shows that our model successfully generates diverse typographic designs while preserving a consistent typographic structure.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们考虑了 typography 生成任务，旨在为给定的图文文档生成多样化的 typography 风格。我们将 typography 生成定义为多个文本元素的细化特征生成，并构建了自适应模型来生成匹配输入设计Context 的多样化 typography。我们还提出了一种简单 yet 有效的采样方法，使得生成的例子遵循 typography 的一致性和差异原则，以保证生成的 typography 风格具有一致性。我们的实验表明，我们的模型成功地生成了多样化的 typography 设计，同时保持一致的 typography 结构。Here's the breakdown of the translation:* "typography" is translated as "typography" (字体设计)* "graphic document" is translated as "图文文档" (图文文档)* "fine-grained attribute generation" is translated as "细化特征生成" (细化特征生成)* "autoregressive model" is translated as "自适应模型" (自适应模型)* "consistency and distinction principle" is translated as "一致性和差异原则" (一致性和差异原则)* "empirical study" is translated as "实验研究" (实验研究)
</details></li>
</ul>
<hr>
<h2 id="DeNISE-Deep-Networks-for-Improved-Segmentation-Edges"><a href="#DeNISE-Deep-Networks-for-Improved-Segmentation-Edges" class="headerlink" title="DeNISE: Deep Networks for Improved Segmentation Edges"></a>DeNISE: Deep Networks for Improved Segmentation Edges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02091">http://arxiv.org/abs/2309.02091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sander Riisøen Jyhne, Per-Arne Andersen, Morten Goodwin</li>
<li>for: 提高分割图像边界质量</li>
<li>methods: 使用边检测和分割模型提高分割edge的准确性</li>
<li>results: 在航空图像 segmentation  task 中，使用 DeNISE 技术可以提高基elineresult的 IoU 至 78.9%Here’s a breakdown of each point:</li>
<li>for: The paper is written for improving the boundary quality of segmentation masks in aerial images.</li>
<li>methods: The paper uses edge detection and segmentation models to improve the accuracy of the predicted segmentation edge. The technique is not trained end-to-end and can be applied to all types of neural networks.</li>
<li>results: The paper demonstrates the potential of the DeNISE technique by improving the baseline results with a building IoU of 78.9% in aerial images.<details>
<summary>Abstract</summary>
This paper presents Deep Networks for Improved Segmentation Edges (DeNISE), a novel data enhancement technique using edge detection and segmentation models to improve the boundary quality of segmentation masks. DeNISE utilizes the inherent differences in two sequential deep neural architectures to improve the accuracy of the predicted segmentation edge. DeNISE applies to all types of neural networks and is not trained end-to-end, allowing rapid experiments to discover which models complement each other. We test and apply DeNISE for building segmentation in aerial images. Aerial images are known for difficult conditions as they have a low resolution with optical noise, such as reflections, shadows, and visual obstructions. Overall the paper demonstrates the potential for DeNISE. Using the technique, we improve the baseline results with a building IoU of 78.9%.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了深度网络提高分割边缘（DeNISE），一种使用边检测和分割模型提高分割框架质量的数据优化技术。DeNISE利用两个顺序的深度神经网络之间的自然差异来提高预测分割边的准确性。DeNISE适用于所有类型的神经网络，不需要练习端到端，可以快速进行实验，找到哪些模型相互补充。我们测试并应用DeNISE于航空图像分割。航空图像known for difficult conditions, such as low resolution, optical noise, reflections, shadows, and visual obstructions. Overall, the paper demonstrates the potential of DeNISE. Using the technique, we improve the baseline results with a building IoU of 78.9%.
</details></li>
</ul>
<hr>
<h2 id="Histograms-of-Points-Orientations-and-Dynamics-of-Orientations-Features-for-Hindi-Online-Handwritten-Character-Recognition"><a href="#Histograms-of-Points-Orientations-and-Dynamics-of-Orientations-Features-for-Hindi-Online-Handwritten-Character-Recognition" class="headerlink" title="Histograms of Points, Orientations, and Dynamics of Orientations Features for Hindi Online Handwritten Character Recognition"></a>Histograms of Points, Orientations, and Dynamics of Orientations Features for Hindi Online Handwritten Character Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02067">http://arxiv.org/abs/2309.02067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Sharma, A. G. Ramakrishnan</li>
<li>for: 这个研究的目的是提出一种独立于字形笔触方向和顺序变化的手写字符识别方法。</li>
<li>methods: 该方法将特征分别映射到点的坐标值和点的方向orientation，并计算这些特征的 histogram 从不同的空间地图中。 此外，该方法还考虑了其他已经在其他研究中用于训练分类器的不同特征，如 spatio-temporal、柯西 transform、杰氏 transform、浪涌 transform 和 histogram of oriented gradients。</li>
<li>results: SVM 分类器使用该提出的特征达到了最高的92.9%的分类精度，比其他特征的分类器的性能更高。<details>
<summary>Abstract</summary>
A set of features independent of character stroke direction and order variations is proposed for online handwritten character recognition. A method is developed that maps features like co-ordinates of points, orientations of strokes at points, and dynamics of orientations of strokes at points spatially as a function of co-ordinate values of the points and computes histograms of these features from different regions in the spatial map.   Different features like spatio-temporal, discrete Fourier transform, discrete cosine transform, discrete wavelet transform, spatial, and histograms of oriented gradients used in other studies for training classifiers for character recognition are considered. The classifier chosen for classification performance comparison, when trained with different features, is support vector machines (SVM).   The character datasets used for training and testing the classifiers consist of online handwritten samples of 96 different Hindi characters. There are 12832 and 2821 samples in training and testing datasets, respectively.   SVM classifiers trained with the proposed features has the highest classification accuracy of 92.9\% when compared to the performances of SVM classifiers trained with the other features and tested on the same testing dataset. Therefore, the proposed features have better character discriminative capability than the other features considered for comparison.
</details>
<details>
<summary>摘要</summary>
“一组独立于字符笔触方向和顺序变化的特征集被提议用于在线手写字符识别。一种方法将这些特征，如点坐标、点上笔orientation、笔orientation的动态变化，空间地映射为坐标值的函数，并从不同区域的空间地图中计算 histogram。这些特征包括空间时间、离散傅里叶变换、离散佩瑞茨变换、离散波浪变换、空间和 histogram of oriented gradients，这些特征在其他研究中用于训练类ifier进行字符识别。类ifier选择为支持向量机（SVM）。字符数据集用于训练和测试类ifier包括96个不同的印地语字符的在线手写样本，共有12832和2821个样本。 SVM类ifier使用提议的特征得到最高的92.9%的分类精度，比其他特征和测试集进行比较，因此提议的特征具有更好的字符识别能力。”
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Spatial-Temporal-Local-Feature-Difference-Method-for-Infrared-Small-moving-Target-Detection"><a href="#An-Adaptive-Spatial-Temporal-Local-Feature-Difference-Method-for-Infrared-Small-moving-Target-Detection" class="headerlink" title="An Adaptive Spatial-Temporal Local Feature Difference Method for Infrared Small-moving Target Detection"></a>An Adaptive Spatial-Temporal Local Feature Difference Method for Infrared Small-moving Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02054">http://arxiv.org/abs/2309.02054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongkang Zhao, Chuang Zhu, Yuan Li, Shuaishuai Wang, Zihan Lan, Yuanyuan Qiao</li>
<li>for: 这个研究旨在提高红外线小运动目标准确的检测，并且提出了一个新的方法。</li>
<li>methods: 这个方法使用了空间和时间领域的滤波器，并且将像素级别的背景降噪模组组合到出力中，以增强目标和背景的 контра斯特。</li>
<li>results: 实验结果显示，提案的方法在红外线小运动目标检测方面比现有的方法表现更好。<details>
<summary>Abstract</summary>
Detecting small moving targets accurately in infrared (IR) image sequences is a significant challenge. To address this problem, we propose a novel method called spatial-temporal local feature difference (STLFD) with adaptive background suppression (ABS). Our approach utilizes filters in the spatial and temporal domains and performs pixel-level ABS on the output to enhance the contrast between the target and the background. The proposed method comprises three steps. First, we obtain three temporal frame images based on the current frame image and extract two feature maps using the designed spatial domain and temporal domain filters. Next, we fuse the information of the spatial domain and temporal domain to produce the spatial-temporal feature maps and suppress noise using our pixel-level ABS module. Finally, we obtain the segmented binary map by applying a threshold. Our experimental results demonstrate that the proposed method outperforms existing state-of-the-art methods for infrared small-moving target detection.
</details>
<details>
<summary>摘要</summary>
探测红外图像序列中小目标的准确性是一项非常重要的挑战。为解决这个问题，我们提出了一种新的方法，即空间-时间本地特征差分（STLFD）与适应背景抑制（ABS）。我们的方法使用空间和时间Domain的滤波器，并在输出中进行像素级ABS处理，以增强目标和背景的对比度。我们的方法包括三步：第一步，基于当前帧图像，获取三帧图像，并使用我们设计的空间频谱和时间频谱滤波器来提取两个特征图。第二步，将空间频谱和时间频谱的信息融合，生成空间-时间特征图，并使用我们的像素级ABS模块来抑制噪声。第三步，应用阈值来获得分割的二值图。我们的实验结果表明，我们的方法可以超过现有的状态艺术方法对红外小目标探测的性能。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-3D-Object-Detection-with-Random-Boxes"><a href="#Diffusion-based-3D-Object-Detection-with-Random-Boxes" class="headerlink" title="Diffusion-based 3D Object Detection with Random Boxes"></a>Diffusion-based 3D Object Detection with Random Boxes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02049">http://arxiv.org/abs/2309.02049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Zhou, Jinghua Hou, Tingting Yao, Dingkang Liang, Zhe Liu, Zhikang Zou, Xiaoqing Ye, Jianwei Cheng, Xiang Bai</li>
<li>for: 三角形物体探测是自动驾驶的关键任务，现有的锚点基于方法依赖于经验性的锚点设置，导致算法缺乏启发性。</li>
<li>methods: 我们提出的Diff3Det方法将Diffusion模型应用到提议生成阶段，视为探测目标的生成。在训练阶段，物体框从真实框架噪声过程中演化到高斯分布，解码器学习反转噪声过程。在推断阶段，模型不断级联一组随机框架，进行预测结果的精细化。</li>
<li>results: 我们在KITTI测试benchmark上进行了详细的实验，与传统锚点基于3D探测方法进行比较，实现了显著的性能提升。<details>
<summary>Abstract</summary>
3D object detection is an essential task for achieving autonomous driving. Existing anchor-based detection methods rely on empirical heuristics setting of anchors, which makes the algorithms lack elegance. In recent years, we have witnessed the rise of several generative models, among which diffusion models show great potential for learning the transformation of two distributions. Our proposed Diff3Det migrates the diffusion model to proposal generation for 3D object detection by considering the detection boxes as generative targets. During training, the object boxes diffuse from the ground truth boxes to the Gaussian distribution, and the decoder learns to reverse this noise process. In the inference stage, the model progressively refines a set of random boxes to the prediction results. We provide detailed experiments on the KITTI benchmark and achieve promising performance compared to classical anchor-based 3D detection methods.
</details>
<details>
<summary>摘要</summary>
三维对象检测是自动驾驶的关键任务。现有的锚点基于方法靠 Empirical 规则设置锚点，这使得算法缺乏吟芳。近年来，我们所见证到了多种生成模型的出现，其中扩散模型在学习两个分布之间的变换方面表现出了极大的潜力。我们提出的 Diff3Det 将扩散模型应用到提议生成中，并考虑检测框为生成目标。在训练阶段，对象框从真实框 diffuse 到 Gaussian 分布，decoder 学习恢复这种噪声过程。在推测阶段，模型逐渐精细化一组随机框到预测结果。我们对 KITTI 测试准则进行详细的实验，并与经典锚点基于三维检测方法比较得出了良好的表现。
</details></li>
</ul>
<hr>
<h2 id="Decomposed-Guided-Dynamic-Filters-for-Efficient-RGB-Guided-Depth-Completion"><a href="#Decomposed-Guided-Dynamic-Filters-for-Efficient-RGB-Guided-Depth-Completion" class="headerlink" title="Decomposed Guided Dynamic Filters for Efficient RGB-Guided Depth Completion"></a>Decomposed Guided Dynamic Filters for Efficient RGB-Guided Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02043">http://arxiv.org/abs/2309.02043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YufeiWang777/DGDF">https://github.com/YufeiWang777/DGDF</a></li>
<li>paper_authors: Yufei Wang, Yuxin Mao, Qi Liu, Yuchao Dai</li>
<li>for: 这个 paper 目的是完善 RGB 图像中的深度映射，使用 sparse depth measurement 和对应的 RGB 图像。</li>
<li>methods: 使用 guided dynamic filters，将 RGB 特征变换为深度特征，并使用内容适应的混合层来将 filters 分解为具有内容属性的分布式元件。</li>
<li>results: 根据 proposed 的想法，可以实现对 RGB-D 复合 зада зада的优化，并在 KITTI 数据集上实现 state-of-the-art 的表现，同时在 NYUv2 数据集上实现相似的表现。<details>
<summary>Abstract</summary>
RGB-guided depth completion aims at predicting dense depth maps from sparse depth measurements and corresponding RGB images, where how to effectively and efficiently exploit the multi-modal information is a key issue. Guided dynamic filters, which generate spatially-variant depth-wise separable convolutional filters from RGB features to guide depth features, have been proven to be effective in this task. However, the dynamically generated filters require massive model parameters, computational costs and memory footprints when the number of feature channels is large. In this paper, we propose to decompose the guided dynamic filters into a spatially-shared component multiplied by content-adaptive adaptors at each spatial location. Based on the proposed idea, we introduce two decomposition schemes A and B, which decompose the filters by splitting the filter structure and using spatial-wise attention, respectively. The decomposed filters not only maintain the favorable properties of guided dynamic filters as being content-dependent and spatially-variant, but also reduce model parameters and hardware costs, as the learned adaptors are decoupled with the number of feature channels. Extensive experimental results demonstrate that the methods using our schemes outperform state-of-the-art methods on the KITTI dataset, and rank 1st and 2nd on the KITTI benchmark at the time of submission. Meanwhile, they also achieve comparable performance on the NYUv2 dataset. In addition, our proposed methods are general and could be employed as plug-and-play feature fusion blocks in other multi-modal fusion tasks such as RGB-D salient object detection.
</details>
<details>
<summary>摘要</summary>
RGB-导向深度完成任务是预测粗略深度图像从稀疏深度测量和对应的RGB图像，其中如何有效地和有效地利用多Modal信息是关键问题。受导动 филь特，生成从RGB特征生成空间不同的depth-wise分解卷积滤波器，用于导航深度特征，已经证明是有效的。然而，生成的滤波器需要大量的模型参数，计算成本和内存占用率，特别是当特征通道数大的时候。在这篇论文中，我们提议将导动 filters decomposed into a spatially-shared component multiplied by content-adaptive adaptors at each spatial location。根据我们的想法，我们引入了A和B两种分解方案，通过在空间位置上分解滤波器结构并使用空间层别注意，分解滤波器。这些分解滤波器不仅保持了导动 filters 的有利特性，例如具有内容依赖和空间不同的特性，同时还减少了模型参数和硬件成本，因为学习的适应器被分离到特征通道数。我们的方法在KITTI dataset上实现了比州-of-the-art的表现，并在KITTI benchmark上 ranked 1st and 2nd at the time of submission。此外，我们的方法也实现了与NYUv2 dataset上的相似表现。此外，我们的提议是通用的，可以作为RGB-D突出对象检测中的特性融合块。
</details></li>
</ul>
<hr>
<h2 id="Learning-Cross-Modal-Affinity-for-Referring-Video-Object-Segmentation-Targeting-Limited-Samples"><a href="#Learning-Cross-Modal-Affinity-for-Referring-Video-Object-Segmentation-Targeting-Limited-Samples" class="headerlink" title="Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples"></a>Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02041">http://arxiv.org/abs/2309.02041</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hengliusky/few_shot_rvos">https://github.com/hengliusky/few_shot_rvos</a></li>
<li>paper_authors: Guanghui Li, Mingqi Gao, Heng Liu, Xiantong Zhen, Feng Zheng</li>
<li>for: 本研究旨在提出一种基于Transformer架构的简单 yet有效的模型，以解决现有的 Referring Video Object Segmentation（RVOS）方法在面临有限样本的情况下表现不佳的问题。</li>
<li>methods: 我们提出了一种新的交叉模式相互关联（CMA）模块，该模块通过建立多模式相互关联，快速学习新的semantic信息，使模型能够适应不同的场景。</li>
<li>results: 我们的模型在几种不同的场景下，只使用了几个样本来学习，仍然可以达到当前最佳性能，比如在Mini-Ref-YouTube-VOS上的平均性能为53.1 J和54.8 F，比基eline上高出10%。此外，我们在Mini-Ref-SAIL-VOS上也取得了很出色的结果，达到77.7 J和74.8 F，与基eline相比明显优于。<details>
<summary>Abstract</summary>
Referring video object segmentation (RVOS), as a supervised learning task, relies on sufficient annotated data for a given scene. However, in more realistic scenarios, only minimal annotations are available for a new scene, which poses significant challenges to existing RVOS methods. With this in mind, we propose a simple yet effective model with a newly designed cross-modal affinity (CMA) module based on a Transformer architecture. The CMA module builds multimodal affinity with a few samples, thus quickly learning new semantic information, and enabling the model to adapt to different scenarios. Since the proposed method targets limited samples for new scenes, we generalize the problem as - few-shot referring video object segmentation (FS-RVOS). To foster research in this direction, we build up a new FS-RVOS benchmark based on currently available datasets. The benchmark covers a wide range and includes multiple situations, which can maximally simulate real-world scenarios. Extensive experiments show that our model adapts well to different scenarios with only a few samples, reaching state-of-the-art performance on the benchmark. On Mini-Ref-YouTube-VOS, our model achieves an average performance of 53.1 J and 54.8 F, which are 10% better than the baselines. Furthermore, we show impressive results of 77.7 J and 74.8 F on Mini-Ref-SAIL-VOS, which are significantly better than the baselines. Code is publicly available at https://github.com/hengliusky/Few_shot_RVOS.
</details>
<details>
<summary>摘要</summary>
描述视频对象分割（RVOS）作为一种监督学习任务，需要具备充足的标注数据，以便在新场景中进行学习。然而，在更真实的场景中，只有有限的标注数据可用于新场景，这会对现有的RVOS方法带来重大挑战。为了解决这个问题，我们提出了一种简单 yet effective的模型，基于 transformer 架构的 cross-modal affinity（CMA）模块。CMA模块可以快速学习新的semantic信息，并在不同的场景中建立多modal的联系，使模型能够适应不同的场景。由于我们的方法targets限样数据，我们将问题推广为- few-shot referring video object segmentation（FS-RVOS）。为了推动这个方向的研究，我们建立了一个新的FS-RVOS benchmark，该benchmark包括了多种情况，可以最大化 simulate real-world scenarios。我们的实验表明，我们的模型能够适应不同的场景，只需要几个样本，并且在 benchmark 上达到了state-of-the-art的性能。在 Mini-Ref-YouTube-VOS 上，我们的模型取得了53.1 J和54.8 F的性能，比基eline高出10%。此外，我们在 Mini-Ref-SAIL-VOS 上取得了77.7 J和74.8 F的性能，与基eline significatively better。我们的代码可以在 <https://github.com/hengliusky/Few_shot_RVOS> 上下载。
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-efficient-vision-transformers-algorithms-techniques-and-performance-benchmarking"><a href="#A-survey-on-efficient-vision-transformers-algorithms-techniques-and-performance-benchmarking" class="headerlink" title="A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking"></a>A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02031">http://arxiv.org/abs/2309.02031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Papa, Paolo Russo, Irene Amerini, Luping Zhou</li>
<li>for: 提高vision transformer（ViT）的效率和可扩展性，以便在实际应用中使用。</li>
<li>methods: 研究和分析了四种高效策略，包括减少模型大小、知识传播、量化和减少计算成本。</li>
<li>results: 通过对不同应用场景进行分析和讨论，探讨了现有高效策略的性能。同时，也提出了一些未来研究的挑战和机遇。<details>
<summary>Abstract</summary>
Vision Transformer (ViT) architectures are becoming increasingly popular and widely employed to tackle computer vision applications. Their main feature is the capacity to extract global information through the self-attention mechanism, outperforming earlier convolutional neural networks. However, ViT deployment and performance have grown steadily with their size, number of trainable parameters, and operations. Furthermore, self-attention's computational and memory cost quadratically increases with the image resolution. Generally speaking, it is challenging to employ these architectures in real-world applications due to many hardware and environmental restrictions, such as processing and computational capabilities. Therefore, this survey investigates the most efficient methodologies to ensure sub-optimal estimation performances. More in detail, four efficient categories will be analyzed: compact architecture, pruning, knowledge distillation, and quantization strategies. Moreover, a new metric called Efficient Error Rate has been introduced in order to normalize and compare models' features that affect hardware devices at inference time, such as the number of parameters, bits, FLOPs, and model size. Summarizing, this paper firstly mathematically defines the strategies used to make Vision Transformer efficient, describes and discusses state-of-the-art methodologies, and analyzes their performances over different application scenarios. Toward the end of this paper, we also discuss open challenges and promising research directions.
</details>
<details>
<summary>摘要</summary>
computer vision 应用中vision transformer（ViT）架构受到越来越多的关注和推广使用，主要特点是通过自注意机制提取全局信息，超越了先前的卷积神经网络。然而，ViT的部署和性能随其大小、可训练参数数量和运算数量的增长。此外，自注意的计算和内存成本随图像分辨率的增长而呈 quadratic 增长。因此，在真实世界应用中使用这些架构很困难，主要因为硬件和环境限制，如处理和计算能力。因此，本纪要Investigate the most efficient methodologies to ensure sub-optimal estimation performances。具体来说，本文分析了四种高效的类别：压缩架构、采样、知识继承和量化策略。此外，我们还引入了一个新的度量，称为高效错误率，以便对各种模型的特征进行Normalize和比较，这些特征在推理时影响硬件设备，如参数数量、位数、FLOPs和模型大小。总之，本文首先数学定义了使Vision Transformer高效的策略，描述了和讨论了当前领域的状态泰尊方法，并对不同应用场景进行了分析。在本文的末尾，我们还讨论了开放的挑战和潜在的研究方向。
</details></li>
</ul>
<hr>
<h2 id="RawHDR-High-Dynamic-Range-Image-Reconstruction-from-a-Single-Raw-Image"><a href="#RawHDR-High-Dynamic-Range-Image-Reconstruction-from-a-Single-Raw-Image" class="headerlink" title="RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image"></a>RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02020">http://arxiv.org/abs/2309.02020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jackzou233/rawhdr">https://github.com/jackzou233/rawhdr</a></li>
<li>paper_authors: Yunhao Zou, Chenggang Yan, Ying Fu</li>
<li>for: 这 paper 的目的是生成高动态范围 (HDR) 图像，从 Raw 感知器数据中恢复场景信息。</li>
<li>methods: 该 paper 使用了一种专门为 Raw 图像设计的模型，利用 Raw 数据的特点来促进 Raw-to-HDR 映射。具体来说，它学习了曝光面积来分 distinguishing 高动态场景中的软和硬区。然后，它引入了两种重要的导航：双感知导航和全球空间导航，以帮助恢复场景信息。</li>
<li>results: 该 paper 的实验表明，提出的 Raw-to-HDR 重建模型在训练和测试 datasets 上具有superiority，同时新captured dataset 也在实验中得到了验证。<details>
<summary>Abstract</summary>
High dynamic range (HDR) images capture much more intensity levels than standard ones. Current methods predominantly generate HDR images from 8-bit low dynamic range (LDR) sRGB images that have been degraded by the camera processing pipeline. However, it becomes a formidable task to retrieve extremely high dynamic range scenes from such limited bit-depth data. Unlike existing methods, the core idea of this work is to incorporate more informative Raw sensor data to generate HDR images, aiming to recover scene information in hard regions (the darkest and brightest areas of an HDR scene). To this end, we propose a model tailor-made for Raw images, harnessing the unique features of Raw data to facilitate the Raw-to-HDR mapping. Specifically, we learn exposure masks to separate the hard and easy regions of a high dynamic scene. Then, we introduce two important guidances, dual intensity guidance, which guides less informative channels with more informative ones, and global spatial guidance, which extrapolates scene specifics over an extended spatial domain. To verify our Raw-to-HDR approach, we collect a large Raw/HDR paired dataset for both training and testing. Our empirical evaluations validate the superiority of the proposed Raw-to-HDR reconstruction model, as well as our newly captured dataset in the experiments.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）图像捕捉到了标准图像的多倍级别。现有方法主要从8位低动态范围（LDR）sRGB图像中生成HDR图像，这些图像在摄像头处理管道中受到了很大的压缩。然而，从这些有限位数据中恢复极高动态范围场景变得非常困难。与现有方法不同，本工作的核心思想是利用Raw感知器数据来生成HDR图像，以便恢复场景信息在极端区域（HDR场景中最暗和最亮区域）。为此，我们提出了专门为Raw图像设计的模型，利用Raw数据的独特特性来促进Raw-to-HDR映射。具体来说，我们学习出光mask，以分类场景中的困难区域和易于处理区域。然后，我们引入两种重要的导航，分别是双感知导航和全球空间导航。双感知导航使用不具有很多信息的通道与具有更多信息的通道相互拥抱，而全球空间导航通过把场景特点推广到更广泛的空间领域来描述场景。为验证我们的Raw-to-HDR重建模型，我们收集了大量Raw/HDR对应的数据集，用于训练和测试。我们的实验证明了我们提出的Raw-to-HDR重建模型的优越性，以及我们 newly captured数据集在实验中的有用性。
</details></li>
</ul>
<hr>
<h2 id="Logarithmic-Mathematical-Morphology-theory-and-applications"><a href="#Logarithmic-Mathematical-Morphology-theory-and-applications" class="headerlink" title="Logarithmic Mathematical Morphology: theory and applications"></a>Logarithmic Mathematical Morphology: theory and applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02007">http://arxiv.org/abs/2309.02007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillaume Noyel</li>
<li>for:  Addressing the issue of lighting variations in Mathematical Morphology for grey-level functions.</li>
<li>methods:  Defining a new framework called Logarithmic Mathematical Morphology (LMM) with an additive law that varies according to the image amplitude, and using it to define operators that are robust to lighting variations.</li>
<li>results:  Comparing the LMM method with three state-of-the-art approaches on eye-fundus images with non-uniform lighting variations, and showing that the LMM approach has better robustness to such variations.Here’s the full text in Simplified Chinese:</li>
<li>for:  Mathematical Morphology 中的批处理问题，即在图像中处理不同亮度的问题。</li>
<li>methods:  Logarithmic Mathematical Morphology (LMM)  Framework，使用不同亮度的加法则来处理图像，以提高对不同亮度的Robustness。</li>
<li>results:  LMM 方法与三种现有方法进行比较，在不同亮度下的眼膜图像中 segmentation  vessles 的问题上，LMM 方法表现更好，具有更高的Robustness。<details>
<summary>Abstract</summary>
Classically, in Mathematical Morphology, an image (i.e., a grey-level function) is analysed by another image which is named the structuring element or the structuring function. This structuring function is moved over the image domain and summed to the image. However, in an image presenting lighting variations, the analysis by a structuring function should require that its amplitude varies according to the image intensity. Such a property is not verified in Mathematical Morphology for grey level functions, when the structuring function is summed to the image with the usual additive law. In order to address this issue, a new framework is defined with an additive law for which the amplitude of the structuring function varies according to the image amplitude. This additive law is chosen within the Logarithmic Image Processing framework and models the lighting variations with a physical cause such as a change of light intensity or a change of camera exposure-time. The new framework is named Logarithmic Mathematical Morphology (LMM) and allows the definition of operators which are robust to such lighting variations. In images with uniform lighting variations, those new LMM operators perform better than usual morphological operators. In eye-fundus images with non-uniform lighting variations, a LMM method for vessel segmentation is compared to three state-of-the-art approaches. Results show that the LMM approach has a better robustness to such variations than the three others.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Retail-store-customer-behavior-analysis-system-Design-and-Implementation"><a href="#Retail-store-customer-behavior-analysis-system-Design-and-Implementation" class="headerlink" title="Retail store customer behavior analysis system: Design and Implementation"></a>Retail store customer behavior analysis system: Design and Implementation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03232">http://arxiv.org/abs/2309.03232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Dinh Nguyen, Keisuke Hihara, Tung Cao Hoang, Yumeka Utada, Akihiko Torii, Naoki Izumi, Nguyen Thanh Thuy, Long Quoc Tran<br>for: 这个研究的目的是提高零售店内客户满意度，通过个性化服务提供值。methods: 这个研究使用了深度学习技术，包括深度神经网络，来分析客户在店内互动的行为。results: 研究结果表明，使用深度学习技术可以更好地检测客户行为，并提供有用的客户行为数据可视化。<details>
<summary>Abstract</summary>
Understanding customer behavior in retail stores plays a crucial role in improving customer satisfaction by adding personalized value to services. Behavior analysis reveals both general and detailed patterns in the interaction of customers with a store items and other people, providing store managers with insight into customer preferences. Several solutions aim to utilize this data by recognizing specific behaviors through statistical visualization. However, current approaches are limited to the analysis of small customer behavior sets, utilizing conventional methods to detect behaviors. They do not use deep learning techniques such as deep neural networks, which are powerful methods in the field of computer vision. Furthermore, these methods provide limited figures when visualizing the behavioral data acquired by the system. In this study, we propose a framework that includes three primary parts: mathematical modeling of customer behaviors, behavior analysis using an efficient deep learning based system, and individual and group behavior visualization. Each module and the entire system were validated using data from actual situations in a retail store.
</details>
<details>
<summary>摘要</summary>
理解顾客在商场中的行为对于提高顾客满意度具有关键作用。行为分析可以揭示顾客与店内物品和其他人的互动特征，为店长提供顾客偏好的信息。然而，目前的方法受到小型顾客行为集的分析的限制，并且使用传统方法来探测行为。这些方法不使用深度学习技术，如深度神经网络，这些技术在计算机视觉领域具有强大能力。此外，这些方法只能提供有限的行为数据视图。在本研究中，我们提出了一个框架，包括三个主要部分：顾客行为数学模型、深度学习基于系统的行为分析和个人和群体行为视图。每个模块和整个系统都被使用实际情况中的商场数据验证。
</details></li>
</ul>
<hr>
<h2 id="NICE-CVPR-2023-Challenge-on-Zero-shot-Image-Captioning"><a href="#NICE-CVPR-2023-Challenge-on-Zero-shot-Image-Captioning" class="headerlink" title="NICE: CVPR 2023 Challenge on Zero-shot Image Captioning"></a>NICE: CVPR 2023 Challenge on Zero-shot Image Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01961">http://arxiv.org/abs/2309.01961</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taehoon Kim, Pyunghwan Ahn, Sangyun Kim, Sihaeng Lee, Mark Marsden, Alessandra Sala, Seung Hwan Kim, Bohyung Han, Kyoung Mu Lee, Honglak Lee, Kyounghoon Bae, Xiangyu Wu, Yi Gao, Hailiang Zhang, Yang Yang, Weili Guo, Jianfeng Lu, Youngtaek Oh, Jae Won Cho, Dong-jin Kim, In So Kweon, Junmo Kim, Wooyoung Kang, Won Young Jhoo, Byungseok Roh, Jonghwan Mun, Solgil Oh, Kenan Emir Ak, Gwang-Gook Lee, Yan Xu, Mingwei Shen, Kyomin Hwang, Wonsik Shin, Kamin Lee, Wonhark Park, Dongkwan Lee, Nojun Kwak, Yujin Wang, Yimu Wang, Tiancheng Gu, Xingchang Lv, Mingmao Sun</li>
<li>for: 挑战计划推动计算机视觉领域内的模型精度和公平性进步。</li>
<li>methods: 使用新的评估数据集，挑战计算机视觉模型在多个领域中处理新类型的图像描述。</li>
<li>results: 挑战结果包括新的评估数据集、评估方法和优秀入选结果等，帮助提高各种视觉语言任务的AI模型。<details>
<summary>Abstract</summary>
In this report, we introduce NICE (New frontiers for zero-shot Image Captioning Evaluation) project and share the results and outcomes of 2023 challenge. This project is designed to challenge the computer vision community to develop robust image captioning models that advance the state-of-the-art both in terms of accuracy and fairness. Through the challenge, the image captioning models were tested using a new evaluation dataset that includes a large variety of visual concepts from many domains. There was no specific training data provided for the challenge, and therefore the challenge entries were required to adapt to new types of image descriptions that had not been seen during training. This report includes information on the newly proposed NICE dataset, evaluation methods, challenge results, and technical details of top-ranking entries. We expect that the outcomes of the challenge will contribute to the improvement of AI models on various vision-language tasks.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我们介绍了NICE（新领域零基础图像描述评价）项目，并分享2023年度挑战的结果和成果。这个项目的目的是挑战计算机视觉社区，以开发能够在精度和公平性两个方面提高的图像描述模型。通过挑战，图像描述模型被测试使用了一个新的评价数据集，该数据集包含了多个视觉领域的各种图像描述。没有提供专门的训练数据，因此挑战参赛作品需要适应新的图像描述类型，这些类型在训练过程中未被见过。这份报告包括NICE数据集的新提案、评价方法、挑战结果以及技术细节。我们期望这些成果将对各种视觉语言任务的AI模型产生改进。
</details></li>
</ul>
<hr>
<h2 id="Empowering-Low-Light-Image-Enhancer-through-Customized-Learnable-Priors"><a href="#Empowering-Low-Light-Image-Enhancer-through-Customized-Learnable-Priors" class="headerlink" title="Empowering Low-Light Image Enhancer through Customized Learnable Priors"></a>Empowering Low-Light Image Enhancer through Customized Learnable Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01958">http://arxiv.org/abs/2309.01958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zheng980629/cue">https://github.com/zheng980629/cue</a></li>
<li>paper_authors: Naishan Zheng, Man Zhou, Yanmeng Dong, Xiangyu Rui, Jie Huang, Chongyi Li, Feng Zhao</li>
<li>for: 提高低光照图像的品质，提高图像的亮度和降低噪音。</li>
<li>methods: 使用自定义学习的偏好来改进深度 unfolding 架构的透明度和解释能力，包括结构流和优化流两种方法。</li>
<li>results: 对多个低光照图像进行了广泛的实验，并证明了我们提出的方法在比 estado del arte 方法之上有superiority。<details>
<summary>Abstract</summary>
Deep neural networks have achieved remarkable progress in enhancing low-light images by improving their brightness and eliminating noise. However, most existing methods construct end-to-end mapping networks heuristically, neglecting the intrinsic prior of image enhancement task and lacking transparency and interpretability. Although some unfolding solutions have been proposed to relieve these issues, they rely on proximal operator networks that deliver ambiguous and implicit priors. In this work, we propose a paradigm for low-light image enhancement that explores the potential of customized learnable priors to improve the transparency of the deep unfolding paradigm. Motivated by the powerful feature representation capability of Masked Autoencoder (MAE), we customize MAE-based illumination and noise priors and redevelop them from two perspectives: 1) \textbf{structure flow}: we train the MAE from a normal-light image to its illumination properties and then embed it into the proximal operator design of the unfolding architecture; and m2) \textbf{optimization flow}: we train MAE from a normal-light image to its gradient representation and then employ it as a regularization term to constrain noise in the model output. These designs improve the interpretability and representation capability of the model.Extensive experiments on multiple low-light image enhancement datasets demonstrate the superiority of our proposed paradigm over state-of-the-art methods. Code is available at https://github.com/zheng980629/CUE.
</details>
<details>
<summary>摘要</summary>
深度神经网络已经取得了优化低光照图像的remarkable进步，提高图像的亮度和降低噪声。然而，大多数现有方法是通过规则性的映射网络来实现，忽略了图像优化任务的内在先验知识，lacking transparency和可读性。虽然一些解开解决方案已经被提出，但它们基于 proximal operator networks，导致权重不明确和隐式先验知识。在这种情况下，我们提出了一种低光照图像优化 paradigma，exploring the potential of customized learnable priors to improve the transparency of the deep unfolding paradigm。我们的方法是通过以下两种方法来改进 interpretable和 representation capability of the model：1. 结构流：我们从一个正常光照图像开始，通过训练MAE来学习图像的照明特性，然后将其embed到 proximal operator 设计中。2. 优化流：我们从一个正常光照图像开始，通过训练MAE来学习图像的梯度表示，然后使其作为模型输出中的常数约束项。我们的方法在多个低光照图像优化数据集上进行了广泛的实验，并证明了我们的方法的优越性。代码可以在https://github.com/zheng980629/CUE中找到。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bayesian-Computational-Imaging-with-a-Surrogate-Score-Based-Prior"><a href="#Efficient-Bayesian-Computational-Imaging-with-a-Surrogate-Score-Based-Prior" class="headerlink" title="Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior"></a>Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01949">http://arxiv.org/abs/2309.01949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Berthy T. Feng, Katherine L. Bouman</li>
<li>for: 这个论文的目的是提出一种代理函数，以便高效地使用分数基的先验知识来解决难定的图像重构问题。</li>
<li>methods: 这个论文使用了分数基扩散模型，将其转化为probabilistic priors，以解决难定的图像重构问题。</li>
<li>results:  compared to之前的精确先验，这个代理先验可以加速对大型图像的变量推理的优化，至少提高两个数量级。此外，我们的原则途径也可以获得更高的准确性图像，比非泊尔baseline。<details>
<summary>Abstract</summary>
We propose a surrogate function for efficient use of score-based priors for Bayesian inverse imaging. Recent work turned score-based diffusion models into probabilistic priors for solving ill-posed imaging problems by appealing to an ODE-based log-probability function. However, evaluating this function is computationally inefficient and inhibits posterior estimation of high-dimensional images. Our proposed surrogate prior is based on the evidence lower-bound of a score-based diffusion model. We demonstrate the surrogate prior on variational inference for efficient approximate posterior sampling of large images. Compared to the exact prior in previous work, our surrogate prior accelerates optimization of the variational image distribution by at least two orders of magnitude. We also find that our principled approach achieves higher-fidelity images than non-Bayesian baselines that involve hyperparameter-tuning at inference. Our work establishes a practical path forward for using score-based diffusion models as general-purpose priors for imaging.
</details>
<details>
<summary>摘要</summary>
我们提议一种代理函数，以便高效地使用分数基金函数对bayesian反射干涉进行减少。在最近的工作中，将分数基 diffusion 模型转换成了 probabilistic prior，以解决ill-posed imaging问题。然而，计算这个函数的计算复杂度高，使得 posterior 估计高维图像的权重估计受到限制。我们的提议的代理假设基于分数基 diffusion 模型的证据下界。我们在变量推理中使用这种代理假设，以便高效地批量样本大图像的变量分布。与前一个工作中的精确假设相比，我们的代理假设可以加速变量图像分布的估计，至少提高两个数量级。此外，我们发现我们的原则途径可以在非泊利基eline上实现更高的图像质量。我们的工作建立了使用分数基 diffusion 模型作为通用假设的实用方法，以解决反射干涉问题。
</details></li>
</ul>
<hr>
<h2 id="Extract-and-Adaptation-Network-for-3D-Interacting-Hand-Mesh-Recovery"><a href="#Extract-and-Adaptation-Network-for-3D-Interacting-Hand-Mesh-Recovery" class="headerlink" title="Extract-and-Adaptation Network for 3D Interacting Hand Mesh Recovery"></a>Extract-and-Adaptation Network for 3D Interacting Hand Mesh Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01943">http://arxiv.org/abs/2309.01943</a></li>
<li>repo_url: None</li>
<li>paper_authors: JoonKyu Park, Daniel Sungho Jung, Gyeongsik Moon, Kyoung Mu Lee</li>
<li>for: 本研究旨在提高3D互动手套逻辑恢复的准确性，即使两手的姿势很不同。</li>
<li>methods: 我们提出了一种名为EANet的提取和适应网络，其中包括EABlock作为网络的主要组件。而不是直接使用两手特征作为输入 токен，我们的EABlock使用了两种新的类型的特征 токен：SimToken和JoinToken。这两种特征 токен是从分离的两手特征组合而成，因此更 robust于远程特征问题。</li>
<li>results: 我们的EANet实现了3D互动手套测试benchmark上的状态之最好性能。代码可以在<a target="_blank" rel="noopener" href="https://github.com/jkpark0825/EANet%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/jkpark0825/EANet中下载。</a><details>
<summary>Abstract</summary>
Understanding how two hands interact with each other is a key component of accurate 3D interacting hand mesh recovery. However, recent Transformer-based methods struggle to learn the interaction between two hands as they directly utilize two hand features as input tokens, which results in distant token problem. The distant token problem represents that input tokens are in heterogeneous spaces, leading Transformer to fail in capturing correlation between input tokens. Previous Transformer-based methods suffer from the problem especially when poses of two hands are very different as they project features from a backbone to separate left and right hand-dedicated features. We present EANet, extract-and-adaptation network, with EABlock, the main component of our network. Rather than directly utilizing two hand features as input tokens, our EABlock utilizes two complementary types of novel tokens, SimToken and JoinToken, as input tokens. Our two novel tokens are from a combination of separated two hand features; hence, it is much more robust to the distant token problem. Using the two type of tokens, our EABlock effectively extracts interaction feature and adapts it to each hand. The proposed EANet achieves the state-of-the-art performance on 3D interacting hands benchmarks. The codes are available at https://github.com/jkpark0825/EANet.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose EANet, an extract-and-adaptation network that utilizes two novel types of tokens, SimToken and JoinToken, as input tokens. These tokens are derived from a combination of separated two hand features, making them more robust to the distant token problem. Our EABlock effectively extracts interaction features and adapts them to each hand using these tokens.The proposed EANet achieves state-of-the-art performance on 3D interacting hands benchmarks. The codes are available at https://github.com/jkpark0825/EANet.
</details></li>
</ul>
<hr>
<h2 id="DR-Pose-A-Two-stage-Deformation-and-Registration-Pipeline-for-Category-level-6D-Object-Pose-Estimation"><a href="#DR-Pose-A-Two-stage-Deformation-and-Registration-Pipeline-for-Category-level-6D-Object-Pose-Estimation" class="headerlink" title="DR-Pose: A Two-stage Deformation-and-Registration Pipeline for Category-level 6D Object Pose Estimation"></a>DR-Pose: A Two-stage Deformation-and-Registration Pipeline for Category-level 6D Object Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01925">http://arxiv.org/abs/2309.01925</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zray26/dr-pose">https://github.com/zray26/dr-pose</a></li>
<li>paper_authors: Lei Zhou, Zhiyang Liu, Runze Gan, Haozhe Wang, Marcelo H. Ang Jr</li>
<li>for: 提高对象pose estimation的精度，使用两个阶段管道设计</li>
<li>methods: 使用完成帮助型折叠阶段和缩放注册阶段，首先使用点云完成方法生成目标对象的未见部分，然后使用注册网络提取pose敏感特征并预测对象部分点云在坐标空间的表示</li>
<li>results: 在CAMERA25和REAL275测试数据集上，DR-Pose比前状态艺术形式基于方法提供了更高的精度Here’s the same information in Simplified Chinese:</li>
<li>for: 提高对象pose estimation的精度，使用两个阶段管道设计</li>
<li>methods: 使用完成帮助型折叠阶段和缩放注册阶段，首先使用点云完成方法生成目标对象的未见部分，然后使用注册网络提取pose敏感特征并预测对象部分点云在坐标空间的表示</li>
<li>results: 在CAMERA25和REAL275测试数据集上，DR-Pose比前状态艺术形式基于方法提供了更高的精度<details>
<summary>Abstract</summary>
Category-level object pose estimation involves estimating the 6D pose and the 3D metric size of objects from predetermined categories. While recent approaches take categorical shape prior information as reference to improve pose estimation accuracy, the single-stage network design and training manner lead to sub-optimal performance since there are two distinct tasks in the pipeline. In this paper, the advantage of two-stage pipeline over single-stage design is discussed. To this end, we propose a two-stage deformation-and registration pipeline called DR-Pose, which consists of completion-aided deformation stage and scaled registration stage. The first stage uses a point cloud completion method to generate unseen parts of target object, guiding subsequent deformation on the shape prior. In the second stage, a novel registration network is designed to extract pose-sensitive features and predict the representation of object partial point cloud in canonical space based on the deformation results from the first stage. DR-Pose produces superior results to the state-of-the-art shape prior-based methods on both CAMERA25 and REAL275 benchmarks. Codes are available at https://github.com/Zray26/DR-Pose.git.
</details>
<details>
<summary>摘要</summary>
Category-level object pose estimation 涉及到对 predetermined categories 中对象的 6D 姿 pose 和 3D  метри尺度的估算。Recent approaches 使用 categorical shape prior information 作为参考来提高姿 pose 估算精度，但是单Stage 网络设计和训练方式会导致下游表现不佳，因为这有两个不同的任务在管道中。在本文中，我们讨论了 two-stage 管道的优势。为了实现这一目标，我们提出了一种 two-stage deformation-and registration 管道，称为 DR-Pose，它包括 completion-aided deformation stage 和 scaled registration stage。在第一个阶段中，我们使用一种点云完成方法来生成目标对象的未看到部分，并用这些部分作为导向后续形态变换的引导。在第二个阶段中，我们设计了一种新的注册网络，用于提取姿 pose-sensitive 特征并预测对象 partial point cloud 在 canonical space 中的表示，基于形态变换结果。DR-Pose 在 CAMERA25 和 REAL275 测试准则上produce了与shape prior-based方法相比的更高精度结果。codes 可以在 <https://github.com/Zray26/DR-Pose.git> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Causal-Scoring-Medical-Image-Explanations-A-Case-Study-On-Ex-vivo-Kidney-Stone-Images"><a href="#Causal-Scoring-Medical-Image-Explanations-A-Case-Study-On-Ex-vivo-Kidney-Stone-Images" class="headerlink" title="Causal Scoring Medical Image Explanations: A Case Study On Ex-vivo Kidney Stone Images"></a>Causal Scoring Medical Image Explanations: A Case Study On Ex-vivo Kidney Stone Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01921">http://arxiv.org/abs/2309.01921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Armando Villegas-Jimenez, Daniel Flores-Araiza, Francisco Lopez-Tiro, Gilberto Ochoa-Ruiz andand Christian Daul</li>
<li>for: 本研究旨在提供一种量化测试explainable方法的效果，以便不同水平的用户都能够理解模型输出的 causa causal relationships。</li>
<li>methods: 本研究使用了一种名为Causal Explanation Score（CaES）的技术，该技术可以量化测试explainable方法的效果，并且可以对不同水平的用户进行个性化的评估。</li>
<li>results: 实验结果表明，CaES可以提供更好的量化测试结果，并且可以帮助用户更好地理解模型输出的causal relationships。<details>
<summary>Abstract</summary>
On the promise that if human users know the cause of an output, it would enable them to grasp the process responsible for the output, and hence provide understanding, many explainable methods have been proposed to indicate the cause for the output of a model based on its input. Nonetheless, little has been reported on quantitative measurements of such causal relationships between the inputs, the explanations, and the outputs of a model, leaving the assessment to the user, independent of his level of expertise in the subject. To address this situation, we explore a technique for measuring the causal relationship between the features from the area of the object of interest in the images of a class and the output of a classifier. Our experiments indicate improvement in the causal relationships measured when the area of the object of interest per class is indicated by a mask from an explainable method than when it is indicated by human annotators. Hence the chosen name of Causal Explanation Score (CaES)
</details>
<details>
<summary>摘要</summary>
“由于知道输出的原因可以让人类用户理解模型的处理过程，因此许多可解释方法已经被提出来表示模型的输出和输入之间的原因关系。然而，很少有报告关于量化测量这些 causal 关系的方法， leaving the assessment to the user, regardless of their level of expertise in the subject.为解决这个问题，我们研究了一种测量模型输出和输入之间的 causal 关系的技术。我们的实验表明，当使用可解释方法指定模型输出的区域对象的像素点时， causal 关系的测量得到了改进，而不是由人工标注器指定。因此，我们选择了名为 Causal Explanation Score（CaES）的技术。”
</details></li>
</ul>
<hr>
<h2 id="Improving-Drone-Imagery-For-Computer-Vision-Machine-Learning-in-Wilderness-Search-and-Rescue"><a href="#Improving-Drone-Imagery-For-Computer-Vision-Machine-Learning-in-Wilderness-Search-and-Rescue" class="headerlink" title="Improving Drone Imagery For Computer Vision&#x2F;Machine Learning in Wilderness Search and Rescue"></a>Improving Drone Imagery For Computer Vision&#x2F;Machine Learning in Wilderness Search and Rescue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01904">http://arxiv.org/abs/2309.01904</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/crasar/wisar">https://github.com/crasar/wisar</a></li>
<li>paper_authors: Robin Murphy, Thomas Manzini</li>
<li>for: 本研究旨在掌握无人机成像数据的搜寻问题，以便在计算机视觉&#x2F;机器学习（CV&#x2F;ML）模型的后处理中使用。</li>
<li>methods: 本研究提出五项建议，以提高无人机成像数据的适用性。这些建议包括在搜寻阶段使用自动化数据收集软件，以及根据计算机视觉&#x2F;机器学习模型的特点进行飞行优化。</li>
<li>results: 本研究通过使用2023年日本 Wu-Murad搜寻案例进行实证研究，发现大量的搜寻数据可以通过计算机视觉&#x2F;机器学习技术进行自动化分析，从而提高搜寻效率。<details>
<summary>Abstract</summary>
This paper describes gaps in acquisition of drone imagery that impair the use with computer vision/machine learning (CV/ML) models and makes five recommendations to maximize image suitability for CV/ML post-processing. It describes a notional work process for the use of drones in wilderness search and rescue incidents. The large volume of data from the wide area search phase offers the greatest opportunity for CV/ML techniques because of the large number of images that would otherwise have to be manually inspected. The 2023 Wu-Murad search in Japan, one of the largest missing person searches conducted in that area, serves as a case study. Although drone teams conducting wide area searches may not know in advance if the data they collect is going to be used for CV/ML post-processing, there are data collection procedures that can improve the search in general with automated collection software. If the drone teams do expect to use CV/ML, then they can exploit knowledge about the model to further optimize flights.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇文章描述了无人机图像获取的缺陷，并提出五项建议来最大化图像的适用性 для计算机视觉/机器学习（CV/ML）模型的后处理。文章还描述了一种假设的无人机在远程搜索和救援任务中的应用。在广泛搜索阶段中，由于图像的大量数据，CV/ML技术可以更加有效，因为可以避免大量的手动检查。2023年在日本的武村搜索是一个最大的失踪人搜索案例，作为案例研究。虽然无人机队在广搜案件中可能不知道他们的数据将被用于CV/ML后处理，但是他们可以通过自动收集软件来改进搜索。如果无人机队知道他们的数据将被用于CV/ML，那么他们可以根据模型的知识来进一步优化飞行。
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Plant-Disease-Diagnosis-with-Hard-sample-Re-mining-Strategy"><a href="#Towards-Robust-Plant-Disease-Diagnosis-with-Hard-sample-Re-mining-Strategy" class="headerlink" title="Towards Robust Plant Disease Diagnosis with Hard-sample Re-mining Strategy"></a>Towards Robust Plant Disease Diagnosis with Hard-sample Re-mining Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01903">http://arxiv.org/abs/2309.01903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Huu Cap, Atsushi Fukuda, Satoshi Kagiwada, Hiroyuki Uga, Nobusuke Iwasaki, Hitoshi Iyatomi</li>
<li>For: 提高自动植物疾病诊断系统的准确率和效率，特别是处理大量未标注的健康数据。* Methods: 提出了一种简单 yet effective 的训练策略 called hard-sample re-mining (HSReM)，通过筛选适度难度的训练图像来提高健康数据的诊断性能并同时提高疾病数据的诊断性能。* Results: 对于实际的Field eight-class cucumber和ten-class tomato数据集（42.7K和35.6K张图像）进行了实验，结果表明，使用 HSReM 训练策略可以提高大规模未seen数据的总诊断性能，并且比原始 объек detection 模型和分类基于 EfficientNetV2-Large 模型更高。<details>
<summary>Abstract</summary>
With rich annotation information, object detection-based automated plant disease diagnosis systems (e.g., YOLO-based systems) often provide advantages over classification-based systems (e.g., EfficientNet-based), such as the ability to detect disease locations and superior classification performance. One drawback of these detection systems is dealing with unannotated healthy data with no real symptoms present. In practice, healthy plant data appear to be very similar to many disease data. Thus, those models often produce mis-detected boxes on healthy images. In addition, labeling new data for detection models is typically time-consuming. Hard-sample mining (HSM) is a common technique for re-training a model by using the mis-detected boxes as new training samples. However, blindly selecting an arbitrary amount of hard-sample for re-training will result in the degradation of diagnostic performance for other diseases due to the high similarity between disease and healthy data. In this paper, we propose a simple but effective training strategy called hard-sample re-mining (HSReM), which is designed to enhance the diagnostic performance of healthy data and simultaneously improve the performance of disease data by strategically selecting hard-sample training images at an appropriate level. Experiments based on two practical in-field eight-class cucumber and ten-class tomato datasets (42.7K and 35.6K images) show that our HSReM training strategy leads to a substantial improvement in the overall diagnostic performance on large-scale unseen data. Specifically, the object detection model trained using the HSReM strategy not only achieved superior results as compared to the classification-based state-of-the-art EfficientNetV2-Large model and the original object detection model, but also outperformed the model using the HSM strategy.
</details>
<details>
<summary>摘要</summary>
With rich annotation information, 对 automatized plant disease diagnosis system (e.g., YOLO-based system) 来说，具有病变部署的方式比 классификаation-based system (e.g., EfficientNet-based) 有所优势，如病变部署和高精度的分类性能。然而，这些检测系统面临着处理无标注的健康数据的挑战，因为健康植物数据和疾病数据很相似。因此，这些模型通常会在健康图像上产生错误的框架。此外，为检测模型增加新数据标注是时间消耗的。为了解决这个问题，我们提出了一种简单 yet effective 的训练策略，即 hard-sample re-mining (HSReM)，它可以提高健康数据的诊断性能，同时也可以提高疾病数据的诊断性能。我们在两个实际的大规模验证 dataset (42.7K和35.6K图像) 上进行了实验，结果表明，使用 HSReM 训练策略可以在大规模未看到的数据上提高总诊断性能。具体来说，使用 HSReM 训练模型不仅超过了基于类型的 state-of-the-art EfficientNetV2-Large 模型和原始检测模型，还超过了使用 HSM 策略的模型。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Skin-Lesion-Segmentation-via-Structural-Entropy-Minimization-on-Multi-Scale-Superpixel-Graphs"><a href="#Unsupervised-Skin-Lesion-Segmentation-via-Structural-Entropy-Minimization-on-Multi-Scale-Superpixel-Graphs" class="headerlink" title="Unsupervised Skin Lesion Segmentation via Structural Entropy Minimization on Multi-Scale Superpixel Graphs"></a>Unsupervised Skin Lesion Segmentation via Structural Entropy Minimization on Multi-Scale Superpixel Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01899">http://arxiv.org/abs/2309.01899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/selgroup/sled">https://github.com/selgroup/sled</a></li>
<li>paper_authors: Guangjie Zeng, Hao Peng, Angsheng Li, Zhiwei Liu, Chunyang Liu, Philip S. Yu, Lifang He</li>
<li>For: 这个论文的目的是提出一种新的无监督性肤肉病变分割方法，以解决现有深度学习方法缺乏可解释性的问题。* Methods: 这种方法基于结构 entropy和孤岛森林异常检测，使用了superpixel图构建自dermoscopic图像，然后使用多尺度异常检测来提高分割精度。* Results: 在四个肤肉病变benchmark上进行了实验，并与九种代表性的无监督分割方法进行比较。实验结果表明提案的方法具有优越性，同时还进行了一些 caso study来证明方法的有效性。<details>
<summary>Abstract</summary>
Skin lesion segmentation is a fundamental task in dermoscopic image analysis. The complex features of pixels in the lesion region impede the lesion segmentation accuracy, and existing deep learning-based methods often lack interpretability to this problem. In this work, we propose a novel unsupervised Skin Lesion sEgmentation framework based on structural entropy and isolation forest outlier Detection, namely SLED. Specifically, skin lesions are segmented by minimizing the structural entropy of a superpixel graph constructed from the dermoscopic image. Then, we characterize the consistency of healthy skin features and devise a novel multi-scale segmentation mechanism by outlier detection, which enhances the segmentation accuracy by leveraging the superpixel features from multiple scales. We conduct experiments on four skin lesion benchmarks and compare SLED with nine representative unsupervised segmentation methods. Experimental results demonstrate the superiority of the proposed framework. Additionally, some case studies are analyzed to demonstrate the effectiveness of SLED.
</details>
<details>
<summary>摘要</summary>
皮肤病变分割是肤肤影像分析的基本任务。病变区域像素特征的复杂性使得病变分割精度受到限制，现有的深度学习基于方法frequently lack interpretability to this problem. In this work, we propose a novel unsupervised Skin Lesion Segmentation framework based on structural entropy and isolation forest outlier Detection, namely SLED. Specifically, skin lesions are segmented by minimizing the structural entropy of a superpixel graph constructed from the dermoscopic image. Then, we characterize the consistency of healthy skin features and devise a novel multi-scale segmentation mechanism by outlier detection, which enhances the segmentation accuracy by leveraging the superpixel features from multiple scales. We conduct experiments on four skin lesion benchmarks and compare SLED with nine representative unsupervised segmentation methods. Experimental results demonstrate the superiority of the proposed framework. Additionally, some case studies are analyzed to demonstrate the effectiveness of SLED.Here's the translation in Traditional Chinese:皮肤病变分割是肤肤影像分析的基本任务。病变区域像素特征的复杂性使得病变分割精度受到限制，现有的深度学习基于方法frequently lack interpretability to this problem. In this work, we propose a novel unsupervised Skin Lesion Segmentation framework based on structural entropy and isolation forest outlier Detection, namely SLED. Specifically, skin lesions are segmented by minimizing the structural entropy of a superpixel graph constructed from the dermoscopic image. Then, we characterize the consistency of healthy skin features and devise a novel multi-scale segmentation mechanism by outlier detection, which enhances the segmentation accuracy by leveraging the superpixel features from multiple scales. We conduct experiments on four skin lesion benchmarks and compare SLED with nine representative unsupervised segmentation methods. Experimental results demonstrate the superiority of the proposed framework. Additionally, some case studies are analyzed to demonstrate the effectiveness of SLED.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Domain-Diffusion-Models-for-Image-Synthesis"><a href="#Gradient-Domain-Diffusion-Models-for-Image-Synthesis" class="headerlink" title="Gradient Domain Diffusion Models for Image Synthesis"></a>Gradient Domain Diffusion Models for Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01875">http://arxiv.org/abs/2309.01875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanhao Gong</li>
<li>for: 本研究旨在提高扩散模型在生成图像和视频 sintesis 中的效率，通过在梯度领域进行扩散过程。</li>
<li>methods: 本研究提议在梯度领域进行扩散过程，利用梯度领域的特点，即梯度领域与原始图像领域之间的数学等价性，以及梯度领域的稀疏性，使扩散过程更快 converges。</li>
<li>results: 数值实验表明，梯度领域扩散模型比原始扩散模型更高效。这种方法可以在图像处理、计算机视觉和机器学习任务中应用。<details>
<summary>Abstract</summary>
Diffusion models are getting popular in generative image and video synthesis. However, due to the diffusion process, they require a large number of steps to converge. To tackle this issue, in this paper, we propose to perform the diffusion process in the gradient domain, where the convergence becomes faster. There are two reasons. First, thanks to the Poisson equation, the gradient domain is mathematically equivalent to the original image domain. Therefore, each diffusion step in the image domain has a unique corresponding gradient domain representation. Second, the gradient domain is much sparser than the image domain. As a result, gradient domain diffusion models converge faster. Several numerical experiments confirm that the gradient domain diffusion models are more efficient than the original diffusion models. The proposed method can be applied in a wide range of applications such as image processing, computer vision and machine learning tasks.
</details>
<details>
<summary>摘要</summary>
Diffusion模型在生成图像和视频 синтеisis中越来越受欢迎。然而，由于扩散过程，它们需要大量的步骤以至于相对较慢。在这篇论文中，我们提议在梯度领域进行扩散过程，这会使扩散过程更快。我们有两点原因：首先，由于波尔兹方程，梯度领域与原始图像领域之间存在数学等价关系，因此每个扩散步骤在图像领域有唯一的对应梯度领域表示。其次，梯度领域比图像领域更加稀疏，因此梯度领域的扩散模型更快 converges。我们通过多个数学实验证明，梯度领域扩散模型比原始扩散模型更高效。该方法可以应用于广泛的图像处理、计算机视觉和机器学习任务。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.CV_2023_09_05/" data-id="clp9qz83w00jbok885lvm3qhe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.AI_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T12:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.AI_2023_09_05/">cs.AI - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds"><a href="#Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds" class="headerlink" title="Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds"></a>Utilizing Generative Adversarial Networks for Stable Structure Generation in Angry Birds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02614">http://arxiv.org/abs/2309.02614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Blaxzter/Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds">https://github.com/Blaxzter/Utilizing-Generative-Adversarial-Networks-for-Stable-Structure-Generation-in-Angry-Birds</a></li>
<li>paper_authors: Frederic Abraham, Matthew Stephenson</li>
<li>for:  investigate the suitability of using Generative Adversarial Networks (GANs) to generate stable structures for the physics-based puzzle game Angry Birds.</li>
<li>methods:  using a detailed encoding&#x2F;decoding process to convert between Angry Birds level descriptions and a suitable grid-based representation, and utilizing state-of-the-art GAN architectures and training methods to produce new structure designs.</li>
<li>results:  GANs can be successfully applied to generate a varied range of complex and stable Angry Birds structures.<details>
<summary>Abstract</summary>
This paper investigates the suitability of using Generative Adversarial Networks (GANs) to generate stable structures for the physics-based puzzle game Angry Birds. While previous applications of GANs for level generation have been mostly limited to tile-based representations, this paper explores their suitability for creating stable structures made from multiple smaller blocks. This includes a detailed encoding/decoding process for converting between Angry Birds level descriptions and a suitable grid-based representation, as well as utilizing state-of-the-art GAN architectures and training methods to produce new structure designs. Our results show that GANs can be successfully applied to generate a varied range of complex and stable Angry Birds structures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Detection-of-Unknown-Unknowns-in-Cyber-Physical-Systems-using-Statistical-Conformance-with-Physics-Guided-Process-Models"><a href="#Detection-of-Unknown-Unknowns-in-Cyber-Physical-Systems-using-Statistical-Conformance-with-Physics-Guided-Process-Models" class="headerlink" title="Detection of Unknown-Unknowns in Cyber-Physical Systems using Statistical Conformance with Physics Guided Process Models"></a>Detection of Unknown-Unknowns in Cyber-Physical Systems using Statistical Conformance with Physics Guided Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02603">http://arxiv.org/abs/2309.02603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aranyak Maity, Ayan Banerjee, Sandeep Gupta</li>
<li>for: 这 paper 是关于 cyber-physical system  unknown unknown 情况下的分析和评估的研究。</li>
<li>methods: 该 paper 使用 dynamics-induced hybrid recurrent neural networks (DiH-RNN) 和 physics-guided surrogate model (PGSM) 来检测 operational output 特性的模型兼容性。</li>
<li>results: 该 paper 通过使用 DiH-RNN 和 PGSM 检测 operational output 特性的模型兼容性，可以检测到 unknown insulin cartridge errors 的影响。<details>
<summary>Abstract</summary>
Unknown unknowns are operational scenarios in a cyber-physical system that are not accounted for in the design and test phase. As such under unknown-unknown scenarios, the operational behavior of the CPS is not guaranteed to meet requirements such as safety and efficacy specified using Signal Temporal Logic (STL) on the output trajectories. We propose a novel framework for analyzing the stochastic conformance of operational output characteristics of safety-critical cyber-physical systems that can discover unknown-unknown scenarios and evaluate potential safety hazards. We propose dynamics-induced hybrid recurrent neural networks (DiH-RNN) to mine a physics-guided surrogate model (PGSM) which is used to check the model conformance using STL on the model coefficients. We demonstrate the detection of operational changes in an Artificial Pancreas(AP) due to unknown insulin cartridge errors.
</details>
<details>
<summary>摘要</summary>
未知未知情况是遗传逻辑系统中运行阶段没有考虑的情况。因此，在未知未知情况下，遗传逻辑系统的运行行为不能保证符合要求，如安全性和有效性，使用信号时间逻辑（STL）表示输出特性的轨迹。我们提出了一种新的框架，用于分析安全关键遗传逻辑系统的输出特性的杂乱兼容性，可以发现未知未知情况并评估可能的安全隐患。我们提出了动力学引导的混合回归神经网络（DiH-RNN），用于挖掘基于物理学的模型（PGSM），并使用STL来检查模型的一致性。我们示例了适用于人工肾脏（AP）的操作变化检测，即因未知的胰岛素卡通错误而导致的操作变化。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Evaluation-of-Metaheuristic-Algorithms-for-Hyperparameter-Selection-in-Short-Term-Weather-Forecasting"><a href="#Comparative-Evaluation-of-Metaheuristic-Algorithms-for-Hyperparameter-Selection-in-Short-Term-Weather-Forecasting" class="headerlink" title="Comparative Evaluation of Metaheuristic Algorithms for Hyperparameter Selection in Short-Term Weather Forecasting"></a>Comparative Evaluation of Metaheuristic Algorithms for Hyperparameter Selection in Short-Term Weather Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02600">http://arxiv.org/abs/2309.02600</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anuvab Sen, Arul Rhik Mazumder, Dibyarup Dutta, Udayon Sen, Pathikrit Syam, Sandipan Dhar</li>
<li>for: 这篇论文主要针对于准确预测天气系统的复杂动态，以便于传统统计模型不能准确地捕捉天气系统的复杂性。</li>
<li>methods: 本论文使用了深度学习技术（包括普通的ANN、LSTM和GRU网络），以及元heidursive算法（GA、DE、PSO）来自动搜索优化参数。</li>
<li>results: 研究发现，元heidursive算法可以准确地搜索优化参数，从而提高天气预测的准确性。研究还发现，不同的模型结构和元heidursive算法之间存在着积极的交互关系。<details>
<summary>Abstract</summary>
Weather forecasting plays a vital role in numerous sectors, but accurately capturing the complex dynamics of weather systems remains a challenge for traditional statistical models. Apart from Auto Regressive time forecasting models like ARIMA, deep learning techniques (Vanilla ANNs, LSTM and GRU networks), have shown promise in improving forecasting accuracy by capturing temporal dependencies. This paper explores the application of metaheuristic algorithms, namely Genetic Algorithm (GA), Differential Evolution (DE), and Particle Swarm Optimization (PSO), to automate the search for optimal hyperparameters in these model architectures. Metaheuristic algorithms excel in global optimization, offering robustness, versatility, and scalability in handling non-linear problems. We present a comparative analysis of different model architectures integrated with metaheuristic optimization, evaluating their performance in weather forecasting based on metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE). The results demonstrate the potential of metaheuristic algorithms in enhancing weather forecasting accuracy \& helps in determining the optimal set of hyper-parameters for each model. The paper underscores the importance of harnessing advanced optimization techniques to select the most suitable metaheuristic algorithm for the given weather forecasting task.
</details>
<details>
<summary>摘要</summary>
天气预报中的准确预测是许多领域的关键任务，但是传统的统计模型难以准确地捕捉天气系统的复杂动态。 apart from ARIMA模型，深度学习技术（Vanilla ANNs、LSTM和GRU网络）已经显示出提高预测准确性的承诺， capture temporal dependencies。 this paper explores the application of metaheuristic algorithms， namely Genetic Algorithm (GA)、Differential Evolution (DE) and Particle Swarm Optimization (PSO)， to automate the search for optimal hyperparameters in these model architectures。 metaheuristic algorithms excel in global optimization， offering robustness、versatility and scalability in handling non-linear problems。 we present a comparative analysis of different model architectures integrated with metaheuristic optimization， evaluating their performance in weather forecasting based on metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage Error (MAPE)。 the results demonstrate the potential of metaheuristic algorithms in enhancing weather forecasting accuracy  and helps in determining the optimal set of hyper-parameters for each model。 the paper underscores the importance of harnessing advanced optimization techniques to select the most suitable metaheuristic algorithm for the given weather forecasting task。
</details></li>
</ul>
<hr>
<h2 id="Approximating-High-Dimensional-Minimal-Surfaces-with-Physics-Informed-Neural-Networks"><a href="#Approximating-High-Dimensional-Minimal-Surfaces-with-Physics-Informed-Neural-Networks" class="headerlink" title="Approximating High-Dimensional Minimal Surfaces with Physics-Informed Neural Networks"></a>Approximating High-Dimensional Minimal Surfaces with Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02589">http://arxiv.org/abs/2309.02589</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Zhou, Xiaojing Ye</li>
<li>for: 这 paper 是计算高维 minimal surface 数学近似的，解决了高维埃尔文·芮贝格·约瑟夫问题。</li>
<li>methods: 本 paper 使用 Physics-Informed Neural Network (PINN) 方法，通过训练深度神经网络 (DNN) 来解决 minimal surface 方程。</li>
<li>results: 本 paper 的结果表明，PINN 方法可以在高维空间中计算 minimal surface，并且可以快速地训练和运行在笔记型计算机上，无需高性能计算机。<details>
<summary>Abstract</summary>
In this paper, we compute numerical approximations of the minimal surfaces, an essential type of Partial Differential Equation (PDE), in higher dimensions. Classical methods cannot handle it in this case because of the Curse of Dimensionality, where the computational cost of these methods increases exponentially fast in response to higher problem dimensions, far beyond the computing capacity of any modern supercomputers. Only in the past few years have machine learning researchers been able to mitigate this problem. The solution method chosen here is a model known as a Physics-Informed Neural Network (PINN) which trains a deep neural network (DNN) to solve the minimal surface PDE. It can be scaled up into higher dimensions and trained relatively quickly even on a laptop with no GPU. Due to the inability to view the high-dimension output, our data is presented as snippets of a higher-dimension shape with enough fixed axes so that it is viewable with 3-D graphs. Not only will the functionality of this method be tested, but we will also explore potential limitations in the method's performance.
</details>
<details>
<summary>摘要</summary>
在本文中，我们计算高维精灵散函数（PDE）的数学近似值，这是一种基础方程的重要类型。传统方法在这种情况下无法处理，因为维度味问题的计算成本会快速增长，超过现代超级计算机的处理能力。仅在过去几年，机器学习研究人员才能够解决这个问题。选择的方法是一种称为物理学习神经网络（PINN），它使用深度神经网络（DNN）解决精灵散函数PDE。它可以扩展到更高维度，并在笔记计算机上训练相对快速，即使没有GPU。由于无法查看高维度输出，我们的数据被示为一些固定轴的高维度形状的截面，可以通过3D图表查看。不仅会测试这种方法的功能，我们还将探讨这种方法的可能的局限性。
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-for-Sequential-Volumetric-Design-Tasks"><a href="#Representation-Learning-for-Sequential-Volumetric-Design-Tasks" class="headerlink" title="Representation Learning for Sequential Volumetric Design Tasks"></a>Representation Learning for Sequential Volumetric Design Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02583">http://arxiv.org/abs/2309.02583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Ferdous Alam, Yi Wang, Linh Tran, Chin-Yi Cheng, Jieliang Luo</li>
<li>for: 这个论文的目的是提出一种基于 transformer 模型的整体设计系统，以便自动生成符合设计师的设计解决方案。</li>
<li>methods: 该论文使用了 transformer 模型来编码设计知识，并从一系列专家或高性能的设计序列中提取有用的表示。然后，它使用这些表示来进行设计偏好评估和生成过程设计。</li>
<li>results: 该论文通过使用一个 novel dataset of thousands of sequential volumetric designs 来证明其方法的有效性。其中，设计偏好评估模型可以准确地评估两个任意给定的设计序列之间的差异，并且可以自动完成一个volumetric设计序列从一个半完整的设计序列中。<details>
<summary>Abstract</summary>
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the preference model by estimating the density of the learned representations whereas we train an autoregressive transformer model for sequential design generation. We demonstrate our ideas by leveraging a novel dataset of thousands of sequential volumetric designs. Our preference model can compare two arbitrarily given design sequences and is almost 90% accurate in evaluation against random design sequences. Our autoregressive model is also capable of autocompleting a volumetric design sequence from a partial design sequence.
</details>
<details>
<summary>摘要</summary>
volumetric design，也称为质量设计，是职业建筑设计的首要步骤，具有顺序性。由于volumetric design过程复杂，其下面顺序设计过程含有价值信息。许多努力已经被made to automatically生成合理的涂抹设计，但生成的设计解决方案质量不一，评估设计解决方案需要 Either a comprehensive set of metrics or expensive human expertise。而前一些方法只是学习最终的设计而不是顺序设计任务，我们提议将专家或高性能的设计序列知识编码到模型中，并使用 transformer-based 模型提取有用的表示。后续，我们将学习的表示用于重要的下游应用，如设计偏好评估和 procedural 设计生成。我们开发了偏好模型，通过估计学习的表示密度来评估两个任意给定的设计序列，并训练一个 autoregressive transformer 模型来生成顺序设计序列。我们通过使用一个 novel 的千个顺序涂抹设计数据集来证明我们的想法。我们的偏好模型可以比较两个任意给定的设计序列，准确率接近 90%，而我们的 autoregressive 模型也可以自动完成一个顺序涂抹设计序列从一个部分设计序列。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Intractable-Epileptogenic-Brain-Networks-with-Deep-Learning-Algorithms-A-Novel-and-Comprehensive-Framework-for-Scalable-Seizure-Prediction-with-Unimodal-Neuroimaging-Data-in-Pediatric-Patients"><a href="#Unveiling-Intractable-Epileptogenic-Brain-Networks-with-Deep-Learning-Algorithms-A-Novel-and-Comprehensive-Framework-for-Scalable-Seizure-Prediction-with-Unimodal-Neuroimaging-Data-in-Pediatric-Patients" class="headerlink" title="Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients"></a>Unveiling Intractable Epileptogenic Brain Networks with Deep Learning Algorithms: A Novel and Comprehensive Framework for Scalable Seizure Prediction with Unimodal Neuroimaging Data in Pediatric Patients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02580">http://arxiv.org/abs/2309.02580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bliss Singhal, Fnu Pooja<br>for: 这项研究的目的是预测儿童患有不可逆的癫痫病例中的癫痫发作。methods: 该研究使用机器学习算法对单modal神经成像数据进行评估，包括电энцеfalogram信号。研究使用了频带滤波和独立组分分析来减少数据中的噪声和artefacts。results: 研究发现，深度学习算法在预测癫痫发作方面比逻子回归和k最近邻居更成功。Long short-term memory（LSTM）在精度和F1分数方面表现出色， convolutional neural network（CNN）在特征特征方面表现出色。这项研究有重要的应用前瞻性，可能改变临床实践，并提高儿童护理水平。<details>
<summary>Abstract</summary>
Epilepsy is a prevalent neurological disorder affecting 50 million individuals worldwide and 1.2 million Americans. There exist millions of pediatric patients with intractable epilepsy, a condition in which seizures fail to come under control. The occurrence of seizures can result in physical injury, disorientation, unconsciousness, and additional symptoms that could impede children's ability to participate in everyday tasks. Predicting seizures can help parents and healthcare providers take precautions, prevent risky situations, and mentally prepare children to minimize anxiety and nervousness associated with the uncertainty of a seizure. This research proposes a novel and comprehensive framework to predict seizures in pediatric patients by evaluating machine learning algorithms on unimodal neuroimaging data consisting of electroencephalogram signals. The bandpass filtering and independent component analysis proved to be effective in reducing the noise and artifacts from the dataset. Various machine learning algorithms' performance is evaluated on important metrics such as accuracy, precision, specificity, sensitivity, F1 score and MCC. The results show that the deep learning algorithms are more successful in predicting seizures than logistic Regression, and k nearest neighbors. The recurrent neural network (RNN) gave the highest precision and F1 Score, long short-term memory (LSTM) outperformed RNN in accuracy and convolutional neural network (CNN) resulted in the highest Specificity. This research has significant implications for healthcare providers in proactively managing seizure occurrence in pediatric patients, potentially transforming clinical practices, and improving pediatric care.
</details>
<details>
<summary>摘要</summary>
“凝视症是一种流行的神经系统疾病，全球病人约5000万人，美国病人约120万人。有数百万名儿童患有不治疗的凝视症，其中发作不能控制的症状可能导致物理伤害、混乱、失去意识和其他 симптом，影响儿童日常生活。预测发作可以帮助家长和医疗提供者制定预防措施，避免危险情况，并帮助儿童减少发作症状所带来的焦虑和不安。本研究提出了一个全面的预测发作框架，通过评估机器学习算法在单一神经内成像数据上进行评估。对于重要的 метри几何，例如准确性、特异性、敏感度和合理性，评估了不同的机器学习算法的表现。结果显示，深度学习算法比逻辑回传和k最近邻居更 successful 预测发作。Long short-term memory（LSTM）在准确性和敏感度方面表现出色， convolutional neural network（CNN）在特异性方面表现最好。这些研究结果具有重要的实践意义，可能将影响医疗提供者在管理儿童发作的方法，并改善儿童医疗。”
</details></li>
</ul>
<hr>
<h2 id="Recurrence-Free-Survival-Prediction-for-Anal-Squamous-Cell-Carcinoma-Chemoradiotherapy-using-Planning-CT-based-Radiomics-Model"><a href="#Recurrence-Free-Survival-Prediction-for-Anal-Squamous-Cell-Carcinoma-Chemoradiotherapy-using-Planning-CT-based-Radiomics-Model" class="headerlink" title="Recurrence-Free Survival Prediction for Anal Squamous Cell Carcinoma Chemoradiotherapy using Planning CT-based Radiomics Model"></a>Recurrence-Free Survival Prediction for Anal Squamous Cell Carcinoma Chemoradiotherapy using Planning CT-based Radiomics Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02562">http://arxiv.org/abs/2309.02562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Tang, Kai Wang, David Hein, Gloria Lin, Nina N. Sanford, Jing Wang</li>
<li>for: 这项研究是为了开发一个基于辐射预计划CT图像的模型，以预测非转移性分化细胞癌（ASCC）患者 после化疗（CRT）的再次出现率。</li>
<li>methods: 研究人员使用了 радиом扬特征来预测ASCC患者的再次出现率，并在多ivariate Cox准确比率模型中选择了最佳特征集。</li>
<li>results: 研究发现，基于Shape和Texture的 радиом扬特征可以有效预测ASCC患者的再次出现率，并且combined模型在测试组中表现更好，其C-指数和ROC曲线都高于仅使用临床特征模型。<details>
<summary>Abstract</summary>
Objectives: Approximately 30% of non-metastatic anal squamous cell carcinoma (ASCC) patients will experience recurrence after chemoradiotherapy (CRT), and currently available clinical variables are poor predictors of treatment response. We aimed to develop a model leveraging information extracted from radiation pretreatment planning CT to predict recurrence-free survival (RFS) in ASCC patients after CRT. Methods: Radiomics features were extracted from planning CT images of 96 ASCC patients. Following pre-feature selection, the optimal feature set was selected via step-forward feature selection with a multivariate Cox proportional hazard model. The RFS prediction was generated from a radiomics-clinical combined model based on an optimal feature set with five repeats of five-fold cross validation. The risk stratification ability of the proposed model was evaluated with Kaplan-Meier analysis. Results: Shape- and texture-based radiomics features significantly predicted RFS. Compared to a clinical-only model, radiomics-clinical combined model achieves better performance in the testing cohort with higher C-index (0.80 vs 0.73) and AUC (0.84 vs 0.79 for 1-year RFS, 0.84 vs 0.78 for 2-year RFS, and 0.86 vs 0.83 for 3-year RFS), leading to distinctive high- and low-risk of recurrence groups (p<0.001). Conclusions: A treatment planning CT based radiomics and clinical combined model had improved prognostic performance in predicting RFS for ASCC patients treated with CRT as compared to a model using clinical features only.
</details>
<details>
<summary>摘要</summary>
目的：约30%的非转移性 anal squamous cell carcinoma（ASCC）患者会经受化疗后再次出现，现有的临床变量不能准确预测治疗效果。我们目标是利用从化疗前规划CT图像中提取的信息，预测ASCC患者在化疗后的再次出现率（RFS）。方法：从96名ASCC患者的规划CT图像中提取了 радиом学特征。经过预选feature，选择了最佳特征集。然后，通过五次十分割分 Validation进行验证。Result：Shape和 texture基的 радиом学特征能够有效预测RFS。与仅使用临床特征模型相比，radiomics-临床共同模型在测试组中表现更好，其C-指数（0.80 vs 0.73）和ROC（0.84 vs 0.79 for 1-year RFS, 0.84 vs 0.78 for 2-year RFS, and 0.86 vs 0.83 for 3-year RFS）都高于临床特征模型，从而导致了高和低风险组分化（p<0.001）。结论：基于规划CT图像的 радиомics和临床共同模型在预测ASCC患者化疗后RFS方面表现出了改善的预测能力，与仅使用临床特征模型相比。
</details></li>
</ul>
<hr>
<h2 id="Physically-Grounded-Vision-Language-Models-for-Robotic-Manipulation"><a href="#Physically-Grounded-Vision-Language-Models-for-Robotic-Manipulation" class="headerlink" title="Physically Grounded Vision-Language Models for Robotic Manipulation"></a>Physically Grounded Vision-Language Models for Robotic Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02561">http://arxiv.org/abs/2309.02561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Stanford-ILIAD/pg-vlm">https://github.com/Stanford-ILIAD/pg-vlm</a></li>
<li>paper_authors: Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, Dorsa Sadigh</li>
<li>for: 提高视觉问答和图像描述任务的性能，使模型更能理解物理世界。</li>
<li>methods: 使用普通人的协助和自动化的物理概念标注数据集PhysObjects进行训练，以捕捉人类对物理对象的Visual priors。</li>
<li>results: 在含有物理概念的任务中，使用物理grounded VLM进行规划，可以提高任务成功率，并在实际机器人中进行实践。<details>
<summary>Abstract</summary>
Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically-grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically-grounded VLMs. We additionally illustrate the benefits of our physically-grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/.
</details>
<details>
<summary>摘要</summary>
近期的视力语言模型（VLM）的进步已导致视觉问答和图像描述等任务的表现得到改善。因此，这些模型现在更容易进行物理世界中的理解，特别是在机器人操作领域。然而，当前的VLM仍有限制，它们对常见物品的物理概念（例如材料和脆弱性）的理解不够，这限制了它们在机器人操作任务中的用途。为解决这个问题，我们提出了PhysObjects，一个包含39600个人工标注和417000个自动标注的常见家用物品的物理概念数据集。我们示示了 fine-tuning VLM 在 PhysObjects 上可以提高它对物理 объек 概念的理解，包括泛化到未经过 обучение的概念。我们将这种物理基础 VLM 与大语言模型基础的机器人规划器集成，并表明了不使用物理基础 VLM 的基eline 的规划性能相对较差。我们还在真实的机器人上运行了这种物理基础 VLM，并证明了它可以提高任务成功率。我们在 https://iliad.stanford.edu/pg-vlm/ 发布了我们的数据集，并在结果中提供了更多的详细信息和视觉化。
</details></li>
</ul>
<hr>
<h2 id="Automating-Behavioral-Testing-in-Machine-Translation"><a href="#Automating-Behavioral-Testing-in-Machine-Translation" class="headerlink" title="Automating Behavioral Testing in Machine Translation"></a>Automating Behavioral Testing in Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02553">http://arxiv.org/abs/2309.02553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Javier Ferrando, Matthias Sperber, Hendra Setiawan, Dominic Telaar, Saša Hasan</li>
<li>for: 评估机器翻译系统的语言能力，包括语料生成和输入输出行为的分析。</li>
<li>methods: 使用大语言模型生成多样化的源句子，以测试机器翻译系统在不同情况下的行为。</li>
<li>results: 通过对多个可用的机器翻译系统进行评估，发现 passer  rates 随着传统精度指标的趋势相似，但方法找到了一些重要的差异和潜在的错误，这些错误在仅仅依据精度时未被发现。<details>
<summary>Abstract</summary>
Behavioral testing in NLP allows fine-grained evaluation of systems by examining their linguistic capabilities through the analysis of input-output behavior. Unfortunately, existing work on behavioral testing in Machine Translation (MT) is currently restricted to largely handcrafted tests covering a limited range of capabilities and languages. To address this limitation, we propose to use Large Language Models (LLMs) to generate a diverse set of source sentences tailored to test the behavior of MT models in a range of situations. We can then verify whether the MT model exhibits the expected behavior through matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metrics, our method was able to uncover several important differences and potential bugs that go unnoticed when relying only on accuracy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT行为测试在自然语言处理（NLP）中允许细化评估系统的语言能力通过输入输出行为的分析。 Unfortunately，现有的机器翻译（MT）行为测试工作现在受到了较少的手动测试覆盖的限制，只有一些特定的语言和能力。 To address this limitation， we propose using Large Language Models（LLMs）来生成一组多样化的源句子，用于测试MT模型在多种情况下的行为。 We can then verify whether the MT model exhibits the expected behavior by matching candidate sets that are also generated using LLMs. Our approach aims to make behavioral testing of MT systems practical while requiring only minimal human effort. In our experiments, we apply our proposed evaluation framework to assess multiple available MT systems, revealing that while in general pass-rates follow the trends observable from traditional accuracy-based metrics, our method was able to uncover several important differences and potential bugs that go unnoticed when relying only on accuracy.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="Continual-Improvement-of-Threshold-Based-Novelty-Detection"><a href="#Continual-Improvement-of-Threshold-Based-Novelty-Detection" class="headerlink" title="Continual Improvement of Threshold-Based Novelty Detection"></a>Continual Improvement of Threshold-Based Novelty Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02551">http://arxiv.org/abs/2309.02551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abe Ejilemele, Jorge Mendez-Mendez</li>
<li>for: 解决 neural network 在动态开放世界中探测未经见过的类型时存在问题，使得不可预知 novelty 探测方法无法适应实际环境中的数据特性。</li>
<li>methods: 我们提出了一种新的方法，利用直线搜索和留一个样本验证来自动选择阈值，以提高总准确率在 MNIST、Fashion MNIST 和 CIFAR-10 上。</li>
<li>results: 我们的方法在三个 dataset 上都达到了提高的总准确率，表明自动选择阈值可以更好地适应不同的数据特性。<details>
<summary>Abstract</summary>
When evaluated in dynamic, open-world situations, neural networks struggle to detect unseen classes. This issue complicates the deployment of continual learners in realistic environments where agents are not explicitly informed when novel categories are encountered. A common family of techniques for detecting novelty relies on thresholds of similarity between observed data points and the data used for training. However, these methods often require manually specifying (ahead of time) the value of these thresholds, and are therefore incapable of adapting to the nature of the data. We propose a new method for automatically selecting these thresholds utilizing a linear search and leave-one-out cross-validation on the ID classes. We demonstrate that this novel method for selecting thresholds results in improved total accuracy on MNIST, Fashion MNIST, and CIFAR-10.
</details>
<details>
<summary>摘要</summary>
在动态开放环境中评估神经网络时，它们很难探测未看过的类别。这个问题使得在实际环境中部署不断学习者变得更加复杂。一种常见的新类探测技术是基于训练数据点和观察数据点之间的相似度阈值。然而，这些方法通常需要手动指定阈值的值（在过程中），因此无法适应数据的特点。我们提出了一种新的方法，使用线性搜索和留下一个样本进行交叉验证，以自动选择阈值。我们示出，这种新的阈值选择方法可以提高MNIST、Fashion MNIST和CIFAR-10等三个 dataset 的总准确率。
</details></li>
</ul>
<hr>
<h2 id="Structural-Concept-Learning-via-Graph-Attention-for-Multi-Level-Rearrangement-Planning"><a href="#Structural-Concept-Learning-via-Graph-Attention-for-Multi-Level-Rearrangement-Planning" class="headerlink" title="Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning"></a>Structural Concept Learning via Graph Attention for Multi-Level Rearrangement Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02547">http://arxiv.org/abs/2309.02547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manav Kulshrestha, Ahmed H. Qureshi</li>
<li>for: 该论文关注了机器人 manipulate 任务，如物体重新排序，以便 robot 与复杂而不受限制的环境进行交互。</li>
<li>methods: 该论文提出了一种深度学习方法，名为结构概念学习（SCL），它利用图注意网络来实现多层物体重新排序规划。SCL 可以处理具有结构依赖层次的Scene，并可以在未经见过的场景中进行任务并行化和灵活化。</li>
<li>results: 作者通过对一系列经典和模型基础的基准方法进行比较，证明了 SCL 能够更好地利用场景理解来实现更高的性能、灵活性和效率。<details>
<summary>Abstract</summary>
Robotic manipulation tasks, such as object rearrangement, play a crucial role in enabling robots to interact with complex and arbitrary environments. Existing work focuses primarily on single-level rearrangement planning and, even if multiple levels exist, dependency relations among substructures are geometrically simpler, like tower stacking. We propose Structural Concept Learning (SCL), a deep learning approach that leverages graph attention networks to perform multi-level object rearrangement planning for scenes with structural dependency hierarchies. It is trained on a self-generated simulation data set with intuitive structures, works for unseen scenes with an arbitrary number of objects and higher complexity of structures, infers independent substructures to allow for task parallelization over multiple manipulators, and generalizes to the real world. We compare our method with a range of classical and model-based baselines to show that our method leverages its scene understanding to achieve better performance, flexibility, and efficiency. The dataset, supplementary details, videos, and code implementation are available at: https://manavkulshrestha.github.io/scl
</details>
<details>
<summary>摘要</summary>
瑜珈机器人操作任务，如物品重新排序，对机器人在复杂且随机环境中进行交互起到关键作用。现有研究主要集中在单个层次重新排序规划上，即使有多个层次存在，dependency关系 among substructures几乎都是 геометрически简单的，如筒堆。我们提出了Structural Concept Learning（SCL），一种深度学习方法，通过图像注意力网络来实现多层次物品重新排序规划。它在自己生成的 simulatedata set上接受intuitive结构，可以处理未看过的场景，无论有多少对象和更高的结构复杂度，推导独立的substructure，以便在多个机器人上分布task，并且可以在实际世界中普适。我们与一系列的古典和基于模型的基准进行比较，显示我们的方法借助于场景理解来实现更好的性能、灵活性和效率。数据集、补充细节、视频和代码实现可以在：https://manavkulshrestha.github.io/scl  obtener
</details></li>
</ul>
<hr>
<h2 id="Experience-and-Prediction-A-Metric-of-Hardness-for-a-Novel-Litmus-Test"><a href="#Experience-and-Prediction-A-Metric-of-Hardness-for-a-Novel-Litmus-Test" class="headerlink" title="Experience and Prediction: A Metric of Hardness for a Novel Litmus Test"></a>Experience and Prediction: A Metric of Hardness for a Novel Litmus Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02534">http://arxiv.org/abs/2309.02534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicos Isaak, Loizos Michael</li>
<li>for: 本研究旨在开发一种基于机器学习（ML）的系统，用于评估winograd schema的困难程度，并且比之前的方法更快速和准确。</li>
<li>methods: 本研究采用了两种不同的方法，namely random forest和深度学习（LSTM-based），以评估winograd schema的困难程度。</li>
<li>results: 研究发现，人类对winograd schema的表现异常 vary，并且与schema的困难程度有直接关系。此外，我们还发现了一些特定的winograd schema可以用于测试人类的智能水平。<details>
<summary>Abstract</summary>
In the last decade, the Winograd Schema Challenge (WSC) has become a central aspect of the research community as a novel litmus test. Consequently, the WSC has spurred research interest because it can be seen as the means to understand human behavior. In this regard, the development of new techniques has made possible the usage of Winograd schemas in various fields, such as the design of novel forms of CAPTCHAs.   Work from the literature that established a baseline for human adult performance on the WSC has shown that not all schemas are the same, meaning that they could potentially be categorized according to their perceived hardness for humans. In this regard, this \textit{hardness-metric} could be used in future challenges or in the WSC CAPTCHA service to differentiate between Winograd schemas.   Recent work of ours has shown that this could be achieved via the design of an automated system that is able to output the hardness-indexes of Winograd schemas, albeit with limitations regarding the number of schemas it could be applied on. This paper adds to previous research by presenting a new system that is based on Machine Learning (ML), able to output the hardness of any Winograd schema faster and more accurately than any other previously used method. Our developed system, which works within two different approaches, namely the random forest and deep learning (LSTM-based), is ready to be used as an extension of any other system that aims to differentiate between Winograd schemas, according to their perceived hardness for humans. At the same time, along with our developed system we extend previous work by presenting the results of a large-scale experiment that shows how human performance varies across Winograd schemas.
</details>
<details>
<summary>摘要</summary>
过去一个 décennio，Winograd Schema Challenge（WSC）已成为研究社区中的中心方面，作为一种新的考验。因此，WSC 引发了研究者的兴趣，因为它可以用来理解人类行为。在这个意义上，开发新技术使得 Winograd  schema 可以在不同领域中使用，如设计新型 CAPTCHAs。  据文献记录，人类成人在 WSC 中的表现达标准，表明不同的 Winograd schema 可能有不同的抵抗程度。在这个意义上，这个“抵抗度指标”可以在未来的挑战中或 WSC CAPTCHA 服务中使用来区分 Winograd schema。  我们最近的工作表明，这可以通过设计一个自动化系统来实现，该系统可以输出 Winograd schema 的抵抗度指标，但是只能应用于有限数量的 schema。本文添加了先前的研究，提出了一个基于机器学习（ML）的新系统，可以更快、更准确地输出 Winograd schema 的抵抗度指标。我们开发的系统采用了两种不同的方法，即随机森林和深度学习（LSTM）。这个系统可以作为任何其他系统的扩展，以区分 Winograd schema 根据人类对它们的抵抗度。同时，我们也扩展了先前的工作，通过发表大规模实验，显示了人类在不同 Winograd schema 中的表现差异。
</details></li>
</ul>
<hr>
<h2 id="Do-You-Trust-ChatGPT-–-Perceived-Credibility-of-Human-and-AI-Generated-Content"><a href="#Do-You-Trust-ChatGPT-–-Perceived-Credibility-of-Human-and-AI-Generated-Content" class="headerlink" title="Do You Trust ChatGPT? – Perceived Credibility of Human and AI-Generated Content"></a>Do You Trust ChatGPT? – Perceived Credibility of Human and AI-Generated Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02524">http://arxiv.org/abs/2309.02524</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Huschens, Martin Briesch, Dominik Sobania, Franz Rothlauf</li>
<li>for: 这个研究探讨用户对人类作者vs大语言模型生成内容的信任程度如何不同的用户界面版本。</li>
<li>methods: 研究使用了具有不同用户界面版本的人类作者和大语言模型生成的内容，评估用户对这两种内容的信任程度和技能感。</li>
<li>results: 结果显示，尽管用户界面版本不同，但参与者对人类作者和大语言模型生成的内容的信任程度几乎相同。同时，参与者认为AI生成的内容 clearer和更有吸引力。这些发现告诉我们需要更加谨慎地评估信息来源，促使用者予以慎重和批判性思维。<details>
<summary>Abstract</summary>
This paper examines how individuals perceive the credibility of content originating from human authors versus content generated by large language models, like the GPT language model family that powers ChatGPT, in different user interface versions. Surprisingly, our results demonstrate that regardless of the user interface presentation, participants tend to attribute similar levels of credibility. While participants also do not report any different perceptions of competence and trustworthiness between human and AI-generated content, they rate AI-generated content as being clearer and more engaging. The findings from this study serve as a call for a more discerning approach to evaluating information sources, encouraging users to exercise caution and critical thinking when engaging with content generated by AI systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Efficient-RL-via-Disentangled-Environment-and-Agent-Representations"><a href="#Efficient-RL-via-Disentangled-Environment-and-Agent-Representations" class="headerlink" title="Efficient RL via Disentangled Environment and Agent Representations"></a>Efficient RL via Disentangled Environment and Agent Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02435">http://arxiv.org/abs/2309.02435</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Gmelin, Shikhar Bahl, Russell Mendonca, Deepak Pathak</li>
<li>for: 提高RL算法的视觉理解和表示能力</li>
<li>methods: 使用自己的视觉知识（如形状或面具）来学习结构化表示，并将其integrated into RL目标函数中</li>
<li>results: 在18个不同的视觉 simulations环境中，对5种不同的机器人进行了比较，并得到了模型自由方法的性能提升<details>
<summary>Abstract</summary>
Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, Structured Environment-Agent Representations, outperforms state-of-the-art model-free approaches over 18 different challenging visual simulation environments spanning 5 different robots. Website at https://sear-rl.github.io/
</details>
<details>
<summary>摘要</summary>
Agent 可以感知自己和环境之间的分离，可以利用这种理解来形成有效的视觉输入表示。我们提出一种使用视觉知识，如机器人的形状或面具，可以轻松获得的方法来学习这些结构化表示。这种方法被 integrate 到RL目标中使用简单的辅助损失。我们显示，我们的方法 Structured Environment-Agent Representations 在 18 个不同的复杂视觉 simulate 环境中，使用 5 种不同的机器人，超过了现状的模型自由方法。网站地址为 <https://sear-rl.github.io/>。Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Building-a-Winning-Team-Selecting-Source-Model-Ensembles-using-a-Submodular-Transferability-Estimation-Approach"><a href="#Building-a-Winning-Team-Selecting-Source-Model-Ensembles-using-a-Submodular-Transferability-Estimation-Approach" class="headerlink" title="Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach"></a>Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02429">http://arxiv.org/abs/2309.02429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vimal K B, Saketh Bachu, Tanmay Garg, Niveditha Lakshmi Narasimhan, Raghavan Konuru, Vineeth N Balasubramanian</li>
<li>for: 这篇论文主要应用于估计公开可用的预训模型在目标任务上的转移可能性。</li>
<li>methods: 本论文使用了一个新的Optimal tranSport-based suBmOdular tRaNsferability metric（OSBORN）来估计多来源模型的转移可能性。OSBORN考虑了图像领域差异、任务差异和模型集合中的凝聚性，以提供可靠的转移可能性估计。</li>
<li>results: 本论文通过对28个源数据集、11个目标数据集、5种模型架构和2种预训方法进行 benchmarking，发现OSBORN可以与现有的state-of-the-art度量metric MS-LEEP和E-LEEP相比，并在该方法下表现出色。<details>
<summary>Abstract</summary>
Estimating the transferability of publicly available pretrained models to a target task has assumed an important place for transfer learning tasks in recent years. Existing efforts propose metrics that allow a user to choose one model from a pool of pre-trained models without having to fine-tune each model individually and identify one explicitly. With the growth in the number of available pre-trained models and the popularity of model ensembles, it also becomes essential to study the transferability of multiple-source models for a given target task. The few existing efforts study transferability in such multi-source ensemble settings using just the outputs of the classification layer and neglect possible domain or task mismatch. Moreover, they overlook the most important factor while selecting the source models, viz., the cohesiveness factor between them, which can impact the performance and confidence in the prediction of the ensemble. To address these gaps, we propose a novel Optimal tranSport-based suBmOdular tRaNsferability metric (OSBORN) to estimate the transferability of an ensemble of models to a downstream task. OSBORN collectively accounts for image domain difference, task difference, and cohesiveness of models in the ensemble to provide reliable estimates of transferability. We gauge the performance of OSBORN on both image classification and semantic segmentation tasks. Our setup includes 28 source datasets, 11 target datasets, 5 model architectures, and 2 pre-training methods. We benchmark our method against current state-of-the-art metrics MS-LEEP and E-LEEP, and outperform them consistently using the proposed approach.
</details>
<details>
<summary>摘要</summary>
估计公共可用预训练模型在目标任务中的转移性在过去几年中得到了重要地位。现有的努力提出了用于选择预训练模型池中的一个模型而无需 individually fine-tune each model和特定地标出一个的指标。随着可用的预训练模型的数量的增加和模型组合的流行，也变得必要研究多源模型在给定目标任务中的转移性。现有的努力研究了这种多源模型的转移性使用输出类别层的结果，而忽略了可能存在的领域或任务差异，而且也忽略了选择源模型时最重要的因素——模型集合中的凝结度，这可能会影响预测 ensemble 的性能和信任度。为了解决这些差距，我们提出了一种新的 Optimal tranSport-based suBmOdular tRaNsferability 指标（OSBORN），用于估计 ensemble 模型到下游任务的转移性。OSBORN 共同考虑图像领域差异、任务差异和模型集合中的凝结度，以提供可靠的转移性估计。我们在图像分类和 semantic segmentation 任务上测试了我们的方法，并与当前状态的метрик MS-LEEP 和 E-LEEP 进行了比较，并一致地超越了它们。
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Architectures-for-Language-Agents"><a href="#Cognitive-Architectures-for-Language-Agents" class="headerlink" title="Cognitive Architectures for Language Agents"></a>Cognitive Architectures for Language Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02427">http://arxiv.org/abs/2309.02427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysymyth/awesome-language-agents">https://github.com/ysymyth/awesome-language-agents</a></li>
<li>paper_authors: Theodore Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L. Griffiths</li>
<li>for: 本研究旨在开发一种新一代的语言智能代理人，以帮助语言模型（LLM）进行更好的理解和决策。</li>
<li>methods: 本研究使用了符号人工智能的历史经验，将生成语言模型（LLM）与外部资源（如互联网）或内部控制流（如提示链）结合起来，以建立一个完整的语言代理人系统。</li>
<li>results: 研究表明，LLMs 具有许多生产系统的特性，而最近尝试改进 LLMS 的理解和基础设施的努力，与生产系统驱动的认知架构的发展具有很大的相似性。<details>
<summary>Abstract</summary>
Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning. However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent. To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents. We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework. Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future.
</details>
<details>
<summary>摘要</summary>
We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework.Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future.
</details></li>
</ul>
<hr>
<h2 id="A-Context-Sensitive-Approach-to-XAI-in-Music-Performance"><a href="#A-Context-Sensitive-Approach-to-XAI-in-Music-Performance" class="headerlink" title="A Context-Sensitive Approach to XAI in Music Performance"></a>A Context-Sensitive Approach to XAI in Music Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.04491">http://arxiv.org/abs/2309.04491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicola Privato, Jack Armitage</li>
<li>for: 提出了一种Explainable Pragmatism（EP）框架，用于解释人工智能（AI）系统在音乐表演中的工作原理。</li>
<li>methods: 提出了一种基于上下文和听众的解释需求开发方法，并在实际应用中进行了详细的描述和分析。</li>
<li>results: EP框架可以帮助提高AI系统在艺术应用中的透明度和可解性，并且可以根据听众反馈和改进。<details>
<summary>Abstract</summary>
The rapidly evolving field of Explainable Artificial Intelligence (XAI) has generated significant interest in developing methods to make AI systems more transparent and understandable. However, the problem of explainability cannot be exhaustively solved in the abstract, as there is no single approach that can be universally applied to generate adequate explanations for any given AI system, and this is especially true in the arts. In this position paper, we propose an Explanatory Pragmatism (EP) framework for XAI in music performance, emphasising the importance of context and audience in the development of explainability requirements. By tailoring explanations to specific audiences and continuously refining them based on feedback, EP offers a promising direction for enhancing the transparency and interpretability of AI systems in broad artistic applications and more specifically to music performance.
</details>
<details>
<summary>摘要</summary>
rapidly evolving field of 可解释人工智能（XAI）已引起了开发方法来使AI系统更透明和理解的广泛关注。然而，问题的解释不可能在抽象中完全解决，因为没有一种通用的方法可以对任何AI系统生成足够的解释，这特别是在艺术领域。在这篇Position paper中，我们提出了一种 Pragmatism（EP）框架 для XAI在音乐表演中，强调了解释的上下文和听众的重要性。通过对specific audiences tailoring explanations和基于反馈不断修改，EP提供了一个有前途的方向来提高AI系统在艺术应用中的透明度和可解释性。
</details></li>
</ul>
<hr>
<h2 id="Information-Processing-by-Neuron-Populations-in-the-Central-Nervous-System-Mathematical-Structure-of-Data-and-Operations"><a href="#Information-Processing-by-Neuron-Populations-in-the-Central-Nervous-System-Mathematical-Structure-of-Data-and-Operations" class="headerlink" title="Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations"></a>Information Processing by Neuron Populations in the Central Nervous System: Mathematical Structure of Data and Operations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02332">http://arxiv.org/abs/2309.02332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin N. P. Nilsson<br>for:这篇论文旨在探讨神经元集团中的信息编码和操作方法。methods:该论文使用一种现代化的神经元模型，并通过描述这些神经元集团的数学结构，探讨了这些集团在信息处理方面的能力。results:研究发现，这些神经元集团可以通过一种简单的代数结构来表示和处理信息，并且可以实现许多操作，如特殊化、通用化、新鲜度检测、维度减少、逆模型、预测和关联记忆等。这些结果可能有助于我们更好地理解脑中的信息处理机制，并在认知科学和人工智能领域进行进一步的研究。<details>
<summary>Abstract</summary>
In the intricate architecture of the mammalian central nervous system, neurons form populations. Axonal bundles communicate between these clusters using spike trains as their medium. However, these neuron populations' precise encoding and operations have yet to be discovered. In our analysis, the starting point is a state-of-the-art mechanistic model of a generic neuron endowed with plasticity. From this simple framework emerges a profound mathematical construct: The representation and manipulation of information can be precisely characterized by an algebra of finite convex cones. Furthermore, these neuron populations are not merely passive transmitters. They act as operators within this algebraic structure, mirroring the functionality of a low-level programming language. When these populations interconnect, they embody succinct yet potent algebraic expressions. These networks allow them to implement many operations, such as specialization, generalization, novelty detection, dimensionality reduction, inverse modeling, prediction, and associative memory. In broader terms, this work illuminates the potential of matrix embeddings in advancing our understanding in fields like cognitive science and AI. These embeddings enhance the capacity for concept processing and hierarchical description over their vector counterparts.
</details>
<details>
<summary>摘要</summary>
在哺乳动物中枢神经系统的复杂建筑中， neurons 组成 populations。 axon 短列传输 между这些群体使用冲击车作为媒介。然而，这些 neuron  populations 的准确编码和操作仍未被发现。在我们的分析中，开始点是一种现代机制模型，拥有 пластичность的 generic neuron。从这个简单的框架中，出现了深刻的数学构造：表示和操作信息的 algebra of finite convex cones。此外，这些 neuron populations 不仅是 passive 的传输器。它们作为这些数学结构中的操作员，反映了低级编程语言的功能。当这些 populations 相互连接时，它们实现了简洁而强大的数学表达。这些网络允许它们实现许多操作，如特性化、泛化、发现新的、维度减少、逆模型、预测和相关记忆。在更广泛的意义上，这些 embedding 在认知科学和 AI 领域的发展中具有潜在的潜力。这些 embedding 可以提高概念处理的能力和层次描述的能力，比vector counterparts 更高效。
</details></li>
</ul>
<hr>
<h2 id="Neurosymbolic-Meta-Reinforcement-Lookahead-Learning-Achieves-Safe-Self-Driving-in-Non-Stationary-Environments"><a href="#Neurosymbolic-Meta-Reinforcement-Lookahead-Learning-Achieves-Safe-Self-Driving-in-Non-Stationary-Environments" class="headerlink" title="Neurosymbolic Meta-Reinforcement Lookahead Learning Achieves Safe Self-Driving in Non-Stationary Environments"></a>Neurosymbolic Meta-Reinforcement Lookahead Learning Achieves Safe Self-Driving in Non-Stationary Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02328">http://arxiv.org/abs/2309.02328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haozhe Lei, Quanyan Zhu</li>
<li>for: This paper focuses on the integration of machine learning into self-driving technology, with a specific emphasis on ensuring safety and efficiency in real-world applications.</li>
<li>methods: The paper introduces an algorithm for online meta-reinforcement learning, called Neurosymbolic Meta-Reinforcement Lookahead Learning (NUMERLA), which combines lookahead symbolic constraints with online adaptation to ensure both efficiency and safety.</li>
<li>results: The experimental results demonstrate that NUMERLA enables the self-driving agent to adapt in real-time to non-stationary urban human-vehicle interaction scenarios, leading to safe and self-adaptive driving.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文关注机器学习在自动驾驶技术中的集成，特别是在实际应用中保证安全性和效率的问题。</li>
<li>methods: 这篇论文提出了一种名为数字符号化多因素奖励前Lookahead学习算法（NUMERLA），它将数字符号化约束与在线调整结合起来，以确保效率和安全性均得到保障。</li>
<li>results: 实验结果表明， NUMERLA可以使自动驾驶机器人在非站立城市人机交互enario下实现安全和自适应驾驶。<details>
<summary>Abstract</summary>
In the area of learning-driven artificial intelligence advancement, the integration of machine learning (ML) into self-driving (SD) technology stands as an impressive engineering feat. Yet, in real-world applications outside the confines of controlled laboratory scenarios, the deployment of self-driving technology assumes a life-critical role, necessitating heightened attention from researchers towards both safety and efficiency. To illustrate, when a self-driving model encounters an unfamiliar environment in real-time execution, the focus must not solely revolve around enhancing its anticipated performance; equal consideration must be given to ensuring its execution or real-time adaptation maintains a requisite level of safety. This study introduces an algorithm for online meta-reinforcement learning, employing lookahead symbolic constraints based on \emph{Neurosymbolic Meta-Reinforcement Lookahead Learning} (NUMERLA). NUMERLA proposes a lookahead updating mechanism that harmonizes the efficiency of online adaptations with the overarching goal of ensuring long-term safety. Experimental results demonstrate NUMERLA confers the self-driving agent with the capacity for real-time adaptability, leading to safe and self-adaptive driving under non-stationary urban human-vehicle interaction scenarios.
</details>
<details>
<summary>摘要</summary>
在学习驱动人工智能的发展领域，将机器学习（ML） integrates into自动驾驶（SD）技术是一项印象深刻的工程成果。然而，在实际应用中，自动驾驶技术的部署具有生命 crítical 的重要性，需要研究人员强调安全性和效率之间的平衡。例如，当一个自动驾驶模型在实时执行中遇到未知环境时，不能 solely 围绕增强其预期性能进行强调，也需要确保其执行或实时适应保持一定的安全水平。本研究提出了一种在线meta-学习算法，使用lookahead符号约束，基于Neurosymbolic Meta-Reinforcement Lookahead Learning（NUMERLA）。NUMERLA提出了一种协调在线适应的效率和长期安全的目标，使得自动驾驶机器人能够在非站ARY urban human-vehicle interactionenario下进行安全和自适应驾驶。实验结果表明，NUMERLA使得自动驾驶机器人具有了实时适应的能力，并在非站ARY urban human-vehicle interactionenario下保持了安全和自适应的驾驶。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-File-Context-for-Source-Code-Summarization"><a href="#Revisiting-File-Context-for-Source-Code-Summarization" class="headerlink" title="Revisiting File Context for Source Code Summarization"></a>Revisiting File Context for Source Code Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02326">http://arxiv.org/abs/2309.02326</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apcl-research/transformerfc">https://github.com/apcl-research/transformerfc</a></li>
<li>paper_authors: Aakash Bansal, Chia-Yi Su, Collin McMillan</li>
<li>for: 这个论文主要是为了提高代码概要的生成。</li>
<li>methods: 该论文使用了改进的 transformer 架构，用于编码文件上下文信息，以帮助解决一些困难的例子。</li>
<li>results: 研究发现，文件上下文信息可以帮助解决一些困难的例子，并且提高代码概要的生成质量。<details>
<summary>Abstract</summary>
Source code summarization is the task of writing natural language descriptions of source code. A typical use case is generating short summaries of subroutines for use in API documentation. The heart of almost all current research into code summarization is the encoder-decoder neural architecture, and the encoder input is almost always a single subroutine or other short code snippet. The problem with this setup is that the information needed to describe the code is often not present in the code itself -- that information often resides in other nearby code. In this paper, we revisit the idea of ``file context'' for code summarization. File context is the idea of encoding select information from other subroutines in the same file. We propose a novel modification of the Transformer architecture that is purpose-built to encode file context and demonstrate its improvement over several baselines. We find that file context helps on a subset of challenging examples where traditional approaches struggle.
</details>
<details>
<summary>摘要</summary>
源代码概要是将源代码写成自然语言描述的任务。一个常见的用例是生成 API 文档中的简短描述。现有的大多数研究都使用 encoder-decoder 神经网络架构，其中 encoder 输入通常是单个子routine 或其他短代码副本。问题在于，代码描述所需的信息不总是在代码中存在，这些信息通常位于附近的代码中。在这篇论文中，我们重新考虑了 ``file context'' 的想法，即在代码概要中使用其他文件中的选择信息。我们提出了一种 modificated Transformer 架构，专门用于编码文件上下文，并证明其在多个基线上显著提高了性能。我们发现，文件上下文有助于一些困难的例子，传统方法在这些例子中困难。
</details></li>
</ul>
<hr>
<h2 id="SeisCLIP-A-seismology-foundation-model-pre-trained-by-multi-modal-data-for-multi-purpose-seismic-feature-extraction"><a href="#SeisCLIP-A-seismology-foundation-model-pre-trained-by-multi-modal-data-for-multi-purpose-seismic-feature-extraction" class="headerlink" title="SeisCLIP: A seismology foundation model pre-trained by multi-modal data for multi-purpose seismic feature extraction"></a>SeisCLIP: A seismology foundation model pre-trained by multi-modal data for multi-purpose seismic feature extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02320">http://arxiv.org/abs/2309.02320</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sixu0/SeisCLIP">https://github.com/sixu0/SeisCLIP</a></li>
<li>paper_authors: Xu Si, Xinming Wu, Hanlin Sheng, Jun Zhu, Zefeng Li</li>
<li>for: 这篇论文的目的是发展一个基础模型，供不同领域的地震学家使用。</li>
<li>methods: 这篇论文使用了对比式学习方法，将多modal数据集融合到一个基础模型中。</li>
<li>results: 这篇论文的实验结果显示，基础模型可以在不同地区的数据集上表现出色，并且在不同任务上表现更好 than 基于点的方法。<details>
<summary>Abstract</summary>
Training specific deep learning models for particular tasks is common across various domains within seismology. However, this approach encounters two limitations: inadequate labeled data for certain tasks and limited generalization across regions. To address these challenges, we develop SeisCLIP, a seismology foundation model trained through contrastive learning from multi-modal data. It consists of a transformer encoder for extracting crucial features from time-frequency seismic spectrum and an MLP encoder for integrating the phase and source information of the same event. These encoders are jointly pre-trained on a vast dataset and the spectrum encoder is subsequently fine-tuned on smaller datasets for various downstream tasks. Notably, SeisCLIP's performance surpasses that of baseline methods in event classification, localization, and focal mechanism analysis tasks, employing distinct datasets from different regions. In conclusion, SeisCLIP holds significant potential as a foundational model in the field of seismology, paving the way for innovative directions in foundation-model-based seismology research.
</details>
<details>
<summary>摘要</summary>
通常在不同领域内的地震学中都会使用特定任务的深度学习模型训练。然而，这种方法存在两个限制：一是没有充足的标注数据 для某些任务，二是限制了在不同地区的泛化。为了解决这些挑战，我们开发了SeisCLIP，一个基于对比学习的地震学基础模型。它包括一个变换器编码器，用于从时频地震谱中提取关键特征，以及一个多层感知编码器，用于 инте integrating频谱信息和源信息。这两个编码器被共同预训练在庞大的数据集上，并且 spectrum编码器在更小的数据集上进行细化训练以适应不同下游任务。值得注意的是，SeisCLIP的性能在不同地区的事件分类、地点定位和焦点机制分析任务中都超过了基eline方法的性能。因此，SeisCLIP在地震学领域中具有重要的潜在价值，可能开创出新的基础模型基于的地震学研究方向。
</details></li>
</ul>
<hr>
<h2 id="Graph-Self-Contrast-Representation-Learning"><a href="#Graph-Self-Contrast-Representation-Learning" class="headerlink" title="Graph Self-Contrast Representation Learning"></a>Graph Self-Contrast Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02304">http://arxiv.org/abs/2309.02304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GRAND-Lab/MERIT">https://github.com/GRAND-Lab/MERIT</a></li>
<li>paper_authors: Minjie Chen, Yao Cheng, Ye Wang, Xiang Li, Ming Gao</li>
<li>for: 本文提出了一种新的图自对抗框架GraphSC，用于图表示学习。</li>
<li>methods:  GraphSC使用了一个positive和一个negative样本，并使用三元损失函数。具体来说， GraphSC使用图生成函数来生成图样本的多种强度的负样本，并使用HSIC来因素化表示。</li>
<li>results: 在对19种当前状态的方法进行了广泛的实验测试后，GraphSC在无监督和转移学习Setting中表现出了优秀的表现。<details>
<summary>Abstract</summary>
Graph contrastive learning (GCL) has recently emerged as a promising approach for graph representation learning. Some existing methods adopt the 1-vs-K scheme to construct one positive and K negative samples for each graph, but it is difficult to set K. For those methods that do not use negative samples, it is often necessary to add additional strategies to avoid model collapse, which could only alleviate the problem to some extent. All these drawbacks will undoubtedly have an adverse impact on the generalizability and efficiency of the model. In this paper, to address these issues, we propose a novel graph self-contrast framework GraphSC, which only uses one positive and one negative sample, and chooses triplet loss as the objective. Specifically, self-contrast has two implications. First, GraphSC generates both positive and negative views of a graph sample from the graph itself via graph augmentation functions of various intensities, and use them for self-contrast. Second, GraphSC uses Hilbert-Schmidt Independence Criterion (HSIC) to factorize the representations into multiple factors and proposes a masked self-contrast mechanism to better separate positive and negative samples. Further, Since the triplet loss only optimizes the relative distance between the anchor and its positive/negative samples, it is difficult to ensure the absolute distance between the anchor and positive sample. Therefore, we explicitly reduced the absolute distance between the anchor and positive sample to accelerate convergence. Finally, we conduct extensive experiments to evaluate the performance of GraphSC against 19 other state-of-the-art methods in both unsupervised and transfer learning settings.
</details>
<details>
<summary>摘要</summary>
graph contrastive learning (GCL) 近期出现为图表示学习的有力的方法之一。一些现有方法采用1对K的方案来建立一个图和K个负样本，但是很难设置K。对于不使用负样本的方法，通常需要添加额外策略以避免模型塌陷，这可以只是减轻问题的程度。这些缺点会对模型的普适性和效率产生负面影响。在这篇论文中，我们提出一种新的图自相关框架GraphSC，它仅使用一个图和一个负样本，并选择三元损失为目标。具体来说，自相关有两个含义。首先，GraphSC通过图像函数的多种强度生成了图样本的正面和负面视图，并将它们用于自相关。其次，GraphSC使用希尔伯特-施密特独立度标准（HSIC）来因素化表示，并提出了屏蔽自相关机制以更好地分离正面和负面样本。此外，因为三元损失仅仅优化了anchor和正样本之间的相对距离，因此我们显式减小了anchor和正样本之间的绝对距离，以加速收敛。最后，我们进行了广泛的实验，以评估GraphSC在无监督和转移学习设置下的性能，与19种当前状态的方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Semantic-Communication-with-Deep-Generative-Models-–-An-ICASSP-Special-Session-Overview"><a href="#Enhancing-Semantic-Communication-with-Deep-Generative-Models-–-An-ICASSP-Special-Session-Overview" class="headerlink" title="Enhancing Semantic Communication with Deep Generative Models – An ICASSP Special Session Overview"></a>Enhancing Semantic Communication with Deep Generative Models – An ICASSP Special Session Overview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02478">http://arxiv.org/abs/2309.02478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eleonora Grassucci, Yuki Mitsufuji, Ping Zhang, Danilo Comminiello</li>
<li>for: 本研究旨在探讨 semantic communication 在 future AI-driven communication systems 中的突出作用，以及如何通过深度生成模型来解决 semantic information EXTRACTION 和 semantically consistent data 生成的挑战。</li>
<li>methods: 本研究使用 deep generative models 来 Addressing semantic communication challenges from the machine learning perspective，包括 dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions。</li>
<li>results: 本研究 Chart novel research pathways for the next generative semantic communication frameworks, 并预示了 deep generative models 在 semantic communication 中的突出作用。<details>
<summary>Abstract</summary>
Semantic communication is poised to play a pivotal role in shaping the landscape of future AI-driven communication systems. Its challenge of extracting semantic information from the original complex content and regenerating semantically consistent data at the receiver, possibly being robust to channel corruptions, can be addressed with deep generative models. This ICASSP special session overview paper discloses the semantic communication challenges from the machine learning perspective and unveils how deep generative models will significantly enhance semantic communication frameworks in dealing with real-world complex data, extracting and exploiting semantic information, and being robust to channel corruptions. Alongside establishing this emerging field, this paper charts novel research pathways for the next generative semantic communication frameworks.
</details>
<details>
<summary>摘要</summary>
semantic communication 将在未来的 AI 驱动通信系统中扮演重要的角色。其挑战是从原始复杂内容中提取 semantics 信息并在接收端生成具有 semantics 一致性的数据，可能在通道损害情况下保持稳定性。这篇 ICASSP 特别会议简述paper 揭示了从机器学习角度来看 semantic communication 的挑战和 deep generative models 如何在实际世界中处理复杂数据、提取和利用 semantics 信息，并在通道损害情况下保持稳定性。此外，这篇 paper 还映示了未来的 generative semantic communication 框架的新研究路径。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Observation-Intervention-Trade-Off-in-Optimisation-Problems-with-Causal-Structure"><a href="#Optimal-Observation-Intervention-Trade-Off-in-Optimisation-Problems-with-Causal-Structure" class="headerlink" title="Optimal Observation-Intervention Trade-Off in Optimisation Problems with Causal Structure"></a>Optimal Observation-Intervention Trade-Off in Optimisation Problems with Causal Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02287">http://arxiv.org/abs/2309.02287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kim Hammar, Neil Dhir</li>
<li>for: 优化成本高的灰色案例目标函数，在有限预算下，基于 causal 结构的知识。</li>
<li>methods: 使用非幼目的最优停止问题，考虑观察 intervención 费用负面选择。</li>
<li>results: 实验结果表明，我们的表述可以增强现有的算法在真实和 sintética 标准准的表现。<details>
<summary>Abstract</summary>
We consider the problem of optimising an expensive-to-evaluate grey-box objective function, within a finite budget, where known side-information exists in the form of the causal structure between the design variables. Standard black-box optimisation ignores the causal structure, often making it inefficient and expensive. The few existing methods that consider the causal structure are myopic and do not fully accommodate the observation-intervention trade-off that emerges when estimating causal effects. In this paper, we show that the observation-intervention trade-off can be formulated as a non-myopic optimal stopping problem which permits an efficient solution. We give theoretical results detailing the structure of the optimal stopping times and demonstrate the generality of our approach by showing that it can be integrated with existing causal Bayesian optimisation algorithms. Experimental results show that our formulation can enhance existing algorithms on real and synthetic benchmarks.
</details>
<details>
<summary>摘要</summary>
我们考虑一个评估成本高的灰色obox目标函数优化问题，在有限预算内进行优化，其中知道变量之间的 causal 结构。标准的黑色obox优化忽略了 causal 结构，经常使其不fficient和昂贵。现有的方法只考虑了 causal 结构，但它们是偏短视的，不完全考虑观测 intervención 费用的负面作用。在这篇论文中，我们表明观测 intervención 费用可以形式化为非偏短视的最优停止问题，允许高效解决。我们提供了理论结果，详细说明优止时间的结构，并证明我们的方法可以与现有的 causal Bayesian 优化算法结合使用。实验结果表明，我们的形式化可以提高现有算法的性能在真实和synthetic 标准测试上。
</details></li>
</ul>
<hr>
<h2 id="s-ID-Causal-Effect-Identification-in-a-Sub-Population"><a href="#s-ID-Causal-Effect-Identification-in-a-Sub-Population" class="headerlink" title="s-ID: Causal Effect Identification in a Sub-Population"></a>s-ID: Causal Effect Identification in a Sub-Population</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02281">http://arxiv.org/abs/2309.02281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Mohammad Abouei, Ehsan Mokhtarian, Negar Kiyavash</li>
<li>for: 本文目的是解决在特定子 populations 中 causal inference 问题，即从 observational data 中确定干预对特定子 populations 的影响。</li>
<li>methods: 本文提出了一种新的 causal inference 问题，称为 s-ID 问题，其中只有 observational data 可用，并且不知道整个人口的数据分布。作者提供了必需的和完整的条件，以确定 causal effect 在子 populations 中的可 identificability。</li>
<li>results: 本文提出的方法可以在 observational data 中确定 causal effect 在子 populations 中，并且可以在不同的 causal graph 下进行 identification。这种方法可以解决现有方法在 sub-populations 中的局限性。<details>
<summary>Abstract</summary>
Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.
</details>
<details>
<summary>摘要</summary>
causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.Here's the translation in Traditional Chinese as well: causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup within a larger population. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub-population. Given these conditions, we present a sound and complete algorithm for the s-ID problem.
</details></li>
</ul>
<hr>
<h2 id="MA-VAE-Multi-head-Attention-based-Variational-Autoencoder-Approach-for-Anomaly-Detection-in-Multivariate-Time-series-Applied-to-Automotive-Endurance-Powertrain-Testing"><a href="#MA-VAE-Multi-head-Attention-based-Variational-Autoencoder-Approach-for-Anomaly-Detection-in-Multivariate-Time-series-Applied-to-Automotive-Endurance-Powertrain-Testing" class="headerlink" title="MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing"></a>MA-VAE: Multi-head Attention-based Variational Autoencoder Approach for Anomaly Detection in Multivariate Time-series Applied to Automotive Endurance Powertrain Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02253">http://arxiv.org/abs/2309.02253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcs-crr/ma-vae">https://github.com/lcs-crr/ma-vae</a></li>
<li>paper_authors: Lucas Correia, Jan-Christoph Goos, Philipp Klein, Thomas Bäck, Anna V. Kononova</li>
<li>for:  automatous anomaly detection in automotive testing</li>
<li>methods:  variational autoencoder with multi-head attention (MA-VAE)</li>
<li>results:  detects the majority of anomalies with few false positives, avoids bypass phenomenon, and introduces a new method for remapping individual windows to a continuous time series.Here’s the breakdown of each point:</li>
<li>for: The paper is written for the purpose of proposing a novel approach to automatic anomaly detection in automotive testing, which is a real-world application with massive, diverse, multivariate, and temporal data.</li>
<li>methods: The proposed approach uses a variational autoencoder with multi-head attention (MA-VAE) to model the testee behavior and detect anomalies. The MA-VAE is trained on unlabelled data and has the ability to provide few false positives and detect the majority of anomalies.</li>
<li>results: The approach is tested on a real-world industrial data set and the results show that it can detect 67% of the anomalies present with 9% false positives. Additionally, the approach has the potential to perform well with only a fraction of the training and validation subset, but a more sophisticated threshold estimation method is required to extract it.<details>
<summary>Abstract</summary>
A clear need for automatic anomaly detection applied to automotive testing has emerged as more and more attention is paid to the data recorded and manual evaluation by humans reaches its capacity. Such real-world data is massive, diverse, multivariate and temporal in nature, therefore requiring modelling of the testee behaviour. We propose a variational autoencoder with multi-head attention (MA-VAE), which, when trained on unlabelled data, not only provides very few false positives but also manages to detect the majority of the anomalies presented. In addition to that, the approach offers a novel way to avoid the bypass phenomenon, an undesirable behaviour investigated in literature. Lastly, the approach also introduces a new method to remap individual windows to a continuous time series. The results are presented in the context of a real-world industrial data set and several experiments are undertaken to further investigate certain aspects of the proposed model. When configured properly, it is 9% of the time wrong when an anomaly is flagged and discovers 67% of the anomalies present. Also, MA-VAE has the potential to perform well with only a fraction of the training and validation subset, however, to extract it, a more sophisticated threshold estimation method is required.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于自动异常检测在汽车测试中的需求，现在越来越明显，因为人类的手动评估已经达到了其容量。这些真实世界数据是庞大、多样、多变和时间序列的，因此需要测试对象的行为模型。我们提议一种多头注意力自适应变换器（MA-VAE），当训练于无标签数据时，不仅可以减少假阳性数量，而且能够检测大多数异常现象。此外，该方法还可以避免快船现象，这是文献中 investigate 的不良行为。最后，该方法还引入了一种新的时间序列映射方法。结果在实际工业数据集上展示，并进行了一些实验来更深入探索ertain aspect of the proposed model。当配置正确时，MA-VAE的错误率为9%，并检测到67%的异常现象。此外，MA-VAE还有可能在只使用一小部分的训练和验证subset中表现良好，但是要EXTRACT 它，需要一种更加复杂的阈值估计方法。
</details></li>
</ul>
<hr>
<h2 id="Encoding-Seasonal-Climate-Predictions-for-Demand-Forecasting-with-Modular-Neural-Network"><a href="#Encoding-Seasonal-Climate-Predictions-for-Demand-Forecasting-with-Modular-Neural-Network" class="headerlink" title="Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network"></a>Encoding Seasonal Climate Predictions for Demand Forecasting with Modular Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02248">http://arxiv.org/abs/2309.02248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Smit Marvaniya, Jitendra Singh, Nicolas Galichet, Fred Ochieng Otieno, Geeth De Mel, Kommy Weldemariam</li>
<li>for: 提高供应链功能的时间序列预测精度</li>
<li>methods: 使用模块化神经网络架构，高效地编码季节气候预测结果，以及其他时间序列数据（例如买家模式），从而学习具有坚实性和可靠性的秘密表示</li>
<li>results: 比对 existed 预测方法，实验结果显示，使用该模型增加了约13%到17%的预测精度，在多个实际数据集上<details>
<summary>Abstract</summary>
Current time-series forecasting problems use short-term weather attributes as exogenous inputs. However, in specific time-series forecasting solutions (e.g., demand prediction in the supply chain), seasonal climate predictions are crucial to improve its resilience. Representing mid to long-term seasonal climate forecasts is challenging as seasonal climate predictions are uncertain, and encoding spatio-temporal relationship of climate forecasts with demand is complex.   We propose a novel modeling framework that efficiently encodes seasonal climate predictions to provide robust and reliable time-series forecasting for supply chain functions. The encoding framework enables effective learning of latent representations -- be it uncertain seasonal climate prediction or other time-series data (e.g., buyer patterns) -- via a modular neural network architecture. Our extensive experiments indicate that learning such representations to model seasonal climate forecast results in an error reduction of approximately 13\% to 17\% across multiple real-world data sets compared to existing demand forecasting methods.
</details>
<details>
<summary>摘要</summary>
当前时间序列预测问题通常使用短期天气特征作为外生输入。然而，在特定的时间序列预测解决方案（如购物者patterns）中，季节气候预测是关键以提高其抗难度。表示中期至长期季节气候预测的问题是复杂的，因为季节气候预测具有不确定性，而且与需求的空间时间关系复杂。我们提出了一种新的模型框架，可以效率地编码季节气候预测。该框架允许效果学习季节气候预测的秘密表示，并且可以吸收其他时间序列数据（如购物者patterns）的学习。我们的广泛实验表明，通过学习这些表示来模型季节气候预测可以减少错误率约13%到17%，相比之前的需求预测方法。
</details></li>
</ul>
<hr>
<h2 id="AGIBench-A-Multi-granularity-Multimodal-Human-referenced-Auto-scoring-Benchmark-for-Large-Language-Models"><a href="#AGIBench-A-Multi-granularity-Multimodal-Human-referenced-Auto-scoring-Benchmark-for-Large-Language-Models" class="headerlink" title="AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models"></a>AGIBench: A Multi-granularity, Multimodal, Human-referenced, Auto-scoring Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.06495">http://arxiv.org/abs/2309.06495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Tang, Wanling Gao, Luzhou Peng, Jianfeng Zhan<br>for:* 本研究的目的是为了评估大型自然语言模型（LLM）的问题解决能力和智能水平。methods:* 本研究提出了一种多级划分、多modal、人参照的benchmarking方法，称为AGIBench，用于评估LLM的问题解决能力。results:* AGIBench支持多级划分benchmarking，包括每个问题、每个能力分支、每个知识、每个模式和每个难度层次的 granularity。Note:* 本文使用Simplified Chinese text format.* 所有的中文句子使用标准的标点符号和格式。<details>
<summary>Abstract</summary>
Large language models (LLMs) like ChatGPT have revealed amazing intelligence. How to evaluate the question-solving abilities of LLMs and their degrees of intelligence is a hot-spot but challenging issue. First, the question-solving abilities are interlaced with different ability branches like understanding and massive knowledge categories like mathematics. Second, the inputs of questions are multimodal that may involve text and images. Third, the response format of LLMs is diverse and thus poses great challenges for result extraction and evaluation. In this paper, we propose AGIBench -- a multi-granularity, multimodal, human-referenced, and auto-scoring benchmarking methodology for LLMs. Instead of a collection of blended questions, AGIBench focuses on three typical ability branches and adopts a four-tuple <ability branch, knowledge, difficulty, modal> to label the attributes of each question. First, it supports multi-granularity benchmarking, e.g., per-question, per-ability branch, per-knowledge, per-modal, per-dataset, and per-difficulty level granularities. Second, it contains multimodal input, including text and images. Third, it classifies all the questions into five degrees of difficulty according to the average accuracy rate of abundant educated humans (human-referenced). Fourth, it adopts zero-shot learning to avoid introducing additional unpredictability and provides an auto-scoring method to extract and judge the result. Finally, it defines multi-dimensional metrics, including accuracy under the average, worst, best, and majority voting cases, and repeatability. AGIBench is publically available from \url{https://www.benchcouncil.org/agibench}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如ChatGPT的出色智能引发了评估这类模型的问题解决能力和智能水平的热点问题。然而，这些能力存在多种能力分支和多种输入模式，使评估变得具有挑战性。在本文中，我们提出了AGIBench方法，它是一种多级、多Modal、人参照的自动评分 benchMarking方法。在AGIBench中，每个问题被标记为四元组（能力分支、知识、Difficulty、Modal），以便支持多级别的评估。此外，AGIBench还支持多Modal输入，包括文本和图像。此外，它采用人参照的方式将问题分为五个Difficulty水平，并采用零投入学习以避免引入额外的不确定性。最后，它定义了多维度纪录，包括均值、最差、最佳、多数投票等纪录。AGIBench公共可用于 \url{https://www.benchcouncil.org/agibench}。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Model-based-Reinforcement-Learning-with-Large-State-Spaces"><a href="#Distributionally-Robust-Model-based-Reinforcement-Learning-with-Large-State-Spaces" class="headerlink" title="Distributionally Robust Model-based Reinforcement Learning with Large State Spaces"></a>Distributionally Robust Model-based Reinforcement Learning with Large State Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02236">http://arxiv.org/abs/2309.02236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shyam Sundhar Ramesh, Pier Giuseppe Sessa, Yifan Hu, Andreas Krause, Ilija Bogunovic</li>
<li>for: 本研究旨在解决机器学习中的复杂动态系统、数据收集成本高和实际环境不符合训练环境的问题。</li>
<li>methods: 本文使用分布robust Markov决策过程（DRMP），利用 Gaussian Processes 和最大差异减少算法，效率地学习多输出 номинаル过程动态模型，并可以快速适应不同的不确定性集。</li>
<li>results: 研究人员通过 theoretically 和实验来证明提议的方法可以快速和高效地学习分布robust策略，并且可以适应不同的不确定性集。实验结果表明，该方法可以快速适应分布shift，并且在许多实际应用中表现出色。<details>
<summary>Abstract</summary>
Three major challenges in reinforcement learning are the complex dynamical systems with large state spaces, the costly data acquisition processes, and the deviation of real-world dynamics from the training environment deployment. To overcome these issues, we study distributionally robust Markov decision processes with continuous state spaces under the widely used Kullback-Leibler, chi-square, and total variation uncertainty sets. We propose a model-based approach that utilizes Gaussian Processes and the maximum variance reduction algorithm to efficiently learn multi-output nominal transition dynamics, leveraging access to a generative model (i.e., simulator). We further demonstrate the statistical sample complexity of the proposed method for different uncertainty sets. These complexity bounds are independent of the number of states and extend beyond linear dynamics, ensuring the effectiveness of our approach in identifying near-optimal distributionally-robust policies. The proposed method can be further combined with other model-free distributionally robust reinforcement learning methods to obtain a near-optimal robust policy. Experimental results demonstrate the robustness of our algorithm to distributional shifts and its superior performance in terms of the number of samples needed.
</details>
<details>
<summary>摘要</summary>
三大挑战在强化学习中是复杂的动力系统和大状态空间，以及实际世界中的动力不同于训练环境部署。为了解决这些问题，我们研究了分布robust Markov决策过程（MDP），并使用了 kontinuous state space 下的 Kullback-Leibler、chi-square 和 total variation 不确定性集。我们提出了一种基于模型的方法，利用 Gaussian Processes 和最大差异减少算法，高效地学习多输出 номинал传递动力学，利用 simulator 的访问权限。我们进一步证明了我们的方法的统计样本复杂度，这些复杂度独立于状态数量，并超越了线性动力学，保证了我们的方法在分布不稳定下可以适应最佳的分布robust策略。我们的方法可以与其他分布robust强化学习方法相结合，以获得最佳的 robust 策略。实验结果表明，我们的算法具有分布不稳定的Robustness和较少样本数量的优势。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Black-box-LLMs-with-Medical-Textbooks-for-Clinical-Question-Answering"><a href="#Augmenting-Black-box-LLMs-with-Medical-Textbooks-for-Clinical-Question-Answering" class="headerlink" title="Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering"></a>Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02233">http://arxiv.org/abs/2309.02233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yubo Wang, Xueguang Ma, Wenhu Chen</li>
<li>for: 这个研究旨在应用大规模语言模型（LLM）到医疗领域，但是这类模型在医疗领域中的应用仍然具有挑战，主要是因为它们无法充分利用领域专门知识。</li>
<li>methods: 这个研究提出了一个名为Large-scale Language Models Augmented with Medical Textbooks（LLM-AMT）的解决方案，它通过将专业医学书籍作为设计的核心，通过插件式模组来扩展LLM的能力，包括混合文献搜寻器和询问增强器。</li>
<li>results: 实验结果显示，在三个开放领域医学问题解答任务上，使用LLM-AMT可以提高LLM的专业性和准确性，提高范围在11.4%到13.2%。此外，医学书籍作为搜寻 корпу的使用比wikipedia更有价值，实验结果显示，对于医学领域来说，使用医学书籍进行扩展可以提高性能范围在9.7%到12.2%。<details>
<summary>Abstract</summary>
Large-scale language models (LLMs), such as ChatGPT, are capable of generating human-like responses for various downstream tasks, such as task-oriented dialogues and question answering. However, applying LLMs to medical domains remains challenging due to their inability to leverage domain-specific knowledge. In this study, we present the Large-scale Language Models Augmented with Medical Textbooks (LLM-AMT), which integrates authoritative medical textbooks as the cornerstone of its design, enhancing its proficiency in the specialized domain through plug-and-play modules, comprised of a Hybrid Textbook Retriever, supplemented by the Query Augmenter and the LLM Reader. Experimental evaluation on three open-domain medical question-answering tasks reveals a substantial enhancement in both the professionalism and accuracy of the LLM responses when utilizing LLM-AMT, exhibiting an improvement ranging from 11.4% to 13.2%. Despite being 100 times smaller, we found that medical textbooks as the retrieval corpus serves as a more valuable external knowledge source than Wikipedia in the medical domain. Our experiments show that textbook augmentation results in a performance improvement ranging from 9.7% to 12.2% over Wikipedia augmentation.
</details>
<details>
<summary>摘要</summary>
大规模语言模型（LLM），如ChatGPT，可以生成人类化回答 для多种下游任务，如任务导向对话和问答。然而，在医疗领域中应用LLM仍然是一个挑战，因为它们无法借鉴医疗领域专业知识。在本研究中，我们提出了医疗领域语言模型增强器（LLM-AMT），它将权威的医疗文献作为设计的核心，通过插件式模块来增强其在专业领域的能力。我们的实验表明，在三个开放领域医学问答任务上，使用LLM-AMT可以substantially提高LLM的专业性和准确性，提高回答的质量，其中提高范围为11.4%到13.2%。我们发现，医疗文献作为搜索库是医疗领域更有价值的外部知识源，而不是Wikipedia。我们的实验表明，在使用医疗文献扩展时，表现提高的范围为9.7%到12.2%。
</details></li>
</ul>
<hr>
<h2 id="FSD-An-Initial-Chinese-Dataset-for-Fake-Song-Detection"><a href="#FSD-An-Initial-Chinese-Dataset-for-Fake-Song-Detection" class="headerlink" title="FSD: An Initial Chinese Dataset for Fake Song Detection"></a>FSD: An Initial Chinese Dataset for Fake Song Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02232">http://arxiv.org/abs/2309.02232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xieyuankun/fsd-dataset">https://github.com/xieyuankun/fsd-dataset</a></li>
<li>paper_authors: Yuankun Xie, Jingjing Zhou, Xiaolin Lu, Zhenghao Jiang, Yuxin Yang, Haonan Cheng, Long Ye</li>
<li>for: 本研究旨在提供一个特有的歌曲深度变异检测数据集，并利用这个数据集进行歌曲深度变异检测模型的训练和评估。</li>
<li>methods: 本研究使用了五种当前最佳的嗓音合成和嗓音转换技术来生成假歌曲，并使用这些假歌曲来初始化一个中文歌曲深度变异检测数据集（FSD）。然后，我们使用FSD数据集进行歌曲深度变异检测模型的训练和评估。</li>
<li>results: 我们的实验结果表明，将歌曲特有的特征提取和处理到深度变异检测模型中，可以减少平均错误率38.58%，相比Speech-trained ADD模型在FSD测试集上。<details>
<summary>Abstract</summary>
Singing voice synthesis and singing voice conversion have significantly advanced, revolutionizing musical experiences. However, the rise of "Deepfake Songs" generated by these technologies raises concerns about authenticity. Unlike Audio DeepFake Detection (ADD), the field of song deepfake detection lacks specialized datasets or methods for song authenticity verification. In this paper, we initially construct a Chinese Fake Song Detection (FSD) dataset to investigate the field of song deepfake detection. The fake songs in the FSD dataset are generated by five state-of-the-art singing voice synthesis and singing voice conversion methods. Our initial experiments on FSD revealed the ineffectiveness of existing speech-trained ADD models for the task of song deepFake detection. Thus, we employ the FSD dataset for the training of ADD models. We subsequently evaluate these models under two scenarios: one with the original songs and another with separated vocal tracks. Experiment results show that song-trained ADD models exhibit a 38.58% reduction in average equal error rate compared to speech-trained ADD models on the FSD test set.
</details>
<details>
<summary>摘要</summary>
《声音合成和声音转换技术在音乐经验方面已经取得了 significativ advancement，但是这些技术的出现也引发了 authenticity的问题。与Audio DeepFake Detection（ADD）不同的是，歌曲深伪检测领域没有专门的数据集或方法进行歌曲的真实性验证。在这篇论文中，我们首先构建了中文伪歌曲检测（FSD）数据集，以探讨歌曲深伪检测领域的问题。这些伪歌曲在FSD数据集中是由五种当前最好的声音合成和声音转换方法生成的。我们的初始实验表明，现有的speech-trained ADD模型对歌曲深伪检测任务并不有效。因此，我们使用FSD数据集来训练ADD模型。我们之后对这些模型进行了两种enario的评估：一种是使用原始的歌曲，另一种是使用分离的vocal轨。实验结果显示，使用歌曲训练的ADD模型在FSD测试集上比使用speech训练的ADD模型减少了38.58%的平均等错率。
</details></li>
</ul>
<hr>
<h2 id="DCP-Net-A-Distributed-Collaborative-Perception-Network-for-Remote-Sensing-Semantic-Segmentation"><a href="#DCP-Net-A-Distributed-Collaborative-Perception-Network-for-Remote-Sensing-Semantic-Segmentation" class="headerlink" title="DCP-Net: A Distributed Collaborative Perception Network for Remote Sensing Semantic Segmentation"></a>DCP-Net: A Distributed Collaborative Perception Network for Remote Sensing Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02230">http://arxiv.org/abs/2309.02230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhechao Wang, Peirui Cheng, Shujing Duan, Kaiqiang Chen, Zhirui Wang, Xinming Li, Xian Sun</li>
<li>for: 提高远程感知任务中紧急情况下多平台协同观测的精度和效率。</li>
<li>methods: 提出了一种分布式协同感知网络（DCP-Net），通过将不同平台的特征集成而提高感知性能。同时，通过自适应匹配模块和相关特征融合模块来实现多平台协同观测。</li>
<li>results: 经过广泛的实验和视觉分析，DCP-Net在三个semantic segmentation dataset上表现出了明显的优势，与exist方法相比，提高了mIoU值2.61%~16.89%，达到了当前最佳水平。<details>
<summary>Abstract</summary>
Onboard intelligent processing is widely applied in emergency tasks in the field of remote sensing. However, it is predominantly confined to an individual platform with a limited observation range as well as susceptibility to interference, resulting in limited accuracy. Considering the current state of multi-platform collaborative observation, this article innovatively presents a distributed collaborative perception network called DCP-Net. Firstly, the proposed DCP-Net helps members to enhance perception performance by integrating features from other platforms. Secondly, a self-mutual information match module is proposed to identify collaboration opportunities and select suitable partners, prioritizing critical collaborative features and reducing redundant transmission cost. Thirdly, a related feature fusion module is designed to address the misalignment between local and collaborative features, improving the quality of fused features for the downstream task. We conduct extensive experiments and visualization analyses using three semantic segmentation datasets, including Potsdam, iSAID and DFC23. The results demonstrate that DCP-Net outperforms the existing methods comprehensively, improving mIoU by 2.61%~16.89% at the highest collaboration efficiency, which promotes the performance to a state-of-the-art level.
</details>
<details>
<summary>摘要</summary>
在远程感知领域的紧急任务中，船载智能处理广泛应用。然而，它主要受限于个人平台的有限观测范围以及易受干扰的问题，导致准确性有限。针对当前多平台合作观测的状况，本文创新提出了分布式合作感知网络（DCP-Net）。首先，提议的DCP-Net帮助成员提高感知性能，将其他平台的特征集成到自己平台上。其次，基于自我相互信息匹配模块，用于识别合作机会，选择适合的合作伙伴，优先级划分关键合作特征，降低重复传输成本。第三，关联特征融合模块用于解决本地特征与合作特征之间的不一致问题，提高下游任务的质量。我们对三个semantic segmentation数据集进行了广泛的实验和视觉分析，包括Potsdam、iSAID和DFC23。结果表明，DCP-Net与现有方法相比，全面性地提高了miou值，在最高的合作效率下提高了2.61%~16.89%，提升性能至当前领先水平。
</details></li>
</ul>
<hr>
<h2 id="Dense-Object-Grounding-in-3D-Scenes"><a href="#Dense-Object-Grounding-in-3D-Scenes" class="headerlink" title="Dense Object Grounding in 3D Scenes"></a>Dense Object Grounding in 3D Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02224">http://arxiv.org/abs/2309.02224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wencan Huang, Daizong Liu, Wei Hu</li>
<li>for: 本研究旨在解决现有3D物体准确定位方法的限制，即只能根据单个句子描述一个物体进行定位。为了解决这个问题，我们引入了3D密集物体定位（3D DOG）任务，即在更复杂的段落中找到多个物体的定位。</li>
<li>methods: 我们提出了一种新的核心 трансформа器基于框架，名为3DOGSFormer。该框架包括一个地址驱动的本地变换器解码器，以及一个提案驱动的全球变换器解码器。这两个解码器协作以生成更加准确的定位提案。</li>
<li>results: 我们的3DOGSFormer在三个不同的测试集（Nr3D、Sr3D和ScanRefer）上进行了广泛的实验，结果表明，我们的方法在比较复杂的3D场景中对多个物体的定位有较高的准确率，与现有的3D单物体定位方法和密集物体定位方法相比，具有显著的优势。<details>
<summary>Abstract</summary>
Localizing objects in 3D scenes according to the semantics of a given natural language is a fundamental yet important task in the field of multimedia understanding, which benefits various real-world applications such as robotics and autonomous driving. However, the majority of existing 3D object grounding methods are restricted to a single-sentence input describing an individual object, which cannot comprehend and reason more contextualized descriptions of multiple objects in more practical 3D cases. To this end, we introduce a new challenging task, called 3D Dense Object Grounding (3D DOG), to jointly localize multiple objects described in a more complicated paragraph rather than a single sentence. Instead of naively localizing each sentence-guided object independently, we found that dense objects described in the same paragraph are often semantically related and spatially located in a focused region of the 3D scene. To explore such semantic and spatial relationships of densely referred objects for more accurate localization, we propose a novel Stacked Transformer based framework for 3D DOG, named 3DOGSFormer. Specifically, we first devise a contextual query-driven local transformer decoder to generate initial grounding proposals for each target object. Then, we employ a proposal-guided global transformer decoder that exploits the local object features to learn their correlation for further refining initial grounding proposals. Extensive experiments on three challenging benchmarks (Nr3D, Sr3D, and ScanRefer) show that our proposed 3DOGSFormer outperforms state-of-the-art 3D single-object grounding methods and their dense-object variants by significant margins.
</details>
<details>
<summary>摘要</summary>
本文提出了一个新的挑战任务，即3D密集物地理（3D DOG），它的目的是在3D场景中对自然语言中提供的多个对象进行同时地理化。现有的大多数3D物理地理方法都是基于单句输入，无法处理更复杂的多对象描述。为此，我们提出了一种新的框架，即3DOGSFormer，它利用了堆叠的变换器来探索多个对象之间的含义和空间关系，从而实现更高精度的物理地理。我们首先设计了一种基于上下文的查询驱动的本地变换器嵌入器，以生成每个目标对象的初步锚点提案。然后，我们使用一种提案驱动的全球变换器嵌入器，利用本地对象特征来学习它们之间的相互关系，进一步细化初步锚点提案。我们在三个具有挑战性的测试基准（Nr3D、Sr3D和ScanRefer）上进行了广泛的实验，结果显示，我们的提案的3DOGSFormer在与现有的3D单个对象地理方法和密集对象变体之间具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Improving-equilibrium-propagation-without-weight-symmetry-through-Jacobian-homeostasis"><a href="#Improving-equilibrium-propagation-without-weight-symmetry-through-Jacobian-homeostasis" class="headerlink" title="Improving equilibrium propagation without weight symmetry through Jacobian homeostasis"></a>Improving equilibrium propagation without weight symmetry through Jacobian homeostasis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02214">http://arxiv.org/abs/2309.02214</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/laborieux-axel/generalized-holo-ep">https://github.com/laborieux-axel/generalized-holo-ep</a></li>
<li>paper_authors: Axel Laborieux, Friedemann Zenke</li>
<li>for: 这个论文是为了研究等温傅振（EP）算法在生物或分析型神经网络上的应用。</li>
<li>methods: 这个论文使用了等温傅振算法，但是它需要权重对称和极小的平衡冲击来计算神经网络中的梯度。</li>
<li>results: 研究发现，权重不对称会导致等温傅振算法的表现不佳，而且可能会导致学习任务的低效。为了解决这个问题， authors propose了一种新的自适应目标函数，可以直接惩罚神经网络中权重的不对称性。这种自适应目标函数可以帮助神经网络更好地解决复杂的任务，如 ImageNet 32x32。<details>
<summary>Abstract</summary>
Equilibrium propagation (EP) is a compelling alternative to the backpropagation of error algorithm (BP) for computing gradients of neural networks on biological or analog neuromorphic substrates. Still, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to estimate unbiased gradients efficiently. Both requirements are challenging to implement in physical systems. Yet, whether and how weight asymmetry affects its applicability is unknown because, in practice, it may be masked by biases introduced through the finite nudge. To address this question, we study generalized EP, which can be formulated without weight symmetry, and analytically isolate the two sources of bias. For complex-differentiable non-symmetric networks, we show that the finite nudge does not pose a problem, as exact derivatives can still be estimated via a Cauchy integral. In contrast, weight asymmetry introduces bias resulting in low task performance due to poor alignment of EP's neuronal error vectors compared to BP. To mitigate this issue, we present a new homeostatic objective that directly penalizes functional asymmetries of the Jacobian at the network's fixed point. This homeostatic objective dramatically improves the network's ability to solve complex tasks such as ImageNet 32x32. Our results lay the theoretical groundwork for studying and mitigating the adverse effects of imperfections of physical networks on learning algorithms that rely on the substrate's relaxation dynamics.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>平衡传播（EP）是一种有吸引力的替代品 для误差传播算法（BP），用于计算神经网络的梯度。然而，EP算法需要权重的对称和微小的平衡干扰，以便效率地计算梯度。这两个需求在物理系统中实现可以是问题。但是，权重的不对称性是否会影响EP的应用是未知的，因为在实践中可能会遭受由固定干扰引入的偏见。为了解决这个问题，我们研究了一种简化EP的方法，可以不需要权重的对称。我们还可以分析EP中两种source of bias，并证明在复杂不对称的神经网络上，finite nudge不会对梯度的计算产生影响。然而，权重的不对称性会导致梯度的误差，从而降低任务的性能。为了解决这个问题，我们提出了一个新的家ostatic objective，可以直接 penalty函数的不对称性。这个家ostatic objective可以对任务如ImageNet 32x32进行解决，并获得了良好的性能。我们的结果对于研究和解决物理网络上学习算法的不完善性问题提供了理论基础。
</details></li>
</ul>
<hr>
<h2 id="Exchanging-based-Multimodal-Fusion-with-Transformer"><a href="#Exchanging-based-Multimodal-Fusion-with-Transformer" class="headerlink" title="Exchanging-based Multimodal Fusion with Transformer"></a>Exchanging-based Multimodal Fusion with Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02190">http://arxiv.org/abs/2309.02190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/recklessronan/muse">https://github.com/recklessronan/muse</a></li>
<li>paper_authors: Renyu Zhu, Chengcheng Han, Yong Qian, Qiushi Sun, Xiang Li, Ming Gao, Xuezhi Cao, Yunsen Xian</li>
<li>for: 本文研究了多模态融合的问题，特别是用于文本视频融合。</li>
<li>methods: 本文提出了一种基于Transformer的新的多模态融合模型MuSE，使用了两个编码器将多modal输入映射到不同的低维度空间中，并使用了两个解码器来规范嵌入并将其拟合到同一个空间中。</li>
<li>results: 对多模态命名实体识别和多模态情感分析两个任务进行了广泛的实验，结果表明MuSE比其他竞争者更高效。<details>
<summary>Abstract</summary>
We study the problem of multimodal fusion in this paper. Recent exchanging-based methods have been proposed for vision-vision fusion, which aim to exchange embeddings learned from one modality to the other. However, most of them project inputs of multimodalities into different low-dimensional spaces and cannot be applied to the sequential input data. To solve these issues, in this paper, we propose a novel exchanging-based multimodal fusion model MuSE for text-vision fusion based on Transformer. We first use two encoders to separately map multimodal inputs into different low-dimensional spaces. Then we employ two decoders to regularize the embeddings and pull them into the same space. The two decoders capture the correlations between texts and images with the image captioning task and the text-to-image generation task, respectively. Further, based on the regularized embeddings, we present CrossTransformer, which uses two Transformer encoders with shared parameters as the backbone model to exchange knowledge between multimodalities. Specifically, CrossTransformer first learns the global contextual information of the inputs in the shallow layers. After that, it performs inter-modal exchange by selecting a proportion of tokens in one modality and replacing their embeddings with the average of embeddings in the other modality. We conduct extensive experiments to evaluate the performance of MuSE on the Multimodal Named Entity Recognition task and the Multimodal Sentiment Analysis task. Our results show the superiority of MuSE against other competitors. Our code and data are provided at https://github.com/RecklessRonan/MuSE.
</details>
<details>
<summary>摘要</summary>
我们在这篇论文中研究了多模态融合问题。在最近的交换基本方法中，有些方法用于视觉融合，其目的是将一个模态的嵌入交换到另一个模态中。然而，大多数方法将多模态输入映射到不同的低维度空间，无法应用于顺序输入数据。为解决这些问题，在这篇论文中，我们提出了一种基于Transformer的新的多模态融合模型MuSE，用于文本视觉融合。我们首先使用两个encoder将多模态输入映射到不同的低维度空间。然后，我们employs两个decoder来规范嵌入并将其拖入同一个空间。两个decoder使得文本和图像之间的相关性能够更好地捕捉，并通过图像描述任务和文本到图像生成任务来规范嵌入。此外，基于规范嵌入，我们还提出了交换Transformer，它使用两个Transformer encoder的共享参数作为后备模型，以交换多模态之间的知识。具体来说，交换Transformer先学习输入的全局 контекст信息，然后进行交换，选择一个模态中的一些Token，并将其嵌入换成另一个模态中的均值。我们对MuSE进行了广泛的实验，以评估其在多模态命名实体识别任务和多模态情感分析任务中的表现。我们的结果显示MuSE在与其他竞争对手相比，具有更高的表现。我们的代码和数据可以在https://github.com/RecklessRonan/MuSE上获取。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-BERT-Language-Models-for-Multi-Lingual-ESG-Issue-Identification"><a href="#Leveraging-BERT-Language-Models-for-Multi-Lingual-ESG-Issue-Identification" class="headerlink" title="Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification"></a>Leveraging BERT Language Models for Multi-Lingual ESG Issue Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02189">http://arxiv.org/abs/2309.02189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elvys Linhares Pontes, Mohamed Benjannet, Lam Kim Ming</li>
<li>for: 这项研究的目的是为投资者更好地了解公司的可持续发展和社会责任，通过分类新闻文章的ESG Issue标签来提高投资决策的可持续性。</li>
<li>methods: 该研究使用BERT语言模型来实现新闻文章的分类，并 comparing different BERT语言模型和SVM折衔模型的表现。</li>
<li>results: 研究发现，使用RoBERTa分类器得到了英文测试集第二名的成绩，并与法语测试集第五名相当。此外，SVM折衔模型特制 для中文语言也表现出色，在测试集上排名第二。<details>
<summary>Abstract</summary>
Environmental, Social, and Governance (ESG) has been used as a metric to measure the negative impacts and enhance positive outcomes of companies in areas such as the environment, society, and governance. Recently, investors have increasingly recognized the significance of ESG criteria in their investment choices, leading businesses to integrate ESG principles into their operations and strategies. The Multi-Lingual ESG Issue Identification (ML-ESG) shared task encompasses the classification of news documents into 35 distinct ESG issue labels. In this study, we explored multiple strategies harnessing BERT language models to achieve accurate classification of news documents across these labels. Our analysis revealed that the RoBERTa classifier emerged as one of the most successful approaches, securing the second-place position for the English test dataset, and sharing the fifth-place position for the French test dataset. Furthermore, our SVM-based binary model tailored for the Chinese language exhibited exceptional performance, earning the second-place rank on the test dataset.
</details>
<details>
<summary>摘要</summary>
环境、社会和管理（ESG）被用作公司负面影响和改善效果的度量。近期，投资者对ESG标准的重要性日益认识，导致企业将ESG原则 integrate into their operations and strategies。这个多语言ESG问题识别（ML-ESG）共同任务涵盖35个不同的ESG问题标签。本研究通过BERT语言模型的多种策略来实现新闻文档的准确分类。我们的分析发现，RoBERTa分类器在英语测试数据集中获得了第二名的成绩，并在法语测试数据集中与其他模型并列第五名。此外，我们对中文语言的SVM二分类模型也展现出了出色的表现，在测试数据集中获得了第二名。
</details></li>
</ul>
<hr>
<h2 id="AniPortraitGAN-Animatable-3D-Portrait-Generation-from-2D-Image-Collections"><a href="#AniPortraitGAN-Animatable-3D-Portrait-Generation-from-2D-Image-Collections" class="headerlink" title="AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections"></a>AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02186">http://arxiv.org/abs/2309.02186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YueWuHKUST/AniPortraitGAN">https://github.com/YueWuHKUST/AniPortraitGAN</a></li>
<li>paper_authors: Yue Wu, Sicheng Xu, Jianfeng Xiang, Fangyun Wei, Qifeng Chen, Jiaolong Yang, Xin Tong</li>
<li>for: 生成高质量的3D人像视频</li>
<li>methods: 基于生成光谱抽象表示法和可学习头部和肩部变形的3D人像生成模型</li>
<li>results: 使用不结构化2D图像集训练的方法可以生成多样性和高质量的3D人像视频，并可以控制不同属性的表达Here’s a more detailed explanation of each point:</li>
<li>for: The paper is focused on generating high-quality 3D portrait videos, which are relatively rare in real life and are challenging to generate using existing methods.</li>
<li>methods: The proposed method is based on a generative radiance manifold representation and includes learnable facial and head-shoulder deformations. The method also uses a dual-camera rendering and adversarial learning scheme to improve the quality of the generated faces.</li>
<li>results: The method, trained on unstructured 2D image collections, can generate diverse and high-quality 3D portraits with desired control over different properties, such as facial expression, head pose, and shoulder movements.<details>
<summary>Abstract</summary>
Previous animatable 3D-aware GANs for human generation have primarily focused on either the human head or full body. However, head-only videos are relatively uncommon in real life, and full body generation typically does not deal with facial expression control and still has challenges in generating high-quality results. Towards applicable video avatars, we present an animatable 3D-aware GAN that generates portrait images with controllable facial expression, head pose, and shoulder movements. It is a generative model trained on unstructured 2D image collections without using 3D or video data. For the new task, we base our method on the generative radiance manifold representation and equip it with learnable facial and head-shoulder deformations. A dual-camera rendering and adversarial learning scheme is proposed to improve the quality of the generated faces, which is critical for portrait images. A pose deformation processing network is developed to generate plausible deformations for challenging regions such as long hair. Experiments show that our method, trained on unstructured 2D images, can generate diverse and high-quality 3D portraits with desired control over different properties.
</details>
<details>
<summary>摘要</summary>
以前的可动画3D意识GANs为人类生成都主要集中在人头或全身上。然而，头部视频较少出现在实际生活中，全身生成通常没有控制表情和脸部表现的能力，并且仍有高质量结果生成的挑战。为应用视频化身，我们提出了可动画3D意识GAN，该模型可生成带有可控表情、头部姿势和肩部运动的肖像图像。我们基于生成抛光扩散表示，并增加了可学习的脸部和头部运动扭曲。我们还提出了双摄像头渲染和对抗学习方案，以提高生成的脸部质量，这是对肖像图像的关键。此外，我们还开发了一个挑战区域 such as long hair 的姿势处理网络，以生成可能的姿势扭曲。实验表明，我们的方法，通过未结构化的2D图像集训练，可以生成多样化和高质量的3D肖像图像，并且可以控制不同的属性。
</details></li>
</ul>
<hr>
<h2 id="BEVTrack-A-Simple-Baseline-for-3D-Single-Object-Tracking-in-Bird’s-Eye-View"><a href="#BEVTrack-A-Simple-Baseline-for-3D-Single-Object-Tracking-in-Bird’s-Eye-View" class="headerlink" title="BEVTrack: A Simple Baseline for 3D Single Object Tracking in Bird’s-Eye View"></a>BEVTrack: A Simple Baseline for 3D Single Object Tracking in Bird’s-Eye View</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02185">http://arxiv.org/abs/2309.02185</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmm-prio/bevtrack">https://github.com/xmm-prio/bevtrack</a></li>
<li>paper_authors: Yuxiang Yang, Yingqi Deng, Jiahao Nie, Jing Zhang</li>
<li>for: 3D single object tracking (SOT) in point clouds, specifically in autonomous driving scenarios where target objects maintain spatial adjacency across frames.</li>
<li>methods: converts consecutive point clouds into Bird’s-Eye View representation, encodes spatial proximity and captures motion cues via simple element-wise operation and convolutional layers, and directly learns the underlying motion distribution without making assumptions.</li>
<li>results: achieves state-of-the-art performance on KITTI and NuScenes datasets with a high inference speed of 122 FPS.<details>
<summary>Abstract</summary>
3D single object tracking (SOT) in point clouds is still a challenging problem due to appearance variation, distractors, and high sparsity of point clouds. Notably, in autonomous driving scenarios, the target object typically maintains spatial adjacency across consecutive frames, predominantly moving horizontally. This spatial continuity offers valuable prior knowledge for target localization. However, existing trackers, which often employ point-wise representations, struggle to efficiently utilize this knowledge owing to the irregular format of such representations. Consequently, they require elaborate designs and solving multiple subtasks to establish spatial correspondence. In this paper, we introduce BEVTrack, a simple yet strong baseline framework for 3D SOT. After converting consecutive point clouds into the common Bird's-Eye View representation, BEVTrack inherently encodes spatial proximity and adeptly captures motion cues for tracking via a simple element-wise operation and convolutional layers. Additionally, to better deal with objects having diverse sizes and moving patterns, BEVTrack directly learns the underlying motion distribution rather than making a fixed Laplacian or Gaussian assumption as in previous works. Without bells and whistles, BEVTrack achieves state-of-the-art performance on KITTI and NuScenes datasets while maintaining a high inference speed of 122 FPS. The code will be released at https://github.com/xmm-prio/BEVTrack.
</details>
<details>
<summary>摘要</summary>
三元素 объек tracking (SOT) in point clouds 仍然是一个挑战，主要因为外观变化、干扰和点云的稀疏性。值得注意的是，在自动驾驶场景中，目标对象通常在连续帧中保持空间邻近，主要在水平方向上移动。这种空间继续性提供了有价值的先知知识 для目标位置确定。然而，现有的跟踪器，通常使用点 wise 表示，困难减少这种知识，因为点云的不规则格式。因此，它们需要较复杂的设计和解决多个子任务来确立空间匹配。在这篇论文中，我们介绍了 BEVTrack，一个简单却强大的基eline框架 для 3D SOT。将 consecutive point clouds 转化为共同 Bird's-Eye View 表示后，BEVTrack 自然地编码了空间 proximity 并善于捕捉运动指示符，通过简单的元素 wise 操作和卷积层来跟踪。此外，为了更好地处理具有不同尺寸和移动模式的对象，BEVTrack 直接学习下流动分布而不是在前一些作品中做固定 Laplacian 或 Gaussian 假设。无论各种饰物，BEVTrack  achieve state-of-the-art 性能在 KITTI 和 NuScenes 数据集上，并保持高速推理速度为 122 FPS。代码将在 https://github.com/xmm-prio/BEVTrack 上发布。
</details></li>
</ul>
<hr>
<h2 id="Dual-Relation-Alignment-for-Composed-Image-Retrieval"><a href="#Dual-Relation-Alignment-for-Composed-Image-Retrieval" class="headerlink" title="Dual Relation Alignment for Composed Image Retrieval"></a>Dual Relation Alignment for Composed Image Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02169">http://arxiv.org/abs/2309.02169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xintong Jiang, Yaxiong Wang, Yujiao Wu, Meng Wang, Xueming Qian<br>for: 本研究旨在提高组合图像检索性能，通过融合两种关系：Explicit Relation（图像参考和补充文本）和Implicit Relation（图像参考和目标图像）。methods: 我们提出了一种新的框架，称为双关系对齐，它将Explicit Relation和Implicit Relation完全融合，以便充分利用这些对象之间的相互关系。我们设计了一个视觉混合器，将参考图像和目标图像 fusion，然后将结果表示作两种角色：（1）对Semantic Alignment with 补充文本进行对应，（2）为Explicit Relation模型增强。results: 我们在CIRR和FashionIQ两个Popular Dataset上进行了广泛的实验，结果表明我们的双关系学习方法可以明显提高组合图像检索性能。<details>
<summary>Abstract</summary>
Composed image retrieval, a task involving the search for a target image using a reference image and a complementary text as the query, has witnessed significant advancements owing to the progress made in cross-modal modeling. Unlike the general image-text retrieval problem with only one alignment relation, i.e., image-text, we argue for the existence of two types of relations in composed image retrieval. The explicit relation pertains to the reference image & complementary text-target image, which is commonly exploited by existing methods. Besides this intuitive relation, the observations during our practice have uncovered another implicit yet crucial relation, i.e., reference image & target image-complementary text, since we found that the complementary text can be inferred by studying the relation between the target image and the reference image. Regrettably, existing methods largely focus on leveraging the explicit relation to learn their networks, while overlooking the implicit relation. In response to this weakness, We propose a new framework for composed image retrieval, termed dual relation alignment, which integrates both explicit and implicit relations to fully exploit the correlations among the triplets. Specifically, we design a vision compositor to fuse reference image and target image at first, then the resulted representation will serve two roles: (1) counterpart for semantic alignment with the complementary text and (2) compensation for the complementary text to boost the explicit relation modeling, thereby implant the implicit relation into the alignment learning. Our method is evaluated on two popular datasets, CIRR and FashionIQ, through extensive experiments. The results confirm the effectiveness of our dual-relation learning in substantially enhancing composed image retrieval performance.
</details>
<details>
<summary>摘要</summary>
新型图像检索任务：基于参考图像和补充文本的目标图像检索，受到跨模型的进步所见证。不同于一般的图像文本检索问题，我们认为图像检索任务存在两种关系：一种是明确的关系，即参考图像和补充文本-目标图像，这种关系通常被现有方法利用。此外，我们在实践中发现了一种隐式 yet crucial 的关系，即参考图像和目标图像-补充文本，因为我们发现了补充文本可以通过研究参考图像和目标图像之间的关系来推导。然而，现有方法主要是通过明确的关系来学习其网络。为了解决这个弱点，我们提出了一种新的框架，即双关系协调，这种框架将明确和隐式关系完全利用，以便充分利用参考图像、目标图像和补充文本之间的相关性。我们设计了一个视觉笔记，用于将参考图像和目标图像 fusion，然后得到的表示将扮演两个角色：（1）对应文本的 semantic alignment 和（2）用于强化明确关系模型，以便在 alignment 学习中嵌入隐式关系。我们的方法在 CIRR 和 FashionIQ 两个流行的数据集上进行了广泛的实验，结果证明了我们的双关系学习在图像检索性能上具有显著提高的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Imitation-Learning-Algorithms-Recent-Developments-and-Challenges"><a href="#A-Survey-of-Imitation-Learning-Algorithms-Recent-Developments-and-Challenges" class="headerlink" title="A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges"></a>A Survey of Imitation Learning: Algorithms, Recent Developments, and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02473">http://arxiv.org/abs/2309.02473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maryam Zare, Parham M. Kebria, Abbas Khosravi, Saeid Nahavandi</li>
<li>For: This paper provides an introduction to imitation learning (IL) and offers an overview of its underlying assumptions and approaches in the context of robotics and artificial intelligence (AI).* Methods: The paper discusses recent advances and emerging areas of research in IL, including the use of demonstrations to learn desired behavior, and addresses common challenges associated with IL.* Results: The paper provides a comprehensive guide to the growing field of IL in robotics and AI, including potential directions for future research.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文是提供对人工智能和机器人领域内的学习模式的引入和概述，包括学习从专家示例的方法。</li>
<li>methods: 论文讨论了现有的IL技术和新兴领域的研究，以及面临IL的常见挑战。</li>
<li>results: 论文提供了人工智能和机器人领域内IL的总结和未来研究方向。<details>
<summary>Abstract</summary>
In recent years, the development of robotics and artificial intelligence (AI) systems has been nothing short of remarkable. As these systems continue to evolve, they are being utilized in increasingly complex and unstructured environments, such as autonomous driving, aerial robotics, and natural language processing. As a consequence, programming their behaviors manually or defining their behavior through reward functions (as done in reinforcement learning (RL)) has become exceedingly difficult. This is because such environments require a high degree of flexibility and adaptability, making it challenging to specify an optimal set of rules or reward signals that can account for all possible situations. In such environments, learning from an expert's behavior through imitation is often more appealing. This is where imitation learning (IL) comes into play - a process where desired behavior is learned by imitating an expert's behavior, which is provided through demonstrations.   This paper aims to provide an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, the paper discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research. Overall, the goal of the paper is to provide a comprehensive guide to the growing field of IL in robotics and AI.
</details>
<details>
<summary>摘要</summary>
This paper provides an introduction to IL and an overview of its underlying assumptions and approaches. It also offers a detailed description of recent advances and emerging areas of research in the field. Additionally, the paper discusses how researchers have addressed common challenges associated with IL and provides potential directions for future research. The goal of the paper is to provide a comprehensive guide to the growing field of IL in robotics and AI.
</details></li>
</ul>
<hr>
<h2 id="Model-based-Offline-Policy-Optimization-with-Adversarial-Network"><a href="#Model-based-Offline-Policy-Optimization-with-Adversarial-Network" class="headerlink" title="Model-based Offline Policy Optimization with Adversarial Network"></a>Model-based Offline Policy Optimization with Adversarial Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02157">http://arxiv.org/abs/2309.02157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junming-yang/moan">https://github.com/junming-yang/moan</a></li>
<li>paper_authors: Junming Yang, Xingguo Chen, Shengyuan Wang, Bolei Zhang</li>
<li>for: 提出了一种基于模型的线上权限学习（RL）方法，以避免在线环境中的成本交互，并且可以在离线数据集上进行策略优化。</li>
<li>methods: 使用了对抗学习来建立一个更好的逻辑分布，并通过对抗网络来提供模型的不确定性量化。</li>
<li>results: 比较了现有的基于模型的离线RL方法，并取得了更高的性能和更准确的不确定性量化。<details>
<summary>Abstract</summary>
Model-based offline reinforcement learning (RL), which builds a supervised transition model with logging dataset to avoid costly interactions with the online environment, has been a promising approach for offline policy optimization. As the discrepancy between the logging data and online environment may result in a distributional shift problem, many prior works have studied how to build robust transition models conservatively and estimate the model uncertainty accurately. However, the over-conservatism can limit the exploration of the agent, and the uncertainty estimates may be unreliable. In this work, we propose a novel Model-based Offline policy optimization framework with Adversarial Network (MOAN). The key idea is to use adversarial learning to build a transition model with better generalization, where an adversary is introduced to distinguish between in-distribution and out-of-distribution samples. Moreover, the adversary can naturally provide a quantification of the model's uncertainty with theoretical guarantees. Extensive experiments showed that our approach outperforms existing state-of-the-art baselines on widely studied offline RL benchmarks. It can also generate diverse in-distribution samples, and quantify the uncertainty more accurately.
</details>
<details>
<summary>摘要</summary>
模型基于的线上强化学习（RL），通过使用日志数据建立一个监督式过渡模型，以避免在线环境中的成本性交互，已经是无线环境中的一种有前途的方法。然而， logging 数据和在线环境之间的差异可能会导致分布性Shift问题，许多前作都研究了如何建立保守的过渡模型和准确地估计模型的不确定性。然而，过于保守的建模可能会限制 agent 的探索，而估计的不确定性可能是不可靠的。在这个工作中，我们提出了一种基于 Model-based Offline policy optimization 框架的 Adversarial Network (MOAN)。关键思想是使用对抗学习建立一个更好的泛化过渡模型，其中一个对手可以分辨在 Distribution 和 Out-of-Distribution 样本之间。此外，对手还可以自然地提供一个量化的模型不确定性的理论保证。我们的方法在 widely  studied 的 offline RL 标准准样本上进行了广泛的实验，并显示了我们的方法在性能和多样性方面的超过现有基eline。它还可以更准确地量化不确定性。
</details></li>
</ul>
<hr>
<h2 id="Making-Large-Language-Models-Better-Reasoners-with-Alignment"><a href="#Making-Large-Language-Models-Better-Reasoners-with-Alignment" class="headerlink" title="Making Large Language Models Better Reasoners with Alignment"></a>Making Large Language Models Better Reasoners with Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02144">http://arxiv.org/abs/2309.02144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, Zhifang Sui</li>
<li>for: 提升大语言模型（LLM）的理解能力，特别是在链式思维（COT）理解过程中。</li>
<li>methods: 通过对 LLM 进行特定的微调，使其在 COT 理解过程中提高其理解能力。</li>
<li>results: 通过实施新的对齐练习（AFT）方法，可以有效地解决 LLM 在 COT 理解过程中存在的评价不一致问题，并提高其理解能力。<details>
<summary>Abstract</summary>
Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textit{Assessment Misalignment} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \textit{Alignment Fine-Tuning (AFT)} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.
</details>
<details>
<summary>摘要</summary>
理智是认知过程中的证据使用，以达到正确的结论。理智能力是人工通用智能代理人的关键能力。现代研究表明，对大语言模型（LLM）进行适应过程可以显著提高其理智能力。然而，我们发现，经过精度调整后的LLM受到了评价不一致（Assessment Misalignment）问题的限制，即它们经常对低质量的链条思维（COT）进行高分评价，可能导致其理智能力受到限制。为解决这个问题，我们提出了一种Alignment Fine-Tuning（AFT）方法，包括以下三步：1）对LLM进行COT训练数据的精度调整；2）为每个问题生成多个COT响应，并将它们分为正确和错误的两类 based on whether they achieve the correct answer; 3）对LLM对正确和错误响应的分配分数进行Calibration，使其符合一个新的约束Alignmentloss。具体来说，Alignmentloss有两个目标：a）Alignment，确保正确的分数高于错误的分数，以鼓励高质量的COT; b）Constraint，使错误的分数尽可能地受限，以避免模型下降。此外，我们还发现，当有排名反馈时，这种约束可以轻松地适应到排名情况下。此外，我们还对最近的排名基于Alignment方法，如DPO、RRHF和PRO进行了深入研究，发现，这种约束也是这些方法的关键因素。我们在四个理智benchmark上进行了广泛的实验，并证明了AFT的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Rapid-and-Efficient-Deep-Convolutional-Network-for-Chest-X-Ray-Tuberculosis-Detection"><a href="#A-Lightweight-Rapid-and-Efficient-Deep-Convolutional-Network-for-Chest-X-Ray-Tuberculosis-Detection" class="headerlink" title="A Lightweight, Rapid and Efficient Deep Convolutional Network for Chest X-Ray Tuberculosis Detection"></a>A Lightweight, Rapid and Efficient Deep Convolutional Network for Chest X-Ray Tuberculosis Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02140">http://arxiv.org/abs/2309.02140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dani-capellan/LightTBNet">https://github.com/dani-capellan/LightTBNet</a></li>
<li>paper_authors: Daniel Capellán-Martín, Juan J. Gómez-Valverde, David Bermejo-Peláez, María J. Ledesma-Carbayo</li>
<li>for: 您的论文旨在提高肺部X射线图像的诊断精度，减少误判。</li>
<li>methods: 您使用了深度学习技术，开发了一种特制的轻量级、快速、计算效率低的快速预测模型，以提高肺部X射线图像的诊断精度。</li>
<li>results: 您的模型在独立测试集上达到了0.906、0.907和0.961的准确率、F1分数和ROC曲线值，表明模型在诊断肺部TB的能力强，并且具有快速预测和低计算和存储需求，适用于在低TB发病地区使用。<details>
<summary>Abstract</summary>
Tuberculosis (TB) is still recognized as one of the leading causes of death worldwide. Recent advances in deep learning (DL) have shown to enhance radiologists' ability to interpret chest X-ray (CXR) images accurately and with fewer errors, leading to a better diagnosis of this disease. However, little work has been done to develop models capable of diagnosing TB that offer good performance while being efficient, fast and computationally inexpensive. In this work, we propose LightTBNet, a novel lightweight, fast and efficient deep convolutional network specially customized to detect TB from CXR images. Using a total of 800 frontal CXR images from two publicly available datasets, our solution yielded an accuracy, F1 and area under the ROC curve (AUC) of 0.906, 0.907 and 0.961, respectively, on an independent test subset. The proposed model demonstrates outstanding performance while delivering a rapid prediction, with minimal computational and memory requirements, making it highly suitable for deployment in handheld devices that can be used in low-resource areas with high TB prevalence. Code publicly available at https://github.com/dani-capellan/LightTBNet.
</details>
<details>
<summary>摘要</summary>
肺炎病毒 (TB) 仍然被认为全球主要的死亡原因之一。latest advances in deep learning (DL) 已经显示了改善医生解读胸部X射线 (CXR) 像素的能力，导致更好的这病的诊断。然而， little work has been done to develop models capable of diagnosing TB that offer good performance while being efficient, fast and computationally inexpensive. In this work, we propose LightTBNet, a novel lightweight, fast and efficient deep convolutional network specially customized to detect TB from CXR images. Using a total of 800 frontal CXR images from two publicly available datasets, our solution yielded an accuracy, F1 and area under the ROC curve (AUC) of 0.906, 0.907 and 0.961, respectively, on an independent test subset. The proposed model demonstrates outstanding performance while delivering a rapid prediction, with minimal computational and memory requirements, making it highly suitable for deployment in handheld devices that can be used in low-resource areas with high TB prevalence. Code publicly available at https://github.com/dani-capellan/LightTBNet.Note: Please note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Pre-Training-Boosts-Semantic-Scene-Segmentation-on-LiDAR-data"><a href="#Self-Supervised-Pre-Training-Boosts-Semantic-Scene-Segmentation-on-LiDAR-data" class="headerlink" title="Self-Supervised Pre-Training Boosts Semantic Scene Segmentation on LiDAR data"></a>Self-Supervised Pre-Training Boosts Semantic Scene Segmentation on LiDAR data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02139">http://arxiv.org/abs/2309.02139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marionacaros/barlow-twins-for-sem-seg">https://github.com/marionacaros/barlow-twins-for-sem-seg</a></li>
<li>paper_authors: Mariona Carós, Ariadna Just, Santi Seguí, Jordi Vitrià</li>
<li>for: 本研究旨在实现从无标注数据中学习Semantic Scene Segmentation（实物景像分类），以减少需要标注的数据量。</li>
<li>methods: 本研究使用Barlow Twins自我超vised encoder进行预训练，并将其用作实物景像分类任务中的预训练网络。</li>
<li>results: 实验结果显示，我们的无标注预训练策略可以增加实物景像分类任务中的表现，特别是对于少数类别的表现。<details>
<summary>Abstract</summary>
Airborne LiDAR systems have the capability to capture the Earth's surface by generating extensive point cloud data comprised of points mainly defined by 3D coordinates. However, labeling such points for supervised learning tasks is time-consuming. As a result, there is a need to investigate techniques that can learn from unlabeled data to significantly reduce the number of annotated samples. In this work, we propose to train a self-supervised encoder with Barlow Twins and use it as a pre-trained network in the task of semantic scene segmentation. The experimental results demonstrate that our unsupervised pre-training boosts performance once fine-tuned on the supervised task, especially for under-represented categories.
</details>
<details>
<summary>摘要</summary>
空中探测LiDAR系统可以捕捉地球表面，生成大量的点云数据，主要由3D坐标定义。但标注这些点云数据用于监督学习任务是时间消耗大。因此，我们需要研究如何从无标注数据中学习，以大幅减少需要标注的样本数量。在这个工作中，我们提议使用自我监督编码器和Barlow Twins进行预训练，并将其作为semantic scene segmentation任务的预训练网络。实验结果表明，我们的无监督预训练可以大幅提高任务的性能，尤其是对于少数概率类别。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Simplicial-Attention-Neural-Networks"><a href="#Generalized-Simplicial-Attention-Neural-Networks" class="headerlink" title="Generalized Simplicial Attention Neural Networks"></a>Generalized Simplicial Attention Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02138">http://arxiv.org/abs/2309.02138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luciatesta97/generalized-simplicial-attention-neural-networks">https://github.com/luciatesta97/generalized-simplicial-attention-neural-networks</a></li>
<li>paper_authors: Claudio Battiloro, Lucia Testa, Lorenzo Giusti, Stefania Sardellitti, Paolo Di Lorenzo, Sergio Barbarossa</li>
<li>for: 本研究旨在介绍通用 simplicial 注意力神经网络 (GSAN)，即利用面积掩码自我注意 layers 处理定义在 simplicial 复合体上的数据的新神经网络体系。</li>
<li>methods: 作者提出了一系列基于 topological signal processing 原理的自我注意方案，可以处理不同 simplicial 顺序上的数据组件，如节点、边、三角形等，并通过 Dirac 算子和其分解学习对 simplicial 领域的邻居重要性进行权重。</li>
<li>results: 作者证明了 GSAN 具有交换对称和 simplicial 意识，并通过应用于多个（推导和推理）任务，如 trajectory prediction、缺失数据填充、图 классификация和 simplex prediction，与其他方法进行比较，得到了比较好的结果。<details>
<summary>Abstract</summary>
The aim of this work is to introduce Generalized Simplicial Attention Neural Networks (GSANs), i.e., novel neural architectures designed to process data defined on simplicial complexes using masked self-attentional layers. Hinging on topological signal processing principles, we devise a series of self-attention schemes capable of processing data components defined at different simplicial orders, such as nodes, edges, triangles, and beyond. These schemes learn how to weight the neighborhoods of the given topological domain in a task-oriented fashion, leveraging the interplay among simplices of different orders through the Dirac operator and its Dirac decomposition. We also theoretically establish that GSANs are permutation equivariant and simplicial-aware. Finally, we illustrate how our approach compares favorably with other methods when applied to several (inductive and transductive) tasks such as trajectory prediction, missing data imputation, graph classification, and simplex prediction.
</details>
<details>
<summary>摘要</summary>
文章的目的是介绍通用 simplicial 注意力神经网络（GSAN），即新的神经网络架构，用于处理定义在 simplicial 复合体上的数据，使用假自注意层。基于 topological signal processing 原则，我们设计了一系列自注意方案，可以处理不同 simplicial 顺序的数据组件，如节点、边、triangle 等。这些方案可以Weight neighborhoods of the given topological domain in a task-oriented fashion，利用不同 simplices 之间的交互，通过 Дирак算符和其 Дирак分解。我们还证明了 GSANs 是 permutation equivariant 和 simplicial-aware。最后，我们比较了我们的方法与其他方法在 inductive 和 transductive 任务上的性能，包括 trajectory prediction、missing data imputation、graph classification 和 simplex prediction。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Intersection-of-Complex-Aesthetics-and-Generative-AI-for-Promoting-Cultural-Creativity-in-Rural-China-after-the-Post-Pandemic-Era"><a href="#Exploring-the-Intersection-of-Complex-Aesthetics-and-Generative-AI-for-Promoting-Cultural-Creativity-in-Rural-China-after-the-Post-Pandemic-Era" class="headerlink" title="Exploring the Intersection of Complex Aesthetics and Generative AI for Promoting Cultural Creativity in Rural China after the Post-Pandemic Era"></a>Exploring the Intersection of Complex Aesthetics and Generative AI for Promoting Cultural Creativity in Rural China after the Post-Pandemic Era</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02136">http://arxiv.org/abs/2309.02136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengyao Guo, Xiaolin Zhang, Yuan Zhuang, Jing Chen, Pengfei Wang, Ze Gao</li>
<li>for: 这个论文探讨了在中国农村地区使用生成AI和艺术来促进文化创新，尤其是在COVID-19的影响下。</li>
<li>methods: 该论文通过文献综述、案例研究、问卷调查和文本分析方法来研究艺术和科技在农村 context中的应用，并找到了关键的挑战。</li>
<li>results: 研究发现艺术作品经常无法在当地 resonate，而依赖于外部艺术家的支持限制了可持续性。因此，抚养“村村艺术家”通过AI被提议。我们的方法是通过对主观美学进行机器学习训练，生成文化相关的内容。交互式AI媒体还可以提高旅游业，保护遗产。这项先导性的研究提出了对AI和艺术的交互关系的新视角，并强调AI的创作能力 versus 取代性。最后，它为了使用AI创新来促进农村社区的发展奠定了基础。<details>
<summary>Abstract</summary>
This paper explores using generative AI and aesthetics to promote cultural creativity in rural China amidst COVID-19's impact. Through literature reviews, case studies, surveys, and text analysis, it examines art and technology applications in rural contexts and identifies key challenges. The study finds artworks often fail to resonate locally, while reliance on external artists limits sustainability. Hence, nurturing grassroots "artist villagers" through AI is proposed. Our approach involves training machine learning on subjective aesthetics to generate culturally relevant content. Interactive AI media can also boost tourism while preserving heritage. This pioneering research puts forth original perspectives on the intersection of AI and aesthetics to invigorate rural culture. It advocates holistic integration of technology and emphasizes AI's potential as a creative enabler versus replacement. Ultimately, it lays the groundwork for further exploration of leveraging AI innovations to empower rural communities. This timely study contributes to growing interest in emerging technologies to address critical issues facing rural China.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multi-label-affordance-mapping-from-egocentric-vision"><a href="#Multi-label-affordance-mapping-from-egocentric-vision" class="headerlink" title="Multi-label affordance mapping from egocentric vision"></a>Multi-label affordance mapping from egocentric vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02120">http://arxiv.org/abs/2309.02120</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lmur98/epic_kitchens_affordances">https://github.com/lmur98/epic_kitchens_affordances</a></li>
<li>paper_authors: Lorenzo Mur-Labadia, Jose J. Guerrero, Ruben Martinez-Cantin</li>
<li>for: 本研究旨在提供高精度的交互场景中的可用性检测和分割方法，用于支持人工智能系统的发展。</li>
<li>methods: 本研究使用新的多标签检测方法，可以准确地检测和分割多个可用性在同一个空间中。</li>
<li>results: 研究人员通过使用多标签检测方法，成功地提取了高精度的交互场景中的可用性信息，并构建了大量和完整的交互可用性数据集（EPIC-Aff）。此外，研究人员还提出了一种新的多标签检测方法，可以处理多个可用性同时存在同一个空间中的情况。<details>
<summary>Abstract</summary>
Accurate affordance detection and segmentation with pixel precision is an important piece in many complex systems based on interactions, such as robots and assitive devices. We present a new approach to affordance perception which enables accurate multi-label segmentation. Our approach can be used to automatically extract grounded affordances from first person videos of interactions using a 3D map of the environment providing pixel level precision for the affordance location. We use this method to build the largest and most complete dataset on affordances based on the EPIC-Kitchen dataset, EPIC-Aff, which provides interaction-grounded, multi-label, metric and spatial affordance annotations. Then, we propose a new approach to affordance segmentation based on multi-label detection which enables multiple affordances to co-exists in the same space, for example if they are associated with the same object. We present several strategies of multi-label detection using several segmentation architectures. The experimental results highlight the importance of the multi-label detection. Finally, we show how our metric representation can be exploited for build a map of interaction hotspots in spatial action-centric zones and use that representation to perform a task-oriented navigation.
</details>
<details>
<summary>摘要</summary>
重要的一部分是许多复杂系统中的互动，如 робоット和协助设备。我们提出了一个新的方法来检测和分类可用性，可以从首人视频中自动提取固定的可用性。我们使用这种方法建立了最大和最完整的可用性数据集，EPIC-Aff，其提供了互动基于环境的三维地图，以像素精度检测可用性位置。接下来，我们提出了一个新的可用性分类方法，可以同时检测多个可用性，例如它们与同一个物品相关。我们提出了多种多label检测方法，包括多个分类架构。实验结果显示了多label检测的重要性。最后，我们显示了如何使用我们的度量表示法建立互动热点地图，并使用该表示法进行任务导向的探索。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Label-Information-for-Multimodal-Emotion-Recognition"><a href="#Leveraging-Label-Information-for-Multimodal-Emotion-Recognition" class="headerlink" title="Leveraging Label Information for Multimodal Emotion Recognition"></a>Leveraging Label Information for Multimodal Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02106">http://arxiv.org/abs/2309.02106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Digimonseeker/LE-MER">https://github.com/Digimonseeker/LE-MER</a></li>
<li>paper_authors: Peiying Wang, Sunlu Zeng, Junqing Chen, Lu Fan, Meng Chen, Youzheng Wu, Xiaodong He</li>
<li>for: 本研究旨在提高多模态情感识别（MER）的性能，通过结合语音和文本信息。</li>
<li>methods: 我们提出了一种新的方法，利用标签信息来提高MER的性能。我们首先获取语音和文本模态的表示性标签嵌入，然后通过标签-token和标签-帧交互来学习每个语音的标签感知表示。最后，我们提出了一种新的标签导向拟合模块，将标签意识文本和语音表示进行情感分类。</li>
<li>results: 我们在公共的IEMOCAP dataset上进行了广泛的实验，结果表明，我们的提议的方法在比较基eline和现有方法的情况下，实现了新的国际顶点性能。<details>
<summary>Abstract</summary>
Multimodal emotion recognition (MER) aims to detect the emotional status of a given expression by combining the speech and text information. Intuitively, label information should be capable of helping the model locate the salient tokens/frames relevant to the specific emotion, which finally facilitates the MER task. Inspired by this, we propose a novel approach for MER by leveraging label information. Specifically, we first obtain the representative label embeddings for both text and speech modalities, then learn the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions. Finally, we devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations for emotion classification. Extensive experiments were conducted on the public IEMOCAP dataset, and experimental results demonstrate that our proposed approach outperforms existing baselines and achieves new state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
多Modal情感识别（MER）目标是通过 Speech 和文本信息检测表达的情感状态。直觉地，标签信息应该能够帮助模型定位特定情感的关键词/帧，从而实现MER任务。 inspirited by这个想法，我们提出了一种新的MER方法，利用标签信息。具体来说，我们首先获得文本和Speech模态的表示性标签嵌入，然后通过标签-token和标签-帧交互学习每个语音的标签感知表示。最后，我们设计了一种新的标签引导束合模块，将标签意识的文本和Speech表示进行情感分类。我们在公共的IEMOCAP数据集上进行了广泛的实验，实验结果表明，我们提出的方法比现有的基eline和实现新的状态。
</details></li>
</ul>
<hr>
<h2 id="Improving-Query-Focused-Meeting-Summarization-with-Query-Relevant-Knowledge"><a href="#Improving-Query-Focused-Meeting-Summarization-with-Query-Relevant-Knowledge" class="headerlink" title="Improving Query-Focused Meeting Summarization with Query-Relevant Knowledge"></a>Improving Query-Focused Meeting Summarization with Query-Relevant Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02105">http://arxiv.org/abs/2309.02105</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Tiezheng Yu, Ziwei Ji, Pascale Fung</li>
<li>for: 这篇论文旨在提供一个可以根据查询生成会议总结的方法。</li>
<li>methods: 本文提出了一个知识增强的两阶段框架，名为知识感知SUMmarizer (KAS)，以解决长Input文本长度和会议总结中罕见的查询相关信息的问题。在第一阶段，我们引入知识感知分析来提高查询相关段别的提取精度。在第二阶段，我们将查询相关知识 integrate到总结生成中。</li>
<li>results: 实验结果显示，我们的方法在QMSum dataset上实现了现有最佳性能。进一步的分析显示，我们的方法能够生成相关 faithful和有用的总结。<details>
<summary>Abstract</summary>
Query-Focused Meeting Summarization (QFMS) aims to generate a summary of a given meeting transcript conditioned upon a query. The main challenges for QFMS are the long input text length and sparse query-relevant information in the meeting transcript. In this paper, we propose a knowledge-enhanced two-stage framework called Knowledge-Aware Summarizer (KAS) to tackle the challenges. In the first stage, we introduce knowledge-aware scores to improve the query-relevant segment extraction. In the second stage, we incorporate query-relevant knowledge in the summary generation. Experimental results on the QMSum dataset show that our approach achieves state-of-the-art performance. Further analysis proves the competency of our methods in generating relevant and faithful summaries.
</details>
<details>
<summary>摘要</summary>
Query-Focused Meeting Summarization (QFMS) 目标是根据查询生成会议笔记摘要。主要挑战是输入文本长度较长，会议笔记中关键信息罕见。在这篇论文中，我们提出了知识增强的两Stage框架，称为知识感知摘要器（KAS），以解决这些挑战。首先，我们在查询相关段落提取中引入了知识感知分数。然后，我们在摘要生成过程中引入了查询相关知识。实验结果表明，我们的方法在 QMSum 数据集上达到了顶尖性能。进一步分析表明，我们的方法能够生成相关和准确的摘要。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Superquadric-Recomposition-of-3D-Objects-from-Multiple-Views"><a href="#Iterative-Superquadric-Recomposition-of-3D-Objects-from-Multiple-Views" class="headerlink" title="Iterative Superquadric Recomposition of 3D Objects from Multiple Views"></a>Iterative Superquadric Recomposition of 3D Objects from Multiple Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02102">http://arxiv.org/abs/2309.02102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/explainableml/isco">https://github.com/explainableml/isco</a></li>
<li>paper_authors: Stephan Alaniz, Massimiliano Mancini, Zeynep Akata</li>
<li>for: 本研究旨在提出一种描述对象的概念模型，帮助机器学习模型更好地理解和重建物体的三维结构。</li>
<li>methods: 该方法使用3D超quadrics作为semantic part来直接从2D视图中重建物体，而不需要训练任何3Dsupervision模型。该方法通过优化超quadrics参数，以实现高精度的3D重建。</li>
<li>results: 实验表明，相比最近的单个实例超quadrics重建方法，ISCO方法能够提供更高精度的3D重建结果，即使是从野生图像中。代码可以在<a target="_blank" rel="noopener" href="https://github.com/ExplainableML/ISCO%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/ExplainableML/ISCO上下载。</a><details>
<summary>Abstract</summary>
Humans are good at recomposing novel objects, i.e. they can identify commonalities between unknown objects from general structure to finer detail, an ability difficult to replicate by machines. We propose a framework, ISCO, to recompose an object using 3D superquadrics as semantic parts directly from 2D views without training a model that uses 3D supervision. To achieve this, we optimize the superquadric parameters that compose a specific instance of the object, comparing its rendered 3D view and 2D image silhouette. Our ISCO framework iteratively adds new superquadrics wherever the reconstruction error is high, abstracting first coarse regions and then finer details of the target object. With this simple coarse-to-fine inductive bias, ISCO provides consistent superquadrics for related object parts, despite not having any semantic supervision. Since ISCO does not train any neural network, it is also inherently robust to out-of-distribution objects. Experiments show that, compared to recent single instance superquadrics reconstruction approaches, ISCO provides consistently more accurate 3D reconstructions, even from images in the wild. Code available at https://github.com/ExplainableML/ISCO .
</details>
<details>
<summary>摘要</summary>
人类具有将新物体复制成已知结构的能力，即可以从总结构到细节上识别未知物体的共同点，这是机器不易复制的能力。我们提出了一个框架，即ISCO，可以通过直接从2D视图中提取3D超quadric作为semantic part来重新组合物体。为了实现这一点，我们优化了超quadric参数，以使其能够组合特定物体的实例，并比较其渲染的3D视图和2D图像轮廓。我们的ISCO框架会逐渐添加新的超quadric，以降低重建错误，从总体到细节地抽象物体的target part。由于ISCO没有任何semantic supervision，它具有简单的卷积偏好，可以适应各种不同的物体。此外，由于ISCO不需要训练任何神经网络，它也是对外部数据集的抗耗性的。实验显示，相比最近的单个实例超quadrics重建方法，ISCO可以提供更加准确的3D重建结果，甚至来自野外图像。代码可以在https://github.com/ExplainableML/ISCO上获取。
</details></li>
</ul>
<hr>
<h2 id="TensorBank-Tensor-Lakehouse-for-Foundation-Model-Training"><a href="#TensorBank-Tensor-Lakehouse-for-Foundation-Model-Training" class="headerlink" title="TensorBank:Tensor Lakehouse for Foundation Model Training"></a>TensorBank:Tensor Lakehouse for Foundation Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02094">http://arxiv.org/abs/2309.02094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romeo Kienzler, Benedikt Blumenstiel, Zoltan Arnold Nagy, S. Karthik Mukkavilli, Johannes Schmude, Marcus Freitag, Michael Behrendt, Daniel Salles Civitarese, Naomi Simumba, Daiki Kimura, Hendrik Hamann</li>
<li>For: 用于训练基础模型的高维数据存储和流处理成为现代自然语言之外的核心需求。* Methods: 使用复杂关系查询加速 Hierarchical Statistical Indices (HSI) 来从Cloud Object Store (COS) 流动到 GPU 内存中的tensor lakehouse。* Results: 可以通过 direktly 地址tensor的块级别使用 HTTP 范围读取来快速地从Cloud Object Store (COS) 流动tensor到 GPU 内存中，并使用 PyTorch 转换来转换数据。<details>
<summary>Abstract</summary>
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making heavy use of open-source technology. Although, hardened for production use using geospatial-temporal data, this architecture generalizes to other use case like computer vision, computational neuroscience, biological sequence analysis and more.
</details>
<details>
<summary>摘要</summary>
存储和流动高维数据 для基础模型训练成为了基础模型的发展的关键要求。在这篇论文中，我们介绍TensorBank，一个 petabyte 级 tensor 湖居 capable of streaming tensors from Cloud Object Store (COS) to GPU 内存 based on complex relational queries。我们使用 Hierarchical Statistical Indices (HSI) for query acceleration。我们的架构允许直接地址 tensors 在块级别使用 HTTP 范围读。一旦在 GPU 内存中，数据可以通过 PyTorch 转换。我们提供一个通用 PyTorch 数据集类型，并提供一个对应的数据工厂，该工厂将关系查询和请求的转换翻译为实例。通过使用 HSI，我们可以跳过无关块，因为它们包含不同层次分辨率水平上的统计信息。这是一种基于开源技术的意见 arquitecture，并且通过使用 geospatial-temporal 数据进行硬化，这种架构可以普及到其他应用场景，如计算机视觉、计算神经科学、生物序列分析等。
</details></li>
</ul>
<hr>
<h2 id="Dual-Adversarial-Alignment-for-Realistic-Support-Query-Shift-Few-shot-Learning"><a href="#Dual-Adversarial-Alignment-for-Realistic-Support-Query-Shift-Few-shot-Learning" class="headerlink" title="Dual Adversarial Alignment for Realistic Support-Query Shift Few-shot Learning"></a>Dual Adversarial Alignment for Realistic Support-Query Shift Few-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02088">http://arxiv.org/abs/2309.02088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyang Jiang, Rui Fang, Hsi-Wen Chen, Wei Ding, Ming-Syan Chen</li>
<li>For: 实际情况下，支持集和查询集之间存在多种不确定的扩散shift，这使得traditional的支持集查询集学习变得困难。这篇论文提出了一个新的挑战，即Realistic Support-Query Shift few-shot learning（RSQS），旨在在不确定的扩散shift下进行几个shot学习。* Methods: 我们提出了一种新的对抗性特征平衡方法called DUal adversarial ALignment framework（DuaL），用于缓解RSQS中的两种方面：inter-domain bias和intra-domain variance。一方面，我们在特定的数据集上进行了预处理，并使用生成的各种偏移输入来训练修复网络，以最小化特征层的距离。另一方面，我们提出了一种生成器网络，用于自顺序生成硬example，即less similar的支持集中的示例，并通过整合最优运输来获得一个平滑的运输计划。* Results: 我们建立了RSQS的benchmark，包括了several state-of-the-art baselines，并进行了实验研究。结果表明，DuaL在我们的benchmark中显著超过了state-of-the-art方法。<details>
<summary>Abstract</summary>
Support-query shift few-shot learning aims to classify unseen examples (query set) to labeled data (support set) based on the learned embedding in a low-dimensional space under a distribution shift between the support set and the query set. However, in real-world scenarios the shifts are usually unknown and varied, making it difficult to estimate in advance. Therefore, in this paper, we propose a novel but more difficult challenge, RSQS, focusing on Realistic Support-Query Shift few-shot learning. The key feature of RSQS is that the individual samples in a meta-task are subjected to multiple distribution shifts in each meta-task. In addition, we propose a unified adversarial feature alignment method called DUal adversarial ALignment framework (DuaL) to relieve RSQS from two aspects, i.e., inter-domain bias and intra-domain variance. On the one hand, for the inter-domain bias, we corrupt the original data in advance and use the synthesized perturbed inputs to train the repairer network by minimizing distance in the feature level. On the other hand, for intra-domain variance, we proposed a generator network to synthesize hard, i.e., less similar, examples from the support set in a self-supervised manner and introduce regularized optimal transportation to derive a smooth optimal transportation plan. Lastly, a benchmark of RSQS is built with several state-of-the-art baselines among three datasets (CIFAR100, mini-ImageNet, and Tiered-Imagenet). Experiment results show that DuaL significantly outperforms the state-of-the-art methods in our benchmark.
</details>
<details>
<summary>摘要</summary>
支持问题Shift几何学学习目标是将未经见过的示例（查询集）分类到已经标注的数据（支持集）基于学习得到的嵌入在低维度空间下，但在实际场景中，这些变化通常是未知且多样的，使得预测变化很困难。因此，在这篇论文中，我们提出了一个新的挑战，即真实支持问题Shift几何学学习（RSQS）。RSQS的关键特点是每个元任务中的个体样本会面临多种分布变化。此外，我们提出了一种整合式对抗特征对齐方法，即DUal adversarial ALignment framework（DuaL），以解决RSQS中的两个方面：间域偏见和内域变异。一方面，为了间域偏见，我们在提前损害原始数据后，使用生成的妨害输入来训练维护网络，并在特征层面下将其距离最小化。另一方面，为了内域变异，我们提出了一种生成器网络，通过自适应方式生成硬例（即更不相似的示例），并通过可惩正的优化运输来 derivation 一个平滑的优化运输计划。最后，我们建立了RSQS的标准准则，包括三个数据集（CIFAR100、mini-ImageNet和Tiered-Imagenet）上的state-of-the-art基elines。实验结果显示，DuaL明显超过了state-of-the-art方法在我们的标准准则中。
</details></li>
</ul>
<hr>
<h2 id="Natural-Example-Based-Explainability-a-Survey"><a href="#Natural-Example-Based-Explainability-a-Survey" class="headerlink" title="Natural Example-Based Explainability: a Survey"></a>Natural Example-Based Explainability: a Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03234">http://arxiv.org/abs/2309.03234</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danderfer/Comp_Sci_Sem_2">https://github.com/danderfer/Comp_Sci_Sem_2</a></li>
<li>paper_authors: Antonin Poché, Lucas Hervier, Mohamed-Chafik Bakkay</li>
<li>for: 本研究旨在提供一份对当前自然示例基于XAI技术的概述，以便了解不同方法的优缺点和应用场景。</li>
<li>methods: 本研究主要涉及的方法包括相似示例、 counterfactual 和 semi-factual 示例、重要实例、原型和概念。</li>
<li>results: 本研究提供了这些方法的 semantic definition、认知影响和added value的比较，以便激励和促进未来的自然示例基于XAI技术发展。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) has become increasingly significant for improving the interpretability and trustworthiness of machine learning models. While saliency maps have stolen the show for the last few years in the XAI field, their ability to reflect models' internal processes has been questioned. Although less in the spotlight, example-based XAI methods have continued to improve. It encompasses methods that use examples as explanations for a machine learning model's predictions. This aligns with the psychological mechanisms of human reasoning and makes example-based explanations natural and intuitive for users to understand. Indeed, humans learn and reason by forming mental representations of concepts based on examples.   This paper provides an overview of the state-of-the-art in natural example-based XAI, describing the pros and cons of each approach. A "natural" example simply means that it is directly drawn from the training data without involving any generative process. The exclusion of methods that require generating examples is justified by the need for plausibility which is in some regards required to gain a user's trust. Consequently, this paper will explore the following family of methods: similar examples, counterfactual and semi-factual, influential instances, prototypes, and concepts. In particular, it will compare their semantic definition, their cognitive impact, and added values. We hope it will encourage and facilitate future work on natural example-based XAI.
</details>
<details>
<summary>摘要</summary>
很好！我们将提供一个关于自然例子基于Explainable Artificial Intelligence（XAI）的概览，涵盖了不同方法的优点和缺点。XAI在机器学习模型的解释性和可信度方面具有越来越重要的地位。虽然在过去几年中，saliency map在XAI领域中受到了很多关注，但其能够反映机器学习模型内部过程的能力受到了质疑。在这篇文章中，我们将探讨不同的自然例子基于XAI方法，包括相似的例子、counterfactual和semi-factual、重要的实例、原型和概念。尤其是在比较这些方法的semantic定义、认知影响和加值方面。我们希望这篇文章可以对未来的自然例子基于XAI工作提供启发和促进。Note: "自然"（natural）在这里指的是直接从训练数据中提取的例子，而不是通过生成过程来生成的例子。这种要求可信度的需求是因为人们需要在理解模型的预测时有足够的信任感。
</details></li>
</ul>
<hr>
<h2 id="DeepVol-A-Deep-Transfer-Learning-Approach-for-Universal-Asset-Volatility-Modeling"><a href="#DeepVol-A-Deep-Transfer-Learning-Approach-for-Universal-Asset-Volatility-Modeling" class="headerlink" title="DeepVol: A Deep Transfer Learning Approach for Universal Asset Volatility Modeling"></a>DeepVol: A Deep Transfer Learning Approach for Universal Asset Volatility Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02072">http://arxiv.org/abs/2309.02072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liu, Minh-Ngoc Tran, Chao Wang, Richard Gerlach, Robert Kohn</li>
<li>for: 这篇论文旨在提出一种深度学习模型，以更好地模型金融资产的波动性。</li>
<li>methods: 该模型使用了转移学习，可以有效地捕捉和模型所有金融资产的波动性动态，只需要一个通用模型。这与经济学 литераature中常见的单独训练每个数据集的方法不同。</li>
<li>results: 这个模型在模型泛化性方面表现出色，可以更好地预测金融资产的波动性。这对金融预测和管理具有广泛的应用前景。<details>
<summary>Abstract</summary>
This paper introduces DeepVol, a promising new deep learning volatility model that outperforms traditional econometric models in terms of model generality. DeepVol leverages the power of transfer learning to effectively capture and model the volatility dynamics of all financial assets, including previously unseen ones, using a single universal model. This contrasts to the prevailing practice in econometrics literature, which necessitates training separate models for individual datasets. The introduction of DeepVol opens up new avenues for volatility modeling and forecasting in the finance industry, potentially transforming the way volatility is understood and predicted.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了深度风险模型（DeepVol），这是一种有前途的深度学习模型，可以在经济学领域中超越传统 econometric 模型，并且可以更好地捕捉和模型所有金融资产的风险动态。 DeepVol 利用了传输学习的力量，可以通过单一的通用模型来模型所有金融资产，包括之前未见的资产。这与经济学 литераature 中的常见做法不同，需要为每个数据集训练 separte 模型。 DeepVol 的出现将为金融行业带来新的风险模型和预测方法，可能会改变风险的理解和预测方式。
</details></li>
</ul>
<hr>
<h2 id="Enhance-Multi-domain-Sentiment-Analysis-of-Review-Texts-through-Prompting-Strategies"><a href="#Enhance-Multi-domain-Sentiment-Analysis-of-Review-Texts-through-Prompting-Strategies" class="headerlink" title="Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies"></a>Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02045">http://arxiv.org/abs/2309.02045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yajing Wang, Zongwei Luo</li>
<li>for: 这篇论文旨在提高大型自然语言处理模型（LLMs）在特定任务中的性能，具体来说是在 Sentiment Analysis 任务中。</li>
<li>methods: 这篇论文使用了两种新的提示策略，即 RolePlaying（RP）提示和 Chain-of-thought（CoT）提示，并提出了 RP-CoT 提示策略。</li>
<li>results: 实验结果显示，采用提出的提示策略可以明显提高 Sentiment Analysis 的准确率，其中 CoT 提示策略对隐式情感分析具有显著的影响，RP-CoT 提示策略则在所有策略中表现最佳。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate that the adoption of the proposed prompting strategies leads to a increasing enhancement in sentiment analysis accuracy. Further, the CoT prompting strategy exhibits a notable impact on implicit sentiment analysis, with the RP-CoT prompting strategy delivering the most superior performance among all strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diffusion-Generative-Inverse-Design"><a href="#Diffusion-Generative-Inverse-Design" class="headerlink" title="Diffusion Generative Inverse Design"></a>Diffusion Generative Inverse Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02040">http://arxiv.org/abs/2309.02040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marin Vlastelica, Tatiana López-Guevara, Kelsey Allen, Peter Battaglia, Arnaud Doucet, Kimberley Stachenfeld</li>
<li>for:  solving inverse design problems efficiently</li>
<li>methods:  using denoising diffusion models (DDMs) and a particle sampling algorithm</li>
<li>results:  reducing the number of calls to the simulator compared to standard techniques, with improved efficiency<details>
<summary>Abstract</summary>
Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differentiable estimation of simulator dynamics, and support high-quality design optimization with gradient- or sampling-based optimization procedures. However, optimizing designs from scratch requires many expensive model queries, and these procedures exhibit basic failures on either non-convex or high-dimensional problems. In this work, we show how denoising diffusion models (DDMs) can be used to solve inverse design problems efficiently and propose a particle sampling algorithm for further improving their efficiency. We perform experiments on a number of fluid dynamics design challenges, and find that our approach substantially reduces the number of calls to the simulator compared to standard techniques.
</details>
<details>
<summary>摘要</summary>
“ inverse 设计”指的是对目标函数的输入优化，以实现一个目标结果。在许多实际工程问题中，目标函数通常是一个预测系统状态在时间推移中的模拟器，并且设计挑战是确定初始条件以实现目标结果。现有的学习模拟技术发展已经表明，图 neural network (GNNs) 可以用于准确、高效、可导estiimation of simulator dynamics，并支持高质量的设计优化。然而，从头开始优化设计需要许多昂贵的模拟器调用，这些过程在非凸或高维问题上会表现出基本的失败。在这种情况下，我们提出了使用 denoising diffusion models (DDMs) 来解决 inverse 设计问题，并提出了一种粒子抽象算法来进一步提高其效率。我们在一些流体动力学设计挑战中进行了实验，并发现我们的方法可以减少对模拟器的调用数量相比标准技术。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Artificial-Intelligence-on-the-Evolution-of-Digital-Education-A-Comparative-Study-of-OpenAI-Text-Generation-Tools-including-ChatGPT-Bing-Chat-Bard-and-Ernie"><a href="#The-Impact-of-Artificial-Intelligence-on-the-Evolution-of-Digital-Education-A-Comparative-Study-of-OpenAI-Text-Generation-Tools-including-ChatGPT-Bing-Chat-Bard-and-Ernie" class="headerlink" title="The Impact of Artificial Intelligence on the Evolution of Digital Education: A Comparative Study of OpenAI Text Generation Tools including ChatGPT, Bing Chat, Bard, and Ernie"></a>The Impact of Artificial Intelligence on the Evolution of Digital Education: A Comparative Study of OpenAI Text Generation Tools including ChatGPT, Bing Chat, Bard, and Ernie</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02029">http://arxiv.org/abs/2309.02029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Negin Yazdani Motlagh, Matin Khajavi, Abbas Sharifi, Mohsen Ahmadi</li>
<li>for: This paper aims to explore the potential of OpenAI’s text generation tools, particularly ChatGPT, in revolutionizing education and to highlight the challenges and opportunities of AI in education.</li>
<li>methods: The paper uses a typology that views education through the lenses of system, process, and result to examine the multifaceted applications of AI in education, including decentralizing global education, personalizing curriculums, and digitally documenting competence-based outcomes.</li>
<li>results: The paper highlights ChatGPT’s meteoric rise to one million users in just five days and its potential in democratizing education, fostering autodidacticism, and magnifying student engagement. However, the study also acknowledges the potential challenges of AI in education, such as the need for ethical guidelines, pedagogical adaptations, and strategic collaborations to ensure the responsible use of AI tools.<details>
<summary>Abstract</summary>
In the digital era, the integration of artificial intelligence (AI) in education has ushered in transformative changes, redefining teaching methodologies, curriculum planning, and student engagement. This review paper delves deep into the rapidly evolving landscape of digital education by contrasting the capabilities and impact of OpenAI's pioneering text generation tools like Bing Chat, Bard, Ernie with a keen focus on the novel ChatGPT. Grounded in a typology that views education through the lenses of system, process, and result, the paper navigates the multifaceted applications of AI. From decentralizing global education and personalizing curriculums to digitally documenting competence-based outcomes, AI stands at the forefront of educational modernization. Highlighting ChatGPT's meteoric rise to one million users in just five days, the study underscores its role in democratizing education, fostering autodidacticism, and magnifying student engagement. However, with such transformative power comes the potential for misuse, as text-generation tools can inadvertently challenge academic integrity. By juxtaposing the promise and pitfalls of AI in education, this paper advocates for a harmonized synergy between AI tools and the educational community, emphasizing the urgent need for ethical guidelines, pedagogical adaptations, and strategic collaborations.
</details>
<details>
<summary>摘要</summary>
在数字时代，人工智能（AI）在教育领域的整合已经带来了转变性的变革，重定义了教学方法、课程规划和学生参与度。这篇评论文章深入探讨在数字教育领域的迅速发展，并对OpenAI的创新性文本生成工具如Bing Chat、Bard、Ernie等进行了着力强调，特别是新出现的ChatGPT。根据教育视为系统、过程和结果的三个视角，文章探讨了AI在教育中的多方面应用。从全球教育的减少到个性化课程、数字记录竞争力具体成果等方面，AI在教育现代化中扮演着重要的角色。文章指出ChatGPT在只需五天内吸引了一百万用户，其在推动自主学习、提高学生参与度和全球教育民主化方面具有重要的作用。然而，与此同时，AI在教育领域的应用也存在潜在的风险，文本生成工具可能会不必要地挑战学术Integrity。通过对AI在教育中的推荐和风险的对比，这篇文章强调需要在AI工具和教育社区之间建立和谐的合作，并提出了优先级的道德规范、教学改进和战略合作。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Early-Exiting-Predictive-Coding-Neural-Networks"><a href="#Dynamic-Early-Exiting-Predictive-Coding-Neural-Networks" class="headerlink" title="Dynamic Early Exiting Predictive Coding Neural Networks"></a>Dynamic Early Exiting Predictive Coding Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02022">http://arxiv.org/abs/2309.02022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alaa Zniber, Ouassim Karrakchou, Mounir Ghogho</li>
<li>for: 这篇论文是为了提高深度学习模型在互联网络预测应用中的效率和低功耗。</li>
<li>methods: 本研究使用预测编码理论和动态早期终止技术，构建了一个浅层双向网络，并与VGG-16模型进行比较。</li>
<li>results: 本研究获得了与VGG-16模型相似的准确率，但具有更少的参数和computational complexity。<details>
<summary>Abstract</summary>
Internet of Things (IoT) sensors are nowadays heavily utilized in various real-world applications ranging from wearables to smart buildings passing by agrotechnology and health monitoring. With the huge amounts of data generated by these tiny devices, Deep Learning (DL) models have been extensively used to enhance them with intelligent processing. However, with the urge for smaller and more accurate devices, DL models became too heavy to deploy. It is thus necessary to incorporate the hardware's limited resources in the design process. Therefore, inspired by the human brain known for its efficiency and low power consumption, we propose a shallow bidirectional network based on predictive coding theory and dynamic early exiting for halting further computations when a performance threshold is surpassed. We achieve comparable accuracy to VGG-16 in image classification on CIFAR-10 with fewer parameters and less computational complexity.
</details>
<details>
<summary>摘要</summary>
互联网智能设备（IoT）的感应器在不同的实际应用中广泛使用，由穿梭到智能建筑和医疗监控。这些小型设备产生的大量数据，导致深度学习（DL）模型广泛应用于增强过程中。但是，对于更小和更精确的设备，DL模型已经变得太重且不可行。因此，我们受人脑的效率和低功耗骄傲，提出了一个浅层双向网络，基于预测编码理论和动态早期退出，以避免进一步的计算。我们在CIFAR-10类图标签准则中实现了与VGG-16相同的准确性，但具有较少的参数和计算复杂性。
</details></li>
</ul>
<hr>
<h2 id="iLoRE-Dynamic-Graph-Representation-with-Instant-Long-term-Modeling-and-Re-occurrence-Preservation"><a href="#iLoRE-Dynamic-Graph-Representation-with-Instant-Long-term-Modeling-and-Re-occurrence-Preservation" class="headerlink" title="iLoRE: Dynamic Graph Representation with Instant Long-term Modeling and Re-occurrence Preservation"></a>iLoRE: Dynamic Graph Representation with Instant Long-term Modeling and Re-occurrence Preservation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02012">http://arxiv.org/abs/2309.02012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Zhang, Yun Xiong, Yao Zhang, Xixi Wu, Yiheng Sun, Jiawei Zhang</li>
<li>for: 这篇论文旨在提出一个新的动态グラフ模型化方法，以解决现有方法的三个限制，提高其数据测试和应用范围。</li>
<li>methods: 这篇论文使用了一个具有自适应短期更新和长期更新的模型，具有自动删除无用或杂散的边的能力，以及一个具有识别注意力机制的传统transformer-based更新器，以更好地捕捉长期循环pattern。</li>
<li>results: 论文的实验结果显示，iLoRE方法能够有效地模型动态グラフ，并且在实验中获得了更高的表现。<details>
<summary>Abstract</summary>
Continuous-time dynamic graph modeling is a crucial task for many real-world applications, such as financial risk management and fraud detection. Though existing dynamic graph modeling methods have achieved satisfactory results, they still suffer from three key limitations, hindering their scalability and further applicability. i) Indiscriminate updating. For incoming edges, existing methods would indiscriminately deal with them, which may lead to more time consumption and unexpected noisy information. ii) Ineffective node-wise long-term modeling. They heavily rely on recurrent neural networks (RNNs) as a backbone, which has been demonstrated to be incapable of fully capturing node-wise long-term dependencies in event sequences. iii) Neglect of re-occurrence patterns. Dynamic graphs involve the repeated occurrence of neighbors that indicates their importance, which is disappointedly neglected by existing methods. In this paper, we present iLoRE, a novel dynamic graph modeling method with instant node-wise Long-term modeling and Re-occurrence preservation. To overcome the indiscriminate updating issue, we introduce the Adaptive Short-term Updater module that will automatically discard the useless or noisy edges, ensuring iLoRE's effectiveness and instant ability. We further propose the Long-term Updater to realize more effective node-wise long-term modeling, where we innovatively propose the Identity Attention mechanism to empower a Transformer-based updater, bypassing the limited effectiveness of typical RNN-dominated designs. Finally, the crucial re-occurrence patterns are also encoded into a graph module for informative representation learning, which will further improve the expressiveness of our method. Our experimental results on real-world datasets demonstrate the effectiveness of our iLoRE for dynamic graph modeling.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Continuous-time dynamic graph modeling is a crucial task for many real-world applications, such as financial risk management and fraud detection. Though existing dynamic graph modeling methods have achieved satisfactory results, they still suffer from three key limitations, hindering their scalability and further applicability. i) Indiscriminate updating. For incoming edges, existing methods would indiscriminately deal with them, which may lead to more time consumption and unexpected noisy information. ii) Ineffective node-wise long-term modeling. They heavily rely on recurrent neural networks (RNNs) as a backbone, which has been demonstrated to be incapable of fully capturing node-wise long-term dependencies in event sequences. iii) Neglect of re-occurrence patterns. Dynamic graphs involve the repeated occurrence of neighbors that indicates their importance, which is disappointedly neglected by existing methods. In this paper, we present iLoRE, a novel dynamic graph modeling method with instant node-wise Long-term modeling and Re-occurrence preservation. To overcome the indiscriminate updating issue, we introduce the Adaptive Short-term Updater module that will automatically discard the useless or noisy edges, ensuring iLoRE's effectiveness and instant ability. We further propose the Long-term Updater to realize more effective node-wise long-term modeling, where we innovatively propose the Identity Attention mechanism to empower a Transformer-based updater, bypassing the limited effectiveness of typical RNN-dominated designs. Finally, the crucial re-occurrence patterns are also encoded into a graph module for informative representation learning, which will further improve the expressiveness of our method. Our experimental results on real-world datasets demonstrate the effectiveness of our iLoRE for dynamic graph modeling.")Here's the translation in Simplified Chinese:<<SYS>> continuous-time动态图模型化是许多实际应用中的关键任务，如金融风险管理和欺诈探测。虽然现有的动态图模型化方法已经达到了一定的成果，但它们仍然受到三个关键限制，使其可扩展性和更多的应用场景受到限制。i) 随机更新。现有的方法会随机处理入coming edges，这可能会导致更多的时间开销和意外的噪声信息。ii) 不够有效的节点级长期模型化。它们依赖于循环神经网络（RNN）作为底层，这已经被证明无法全面捕捉节点级长期依赖关系。iii) 忽略重复模式。动态图中的重复 neighboor 表示其重要性，这是现有方法忽略的。在这篇论文中，我们提出了 iLoRE，一种新的动态图模型化方法，具有即时节点级长期模型化和重复模式保存。为了解决随机更新问题，我们引入了适应短期更新模块，可以自动排除无用或噪声的边，保证 iLoRE 的有效性和即时能力。我们进一步提出了长期更新模块，以实现更有效的节点级长期模型化。我们创新地提出了标识注意力机制，以强化基于 Transformer 的更新器，超越传统 RNN  доминиated 设计的局限性。最后，我们还编码了重复模式到图模块，以进一步提高我们方法的表达能力。我们在实际 datasets 上进行了实验， demonstate 了我们 iLoRE 在动态图模型化中的效果。
</details></li>
</ul>
<hr>
<h2 id="Belief-revision-and-incongruity-is-it-a-joke"><a href="#Belief-revision-and-incongruity-is-it-a-joke" class="headerlink" title="Belief revision and incongruity: is it a joke?"></a>Belief revision and incongruity: is it a joke?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02009">http://arxiv.org/abs/2309.02009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florence Dupin de Saint Cyr - Bannay, Henri Prade</li>
<li>for: 本文是一种智能行为 formalization的尝试，描述一个智能代理在听笑话时的行为。</li>
<li>methods: 本文使用了改变信念、出乎意料和违反 norms 等方法来形式化这种智能行为。</li>
<li>results: 本文的研究结果表明，在听笑话时，智能代理可以通过改变信念和出乎意料来产生幽默的感受，并且可以通过违反 norms 来提高幽默的效果。<details>
<summary>Abstract</summary>
Incongruity often makes people laugh. You have to be smart to say stupid things. It requires to be even smarter for understanding them. This paper is a shameless attempt to formalize this intelligent behavior in the case of an agent listening to a joke. All this is a matter of revision of beliefs, surprise and violation of norms.
</details>
<details>
<summary>摘要</summary>
冲突可以让人 laugh。你需要聪明才能说些愚蠢的话。更需要聪明才能理解它们。这篇文章是一种不害怕的尝试，用于形式化代理人听笑话时的智慧行为。这一切都是对信念的修订，对听者的意外和规范的违反。
</details></li>
</ul>
<hr>
<h2 id="Aggregating-Correlated-Estimations-with-Almost-no-Training"><a href="#Aggregating-Correlated-Estimations-with-Almost-no-Training" class="headerlink" title="Aggregating Correlated Estimations with (Almost) no Training"></a>Aggregating Correlated Estimations with (Almost) no Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02005">http://arxiv.org/abs/2309.02005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Theo Delemazure, François Durand, Fabien Mathieu</li>
<li>for: 本研究旨在提出一些考虑关联错误的汇集规则，以解决许多决策问题无法得到精确解决方案。</li>
<li>methods: 本文提出了一些考虑关联错误的汇集规则，并对它们进行了多种实验，以评估它们在不同的数据集上的性能。</li>
<li>results: 研究结果表明，当知道错误的相关性信息时，最大似然汇集方法应该被首选。否则，通常在受限的训练数据下，我们建议使用嵌入式选举方法（EV）。<details>
<summary>Abstract</summary>
Many decision problems cannot be solved exactly and use several estimation algorithms that assign scores to the different available options. The estimation errors can have various correlations, from low (e.g. between two very different approaches) to high (e.g. when using a given algorithm with different hyperparameters). Most aggregation rules would suffer from this diversity of correlations. In this article, we propose different aggregation rules that take correlations into account, and we compare them to naive rules in various experiments based on synthetic data. Our results show that when sufficient information is known about the correlations between errors, a maximum likelihood aggregation should be preferred. Otherwise, typically with limited training data, we recommend a method that we call Embedded Voting (EV).
</details>
<details>
<summary>摘要</summary>
很多决策问题无法精确解决，需要使用估计算法赋分不同选项的分数。估计误差可能存在多种相关性，从低（例如两种完全不同的方法）到高（例如使用同一算法的不同Hyperparameter）。大多数汇集规则都会受到这种多样性的影响。在这篇文章中，我们提出了考虑相关性的不同汇集规则，并与无知规则进行了多个实验，基于 sintetic 数据。我们的结果表明，当知道估计误差之间的相关性信息充分时，最大 likelihood 汇集应该被首选。否则，通常在受限的训练数据情况下，我们建议一种我们称为 Embedded Voting（EV）方法。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-domain-shift-when-using-additional-data-for-the-MICCAI-KiTS23-Challenge"><a href="#Analyzing-domain-shift-when-using-additional-data-for-the-MICCAI-KiTS23-Challenge" class="headerlink" title="Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge"></a>Analyzing domain shift when using additional data for the MICCAI KiTS23 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02001">http://arxiv.org/abs/2309.02001</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Stoica, Mihaela Breaban, Vlad Barbu</li>
<li>for: 提高医疗影像3D segmentation的结果，尤其是在训练材料稀缺的情况下。</li>
<li>methods: 使用 histogram matching 来缓解频谱shift，以便将新数据与原始训练数据一起使用。</li>
<li>results: 对于 histogram matching 的应用，比使用 simple normalization 得到了更好的结果。<details>
<summary>Abstract</summary>
Using additional training data is known to improve the results, especially for medical image 3D segmentation where there is a lack of training material and the model needs to generalize well from few available data. However, the new data could have been acquired using other instruments and preprocessed such its distribution is significantly different from the original training data. Therefore, we study techniques which ameliorate domain shift during training so that the additional data becomes better usable for preprocessing and training together with the original data. Our results show that transforming the additional data using histogram matching has better results than using simple normalization.
</details>
<details>
<summary>摘要</summary>
使用额外训练数据可以提高结果，特别是医学图像三维分割，因为这个领域缺乏训练材料，模型需要将少量可用数据总结化好。然而，新的数据可能是使用不同的仪器获取的，其分布与原始训练数据有很大差异。因此，我们研究如何在训练过程中缓解领域差异，使得额外数据更容易与原始数据一起预处理和训练。我们的结果表明，对额外数据进行 histogram matching 变换比使用简单 нормализация更有效。
</details></li>
</ul>
<hr>
<h2 id="Photonic-Structures-Optimization-Using-Highly-Data-Efficient-Deep-Learning-Application-To-Nanofin-And-Annular-Groove-Phase-Masks"><a href="#Photonic-Structures-Optimization-Using-Highly-Data-Efficient-Deep-Learning-Application-To-Nanofin-And-Annular-Groove-Phase-Masks" class="headerlink" title="Photonic Structures Optimization Using Highly Data-Efficient Deep Learning: Application To Nanofin And Annular Groove Phase Masks"></a>Photonic Structures Optimization Using Highly Data-Efficient Deep Learning: Application To Nanofin And Annular Groove Phase Masks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01995">http://arxiv.org/abs/2309.01995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kaeryv/acsphot23suppl">https://github.com/kaeryv/acsphot23suppl</a></li>
<li>paper_authors: Nicolas Roy, Lorenzo König, Olivier Absil, Charlotte Beauthier, Alexandre Mayer, Michaël Lobet<br>for: This paper aims to introduce a surrogate optimization framework for metasurfaces, specifically for the manipulation of light properties in astronomical high-contrast imaging.methods: The paper uses computational intelligence techniques, such as partial least squares Kriging, radial basis functions, and neural networks, to optimize the geometric features of vortex phase masks (VPMs). However, these methods are shown to be inadequate for modeling the performance of VPMs, so a data-efficient evolutionary optimization setup using a deep neural network is proposed instead.results: The paper demonstrates the effectiveness of the proposed optimization setup by developing optimal designs for two design candidates, with the surrogate model improving the reliability and efficiency of the procedure. In the most complex case, evolutionary optimization enables optimization of the design that would otherwise be impractical (requiring too much simulations). The use of the surrogate model reduces the required number of simulations by up to 75% compared to conventional optimization techniques.Here is the text in Simplified Chinese:for: 这篇论文目标是引入一种基于Vortex phase masks（VPMs）的高精度光学设计优化框架。methods: 论文使用了计算智能技术，如部分最小值Kriging、基函数和神经网络，来优化VPMs的几何特征。然而，这些方法不足以模型VPMs的性能，因此提出了一种数据高效的进化优化方案。results: 论文证明了提案的优化方案的有效性，通过开发了两种设计候选人。在最复杂的情况下，进化优化可以实现对设计的优化，而无需进行过多的Simulations。使用代表性模型可以大大降低需要的Simulations数量，相比传统优化技术。<details>
<summary>Abstract</summary>
Metasurfaces offer a flexible framework for the manipulation of light properties in the realm of thin film optics. Specifically, the polarization of light can be effectively controlled through the use of thin phase plates. This study aims to introduce a surrogate optimization framework for these devices. The framework is applied to develop two kinds of vortex phase masks (VPMs) tailored for application in astronomical high-contrast imaging. Computational intelligence techniques are exploited to optimize the geometric features of these devices. The large design space and computational limitations necessitate the use of surrogate models like partial least squares Kriging, radial basis functions, or neural networks. However, we demonstrate the inadequacy of these methods in modeling the performance of VPMs. To address the shortcomings of these methods, a data-efficient evolutionary optimization setup using a deep neural network as a highly accurate and efficient surrogate model is proposed. The optimization process in this study employs a robust particle swarm evolutionary optimization scheme, which operates on explicit geometric parameters of the photonic device. Through this approach, optimal designs are developed for two design candidates. In the most complex case, evolutionary optimization enables optimization of the design that would otherwise be impractical (requiring too much simulations). In both cases, the surrogate model improves the reliability and efficiency of the procedure, effectively reducing the required number of simulations by up to 75% compared to conventional optimization techniques.
</details>
<details>
<summary>摘要</summary>
追踪板（Metasurfaces）提供了膜片光学中的灵活框架，可以有效控制光的属性。本研究旨在介绍一种代理优化框架，用于这些设备。这种框架应用于开发两种星系高对比图像的旋转相位面镜（VPM）。通过利用计算智能技术，可以优化这些设备的几何特征。由于设计空间很大，计算限制，因此需要使用代理模型，如多项式拟合、径向基函数或神经网络。然而，我们发现这些方法无法模型VPM的性能。为了解决这些方法的缺陷，我们提出了一种数据有效的进化优化设计，使用深度神经网络作为高精度和高效的代理模型。优化过程中，我们使用一种稳定的粒子群演化优化方法，该方法操作于膜片光学设备的显式几何参数。通过这种方法，我们开发了两个设计候选人。在最复杂的情况下，演化优化使得设计可以实现，而不是通过传统优化技术来实现。在两个情况下，代理模型提高了过程的可靠性和效率，实际减少了需要的模拟数量，相对于传统优化技术，减少了75%。
</details></li>
</ul>
<hr>
<h2 id="sasdim-self-adaptive-noise-scaling-diffusion-model-for-spatial-time-series-imputation"><a href="#sasdim-self-adaptive-noise-scaling-diffusion-model-for-spatial-time-series-imputation" class="headerlink" title="sasdim: self-adaptive noise scaling diffusion model for spatial time series imputation"></a>sasdim: self-adaptive noise scaling diffusion model for spatial time series imputation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01988">http://arxiv.org/abs/2309.01988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shunyang Zhang, Senzhang Wang, Xianzhen Tan, Ruochen Liu, Jian Zhang, Jianxin Wang</li>
<li>for:  spatial time series imputation</li>
<li>methods:  self-adaptive noise scaling diffusion model (SaSDim) with a new loss function and across spatial-temporal global convolution module</li>
<li>results:  effective imputation performance on three real-world datasets, with comparison to current state-of-the-art baselines<details>
<summary>Abstract</summary>
Spatial time series imputation is critically important to many real applications such as intelligent transportation and air quality monitoring. Although recent transformer and diffusion model based approaches have achieved significant performance gains compared with conventional statistic based methods, spatial time series imputation still remains as a challenging issue due to the complex spatio-temporal dependencies and the noise uncertainty of the spatial time series data. Especially, recent diffusion process based models may introduce random noise to the imputations, and thus cause negative impact on the model performance. To this end, we propose a self-adaptive noise scaling diffusion model named SaSDim to more effectively perform spatial time series imputation. Specially, we propose a new loss function that can scale the noise to the similar intensity, and propose the across spatial-temporal global convolution module to more effectively capture the dynamic spatial-temporal dependencies. Extensive experiments conducted on three real world datasets verify the effectiveness of SaSDim by comparison with current state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>> spatial time series imputation 是很重要的几个实际应用，如智能交通和空气质量监测。尽管最近的 transformer 和 diffusion model 基于方法已经实现了与传统统计学基于方法相比的显著性能提升，但是 spatial time series imputation 仍然是一个具有复杂的空间时间相关性和空间时间数据的噪声不确定性的挑战。特别是，最近的 diffusion process 基于模型可能会将随机噪声引入到插入中，从而影响模型性能。为此，我们提出了一种自适应噪声扩大扩散模型名为 SaSDim，以更有效地进行 spatial time series imputation。特别是，我们提出了一个新的损失函数，可以扩大噪声到类似的强度，并提出了跨空间时间全球 convolution 模块，以更好地捕捉空间时间相关性的动态变化。广泛的实验在三个真实世界数据集上验证了 SaSDim 的效果，与当前状态的先进基elines进行比较。
</details></li>
</ul>
<hr>
<h2 id="Graph-Based-Interaction-Aware-Multimodal-2D-Vehicle-Trajectory-Prediction-using-Diffusion-Graph-Convolutional-Networks"><a href="#Graph-Based-Interaction-Aware-Multimodal-2D-Vehicle-Trajectory-Prediction-using-Diffusion-Graph-Convolutional-Networks" class="headerlink" title="Graph-Based Interaction-Aware Multimodal 2D Vehicle Trajectory Prediction using Diffusion Graph Convolutional Networks"></a>Graph-Based Interaction-Aware Multimodal 2D Vehicle Trajectory Prediction using Diffusion Graph Convolutional Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01981">http://arxiv.org/abs/2309.01981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keshu Wu, Yang Zhou, Haotian Shi, Xiaopeng Li, Bin Ran</li>
<li>for: 预测汽车轨迹，以提高自动化汽车运行效率和安全，特别在拥堵多车道高速公路上。</li>
<li>methods: 使用 Graph-based Interaction-aware Multi-modal Trajectory Prediction (GIMTP) 框架，利用图表示汽车的动态互动关系，并通过Diffusion Graph Convolutional Network (DGCN) 捕捉空间和时间两种依赖关系。</li>
<li>results: 提供了两维预测结果，包括 longitudinal 和 lateral 驾驶行为，并提供了相应的概率分布，以便更好地预测汽车的未来行为。<details>
<summary>Abstract</summary>
Predicting vehicle trajectories is crucial for ensuring automated vehicle operation efficiency and safety, particularly on congested multi-lane highways. In such dynamic environments, a vehicle's motion is determined by its historical behaviors as well as interactions with surrounding vehicles. These intricate interactions arise from unpredictable motion patterns, leading to a wide range of driving behaviors that warrant in-depth investigation. This study presents the Graph-based Interaction-aware Multi-modal Trajectory Prediction (GIMTP) framework, designed to probabilistically predict future vehicle trajectories by effectively capturing these interactions. Within this framework, vehicles' motions are conceptualized as nodes in a time-varying graph, and the traffic interactions are represented by a dynamic adjacency matrix. To holistically capture both spatial and temporal dependencies embedded in this dynamic adjacency matrix, the methodology incorporates the Diffusion Graph Convolutional Network (DGCN), thereby providing a graph embedding of both historical states and future states. Furthermore, we employ a driving intention-specific feature fusion, enabling the adaptive integration of historical and future embeddings for enhanced intention recognition and trajectory prediction. This model gives two-dimensional predictions for each mode of longitudinal and lateral driving behaviors and offers probabilistic future paths with corresponding probabilities, addressing the challenges of complex vehicle interactions and multi-modality of driving behaviors. Validation using real-world trajectory datasets demonstrates the efficiency and potential.
</details>
<details>
<summary>摘要</summary>
预测 vehicular trajectories 是确保自动化交通效率和安全的关键，尤其在拥堵的多车道高速公路上。在这种动态环境中，车辆的运动受到历史行为以及与周围车辆的交互影响。这些复杂的交互关系导致了车辆的驾驶行为的各种多样性，需要进一步的研究。本研究提出的 Graph-based Interaction-aware Multi-modal Trajectory Prediction（GIMTP）框架，可以 probabilistically 预测未来车辆的 trajectories，并有效地捕捉这些交互关系。在这个框架中，车辆的运动被视为时间变化的图形中的节点，交通交互被表示为动态邻接矩阵。为了全面捕捉这些图形中的空间和时间相关性，我们采用了卷积图грам（DGCN），从而提供了图形 embedding  both historical states 和 future states。此外，我们采用了驾驶意图特征融合，以适应不同驾驶意图的 embeddings 的权重调整，从而提高了驾驶意图识别和车辆预测。这个模型为每种方向的两维预测提供了两维预测结果，并提供了对应的概率，解决了车辆间复杂的交互关系和驾驶行为多样性的问题。验证使用实际 trajectory 数据表明该模型的效率和潜力。
</details></li>
</ul>
<hr>
<h2 id="Linear-Regression-using-Heterogeneous-Data-Batches"><a href="#Linear-Regression-using-Heterogeneous-Data-Batches" class="headerlink" title="Linear Regression using Heterogeneous Data Batches"></a>Linear Regression using Heterogeneous Data Batches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01973">http://arxiv.org/abs/2309.01973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Jain, Rajat Sen, Weihao Kong, Abhimanyu Das, Alon Orlitsky<br>for:The paper is written to address the problem of learning input-output relationships from multiple sources with insufficient data.methods:The paper proposes a novel gradient-based algorithm that improves upon existing results by allowing for different, unknown, and heavy-tailed input distributions for each subgroup, recovering all subgroups with a significant proportion of batches, and removing the separation requirement between regression vectors.results:The proposed algorithm extends the applicability of the existing results, allowing for smaller batch sizes and reducing the number of batches needed to achieve accurate regression.<details>
<summary>Abstract</summary>
In many learning applications, data are collected from multiple sources, each providing a \emph{batch} of samples that by itself is insufficient to learn its input-output relationship. A common approach assumes that the sources fall in one of several unknown subgroups, each with an unknown input distribution and input-output relationship. We consider one of this setup's most fundamental and important manifestations where the output is a noisy linear combination of the inputs, and there are $k$ subgroups, each with its own regression vector. Prior work~\cite{kong2020meta} showed that with abundant small-batches, the regression vectors can be learned with only few, $\tilde\Omega( k^{3/2})$, batches of medium-size with $\tilde\Omega(\sqrt k)$ samples each. However, the paper requires that the input distribution for all $k$ subgroups be isotropic Gaussian, and states that removing this assumption is an ``interesting and challenging problem". We propose a novel gradient-based algorithm that improves on the existing results in several ways. It extends the applicability of the algorithm by: (1) allowing the subgroups' underlying input distributions to be different, unknown, and heavy-tailed; (2) recovering all subgroups followed by a significant proportion of batches even for infinite $k$; (3) removing the separation requirement between the regression vectors; (4) reducing the number of batches and allowing smaller batch sizes.
</details>
<details>
<summary>摘要</summary>
在许多学习应用中，数据来源来自多个不同的源泉，每个源泉提供一个不够的批处理，用于学习其输入输出关系。一种常见的方法假设这些源泉可以分为多个未知的子组，每个子组有未知的输入分布和输入输出关系。我们考虑这个设置的一个最基本和最重要的情况，其中输出是噪声加权的输入的线性组合，并且有 $k$ 个子组，每个子组有自己的回归 вектор。先前的工作（\ref{kong2020meta}) 显示，只要有充足的小批处理，可以通过只需几个 $\tilde\Omega(k^{3/2})$ 批处理，每批处理中有 $\tilde\Omega(\sqrt k)$ 个样本，学习这些回归 вектор。然而，这篇文章要求所有 $k$ 个子组的输入分布都是均匀的 Gaussian，并且认为从不同输入分布的批处理中学习回归 вектор是一个“有趣和挑战的问题”。我们提出了一种新的梯度法，它在以下方面改进了现有结果：1. 允许子组的下面分布不同、未知、重 tailed;2. 可以在有限 $k$ 的情况下，将所有子组都回归;3. 取消回归向量之间的分离要求;4. 减少批处理的数量，并允许更小的批处理大小。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Interpretable-Cross-modal-Reasoning"><a href="#A-Survey-on-Interpretable-Cross-modal-Reasoning" class="headerlink" title="A Survey on Interpretable Cross-modal Reasoning"></a>A Survey on Interpretable Cross-modal Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01955">http://arxiv.org/abs/2309.01955</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZuyiZhou/Awesome-Interpretable-Cross-modal-Reasoning">https://github.com/ZuyiZhou/Awesome-Interpretable-Cross-modal-Reasoning</a></li>
<li>paper_authors: Dizhan Xue, Shengsheng Qian, Zuyi Zhou, Changsheng Xu</li>
<li>for: 本文旨在探讨可解释的跨模态理解（I-CMR），即不 только实现高预测性能，还能提供人类可理解的解释结果。</li>
<li>methods: 本文使用三级分类法概述I-CMR的典型方法，并对现有的CMR数据集进行了解释注释。</li>
<li>results: 本文总结了I-CMR的挑战和未来发展方向，并提供了一个包含相关方法、数据集和资源的GitHub项目。<details>
<summary>Abstract</summary>
In recent years, cross-modal reasoning (CMR), the process of understanding and reasoning across different modalities, has emerged as a pivotal area with applications spanning from multimedia analysis to healthcare diagnostics. As the deployment of AI systems becomes more ubiquitous, the demand for transparency and comprehensibility in these systems' decision-making processes has intensified. This survey delves into the realm of interpretable cross-modal reasoning (I-CMR), where the objective is not only to achieve high predictive performance but also to provide human-understandable explanations for the results. This survey presents a comprehensive overview of the typical methods with a three-level taxonomy for I-CMR. Furthermore, this survey reviews the existing CMR datasets with annotations for explanations. Finally, this survey summarizes the challenges for I-CMR and discusses potential future directions. In conclusion, this survey aims to catalyze the progress of this emerging research area by providing researchers with a panoramic and comprehensive perspective, illuminating the state of the art and discerning the opportunities. The summarized methods, datasets, and other resources are available at https://github.com/ZuyiZhou/Awesome-Interpretable-Cross-modal-Reasoning.
</details>
<details>
<summary>摘要</summary>
This survey provides a comprehensive overview of I-CMR methods, using a three-level taxonomy to categorize them. Additionally, it reviews existing CMR datasets with annotations for explanations. Finally, it discusses the challenges facing I-CMR and outlines potential future directions.The main goal of this survey is to advance the progress of this emerging research area by providing researchers with a comprehensive perspective, highlighting the current state of the art and identifying opportunities for future research. The survey's findings, methods, datasets, and other resources are available at https://github.com/ZuyiZhou/Awesome-Interpretable-Cross-modal-Reasoning.
</details></li>
</ul>
<hr>
<h2 id="RADIO-Reference-Agnostic-Dubbing-Video-Synthesis"><a href="#RADIO-Reference-Agnostic-Dubbing-Video-Synthesis" class="headerlink" title="RADIO: Reference-Agnostic Dubbing Video Synthesis"></a>RADIO: Reference-Agnostic Dubbing Video Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01950">http://arxiv.org/abs/2309.01950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyeun Lee, Chaewon Kim, Sangjoon Yu, Jaejun Yoo, Gyeong-Moon Park</li>
<li>for: 高精度 talking head生成中最大的挑战是实现高精度细节，同时保证 precisions synchronization.</li>
<li>methods: 我们提出了 RADIO 框架，通过修改decoder层的 latent space 来实现高质量的 dubbed video。此外，我们还 incorporated ViT blocks 来强调高精度细节，特别是在唇部分。</li>
<li>results: 我们的实验结果表明，RADIO 能够保持高度同步，同时不失高精度。特别在参考帧与实际帧有很大差异时，我们的方法表现出了更高的稳定性和可靠性。<details>
<summary>Abstract</summary>
One of the most challenging problems in audio-driven talking head generation is achieving high-fidelity detail while ensuring precise synchronization. Given only a single reference image, extracting meaningful identity attributes becomes even more challenging, often causing the network to mirror the facial and lip structures too closely. To address these issues, we introduce RADIO, a framework engineered to yield high-quality dubbed videos regardless of the pose or expression in reference images. The key is to modulate the decoder layers using latent space composed of audio and reference features. Additionally, we incorporate ViT blocks into the decoder to emphasize high-fidelity details, especially in the lip region. Our experimental results demonstrate that RADIO displays high synchronization without the loss of fidelity. Especially in harsh scenarios where the reference frame deviates significantly from the ground truth, our method outperforms state-of-the-art methods, highlighting its robustness. Pre-trained model and codes will be made public after the review.
</details>
<details>
<summary>摘要</summary>
一个非常挑战的问题在听音驱动的头部生成中是达到高精度细节，同时保证准确的同步。只有一个参考图片，提取有意义的人脸特征变得更加挑战，常常使网络模式lip和脸部结构，这会导致网络模式的产生。为解决这些问题，我们介绍了RADIO框架，可以生成高质量的重音视频，无论参考图片的pose或表情。关键在于在decoder层中模拟latent空间中的音频和参考特征。此外，我们在decoder中添加了ViT块，以强调高精度细节，特别是在唇区。我们的实验结果表明，RADIO可以保持高同步性，而不失去精度。尤其在参考图片与实际真实情况有很大差异时，我们的方法比状态艺术法更高效，这展示了其Robustness。我们将在审核后发布预训练模型和代码。
</details></li>
</ul>
<hr>
<h2 id="OHQ-On-chip-Hardware-aware-Quantization"><a href="#OHQ-On-chip-Hardware-aware-Quantization" class="headerlink" title="OHQ: On-chip Hardware-aware Quantization"></a>OHQ: On-chip Hardware-aware Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01945">http://arxiv.org/abs/2309.01945</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Haotong Qin, Yangdong Liu, Jingzhuo Liang, Yifu Ding, Ying Li, Xianglong Liu</li>
<li>for: 这个论文旨在提出一个在硬件上进行数值化的框架，以便在资源有限的硬件上部署进步的深度模型。</li>
<li>methods: 这个框架使用了硬件感知的混合精度数值化，并且使用了面精度调节来优化数值化的精度和效率。</li>
<li>results: 这个框架可以实现在硬件上进行加速的推理，并且可以获得70%和73%的准确率 для ResNet-18和MobileNetV3。同时，这个框架可以提高对INT8的延迟时间，比较INT8在部署时的性能。<details>
<summary>Abstract</summary>
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique to efficiently estimate the accuracy metrics of operators under the constraints of on-chip-level computing power.By synthesizing network and hardware insights through linear programming, we obtain optimized bit-width configurations. Notably, the quantization process occurs on-chip entirely without any additional computing devices and data access. We demonstrate accelerated inference after quantization for various architectures and compression ratios, achieving 70% and 73% accuracy for ResNet-18 and MobileNetV3, respectively. OHQ improves latency by 15~30% compared to INT8 on deployment.
</details>
<details>
<summary>摘要</summary>
“量化技术在资源有限的硬件上部署高级深度模型的可能性几乎无限大。混合精度量化利用多个bit Width架构实现量化模型的精度和效率潜力。然而，现有的混合精度量化受到极大的搜索空间压力，导致计算开销很大。因此，量化过程通常需要分离的高性能设备，这也导致了实际部署与考虑的硬件指标之间存在很大的差距。在这篇论文中，我们提出了在硬件上完全没有访问外部设备的On-chip Hardware-aware Quantization（OHQ）框架。首先，我们构建了On-chip Quantization Awareness（OQA）管道，使得量化操作的实际效率指标可以在硬件上被感知。其次，我们提出了面具指导量化估计（MQE）技术，以计算量化操作在硬件上的精度指标。通过将网络和硬件知识融合到线性规划中，我们得到了优化的位数配置。需要注意的是，量化过程完全发生在硬件上，没有任何外部计算设备和数据访问。我们在不同的架构和压缩比例上进行加速的推理，实现了ResNet-18和MobileNetV3的70%和73%的准确率。OHQ提高了INT8在部署时的延迟时间，相对于INT8，OHQ提高了15~30%。”
</details></li>
</ul>
<hr>
<h2 id="Quantum-AI-empowered-Intelligent-Surveillance-Advancing-Public-Safety-Through-Innovative-Contraband-Detection"><a href="#Quantum-AI-empowered-Intelligent-Surveillance-Advancing-Public-Safety-Through-Innovative-Contraband-Detection" class="headerlink" title="Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection"></a>Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03231">http://arxiv.org/abs/2309.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Atif Ali Shah, Nasir Algeelani, Najeeb Al-Sammarraie</li>
<li>for: 这个研究旨在发展一个基于量子人工智能的实时类型识别系统，以解决现有的实时识别过程中的速度问题。</li>
<li>methods: 本研究使用了Quantum CNN的技术，实现了实时类型识别的高精度和高速度。</li>
<li>results: Quantum-RetinaNet模型在实验中表现出色，能够实现高精度和高速度的实时类型识别，提供了一个可行的解决方案 для实时识别过程中的速度问题。<details>
<summary>Abstract</summary>
Surveillance systems have emerged as crucial elements in upholding peace and security in the modern world. Their ubiquity aids in monitoring suspicious activities effectively. However, in densely populated environments, continuous active monitoring becomes impractical, necessitating the development of intelligent surveillance systems. AI integration in the surveillance domain was a big revolution, however, speed issues have prevented its widespread implementation in the field. It has been observed that quantum artificial intelligence has led to a great breakthrough. Quantum artificial intelligence-based surveillance systems have shown to be more accurate as well as capable of performing well in real-time scenarios, which had never been seen before. In this research, a RentinaNet model is integrated with Quantum CNN and termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN, Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative integration positions it as a game-changer, addressing the challenges of active monitoring in densely populated scenarios. As demand for efficient surveillance solutions continues to grow, Quantum-RetinaNet offers a compelling alternative to existing CNN models, upholding accuracy standards without sacrificing real-time performance. The unique attributes of Quantum-RetinaNet have far-reaching implications for the future of intelligent surveillance. With its enhanced processing speed, it is poised to revolutionize the field, catering to the pressing need for rapid yet precise monitoring. As Quantum-RetinaNet becomes the new standard, it ensures public safety and security while pushing the boundaries of AI in surveillance.
</details>
<details>
<summary>摘要</summary>
现代世界中维护和平安全的重要元素之一是监控系统。它们的普遍性使得可以有效监控异常活动。然而，在高度密集的环境中，不断的活动监控变得不实际，需要开发智能监控系统。人工智能在监控领域的整合是一次大革命，但速度问题阻碍了其广泛应用。研究表明，量子人工智能在监控领域带来了巨大突破。基于量子人工智能的监控系统显示更高精度，并在实时场景中表现出色，这从未被见过。本研究将RentinaNet模型与量子神经网络（QCNN）结合，称为量子-RetinaNet。通过利用量子神经网络的量子特性，量子-RetinaNet实现了精度和速度之间的平衡。这种创新的集成，将成为监控领域的游戏 changer，解决了高度密集enario中不断监控的挑战。随着有效监控解决方案的需求不断增长，量子-RetinaNet对现有的Convolutional Neural Network（CNN）模型提供了一种有力的替代，保持精度标准而不是速度性能的牺牲。量子-RetinaNet的独特特点有广泛的未来预测，它在监控领域的扩展将成为一个革命，为公共安全和安全提供保障，同时推动人工智能在监控领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Brain-Transformer-with-Multi-level-Attention-for-Functional-Brain-Network-Analysis"><a href="#Dynamic-Brain-Transformer-with-Multi-level-Attention-for-Functional-Brain-Network-Analysis" class="headerlink" title="Dynamic Brain Transformer with Multi-level Attention for Functional Brain Network Analysis"></a>Dynamic Brain Transformer with Multi-level Attention for Functional Brain Network Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01941">http://arxiv.org/abs/2309.01941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Wayfear/Dynamic-Brain-Transformer">https://github.com/Wayfear/Dynamic-Brain-Transformer</a></li>
<li>paper_authors: Xuan Kan, Antonio Aodong Chen Gu, Hejie Cui, Ying Guo, Carl Yang</li>
<li>for: 这篇论文的目的是提出一种新的方法，以便更好地分析大脑功能。</li>
<li>methods: 这篇论文使用了Dynamic bRAin Transformer（DART）方法，融合静止大脑网络和动态大脑网络，以提高大脑功能分析的精度和多元性。</li>
<li>results: 这篇论文的结果显示，DRAT方法可以更有效地预测临床结果和分类个人，并且可以提供更多的几何资讯，例如哪些大脑网络或动态网络在最终预测中做出了贡献。<details>
<summary>Abstract</summary>
Recent neuroimaging studies have highlighted the importance of network-centric brain analysis, particularly with functional magnetic resonance imaging. The emergence of Deep Neural Networks has fostered a substantial interest in predicting clinical outcomes and categorizing individuals based on brain networks. However, the conventional approach involving static brain network analysis offers limited potential in capturing the dynamism of brain function. Although recent studies have attempted to harness dynamic brain networks, their high dimensionality and complexity present substantial challenges. This paper proposes a novel methodology, Dynamic bRAin Transformer (DART), which combines static and dynamic brain networks for more effective and nuanced brain function analysis. Our model uses the static brain network as a baseline, integrating dynamic brain networks to enhance performance against traditional methods. We innovatively employ attention mechanisms, enhancing model explainability and exploiting the dynamic brain network's temporal variations. The proposed approach offers a robust solution to the low signal-to-noise ratio of blood-oxygen-level-dependent signals, a recurring issue in direct DNN modeling. It also provides valuable insights into which brain circuits or dynamic networks contribute more to final predictions. As such, DRAT shows a promising direction in neuroimaging studies, contributing to the comprehensive understanding of brain organization and the role of neural circuits.
</details>
<details>
<summary>摘要</summary>
latest neuroimaging studies have highlighted the importance of network-centric brain analysis, particularly with functional magnetic resonance imaging. The emergence of Deep Neural Networks has fostered a substantial interest in predicting clinical outcomes and categorizing individuals based on brain networks. However, the conventional approach involving static brain network analysis offers limited potential in capturing the dynamism of brain function. Although recent studies have attempted to harness dynamic brain networks, their high dimensionality and complexity present substantial challenges. This paper proposes a novel methodology, Dynamic bRAin Transformer (DART), which combines static and dynamic brain networks for more effective and nuanced brain function analysis. Our model uses the static brain network as a baseline, integrating dynamic brain networks to enhance performance against traditional methods. We innovatively employ attention mechanisms, enhancing model explainability and exploiting the dynamic brain network's temporal variations. The proposed approach offers a robust solution to the low signal-to-noise ratio of blood-oxygen-level-dependent signals, a recurring issue in direct DNN modeling. It also provides valuable insights into which brain circuits or dynamic networks contribute more to final predictions. As such, DRAT shows a promising direction in neuroimaging studies, contributing to the comprehensive understanding of brain organization and the role of neural circuits.
</details></li>
</ul>
<hr>
<h2 id="CodeApex-A-Bilingual-Programming-Evaluation-Benchmark-for-Large-Language-Models"><a href="#CodeApex-A-Bilingual-Programming-Evaluation-Benchmark-for-Large-Language-Models" class="headerlink" title="CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models"></a>CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01940">http://arxiv.org/abs/2309.01940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apexlab/codeapex">https://github.com/apexlab/codeapex</a></li>
<li>paper_authors: Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu</li>
<li>for: 这个论文主要是用来评估大型自然语言模型（LLMs）在编程方面的能力。</li>
<li>methods: 该论文使用了一个名为CodeApex的双语 bencmark dataset，以评估 LLMS 在编程理解和代码生成方面的能力。 CodeApex 包括三类多选问题：概念理解、通用理性和多步理性，用于评估 LLMS 的编程理解能力。</li>
<li>results: 研究人员使用 14 个当前状态的 LLMS，包括一般型和专门型模型，进行评估。 GPT 表现出了最好的编程能力，在两个任务上的准确率分别为 50% 和 56%。 这显示了 LLMS 在编程任务上仍有很大的改进空间。<details>
<summary>Abstract</summary>
With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. We propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension and code generation abilities of LLMs. CodeApex comprises three types of multiple-choice questions: conceptual understanding, commonsense reasoning, and multi-hop reasoning, designed to evaluate LLMs on programming comprehension tasks. Additionally, CodeApex utilizes algorithmic questions and corresponding test cases to assess the code quality generated by LLMs. We evaluate 14 state-of-the-art LLMs, including both general-purpose and specialized models. GPT exhibits the best programming capabilities, achieving approximate accuracies of 50% and 56% on the two tasks, respectively. There is still significant room for improvement in programming tasks. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth. Datasets are released at https://github.com/APEXLAB/CodeApex.git. CodeApex submission website is https://apex.sjtu.edu.cn/codeapex/.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使大语言模型（LLM）的出现，程序设计和生成能力得到了显著改善，引起了研究人员的关注。我们提出了 CodeApex，一个双语测试集，旨在评估 LLM 在程序理解和代码生成任务上的能力。CodeApex 包括三种多选问题：概念理解、常识逻辑和多步逻辑，用于评估 LLM 在程序理解任务上的能力。此外，CodeApex 还使用了算法问题和相应的测试用例，来评估 LLM 生成的代码质量。我们对 14 个当前state-of-the-art LLM 进行评估，包括一般目标和专门目标模型。GPT 在两个任务上显示出了最好的编程能力，即 aproximate 的准确率为 50% 和 56%。然而，还有很多空间可以进一步改进程序任务。我们希望 CodeApex 能够成为 LLM 编程能力的参考，并促进其发展和成长。测试集可以在 https://github.com/APEXLAB/CodeApex.git 上下载。CodeApex 提交website是 https://apex.sjtu.edu.cn/codeapex/。
</details></li>
</ul>
<hr>
<h2 id="Provably-safe-systems-the-only-path-to-controllable-AGI"><a href="#Provably-safe-systems-the-only-path-to-controllable-AGI" class="headerlink" title="Provably safe systems: the only path to controllable AGI"></a>Provably safe systems: the only path to controllable AGI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01933">http://arxiv.org/abs/2309.01933</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Tegmark, Steve Omohundro</li>
<li>for: 这篇论文旨在帮助人类安全蒸蒸成长，并使用强大的人工智能（AGI）来实现这一目标。</li>
<li>methods: 这篇论文提出了使用高级人工智能进行正式验证和机制解释来建构AGI，并 garantía AGI满足人类指定的要求。</li>
<li>results: 这篇论文认为，这种方法是保证安全控制AGI的唯一道路。<details>
<summary>Abstract</summary>
We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.
</details>
<details>
<summary>摘要</summary>
我们描述了一条人类安全快速发展的人工通用智能（AGI）路径，通过让AGI建立可靠满足人类规定的条件。我们认为这很快会科技上可行，使用进步的AI进行正式验证和机械阅读性。我们还认为这是唯一能 guarantee safe controlled AGI 的路径。我们列出了一些挑战问题的解决方案，并邀请读者参与这个工作。Note that "人类安全快速发展" (rénxīn ànqù suǒzhòng fāzhǎng) is a bit of a mouthful in Chinese, so you may see variations of the phrase that use shorter words or different phrasing.
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-Policy-Gradient-Algorithm-for-Infinite-Horizon-Average-Reward-Markov-Decision-Processes"><a href="#Regret-Analysis-of-Policy-Gradient-Algorithm-for-Infinite-Horizon-Average-Reward-Markov-Decision-Processes" class="headerlink" title="Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes"></a>Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01922">http://arxiv.org/abs/2309.01922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal</li>
<li>for: 这个论文关注的是无穷远景平均奖励Markov决策过程（MDP）。与现有工作不同，我们的方法不假设MDP结构是线性的，而是利用通用的政策梯度算法，从而解放其吞吐量。</li>
<li>methods: 我们提出了一种政策梯度算法，并证明其全球吞吐量性。此外，我们还 Compute regret bound，这是首次在平均奖励场景中对通用参数化政策梯度算法进行投入的尝试。</li>
<li>results: 我们证明了该算法的 regret bound为 $\tilde{\mathcal{O}({T}^{3&#x2F;4})$。这意味着，在平均奖励场景中，我们的算法可以在很短的时间内达到理想的决策。<details>
<summary>Abstract</summary>
In this paper, we consider an infinite horizon average reward Markov Decision Process (MDP). Distinguishing itself from existing works within this context, our approach harnesses the power of the general policy gradient-based algorithm, liberating it from the constraints of assuming a linear MDP structure. We propose a policy gradient-based algorithm and show its global convergence property. We then prove that the proposed algorithm has $\tilde{\mathcal{O}({T}^{3/4})$ regret. Remarkably, this paper marks a pioneering effort by presenting the first exploration into regret-bound computation for the general parameterized policy gradient algorithm in the context of average reward scenarios.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了一个无穷horizon平均奖励Markov决策过程（MDP）。与现有的研究不同，我们的方法利用了通用的policy梯度基本算法，从linear MDP结构的假设中解放出来。我们提出了一种policy梯度基本算法，并证明其全球归一化性。然后，我们证明了提案的算法有$\tilde{\mathcal{O}({T}^{3/4})$的 regret。值得注意的是，这篇论文是第一篇在average奖励场景中计算 regret bound的general parameterized policy gradient算法的探索。
</details></li>
</ul>
<hr>
<h2 id="SyntheWorld-A-Large-Scale-Synthetic-Dataset-for-Land-Cover-Mapping-and-Building-Change-Detection"><a href="#SyntheWorld-A-Large-Scale-Synthetic-Dataset-for-Land-Cover-Mapping-and-Building-Change-Detection" class="headerlink" title="SyntheWorld: A Large-Scale Synthetic Dataset for Land Cover Mapping and Building Change Detection"></a>SyntheWorld: A Large-Scale Synthetic Dataset for Land Cover Mapping and Building Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01907">http://arxiv.org/abs/2309.01907</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JTRNEO/SyntheWorld">https://github.com/JTRNEO/SyntheWorld</a></li>
<li>paper_authors: Jian Song, Hongruixuan Chen, Naoto Yokoya</li>
<li>for: 提高计算机视觉任务和技术的研究，尤其是远程感知图像处理领域。</li>
<li>methods: 使用 Synthetic dataset，包括40,000个图像，每个图像具有 submeter 精度的像素和 eight 类地形分类注解，以及40,000个对比图像，用于检测建筑变化。</li>
<li>results: 通过在多个标准远程感知图像集上进行实验，证明 SyntheticWorld 的高质量和多样性，并 investigate 了在不同条件下 synthetic data 的优势。<details>
<summary>Abstract</summary>
Synthetic datasets, recognized for their cost effectiveness, play a pivotal role in advancing computer vision tasks and techniques. However, when it comes to remote sensing image processing, the creation of synthetic datasets becomes challenging due to the demand for larger-scale and more diverse 3D models. This complexity is compounded by the difficulties associated with real remote sensing datasets, including limited data acquisition and high annotation costs, which amplifies the need for high-quality synthetic alternatives. To address this, we present SyntheWorld, a synthetic dataset unparalleled in quality, diversity, and scale. It includes 40,000 images with submeter-level pixels and fine-grained land cover annotations of eight categories, and it also provides 40,000 pairs of bitemporal image pairs with building change annotations for building change detection task. We conduct experiments on multiple benchmark remote sensing datasets to verify the effectiveness of SyntheWorld and to investigate the conditions under which our synthetic data yield advantages. We will release SyntheWorld to facilitate remote sensing image processing research.
</details>
<details>
<summary>摘要</summary>
《 synthetic datasets 》，被广泛应用于计算机视觉任务和技术的进步，因为它们的成本效益很高。然而，当涉及到远程感知图像处理时，创建 synthetic datasets 变得更加困难，因为需要更大规模和更多的 3D 模型。这种复杂性由实际远程感知数据的限制和高注释成本带来，这使得高质量的 synthetic  altenativas 变得更加重要。为解决这一问题，我们介绍 SyntheWorld，一个无与伦比的 synthetic dataset，包括 40,000 张图像，每张图像有 submeter 级像素和细化的地形分类注释，同时还提供了 40,000 对时间双写图像对，用于建筑变化检测任务的注释。我们在多个标准远程感知数据集上进行了实验，以验证 SyntheWorld 的有效性和在不同条件下synthetic 数据的优势。我们将在未来发布 SyntheWorld，以便促进远程感知图像处理研究。
</details></li>
</ul>
<hr>
<h2 id="Towards-General-and-Efficient-Online-Tuning-for-Spark"><a href="#Towards-General-and-Efficient-Online-Tuning-for-Spark" class="headerlink" title="Towards General and Efficient Online Tuning for Spark"></a>Towards General and Efficient Online Tuning for Spark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01901">http://arxiv.org/abs/2309.01901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Huaijun Jiang, Yu Shen, Yide Fang, Xiaofeng Yang, Danqing Huang, Xinyi Zhang, Wentao Zhang, Ce Zhang, Peng Chen, Bin Cui</li>
<li>for: 提高 Spark 的性能和可扩展性，解决自动调整问题。</li>
<li>methods: 提出一个通用和高效的 Spark 自动调整框架，包括一个通用优化形式ulation、搜索方法、安全获取方法和三种创新技术。</li>
<li>results: 实现了在云端提供独立的 Spark 调整服务，并在实际生产任务中实现了减少内存成本57.00%和CPU成本34.93%的效果，提高了实用性、通用性和效率。<details>
<summary>Abstract</summary>
The distributed data analytic system -- Spark is a common choice for processing massive volumes of heterogeneous data, while it is challenging to tune its parameters to achieve high performance. Recent studies try to employ auto-tuning techniques to solve this problem but suffer from three issues: limited functionality, high overhead, and inefficient search.   In this paper, we present a general and efficient Spark tuning framework that can deal with the three issues simultaneously. First, we introduce a generalized tuning formulation, which can support multiple tuning goals and constraints conveniently, and a Bayesian optimization (BO) based solution to solve this generalized optimization problem. Second, to avoid high overhead from additional offline evaluations in existing methods, we propose to tune parameters along with the actual periodic executions of each job (i.e., online evaluations). To ensure safety during online job executions, we design a safe configuration acquisition method that models the safe region. Finally, three innovative techniques are leveraged to further accelerate the search process: adaptive sub-space generation, approximate gradient descent, and meta-learning method.   We have implemented this framework as an independent cloud service, and applied it to the data platform in Tencent. The empirical results on both public benchmarks and large-scale production tasks demonstrate its superiority in terms of practicality, generality, and efficiency. Notably, this service saves an average of 57.00% memory cost and 34.93% CPU cost on 25K in-production tasks within 20 iterations, respectively.
</details>
<details>
<summary>摘要</summary>
分布式数据分析系统---Spark 是一种常用的处理巨量不同类型数据的工具，但是调整其参数以 достичь高性能是一个挑战。最近的研究尝试使用自动调整技术解决这个问题，但它们受到三种问题的限制：功能受限，高过程成本，和不fficient搜索。在这篇论文中，我们提出了一个通用和高效的Spark调整框架，可以同时解决这三种问题。首先，我们引入一个通用的调整形式，可以方便地支持多个调整目标和约束，并使用抽象优化（BO）来解决这个通用优化问题。其次，为了避免现有方法的高过程成本，我们提议在实际 periodic执行每个任务时（即在线评估）进行参数调整。为确保在线任务执行安全，我们设计了一种安全配置获取方法，可以模拟安全区域。最后，我们采用了三种创新技术来加速搜索过程：适应子空间生成、 Approximate Gradient Descent 和元学习方法。我们实现了这个框架作为独立的云服务，并应用于腾讯数据平台。实际结果表明，这个框架在实际应用中具有了优秀的实用性、通用性和高效性。特别是，这个服务在25000个生产任务上平均占用内存成本下降57.00%，并且CPU成本下降34.93%，在20个迭代中分别达到这些值。
</details></li>
</ul>
<hr>
<h2 id="Inferring-Actual-Treatment-Pathways-from-Patient-Records"><a href="#Inferring-Actual-Treatment-Pathways-from-Patient-Records" class="headerlink" title="Inferring Actual Treatment Pathways from Patient Records"></a>Inferring Actual Treatment Pathways from Patient Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01897">http://arxiv.org/abs/2309.01897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Wilkins-Caruana, Madhushi Bandara, Katarzyna Musial, Daniel Catchpoole, Paul J. Kennedy</li>
<li>for: This paper aims to infer actual treatment steps for a particular patient group from administrative health records (AHRs), addressing gaps in treatment pathway-inference research.</li>
<li>methods: The method introduced in this paper is called Defrag, which learns the semantic and temporal meaning of healthcare event sequences using a neural network (NN) and a self-supervised learning objective.</li>
<li>results: Defrag significantly outperforms several existing pathway-inference methods and is effective in identifying best-practice pathway fragments for breast cancer, lung cancer, and melanoma in public healthcare records.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是从行政医疗记录（AHR）中推断特定患者群体的实际治疗步骤，解决治疗路径推断研究中的技术和方法上的缺陷。</li>
<li>methods: 该论文提出的方法是名为“Defrag”的方法，它利用神经网络（NN）和一种自我超vised学习目标来学习医疗事件序列的semantic和时间意义。</li>
<li>results: Defrag Significantly Outperforms Several Existing Pathway-Inference Methods and Effective in Identifying Best-Practice Pathway Fragments for Breast Cancer, Lung Cancer, and Melanoma in Public Healthcare Records。<details>
<summary>Abstract</summary>
Treatment pathways are step-by-step plans outlining the recommended medical care for specific diseases; they get revised when different treatments are found to improve patient outcomes. Examining health records is an important part of this revision process, but inferring patients' actual treatments from health data is challenging due to complex event-coding schemes and the absence of pathway-related annotations. This study aims to infer the actual treatment steps for a particular patient group from administrative health records (AHR) - a common form of tabular healthcare data - and address several technique- and methodology-based gaps in treatment pathway-inference research. We introduce Defrag, a method for examining AHRs to infer the real-world treatment steps for a particular patient group. Defrag learns the semantic and temporal meaning of healthcare event sequences, allowing it to reliably infer treatment steps from complex healthcare data. To our knowledge, Defrag is the first pathway-inference method to utilise a neural network (NN), an approach made possible by a novel, self-supervised learning objective. We also developed a testing and validation framework for pathway inference, which we use to characterise and evaluate Defrag's pathway inference ability and compare against baselines. We demonstrate Defrag's effectiveness by identifying best-practice pathway fragments for breast cancer, lung cancer, and melanoma in public healthcare records. Additionally, we use synthetic data experiments to demonstrate the characteristics of the Defrag method, and to compare Defrag to several baselines where it significantly outperforms non-NN-based methods. Defrag significantly outperforms several existing pathway-inference methods and offers an innovative and effective approach for inferring treatment pathways from AHRs. Open-source code is provided to encourage further research in this area.
</details>
<details>
<summary>摘要</summary>
医疗路径是一系列步骤计划，用于确定特定疾病的建议的医疗方案。这些路径不断地得到更新，当新的治疗方法提高患者结果时。查看医疗记录是这个 revision 过程的重要组成部分，但从医疗数据中推断患者的具体治疗步骤是困难的，因为医疗事件编码方案复杂，而且缺乏路径相关的注释。本研究旨在从医疗记录中推断患者特定群体的实际治疗步骤，并解决了一些技术和方法基础上的差距。我们提出了一种名为Defrag的方法，可以从医疗记录中推断实际治疗步骤。Defrag可以学习医疗事件序列的semantic和temporal意义，以可靠地从复杂医疗数据中推断治疗步骤。我们知道，Defrag是首个利用神经网络（NN）的医疗路径推断方法，这是由于我们提出的一种新的自主学习目标。我们还开发了一个用于医疗路径推断的测试和验证框架，用于评估和比较Defrag的路径推断能力，并与基eline相比。我们在公共医疗记录中identified breast cancer, lung cancer和melanoma的best-practice路径片段。此外，我们通过 sintetic data experiment demonstrates Defrag的特点，并与其他基eline相比，Defrag显示出显著的优势。Defrag signifiantly outperforms several existing pathway-inference methods and offers an innovative and effective approach for inferring treatment pathways from AHRs.我们提供了开源代码，以便进一步研究这个领域。
</details></li>
</ul>
<hr>
<h2 id="On-the-Planning-Search-and-Memorization-Capabilities-of-Large-Language-Models"><a href="#On-the-Planning-Search-and-Memorization-Capabilities-of-Large-Language-Models" class="headerlink" title="On the Planning, Search, and Memorization Capabilities of Large Language Models"></a>On the Planning, Search, and Memorization Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01868">http://arxiv.org/abs/2309.01868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhao Yang, Anshul Tomar</li>
<li>for: 这项研究探讨了使用最新的大语言模型GPT-4进行规划任务的可能性，并在多个规划子领域进行了广泛的检验。</li>
<li>methods: 本研究使用GPT-4进行规划领域EXTRACTION、图搜索路径规划和反对抗规划等多个任务的实验分析。</li>
<li>results: 研究发现GPT-4在规划领域中表现出色，但也存在一些约束限制其应用范围。提出了一种精通语言模型特定领域的微调方法来提高CoT能力。<details>
<summary>Abstract</summary>
The rapid advancement of large language models, such as the Generative Pre-trained Transformer (GPT) series, has had significant implications across various disciplines. In this study, we investigate the potential of the state-of-the-art large language model (GPT-4) for planning tasks. We explore its effectiveness in multiple planning subfields, highlighting both its strengths and limitations. Through a comprehensive examination, we identify areas where large language models excel in solving planning problems and reveal the constraints that limit their applicability. Our empirical analysis focuses on GPT-4's performance in planning domain extraction, graph search path planning, and adversarial planning. We then propose a way of fine-tuning a domain-specific large language model to improve its Chain of Thought (CoT) capabilities for the above-mentioned tasks. The results provide valuable insights into the potential applications of large language models in the planning domain and pave the way for future research to overcome their limitations and expand their capabilities.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>大语言模型的快速发展，如生成预训练变换器（GPT）系列，对各个领域产生了深远的影响。在这项研究中，我们研究了最新的州阶势language model（GPT-4）在规划任务中的潜力。我们探索它在多个规划子领域的效果，把握其优势和局限性。通过全面的分析，我们确定了大语言模型在解决规划问题的场景，以及它们的应用约束。我们的实验分析关注GPT-4在规划领域抽取、图搜索路径规划和反对抗规划等方面的性能。然后，我们提出了一种 fine-tuning 域特定的大语言模型来提高它的链条思维（CoT）能力，以便更好地应用于以上任务。结果提供了对大语言模型在规划领域的应用潜力和未来研究的指导。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Query-Based-Attack-against-ML-Based-Android-Malware-Detection-under-Zero-Knowledge-Setting"><a href="#Efficient-Query-Based-Attack-against-ML-Based-Android-Malware-Detection-under-Zero-Knowledge-Setting" class="headerlink" title="Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting"></a>Efficient Query-Based Attack against ML-Based Android Malware Detection under Zero Knowledge Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01866">http://arxiv.org/abs/2309.01866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping He, Yifan Xia, Xuhong Zhang, Shouling Ji</li>
<li>for: 本研究旨在提出一种高效的查询式攻击框架，用于对基于机器学习的Android黑客检测（AMD）方法进行攻击。</li>
<li>methods: 本研究使用了一种基于零知识的查询式攻击方法，可以在各种实际场景中进行攻击。</li>
<li>results: 对多种主流的机器学习基于AMD方法和现实世界的抗病毒解决方案进行了广泛的评估，并取得了出色的成绩。<details>
<summary>Abstract</summary>
The widespread adoption of the Android operating system has made malicious Android applications an appealing target for attackers. Machine learning-based (ML-based) Android malware detection (AMD) methods are crucial in addressing this problem; however, their vulnerability to adversarial examples raises concerns. Current attacks against ML-based AMD methods demonstrate remarkable performance but rely on strong assumptions that may not be realistic in real-world scenarios, e.g., the knowledge requirements about feature space, model parameters, and training dataset. To address this limitation, we introduce AdvDroidZero, an efficient query-based attack framework against ML-based AMD methods that operates under the zero knowledge setting. Our extensive evaluation shows that AdvDroidZero is effective against various mainstream ML-based AMD methods, in particular, state-of-the-art such methods and real-world antivirus solutions.
</details>
<details>
<summary>摘要</summary>
Android 操作系统的普及使得恶意应用程序成为了袭击者的目标。基于机器学习（ML）的 Android 恶意软件检测（AMD）方法是解决这个问题的关键，但它们受到了对抗示例的攻击的担忧。现有的对 ML-based AMD 方法的攻击方法具有惊人的性能，但它们假设了可能不是实际场景中的假设，例如特征空间、模型参数和训练集的知识要求。为解决这个限制，我们介绍了 AdvDroidZero，一种基于查询的攻击框架，在零知识设定下运行。我们的广泛评估表明，AdvDroidZero 对主流 ML-based AMD 方法和实际的反病毒解决方案都具有高效性。
</details></li>
</ul>
<hr>
<h2 id="BigFUSE-Global-Context-Aware-Image-Fusion-in-Dual-View-Light-Sheet-Fluorescence-Microscopy-with-Image-Formation-Prior"><a href="#BigFUSE-Global-Context-Aware-Image-Fusion-in-Dual-View-Light-Sheet-Fluorescence-Microscopy-with-Image-Formation-Prior" class="headerlink" title="BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior"></a>BigFUSE: Global Context-Aware Image Fusion in Dual-View Light-Sheet Fluorescence Microscopy with Image Formation Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01865">http://arxiv.org/abs/2309.01865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Liu, Gesine Muller, Nassir Navab, Carsten Marr, Jan Huisken, Tingying Peng</li>
<li>for: 提高LSFM图像质量，解决薄样品中光散射引起的图像模糊问题</li>
<li>methods: 使用双视图图像融合技术，根据两个视图的图像质量进行地方性比较，以确定具有高对比度的焦点 pixels</li>
<li>results: 提出了BigFUSE全局上下文感知图像融合方法，可以在LSFM中稳定图像融合，并且可以排除结构化噪声，从而提高图像质量<details>
<summary>Abstract</summary>
Light-sheet fluorescence microscopy (LSFM), a planar illumination technique that enables high-resolution imaging of samples, experiences defocused image quality caused by light scattering when photons propagate through thick tissues. To circumvent this issue, dualview imaging is helpful. It allows various sections of the specimen to be scanned ideally by viewing the sample from opposing orientations. Recent image fusion approaches can then be applied to determine in-focus pixels by comparing image qualities of two views locally and thus yield spatially inconsistent focus measures due to their limited field-of-view. Here, we propose BigFUSE, a global context-aware image fuser that stabilizes image fusion in LSFM by considering the global impact of photon propagation in the specimen while determining focus-defocus based on local image qualities. Inspired by the image formation prior in dual-view LSFM, image fusion is considered as estimating a focus-defocus boundary using Bayes Theorem, where (i) the effect of light scattering onto focus measures is included within Likelihood; and (ii) the spatial consistency regarding focus-defocus is imposed in Prior. The expectation-maximum algorithm is then adopted to estimate the focus-defocus boundary. Competitive experimental results show that BigFUSE is the first dual-view LSFM fuser that is able to exclude structured artifacts when fusing information, highlighting its abilities of automatic image fusion.
</details>
<details>
<summary>摘要</summary>
光Sheet fluorescence微scopía（LSFM），一种平面照明技术，可以实现高分辨率图像的取得，但光子在厚度的样本中传播时会导致图像模糊。为了解决这问题，双视图成像是有帮助的。它可以在不同的方向上扫描样本，从而实现不同部分的样本的高分辨率扫描。然而，当应用最新的图像融合方法时，由于其有限的场景视野，会导致图像融合失真。在这种情况下，我们提出了BigFUSE，一种全局上下文认知的图像融合器，可以在LSFM中稳定图像融合，并且考虑了光子在样本中的全局影响。通过对本地图像质量进行比较，BigFUSE可以计算出各个像素的封闭度，并且通过 bayes定理来确定注重点。在应用期望最大算法时，BigFUSE可以优先地除掉结构化遗憾。实验结果表明，BigFUSE是第一个可以自动执行图像融合的双视图LSFM融合器。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.AI_2023_09_05/" data-id="clp9qz7z0003fok880y5xhs4i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.CL_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T11:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.CL_2023_09_05/">cs.CL - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning"><a href="#Scaling-Autoregressive-Multi-Modal-Models-Pretraining-and-Instruction-Tuning" class="headerlink" title="Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning"></a>Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02591">http://arxiv.org/abs/2309.02591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/CM3Leon">https://github.com/kyegomez/CM3Leon</a></li>
<li>paper_authors: Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, Armen Aghajanyan</li>
<li>for: 这篇论文是为了描述一种基于多模态语言模型的文本和图像生成模型CM3Leon，以及该模型在不同任务上的性能。</li>
<li>methods: 该模型使用了CM3多模态架构，并在大规模的采集和调参数数据上进行了扩展和优化。它还包括一个大规模的预训练阶段和一个多任务练熟环境（SFT）阶段。</li>
<li>results: 实验结果显示，这种方法对多模态模型是非常有效的，CM3Leon在文本到图像生成任务中达到了状态对的性能（FID&#x3D;4.88），并且在语言指导图像编辑、图像控制生成和分割等任务中也可以达到了不可思议的水平。<details>
<summary>Abstract</summary>
We present CM3Leon (pronounced "Chameleon"), a retrieval-augmented, token-based, decoder-only multi-modal language model capable of generating and infilling both text and images. CM3Leon uses the CM3 multi-modal architecture but additionally shows the extreme benefits of scaling up and tuning on more diverse instruction-style data. It is the first multi-modal model trained with a recipe adapted from text-only language models, including a large-scale retrieval-augmented pre-training stage and a second multi-task supervised fine-tuning (SFT) stage. It is also a general-purpose model that can do both text-to-image and image-to-text generation, allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs. Extensive experiments demonstrate that this recipe is highly effective for multi-modal models. CM3Leon achieves state-of-the-art performance in text-to-image generation with 5x less training compute than comparable methods (zero-shot MS-COCO FID of 4.88). After SFT, CM3Leon can also demonstrate unprecedented levels of controllability in tasks ranging from language-guided image editing to image-controlled generation and segmentation.
</details>
<details>
<summary>摘要</summary>
我们提出CM3Leon（发音为“卡美伦”），这是一个基于搜索修正的、符号基于的解码器只多模态语言模型，可以生成和填充文本和图像。CM3Leon使用CM3多模态架构，但还有更加极端的优势，来自更多的指令样式数据的扩大和调整。它是首个基于文本only语言模型的多模态模型，通过一个大规模的搜索修正预训练阶段和第二个多任务监督练练（SFT）阶段进行训练。它还是一个通用的模型，可以进行文本到图像和图像到文本的生成，allowing us to introduce self-contained contrastive decoding methods that produce high-quality outputs。广泛的实验表明，这种方法对多模态模型非常有效。CM3Leon在文本到图像生成中达到了比较方法的状态机器（零shot MS-COCO FID of 4.88）。在SFT后，CM3Leon也可以展示无 precedent的可控性，从语言引导的图像修改到图像控制生成和分割。
</details></li>
</ul>
<hr>
<h2 id="Substitution-based-Semantic-Change-Detection-using-Contextual-Embeddings"><a href="#Substitution-based-Semantic-Change-Detection-using-Contextual-Embeddings" class="headerlink" title="Substitution-based Semantic Change Detection using Contextual Embeddings"></a>Substitution-based Semantic Change Detection using Contextual Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02403">http://arxiv.org/abs/2309.02403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dallascard/SBSCD">https://github.com/dallascard/SBSCD</a></li>
<li>paper_authors: Dallas Card</li>
<li>for: 本研究旨在使用上下文嵌入来度量语义变化，并且提出了一种简单有效的方法，以优化现有的方法。</li>
<li>methods: 本研究使用最有可能的替换词来度量语义变化，这种方法不仅直观可解，而且更有效率，可以更好地探讨语义变化。</li>
<li>results: 本研究在最常引用的数据集上达到了最高的均值性能，并且可以更好地探讨语义变化，比静止词vec更有利于理解语义变化。<details>
<summary>Abstract</summary>
Measuring semantic change has thus far remained a task where methods using contextual embeddings have struggled to improve upon simpler techniques relying only on static word vectors. Moreover, many of the previously proposed approaches suffer from downsides related to scalability and ease of interpretation. We present a simplified approach to measuring semantic change using contextual embeddings, relying only on the most probable substitutes for masked terms. Not only is this approach directly interpretable, it is also far more efficient in terms of storage, achieves superior average performance across the most frequently cited datasets for this task, and allows for more nuanced investigation of change than is possible with static word vectors.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="nanoT5-A-PyTorch-Framework-for-Pre-training-and-Fine-tuning-T5-style-Models-with-Limited-Resources"><a href="#nanoT5-A-PyTorch-Framework-for-Pre-training-and-Fine-tuning-T5-style-Models-with-Limited-Resources" class="headerlink" title="nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources"></a>nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02373">http://arxiv.org/abs/2309.02373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/piotrnawrot/nanot5">https://github.com/piotrnawrot/nanot5</a></li>
<li>paper_authors: Piotr Nawrot</li>
<li>for: 提高语言模型研究的可用性和资源利用率，使更多研究者能够访问和使用T5模型。</li>
<li>methods: 通过优化PyTorch框架和优化器，实现高效的T5模型预训练和精度调整，以及开源框架和配置等资源的提供，旨在拓宽语言模型研究领域的可用性和资源利用率。</li>
<li>results: 在单个GPU上预训练T5-Base模型只需16个小时，不会影响性能，并提供了多种配置和软硬件准则，以及开源框架和预训练模型，以满足研究者对T5模型的需求。<details>
<summary>Abstract</summary>
State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations. Our contributions, including configurations, codebase, software/hardware insights, and pre-trained models, are available to the public, aiming to strike a balance between research accessibility and resource constraints in NLP.
</details>
<details>
<summary>摘要</summary>
现代语言模型如T5已经革命化了NLPT中的景象，但它们的计算需求限制了大量研究人员。为解决这个挑战，我们现在提出nanoT5，一个特殊优化的PyTorch框架，用于高效地预训练和精度调整T5模型。通过优化器差异和高效性的启发，nanoT5可以在单个GPU上预训练T5-Base模型，只需16个小时，而无损失性表现。我们通过这个开源框架，希望扩大语言模型研究的访问权限，并为NLPT社区提供更加用户友好的T5（Encoder-Decoder）实现。我们的贡献包括配置、代码库、软硬件杂志和预训练模型，都对公众开放，以实现NLPT研究资源的平衡。
</details></li>
</ul>
<hr>
<h2 id="Weigh-Your-Own-Words-Improving-Hate-Speech-Counter-Narrative-Generation-via-Attention-Regularization"><a href="#Weigh-Your-Own-Words-Improving-Hate-Speech-Counter-Narrative-Generation-via-Attention-Regularization" class="headerlink" title="Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization"></a>Weigh Your Own Words: Improving Hate Speech Counter Narrative Generation via Attention Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02311">http://arxiv.org/abs/2309.02311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/milanlproc/weigh-your-own-words">https://github.com/milanlproc/weigh-your-own-words</a></li>
<li>paper_authors: Helena Bonaldi, Giuseppe Attanasio, Debora Nozza, Marco Guerini</li>
<li>for: 防止在线仇恨言语的发展，提出了一种基于预训练语言模型（PLMs）的自动生成对话方法。</li>
<li>methods: 本研究使用了一种基于注意力的违规常量来改进PLMs的泛化能力，以便在不同的目标和实际垃圾语言上生成更加多样化和更加丰富的对话。</li>
<li>results: 对英语 benchmark 数据集进行实验表明，使用了注意力违规常量的改进方法可以生成更好的对话，特别是在训练数据中不包含仇恨目标时。<details>
<summary>Abstract</summary>
Recent computational approaches for combating online hate speech involve the automatic generation of counter narratives by adapting Pretrained Transformer-based Language Models (PLMs) with human-curated data. This process, however, can produce in-domain overfitting, resulting in models generating acceptable narratives only for hatred similar to training data, with little portability to other targets or to real-world toxic language. This paper introduces novel attention regularization methodologies to improve the generalization capabilities of PLMs for counter narratives generation. Overfitting to training-specific terms is then discouraged, resulting in more diverse and richer narratives. We experiment with two attention-based regularization techniques on a benchmark English dataset. Regularized models produce better counter narratives than state-of-the-art approaches in most cases, both in terms of automatic metrics and human evaluation, especially when hateful targets are not present in the training data. This work paves the way for better and more flexible counter-speech generation models, a task for which datasets are highly challenging to produce.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:近期计算方法对于在线仇恨言语的应对包括使用预训练的变换器基于语言模型（PLMs）自动生成反对narritives。然而，这个过程可能会导致域内过拟合，使模型只能生成与训练数据相似的acceptable narritives，具有小的可移植性到其他目标或实际世界中的恶语言。本文提出了一种新的注意力规范方法来提高PLMs的泛化能力 для反对narritives生成。通过避免训练数据特定的注意力过拟合，模型可以生成更多元和更加丰富的narritives。我们在一个英语 benchmark 数据集上实验了两种注意力基于规范技术，并发现正则化模型在大多数情况下可以生成更好的反对narritives，特别是当仇恨目标不在训练数据中时。这项工作为Counter-speech生成模型的更好和更灵活的模型开创了道路，这个任务的数据非常困难生产。
</details></li>
</ul>
<hr>
<h2 id="PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt"><a href="#PromptTTS-2-Describing-and-Generating-Voices-with-Text-Prompt" class="headerlink" title="PromptTTS 2: Describing and Generating Voices with Text Prompt"></a>PromptTTS 2: Describing and Generating Voices with Text Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02285">http://arxiv.org/abs/2309.02285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichong Leng, Zhifang Guo, Kai Shen, Xu Tan, Zeqian Ju, Yanqing Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiang-Yang Li, Sheng Zhao, Tao Qin, Jiang Bian</li>
<li>for: 这个研究是为了解决基于文本提示的语音生成方法中的一个问题，即使用文本提示来生成语音时，不能完全捕捉语音中的声音变化信息。</li>
<li>methods: 这个研究使用了一种变换网络，该网络可以根据文本提示来预测语音中的声音变化信息，以及一个提取ipeline，该ipeline可以使用语音理解模型来识别语音中的声音特征（例如性别、速度等），并使用大型自然语言处理模型来形成文本提示。</li>
<li>results: 实验结果表明，与前一代方法相比，PromptTTS 2可以更好地根据文本提示生成语音，并且支持采样多种语音变化，因此可以为用户提供更多的语音选择。此外，提取ipeline可以生成高质量的文本提示，从而消除大量的标注成本。<details>
<summary>Abstract</summary>
Speech conveys more information than just text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompt for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variation network predicts the representation extracted from the reference speech (which contains full information about voice) based on the text prompt representation. For the prompt generation pipeline, it generates text prompts for speech with a speech understanding model to recognize voice attributes (e.g., gender, speed) from speech and a large language model to formulate text prompt based on the recognition results. Experiments on a large-scale (44K hours) speech dataset demonstrate that compared to the previous works, PromptTTS 2 generates voices more consistent with text prompts and supports the sampling of diverse voice variability, thereby offering users more choices on voice generation. Additionally, the prompt generation pipeline produces high-quality prompts, eliminating the large labeling cost. The demo page of PromptTTS 2 is available online\footnote{https://speechresearch.github.io/prompttts2}.
</details>
<details>
<summary>摘要</summary>
文本中的语音包含更多信息，因为同一个词可以在不同的声音下被读出，表达多种信息。相比传统的文本识别（TTS）方法，利用语音提示（参考语音）来实现声音多样性，使用文本提示（描述）更加用户友好，因为语音提示可能困难找或者不存在。TTS方法基于文本提示面临两个挑战：1）一个多个问题，即文本提示中不能完全表达声音多样性的细节信息；2）文本提示数据集的有限性，需要供应商和大量的数据标注来编写文本提示。在这项工作中，我们介绍PromptTTS 2，以解决这两个挑战。PromptTTS 2使用变化网络提供不同声音的多样性信息，并使用大语言模型（LLM）组合高质量文本提示来生成语音。具体来说，变化网络预测基于参考语音（含有全部声音信息）的表示，根据文本提示表示。为生成文本提示，我们使用语音理解模型认识语音特征（例如性别、速度），并使用大语言模型根据认识结果组合文本提示。实验表明，Compared to previous works，PromptTTS 2可以更好地根据文本提示生成声音，并支持采样多样的声音选择。此外，提示生成管道可以生成高质量的提示，减少大量标注成本。PromptTTS 2的demo页面可以在线查看\footnotesize{\url{https://speechresearch.github.io/prompttts2}.
</details></li>
</ul>
<hr>
<h2 id="Dialog-Action-Aware-Transformer-for-Dialog-Policy-Learning"><a href="#Dialog-Action-Aware-Transformer-for-Dialog-Policy-Learning" class="headerlink" title="Dialog Action-Aware Transformer for Dialog Policy Learning"></a>Dialog Action-Aware Transformer for Dialog Policy Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02240">http://arxiv.org/abs/2309.02240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huimin Wang, Wai-Chung Kwan, Kam-Fai Wong</li>
<li>for: 这个研究旨在提高对话策略学习（Dialog Policy Learning，DPL）的效率，使用对话数据来增强RL代理人的学习速度。</li>
<li>methods: 本研究提出了一个叫做“对话动作意识”的对话动作批评（DaTrans），该批评通过一个新的调整程序“对话最后一个动作任务”来增强DaTrans的对话意识和动作特征。</li>
<li>results: 研究结果显示，这个方法可以快速地将RL代理人带到最佳的对话策略，并且在人类评价中得到了良好的评价。<details>
<summary>Abstract</summary>
Recent works usually address Dialog policy learning DPL by training a reinforcement learning (RL) agent to determine the best dialog action. However, existing works on deep RL require a large volume of agent-user interactions to achieve acceptable performance. In this paper, we propose to make full use of the plain text knowledge from the pre-trained language model to accelerate the RL agent's learning speed. Specifically, we design a dialog action-aware transformer encoder (DaTrans), which integrates a new fine-tuning procedure named masked last action task to encourage DaTrans to be dialog-aware and distils action-specific features. Then, DaTrans is further optimized in an RL setting with ongoing interactions and evolves through exploration in the dialog action space toward maximizing long-term accumulated rewards. The effectiveness and efficiency of the proposed model are demonstrated with both simulator evaluation and human evaluation.
</details>
<details>
<summary>摘要</summary>
现代工作通常采用对话策略学习（Dialog Policy Learning，DPL），通过训练一个强化学习（Reinforcement Learning，RL）代理人来确定最佳对话动作。然而，现有的深度RL需要大量的代理人-用户互动来 достичьacceptable的性能。在这篇论文中，我们提议利用预先训练的自然语言模型的普通文本知识，以加速RL代理人的学习速度。特别是，我们设计了对话动作意识的 transformer 编码器（DaTrans），通过一种新的精细调整过程名为遮盖最后一个动作任务来鼓励 DaTrans 成为对话意识的。然后，DaTrans 在RL Setting中进行了进一步优化，通过在对话动作空间中的探索来最大化长期积累的奖励。我们通过 simulate 评估和人类评估来证明提案的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Dictionaries-into-a-Neural-Network-Architecture-to-Extract-COVID-19-Medical-Concepts-From-Social-Media"><a href="#Incorporating-Dictionaries-into-a-Neural-Network-Architecture-to-Extract-COVID-19-Medical-Concepts-From-Social-Media" class="headerlink" title="Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media"></a>Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02188">http://arxiv.org/abs/2309.02188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abul Hasan, Mark Levene, David Weston</li>
<li>for: 这研究探讨了将字典信息 integrate into neural network architecture for natural language processing的可能性。</li>
<li>methods: 这研究使用了一种基于字典的深度学习模型，用于提取COVID-19相关的概念。</li>
<li>results: 研究结果显示，将小领域字典 integrate into深度学习模型可以提高概念提取任务的性能，并且这些模型可以在不同数据集上进行转移。<details>
<summary>Abstract</summary>
We investigate the potential benefit of incorporating dictionary information into a neural network architecture for natural language processing. In particular, we make use of this architecture to extract several concepts related to COVID-19 from an on-line medical forum. We use a sample from the forum to manually curate one dictionary for each concept. In addition, we use MetaMap, which is a tool for extracting biomedical concepts, to identify a small number of semantic concepts. For a supervised concept extraction task on the forum data, our best model achieved a macro $F_1$ score of 90\%. A major difficulty in medical concept extraction is obtaining labelled data from which to build supervised models. We investigate the utility of our models to transfer to data derived from a different source in two ways. First for producing labels via weak learning and second to perform concept extraction. The dataset we use in this case comprises COVID-19 related tweets and we achieve an $F_1$ score 81\% for symptom concept extraction trained on weakly labelled data. The utility of our dictionaries is compared with a COVID-19 symptom dictionary that was constructed directly from Twitter. Further experiments that incorporate BERT and a COVID-19 version of BERTweet demonstrate that the dictionaries provide a commensurate result. Our results show that incorporating small domain dictionaries to deep learning models can improve concept extraction tasks. Moreover, models built using dictionaries generalize well and are transferable to different datasets on a similar task.
</details>
<details>
<summary>摘要</summary>
我们研究将词典信息 integrate into neural network architecture for natural language processing的潜在优点。特别是我们使用这种架构提取COVID-19相关概念从在线医学讨论区。我们使用样本从讨论区手动精心抽取一个词典 для每个概念。此外，我们使用MetaMap工具提取生物医学概念，以确定一些semantic概念。对于基于讨论区数据的抽象概念提取任务，我们的最佳模型达到了90%的macro $F_1$ 分数。医疗概念提取的主要挑战之一是获得可靠的标签数据，用于建立supervised模型。我们研究将我们的模型传输到不同来源数据上进行两种方式。第一种是通过弱学习生成标签，第二种是进行概念提取。我们使用COVID-19相关推特来构建数据集，并实现了基于弱标签的概念提取Task中的81%的$F_1$ 分数。我们的词典与直接从Twitter中构建的COVID-19症状词典进行比较。进一步的实验表明，我们的词典提供了相似的结果。我们的结果表明，将小域词典 integrate into深度学习模型可以提高概念提取任务的性能。此外，使用词典建立的模型具有良好的泛化能力和可传播性。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Text-to-GLOSS-Neural-Translation-Using-a-Novel-Hyper-parameter-Optimization-Technique"><a href="#Advancing-Text-to-GLOSS-Neural-Translation-Using-a-Novel-Hyper-parameter-Optimization-Technique" class="headerlink" title="Advancing Text-to-GLOSS Neural Translation Using a Novel Hyper-parameter Optimization Technique"></a>Advancing Text-to-GLOSS Neural Translation Using a Novel Hyper-parameter Optimization Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02162">http://arxiv.org/abs/2309.02162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Younes Ouargani, Noussaima El Khattabi</li>
<li>for: 这paper是 investigate transformers for Neural Machine Translation of text-to-GLOSS, 用于提高Deaf和听力不良的通信中的GLOSS翻译的精度和流畅性。</li>
<li>methods: 这paper使用了一种新的超参数搜索技术，搜索了不同的架构参数，并构建了一个优化的 transformer-based 架构，特意适用于text-to-GLOSS翻译任务。</li>
<li>results: 实验结果表明，最佳的 transformer 架构在 PHOENIX14T 数据集上达到了 ROUGE 分数55.18% 和 BLEU-1 分数63.6%，超过了之前在同一数据集上的最佳结果，升级了 BLEU1 和 ROUGE 分数的状态之作。<details>
<summary>Abstract</summary>
In this paper, we investigate the use of transformers for Neural Machine Translation of text-to-GLOSS for Deaf and Hard-of-Hearing communication. Due to the scarcity of available data and limited resources for text-to-GLOSS translation, we treat the problem as a low-resource language task. We use our novel hyper-parameter exploration technique to explore a variety of architectural parameters and build an optimal transformer-based architecture specifically tailored for text-to-GLOSS translation. The study aims to improve the accuracy and fluency of Neural Machine Translation generated GLOSS. This is achieved by examining various architectural parameters including layer count, attention heads, embedding dimension, dropout, and label smoothing to identify the optimal architecture for improving text-to-GLOSS translation performance. The experiments conducted on the PHOENIX14T dataset reveal that the optimal transformer architecture outperforms previous work on the same dataset. The best model reaches a ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score of 55.18% and a BLEU-1 (BiLingual Evaluation Understudy 1) score of 63.6%, outperforming state-of-the-art results on the BLEU1 and ROUGE score by 8.42 and 0.63 respectively.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究使用变换器来进行神经机器翻译文本到GLOSS，以便为听力异常和耳聋人士进行交流。由于文本到GLOSS翻译数据的稀缺和限制了资源，我们将这个问题视为低资源语言任务。我们使用我们的新的 гипер参数探索技术来探索各种建筑 Parameters，并构建一个优化的变换器基础结构，专门适用于文本到GLOSS翻译。研究的目的是提高神经机器翻译生成的GLOSS的准确率和流畅度。我们通过检查层数、注意头数、嵌入维度、dropout和标签平滑来确定优化文本到GLOSS翻译性能的最佳建筑 Parameters。在PHOENIX14T数据集上进行的实验表明，优化的变换器结构可以超越之前在同一数据集上的成果。最佳模型在ROUGE（Recall-Oriented Understudy for Gisting Evaluation）分数上达到55.18%，并在BLEU-1（BiLingual Evaluation Understudy 1）分数上达到63.6%，超越了当前的BLEU1和ROUGE分数的状态态度。
</details></li>
</ul>
<hr>
<h2 id="Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition"><a href="#Bring-the-Noise-Introducing-Noise-Robustness-to-Pretrained-Automatic-Speech-Recognition" class="headerlink" title="Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition"></a>Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02145">http://arxiv.org/abs/2309.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Eickhoff, Matthias Möller, Theresa Pekarek Rosin, Johannes Twiefel, Stefan Wermter</li>
<li>for: 这研究旨在提高自动语音识别（ASR）系统的性能，特别是在听力条件不佳的情况下。</li>
<li>methods: 我们提出了一种新的方法，可以将大型端到端（E2E）模型中的干净能力提取出来，并将其应用于任何encoder-decoder架构。我们的方法基于Conformer ASR模型的隐藏活动，通过一个decoder来预测干净spectrogram。</li>
<li>results: 我们的模型可以成功地过滤听力条件下的噪音，并且可以提高下游模型在噪音条件下的总词错率（WER）。我们的模型可以作为前端应用于预训练的Conformer ASR模型，以及从头开始训练小型Conformer ASR模型。<details>
<summary>Abstract</summary>
In recent research, in the domain of speech processing, large End-to-End (E2E) systems for Automatic Speech Recognition (ASR) have reported state-of-the-art performance on various benchmarks. These systems intrinsically learn how to handle and remove noise conditions from speech. Previous research has shown, that it is possible to extract the denoising capabilities of these models into a preprocessor network, which can be used as a frontend for downstream ASR models. However, the proposed methods were limited to specific fully convolutional architectures. In this work, we propose a novel method to extract the denoising capabilities, that can be applied to any encoder-decoder architecture. We propose the Cleancoder preprocessor architecture that extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our pre-processor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs. Then, we evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. We show that the Cleancoder is able to filter noise from speech and that it improves the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details>
<details>
<summary>摘要</summary>
Recent research in speech processing has shown that large End-to-End (E2E) systems for Automatic Speech Recognition (ASR) have achieved state-of-the-art performance on various benchmarks. These systems have the ability to intrinsically handle and remove noise from speech. Previous studies have demonstrated that the denoising capabilities of these models can be extracted and used as a frontend for downstream ASR models. However, these methods were limited to specific fully convolutional architectures.In this study, we propose a novel method to extract the denoising capabilities that can be applied to any encoder-decoder architecture. We introduce the Cleancoder preprocessor architecture, which extracts hidden activations from the Conformer ASR model and feeds them to a decoder to predict denoised spectrograms. We train our pre-processor on the Noisy Speech Database (NSD) to reconstruct denoised spectrograms from noisy inputs.We evaluate our model as a frontend to a pretrained Conformer ASR model as well as a frontend to train smaller Conformer ASR models from scratch. Our results show that the Cleancoder is able to filter noise from speech and improve the total Word Error Rate (WER) of the downstream model in noisy conditions for both applications.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion"><a href="#Evaluating-Methods-for-Ground-Truth-Free-Foreign-Accent-Conversion" class="headerlink" title="Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion"></a>Evaluating Methods for Ground-Truth-Free Foreign Accent Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02133">http://arxiv.org/abs/2309.02133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/unilight/seq2seq-vc">https://github.com/unilight/seq2seq-vc</a></li>
<li>paper_authors: Wen-Chin Huang, Tomoki Toda</li>
<li>For: 本研究旨在评估三种最近提出的无ground truth基础的外语变换方法（FAC），以实现将非本地语言speaker的语音转换为本地语言speaker的语音，同时保持speaker identity。* Methods: 本研究使用的方法包括seq2seq和非并行的VC模型，以实现控制speaker identity和降低外语变换的困难性。* Results: 我们的实验评估结果显示，无一个方法在所有评估轴上表现出色，与之前的研究结论不同。我们还分析了seq2seq模型的训练输入和输出，以及非并行VC模型的设计选择，并发现Intelligibility指标与主观外语程度之间没有直接关系。<details>
<summary>Abstract</summary>
Foreign accent conversion (FAC) is a special application of voice conversion (VC) which aims to convert the accented speech of a non-native speaker to a native-sounding speech with the same speaker identity. FAC is difficult since the native speech from the desired non-native speaker to be used as the training target is impossible to collect. In this work, we evaluate three recently proposed methods for ground-truth-free FAC, where all of them aim to harness the power of sequence-to-sequence (seq2seq) and non-parallel VC models to properly convert the accent and control the speaker identity. Our experimental evaluation results show that no single method was significantly better than the others in all evaluation axes, which is in contrast to conclusions drawn in previous studies. We also explain the effectiveness of these methods with the training input and output of the seq2seq model and examine the design choice of the non-parallel VC model, and show that intelligibility measures such as word error rates do not correlate well with subjective accentedness. Finally, our implementation is open-sourced to promote reproducible research and help future researchers improve upon the compared systems.
</details>
<details>
<summary>摘要</summary>
外国腔转换（FAC）是voice转换（VC）的特殊应用，旨在将非本地语言 speaker的折衣语音转换为本地语言 speaker的Native-sounding speech，同时保持 speaker identity。FAC具有困难，因为不可收集欲使用的Native speech from the desired non-native speaker作为训练目标。在这项工作中，我们评估了三种最近提出的ground-truth-free FAC方法，其中所有方法均企图利用 seq2seq和non-parallel VC模型来正确地转换腔和控制 speaker identity。我们的实验评估结果表明，没有任何方法在所有评估轴上表现出显著优势，这与之前的研究结论不符。我们还解释了这些方法的效iveness，并检查了seq2seq模型的训练输入和输出，以及非平行VC模型的设计选择。最后，我们发现Intelligibility measure如word error rates与主观腔度之间没有正确的相关性。 finally,我们开源了我们的实现，以便促进可重复性的研究和未来的研究人员可以在此基础上改进相关的系统。
</details></li>
</ul>
<hr>
<h2 id="Wordle-A-Microcosm-of-Life-Luck-Skill-Cheating-Loyalty-and-Influence"><a href="#Wordle-A-Microcosm-of-Life-Luck-Skill-Cheating-Loyalty-and-Influence" class="headerlink" title="Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!"></a>Wordle: A Microcosm of Life. Luck, Skill, Cheating, Loyalty, and Influence!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02110">http://arxiv.org/abs/2309.02110</a></li>
<li>repo_url: None</li>
<li>paper_authors: James P. Dilger</li>
<li>for: 这个研究是为了研究Wordle游戏中玩家的做法和习惯。</li>
<li>methods: 这个研究使用了信息理论来评估玩家的幸运和技巧，并将数据显示在Wordle游戏中的第一、第二、…、第六个猜测中。</li>
<li>results: 研究发现每天约有0.2-0.5%的玩家在第一次猜测中解题成功，这意味着4,000-10,000名玩家可能通过外部获取目标词语来夺冠。此外，研究还发现至少1&#x2F;3的玩家有一个喜爱的开头词，而且大多数玩家会保持loyal于他们的开头词，即使该词语已经出现过。8月15日，约有30,000名玩家突然改变了他们的开头词，这可能是基于十字WORD的游戏提示。<details>
<summary>Abstract</summary>
Wordle is a popular, online word game offered by the New York Times (nytimes.com). Currently there are some 2 million players of the English version worldwide. Players have 6 attempts to guess the daily word (target word) and after each attempt, the player receives color-coded information about the correctness and position of each letter in the guess. After either a successful completion of the puzzle or the final unsuccessful attempt, software can assess the player's luck and skill using Information Theory and can display data for the first, second, ..., sixth guesses of a random sample of all players. Recently, I discovered that the latter data is presented in a format that can easily be copied and pasted into a spreadsheet. I compiled data on Wordle players' first guesses from May 2023 - August 2023 and inferred some interesting information about Wordle players. A) Every day, about 0.2-0.5% of players solve the puzzle in one attempt. Because the odds of guessing the one of 2,315 possible target words at random is 0.043%, this implies that 4,000 - 10,000 players cheat by obtaining the target word outside of playing the game! B) At least 1/3 of the players have a favorite starting word, or cycle through several. And even though players should be aware that target words are never repeated, most players appear to remain loyal to their starting word even after its appearance as a target word. C) On August 15, 2023, about 30,000 players abruptly changed their starting word, presumably based on a crossword puzzle clue! Wordle players can be influenced! This study goes beyond social media postings, surveys, and Google Trends to provide solid, quantitative evidence about cheating in Wordle.
</details>
<details>
<summary>摘要</summary>
wordle是一款受欢迎的在线单词游戏，提供于纽约时报（nytimes.com）上。目前全球玩家约200万人。玩家有6次尝试猜测每天的目标词（target word），每次猜测后，玩家会收到颜色标注的正确性和位置信息。完成游戏或最后一次无法猜测后，软件可以根据信息理论评估玩家的运气和技巧，并显示数据 для第一、第二、...、第六次猜测的随机样本玩家。我最近发现这些数据可以轻松地复制并粘贴到表格中。我 compile了5月2023年-8月2023年的Wordle玩家首次猜测数据，并从中推导出了一些有趣的信息。A) 每天大约0.2%-0.5%的玩家在第一次猜测中解题成功。由于随机猜测target word的概率为0.043%，这 imply That 4,000-10,000名玩家通过外部方式获得target word！B) 至少1/3的玩家有一个喜爱的开始词，或者循环使用多个。尽管玩家应该知道target words never repeated，但大多数玩家仍然偏向自己的开始词，即使该词已经出现在目标词中。C) 2023年8月15日，约30,000名玩家 suddenly changed their starting word， apparently based on a crossword puzzle clue! Wordle players can be influenced！这项研究超过社交媒体帖子、调查和Google Trends提供的轻量级证据，以准确的数据证明Wordle玩家的作弊行为。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Emotion-Role-Labeling-and-Appraisal-based-Emotion-Analysis"><a href="#Bridging-Emotion-Role-Labeling-and-Appraisal-based-Emotion-Analysis" class="headerlink" title="Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis"></a>Bridging Emotion Role Labeling and Appraisal-based Emotion Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02092">http://arxiv.org/abs/2309.02092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roman Klinger</li>
<li>for: 本研究旨在探讨情感分析在文本中的应用，具体来说是情感分类和情感角色标注两个方面。</li>
<li>methods: 本研究使用了多种自然语言处理技术，包括情感分类和情感角色标注等。</li>
<li>results: 研究发现了情感分类和情感角色标注两个方面的问题，并提出了一些未解决的研究问题。<details>
<summary>Abstract</summary>
The term emotion analysis in text subsumes various natural language processing tasks which have in common the goal to enable computers to understand emotions. Most popular is emotion classification in which one or multiple emotions are assigned to a predefined textual unit. While such setting is appropriate to identify the reader's or author's emotion, emotion role labeling adds the perspective of mentioned entities and extracts text spans that correspond to the emotion cause. The underlying emotion theories agree on one important point; that an emotion is caused by some internal or external event and comprises several subcomponents, including the subjective feeling and a cognitive evaluation. We therefore argue that emotions and events are related in two ways. (1) Emotions are events; and this perspective is the fundament in NLP for emotion role labeling. (2) Emotions are caused by events; a perspective that is made explicit with research how to incorporate psychological appraisal theories in NLP models to interpret events. These two research directions, role labeling and (event-focused) emotion classification, have by and large been tackled separately. We contributed to both directions with the projects SEAT (Structured Multi-Domain Emotion Analysis from Text) and CEAT (Computational Event Evaluation based on Appraisal Theories for Emotion Analysis), both funded by the German Research Foundation. In this paper, we consolidate the findings and point out open research questions.
</details>
<details>
<summary>摘要</summary>
“情感分析”是一种自然语言处理任务的总称，它的目的是让计算机理解人类的情感。最受欢迎的是情感分类，在这种设定下，一个或多个情感被分配给已知文本单位。而情感角色标注则添加了提及对象的视角，并提取与情感相关的文本块。在情感理论中，所有情感都是由内部或外部事件引起的，并包括一些主观感受和认知评价。因此，我们认为情感和事件之间存在两种关系。第一种是情感是事件的角度，这是NP的基础。第二种是情感是由事件引起的，这种角度通过涉及心理评价理论来在NP模型中表示。这两个研究方向一直处理了分开，我们通过项目《SEAT》（结构多元领域情感分析从文本）和《CEAT》（基于评价理论的计算事件评价为情感分析），均得到了德国研究基金的资金支持。在这篇论文中，我们汇总了发现和提出了未来研究的问题。
</details></li>
</ul>
<hr>
<h2 id="An-Automatic-Evaluation-Framework-for-Multi-turn-Medical-Consultations-Capabilities-of-Large-Language-Models"><a href="#An-Automatic-Evaluation-Framework-for-Multi-turn-Medical-Consultations-Capabilities-of-Large-Language-Models" class="headerlink" title="An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models"></a>An Automatic Evaluation Framework for Multi-turn Medical Consultations Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02077">http://arxiv.org/abs/2309.02077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusheng Liao, Yutong Meng, Hongcheng Liu, Yanfeng Wang, Yu Wang</li>
<li>for: 这篇论文旨在评估大语言模型（LLMs）在虚拟医生环境中的实际能力。</li>
<li>methods: 该论文提出了一种自动评估框架，用于评估 LLMs 在多turn 询问中的实际能力。该框架包括设计了供询问任务，要求 LLMs 了解自己所不知道的信息，并从患者那里收集缺失的医疗信息。</li>
<li>results: 实验结果显示，通过 fine-tuning 训练集可以减轻 LLMs 的假设现象，提高其在提posed的benchmark上的表现。这些结果得到了广泛的实验和剥夺学调查的验证。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved significant success in interacting with human. However, recent studies have revealed that these models often suffer from hallucinations, leading to overly confident but incorrect judgments. This limits their application in the medical domain, where tasks require the utmost accuracy. This paper introduces an automated evaluation framework that assesses the practical capabilities of LLMs as virtual doctors during multi-turn consultations. Consultation tasks are designed to require LLMs to be aware of what they do not know, to inquire about missing medical information from patients, and to ultimately make diagnoses. To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets. A medical consultation training set is further constructed to improve the consultation ability of LLMs. The results of the experiments show that fine-tuning with the training set can alleviate hallucinations and improve LLMs' performance on the proposed benchmark. Extensive experiments and ablation studies are conducted to validate the effectiveness and robustness of the proposed framework.
</details>
<details>
<summary>摘要</summary>
Consultation tasks are designed to require LLMs to be aware of what they do not know, to inquire about missing medical information from patients, and to ultimately make diagnoses. To evaluate the performance of LLMs for these tasks, a benchmark is proposed by reformulating medical multiple-choice questions from the United States Medical Licensing Examinations (USMLE), and comprehensive evaluation metrics are developed and evaluated on three constructed test sets. A medical consultation training set is further constructed to improve the consultation ability of LLMs. The results of the experiments show that fine-tuning with the training set can alleviate hallucinations and improve LLMs' performance on the proposed benchmark.Extensive experiments and ablation studies are conducted to validate the effectiveness and robustness of the proposed framework.
</details></li>
</ul>
<hr>
<h2 id="Bilevel-Scheduled-Sampling-for-Dialogue-Generation"><a href="#Bilevel-Scheduled-Sampling-for-Dialogue-Generation" class="headerlink" title="Bilevel Scheduled Sampling for Dialogue Generation"></a>Bilevel Scheduled Sampling for Dialogue Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01953">http://arxiv.org/abs/2309.01953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawen Liu, Kan Li</li>
<li>for:  mitigating exposure bias in natural language processing tasks, particularly in dialog generation.</li>
<li>methods:  proposed a bilevel scheduled sampling model that takes sentence-level information into account and incorporates it with word-level quality, and a smooth function that maps the combined result to an appropriate range for probabilistic sampling.</li>
<li>results:  significantly alleviated the exposure bias problem and outperformed state-of-the-art scheduled sampling methods in experiments conducted on the DailyDialog and PersonaChat datasets.<details>
<summary>Abstract</summary>
Exposure bias poses a common challenge in numerous natural language processing tasks, particularly in the dialog generation. In response to this issue, researchers have devised various techniques, among which scheduled sampling has proven to be an effective method for mitigating exposure bias. However, the existing state-of-the-art scheduled sampling methods solely consider the current sampling words' quality for threshold truncation sampling, which overlooks the importance of sentence-level information and the method of threshold truncation warrants further discussion. In this paper, we propose a bilevel scheduled sampling model that takes the sentence-level information into account and incorporates it with word-level quality. To enhance sampling diversity and improve the model's adaptability, we propose a smooth function that maps the combined result of sentence-level and word-level information to an appropriate range, and employ probabilistic sampling based on the mapped values instead of threshold truncation. Experiments conducted on the DailyDialog and PersonaChat datasets demonstrate the effectiveness of our proposed methods, which significantly alleviate the exposure bias problem and outperform state-of-the-art scheduled sampling methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate("Exposure bias poses a common challenge in numerous natural language processing tasks, particularly in dialog generation. In response to this issue, researchers have devised various techniques, among which scheduled sampling has proven to be an effective method for mitigating exposure bias. However, the existing state-of-the-art scheduled sampling methods solely consider the current sampling words' quality for threshold truncation sampling, which overlooks the importance of sentence-level information and the method of threshold truncation warrants further discussion. In this paper, we propose a bilevel scheduled sampling model that takes the sentence-level information into account and incorporates it with word-level quality. To enhance sampling diversity and improve the model's adaptability, we propose a smooth function that maps the combined result of sentence-level and word-level information to an appropriate range, and employ probabilistic sampling based on the mapped values instead of threshold truncation. Experiments conducted on the DailyDialog and PersonaChat datasets demonstrate the effectiveness of our proposed methods, which significantly alleviate the exposure bias problem and outperform state-of-the-art scheduled sampling methods.")]Here's the translation:<<SYS>>交叉偏见是许多自然语言处理任务中的常见挑战，尤其是对话生成。为了解决这个问题，研究人员已经提出了多种技术，其中规则采样已经被证明是有效的方法来减少交叉偏见。然而，现有的状态艺术规则采样方法只考虑当前采样词语的质量，忽略了句子水平信息，这种方法不充分考虑句子级别的信息和规则采样的问题。在这篇论文中，我们提出了一种两级规则采样模型，该模型考虑了句子水平信息，并将其与单词水平信息结合。为了增强采样多样性和模型适应性，我们提出了一种缓动函数，将合并的句子水平和单词水平信息映射到适当的范围内，然后使用概率采样基于映射值而不是阈值 truncation。经过 DailyDialog 和 PersonaChat 数据集的实验，我们的提议方法显示效果，可以减少交叉偏见问题，并在现有的规则采样方法中具有优势。
</details></li>
</ul>
<hr>
<h2 id="TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models"><a href="#TODM-Train-Once-Deploy-Many-Efficient-Supernet-Based-RNN-T-Compression-For-On-device-ASR-Models" class="headerlink" title="TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models"></a>TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01947">http://arxiv.org/abs/2309.01947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Shangguan, Haichuan Yang, Danni Li, Chunyang Wu, Yassir Fathullah, Dilin Wang, Ayushi Dalmia, Raghuraman Krishnamoorthi, Ozlem Kalinli, Junteng Jia, Jay Mahadeokar, Xin Lei, Mike Seltzer, Vikas Chandra</li>
<li>for: 这篇论文的目的是提出一个名为TODM（Train Once Deploy Many）的新方法，用于快速训练适合不同硬件的实时语音识别（ASR）模型，并且可以与单一训练作业相比减少训练时间和资源。</li>
<li>methods: 这篇论文使用了以往的Supernet研究，将RNN-T模型的层级和宽度缩减为更小的subnetworks，以适应不同的硬件类型。此外，论文还提出了三种技术来提高TODM Supernet的效果：适应性Dropout、对Alpha-divergence知识传递和Scale Adam优化器。</li>
<li>results: 论文通过比较Supernet训练 versus个别调整Multi-Head State Space Model (MH-SSM) RNN-T使用LibriSpeech数据库，发现TODM Supernet可以与手动调整模型相比，在字元错误率（WER）上提高至3%以上的表现，而且可以快速地训练多个模型，并且仅需小于单一训练作业的训练时间和资源。<details>
<summary>Abstract</summary>
Automatic Speech Recognition (ASR) models need to be optimized for specific hardware before they can be deployed on devices. This can be done by tuning the model's hyperparameters or exploring variations in its architecture. Re-training and re-validating models after making these changes can be a resource-intensive task. This paper presents TODM (Train Once Deploy Many), a new approach to efficiently train many sizes of hardware-friendly on-device ASR models with comparable GPU-hours to that of a single training job. TODM leverages insights from prior work on Supernet, where Recurrent Neural Network Transducer (RNN-T) models share weights within a Supernet. It reduces layer sizes and widths of the Supernet to obtain subnetworks, making them smaller models suitable for all hardware types. We introduce a novel combination of three techniques to improve the outcomes of the TODM Supernet: adaptive dropouts, an in-place Alpha-divergence knowledge distillation, and the use of ScaledAdam optimizer. We validate our approach by comparing Supernet-trained versus individually tuned Multi-Head State Space Model (MH-SSM) RNN-T using LibriSpeech. Results demonstrate that our TODM Supernet either matches or surpasses the performance of manually tuned models by up to a relative of 3% better in word error rate (WER), while efficiently keeping the cost of training many models at a small constant.
</details>
<details>
<summary>摘要</summary>
自动话语识别（ASR）模型需要根据特定硬件进行优化，以便在设备上部署。这可以通过调整模型的超参数或探索其结构的变化来实现。然而，在进行这些变化后，需要重新训练和验证模型，这可能会占用资源。本文介绍了一种新的方法—— Train Once Deploy Many（TODM），可以高效地在不同硬件类型上训练多个适合硬件的语音识别模型，并且与单个训练任务相比，它的GPU时间相同。TODM利用了先前的Supernet研究，在Supernet中，Recurrent Neural Network Transducer（RNN-T）模型共享权重。它采用了减小Supernet层数和宽度，从而得到了适合所有硬件类型的子网络，这些子网络是小型模型。我们介绍了一种新的组合技术，包括适应性Dropout、在位Alpha-分布知识继承和Scale Adam优化器，以提高TODM Supernet的结果。我们通过对Supernet训练 versus 手动调整Multi-Head State Space Model（MH-SSM）RNN-T使用LibriSpeech进行比较，结果表明，我们的TODM Supernet可以与手动调整模型相比，在字节错误率（WER）方面提高到3%之间的Relative。同时，我们efficient地保持了训练多个模型的成本，占用小的常量。
</details></li>
</ul>
<hr>
<h2 id="QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm"><a href="#QuantEase-Optimization-based-Quantization-for-Language-Models-–-An-Efficient-and-Intuitive-Algorithm" class="headerlink" title="QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm"></a>QuantEase: Optimization-based Quantization for Language Models – An Efficient and Intuitive Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01885">http://arxiv.org/abs/2309.01885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayhan Behdin, Ayan Acharya, Aman Gupta, Sathiya Keerthi, Rahul Mazumder</li>
<li>for: 本研究针对大型自然语言模型（LLMs）的快速部署实现了压缩技术，尤其是Post-Training Quantization（PTQ）。</li>
<li>methods: 本研究提出了一个层别压缩框架QuantEase，各层独立进行压缩，并使用了coordinate descent（CD）技术来解决非凸网络问题。</li>
<li>results: 实验结果显示，QuantEase在不同的LLMs和数据集上的误差率和零shot准确率方面具有国际级的表现，与比较方法GPTQ之间的改善为15%之间。尤其是对于具有重要权重（outliers）的情况下，我们的方法可以实现近乎3位数字的压缩，不需要非凸压缩或分组技术，与比较方法SpQR的改善为2倍以上。<details>
<summary>Abstract</summary>
With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ. Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLM）的 популяр化，压缩技术的研究吸引了越来越多的关注。这项研究关注于LLM的Post-Training Quantization（PTQ）。基于最新的进展，我们提出了QuantEase，一个层 wise量化框架，其中每层都进行独立的量化。问题被定义为一个逻辑结构化非核心的优化问题，这使得我们可以基于坐标降低（CD）技术开发高质量的解决方案。这些CD基本的方法可以提供高质量的解决方案，并且具有简单的更新，只需要基于矩阵和向量的操作，不需要矩阵反射或分解。我们还探索了一种具有异常检测的变体，可以保留重要的权重（异常），并且完全保留精度。我们的提议在实验中达到了 LLM 的状态zegart 性能，包括词 Error 和零培训精度，与比如 GPTQ 的方法相比，提高了15%。特别是我们的异常检测变体可以在不同批量化或分组技术的情况下，实现 LLM 的近或下三位量化，超过 SpQR 的性能，提高了至多两倍。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.CL_2023_09_05/" data-id="clp9qz81g00baok88ac5m71am" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/cs.LG_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T10:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/cs.LG_2023_09_05/">cs.LG - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters"><a href="#Superclustering-by-finding-statistically-significant-separable-groups-of-optimal-gaussian-clusters" class="headerlink" title="Superclustering by finding statistically significant separable groups of optimal gaussian clusters"></a>Superclustering by finding statistically significant separable groups of optimal gaussian clusters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02623">http://arxiv.org/abs/2309.02623</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berng/GMSDB">https://github.com/berng/GMSDB</a></li>
<li>paper_authors: Oleg I. Berngardt</li>
<li>For: The paper proposes an algorithm for clustering a dataset into optimal superclusters based on the BIC criterion and statistical separability.* Methods: The algorithm consists of three stages: representing the dataset as a mixture of Gaussian distributions, estimating distances and cluster sizes using the Mahalanobis distance, and combining clusters into superclusters using the DBSCAN method with a statistical significance level.* Results: The algorithm demonstrates good results on test datasets in both noisy and noiseless situations, and can predict correct superclusters for new data based on already trained clusterer. However, the algorithm has low speed and stochastic nature, and requires a sufficiently large dataset for clustering.Here is the same information in Simplified Chinese text:* For: 文章提出一种算法，用于基于BIC criterion和统计分离性 clustering dataset。* Methods: 算法包括三个阶段：将dataset表示为一个 mixture of Gaussian distributions，使用 Mahalanobis distance 估计 distances 和 cluster size，并使用 DBSCAN 方法将 clusters 组合成 superclusters，并使用统计significance level。* Results: 算法在测试dataset上显示了良好的结果，能够预测新数据中 correct superclusters，基于已经训练好的 clusterer。然而，算法有低速度和随机性，需要一个充分大的 dataset 进行 clustering。<details>
<summary>Abstract</summary>
The paper presents the algorithm for clustering a dataset by grouping the optimal, from the point of view of the BIC criterion, number of Gaussian clusters into the optimal, from the point of view of their statistical separability, superclusters.   The algorithm consists of three stages: representation of the dataset as a mixture of Gaussian distributions - clusters, which number is determined based on the minimum of the BIC criterion; using the Mahalanobis distance, to estimate the distances between the clusters and cluster sizes; combining the resulting clusters into superclusters using the DBSCAN method by finding its hyperparameter (maximum distance) providing maximum value of introduced matrix quality criterion at maximum number of superclusters. The matrix quality criterion corresponds to the proportion of statistically significant separated superclusters among all found superclusters.   The algorithm has only one hyperparameter - statistical significance level, and automatically detects optimal number and shape of superclusters based of statistical hypothesis testing approach. The algorithm demonstrates a good results on test datasets in noise and noiseless situations. An essential advantage of the algorithm is its ability to predict correct supercluster for new data based on already trained clusterer and perform soft (fuzzy) clustering. The disadvantages of the algorithm are: its low speed and stochastic nature of the final clustering. It requires a sufficiently large dataset for clustering, which is typical for many statistical methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Representing the dataset as a mixture of Gaussian distributions (clusters) and determining the number of clusters based on the minimum of the BIC criterion.2. Using the Mahalanobis distance to estimate the distances between the clusters and their sizes.3. Combining the resulting clusters into superclusters using the DBSCAN method by finding the maximum distance that provides the maximum value of the introduced matrix quality criterion at the maximum number of superclusters.The algorithm has only one hyperparameter, the statistical significance level, and automatically detects the optimal number and shape of superclusters based on a statistical hypothesis testing approach. The algorithm demonstrates good results on test datasets in both noisy and noise-free situations. An advantage of the algorithm is its ability to predict correct superclusters for new data based on an already trained clusterer and perform soft (fuzzy) clustering. However, the algorithm has some disadvantages, such as low speed and stochastic nature of the final clustering, and requires a sufficient dataset for clustering, which is common for many statistical methods.</details></li>
</ol>
<hr>
<h2 id="Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts"><a href="#Generative-AI-aided-Joint-Training-free-Secure-Semantic-Communications-via-Multi-modal-Prompts" class="headerlink" title="Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts"></a>Generative AI-aided Joint Training-free Secure Semantic Communications via Multi-modal Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02616">http://arxiv.org/abs/2309.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyang Du, Guangyuan Liu, Dusit Niyato, Jiayi Zhang, Jiawen Kang, Zehui Xiong, Bo Ai, Dong In Kim</li>
<li>for: 提高网络资源使用效率和实现通信目标</li>
<li>methods: 使用生成人工智能（GAI）模型，增强 semantic decoder 的可重构能力，并应用多模型提示进行精准内容解码</li>
<li>results: 实现精准内容解码和安全传输源消息，并提高网络资源使用效率<details>
<summary>Abstract</summary>
Semantic communication (SemCom) holds promise for reducing network resource consumption while achieving the communications goal. However, the computational overheads in jointly training semantic encoders and decoders-and the subsequent deployment in network devices-are overlooked. Recent advances in Generative artificial intelligence (GAI) offer a potential solution. The robust learning abilities of GAI models indicate that semantic decoders can reconstruct source messages using a limited amount of semantic information, e.g., prompts, without joint training with the semantic encoder. A notable challenge, however, is the instability introduced by GAI's diverse generation ability. This instability, evident in outputs like text-generated images, limits the direct application of GAI in scenarios demanding accurate message recovery, such as face image transmission. To solve the above problems, this paper proposes a GAI-aided SemCom system with multi-model prompts for accurate content decoding. Moreover, in response to security concerns, we introduce the application of covert communications aided by a friendly jammer. The system jointly optimizes the diffusion step, jamming, and transmitting power with the aid of the generative diffusion models, enabling successful and secure transmission of the source messages.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts"><a href="#Generative-Algorithms-for-Fusion-of-Physics-Based-Wildfire-Spread-Models-with-Satellite-Data-for-Initializing-Wildfire-Forecasts" class="headerlink" title="Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts"></a>Generative Algorithms for Fusion of Physics-Based Wildfire Spread Models with Satellite Data for Initializing Wildfire Forecasts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02615">http://arxiv.org/abs/2309.02615</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bshaddy/cWGAN_fire_arrival_time_inference">https://github.com/bshaddy/cWGAN_fire_arrival_time_inference</a></li>
<li>paper_authors: Bryan Shaddy, Deep Ray, Angel Farguell, Valentina Calaza, Jan Mandel, James Haley, Kyle Hilburn, Derek V. Mallia, Adam Kochanski, Assad Oberai<br>for: 这个研究的目的是开发一种基于卫星数据的高分辨率野火行为模型，以便预测野火的 sprech.methods: 这个研究使用了一种称为conditional Wasserstein Generative Adversarial Network (cWGAN)的方法，用于从卫星活动火数据中推断野火的到达时间。results: 研究发现，使用cWGAN来预测野火的到达时间可以非常准确， average Sorensen’s coefficient of 0.81 for the fire perimeters和average ignition time error of 32 minutes。<details>
<summary>Abstract</summary>
Increases in wildfire activity and the resulting impacts have prompted the development of high-resolution wildfire behavior models for forecasting fire spread. Recent progress in using satellites to detect fire locations further provides the opportunity to use measurements to improve fire spread forecasts from numerical models through data assimilation. This work develops a method for inferring the history of a wildfire from satellite measurements, providing the necessary information to initialize coupled atmosphere-wildfire models from a measured wildfire state in a physics-informed approach. The fire arrival time, which is the time the fire reaches a given spatial location, acts as a succinct representation of the history of a wildfire. In this work, a conditional Wasserstein Generative Adversarial Network (cWGAN), trained with WRF-SFIRE simulations, is used to infer the fire arrival time from satellite active fire data. The cWGAN is used to produce samples of likely fire arrival times from the conditional distribution of arrival times given satellite active fire detections. Samples produced by the cWGAN are further used to assess the uncertainty of predictions. The cWGAN is tested on four California wildfires occurring between 2020 and 2022, and predictions for fire extent are compared against high resolution airborne infrared measurements. Further, the predicted ignition times are compared with reported ignition times. An average Sorensen's coefficient of 0.81 for the fire perimeters and an average ignition time error of 32 minutes suggest that the method is highly accurate.
</details>
<details>
<summary>摘要</summary>
人类活动增加了野火的活动，并导致了一些影响。为了预测野火的扩散，人们已经开发了高分辨率野火行为模型。近年来，通过卫星探测火灾位置，可以使用测量数据进行数据吸收，以改进数字模型中的火灾扩散预测。这种工作通过卫星活动火灾数据来推算火灾历史，以便在物理学 informed 的方法下初始化气候-野火模型。火灾到达时间，即火灾到某个空间位置的时间， acted as a succinct representation of the history of a wildfire。在这种工作中，一种 conditional Wasserstein 生成 adversarial network (cWGAN) 被用来从卫星活动火灾数据中推算火灾到达时间。cWGAN 被用来生成 conditional 分布中的可能的火灾到达时间样本。这些样本被用来评估预测的不确定性。cWGAN 在加利福尼亚州2020-2022年四次野火中进行测试，并将预测的火灾范围与高分辨率空中热成像测量进行比较。此外，预测的点燃时间也与报告的点燃时间进行比较。 Sorensen 公式的平均值为0.81，表明方法的准确性很高。
</details></li>
</ul>
<hr>
<h2 id="T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data"><a href="#T-SaS-Toward-Shift-aware-Dynamic-Adaptation-for-Streaming-Data" class="headerlink" title="T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data"></a>T-SaS: Toward Shift-aware Dynamic Adaptation for Streaming Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02610">http://arxiv.org/abs/2309.02610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijieying Ren, Tianxiang Zhao, Wei Qin, Kunpeng Liu</li>
<li>for: 这篇研究是为了解决流数据中的分布类型逐步变化问题，特别是在没有预测的情况下，数据流中突然出现分布shift。</li>
<li>methods: 这篇研究使用了一个 Bayesian 框架，名为 T-SaS，并将分布类型变化变数纳入模型中，以捕捉数据流中突然的分布shift。然后，这篇研究设计了一个可动的网络选择策略，以适应不同的分布类型。</li>
<li>results: 实验结果显示，这篇研究的方法可以优于在数据流中准确地探测分布shift的范围，并对下游预测或分类任务进行有效适应。<details>
<summary>Abstract</summary>
In many real-world scenarios, distribution shifts exist in the streaming data across time steps. Many complex sequential data can be effectively divided into distinct regimes that exhibit persistent dynamics. Discovering the shifted behaviors and the evolving patterns underlying the streaming data are important to understand the dynamic system. Existing methods typically train one robust model to work for the evolving data of distinct distributions or sequentially adapt the model utilizing explicitly given regime boundaries. However, there are two challenges: (1) shifts in data streams could happen drastically and abruptly without precursors. Boundaries of distribution shifts are usually unavailable, and (2) training a shared model for all domains could fail to capture varying patterns. This paper aims to solve the problem of sequential data modeling in the presence of sudden distribution shifts that occur without any precursors. Specifically, we design a Bayesian framework, dubbed as T-SaS, with a discrete distribution-modeling variable to capture abrupt shifts of data. Then, we design a model that enable adaptation with dynamic network selection conditioned on that discrete variable. The proposed method learns specific model parameters for each distribution by learning which neurons should be activated in the full network. A dynamic masking strategy is adopted here to support inter-distribution transfer through the overlapping of a set of sparse networks. Extensive experiments show that our proposed method is superior in both accurately detecting shift boundaries to get segments of varying distributions and effectively adapting to downstream forecast or classification tasks.
</details>
<details>
<summary>摘要</summary>
在许多实际场景中，流动数据中的分布shift存在，这些shift可能是随机的和突然的。许多复杂的顺序数据可以被有效地分解为不同的领域，每个领域都具有持续的动力学。了解流动数据中的shift和下沉pattern是理解动态系统的关键。现有方法通常是在不同的分布下训练一个Robust模型，或者采用显式给出的领域边界来逐步修改模型。然而，存在两个挑战：（1）数据流中的shift可能会发生急剧和突然，无法预测；（2）训练共享模型可能无法捕捉不同领域的变化模式。这篇论文的目的是解决流动数据中的顺序数据模型化问题，具体来说是在无前兆的分布shift下进行适应。我们提出了一种抽象 Bayesian 框架，名为T-SaS，它包含一个简单的分布模型变量，用于捕捉数据的突然shift。然后，我们设计了一种可动的网络选择conditioned于这个分布变量，以便适应不同的分布。我们的方法可以学习每个分布的特定参数，并且通过在全网络中活跃的 neuron 来确定哪些参数是有用的。我们采用了一种动态遮盾策略来支持 между分布传递，这种策略可以在不同的分布下共享一部分稀缺网络。我们的方法在 Segment 分布boundary 检测和适应下沉任务中具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Variational-Inference-for-Online-Supervised-Learning"><a href="#Distributed-Variational-Inference-for-Online-Supervised-Learning" class="headerlink" title="Distributed Variational Inference for Online Supervised Learning"></a>Distributed Variational Inference for Online Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02606">http://arxiv.org/abs/2309.02606</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pptx/distributed-mapping">https://github.com/pptx/distributed-mapping</a></li>
<li>paper_authors: Parth Paritosh, Nikolay Atanasov, Sonia Martinez</li>
<li>for: 这篇论文旨在提出一种扩展可行的分布式概率推理算法，用于智能传感器网络中的推理问题。</li>
<li>methods: 该论文提出了一种分布式 probabilistic inference algorithm，可以应用于连续变量、不可解 posteriors 和大规模实时数据。在中央设置下，Variational inference 是一种基本的技术，用于perform approximate Bayesian estimation，其中一个难以求解 posterior density 被approximated 为一个参数化density。</li>
<li>results: 论文的关键贡献在于 derive 了一个分布式lower bound  на centralized estimation objective，这使得在传感器网络中进行分布式variational inference，只需一次 hop 通信。此外，论文还设计了一种在线分布式算法，用于在流动数据中进行分类和回归问题的解决，并特化为 Gaussian variational densities with non-linear likelihoods。最后，论文还 derive 了一个高维模型的 diagonalized 版本，并应用于多机器人概率地图使用indoor LiDAR数据。<details>
<summary>Abstract</summary>
Developing efficient solutions for inference problems in intelligent sensor networks is crucial for the next generation of location, tracking, and mapping services. This paper develops a scalable distributed probabilistic inference algorithm that applies to continuous variables, intractable posteriors and large-scale real-time data in sensor networks. In a centralized setting, variational inference is a fundamental technique for performing approximate Bayesian estimation, in which an intractable posterior density is approximated with a parametric density. Our key contribution lies in the derivation of a separable lower bound on the centralized estimation objective, which enables distributed variational inference with one-hop communication in a sensor network. Our distributed evidence lower bound (DELBO) consists of a weighted sum of observation likelihood and divergence to prior densities, and its gap to the measurement evidence is due to consensus and modeling errors. To solve binary classification and regression problems while handling streaming data, we design an online distributed algorithm that maximizes DELBO, and specialize it to Gaussian variational densities with non-linear likelihoods. The resulting distributed Gaussian variational inference (DGVI) efficiently inverts a $1$-rank correction to the covariance matrix. Finally, we derive a diagonalized version for online distributed inference in high-dimensional models, and apply it to multi-robot probabilistic mapping using indoor LiDAR data.
</details>
<details>
<summary>摘要</summary>
开发高效的推理解决方案是智能感知网络下一代位置跟踪和地图服务的关键。本文提出了一种可扩展的分布式概率推理算法，适用于继续变量、不可解决 posterior 和大规模实时数据。在中央化环境下，变量推理是概率推理的基本技术，用于approximate Bayesian estimation，其中一个难以解决的 posterior density 被approximated 为parametric density。我们的关键贡献在于 derive 一个可分离的下界于中央估计目标函数，这使得分布式变量推理可以在感知网络中进行一 hop 通信。我们的分布式证据下界（DELBO）是一个加权和 observation likelihood 和偏好函数之间的差异，它的差异是由consensus和modeling error 引起的。为了解决流动数据中的二分类和回归问题，我们设计了一种在线分布式算法，该算法可以最大化 DELBO，并特化为 Gaussian 变量推理函数。这导致了一种高效地减少 $1$-rank  corrections 的方法。最后，我们 deriv 了一个 диагональ 版本，用于在线分布式推理高维模型，并应用于多 robot 概率地图使用indoor LiDAR 数据。
</details></li>
</ul>
<hr>
<h2 id="Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet"><a href="#Screening-of-Pneumonia-and-Urinary-Tract-Infection-at-Triage-using-TriNet" class="headerlink" title="Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet"></a>Screening of Pneumonia and Urinary Tract Infection at Triage using TriNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02604">http://arxiv.org/abs/2309.02604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Z. Lu<br>for: 这个论文是为了解决医疗机构的急诊室拥堵和效率下降问题而写的。methods: 这个论文使用机器学习算法来自动化急诊室的医疗指导，以提高急诊室的效率和质量。results: 论文中的TriNet模型在检测患有肺炎和慢性肾炎的病人中显示了高正确率（0.86和0.93），这些模型比现有的临床标准更高，表明机器学习医疗指导可以提供免费、不侵入的检测方式，从而降低急诊室的风险和提高医疗质量。<details>
<summary>Abstract</summary>
Due to the steady rise in population demographics and longevity, emergency department visits are increasing across North America. As more patients visit the emergency department, traditional clinical workflows become overloaded and inefficient, leading to prolonged wait-times and reduced healthcare quality. One of such workflows is the triage medical directive, impeded by limited human workload, inaccurate diagnoses and invasive over-testing. To address this issue, we propose TriNet: a machine learning model for medical directives that automates first-line screening at triage for conditions requiring downstream testing for diagnosis confirmation. To verify screening potential, TriNet was trained on hospital triage data and achieved high positive predictive values in detecting pneumonia (0.86) and urinary tract infection (0.93). These models outperform current clinical benchmarks, indicating that machine-learning medical directives can offer cost-free, non-invasive screening with high specificity for common conditions, reducing the risk of over-testing while increasing emergency department efficiency.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因人口减少和寿命增加，北美洲的急诊室访问量在增长。随着更多患者前往急诊室，传统的临床工作流程变得过载和不具有效率，导致排队时间增长，健康保健质量减退。其中一种工作流程是抢救医疗指南，受到人工负荷、不准确诊断和不必要的检测限制。为解决这一问题，我们提议TriNet：一种基于机器学习的医疗指南，自动化急诊室抢救阶段的首选检测，以确认诊断。为验证这一点，TriNet在医院急诊室数据上进行训练，在患有肺炎和尿感染的病例中达到了0.86的正确预测值，并在患有尿感染的病例中达到了0.93的正确预测值。这些模型比现有的临床标准更高，表明机器学习医疗指南可以提供免费、不侵入的检测，高度特异性 для常见的疾病，降低过测试风险，提高急诊室效率。
</details></li>
</ul>
<hr>
<h2 id="Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach"><a href="#Causal-Structure-Recovery-of-Linear-Dynamical-Systems-An-FFT-based-Approach" class="headerlink" title="Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach"></a>Causal Structure Recovery of Linear Dynamical Systems: An FFT based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02571">http://arxiv.org/abs/2309.02571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mishfad Shaikh Veedu, James Melbourne, Murti V. Salapaka</li>
<li>for: 本研究旨在探讨时间序列观测中的 causal 效应， especial when there are dynamical dependencies between entities.</li>
<li>methods: 我们提出了一种方法，可以减少计算复杂性为 $O(Tn^3 \log N)$，用于回归 causation 结构，从而获得频域频谱 (FD) 表示。</li>
<li>results: 我们发现，对于LTI 系统，可以使用 do-calculus 机制在 FD 中进行 causal 推理，并且可以使用 multivariate Wiener projections 实现 graph 重建，具有 $O(n)$ 复杂性。<details>
<summary>Abstract</summary>
Learning causal effects from data is a fundamental and well-studied problem across science, especially when the cause-effect relationship is static in nature. However, causal effect is less explored when there are dynamical dependencies, i.e., when dependencies exist between entities across time. Identifying dynamic causal effects from time-series observations is computationally expensive when compared to the static scenario. We demonstrate that the computational complexity of recovering the causation structure for the vector auto-regressive (VAR) model is $O(Tn^3N^2)$, where $n$ is the number of nodes, $T$ is the number of samples, and $N$ is the largest time-lag in the dependency between entities. We report a method, with a reduced complexity of $O(Tn^3 \log N)$, to recover the causation structure to obtain frequency-domain (FD) representations of time-series. Since FFT accumulates all the time dependencies on every frequency, causal inference can be performed efficiently by considering the state variables as random variables at any given frequency. We additionally show that, for systems with interactions that are LTI, do-calculus machinery can be realized in the FD resulting in versions of the classical single-door (with cycles), front and backdoor criteria. We demonstrate, for a large class of problems, graph reconstruction using multivariate Wiener projections results in a significant computational advantage with $O(n)$ complexity over reconstruction algorithms such as the PC algorithm which has $O(n^q)$ complexity, where $q$ is the maximum neighborhood size. This advantage accrues due to some remarkable properties of the phase response of the frequency-dependent Wiener coefficients which is not present in any time-domain approach.
</details>
<details>
<summary>摘要</summary>
学习 causal effects 从数据中是科学的基础问题，特别是当 causal relationship 是静态的时候。然而， causal effect 在存在时间相关性时更少研究。从时序观察数据中提取动态 causal effects 的计算复杂度比静态场景更高。我们证明了VAR 模型的 causation 结构恢复计算复杂度为 $O(Tn^3N^2)$, where $n$ 是节点数， $T$ 是样本数， $N$ 是最大时间延迟 между实体。我们报告了一种方法，计算复杂度为 $O(Tn^3 \log N)$, 恢复 causation 结构，以获得频域域 (FD) 表示。由于 FFT 积累了所有时间相关性，因此在频域中进行 causal inference 是高效的。我们还证明了，对于具有 LTI 交互的系统，do-calculus 机械可以在 FD 中实现，导致了类ical single-door (with cycles)、front 和 backdoor  criterion。我们示例了， для 一类问题，使用 multivariate Wiener projections 进行图重建可以获得 $O(n)$ 复杂度的计算优势，比 PC 算法 ($O(n^q)$ 复杂度) 更高效。这个优势来自频域 Wiener 系数的相对应性，不存在在时域方法中。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Partitioning-Around-Medoids"><a href="#Sparse-Partitioning-Around-Medoids" class="headerlink" title="Sparse Partitioning Around Medoids"></a>Sparse Partitioning Around Medoids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02557">http://arxiv.org/abs/2309.02557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Lenssen, Erich Schubert</li>
<li>For: 这篇论文是关于分群算法，具体是使用Partitioning Around Medoids（PAM）和fastPAM方法来解决分群问题。* Methods: 这篇论文使用了PAM和fastPAM方法，并且提出了一个缓存簇数据的方法来解决分群问题。* Results: 这篇论文的结果显示了这个方法可以在实际应用中提供高品质的分群解决方案，并且可以避免过度的缓存和复杂运算。<details>
<summary>Abstract</summary>
Partitioning Around Medoids (PAM, k-Medoids) is a popular clustering technique to use with arbitrary distance functions or similarities, where each cluster is represented by its most central object, called the medoid or the discrete median. In operations research, this family of problems is also known as facility location problem (FLP). FastPAM recently introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. In this chapter, we discuss a sparse and asymmetric variant of this problem, to be used for example on graph data such as road networks. By exploiting sparsity, we can avoid the quadratic runtime and memory requirements, and make this method scalable to even larger problems, as long as we are able to build a small enough graph of sufficient connectivity to perform local optimization. Furthermore, we consider asymmetric cases, where the set of medoids is not identical to the set of points to be covered (or in the interpretation of facility location, where the possible facility locations are not identical to the consumer locations). Because of sparsity, it may be impossible to cover all points with just k medoids for too small k, which would render the problem unsolvable, and this breaks common heuristics for finding a good starting condition. We, hence, consider determining k as a part of the optimization problem and propose to first construct a greedy initial solution with a larger k, then to optimize the problem by alternating between PAM-style "swap" operations where the result is improved by replacing medoids with better alternatives and "remove" operations to reduce the number of k until neither allows further improving the result quality. We demonstrate the usefulness of this method on a problem from electrical engineering, with the input graph derived from cartographic data.
</details>
<details>
<summary>摘要</summary>
分割附近中心（PAM，k-Medoids）是一种流行的聚类技术，可以用于任何距离函数或相似度，每个群由其最中央对象表示，称为中心点或离散中值。在运维研究中，这家团队的问题也称为设施位置问题（FLP）。Recently, FastPAM introduced a speedup for large k to make it applicable for larger problems, but the method still has a runtime quadratic in N. 在本章中，我们讨论了一种稀疏和不均匀的变体，用于应用于图数据，如道路网络。通过利用稀疏性，我们可以避免 quadratic runtime和内存需求，并使这种方法可扩展至更大的问题，只要我们能够构建一个足够紧凑的图，以便进行本地优化。此外，我们考虑了非对称情况，其中中心点与要覆盖的点不同。由于稀疏性，可能无法使用 too small k 覆盖所有点，这会导致问题不可解，并让常见的尝试找到好的初始状态失效。我们因此考虑在优化问题时确定 k 的部分，并提议先构建一个大于 k 的推荐解，然后通过 PAM 样式的 "交换" 操作和 "移除" 操作来优化问题，直到 neither 允许进一步提高结果质量。我们在电力工程中的一个问题上示cases the usefulness of this method.Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China and Singapore. The translation is based on the traditional Chinese characters, but the sentence structure and vocabulary have been adjusted to conform to Simplified Chinese conventions.
</details></li>
</ul>
<hr>
<h2 id="Data-Aggregation-for-Hierarchical-Clustering"><a href="#Data-Aggregation-for-Hierarchical-Clustering" class="headerlink" title="Data Aggregation for Hierarchical Clustering"></a>Data Aggregation for Hierarchical Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02552">http://arxiv.org/abs/2309.02552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elki-project/elki">https://github.com/elki-project/elki</a></li>
<li>paper_authors: Erich Schubert, Andreas Lang</li>
<li>for: 使用Hierarchical Agglomerative Clustering（HAC）进行数据 clustering，但是因为HAC需要全息距离矩阵和完整的层次结构，因此在资源受限的系统上可能会出现问题。</li>
<li>methods: 使用BETULA数据汇集算法，一种稳定的BIRCH数据汇集算法变体，来使HAC在受限资源的系统上可行，只有小的质量损失。</li>
<li>results: 可以使用BETULA数据汇集算法来实现HAC在受限资源的系统上的可行性，但是有小的质量损失。<details>
<summary>Abstract</summary>
Hierarchical Agglomerative Clustering (HAC) is likely the earliest and most flexible clustering method, because it can be used with many distances, similarities, and various linkage strategies. It is often used when the number of clusters the data set forms is unknown and some sort of hierarchy in the data is plausible. Most algorithms for HAC operate on a full distance matrix, and therefore require quadratic memory. The standard algorithm also has cubic runtime to produce a full hierarchy. Both memory and runtime are especially problematic in the context of embedded or otherwise very resource-constrained systems. In this section, we present how data aggregation with BETULA, a numerically stable version of the well known BIRCH data aggregation algorithm, can be used to make HAC viable on systems with constrained resources with only small losses on clustering quality, and hence allow exploratory data analysis of very large data sets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation"><a href="#A-Generalized-Bandsplit-Neural-Network-for-Cinematic-Audio-Source-Separation" class="headerlink" title="A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation"></a>A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02539">http://arxiv.org/abs/2309.02539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karn N. Watcharasupat, Chih-Wei Wu, Yiwei Ding, Iroro Orife, Aaron J. Hipple, Phillip A. Williams, Scott Kramer, Alexander Lerch, William Wolcott</li>
<li>for: 提取对话、音乐和特效的独立音频源</li>
<li>methods: 基于频谱分割的Bandsplit RNN模型，利用听觉学原则定义频率谱，使用1-norm损失函数和共享编码器提高分离性能</li>
<li>results: 在Divide and Remaster数据集上，模型达到了理想比例幕值以上的分离性能<details>
<summary>Abstract</summary>
Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with performance above the ideal ratio mask for the dialogue stem.
</details>
<details>
<summary>摘要</summary>
《电影式音频源分离》是一个相对较新的音频源分离子任务，目的是从它们的混合中提取对话束、音乐束和特效束。在这个工作中，我们开发了一个泛化了Bandsplit RNN模型，用于任何完整或过完整的频谱分解。基于听觉驱动的频谱缩放被用来引导频段定义，这些频段现在具有冗余性，以提高特征提取的可靠性。我们还提出了基于信号噪声比和1-norm减少准则的损失函数。此外，我们还利用了通用编码器设置中的信息共享特性，以降低训练和推断时的计算复杂度，提高对困难总结类声音的分离性能，并允许在推断时进行轻松地分解。我们的最佳模型已经将Divide and Remaster数据集的状态标准化，对对话束的性能超过理想的掩码壁。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-on-the-Probability-Simplex"><a href="#Diffusion-on-the-Probability-Simplex" class="headerlink" title="Diffusion on the Probability Simplex"></a>Diffusion on the Probability Simplex</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02530">http://arxiv.org/abs/2309.02530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Griffin Floto, Thorsteinn Jonsson, Mihai Nica, Scott Sanner, Eric Zhengyu Zhu</li>
<li>for: 生成模型学习数据分布的逆进程，创造一个生成模型。</li>
<li>methods: 使用概率 simplice 进行Diffusion，使用softmax函数应用于Ornstein-Unlenbeck过程。</li>
<li>results: 方法可以自然扩展到包括Diffusion在unit cube上，并有应用于 bounded image generation。Note: “概率 simplice” refers to the probability simplex, which is a geometric object used to represent probability distributions. “Diffusion on the unit cube” refers to a specific type of diffusion process that is applied to a cube-shaped domain, rather than a continuous space.<details>
<summary>Abstract</summary>
Diffusion models learn to reverse the progressive noising of a data distribution to create a generative model. However, the desired continuous nature of the noising process can be at odds with discrete data. To deal with this tension between continuous and discrete objects, we propose a method of performing diffusion on the probability simplex. Using the probability simplex naturally creates an interpretation where points correspond to categorical probability distributions. Our method uses the softmax function applied to an Ornstein-Unlenbeck Process, a well-known stochastic differential equation. We find that our methodology also naturally extends to include diffusion on the unit cube which has applications for bounded image generation.
</details>
<details>
<summary>摘要</summary>
diffusion 模型可以学习将数据分布中的进行进行逆转，以创建一个生成模型。然而，所希望的连续性的噪声过程可能与数据的精度有冲突。为了解决这种连续和精度之间的矛盾，我们提出了将噪声应用到概率 simpliciter 的方法。使用概率 simpliciter 自然地创造了点对应的 categorical 概率分布的解释。我们的方法使用 Ornstein-Unlenbeck 过程和 softmax 函数。我们发现我们的方法也自然地扩展到包括单位立方体上的噪声，这有应用于 bounded 图像生成。Note: "概率 simpliciter" refers to the probability simplex, which is a geometric object used to represent probability distributions. In this context, the method proposed in the text applies diffusion to the probability simplex to create a generative model.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs"><a href="#Adaptive-Adversarial-Training-Does-Not-Increase-Recourse-Costs" class="headerlink" title="Adaptive Adversarial Training Does Not Increase Recourse Costs"></a>Adaptive Adversarial Training Does Not Increase Recourse Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02528">http://arxiv.org/abs/2309.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Hardy, Jayanth Yetukuri, Yang Liu</li>
<li>for: 本研究旨在 investigating the effects of adaptive adversarial training on algorithmic recourse costs.</li>
<li>methods: 本研究使用了 adaptive adversarial training 方法，以对模型的Robustness和Algorithmic recourse costs 进行研究.</li>
<li>results: 研究结果显示，adaptive adversarial training 可以对模型的Robustness进行改善，但是这些改善对Algorithmic recourse costs 没有明显的影响。<details>
<summary>Abstract</summary>
Recent work has connected adversarial attack methods and algorithmic recourse methods: both seek minimal changes to an input instance which alter a model's classification decision. It has been shown that traditional adversarial training, which seeks to minimize a classifier's susceptibility to malicious perturbations, increases the cost of generated recourse; with larger adversarial training radii correlating with higher recourse costs. From the perspective of algorithmic recourse, however, the appropriate adversarial training radius has always been unknown. Another recent line of work has motivated adversarial training with adaptive training radii to address the issue of instance-wise variable adversarial vulnerability, showing success in domains with unknown attack radii. This work studies the effects of adaptive adversarial training on algorithmic recourse costs. We establish that the improvements in model robustness induced by adaptive adversarial training show little effect on algorithmic recourse costs, providing a potential avenue for affordable robustness in domains where recoursability is critical.
</details>
<details>
<summary>摘要</summary>
最近的研究已经连接了 adversarial attack 方法和 algorithmic recourse 方法：它们都寻找最小的输入实例修改，以变更模型的分类决策。已经证明，传统的 adversarial training，寻求对于黑客变化的抑制，将生成的 recourse 成本增加;  avec larger adversarial training radii 相关的 recourse 成本高于。从 algorithmic recourse 的角度来看，适当的 adversarial training radius 一直未知。另一些最近的研究将 adversarial training 与 adaptive training radii 结合，以解决实例对于黑客变化的不确定性，并在不同的实例上显示成功。这项研究 investigate 了 adaptive adversarial training 对于 algorithmic recourse 成本的影响。我们确定了 adaptive adversarial training 对于模型Robustness 的改进，对于 algorithmic recourse 成本的影响几乎无效，提供了可能的折衣预算在域内的途径。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models"><a href="#Comparative-Analysis-of-CPU-and-GPU-Profiling-for-Deep-Learning-Models" class="headerlink" title="Comparative Analysis of CPU and GPU Profiling for Deep Learning Models"></a>Comparative Analysis of CPU and GPU Profiling for Deep Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02521">http://arxiv.org/abs/2309.02521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipesh Gyawali</li>
<li>for: 这个论文是关于深度学习和机器学习应用的研究，旨在探讨 GPU 和 CPU 在训练深度神经网络时的资源分配和消耗。</li>
<li>methods: 该论文使用了 Pytorch 框架来实现深度学习项目，并对 GPU 和 CPU 的操作跟踪进行分析，以了解它们在训练深度神经网络时的资源分配和消耗。</li>
<li>results: 研究显示，在训练深度神经网络时，GPU 的运行时间比 CPU 更低，但是对于更简单的网络，GPU 与 CPU 之间并没有很大的差异。<details>
<summary>Abstract</summary>
Deep Learning(DL) and Machine Learning(ML) applications are rapidly increasing in recent days. Massive amounts of data are being generated over the internet which can derive meaningful results by the use of ML and DL algorithms. Hardware resources and open-source libraries have made it easy to implement these algorithms. Tensorflow and Pytorch are one of the leading frameworks for implementing ML projects. By using those frameworks, we can trace the operations executed on both GPU and CPU to analyze the resource allocations and consumption. This paper presents the time and memory allocation of CPU and GPU while training deep neural networks using Pytorch. This paper analysis shows that GPU has a lower running time as compared to CPU for deep neural networks. For a simpler network, there are not many significant improvements in GPU over the CPU.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）和机器学习（ML）应用在最近几年内快速增长。互联网上的巨量数据可以通过ML和DL算法提取有意义的结果。硬件资源和开源库的出现使得实现这些算法变得更加容易。TensorFlow和PyTorch是实现ML项目的主要框架之一。通过使用这些框架，我们可以跟踪CPU和GPU上执行的操作，以分析资源分配和消耗。本文 presente在训练深度神经网络时CPU和GPU的时间和内存分配。本文分析显示，在深度神经网络训练中，GPU的运行时间比CPU更低。对于简单的网络，GPU上并没有很多显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Towards-User-Guided-Actionable-Recourse"><a href="#Towards-User-Guided-Actionable-Recourse" class="headerlink" title="Towards User Guided Actionable Recourse"></a>Towards User Guided Actionable Recourse</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02517">http://arxiv.org/abs/2309.02517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayanth Yetukuri, Ian Hardy, Yang Liu</li>
<li>for: This paper aims to provide actionable recourse to negatively impacted users in machine learning models, with a focus on capturing user preferences via soft constraints.</li>
<li>methods: The paper proposes using three simple forms of soft constraints to capture user preferences: scoring continuous features, bounding feature values, and ranking categorical features. Additionally, the paper proposes a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR).</li>
<li>results: The paper conducts extensive experiments to verify the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Machine Learning's proliferation in critical fields such as healthcare, banking, and criminal justice has motivated the creation of tools which ensure trust and transparency in ML models. One such tool is Actionable Recourse (AR) for negatively impacted users. AR describes recommendations of cost-efficient changes to a user's actionable features to help them obtain favorable outcomes. Existing approaches for providing recourse optimize for properties such as proximity, sparsity, validity, and distance-based costs. However, an often-overlooked but crucial requirement for actionability is a consideration of User Preference to guide the recourse generation process. In this work, we attempt to capture user preferences via soft constraints in three simple forms: i) scoring continuous features, ii) bounding feature values and iii) ranking categorical features. Finally, we propose a gradient-based approach to identify User Preferred Actionable Recourse (UP-AR). We carried out extensive experiments to verify the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework"><a href="#Enhancing-Deep-Learning-Models-through-Tensorization-A-Comprehensive-Survey-and-Framework" class="headerlink" title="Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework"></a>Enhancing Deep Learning Models through Tensorization: A Comprehensive Survey and Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02428">http://arxiv.org/abs/2309.02428</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mhelal/TensorsPyBook">https://github.com/mhelal/TensorsPyBook</a></li>
<li>paper_authors: Manal Helal</li>
<li>for: 本文旨在概述张量化，它是一种将多维数据转化为二维矩阵的方法，用于提高深度学习模型的表示和分析能力。</li>
<li>methods: 本文使用了多种多方分析方法，包括盲源分离（BSS）等，并评估了这些方法在不同领域的应用。</li>
<li>results: 研究结果表明，使用多维数据的原始形式和多方分析方法可以更好地捕捉数据中的复杂关系，同时减少模型参数数量和加速处理速度。<details>
<summary>Abstract</summary>
The burgeoning growth of public domain data and the increasing complexity of deep learning model architectures have underscored the need for more efficient data representation and analysis techniques. This paper is motivated by the work of Helal (2023) and aims to present a comprehensive overview of tensorization. This transformative approach bridges the gap between the inherently multidimensional nature of data and the simplified 2-dimensional matrices commonly used in linear algebra-based machine learning algorithms. This paper explores the steps involved in tensorization, multidimensional data sources, various multiway analysis methods employed, and the benefits of these approaches. A small example of Blind Source Separation (BSS) is presented comparing 2-dimensional algorithms and a multiway algorithm in Python. Results indicate that multiway analysis is more expressive. Contrary to the intuition of the dimensionality curse, utilising multidimensional datasets in their native form and applying multiway analysis methods grounded in multilinear algebra reveal a profound capacity to capture intricate interrelationships among various dimensions while, surprisingly, reducing the number of model parameters and accelerating processing. A survey of the multi-away analysis methods and integration with various Deep Neural Networks models is presented using case studies in different domains.
</details>
<details>
<summary>摘要</summary>
随着公共领域数据的急速增长和深度学习模型的复杂化，需要更有效的数据表示和分析技术。这篇论文是基于Helal（2023）的研究，旨在提供tensorization的全面介绍。这种转换方法 bridge了数据的自然多维性和通常用于线性代数学习算法中的简单二维矩阵之间的差异。本文探讨tensorization的过程、多维数据源、多方分析方法和其好处。此外，还提供了一个小例子， Comparing 2-dimensional算法和多方算法在Python中。结果表明，多方分析方法更加表达力。与传统的维度惩罚理论相反，使用原始多维数据和应用多方分析方法可以捕捉多维维度之间的复杂关系，同时减少模型参数的数量和加速处理。本文还提供了多方分析方法的survey和与不同领域的各种深度神经网络模型的集成。Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost"><a href="#Monotone-Tree-Based-GAMI-Models-by-Adapting-XGBoost" class="headerlink" title="Monotone Tree-Based GAMI Models by Adapting XGBoost"></a>Monotone Tree-Based GAMI Models by Adapting XGBoost</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02426">http://arxiv.org/abs/2309.02426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Soroush Aramideh, Jie Chen, Vijayan N. Nair<br>for:This paper aims to develop a monotone tree-based functional ANOVA model, called monotone GAMI-Tree, to address the issue of non-monotonicity in existing GAMI models.methods:The proposed method uses a filtering technique to select important interactions, followed by fitting a monotone XGBoost algorithm with the selected interactions. The results are then parsed and purified to obtain a monotone GAMI model.results:The proposed method is demonstrated on simulated datasets and shows better performance than existing GAMI models in terms of monotonicity and accuracy. The results also show that the main effects can be monotone, but the interactions may not be monotone.<details>
<summary>Abstract</summary>
Recent papers have used machine learning architecture to fit low-order functional ANOVA models with main effects and second-order interactions. These GAMI (GAM + Interaction) models are directly interpretable as the functional main effects and interactions can be easily plotted and visualized. Unfortunately, it is not easy to incorporate the monotonicity requirement into the existing GAMI models based on boosted trees, such as EBM (Lou et al. 2013) and GAMI-Lin-T (Hu et al. 2022). This paper considers models of the form $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$ and develops monotone tree-based GAMI models, called monotone GAMI-Tree, by adapting the XGBoost algorithm. It is straightforward to fit a monotone model to $f(x)$ using the options in XGBoost. However, the fitted model is still a black box. We take a different approach: i) use a filtering technique to determine the important interactions, ii) fit a monotone XGBoost algorithm with the selected interactions, and finally iii) parse and purify the results to get a monotone GAMI model. Simulated datasets are used to demonstrate the behaviors of mono-GAMI-Tree and EBM, both of which use piecewise constant fits. Note that the monotonicity requirement is for the full model. Under certain situations, the main effects will also be monotone. But, as seen in the examples, the interactions will not be monotone.
</details>
<details>
<summary>摘要</summary>
现在的研究论文使用机器学习建筑物来适应低阶函数ANOVA模型的主效应和次阶交互。这些GAMI（GAM + 交互）模型可以直接解释为函数主效应和交互，可以轻松地图表和可见化。然而，对于现有的GAMI模型，如EBM（Lou et al. 2013）和GAMI-Lin-T（Hu et al. 2022），不能直接包含幂随机性的要求。这篇论文考虑模型的形式为 $f(x)=\sum_{j,k}f_{j,k}(x_j, x_k)$，并开发了幂随机性基于XGBoost算法的幂随机性GAMI模型，称为幂随机性GAMI-Tree。使用XGBoost算法直接适应幂随机性模型是 straightforward。然而，适应后的模型仍然是黑盒模型。我们采用了不同的方法：i）使用筛选技术确定重要的交互，ii）适应幂随机性XGBoost算法，并iii）解析和纯化结果，以获得幂随机性GAMI模型。使用 simulate datasets 进行了示例，并证明了 mono-GAMI-Tree 和 EBM 都使用 piecewise constant fits 的行为。请注意，幂随机性要求是全模型的，而不是每个主效应或交互。在某些情况下，主效应也可能是幂随机的，但交互通常不是。
</details></li>
</ul>
<hr>
<h2 id="On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback"><a href="#On-the-Minimax-Regret-in-Online-Ranking-with-Top-k-Feedback" class="headerlink" title="On the Minimax Regret in Online Ranking with Top-k Feedback"></a>On the Minimax Regret in Online Ranking with Top-k Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02425">http://arxiv.org/abs/2309.02425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyuan Zhang, Ambuj Tewari</li>
<li>For:  Online ranking with top $k$ feedback* Methods:  Partial monitoring techniques, minimax regret rates* Results:  Full characterization of minimax regret rates for all $k$ and for Pairwise Loss, Discounted Cumulative Gain, and Precision@n, efficient algorithm for Precision@nHere is the Chinese translation of the three information points:* For:  online排名 WITH top $k$ 反馈* Methods:  partial monitoring 技术, minimax regret rates* Results:  all $k$ 和 Pairwise Loss, Discounted Cumulative Gain, Precision@n 中的完整characterization, efficient algorithm for Precision@nI hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In online ranking, a learning algorithm sequentially ranks a set of items and receives feedback on its ranking in the form of relevance scores. Since obtaining relevance scores typically involves human annotation, it is of great interest to consider a partial feedback setting where feedback is restricted to the top-$k$ items in the rankings. Chaudhuri and Tewari [2017] developed a framework to analyze online ranking algorithms with top $k$ feedback. A key element in their work was the use of techniques from partial monitoring. In this paper, we further investigate online ranking with top $k$ feedback and solve some open problems posed by Chaudhuri and Tewari [2017]. We provide a full characterization of minimax regret rates with the top $k$ feedback model for all $k$ and for the following ranking performance measures: Pairwise Loss, Discounted Cumulative Gain, and Precision@n. In addition, we give an efficient algorithm that achieves the minimax regret rate for Precision@n.
</details>
<details>
<summary>摘要</summary>
在在线排名中，一种学习算法顺序排序一组项目，并接收反馈分数以评估其排名的有用性。由于获取反馈分数通常需要人工标注，因此受限于top-$k$反馈的情况是非常有趣。查得和特ва里[2017]提出了在线排名算法的框架，并使用partial monitoring技术进行分析。在这篇论文中，我们进一步调查了在线排名的top-$k$反馈问题，并解决了查得和特wa里[2017]提出的一些开放问题。我们提供了所有$k$的最小最大偏差率的完整 характеристику，以及对Pairwise Loss、Discounted Cumulative Gain和Precision@n的排名性能指标进行分析。此外，我们还提供了实现最小最大偏差率的高效算法。
</details></li>
</ul>
<hr>
<h2 id="Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test"><a href="#Maximum-Mean-Discrepancy-Meets-Neural-Networks-The-Radon-Kolmogorov-Smirnov-Test" class="headerlink" title="Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test"></a>Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02422">http://arxiv.org/abs/2309.02422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghoon Paik, Michael Celentano, Alden Green, Ryan J. Tibshirani</li>
<li>For: The paper is written to introduce a new test for comparing two distributions, called the Radon-Kolmogorov-Smirnov (RKS) test, which is based on the concept of Radon bounded variation (RBV) and has applications in deep learning.* Methods: The RKS test uses the unit ball in the RBV space of a given smoothness order $k \geq 0$ as the function space $\mathcal{F}$ and maximizes the mean difference over samples from one distribution $P$ versus another $Q$. The test is related to neural networks and can be optimized using modern deep learning toolkits.* Results: The paper proves that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not&#x3D; Q$ of distributions, derives its asymptotic null distribution, and conducts extensive experiments to evaluate the strengths and weaknesses of the RKS test compared to the more traditional kernel MMD test.Here is the information in Simplified Chinese:</li>
<li>for: 这篇论文是为了介绍一种新的分布比较测试方法，称为Radon-Kolmogorov-Smirnov（RKS）测试，它基于Radon bounded variation（RBV）的概念，有应用于深度学习。</li>
<li>methods: RKS测试使用 unit ball在RBV空间中的smoothness order $k \geq 0$作为函数空间$\mathcal{F}$，对一个分布$P$和另一个分布$Q$的样本进行最大化mean difference。测试与神经网络有关，可以使用现代深度学习工具包来（近似地）优化测试的 критериion。</li>
<li>results: 论文证明RKS测试在任意不同的分布$P \not&#x3D; Q$中具有极大的权重， derivation of its asymptotic null distribution, 并进行了广泛的实验来评估RKS测试与传统的kernel MMD测试的优劣点。<details>
<summary>Abstract</summary>
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. This allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. We prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weakenesses of the RKS test versus the more traditional kernel MMD test.
</details>
<details>
<summary>摘要</summary>
“最大均值差（MMD）是一种通用的非参数TwoSample测试，它基于对一个分布$P$和另一个分布$Q$的样本之间的均值差进行最大化，而且对所有的数据变换$f$生成在一个函数空间$\mathcal{F}$中。 drawing inspiration from recent work that connects functions of Radon bounded variation (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. this test, which we refer to as the Radon-Kolmogorov-Smirnov (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. it is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree $k$, i.e., a single neuron in a neural network. this allows us to leverage the power of modern deep learning toolkits to (approximately) optimize the criterion that underlies the RKS test. we prove that the RKS test has asymptotically full power at distinguishing any distinct pair $P \not= Q$ of distributions, derive its asymptotic null distribution, and carry out extensive experiments to elucidate the strengths and weaknesses of the RKS test versus the more traditional kernel MMD test.”Note: "Simplified Chinese" is a translation of the text into traditional Chinese characters, but with simpler grammar and vocabulary to make it easier to read and understand.
</details></li>
</ul>
<hr>
<h2 id="Computing-SHAP-Efficiently-Using-Model-Structure-Information"><a href="#Computing-SHAP-Efficiently-Using-Model-Structure-Information" class="headerlink" title="Computing SHAP Efficiently Using Model Structure Information"></a>Computing SHAP Efficiently Using Model Structure Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02417">http://arxiv.org/abs/2309.02417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linwei Hu, Ke Wang</li>
<li>for: 本研究旨在提高 SHAP（SHapley Additive exPlanations）计算的效率，因为现有的方法具有几乎 exponential 时间复杂度。</li>
<li>methods: 本文提出了三种方法来计算 SHAP，包括：1) 基于函数分解的方法，可以在 polynomial 时间内计算 SHAP; 2) 基于模型结构信息的方法，可以在 polynomial 时间内计算 SHAP; 3) 基于迭代方法的方法，可以用于 unknown 模型结构情况下。</li>
<li>results: 三种方法均可以准确计算 SHAP，并且 computationally efficient。与 Castor &amp; Gomez (2008) 的采样方法进行比较，本文的方法在几乎所有情况下具有更高的效率。<details>
<summary>Abstract</summary>
SHAP (SHapley Additive exPlanations) has become a popular method to attribute the prediction of a machine learning model on an input to its features. One main challenge of SHAP is the computation time. An exact computation of Shapley values requires exponential time complexity. Therefore, many approximation methods are proposed in the literature. In this paper, we propose methods that can compute SHAP exactly in polynomial time or even faster for SHAP definitions that satisfy our additivity and dummy assumptions (eg, kernal SHAP and baseline SHAP). We develop different strategies for models with different levels of model structure information: known functional decomposition, known order of model (defined as highest order of interaction in the model), or unknown order. For the first case, we demonstrate an additive property and a way to compute SHAP from the lower-order functional components. For the second case, we derive formulas that can compute SHAP in polynomial time. Both methods yield exact SHAP results. Finally, if even the order of model is unknown, we propose an iterative way to approximate Shapley values. The three methods we propose are computationally efficient when the order of model is not high which is typically the case in practice. We compare with sampling approach proposed in Castor & Gomez (2008) using simulation studies to demonstrate the efficacy of our proposed methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians"><a href="#First-and-zeroth-order-implementations-of-the-regularized-Newton-method-with-lazy-approximated-Hessians" class="headerlink" title="First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians"></a>First and zeroth-order implementations of the regularized Newton method with lazy approximated Hessians</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02412">http://arxiv.org/abs/2309.02412</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Doikov, Geovani Nunes Grapiglia</li>
<li>for:  solving general non-convex optimization problems</li>
<li>methods:  Cubically regularized Newton method with finite difference approximations of the derivatives, and an adaptive search procedure that simultaneously fits the regularization constant and the parameters of the finite difference approximations</li>
<li>results:  global complexity bound of $\mathcal{O}( n^{1&#x2F;2} \epsilon^{-3&#x2F;2})$ function and gradient evaluations for the Hessian-free method, and a bound of $\mathcal{O}( n^{3&#x2F;2} \epsilon^{-3&#x2F;2} )$ function evaluations for the derivative-free method, which significantly improve the previously known ones in terms of the joint dependence on $n$ and $\epsilon$.<details>
<summary>Abstract</summary>
In this work, we develop first-order (Hessian-free) and zero-order (derivative-free) implementations of the Cubically regularized Newton method for solving general non-convex optimization problems. For that, we employ finite difference approximations of the derivatives. We use a special adaptive search procedure in our algorithms, which simultaneously fits both the regularization constant and the parameters of the finite difference approximations. It makes our schemes free from the need to know the actual Lipschitz constants. Additionally, we equip our algorithms with the lazy Hessian update that reuse a previously computed Hessian approximation matrix for several iterations. Specifically, we prove the global complexity bound of $\mathcal{O}( n^{1/2} \epsilon^{-3/2})$ function and gradient evaluations for our new Hessian-free method, and a bound of $\mathcal{O}( n^{3/2} \epsilon^{-3/2} )$ function evaluations for the derivative-free method, where $n$ is the dimension of the problem and $\epsilon$ is the desired accuracy for the gradient norm. These complexity bounds significantly improve the previously known ones in terms of the joint dependence on $n$ and $\epsilon$, for the first-order and zeroth-order non-convex optimization.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们开发了一种基于第一阶 Taylor 展开的卷积规范 Newton 方法，用于解决普通非凸优化问题。我们使用贝叶斯适应搜索算法，以同时调整规范常数和finite differenceapproximation的参数。这使得我们的算法不需要知道实际的Lipschitz常数。此外，我们还在我们的算法中使用懒散Hessian更新，使用已经计算过的Hessian近似矩阵进行多个迭代。我们证明了我们新的Hessian-free方法的全局复杂度上下文为 $\mathcal{O}(n^{1/2} \epsilon^{-3/2})$ 函数和梯度评估数，而derivative-free方法的复杂度上下文为 $\mathcal{O}(n^{3/2} \epsilon^{-3/2})$ 函数评估数，其中 $n$ 是优化问题的维度， $\epsilon$ 是梯度norm的desired accuracy。这些复杂度上下文在之前已知的joint $n$ 和 $\epsilon$ 的依赖关系方面具有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices"><a href="#Delta-LoRA-Fine-Tuning-High-Rank-Parameters-with-the-Delta-of-Low-Rank-Matrices" class="headerlink" title="Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices"></a>Delta-LoRA: Fine-Tuning High-Rank Parameters with the Delta of Low-Rank Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02411">http://arxiv.org/abs/2309.02411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, Lei Zhang</li>
<li>for: 本文提出了Delta-LoRA，一种基于大语言模型的参数高效的细化方法。</li>
<li>methods: Delta-LoRA不仅更新低级矩阵 $\bA$ 和 $\bB$，还通过更新 $\bW$ 中的差值（$\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$）来进行学习。</li>
<li>results: 对比LoRA和其他低级适应方法，Delta-LoRA在下游任务中表现出色，并且与LoRA相比，Delta-LoRA具有相同的内存需求和计算成本。<details>
<summary>Abstract</summary>
In this paper, we present Delta-LoRA, which is a novel parameter-efficient approach to fine-tune large language models (LLMs). In contrast to LoRA and other low-rank adaptation methods such as AdaLoRA, Delta-LoRA not only updates the low-rank matrices $\bA$ and $\bB$, but also propagate the learning to the pre-trained weights $\bW$ via updates utilizing the delta of the product of two low-rank matrices ($\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$). Such a strategy effectively addresses the limitation that the incremental update of low-rank matrices is inadequate for learning representations capable for downstream tasks. Moreover, as the update of $\bW$ does not need to compute the gradients of $\bW$ and store their momentums, Delta-LoRA shares comparable memory requirements and computational costs with LoRA. Extensive experiments show that Delta-LoRA significantly outperforms existing low-rank adaptation methods. We further support these results with comprehensive analyses that underscore the effectiveness of Delta-LoRA.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了Delta-LoRA，它是一种新的参数效率的方法，用于精细调整大型自然语言模型（LLM）。与LoRA和其他低级权 adaptation方法相比，Delta-LoRA不仅更新了低级矩阵$\bA$和$\bB$,还通过使用两个低级矩阵的乘积 delta（$\bA^{(t+1)}\bB^{(t+1)} - \bA^{(t)}\bB^{(t)}$）来升级学习。这种策略有效地解决了低级矩阵逐步更新的限制，使得模型能够更好地适应下游任务。此外，由于更新 $\bW$ 不需要计算 $\bW$ 的梯度和存储它们的动量，Delta-LoRA 与 LoRA 的内存需求和计算成本相同。实验表明，Delta-LoRA 显著超过了现有的低级权 adaptation 方法。我们还提供了全面的分析，以证明 Delta-LoRA 的效果。
</details></li>
</ul>
<hr>
<h2 id="In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms"><a href="#In-Ear-Voice-Towards-Milli-Watt-Audio-Enhancement-With-Bone-Conduction-Microphones-for-In-Ear-Sensing-Platforms" class="headerlink" title="In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms"></a>In-Ear-Voice: Towards Milli-Watt Audio Enhancement With Bone-Conduction Microphones for In-Ear Sensing Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02393">http://arxiv.org/abs/2309.02393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Schilk, Niccolò Polvani, Andrea Ronco, Milos Cernak, Michele Magno</li>
<li>for: 这个论文是为了解决远程会议中音频质量受到扰乱的问题而写的。</li>
<li>methods: 这篇论文使用了新型的MEMS骨传导 Microphone，并采用了个性化语音活动检测算法和循环神经网络来解决问题。</li>
<li>results: 论文评估了一种基于骨传导数据的低功耗个性化语音检测算法，并与传统麦克风输入的方法进行比较。实验结果显示，骨传导系统可以在12.8毫秒内正确地检测到语音，并且具有43小时的电池寿命。<details>
<summary>Abstract</summary>
The recent ubiquitous adoption of remote conferencing has been accompanied by omnipresent frustration with distorted or otherwise unclear voice communication. Audio enhancement can compensate for low-quality input signals from, for example, small true wireless earbuds, by applying noise suppression techniques. Such processing relies on voice activity detection (VAD) with low latency and the added capability of discriminating the wearer's voice from others - a task of significant computational complexity. The tight energy budget of devices as small as modern earphones, however, requires any system attempting to tackle this problem to do so with minimal power and processing overhead, while not relying on speaker-specific voice samples and training due to usability concerns.   This paper presents the design and implementation of a custom research platform for low-power wireless earbuds based on novel, commercial, MEMS bone-conduction microphones. Such microphones can record the wearer's speech with much greater isolation, enabling personalized voice activity detection and further audio enhancement applications. Furthermore, the paper accurately evaluates a proposed low-power personalized speech detection algorithm based on bone conduction data and a recurrent neural network running on the implemented research platform. This algorithm is compared to an approach based on traditional microphone input. The performance of the bone conduction system, achieving detection of speech within 12.8ms at an accuracy of 95\% is evaluated. Different SoC choices are contrasted, with the final implementation based on the cutting-edge Ambiq Apollo 4 Blue SoC achieving 2.64mW average power consumption at 14uJ per inference, reaching 43h of battery life on a miniature 32mAh li-ion cell and without duty cycling.
</details>
<details>
<summary>摘要</summary>
现代无线耳机的普遍采用导致了远程会议中的声音扭曲或不清楚的问题，而声音提升技术可以补做小质量输入信号的问题。这种处理需要具备快速响应和可区分戴户的声音和其他声音的能力，但是由于设备的能量限制，系统不能依赖于特定的说话人样本和训练。本文描述了一种自定义研究平台，基于新型商业MEMS骨传声 microphone，用于低功耗无线耳机。这种 microphone 可以更好地隔离戴户的声音，以便实现个性化声音活动检测和其他声音提升应用。此外，文章详细评估了一种基于骨传声数据和回归神经网络的低功耗个性化声音检测算法。这种算法与传统 микрофон输入的方法进行比较，并评估了骨传声系统的性能，包括检测speech within 12.8ms 的精度为 95%。文章还对不同的SoC选择进行了比较，最终实现基于Ambiq Apollo 4 Blue SoC的实现，占用2.64mW的平均电力consumption，可以达到32mAh电池的43h寿命，无需循环停机。
</details></li>
</ul>
<hr>
<h2 id="Explaining-grokking-through-circuit-efficiency"><a href="#Explaining-grokking-through-circuit-efficiency" class="headerlink" title="Explaining grokking through circuit efficiency"></a>Explaining grokking through circuit efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02390">http://arxiv.org/abs/2309.02390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikrant Varma, Rohin Shah, Zachary Kenton, János Kramár, Ramana Kumar</li>
<li>for: 本研究探讨了神经网络的总结和泛化问题，具体来说是 Grokking 现象，即训练准确率很高 yet 泛化率很低的网络，在进一步训练后会转变为泛化率很高 yet 训练准确率很低。</li>
<li>methods: 作者提出了一种解释 Grokking 现象的假设，即任务存在一个泛化解决方案和一个记忆解决方案，其中泛化解决方案 slower to learn 但更高效，生成更大的 logits 与参数 нор 相同。作者还提出了四个新预测，并证明了其中的四个。</li>
<li>results: 作者通过实验证明了他们的假设，并发现了两种新的行为：ungrokking 和 semi-grokking。ungrokking 是指网络从完美测试率下降到低测试率的现象，而 semi-grokking 是指网络在部分测试数据上显示延迟的泛化行为。<details>
<summary>Abstract</summary>
One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.
</details>
<details>
<summary>摘要</summary>
一种非常有趣的神经网络泛化问题是“grokking”：一个网络在完美训练后却表现出低泛化性能。我们提出，grokking发生在任务允许一个泛化解决方案和一个记忆解决方案，其中泛化解决方案需要更长的时间学习，但生成更大的logits，同样的参数 нор。我们假设记忆化环路在更大的训练数据集上变得更不效率，而泛化环路不变。我们提出四个新预测，并证明了这些预测。最引人注目的是我们发现了两种新的行为：ungrokking和半泛化。ungrokking是指一个网络在完美训练后却降低到低测试精度，而半泛化是指一个网络在部分测试数据上显示延迟的泛化。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation"><a href="#A-Lightweight-and-Transferable-Design-for-Robust-LEGO-Manipulation" class="headerlink" title="A Lightweight and Transferable Design for Robust LEGO Manipulation"></a>A Lightweight and Transferable Design for Robust LEGO Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02354">http://arxiv.org/abs/2309.02354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixuan Liu, Yifan Sun, Changliu Liu</li>
<li>for: 这篇论文是研究如何实现安全高效的机器人LEGO拼接。</li>
<li>methods: 本论文使用硬件软件共设计，设计了一个终端工具（EOAT），以便大型工业机器人轻松地拼接LEGO块。此外，本论文使用进化策略安全地优化机器人运动，以实现100%的成功率。</li>
<li>results: 实验表明，EOAT可靠地进行LEGO拼接，而学习框架可以安全地提高拼接性能，达到100%的成功率。另外，本解决方案在多个机器人（FANUC LR-mate 200id&#x2F;7L和Yaskawa GP4）上进行了多机器人扩展和传输性测试，以证明其通用性和可重复性。最后，我们表明了该解决方案可以实现可持续的机器人LEGO拼接，机器人可以重复地组装和解组不同的原型。<details>
<summary>Abstract</summary>
LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100\% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the proposed solution enables sustainable robotic LEGO prototyping, in which the robot can repeatedly assemble and disassemble different prototypes.
</details>
<details>
<summary>摘要</summary>
LEGO 是一个知名的原型平台，但是机器人 LEGO 拼接 (i.e. 拼接 LEGO 块) 具有挑战性，主要是因为连接紧密和精度要求高。这篇论文 investigate 安全和高效的机器人 LEGO 拼接方法。特别是这篇论文通过硬件软件共设计来降低拼接复杂性。一个终端工具 (EOAT) 被设计，可以减少问题维度，使大型工业机器人更容易地拼接 LEGO 块。此外，这篇论文使用进化策略来安全地优化机器人运动，以达到100% 的成功率。实验表明，EOAT 可靠地 manipulating LEGO 块，并且学习框架可以效果地提高拼接性能。 co-design 被部署到多个机器人 (i.e. FANUC LR-mate 200id/7L 和 Yaskawa GP4)，以示其通用性和传递性。最后，我们示出了我们的解决方案可以实现可持续的机器人 LEGO 拼接，机器人可以重复地组装和解组不同的原型。
</details></li>
</ul>
<hr>
<h2 id="Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics"><a href="#Exact-Inference-for-Continuous-Time-Gaussian-Process-Dynamics" class="headerlink" title="Exact Inference for Continuous-Time Gaussian Process Dynamics"></a>Exact Inference for Continuous-Time Gaussian Process Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02351">http://arxiv.org/abs/2309.02351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Ensinger, Nicholas Tagliapietra, Sebastian Ziesche, Sebastian Trimpe</li>
<li>for: 提取真实连续时间系统的GP模型</li>
<li>methods: 利用多步和泰勒积分器，实现直接GP推理</li>
<li>results: 实验和理论表明，该方法可以准确地表示连续时间系统的GP模型<details>
<summary>Abstract</summary>
Physical systems can often be described via a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. In order to make direct inference tractable, we propose to leverage multistep and Taylor integrators. We demonstrate how to derive flexible inference schemes for these types of integrators. Further, we derive tailored sampling schemes that allow to draw consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model. We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.
</details>
<details>
<summary>摘要</summary>
Physical systems can often be described using a continuous-time dynamical system. However, the true system is often unknown and must be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in certain scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties must be conserved. Therefore, we aim to learn a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. However, many higher-order integrators require dynamics evaluations at intermediate time steps, making exact GP inference intractable.In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is preferable in many scenarios, e.g. due to its mathematical guarantees. To make direct inference tractable, we propose to leverage multistep and Taylor integrators. We derive how to derive flexible inference schemes for these types of integrators. Additionally, we derive tailored sampling schemes that allow drawing consistent dynamics functions from the learned posterior. This is crucial to sample consistent predictions from the dynamics model.We demonstrate empirically and theoretically that our approach yields an accurate representation of the continuous-time system.Note: The text is translated into Simplified Chinese, which is a standardized form of Chinese used in mainland China and Singapore. The translation is written in Traditional Chinese characters, which is the standard form of Chinese used in Taiwan and other countries.这些physical systems可以用一个连续时间的动力系统来描述。然而，真正的系统是未知的，需要从测量数据学习。由于数据通常是在离散时间收集的，例如由感应器收集，因此大多数GP动力系统学习方法是在一步之前预测。这可能会在某些情况下问题，例如测量是在不规则时间步进行的或物理系统特性需要保持。因此，我们的目标是学习一个GP模型的真正连续时间系统。高级数字积分器提供了必要的工具来解决这个问题。然而，许多高级积分器需要在中途时间步进行动力评估，使得GP数据 posterior 不可靠。在过去的工作中，这个问题通常是通过将GP posterior 近似为多标量数据学习的方法来解决。然而，精确的GP数据 posterior 是在许多情况下更好的，例如因为它具有数学保证。为了让直接推理可行，我们提议使用多步和泰勒积分器。我们 derivation 了如何 derivation  flexible inference schemes for these types of integrators. 此外，我们也 derivation 了适合的抽样方案，允许从学习的 posterior 中获得一致的动力函数。这是关键的，以获得一致的预测。我们在实验和理论上证明了我们的方法可以实现一个精确的连续时间系统表示。
</details></li>
</ul>
<hr>
<h2 id="PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference"><a href="#PolyLUT-Learning-Piecewise-Polynomials-for-Ultra-Low-Latency-FPGA-LUT-based-Inference" class="headerlink" title="PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference"></a>PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02334">http://arxiv.org/abs/2309.02334</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Andronic, George A. Constantinides</li>
<li>for: 这个论文是为了提高深度学习推理的启动速度和面积使用Field-programmable gate arrays (FPGAs)的实现。</li>
<li>methods: 这个论文提出了一种使用多ivariate polynomials作为深度学习模型的基本建置物件，并将这些多ivariate polynomials hide在FPGA的Lookup Table (LUTs)中，以避免额外的负载。</li>
<li>results: 这个论文显示了使用多ivariate polynomials可以实现相同的准确性，但是使用许多 fewer layers of soft logic，从而获得了显著的启动速度和面积改善。这个方法在三个任务中得到了证明：网络入侵检测、CERN大 HadronCollider上的戳机识别和MNIST datasets上的手写数字识别。<details>
<summary>Abstract</summary>
Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. We demonstrate the effectiveness of this approach in three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and handwritten digit recognition using the MNIST dataset.
</details>
<details>
<summary>摘要</summary>
Field-programmable gate arrays (FPGAs) 广泛用于深度学习推理实现。标准深度神经网络推理包括交叠的线性映射和非线性活化函数的计算。先前的工作对于超低延迟实现做出了硬编码了线性映射和非线性活化函数在FPGALookup表（LUTs）中的方法。我们的工作是基于FPGA LUTs 可以实现更多种函数的想法。在这篇论文中，我们提出了一种新的方法，使用多变量多项式作为神经网络训练的基本构件。我们的方法利用FPGA soft logic 的灵活性，将多项式评估隐藏在LUTs 中，无损失。我们显示，使用多项式构件可以与使用线性函数相比，实现同等准确性，但是具有显著的延迟和面积改进。我们在三个任务中证明了这种方法的有效性：网络入侵检测、在CERN大 адроン卫星中的戳彩识别和使用MNIST数据集的手写数字识别。
</details></li>
</ul>
<hr>
<h2 id="Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source"><a href="#Resilient-VAE-Unsupervised-Anomaly-Detection-at-the-SLAC-Linac-Coherent-Light-Source" class="headerlink" title="Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source"></a>Resilient VAE: Unsupervised Anomaly Detection at the SLAC Linac Coherent Light Source</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02333">http://arxiv.org/abs/2309.02333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Humble, William Colocho, Finn O’Shea, Daniel Ratner, Eric Darve</li>
<li>for: 这篇论文旨在应用深度学习进行异常检测，但现有方法假设有正常训练集（i.e., 无异常数据）或完全标签训练集。在复杂的工程系统中，例如粒子加速器，标签是罕见和昂贵的；为了进行异常检测在这些情况下，我们必须搁置这些假设，并使用完全无监督的方法。</li>
<li>methods: 这篇论文提出了具有抗衰变性的自适应器（ResVAE），一种深度生成模型，用于异常检测。ResVAE在训练过程中学习每个样本的异常可能性，以及每个个别特征的异常可能性，并将这些可能性用于有效地忽略在训练数据中的异常样本。</li>
<li>results: 这篇论文应用ResVAE进行了加速器异常检测，并使用了射测系统中的枪位监控数据。 results show that ResVAE exhibits exceptional ability in identifying various types of anomalies present in the accelerator, and provides feature-level anomaly attribution.<details>
<summary>Abstract</summary>
Significant advances in utilizing deep learning for anomaly detection have been made in recent years. However, these methods largely assume the existence of a normal training set (i.e., uncontaminated by anomalies) or even a completely labeled training set. In many complex engineering systems, such as particle accelerators, labels are sparse and expensive; in order to perform anomaly detection in these cases, we must drop these assumptions and utilize a completely unsupervised method. This paper introduces the Resilient Variational Autoencoder (ResVAE), a deep generative model specifically designed for anomaly detection. ResVAE exhibits resilience to anomalies present in the training data and provides feature-level anomaly attribution. During the training process, ResVAE learns the anomaly probability for each sample as well as each individual feature, utilizing these probabilities to effectively disregard anomalous examples in the training data. We apply our proposed method to detect anomalies in the accelerator status at the SLAC Linac Coherent Light Source (LCLS). By utilizing shot-to-shot data from the beam position monitoring system, we demonstrate the exceptional capability of ResVAE in identifying various types of anomalies that are visible in the accelerator.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Significant advances" is translated as "重要的进步" (zhòng yì de jìn bo)* "Utilizing deep learning" is translated as "使用深度学习" (shǐ yòu shēn dào xué xí)* "Anomaly detection" is translated as "异常检测" (yì cháng jiǎn té)* "Normal training set" is translated as "正常的训练集" (zhèng cháng de xùn xí jí)* "Completely labeled training set" is translated as "完全标注的训练集" (quán zhì biǎo xiǎo de xùn xí jí)* "Particle accelerators" is translated as "粒子加速器" (dì zhí jiā sù qì)* "Sparse and expensive labels" is translated as "稀疏和昂贵的标签" (xī shū hé gōng jí de biāo jiā)* "Completely unsupervised method" is translated as "完全无监督的方法" (quán zhì wú jiǎn dū de fāng fá)* "Resilient Variational Autoencoder" is translated as "鲁棒的变量自适应器" (ròng bò de biàn yù zì xiǎng qì)* "Feature-level anomaly attribution" is translated as "特征层异常报告" (fèi yì zhì yì cháng bào gāo)* "Shot-to-shot data" is translated as "一把一的数据" (yī bǎ yī de xùn xí)* "Beam position monitoring system" is translated as "贝壳位置监测系统" (bēi kē zhì dào jiān tè xì tuán)* "SLAC Linac Coherent Light Source" is translated as "SLAC激光干涉源" (SLA cè liàng kē gǎn chuī yuán)
</details></li>
</ul>
<hr>
<h2 id="A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction"><a href="#A-study-on-the-impact-of-pre-trained-model-on-Just-In-Time-defect-prediction" class="headerlink" title="A study on the impact of pre-trained model on Just-In-Time defect prediction"></a>A study on the impact of pre-trained model on Just-In-Time defect prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02317">http://arxiv.org/abs/2309.02317</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Guo, Xiaopeng Gao, Zhenyu Zhang, W. K. Chan, Bo Jiang</li>
<li>for: 本研究主要针对Just-In-Time（JIT）缺陷预测任务，旨在探讨不同预训模型之间的关系。</li>
<li>methods: 我们建立了六个模型：RoBERTaJIT、CodeBERTJIT、BARTJIT、PLBARTJIT、GPT2JIT和CodeGPTJIT，每个模型都使用不同的预训模型作为底层模型。我们系统地探讨这些模型之间的差异和连接。</li>
<li>results: 我们发现每个模型都有改进，当预训模型的类似性较高时，需要的训练资源减少得更多。我们 также发现提交代码对缺陷探测具有重要作用，不同的预训模型在少量数据下场景下表现出不同的缺陷探测能力。这些结果为JIT缺陷预测任务中使用预训模型优化提供新的视角，并 highlight了需要更多注意的因素。<details>
<summary>Abstract</summary>
Previous researchers conducting Just-In-Time (JIT) defect prediction tasks have primarily focused on the performance of individual pre-trained models, without exploring the relationship between different pre-trained models as backbones. In this study, we build six models: RoBERTaJIT, CodeBERTJIT, BARTJIT, PLBARTJIT, GPT2JIT, and CodeGPTJIT, each with a distinct pre-trained model as its backbone. We systematically explore the differences and connections between these models. Specifically, we investigate the performance of the models when using Commit code and Commit message as inputs, as well as the relationship between training efficiency and model distribution among these six models. Additionally, we conduct an ablation experiment to explore the sensitivity of each model to inputs. Furthermore, we investigate how the models perform in zero-shot and few-shot scenarios. Our findings indicate that each model based on different backbones shows improvements, and when the backbone's pre-training model is similar, the training resources that need to be consumed are much more closer. We also observe that Commit code plays a significant role in defect detection, and different pre-trained models demonstrate better defect detection ability with a balanced dataset under few-shot scenarios. These results provide new insights for optimizing JIT defect prediction tasks using pre-trained models and highlight the factors that require more attention when constructing such models. Additionally, CodeGPTJIT and GPT2JIT achieved better performance than DeepJIT and CC2Vec on the two datasets respectively under 2000 training samples. These findings emphasize the effectiveness of transformer-based pre-trained models in JIT defect prediction tasks, especially in scenarios with limited training data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Inferring-effective-couplings-with-Restricted-Boltzmann-Machines"><a href="#Inferring-effective-couplings-with-Restricted-Boltzmann-Machines" class="headerlink" title="Inferring effective couplings with Restricted Boltzmann Machines"></a>Inferring effective couplings with Restricted Boltzmann Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02292">http://arxiv.org/abs/2309.02292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs">https://github.com/alfonso-navas/inferring_effective_couplings_with_RBMs</a></li>
<li>paper_authors: Aurélien Decelle, Cyril Furtlehner, Alfonso De Jesus Navas Gómez, Beatriz Seoane</li>
<li>for: 本研究旨在提出一种简单的解决方案，使能够准确地理解生成模型中的物理含义。</li>
<li>methods: 本研究使用了 Restricted Boltzmann Machine（RBM），并提出了一种将 RBM 的能量函数映射到有效磁矩度 Hamiltonian 的方法，该方法包括了所有可能的交互次数，超过了传统的对应 inverse Ising 方法所考虑的对习次数。</li>
<li>results: 研究人员通过控制的数值实验，训练 RBM 使用平衡样本，以验证该方法的有效性。结果表明，该方法可以学习正确的交互网络，并可以应用于模型复杂数据集。此外，研究人员还评估了不同训练方法的模型质量。<details>
<summary>Abstract</summary>
Generative models offer a direct way to model complex data. Among them, energy-based models provide us with a neural network model that aims to accurately reproduce all statistical correlations observed in the data at the level of the Boltzmann weight of the model. However, one challenge is to understand the physical interpretation of such models. In this study, we propose a simple solution by implementing a direct mapping between the energy function of the Restricted Boltzmann Machine and an effective Ising spin Hamiltonian that includes high-order interactions between spins. This mapping includes interactions of all possible orders, going beyond the conventional pairwise interactions typically considered in the inverse Ising approach, and allowing the description of complex datasets. Earlier works attempted to achieve this goal, but the proposed mappings did not do properly treat the complexity of the problem or did not contain direct prescriptions for practical application. To validate our method, we performed several controlled numerical experiments where we trained the RBMs using equilibrium samples of predefined models containing local external fields, two-body and three-body interactions in various low-dimensional topologies. The results demonstrate the effectiveness of our proposed approach in learning the correct interaction network and pave the way for its application in modeling interesting datasets. We also evaluate the quality of the inferred model based on different training methods.
</details>
<details>
<summary>摘要</summary>
<?xml:namespace prefix = "o" ns = "urn:schemas-microsoft-com:office:office" />生成模型提供了直接模型复杂数据的方式。其中，能量基型模型为我们提供了一个基于神经网络的模型，该模型的目标是在数据中识别所有 estadísticos correlations，并在模型的Boltzmann权重水平上准确地复制它们。然而，一个挑战是理解这些模型的物理解释。在这项研究中，我们提议一种简单的解决方案，通过将Restricted Boltzmann Machine（RBM）的能量函数直接映射到包含高阶交互的有效牛顿矩阵 Hamiltoniano中。这种映射包括所有可能的顺序交互，超过了传统的对应 inverse Ising 方法中考虑的对应交互，并允许描述复杂的数据集。先前的工作尝试了实现这个目标，但是提议的映射没有正确地处理问题的复杂性或者没有直接的实践指南。为验证我们的方法，我们进行了一些控制性的数字实验，在 pré-definido 模型中使用平衡样本，包括局部外场、二体和三体交互在多种低维度拓扑中。结果表明了我们提议的方法的效iveness，可以学习正确的交互网络，并为复杂的数据集模型提供了道路。我们还评估了不同的培训方法的模型质量。
</details></li>
</ul>
<hr>
<h2 id="A-Comparison-of-Residual-based-Methods-on-Fault-Detection"><a href="#A-Comparison-of-Residual-based-Methods-on-Fault-Detection" class="headerlink" title="A Comparison of Residual-based Methods on Fault Detection"></a>A Comparison of Residual-based Methods on Fault Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02274">http://arxiv.org/abs/2309.02274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chi-Ching Hsu, Gaetan Frusque, Olga Fink</li>
<li>for: 本研究旨在比较两种残差基于方法：自动编码器和输入输出模型，以检测FAULTS并区分不同的潜在FAULT类型。</li>
<li>methods: 两种方法都使用残差来检测FAULTS，并且都使用健康数据进行训练。</li>
<li>results: 两种方法都可以在约20个循环后检测FAULTS，并保持低的假阳性率。而输入输出模型提供更好的解释力，包括可能的FAULT类型和可能的FAULT Component。<details>
<summary>Abstract</summary>
An important initial step in fault detection for complex industrial systems is gaining an understanding of their health condition. Subsequently, continuous monitoring of this health condition becomes crucial to observe its evolution, track changes over time, and isolate faults. As faults are typically rare occurrences, it is essential to perform this monitoring in an unsupervised manner. Various approaches have been proposed not only to detect faults in an unsupervised manner but also to distinguish between different potential fault types. In this study, we perform a comprehensive comparison between two residual-based approaches: autoencoders, and the input-output models that establish a mapping between operating conditions and sensor readings. We explore the sensor-wise residuals and aggregated residuals for the entire system in both methods. The performance evaluation focuses on three tasks: health indicator construction, fault detection, and health indicator interpretation. To perform the comparison, we utilize the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model, specifically a subset of the turbofan engine dataset containing three different fault types. All models are trained exclusively on healthy data. Fault detection is achieved by applying a threshold that is determined based on the healthy condition. The detection results reveal that both models are capable of detecting faults with an average delay of around 20 cycles and maintain a low false positive rate. While the fault detection performance is similar for both models, the input-output model provides better interpretability regarding potential fault types and the possible faulty components.
</details>
<details>
<summary>摘要</summary>
<<sys.language_model.activate(lang='zh-CN')>>为了探测复杂工业系统中的FAULT， initially understanding its health condition是非常重要的一步。随后，对这个健康状况的持续监测变得非常重要，以观察其发展、跟踪变化并孤立FAULT。由于FAULT是非常罕见的，因此需要在无监督的情况下进行监测。多种方法已经被提议，不仅可以探测FAULT，还可以分辨不同的可能的FAULT类型。在本研究中，我们进行了总比较两种异常检测方法：自适应神经网络和输入输出模型，它们都可以建立运行条件和传感器读ings之间的映射。我们分析了传感器级别和系统级别的差异，并对三个不同的FAULT类型进行了评估。我们使用了商用模块式飞机发动机 simulate（C-MAPSS）动力模型，特别是一个包含三种FAULT类型的液冷发动机数据集。所有模型都是由健康数据进行了唯一的训练。异常检测是通过设置基于健康状况的阈值来实现的。检测结果显示，两种模型都能够在20轮异常检测，并保持低的假阳性率。虽然异常检测性能相似，但输入输出模型提供了更好的可解释性，即可能的FAULT类型和可能的异常组件。<<sys.language_model.deactivate()>>
</details></li>
</ul>
<hr>
<h2 id="Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette"><a href="#Graph-Based-Automatic-Feature-Selection-for-Multi-Class-Classification-via-Mean-Simplified-Silhouette" class="headerlink" title="Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette"></a>Graph-Based Automatic Feature Selection for Multi-Class Classification via Mean Simplified Silhouette</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02272">http://arxiv.org/abs/2309.02272</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlevinwork/GB-AFS">https://github.com/davidlevinwork/GB-AFS</a></li>
<li>paper_authors: David Levin, Gonen Singer</li>
<li>for: 本研究提出了一种新的图structured filter方法，用于自动特征选择（简称GB-AFS），以便在多类分类任务中提高预测性能。</li>
<li>methods: 该方法使用Jeffries-Matusita距离和t-分布随机邻域嵌入（t-SNE）生成一个低维度空间，以反映每个特征在每对类之间如何有效地分化。而选择最小数量的特征则使用我们新提出的平均简化 Silhouette指数（简称MSS），用于评估特征选择任务中的凝结结果。</li>
<li>results: 实验结果表明，提案的GB-AFS方法在公共数据集上表现出优于其他筛子基本方法和自动特征选择方法。此外，GB-AFS方法可以保持使用所有特征时的准确率，只使用$7%$到$30%$的特征，从而降低了分类时间的消耗，从$15%$降低到$70%$.<details>
<summary>Abstract</summary>
This paper introduces a novel graph-based filter method for automatic feature selection (abbreviated as GB-AFS) for multi-class classification tasks. The method determines the minimum combination of features required to sustain prediction performance while maintaining complementary discriminating abilities between different classes. It does not require any user-defined parameters such as the number of features to select. The methodology employs the Jeffries-Matusita (JM) distance in conjunction with t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space reflecting how effectively each feature can differentiate between each pair of classes. The minimum number of features is selected using our newly developed Mean Simplified Silhouette (abbreviated as MSS) index, designed to evaluate the clustering results for the feature selection task. Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. Moreover, the proposed algorithm maintained the accuracy achieved when utilizing all features, while using only $7\%$ to $30\%$ of the features. Consequently, this resulted in a reduction of the time needed for classifications, from $15\%$ to $70\%$.
</details>
<details>
<summary>摘要</summary>
The methodology employs Jeffries-Matusita distance and t-distributed Stochastic Neighbor Embedding (t-SNE) to generate a low-dimensional space that reflects how effectively each feature differentiates between each pair of classes. The minimum number of features is selected using the Mean Simplified Silhouette (MSS) index, which evaluates the clustering results for the feature selection task.Experimental results on public data sets demonstrate the superior performance of the proposed GB-AFS over other filter-based techniques and automatic feature selection approaches. The proposed algorithm achieved the same accuracy as using all features, while using only 7% to 30% of the features, resulting in a significant reduction in classification time, from 15% to 70%.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning"><a href="#Optimal-Sample-Selection-Through-Uncertainty-Estimation-and-Its-Application-in-Deep-Learning" class="headerlink" title="Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning"></a>Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02476">http://arxiv.org/abs/2309.02476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Lin, Chen Liu, Chenlu Ye, Qing Lian, Yuan Yao, Tong Zhang</li>
<li>For: 这个研究旨在提出一个可靠的方法来选择资料subset，以减少深度学习模型的训练成本和错误。* Methods: 本研究使用了潜在子集选择和活动学习，并提出了一个 theoretically 优质的解决方案，名为COPS（uncertainty based OPtimal Sub-sampling），可以对于线性软MAX regression进行最佳化。* Results: 在实验中，COPS 方法与基eline方法比较，结果显示 COPS 方法在深度学习任务中具有superior表现，并且可以对于模型缺失和低密度样本进行下条件调整。<details>
<summary>Abstract</summary>
Modern deep learning heavily relies on large labeled datasets, which often comse with high costs in terms of both manual labeling and computational resources. To mitigate these challenges, researchers have explored the use of informative subset selection techniques, including coreset selection and active learning. Specifically, coreset selection involves sampling data with both input ($\bx$) and output ($\by$), active learning focuses solely on the input data ($\bx$).   In this study, we present a theoretically optimal solution for addressing both coreset selection and active learning within the context of linear softmax regression. Our proposed method, COPS (unCertainty based OPtimal Sub-sampling), is designed to minimize the expected loss of a model trained on subsampled data. Unlike existing approaches that rely on explicit calculations of the inverse covariance matrix, which are not easily applicable to deep learning scenarios, COPS leverages the model's logits to estimate the sampling ratio. This sampling ratio is closely associated with model uncertainty and can be effectively applied to deep learning tasks. Furthermore, we address the challenge of model sensitivity to misspecification by incorporating a down-weighting approach for low-density samples, drawing inspiration from previous works.   To assess the effectiveness of our proposed method, we conducted extensive empirical experiments using deep neural networks on benchmark datasets. The results consistently showcase the superior performance of COPS compared to baseline methods, reaffirming its efficacy.
</details>
<details>
<summary>摘要</summary>
现代深度学习强调大量标注数据，经常带来高的人工标注和计算成本。为了缓解这些挑战，研究人员已经探索了有用的子集选择技术，包括核心集选择和活动学习。特别是，核心集选择是通过采样数据来减少数据量，而活动学习则仅关注输入数据。在本研究中，我们提出了在线性软MAX回归中的理论优化解决方案，名为COPS（uncertainty based Optimal Sub-sampling）。我们的方法旨在降低由subsampled数据训练的模型预测错误的期望损失。与现有方法不同，COPS不需要显式计算 inverse covariance matrix，而是利用模型的logits来估算采样比率。这个采样比率与模型uncertainty有很Close关系，可以有效应用于深度学习任务。此外，我们还解决了模型偏置低密度样本的挑战，通过引入低密度样本下降值策略，这种策略 drew inspiration from previous works。为评估我们的提出方法的有效性，我们在深度神经网络上进行了广泛的实验。结果 consistently showcase COPS比基准方法更好，这再次证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning"><a href="#RoBoSS-A-Robust-Bounded-Sparse-and-Smooth-Loss-Function-for-Supervised-Learning" class="headerlink" title="RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning"></a>RoBoSS: A Robust, Bounded, Sparse, and Smooth Loss Function for Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02250">http://arxiv.org/abs/2309.02250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mtanveer1/RoBoSS">https://github.com/mtanveer1/RoBoSS</a></li>
<li>paper_authors: Mushir Akhtar, M. Tanveer, Mohd. Arshad</li>
<li>For: 本研究提出了一种新的稳定、缩短、稀疏和均匀（RoBoSS）损失函数，用于改进支持向量机（SVM）的超级vised学习算法。* Methods: 本文 integra RoBoSS损失函数到SVM框架中，并提出了一种新的稳定化算法($\mathcal{L}<em>{rbss}$-SVM)。同时，本文也进行了对RoBoSS损失函数的分类准确性和泛化能力的理论分析。* Results: 实验表明，使用提出的$\mathcal{L}</em>{rbss}$-SVM模型，在88个真实世界UCI和KEEL数据集上表现出色，并且在医学领域中，在EEG信号数据集和Breast Cancer（BreaKHis）数据集上也得到了惊喜的结果。<details>
<summary>Abstract</summary>
In the domain of machine learning algorithms, the significance of the loss function is paramount, especially in supervised learning tasks. It serves as a fundamental pillar that profoundly influences the behavior and efficacy of supervised learning algorithms. Traditional loss functions, while widely used, often struggle to handle noisy and high-dimensional data, impede model interpretability, and lead to slow convergence during training. In this paper, we address the aforementioned constraints by proposing a novel robust, bounded, sparse, and smooth (RoBoSS) loss function for supervised learning. Further, we incorporate the RoBoSS loss function within the framework of support vector machine (SVM) and introduce a new robust algorithm named $\mathcal{L}_{rbss}$-SVM. For the theoretical analysis, the classification-calibrated property and generalization ability are also presented. These investigations are crucial for gaining deeper insights into the performance of the RoBoSS loss function in the classification tasks and its potential to generalize well to unseen data. To empirically demonstrate the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM, we evaluate it on $88$ real-world UCI and KEEL datasets from diverse domains. Additionally, to exemplify the effectiveness of the proposed $\mathcal{L}_{rbss}$-SVM within the biomedical realm, we evaluated it on two medical datasets: the electroencephalogram (EEG) signal dataset and the breast cancer (BreaKHis) dataset. The numerical results substantiate the superiority of the proposed $\mathcal{L}_{rbss}$-SVM model, both in terms of its remarkable generalization performance and its efficiency in training time.
</details>
<details>
<summary>摘要</summary>
在机器学习算法领域，损失函数的重要性非常高，特别在指导学习任务中。它作为基础的核心因素，深刻影响了指导学习算法的行为和效果。传统的损失函数，虽广泛使用，但经常难以处理噪音和高维数据，阻碍模型解释性，并导致训练过程中的慢 converges。本文提出了一种新的robust、bounded、稀疏和均匀（RoBoSS）损失函数，用于指导学习。此外，我们将RoBoSS损失函数 integration到支持向量机（SVM）框架中，并提出了一种新的Robust-SVM算法。对于理论分析方面，我们还提出了分类准备性和泛化能力的研究。这些研究对于了解RoBoSS损失函数在分类任务中的性能和泛化能力具有重要意义。为了证明提出的Lrbss-SVM模型的效果，我们对88个真实世界UCI和KEEL数据集进行了实验评估。此外，为了展示Lrbss-SVM模型在医学领域的效果，我们对电enzephalogram（EEG）信号数据集和Breast Cancer（BreaKHis）数据集进行了评估。实验结果表明，提出的Lrbss-SVM模型具有优秀的泛化性和训练效率。
</details></li>
</ul>
<hr>
<h2 id="Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis"><a href="#Self-Similarity-Based-and-Novelty-based-loss-for-music-structure-analysis" class="headerlink" title="Self-Similarity-Based and Novelty-based loss for music structure analysis"></a>Self-Similarity-Based and Novelty-based loss for music structure analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02243">http://arxiv.org/abs/2309.02243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoffroy Peeters</li>
<li>for: 本研究旨在提出一种监督学习方法来解决音乐分割问题。</li>
<li>methods: 该方法同时学习特征和卷积核，并将自同相似矩阵（SSM）和新鲜度分数作为损失函数进行优化。</li>
<li>results: 研究人员通过对学习到的特征进行自我注意力，提高了音乐分割任务的性能。此外，与之前的方法进行比较，该方法在RWC-Pop和SALAMI等标准数据集上表现较优。<details>
<summary>Abstract</summary>
Music Structure Analysis (MSA) is the task aiming at identifying musical segments that compose a music track and possibly label them based on their similarity. In this paper we propose a supervised approach for the task of music boundary detection. In our approach we simultaneously learn features and convolution kernels. For this we jointly optimize -- a loss based on the Self-Similarity-Matrix (SSM) obtained with the learned features, denoted by SSM-loss, and -- a loss based on the novelty score obtained applying the learned kernels to the estimated SSM, denoted by novelty-loss. We also demonstrate that relative feature learning, through self-attention, is beneficial for the task of MSA. Finally, we compare the performances of our approach to previously proposed approaches on the standard RWC-Pop, and various subsets of SALAMI.
</details>
<details>
<summary>摘要</summary>
音乐结构分析（MSA）是目标在音乐轨道中确定乐曲的各个部分，并可能根据它们的相似性进行标签。在这篇论文中，我们提出了一种监督方法来实现音乐边界检测任务。我们同时学习特征和卷积核，并同步优化两种损失函数。其中一种损失函数基于自相似矩阵（SSM），通过学习的特征来获得，被称为SSM-损失；另一种损失函数基于新鲜度分数，通过学习的卷积核应用于估算的SSM中，被称为新鲜度-损失。我们还证明了通过自我注意力来实现相对特征学习是MSA任务中有利的。最后，我们比较了我们的方法与之前提出的方法在标准RWC-Pop和SALAMI中的性能。
</details></li>
</ul>
<hr>
<h2 id="Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research"><a href="#Sample-Size-in-Natural-Language-Processing-within-Healthcare-Research" class="headerlink" title="Sample Size in Natural Language Processing within Healthcare Research"></a>Sample Size in Natural Language Processing within Healthcare Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02237">http://arxiv.org/abs/2309.02237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaya Chaturvedi, Diana Shamsutdinova, Felix Zimmer, Sumithra Velupillai, Daniel Stahl, Robert Stewart, Angus Roberts</li>
<li>for: 本研究是为了提供适合医疗领域文本数据的样本大小选择的建议。</li>
<li>methods: 本研究使用了不同的分类器和样本大小进行了 simulations，以评估不同样本大小对文本分类任务的影响。</li>
<li>results: 研究发现，使用 K-最近邻分类器时，小样本大小可以提供更好的性能指标，而使用支持向量机和BERT模型时，大样本大小提供更好的性能。总之，样本大小大于1000是适合的，可以提供良好的性能指标。<details>
<summary>Abstract</summary>
Sample size calculation is an essential step in most data-based disciplines. Large enough samples ensure representativeness of the population and determine the precision of estimates. This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text. Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies. This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.   Models trained on the MIMIC-III database of critical care records from Beth Israel Deaconess Medical Center were used to classify documents as having or not having Unspecified Essential Hypertension, the most common diagnosis code in the database. Simulations were performed using various classifiers on different sample sizes and class proportions. This was repeated for a comparatively less common diagnosis code within the database of diabetes mellitus without mention of complication.   Smaller sample sizes resulted in better results when using a K-nearest neighbours classifier, whereas larger sample sizes provided better results with support vector machines and BERT models. Overall, a sample size larger than 1000 was sufficient to provide decent performance metrics.   The simulations conducted within this study provide guidelines that can be used as recommendations for selecting appropriate sample sizes and class proportions, and for predicting expected performance, when building classifiers for textual healthcare data. The methodology used here can be modified for sample size estimates calculations with other datasets.
</details>
<details>
<summary>摘要</summary>
Sample size calculation is an essential step in most data-based disciplines. Large enough samples ensure representativeness of the population and determine the precision of estimates. This is true for most quantitative studies, including those that employ machine learning methods, such as natural language processing, where free-text is used to generate predictions and classify instances of text. Within the healthcare domain, the lack of sufficient corpora of previously collected data can be a limiting factor when determining sample sizes for new studies. This paper tries to address the issue by making recommendations on sample sizes for text classification tasks in the healthcare domain.  模型在基于MIMIC-III数据库的医疗记录中训练后，用于分类文档是否有未特定主要高血压症，该数据库中最常见的诊断代码。在不同的样本大小和类别比例下，使用不同的分类器进行了 simulations。这些 simulations 重复使用不同的分类器和不同的样本大小。结果表明，使用 K-最近邻居分类器时，小样本大小得到了更好的结果，而使用支持向量机和BERT模型时，大样本大小得到了更好的结果。总的来说，样本大小大于1000是可以提供不错的性能指标的。  这些 simulations 提供了适用于健康领域文本数据的分类器建立时选择合适的样本大小和类别比例，以及预测性能的指南。这种方法可以适用于其他数据集的样本大小估计计算。
</details></li>
</ul>
<hr>
<h2 id="Distributionally-Robust-Machine-Learning-with-Multi-source-Data"><a href="#Distributionally-Robust-Machine-Learning-with-Multi-source-Data" class="headerlink" title="Distributionally Robust Machine Learning with Multi-source Data"></a>Distributionally Robust Machine Learning with Multi-source Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02211">http://arxiv.org/abs/2309.02211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Wang, Peter Bühlmann, Zijian Guo</li>
<li>for: 这篇文章是用于提高预测性能，当目标分布与源 Population 不同时，传统机器学习方法可能会导致差强预测性能。</li>
<li>methods: 文章提出了一个基于多来源数据的集团分布强化预测模型，这个模型使用了对于target分布的敌方奖励函数来定义，以提高预测性能。</li>
<li>results: 文章的实验结果显示，相比于传统的empirical risk minimization，提案的强化预测模型可以提高预测性能，并且可以让用户对于不同的目标分布进行预测。<details>
<summary>Abstract</summary>
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the convergence rate. Our proposal can be seen as a distributionally robust federated learning approach that is computationally efficient and easy to implement using arbitrary machine learning base algorithms, satisfies some privacy constraints, and has a nice interpretation of different sources' importance for predicting a given target covariate distribution. We demonstrate the performance of our proposed group distributionally robust method on simulated and real data with random forests and neural networks as base-learning algorithms.
</details>
<details>
<summary>摘要</summary>
传统机器学习方法可能会导致预测性能差，因为目标分布与源 популяции不同。这篇论文利用多个源数据，提出了一种分布robust预测模型，定义为最小化一个对target分布的反对抗 reward的 adversarial  objective function。相比于传统的empirical risk minimization，我们的robust预测模型可以提高预测性能 для目标population中的分布shift。我们证明了我们的分布robust预测模型是源population conditional outcome模型的Weighted average。我们利用这个关键的标识结果，以robustify任意机器学习算法，包括随机森林和神经网络。我们开发了一种偏导corrected estimator来估计最佳汇合权，并证明其提高了收敛率。我们的提议可以看作是一种分布robust Federated learning方法，computationally efficient，易于实现，满足一些隐私约束，并具有预测targetcovariate分布的nice interpretation of different sources的重要性。我们在 simulate和实际数据上测试了我们的提议，使用随机森林和神经网络作为基础学习算法。
</details></li>
</ul>
<hr>
<h2 id="Latent-Disentanglement-in-Mesh-Variational-Autoencoders-Improves-the-Diagnosis-of-Craniofacial-Syndromes-and-Aids-Surgical-Planning"><a href="#Latent-Disentanglement-in-Mesh-Variational-Autoencoders-Improves-the-Diagnosis-of-Craniofacial-Syndromes-and-Aids-Surgical-Planning" class="headerlink" title="Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning"></a>Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10825">http://arxiv.org/abs/2309.10825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Foti, Alexander J. Rickart, Bongjin Koo, Eimear O’ Sullivan, Lara S. van de Lande, Athanasios Papaioannou, Roman Khonsari, Danail Stoyanov, N. u. Owase Jeelani, Silvia Schievano, David J. Dunaway, Matthew J. Clarkson</li>
<li>for: 这项研究旨在应用Swap Disentangled Variational Autoencoder（SD-VAE）模型对人类头部复杂结构进行深度学习分析，以便更好地识别和分类各种颅骨畸形。</li>
<li>methods: 这项研究使用了SD-VAE模型，通过对整个头部网格进行分类，同时也可以分析每个区域对颅骨畸形的影响。此外，通过修改生成模型的参数，可以模拟不同的颅骨外科手术结果。</li>
<li>results: 这项研究可以帮助提高颅骨畸形诊断的准确性，帮助外科医生规划手术，以及对手术结果进行客观评估。<details>
<summary>Abstract</summary>
The use of deep learning to undertake shape analysis of the complexities of the human head holds great promise. However, there have traditionally been a number of barriers to accurate modelling, especially when operating on both a global and local level. In this work, we will discuss the application of the Swap Disentangled Variational Autoencoder (SD-VAE) with relevance to Crouzon, Apert and Muenke syndromes. Although syndrome classification is performed on the entire mesh, it is also possible, for the first time, to analyse the influence of each region of the head on the syndromic phenotype. By manipulating specific parameters of the generative model, and producing procedure-specific new shapes, it is also possible to simulate the outcome of a range of craniofacial surgical procedures. This opens new avenues to advance diagnosis, aids surgical planning and allows for the objective evaluation of surgical outcomes.
</details>
<details>
<summary>摘要</summary>
使用深度学习进行人类头部复杂性分析具有很大的抢救性。然而，在全球和地方层次上准确模型化却存在许多障碍。在这项工作中，我们将讨论使用Swap Disentangled Variational Autoencoder（SD-VAE）在Crouzon、Apert和Muenke综合症中的应用。虽然病种分类是基于整个网格进行的，但是也可以，如 nunca antes，分析每个头部区域对综合症fenotip的影响。通过修改生成模型的特定参数，并生成过程特定的新形状，也可以模拟多种颅外科手术的结果。这些新的技术可以提高诊断、帮助手术规划和评估手术结果的 объекivity。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-for-Novelty-Detection-in-System-Call-Traces"><a href="#Language-Models-for-Novelty-Detection-in-System-Call-Traces" class="headerlink" title="Language Models for Novelty Detection in System Call Traces"></a>Language Models for Novelty Detection in System Call Traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02206">http://arxiv.org/abs/2309.02206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Fournier, Daniel Aloise, Leandro R. Costa</li>
<li>for: 本研究旨在提出一种基于系统调用语言模型的新型行为探测方法，用于检测现代计算机系统中的异常行为。</li>
<li>methods: 本研究使用了三种不同的 neural network 架构：LSTM、Transformer 和 Longformer，并对这些架构进行了评估。</li>
<li>results: 研究发现，使用这些架构可以实现高于 95% 的 F-score 和 AuROC 在大多数新型行为上，而且该方法不需要大量的专家手动编制和可以在不同任务上进行数据独立的应用。<details>
<summary>Abstract</summary>
Due to the complexity of modern computer systems, novel and unexpected behaviors frequently occur. Such deviations are either normal occurrences, such as software updates and new user activities, or abnormalities, such as misconfigurations, latency issues, intrusions, and software bugs. Regardless, novel behaviors are of great interest to developers, and there is a genuine need for efficient and effective methods to detect them. Nowadays, researchers consider system calls to be the most fine-grained and accurate source of information to investigate the behavior of computer systems. Accordingly, this paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. Language models estimate the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model. Following the success of neural networks for language models, three architectures are evaluated in this work: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require an enormous amount of data to be trained effectively, and to the best of our knowledge, no massive modern datasets of kernel traces are publicly available. This paper addresses this limitation by introducing a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors. The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic. The source code and trained models are publicly available on GitHub while the datasets are available on Zenodo.
</details>
<details>
<summary>摘要</summary>
Recently, researchers have turned to system calls as the most fine-grained and accurate source of information to investigate the behavior of computer systems. This paper introduces a novelty detection methodology that relies on a probability distribution over sequences of system calls, which can be seen as a language model. The method estimates the likelihood of sequences, and since novelties deviate from previously observed behaviors by definition, they would be unlikely under the model.To evaluate the effectiveness of the method, three neural network architectures were used: the widespread LSTM, the state-of-the-art Transformer, and the lower-complexity Longformer. However, large neural networks typically require a large amount of data to be trained effectively. To address this limitation, this paper introduces a new open-source dataset of kernel traces comprising over 2 million web requests with seven distinct behaviors.The proposed methodology requires minimal expert hand-crafting and achieves an F-score and AuROC greater than 95% on most novelties while being data- and task-agnostic. The source code and trained models are publicly available on GitHub, while the datasets are available on Zenodo.
</details></li>
</ul>
<hr>
<h2 id="On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence"><a href="#On-the-Complexity-of-Differentially-Private-Best-Arm-Identification-with-Fixed-Confidence" class="headerlink" title="On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence"></a>On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02202">http://arxiv.org/abs/2309.02202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Achraf Azize, Marc Jourdan, Aymen Al Marjani, Debabrota Basu</li>
<li>for: 这个论文主要研究的是如何在数据敏感应用中实现最佳臂标识（BAI）问题，包括设计适应性临床试验、调整超参数以及进行用户研究等。</li>
<li>methods: 这篇论文使用了 $\epsilon$-全球隐私（DP）来保证数据隐私，并研究了 $\epsilon$-全球DP下BAI问题的解决方案。作者首先计算了隐私保护成本的下限，并发现了两种隐私模式，即高隐私模式（小 $\epsilon$）和低隐私模式（大 $\epsilon$）。在高隐私模式下，难度受到隐私和一种新的信息论量——总特征时间的共同影响。在低隐私模式下，下界降到非私下的下界。</li>
<li>results: 作者提出了一种名为 AdaP-TT 的 $\epsilon$-全球DP下的 BAI 算法，该算法在 arm-dependent 的扩展集内运行，并添加了 Laplace 噪声来保证好隐私与实用之间的融合。作者 deriv了 AdaP-TT 的 asymptotic 上限，并证明了其与下界之间的相似性。最后，作者进行了实验分析，并证明了理论结论。<details>
<summary>Abstract</summary>
Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of BAI with fixed confidence under $\epsilon$-global Differential Privacy (DP). First, to quantify the cost of privacy, we derive a lower bound on the sample complexity of any $\delta$-correct BAI algorithm satisfying $\epsilon$-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget $\epsilon$. In the high-privacy regime (small $\epsilon$), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the Total Variation Characteristic Time. In the low-privacy regime (large $\epsilon$), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an $\epsilon$-global DP variant of the Top Two algorithm. AdaP-TT runs in arm-dependent adaptive episodes and adds Laplace noise to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results.
</details>
<details>
<summary>摘要</summary>
最佳臂标识问题（BAI）在数据敏感应用中得到普遍应用，如设计适应式临床试验、调整超参数以及进行用户研究等。由于这些应用中的数据隐私问题，我们研究BAI问题在固定信息保护环境下的$\epsilon$-全球隐私（DP）下进行研究。首先，我们定义了隐私成本的量化，即在任意$\delta$-正确BAI算法满足$\epsilon$-全球DP下的样本复杂度下界。我们的下界表明，隐私预算$\epsilon$的两个隐私模式存在：在高隐私模式（小$\epsilon$）下，难度受到隐私和一种新的信息论量度——总特征时间的共同作用的影响。在低隐私模式（大$\epsilon$）下，样本复杂度下界降低到经典非私有下界。其次，我们提出了一种$\epsilon$-全球DP variant的Top Two算法——AdaP-TT。AdaP-TT在臂 dependent的适应性集中运行，并在每个集中添加拉Place噪声以确保良好的隐私利用平衡。我们 deriv了AdaP-TT的 asymptotic 上界样本复杂度，与下界匹配到多项式常数在高隐私模式。最后，我们对AdaP-TT进行实验分析，并证明了我们的理论结果。
</details></li>
</ul>
<hr>
<h2 id="Sparse-Function-space-Representation-of-Neural-Networks"><a href="#Sparse-Function-space-Representation-of-Neural-Networks" class="headerlink" title="Sparse Function-space Representation of Neural Networks"></a>Sparse Function-space Representation of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02195">http://arxiv.org/abs/2309.02195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin</li>
<li>for: 提高深度神经网络（NN）的不确定性估计和新数据 incorporation 能力</li>
<li>methods: 通过将NN从权重空间转换到函数空间，via dual parameterization，实现了一种简洁和原理正确的不确定性捕捉方法，并可以在整个数据集中捕捉信息</li>
<li>results: 通过proof-of-concept示例，证明了该方法在超参量学任务上可以有效地Quantifying uncertainty和 incorporating new data without retraining<details>
<summary>Abstract</summary>
Deep neural networks (NNs) are known to lack uncertainty estimates and struggle to incorporate new data. We present a method that mitigates these issues by converting NNs from weight space to function space, via a dual parameterization. Importantly, the dual parameterization enables us to formulate a sparse representation that captures information from the entire data set. This offers a compact and principled way of capturing uncertainty and enables us to incorporate new data without retraining whilst retaining predictive performance. We provide proof-of-concept demonstrations with the proposed approach for quantifying uncertainty in supervised learning on UCI benchmark tasks.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:深度神经网络（NN）通常缺乏不确定性估计和新数据integrate的能力。我们提出了一种方法，通过将NN从权重空间转换到函数空间，使用双参数化。这种方法可以提供一种紧凑而原理的方式来捕捉不确定性信息，并且可以在不需要 RETRAINING 的情况下，将新数据集入库。我们在 UCI benchmark 任务上提供了证明示范，以证明我们的方法可以在超出学习中量化不确定性。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing"><a href="#Personalized-Federated-Deep-Reinforcement-Learning-based-Trajectory-Optimization-for-Multi-UAV-Assisted-Edge-Computing" class="headerlink" title="Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing"></a>Personalized Federated Deep Reinforcement Learning-based Trajectory Optimization for Multi-UAV Assisted Edge Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02193">http://arxiv.org/abs/2309.02193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengrong Song, Chuan Ma, Ming Ding, Howard H. Yang, Yuwen Qian, Xiangwei Zhou</li>
<li>for: 提高多架空器飞行轨迹优化的通信系统吞吐量</li>
<li>methods: 使用个性化联合深度学习（PF-DRL）方法，为每个代理模型开发个性化模型，以Address数据稀缺和不均匀性问题</li>
<li>results: 在模拟环境中，提议的算法比其他DRL基本方法具有更好的训练性能和更快的 converges速率，并提高服务质量<details>
<summary>Abstract</summary>
In the era of 5G mobile communication, there has been a significant surge in research focused on unmanned aerial vehicles (UAVs) and mobile edge computing technology. UAVs can serve as intelligent servers in edge computing environments, optimizing their flight trajectories to maximize communication system throughput. Deep reinforcement learning (DRL)-based trajectory optimization algorithms may suffer from poor training performance due to intricate terrain features and inadequate training data. To overcome this limitation, some studies have proposed leveraging federated learning (FL) to mitigate the data isolation problem and expedite convergence. Nevertheless, the efficacy of global FL models can be negatively impacted by the high heterogeneity of local data, which could potentially impede the training process and even compromise the performance of local agents. This work proposes a novel solution to address these challenges, namely personalized federated deep reinforcement learning (PF-DRL), for multi-UAV trajectory optimization. PF-DRL aims to develop individualized models for each agent to address the data scarcity issue and mitigate the negative impact of data heterogeneity. Simulation results demonstrate that the proposed algorithm achieves superior training performance with faster convergence rates, and improves service quality compared to other DRL-based approaches.
</details>
<details>
<summary>摘要</summary>
在5G移动通信时代，有一场很大的研究集中在无人飞行器（UAV）和边缘计算技术上。UAV可以在边缘计算环境中服务为智能服务器，最大化通信系统吞吐量。深度违离学（DRL）基于的轨迹优化算法可能因地形特征复杂和训练数据不充分而表现不佳。为了解决这些限制，一些研究已经提议利用联邦学习（FL）来减少数据隔离问题，加速融合。然而，全球FL模型的效果可能受到本地数据的高自similarity的影响，这可能会阻碍训练过程并可能下降本地代理的性能。这项工作提出了一种解决这些挑战的新解决方案，即个性化联邦深度违离学（PF-DRL），用于多个UAV的轨迹优化。PF-DRL的目标是为每个代理开发特定的模型，以解决数据缺乏问题，并减少数据不同性的负面影响。在模拟结果中，提出的算法可以在训练性能和速度上达到更好的表现，并提高服务质量相比其他DRL基于的方法。
</details></li>
</ul>
<hr>
<h2 id="Bias-Propagation-in-Federated-Learning"><a href="#Bias-Propagation-in-Federated-Learning" class="headerlink" title="Bias Propagation in Federated Learning"></a>Bias Propagation in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02160">http://arxiv.org/abs/2309.02160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/privacytrustlab/bias_in_FL">https://github.com/privacytrustlab/bias_in_FL</a></li>
<li>paper_authors: Hongyan Chang, Reza Shokri</li>
<li>for: 这个论文旨在探讨联邦学习中的群体公平问题，具体来说是研究在分布式数据集上如何避免偏见的扩散。</li>
<li>methods: 这个论文使用了联邦学习的实际应用场景，对实际分布式数据集进行分析和解释，探讨偏见如何在联邦学习中传播。</li>
<li>results: 研究发现，在联邦学习中，偏见可以通过网络传播给所有参与方，而且这种偏见的程度高于中央训练模型使用所有数据集的情况。这些结果告诉我们，在联邦学习中需要进行审核和设计具有群体公平性的学习算法。<details>
<summary>Abstract</summary>
We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.
</details>
<details>
<summary>摘要</summary>
我们显示了参与联邦学习可能会对群体公平性造成不良影响。事实上，一些党对受抑表示的群体（通过敏感特征如性别或种族）的偏见可以在网络中传播到所有党。我们分析并解释了联邦学习中偏见传播的现象。我们的分析表明，偏见党在训练过程中隐藏式地将偏见编码到少量的模型参数中，并在训练中不断增加受抑表示的参考。值得注意的是，在联邦学习中体验到的偏见高于中央训练一个模型使用所有数据的情况下所体验到的偏见。这表明，偏见是由算法所导致的。我们的工作呼吁了审核联邦学习中的群体公平性，并设计不受偏见传播的学习算法。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again"><a href="#A-Simple-Asymmetric-Momentum-Make-SGD-Greatest-Again" class="headerlink" title="A Simple Asymmetric Momentum Make SGD Greatest Again"></a>A Simple Asymmetric Momentum Make SGD Greatest Again</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02130">http://arxiv.org/abs/2309.02130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gongyue Zhang, Dinghuang Zhang, Shuwen Zhao, Donghan Liu, Carrie M. Toptan, Honghai Liu</li>
<li>for: 本研究旨在解决深度学习中的枢轴点问题，提出了一种新的损控极大值梯度法（LCAM），不同于传统的梯度下降法，LCAM在计算成本上没有增加，却能够超越现有的优化器。</li>
<li>methods: 本研究使用了质量 conjugation 和拖动效应来解释 LCAM 的现象，并设计了一系列实验来快速降低学习率在特定的积分 epoch 来更好地吸引参数陷入枢轴点。</li>
<li>results: 在 WRN28-10 测试网络上，使用 LCAM 在 Cifar100 测试集上达到了平均测试精度的峰值 around 120 epoch，比原始 WRN 纸上的80.75% 高，而且使用的 convergence time 只有原始 WRN 的一半。<details>
<summary>Abstract</summary>
We propose the simplest SGD enhanced method ever, Loss-Controlled Asymmetric Momentum(LCAM), aimed directly at the Saddle Point problem. Compared to the traditional SGD with Momentum, there's no increase in computational demand, yet it outperforms all current optimizers. We use the concepts of weight conjugation and traction effect to explain this phenomenon. We designed experiments to rapidly reduce the learning rate at specified epochs to trap parameters more easily at saddle points. We selected WRN28-10 as the test network and chose cifar10 and cifar100 as test datasets, an identical group to the original paper of WRN and Cosine Annealing Scheduling(CAS). We compared the ability to bypass saddle points of Asymmetric Momentum with different priorities. Finally, using WRN28-10 on Cifar100, we achieved a peak average test accuracy of 80.78\% around 120 epoch. For comparison, the original WRN paper reported 80.75\%, while CAS was at 80.42\%, all at 200 epoch. This means that while potentially increasing accuracy, we use nearly half convergence time. Our demonstration code is available at\\ https://github.com/hakumaicc/Asymmetric-Momentum-LCAM
</details>
<details>
<summary>摘要</summary>
我们提出了最简单的SGD加强方法之一，损控量子动量（LCAM），直接解决顶点问题。与传统的SGD加强方法相比，我们没有增加计算需求，但它在当前优化器中表现出色。我们使用了权重 conjugation 和拖动效应来解释这种现象。我们设计了实验，以快速降低学习率在 specify 的epoch中，以更容易将参数固定在顶点上。我们选择了 WRN28-10 作为测试网络，并选择了 cifar10 和 cifar100 作为测试集，与原始 WRN 和 Cosine Annealing Scheduling（CAS）的测试集一样。我们比较了不同优先级的偏置量子动量的缺过点途径能力。最后，使用 WRN28-10 在 Cifar100 上达到了约 120 epoch 的峰值平均测试准确率 around 80.78%。相比之下，原始 WRN 文章reported 80.75%，而 CAS 则是 80.42%，都在 200 epoch 上。这意味着，虽然可能提高准确率，但我们使用的是 nearly half 的 converge 时间。我们的示例代码可以在 https://github.com/hakumaicc/Asymmetric-Momentum-LCAM 上找到。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning"><a href="#Exploiting-Spatial-temporal-Data-for-Sleep-Stage-Classification-via-Hypergraph-Learning" class="headerlink" title="Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning"></a>Exploiting Spatial-temporal Data for Sleep Stage Classification via Hypergraph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02124">http://arxiv.org/abs/2309.02124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuze Liu, Ziming Zhao, Tiehua Zhang, Kang Wang, Xin Chen, Xiaowei Huang, Jun Yin, Zhishu Shen</li>
<li>for: 静脉分类是诊断疾病的关键，现有模型主要使用卷积神经网络（CNN）模型几何数据，以及图 convolutional neural networks（GNN）模型非几何数据，但是这些模型无法同时考虑多modal数据的异质性和交互性，以及空间-时间相关性，因此它们的分类性能有限。</li>
<li>methods: 我们提出了一种动态学习框架STHL，该框架引入了Hipergraph来编码空间-时间数据 для静脉分类。Hipergraph可以构造多Modal&#x2F;多类型的数据，而不是使用简单的对两个主体之间的对应。STHL创建空间和时间的Hiperedge分别来建立节点相关性，然后它进行类型特定的Hipergraph学习过程来编码特征到嵌入空间。</li>
<li>results: 我们的提出的STHL在静脉分类任务中超过了当前最佳模型的性能。<details>
<summary>Abstract</summary>
Sleep stage classification is crucial for detecting patients' health conditions. Existing models, which mainly use Convolutional Neural Networks (CNN) for modelling Euclidean data and Graph Convolution Networks (GNN) for modelling non-Euclidean data, are unable to consider the heterogeneity and interactivity of multimodal data as well as the spatial-temporal correlation simultaneously, which hinders a further improvement of classification performance. In this paper, we propose a dynamic learning framework STHL, which introduces hypergraph to encode spatial-temporal data for sleep stage classification. Hypergraphs can construct multi-modal/multi-type data instead of using simple pairwise between two subjects. STHL creates spatial and temporal hyperedges separately to build node correlations, then it conducts type-specific hypergraph learning process to encode the attributes into the embedding space. Extensive experiments show that our proposed STHL outperforms the state-of-the-art models in sleep stage classification tasks.
</details>
<details>
<summary>摘要</summary>
📝 sleep stage classification 是诊断病人健康状况的关键。现有的模型主要使用卷积神经网络（CNN）来模型几何数据，以及图 convolutional neural networks（GNN）来模型非几何数据，但是这些模型无法同时考虑多模态数据的异质性和互动性以及空间-时间相关性，这会限制分类性能的进一步提高。在本文中，我们提出了一个动态学习框架 STHL，该框架利用卷积图来编码空间-时间数据 для睡眠阶段分类。卷积图可以构建多Modal/多类型的数据，而不是使用简单的对两个主题之间的对应关系。STHL 首先在空间和时间上分别创建特性 Edge，然后进行类型特定的卷积图学习过程来编码特征到嵌入空间中。广泛的实验表明，我们提出的 STHL 在睡眠阶段分类任务中表现出了优于现有的模型。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders"><a href="#An-Efficient-Approach-to-Unsupervised-Out-of-Distribution-Detection-with-Variational-Autoencoders" class="headerlink" title="An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders"></a>An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02084">http://arxiv.org/abs/2309.02084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjlab-ammi/vae4ood">https://github.com/zjlab-ammi/vae4ood</a></li>
<li>paper_authors: Zezhen Zeng, Bin Liu</li>
<li>for: 这 paper 关注深度生成模型 (DGM) 的无监督 out-of-distribution (OOD) 检测。特别是我们关注 vanilla Variational Autoencoders (VAE)，使用标准正态分布 для隐藏变量。这些模型具有更小的模型大小，使得它们在资源有限的应用程序中更适合使用，比较复杂的 DGM。</li>
<li>methods: 我们提出了一种新的 OOD 分数，called Error Reduction (ER)，专门为 vanilla VAE 设计。ER 具有重建输入图像的损失 counterpart 的想法，并考虑图像的 Kolmogorov 复杂性。我们在多个数据集上进行了实验，比较基准方法。</li>
<li>results: 我们的实验结果表明，我们的方法在多个数据集上具有显著优势，比较基准方法。我们的代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/ZJLAB-AMMI/VAE4OOD%E3%80%82">https://github.com/ZJLAB-AMMI/VAE4OOD。</a><details>
<summary>Abstract</summary>
This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "DGMs" is translated as "深度生成模型" (shēn dào shēng chuàng mó delè)* "VAE" is translated as "自变量 autoencoder" (zì biàn xiàng autoencoder)* "OOD" is translated as "外围数据" (wài yù shù jí)* "ER" is translated as "错误减少" (cuò wù jiǎn shang)* "Kolmogorov complexity" is translated as "科玛戈罗夫复杂度" (kē mǎ gē luó fù zhòng dù)
</details></li>
</ul>
<hr>
<h2 id="BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification"><a href="#BeeTLe-A-Framework-for-Linear-B-Cell-Epitope-Prediction-and-Classification" class="headerlink" title="BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification"></a>BeeTLe: A Framework for Linear B-Cell Epitope Prediction and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02071">http://arxiv.org/abs/2309.02071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanx749/bcell">https://github.com/yuanx749/bcell</a></li>
<li>paper_authors: Xiao Yuan</li>
<li>for: 这个论文的目的是提出一种新的深度学习基于多任务框架，用于线性B细胞抗体复合体预测和抗体类型特定的复合体分类。</li>
<li>methods: 该论文提出了一种基于序列 neural network 模型，使用回归层和 Transformer 块来实现。此外，还提出了一种基于对角化的胺基encoding方法，以帮助模型学习复合体的表示。</li>
<li>results: 实验结果表明，提出的方法有效地预测B细胞抗体复合体，并且与竞争方法相比，表现出色。<details>
<summary>Abstract</summary>
The process of identifying and characterizing B-cell epitopes, which are the portions of antigens recognized by antibodies, is important for our understanding of the immune system, and for many applications including vaccine development, therapeutics, and diagnostics. Computational epitope prediction is challenging yet rewarding as it significantly reduces the time and cost of laboratory work. Most of the existing tools do not have satisfactory performance and only discriminate epitopes from non-epitopes. This paper presents a new deep learning-based multi-task framework for linear B-cell epitope prediction as well as antibody type-specific epitope classification. Specifically, a sequenced-based neural network model using recurrent layers and Transformer blocks is developed. We propose an amino acid encoding method based on eigen decomposition to help the model learn the representations of epitopes. We introduce modifications to standard cross-entropy loss functions by extending a logit adjustment technique to cope with the class imbalance. Experimental results on data curated from the largest public epitope database demonstrate the validity of the proposed methods and the superior performance compared to competing ones.
</details>
<details>
<summary>摘要</summary>
“识别和描述B细胞结构的过程是免疫系统理解的重要部分，并有许多应用，包括疫苗开发、治疗和诊断。计算epitope预测是具有挑战性的，但是可以大幅降低实验室工作的时间和成本。现有的工具大多数无法达到满意的性能，只能区分epitope和非epitope。本文提出了一个新的深度学习多任务框架，用于直线B细胞epitope预测和抗体类型特定epitope分类。具体来说，我们使用序列化的神经网络模型，包括回传层和Transformer层。我们提出了一个使用对角解析法来编码氨基酸的方法，帮助模型学习epitope的表现。我们也提出了对标准十字熵损失函数进行修改，以应对分布不对称。实验结果显示，提出的方法有效性和与竞争方法相比较高的性能。”
</details></li>
</ul>
<hr>
<h2 id="Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI"><a href="#Efficiency-is-Not-Enough-A-Critical-Perspective-of-Environmentally-Sustainable-AI" class="headerlink" title="Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI"></a>Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02065">http://arxiv.org/abs/2309.02065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Wright, Christian Igel, Gabrielle Samuel, Raghavendra Selvan<br>for: 本文旨在探讨机器学习（ML）技术的环境可持续性问题，并 argue against solely focusing on efficiency as the solution.methods: 本文使用高级数学（DL）和其他ML方法，以及系统思维来探讨ML技术对环境的影响。results: 本文认为，提高ML系统的效率并不能够完全解决其对环境的影响，而需要考虑多个变量的交互作用。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning (DL) which have accelerated progress on many tasks thought to be out of reach of AI. These ML methods can often be compute hungry, energy intensive, and result in significant carbon emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts including and beyond carbon emissions. The solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the efficiency with which ML systems operate in terms of both compute and energy consumption. In this perspective, we argue that efficiency alone is not enough to make ML as a technology environmentally sustainable. We do so by presenting three high level discrepancies between the effect of efficiency on the environmental sustainability of ML when considering the many variables which it interacts with. In doing so, we comprehensively demonstrate, at multiple levels of granularity both technical and non-technical reasons, why efficiency is not enough to fully remedy the environmental impacts of ML. Based on this, we present and argue for systems thinking as a viable path towards improving the environmental sustainability of ML holistically.
</details>
<details>
<summary>摘要</summary>
然而，我们认为效率alone是不足以使ML成为可持续的技术。我们这样做的原因在于，当ML系统与多个变数互动时，增加效率对环境可持续性的影响是复杂的。为了解释这个观点，我们在这篇文章中提出了三个高度不一致的问题，它们是：1. 碳排放和能源消耗之间的复杂关系。2. ML系统的可持续性受到多个因素的影响，包括硬件、软件、供应链和使用者。3. 增加效率可能会导致新的环境和社会影响，例如对于资源的掌控和可持续性。这些问题表明，增加ML系统的效率 alone 不能全面解决环境可持续性的问题。因此，我们提出了以系统思维为基础的可持续性解决方案，以确保ML技术在环境和社会方面的影响是可持续的。
</details></li>
</ul>
<hr>
<h2 id="MvFS-Multi-view-Feature-Selection-for-Recommender-System"><a href="#MvFS-Multi-view-Feature-Selection-for-Recommender-System" class="headerlink" title="MvFS: Multi-view Feature Selection for Recommender System"></a>MvFS: Multi-view Feature Selection for Recommender System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02064">http://arxiv.org/abs/2309.02064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngjune Lee, Yeongjong Jeong, Keunchan Park, SeongKu Kang</li>
<li>for: 提高 recommender systems 中 feature selection 的性能，适应不同数据场景。</li>
<li>methods: 使用多视图网络和独立的重要性分数模型，避免特征选择过程受主导特征的偏袋问题，从而更有效地选择有用的特征。</li>
<li>results: 对实际数据进行实验，证明 MvFS 比state-of-the-art基elines更有效。<details>
<summary>Abstract</summary>
Feature selection, which is a technique to select key features in recommender systems, has received increasing research attention. Recently, Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations in that its selection process could be easily biased to major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective importance score modeling strategy which is applied independently to each field without incurring dependency among features. Experimental results on real-world datasets demonstrate the effectiveness of MvFS compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
Feature selection, which is a technique used in recommender systems to select key features, has recently received increasing research attention. Adaptive Feature Selection (AdaFS) has shown remarkable performance by adaptively selecting features for each data instance, considering that the importance of a given feature field can vary significantly across data. However, this method still has limitations, as its selection process can be easily biased towards major features that frequently occur. To address these problems, we propose Multi-view Feature Selection (MvFS), which selects informative features for each instance more effectively. Most importantly, MvFS employs a multi-view network consisting of multiple sub-networks, each of which learns to measure the feature importance of a part of data with different feature patterns. By doing so, MvFS mitigates the bias problem towards dominant patterns and promotes a more balanced feature selection process. Moreover, MvFS adopts an effective importance score modeling strategy which is applied independently to each field without incurring dependency among features. Experimental results on real-world datasets demonstrate the effectiveness of MvFS compared to state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="No-Regret-Caching-with-Noisy-Request-Estimates"><a href="#No-Regret-Caching-with-Noisy-Request-Estimates" class="headerlink" title="No-Regret Caching with Noisy Request Estimates"></a>No-Regret Caching with Noisy Request Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02055">http://arxiv.org/abs/2309.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Younes Ben Mazziane, Francescomaria Faticanti, Giovanni Neglia, Sara Alouf</li>
<li>for: 这个论文目的是设计缓存策略，以满足在高负荷和&#x2F;或内存约束的情况下的缓存需求。</li>
<li>methods: 这个论文使用了在线学习算法，以实现缓存策略的设计，并且对缓存请求序列进行了预测。</li>
<li>results: 该论文提出了一种名为“听雷雨 Follow the Perturbed Leader”（NFPL）算法，该算法可以在缓存请求序列中受到噪声影响时，实现低 regret 的缓存策略。此外，该论文还进行了对 классические缓存策略的比较，并在实验中验证了提议的方法的可行性。<details>
<summary>Abstract</summary>
Online learning algorithms have been successfully used to design caching policies with regret guarantees. Existing algorithms assume that the cache knows the exact request sequence, but this may not be feasible in high load and/or memory-constrained scenarios, where the cache may have access only to sampled requests or to approximate requests' counters. In this paper, we propose the Noisy-Follow-the-Perturbed-Leader (NFPL) algorithm, a variant of the classic Follow-the-Perturbed-Leader (FPL) when request estimates are noisy, and we show that the proposed solution has sublinear regret under specific conditions on the requests estimator. The experimental evaluation compares the proposed solution against classic caching policies and validates the proposed approach under both synthetic and real request traces.
</details>
<details>
<summary>摘要</summary>
在线学习算法已经成功地用于设计缓存策略，并且提供了 regret 保证。现有的算法假设缓存知道正确的请求序列，但在高负荷和/或内存压力高的enario中，这可能并不是可行的，因为缓存可能只有对请求数据进行采样或估计。在这篇论文中，我们提出了听从噪声扰动领导者（NFPL）算法，这是经典的跟踪扰动领导者（FPL）算法的变种，当请求估计具有噪声时。我们证明了我们的解决方案在特定的请求估计条件下具有下降式 regret。实验评估比较了我们的解决方案与经典缓存策略，并在 synthetic 和实际请求轨迹上验证了我们的方法。
</details></li>
</ul>
<hr>
<h2 id="Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning"><a href="#Model-agnostic-network-inference-enhancement-from-noisy-measurements-via-curriculum-learning" class="headerlink" title="Model-agnostic network inference enhancement from noisy measurements via curriculum learning"></a>Model-agnostic network inference enhancement from noisy measurements via curriculum learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02050">http://arxiv.org/abs/2309.02050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoyuans/manie">https://github.com/xiaoyuans/manie</a></li>
<li>paper_authors: Kai Wu, Yuanyuan Li, Jing Liu</li>
<li>for: 提高网络推理模型对噪声的抵抗性能</li>
<li>methods: curriculum learning + 模型自适应阈值调整 + 数据 augmentation</li>
<li>results: 在多种噪声环境下，提高了各种网络推理模型的性能，特别是在清晰样本充沥的情况下表现出色<details>
<summary>Abstract</summary>
Noise is a pervasive element within real-world measurement data, significantly undermining the performance of network inference models. However, the quest for a comprehensive enhancement framework capable of bolstering noise resistance across a diverse array of network inference models has remained elusive. Here, we present an elegant and efficient framework tailored to amplify the capabilities of network inference models in the presence of noise. Leveraging curriculum learning, we mitigate the deleterious impact of noisy samples on network inference models. Our proposed framework is model-agnostic, seamlessly integrable into a plethora of model-based and model-free network inference methods. Notably, we utilize one model-based and three model-free network inference methods as the foundation. Extensive experimentation across various synthetic and real-world networks, encapsulating diverse nonlinear dynamic processes, showcases substantial performance augmentation under varied noise types, particularly thriving in scenarios enriched with clean samples. This framework's adeptness in fortifying both model-free and model-based network inference methodologies paves the avenue towards a comprehensive and unified enhancement framework, encompassing the entire spectrum of network inference models. Available Code: https://github.com/xiaoyuans/MANIE.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>噪声是现实世界测量数据中的一种普遍存在的元素，对网络推理模型的性能产生了重要的影响。然而，找到一个全面提高抗噪抗性的框架，能够在多种网络推理模型上提高性能，一直未能实现。在这里，我们提出了一个简洁而高效的框架，用于增强网络推理模型在噪声存在下的性能。我们利用课程学习，以mitigate噪声样本对网络推理模型的负面影响。我们提posed的框架是模型无关的，可以轻松地整合到多种模型基于和模型自由的网络推理方法中。特别是，我们使用了一个模型基于的和三个模型自由的网络推理方法作为基础。经过对多种人工和实际网络、包括多种非线性动力学过程的广泛实验，表明我们的框架在不同的噪声类型下具有显著的性能提高，特别是在充满清晰样本的场景下表现出色。这种框架的能力在加强模型自由和模型基于的网络推理方法方面表现出了广泛的可用性，为建立一个涵盖整个网络推理模型谱系的全面和统一的提高框架提供了道路。可以在 GitHub 上获取代码：https://github.com/xiaoyuans/MANIE。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization"><a href="#Probabilistic-Self-supervised-Learning-via-Scoring-Rules-Minimization" class="headerlink" title="Probabilistic Self-supervised Learning via Scoring Rules Minimization"></a>Probabilistic Self-supervised Learning via Scoring Rules Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02048">http://arxiv.org/abs/2309.02048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Vahidi, Simon Schoßer, Lisa Wimmer, Yawei Li, Bernd Bischl, Eyke Hüllermeier, Mina Rezaei</li>
<li>For: 提高表示质量和避免归一化表示* Methods: 使用 probabilistic models 和知识传播来增强表示质量，并提出一种新的损失函数基于适当的分数规则* Results: 在多种下游任务上达到了superior的准确率和准确性，比自我超vised基线在广泛的实验中表现出色，demonstrating scalability and real-world applicability.<details>
<summary>Abstract</summary>
In this paper, we propose a novel probabilistic self-supervised learning via Scoring Rule Minimization (ProSMIN), which leverages the power of probabilistic models to enhance representation quality and mitigate collapsing representations. Our proposed approach involves two neural networks; the online network and the target network, which collaborate and learn the diverse distribution of representations from each other through knowledge distillation. By presenting the input samples in two augmented formats, the online network is trained to predict the target network representation of the same sample under a different augmented view. The two networks are trained via our new loss function based on proper scoring rules. We provide a theoretical justification for ProSMIN's convergence, demonstrating the strict propriety of its modified scoring rule. This insight validates the method's optimization process and contributes to its robustness and effectiveness in improving representation quality. We evaluate our probabilistic model on various downstream tasks, such as in-distribution generalization, out-of-distribution detection, dataset corruption, low-shot learning, and transfer learning. Our method achieves superior accuracy and calibration, surpassing the self-supervised baseline in a wide range of experiments on large-scale datasets like ImageNet-O and ImageNet-C, ProSMIN demonstrates its scalability and real-world applicability.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的概率自编学习方法，即 Scoring Rule Minimization（ProSMIN），它利用概率模型来提高表示质量和消除塑性表示。我们的提议方法包括两个神经网络：在线网络和目标网络，它们相互合作，通过知识传递学习来学习各自的多样化表示分布。我们将输入样本提供两种扩展视图，使在线网络通过预测目标网络对同一个样本的不同扩展视图的表示来训练。我们使用我们新提出的损失函数，基于正确的探索规则。我们提供了对ProSMIN的优化过程的理论正确性的证明，这种视角证明了其优化过程的正确性和效果性，从而提高了表示质量。我们在多种下游任务上评估了我们的概率模型，包括内部分布式、外部分布式、数据损害、低学习率和转移学习等。我们的方法在各种实验中具有优于自编学习基eline的高精度和抗混淆性。
</details></li>
</ul>
<hr>
<h2 id="Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models"><a href="#Data-Juicer-A-One-Stop-Data-Processing-System-for-Large-Language-Models" class="headerlink" title="Data-Juicer: A One-Stop Data Processing System for Large Language Models"></a>Data-Juicer: A One-Stop Data Processing System for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02033">http://arxiv.org/abs/2309.02033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba/data-juicer">https://github.com/alibaba/data-juicer</a></li>
<li>paper_authors: Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, Jingren Zhou</li>
<li>for:  LLama 大语模型数据处理</li>
<li>methods:  Data-Juicer 提供50多个内置Operator和可插入工具，以提高模块化、可组合、可扩展性，用于多种 LLama 数据处理需求。</li>
<li>results:  Empirical validation reveals up to 7.45% relative improvement in LLaMA performance, and up to 88.7% reduction in single-machine processing time.<details>
<summary>Abstract</summary>
The immense evolution in Large Language Models (LLMs) has underscored the importance of massive, diverse, and high-quality data. Despite this, existing open-source tools for LLM data processing remain limited and mostly tailored to specific datasets, with an emphasis on the reproducibility of released data over adaptability and usability, inhibiting potential applications. In response, we propose a one-stop, powerful yet flexible and user-friendly LLM data processing system named Data-Juicer. Our system offers over 50 built-in versatile operators and pluggable tools, which synergize modularity, composability, and extensibility dedicated to diverse LLM data processing needs. By incorporating visualized and automatic evaluation capabilities, Data-Juicer enables a timely feedback loop to accelerate data processing and gain data insights. To enhance usability, Data-Juicer provides out-of-the-box components for users with various backgrounds, and fruitful data recipes for LLM pre-training and post-tuning usages. Further, we employ multi-facet system optimization and seamlessly integrate Data-Juicer with both LLM and distributed computing ecosystems, to enable efficient and scalable data processing. Empirical validation of the generated data recipes reveals considerable improvements in LLaMA performance for various pre-training and post-tuning cases, demonstrating up to 7.45% relative improvement of averaged score across 16 LLM benchmarks and 16.25% higher win rate using pair-wise GPT-4 evaluation. The system's efficiency and scalability are also validated, supported by up to 88.7% reduction in single-machine processing time, 77.1% and 73.1% less memory and CPU usage respectively, and 7.91x processing acceleration when utilizing distributed computing ecosystems. Our system, data recipes, and multiple tutorial demos are released, calling for broader research centered on LLM data.
</details>
<details>
<summary>摘要</summary>
“巨大的语言模型（LLM）演化带来了大量、多样化和高质量数据的重要性。然而，现有的开源工具 для LLM 数据处理仍然有限，主要是为特定数据集设计的，强调数据重现性而不是适应性和用户友好性，这限制了其应用前景。为此，我们提出了一个一站式、强大 yet 灵活和用户友好的 LLM 数据处理系统，名为 Data-Juicer。我们的系统提供了50多种快速组合和可插入工具，以提高模块性、可 compose 性和扩展性，以满足不同 LLM 数据处理需求。通过包含可视化和自动评估功能，Data-Juicer 可以提供时效的反馈循环，加速数据处理并获得数据视图。为了提高可用性，Data-Juicer 提供了适合不同背景的准备组件，以及 LLMA 预训练和后处理用例的有用数据荚。此外，我们采用多方面优化和与 LLM 和分布式计算环境集成，以实现高效和可扩展的数据处理。我们的实验 validate 了生成的数据荚，显示 LLMA 性能提高明显，在16个 LLMBenchmark 和16个 GPT-4 评估中，相对提高7.45%的平均分数，并在对比评估中提高16.25%的胜率。系统的效率和扩展性也得到了 validate，包括单机处理时间减少88.7%、内存和CPU使用量减少77.1%和73.1%，以及使用分布式计算环境时的处理加速7.91倍。我们的系统、数据荚和多个教程示例都已经发布，呼吁更广泛的 LLM 数据研究。”
</details></li>
</ul>
<hr>
<h2 id="Non-Parametric-Representation-Learning-with-Kernels"><a href="#Non-Parametric-Representation-Learning-with-Kernels" class="headerlink" title="Non-Parametric Representation Learning with Kernels"></a>Non-Parametric Representation Learning with Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02028">http://arxiv.org/abs/2309.02028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning">https://github.com/himanshub1007/Alzhimers-Disease-Prediction-Using-Deep-learning</a></li>
<li>paper_authors: Pascal Esser, Maximilian Fleissner, Debarghya Ghoshdastidar</li>
<li>for: 学习无监督的特征表示，从无标签数据中学习有用的特征。</li>
<li>methods: 使用kernel-based方法进行表示学习，包括对冲损函数和自适应 Encoder（AE）模型。</li>
<li>results: 提出了新的表示理论，并 derive了泛化误差上限，进行表示学习的评估。<details>
<summary>Abstract</summary>
Unsupervised and self-supervised representation learning has become popular in recent years for learning useful features from unlabelled data. Representation learning has been mostly developed in the neural network literature, and other models for representation learning are surprisingly unexplored. In this work, we introduce and analyze several kernel-based representation learning approaches: Firstly, we define two kernel Self-Supervised Learning (SSL) models using contrastive loss functions and secondly, a Kernel Autoencoder (AE) model based on the idea of embedding and reconstructing data. We argue that the classical representer theorems for supervised kernel machines are not always applicable for (self-supervised) representation learning, and present new representer theorems, which show that the representations learned by our kernel models can be expressed in terms of kernel matrices. We further derive generalisation error bounds for representation learning with kernel SSL and AE, and empirically evaluate the performance of these methods in both small data regimes as well as in comparison with neural network based models.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese无监督和自监督表示学习在最近几年内变得非常流行，以学习无标记数据中的有用特征。表示学习主要发展在神经网络文献中，而其他模型表示学习却尚未得到探索。在这项工作中，我们引入并分析了几种基于核函数的表示学习方法：首先，我们定义了两种抽象损失函数基于自我监督学习（SSL）模型，其次，基于数据嵌入和重建的核自动编码（AE）模型。我们认为经典的supervised机器学习的表示定理不一定适用于（自监督）表示学习，并提出了新的表示定理，其中表示学习得到的表示可以表示为核矩阵。我们进一步 deriv Generalization Error bounds for representation learning with kernel SSL和AE，并对这些方法在小数据 régime和与神经网络模型相比进行实验评估。Note: "Simplified Chinese" is a romanization of Chinese characters, which is used to represent the language in the Latin alphabet. It is not a translation of the text into Traditional Chinese, which is a different writing system.
</details></li>
</ul>
<hr>
<h2 id="Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length"><a href="#Granger-Causal-Inference-in-Multivariate-Hawkes-Processes-by-Minimum-Message-Length" class="headerlink" title="Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length"></a>Granger Causal Inference in Multivariate Hawkes Processes by Minimum Message Length</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02027">http://arxiv.org/abs/2309.02027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katerina Hlavackova-Schindler, Anna Melnykova, Irene Tubikanec</li>
<li>for: 这个论文主要针对多变量郝克过程（MHPs）中的连接图生成和选择问题，并提出一种基于最小消息长度（MML）原理的优化 критерион和模型选择算法。</li>
<li>methods: 该论文使用了扩展衰减函数和优化 критерион，并通过比较不同模型对数据的适应度和 concise度来选择最佳模型。</li>
<li>results: 对于短时间适应度较高的场景，该方法可以达到最高的 F1 分数，并在具有特定稀疏图设置下进行了数值研究。 更进一步，通过应用于 G7 财政债券数据，该方法可以获得一致的 causal 连接，与专业知识一致。<details>
<summary>Abstract</summary>
Multivariate Hawkes processes (MHPs) are versatile probabilistic tools used to model various real-life phenomena: earthquakes, operations on stock markets, neuronal activity, virus propagation and many others. In this paper, we focus on MHPs with exponential decay kernels and estimate connectivity graphs, which represent the Granger causal relations between their components. We approach this inference problem by proposing an optimization criterion and model selection algorithm based on the minimum message length (MML) principle. MML compares Granger causal models using the Occam's razor principle in the following way: even when models have a comparable goodness-of-fit to the observed data, the one generating the most concise explanation of the data is preferred. While most of the state-of-art methods using lasso-type penalization tend to overfitting in scenarios with short time horizons, the proposed MML-based method achieves high F1 scores in these settings. We conduct a numerical study comparing the proposed algorithm to other related classical and state-of-art methods, where we achieve the highest F1 scores in specific sparse graph settings. We illustrate the proposed method also on G7 sovereign bond data and obtain causal connections, which are in agreement with the expert knowledge available in the literature.
</details>
<details>
<summary>摘要</summary>
多变量庞克过程（MHP）是一种通用的概率工具，用于模拟各种实际场景：地震、股票市场交易、神经元活动、病毒传播等。在这篇论文中，我们关注MHP中的凝聚函数和抽象树的估计问题。我们使用最小消息长度（MML）原理来解决这个问题，MML比较不同庞克模型的适应度，并选择最简洁的模型。大多数当前的方法使用lasso类型的惩罚往往会过拟合短时间尺度下的场景，而我们提出的MML基于方法在这些设置下达到了最高的F1分数。我们进行了一个数字实验，比较了我们的方法和其他相关的古典和当前状态的方法，我们在特定的稀疏图设置下达到了最高的F1分数。我们还应用了我们的方法在G7国债数据中，并获得了一致的 causal 连接，与文献中的专家知识一致。
</details></li>
</ul>
<hr>
<h2 id="RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning"><a href="#RDGSL-Dynamic-Graph-Representation-Learning-with-Structure-Learning" class="headerlink" title="RDGSL: Dynamic Graph Representation Learning with Structure Learning"></a>RDGSL: Dynamic Graph Representation Learning with Structure Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02025">http://arxiv.org/abs/2309.02025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siwei Zhang, Yun Xiong, Yao Zhang, Yiheng Sun, Xi Chen, Yizhu Jiao, Yangyong Zhu</li>
<li>for: 本研究旨在学习 kontinuous-time 动态图 Representation，以提高下游任务的效果。</li>
<li>methods: 本研究提出了 RDGSL 方法，具有 dynamic graph structure learning 和 Temporal Embedding Learner 两个重要组成部分。dynamic graph structure learning 可以有效地抑制噪声，Temporal Embedding Learner 可以选择ively ignore 噪声边，以提高 Representation 的表达力。</li>
<li>results: 本研究的方法在 downstream 任务中表现出了5.1% 绝对 AUC 提升，与第二个基线相比。<details>
<summary>Abstract</summary>
Temporal Graph Networks (TGNs) have shown remarkable performance in learning representation for continuous-time dynamic graphs. However, real-world dynamic graphs typically contain diverse and intricate noise. Noise can significantly degrade the quality of representation generation, impeding the effectiveness of TGNs in downstream tasks. Though structure learning is widely applied to mitigate noise in static graphs, its adaptation to dynamic graph settings poses two significant challenges. i) Noise dynamics. Existing structure learning methods are ill-equipped to address the temporal aspect of noise, hampering their effectiveness in such dynamic and ever-changing noise patterns. ii) More severe noise. Noise may be introduced along with multiple interactions between two nodes, leading to the re-pollution of these nodes and consequently causing more severe noise compared to static graphs. In this paper, we present RDGSL, a representation learning method in continuous-time dynamic graphs. Meanwhile, we propose dynamic graph structure learning, a novel supervisory signal that empowers RDGSL with the ability to effectively combat noise in dynamic graphs. To address the noise dynamics issue, we introduce the Dynamic Graph Filter, where we innovatively propose a dynamic noise function that dynamically captures both current and historical noise, enabling us to assess the temporal aspect of noise and generate a denoised graph. We further propose the Temporal Embedding Learner to tackle the challenge of more severe noise, which utilizes an attention mechanism to selectively turn a blind eye to noisy edges and hence focus on normal edges, enhancing the expressiveness for representation generation that remains resilient to noise. Our method demonstrates robustness towards downstream tasks, resulting in up to 5.1% absolute AUC improvement in evolving classification versus the second-best baseline.
</details>
<details>
<summary>摘要</summary>
temps 图网络（TGNs）在学习 continuous-time 动态图 Representation 方面表现出色，但实际世界中的动态图通常具有多样化和复杂的噪音。噪音可以对 Representation 生成质量产生重要影响，从而降低 TGNs 在下游任务中的效果。虽然结构学习在静止图中广泛应用，但其在动态图设置中存在两个主要挑战。i) 噪音动态性。现有的结构学习方法无法 Address 动态噪音的问题，因此其效iveness 在这些动态和改变中的噪音模式下受限。ii) 更严重的噪音。噪音可能会在两个节点之间多种交互中引入，导致这些节点重新污染，从而导致更严重的噪音 compared to 静止图。在这篇文章中，我们提出了 RDGSL，一种在 continuous-time 动态图上进行 Representation 学习的方法。同时，我们提出了动态图结构学习，一种新的监督信号，可以让 RDGSL 在动态图上有效地抗抗噪音。为 Address 噪音动态性问题，我们引入了动态噪音函数，可以动态地捕捉当前和历史噪音，使我们可以评估动态图中噪音的 temporal 方面，并生成一个 Denoised 图。此外，我们还提出了时间 Embedding Learner，可以在更严重的噪音下提高 Representation 生成的表达能力。我们的方法在下游任务中表现了Robustness，相比第二Best baseline，我们的方法在 evolving 分类中实现了5.1%的绝对 AUC 提升。
</details></li>
</ul>
<hr>
<h2 id="PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates"><a href="#PROMISE-Preconditioned-Stochastic-Optimization-Methods-by-Incorporating-Scalable-Curvature-Estimates" class="headerlink" title="PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates"></a>PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02014">http://arxiv.org/abs/2309.02014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary Frangella, Pratik Rathore, Shipu Zhao, Madeleine Udell</li>
<li>for: 解决大规模的几何优化问题，如机器学习中的ridge和logistic回归问题。</li>
<li>methods: 使用绘制技术来实现预处理的渐进搜索法，包括SVRG、SAGA和Katyusha等方法，每个方法都有强大的理论分析和有效的默认超参数设置。</li>
<li>results: 经验表明，提出的方法可以在 default 超参数设置下超过或与通过手动调整的梯度搜索优化器相比，并且在实际中也能够更快地达到 globally 线性减少。<details>
<summary>Abstract</summary>
This paper introduces PROMISE ($\textbf{Pr}$econditioned Stochastic $\textbf{O}$ptimization $\textbf{M}$ethods by $\textbf{I}$ncorporating $\textbf{S}$calable Curvature $\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine learning repositories. On the theoretical side, this paper introduces the notion of quadratic regularity in order to establish linear convergence of all proposed methods even when the preconditioner is updated infrequently. The speed of linear convergence is determined by the quadratic regularity ratio, which often provides a tighter bound on the convergence rate compared to the condition number, both in theory and in practice, and explains the fast global linear convergence of the proposed methods.
</details>
<details>
<summary>摘要</summary>
Empirically, we demonstrate the superiority of the proposed algorithms by showing that they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems from benchmark machine learning repositories, using default hyperparameter values.Theoretically, this paper introduces the concept of quadratic regularity to establish the linear convergence of all proposed methods, even when the preconditioner is updated infrequently. The speed of linear convergence is determined by the quadratic regularity ratio, which often provides a tighter bound on the convergence rate compared to the condition number, both in theory and in practice. This explains the fast global linear convergence of the proposed methods.
</details></li>
</ul>
<hr>
<h2 id="Representation-Learning-Dynamics-of-Self-Supervised-Models"><a href="#Representation-Learning-Dynamics-of-Self-Supervised-Models" class="headerlink" title="Representation Learning Dynamics of Self-Supervised Models"></a>Representation Learning Dynamics of Self-Supervised Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02011">http://arxiv.org/abs/2309.02011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascal Esser, Satyaki Mukherjee, Debarghya Ghoshdastidar</li>
<li>for: 本研究探讨了无监督学习（Self-Supervised Learning）模型中的学习动力学，具体来说是对减少对比和非对比损失来获得的表示进行研究。</li>
<li>methods: 作者使用了多变量回归模型的动力学来探讨SSL模型的学习动力学，并提出了包含对齐约束的SSL目标函数。</li>
<li>results: 研究发现，使用 gradient descent 在 Grassmannian  manifold 上训练 SSL 模型时，模型会学习简单的标量表示，导致维度减少现象出现。作者还证明了无监督学习模型在无穷宽approximation中与supervised模型之间存在很大差异。<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) is an important paradigm for learning representations from unlabelled data, and SSL with neural networks has been highly successful in practice. However current theoretical analysis of SSL is mostly restricted to generalisation error bounds. In contrast, learning dynamics often provide a precise characterisation of the behaviour of neural networks based models but, so far, are mainly known in supervised settings. In this paper, we study the learning dynamics of SSL models, specifically representations obtained by minimising contrastive and non-contrastive losses. We show that a naive extension of the dymanics of multivariate regression to SSL leads to learning trivial scalar representations that demonstrates dimension collapse in SSL. Consequently, we formulate SSL objectives with orthogonality constraints on the weights, and derive the exact (network width independent) learning dynamics of the SSL models trained using gradient descent on the Grassmannian manifold. We also argue that the infinite width approximation of SSL models significantly deviate from the neural tangent kernel approximations of supervised models. We numerically illustrate the validity of our theoretical findings, and discuss how the presented results provide a framework for further theoretical analysis of contrastive and non-contrastive SSL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning"><a href="#Establishing-a-real-time-traffic-alarm-in-the-city-of-Valencia-with-Deep-Learning" class="headerlink" title="Establishing a real-time traffic alarm in the city of Valencia with Deep Learning"></a>Establishing a real-time traffic alarm in the city of Valencia with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02010">http://arxiv.org/abs/2309.02010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Folgado, Veronica Sanz, Johannes Hirn, Edgar Lorenzo-Saez, Javier Urchueguia</li>
<li>for: 这项研究的目的是分析城市劳伦сия（Valencia，西班牙）的交通征流与污染物的相关性，以及开发一种用于预测下一30分钟内特定街区是否会出现异常高交通流的警报系统。</li>
<li>methods: 该研究使用了2018年的交通数据，通过Long Short-Term Memory（LSTM）神经网络进行预测，并在2019年的交通数据上进行测试。</li>
<li>results: 研究发现，交通征流对某些污染物（特别是$\text{NO}_\text{x}$）的水平有显著影响。同时，该研究开发出了一种独立的三级水平警报系统，可以预测特定街区在下一30分钟内是否会出现异常高交通流。<details>
<summary>Abstract</summary>
Urban traffic emissions represent a significant concern due to their detrimental impacts on both public health and the environment. Consequently, decision-makers have flagged their reduction as a crucial goal. In this study, we first analyze the correlation between traffic flux and pollution in the city of Valencia, Spain. Our results demonstrate that traffic has a significant impact on the levels of certain pollutants (especially $\text{NO}_\text{x}$). Secondly, we develop an alarm system to predict if a street is likely to experience unusually high traffic in the next 30 minutes, using an independent three-tier level for each street. To make the predictions, we use traffic data updated every 10 minutes and Long Short-Term Memory (LSTM) neural networks. We trained the LSTM using traffic data from 2018, and tested it using traffic data from 2019.
</details>
<details>
<summary>摘要</summary>
城市交通排放对公共健康和环境产生了重要的影响，因此决策者们将其减少列为重要目标。在这项研究中，我们首先分析了Valencia市的交通流和污染物之间的相关性。我们的结果显示，交通具有对某些污染物（尤其是$\text{NO}_\text{x}$）的显著影响。其次，我们开发了一个预测在下一个30分钟内街道是否会出现异常高交通流的警示系统，并将每条街道分为三级水平。为了进行预测，我们使用了每10分钟更新的交通数据和Long Short-Term Memory（LSTM）神经网络。我们使用2018年的交通数据进行训练，并在2019年的交通数据上进行测试。
</details></li>
</ul>
<hr>
<h2 id="An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability"><a href="#An-LSTM-Based-Predictive-Monitoring-Method-for-Data-with-Time-varying-Variability" class="headerlink" title="An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability"></a>An LSTM-Based Predictive Monitoring Method for Data with Time-varying Variability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01978">http://arxiv.org/abs/2309.01978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Qiu, Yu Lin, Inez Zwetsloot</li>
<li>for: 本文旨在探讨使用循环神经网络（RNN）和其变体来实现预测性监测，以检测数据中的异常现象。</li>
<li>methods: 本文提出了基于长短期记忆（LSTM）预测 интерVAL的控制图，用于监测时间变化的数据。</li>
<li>results:  simulations 和实际应用表明，提出的方法在检测平均值变化时表现出色，并且在实际时系列感知器数据中得到了证明。<details>
<summary>Abstract</summary>
The recurrent neural network and its variants have shown great success in processing sequences in recent years. However, this deep neural network has not aroused much attention in anomaly detection through predictively process monitoring. Furthermore, the traditional statistic models work on assumptions and hypothesis tests, while neural network (NN) models do not need that many assumptions. This flexibility enables NN models to work efficiently on data with time-varying variability, a common inherent aspect of data in practice. This paper explores the ability of the recurrent neural network structure to monitor processes and proposes a control chart based on long short-term memory (LSTM) prediction intervals for data with time-varying variability. The simulation studies provide empirical evidence that the proposed model outperforms other NN-based predictive monitoring methods for mean shift detection. The proposed method is also applied to time series sensor data, which confirms that the proposed method is an effective technique for detecting abnormalities.
</details>
<details>
<summary>摘要</summary>
“Recurrent neural network（RNN）和其变体在过去几年内得到了广泛的成功，但它尚未吸引过多的注意力在预测过程监测中。此外，传统的统计模型基于假设和假设测试，而神经网络（NN）模型则不需要这么多假设。这种灵活性使得NN模型在数据中具有时变变量的效率，这是实际数据中的一个常见特征。本文探讨了RNN结构在监测过程中的能力，并提出了基于长期快短时尺度内预测 интерval的控制图。实验研究表明，提议的模型在mean shift探测方面表现出色，并且在时间序列感知数据中进行了有效的异常检测。”Note: The translation is in Simplified Chinese, which is one of the two standard Chinese dialects. The other dialect is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis"><a href="#AdaPlus-Integrating-Nesterov-Momentum-and-Precise-Stepsize-Adjustment-on-AdamW-Basis" class="headerlink" title="AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis"></a>AdaPlus: Integrating Nesterov Momentum and Precise Stepsize Adjustment on AdamW Basis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01966">http://arxiv.org/abs/2309.01966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Guan</li>
<li>for: 提出了一种高效的优化器 called AdaPlus，它将Nesterov冲击和精确步长调整与AdamW基础结合。</li>
<li>methods: 使用了AdamW、Nadam和AdaBelief的优点，而无需添加额外的超参数。</li>
<li>results: 通过对三个机器学习任务进行广泛的实验评估，证明了AdaPlus在图像分类任务中表现最优，并在语言模型任务和生成器任务中表现出最高的稳定性。<details>
<summary>Abstract</summary>
This paper proposes an efficient optimizer called AdaPlus which integrates Nesterov momentum and precise stepsize adjustment on AdamW basis. AdaPlus combines the advantages of AdamW, Nadam, and AdaBelief and, in particular, does not introduce any extra hyper-parameters. We perform extensive experimental evaluations on three machine learning tasks to validate the effectiveness of AdaPlus. The experiment results validate that AdaPlus (i) is the best adaptive method which performs most comparable with (even slightly better than) SGD with momentum on image classification tasks and (ii) outperforms other state-of-the-art optimizers on language modeling tasks and illustrates the highest stability when training GANs. The experiment code of AdaPlus is available at: https://github.com/guanleics/AdaPlus.
</details>
<details>
<summary>摘要</summary>
这份论文提出了一种高效的优化器called AdaPlus，它将Nesterov势量和精确步长调整 integrate到AdamW基础上。AdaPlus结合了AdamW、Nadam和AdaBelief的优点，并不添加任何额外hyper参数。我们在三个机器学习任务上进行了广泛的实验评估，以验证AdaPlus的效果。实验结果表明，AdaPlus：（1）在图像分类任务上与SGD势量几乎相同，甚至有slightly better的性能。（2）在语言模型任务上超过其他当前顶尖优化器。（3）在训练GAN时显示出最高稳定性。AdaPlus的实验代码可以在https://github.com/guanleics/AdaPlus中找到。
</details></li>
</ul>
<hr>
<h2 id="Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D"><a href="#Developing-A-Fair-Individualized-Polysocial-Risk-Score-iPsRS-for-Identifying-Increased-Social-Risk-of-Hospitalizations-in-Patients-with-Type-2-Diabetes-T2D" class="headerlink" title="Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)"></a>Developing A Fair Individualized Polysocial Risk Score (iPsRS) for Identifying Increased Social Risk of Hospitalizations in Patients with Type 2 Diabetes (T2D)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02467">http://arxiv.org/abs/2309.02467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Huang, Jingchuan Guo, William T Donahoo, Zhengkang Fan, Ying Lu, Wei-Han Chen, Huilin Tang, Lori Bilello, Elizabeth A Shenkman, Jiang Bian<br>for: 这个研究的目的是开发一个基于电子健康记录（EHR）的机器学习（ML）分析管道，以识别患有型二糖尿病（T2D）患者的社会需求不足，并且对这些需求进行解释性AI（XAI）评估和优化。methods: 这个研究使用了大学佐华利 Integrated Data Repository（UFH IR）中的EHR数据，包括社会Determinants of health（SDoH）和个体级SDoH，并开发了一个基于EHR的ML分析管道，称为个体化多社会风险分数（iPsRS），以识别患有T2D的患者中的高社会风险。results: 我们的iPsRS在各个种族-民族群体中进行了公平优化后，C statistic为0.72，可以准确预测患有T2D的患者1年内的入院风险。iPsRS能够准确捕捉高入院风险的个体，实际1年内top 5%的iPsRS的入院率比底层分数段高约13倍。<details>
<summary>Abstract</summary>
Background: Racial and ethnic minority groups and individuals facing social disadvantages, which often stem from their social determinants of health (SDoH), bear a disproportionate burden of type 2 diabetes (T2D) and its complications. It is therefore crucial to implement effective social risk management strategies at the point of care. Objective: To develop an EHR-based machine learning (ML) analytical pipeline to identify the unmet social needs associated with hospitalization risk in patients with T2D. Methods: We identified 10,192 T2D patients from the EHR data (from 2012 to 2022) from the University of Florida Health Integrated Data Repository, including contextual SDoH (e.g., neighborhood deprivation) and individual-level SDoH (e.g., housing stability). We developed an electronic health records (EHR)-based machine learning (ML) analytic pipeline, namely individualized polysocial risk score (iPsRS), to identify high social risk associated with hospitalizations in T2D patients, along with explainable AI (XAI) techniques and fairness assessment and optimization. Results: Our iPsRS achieved a C statistic of 0.72 in predicting 1-year hospitalization after fairness optimization across racial-ethnic groups. The iPsRS showed excellent utility for capturing individuals at high hospitalization risk; the actual 1-year hospitalization rate in the top 5% of iPsRS was ~13 times as high as the bottom decile. Conclusion: Our ML pipeline iPsRS can fairly and accurately screen for patients who have increased social risk leading to hospitalization in T2D patients.
</details>
<details>
<summary>摘要</summary>
背景：种族和民族少数群体和受到社会不利条件影响的个人患有类型2糖尿病（T2D）和其并发症的负担较大。因此，在点患者处实施有效的社会风险管理策略是非常重要。目标：开发基于电子健康纪录（EHR）的机器学习（ML）分析管道，以识别患有T2D患者的社会需求不足，与住院风险相关。方法：我们从2012年至2022年的University of Florida Health Integrated Data Repository中提取了10,192名T2D患者的EHR数据，包括上下文性社会 determinants of health（SDoH）和个体级SDoH（如住房稳定）。我们开发了基于EHR的ML分析管道，称为个体化多社会风险分数（iPsRS），以识别患有T2D患者的高社会风险，同时使用可解释AI（XAI）技术和公平评估和优化。结果：我们的iPsRS在公平优化后，在不同种族-民族群体中的CStatistic为0.72，能够准确预测患有T2D患者1年内的住院风险。iPsRS表现出色地捕捉了高住院风险的个体，实际1年内患有T2D患者在top5%的iPsRS中住院率高达13倍于bottom decile。结论：我们的ML管道iPsRS可以公平、准确地在患有T2D患者中识别具有高社会风险的患者，并且可以通过可解释AI技术和公平评估和优化来提高其效果。
</details></li>
</ul>
<hr>
<h2 id="RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking"><a href="#RoboAgent-Generalization-and-Efficiency-in-Robot-Manipulation-via-Semantic-Augmentations-and-Action-Chunking" class="headerlink" title="RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking"></a>RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01918">http://arxiv.org/abs/2309.01918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Homanga Bharadhwaj, Jay Vakil, Mohit Sharma, Abhinav Gupta, Shubham Tulsiani, Vikash Kumar</li>
<li>for: 这篇论文旨在开发一种能够快速 multiply 现有数据集，并提取高性能策略的 universal agent 训练系统。</li>
<li>methods: 该系统使用 semantic augmentations 和 action representations 来快速训练 universal agent，并使用可靠的任务条件和表达能力架构来实现多种 manipulate 技能。</li>
<li>results: 通过使用仅 7500 示例，该系统可以训练一个可以执行 12 种技能的 universal agent，并在不同的 kitchen 场景中展示其普遍性和多样性。在未seen 情况下，RoboAgent 的性能高于先前方法，并且更加 Sample Efficient。视频详情请参考 <a target="_blank" rel="noopener" href="https://robopen.github.io/">https://robopen.github.io/</a>.<details>
<summary>Abstract</summary>
The grand aim of having a single robot that can manipulate arbitrary objects in diverse settings is at odds with the paucity of robotics datasets. Acquiring and growing such datasets is strenuous due to manual efforts, operational costs, and safety challenges. A path toward such an universal agent would require a structured framework capable of wide generalization but trained within a reasonable data budget. In this paper, we develop an efficient system (RoboAgent) for training universal agents capable of multi-task manipulation skills using (a) semantic augmentations that can rapidly multiply existing datasets and (b) action representations that can extract performant policies with small yet diverse multi-modal datasets without overfitting. In addition, reliable task conditioning and an expressive policy architecture enable our agent to exhibit a diverse repertoire of skills in novel situations specified using language commands. Using merely 7500 demonstrations, we are able to train a single agent capable of 12 unique skills, and demonstrate its generalization over 38 tasks spread across common daily activities in diverse kitchen scenes. On average, RoboAgent outperforms prior methods by over 40% in unseen situations while being more sample efficient and being amenable to capability improvements and extensions through fine-tuning. Videos at https://robopen.github.io/
</details>
<details>
<summary>摘要</summary>
“我们的大目标是建立一个可以 manipulate 任意物品的多功能机器人，但是实际上存在着机器人学习数据的稀缺。 acquiring 和 growing 这些数据需要许多人工干预、操作成本和安全挑战。为了实现这个目标，我们需要一个结构化的框架，可以实现广泛的普遍化，并在有限的数据预算下训练。在这篇论文中，我们开发了一个高效的系统（RoboAgent），可以通过（a）实义增强和（b）动作表示来快速增加现有数据，并将小量多 modal 数据中的精致政策EXTRACT。此外，我们的任务条件和表达政策架构可以让我们的代理人在新的语言指令下展现多元的技能。仅从7500次示例中，我们能够训练一个可以拥有12种技能的单一代理人，并在38个任务中展现其普遍性。在未见的情况下，RoboAgent 比PRIOR METHODS 高出40%的性能，同时更加sample efficient 和可以通过精致化和扩展来提高能力。影片请参考https://robopen.github.io/”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems"><a href="#A-Survey-on-Physics-Informed-Reinforcement-Learning-Review-and-Open-Problems" class="headerlink" title="A Survey on Physics Informed Reinforcement Learning: Review and Open Problems"></a>A Survey on Physics Informed Reinforcement Learning: Review and Open Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01909">http://arxiv.org/abs/2309.01909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chayan Banerjee, Kien Nguyen, Clinton Fookes, Maziar Raissi</li>
<li>for: 这种研究旨在推动physics-informed reinforcement learning（PIRL）的发展，增强机器学习框架中的物理信息 incorporation，以提高physical plausibility和数据效率。</li>
<li>methods: 这篇文章通过对现有的physics-informed reinforcement learning（PIRL）方法进行系统性的综述和分类，批判性地分析了它们的不同特点和适用场景，从而提供了一个权威的taxonomy。</li>
<li>results: 该文章提供了一个全面的视角，把physics-informed reinforcement learning（PIRL）的实现方法分类为不同的类别，并指出了这个领域的应用场景、潜在的挑战和未来研究的方向。<details>
<summary>Abstract</summary>
The inclusion of physical information in machine learning frameworks has revolutionized many application areas. This involves enhancing the learning process by incorporating physical constraints and adhering to physical laws. In this work we explore their utility for reinforcement learning applications. We present a thorough review of the literature on incorporating physics information, as known as physics priors, in reinforcement learning approaches, commonly referred to as physics-informed reinforcement learning (PIRL). We introduce a novel taxonomy with the reinforcement learning pipeline as the backbone to classify existing works, compare and contrast them, and derive crucial insights. Existing works are analyzed with regard to the representation/ form of the governing physics modeled for integration, their specific contribution to the typical reinforcement learning architecture, and their connection to the underlying reinforcement learning pipeline stages. We also identify core learning architectures and physics incorporation biases (i.e., observational, inductive and learning) of existing PIRL approaches and use them to further categorize the works for better understanding and adaptation. By providing a comprehensive perspective on the implementation of the physics-informed capability, the taxonomy presents a cohesive approach to PIRL. It identifies the areas where this approach has been applied, as well as the gaps and opportunities that exist. Additionally, the taxonomy sheds light on unresolved issues and challenges, which can guide future research. This nascent field holds great potential for enhancing reinforcement learning algorithms by increasing their physical plausibility, precision, data efficiency, and applicability in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
机器学习框架中包含物理信息的包含已经革命化了许多应用领域。这些包含物理约束和遵循物理法律，以提高学习过程的精度和有效性。在这项工作中，我们探讨物理信息在强化学习应用中的用途。我们提出了一种新的分类方法，将现有的强化学习方法分为三类：观察性、推理性和学习性。我们还对现有的强化学习方法进行了分析，包括物理模型的表示形式、在强化学习架构中的特点和与强化学习流程的连接。此外，我们还发现了现有的强化学习方法的核心学习架构和物理包含偏好。这种分类方法为未来研究提供了一个整体的视角，并且为实现物理信息的包含提供了一个有效的方法。此外，这种分类方法还透视了物理信息的包含在强化学习中的潜在问题和挑战，以及未来研究的可能性。这个领域的发展潜力很大，可以提高强化学习算法的物理可能性、精度、数据效率和实际应用场景中的适用性。
</details></li>
</ul>
<hr>
<h2 id="Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis"><a href="#Extended-Symmetry-Preserving-Attention-Networks-for-LHC-Analysis" class="headerlink" title="Extended Symmetry Preserving Attention Networks for LHC Analysis"></a>Extended Symmetry Preserving Attention Networks for LHC Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01886">http://arxiv.org/abs/2309.01886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael James Fenton, Alexander Shmakov, Hideki Okawa, Yuji Li, Ko-Yang Hsiao, Shih-Chieh Hsu, Daniel Whiteson, Pierre Baldi</li>
<li>for: 这篇论文是用来探索重积过的大型内部积体（ttH）搜寻、Top颗粒子质量测量和重Z’粒子衰变到Top颗粒子对的搜寻。</li>
<li>methods: 这篇论文使用了一种简化的注意力机制——对称保持注意力网络（SPANet），并将其扩展到考虑多个输入流，例如电子和全事件特征。</li>
<li>results: 研究发现在 semi-leptonic 探索中，使用扩展的 SPANet 可以获得 significative 的改善，并在 three 个代表性的研究中提供了详细的结果。<details>
<summary>Abstract</summary>
Reconstructing unstable heavy particles requires sophisticated techniques to sift through the large number of possible permutations for assignment of detector objects to partons. An approach based on a generalized attention mechanism, symmetry preserving attention networks (SPANet), has been previously applied to top quark pair decays at the Large Hadron Collider, which produce six hadronic jets. Here we extend the SPANet architecture to consider multiple input streams, such as leptons, as well as global event features, such as the missing transverse momentum. In addition, we provide regression and classification outputs to supplement the parton assignment. We explore the performance of the extended capability of SPANet in the context of semi-leptonic decays of top quark pairs as well as top quark pairs produced in association with a Higgs boson. We find significant improvements in the power of three representative studies: search for ttH, measurement of the top quark mass and a search for a heavy Z' decaying to top quark pairs. We present ablation studies to provide insight on what the network has learned in each case.
</details>
<details>
<summary>摘要</summary>
重新建构不稳定的重子 particle需要使用复杂的技术来筛选大量的可能性，以将测器对象分配给束子。一种基于通用注意机制的 Symmetry Preserving Attention Networks (SPANet) 已经在大引子中子粒子机器人中应用于 top quark pair 减谱，该过程产生六个有征的树脂。在这里，我们扩展了 SPANet 架构，考虑多个输入流，如电子和全局事件特征，如转移质量。此外，我们还提供了 regression 和 classification 输出，以补充束子分配。我们在 semi-leptonic  decay 中研究了 top quark pair 的扩展能力，以及 top quark pair 与 Higgs  boson 共生生成的情况。我们发现，在三个表型研究中，使用扩展的 SPANet 能力具有显著改善。我们还进行了剥离研究，以了解每个情况中网络学习的内容。
</details></li>
</ul>
<hr>
<h2 id="Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies"><a href="#Task-Generalization-with-Stability-Guarantees-via-Elastic-Dynamical-System-Motion-Policies" class="headerlink" title="Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies"></a>Task Generalization with Stability Guarantees via Elastic Dynamical System Motion Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01884">http://arxiv.org/abs/2309.01884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Li, Nadia Figueroa</li>
<li>for: 本研究旨在提出一种基于动力系统（DS）的学习 FROM DEMONSTRATION（LfD）方法，以实现从少量轨迹学习稳定和准确的激发动作策略，并能够扩展到新的任务实例。</li>
<li>methods: 该方法基于线性参数变化（LPV）DS模型，并使用 Gaussian Mixture Model（GMM）来捕捉任务相关的参数变化。在新任务实例&#x2F;上下文中，GMM将被改变并使用Laplacian Editing来重新估计LPV-DS策略。</li>
<li>results: 在许多 simulate 和实际 робо臂实验中，Elastic-DS 表现出了高度的灵活性和扩展性，同时保持了控制理论上的保证。详细视频可以在<a target="_blank" rel="noopener" href="https://sites.google.com/view/elastic-ds">https://sites.google.com/view/elastic-ds</a> 找到。<details>
<summary>Abstract</summary>
Dynamical System (DS) based Learning from Demonstration (LfD) allows learning of reactive motion policies with stability and convergence guarantees from a few trajectories. Yet, current DS learning techniques lack the flexibility to generalize to new task instances as they ignore explicit task parameters that inherently change the underlying trajectories. In this work, we propose Elastic-DS, a novel DS learning, and generalization approach that embeds task parameters into the Gaussian Mixture Model (GMM) based Linear Parameter Varying (LPV) DS formulation. Central to our approach is the Elastic-GMM, a GMM constrained to SE(3) task-relevant frames. Given a new task instance/context, the Elastic-GMM is transformed with Laplacian Editing and used to re-estimate the LPV-DS policy. Elastic-DS is compositional in nature and can be used to construct flexible multi-step tasks. We showcase its strength on a myriad of simulated and real-robot experiments while preserving desirable control-theoretic guarantees. Supplementary videos can be found at https://sites.google.com/view/elastic-ds
</details>
<details>
<summary>摘要</summary>
dynamical system (DS) 基于学习from Demonstration (LfD) 可以从一些轨迹学习反应性动作策略，并且有稳定性和收敛保证。然而，当前的 DS 学习技术 ignore 表达式 task 参数，这些参数直接影响下面的轨迹，从而导致学习不具有普适性。在这项工作中，我们提出了 Elastic-DS，一种新的 DS 学习和总结方法，该方法在 Gaussian Mixture Model (GMM) 基于 Linear Parameter Varying (LPV) DS 形式ulation中嵌入任务参数。中心思想是 Elastic-GMM，一个受 SE(3) 任务相关框架约束的 GMM。给定一个新任务实例/上下文，Elastic-GMM 将通过 Laplacian Editing 变换，并用来重新估计 LPV-DS 政策。Elastic-DS 是可组合的性的，可以用于构建灵活的多步任务。我们在许多模拟和真实机器人实验中证明了其强大，同时保持了控制理论上的保证。补充视频可以在 https://sites.google.com/view/elastic-ds 找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/cs.LG_2023_09_05/" data-id="clp9qz86v00r4ok880elk1vlg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/eess.IV_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T09:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/eess.IV_2023_09_05/">eess.IV - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images"><a href="#An-Improved-Upper-Bound-on-the-Rate-Distortion-Function-of-Images" class="headerlink" title="An Improved Upper Bound on the Rate-Distortion Function of Images"></a>An Improved Upper Bound on the Rate-Distortion Function of Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02574">http://arxiv.org/abs/2309.02574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/duanzhiihao/lossy-vae">https://github.com/duanzhiihao/lossy-vae</a></li>
<li>paper_authors: Zhihao Duan, Jack Ma, Jiangpeng He, Fengqing Zhu</li>
<li>for: 这个论文的目的是提出一种基于Variational Autoencoders（VAEs）的图像损失 compression算法，以实现图像的损失 compression。</li>
<li>methods: 该论文使用了一种新的 VAE 模型架构，并应用了可变比率压缩技术，以及一种新的 \ourfunction{} 来稳定训练。</li>
<li>results: 该论文的实验结果表明，可以通过该算法实现至少 30% BD-rate 减少，相比于 VVC 编码器的内部预测模式，这表明仍然有很大的潜在提高损失图像压缩的potential。<details>
<summary>Abstract</summary>
Recent work has shown that Variational Autoencoders (VAEs) can be used to upper-bound the information rate-distortion (R-D) function of images, i.e., the fundamental limit of lossy image compression. In this paper, we report an improved upper bound on the R-D function of images implemented by (1) introducing a new VAE model architecture, (2) applying variable-rate compression techniques, and (3) proposing a novel \ourfunction{} to stabilize training. We demonstrate that at least 30\% BD-rate reduction w.r.t. the intra prediction mode in VVC codec is achievable, suggesting that there is still great potential for improving lossy image compression. Code is made publicly available at https://github.com/duanzhiihao/lossy-vae.
</details>
<details>
<summary>摘要</summary>
最近的研究表明，变量自动编码器（VAEs）可以用来Upper-bound the information rate-distortion（R-D）函数图像，即图像损失压缩的基本限制。在这篇论文中，我们报告了一种新的 VAE 模型架构，以及对变量比特率压缩技术的应用，以及一种新的 \ourfunction{} 来稳定训练。我们示出，至少可以实现30%的BD-rate减少相对于VVC编码器的内部预测模式，这表明还有很大的潜在改进损失图像压缩的可能性。代码在https://github.com/duanzhiihao/lossy-vae中公开。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation"><a href="#Fast-and-High-Performance-Learned-Image-Compression-With-Improved-Checkerboard-Context-Model-Deformable-Residual-Module-and-Knowledge-Distillation" class="headerlink" title="Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation"></a>Fast and High-Performance Learned Image Compression With Improved Checkerboard Context Model, Deformable Residual Module, and Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02529">http://arxiv.org/abs/2309.02529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haisheng Fu, Feng Liang, Jie Liang, Yongqiang Wang, Guohe Zhang, Jingning Han</li>
<li>for: 提高图像压缩速度和质量之间的平衡</li>
<li>methods: 引入四种技术：扭曲conv模块、平行上下文模型、改进的三步知识传递训练方法和$L_1$正则化</li>
<li>results: 比对 estado-of-the-art 学习图像编码方案，我们的方法可以在编码和解码过程中减少时间，并且在 PSNR 和 MS-SSIM 指标上提高 $2.3%$，在 Kodak 和 Tecnick-40 数据集上测试得到更高的性能。<details>
<summary>Abstract</summary>
Deep learning-based image compression has made great progresses recently. However, many leading schemes use serial context-adaptive entropy model to improve the rate-distortion (R-D) performance, which is very slow. In addition, the complexities of the encoding and decoding networks are quite high and not suitable for many practical applications. In this paper, we introduce four techniques to balance the trade-off between the complexity and performance. We are the first to introduce deformable convolutional module in compression framework, which can remove more redundancies in the input image, thereby enhancing compression performance. Second, we design a checkerboard context model with two separate distribution parameter estimation networks and different probability models, which enables parallel decoding without sacrificing the performance compared to the sequential context-adaptive model. Third, we develop an improved three-step knowledge distillation and training scheme to achieve different trade-offs between the complexity and the performance of the decoder network, which transfers both the final and intermediate results of the teacher network to the student network to help its training. Fourth, we introduce $L_{1}$ regularization to make the numerical values of the latent representation more sparse. Then we only encode non-zero channels in the encoding and decoding process, which can greatly reduce the encoding and decoding time. Experiments show that compared to the state-of-the-art learned image coding scheme, our method can be about 20 times faster in encoding and 70-90 times faster in decoding, and our R-D performance is also $2.3 \%$ higher. Our method outperforms the traditional approach in H.266/VVC-intra (4:4:4) and some leading learned schemes in terms of PSNR and MS-SSIM metrics when testing on Kodak and Tecnick-40 datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于图像压缩的技术在最近几年来已经取得了大量的进步。然而，许多领先的方案仍然使用序列Context-adaptive entropy模型来提高Rate-distortion（R-D）性能，这很慢。此外，压缩和解压缩网络的复杂度很高，不适合许多实际应用。在这篇论文中，我们提出了四种技术来平衡复杂度和性能的负担。我们是首次在压缩框架中引入可变 convolutional模块，可以更好地从输入图像中除去红UNDERSCOREundancy，从而提高压缩性能。其次，我们设计了Checkerboard Context模型，它使用两个独立的分布参数估计网络和不同的概率模型，可以在平行解码过程中保持同样的性能，而不需要顺序Context-adaptive模型。第三，我们开发了一种改进的三步知识传递和训练方案，可以在不同的负担和性能之间进行平衡。最后，我们引入L1正则化，使得干扰表示的数字值更加稀疏。然后，我们只编码非零通道，从而大幅减少编码和解码时间。实验结果显示，相比于当前最佳学习图像编码方案，我们的方法可以在编码过程中提高20倍，并在解码过程中提高70-90倍，同时R-D性能也提高了2.3%。我们的方法在H.266/VVC-intra（4:4:4）和一些领先的学习图像编码方案之上具有较高的PSNR和MS-SSIM指标，当测试在Kodak和Tecnick-40 dataset时。
</details></li>
</ul>
<hr>
<h2 id="An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria"><a href="#An-automated-high-resolution-phenotypic-assay-for-adult-Brugia-malayi-and-microfilaria" class="headerlink" title="An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria"></a>An automated, high-resolution phenotypic assay for adult Brugia malayi and microfilaria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03235">http://arxiv.org/abs/2309.03235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Upender Kalwa, Yunsoo Park, Michael J. Kimber, Santosh Pandey</li>
<li>for: 这项研究用于测试抗helmintic药物的有效性，以提高现有的抗helmintic药物治疗生ariasis的效果。</li>
<li>methods: 这种多参数型生物力学试验基于tracking成年布鲁格IA的运动能力，以评估三种抗helmintic药物的效果。</li>
<li>results: 研究发现，这三种抗helmintic药物可以减少成年布鲁格IA的运动能力，且具有不同的机理和效果。<details>
<summary>Abstract</summary>
Brugia malayi are thread-like parasitic worms and one of the etiological agents of Lymphatic filariasis (LF). Existing anthelmintic drugs to treat LF are effective in reducing the larval microfilaria (mf) counts in human bloodstream but are less effective on adult parasites. To test potential drug candidates, we report a multi-parameter phenotypic assay based on tracking the motility of adult B. malayi and mf in vitro. For adult B. malayi, motility is characterized by the centroid velocity, path curvature, angular velocity, eccentricity, extent, and Euler Number. These parameters are evaluated in experiments with three anthelmintic drugs. For B. malayi mf, motility is extracted from the evolving body skeleton to yield positional data and bending angles at 74 key point. We achieved high-fidelity tracking of complex worm postures (self-occlusions, omega turns, body bending, and reversals) while providing a visual representation of pose estimates and behavioral attributes in both space and time scales.
</details>
<details>
<summary>摘要</summary>
布鲁迪亚马LAY是线状寄生虫，是淋巴炎病（LF）的etiological agent之一。现有的安定虫药可以降低人体血液中幼虫微血短的数量，但对成熟虫有效性较差。为测试潜在药物候选者，我们报告了一种多参数现象学测试方法，基于成人布鲁迪亚马LAY和幼虫的运动追踪。成人布鲁迪亚马LAY的运动特征包括中心速度、轨迹弯曲、angular velocity、eccentricity、范围和Euler数。这些参数在三种安定虫药实验中被评估。布鲁迪亚马LAY幼虫的运动被提取自发展的身体骨架中，以获得位坐数据和弯曲角度。我们实现了高精度的跟踪复杂的虫姿势（自相交、卷曲、身体弯曲和反转），并提供了Visual representation of pose estimates和行为特征在空间和时间尺度上。
</details></li>
</ul>
<hr>
<h2 id="Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network"><a href="#Duration-adaptive-Video-Highlight-Pre-caching-for-Vehicular-Communication-Network" class="headerlink" title="Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network"></a>Duration-adaptive Video Highlight Pre-caching for Vehicular Communication Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01944">http://arxiv.org/abs/2309.01944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Xu, Deshi Li, Kaitao Meng, Mingliu Liu, Shuya Zhu</li>
<li>for: 本文 targets  vehicular communication networks (VCNs) and aims to improve video highlight pre-caching.</li>
<li>methods: 本文提出了一种高效的视频精华预取方案，基于服务持续时间的变化和视频段的吸引力和续接性。</li>
<li>results:  simulations based on real-world video datasets show that the proposed method significantly improves highlight entropy and jitter compared to benchmark schemes.<details>
<summary>Abstract</summary>
Video traffic in vehicular communication networks (VCNs) faces exponential growth. However, different segments of most videos reveal various attractiveness for viewers, and the pre-caching decision is greatly affected by the dynamic service duration that edge nodes can provide services for mobile vehicles driving along a road. In this paper, we propose an efficient video highlight pre-caching scheme in the vehicular communication network, adapting to the service duration. Specifically, a highlight entropy model is devised with the consideration of the segments' popularity and continuity between segments within a period of time, based on which, an optimization problem of video highlight pre-caching is formulated. As this problem is non-convex and lacks a closed-form expression of the objective function, we decouple multiple variables by deriving candidate highlight segmentations of videos through wavelet transform, which can significantly reduce the complexity of highlight pre-caching. Then the problem is solved iteratively by a highlight-direction trimming algorithm, which is proven to be locally optimal. Simulation results based on real-world video datasets demonstrate significant improvement in highlight entropy and jitter compared to benchmark schemes.
</details>
<details>
<summary>摘要</summary>
Video流量在交通网络（VCN）中正在呈指数增长趋势。然而，不同的视频片段吸引了不同的观众，并且边缘节点可以为移动 vehicles提供不同的服务时间，这会对预缓存决策产生很大的影响。在这篇论文中，我们提出了一种高效的视频突出点预缓存方案，适应到服务时间的变化。具体来说，我们开发了一个高光 entropy 模型，考虑了视频片段的吸引力和时间上的连续性，基于这个模型，我们将预缓存问题转化为优化问题。由于这个问题是非凸的，而且无法取得目标函数的闭合表达，我们将变量分解成多个变量，通过wavelet 变换 derivation 得到候选的高光片段，这可以很大地降低预缓存的复杂度。然后，我们通过一种高光方向截断算法来解决这个问题，这个算法被证明是本地优化的。实验结果基于实际的视频数据集表明，与参考方案相比，我们的方案具有显著提高高光 entropy和抖动的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/eess.IV_2023_09_05/" data-id="clp9qz8e0019wok88fbf3djqq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/05/eess.SP_2023_09_05/" class="article-date">
  <time datetime="2023-09-05T08:00:00.000Z" itemprop="datePublished">2023-09-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/05/eess.SP_2023_09_05/">eess.SP - 2023-09-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semantic-Communications-Based-on-Adaptive-Generative-Models-and-Information-Bottleneck"><a href="#Semantic-Communications-Based-on-Adaptive-Generative-Models-and-Information-Bottleneck" class="headerlink" title="Semantic Communications Based on Adaptive Generative Models and Information Bottleneck"></a>Semantic Communications Based on Adaptive Generative Models and Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02387">http://arxiv.org/abs/2309.02387</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Barbarossa, D. Comminiello, E. Grassucci, F. Pezone, S. Sardellitti, P. Di Lorenzo</li>
<li>for: 本文提出了一种基于三个基本想法的含义通信方法，即使用拓扑空间来表示数据，根据关系来捕捉 semantics，使用信息瓶颈理论来确定可信度和延迟，以及使用概率生成模型来适应无线通信频率和图像重建或执行分类任务。</li>
<li>methods: 本文使用的方法包括使用拓扑空间来表示数据，根据关系来捕捉 semantics，使用信息瓶颈理论来确定可信度和延迟，以及使用概率生成模型来适应无线通信频率和图像重建或执行分类任务。</li>
<li>results: 本文的结果表明，基于这三个基本想法的含义通信方法可以减少无线通信中的传输数据量，同时保持图像重建和分类任务的高精度。<details>
<summary>Abstract</summary>
Semantic communications represent a significant breakthrough with respect to the current communication paradigm, as they focus on recovering the meaning behind the transmitted sequence of symbols, rather than the symbols themselves. In semantic communications, the scope of the destination is not to recover a list of symbols symbolically identical to the transmitted ones, but rather to recover a message that is semantically equivalent to the semantic message emitted by the source. This paradigm shift introduces many degrees of freedom to the encoding and decoding rules that can be exploited to make the design of communication systems much more efficient. In this paper, we present an approach to semantic communication building on three fundamental ideas: 1) represent data over a topological space as a formal way to capture semantics, as expressed through relations; 2) use the information bottleneck principle as a way to identify relevant information and adapt the information bottleneck online, as a function of the wireless channel state, in order to strike an optimal trade-off between transmit power, reconstruction accuracy and delay; 3) exploit probabilistic generative models as a general tool to adapt the transmission rate to the wireless channel state and make possible the regeneration of the transmitted images or run classification tasks at the receiver side.
</details>
<details>
<summary>摘要</summary>
semantic communications represent a significant breakthrough in terms of the current communication paradigm, as they focus on recovering the meaning behind the transmitted sequence of symbols, rather than the symbols themselves. In semantic communications, the scope of the destination is not to recover a list of symbols symbolically identical to the transmitted ones, but rather to recover a message that is semantically equivalent to the semantic message emitted by the source. This paradigm shift introduces many degrees of freedom to the encoding and decoding rules that can be exploited to make the design of communication systems much more efficient. In this paper, we present an approach to semantic communication building on three fundamental ideas: 1) represent data over a topological space as a formal way to capture semantics, as expressed through relations; 2) use the information bottleneck principle as a way to identify relevant information and adapt the information bottleneck online, as a function of the wireless channel state, in order to strike an optimal trade-off between transmit power, reconstruction accuracy, and delay; 3) exploit probabilistic generative models as a general tool to adapt the transmission rate to the wireless channel state and make possible the regeneration of the transmitted images or run classification tasks at the receiver side.
</details></li>
</ul>
<hr>
<h2 id="Sensing-With-Random-Signals"><a href="#Sensing-With-Random-Signals" class="headerlink" title="Sensing With Random Signals"></a>Sensing With Random Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02375">http://arxiv.org/abs/2309.02375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jpatsenker/noisy_random_projection_sparse_signal_recon">https://github.com/jpatsenker/noisy_random_projection_sparse_signal_recon</a></li>
<li>paper_authors: Shihang Lu, Fan Liu, Fuwang Dong, Yifeng Xiong, Jie Xu, Ya-Feng Liu</li>
<li>for: 本文研究了使用随机ISAC信号进行目标探测，并对多antenna系统进行分析。</li>
<li>methods: 本文定义了一个新的探测性能指标，即随机线性最小均方误差（ELMMSE），用于描述ISAC信号Randomness中的估计误差。然后，本文研究了基于数据依赖的探测矩阵编码方案，以实现优化的探测性能，并提出了一种数据独立的探测矩阵编码方案和一种Stochastic Gradient Projection（SGP）算法来实现ELMMSE最小化。</li>
<li>results: 通过Simulations，本文示出了提议方法的优越性。<details>
<summary>Abstract</summary>
Radar systems typically employ well-designed deterministic signals for target sensing. In contrast to that, integrated sensing and communications (ISAC) systems have to use random signals to convey useful information, potentially causing sensing performance degradation. This paper analyzes the sensing performance via random ISAC signals over a multi-antenna system. Towards this end, we define a new sensing performance metric, namely, ergodic linear minimum mean square error (ELMMSE), which characterizes the estimation error averaged over the randomness of ISAC signals. Then, we investigate a data-dependent precoding scheme to minimize the ELMMSE, which attains the {optimized} sensing performance at the price of high computational complexity. To reduce the complexity, we present an alternative data-independent precoding scheme and propose a stochastic gradient projection (SGP) algorithm for ELMMSE minimization, which can be trained offline by locally generated signal samples. Finally, we demonstrate the superiority of the proposed methods by simulations.
</details>
<details>
<summary>摘要</summary>
雷达系统通常使用高效的决定性信号进行目标探测。然而，集成感知通信（ISAC）系统需要使用随机信号传输有用信息，可能导致探测性能下降。本文分析了使用随机ISAC信号在多antenna系统上的探测性能。为此，我们定义了一个新的探测性能指标，即ergodic线性最小均方误差（ELMMSE），该指标表示随机ISAC信号中的估计误差的平均值。然后，我们 investigate了一种数据依赖的 precoding 策略，以实现最优的探测性能，但是计算复杂性高。为了降低复杂性，我们提出了一种数据独立的 precoding 策略，并提出了一种Stochastic Gradient Projection（SGP）算法，用于ELMMSE最小化，该算法可以在本地生成的信号样本上进行线上培育。最后，我们通过 simulate 表明了我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Optimization-of-RSMA-for-Uplink-Communication-based-on-Intelligent-Reflecting-Surface"><a href="#Fairness-Optimization-of-RSMA-for-Uplink-Communication-based-on-Intelligent-Reflecting-Surface" class="headerlink" title="Fairness Optimization of RSMA for Uplink Communication based on Intelligent Reflecting Surface"></a>Fairness Optimization of RSMA for Uplink Communication based on Intelligent Reflecting Surface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02264">http://arxiv.org/abs/2309.02264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Zhang, Wen Chen</li>
<li>for: 提高上行通信系统的公平性</li>
<li>methods: 使用环境反射表（IRS）减轻通信路径损失，并使用最大最小偏好优化问题获取资源分配，包括接收束形成和相位偏移束形成</li>
<li>results:  simulation 结果表明，提议的方案可以提高上行通信的公平性<details>
<summary>Abstract</summary>
In this paper, we propose a rate-splitting multiple access (RSMA) scheme for uplink wireless communication systems with intelligent reflecting surface (IRS) aided. In the considered model, IRS is adopted to overcome power attenuation caused by path loss. We construct a max-min fairness optimization problem to obtain the resource allocation, including the receive beamforming at the base station (BS) and phase-shift beamforming at IRS. We also introduce a successive group decoding (SGD) algorithm at the receiver, which trades off the fairness and complexity of decoding. In the simulation, the results show that the proposed scheme has superiority in improving the fairness of uplink communication.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于多访问率拆分（RSMA）的上行无线通信系统中使用智能反射 superficie（IRS）的方案。在考虑的模型中，我们采用了IRS以超越由路径损失引起的功率损失。我们构建了最大最小公平性优化问题来获取资源分配，包括接收天线的扫描方向和phaseshift天线的调制。我们还引入了Successive Group Decoding（SGD）算法，以考虑公平性和解码复杂性之间的贸易。在仿真中，结果显示，我们提出的方案可以提高上行通信的公平性。
</details></li>
</ul>
<hr>
<h2 id="Design-of-a-New-CIM-DCSK-Based-Ambient-Backscatter-Communication-System"><a href="#Design-of-a-New-CIM-DCSK-Based-Ambient-Backscatter-Communication-System" class="headerlink" title="Design of a New CIM-DCSK-Based Ambient Backscatter Communication System"></a>Design of a New CIM-DCSK-Based Ambient Backscatter Communication System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02259">http://arxiv.org/abs/2309.02259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruipeng Yang, Yi Fang, Pingping Chen, Huan Ma<br>for: 提高Diffusion Chaos Shift Keying（DCSK）基于Ambient Backscatter Communication（AmBC）系统的数据率，我们提出了一种基于Code Index Modulation（CIM）的AmBC系统，称为CIM-DCSK-AmBC系统。methods: 在提出的系统中，CIM-DCSK信号在直接链路上传输，并用作背scatter链路的Radio Frequency源。在背scatter链路中，信号格式设计用于提高数据率，同时消除直接链路信号干扰。因此，直接链路信号和背scatter链路信号可以同时接收和解模ulation。results: 我们 derivated and validated the theoretical bit error rate（BER）表达式 Of CIM-DCSK-AmBC系统 Over multipath Rayleigh fading channels。Compared with准确参照DCSK-based AmBC（SR-DCSK-AmBC）系统作为参考系统，numerical results reveal that CIM-DCSK-AmBC系统可以在直接链路中 achieve better BER性能和在背scatter链路中 higher throughput than benchmark system。<details>
<summary>Abstract</summary>
To improve the data rate in differential chaos shift keying (DCSK) based ambient backscatter communication (AmBC) system, we propose a new AmBC system based on code index modulation (CIM), referred to as CIM-DCSK-AmBC system. In the proposed system, the CIM-DCSK signal transmitted in the direct link is used as the radio frequency source of the backscatter link. The signal format in the backscatter link is designed to increase the data rate as well as eliminate the interference of the direct link signal. As such, the direct link signal and the backscatter link signal can be received and demodulated simultaneously. Moreover, we derive and validate the theoretical bit error rate (BER) expressions of the CIM-DCSK-AmBC system over multipath Rayleigh fading channels. Regarding the short reference DCSK-based AmBC (SR-DCSK-AmBC) system as a benchmark system, numerical results reveal that the CIM-DCSK-AmBC system can achieve better BER performance in the direct link and higher throughput in the backscatter link than the benchmark system.
</details>
<details>
<summary>摘要</summary>
为了提高Diffusion Chaos Shift Keying（DCSK）基于Ambient Backscatter Communication（AmBC）系统的数据速率，我们提议一种基于Code Index Modulation（CIM）的AmBC系统，称为CIM-DCSK-AmBC系统。在该系统中，在直接链路中发送的CIM-DCSK信号被用作背scatter链路的Radio Frequency源。在背scatter链路中，信号格式设计为提高数据速率，同时消除直接链路信号干扰。因此，直接链路信号和背scatter链路信号可以同时接收和解模式。此外，我们 derive了和验证了CIM-DCSK-AmBC系统在多path Rayleigh抖振通道上的符号错误率（BER）表达式。对于参考系统SR-DCSK-AmBC系统作为标准系统，数字结果表明，CIM-DCSK-AmBC系统在直接链路中的BER性能比标准系统更好，而在背scatter链路中的 Throughput更高。
</details></li>
</ul>
<hr>
<h2 id="PyPVRoof-a-Python-package-for-extracting-the-characteristics-of-rooftop-PV-installations-using-remote-sensing-data"><a href="#PyPVRoof-a-Python-package-for-extracting-the-characteristics-of-rooftop-PV-installations-using-remote-sensing-data" class="headerlink" title="PyPVRoof: a Python package for extracting the characteristics of rooftop PV installations using remote sensing data"></a>PyPVRoof: a Python package for extracting the characteristics of rooftop PV installations using remote sensing data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07143">http://arxiv.org/abs/2309.07143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabrielkasmi/pypvroof">https://github.com/gabrielkasmi/pypvroof</a></li>
<li>paper_authors: Yann Tremenbert, Gabriel Kasmi, Laurent Dubus, Yves-Marie Saint-Drenan, Philippe Blanc</li>
<li>for: 这篇论文是为了提供一个Python包（{\tt PyPVRoof）来自动提取庭院式太阳能系统的主要特征（倾角、方位、表面、地点和安装容量）。</li>
<li>methods: 该论文使用了一种benchmark方法来评估{\tt PyPVRoof}的准确性，并提供了数据来复制这些精度测试。</li>
<li>results: 该论文的结果表明，{\tt PyPVRoof}可以高效地自动提取庭院式太阳能系统的主要特征，并且可以满足不同的数据可用性和用户需求。<details>
<summary>Abstract</summary>
Photovoltaic (PV) energy grows at an unprecedented pace, which makes it difficult to maintain up-to-date and accurate PV registries, which are critical for many applications such as PV power generation estimation. This lack of qualitative data is especially true in the case of rooftop PV installations. As a result, extensive efforts are put into the constitution of PV inventories. However, although valuable, these registries cannot be directly used for monitoring the deployment of PV or estimating the PV power generation, as these tasks usually require PV systems {\it characteristics}. To seamlessly extract these characteristics from the global inventories, we introduce {\tt PyPVRoof}. {\tt PyPVRoof} is a Python package to extract essential PV installation characteristics. These characteristics are tilt angle, azimuth, surface, localization, and installed capacity. {\tt PyPVRoof} is designed to cover all use cases regarding data availability and user needs and is based on a benchmark of the best existing methods. Data for replicating our accuracy benchmarks are available on our Zenodo repository \cite{tremenbert2023pypvroof}, and the package code is accessible at this URL: \url{https://github.com/gabrielkasmi/pypvroof}.
</details>
<details>
<summary>摘要</summary>
彩票太阳能（PV）在不可思议的速度下增长，使得保持最新和准确的PV注册记录变得很困难，这些注册记录对许多应用来说非常重要，例如PV电力生产估算。特别是在悬挂PV设备上，缺乏质量的数据是非常真实的。因此，大量的努力被投入到PV库的编制中。虽然这些注册记录非常有价值，但它们无法直接用于监测PV的部署或估算PV电力生产，因为这些任务通常需要PV系统的特征。为了快速提取这些特征，我们介绍了PyPVRoof。PyPVRoof是一个基于Python的包，用于提取PV设备的关键特征，包括倾斜角度、方位、面积、地点和安装容量。PyPVRoof针对所有的数据可用性和用户需求进行了设计，并基于最佳现有方法的准确性标准。数据用于复制我们准确性标准的数据可以在我们Zenodo存储库中找到 \cite{tremenbert2023pypvroof}, 并且包代码可以在以下URL上获取：\url{https://github.com/gabrielkasmi/pypvroof}。
</details></li>
</ul>
<hr>
<h2 id="A-Wideband-MIMO-Channel-Model-for-Aerial-Intelligent-Reflecting-Surface-Assisted-Wireless-Communications"><a href="#A-Wideband-MIMO-Channel-Model-for-Aerial-Intelligent-Reflecting-Surface-Assisted-Wireless-Communications" class="headerlink" title="A Wideband MIMO Channel Model for Aerial Intelligent Reflecting Surface-Assisted Wireless Communications"></a>A Wideband MIMO Channel Model for Aerial Intelligent Reflecting Surface-Assisted Wireless Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02171">http://arxiv.org/abs/2309.02171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoyi Liu, Nan Ma, Yaning Chen, Ke Peng, Dongsheng Xue</li>
<li>for: 本研究旨在提出一种三维宽频通道模型，用于描述空中智能反射表(AIRS)和智能反射表(IRS)合作多输入多出力(MIMO)通信系统中的通道特性。</li>
<li>methods: 本文提出了一种三维宽频通道模型，考虑了AIRS的旋转度量和空间运动角度。基于该模型，提出了一些可行的共同相位调整策略。</li>
<li>results: 实验结果表明，提出的模型能准确捕捉通道特性，并且提出的相位调整策略可以有效地改善通道统计特性和系统容量。此外，我们发现在某些情况下，IRS和直线视线(LoS)路径之间的道路具有类似特性。这些发现可以为未来智能通信系统的发展提供有价值的指导。<details>
<summary>Abstract</summary>
Compared to traditional intelligent reflecting surfaces(IRS), aerial IRS (AIRS) has unique advantages, such as more flexible deployment and wider service coverage. However, modeling AIRS in the channel presents new challenges due to their mobility. In this paper, a three-dimensional (3D) wideband channel model for AIRS and IRS joint-assisted multiple-input multiple-output (MIMO) communication system is proposed, where considering the rotational degrees of freedom in three directions and the motion angles of AIRS in space. Based on the proposed model, the channel impulse response (CIR), correlation function, and channel capacity are derived, and several feasible joint phase shifts schemes for AIRS and IRS units are proposed. Simulation results show that the proposed model can capture the channel characteristics accurately, and the proposed phase shifts methods can effectively improve the channel statistical characteristics and increase the system capacity. Additionally, we observe that in certain scenarios, the paths involving the IRS and the line-of-sight (LoS) paths exhibit similar characteristics. These findings provide valuable insights for the future development of intelligent communication systems.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:与传统的智能反射表面（IRS）相比，空中智能反射表面（AIRS）具有更多的灵活部署和更广泛的服务覆盖。然而，为AIRS在通道进行模型化带来了新的挑战，因为它们的移动会导致通道的差异。在这篇论文中，我们提出了一个三维（3D）宽频通道模型，用于AIRS和IRS共同协助多输入多出力（MIMO）通信系统。该模型考虑了AIRS在三个方向上的旋转度量和空间中的运动角度。根据提出的模型，我们 derivated了通道响应函数（CIR）、相关函数和通道容量。此外，我们还提出了一些可能的共同相位shift方案 дляAIRS和IRS单元。实验结果表明，提出的模型可以准确捕捉通道特性，并且提出的相位shift方案可以有效改善通道统计特性和系统容量。此外，我们还发现在某些场景下，IRS和直线视线（LoS）路径之间的道路具有相似的特性。这些发现提供了智能通信系统的未来发展中的有价值意见。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-SAR-ADC-Mismatch-on-Quantized-Massive-MU-MIMO-Systems"><a href="#The-Impact-of-SAR-ADC-Mismatch-on-Quantized-Massive-MU-MIMO-Systems" class="headerlink" title="The Impact of SAR-ADC Mismatch on Quantized Massive MU-MIMO Systems"></a>The Impact of SAR-ADC Mismatch on Quantized Massive MU-MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02168">http://arxiv.org/abs/2309.02168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jérémy Guichemerre, Christoph Studer</li>
<li>for: 这篇论文主要探讨了低分辨率的数字化数组（ADC）在大规模多用户（MU）多输入多Output（MIMO）无线系统中的应用。</li>
<li>methods: 论文使用了Bussgang的分解来模型了ADC的发散错误，并分析了这些错误对ADC的性能影响。</li>
<li>results: 论文发现，即使使用低分辨率的SAR ADC，但是发散错误仍然会影响系统的性能。<details>
<summary>Abstract</summary>
Low-resolution analog-to-digital converters (ADCs) in massive multi-user (MU) multiple-input multiple-output (MIMO) wireless systems can significantly reduce the power, cost, and interconnect data rates of infrastructure basestations. Thus, recent research on the theory and algorithm sides has extensively focused on such architectures, but with idealistic quantization models. However, real-world ADCs do not behave like ideal quantizers, and are affected by fabrication mismatches. We analyze the impact of capacitor-array mismatches in successive approximation register (SAR) ADCs, which are widely used in wireless systems. We use Bussgang's decomposition to model the effects of such mismatches, and we analyze their impact on the performance of a single ADC. We then simulate a massive MU-MIMO system to demonstrate that capacitor mismatches should not be ignored, even in basestations that use low-resolution SAR ADCs.
</details>
<details>
<summary>摘要</summary>
低分辨率的analog-to-digital converter (ADC)在大规模多用户多输入多输出（MU-MIMO）无线系统中可以显著降低基站的能耗、成本和 интер连接数据率。因此，当前的研究把焦点在这些架构上，但是使用理想的量化模型。然而，实际的ADC不是理想的量化器，它们受到制造偏差的影响。我们分析了Successive Approximation Register（SAR）ADC中的电容器数组偏差的影响，使用Bussgang的分解来模型这些影响。我们分析了单个ADC的性能受到这些偏差的影响，然后通过模拟大规模MU-MIMO系统来证明，即使使用低分辨率SAR ADC，也不能忽略电容器偏差。
</details></li>
</ul>
<hr>
<h2 id="Wiometrics-Comparative-Performance-of-Artificial-Neural-Networks-for-Wireless-Navigation"><a href="#Wiometrics-Comparative-Performance-of-Artificial-Neural-Networks-for-Wireless-Navigation" class="headerlink" title="Wiometrics: Comparative Performance of Artificial Neural Networks for Wireless Navigation"></a>Wiometrics: Comparative Performance of Artificial Neural Networks for Wireless Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02121">http://arxiv.org/abs/2309.02121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Russ Whiton, Junshi Chen, Fredrik Tufvesson</li>
<li>for: 本研究用于Navigation aid的 Radio signals 的利用，以及现有和未来的 terrestrial wireless communication systems 的双用性。</li>
<li>methods: 本文使用 artificial neural networks 进行了 vehicular location and heading estimation，并使用了 software-defined radio 和庞大的天线数组。</li>
<li>results: 实验结果显示，使用不同的 artificial neural network 架构和输入数据表示，可以实现精度在几米之间，并且方向精度在几度之间。<details>
<summary>Abstract</summary>
Radio signals are used broadly as navigation aids, and current and future terrestrial wireless communication systems have properties that make their dual-use for this purpose attractive. Sub-6 GHz carrier frequencies enable widespread coverage for data communication and navigation, but typically offer smaller bandwidths and limited resolution for precise estimation of geometries, particularly in environments where propagation channels are diffuse in time and/or space. Non-parametric methods have been employed with some success for such scenarios both commercially and in literature, but often with an emphasis on low-cost hardware and simple models of propagation, or with simulations that do not fully capture hardware impairments and complex propagation mechanisms. In this article, we make opportunistic observations of downlink signals transmitted by commercial cellular networks by using a software-defined radio and massive antenna array mounted on a passenger vehicle in an urban non line-of-sight scenario, together with a ground truth reference for vehicle pose. With these observations as inputs, we employ artificial neural networks to generate estimates of vehicle location and heading for various artificial neural network architectures and different representations of the input observation data, which we call wiometrics, and compare the performance for navigation. Position accuracy on the order of a few meters, and heading accuracy of a few degrees, are achieved for the best-performing combinations of networks and wiometrics. Based on the results of the experiments we draw conclusions regarding possible future directions for wireless navigation using statistical methods.
</details>
<details>
<summary>摘要</summary>
无线信号广泛用于导航帮助，现有和未来的陆地无线通信系统具有许多有用的双用特性。低于6GHz的载波频率提供了广泛的覆盖率 для数据通信和导航，但通常具有较小的带宽和限制的分辨率，尤其在时间和空间方向上的噪声通道杂化环境中。非参数方法在这些场景中已经得到了一定的成功，但经常强调低成本硬件和简单的噪声传播模型，或者使用不完全捕捉硬件障碍和复杂噪声传播机制的仿真。在这篇文章中，我们利用软件定义的广播Receiver和巨大的天线数组，在城市非直线视线场景中观察商业无线网络的下行信号，并使用软件定义的人工神经网络生成车辆位置和方向估计。我们使用不同的人工神经网络架构和输入数据的不同表示方式，称为“wiometrics”，并比较这些组合的性能。实验结果显示，最佳组合可以实现位置精度在几米之间，并且方向精度在几度之间。根据实验结果，我们对未来无线导航使用统计方法的可能性进行了结论。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Phase-Search-for-Probabilistic-Amplitude-Shaping"><a href="#Bayesian-Phase-Search-for-Probabilistic-Amplitude-Shaping" class="headerlink" title="Bayesian Phase Search for Probabilistic Amplitude Shaping"></a>Bayesian Phase Search for Probabilistic Amplitude Shaping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02003">http://arxiv.org/abs/2309.02003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Taha Askari, Lutz Lampe</li>
<li>for: 这篇论文是为了提出一种可靠的数据恢复（CPR）算法，可以在低信号至杂音比例（SNR）的情况下进行恢复。</li>
<li>methods: 这篇论文使用的方法是 bayesian 数据恢复（CPR）算法，并且将其应用于可能性束形成（PAS）。</li>
<li>results: 结果显示这个新算法可以超越干扰监测CPR的降解情况，并且在PAS中获得更好的效果。<details>
<summary>Abstract</summary>
We introduce a Bayesian carrier phase recovery (CPR) algorithm which is robust against low signal-to-noise ratio scenarios. It is therefore effective for phase recovery for probabilistic amplitude shaping (PAS). Results validate that the new algorithm overcomes the degradation experienced by blind phase-search CPR for PAS.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种 bayesian 承载阶段恢复（CPR）算法，可以在低信号噪响比enario下展示Robust性。因此，这种算法是probabilistic amplitude shaping（PAS）中的phase恢复效果的好选择。结果表明，新算法可以超越blind phase-search CPR对PAS的干扰。Here's a breakdown of the translation:* "We introduce" is translated as "我团队提出" (wǒ tuán zǔ tím shuō)* "Bayesian carrier phase recovery" is translated as "bayesian 承载阶段恢复" (bayesian zhāng chēng jīe duō zhī yì)* "algorithm" is translated as "算法" (suān fǎ)* "robust against low signal-to-noise ratio scenarios" is translated as "可以在低信号噪响比enario下展示Robust性" (kě yǐ zài shàng xīn xiāng bīng yè xiàng)* "It is therefore effective for phase recovery for probabilistic amplitude shaping" is translated as "因此，这种算法是probabilistic amplitude shaping（PAS）中的phase恢复效果的好选择" (yīn qù, zhè zhōng suān fǎ shì PAS 中的phase zhī yì de hǎo jiǎo)* "Results validate" is translated as "结果表明" (jiégù bǎo míng)* "that the new algorithm overcomes the degradation experienced by blind phase-search CPR for PAS" is translated as "新算法可以超越blind phase-search CPR对PAS的干扰" (xīn suān fǎ kě yǐ chāo yù blind phase-search CPR duō PAS de gōng kē)
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/05/eess.SP_2023_09_05/" data-id="clp9qz8fm01dwok886ydcdxbj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.SD_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T15:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.SD_2023_09_04/">cs.SD - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models"><a href="#Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models" class="headerlink" title="Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models"></a>Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01535">http://arxiv.org/abs/2309.01535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eike J. Nustede, Jörn Anemüller</li>
<li>for: 提高混响 speech enhancement 性能</li>
<li>methods:  integrate probabilistic (i.e., variational) latent space model into U-Network architecture</li>
<li>results: 在 MS-DNS 2020 和 Voicebank+Demand 数据集上实现了高效的混响 speech enhancement，比如 SI-SDR 达到 20.2 dB，与无 probabilistic latent space 版本相比提高约 0.5-1.4 dB，并且高于 WaveUNet 和 PHASEN。<details>
<summary>Abstract</summary>
In this paper, we propose to extend the deep, complex U-Network architecture for speech enhancement by incorporating a probabilistic (i.e., variational) latent space model. The proposed model is evaluated against several ablated versions of itself in order to study the effects of the variational latent space model, complex-value processing, and self-attention. Evaluation on the MS-DNS 2020 and Voicebank+Demand datasets yields consistently high performance. E.g., the proposed model achieves an SI-SDR of up to 20.2 dB, about 0.5 to 1.4 dB higher than its ablated version without probabilistic latent space, 2-2.4 dB higher than WaveUNet, and 6.7 dB above PHASEN. Compared to real-valued magnitude spectrogram processing with a variational U-Net, the complex U-Net achieves an improvement of up to 4.5 dB SI-SDR. Complex spectrum encoding as magnitude and phase yields best performance in anechoic conditions whereas real and imaginary part representation results in better generalization to (novel) reverberation conditions, possibly due to the underlying physics of sound.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提议扩展深度、复杂的U-网络架构以提高语音增强。我们的提议模型包括 probabilistic（即变量）latent space模型。我们对模型的几个版本进行了ablationstudy，以研究变量latent space模型、复杂值处理和自注意的效果。我们对MS-DNS 2020和Voicebank+Demand dataset进行了评估，得到了出色的表现。例如，我们的模型在SI-SDR方面可以达到20.2 dB，与无 probabilistic latent space版本相比高出0.5-1.4 dB，与WaveUNet相比高出2-2.4 dB，与PHASEN相比高出6.7 dB。与实数值spectrogram处理的变量U-Net相比，复杂spectrum编码为实数值和相位的方法可以在静音条件下达到最佳性能，而实部和虚部表示的方法可以更好地泛化到（新的）噪音条件，可能是因为声音的物理学习。
</details></li>
</ul>
<hr>
<h2 id="Quid-Manumit-–-Freeing-the-Qubit-for-Art"><a href="#Quid-Manumit-–-Freeing-the-Qubit-for-Art" class="headerlink" title="Quid Manumit – Freeing the Qubit for Art"></a>Quid Manumit – Freeing the Qubit for Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03104">http://arxiv.org/abs/2309.03104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Carney</li>
<li>for: 这篇论文描述了如何使用嵌入式量子 simulateur 创造出具有艺术性的量子乐器和乐效。</li>
<li>methods: 该论文利用了 previously released 的 ARM-based Raspberry Pi Pico嵌入式微控制器的量子 simulateur 代码，并提供了一些示例，包括一个量子 MIDI 处理器，可以根据输入音符生成附加的伴奏和具有量子生成的乐器，以及一个量子扭曲模块，可以根据量子Circuit来修改乐器的原始声音。</li>
<li>results: 该论文提供了一些示例，包括一个自包含的Quantum Stylophone和一个效果模块插件called ‘QubitCrusher’ для Korg Nu:Tekt NTS-1。这篇论文还讨论了未来的工作和方向，并提供了所有示例作为开源代码。这是作者所知道的第一个嵌入式量子 simulateur 用于乐器音乐（另一个 QSIM）。<details>
<summary>Abstract</summary>
This paper describes how to `Free the Qubit' for art, by creating standalone quantum musical effects and instruments. Previously released quantum simulator code for an ARM-based Raspberry Pi Pico embedded microcontroller is utilised here, and several examples are built demonstrating different methods of utilising embedded resources: The first is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. The second is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, which is presented in two forms; a self-contained Quantum Stylophone, and an effect module plugin called 'QubitCrusher' for the Korg Nu:Tekt NTS-1. This paper also discusses future work and directions for quantum instruments, and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (another QSIM).
</details>
<details>
<summary>摘要</summary>
The first example is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum-generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. The second example is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, presented in two forms: a self-contained Quantum Stylophone and an effect module plugin called "QubitCrusher" for the Korg Nu:Tekt NTS-1.The paper also discusses future work and directions for quantum instruments and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (QSIM).
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.SD_2023_09_04/" data-id="clp9qz89p00yook888qv5d5ol" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.CV_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T13:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.CV_2023_09_04/">cs.CV - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NLLB-CLIP-–-train-performant-multilingual-image-retrieval-model-on-a-budget"><a href="#NLLB-CLIP-–-train-performant-multilingual-image-retrieval-model-on-a-budget" class="headerlink" title="NLLB-CLIP – train performant multilingual image retrieval model on a budget"></a>NLLB-CLIP – train performant multilingual image retrieval model on a budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01859">http://arxiv.org/abs/2309.01859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Visheratin</li>
<li>for:  investigate whether someone without access to massive computing resources can make a valuable scientific contribution in the field of multilingual image retrieval.</li>
<li>methods:  trained a CLIP model with a text encoder from the NLLB model using an automatically created dataset of 106,246 good-quality images with captions in 200 languages, and used various sizes of image and text encoders and froze different parts of the model during training.</li>
<li>results:  NLLB-CLIP is comparable in quality to state-of-the-art models and significantly outperforms them on low-resource languages.<details>
<summary>Abstract</summary>
Today, the exponential rise of large models developed by academic and industrial institutions with the help of massive computing resources raises the question of whether someone without access to such resources can make a valuable scientific contribution. To explore this, we tried to solve the challenging task of multilingual image retrieval having a limited budget of $1,000. As a result, we present NLLB-CLIP - CLIP model with a text encoder from the NLLB model. To train the model, we used an automatically created dataset of 106,246 good-quality images with captions in 201 languages derived from the LAION COCO dataset. We trained multiple models using image and text encoders of various sizes and kept different parts of the model frozen during the training. We thoroughly analyzed the trained models using existing evaluation datasets and newly created XTD200 and Flickr30k-200 datasets. We show that NLLB-CLIP is comparable in quality to state-of-the-art models and significantly outperforms them on low-resource languages.
</details>
<details>
<summary>摘要</summary>
Note: "NLLB" stands for "No Language Labels Born", which is a technique for training machine learning models without language labels. "CLIP" stands for "Contrastive Language-Image Pre-training".
</details></li>
</ul>
<hr>
<h2 id="Towards-Universal-Image-Embeddings-A-Large-Scale-Dataset-and-Challenge-for-Generic-Image-Representations"><a href="#Towards-Universal-Image-Embeddings-A-Large-Scale-Dataset-and-Challenge-for-Generic-Image-Representations" class="headerlink" title="Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations"></a>Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01858">http://arxiv.org/abs/2309.01858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolaos-Antonios Ypsilantis, Kaifeng Chen, Bingyi Cao, Mário Lipovský, Pelin Dogan-Schönberger, Grzegorz Makosa, Boris Bluntschli, Mojtaba Seyedhosseini, Ondřej Chum, André Araujo</li>
<li>for: The paper aims to address the problem of universal image embedding, where a single universal model is trained and used in multiple domains.</li>
<li>methods: The paper proposes a new large-scale public benchmark for the evaluation of universal image embeddings, with 241k query images, 1.4M index images, and 2.8M training images across 8 different domains and 349k classes. The authors also provide a comprehensive experimental evaluation on the new dataset and conduct a public research competition to foster future research in this area.</li>
<li>results: The paper shows that existing approaches and simplistic extensions lead to worse performance than an assembly of models trained for each domain separately. Additionally, the public research competition attracted the participation of more than 1k teams worldwide and generated many interesting research ideas and findings.<details>
<summary>Abstract</summary>
Fine-grained and instance-level recognition methods are commonly trained and evaluated on specific domains, in a model per domain scenario. Such an approach, however, is impractical in real large-scale applications. In this work, we address the problem of universal image embedding, where a single universal model is trained and used in multiple domains. First, we leverage existing domain-specific datasets to carefully construct a new large-scale public benchmark for the evaluation of universal image embeddings, with 241k query images, 1.4M index images and 2.8M training images across 8 different domains and 349k classes. We define suitable metrics, training and evaluation protocols to foster future research in this area. Second, we provide a comprehensive experimental evaluation on the new dataset, demonstrating that existing approaches and simplistic extensions lead to worse performance than an assembly of models trained for each domain separately. Finally, we conducted a public research competition on this topic, leveraging industrial datasets, which attracted the participation of more than 1k teams worldwide. This exercise generated many interesting research ideas and findings which we present in detail. Project webpage: https://cmp.felk.cvut.cz/univ_emb/
</details>
<details>
<summary>摘要</summary>
通常，细化和实例级认识方法在特定领域上进行训练和评估，这种方法在实际大规模应用中不实用。在这种工作中，我们解决了图像嵌入的问题，其中一个通用模型在多个领域进行训练和使用。我们首先利用现有的领域特定数据集， méticulously construct了一个大规模的公共数据集，用于图像嵌入的评估，该数据集包括8个领域、349个类型，共计241k个查询图像、1.4M个指定图像和2.8M个训练图像。我们定义了适当的度量、训练和评估协议，以促进未来的研究。其次，我们对新数据集进行了完整的实验评估，表明现有方法和简单的扩展在多个领域中的性能较差于每个领域 separately trained models。最后，我们在这个主题上进行了公共研究竞赛，使用了来自产业的数据集，这引起了全球1k多个团队的参与。这个实验生成了许多有趣的研究想法和发现，我们在详细地展示。项目网页：https://cmp.felk.cvut.cz/univ_emb/
</details></li>
</ul>
<hr>
<h2 id="SMPLitex-A-Generative-Model-and-Dataset-for-3D-Human-Texture-Estimation-from-Single-Image"><a href="#SMPLitex-A-Generative-Model-and-Dataset-for-3D-Human-Texture-Estimation-from-Single-Image" class="headerlink" title="SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image"></a>SMPLitex: A Generative Model and Dataset for 3D Human Texture Estimation from Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01855">http://arxiv.org/abs/2309.01855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Casas, Marc Comino-Trinidad</li>
<li>for: 该论文旨在 estimating and manipulating 人体3D外观从单个图像中。</li>
<li>methods: 该方法基于 reciently proposed generative models for 2D images，并将其扩展到3D领域通过计算输入图像中像素与表面之间的对应关系。</li>
<li>results: 该方法在3个公共可用的数据集上进行了量化和质量评估，并表明 SMPLitex 可以对人体Texture estimation 进行更好的表现，同时允许更多的任务，如编辑、 sintesis 和 manipulate。<details>
<summary>Abstract</summary>
We propose SMPLitex, a method for estimating and manipulating the complete 3D appearance of humans captured from a single image. SMPLitex builds upon the recently proposed generative models for 2D images, and extends their use to the 3D domain through pixel-to-surface correspondences computed on the input image. To this end, we first train a generative model for complete 3D human appearance, and then fit it into the input image by conditioning the generative model to the visible parts of the subject. Furthermore, we propose a new dataset of high-quality human textures built by sampling SMPLitex conditioned on subject descriptions and images. We quantitatively and qualitatively evaluate our method in 3 publicly available datasets, demonstrating that SMPLitex significantly outperforms existing methods for human texture estimation while allowing for a wider variety of tasks such as editing, synthesis, and manipulation
</details>
<details>
<summary>摘要</summary>
我们提出SMPLitex方法，用于从单张图像中估计和操纵人体的完整3D外观。SMPLitex基于最近提出的生成模型 для2D图像，并将其扩展到3D领域通过图像上的像素到表面匹配。为此，我们首先培训了一个完整3D人体外观生成模型，然后将其适应到输入图像中可见部分的条件下。此外，我们还提出了一个新的高质量人体xture样本，通过SMPLitex conditioned on subject descriptions和图像来建立。我们在3个公共可用的数据集上Quantitatively和Qualitatively评估了我们的方法，结果显示SMPLitex Significantly Outperforms现有的人体xture估计方法，同时允许更多的任务，如编辑、生成和操作。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-in-AI-Evaluating-Deep-Neural-Networks-on-Out-of-Distribution-Images"><a href="#Uncertainty-in-AI-Evaluating-Deep-Neural-Networks-on-Out-of-Distribution-Images" class="headerlink" title="Uncertainty in AI: Evaluating Deep Neural Networks on Out-of-Distribution Images"></a>Uncertainty in AI: Evaluating Deep Neural Networks on Out-of-Distribution Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01850">http://arxiv.org/abs/2309.01850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamiu Idowu, Ahmed Almasoud</li>
<li>for: 这篇论文探讨了深度神经网络（包括ResNet-50、VGG16、DenseNet121、AlexNet和GoogleNet）在不正常数据（out-of-distribution，OOD）或受到干扰（perturbed）情况下的表现不一致。</li>
<li>methods: 这篇论文采用了三个实验方法：首先，使用预训练模型将OOD图像分类，以评估它们的表现。其次，建立了模型预测的 ensemble，使用概率平均来寻求多数票的优势。 ensemble的不确定性由average probability、variance和entropy指标来衡量。最后，对新生成的DALL-E图像或实际捕捉图像进行了干扰（filters、rotations等）测试，以评估模型的 robustness。</li>
<li>results: 结果显示，ResNet-50是OOD图像中最准确的单个模型，但ensemble perfom even better，对所有图像进行正确分类。此外，对DALL-E图像和实际捕捉图像进行干扰测试后，ResNet-50模型的表现出现了明显的敏感性，对4&#x2F;5不受干扰图像进行正确分类，但对所有受干扰图像进行错误分类，这些错误分类也可以由人类观察到，反映AI模型的局限性。使用Saliency map可以确定模型对图像的重要区域做出决策。<details>
<summary>Abstract</summary>
As AI models are increasingly deployed in critical applications, ensuring the consistent performance of models when exposed to unusual situations such as out-of-distribution (OOD) or perturbed data, is important. Therefore, this paper investigates the uncertainty of various deep neural networks, including ResNet-50, VGG16, DenseNet121, AlexNet, and GoogleNet, when dealing with such data. Our approach includes three experiments. First, we used the pretrained models to classify OOD images generated via DALL-E to assess their performance. Second, we built an ensemble from the models' predictions using probabilistic averaging for consensus due to its advantages over plurality or majority voting. The ensemble's uncertainty was quantified using average probabilities, variance, and entropy metrics. Our results showed that while ResNet-50 was the most accurate single model for OOD images, the ensemble performed even better, correctly classifying all images. Third, we tested model robustness by adding perturbations (filters, rotations, etc.) to new epistemic images from DALL-E or real-world captures. ResNet-50 was chosen for this being the best performing model. While it classified 4 out of 5 unperturbed images correctly, it misclassified all of them post-perturbation, indicating a significant vulnerability. These misclassifications, which are clear to human observers, highlight AI models' limitations. Using saliency maps, we identified regions of the images that the model considered important for their decisions.
</details>
<details>
<summary>摘要</summary>
As AI模型在关键应用中得到广泛应用，确保模型在不常见的情况下（如外部数据）的稳定性是重要的。因此，这篇论文研究了各种深度神经网络（包括ResNet-50、VGG16、DenseNet121、AlexNet和GoogleNet）在处理不常见数据时的不确定性。我们的方法包括三个实验。第一个实验是使用预训练模型来分类DALL-E生成的外部数据，以评估它们的性能。第二个实验是使用概率权重平均来构建一个ensemble，并使用概率、方差和 entropy 度量来衡量ensemble的不确定性。我们的结果表明，虽然ResNet-50是外部数据上最准确的单个模型，但ensemble perfom even better， correctly classifying all images。第三个实验是测试模型的Robustness，我们添加了 filters、旋转等扰动到DALL-E生成的新知识图像或实际捕捉图像。ResNet-50是我们选择的，因为它是最佳性能的模型。而在添加扰动后，ResNet-50对5个未扰动图像中的4个正确分类，但对所有扰动图像 incorrect classification，这表明模型有 significiant vulnerability。这些错误分类，对人类来说明显， highlight AI模型的局限性。使用saliency maps，我们identified模型对图像决策中的重要区域。
</details></li>
</ul>
<hr>
<h2 id="StereoFlowGAN-Co-training-for-Stereo-and-Flow-with-Unsupervised-Domain-Adaptation"><a href="#StereoFlowGAN-Co-training-for-Stereo-and-Flow-with-Unsupervised-Domain-Adaptation" class="headerlink" title="StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation"></a>StereoFlowGAN: Co-training for Stereo and Flow with Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01842">http://arxiv.org/abs/2309.01842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhexiao Xiong, Feng Qiao, Yu Zhang, Nathan Jacobs</li>
<li>for: 这 paper 用于提出一种基于图像到图像翻译的新训练策略，用于立体匹配和光流估算，以便在真实图像场景下实现优秀的性能。</li>
<li>methods: 这 paper 使用了一种图像到图像翻译的方法，通过在真实图像和Synthetic图像之间进行图像翻译，以便在真实图像场景下训练模型。它还引入了一种bidirectional feature warping模块，可以处理左右和前后方向的图像翻译。</li>
<li>results: 实验结果表明，这 paper 的提出的方法可以比前一些基于域变换的方法更高效地进行立体匹配和光流估算，这证明了该方法的有效性。<details>
<summary>Abstract</summary>
We introduce a novel training strategy for stereo matching and optical flow estimation that utilizes image-to-image translation between synthetic and real image domains. Our approach enables the training of models that excel in real image scenarios while relying solely on ground-truth information from synthetic images. To facilitate task-agnostic domain adaptation and the training of task-specific components, we introduce a bidirectional feature warping module that handles both left-right and forward-backward directions. Experimental results show competitive performance over previous domain translation-based methods, which substantiate the efficacy of our proposed framework, effectively leveraging the benefits of unsupervised domain adaptation, stereo matching, and optical flow estimation.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的训练策略，用于stereo匹配和光流估算，该策略利用图像到图像翻译来在生成图像和实际图像域之间进行图像-图像翻译。我们的方法允许在实际图像场景下训练出 excel 的模型，只靠基于生成图像的真实信息进行训练。为了实现任务不受限制的领域适应和任务特定组件的训练，我们引入了双向特征扭曲模块，可以处理左右和前后两个方向。实验结果显示，我们的提出的框架可以与前一些基于领域翻译的方法相比，并且实际上利用了无监督领域适应、stereo匹配和光流估算的优点。
</details></li>
</ul>
<hr>
<h2 id="On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision"><a href="#On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision" class="headerlink" title="On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision"></a>On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01824">http://arxiv.org/abs/2309.01824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishmeet Kaur, Adwaita Janardhan Jadhav</li>
<li>for: This paper aims to improve the deployability of state-of-the-art computer vision techniques on resource-constrained edge devices.</li>
<li>methods: The paper proposes a novel technique called AdaptiveActivation, which dynamically adjusts the sparsity and precision of a DNN’s activation function during run-time to improve accuracy and energy consumption.</li>
<li>results: The authors conduct experiments on popular edge devices and show that their approach achieves accuracy within 1.5% of the baseline while requiring 10%–38% less memory, providing more accuracy-efficiency tradeoff options.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高现代计算机视觉技术在受限的边缘设备上的部署可能性。</li>
<li>methods: 论文提出了一种名为 AdaptiveActivation 的新技术，它在运行时动态调整 DNN 的激活函数输出范围，以提高准确率和能耗。</li>
<li>results: 作者在各种边缘设备上进行了实验，并显示其方法可以达到基eline的准确率（ Within 1.5%），同时需要10%-38% Less memory，提供更多的准确率-效率质量权衡选项。<details>
<summary>Abstract</summary>
Processing visual data on mobile devices has many applications, e.g., emergency response and tracking. State-of-the-art computer vision techniques rely on large Deep Neural Networks (DNNs) that are usually too power-hungry to be deployed on resource-constrained edge devices. Many techniques improve the efficiency of DNNs by using sparsity or quantization. However, the accuracy and efficiency of these techniques cannot be adapted for diverse edge applications with different hardware constraints and accuracy requirements. This paper presents a novel technique to allow DNNs to adapt their accuracy and energy consumption during run-time, without the need for any re-training. Our technique called AdaptiveActivation introduces a hyper-parameter that controls the output range of the DNNs' activation function to dynamically adjust the sparsity and precision in the DNN. AdaptiveActivation can be applied to any existing pre-trained DNN to improve their deployability in diverse edge environments. We conduct experiments on popular edge devices and show that the accuracy is within 1.5% of the baseline. We also show that our approach requires 10%--38% less memory than the baseline techniques leading to more accuracy-efficiency tradeoff options
</details>
<details>
<summary>摘要</summary>
处理移动设备上的视觉数据有很多应用，例如紧急响应和跟踪。现状顶尖计算机视觉技术依靠大深度神经网络（DNNs），但这些大DNNs通常是资源受限的边缘设备上不可deploy。许多技术改进DNNs的效率，使用稀疏性或量化。然而，这些技术不能适应多样化的边缘应用程序不同的硬件限制和准确要求。这篇论文提出了一种新的技术，允许DNNs在运行时自适应准确和能耗，无需任何再训练。我们的技术被称为AdaptiveActivation，它在DNNs的活化函数输出范围中引入了一个超参数，以动态调整DNNs的稀疏性和精度。AdaptiveActivation可以应用于任何现有的预训练DNN，以提高它们在多样化边缘环境中的部署可能性。我们在受欢迎的边缘设备上进行了实验，并证明了准确性与基准值相差1.5%。我们还证明了我们的方法需要10%-38% menos的内存，从而提供更多的准确度-效率质量评估选项
</details></li>
</ul>
<hr>
<h2 id="Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations"><a href="#Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations" class="headerlink" title="Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations"></a>Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01823">http://arxiv.org/abs/2309.01823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoyan Pan, Yiqiao Liu, Sarah Halek, Michal Tomaszewski, Shubing Wang, Richard Baumgartner, Jianda Yuan, Gregory Goldmacher, Antong Chen</li>
<li>for: 这个研究旨在提高了肿瘤辐射成像中的肿瘤 segmentation精度，以便为肿瘤生长模型的研究提供更多的数据。</li>
<li>methods: 该研究使用了一种新的模型，即多维度统一Swin transformer（MDU-ST），将2D和3D输入都可以进行学习，并且可以从大量未标注的3D肿瘤量据中学习肿瘤形态下的基本特征。</li>
<li>results: 该研究发现，使用该模型可以在肿瘤成像中提高肿瘤 segmentation精度，并且在评估中得到了与其他模型相比的显著提高。这种方法可以用于自动化肿瘤 segmentation，以便为肿瘤生长模型的研究提供更多的数据。<details>
<summary>Abstract</summary>
In oncology research, accurate 3D segmentation of lesions from CT scans is essential for the modeling of lesion growth kinetics. However, following the RECIST criteria, radiologists routinely only delineate each lesion on the axial slice showing the largest transverse area, and delineate a small number of lesions in 3D for research purposes. As a result, we have plenty of unlabeled 3D volumes and labeled 2D images, and scarce labeled 3D volumes, which makes training a deep-learning 3D segmentation model a challenging task. In this work, we propose a novel model, denoted a multi-dimension unified Swin transformer (MDU-ST), for 3D lesion segmentation. The MDU-ST consists of a Shifted-window transformer (Swin-transformer) encoder and a convolutional neural network (CNN) decoder, allowing it to adapt to 2D and 3D inputs and learn the corresponding semantic information in the same encoder. Based on this model, we introduce a three-stage framework: 1) leveraging large amount of unlabeled 3D lesion volumes through self-supervised pretext tasks to learn the underlying pattern of lesion anatomy in the Swin-transformer encoder; 2) fine-tune the Swin-transformer encoder to perform 2D lesion segmentation with 2D RECIST slices to learn slice-level segmentation information; 3) further fine-tune the Swin-transformer encoder to perform 3D lesion segmentation with labeled 3D volumes. The network's performance is evaluated by the Dice similarity coefficient (DSC) and Hausdorff distance (HD) using an internal 3D lesion dataset with 593 lesions extracted from multiple anatomical locations. The proposed MDU-ST demonstrates significant improvement over the competing models. The proposed method can be used to conduct automated 3D lesion segmentation to assist radiomics and tumor growth modeling studies. This paper has been accepted by the IEEE International Symposium on Biomedical Imaging (ISBI) 2023.
</details>
<details>
<summary>摘要</summary>
在肿瘤研究中，准确的3D肿瘤分割从CT扫描图是非常重要的，以模拟肿瘤增长趋势。然而，根据RECIST标准，医生通常只在最大横向面的AXIAL slice上画出肿瘤，并且只为研究目的画出少量3D肿瘤。这意味着我们有大量未标注的3D卷积和标注的2D图像，以及罕见的标注3D卷积，这使得训练深度学习3D分割模型成为一项挑战。在这种情况下，我们提出了一种新的模型，即多维度统一Swin变换（MDU-ST），用于肿瘤分割。MDU-ST包括Swin变换器（Swin-transformer）编码器和卷积神经网络（CNN）解码器，可以适应2D和3D输入，并在同一个编码器中学习相应的 semantic 信息。基于这种模型，我们提出了一个三个阶段的框架：1）通过自动驱动的预TEXT任务来利用大量未标注的3D肿瘤卷积来学习肿瘤生物学的下面纹理；2）根据2D RECIST slice 进行精度调整Swin-transformer编码器，以学习 slice-level 分割信息；3）进一步精度调整Swin-transformer编码器，以进行3D肿瘤分割使用标注3D卷积。网络的性能被评估于 internal 3D 肿瘤数据集中，包括593个肿瘤，从多个 анаatomical 位置中提取。提出的 MDU-ST 表现出色，胜过竞争模型。该方法可以用于自动进行3D肿瘤分割，以帮助 радиOmics 和肿瘤增长模型研究。这篇文章已经被Accepted by IEEE International Symposium on Biomedical Imaging（ISBI）2023。
</details></li>
</ul>
<hr>
<h2 id="Instant-Continual-Learning-of-Neural-Radiance-Fields"><a href="#Instant-Continual-Learning-of-Neural-Radiance-Fields" class="headerlink" title="Instant Continual Learning of Neural Radiance Fields"></a>Instant Continual Learning of Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01811">http://arxiv.org/abs/2309.01811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Po, Zhengyang Dong, Alexander W. Bergman, Gordon Wetzstein</li>
<li>for:  novle-view synthesis和3D scene reconstruction</li>
<li>methods:  replay-based methods combined with a hybrid explicit–implicit scene representation</li>
<li>results:  higher reconstruction quality and faster training than previous methods<details>
<summary>Abstract</summary>
Neural radiance fields (NeRFs) have emerged as an effective method for novel-view synthesis and 3D scene reconstruction. However, conventional training methods require access to all training views during scene optimization. This assumption may be prohibitive in continual learning scenarios, where new data is acquired in a sequential manner and a continuous update of the NeRF is desired, as in automotive or remote sensing applications. When naively trained in such a continual setting, traditional scene representation frameworks suffer from catastrophic forgetting, where previously learned knowledge is corrupted after training on new data. Prior works in alleviating forgetting with NeRFs suffer from low reconstruction quality and high latency, making them impractical for real-world application. We propose a continual learning framework for training NeRFs that leverages replay-based methods combined with a hybrid explicit--implicit scene representation. Our method outperforms previous methods in reconstruction quality when trained in a continual setting, while having the additional benefit of being an order of magnitude faster.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain"><a href="#Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain" class="headerlink" title="Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain"></a>Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01797">http://arxiv.org/abs/2309.01797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchang Jiang, Marius Rüetschi, Vivien Sainte Fare Garnot, Mauro Marty, Konrad Schindler, Christian Ginzler, Jan D. Wegner</li>
<li>for: 提高瑞士国家森林评估的时间分辨率</li>
<li>methods: 使用卫星遥感和深度学习生成大规模的植被高程地图</li>
<li>results: 实现年度、国家范围内的植被高程地图，并对植被高程地图进行变化探测，检测到小于250平方米的变化<details>
<summary>Abstract</summary>
Monitoring and understanding forest dynamics is essential for environmental conservation and management. This is why the Swiss National Forest Inventory (NFI) provides countrywide vegetation height maps at a spatial resolution of 0.5 m. Its long update time of 6 years, however, limits the temporal analysis of forest dynamics. This can be improved by using spaceborne remote sensing and deep learning to generate large-scale vegetation height maps in a cost-effective way. In this paper, we present an in-depth analysis of these methods for operational application in Switzerland. We generate annual, countrywide vegetation height maps at a 10-meter ground sampling distance for the years 2017 to 2020 based on Sentinel-2 satellite imagery. In comparison to previous works, we conduct a large-scale and detailed stratified analysis against a precise Airborne Laser Scanning reference dataset. This stratified analysis reveals a close relationship between the model accuracy and the topology, especially slope and aspect. We assess the potential of deep learning-derived height maps for change detection and find that these maps can indicate changes as small as 250 $m^2$. Larger-scale changes caused by a winter storm are detected with an F1-score of 0.77. Our results demonstrate that vegetation height maps computed from satellite imagery with deep learning are a valuable, complementary, cost-effective source of evidence to increase the temporal resolution for national forest assessments.
</details>
<details>
<summary>摘要</summary>
监测和理解森林动态是环境保护和管理的关键。为此，瑞士国家森林调查（NFI）提供了全国覆盖率0.5米的植被高度地图。然而，NFI的更新周期为6年，限制了森林动态的时间分析。这可以通过使用空间Remote sensing和深度学习生成大规模的植被高度地图来改进。在这篇论文中，我们对这些方法进行了详细的分析，并在瑞士进行了操作应用。我们生成了2017年至2020年的年度、全国覆盖率10米的植被高度地图，基于Sentinel-2卫星图像。与之前的研究相比，我们进行了大规模的 stratified 分析，并对精确的空中雷达扫描参照数据进行了比较。这种 stratified 分析表明模型精度与地形特征（坡度和方向）之间存在紧密的关系。我们评估了深度学习得到的高度地图在变化检测方面的潜力，并发现这些地图可以检测变化为小为250平方米。在更大规模的变化（冬季风暴）方面，我们获得了F1分数0.77。我们的结果表明，通过卫星图像使用深度学习计算的植被高度地图是一种有价值的、补充性、成本效果的证据，可以增加国家森林评估的时间分辨率。
</details></li>
</ul>
<hr>
<h2 id="Safe-and-Robust-Watermark-Injection-with-a-Single-OoD-Image"><a href="#Safe-and-Robust-Watermark-Injection-with-a-Single-OoD-Image" class="headerlink" title="Safe and Robust Watermark Injection with a Single OoD Image"></a>Safe and Robust Watermark Injection with a Single OoD Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01786">http://arxiv.org/abs/2309.01786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyang Yu, Junyuan Hong, Haobo Zhang, Haotao Wang, Zhangyang Wang, Jiayu Zhou</li>
<li>for: 保护深度神经网络模型的知识产权和商业所有权</li>
<li>methods: 使用一个单一的 OUT-OF-distribution（OoD）图像作为秘密钥刃，并通过随机偏移模型参数来防御常见的水印移除攻击</li>
<li>results: 提出了一种安全和可靠的水印插入技术，可以在不需要训练数据的情况下，在不同的模型版本上保持水印的可读性和不朽性。<details>
<summary>Abstract</summary>
Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.
</details>
<details>
<summary>摘要</summary>
训练高性能深度神经网络需要大量的数据和计算资源。保护深度模型的知识产权（IP）和商业所有权是一项挑战，但也变得越来越重要。一条主要的水印策略是通过毒化训练样本来植入可靠的后门 triggers，但这些常常因数据隐私和安全问题而不切实际，并且容易受到微型化改进的影响。为了解决这些挑战，我们提议一种安全和可靠的后门基于水印注入技术，利用单个 OUT-OF-distribution（OoD）图像作为秘密键 для IP 验证。训练数据的独立性使其不受第三方的IP安全承诺影响。我们通过在水印注入过程中随机偏移模型参数来增强鲁棒性，以抵御通常的水印移除攻击，包括微型化、剪辑和模型提取。我们的实验结果表明，我们提议的水印策略不仅时间和样本效率高，而且具有很好的鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="StyleAdapter-A-Single-Pass-LoRA-Free-Model-for-Stylized-Image-Generation"><a href="#StyleAdapter-A-Single-Pass-LoRA-Free-Model-for-Stylized-Image-Generation" class="headerlink" title="StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation"></a>StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01770">http://arxiv.org/abs/2309.01770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi, Ying Shan, Wenping Wang, Ping Luo</li>
<li>for: 本研究旨在提出一种不需要LoRA的图像美化方法，该方法可以根据文本提示和样式参考图像来生成输出图像，而不需要训练每种样式的LoRA。</li>
<li>methods: 本方法使用了两种组件：一个两路扩充模块（TPCA）和三种解除策略。这些组件使得我们的模型可以分离文本提示和样式参考特征，并减少样式参考中的强相关性，从而提高图像质量和多样性。</li>
<li>results: 实验表明，我们的方法可以生成高质量的图像，并且可以适应不同的样式（包括未经见过的样式），而不需要多个LoRA。相比之下，现有方法 Less flexible and less efficient。<details>
<summary>Abstract</summary>
This paper presents a LoRA-free method for stylized image generation that takes a text prompt and style reference images as inputs and produces an output image in a single pass. Unlike existing methods that rely on training a separate LoRA for each style, our method can adapt to various styles with a unified model. However, this poses two challenges: 1) the prompt loses controllability over the generated content, and 2) the output image inherits both the semantic and style features of the style reference image, compromising its content fidelity. To address these challenges, we introduce StyleAdapter, a model that comprises two components: a two-path cross-attention module (TPCA) and three decoupling strategies. These components enable our model to process the prompt and style reference features separately and reduce the strong coupling between the semantic and style information in the style references. StyleAdapter can generate high-quality images that match the content of the prompts and adopt the style of the references (even for unseen styles) in a single pass, which is more flexible and efficient than previous methods. Experiments have been conducted to demonstrate the superiority of our method over previous works.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BLiSS-Bootstrapped-Linear-Shape-Space"><a href="#BLiSS-Bootstrapped-Linear-Shape-Space" class="headerlink" title="BLiSS: Bootstrapped Linear Shape Space"></a>BLiSS: Bootstrapped Linear Shape Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01765">http://arxiv.org/abs/2309.01765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjeev Muralikrishnan, Chun-Hao Paul Huang, Duygu Ceylan, Niloy J. Mitra</li>
<li>for: 创建人类形态模型，提高人类形态数据的表示和分析能力</li>
<li>methods: 使用自适应扩展模型，通过精细调整和非rigid registration来实现人类形态数据的匹配</li>
<li>results: 提出BLiSS方法，可以自动将新的不注册扫描数据匹配到已有的注册扫描数据中，提高人类形态数据的表示和分析能力<details>
<summary>Abstract</summary>
Morphable models are fundamental to numerous human-centered processes as they offer a simple yet expressive shape space. Creating such morphable models, however, is both tedious and expensive. The main challenge is establishing dense correspondences across raw scans that capture sufficient shape variation. This is often addressed using a mix of significant manual intervention and non-rigid registration. We observe that creating a shape space and solving for dense correspondence are tightly coupled -- while dense correspondence is needed to build shape spaces, an expressive shape space provides a reduced dimensional space to regularize the search. We introduce BLiSS, a method to solve both progressively. Starting from a small set of manually registered scans to bootstrap the process, we enrich the shape space and then use that to get new unregistered scans into correspondence automatically. The critical component of BLiSS is a non-linear deformation model that captures details missed by the low-dimensional shape space, thus allowing progressive enrichment of the space.
</details>
<details>
<summary>摘要</summary>
《膨润模型是人类中心的过程中的基础模型，它们提供了简单 yet 表达力强的形态空间。然而，创建这些膨润模型是时间和成本的挑战。主要挑战在于在原始扫描图像之间建立密集的对匹配，以捕捉足够的形态变化。通常通过手动干预和非RIGID注册来解决这个问题。我们发现创建形态空间和 dense correspondence 是紧密相关的——而 dense correspondence 是建立形态空间的必要条件，而且一个表达力强的形态空间可以提供一个减少维度的空间来规范搜索。我们介绍了 BLiSS，一种解决这两个问题的方法。从一个小型手动注册的扫描图像开始，我们在形态空间中增强表达力，然后用这个空间来自动将新的未注册扫描图像与其他扫描图像进行对匹配。BLiSS 的关键组成部分是一种非线性塑形模型，它可以捕捉低维度形态空间所过度的细节，从而允许进行进一步的表达力增强。》
</details></li>
</ul>
<hr>
<h2 id="Multispectral-Indices-for-Wildfire-Management"><a href="#Multispectral-Indices-for-Wildfire-Management" class="headerlink" title="Multispectral Indices for Wildfire Management"></a>Multispectral Indices for Wildfire Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01751">http://arxiv.org/abs/2309.01751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afonso Oliveira, João P. Matos-Carvalho, Filipe Moutinho, Nuno Fachada</li>
<li>for: 这篇论文旨在为火灾预防和管理提供 Multispectral 指标和相关方法。</li>
<li>methods: 论文检查了多种领域，其中 Multispectral 指标与野火预防和管理有着 closest 关系，包括植被和土壤特征提取、水特征映射、人工结构识别和火灾后烧区面积估计。</li>
<li>results: 论文强调了 Multispectral 指标在野火管理中的 universality 和有效性，并提供了具体的指标，如 NDVI 和 NDWI。 同时，为了提高准确性和解决个体指标应用中的局限性，建议 integra  complementary 处理解决方案和其他数据源，如高分辨率图像和地面测量。<details>
<summary>Abstract</summary>
This paper highlights and summarizes the most important multispectral indices and associated methodologies for fire management. Various fields of study are examined where multispectral indices align with wildfire prevention and management, including vegetation and soil attribute extraction, water feature mapping, artificial structure identification, and post-fire burnt area estimation. The versatility and effectiveness of multispectral indices in addressing specific issues in wildfire management are emphasized. Fundamental insights for optimizing data extraction are presented. Concrete indices for each task, including the NDVI and the NDWI, are suggested. Moreover, to enhance accuracy and address inherent limitations of individual index applications, the integration of complementary processing solutions and additional data sources like high-resolution imagery and ground-based measurements is recommended. This paper aims to be an immediate and comprehensive reference for researchers and stakeholders working on multispectral indices related to the prevention and management of fires.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:这篇论文探讨了多spectral指标的最重要应用和方法在野火管理中，包括植被和土壤特征提取、水特征地图、人工结构识别和火灾后烧Area估计。论文强调了多spectral指标在野火预防和管理中的 versatility和有效性。提供了数据提取优化的基本理念，并建议使用NDVI和NDWI等指标。此外，为了提高准确性和解决个体指标应用的局限性，建议结合补充处理解决方案和高分辨率图像以及地面测量数据。这篇论文旨在为研究人员和相关方 working on多spectral指标与野火预防和管理的人提供立即和全面的参考。
</details></li>
</ul>
<hr>
<h2 id="Generative-based-Fusion-Mechanism-for-Multi-Modal-Tracking"><a href="#Generative-based-Fusion-Mechanism-for-Multi-Modal-Tracking" class="headerlink" title="Generative-based Fusion Mechanism for Multi-Modal Tracking"></a>Generative-based Fusion Mechanism for Multi-Modal Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01728">http://arxiv.org/abs/2309.01728</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangyong-tang/gmmt">https://github.com/zhangyong-tang/gmmt</a></li>
<li>paper_authors: Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Xiao-Jun Wu, Josef Kittler</li>
<li>for: 本研究探讨了如何使用生成模型技术来解决多Modal跟踪中的信息融合挑战。</li>
<li>methods: 本研究使用了两种常见的生成模型技术， namely Conditional Generative Adversarial Networks (CGANs) 和 Diffusion Models (DMs)。这些技术在传统的融合过程中直接将每种模式的特征传输到融合块，而不是直接将特征传输。</li>
<li>results: 经验表明，使用生成模型技术可以提高多Modal跟踪的性能，并在 LasHeR 和 RGBD1K 上设置新的纪录。<details>
<summary>Abstract</summary>
Generative models (GMs) have received increasing research interest for their remarkable capacity to achieve comprehensive understanding. However, their potential application in the domain of multi-modal tracking has remained relatively unexplored. In this context, we seek to uncover the potential of harnessing generative techniques to address the critical challenge, information fusion, in multi-modal tracking. In this paper, we delve into two prominent GM techniques, namely, Conditional Generative Adversarial Networks (CGANs) and Diffusion Models (DMs). Different from the standard fusion process where the features from each modality are directly fed into the fusion block, we condition these multi-modal features with random noise in the GM framework, effectively transforming the original training samples into harder instances. This design excels at extracting discriminative clues from the features, enhancing the ultimate tracking performance. To quantitatively gauge the effectiveness of our approach, we conduct extensive experiments across two multi-modal tracking tasks, three baseline methods, and three challenging benchmarks. The experimental results demonstrate that the proposed generative-based fusion mechanism achieves state-of-the-art performance, setting new records on LasHeR and RGBD1K.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:生成模型（GM）已经收到了研究的增加兴趣，特别是在多modal跟踪领域中。在这个预期中，我们想要探索生成技术的潜在应用，以解决多modal跟踪中的关键挑战——信息融合。在这篇论文中，我们探究了两种主要的生成技术——条件生成对抗网络（CGAN）和扩散模型（DM）。与标准融合过程不同，我们在生成模型框架中，通过conditioning多modal特征来增强特征的抽象能力，从而提高最终跟踪性能。为了量化评估我们的方法的效果，我们在两个多modal跟踪任务、三个基eline方法和三个挑战性 benchmark 上进行了广泛的实验。实验结果表明，我们提出的生成基于融合机制可以达到状态级表现，在 LasHeR 和 RGBD1K 上设置新的纪录。
</details></li>
</ul>
<hr>
<h2 id="SAF-IS-a-Spatial-Annotation-Free-Framework-for-Instance-Segmentation-of-Surgical-Tools"><a href="#SAF-IS-a-Spatial-Annotation-Free-Framework-for-Instance-Segmentation-of-Surgical-Tools" class="headerlink" title="SAF-IS: a Spatial Annotation Free Framework for Instance Segmentation of Surgical Tools"></a>SAF-IS: a Spatial Annotation Free Framework for Instance Segmentation of Surgical Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01723">http://arxiv.org/abs/2309.01723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Sestini, Benoit Rosa, Elena De Momi, Giancarlo Ferrigno, Nicolas Padoy</li>
<li>for:  This paper aims to develop a framework for instance segmentation of surgical instruments without requiring expensive pixel-level annotations.</li>
<li>methods: The proposed solution uses binary tool masks and binary tool presence labels to train a tool instance classifier, leveraging unsupervised binary segmentation models to obtain the masks.</li>
<li>results: The approach outperforms several state-of-the-art fully-supervised segmentation methods and is completely free from spatial annotations.<details>
<summary>Abstract</summary>
Instance segmentation of surgical instruments is a long-standing research problem, crucial for the development of many applications for computer-assisted surgery. This problem is commonly tackled via fully-supervised training of deep learning models, requiring expensive pixel-level annotations to train. In this work, we develop a framework for instance segmentation not relying on spatial annotations for training. Instead, our solution only requires binary tool masks, obtainable using recent unsupervised approaches, and binary tool presence labels, freely obtainable in robot-assisted surgery. Based on the binary mask information, our solution learns to extract individual tool instances from single frames, and to encode each instance into a compact vector representation, capturing its semantic features. Such representations guide the automatic selection of a tiny number of instances (8 only in our experiments), displayed to a human operator for tool-type labelling. The gathered information is finally used to match each training instance with a binary tool presence label, providing an effective supervision signal to train a tool instance classifier. We validate our framework on the EndoVis 2017 and 2018 segmentation datasets. We provide results using binary masks obtained either by manual annotation or as predictions of an unsupervised binary segmentation model. The latter solution yields an instance segmentation approach completely free from spatial annotations, outperforming several state-of-the-art fully-supervised segmentation approaches.
</details>
<details>
<summary>摘要</summary>
Instance segmentation of surgical instruments是长期的研究问题，对计算机助手手术应用的发展非常重要。通常通过深度学习模型的全导学习来解决这个问题，需要昂贵的像素级注解来训练。在这种工作中，我们开发了不需要空间注解的实例分割框架。而是利用最近的无监督方法获取的二进制工具面积，以及在机器人助手手术中自由获得的二进制工具存在标签。基于二进制面积信息，我们的解决方案可以从单帧中提取个体工具实例，并将每个实例编码为一个紧凑的向量表示，捕捉其 semantic 特征。这些表示导引人工操作员选择工具类型标签。最终，我们使用这些标签来匹配每个训练实例与二进制工具存在标签，以提供有效的超级视图信号，用于训练工具实例分类器。我们在EndoVis 2017和2018 segmentation dataset上验证了我们的框架。我们使用手动注解或predictions of unsupervised binary segmentation model来获取二进制面积。后者解决方案可以完全免除空间注解，并在数个状态之前的全导学习方法之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ControlMat-A-Controlled-Generative-Approach-to-Material-Capture"><a href="#ControlMat-A-Controlled-Generative-Approach-to-Material-Capture" class="headerlink" title="ControlMat: A Controlled Generative Approach to Material Capture"></a>ControlMat: A Controlled Generative Approach to Material Capture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01700">http://arxiv.org/abs/2309.01700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Vecchio, Rosalie Martin, Arthur Roullier, Adrien Kaiser, Romain Rouffet, Valentin Deschaintre, Tamy Boubekeur<br>for: 提出了一种控制的数据生成方法，用于从单个照片中生成可信、可缩放、物理基础的数字材料。methods: 使用了生成深度网络进行控制synthesis，并采用了多通道输出的diffusion模型，采样过程进行多尺度信息融合，并引入了折叠diffusion来实现高分辨率输出和缩放性。results: 比较了与推论和秘密空间优化方法，显示了控制Mat的超越性，并且仔细验证了diffusion过程的设计选择。<details>
<summary>Abstract</summary>
Material reconstruction from a photograph is a key component of 3D content creation democratization. We propose to formulate this ill-posed problem as a controlled synthesis one, leveraging the recent progress in generative deep networks. We present ControlMat, a method which, given a single photograph with uncontrolled illumination as input, conditions a diffusion model to generate plausible, tileable, high-resolution physically-based digital materials. We carefully analyze the behavior of diffusion models for multi-channel outputs, adapt the sampling process to fuse multi-scale information and introduce rolled diffusion to enable both tileability and patched diffusion for high-resolution outputs. Our generative approach further permits exploration of a variety of materials which could correspond to the input image, mitigating the unknown lighting conditions. We show that our approach outperforms recent inference and latent-space-optimization methods, and carefully validate our diffusion process design choices. Supplemental materials and additional details are available at: https://gvecchio.com/controlmat/.
</details>
<details>
<summary>摘要</summary>
Material 重建从照片是3D内容创造的关键组件。我们提议将这个不定性问题转化为控制的合成问题，利用最近的生成深度网络的进步。我们提出ControlMat方法，给定一个具有不控制照明的照片输入，使用扩散模型生成可信、可缩放、基于物理的高分辨率数字材料。我们仔细分析扩散模型的多通道输出行为，适应多尺度信息的融合和滤波rolled扩散，以实现高分辨率输出的瓦片可重复性和补充扩散。我们的生成方法还允许探索输入图像所对应的多种材料， mitigate不确定的照明条件。我们比较了我们的扩散过程设计选择，并证明我们的方法超越了最近的推理和latent空间优化方法。补充材料和更多细节可以在：https://gvecchio.com/controlmat/。
</details></li>
</ul>
<hr>
<h2 id="Mask-Attention-Free-Transformer-for-3D-Instance-Segmentation"><a href="#Mask-Attention-Free-Transformer-for-3D-Instance-Segmentation" class="headerlink" title="Mask-Attention-Free Transformer for 3D Instance Segmentation"></a>Mask-Attention-Free Transformer for 3D Instance Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01692">http://arxiv.org/abs/2309.01692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/mask-attention-free-transformer">https://github.com/dvlab-research/mask-attention-free-transformer</a></li>
<li>paper_authors: Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, Jiaya Jia</li>
<li>for: 提高3D实例分割 task 的速度和准确率，即使初始masks的召回率低。</li>
<li>methods: 弃用mask attention设计，改用auxiliary center regression任务，通过positional prior来进行cross-attention和迭代改进。</li>
<li>results: 与现有工作相比，我们的方法可以在ScanNetv2 3D实例分割benchmark上 converge 4x faster，并且在多个dataset上显示出超过现有方法的性能。<details>
<summary>Abstract</summary>
Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.
</details>
<details>
<summary>摘要</summary>
最近，基于 transformer 的方法在 3D 实例分割中占据了主导地位，其中 mask attention 是通常 involve 的一部分。具体来说，对象查询在第一次 cross-attention 中被 guid 由初始实例面积的面积注意力，然后在类似的方式进行 iterative refinement。但我们发现，mask attention 管道通常会导致慢速收敛，因为初始实例面积的准确率较低。因此，我们放弃了面积注意力设计，转而使用 auxillary center regression 任务来解决这个问题。通过 center regression，我们可以有效地超越低准确率的问题，并通过做Positional Prior来进行 cross-attention。为了实现这个目标，我们开发了一系列的位置意识设计。首先，我们学习了 3D 空间中的位置分布，作为初始的位置查询。它们在 3D 空间中分布 densely，可以轻松地捕捉 scene 中的对象，并且具有高准确率。此外，我们还提供了相对位置编码 для cross-attention 和 iterative refinement，以便更准确地确定位置查询。实验表明，我们的方法可以在 ScanNetv2 3D 实例分割 benchmark 上 converges 4x  faster than 现有的工作，并且在多个 dataset 上也表现出了superior的性能。代码和模型可以在 https://github.com/dvlab-research/Mask-Attention-Free-Transformer 上获取。
</details></li>
</ul>
<hr>
<h2 id="Prior-Knowledge-Guided-Network-for-Video-Anomaly-Detection"><a href="#Prior-Knowledge-Guided-Network-for-Video-Anomaly-Detection" class="headerlink" title="Prior Knowledge Guided Network for Video Anomaly Detection"></a>Prior Knowledge Guided Network for Video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01682">http://arxiv.org/abs/2309.01682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhewen Deng, Dongyue Chen, Shizhuo Deng</li>
<li>for: video anomaly detection (VAD)</li>
<li>methods: 使用教师学生网络、自适应网络、知识填充等方法提高模型的泛化能力和多尺度检测能力</li>
<li>results: 实验结果表明，我们的方法可以更高效、更准确地检测视频异常事件，比现有的方法更高效。<details>
<summary>Abstract</summary>
Video Anomaly Detection (VAD) involves detecting anomalous events in videos, presenting a significant and intricate task within intelligent video surveillance. Existing studies often concentrate solely on features acquired from limited normal data, disregarding the latent prior knowledge present in extensive natural image datasets. To address this constraint, we propose a Prior Knowledge Guided Network(PKG-Net) for the VAD task. First, an auto-encoder network is incorporated into a teacher-student architecture to learn two designated proxy tasks: future frame prediction and teacher network imitation, which can provide better generalization ability on unknown samples. Second, knowledge distillation on proper feature blocks is also proposed to increase the multi-scale detection ability of the model. In addition, prediction error and teacher-student feature inconsistency are combined to evaluate anomaly scores of inference samples more comprehensively. Experimental results on three public benchmarks validate the effectiveness and accuracy of our method, which surpasses recent state-of-the-arts.
</details>
<details>
<summary>摘要</summary>
视频异常检测（VAD）涉及到视频中异常事件的检测，是智能视频监测中的一个复杂和繁复任务。现有研究 часто仅仅使用有限的正常数据来学习特征，忽略了大量自然图像数据中的隐藏知识。为解决这一问题，我们提议一种基于先前知识指导网络（PKG-Net）的方法。首先，我们在教师-学生架构中 integrate 一个自编码器网络，以学习两个指定的代理任务：未来帧预测和教师网络模仿，以提高对未知样本的泛化能力。其次，我们还提出了在正确的特征块上进行知识填充，以增强模型的多尺度检测能力。此外，我们还结合预测错误和教师-学生特征不一致来评估推理样本的异常分数。实验结果表明，我们的方法可以在三个公共标准测试集上达到更高的准确率，超过当前状态的艺术。
</details></li>
</ul>
<hr>
<h2 id="Building-Footprint-Extraction-in-Dense-Areas-using-Super-Resolution-and-Frame-Field-Learning"><a href="#Building-Footprint-Extraction-in-Dense-Areas-using-Super-Resolution-and-Frame-Field-Learning" class="headerlink" title="Building Footprint Extraction in Dense Areas using Super Resolution and Frame Field Learning"></a>Building Footprint Extraction in Dense Areas using Super Resolution and Frame Field Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01656">http://arxiv.org/abs/2309.01656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vuong Nguyen, Anh Ho, Duc-Anh Vu, Nguyen Thi Ngoc Anh, Tran Ngoc Thang</li>
<li>for: 提高叠 edifices 的精度和精细度，使得在压杂的区域中提取建筑物的轮廓更加精准。</li>
<li>methods: 使用超分解提高空中图像的空间分辨率，然后使用多任务学习模块进行分割和框架场景学习，以处理不规则的建筑结构。</li>
<li>results: 对印度一个贫民区的实验表明，提出的方法可以明显超越当前状态的方法，具有较高的精度和精细度。<details>
<summary>Abstract</summary>
Despite notable results on standard aerial datasets, current state-of-the-arts fail to produce accurate building footprints in dense areas due to challenging properties posed by these areas and limited data availability. In this paper, we propose a framework to address such issues in polygonal building extraction. First, super resolution is employed to enhance the spatial resolution of aerial image, allowing for finer details to be captured. This enhanced imagery serves as input to a multitask learning module, which consists of a segmentation head and a frame field learning head to effectively handle the irregular building structures. Our model is supervised by adaptive loss weighting, enabling extraction of sharp edges and fine-grained polygons which is difficult due to overlapping buildings and low data quality. Extensive experiments on a slum area in India that mimics a dense area demonstrate that our proposed approach significantly outperforms the current state-of-the-art methods by a large margin.
</details>
<details>
<summary>摘要</summary>
尽管现有的州际数据集上得到了可注目的结果，但现今的状态艺术无法在受挑战的区域中生成准确的建筑面积，这是因为这些区域具有复杂的属性和有限的数据可用性。在这篇论文中，我们提出了一种框架来解决这些问题。我们首先使用超分解来提高飞行图像的空间分辨率，以便更好地捕捉详细的建筑结构。这个提高的图像作为输入，我们的模型包括一个分割头和一个帧场学习头，以有效地处理不规则的建筑结构。我们的模型被超过适应损失质量调整，以提取锐利的边和细腻的多边形，这是由于 overlap 的建筑和低质量数据所致。我们的提出的方法在印度一个模拟 dense 区域的实验中显示出了与当前状态艺术方法之间的大幅提高。
</details></li>
</ul>
<hr>
<h2 id="Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis"><a href="#Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis" class="headerlink" title="Relay Diffusion: Unifying diffusion process across resolutions for image synthesis"></a>Relay Diffusion: Unifying diffusion process across resolutions for image synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03350">http://arxiv.org/abs/2309.03350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUDM/RelayDiffusion">https://github.com/THUDM/RelayDiffusion</a></li>
<li>paper_authors: Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, Jie Tang</li>
<li>for: 这 paper 是用于描述一种基于抽象扩散模型的高分辨率图像生成方法。</li>
<li>methods: 这 paper 使用了一种叫做 Relay Diffusion Model (RDM)，它可以将低分辨率图像或噪声转换成等效的高分辨率图像，从而让扩散过程可以继续无间断地进行在任何新的分辨率或模型中。</li>
<li>results: 这 paper 的实验结果表明，RDM 可以在 CelebA-HQ 和 ImageNet 256$\times$256 上 achieved state-of-the-art FID 和 sFID Result，大幅超越过去的 ADM、LDM 和 DiT 等方法。<details>
<summary>Abstract</summary>
Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that \emph{the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain}. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.
</details>
<details>
<summary>摘要</summary>
Diffusion models have achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find that the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256×256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.Here's the word-for-word translation of the text:Diffusion models 已经取得了很大的成功在图像生成中，但仍然面临高分辨率生成的挑战。通过抽象幂transform的孔径，我们发现主要的问题在于，在更高的分辨率下，相同的噪声水平会导致更高的信号噪声比在频域中。在这项工作中，我们提出了Relay Diffusion Model（RDM），它通过混合扩散和块噪声来将低分辨率图像或噪声转换成与扩散模型相对应的高分辨率图像。因此，扩散过程可以不间断继续在任何新的分辨率或模型上进行，不需要从纯噪声或低分辨率conditioning重新开始。RDM在CelebA-HQ和ImageNet 256×256上 achieve state-of-the-art FID和sFID，大幅超过了前一些工作，如ADM、LDM和DiT。所有的代码和检查点都是开源的，可以在 \url{https://github.com/THUDM/RelayDiffusion} 上找到。
</details></li>
</ul>
<hr>
<h2 id="ReLoc-PDR-Visual-Relocalization-Enhanced-Pedestrian-Dead-Reckoning-via-Graph-Optimization"><a href="#ReLoc-PDR-Visual-Relocalization-Enhanced-Pedestrian-Dead-Reckoning-via-Graph-Optimization" class="headerlink" title="ReLoc-PDR: Visual Relocalization Enhanced Pedestrian Dead Reckoning via Graph Optimization"></a>ReLoc-PDR: Visual Relocalization Enhanced Pedestrian Dead Reckoning via Graph Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01646">http://arxiv.org/abs/2309.01646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongyang Chen, Xianfei Pan, Changhao Chen</li>
<li>for: 本研究旨在提供一种精准地位化行人在卫星排除条件下，使用低成本抗 gravitational 倾斜传感器。</li>
<li>methods: 该研究提出了一种 combining PDR 和视觉重定位技术，使用图像优化算法和学习描述符来实现Robust位置估算。</li>
<li>results: 实验表明，我们的 ReLoc-PDR 在不良环境中表现出了较高的准确性和可靠性，可以在较少的文本环境和黑夜场景中实现高精度的行人位置估算。<details>
<summary>Abstract</summary>
Accurately and reliably positioning pedestrians in satellite-denied conditions remains a significant challenge. Pedestrian dead reckoning (PDR) is commonly employed to estimate pedestrian location using low-cost inertial sensor. However, PDR is susceptible to drift due to sensor noise, incorrect step detection, and inaccurate stride length estimation. This work proposes ReLoc-PDR, a fusion framework combining PDR and visual relocalization using graph optimization. ReLoc-PDR leverages time-correlated visual observations and learned descriptors to achieve robust positioning in visually-degraded environments. A graph optimization-based fusion mechanism with the Tukey kernel effectively corrects cumulative errors and mitigates the impact of abnormal visual observations. Real-world experiments demonstrate that our ReLoc-PDR surpasses representative methods in accuracy and robustness, achieving accurte and robust pedestrian positioning results using only a smartphone in challenging environments such as less-textured corridors and dark nighttime scenarios.
</details>
<details>
<summary>摘要</summary>
<<SYS>>精度和可靠地定位行人在卫星探测不可靠情况下是一个重要挑战。行人死reckoning（PDR）通常用低成本惯性传感器来估算行人位置。然而，PDR受到传感器噪声、错误的步动检测和不准确的步长估算的影响，导致偏移。本工作提出了Reloc-PDR，一种混合框架， комбини了PDR和视觉重定位使用图像优化。Reloc-PDR利用时间相关的视觉观察和学习的特征来实现在视觉弱化环境中Robust的定位。一种图像优化基于Tukeykernel的混合机制，有效地纠正累累的错误和减少了不正常的视觉观察的影响。实际实验表明，我们的Reloc-PDR在精度和Robust性方面超过了代表性的方法，在杂糌走廊和黑夜enario中实现了高精度和Robust的行人定位，只使用了智能手机。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Cross-Consistent-Deep-Unfolding-Network-for-Adaptive-All-In-One-Video-Restoration"><a href="#Cross-Consistent-Deep-Unfolding-Network-for-Adaptive-All-In-One-Video-Restoration" class="headerlink" title="Cross-Consistent Deep Unfolding Network for Adaptive All-In-One Video Restoration"></a>Cross-Consistent Deep Unfolding Network for Adaptive All-In-One Video Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01627">http://arxiv.org/abs/2309.01627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanshuo Cheng, Mingwen Shao, Yecong Wan, Lixu Zhang, Wangmeng Zuo, Deyu Meng</li>
<li>for: 提高实际应用中视频修复（VR）方法的可扩展性和可靠性。</li>
<li>methods: 提议了一种 Cross-consistent Deep Unfolding Network（CDUN），可以通过单一模型来消除多种降低效果。</li>
<li>results: 实验表明，提议的方法可以在All-In-One VR中实现状态最佳的表现。<details>
<summary>Abstract</summary>
Existing Video Restoration (VR) methods always necessitate the individual deployment of models for each adverse weather to remove diverse adverse weather degradations, lacking the capability for adaptive processing of degradations. Such limitation amplifies the complexity and deployment costs in practical applications. To overcome this deficiency, in this paper, we propose a Cross-consistent Deep Unfolding Network (CDUN) for All-In-One VR, which enables the employment of a single model to remove diverse degradations for the first time. Specifically, the proposed CDUN accomplishes a novel iterative optimization framework, capable of restoring frames corrupted by corresponding degradations according to the degradation features given in advance. To empower the framework for eliminating diverse degradations, we devise a Sequence-wise Adaptive Degradation Estimator (SADE) to estimate degradation features for the input corrupted video. By orchestrating these two cascading procedures, CDUN achieves adaptive processing for diverse degradation. In addition, we introduce a window-based inter-frame fusion strategy to utilize information from more adjacent frames. This strategy involves the progressive stacking of temporal windows in multiple iterations, effectively enlarging the temporal receptive field and enabling each frame's restoration to leverage information from distant frames. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance in All-In-One VR.
</details>
<details>
<summary>摘要</summary>
现有的视频修复（VR）方法总是需要采取各种特定的模型来消除不同的降低因素，缺乏适应处理降低因素的能力。这种局限性会增加实际应用中的复杂性和投入成本。为了解决这一缺点，在这篇论文中，我们提出了一种名为 Cross-consistent Deep Unfolding Network（CDUN）的所有在一个VR方法，可以使用单个模型来消除多种降低因素。具体来说，我们提出了一种新的迭代优化框架，可以根据输入降低因素的特征来修复降低因素所影响的帧。为了使这种框架能够消除多种降低因素，我们设计了一种适应性降低因素估计器（SADE），可以为输入降低因素提供适应性的估计。通过这两种顺序执行的过程，CDUN实现了适应处理多种降低因素。此外，我们还提出了一种窗口基本的Inter-frame融合策略，可以利用更多的邻近帧中的信息。这种策略通过在多个迭代中逐渐堆叠窗口，实现了提高时间感知范围，使每帧的修复可以利用更远的帧中的信息。广泛的实验表明，我们提出的方法在All-In-One VR中实现了state-of-the-art的性能。
</details></li>
</ul>
<hr>
<h2 id="AGG-Net-Attention-Guided-Gated-convolutional-Network-for-Depth-Image-Completion"><a href="#AGG-Net-Attention-Guided-Gated-convolutional-Network-for-Depth-Image-Completion" class="headerlink" title="AGG-Net: Attention Guided Gated-convolutional Network for Depth Image Completion"></a>AGG-Net: Attention Guided Gated-convolutional Network for Depth Image Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01624">http://arxiv.org/abs/2309.01624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyue Chen, Tingxuan Huang, Zhimin Song, Shizhuo Deng, Tong Jia</li>
<li>for: 提高RGBD相机遥感几何图像质量</li>
<li>methods: 提出了一种基于Attention Guided Gated-convolutional Network（AGG-Net）的深度图像完成方法，通过实现颜色和深度特征的综合 fusion，提高了图像的准确性和可靠性</li>
<li>results: 对比于状态艺术方法，该方法在NYU-Depth V2、DIML和SUN RGB-D等标准底本上达到了更高的完成率<details>
<summary>Abstract</summary>
Recently, stereo vision based on lightweight RGBD cameras has been widely used in various fields. However, limited by the imaging principles, the commonly used RGB-D cameras based on TOF, structured light, or binocular vision acquire some invalid data inevitably, such as weak reflection, boundary shadows, and artifacts, which may bring adverse impacts to the follow-up work. In this paper, we propose a new model for depth image completion based on the Attention Guided Gated-convolutional Network (AGG-Net), through which more accurate and reliable depth images can be obtained from the raw depth maps and the corresponding RGB images. Our model employs a UNet-like architecture which consists of two parallel branches of depth and color features. In the encoding stage, an Attention Guided Gated-Convolution (AG-GConv) module is proposed to realize the fusion of depth and color features at different scales, which can effectively reduce the negative impacts of invalid depth data on the reconstruction. In the decoding stage, an Attention Guided Skip Connection (AG-SC) module is presented to avoid introducing too many depth-irrelevant features to the reconstruction. The experimental results demonstrate that our method outperforms the state-of-the-art methods on the popular benchmarks NYU-Depth V2, DIML, and SUN RGB-D.
</details>
<details>
<summary>摘要</summary>
近些年来，基于轻量级RGBD相机的斯tereo视觉已经广泛应用于多个领域。然而，由于捕捉原理的限制，常用的RGB-D相机基于TOF、结构光或双目视觉都会不可避免地获得一些无效数据，如弱反射、边缘阴影和artefacts，这些无效数据可能会对后续工作产生负面影响。在这篇论文中，我们提出了一种基于Attention Guided Gated-convolutional Network（AGG-Net）的深度图像完成模型，通过这种模型，从原始的深度图和对应的RGB图中获得更加准确和可靠的深度图像。我们的模型采用了UNet-like的架构，该架构包括两个平行的深度和颜色特征分支。在编码阶段，我们提出了一种Attention Guided Gated-Convolution（AG-GConv）模块，用于在不同的尺度上进行深度和颜色特征的 fusión，以降低无效深度数据对重建的负面影响。在解码阶段，我们提出了一种Attention Guided Skip Connection（AG-SC）模块，以避免在重建过程中引入过多不相关的深度特征。实验结果表明，我们的方法在流行的benchmark上（NYU-Depth V2、DIML和SUN RGB-D）具有出色的性能。
</details></li>
</ul>
<hr>
<h2 id="Hindering-Adversarial-Attacks-with-Multiple-Encrypted-Patch-Embeddings"><a href="#Hindering-Adversarial-Attacks-with-Multiple-Encrypted-Patch-Embeddings" class="headerlink" title="Hindering Adversarial Attacks with Multiple Encrypted Patch Embeddings"></a>Hindering Adversarial Attacks with Multiple Encrypted Patch Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01620">http://arxiv.org/abs/2309.01620</a></li>
<li>repo_url: None</li>
<li>paper_authors: AprilPyone MaungMaung, Isao Echizen, Hitoshi Kiya</li>
<li>for: 本研究提出了一种新的钥匙基防御方法，旨在提高防御效果和可靠性。</li>
<li>methods: 本研究基于之前的钥匙基防御方法，并做出了两个主要改进：（1）高效地训练，（2）可选的随机化。提议的防御使用一或多个秘密质量嵌入和一个预训练的卷积网络来实现。当使用多个秘密嵌入时，提议的防御允许在推理过程中进行随机化。</li>
<li>results: 对于ImageNet dataset上的一系列攻击，包括适应性攻击，提议的防御得到了高度的鲁棒性精度和相当的清洁精度。<details>
<summary>Abstract</summary>
In this paper, we propose a new key-based defense focusing on both efficiency and robustness. Although the previous key-based defense seems effective in defending against adversarial examples, carefully designed adaptive attacks can bypass the previous defense, and it is difficult to train the previous defense on large datasets like ImageNet. We build upon the previous defense with two major improvements: (1) efficient training and (2) optional randomization. The proposed defense utilizes one or more secret patch embeddings and classifier heads with a pre-trained isotropic network. When more than one secret embeddings are used, the proposed defense enables randomization on inference. Experiments were carried out on the ImageNet dataset, and the proposed defense was evaluated against an arsenal of state-of-the-art attacks, including adaptive ones. The results show that the proposed defense achieves a high robust accuracy and a comparable clean accuracy compared to the previous key-based defense.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的钥匙基础防御，旨在同时增加效率和鲁棒性。先前的钥匙基础防御可以有效防御对抗例子，但是特制的适应攻击可以绕过先前的防御，并且在大量的ImageNet数据集上训练是困难的。我们基于先前的防御，提出了两大改进：（1）高效的训练和（2）可选的随机化。我们提出的防御使用一个或多个秘密贴图嵌入和一个或多个预训练的卷积网络。当使用多个秘密贴图嵌入时，我们的防御允许在推理时进行随机化。我们在ImageNet数据集上进行了实验，并评估了一系列当今最佳攻击。结果表明，我们的防御可以 дости得高效率和相对较高的清洁率，与先前的钥匙基础防御相比。
</details></li>
</ul>
<hr>
<h2 id="On-the-Query-Strategies-for-Efficient-Online-Active-Distillation"><a href="#On-the-Query-Strategies-for-Efficient-Online-Active-Distillation" class="headerlink" title="On the Query Strategies for Efficient Online Active Distillation"></a>On the Query Strategies for Efficient Online Active Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01612">http://arxiv.org/abs/2309.01612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Boldo, Enrico Martini, Mirco De Marchi, Stefano Aldegheri, Nicola Bombieri</li>
<li>for: 本研究旨在提高人姿估算（HPE）模型的训练效率和实时适应性，通过Active Learning（AL）和在线热退化。</li>
<li>methods: 本研究使用两种方法进行评估：一种是传统的离线方法，另一种是通过知识退化进行在线评估。</li>
<li>results: 研究表明，通过选择合适的帧进行训练，可以减少模型的计算复杂度，同时保持模型的准确性。这种方法可以应用于实时人姿估算场景，并且可以帮助模型在新的上下文中进行有效的适应。<details>
<summary>Abstract</summary>
Deep Learning (DL) requires lots of time and data, resulting in high computational demands. Recently, researchers employ Active Learning (AL) and online distillation to enhance training efficiency and real-time model adaptation. This paper evaluates a set of query strategies to achieve the best training results. It focuses on Human Pose Estimation (HPE) applications, assessing the impact of selected frames during training using two approaches: a classical offline method and a online evaluation through a continual learning approach employing knowledge distillation, on a popular state-of-the-art HPE dataset. The paper demonstrates the possibility of enabling training at the edge lightweight models, adapting them effectively to new contexts in real-time.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）需要很多时间和数据，导致计算成本很高。最近，研究人员使用活动学习（AL）和在线热针蒸馈来提高训练效率和实时模型适应。这篇论文评估了一组查询策略，以实现最佳训练结果。它专注于人姿估计（HPE）应用，评估在训练过程中选择的帧的影响，使用两种方法：一种传统的离线方法和一种在线评估过程，通过知识传递来适应新上下文。论文展示了在边缘训练轻量级模型的可能性，并在实时中效地适应新上下文。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-of-3D-pore-space-from-CT-images-using-curvilinear-skeleton-application-to-numerical-simulation-of-microbial-decomposition"><a href="#Segmentation-of-3D-pore-space-from-CT-images-using-curvilinear-skeleton-application-to-numerical-simulation-of-microbial-decomposition" class="headerlink" title="Segmentation of 3D pore space from CT images using curvilinear skeleton: application to numerical simulation of microbial decomposition"></a>Segmentation of 3D pore space from CT images using curvilinear skeleton: application to numerical simulation of microbial decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01611">http://arxiv.org/abs/2309.01611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Monga, Zakaria Belghali, Mouad Klai, Lucie Druoton, Dominique Michelucci, Valerie Pot</li>
<li>For: The paper aims to present a new method for describing the pore space of soil using the curvilinear skeleton, which can improve the accuracy and efficiency of numerical simulations of microbial decomposition and diffusion processes.* Methods: The authors use 3D X-ray CT scanner images to extract the pore space of soil and then use the curvilinear skeleton to segment the pore space into connected regions. They compare the results with other methods using different geometric representations of pore space, such as balls and voxels.* Results: The authors validate the simulation outputs using different pore space geometrical representations and show that the curvilinear skeleton-based method can provide more accurate and efficient simulations of microbial decomposition and diffusion processes in soil.<details>
<summary>Abstract</summary>
Recent advances in 3D X-ray Computed Tomographic (CT) sensors have stimulated research efforts to unveil the extremely complex micro-scale processes that control the activity of soil microorganisms. Voxel-based description (up to hundreds millions voxels) of the pore space can be extracted, from grey level 3D CT scanner images, by means of simple image processing tools. Classical methods for numerical simulation of biological dynamics using mesh of voxels, such as Lattice Boltzmann Model (LBM), are too much time consuming. Thus, the use of more compact and reliable geometrical representations of pore space can drastically decrease the computational cost of the simulations. Several recent works propose basic analytic volume primitives (e.g. spheres, generalized cylinders, ellipsoids) to define a piece-wise approximation of pore space for numerical simulation of draining, diffusion and microbial decomposition. Such approaches work well but the drawback is that it generates approximation errors. In the present work, we study another alternative where pore space is described by means of geometrically relevant connected subsets of voxels (regions) computed from the curvilinear skeleton. Indeed, many works use the curvilinear skeleton (3D medial axis) for analyzing and partitioning 3D shapes within various domains (medicine, material sciences, petroleum engineering, etc.) but only a few ones in soil sciences. Within the context of soil sciences, most studies dealing with 3D medial axis focus on the determination of pore throats. Here, we segment pore space using curvilinear skeleton in order to achieve numerical simulation of microbial decomposition (including diffusion processes). We validate simulation outputs by comparison with other methods using different pore space geometrical representations (balls, voxels).
</details>
<details>
<summary>摘要</summary>
In the present work, we study another alternative where pore space is described by means of geometrically relevant connected subsets of voxels (regions) computed from the curvilinear skeleton. The curvilinear skeleton is a 3D medial axis that has been widely used in various domains such as medicine, material sciences, and petroleum engineering to analyze and partition 3D shapes. However, only a few studies have applied this technique in soil sciences, and most of them have focused on determining pore throats. Here, we segment pore space using the curvilinear skeleton to achieve numerical simulation of microbial decomposition, including diffusion processes. We validate the simulation outputs by comparing them with other methods using different pore space geometrical representations (balls, voxels).
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models"><a href="#Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models" class="headerlink" title="Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models"></a>Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01590">http://arxiv.org/abs/2309.01590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdst-team/probablistic_precision_recall">https://github.com/kdst-team/probablistic_precision_recall</a></li>
<li>paper_authors: Dogyun Park, Suhyun Kim<br>for: This paper focuses on evaluating the fidelity and diversity of generative models, specifically addressing the limitations of existing k-Nearest Neighbor (kNN) based precision-recall metrics.methods: The authors propose novel metrics, P-precision and P-recall (PP&amp;PR), based on a probabilistic approach to address the oversimplified assumptions and undesirable properties of kNN.results: The authors show through extensive toy experiments and state-of-the-art generative models that their PP&amp;PR provide more reliable estimates for comparing fidelity and diversity than the existing metrics.<details>
<summary>Abstract</summary>
Assessing the fidelity and diversity of the generative model is a difficult but important issue for technological advancement. So, recent papers have introduced k-Nearest Neighbor ($k$NN) based precision-recall metrics to break down the statistical distance into fidelity and diversity. While they provide an intuitive method, we thoroughly analyze these metrics and identify oversimplified assumptions and undesirable properties of kNN that result in unreliable evaluation, such as susceptibility to outliers and insensitivity to distributional changes. Thus, we propose novel metrics, P-precision and P-recall (PP\&PR), based on a probabilistic approach that address the problems. Through extensive investigations on toy experiments and state-of-the-art generative models, we show that our PP\&PR provide more reliable estimates for comparing fidelity and diversity than the existing metrics. The codes are available at \url{https://github.com/kdst-team/Probablistic_precision_recall}.
</details>
<details>
<summary>摘要</summary>
【评估生成模型的准确性和多样性是技术发展中的一个重要问题。因此，最近的论文已经引入了k-最近邻居（$k$NN）基于精度-回归指标来细分统计距离。尽管它们提供了直观的方法，但我们在这些指标中进行了全面的分析，并发现了它们的假设过于简化，以及不良的性质，如受到异常值的影响和分布变化的敏感性不足。因此，我们提出了新的指标，即P-精度和P-回归（PP&PR），基于概率方法，以解决这些问题。我们在各种实验中进行了广泛的调查，并显示了我们的PP&PR在比较准确性和多样性方面提供了更可靠的估计。代码可以在<https://github.com/kdst-team/Probablistic_precision_recall>获取。】Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices"><a href="#SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices" class="headerlink" title="SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices"></a>SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01587">http://arxiv.org/abs/2309.01587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Montgomerie-Corcoran, Petros Toupas, Zhewen Yu, Christos-Savvas Bouganis</li>
<li>for: 这个论文是为了解决现代智能视觉和图像处理任务中的对象检测问题，以实现现实生活中的各种应用程序，如自动驾驶到医疗影像处理。</li>
<li>methods: 这个论文使用了流处理架构和自动化工具流来加速YOLO模型，以解决将现代对象检测模型部署到FPGA设备上的挑战。</li>
<li>results: 这个论文的研究结果表明，使用流处理架构和自动化工具流可以生成高性能的FPGA加速器，可以与GPU设备相比，并且超越当前状态的FPGA加速器。<details>
<summary>Abstract</summary>
AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Visual-Quality-and-Transferability-of-Adversarial-Attacks-on-Face-Recognition-Simultaneously-with-Adversarial-Restoration"><a href="#Improving-Visual-Quality-and-Transferability-of-Adversarial-Attacks-on-Face-Recognition-Simultaneously-with-Adversarial-Restoration" class="headerlink" title="Improving Visual Quality and Transferability of Adversarial Attacks on Face Recognition Simultaneously with Adversarial Restoration"></a>Improving Visual Quality and Transferability of Adversarial Attacks on Face Recognition Simultaneously with Adversarial Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01582">http://arxiv.org/abs/2309.01582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengfan Zhou, Hefei Ling, Yuxuan Shi, Jiazhong Chen, Ping Li</li>
<li>for: 该论文旨在提高黑客脸部例子的视觉质量和传输性。</li>
<li>methods: 该论文提出了一种新的黑客攻击技术，即黑客恢复（AdvRestore），它利用了一种面Restoration Latent Diffusion Model（RLDM）来提高黑客脸部例子的视觉质量和传输性。</li>
<li>results: 该论文的实验结果表明，黑客恢复技术可以备受提高黑客脸部例子的传输性和视觉质量。<details>
<summary>Abstract</summary>
Adversarial face examples possess two critical properties: Visual Quality and Transferability. However, existing approaches rarely address these properties simultaneously, leading to subpar results. To address this issue, we propose a novel adversarial attack technique known as Adversarial Restoration (AdvRestore), which enhances both visual quality and transferability of adversarial face examples by leveraging a face restoration prior. In our approach, we initially train a Restoration Latent Diffusion Model (RLDM) designed for face restoration. Subsequently, we employ the inference process of RLDM to generate adversarial face examples. The adversarial perturbations are applied to the intermediate features of RLDM. Additionally, by treating RLDM face restoration as a sibling task, the transferability of the generated adversarial face examples is further improved. Our experimental results validate the effectiveness of the proposed attack method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>发现对抗面部示例具有两个关键性能：视觉质量和传输性。然而，现有的方法几乎从未同时考虑这两个性能，导致效果不佳。为解决这问题，我们提出了一种新的对抗攻击技术，即对抗恢复（AdvRestore）。我们首先在RLDM（Restoration Latent Diffusion Model）中训练一个面Restoration模型。然后，我们使用RLDM的推理过程来生成对抗面部示例。对抗扰动被应用于RLDM中的中间特征。此外，通过将RLDM的面Restoration视为姐妹任务，我们进一步改进了生成的对抗面部示例的传输性。我们的实验结果证明了我们的攻击方法的效果。Note: "RLDM" stands for "Restoration Latent Diffusion Model" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion"><a href="#DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion" class="headerlink" title="DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion"></a>DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01575">http://arxiv.org/abs/2309.01575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez</li>
<li>for: 这篇论文是为了提出一种新的3D人姿估计方法（DiffHPE），通过灵活的扩散模型来提高人姿估计的准确性、可靠性和一致性。</li>
<li>methods: 这篇论文使用了扩散模型，并将其与现有的指导模型相结合，以提高人姿估计的精度和可靠性。</li>
<li>results: 论文通过使用扩散模型，提高了人姿估计的时间一致性和三角均衡性，并在干扰情况下表现更加稳定。在人类3.6M数据集上，这种方法表现出色，并在不同的干扰情况下保持稳定性。<details>
<summary>Abstract</summary>
We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by integrating cutting-edge diffusion models, which have revolutionized diverse fields, but are relatively unexplored in 3D-HPE. We show that diffusion models enhance the accuracy, robustness, and coherence of human pose estimations. We introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE, and demonstrate its ability to refine standard supervised 3D-HPE. We also show how diffusion models lead to more robust estimations in the face of occlusions, and improve the time-coherence and the sagittal symmetry of predictions. Using the Human\,3.6M dataset, we illustrate the effectiveness of our approach and its superiority over existing models, even under adverse situations where the occlusion patterns in training do not match those in inference. Our findings indicate that while standalone diffusion models provide commendable performance, their accuracy is even better in combination with supervised models, opening exciting new avenues for 3D-HPE research.
</details>
<details>
<summary>摘要</summary>
我们提出了一种创新的三维人姿估计（3D-HPE）方法，通过结合前沿扩散模型，这些模型在多个领域中引领了革命，但在3D-HPE中尚未得到广泛研究。我们证明了扩散模型可以提高人姿估计的准确性、可靠性和相对性。我们提出了一种新的推 diffusionHPE 方法，并证明它可以在标准的三维人姿估计上进行精细调整。我们还表明了扩散模型可以在 occlusion 情况下提供更加稳定的估计，并改善时间相关性和顺序协调性。使用 Human\,3.6M 数据集，我们证明了我们的方法的效iveness，并与现有模型相比，即使在训练和推测中 occlusion  patrerns 不同时，也能够 достичь更高的性能。我们的发现表明，单独使用扩散模型可以提供很好的性能，但是将它们与直接学习模型相结合，可以带来更高的准确性，开启了3D-HPE研究的新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Raw-Data-Is-All-You-Need-Virtual-Axle-Detector-with-Enhanced-Receptive-Field"><a href="#Raw-Data-Is-All-You-Need-Virtual-Axle-Detector-with-Enhanced-Receptive-Field" class="headerlink" title="Raw Data Is All You Need: Virtual Axle Detector with Enhanced Receptive Field"></a>Raw Data Is All You Need: Virtual Axle Detector with Enhanced Receptive Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01574">http://arxiv.org/abs/2309.01574</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henik Riedel, Robert Steven Lorenzen, Clemens Hübler</li>
<li>for: 本研究旨在开发一种新的车辆轴承检测方法，以实现实时应用桥式Weight-In-Motion（BWIM）系统，不需要专门的车辆检测器。</li>
<li>methods: 该方法基于虚拟车辆检测器（VAD）模型，利用原始加速度数据进行处理，从而提高了感知范围。</li>
<li>results: 比较 experiments 表明，与现有VAD方法相比，提出的虚拟车辆检测器with Enhanced Receptive field（VADER）可以提高(F_1) score 73%，空间精度 39%，同时降低计算和内存成本99%。VADER在使用代表性训练集和功能传感器时，(F_1) score 达99.4%，空间错误为4.13cm。此外，我们还提出了一种基于对象大小驱动的 CNN 架构设计规则，结果表明，使用原始数据可以达到更好的性能，从而成为考虑原始数据作为输入的优势。<details>
<summary>Abstract</summary>
Rising maintenance costs of ageing infrastructure necessitate innovative monitoring techniques. This paper presents a new approach for axle detection, enabling real-time application of Bridge Weigh-In-Motion (BWIM) systems without dedicated axle detectors. The proposed method adapts the Virtual Axle Detector (VAD) model to handle raw acceleration data, which allows the receptive field to be increased. The proposed Virtual Axle Detector with Enhanced Receptive field (VADER) improves the \(F_1\) score by 73\% and spatial accuracy by 39\%, while cutting computational and memory costs by 99\% compared to the state-of-the-art VAD. VADER reaches a \(F_1\) score of 99.4\% and a spatial error of 4.13~cm when using a representative training set and functional sensors. We also introduce a novel receptive field (RF) rule for an object-size driven design of Convolutional Neural Network (CNN) architectures. Based on this rule, our results suggest that models using raw data could achieve better performance than those using spectrograms, offering a compelling reason to consider raw data as input.
</details>
<details>
<summary>摘要</summary>
提高老化基础设施维护成本的必要性，这篇论文提出了一种新的车轮检测方法，允许实时应用桥梁运动测量系统（BWIM）无需专门的车轮检测器。该方法基于虚拟车轮检测器（VAD）模型，可以处理原始加速度数据，从而提高感知范围。提出的虚拟车轮检测器增强型（VADER）提高了\(F_1\)分数 by 73%和空间准确率 by 39%，同时降低计算和存储成本 by 99%比领先的VAD。VADER在使用代表性训练集和功能传感器时达到了\(F_1\)分数99.4%和空间错误4.13cm。我们还提出了一种新的接收场规则（RF），用于设计基于卷积神经网络（CNN）架构。根据这个规则，我们的结果表明使用原始数据可以实现更好的性能，这为使用原始数据作为输入提供了一个吸引人的理由。
</details></li>
</ul>
<hr>
<h2 id="Locality-Aware-Hyperspectral-Classification"><a href="#Locality-Aware-Hyperspectral-Classification" class="headerlink" title="Locality-Aware Hyperspectral Classification"></a>Locality-Aware Hyperspectral Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01561">http://arxiv.org/abs/2309.01561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhoufangqin/hylite">https://github.com/zhoufangqin/hylite</a></li>
<li>paper_authors: Fangqin Zhou, Mert Kilickaya, Joaquin Vanschoren</li>
<li>for: 本研究旨在提高干扰影像分类精度，利用视Transformers自动化干扰影像分类。</li>
<li>methods: 本研究提出了三大贡献：一、引入干扰本地信息图像变换器（HyLITE），二、一种新的规范函数，以及三、提出的方法在竞争对手中具有明显的性能优势，升高了准确率。</li>
<li>results: 本研究的实验结果表明，提出的方法可以在干扰影像分类任务中升高准确率，与竞争对手相比，具有明显的性能优势，最高准确率提升10%。<details>
<summary>Abstract</summary>
Hyperspectral image classification is gaining popularity for high-precision vision tasks in remote sensing, thanks to their ability to capture visual information available in a wide continuum of spectra. Researchers have been working on automating Hyperspectral image classification, with recent efforts leveraging Vision-Transformers. However, most research models only spectra information and lacks attention to the locality (i.e., neighboring pixels), which may be not sufficiently discriminative, resulting in performance limitations. To address this, we present three contributions: i) We introduce the Hyperspectral Locality-aware Image TransformEr (HyLITE), a vision transformer that models both local and spectral information, ii) A novel regularization function that promotes the integration of local-to-global information, and iii) Our proposed approach outperforms competing baselines by a significant margin, achieving up to 10% gains in accuracy. The trained models and the code are available at HyLITE.
</details>
<details>
<summary>摘要</summary>
干扰图像分类在远程感知中得到推广，感谢它们可以捕捉视觉信息的广泛谱 spectrum。研究人员在自动化干扰图像分类方面努力，其中最新的努力是利用视力变换器。然而，大多数研究模型只考虑spectra信息，缺乏对邻近像素（即地方信息）的注意力，这可能导致表现有限制。为此，我们提出了三项贡献：1. 我们介绍了干扰图像特征地址Transformer（HyLITE），一种视力变换器，可以同时模型本地和spectra信息。2. 一种新的规范函数，可以促进本地信息与全局信息的集成。3. 我们的提议方法在比较基eline上表现出特别的准确性提升，达到10%的提升率。我们的训练模型和代码可以在HyLITE上下载。
</details></li>
</ul>
<hr>
<h2 id="TSTTC-A-Large-Scale-Dataset-for-Time-to-Contact-Estimation-in-Driving-Scenarios"><a href="#TSTTC-A-Large-Scale-Dataset-for-Time-to-Contact-Estimation-in-Driving-Scenarios" class="headerlink" title="TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios"></a>TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01539">http://arxiv.org/abs/2309.01539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tusen-ai/TSTTC">https://github.com/tusen-ai/TSTTC</a></li>
<li>paper_authors: Yuheng Shi, Zehao Huang, Yan Yan, Naiyan Wang, Xiaojie Guo</li>
<li>for: 这篇论文主要旨在提供一个大规模的行为对象驱动距离时间联系（TTC）数据集，以便促进TTC估计方法的研究和发展。</li>
<li>methods: 这篇论文使用了大量的驾驶数据，并采用了最新的神经网络生成技术来增加小TTC情况的数据量。</li>
<li>results: 该论文提供了一个大规模的TTC数据集，并提供了一些简单 yet 有效的TTC估计基线。这些基线在提posed dataset上进行了广泛的评估，以证明其效果。<details>
<summary>Abstract</summary>
Time-to-Contact (TTC) estimation is a critical task for assessing collision risk and is widely used in various driver assistance and autonomous driving systems. The past few decades have witnessed development of related theories and algorithms. The prevalent learning-based methods call for a large-scale TTC dataset in real-world scenarios. In this work, we present a large-scale object oriented TTC dataset in the driving scene for promoting the TTC estimation by a monocular camera. To collect valuable samples and make data with different TTC values relatively balanced, we go through thousands of hours of driving data and select over 200K sequences with a preset data distribution. To augment the quantity of small TTC cases, we also generate clips using the latest Neural rendering methods. Additionally, we provide several simple yet effective TTC estimation baselines and evaluate them extensively on the proposed dataset to demonstrate their effectiveness. The proposed dataset is publicly available at https://open-dataset.tusen.ai/TSTTC.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>时间到contact（TTC）估计是评估冲突风险的关键任务，广泛应用于不同的驾驶助手和自动驾驶系统中。过去几十年内，相关理论和算法的发展都有所成就。现有的学习型方法需要大量的TTC实际场景数据。在这项工作中，我们提供了一个大规模的 объек oriented TTC数据集，用于推广TTC估计。为了收集有价值的样本并使数据具有不同TTC值相对均衡，我们通过了数千小时的驾驶数据，选择了超过200K个序列，并采用了一个预设的数据分布。为了增加小TTC情况的数量，我们还使用了最新的神经网络渲染方法生成clip。此外，我们还提供了一些简单 yet有效的TTC估计基线，并在提posed数据集上进行了广泛的评估，以示其效果。提出的数据集可以在https://open-dataset.tusen.ai/TSTTC上公开获取。
</details></li>
</ul>
<hr>
<h2 id="On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging"><a href="#On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging" class="headerlink" title="On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging"></a>On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01488">http://arxiv.org/abs/2309.01488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryanthony/mahalanobis-ood-detection">https://github.com/harryanthony/mahalanobis-ood-detection</a></li>
<li>paper_authors: Harry Anthony, Konstantinos Kamnitsas</li>
<li>for: 本研究旨在探讨在医疗应用中使用神经网络时，如何探测输入数据与训练数据之间的差异，以避免不可靠的预测。</li>
<li>methods: 本研究使用了距离基于方法，如 Mahalanobis 距离，来探测输入数据与训练数据之间的差异。</li>
<li>results: 本研究发现，使用 Mahalanobis 距离探测输入数据与训练数据之间的差异，并不是一个一致的解决方案。相反， Results 表明，选择合适的层或层组，可以提高探测不同类型的异常情况的灵活性。此外，研究还发现，将 OOD 探测器分解成不同深度的网络层可以增强网络的稳定性。这些发现都被 validate 在实际 OOD 任务上，使用 CheXpert 胸部X射影像，并使用不同的 pacemaker 和性别作为 OOD 例子。<details>
<summary>Abstract</summary>
Implementing neural networks for clinical use in medical applications necessitates the ability for the network to detect when input data differs significantly from the training data, with the aim of preventing unreliable predictions. The community has developed several methods for out-of-distribution (OOD) detection, within which distance-based approaches - such as Mahalanobis distance - have shown potential. This paper challenges the prevailing community understanding that there is an optimal layer, or combination of layers, of a neural network for applying Mahalanobis distance for detection of any OOD pattern. Using synthetic artefacts to emulate OOD patterns, this paper shows the optimum layer to apply Mahalanobis distance changes with the type of OOD pattern, showing there is no one-fits-all solution. This paper also shows that separating this OOD detector into multiple detectors at different depths of the network can enhance the robustness for detecting different OOD patterns. These insights were validated on real-world OOD tasks, training models on CheXpert chest X-rays with no support devices, then using scans with unseen pacemakers (we manually labelled 50% of CheXpert for this research) and unseen sex as OOD cases. The results inform best-practices for the use of Mahalanobis distance for OOD detection. The manually annotated pacemaker labels and the project's code are available at: https://github.com/HarryAnthony/Mahalanobis-OOD-detection.
</details>
<details>
<summary>摘要</summary>
使用神经网络进行医疗应用时，需要神经网络能够检测输入数据与训练数据之间的差异，以避免不可靠的预测。社区已经开发出了许多对外部数据（OOD）检测方法，其中距离基于方法，如马哈拉诺比斯距离，表现出了潜在。这篇论文挑战了社区认知，即在任何OOD模式下都有一个最佳层或组合层可以应用马哈拉诺比斯距离来检测OOD模式。使用 sintetic artifacts 模拟 OOD 模式，这篇论文表明了在不同类型的 OOD 模式时，适用马哈拉诺比斯距离的层不同，而且没有一个通用的解决方案。此外，这篇论文还表明，将 OOD 检测器分解成不同深度的网络层可以提高对不同 OOD 模式的检测稳定性。这些发现得到了实际 OOD 任务的验证，使用 CheXpert 胸部X射影片进行训练，然后使用未知的心 pacemaker 和性别作为 OOD 例外。结果提供了使用马哈拉诺比斯距离进行 OOD 检测的最佳实践。手动标注 pacemaker 标签和项目代码可以在 GitHub 上获取：https://github.com/HarryAnthony/Mahalanobis-OOD-detection。
</details></li>
</ul>
<hr>
<h2 id="GenSelfDiff-HIS-Generative-Self-Supervision-Using-Diffusion-for-Histopathological-Image-Segmentation"><a href="#GenSelfDiff-HIS-Generative-Self-Supervision-Using-Diffusion-for-Histopathological-Image-Segmentation" class="headerlink" title="GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation"></a>GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01487">http://arxiv.org/abs/2309.01487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vishnuvardhan Purma, Suhas Srinath, Seshan Srirangarajan, Aanchal Kakkar, Prathosh A. P<br>for:这个研究目的是提出一种基于自类学习的几何像分割方法，来减轻几何像分析的传统人工分析压力。methods:这个方法基于对无标注数据的生成扩散模型，并使用多元损失函数进行精致化。results:研究结果显示，这个方法可以在两个公开available的数据集上取得良好的效果，并且在一个新提出的头颈癌（HN）数据集上也取得了良好的效果。<details>
<summary>Abstract</summary>
Histopathological image segmentation is a laborious and time-intensive task, often requiring analysis from experienced pathologists for accurate examinations. To reduce this burden, supervised machine-learning approaches have been adopted using large-scale annotated datasets for histopathological image analysis. However, in several scenarios, the availability of large-scale annotated data is a bottleneck while training such models. Self-supervised learning (SSL) is an alternative paradigm that provides some respite by constructing models utilizing only the unannotated data which is often abundant. The basic idea of SSL is to train a network to perform one or many pseudo or pretext tasks on unannotated data and use it subsequently as the basis for a variety of downstream tasks. It is seen that the success of SSL depends critically on the considered pretext task. While there have been many efforts in designing pretext tasks for classification problems, there haven't been many attempts on SSL for histopathological segmentation. Motivated by this, we propose an SSL approach for segmenting histopathological images via generative diffusion models in this paper. Our method is based on the observation that diffusion models effectively solve an image-to-image translation task akin to a segmentation task. Hence, we propose generative diffusion as the pretext task for histopathological image segmentation. We also propose a multi-loss function-based fine-tuning for the downstream task. We validate our method using several metrics on two publically available datasets along with a newly proposed head and neck (HN) cancer dataset containing hematoxylin and eosin (H\&E) stained images along with annotations. Codes will be made public at https://github.com/PurmaVishnuVardhanReddy/GenSelfDiff-HIS.git.
</details>
<details>
<summary>摘要</summary>
历史 PATHOLOGICAL 图像分割是一项劳动密集和时间consuming的任务，经验训练的病理学家通常需要进行准确的检查。为了减轻这个负担，有些人使用了Supervised 机器学习方法，使用大规模的标注数据进行历史 PATHOLOGICAL 图像分析。然而，在一些情况下，获得大规模的标注数据是一个瓶颈，而自我supervised 学习（SSL）是一种代替方案，它可以使用只有未标注的数据进行训练。基本上，SSL 的想法是训练一个网络，使其在未标注数据上完成一些pseudo或预text任务，然后用这些任务作为基础进行多种下游任务。它的成功取决于考虑的预text任务。虽然有很多人在设计 Classification 的预text任务方面做出了努力，但是对历史 PATHOLOGICAL 图像分割的 SSL 方法还未有很多尝试。在这篇论文中，我们提出了一种基于生成扩散模型的 SSL 方法，用于分割历史 PATHOLOGICAL 图像。我们基于图像到图像的翻译任务的观察，因此我们提出了生成扩散作为预text任务。此外，我们还提出了基于多个损失函数的细化。我们使用了多种度量来验证我们的方法，并在两个公共可用的数据集上进行验证，以及一个新提出的头颈癌（HN）癌症数据集，该数据集包含HE染料的历史 PATHOLOGICAL 图像以及注解。代码将在 <https://github.com/PurmaVishnuVardhanReddy/GenSelfDiff-HIS.git> 上公开。
</details></li>
</ul>
<hr>
<h2 id="CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification"><a href="#CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification" class="headerlink" title="CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification"></a>CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01483">http://arxiv.org/abs/2309.01483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Zhang, Zhibin Zhao, Deyu Meng, Xingwu Zhang, Xuefeng Chen</li>
<li>for: 提高机器学习模型在真实世界中的部署，实现一类分类（OCC）。</li>
<li>methods: 使用适应特定目标数据集的预训练特征，并将其扩展到未知类数。</li>
<li>results: 在不同类数（1-1024）的训练数据上，一直高于当前状态艺术方法，提高OCC性能。<details>
<summary>Abstract</summary>
One-class classification (OCC), i.e., identifying whether an example belongs to the same distribution as the training data, is essential for deploying machine learning models in the real world. Adapting the pre-trained features on the target dataset has proven to be a promising paradigm for improving OCC performance. Existing methods are constrained by assumptions about the number of classes. This contradicts the real scenario where the number of classes is unknown. In this work, we propose a simple class-agnostic adaptive feature adaptation method (CA2). We generalize the center-based method to unknown classes and optimize this objective based on the prior existing in the pre-trained network, i.e., pre-trained features that belong to the same class are adjacent. CA2 is validated to consistently improve OCC performance across a spectrum of training data classes, spanning from 1 to 1024, outperforming current state-of-the-art methods. Code is available at https://github.com/zhangzilongc/CA2.
</details>
<details>
<summary>摘要</summary>
一类分类（OCC），即确定输入例子是否属于训练数据的同一分布，在实际应用中是非常重要的。适应预训练特征onto目标数据集已经证明是提高OCC性能的有效方法。现有方法受限于类别数量的假设，这与实际场景不符。在这种情况下，我们提出了一种简单的类型不可知的适应特征调整方法（CA2）。我们将中心基于方法扩展到未知类别，并基于预训练网络中的先前存在的对称性来优化这个目标函数。CA2被证明可以在训练数据类型范围从1到1024之间，在不同类别的情况下一致提高OCC性能，超越当前状态的最佳方法。代码可以在https://github.com/zhangzilongc/CA2上下载。
</details></li>
</ul>
<hr>
<h2 id="Parameter-and-Computation-Efficient-Transfer-Learning-for-Vision-Language-Pre-trained-Models"><a href="#Parameter-and-Computation-Efficient-Transfer-Learning-for-Vision-Language-Pre-trained-Models" class="headerlink" title="Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models"></a>Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01479">http://arxiv.org/abs/2309.01479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiong Wu, Wei Yu, Yiyi Zhou, Shubin Huang, Xiaoshuai Sun, Rongrong Ji</li>
<li>for: 这篇研究目的是提出一种 Parameter and Computation Efficient Transfer Learning (PCETL) 方法，以提高 Vision-Language Pre-trained (VLP) 模型在下游任务中的适应性。</li>
<li>methods: 本研究提出了一种叫做 Dynamic Architecture Skipping (DAS) 方法，它通过观察 VLP 模型的模块之间的相互关联，并使用强化学习 (RL) 来决定哪些模块是可以被跳过的。这样可以将 VLP 模型的trainable参数数量降低，同时保持其在下游任务中的性能。</li>
<li>results: 实验结果显示 DAS 方法可以将 VLP 模型的 Computational Complexity 降低到 -11.97% FLOPs，并且与现有的 PETL 方法相比，DAS 方法在参数给数和性能之间能够取得平衡。<details>
<summary>Abstract</summary>
With ever increasing parameters and computation, vision-language pre-trained (VLP) models exhibit prohibitive expenditure in downstream task adaption. Recent endeavors mainly focus on parameter efficient transfer learning (PETL) for VLP models by only updating a small number of parameters. However, excessive computational overhead still plagues the application of VLPs. In this paper, we aim at parameter and computation efficient transfer learning (PCETL) for VLP models. In particular, PCETL not only needs to limit the number of trainable parameters in VLP models, but also to reduce the computational redundancy during inference, thus enabling a more efficient transfer. To approach this target, we propose a novel dynamic architecture skipping (DAS) approach towards effective PCETL. Instead of directly optimizing the intrinsic architectures of VLP models, DAS first observes the significances of their modules to downstream tasks via a reinforcement learning (RL) based process, and then skips the redundant ones with lightweight networks, i.e., adapters, according to the obtained rewards. In this case, the VLP model can well maintain the scale of trainable parameters while speeding up its inference on downstream tasks. To validate DAS, we apply it to two representative VLP models, namely ViLT and METER, and conduct extensive experiments on a bunch of VL tasks. The experimental results not only show the great advantages of DAS in reducing computational complexity, e.g. -11.97% FLOPs of METER on VQA2.0, but also confirm its competitiveness against existing PETL methods in terms of parameter scale and performance. Our source code is given in our appendix.
</details>
<details>
<summary>摘要</summary>
随着参数和计算的增加，视觉语言预训练（VLP）模型在下游任务适应中存在拥堵性问题。现有的努力主要集中在视觉语言预训练（PETL）模型中，通过只更新一小部分参数进行 parameter efficient transfer learning。然而，计算开销仍然困扰着VLP的应用。在这篇论文中，我们目标是在VLP模型中实现参数和计算效率的传输学习（PCETL）。具体来说，PCETL不仅需要限制VLP模型的可训练参数数量，还需要在推理过程中减少计算重复性，以便更有效地进行传输。为达到这个目标，我们提出了一种新的动态architecture skipping（DAS）方法。在DAS方法中，我们首先通过 reinforcement learning（RL）基于的过程来评估VLP模型中各个模块的下游任务意义，然后根据获得的奖励，使用轻量级网络（adapter）将红undeniable模块替换掉。这样，VLP模型可以保持参数数量的扩展性，同时快速完成下游任务的推理。为验证DAS方法，我们在两个代表性的VLP模型，namely ViLT和METER上进行了广泛的实验。实验结果不仅表明DAS方法可以减少计算复杂度，例如METER模型在VQA2.0任务上的计算复杂度减少了11.97%，而且也证明了它在参数数量和性能方面与现有的PETL方法相当竞争。我们的源代码在附录中提供。
</details></li>
</ul>
<hr>
<h2 id="Defect-Detection-in-Synthetic-Fibre-Ropes-using-Detectron2-Framework"><a href="#Defect-Detection-in-Synthetic-Fibre-Ropes-using-Detectron2-Framework" class="headerlink" title="Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework"></a>Defect Detection in Synthetic Fibre Ropes using Detectron2 Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01469">http://arxiv.org/abs/2309.01469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anju Rani, Daniel O. Arroyo, Petar Durdevic</li>
<li>for:  This paper aims to develop an automated and efficient method for detecting defects in synthetic fibre ropes (SFRs) using deep learning (DL) models, specifically the Detectron2 library with Mask R-CNN architecture.</li>
<li>methods: The study uses an experimentally obtained dataset of high-dimensional images of SFRs, with seven damage classes, to train and test Mask R-CNN with various backbone configurations.</li>
<li>results: The use of Detectron2 and Mask R-CNN with different backbone configurations can effectively detect defects in SFRs, enhancing the inspection process and ensuring the safety of the fibre ropes.<details>
<summary>Abstract</summary>
Fibre ropes with the latest technology have emerged as an appealing alternative to steel ropes for offshore industries due to their lightweight and high tensile strength. At the same time, frequent inspection of these ropes is essential to ensure the proper functioning and safety of the entire system. The development of deep learning (DL) models in condition monitoring (CM) applications offers a simpler and more effective approach for defect detection in synthetic fibre ropes (SFRs). The present paper investigates the performance of Detectron2, a state-of-the-art library for defect detection and instance segmentation. Detectron2 with Mask R-CNN architecture is used for segmenting defects in SFRs. Mask R-CNN with various backbone configurations has been trained and tested on an experimentally obtained dataset comprising 1,803 high-dimensional images containing seven damage classes (loop high, loop medium, loop low, compression, core out, abrasion, and normal respectively) for SFRs. By leveraging the capabilities of Detectron2, this study aims to develop an automated and efficient method for detecting defects in SFRs, enhancing the inspection process, and ensuring the safety of the fibre ropes.
</details>
<details>
<summary>摘要</summary>
合成纤维绳（Synthetic Fiber Ropes，SFR）的 Condition Monitoring（CM）应用中，latest technology的纤维绳已经出现为海上工业的吸引力，因为它们具有轻量和高强度特点。同时，为保证整个系统的正常工作和安全，这些纤维绳的常规检查是必须的。在这种情况下，深度学习（Deep Learning，DL）模型在CM应用中的开发提供了一种更加简单和有效的方法来检测SFR中的缺陷。本文 investigate了Detectron2库在SFR中的缺陷检测和实例分割方面的表现。通过使用Mask R-CNN架构，Detectron2在SFR中 segmenting 缺陷。Mask R-CNN采用了不同的背景配置，在实验获得的1,803个高维度图像中进行了7种损害类（循环高、循环中、循环低、压缩、核心缺陷、 Abrasion 和正常）的训练和测试。通过利用Detectron2的能力，本研究旨在开发一种自动化和高效的SFR缺陷检测方法，提高检查过程，并确保纤维绳的安全。
</details></li>
</ul>
<hr>
<h2 id="Toward-Defensive-Letter-Design"><a href="#Toward-Defensive-Letter-Design" class="headerlink" title="Toward Defensive Letter Design"></a>Toward Defensive Letter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01452">http://arxiv.org/abs/2309.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Rentaro Kataoka, Akisato Kimura, Seiichi Uchida</li>
<li>for: 防御 adversarial 攻击</li>
<li>methods: 使用 Iterative Fast Gradient Sign Method (I-FGSM) 和深度回归模型测试字符图像的防御能力，并基于生成 adversarial 网络 (GAN) 提出一种两步方法生成更加防御性的字符图像。</li>
<li>results: 通过测试和实验，提出了一种基于字符图像的防御机制，可以帮助防御 adversarial 攻击。<details>
<summary>Abstract</summary>
A major approach for defending against adversarial attacks aims at controlling only image classifiers to be more resilient, and it does not care about visual objects, such as pandas and cars, in images. This means that visual objects themselves cannot take any defensive actions, and they are still vulnerable to adversarial attacks. In contrast, letters are artificial symbols, and we can freely control their appearance unless losing their readability. In other words, we can make the letters more defensive to the attacks. This paper poses three research questions related to the adversarial vulnerability of letter images: (1) How defensive are the letters against adversarial attacks? (2) Can we estimate how defensive a given letter image is before attacks? (3) Can we control the letter images to be more defensive against adversarial attacks? For answering the first and second questions, we measure the defensibility of letters by employing Iterative Fast Gradient Sign Method (I-FGSM) and then build a deep regression model for estimating the defensibility of each letter image. We also propose a two-step method based on a generative adversarial network (GAN) for generating character images with higher defensibility, which solves the third research question.
</details>
<details>
<summary>摘要</summary>
一种主要方法对抗反对攻击是控制图像分类器更加抗击强，不管图像中的物体，如熊猫和车辆，是否受到攻击。这意味着视觉物体本身无法采取任何防御行动，仍然易受到反对攻击。相比之下，字符是人工符号，我们可以自由地控制它们的显示，只要不导致不可读性。即使在攻击时，我们可以使字符更加抗击强。这篇论文提出了三个研究问题 relacionadas 反对攻击的抗击性：1. 字符对反对攻击的抗击性如何？2. 我们可以在攻击之前对给定的字符图像进行估算，该图像在攻击中的抗击性如何？3. 我们可以通过生成推荐网络（GAN）来生成具有更高抗击性的字符图像，以解决第三个研究问题。为了回答第一个和第二个问题，我们使用迭代快速梯度签名方法（I-FGSM）测量字符的抗击性，并建立深度回归模型来估算每个字符图像的抗击性。我们还提出了一种基于GAN的两步方法，用于生成具有更高抗击性的字符图像，解决第三个研究问题。
</details></li>
</ul>
<hr>
<h2 id="Large-Separable-Kernel-Attention-Rethinking-the-Large-Kernel-Attention-Design-in-CNN"><a href="#Large-Separable-Kernel-Attention-Rethinking-the-Large-Kernel-Attention-Design-in-CNN" class="headerlink" title="Large Separable Kernel Attention: Rethinking the Large Kernel Attention Design in CNN"></a>Large Separable Kernel Attention: Rethinking the Large Kernel Attention Design in CNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01439">http://arxiv.org/abs/2309.01439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kin Wai Lau, Lai-Man Po, Yasar Abbas Ur Rehman</li>
<li>for: 提高vision-based任务中VAN的性能，并降低计算和存储的占用率。</li>
<li>methods: 提出Large Separable Kernel Attention（LSKA）模块，将深度wise convolutional layer中的2D卷积核 decomposition为水平和垂直的1D卷积核，从而避免了额外块的使用。</li>
<li>results: 对VAN、ViTs和ConvNeXt进行了修饰，并在Object recognition、Object detection、Semantic segmentation和Robustness测试中提供了相对较好的性能。在不同的kernel size下，LSKA模块可以减少计算和存储的占用率，并且在Object recognition、Object detection、Semantic segmentation和Robustness测试中具有较好的性能。<details>
<summary>Abstract</summary>
Visual Attention Networks (VAN) with Large Kernel Attention (LKA) modules have been shown to provide remarkable performance, that surpasses Vision Transformers (ViTs), on a range of vision-based tasks. However, the depth-wise convolutional layer in these LKA modules incurs a quadratic increase in the computational and memory footprints with increasing convolutional kernel size. To mitigate these problems and to enable the use of extremely large convolutional kernels in the attention modules of VAN, we propose a family of Large Separable Kernel Attention modules, termed LSKA. LSKA decomposes the 2D convolutional kernel of the depth-wise convolutional layer into cascaded horizontal and vertical 1-D kernels. In contrast to the standard LKA design, the proposed decomposition enables the direct use of the depth-wise convolutional layer with large kernels in the attention module, without requiring any extra blocks. We demonstrate that the proposed LSKA module in VAN can achieve comparable performance with the standard LKA module and incur lower computational complexity and memory footprints. We also find that the proposed LSKA design biases the VAN more toward the shape of the object than the texture with increasing kernel size. Additionally, we benchmark the robustness of the LKA and LSKA in VAN, ViTs, and the recent ConvNeXt on the five corrupted versions of the ImageNet dataset that are largely unexplored in the previous works. Our extensive experimental results show that the proposed LSKA module in VAN provides a significant reduction in computational complexity and memory footprints with increasing kernel size while outperforming ViTs, ConvNeXt, and providing similar performance compared to the LKA module in VAN on object recognition, object detection, semantic segmentation, and robustness tests.
</details>
<details>
<summary>摘要</summary>
视觉注意网络（VAN）配置了大kernel注意模块（LKA）可以提供出色的表现，超过视transformer（ViT），在视觉任务中。然而，深度 wise convolutional层在这些LKA模块中会导致计算和存储空间呈 quadratic 增长，随着核心大小的增加。为了解决这些问题并使用极大的核心大小在VAN中的注意模块中，我们提出了一个家族Large Separable Kernel Attention（LSKA）模块。LSKA将二维核心层的深度wise convolutional层 decomposed into cascaded horizontal和vertical 1-D核心。与标准LKA设计不同，我们的分解方式可以 direct 使用深度wise convolutional层中的大核心在注意模块中，无需额外块。我们的实验结果表明，在VAN中使用我们提出的LSKA模块可以与标准LKA模块相当，同时具有较低的计算复杂度和存储空间占用。此外，我们发现LSKA设计偏向对象的形状，而不是Texture，随着核心大小的增加。此外，我们对VAN、ViTs和最近的ConvNeXt在ImageNet数据集上进行了大规模的robustness测试，发现LSKA模块在对象认知、物体检测、semantic segmentation和Robustness测试中具有优秀的表现。
</details></li>
</ul>
<hr>
<h2 id="DAT-Spatially-Dynamic-Vision-Transformer-with-Deformable-Attention"><a href="#DAT-Spatially-Dynamic-Vision-Transformer-with-Deformable-Attention" class="headerlink" title="DAT++: Spatially Dynamic Vision Transformer with Deformable Attention"></a>DAT++: Spatially Dynamic Vision Transformer with Deformable Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01430">http://arxiv.org/abs/2309.01430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leaplabthu/dat">https://github.com/leaplabthu/dat</a></li>
<li>paper_authors: Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, Gao Huang</li>
<li>for: 这篇论文的目的是提出一种可靠且高效的视觉处理模型，能够解决传统的视觉模型在实现全球注意力和对特定区域的适应能力之间的矛盾。</li>
<li>methods: 这篇论文提出了一种名为“弹性多头注意模组”的新型注意力模块，具有自动分配鉴定点的功能，以实现对应区域的适应注意。这个模组可以与传统的 dense attention 结合，以提高视觉模型的表现力。</li>
<li>results: 根据实验结果，这篇论文的提案DAT++ 能够在多个视觉识别任务上取得顶尖的成绩，包括 ImageNet 的准确率85.9%、COCO 的实例分割精度54.5和47.0，以及 ADE20K 的 semantics 分割精度51.5。<details>
<summary>Abstract</summary>
Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.
</details>
<details>
<summary>摘要</summary>
《 transformers 在视觉任务中表现出色，其大 receive 场能够提供更高的表示力 than CNN 模型。然而，简单地扩大 receive 场也存在一些问题。一方面，使用 dense attention 在 ViT 中会导致额外的内存和计算成本增加，而且特征可能受到 beyond 的无关部分的影响。另一方面，手动设置的 attention 在 PVT 或 Swin Transformer 中可能会限制对长距离关系的模型化。为解决这个 dilemma，我们提出了一种 novel deformable multi-head attention 模块，其中 key 和 value 对在 self-attention 中的位置会在数据依存地分配。这种灵活的方案允许我们的 propose deformable attention 动态关注相关的区域，同时保持 global attention 的表示力。基于这个基础，我们提出了 Deformable Attention Transformer (DAT)，一种通用的视觉基础结构，高效精准地进行视觉识别。此外，我们还提出了 DAT++，一种进一步提高 DAT 的版本。广泛的实验表明，我们的 DAT++ 在多种视觉识别benchmark上达到了最佳结果，其中 ImageNet 准确率为 85.9%，COCO 实例分割 mAP 为 54.5 和 47.0，ADE20K semantic segmentation mIoU 为 51.5。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Segment-Anything-Model-for-Change-Detection-in-HR-Remote-Sensing-Images"><a href="#Adapting-Segment-Anything-Model-for-Change-Detection-in-HR-Remote-Sensing-Images" class="headerlink" title="Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images"></a>Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01429">http://arxiv.org/abs/2309.01429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ggsding/sam-cd">https://github.com/ggsding/sam-cd</a></li>
<li>paper_authors: Lei Ding, Kun Zhu, Daifeng Peng, Hao Tang, Haitao Guo<br>for: 这个研究目的是将视觉基础模型（VFM）应用于高分辨率远程感知图像（RSIs）中的变化探测。methods: 这个研究使用了快速SAM的视觉编码器来提取RS scene中的视觉表现，并使用了一个 convolutional adaptor 来聚合任务化变化信息。此外，这个研究还引入了一个任务无关的 semantic learning branch 来模型RSIs中的semantic latent space。results: 这个研究获得了与顶尖方法相比的更高的准确性，并且展示了与半指导CD方法相似的样本效率学习能力。根据我们所知，这是首次将VFM应用于HR RSIs中的变化探测。<details>
<summary>Abstract</summary>
Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM) allow zero-shot or interactive segmentation of visual contents, thus they are quickly applied in a variety of visual scenes. However, their direct use in many Remote Sensing (RS) applications is often unsatisfactory due to the special imaging characteristics of RS images. In this work, we aim to utilize the strong visual recognition capabilities of VFMs to improve the change detection of high-resolution Remote Sensing Images (RSIs). We employ the visual encoder of FastSAM, an efficient variant of the SAM, to extract visual representations in RS scenes. To adapt FastSAM to focus on some specific ground objects in the RS scenes, we propose a convolutional adaptor to aggregate the task-oriented change information. Moreover, to utilize the semantic representations that are inherent to SAM features, we introduce a task-agnostic semantic learning branch to model the semantic latent in bi-temporal RSIs. The resulting method, SAMCD, obtains superior accuracy compared to the SOTA methods and exhibits a sample-efficient learning ability that is comparable to semi-supervised CD methods. To the best of our knowledge, this is the first work that adapts VFMs for the CD of HR RSIs.
</details>
<details>
<summary>摘要</summary>
各种视觉基础模型（VFM），如分割任何模型（SAM），可以实现零shot或交互分割视觉内容，因此它们很快地应用于多种视觉场景。然而，直接使用它们在许多远程感知（RS）应用中 often 不满足要求，因为RS图像的特殊捕捉特性。在这项工作中，我们希望利用VFM的强大视觉识别能力来改进高分辨率远程感知图像（RSIs）中的变化检测。我们使用 FastSAM 的视觉编码器来提取 RS 场景中的视觉表示。为了使 FastSAM 在RS场景中专注于某些特定的地面 объек，我们提议一种 convolutional adapter 来聚合任务关注的变化信息。此外，我们引入一种任务无关的 semantic learning branch 来模型RSIs中的semantic latent space。得到的方法，SAMCD，与state-of-the-art方法相比，显示出了更高的准确率，并且展现了与 semi-supervised CD 方法类似的样本效率学习能力。到目前为止，这是首次应用 VFM 于高分辨率 RSIs 的变化检测。
</details></li>
</ul>
<hr>
<h2 id="Unified-Pre-training-with-Pseudo-Texts-for-Text-To-Image-Person-Re-identification"><a href="#Unified-Pre-training-with-Pseudo-Texts-for-Text-To-Image-Person-Re-identification" class="headerlink" title="Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification"></a>Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01420">http://arxiv.org/abs/2309.01420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyin Shao, Xinyu Zhang, Changxing Ding, Jian Wang, Jingdong Wang</li>
<li>for: 本研究旨在提高文本到图像人重识别（T2I-ReID）任务的性能，因为存在两种基础问题：数据不一致和训练不一致。</li>
<li>methods: 我们提出了一个新的统一预训策略（UniPT），包括建立大规模的文本标注人像数据集“LUPerson-T”，并使用简单的视觉语言预训策略来对图像和文本模态的特征空间进行Alignment。</li>
<li>results: 我们的UniPT可以在不需要任何辅助工具的情况下达到竞争性的排名1精度（68.50%、60.09%和51.85%）在CUHK-PEDES、ICFG-PEDES和RSTPReid等三个任务上。<details>
<summary>Abstract</summary>
The pre-training task is indispensable for the text-to-image person re-identification (T2I-ReID) task. However, there are two underlying inconsistencies between these two tasks that may impact the performance; i) Data inconsistency. A large domain gap exists between the generic images/texts used in public pre-trained models and the specific person data in the T2I-ReID task. This gap is especially severe for texts, as general textual data are usually unable to describe specific people in fine-grained detail. ii) Training inconsistency. The processes of pre-training of images and texts are independent, despite cross-modality learning being critical to T2I-ReID. To address the above issues, we present a new unified pre-training pipeline (UniPT) designed specifically for the T2I-ReID task. We first build a large-scale text-labeled person dataset "LUPerson-T", in which pseudo-textual descriptions of images are automatically generated by the CLIP paradigm using a divide-conquer-combine strategy. Benefiting from this dataset, we then utilize a simple vision-and-language pre-training framework to explicitly align the feature space of the image and text modalities during pre-training. In this way, the pre-training task and the T2I-ReID task are made consistent with each other on both data and training levels. Without the need for any bells and whistles, our UniPT achieves competitive Rank-1 accuracy of, ie, 68.50%, 60.09%, and 51.85% on CUHK-PEDES, ICFG-PEDES and RSTPReid, respectively. Both the LUPerson-T dataset and code are available at https;//github.com/ZhiyinShao-H/UniPT.
</details>
<details>
<summary>摘要</summary>
“预训作业是文本识别人重识别（T2I-ReID）任务的不可或缺的一部分。然而，有两个隐含的差异可能影响性能；一是数据不一致。 generic images/texts在公共预训模型中使用的大频谱与特定人数据在T2I-ReID任务中存在巨大的频谱差异。这种差异特别是对文本而言，通常文本数据无法细化特定人的描述。二是训练不一致。图像和文本的预训过程是独立的，尽管交叉模态学习是T2I-ReID任务中 krit。为解决以上问题，我们提出了一个新的一体化预训管线（UniPT），专门为T2I-ReID任务设计。我们首先建立了大规模的文本标注人数据集"LUPerson-T",其中图像中的文本描述使用CLIP парадигмы自动生成pseudo文本描述。利用这个数据集，我们然后使用简单的视觉语言预训框架，在预训过程中显式对图像和文本模式之间的特征空间进行对接。这样，预训任务和T2I-ReID任务在数据和训练水平上得到了一致。不需要任何附加功能，我们的UniPT实现了竞争力强的排名1准确率，即68.50%、60.09%和51.85%在CUHK-PEDES、ICFG-PEDES和RSTPReid等三个任务中。LUPerson-T数据集和代码都可以在https://github.com/ZhiyinShao-H/UniPT上获取。”
</details></li>
</ul>
<hr>
<h2 id="Implicit-Neural-Image-Stitching-With-Enhanced-and-Blended-Feature-Reconstruction"><a href="#Implicit-Neural-Image-Stitching-With-Enhanced-and-Blended-Feature-Reconstruction" class="headerlink" title="Implicit Neural Image Stitching With Enhanced and Blended Feature Reconstruction"></a>Implicit Neural Image Stitching With Enhanced and Blended Feature Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01409">http://arxiv.org/abs/2309.01409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minshu-kim/NIS">https://github.com/minshu-kim/NIS</a></li>
<li>paper_authors: Minsu Kim, Jaewon Lee, Byeonghun Lee, Sunghoon Im, Kyong Hwan Jin</li>
<li>for: 提高图像拼接的质量和精度，解决传统框架中的锐利artefacts和照明、深度等级的不一致问题。</li>
<li>methods: 基于隐藏层 neural network 的图像拼接方法，通过估计图像的福洛coefficients来提高图像质量，并在幂值空间进行颜色匹配和重差调整，最终decode为RGB值得拼接图像。</li>
<li>results: 比传统框架更高效地解决低分辨率图像拼接问题，并且可以融合加速图像提高方法，实现更高质量的拼接图像。<details>
<summary>Abstract</summary>
Existing frameworks for image stitching often provide visually reasonable stitchings. However, they suffer from blurry artifacts and disparities in illumination, depth level, etc. Although the recent learning-based stitchings relax such disparities, the required methods impose sacrifice of image qualities failing to capture high-frequency details for stitched images. To address the problem, we propose a novel approach, implicit Neural Image Stitching (NIS) that extends arbitrary-scale super-resolution. Our method estimates Fourier coefficients of images for quality-enhancing warps. Then, the suggested model blends color mismatches and misalignment in the latent space and decodes the features into RGB values of stitched images. Our experiments show that our approach achieves improvement in resolving the low-definition imaging of the previous deep image stitching with favorable accelerated image-enhancing methods. Our source code is available at https://github.com/minshu-kim/NIS.
</details>
<details>
<summary>摘要</summary>
现有的图像拼接框架通常提供可见的合理拼接结果，但它们受到锐化缺陷和照明、深度等因素的影响，导致拼接图像具有模糊效果。虽然最近的学习型拼接方法可以减轻这些缺陷，但它们需要牺牲图像质量，无法捕捉高频环境的细节。为解决这问题，我们提出了一种新的方法：隐式神经图像拼接（NIS），它扩展了自适应超分辨率。我们的方法估算图像的快推函数，然后使用建议的模型在幽DefaultsLatent空间进行混合和调整，最后 decode到RGB值来生成拼接图像。我们的实验表明，我们的方法可以提高前期深度图像拼接的低分辨率问题，并且可以利用加速的图像改进方法来加速图像进行改进。我们的源代码可以在 GitHub 上找到：https://github.com/minshu-kim/NIS。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design"><a href="#Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design" class="headerlink" title="Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design"></a>Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01408">http://arxiv.org/abs/2309.01408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Engel, Leon Sick, Timo Ropinski</li>
<li>for: 用于量 Rendering 中的结构分类和质量属性分配</li>
<li>methods: 使用自然语言描述的自适应 Transfer Function 定义方法</li>
<li>results: 减少标注量和计算时间，提高分割精度和用户体验<details>
<summary>Abstract</summary>
In volume rendering, transfer functions are used to classify structures of interest, and to assign optical properties such as color and opacity. They are commonly defined as 1D or 2D functions that map simple features to these optical properties. As the process of designing a transfer function is typically tedious and unintuitive, several approaches have been proposed for their interactive specification. In this paper, we present a novel method to define transfer functions for volume rendering by leveraging the feature extraction capabilities of self-supervised pre-trained vision transformers. To design a transfer function, users simply select the structures of interest in a slice viewer, and our method automatically selects similar structures based on the high-level features extracted by the neural network. Contrary to previous learning-based transfer function approaches, our method does not require training of models and allows for quick inference, enabling an interactive exploration of the volume data. Our approach reduces the amount of necessary annotations by interactively informing the user about the current classification, so they can focus on annotating the structures of interest that still require annotation. In practice, this allows users to design transfer functions within seconds, instead of minutes. We compare our method to existing learning-based approaches in terms of annotation and compute time, as well as with respect to segmentation accuracy. Our accompanying video showcases the interactivity and effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
在量rendering中，传输函数用于分类结构物体，并将光学性质如颜色和透明度分配给这些结构物体。传输函数通常是1D或2D函数，它们将简单特征映射到这些光学性质。由于设计传输函数的过程通常是慢搬和不直观的，因此有几种方法被提议用于其交互式规定。在本文中，我们提出了一种使用自然语言处理器来定义传输函数的新方法。用户只需在切片查看器中选择结构物体，我们的方法会自动选择与结构物体相似的结构，基于由神经网络提取的高级特征。与之前的学习基于的传输函数方法不同，我们的方法不需要训练模型，可以快速进行推理，从而允许用户在数秒钟内设计传输函数，而不是需要数分钟。此外，我们的方法可以减少必须的注释量，通过在用户操作时提供反馈，使用户能够更快地注释需要注释的结构物体。在实践中，我们的方法比既有学习基于的方法更快，更准确。我们的视频辑演示了我们的方法的互动性和效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-Residual-Elastic-Warps-for-Image-Stitching-under-Dirichlet-Boundary-Condition"><a href="#Learning-Residual-Elastic-Warps-for-Image-Stitching-under-Dirichlet-Boundary-Condition" class="headerlink" title="Learning Residual Elastic Warps for Image Stitching under Dirichlet Boundary Condition"></a>Learning Residual Elastic Warps for Image Stitching under Dirichlet Boundary Condition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01406">http://arxiv.org/abs/2309.01406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minshu-kim/REwarp">https://github.com/minshu-kim/REwarp</a></li>
<li>paper_authors: Minsu Kim, Yongjun Lee, Woo Kyoung Han, Kyong Hwan Jin</li>
<li>for: 解决深度学习图像拼接中大偏移误差所导致的缺陷，提高图像拼接的精度和效率。</li>
<li>methods: 使用 Dirichlet 边界条件和循环学习减少误差，预测homography和Thin-plate Spline（TPS）来实现缺陷和孔洞自适应拼接。</li>
<li>results: 在实验中，REwarp 表现出了优于现有方法的精度和计算效率，能够提供高质量的图像拼接 results。<details>
<summary>Abstract</summary>
Trendy suggestions for learning-based elastic warps enable the deep image stitchings to align images exposed to large parallax errors. Despite the remarkable alignments, the methods struggle with occasional holes or discontinuity between overlapping and non-overlapping regions of a target image as the applied training strategy mostly focuses on overlap region alignment. As a result, they require additional modules such as seam finder and image inpainting for hiding discontinuity and filling holes, respectively. In this work, we suggest Recurrent Elastic Warps (REwarp) that address the problem with Dirichlet boundary condition and boost performances by residual learning for recurrent misalign correction. Specifically, REwarp predicts a homography and a Thin-plate Spline (TPS) under the boundary constraint for discontinuity and hole-free image stitching. Our experiments show the favorable aligns and the competitive computational costs of REwarp compared to the existing stitching methods. Our source code is available at https://github.com/minshu-kim/REwarp.
</details>
<details>
<summary>摘要</summary>
当前建议：学习基于弹性折叠的方法可以使深度图像融合到大量偏差错误中进行对齐。尽管这些方法能够实现出色的对齐，但它们在处理重叠和非重叠区域之间的缺陷和缺口问题上却陷入困难。因此，它们通常需要额外的模块，如缺陷找到器和图像填充，以隐藏缺陷和填充缺口。在这种情况下，我们建议使用循环弹性折叠（REwarp）方法，该方法通过 Dirichlet 边界条件和循环弹性学习来解决缺陷和缺口问题，从而实现不间断和缺陷自适应的图像融合。我们的实验表明，REwarp 方法可以具有优秀的对齐性和竞争性的计算成本。我们的源代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="SSVOD-Semi-Supervised-Video-Object-Detection-with-Sparse-Annotations"><a href="#SSVOD-Semi-Supervised-Video-Object-Detection-with-Sparse-Annotations" class="headerlink" title="SSVOD: Semi-Supervised Video Object Detection with Sparse Annotations"></a>SSVOD: Semi-Supervised Video Object Detection with Sparse Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01391">http://arxiv.org/abs/2309.01391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Mahmud, Chun-Hao Liu, Burhaneddin Yaman, Diana Marculescu</li>
<li>for: 这篇论文是为了提出一种基于semi-supervised learning的视频对象检测方法，以解决现有视频对象检测方法所存在的一些问题，例如需要大量注释框架来实现良好的监督学习效果。</li>
<li>methods: 这篇论文使用了一种基于流动的策略，即使用流动的预测来选择合适的 pseudo-labels，以便在大量无注释框架上进行学习。具体来说，这篇论文引入了两种选择方法：一种是基于流动的预测和匹配的方法，另一种是基于交叉 IoU 和交叉异同度的方法。</li>
<li>results: 根据论文的结果，这种 semi-supervised 视频对象检测方法可以在 ImageNet-VID、Epic-KITCHENS 和 YouTube-VIS 等 datasets 上达到显著的性能改进，比如在 ImageNet-VID 上的 mAP 提高了 10.3%，在 Epic-KITCHENS 上的 mAP 提高了 13.1%，在 YouTube-VIS 上的 mAP 提高了 11.4%。<details>
<summary>Abstract</summary>
Despite significant progress in semi-supervised learning for image object detection, several key issues are yet to be addressed for video object detection: (1) Achieving good performance for supervised video object detection greatly depends on the availability of annotated frames. (2) Despite having large inter-frame correlations in a video, collecting annotations for a large number of frames per video is expensive, time-consuming, and often redundant. (3) Existing semi-supervised techniques on static images can hardly exploit the temporal motion dynamics inherently present in videos. In this paper, we introduce SSVOD, an end-to-end semi-supervised video object detection framework that exploits motion dynamics of videos to utilize large-scale unlabeled frames with sparse annotations. To selectively assemble robust pseudo-labels across groups of frames, we introduce \textit{flow-warped predictions} from nearby frames for temporal-consistency estimation. In particular, we introduce cross-IoU and cross-divergence based selection methods over a set of estimated predictions to include robust pseudo-labels for bounding boxes and class labels, respectively. To strike a balance between confirmation bias and uncertainty noise in pseudo-labels, we propose confidence threshold based combination of hard and soft pseudo-labels. Our method achieves significant performance improvements over existing methods on ImageNet-VID, Epic-KITCHENS, and YouTube-VIS datasets. Code and pre-trained models will be released.
</details>
<details>
<summary>摘要</summary>
尽管在半监督学习方面已经取得了显著的进步，但视频对象检测中仍有一些关键问题需要解决：（1）在视频中达到良好的性能需要大量的注释帧。（2）即使在视频中存在大量的间隔帧相互关系，仍然收集注释帧的成本高、时间费时、重复的问题。（3）现有的半监督技术在静止图像上基本无法利用视频中的时间动态。本文介绍SSVOD，一种终端到终点的半监督视频对象检测框架，利用视频中的时间动态来利用大量的无注释帧。为了选择性地组合坚实的pseudo标签，我们引入了流动抗压采用邻近帧的预测值进行时间一致性估计。具体来说，我们引入了交叉IoU和交叉异常值基于选择方法，以包括坚实的pseudo标签 для bounding box和类别标签。为了保持pseudo标签中的确认偏见和不确定噪音的平衡，我们提议使用信任值阈值基于组合硬化和软化pseudo标签。我们的方法在ImageNet-VID、Epic-KITCHENS和YouTube-VIS数据集上取得了显著的性能提升。我们将代码和预训练模型发布。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Video-Scenes-through-Text-Insights-from-Text-based-Video-Question-Answering"><a href="#Understanding-Video-Scenes-through-Text-Insights-from-Text-based-Video-Question-Answering" class="headerlink" title="Understanding Video Scenes through Text: Insights from Text-based Video Question Answering"></a>Understanding Video Scenes through Text: Insights from Text-based Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01380">http://arxiv.org/abs/2309.01380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, C. V. Jawahar</li>
<li>for: 本研究探讨了两个新引入的 dataset，NewsVideoQA 和 M4-ViteVQA，用于基于文本内容回答视频问题。</li>
<li>methods: 研究者使用了 BERT-QA，一种文本只模型，对这两个 dataset 进行了实验，并发现它在这两个 dataset 上 display 相似的表现。</li>
<li>results: 研究发现，训练在 M4-ViteVQA 上并不能 directly apply to NewsVideoQA，且对于 out-of-domain 训练，需要进行适应。<details>
<summary>Abstract</summary>
Researchers have extensively studied the field of vision and language, discovering that both visual and textual content is crucial for understanding scenes effectively. Particularly, comprehending text in videos holds great significance, requiring both scene text understanding and temporal reasoning. This paper focuses on exploring two recently introduced datasets, NewsVideoQA and M4-ViteVQA, which aim to address video question answering based on textual content. The NewsVideoQA dataset contains question-answer pairs related to the text in news videos, while M4-ViteVQA comprises question-answer pairs from diverse categories like vlogging, traveling, and shopping. We provide an analysis of the formulation of these datasets on various levels, exploring the degree of visual understanding and multi-frame comprehension required for answering the questions. Additionally, the study includes experimentation with BERT-QA, a text-only model, which demonstrates comparable performance to the original methods on both datasets, indicating the shortcomings in the formulation of these datasets. Furthermore, we also look into the domain adaptation aspect by examining the effectiveness of training on M4-ViteVQA and evaluating on NewsVideoQA and vice-versa, thereby shedding light on the challenges and potential benefits of out-of-domain training.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:研究人员对视觉和语言领域进行了广泛的研究，发现视觉和文本内容都是理解场景的关键因素。特别是在视频中理解文本内容的重要性，需要场景文本理解和时间理解。本文关注两个最近引入的dataset，新闻视频问答集和M4-ViteVQA集，以解决基于文本内容的视频问答问题。新闻视频问答集包含新闻视频中文本内容相关的问题答案对，而M4-ViteVQA集包含多种类别的问题答案对，如博客、旅行和购物。我们对这些dataset的形式化进行了多种层次的分析，探讨响应问题需要的视觉理解和多帧理解水平。此外，我们还进行了BERT-QA模型的实验，这是一种只有文本内容的模型，它在这两个dataset上达到了相当的性能，表明这些dataset的形式化存在缺陷。此外，我们还研究了域适应问题，包括在M4-ViteVQA集上训练并在新闻视频问答集上测试的效果，以及 vice versa，从而探讨域外训练的挑战和优点。
</details></li>
</ul>
<hr>
<h2 id="ImmersiveNeRF-Hybrid-Radiance-Fields-for-Unbounded-Immersive-Light-Field-Reconstruction"><a href="#ImmersiveNeRF-Hybrid-Radiance-Fields-for-Unbounded-Immersive-Light-Field-Reconstruction" class="headerlink" title="ImmersiveNeRF: Hybrid Radiance Fields for Unbounded Immersive Light Field Reconstruction"></a>ImmersiveNeRF: Hybrid Radiance Fields for Unbounded Immersive Light Field Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01374">http://arxiv.org/abs/2309.01374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohang Yu, Haoxiang Wang, Yuqi Han, Lei Yang, Tao Yu, Qionghai Dai<br>for: This paper proposes a method for unbounded immersive light field reconstruction, which supports high-quality rendering and aggressive view extrapolation.methods: The method uses a hybrid radiance field representation, with separate radiance fields for the foreground and background, and adaptive sampling and segmentation regularization to improve performance.results: The proposed method achieves strong performance for unbounded immersive light field reconstruction, and contributes a novel dataset for further research and applications in the immersive light field domain.Here’s the text in Simplified Chinese:for: 这篇论文提出了一种用于无限维度吸引光场重建的方法，支持高质量渲染和较为侵略性的视角推导。methods: 该方法使用了混合的光场场景表示，将前景和背景分别表示为两个不同的空间映射策略，并使用了适应性的采样和分割规则来提高性能。results: 提出的方法在无限维度吸引光场重建中实现了强大的表现，并为未来的研究和应用在吸引光场领域提供了一个新的数据集。<details>
<summary>Abstract</summary>
This paper proposes a hybrid radiance field representation for unbounded immersive light field reconstruction which supports high-quality rendering and aggressive view extrapolation. The key idea is to first formally separate the foreground and the background and then adaptively balance learning of them during the training process. To fulfill this goal, we represent the foreground and background as two separate radiance fields with two different spatial mapping strategies. We further propose an adaptive sampling strategy and a segmentation regularizer for more clear segmentation and robust convergence. Finally, we contribute a novel immersive light field dataset, named THUImmersive, with the potential to achieve much larger space 6DoF immersive rendering effects compared with existing datasets, by capturing multiple neighboring viewpoints for the same scene, to stimulate the research and AR/VR applications in the immersive light field domain. Extensive experiments demonstrate the strong performance of our method for unbounded immersive light field reconstruction.
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种混合辐射场表示方法，用于无限尺度吸引辐射场重建，支持高质量渲染和激进视点推演。关键思想是首先正式分离背景和前景，然后在训练过程中适应地学习它们。为达到这个目标，我们将背景和前景表示为两个不同的辐射场，使用两种不同的空间映射策略。我们还提出了一种适应 sampling 策略和一种分割规范，以实现更清晰的分割和更稳定的收敛。最后，我们发布了一个新的 immerse 辐射场数据集，名为 THUImmersive，它可以实现更大的空间 six-degree-of-freedom 吸引辐射渲染效果，比现有数据集更大，通过记录同一场景的多个邻居视点，刺激研究和 AR/VR 应用在 immerse 辐射场领域。广泛的实验表明我们的方法在无限尺度吸引辐射场重建中具有强大表现。
</details></li>
</ul>
<hr>
<h2 id="DiverseMotion-Towards-Diverse-Human-Motion-Generation-via-Discrete-Diffusion"><a href="#DiverseMotion-Towards-Diverse-Human-Motion-Generation-via-Discrete-Diffusion" class="headerlink" title="DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion"></a>DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01372">http://arxiv.org/abs/2309.01372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/axdfhj/mdd">https://github.com/axdfhj/mdd</a></li>
<li>paper_authors: Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, Yi Yang</li>
<li>for: 这个论文的目的是生成基于文本描述的高质量人体动作，同时保持动作多样性。</li>
<li>methods: 该论文使用了一种新的方法，即 Hierarchical Semantic Aggregation (HSA) 模块和 Motion Discrete Diffusion (MDD) 框架，以确保动作质量和多样性之间的平衡。</li>
<li>results: 该论文通过实验证明，其方法可以在 HumanML3D 和 KIT-ML 上达到 state-of-the-art 的动作质量和竞争力动作多样性。<details>
<summary>Abstract</summary>
We present DiverseMotion, a new approach for synthesizing high-quality human motions conditioned on textual descriptions while preserving motion diversity.Despite the recent significant process in text-based human motion generation,existing methods often prioritize fitting training motions at the expense of action diversity. Consequently, striking a balance between motion quality and diversity remains an unresolved challenge. This problem is compounded by two key factors: 1) the lack of diversity in motion-caption pairs in existing benchmarks and 2) the unilateral and biased semantic understanding of the text prompt, focusing primarily on the verb component while neglecting the nuanced distinctions indicated by other words.In response to the first issue, we construct a large-scale Wild Motion-Caption dataset (WMC) to extend the restricted action boundary of existing well-annotated datasets, enabling the learning of diverse motions through a more extensive range of actions. To this end, a motion BLIP is trained upon a pretrained vision-language model, then we automatically generate diverse motion captions for the collected motion sequences. As a result, we finally build a dataset comprising 8,888 motions coupled with 141k text.To comprehensively understand the text command, we propose a Hierarchical Semantic Aggregation (HSA) module to capture the fine-grained semantics.Finally,we involve the above two designs into an effective Motion Discrete Diffusion (MDD) framework to strike a balance between motion quality and diversity. Extensive experiments on HumanML3D and KIT-ML show that our DiverseMotion achieves the state-of-the-art motion quality and competitive motion diversity. Dataset, code, and pretrained models will be released to reproduce all of our results.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的方法——多样化动作（DiverseMotion），可以生成高质量的人体动作，基于文本描述而conditioning，同时保持动作多样性。尽管最近有 significative progress in text-based human motion generation,但现有方法通常会偏好适应训练动作，而忽略动作多样性。这种问题被两个关键因素困扰：1）现有的动作-caption对不够多样化，2）文本提示的semantic理解偏执一面，即便只重视verb部分，而忽略其他词语的细微差别。为了解决第一个问题，我们构建了一个大规模的野动作-caption数据集（WMC），以扩展现有的动作边界，让学习动作的多样性。为此，我们首先训练了一个动作BLIP在一个预训练的视力语言模型上，然后自动生成了多样的动作caption。最终，我们建立了一个包含8,888个动作和141,000个文本的数据集。为了全面理解文本命令，我们提出了一个层次semantic汇集（HSA）模块，以捕捉细节semantic。最后，我们将上述两种设计 integrate into an effective Motion Discrete Diffusion（MDD）框架，以平衡动作质量和多样性。我们的多样化动作在HumanML3D和KIT-ML上进行了广泛的实验，并达到了状态之arte motion质量和竞争力动作多样性。数据集、代码和预训练模型将被发布，以便复制所有我们的结果。
</details></li>
</ul>
<hr>
<h2 id="Attention-as-Annotation-Generating-Images-and-Pseudo-masks-for-Weakly-Supervised-Semantic-Segmentation-with-Diffusion"><a href="#Attention-as-Annotation-Generating-Images-and-Pseudo-masks-for-Weakly-Supervised-Semantic-Segmentation-with-Diffusion" class="headerlink" title="Attention as Annotation: Generating Images and Pseudo-masks for Weakly Supervised Semantic Segmentation with Diffusion"></a>Attention as Annotation: Generating Images and Pseudo-masks for Weakly Supervised Semantic Segmentation with Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01369">http://arxiv.org/abs/2309.01369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryota Yoshihashi, Yuya Otsuka, Kenji Doi, Tomohiro Tanaka<br>for:* 这个论文的目的是提出一种没有使用真实图像和手动标注的Semantic segmentation训练方法。methods:* 该方法使用文本到图像扩散模型生成的图像，并使用图像的内部文本到图像十字关注作为监督 Pseudo-mask。results:* 实验表明，attn2mask可以在PASCAL VOC中取得良好的结果，而不需要使用真实的训练数据。* 该方法还能够扩展到更多类别的场景，如ImageNet segmentation。* 它还显示了对LoRA基于的细化调整的适应能力，可以将segmenation转移到远程领域，如Cityscapes。<details>
<summary>Abstract</summary>
Although recent advancements in diffusion models enabled high-fidelity and diverse image generation, training of discriminative models largely depends on collections of massive real images and their manual annotation. Here, we present a training method for semantic segmentation that neither relies on real images nor manual annotation. The proposed method {\it attn2mask} utilizes images generated by a text-to-image diffusion model in combination with its internal text-to-image cross-attention as supervisory pseudo-masks. Since the text-to-image generator is trained with image-caption pairs but without pixel-wise labels, attn2mask can be regarded as a weakly supervised segmentation method overall. Experiments show that attn2mask achieves promising results in PASCAL VOC for not using real training data for segmentation at all, and it is also useful to scale up segmentation to a more-class scenario, i.e., ImageNet segmentation. It also shows adaptation ability with LoRA-based fine-tuning, which enables the transfer to a distant domain i.e., Cityscapes.
</details>
<details>
<summary>摘要</summary>
尽管最近的扩散模型可以生成高精度和多样的图像，但训练推理模型大多依赖于庞大的真实图像和手动注释。在这里，我们提出了一种不需要真实图像和手动注释的 semantic segmentation 训练方法。我们称之为“attn2mask”，它利用由文本到图像扩散模型生成的图像，并与其内部的文本到图像交叉注意力作为超级vision pseudo-mask。由于文本到图像生成器在没有像素级标注的情况下训练，可以视为一种弱supervised segmentation方法。实验表明，attn2mask 在 PASCAL VOC 上表现出色，不使用实际训练数据进行 segmentation 的情况下，并且在更多类enario中也表现出了好的扩展能力。它还表现出了 LoRA-based fine-tuning 的适应能力，可以在远程领域 i.e., Cityscapes 中进行转移。
</details></li>
</ul>
<hr>
<h2 id="High-Frequency-High-Accuracy-Pointing-onboard-Nanosats-using-Neuromorphic-Event-Sensing-and-Piezoelectric-Actuation"><a href="#High-Frequency-High-Accuracy-Pointing-onboard-Nanosats-using-Neuromorphic-Event-Sensing-and-Piezoelectric-Actuation" class="headerlink" title="High Frequency, High Accuracy Pointing onboard Nanosats using Neuromorphic Event Sensing and Piezoelectric Actuation"></a>High Frequency, High Accuracy Pointing onboard Nanosats using Neuromorphic Event Sensing and Piezoelectric Actuation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01361">http://arxiv.org/abs/2309.01361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasir Latif, Peter Anastasiou, Yonhon Ng, Zebb Prime, Tien-Fu Lu, Matthew Tetlow, Robert Mahony, Tat-Jun Chin</li>
<li>for: 这个论文旨在提高小型卫星的稳定点击精度，以便为空间域意识任务（SDA）提供更高精度的点击。</li>
<li>methods: 该论文提出了一种新的 payload，它利用 neuromorphic event sensor 和 piezoelectric stage 实现高精度和高频率的相对位态估计和控制。</li>
<li>results:  experiments 表明，使用该 payload 可以在 1-5 度的精度下实现稳定点击，并且可以在 50Hz 的操作频率下运行。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
As satellites become smaller, the ability to maintain stable pointing decreases as external forces acting on the satellite come into play. At the same time, reaction wheels used in the attitude determination and control system (ADCS) introduce high frequency jitter which can disrupt pointing stability. For space domain awareness (SDA) tasks that track objects tens of thousands of kilometres away, the pointing accuracy offered by current nanosats, typically in the range of 10 to 100 arcseconds, is not sufficient. In this work, we develop a novel payload that utilises a neuromorphic event sensor (for high frequency and highly accurate relative attitude estimation) paired in a closed loop with a piezoelectric stage (for active attitude corrections) to provide highly stable sensor-specific pointing. Event sensors are especially suited for space applications due to their desirable characteristics of low power consumption, asynchronous operation, and high dynamic range. We use the event sensor to first estimate a reference background star field from which instantaneous relative attitude is estimated at high frequency. The piezoelectric stage works in a closed control loop with the event sensor to perform attitude corrections based on the discrepancy between the current and desired attitude. Results in a controlled setting show that we can achieve a pointing accuracy in the range of 1-5 arcseconds using our novel payload at an operating frequency of up to 50Hz using a prototype built from commercial-off-the-shelf components. Further details can be found at https://ylatif.github.io/ultrafinestabilisation
</details>
<details>
<summary>摘要</summary>
为了提高小型卫星的稳定性，我们开发了一种新的 payload，它使用神经元事件传感器（高频和高精度相对姿态估计）和一个 piezoelectric stage（用于活动姿态 corrections）。事件传感器在空间应用中特别有利，因为它们具有低功耗、异步操作和高动态范围等极佳特点。我们使用事件传感器来估计背景星场，并根据差异来使用 piezoelectric stage 进行姿态 corrections。在控制的环境中，我们可以使用我们的新型payload实现1-5弧矩度精度的指向稳定，并且可以在50Hz的运行频率下达到这个精度。更多细节可以在https://ylatif.github.io/ultrafinestabilisation 查看。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Classifiers-To-Changing-Class-Priors-During-Deployment"><a href="#Adapting-Classifiers-To-Changing-Class-Priors-During-Deployment" class="headerlink" title="Adapting Classifiers To Changing Class Priors During Deployment"></a>Adapting Classifiers To Changing Class Priors During Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01357">http://arxiv.org/abs/2309.01357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natnael Daba, Bruce McIntosh, Abhijit Mahalanobis</li>
<li>for: 这篇论文是关于如何在不同的部署场景中使用通用分类器的。</li>
<li>methods: 论文使用了基于分类器自身输出的方法来估算类偏好。</li>
<li>results: 结果表明，在部署场景中 incorporating 估算的类偏好可以使分类器在运行时准确率提高。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Conventional classifiers are trained and evaluated using balanced data sets in which all classes are equally present. Classifiers are now trained on large data sets such as ImageNet, and are now able to classify hundreds (if not thousands) of different classes. On one hand, it is desirable to train such general-purpose classifier on a very large number of classes so that it performs well regardless of the settings in which it is deployed. On the other hand, it is unlikely that all classes known to the classifier will occur in every deployment scenario, or that they will occur with the same prior probability. In reality, only a relatively small subset of the known classes may be present in a particular setting or environment. For example, a classifier will encounter mostly animals if its deployed in a zoo or for monitoring wildlife, aircraft and service vehicles at an airport, or various types of automobiles and commercial vehicles if it is used for monitoring traffic. Furthermore, the exact class priors are generally unknown and can vary over time. In this paper, we explore different methods for estimating the class priors based on the output of the classifier itself. We then show that incorporating the estimated class priors in the overall decision scheme enables the classifier to increase its run-time accuracy in the context of its deployment scenario.
</details>
<details>
<summary>摘要</summary>
传统的分类器通常在具有平衡数据集的情况下训练和评估。现在，分类器被训练在大量数据集如ImageNet上，能够分类百计以上不同的类别。一方面，悉心地训练这种通用分类器，以便它在不同的部署enario中都能表现出色。另一方面，实际情况下，分类器可能只会遇到部分已知的类别，而且这些类别的发生概率可能不同，甚至随着时间的推移而变化。例如，如果把分类器部署在动物园或野生动物监测中，它就会遇到大量的动物类别。 similarly， if it is used for monitoring traffic, it will encounter mostly automobiles and commercial vehicles. 为了解决这个问题，我们在这篇论文中研究了不同的方法来估算类别概率，基于分类器的输出。然后，我们表明，在部署scenario中，通过包含估算后的类别概率在总决策方案中，可以使分类器在运行时准确性提高。
</details></li>
</ul>
<hr>
<h2 id="Real-time-pedestrian-recognition-on-low-computational-resources"><a href="#Real-time-pedestrian-recognition-on-low-computational-resources" class="headerlink" title="Real-time pedestrian recognition on low computational resources"></a>Real-time pedestrian recognition on low computational resources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01353">http://arxiv.org/abs/2309.01353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guifan Weng</li>
<li>for: 这篇文章的目的是实现实时行人识别在小型移动设备上，以提高安全性和自动驾驶等应用的效能。</li>
<li>methods: 这篇文章使用了三种方法来实现实时行人识别，包括提高了本地二进制特征和 AdaBoost 分类器、优化了几何特征和支持向量机制、以及实现了快速梯度下降神经网络。</li>
<li>results: 这篇文章的结果显示了三种方法可以在小型物理设备上实现实时行人识别，并且获得了高于95%的准确率和高于5 fps的速度。这些方法可以轻松地应用到小型移动设备上，并且具有高相容性和通用性。<details>
<summary>Abstract</summary>
Pedestrian recognition has successfully been applied to security, autonomous cars, Aerial photographs. For most applications, pedestrian recognition on small mobile devices is important. However, the limitations of the computing hardware make this a challenging task. In this work, we investigate real-time pedestrian recognition on small physical-size computers with low computational resources for faster speed. This paper presents three methods that work on the small physical size CPUs system. First, we improved the Local Binary Pattern (LBP) features and Adaboost classifier. Second, we optimized the Histogram of Oriented Gradients (HOG) and Support Vector Machine. Third, We implemented fast Convolutional Neural Networks (CNNs). The results demonstrate that the three methods achieved real-time pedestrian recognition at an accuracy of more than 95% and a speed of more than 5 fps on a small physical size computational platform with a 1.8 GHz Intel i5 CPU. Our methods can be easily applied to small mobile devices with high compatibility and generality.
</details>
<details>
<summary>摘要</summary>
人体识别已成功应用于安全、自动驾驶、航空图像等领域。大多数应用中，人体识别在小型移动设备上是非常重要。然而，计算硬件的限制使得这成为一项挑战。在这项工作中，我们调查了小型物理尺寸计算机上的实时人体识别方法。本文提出了三种方法，它们在小型物理尺寸计算机系统上实现了实时人体识别，并且具有高准确率和快速速度。首先，我们改进了本地二进制特征（LBP）和权重融合分类器。其次，我们优化了梯度图 histogram 和支持向量机。最后，我们实现了快速的卷积神经网络（CNNs）。结果表明，三种方法在一个小型物理尺寸计算平台上实现了实时人体识别，准确率高于 95%，速度高于 5 fps。我们的方法可以轻松应用于小型移动设备，具有高兼容性和通用性。
</details></li>
</ul>
<hr>
<h2 id="Adv3D-Generating-3D-Adversarial-Examples-in-Driving-Scenarios-with-NeRF"><a href="#Adv3D-Generating-3D-Adversarial-Examples-in-Driving-Scenarios-with-NeRF" class="headerlink" title="Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF"></a>Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01351">http://arxiv.org/abs/2309.01351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leheng Li, Qing Lian, Ying-Cong Chen</li>
<li>for: 这个研究旨在测试深度神经网络（DNNs）对于恶作剧示例的敏感性，并且针对基于DNN的自动驾驶架构（i.e., 3D物体探测）。</li>
<li>methods: 这个研究使用了模型恶作剧示例为Neural Radiance Fields（NeRFs），并且提出了primitive-aware sampling和semantic-guided regularization以生成可能的恶作剧示例。</li>
<li>results: 实验结果显示，训练了恶作剧NeRF可以对不同的 pose、scene 和3D探测器进行大规模的性能降低。此外，这个研究也提出了一种防御方法，即通过数据增强训练来防止这些攻击。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have been proven extremely susceptible to adversarial examples, which raises special safety-critical concerns for DNN-based autonomous driving stacks (i.e., 3D object detection). Although there are extensive works on image-level attacks, most are restricted to 2D pixel spaces, and such attacks are not always physically realistic in our 3D world. Here we present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic appearances and 3D accurate generation, yielding a more realistic and realizable adversarial example. We train our adversarial NeRF by minimizing the surrounding objects' confidence predicted by 3D detectors on the training set. Then we evaluate Adv3D on the unseen validation set and show that it can cause a large performance reduction when rendering NeRF in any sampled pose. To generate physically realizable adversarial examples, we propose primitive-aware sampling and semantic-guided regularization that enable 3D patch attacks with camouflage adversarial texture. Experimental results demonstrate that the trained adversarial NeRF generalizes well to different poses, scenes, and 3D detectors. Finally, we provide a defense method to our attacks that involves adversarial training through data augmentation. Project page: https://len-li.github.io/adv3d-web
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）已经被证明非常易受到敌意示例的影响，这引发了特别的安全关注，特别是在基于DNN的自动驾驶堆栈中（即3D对象检测）。虽然有大量的图像级别攻击工作，但大多数都是在2D像素空间中进行，这些攻击并不总是物理上真实的在我们的3D世界中。在这里，我们提出了模型敌意示例为神经辐射场（NeRF）的首次探索。NeRF的进步提供了真实的外观和准确的3D生成，从而导致更真实和可能的敌意示例。我们在训练敌意NeRF时，将培育周围对象的信任预测值作为3D检测器的训练集中的一部分。然后，我们在无法见验证集上评估Adv3D，并证明它可以在任意抽象 pose 中进行3D patch攻击。为生成物理可能的敌意示例，我们提出了基于元素的sampling和semantic-guided regularization，允许3D质量攻击。实验结果表明，我们的训练敌意NeRF可以在不同的 pose、场景和3D检测器上进行广泛的应用。最后，我们提出了防御方法，通过数据增强来进行对敌意示例的防御。项目页面：https://len-li.github.io/adv3d-web
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification"><a href="#Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification" class="headerlink" title="Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification"></a>Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01342">http://arxiv.org/abs/2309.01342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marzi Heidari, Abdullah Alchihabi, Qing En, Yuhong Guo</li>
<li>for: 这篇论文是为了解决跨领域少数检索分类问题。</li>
<li>methods: 本文提出了一种名为 Adaptive Parametric Prototype Learning（APPL）的新方法，它是基于元学习惯例的。不同于现有的标本性几少方法，我们在支持集合上学习分类标本，并将标本获得到几少检索集合中的条件强制整理。</li>
<li>results: 我们在多个跨领域少数检索资料集上实验了这种方法，结果显示APPL在跨领域少数检索分类中表现更好，比较多数现有的方法。<details>
<summary>Abstract</summary>
Cross-domain few-shot classification induces a much more challenging problem than its in-domain counterpart due to the existence of domain shifts between the training and test tasks. In this paper, we develop a novel Adaptive Parametric Prototype Learning (APPL) method under the meta-learning convention for cross-domain few-shot classification. Different from existing prototypical few-shot methods that use the averages of support instances to calculate the class prototypes, we propose to learn class prototypes from the concatenated features of the support set in a parametric fashion and meta-learn the model by enforcing prototype-based regularization on the query set. In addition, we fine-tune the model in the target domain in a transductive manner using a weighted-moving-average self-training approach on the query instances. We conduct experiments on multiple cross-domain few-shot benchmark datasets. The empirical results demonstrate that APPL yields superior performance than many state-of-the-art cross-domain few-shot learning methods.
</details>
<details>
<summary>摘要</summary>
跨领域少量分类问题比其内领域对应的问题更加挑战性，这是因为训练和测试任务之间存在领域偏移。在这篇论文中，我们开发了一种名为 Adaptive Parametric Prototype Learning（APPL）的新方法，该方法基于元学习准则进行跨领域少量分类。与现有的概率性几何方法不同，我们在支持集合的 concatenated 特征上学习类prototype，并在元学习中加入prototype-based regularization。此外，我们在目标领域中进行了权重移动平均自适应更新方法，以便在查询集中进行微调。我们在多个跨领域少量分类 benchmark 数据集上进行了实验，结果表明，APPL 的性能较多现状顶尖跨领域少量分类方法优秀。
</details></li>
</ul>
<hr>
<h2 id="MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and"><a href="#MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and" class="headerlink" title="MDSC: Towards Evaluating the Style Consistency Between Music and"></a>MDSC: Towards Evaluating the Style Consistency Between Music and</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01340">http://arxiv.org/abs/2309.01340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zixiangzhou916/mdsc">https://github.com/zixiangzhou916/mdsc</a></li>
<li>paper_authors: Zixiang Zhou, Baoyuan Wang</li>
<li>for: 评估音乐与舞蹈风格的匹配度（assessing the matching degree between music and dance styles）</li>
<li>methods: 使用音乐编码器和动作编码器进行匹配和对齐（using music and motion encoders for matching and alignment）</li>
<li>results: 提出了一种新的评估metric——音乐动作风格一致度（MDSC），并通过用户研究发现其可以准确评估音乐与动作风格之间的匹配度（accurately assessing the matching degree between music and dance styles）。Here is the summary in English for reference:</li>
<li>for: Assessing the matching degree between music and dance styles</li>
<li>methods: Using music and motion encoders for matching and alignment</li>
<li>results: Proposed a new evaluation metric called Music-Dance Style Consistency (MDSC) and validated its effectiveness through user studies, demonstrating its ability to accurately assess the matching degree between music and dance styles.<details>
<summary>Abstract</summary>
We propose MDSC(Music-Dance-Style Consistency), the first evaluation metric which assesses to what degree the dance moves and music match. Existing metrics can only evaluate the fidelity and diversity of motion and the degree of rhythmic matching between music and motion. MDSC measures how stylistically correlated the generated dance motion sequences and the conditioning music sequences are. We found that directly measuring the embedding distance between motion and music is not an optimal solution. We instead tackle this through modelling it as a clustering problem. Specifically, 1) we pre-train a music encoder and a motion encoder, then 2) we learn to map and align the motion and music embedding in joint space by jointly minimizing the intra-cluster distance and maximizing the inter-cluster distance, and 3) for evaluation purpose, we encode the dance moves into embedding and measure the intra-cluster and inter-cluster distances, as well as the ratio between them. We evaluate our metric on the results of several music-conditioned motion generation methods, combined with user study, we found that our proposed metric is a robust evaluation metric in measuring the music-dance style correlation. The code is available at: https://github.com/zixiangzhou916/MDSC.
</details>
<details>
<summary>摘要</summary>
我们提出了MDSC（音乐舞蹈风格一致性），第一个评估预测metric，评估音乐和舞蹈动作之间的匹配程度。现有的metric只能评估动作和音乐的精度和多样性，以及音乐和动作的节奏匹配程度。而MDSC则测量了生成的舞蹈动作序列和条件音乐序列之间的风格相关性。我们发现直接测量动作和音乐的嵌入距离并不是最佳解决方案。我们相反地通过模elling它为一个聚类问题来解决。具体来说，我们先预训一个音乐编码器和一个动作编码器，然后学习将动作和音乐嵌入在共同空间中进行对应。最后，我们用计算内层距离和外层距离，以及内层和外层之间的比率来评估。我们在一些音乐条件动作生成方法的结果上进行了评估，并结合用户研究发现，我们的提出的metric是一种可靠的评估metric，可以量化音乐舞蹈风格相关性。代码可以在以下链接中找到：https://github.com/zixiangzhou916/MDSC。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Constraint-Matching-Transformer-for-Weakly-Supervised-Object-Localization"><a href="#Semantic-Constraint-Matching-Transformer-for-Weakly-Supervised-Object-Localization" class="headerlink" title="Semantic-Constraint Matching Transformer for Weakly Supervised Object Localization"></a>Semantic-Constraint Matching Transformer for Weakly Supervised Object Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01331">http://arxiv.org/abs/2309.01331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Cao, Yukun Su, Wenjun Wang, Yanxia Liu, Qingyao Wu</li>
<li>for: 本研究旨在解决weakly supervised object localization中的partial activation问题，即使用只有图像水平级别的指导，学习检测器能够准确地本地化对象。</li>
<li>methods: 本研究使用Vision Transformer（Transformer）来解决partial activation问题，并通过自动注意力机制获取长距离特征依赖关系。此外，还提出了一种本地匹配策略，通过对局部图像进行洗混，保证全局一致性。</li>
<li>results: 经验结果表明，我们的方法可以在CUB-200-2011和ILSVRC datasets上达到新的状态级表现，与之前的方法相比具有显著的超越。<details>
<summary>Abstract</summary>
Weakly supervised object localization (WSOL) strives to learn to localize objects with only image-level supervision. Due to the local receptive fields generated by convolution operations, previous CNN-based methods suffer from partial activation issues, concentrating on the object's discriminative part instead of the entire entity scope. Benefiting from the capability of the self-attention mechanism to acquire long-range feature dependencies, Vision Transformer has been recently applied to alleviate the local activation drawbacks. However, since the transformer lacks the inductive localization bias that are inherent in CNNs, it may cause a divergent activation problem resulting in an uncertain distinction between foreground and background. In this work, we proposed a novel Semantic-Constraint Matching Network (SCMN) via a transformer to converge on the divergent activation. Specifically, we first propose a local patch shuffle strategy to construct the image pairs, disrupting local patches while guaranteeing global consistency. The paired images that contain the common object in spatial are then fed into the Siamese network encoder. We further design a semantic-constraint matching module, which aims to mine the co-object part by matching the coarse class activation maps (CAMs) extracted from the pair images, thus implicitly guiding and calibrating the transformer network to alleviate the divergent activation. Extensive experimental results conducted on two challenging benchmarks, including CUB-200-2011 and ILSVRC datasets show that our method can achieve the new state-of-the-art performance and outperform the previous method by a large margin.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的半监督物体地址 localization（WSOL）方法，它的目的是通过只有图像级别的指导来学习地址物体。由于图像 convolution 操作生成的局部感知范围，前一代 CNN 基于方法容易出现部分活跃问题，即将对象的特征部分进行激活而不是整个实体范围。在应用自注意机制可以获得长范围特征依赖关系的情况下，我们使用了 Vision Transformer 来缓解本地活动问题。然而，由于 transformer 缺乏 CNN 中带有适应性的 inductive 地址偏好，可能会导致不确定的背景和前景分割。为了解决这个问题，我们提出了一种新的 Semantic-Constraint Matching Network（SCMN），通过 transformer 来实现对分歧的激活的控制。具体来说，我们首先提出了一种本地小块洗版策略，用于构建图像对。这种策略可以在保证全局一致性的情况下，对图像进行局部洗版。然后，我们将这些包含共同物体的图像对feed到 Siamese 网络Encoder 中。我们还设计了一种 semantic-constraint 匹配模块，该模块的目的是通过匹配 CAMs 提取自对图像对中的共同部分，从而隐式地引导和调整 transformer 网络，以缓解分歧的激活。我们在 CUB-200-2011 和 ILSVRC 数据集上进行了广泛的实验，结果表明，我们的方法可以达到新的状态码性能，并将之前的方法超越。
</details></li>
</ul>
<hr>
<h2 id="SKoPe3D-A-Synthetic-Dataset-for-Vehicle-Keypoint-Perception-in-3D-from-Traffic-Monitoring-Cameras"><a href="#SKoPe3D-A-Synthetic-Dataset-for-Vehicle-Keypoint-Perception-in-3D-from-Traffic-Monitoring-Cameras" class="headerlink" title="SKoPe3D: A Synthetic Dataset for Vehicle Keypoint Perception in 3D from Traffic Monitoring Cameras"></a>SKoPe3D: A Synthetic Dataset for Vehicle Keypoint Perception in 3D from Traffic Monitoring Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01324">http://arxiv.org/abs/2309.01324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Himanshu Pahadia, Duo Lu, Bharatesh Chakravarthi, Yezhou Yang<br>for:SKoPe3D is a synthetic vehicle keypoint dataset generated using the CARLA simulator, aiming to address the challenges of vehicle keypoint detection in vision-based vehicle monitoring for ITS.methods:The dataset includes generated images with bounding boxes, tracking IDs, and 33 keypoints for each vehicle, spanning over 25k images across 28 scenes with over 150k vehicle instances and 4.9 million keypoints.results:The dataset has the potential to enable advancements in vehicle keypoint detection for ITS, as demonstrated by training a keypoint R-CNN model on the dataset and conducting a thorough evaluation. The dataset’s applicability and the potential for knowledge transfer between synthetic and real-world data are highlighted.<details>
<summary>Abstract</summary>
Intelligent transportation systems (ITS) have revolutionized modern road infrastructure, providing essential functionalities such as traffic monitoring, road safety assessment, congestion reduction, and law enforcement. Effective vehicle detection and accurate vehicle pose estimation are crucial for ITS, particularly using monocular cameras installed on the road infrastructure. One fundamental challenge in vision-based vehicle monitoring is keypoint detection, which involves identifying and localizing specific points on vehicles (such as headlights, wheels, taillights, etc.). However, this task is complicated by vehicle model and shape variations, occlusion, weather, and lighting conditions. Furthermore, existing traffic perception datasets for keypoint detection predominantly focus on frontal views from ego vehicle-mounted sensors, limiting their usability in traffic monitoring. To address these issues, we propose SKoPe3D, a unique synthetic vehicle keypoint dataset generated using the CARLA simulator from a roadside perspective. This comprehensive dataset includes generated images with bounding boxes, tracking IDs, and 33 keypoints for each vehicle. Spanning over 25k images across 28 scenes, SKoPe3D contains over 150k vehicle instances and 4.9 million keypoints. To demonstrate its utility, we trained a keypoint R-CNN model on our dataset as a baseline and conducted a thorough evaluation. Our experiments highlight the dataset's applicability and the potential for knowledge transfer between synthetic and real-world data. By leveraging the SKoPe3D dataset, researchers and practitioners can overcome the limitations of existing datasets, enabling advancements in vehicle keypoint detection for ITS.
</details>
<details>
<summary>摘要</summary>
现代交通基础设施中的智能交通系统（ITS）已经革命化了现代交通基础设施，提供了重要的功能，如交通监测、路安全评估、减压和法律执法。在视觉基础上，精准的车辆检测和车辆位置估计是ITS的关键，特别是使用路边安装的单目camera。车辆特征和形态变化、遮挡、天气和照明条件会增加车辆检测的复杂度。此外，现有的交通感知数据集主要集中在前视角，限制了它们的应用范围。为解决这些问题，我们提出了SKoPe3D数据集，这是一个基于CARLA simulate器生成的路边视角的唯一的车辆关键点数据集。这个全面的数据集包括生成的图像、 bounding box、跟踪ID和33个关键点，涵盖了25000多张图像、28个场景，共计150000辆车辆和4900万个关键点。为证明其可用性，我们在我们的数据集上训练了一个关键点R-CNN模型，并进行了系统性的评估。我们的实验表明，SKoPe3D数据集可以应用于车辆关键点检测，并且可以在实际数据上传递知识。通过利用SKoPe3D数据集，研究人员和实践者可以超越现有数据集的限制，促进车辆关键点检测的进步，以推动ITS的发展。
</details></li>
</ul>
<hr>
<h2 id="FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation"><a href="#FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation" class="headerlink" title="FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation"></a>FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01322">http://arxiv.org/abs/2309.01322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Cesar Quihui-Rubio, Daniel Flores-Araiza, Miguel Gonzalez-Mendoza, Christian Mata, Gilberto Ochoa-Ruiz</li>
<li>for: 这篇论文是为了提出一种基于深度学习的抑制肾脏区域分 segmentation 方法，以提高肾脏癌检测和诊断的工作流程。</li>
<li>methods: 该方法使用 U-Net 网络结合 additive 和 feature pyramid attention 模块，以提高分 segmentation 精度。</li>
<li>results: 比较 seven 种不同的 U-Net 架构，提出的方法在测试集中 achieved  mean DSC 84.15% 和 IoU 76.9%，与大多数研究模型相比，只有 R2U-Net 和 attention R2U-Net 架构超越。<details>
<summary>Abstract</summary>
This contribution presents a deep learning method for the segmentation of prostate zones in MRI images based on U-Net using additive and feature pyramid attention modules, which can improve the workflow of prostate cancer detection and diagnosis. The proposed model is compared to seven different U-Net-based architectures. The automatic segmentation performance of each model of the central zone (CZ), peripheral zone (PZ), transition zone (TZ) and Tumor were evaluated using Dice Score (DSC), and the Intersection over Union (IoU) metrics. The proposed alternative achieved a mean DSC of 84.15% and IoU of 76.9% in the test set, outperforming most of the studied models in this work except from R2U-Net and attention R2U-Net architectures.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "prostate zones" is translated as "陌生区域" (pinyin: zhèng xìng qū yù)* "MRI images" is translated as "MRI图像" (pinyin: MRI tú xiàng)* "U-Net" is translated as "U-Net" (pinyin: Yù nét)* "additive and feature pyramid attention modules" is translated as "加法和特征层 pyramid 注意模块" (pinyin: jiā fàng yǔ tiě xìng piào qián yǐng module)* "can improve the workflow of prostate cancer detection and diagnosis" is translated as "可以改善陌生癌病检测和诊断的工作流程" (pinyin: kě yǐ gǎi shàn zhèng xìng ài yì jīng yì gòng zuò liú xíng)* "the proposed model" is translated as "提议的模型" (pinyin: tím yì de mó del)* "seven different U-Net-based architectures" is translated as "七种不同的 U-Net 基于架构" (pinyin: qī zhǒng bù dìng de U-Net bìng yù jià gòng)* "automatic segmentation performance" is translated as "自动分割性能" (pinyin: zì dìan fēn xiǎn yè nuò)* "Dice Score (DSC)" is translated as "Dice Score (DSC)" (pinyin: Dice Score (DSC))* "Intersection over Union (IoU)" is translated as "交叠率 (IoU)" (pinyin: jiāo fù rátio (IoU))* "test set" is translated as "测试集" (pinyin: cè shì jí)* "outperforming most of the studied models" is translated as "超越大多数研究的模型" (pinyin: chāo yú dà duō shù yán jí de mó del)* "except for R2U-Net and attention R2U-Net architectures" is translated as "除了 R2U-Net 和注意 R2U-Net 架构" (pinyin: chú le R2U-Net hé zhù yì R2U-Net jià gòng)
</details></li>
</ul>
<hr>
<h2 id="An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery"><a href="#An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery" class="headerlink" title="An FPGA smart camera implementation of segmentation models for drone wildfire imagery"></a>An FPGA smart camera implementation of segmentation models for drone wildfire imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01318">http://arxiv.org/abs/2309.01318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Guarduño-Martinez, Jorge Ciprian-Sanchez, Gerardo Valente, Vazquez-Garcia, Gerardo Rodriguez-Hernandez, Adriana Palacios-Rosas, Lucile Rossi-Tisson, Gilberto Ochoa-Ruiz</li>
<li>for: 这个研究旨在开发一个可行的、低功耗的计算机构架，以实现在无人机上进行火灾探测和评估。</li>
<li>methods: 这个研究使用了智能相机，基于低功耗的可程式逻辑阵列（FPGAs），并与二进制神经网络（BNNs）结合，以实现在边缘计算上的高效执行。</li>
<li>results: 研究人员透过优化和对缩减原始模型的实现，实现了从8帧每秒（FPS）提高至33.63 FPS的速度提升，而且无损于标注性能：模型在沃尔夫-科赫茨曼统计指标（MCC）、F1分数和哈菲安质量指标（HAF）中获得0.912、0.915和0.870的分数，并且与原始全精度模型的标注结果相似。<details>
<summary>Abstract</summary>
Wildfires represent one of the most relevant natural disasters worldwide, due to their impact on various societal and environmental levels. Thus, a significant amount of research has been carried out to investigate and apply computer vision techniques to address this problem. One of the most promising approaches for wildfire fighting is the use of drones equipped with visible and infrared cameras for the detection, monitoring, and fire spread assessment in a remote manner but in close proximity to the affected areas. However, implementing effective computer vision algorithms on board is often prohibitive since deploying full-precision deep learning models running on GPU is not a viable option, due to their high power consumption and the limited payload a drone can handle. Thus, in this work, we posit that smart cameras, based on low-power consumption field-programmable gate arrays (FPGAs), in tandem with binarized neural networks (BNNs), represent a cost-effective alternative for implementing onboard computing on the edge. Herein we present the implementation of a segmentation model applied to the Corsican Fire Database. We optimized an existing U-Net model for such a task and ported the model to an edge device (a Xilinx Ultra96-v2 FPGA). By pruning and quantizing the original model, we reduce the number of parameters by 90%. Furthermore, additional optimizations enabled us to increase the throughput of the original model from 8 frames per second (FPS) to 33.63 FPS without loss in the segmentation performance: our model obtained 0.912 in Matthews correlation coefficient (MCC),0.915 in F1 score and 0.870 in Hafiane quality index (HAF), and comparable qualitative segmentation results when contrasted to the original full-precision model. The final model was integrated into a low-cost FPGA, which was used to implement a neural network accelerator.
</details>
<details>
<summary>摘要</summary>
野火是全球最重要的自然灾害之一，它对社会和环境层次产生了深远的影响。因此，许多研究已经进行了，以应用计算机见识技术来解决这个问题。一种最具吸引力的方法是使用具有可见光和红外线摄像头的无人机，以无人机遥测、监控和评估野火传播的方式进行远程监控，但是在邻近灾区进行这些操作。然而，实现有效的计算机见识算法在无人机上是经常不可能的，因为将全精度深度学习模型在GPU上运行是不可避免的，因为它们的电力消耗量太高，无人机的载重量也是有限的。因此，在这个工作中，我们认为智能相机，基于低功耗的场程可程式阵列（FPGAs），与二进制神经网络（BNNs）共同构成了一个可行的选择。我们在这里显示了对 corsica 火灾数据库进行分类模型的实现。我们修改了现有的 U-Net 模型，并将模型转移到边缘设备（Xilinx Ultra96-v2 FPGA）上。通过剪裁和数值化原始模型，我们缩减了模型的参数数量，从8帧/秒降至33.63帧/秒，并维持了分类性能的稳定。我们的模型在 Matthews 相互关联系数（MCC）、F1 分数（F1）和 Hafiane 质量指数（HAF）中获得了0.912、0.915和0.870的数据，并且在与原始全精度模型进行比较时，获得了相似的分类结果。最终模型被集成到低成本 FPGA 上，实现了一个神经网络加速器。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection"><a href="#Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection" class="headerlink" title="Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection"></a>Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01312">http://arxiv.org/abs/2309.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Audrey Paleczny, Shubham Parab, Maxwell Zhang</li>
<li>for: 预测老年人群中的阿尔ц海默病患者（age 65 and older），以便提供早期诊断和治疗。</li>
<li>methods: 使用深度学习模型（Convolutional Neural Network，CNN）和磁共振成像（Magnetic Resonance Imaging，MRI）进行诊断。</li>
<li>results: 使用OOD检测技术可以减少假阳性诊断，提高诊断的可靠性。模型基于CNN结果的检测精度为98%，分类精度为95%，超过了基于分割体积模型的检测和分类精度（93%和87%）。<details>
<summary>Abstract</summary>
More than 10.7% of people aged 65 and older are affected by Alzheimer's disease. Early diagnosis and treatment are crucial as most Alzheimer's patients are unaware of having it until the effects become detrimental. AI has been known to use magnetic resonance imaging (MRI) to diagnose Alzheimer's. However, models which produce low rates of false diagnoses are critical to prevent unnecessary treatments. Thus, we trained supervised Random Forest models with segmented brain volumes and Convolutional Neural Network (CNN) outputs to classify different Alzheimer's stages. We then applied out-of-distribution (OOD) detection to the CNN model, enabling it to report OOD if misclassification is likely, thereby reducing false diagnoses. With an accuracy of 98% for detection and 95% for classification, our model based on CNN results outperformed our segmented volume model, which had detection and classification accuracies of 93% and 87%, respectively. Applying OOD detection to the CNN model enabled it to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction. By using OOD detection to enhance the reliability of MRI classification using CNNs, we lowered the rate of false positives and eliminated a significant disadvantage of using Machine Learning models for healthcare tasks. Source code available upon request.
</details>
<details>
<summary>摘要</summary>
更多于10.7%的人年龄在65岁及以上有患阿尔茨海默病。早期诊断和治疗是非常重要，因为大多数阿尔茨海默病患者并不知道自己患病 until the effects become detrimental。人工智能可以使用磁共振成像（MRI）进行诊断。然而，模型生成低False Positive率是非常重要，以避免不必要的治疗。因此，我们使用了监督式Random Forest模型，并将Convolutional Neural Network（CNN）输出与分割的脑部volume进行类别。我们然后将CNN模型应用到OOD检测，以便如果误分类可能，则报告OOD，从而降低了False Positive率。使用OOD检测可以提高MRI类别的可靠性，并且使用CNN模型进行健康任务的应用中，消除了一个重要的缺点。代码可以在请求时提供。
</details></li>
</ul>
<hr>
<h2 id="EMR-MSF-Self-Supervised-Recurrent-Monocular-Scene-Flow-Exploiting-Ego-Motion-Rigidity"><a href="#EMR-MSF-Self-Supervised-Recurrent-Monocular-Scene-Flow-Exploiting-Ego-Motion-Rigidity" class="headerlink" title="EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity"></a>EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01296">http://arxiv.org/abs/2309.01296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijie Jiang, Masatoshi Okutomi</li>
<li>for: 本研究旨在提高现有自监学习Scene Flow estimation方法的精度，通过借鉴超vised学习方法的优点，并在减少动态区域的影响下提高固有的姿态稳定性。</li>
<li>methods: 我们提出了一种名为EMR-MSF的改进模型，其中包括在构建ego-motion汇集模块时引入explicit和稳定的几何约束，以及在满足固有的姿态稳定性下使用mask正则化损失。此外，我们还提出了一种运动一致损失和mask正则化损失，以全面发挥静态区域的作用。</li>
<li>results: 我们的提posed方法在KITTIScene Flow benchmark上表现出色，与state-of-the-art自监学习monocular方法的SF-all指标相比，提高44%。此外，我们的方法在深度和视觉征迹等子任务中也达到了supervised方法的水平。<details>
<summary>Abstract</summary>
Self-supervised monocular scene flow estimation, aiming to understand both 3D structures and 3D motions from two temporally consecutive monocular images, has received increasing attention for its simple and economical sensor setup. However, the accuracy of current methods suffers from the bottleneck of less-efficient network architecture and lack of motion rigidity for regularization. In this paper, we propose a superior model named EMR-MSF by borrowing the advantages of network architecture design under the scope of supervised learning. We further impose explicit and robust geometric constraints with an elaborately constructed ego-motion aggregation module where a rigidity soft mask is proposed to filter out dynamic regions for stable ego-motion estimation using static regions. Moreover, we propose a motion consistency loss along with a mask regularization loss to fully exploit static regions. Several efficient training strategies are integrated including a gradient detachment technique and an enhanced view synthesis process for better performance. Our proposed method outperforms the previous self-supervised works by a large margin and catches up to the performance of supervised methods. On the KITTI scene flow benchmark, our approach improves the SF-all metric of the state-of-the-art self-supervised monocular method by 44% and demonstrates superior performance across sub-tasks including depth and visual odometry, amongst other self-supervised single-task or multi-task methods.
</details>
<details>
<summary>摘要</summary>
自我监督单目场景流估算，寻求从两个 consecutively temporally 单目图像中理解三维结构和三维运动。由于现有方法的精度受到网络架构的瓶颈和运动稳定性的限制，因此在这篇论文中，我们提出了一种高效的模型 named EMR-MSF。我们采用了指导了supervised学习中网络架构的优点，并在 elaborate 构建了自身运动汇集模块，在这里我们提出了一种坚定性软面罩来过滤动态区域，以确保稳定的自身运动估算。此外，我们还提出了运动一致损失和面罩规范损失，以便充分利用静止区域。我们还 интегрирова了一些高效的训练策略，包括梯度分离技术和改进的视图合成过程，以提高表现。根据 KITTI 场景流标准测试集，我们的方法在自我监督单目方法中提高了 SF-all 指标44%，并在深度和视觉速度等子任务中表现出色，在其他自我监督单任务或多任务方法中也达到了类似的水平。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.CV_2023_09_04/" data-id="clp9qz83v00j7ok88ceveci0s" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.AI_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T12:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.AI_2023_09_04/">cs.AI - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks"><a href="#Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks" class="headerlink" title="Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks"></a>Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01838">http://arxiv.org/abs/2309.01838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kacemkhaled/defending-extraction">https://github.com/kacemkhaled/defending-extraction</a></li>
<li>paper_authors: Kacem Khaled, Mouna Dhaouadi, Felipe Gohring de Magalhães, Gabriela Nicolescu</li>
<li>For: The paper is written to propose a simple yet effective and efficient defense against model stealing attacks for deep learning models.* Methods: The paper introduces a heuristic approach to perturb the output probabilities of the model to defend against stealing attacks, which can be easily integrated into models without additional training.* Results: The proposed defense is effective in defending against three state-of-the-art stealing attacks, and outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model’s performance. The defense is also effective for quantized CNNs targeting edge devices.Here’s the same information in Simplified Chinese text:* For: 这篇论文是为了提出一种简单又有效的模型盗用攻击防御方案。* Methods: 论文提出一种基于归类抽象的方法，通过对模型输出概率进行扰动来防御盗用攻击。这种方法可以轻松地与现有模型集成，无需进行额外训练。* Results: 提出的防御方法有效地防止了三种state-of-the-art的盗用攻击，并且比现有的防御方法快速37倍，不需要额外的模型和占用较低的模型性能。此外，这种防御方法也适用于采用量化（即压缩）的卷积神经网络（CNN）和边缘设备。<details>
<summary>Abstract</summary>
Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks.We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.
</details></li>
</ul>
<hr>
<h2 id="Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties"><a href="#Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties" class="headerlink" title="Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties"></a>Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03094">http://arxiv.org/abs/2309.03094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Mirzaeifard, Naveen K. D. Venkategowda, Vinay Chakravarthi Gogineni, Stefan Werner</li>
<li>for: 这paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, and proposes a novel single-loop smoothing ADMM algorithm named SIAD to accelerate the convergence speed.</li>
<li>methods: 该paper使用了iterative techniques like coordinate descent and local linear approximation, as well as the alternating direction method of multipliers (ADMM) to facilitate convergence.</li>
<li>results: 数据表示SIAD方法比现有方法更快和稳定，提供了更好的解决方案 для sparse-penalized quantile regression。<details>
<summary>Abstract</summary>
This paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). The non-smooth and non-convex nature of these problems often leads to convergence difficulties for many algorithms. While iterative techniques like coordinate descent and local linear approximation can facilitate convergence, the process is often slow. This sluggish pace is primarily due to the need to run these approximation techniques until full convergence at each step, a requirement we term as a \emph{secondary convergence iteration}. To accelerate the convergence speed, we employ the alternating direction method of multipliers (ADMM) and introduce a novel single-loop smoothing ADMM algorithm with an increasing penalty parameter, named SIAD, specifically tailored for sparse-penalized quantile regression. We first delve into the convergence properties of the proposed SIAD algorithm and establish the necessary conditions for convergence. Theoretically, we confirm a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$ for the sub-gradient bound of augmented Lagrangian. Subsequently, we provide numerical results to showcase the effectiveness of the SIAD algorithm. Our findings highlight that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression.
</details>
<details>
<summary>摘要</summary>
To improve the convergence speed, we use the alternating direction method of multipliers (ADMM) and develop a new single-loop smoothing ADMM algorithm called SIAD. We prove that the SIAD algorithm converges at a rate of $o\big({k^{-\frac{1}{4}}\big)$ for the sub-gradient bound of the augmented Lagrangian.We also conduct numerical experiments to compare the performance of the SIAD algorithm with other methods. Our results show that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression.
</details></li>
</ul>
<hr>
<h2 id="One-Wide-Feedforward-is-All-You-Need"><a href="#One-Wide-Feedforward-is-All-You-Need" class="headerlink" title="One Wide Feedforward is All You Need"></a>One Wide Feedforward is All You Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01826">http://arxiv.org/abs/2309.01826</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Telmo Pessoa Pires, António V. Lopes, Yannick Assogba, Hendra Setiawan</li>
<li>for: 这个论文的目的是探究Transformer架构中的Feed Forward Network（FFN） redundancy，以及如何通过减少FFN的参数数量来提高模型的准确率和响应时间。</li>
<li>methods: 这个论文使用了Transformer架构，并对其中的FFN进行了探究和优化。特别是， authors 发现了FFN的重复性，并通过在解码器层上移除FFN来减少参数数量。此外， authors 还将共享一个FFN来替代原始Transformer Big中的多个FFN，以提高准确率和响应时间。</li>
<li>results: 根据实验结果， authors 发现了减少FFN参数数量可以 achieving substantial gains in both accuracy and latency with respect to the original Transformer Big。具体来说， authors 通过减少解码器层的FFN参数数量，可以提高模型的准确率，同时也可以降低模型的响应时间。<details>
<summary>Abstract</summary>
The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.
</details>
<details>
<summary>摘要</summary>
transformer 架构有两个主要非嵌入组件：注意力和Feed Forward Network (FFN)。注意力捕捉即使词语位置不同也可以互相依赖的关系，而 FFN 非线性变换每个输入token。在这项工作中，我们研究 FFN 的角色，并发现它占用模型参数的一大部分，但它具有很高的重复率。具体来说，我们可以通过去除decoder层的 FFN，并将encoder中的 FFN 共享来减少参数数量，只有一定的精度下降。最后，我们通过增加共享 FFN 的隐藏维度，实现了对原始 transformer Big 的重大提升 both accuracy和延迟时间。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension"><a href="#Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension" class="headerlink" title="Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension"></a>Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02465">http://arxiv.org/abs/2309.02465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idealab-isu/llm4g-code">https://github.com/idealab-isu/llm4g-code</a></li>
<li>paper_authors: Anushrut Jignasu, Kelly Marshall, Baskar Ganapathysubramanian, Aditya Balu, Chinmay Hegde, Adarsh Krishnamurthy</li>
<li>for: 这篇论文旨在描述如何使用现有的大型自然语言模型（LLMs）来理解和修改3D打印机的G-code文件。</li>
<li>methods: 论文使用了六种现有的LLMs，并设计了有效的提示来让这些模型理解和操纵G-code文件。</li>
<li>results: 论文对六种LLMs的性能进行了全面的评估，并分析了它们对完整G-code文件的理解的优劣点。<details>
<summary>Abstract</summary>
3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strengths and weaknesses for understanding complete G-code files. We also discuss the implications and limitations of using LLMs for G-code comprehension.
</details>
<details>
<summary>摘要</summary>
三维打印或加itive制造是一种革命性的技术，允许将数字模型转化为物理 объек的创造。然而，三维打印的质量和准确性取决于G-code的正确性和效率，G-code是一种低级数控制程序语言，用于指示三维打印机如何移动和挤出材料。调试G-code是一项复杂的任务，需要对G-code格式和部件的几何结构具有语义和语法理解。在这篇论文中，我们展示了首次对六种现代基础大语言模型（LLM）的扩展性评估，以便理解和修改G-code文件。我们设计有效的提示，使得预训练的LLM可以理解和操纵G-code，并测试其表现于不同的G-code调试和修改方面，包括常见错误检测和修复以及几何变换能力。我们分析它们在完整G-code文件理解方面的优势和缺陷。我们还讨论了使用LLM进行G-code理解的限制和局限性。
</details></li>
</ul>
<hr>
<h2 id="DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research"><a href="#DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research" class="headerlink" title="DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research"></a>DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01808">http://arxiv.org/abs/2309.01808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ynchuang/discoverpath">https://github.com/ynchuang/discoverpath</a></li>
<li>paper_authors: Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Kwei-Herng Lai, Daochen Zha, Ruixiang Tang, Fan Yang, Alfredo Costilla Reyes, Kaixiong Zhou, Xiaoqian Jiang, Xia Hu</li>
<li>for: 增进生物医学研究中的文献检索效率，尤其是在跨学科领域中，通过使用知识图来提高用户体验。</li>
<li>methods: 使用命名实体识别（NER）和parts-of-speech（POS）标签来从文章摘要中提取 terminologies和关系，并将其整合成知识图。</li>
<li>results: 提供了一个开源的Graphical User Interface，可以帮助用户查找相关的文章和增进知识探索。<details>
<summary>Abstract</summary>
The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical User Interface that provides an intuitive visualization of the KG, query recommendations, and detailed article information, enabling efficient article retrieval, thus fostering interdisciplinary knowledge exploration. DiscoverPath is open-sourced at https://github.com/ynchuang/DiscoverPath.
</details>
<details>
<summary>摘要</summary>
随着学术论文的激增增长，需要更高级的工具来快速检索相关的论文，特别是在交叉学科领域，where diverse terminologies are used to describe similar research. 传统的关键词基本搜索引擎often fails to assist users who are not familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, called DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a knowledge graph (KG). To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes, and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical User Interface that provides an intuitive visualization of the KG, query recommendations, and detailed article information, enabling efficient article retrieval and thus fostering interdisciplinary knowledge exploration. DiscoverPath is open-sourced at <https://github.com/ynchuang/DiscoverPath>.
</details></li>
</ul>
<hr>
<h2 id="Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation"><a href="#Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation" class="headerlink" title="Marginalized Importance Sampling for Off-Environment Policy Evaluation"></a>Marginalized Importance Sampling for Off-Environment Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01807">http://arxiv.org/abs/2309.01807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pulkit Katdare, Nan Jiang, Katherine Driggs-Campbell</li>
<li>for: 评估实际世界中RL策略的性能，不需要真实世界的部署。</li>
<li>methods: 利用Marginalized Importance Sampling（MIS）框架，通过在模拟器中添加真实世界停留数据，评估RL策略的实际世界性能。</li>
<li>results: 对多种Sim2Sim环境和目标策略，以及不同的停留数据收集策略进行了实际评估，并在Sim2Real任务中评估了一个7度 freedomRobotic臂的性能。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation, requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies without deploying them in the real world. The proposed approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separately. The first term is learned with direct supervision and the second term has a small magnitude, thus making it easier to run. We analyze the sample complexity as well as error propagation of our two step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim environments such as Cartpole, Reacher and Half-Cheetah. Our results show that our method generalizes well across a variety of Sim2Sim gap, target policies and offline data collection policies. We also demonstrate the performance of our algorithm on a Sim2Real task of validating the performance of a 7 DOF robotic arm using offline data along with a gazebo based arm simulator.
</details>
<details>
<summary>摘要</summary>
《强化学习（RL）方法通常是样本不fficient，使得在实际世界中训练和部署RL策略变得困难。 même a robust策略在simulation中训练，需要在实际世界中进行评估其性能。这篇论文提出了一种新的方法，用于在实际世界中评估代理策略的性能，不需要将其部署到实际世界中。该方法利用了simulator和实际世界的停止数据，通过Marginalized Importance Sampling（MIS）框架评估任何策略的性能。现有MIS方法面临两个挑战：（1）巨大的概率比率，它们与理解范围内的合理范围偏离很大；（2）间接监督，需要间接地估算概率比率，从而使得估计误差加大。我们的方法解决了这两个挑战，通过引入目标策略在simulator中的存在量作为中间变量，将概率比率分解为两个可分别学习的项。第一项通过直接监督学习，第二项具有小的幅度，因此更容易实现。我们还分析了我们的两步程序的样本复杂度以及误差的卷积。此外，我们也进行了Empirical评估，并证明我们的方法在Cartpole、Reacher和Half-Cheetah等Sim2Sim环境中能够具有良好的泛化性。此外，我们还展示了我们的算法在一个Sim2Real任务中，使用了停止数据和Gazebo基于的arm simulator，验证了一个7度OF robotic arm的性能。
</details></li>
</ul>
<hr>
<h2 id="Neural-Singular-Hessian-Implicit-Neural-Representation-of-Unoriented-Point-Clouds-by-Enforcing-Singular-Hessian"><a href="#Neural-Singular-Hessian-Implicit-Neural-Representation-of-Unoriented-Point-Clouds-by-Enforcing-Singular-Hessian" class="headerlink" title="Neural-Singular-Hessian: Implicit Neural Representation of Unoriented Point Clouds by Enforcing Singular Hessian"></a>Neural-Singular-Hessian: Implicit Neural Representation of Unoriented Point Clouds by Enforcing Singular Hessian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01793">http://arxiv.org/abs/2309.01793</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bearprin/Neural-Singular-Hessian">https://github.com/bearprin/Neural-Singular-Hessian</a></li>
<li>paper_authors: Zixiong Wang, Yunxiao Zhang, Rui Xu, Fan Zhang, Pengshuai Wang, Shuangmin Chen, Shiqing Xin, Wenping Wang, Changhe Tu</li>
<li>for: 该论文旨在拟合点云数据中的表面 reconstruction 问题。</li>
<li>methods: 该方法combines various regularization terms, such as Eikonal和Laplacian energy terms, to enforce the learned neural function to possess the properties of a Signed Distance Function (SDF)。 In addition, the approach enforces the Hessian of the neural implicit function to have a zero determinant for points near the surface, which aligns the gradients for a near-surface point and its on-surface projection point, producing a rough but faithful shape。</li>
<li>results: 经验表明，该方法可以有效地suppress ghost geometry和recover details from unoriented point clouds with better expressiveness than existing fitting-based methods。<details>
<summary>Abstract</summary>
Neural implicit representation is a promising approach for reconstructing surfaces from point clouds. Existing methods combine various regularization terms, such as the Eikonal and Laplacian energy terms, to enforce the learned neural function to possess the properties of a Signed Distance Function (SDF). However, inferring the actual topology and geometry of the underlying surface from poor-quality unoriented point clouds remains challenging. In accordance with Differential Geometry, the Hessian of the SDF is singular for points within the differential thin-shell space surrounding the surface. Our approach enforces the Hessian of the neural implicit function to have a zero determinant for points near the surface. This technique aligns the gradients for a near-surface point and its on-surface projection point, producing a rough but faithful shape within just a few iterations. By annealing the weight of the singular-Hessian term, our approach ultimately produces a high-fidelity reconstruction result. Extensive experimental results demonstrate that our approach effectively suppresses ghost geometry and recovers details from unoriented point clouds with better expressiveness than existing fitting-based methods.
</details>
<details>
<summary>摘要</summary>
神经隐式表示是一种有前途的方法，用于从点云重建表面。现有方法将各种正则化项相结合，如振荡能量和拉普拉斯能量项，以强制学习神经函数具备签名距离函数的性质。然而，从低质量、无法定向的点云中恢复真实的表面 topology 和几何结构仍然是一个搜索。根据 diferencial geometry，在表面 differential thin-shell 空间中，SDF 的哈密顿矩阵是非特征矩阵。我们的方法强制神经隐式函数的哈密顿矩阵在near surface 点附近为零 determinant。这种技术将near surface 点的梯度与其在表面上的投影点的梯度相对align，生成一个粗糙 yet faithful 的形态，只需几个迭代即可。通过渐进式地减小weight的特征矩阵项，我们的方法最终生成高精度重建结果。广泛的实验结果表明，我们的方法可以有效地抑制幽灵几何和从无法定向的点云中恢复细节，比现有的适应型方法更有表达力。
</details></li>
</ul>
<hr>
<h2 id="3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream"><a href="#3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream" class="headerlink" title="3D View Prediction Models of the Dorsal Visual Stream"></a>3D View Prediction Models of the Dorsal Visual Stream</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01782">http://arxiv.org/abs/2309.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Sarch, Hsiao-Yu Fish Tung, Aria Wang, Jacob Prince, Michael Tarr</li>
<li>for: 这个论文是为了测试一种基于3D场景几何的自适应循环神经网络（GRNN）是否能更好地与脑动脉核心视觉区域的功能特性相匹配。</li>
<li>methods: 这个论文使用了一种自适应循环神经网络（GRNN），并使用了一个3D特征记忆来训练这个模型。</li>
<li>results: 研究发现，GRNN能够更好地预测新的摄像头视图，并且对脑动脉核心视觉区域的变化具有更高的准确率。<details>
<summary>Abstract</summary>
Deep neural network representations align well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.
</details>
<details>
<summary>摘要</summary>
深度神经网络表示 aligned well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.Here's the translation in Traditional Chinese:深度神经网络表示 aligned well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.
</details></li>
</ul>
<hr>
<h2 id="On-the-size-of-irredundant-propagation-complete-CNF-formulas"><a href="#On-the-size-of-irredundant-propagation-complete-CNF-formulas" class="headerlink" title="On the size of irredundant propagation complete CNF formulas"></a>On the size of irredundant propagation complete CNF formulas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01750">http://arxiv.org/abs/2309.01750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Petr Savický</li>
<li>for: 这个论文 investigate propagation complete (PC) CNF formulas for a symmetric definite Horn function of $n$ variables.</li>
<li>methods: 论文使用了 minimum size of these formulas 与specific covering numbers closely related, specifically, the smallest number of $k$-subsets of an $n$-set covering all $(k-1)$-subsets for a suitable $k$.</li>
<li>results: 论文展示了一个 irredundant PC formula whose size is larger than the size of a smallest PC formula for the same function by a factor $\Omega(n&#x2F;\ln n)$. This complements a known polynomial upper bound on this factor.<details>
<summary>Abstract</summary>
We investigate propagation complete (PC) CNF formulas for a symmetric definite Horn function of $n$ variables and demonstrate that the minimum size of these formulas is closely related to specific covering numbers, namely, to the smallest number of $k$-subsets of an $n$-set covering all $(k-1)$-subsets for a suitable $k$. As a consequence, we demonstrate an irredundant PC formula whose size is larger than the size of a smallest PC formula for the same function by a factor $\Omega(n/\ln n)$. This complements a known polynomial upper bound on this factor.
</details>
<details>
<summary>摘要</summary>
我们调查完整的几何函数（PC）逻辑式，对于一个对称定义的权Func数学函数，并证明这个函数的最小大小与特定的覆盖数字有密切的关系，即最小的$k$-subsets的集合覆盖所有($k-1$)-subsets。我们从这个结果获得了一个不可简的PC方程，其大小比最小PC方程的大小有$\Omega(n/\ln n)$的因子。这与已知的多项式上界有关。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment"><a href="#Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment" class="headerlink" title="Hybrid data driven&#x2F;thermal simulation model for comfort assessment"></a>Hybrid data driven&#x2F;thermal simulation model for comfort assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01734">http://arxiv.org/abs/2309.01734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Barbedienne, Sara Yasmine Ouerk, Mouadh Yagoubi, Hassan Bouia, Aurelie Kaemmerlen, Benoit Charrier</li>
<li>for: 提高物理模型的速度和质量</li>
<li>methods: 结合实际数据和模拟数据预测室内舒适度</li>
<li>results: 使用Random Forest模型 obtain F1 score 0.999 的promising results<details>
<summary>Abstract</summary>
Machine learning models improve the speed and quality of physical models. However, they require a large amount of data, which is often difficult and costly to acquire. Predicting thermal comfort, for example, requires a controlled environment, with participants presenting various characteristics (age, gender, ...). This paper proposes a method for hybridizing real data with simulated data for thermal comfort prediction. The simulations are performed using Modelica Language. A benchmarking study is realized to compare different machine learning methods. Obtained results look promising with an F1 score of 0.999 obtained using the random forest model.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：机器学习模型可以提高物理模型的速度和质量，但它们需要大量数据，而这些数据常常困难和costly to obtain。预测冷凉舒适性需要控制环境，参与者具有不同特征（年龄、性别、...）。这篇论文提议将实际数据与模拟数据相互融合以预测冷凉舒适性。模拟使用Modelica语言进行。实现了不同机器学习方法的比较研究。获得的结果很有前途，使用随机森林模型获得的F1分数为0.999。Note: "简化中文" refers to Simplified Chinese, which is one of the two standardized Chinese writing systems, used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Softmax-Bias-Correction-for-Quantized-Generative-Models"><a href="#Softmax-Bias-Correction-for-Quantized-Generative-Models" class="headerlink" title="Softmax Bias Correction for Quantized Generative Models"></a>Softmax Bias Correction for Quantized Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01729">http://arxiv.org/abs/2309.01729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, Markus Nagel</li>
<li>for: 提高 Edge 设备上大量生成模型的运行时间和功耗效率，包括稳定扩散或大语言模型。</li>
<li>methods:  investigate 软MAX输出强制性对归一化干扰的影响，并提出一种在部署时进行偏差修正，以提高软MAX的量化可行性。</li>
<li>results: 在稳定扩散 v1.5 和 125M-size OPT 语言模型上，实现了8比特量化软MAX后的准确性提高。<details>
<summary>Abstract</summary>
Post-training quantization (PTQ) is the go-to compression technique for large generative models, such as stable diffusion or large language models. PTQ methods commonly keep the softmax activation in higher precision as it has been shown to be very sensitive to quantization noise. However, this can lead to a significant runtime and power overhead during inference on resource-constraint edge devices. In this work, we investigate the source of the softmax sensitivity to quantization and show that the quantization operation leads to a large bias in the softmax output, causing accuracy degradation. To overcome this issue, we propose an offline bias correction technique that improves the quantizability of softmax without additional compute during deployment, as it can be readily absorbed into the quantization parameters. We demonstrate the effectiveness of our method on stable diffusion v1.5 and 125M-size OPT language model, achieving significant accuracy improvement for 8-bit quantized softmax.
</details>
<details>
<summary>摘要</summary>
Post-training quantization (PTQ) 是大型生成模型的压缩技术，如稳定扩散或大语言模型。PTQ方法通常保留软 макс激活函数的高精度，因为它已经被证明对压缩噪声非常敏感。然而，这可能会导致在资源有限的边缘设备中的运行时间和功耗开销增加。在这项工作中，我们研究软 макс激活函数对压缩的敏感性的源头，发现压缩操作会导致软 макс输出中的大量偏差，从而导致准确性下降。为解决这个问题，我们提出了一种离线偏差修正技术，可以在部署过程中对软 макс进行压缩而不需要额外的计算，因为它可以轻松吸收到压缩参数中。我们在稳定扩散 v1.5 和 125M 大小的 OPT 语言模型上进行了实验，并达到了8位压缩软 макс后的显著准确性改进。
</details></li>
</ul>
<hr>
<h2 id="Interdisciplinary-Fairness-in-Imbalanced-Research-Proposal-Topic-Inference-A-Hierarchical-Transformer-based-Method-with-Selective-Interpolation"><a href="#Interdisciplinary-Fairness-in-Imbalanced-Research-Proposal-Topic-Inference-A-Hierarchical-Transformer-based-Method-with-Selective-Interpolation" class="headerlink" title="Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation"></a>Interdisciplinary Fairness in Imbalanced Research Proposal Topic Inference: A Hierarchical Transformer-based Method with Selective Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01717">http://arxiv.org/abs/2309.01717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Xiao, Min Wu, Ziyue Qiao, Yanjie Fu, Zhiyuan Ning, Yi Du, Yuanchun Zhou</li>
<li>for: 提高自动话题推荐系统的公平性，解决由于人工填写话题导致的偏误和不公平现象。</li>
<li>methods: 基于Transformerencoder-decoder架构实现话题标签推论系统，并利用 interpolate技术生成pseudo-交叉学科提案，以减少模型训练时的偏误。</li>
<li>results: 在实际数据集上进行了广泛的实验，研究结果表明，提posed方法可以减少自动话题推论任务中的不公平现象。<details>
<summary>Abstract</summary>
The objective of topic inference in research proposals aims to obtain the most suitable disciplinary division from the discipline system defined by a funding agency. The agency will subsequently find appropriate peer review experts from their database based on this division. Automated topic inference can reduce human errors caused by manual topic filling, bridge the knowledge gap between funding agencies and project applicants, and improve system efficiency. Existing methods focus on modeling this as a hierarchical multi-label classification problem, using generative models to iteratively infer the most appropriate topic information. However, these methods overlook the gap in scale between interdisciplinary research proposals and non-interdisciplinary ones, leading to an unjust phenomenon where the automated inference system categorizes interdisciplinary proposals as non-interdisciplinary, causing unfairness during the expert assignment. How can we address this data imbalance issue under a complex discipline system and hence resolve this unfairness? In this paper, we implement a topic label inference system based on a Transformer encoder-decoder architecture. Furthermore, we utilize interpolation techniques to create a series of pseudo-interdisciplinary proposals from non-interdisciplinary ones during training based on non-parametric indicators such as cross-topic probabilities and topic occurrence probabilities. This approach aims to reduce the bias of the system during model training. Finally, we conduct extensive experiments on a real-world dataset to verify the effectiveness of the proposed method. The experimental results demonstrate that our training strategy can significantly mitigate the unfairness generated in the topic inference task.
</details>
<details>
<summary>摘要</summary>
translate to Simplified Chinese:研究主题推断在研究提案中的目标是通过funding机构定义的学科系统中获得最适合的学科分类。该机构将根据此分类找到相应的专家评审人员从其数据库中。自动化主题推断可以降低人类手动填充主题的错误，跨学科研究提案和非跨学科研究提案之间的知识差距，并提高系统效率。现有方法是将这视为一个层次多个标签的分类问题，使用生成模型iteratively推断最有利的主题信息。然而，这些方法忽略了跨学科研究提案和非跨学科研究提案之间的规模差异，导致自动推断系统将跨学科研究提案分类为非跨学科研究提案，从而导致了对专家分配的不公正。如何在复杂的学科系统下解决这种数据不匹配问题，从而解决这种不公正呢？在这篇论文中，我们实现了一个基于Transformer编码器-解码器架构的主题标签推断系统。此外，我们使用 interpolate技术在训练期间创建一系列 pseudo-跨学科提案从非跨学科提案中，基于非 Parametric indicator such as cross-topic probabilities和主题发生概率。这种方法 aimsto reduce the bias of the system during model training.最后，我们对实际数据进行了广泛的实验，以验证提案的有效性。实验结果表明，我们的训练策略可以明显减少自动推断 task中的不公正。
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise"><a href="#On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise" class="headerlink" title="On the Robustness of Post-hoc GNN Explainers to Label Noise"></a>On the Robustness of Post-hoc GNN Explainers to Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01706">http://arxiv.org/abs/2309.01706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Zhong, Yangqianzi Jiang, Davide Mottin</li>
<li>for: 本研究旨在探讨post-hoc图 neural network（GNN）解释器在受损标签情况下的可靠性。</li>
<li>methods: 研究使用了多种post-hoc GNN解释器，并在不同的标签噪声水平进行了系统性的实验研究。</li>
<li>results: 研究发现，post-hoc GNN解释器具有抗受损性，但是即使标签噪声较低，解释器也会受到影响，解释质量下降。同时，研究还发现，随着标签噪声水平的增加，解释器的效果逐渐恢复。<details>
<summary>Abstract</summary>
Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
</details>
<details>
<summary>摘要</summary>
提议作为图 neural network（GNN）的黑盒限制解决方案，post-hoc GNN 解释器尝试提供准确和有 insightful 的 GNN 行为解释。尽管在学术和工业上最近有所进步，但post-hoc GNN 解释器的Robustness 仍未被探索，对于标签噪声的情况。为了bridging这个差距，我们进行了系统性的实验研究，评估不同的 post-hoc GNN 解释器在不同的标签噪声水平下的效果。我们的结果显示了以下几点：一、post-hoc GNN 解释器受到标签变动的影响。二、即使标签噪声非常低，也会对 GNN 性能造成很大的影响。三、随着噪声水平的增加，解释效果逐渐恢复。
</details></li>
</ul>
<hr>
<h2 id="No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets"><a href="#No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets" class="headerlink" title="No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets"></a>No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01694">http://arxiv.org/abs/2309.01694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Brigato, Stavroula Mougiakakou</li>
<li>for: 解决现代计算机视觉中小训练集图像分类任务的问题</li>
<li>methods: 使用各种敏感训练策略和模型尺度、训练时间表等参数的调整</li>
<li>results: 在 solely 1% of the original CIFAR-10 training set (i.e., 50 images per class) 和 ciFAIR-10 测试集上达到了 66.5% 的测试精度，与现状最佳方法相当。<details>
<summary>Abstract</summary>
Solving image classification tasks given small training datasets remains an open challenge for modern computer vision. Aggressive data augmentation and generative models are among the most straightforward approaches to overcoming the lack of data. However, the first fails to be agnostic to varying image domains, while the latter requires additional compute and careful design. In this work, we study alternative regularization strategies to push the limits of supervised learning on small image classification datasets. In particular, along with the model size and training schedule scaling, we employ a heuristic to select (semi) optimal learning rate and weight decay couples via the norm of model parameters. By training on only 1% of the original CIFAR-10 training set (i.e., 50 images per class) and testing on ciFAIR-10, a variant of the original CIFAR without duplicated images, we reach a test accuracy of 66.5%, on par with the best state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉中解决小训练集数据的图像分类任务仍然是一个开放的挑战。非常的数据扩展和生成模型是最直接的方法来缓解缺乏数据的问题，但是前者不具备适应不同图像领域的特性，而后者需要额外的计算和精心的设计。在这项工作中，我们研究了不同于supervised学习的regularization策略，以推动小图像分类集数据上的模型训练。特别是，我们采用一种heuristic来选择（semi）优化的学习率和权重衰减couple，通过模型参数的norm来实现。通过使用原始CIFAR-10训练集的1%（即50张每个类）和测试在ciFAIR-10上，我们达到了66.5%的测试精度，与状态元的方法相当。
</details></li>
</ul>
<hr>
<h2 id="Prompt-me-a-Dataset-An-investigation-of-text-image-prompting-for-historical-image-dataset-creation-using-foundation-models"><a href="#Prompt-me-a-Dataset-An-investigation-of-text-image-prompting-for-historical-image-dataset-creation-using-foundation-models" class="headerlink" title="Prompt me a Dataset: An investigation of text-image prompting for historical image dataset creation using foundation models"></a>Prompt me a Dataset: An investigation of text-image prompting for historical image dataset creation using foundation models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01674">http://arxiv.org/abs/2309.01674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hassanhajj910/prompt-me-a-dataset">https://github.com/hassanhajj910/prompt-me-a-dataset</a></li>
<li>paper_authors: Hassan El-Hajj, Matteo Valleriani</li>
<li>for: 这个论文是为了提出一个基于基础模型的图像提取管道，用于从历史文献中提取图像，并评估文本-图像提示的效果在人文领域中。</li>
<li>methods: 该管道采用了GroundDINO和Meta的Segment-Anything-Model（SAM）来从历史文献中检索大量的视觉数据，并评估不同语言提示的影响。</li>
<li>results: 研究发现，使用文本-图像提示可以提高图像提取的效果，并且在不同水平的人文数据集上都有较高的效果。<details>
<summary>Abstract</summary>
In this paper, we present a pipeline for image extraction from historical documents using foundation models, and evaluate text-image prompts and their effectiveness on humanities datasets of varying levels of complexity. The motivation for this approach stems from the high interest of historians in visual elements printed alongside historical texts on the one hand, and from the relative lack of well-annotated datasets within the humanities when compared to other domains. We propose a sequential approach that relies on GroundDINO and Meta's Segment-Anything-Model (SAM) to retrieve a significant portion of visual data from historical documents that can then be used for downstream development tasks and dataset creation, as well as evaluate the effect of different linguistic prompts on the resulting detections.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个图像提取管道，使用基础模型来从历史文献中提取图像，并评估文本图像提示的效果在人文领域中。我们的动机是，历史学家对于与历史文献一起出版的视觉元素具有极高的兴趣，而人文领域内的数据资源相对较少，而且对于其他领域来说更加缺乏准确的标注数据。我们提议一种顺序的方法，利用GroundDINO和Meta的Segment-Anything-Model（SAM）来从历史文献中检索大量的视觉数据，并用于下游开发任务和数据集创建，以及评估不同语言提示的影响。
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Affective-Processing-Capabilities-Emerging-from-Large-Language-Models"><a href="#Fine-grained-Affective-Processing-Capabilities-Emerging-from-Large-Language-Models" class="headerlink" title="Fine-grained Affective Processing Capabilities Emerging from Large Language Models"></a>Fine-grained Affective Processing Capabilities Emerging from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01664">http://arxiv.org/abs/2309.01664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joost Broekens, Bernhard Hilpert, Suzan Verberne, Kim Baraka, Patrick Gebhard, Aske Plaat</li>
<li>for: 这项研究探讨了 ChatGPT 在情感计算任务中的零配置能力，并使用提示alone进行情感分析、情感表达和情感识别等任务。</li>
<li>methods: 这项研究使用了 ChatGPT 进行语言预处理和情感计算任务，并通过提示来实现情感分析、情感表达和情感识别等任务。</li>
<li>results: 研究发现 ChatGPT 可以在 Valence、Arousal 和 Dominance 维度上进行意义性的情感分析，并且有意义的情感表达和情感识别能力。此外， ChatGPT 还可以基于提示实现基本的情绪诱发。这些发现对于情感计算任务和人工智能应用有重要意义。<details>
<summary>Abstract</summary>
Large language models, in particular generative pre-trained transformers (GPTs), show impressive results on a wide variety of language-related tasks. In this paper, we explore ChatGPT's zero-shot ability to perform affective computing tasks using prompting alone. We show that ChatGPT a) performs meaningful sentiment analysis in the Valence, Arousal and Dominance dimensions, b) has meaningful emotion representations in terms of emotion categories and these affective dimensions, and c) can perform basic appraisal-based emotion elicitation of situations based on a prompt-based computational implementation of the OCC appraisal model. These findings are highly relevant: First, they show that the ability to solve complex affect processing tasks emerges from language-based token prediction trained on extensive data sets. Second, they show the potential of large language models for simulating, processing and analyzing human emotions, which has important implications for various applications such as sentiment analysis, socially interactive agents, and social robotics.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>ChatGPT can perform meaningful sentiment analysis in the Valence, Arousal, and Dominance dimensions.2. ChatGPT has meaningful emotion representations in terms of emotion categories and affective dimensions.3. ChatGPT can perform basic appraisal-based emotion elicitation of situations using a prompt-based computational implementation of the OCC appraisal model.These findings are significant:1. They demonstrate that the ability to solve complex affect processing tasks can emerge from language-based token prediction trained on extensive data sets.2. They show the potential of large language models for simulating, processing, and analyzing human emotions, which has important implications for applications such as sentiment analysis, socially interactive agents, and social robotics.</details></li>
</ol>
<hr>
<h2 id="Unveiling-Theory-of-Mind-in-Large-Language-Models-A-Parallel-to-Single-Neurons-in-the-Human-Brain"><a href="#Unveiling-Theory-of-Mind-in-Large-Language-Models-A-Parallel-to-Single-Neurons-in-the-Human-Brain" class="headerlink" title="Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain"></a>Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01660">http://arxiv.org/abs/2309.01660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohsen Jamali, Ziv M. Williams, Jing Cai<br>for: This paper explores the ability of large language models (LLMs) to exhibit a Theory of Mind (ToM), a cognitive capacity related to our conscious mind that allows us to infer another’s beliefs and perspective.methods: The authors drew inspiration from the dorsal medial prefrontal cortex (dmPFC) neurons subserving human ToM and employed a similar methodology to examine whether LLMs exhibit comparable characteristics. They analyzed the hidden embeddings (artificial neurons) within LLMs to see if they could represent another’s perspective.results: The analysis revealed a striking resemblance between the two, as the hidden embeddings within LLMs started to exhibit significant responsiveness to either true- or false-belief trials, suggesting their ability to represent another’s perspective. The authors found that the other’s beliefs could be accurately decoded using the entire embeddings, indicating the presence of the embeddings’ ToM capability at the population level. These findings offer initial evidence of a parallel between the artificial model and neurons in the human brain.<details>
<summary>Abstract</summary>
With their recent development, large language models (LLMs) have been found to exhibit a certain level of Theory of Mind (ToM), a complex cognitive capacity that is related to our conscious mind and that allows us to infer another's beliefs and perspective. While human ToM capabilities are believed to derive from the neural activity of a broadly interconnected brain network, including that of dorsal medial prefrontal cortex (dmPFC) neurons, the precise processes underlying LLM's capacity for ToM or their similarities with that of humans remains largely unknown. In this study, we drew inspiration from the dmPFC neurons subserving human ToM and employed a similar methodology to examine whether LLMs exhibit comparable characteristics. Surprisingly, our analysis revealed a striking resemblance between the two, as hidden embeddings (artificial neurons) within LLMs started to exhibit significant responsiveness to either true- or false-belief trials, suggesting their ability to represent another's perspective. These artificial embedding responses were closely correlated with the LLMs' performance during the ToM tasks, a property that was dependent on the size of the models. Further, the other's beliefs could be accurately decoded using the entire embeddings, indicating the presence of the embeddings' ToM capability at the population level. Together, our findings revealed an emergent property of LLMs' embeddings that modified their activities in response to ToM features, offering initial evidence of a parallel between the artificial model and neurons in the human brain.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的最近发展已经发现具有一定的理论心（ToM）能力，这是与我们意识的大脑网络相关的复杂认知能力，允许我们推断别人的信念和视角。人类ToM能力据信来自大脑的广泛交叉连接的神经活动，包括前 фронталь脑某些 neurons，但precise processes underlying LLM的ToM或与人类相似之处仍然不清楚。在这项研究中，我们 Draw inspiration from human ToM neurons and used a similar methodology to examine whether LLMs exhibit comparable characteristics. Surprisingly, our analysis revealed a striking resemblance between the two, as hidden embeddings（人工神经元）within LLMs started to exhibit significant responsiveness to either true- or false-belief trials， suggesting their ability to represent another's perspective. These artificial embedding responses were closely correlated with the LLMs' performance during the ToM tasks， a property that was dependent on the size of the models. Furthermore， the other's beliefs could be accurately decoded using the entire embeddings， indicating the presence of the embeddings' ToM capability at the population level. Together， our findings revealed an emergent property of LLMs' embeddings that modified their activities in response to ToM features， offering initial evidence of a parallel between the artificial model and neurons in the human brain.
</details></li>
</ul>
<hr>
<h2 id="Which-algorithm-to-select-in-sports-timetabling"><a href="#Which-algorithm-to-select-in-sports-timetabling" class="headerlink" title="Which algorithm to select in sports timetabling?"></a>Which algorithm to select in sports timetabling?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03229">http://arxiv.org/abs/2309.03229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robertomrosati/sa4stt">https://github.com/robertomrosati/sa4stt</a></li>
<li>paper_authors: David Van Bulck, Dries Goossens, Jan-Patrick Clarner, Angelos Dimitsas, George H. G. Fonseca, Carlos Lamas-Fernandez, Martin Mariusz Lester, Jaap Pedersen, Antony E. Phillips, Roberto Maria Rosati</li>
<li>for: 运动赛事时间表调定 (sports timetabling)</li>
<li>methods: 机器学习技术 (machine learning techniques)</li>
<li>results:	+ 提出了一个算法选择系统，可以根据运动赛事问题实例的特征选择最佳的算法。	+  indentified 了选择算法时的重要特征，提供了算法性能的深入了解和提高建议。	+  empirically evaluated the hardness of the instances.In English, this means:</li>
<li>for: Sports timetabling</li>
<li>methods: Machine learning techniques</li>
<li>results:	+ Proposed an algorithm selection system that can select the best algorithm based on the characteristics of a sports timetabling problem instance.	+ Identified the important features in selecting the algorithm, providing deep insights into the performance of the algorithms and suggestions for improvement.	+ Empirically evaluated the hardness of the instances.<details>
<summary>Abstract</summary>
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational experiments involving about 50 years of CPU time on more than 500 newly generated problem instances.
</details>
<details>
<summary>摘要</summary>
任何体育竞赛都需要一份时间表，指定比赛队伍在哪里和何时相遇。最近的国际时间安排竞赛（ITC2021）表明，尽管可以开发通用算法，但每个算法在问题实例上的性能差异较大。本文提供了体育时间安排的实例空间分析，导致了八种当前状态算法的强大洞察和探索。基于机器学习技术，我们提出了一种算法选择系统，可以根据体育时间安排问题实例的特点预测最佳算法。此外，我们还确定了哪些特征对于这种预测具有重要性，从而提供了算法性能的深入了解和改进建议。最后，我们评估了实验难度。我们的结果基于大量计算实验，耗时约50年，使用了500多个新生成的问题实例。
</details></li>
</ul>
<hr>
<h2 id="Design-of-Recognition-and-Evaluation-System-for-Table-Tennis-Players’-Motor-Skills-Based-on-Artificial-Intelligence"><a href="#Design-of-Recognition-and-Evaluation-System-for-Table-Tennis-Players’-Motor-Skills-Based-on-Artificial-Intelligence" class="headerlink" title="Design of Recognition and Evaluation System for Table Tennis Players’ Motor Skills Based on Artificial Intelligence"></a>Design of Recognition and Evaluation System for Table Tennis Players’ Motor Skills Based on Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.07141">http://arxiv.org/abs/2309.07141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuo-yong Shi, Ye-tao Jia, Ke-xin Zhang, Ding-han Wang, Long-meng Ji, Yong Wu</li>
<li>for: 这项研究旨在提高穿戴式设备对特定运动的识别和分析能力。</li>
<li>methods: 该研究使用人工智能技术，设计了一种Device来收集乒乓球运动员的运动信息，并对实际运动数据进行处理。然后，通过分割特征数据库和特征工程来构建运动特征，并通过不同评价指标的损失函数来建立运动技巧的层次评价系统。</li>
<li>results: 研究结果显示，基于特征计算机神经网络的Feature-based BP神经网络在识别乒乓球运动员的运动技巧方面具有更高的识别精度和更强的泛化能力，比传统的卷积神经网络更为出色。<details>
<summary>Abstract</summary>
With the rapid development of electronic science and technology, the research on wearable devices is constantly updated, but for now, it is not comprehensive for wearable devices to recognize and analyze the movement of specific sports. Based on this, this paper improves wearable devices of table tennis sport, and realizes the pattern recognition and evaluation of table tennis players' motor skills through artificial intelligence. Firstly, a device is designed to collect the movement information of table tennis players and the actual movement data is processed. Secondly, a sliding window is made to divide the collected motion data into a characteristic database of six table tennis benchmark movements. Thirdly, motion features were constructed based on feature engineering, and motor skills were identified for different models after dimensionality reduction. Finally, the hierarchical evaluation system of motor skills is established with the loss functions of different evaluation indexes. The results show that in the recognition of table tennis players' motor skills, the feature-based BP neural network proposed in this paper has higher recognition accuracy and stronger generalization ability than the traditional convolutional neural network.
</details>
<details>
<summary>摘要</summary>
随着电子科学和技术的快速发展，穿戴设备的研究不断更新，但目前并不能完全识别和分析特定运动的运动动作。基于这一点，本文提出了一种改进穿戴设备，以便识别和评估乒乓球运动员的动作能力。首先，设备是设计用来收集乒乓球运动员的运动信息，并对实际运动数据进行处理。其次，使用滑动窗口将收集的运动数据分成六种乒乓球标准运动动作的特征库。然后，基于特征工程学，构建了运动特征，并将不同模型中的动作识别为不同的评估指标。最后，建立了基于损失函数的层次评估系统，以评估不同模型的评估指标。结果表明，在识别乒乓球运动员的动作能力方面，基于特征参数的BP神经网络提出的方法在识别精度和泛化能力方面高于传统的卷积神经网络。
</details></li>
</ul>
<hr>
<h2 id="Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD"><a href="#Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD" class="headerlink" title="Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD"></a>Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01640">http://arxiv.org/abs/2309.01640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etay Livne, Gal Kaplun, Eran Malach, Shai Shalev-Schwatz</li>
<li>for: 这篇论文是为了提高 Stochastic Gradient Descent (SGD) 训练机器学习模型时的数据访问效率而设计的。</li>
<li>methods: 该论文提出了一种在云存储的大型数据集上使用 online shuffling 算法，称为 CorgiPile，以提高数据访问效率，但是会导致一定的性能损失。该论文还提出了一种新的两步半数据洗选策略， combinining  offline 迭代 CorgiPile 方法和 online 迭代。</li>
<li>results: 该论文提供了一个全面的理论分析，证明了该方法的收敛性质，并通过实验结果表明，该方法可以在 homogeneous 数据上实现类似于随机访问的性能，而不需要妥协数据访问效率。<details>
<summary>Abstract</summary>
When using Stochastic Gradient Descent (SGD) for training machine learning models, it is often crucial to provide the model with examples sampled at random from the dataset. However, for large datasets stored in the cloud, random access to individual examples is often costly and inefficient. A recent work \cite{corgi}, proposed an online shuffling algorithm called CorgiPile, which greatly improves efficiency of data access, at the cost some performance loss, which is particularly apparent for large datasets stored in homogeneous shards (e.g., video datasets). In this paper, we introduce a novel two-step partial data shuffling strategy for SGD which combines an offline iteration of the CorgiPile method with a subsequent online iteration. Our approach enjoys the best of both worlds: it performs similarly to SGD with random access (even for homogenous data) without compromising the data access efficiency of CorgiPile. We provide a comprehensive theoretical analysis of the convergence properties of our method and demonstrate its practical advantages through experimental results.
</details>
<details>
<summary>摘要</summary>
当使用泛化Gradient Descent（SGD）训练机器学习模型时，通常需要将模型提供随机选择自 dataset 中的示例。然而，对于大规模存储在云端的数据集，随机访问单个示例是经济不可行，不 efficient。一项最近的工作 \cite{corgi} 提出了一种在线洗混算法 called CorgiPile，可以大幅提高数据访问效率，但是会导致一定的性能损失，尤其是对于存储在同一个分区（例如视频集）中的数据。在这篇论文中，我们提出了一种新的两步半数据洗混策略， combinines an offline iteration of the CorgiPile method with a subsequent online iteration。我们的方法可以同SGD with random access（即使对同种数据）获得类似的性能，而无需牺牲 CorgiPile 的数据访问效率。我们提供了完整的理论分析方法，并通过实验结果证明了我们的方法的实际优势。
</details></li>
</ul>
<hr>
<h2 id="Concepts-is-All-You-Need-A-More-Direct-Path-to-AGI"><a href="#Concepts-is-All-You-Need-A-More-Direct-Path-to-AGI" class="headerlink" title="Concepts is All You Need: A More Direct Path to AGI"></a>Concepts is All You Need: A More Direct Path to AGI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01622">http://arxiv.org/abs/2309.01622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Voss, Mladjan Jovanovic</li>
<li>for: 这个论文旨在帮助开发人工通用智能（AGI），以便更快速地实现人类智能水平的计算机。</li>
<li>methods: 该论文采用了认知AI方法，而不是现在广泛使用的统计学和生成方法，以更好地理解人类智能的核心需求，并从而快速实现人类智能水平的计算机。</li>
<li>results: 该论文提出了一种建议的体系和开发计划，以及一些初步的结果，可以帮助开发人工智能快速实现人类智能水平。<details>
<summary>Abstract</summary>
Little demonstrable progress has been made toward AGI (Artificial General Intelligence) since the term was coined some 20 years ago. In spite of the fantastic breakthroughs in Statistical AI such as AlphaZero, ChatGPT, and Stable Diffusion none of these projects have, or claim to have, a clear path to AGI. In order to expedite the development of AGI it is crucial to understand and identify the core requirements of human-like intelligence as it pertains to AGI. From that one can distill which particular development steps are necessary to achieve AGI, and which are a distraction. Such analysis highlights the need for a Cognitive AI approach rather than the currently favored statistical and generative efforts. More specifically it identifies the central role of concepts in human-like cognition. Here we outline an architecture and development plan, together with some preliminary results, that offers a much more direct path to full Human-Level AI (HLAI)/ AGI.
</details>
<details>
<summary>摘要</summary>
“自从AGI（人工通用智能）的概念提出20年前，实际的进步不多。尽管这些年来的统计AI（如AlphaZero、ChatGPT和稳定扩散）实现了非常惊人的突破，但是这些项目都没有或宣称不会有明确的AGI路径。以实现AGI为目标，需要了解和识别人类智能的核心需求，从而决定需要哪些开发步骤，哪些则是骚扰。这种分析表明了需要以认知AI为主，而不是目前受欢迎的统计和生成方法。更 Specifically，它显示了概念在人类智能中的中心角色。以下是一个建筑和开发计划，以及一些先验性结果，它将提供一个许多更直接的人类水准AI（HLAI）/AGI道路。”Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="DeViL-Decoding-Vision-features-into-Language"><a href="#DeViL-Decoding-Vision-features-into-Language" class="headerlink" title="DeViL: Decoding Vision features into Language"></a>DeViL: Decoding Vision features into Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01617">http://arxiv.org/abs/2309.01617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/DeViL">https://github.com/ExplainableML/DeViL</a></li>
<li>paper_authors: Meghal Dani, Isabel Rio-Torto, Stephan Alaniz, Zeynep Akata</li>
<li>for: 本研究旨在提供深度神经网络决策过程的自然语言描述，尤其是对于视觉卷积网络的各层抽象特征。</li>
<li>methods: 我们提出的DeViL方法可以将视觉特征转换为自然语言描述，并不仅高亮特征位置，还生成了对应的文本描述。我们使用了 dropout 技术来进行验证，并使用了预训练的语言模型来生成文本描述。</li>
<li>results: DeViL方法可以生成与图像内容相关的自然语言描述，并且在 CC3M  dataset 上超越了先前的轻量级captioning模型，以及描述了视觉模型中学习的概念。此外，DeViL 还在 MILANNOTATIONS  dataset 上超越了当前的 neuron-wise 描述模型。<details>
<summary>Abstract</summary>
Post-hoc explanation methods have often been criticised for abstracting away the decision-making process of deep neural networks. In this work, we would like to provide natural language descriptions for what different layers of a vision backbone have learned. Our DeViL method decodes vision features into language, not only highlighting the attribution locations but also generating textual descriptions of visual features at different layers of the network. We train a transformer network to translate individual image features of any vision layer into a prompt that a separate off-the-shelf language model decodes into natural language. By employing dropout both per-layer and per-spatial-location, our model can generalize training on image-text pairs to generate localized explanations. As it uses a pre-trained language model, our approach is fast to train, can be applied to any vision backbone, and produces textual descriptions at different layers of the vision network. Moreover, DeViL can create open-vocabulary attribution maps corresponding to words or phrases even outside the training scope of the vision model. We demonstrate that DeViL generates textual descriptions relevant to the image content on CC3M surpassing previous lightweight captioning models and attribution maps uncovering the learned concepts of the vision backbone. Finally, we show DeViL also outperforms the current state-of-the-art on the neuron-wise descriptions of the MILANNOTATIONS dataset. Code available at https://github.com/ExplainableML/DeViL
</details>
<details>
<summary>摘要</summary>
各自使用dropout both per-layer和per-spatial-location，我们的DeViL方法可以进行区域化解释。我们的方法通过将视觉特征翻译成语言提示，然后使用一个独立的语言模型解码成自然语言描述。我们的模型可以快速训练，可以应用于任何视觉后处理器，并且在不同层次上生成文本描述。此外，DeViL还可以生成对于训练词汇外的开 vocabulary扩展映射。我们示示了DeViL可以在CC3M上生成相关的图像内容的文本描述，并且在MILANNOTATIONS数据集中神经元级别的描述也超过了当前状态的最佳性能。代码可以在https://github.com/ExplainableML/DeViL上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data"><a href="#Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data" class="headerlink" title="Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data"></a>Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01593">http://arxiv.org/abs/2309.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqin Li, Jun Liu, Shengliang Zhong, Licheng Zhou, Shoubin Dong, Zejia Liu, Liqun Tang</li>
<li>for: 这个研究是为了找出长 Span 桥梁上的过载车辆，使用结构健康监控数据进行过载车辆识别。</li>
<li>methods: 本研究提出了一个深度学习基本的过载车辆识别方法（DOVI），使用时间卷网络架构对输入序列数据进行抽象，提供了一个端到端的过载车辆识别解决方案，不需要影响线或 velocity 和车辆底盘信息。</li>
<li>results:  результа显示，提出的深度学习过载车辆识别方法比其他机器学习和深度学习方法更有效和更坚固，可以在多辆车辆下进行运行。<details>
<summary>Abstract</summary>
Overloaded vehicles bring great harm to transportation infrastructures. BWIM (bridge weigh-in-motion) method for overloaded vehicle identification is getting more popular because it can be implemented without interruption to the traffic. However, its application is still limited because its effectiveness largely depends on professional knowledge and extra information, and is susceptible to occurrence of multiple vehicles. In this paper, a deep learning based overloaded vehicle identification approach (DOVI) is proposed, with the purpose of overloaded vehicle identification for long-span bridges by the use of structural health monitoring data. The proposed DOVI model uses temporal convolutional architectures to extract the spatial and temporal features of the input sequence data, thus provides an end-to-end overloaded vehicle identification solution which neither needs the influence line nor needs to obtain velocity and wheelbase information in advance and can be applied under the occurrence of multiple vehicles. Model evaluations are conducted on a simply supported beam and a long-span cable-stayed bridge under random traffic flow. Results demonstrate that the proposed deep-learning overloaded vehicle identification approach has better effectiveness and robustness, compared with other machine learning and deep learning approaches.
</details>
<details>
<summary>摘要</summary>
拥载过重车辆对交通基础设施造成严重损害。BWIM（桥上量测方法）方法在过重车辆标识方面获得更多的应用，因为它不需要中断交通。然而，它的应用仍然受限，因为它的效果受职业知识和附加信息的影响，并且容易发生多辆车辆的情况。在本文中，一种基于深度学习的过重车辆标识方法（DOVI）被提出，用于长链桥上的过重车辆标识，通过使用结构健康监测数据。提出的DOVI模型使用时间卷积架构提取输入序列数据的空间和时间特征，因此提供了一个终端到终点的过重车辆标识解决方案，无需影响线 nor 需要先知道速度和车辆跑道信息。模型评估在简支桥和长链悬臂桥上进行随机交通流下。结果表明，提出的深度学习过重车辆标识方法比其他机器学习和深度学习方法更有效和更坚定。
</details></li>
</ul>
<hr>
<h2 id="Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width"><a href="#Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width" class="headerlink" title="Les Houches Lectures on Deep Learning at Large &amp; Infinite Width"></a>Les Houches Lectures on Deep Learning at Large &amp; Infinite Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01592">http://arxiv.org/abs/2309.01592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasaman Bahri, Boris Hanin, Antonin Brossollet, Vittorio Erba, Christian Keup, Rosalba Pacelli, James B. Simon</li>
<li>for: 这些讲座主要关注深度神经网络的无穷宽限和大宽限的特性。</li>
<li>methods: 讲座涉及到深度神经网络的Random化、训练后的连接关系、线性模型、kernels和Gaussian Processes等方面。</li>
<li>results: 讲座讨论了无穷宽限下的深度神经网络的统计和动力学性质，以及训练后的大宽限网络的非平衡和平衡情况。<details>
<summary>Abstract</summary>
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
</details>
<details>
<summary>摘要</summary>
这些讲座，发生在2022年的勒舍夏学院深度学习和机器学习讲座，关注深度神经网络的无限宽限和大宽限。讲座讨论了各种统计和动力学性质，包括随机深度神经网络、训练后深度神经网络与线性模型、核函数和高斯过程之间的连接。此外，讲座还讨论了大宽度网络的非短程和短程训练初始化。
</details></li>
</ul>
<hr>
<h2 id="Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs"><a href="#Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs" class="headerlink" title="Rail Crack Propagation Forecasting Using Multi-horizons RNNs"></a>Rail Crack Propagation Forecasting Using Multi-horizons RNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01569">http://arxiv.org/abs/2309.01569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Yasmine Ouerk, Olivier Vo Van, Mouadh Yagoubi</li>
<li>for: 预测铁路裂隙长度的扩展，以便维护和评估材料和结构的安全性。</li>
<li>methods: 使用机器学习技术，特别是循环神经网络（RNN），来预测时间序列数据。</li>
<li>results: 比较state-of-the-art模型（LSTM和GRU），多个 horizons 模型表现出色，可以更好地预测铁路裂隙长度的扩展。<details>
<summary>Abstract</summary>
The prediction of rail crack length propagation plays a crucial role in the maintenance and safety assessment of materials and structures. Traditional methods rely on physical models and empirical equations such as Paris law, which often have limitations in capturing the complex nature of crack growth. In recent years, machine learning techniques, particularly Recurrent Neural Networks (RNNs), have emerged as promising methods for time series forecasting. They allow to model time series data, and to incorporate exogenous variables into the model. The proposed approach involves collecting real data on the French rail network that includes historical crack length measurements, along with relevant exogenous factors that may influence crack growth. First, a pre-processing phase was performed to prepare a consistent data set for learning. Then, a suitable Bayesian multi-horizons recurrent architecture was designed to model the crack propagation phenomenon. Obtained results show that the Multi-horizons model outperforms state-of-the-art models such as LSTM and GRU.
</details>
<details>
<summary>摘要</summary>
预测铁路裂口长度的传播 игра着关键的角色在材料和结构的维护和安全评估中。传统方法通常基于物理模型和实验方程如巴黎法律，它们经常无法捕捉裂口增长的复杂性。在最近几年，机器学习技术特别是循环神经网络（RNN）已经出现为时间序列预测的有力方法。它允许模拟时间序列数据，并将外生变量纳入模型中。该方法中的提议包括收集法国铁路网络的历史裂口长度测量数据，以及可能影响裂口增长的相关外生因素。首先，一个预处理阶段进行了数据集的准备，以便学习。然后，一种适合的多个镜像感知架构被设计来模拟裂口增长现象。实验结果表明，多镜像模型在LSTM和GRU模型之上表现出了优异的性能。
</details></li>
</ul>
<hr>
<h2 id="OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking"><a href="#OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking" class="headerlink" title="OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking"></a>OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01552">http://arxiv.org/abs/2309.01552</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/outbrain/outrank">https://github.com/outbrain/outrank</a></li>
<li>paper_authors: Blaž Škrlj, Blaž Mramor</li>
<li>for: 这种论文主要用于提高现代推荐系统的设计，以便更好地解决推荐任务。</li>
<li>methods: 本论文使用了一种称为OutRank的系统，用于精细地排序特征和数据质量相关的异常检测。OutRank使用了一种基于分类数据的变体，即对同类特征噪声进行Normalizaation，以便更好地发现有用的信号。此外，该方法还 incorporates 特征相似性和合并相关性信息。</li>
<li>results: 作者们在一个 synthetic 数据集上证明了OutRank的可行性，并在一个真实的点击率预测数据集上比Random Forest-based approaches表现出色，得到了更好的结果。OutRank可以探索更大的特征空间，达到300%更大的特征空间，从而更快地找到更好的模型。<details>
<summary>Abstract</summary>
The design of modern recommender systems relies on understanding which parts of the feature space are relevant for solving a given recommendation task. However, real-world data sets in this domain are often characterized by their large size, sparsity, and noise, making it challenging to identify meaningful signals. Feature ranking represents an efficient branch of algorithms that can help address these challenges by identifying the most informative features and facilitating the automated search for more compact and better-performing models (AutoML). We introduce OutRank, a system for versatile feature ranking and data quality-related anomaly detection. OutRank was built with categorical data in mind, utilizing a variant of mutual information that is normalized with regard to the noise produced by features of the same cardinality. We further extend the similarity measure by incorporating information on feature similarity and combined relevance. The proposed approach's feasibility is demonstrated by speeding up the state-of-the-art AutoML system on a synthetic data set with no performance loss. Furthermore, we considered a real-life click-through-rate prediction data set where it outperformed strong baselines such as random forest-based approaches. The proposed approach enables exploration of up to 300% larger feature spaces compared to AutoML-only approaches, enabling faster search for better models on off-the-shelf hardware.
</details>
<details>
<summary>摘要</summary>
现代推荐系统的设计需要了解哪些特征空间中的特征是解决某个推荐任务的关键。然而，实际世界数据集经常具有大量数据、稀疏性和噪声等特点，使得找到有意义的信号变得困难。特征排名算法可以帮助解决这些挑战，通过识别最有用的特征并实现自动化模型搜索（AutoML）。我们介绍了一个名为OutRank的系统，用于多样化特征排名和数据质量相关异常检测。OutRank采用了分类数据的视角，使用一种对特征噪声产生的减法正则化的相互信息变体。我们进一步扩展了相互信息，通过 integrate feature similarity和共同相关性信息。我们的方法的可行性被证明通过加速现场AutoML系统的速度，而无损失性。此外，我们考虑了一个真实的点击率预测数据集，其中OutRank超过了强基线方法，如随机森林方法。我们的方法可以探索到300%更大的特征空间，比AutoML-只的方法更快地寻找更好的模型，在准备的硬件上。
</details></li>
</ul>
<hr>
<h2 id="ChatRule-Mining-Logical-Rules-with-Large-Language-Models-for-Knowledge-Graph-Reasoning"><a href="#ChatRule-Mining-Logical-Rules-with-Large-Language-Models-for-Knowledge-Graph-Reasoning" class="headerlink" title="ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning"></a>ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01538">http://arxiv.org/abs/2309.01538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan</li>
<li>for:  mines logical rules over knowledge graphs (KGs) to improve reasoning performance and provide interpretable results.</li>
<li>methods:  uses large language models (LLMs) to generate logical rules, leveraging both the semantic and structural information of KGs, and incorporates facts from existing KGs to refine the generated rules.</li>
<li>results:  evaluates the effectiveness and scalability of the proposed method on four large-scale KGs, showing impressive performance and outperforming existing methods.<details>
<summary>Abstract</summary>
Logical rules are essential for uncovering the logical connections between relations, which could improve the reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from the computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, a rule validator harnesses the reasoning ability of LLMs to validate the logical correctness of ranked rules through chain-of-thought reasoning. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用逻辑规则可以探索知识图（KG）中的逻辑连接关系，提高推理性能并提供可读写的结果。 although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, a rule validator harnesses the reasoning ability of LLMs to validate the logical correctness of ranked rules through chain-of-thought reasoning. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.<</SYS>>Here's the translation in Simplified Chinese:使用逻辑规则可以探索知识图（KG）中的逻辑连接关系，提高推理性能并提供可读写的结果。 although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, a rule validator harnesses the reasoning ability of LLMs to validate the logical correctness of ranked rules through chain-of-thought reasoning. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.
</details></li>
</ul>
<hr>
<h2 id="Are-We-Using-Autoencoders-in-a-Wrong-Way"><a href="#Are-We-Using-Autoencoders-in-a-Wrong-Way" class="headerlink" title="Are We Using Autoencoders in a Wrong Way?"></a>Are We Using Autoencoders in a Wrong Way?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01532">http://arxiv.org/abs/2309.01532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GabMartino/icrst_trst_autoencoder">https://github.com/GabMartino/icrst_trst_autoencoder</a></li>
<li>paper_authors: Gabriele Martino, Davide Moroni, Massimo Martinelli</li>
<li>for: This paper is written for revisiting the standard training for undercomplete autoencoders, specifically by modifying the shape of the latent space without using any explicit regularization term in the loss function.</li>
<li>methods: The paper uses the standard training for undercomplete autoencoders, but with a modified shape of the latent space. The model is trained to reconstruct not the same observation in input, but another one sampled from the same class distribution.</li>
<li>results: The paper explores the behavior of the latent space in the case of reconstruction of a random sample from the whole dataset.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了重新评估标准的半完全自动编码器训练方法，具体来说是通过不使用任何显式正则化项来修改半完全自动编码器的幂值空间的形态。</li>
<li>methods: 这篇论文使用标准的半完全自动编码器训练方法，但是半完全自动编码器的幂值空间被修改了。模型被训练以重建不同的输入观测值，而不是原始的输入观测值。</li>
<li>results: 这篇论文探索了随机从整个数据集中采样的整个数据集的行为。<details>
<summary>Abstract</summary>
Autoencoders are certainly among the most studied and used Deep Learning models: the idea behind them is to train a model in order to reconstruct the same input data. The peculiarity of these models is to compress the information through a bottleneck, creating what is called Latent Space. Autoencoders are generally used for dimensionality reduction, anomaly detection and feature extraction. These models have been extensively studied and updated, given their high simplicity and power. Examples are (i) the Denoising Autoencoder, where the model is trained to reconstruct an image from a noisy one; (ii) Sparse Autoencoder, where the bottleneck is created by a regularization term in the loss function; (iii) Variational Autoencoder, where the latent space is used to generate new consistent data. In this article, we revisited the standard training for the undercomplete Autoencoder modifying the shape of the latent space without using any explicit regularization term in the loss function. We forced the model to reconstruct not the same observation in input, but another one sampled from the same class distribution. We also explored the behaviour of the latent space in the case of reconstruction of a random sample from the whole dataset.
</details>
<details>
<summary>摘要</summary>
自然语言处理中的Autoencoder是非常常用的深度学习模型之一，其核心思想是通过训练模型来重建输入数据。Autoencoder模型具有压缩信息的特点，创造了所谓的缓存空间（Latent Space）。这些模型通常用于维度减少、异常检测和特征提取。这些模型已经得到了广泛的研究和更新，因为它们的简单性和力量。例如，（i）噪声Autoencoder，其中模型通过噪声图像重建原始图像；（ii）稀疏Autoencoder，其中瓶颈是通过惩罚项来创造的；（iii）变量Autoencoder，其中缓存空间用于生成新的一致性数据。在这篇文章中，我们重新训练了含有缺失的Autoencoder模型，不使用任何显式的惩罚项在损失函数中。我们让模型重建不同的输入数据，而不是原始输入数据。我们还探索了缓存空间在重建整个数据集中的行为。
</details></li>
</ul>
<hr>
<h2 id="MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval"><a href="#MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval" class="headerlink" title="MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval"></a>MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01516">http://arxiv.org/abs/2309.01516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longkukuhi/multiway-adapter">https://github.com/longkukuhi/multiway-adapter</a></li>
<li>paper_authors: Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</li>
<li>for: 这个研究旨在解决大型多 modal 模型（LMMs）的适应问题，尤其是在新任务上进行高效的适应和传播知识。</li>
<li>methods: 我们提出了一个创新的框架，名为 Multiway-Adapter，它包括一个“对齐增强器”，用于深入对齐不同模式之间的知识。这个方法增加了LMMs中的 fewer than 1.25% 的额外参数，并在零基eline image-text搜寻中表现出色，而且可以降低 fine-tuning 时间达57%。</li>
<li>results: 我们的方法可以实现LMMs的资源有效适应和传播知识，并且可以提高零基eline image-text搜寻的性能，而且可以降低 fine-tuning 时间。<details>
<summary>Abstract</summary>
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and effective adaptation pathway for LMMs, broadening their applicability. The source code is publicly available at: \url{https://github.com/longkukuhi/MultiWay-Adapter}.
</details>
<details>
<summary>摘要</summary>
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57% reduction in fine-tuning time. Our approach offers a resource-efficient and effective adaptation pathway for LMMs, broadening their applicability. The source code is publicly available at: \url{https://github.com/longkukuhi/MultiWay-Adapter}.Here's the translation in Traditional Chinese:为了解决大型多modal模型（LMMs）的 Parameters 规模不断增加所带来的 computationally 和 memory-intensive 挑战，传统的 fine-tuning 方法通常需要隔离的、耗时的 retuning  для each new task，限制模型的多样性。此外，现有的高效的 adaptation 技术 oft overlook modality alignment，专注于新任务的知识提取。为了解决这些问题，我们介绍 Multiway-Adapter，一个创新的框架，包括一个 'Alignment Enhancer'，以深化modal alignment，实现高转移性 без fine-tuning 预训练模型的 Parameters。我们的方法仅增加 LMMs 中的 fewer than 1.25% 的额外参数，例如 BEiT-3 模型在我们的研究中。这导致在 zero-shot 图像文本搜寻性能比完全 fine-tuned 模型更高，同时可以 achieve up to 57% 的 fine-tuning 时间减少。我们的方法提供了资源效率的和有效的 adaptation 通路 для LMMs，扩展其应用范围。source code 公开可用于：\url{https://github.com/longkukuhi/MultiWay-Adapter}.
</details></li>
</ul>
<hr>
<h2 id="RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes"><a href="#RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes" class="headerlink" title="RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes"></a>RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01513">http://arxiv.org/abs/2309.01513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inmo Yeon, Jung-Woo Choi</li>
<li>for: 这个论文是为了提出一种新的室内geometry推导方法，不需要先知道室内形状是 convex 的，也不需要知道墙的数量。</li>
<li>methods: 该方法使用了深度神经网络（DNN），称为 RGI-Net，可以从室内响应函数（RIR）中推导室内形状。 RGI-Net 学习和利用室内高阶反射的复杂关系，因此可以在非拥圆形室内或缺失首际反射情况下估计室内形状。</li>
<li>results: 该方法可以在实际场景中应用，只需要使用一个圆形 Mikrofon 阵列和一个单个扬声器，可以大幅提高实用性。 RGI-Net 还包括评估网络，可以分别评估墙的存在概率，因此不需要先知道墙的数量。<details>
<summary>Abstract</summary>
Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphone array and a single loudspeaker, which greatly improves its practical applicability. RGI-Net includes the evaluation network that separately evaluates the presence probability of walls, so the geometry inference is possible without prior knowledge of the number of walls.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Neural-Vector-Fields-Generalizing-Distance-Vector-Fields-by-Codebooks-and-Zero-Curl-Regularization"><a href="#Neural-Vector-Fields-Generalizing-Distance-Vector-Fields-by-Codebooks-and-Zero-Curl-Regularization" class="headerlink" title="Neural Vector Fields: Generalizing Distance Vector Fields by Codebooks and Zero-Curl Regularization"></a>Neural Vector Fields: Generalizing Distance Vector Fields by Codebooks and Zero-Curl Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01512">http://arxiv.org/abs/2309.01512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianghui Yang, Guosheng Lin, Zhenghao Chen, Luping Zhou</li>
<li>for: 提出了一种新的3D表示方法，即神经Vector Fields（NVF），可以同时优化表示度和拟合精度。</li>
<li>methods: 该方法采用直接预测表面上的变换，而不是通过网络导数来获取方向场，从而解决了表面EXTRACTION的问题。此外，该方法还提出了两种形状代码库，即NVF（Lite或Ultra），以促进跨类重建。</li>
<li>results: 在四个表面重建场景中，NVF（Ultra）表现出色，包括水密vs非水密形状、Category-agnostic重建vs Category-unseen重建、Category-specific重建和跨域重建。<details>
<summary>Abstract</summary>
Recent neural networks based surface reconstruction can be roughly divided into two categories, one warping templates explicitly and the other representing 3D surfaces implicitly. To enjoy the advantages of both, we propose a novel 3D representation, Neural Vector Fields (NVF), which adopts the explicit learning process to manipulate meshes and implicit unsigned distance function (UDF) representation to break the barriers in resolution and topology. This is achieved by directly predicting the displacements from surface queries and modeling shapes as Vector Fields, rather than relying on network differentiation to obtain direction fields as most existing UDF-based methods do. In this way, our approach is capable of encoding both the distance and the direction fields so that the calculation of direction fields is differentiation-free, circumventing the non-trivial surface extraction step. Furthermore, building upon NVFs, we propose to incorporate two types of shape codebooks, \ie, NVFs (Lite or Ultra), to promote cross-category reconstruction through encoding cross-object priors. Moreover, we propose a new regularization based on analyzing the zero-curl property of NVFs, and implement this through the fully differentiable framework of our NVF (ultra). We evaluate both NVFs on four surface reconstruction scenarios, including watertight vs non-watertight shapes, category-agnostic reconstruction vs category-unseen reconstruction, category-specific, and cross-domain reconstruction.
</details>
<details>
<summary>摘要</summary>
最近的神经网络基于表面重建可以大致分为两类，一是显式填充模板，另一是表示3D表面的隐式表示。为了利用这两者的优点，我们提出了一种新的3D表示方法，即神经向量场（NVF），它采用显式学习过程来操纵网格和隐式无符号距离函数（UDF）表示来突破分辨率和结构的限制。在这种方式下，我们直接预测表面上的变位异移，而不是通过网络导数来获取方向场，这样我们可以同时编码距离场和方向场，从而避免了不rivial的表面提取步骤。此外，我们在NVF的基础上，提出了两种形状码库，即NVF（Lite或Ultra），以便通过编码跨物类约束来促进跨类重建。此外，我们还提出了一种基于NVF的零核性分析 regularization，并通过我们的完全导数可 differentiable 框架来实现。我们在四个表面重建场景中评估了NVF，包括非水平 shapes、类型不可知的重建、类型特定重建和跨领域重建。
</details></li>
</ul>
<hr>
<h2 id="Memory-Efficient-Optimizers-with-4-bit-States"><a href="#Memory-Efficient-Optimizers-with-4-bit-States" class="headerlink" title="Memory Efficient Optimizers with 4-bit States"></a>Memory Efficient Optimizers with 4-bit States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01507">http://arxiv.org/abs/2309.01507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/low-bit-optimizers">https://github.com/thu-ml/low-bit-optimizers</a></li>
<li>paper_authors: Bingrui Li, Jianfei Chen, Jun Zhu</li>
<li>for: 降低各种神经网络训练中的内存占用，使得训练模型在给定内存预算内可以达到最大训练模型。</li>
<li>methods: 通过对首 moments和次 moments进行详细的实验分析，下降优化器状态的有效位数至4位。特别是，我们发现了outsider pattern，现有的块 wise quantization无法准确地近似。我们使用更小的块大小，并使用行 wise和列 wise信息进行更好的量化。此外，我们还解决了量化第二 moment的零点问题，使用了 exclude 零点的直线量化器。</li>
<li>results: 我们在各种 benchmark 上评估了我们的4位优化器，包括自然语言理解、机器翻译、图像分类和指令调整。在所有任务上，我们的优化器可以与其全精度对手相当，同时具有更好的内存效率。<details>
<summary>Abstract</summary>
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment"><a href="#BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment" class="headerlink" title="BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment"></a>BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01480">http://arxiv.org/abs/2309.01480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan</li>
<li>for: 本研究旨在攻击非侵入式语音质量评估（NISQA）系统，以实现高度隐蔽的攻击。</li>
<li>methods: 本研究提出了一种基于存在事件的新型后门攻击方法，可以在NISQA任务中实现高度隐蔽的攻击。</li>
<li>results: 实验结果表明，提出的后门攻击方法在四个基准数据集和两个现状最佳NISQA模型下，可以 достиieves an average attack success rate of up to 99% with a poisoning rate of only 3%.<details>
<summary>Abstract</summary>
Non-Intrusive speech quality assessment (NISQA) has gained significant attention for predicting the mean opinion score (MOS) of speech without requiring the reference speech. In practical NISQA scenarios, untrusted third-party resources are often employed during deep neural network training to reduce costs. However, it would introduce a potential security vulnerability as specially designed untrusted resources can launch backdoor attacks against NISQA systems. Existing backdoor attacks primarily focus on classification tasks and are not directly applicable to NISQA which is a regression task. In this paper, we propose a novel backdoor attack on NISQA tasks, leveraging presence events as triggers to achieving highly stealthy attacks. To evaluate the effectiveness of our proposed approach, we conducted experiments on four benchmark datasets and employed two state-of-the-art NISQA models. The results demonstrate that the proposed backdoor attack achieved an average attack success rate of up to 99% with a poisoning rate of only 3%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pure-Monte-Carlo-Counterfactual-Regret-Minimization"><a href="#Pure-Monte-Carlo-Counterfactual-Regret-Minimization" class="headerlink" title="Pure Monte Carlo Counterfactual Regret Minimization"></a>Pure Monte Carlo Counterfactual Regret Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03084">http://arxiv.org/abs/2309.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Qi, Ting Feng, Falun Hei, Zhemei Fang, Yunfeng Luo</li>
<li>for:  solves large-scale incomplete information games</li>
<li>methods:  builds upon CFR and Fictitious Play, combines counterfactual regret and best response strategy</li>
<li>results:  achieves better performance, reduces time and space complexity, and converges faster than MCCFR with a new warm-start algorithm<details>
<summary>Abstract</summary>
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strategies elimination method. Consequently, the PMCCFR with new warm start algorithm can converge by two orders of magnitude faster than the CFR+ algorithm.
</details>
<details>
<summary>摘要</summary>
Counterfactual Regret Minimization (CFR) 和其变种是目前最佳的大规模不完整信息游戏解决方案。本文提出了一种新的算法名为纯Counterfactual Regret Minimization (PCFR)，以实现更好的性能。PCFR可以看作是CFR和虚拟玩家(FP)的组合，沿用CFR中的counterfactual regret（价值）概念，并使用下一轮的最佳回应策略而不是 regret matching 策略。我们提供了理论证明，表明PCFR可以实现黑尔方程（Blackwell）可接近性，这使得PCFR可以与任何CFR变种，包括Monte Carlo CFR (MCCFR)结合。结果是PMCCFR可以显著降低时间和空间复杂度。尤其是PMCCFR的快速收敛速度至少三倍于MCCFR。此外，由于PMCCFR不通过严格dominated策略的路径，我们开发了一种新的暖启动算法， drawing inspiration from the strictly dominated strategies elimination method。因此，PMCCFR with new warm start algorithm可以在CFR+算法 convergence speed two orders of magnitude faster。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Graph-Convolutional-Filtering"><a href="#Interactive-Graph-Convolutional-Filtering" class="headerlink" title="Interactive Graph Convolutional Filtering"></a>Interactive Graph Convolutional Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01453">http://arxiv.org/abs/2309.01453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Zhang, Defu Lian, Hong Xie, Yawen Li, Enhong Chen</li>
<li>for: 这 paper 的目的是提出一种解决交互推荐系统中的冷启动和数据稀缺问题的新方法。</li>
<li>methods: 这 paper 使用了一种基于图模型的交互卷积滤波器模型，并使用了变量推理技术来解决非线性模型的计算困难。它还使用了 bayesian 元学习方法来有效地解决冷启动问题，并 derive 了对方法的理论 regret 下界，以确保方法的稳定性。</li>
<li>results:  experiments 表明，该方法在三个实际 dataset 上表现出色，比存在的基准模型更高。<details>
<summary>Abstract</summary>
Interactive Recommender Systems (IRS) have been increasingly used in various domains, including personalized article recommendation, social media, and online advertising. However, IRS faces significant challenges in providing accurate recommendations under limited observations, especially in the context of interactive collaborative filtering. These problems are exacerbated by the cold start problem and data sparsity problem. Existing Multi-Armed Bandit methods, despite their carefully designed exploration strategies, often struggle to provide satisfactory results in the early stages due to the lack of interaction data. Furthermore, these methods are computationally intractable when applied to non-linear models, limiting their applicability. To address these challenges, we propose a novel method, the Interactive Graph Convolutional Filtering model. Our proposed method extends interactive collaborative filtering into the graph model to enhance the performance of collaborative filtering between users and items. We incorporate variational inference techniques to overcome the computational hurdles posed by non-linear models. Furthermore, we employ Bayesian meta-learning methods to effectively address the cold-start problem and derive theoretical regret bounds for our proposed method, ensuring a robust performance guarantee. Extensive experimental results on three real-world datasets validate our method and demonstrate its superiority over existing baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks"><a href="#Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks" class="headerlink" title="Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks"></a>Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02460">http://arxiv.org/abs/2309.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Ding, Jieming Shi, Qing Li, Jiannong Cao</li>
<li>for: 本研究旨在检测 криптовалюencies 交易网络上的非法帐户，以防止normal用户 suffer 亏金额。</li>
<li>methods: 本文使用 DIAM 模型，该模型包括 Edge2Seq 模块和 Multigraph Discrepancy (MGD) 模块，自动学习有效的节点表示，并 capture 非法节点特征。</li>
<li>results: 对于 4 个大型 криптовалюencies 数据集（Bitcoin 和 Ethereum），DIAM 模型与 14 种现有解决方案进行比较，并 consistently 实现最佳性能，准确地检测非法帐户，而且高效。例如，在一个 Bitcoin 数据集上，DIAM 模型 achieve F1 score 96.55%，significantly higher than 最佳竞争者的 F1 score 83.92%。<details>
<summary>Abstract</summary>
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing the multigraph topology, DIAM employs a new Multigraph Discrepancy (MGD) module with a well-designed message passing mechanism to capture the discrepant features between normal and illicit nodes, supported by an attention mechanism. Assembling all techniques, DIAM is trained in an end-to-end manner. Extensive experiments, comparing against 14 existing solutions on 4 large cryptocurrency datasets of Bitcoin and Ethereum, demonstrate that DIAM consistently achieves the best performance to accurately detect illicit accounts, while being efficient. For instance, on a Bitcoin dataset with 20 million nodes and 203 million edges, DIAM achieves F1 score 96.55%, significantly higher than the F1 score 83.92% of the best competitor.
</details>
<details>
<summary>摘要</summary>
我们研究非法账户检测在 криптовалюencies 交易网络上，这些网络在在线金融市场中变得越来越重要。非法活动对于正常用户而言导致了亿万元的损失。现有的解决方案可以分为两类：一是 tedious 的特征工程来获取手工特征，二是不充分利用 криптовалюencies 交易数据的semantics，导致效果不佳。在这篇论文中，我们将非法账户检测问题定义为一个分类任务，并提出了一种多граaph神经网络模型（DIAM），可以有效地检测非法账户在大规模交易网络上。首先，DIAM包含一个 Edge2Seq 模块，可以自动学习有效的节点表示，保留交易 patrerns 的内在特征，通过考虑边Attributes和指向edge sequence dependencies。然后，DIAM使用一种新的多граaph不匹配度（MGD）模块，通过一种合理的消息传递机制，捕捉非法和正常节点之间的不同特征。最后，DIAM结合所有技术，在端到端方式进行训练。广泛的实验证明，对于4个大的 криптовалюencies dataset（Bitcoin和Ethereum），DIAM在14种现有解决方案中显示出了最高的性能，可以准确地检测非法账户，同时具有高效性。例如，在一个 Bitcoin dataset中，包含2000万个节点和203亿个边，DIAM的 F1 分数为96.55%，远高于最佳竞争者的 F1 分数83.92%。
</details></li>
</ul>
<hr>
<h2 id="Social-Factors-in-P2P-Energy-Trading-Using-Hedonic-Games"><a href="#Social-Factors-in-P2P-Energy-Trading-Using-Hedonic-Games" class="headerlink" title="Social Factors in P2P Energy Trading Using Hedonic Games"></a>Social Factors in P2P Energy Trading Using Hedonic Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01418">http://arxiv.org/abs/2309.01418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Mitrea, Viorica Chifu, Tudor Cioara, Ionut Anghel, Cristina Pop</li>
<li>for: 这篇论文的目的是提出一种基于hedonic game的P2P能源交易模型，以便在能源社区中帮助潜在的购买者和卖家之间进行能源交易，同时考虑社交因素。</li>
<li>methods: 该模型使用hedonic game理论来协调和合作，考虑了社交关系内的能源价格和社会偏好，并且通过了区块链技术来实现P2P能源交易。</li>
<li>results: 在一个实验中，该模型在能源社区中提高了总能源交易量的5%，并且在社交环境下帮助提高了能源交易量的10%，同时帮助实现了社区内的能源需求和供应的更好均衡。<details>
<summary>Abstract</summary>
Lately, the energy communities have gained a lot of attention as they have the potential to significantly contribute to the resilience and flexibility of the energy system, facilitating widespread integration of intermittent renewable energy sources. Within these communities the prosumers can engage in peer-to-peer trading, fostering local collaborations and increasing awareness about energy usage and flexible consumption. However, even under these favorable conditions, prosumer engagement levels remain low, requiring trading mechanisms that are aligned with their social values and expectations. In this paper, we introduce an innovative hedonic game coordination and cooperation model for P2P energy trading among prosumers which considers the social relationships within an energy community to create energy coalitions and facilitate energy transactions among them. We defined a heuristic that optimizes the prosumers coalitions, considering their social and energy price preferences and balancing the energy demand and supply within the community. We integrated the proposed hedonic game model into a state-of-the-art blockchain-based P2P energy flexibility market and evaluated its performance within an energy community of prosumers. The evaluation results on a blockchain-based P2P energy flexibility market show the effectiveness in considering social factors when creating coalitions, increasing the total amount of energy transacted in a market session by 5% compared with other game theory-based solutions. Finally, it shows the importance of the social dimensions of P2P energy transactions, the positive social dynamics in the energy community increasing the amount of energy transacted by more than 10% while contributing to a more balanced energy demand and supply within the community.
</details>
<details>
<summary>摘要</summary>
近些时间，能源社区获得了很多关注，因为它们可以为能源系统的可持续性和灵活性做出重要贡献，激发广泛的可变性可再生能源源泉的Integration。在这些社区中，潜在消费者（prosumer）可以进行Peer-to-Peer（P2P）贸易，促进本地合作和提高能源消耗和灵活消耗的认识。然而，即使在这些有利条件下，潜在消费者参与度仍然低，需要与他们的社会价值观和期望相匹配的交易机制。在这篇论文中，我们介绍了一种创新的 Hedonic Game 协调和合作模型，用于P2P能源贸易中潜在消费者之间的协作。我们定义了一个启发函数，用于优化潜在消费者的协会，考虑其社会和能源价格偏好，并均衡能源需求和供应在社区内。我们将该模型集成到了一个国际领先的区块链技术基础的P2P能源灵活市场中，并对其在能源社区中的潜在消费者进行评估。评估结果表明，考虑社会因素时创建协会可以提高P2P能源贸易市场Session中的总能源交易量，比其他Game theory基础的解决方案提高5%。此外，它还表明了社会维度上的P2P能源贸易的重要性，通过提高能源社区内的能源交易量高于10%，同时为能源需求和供应做出更好的均衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging"><a href="#Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging" class="headerlink" title="Towards frugal unsupervised detection of subtle abnormalities in medical imaging"></a>Towards frugal unsupervised detection of subtle abnormalities in medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02458">http://arxiv.org/abs/2309.02458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geoffroyo/onlineem">https://github.com/geoffroyo/onlineem</a></li>
<li>paper_authors: Geoffroy Oudoumanessah, Carole Lartizien, Michel Dojat, Florence Forbes</li>
<li>for: 这篇论文的目的是提出一种基于混合分布的无监督异常检测方法，来探析医疗影像中的异常现象。</li>
<li>methods: 这篇论文使用了混合分布的方法，包括mixtures of probability distributions，来进行异常检测。这种方法可以处理复杂的多个变量参考模型，并且具有较少的参数和较好的解释性。然而，标准的估计方法，如期望最大化算法，不适合处理大量数据，因为它们需要高度的内存使用。</li>
<li>results: 这篇论文的结果显示，使用混合分布的方法可以实现高度的异常检测精度，并且可以适应不同的医疗影像数据。实验结果显示，这种方法可以检测出 Parkinson 病患的脑部畸形，并且与 Hoehn 和 Yahr 病程 scales 相符。<details>
<summary>Abstract</summary>
Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of normal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-o$\hookleftarrow$ between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design e$\hookleftarrow$ort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and e cient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data volumes as they require high memory usage. To address this issue, we propose to incrementally compute inferential quantities. This online approach is illustrated on the challenging detection of subtle abnormalities in MR brain scans for the follow-up of newly diagnosed Parkinsonian patients. The identified structural abnormalities are consistent with the disease progression, as accounted by the Hoehn and Yahr scale.
</details>
<details>
<summary>摘要</summary>
医学成像异常检测在没有标注异常的情况下是一个挑战。这个问题可以通过无监督异常检测（USD）方法解决，这些方法可以标识不符合参照模型的常见 Profile 中的特征。人工神经网络已经广泛应用于 USD，但它们通常不能达到最佳的准确性和计算成本之间的平衡。作为一个替代方案，我们调查混合概率分布的使用。这种混合分布的多样性使其适用于多种数据和任务，而不需要过度的设计或调整。它的表达能力使其成为质量异常检测中的好选择，但标准估计过程，如期望最大化算法，不适用于大量数据。为解决这个问题，我们提议逐步计算推理量。这种在线方法在对抗性检测MR brain scan中的轻微异常情况中得到了描述。已经标识出的结构异常与疾病进程相符，如根据豪恩-雅尔scale的评估。
</details></li>
</ul>
<hr>
<h2 id="Metric-Learning-for-Projections-Bias-of-Generalized-Zero-shot-Learning"><a href="#Metric-Learning-for-Projections-Bias-of-Generalized-Zero-shot-Learning" class="headerlink" title="Metric Learning for Projections Bias of Generalized Zero-shot Learning"></a>Metric Learning for Projections Bias of Generalized Zero-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01390">http://arxiv.org/abs/2309.01390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Zhang, Mingyu Jin, Qinkai Yu, Haochen Xue, Xiaobo Jin</li>
<li>for: 本研究旨在提高Generalized Zero-shot Learning（GZSL）模型的可靠性和效果，使其能够正确识别未经见过的类别。</li>
<li>methods: 本研究使用了Variational Autoencoder &amp; Generative Adversarial Networks（VAEGAN）框架，并提出了一种新的参数化 Mahalanobis 距离表示法，以便在推理过程中减少偏见。同时，我们改进了VAEGAN 网络结构，以便使用两个分支来分别预测已经见过的样本和通过这个seen样本生成的未经见过的样本。</li>
<li>results: 我们在四个数据集上进行了广泛的评估，并证明了我们的方法在与状态方法相比有superiority。 codes 可以在<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/111hxr">https://anonymous.4open.science/r/111hxr</a> 上获取。<details>
<summary>Abstract</summary>
Generalized zero-shot learning models (GZSL) aim to recognize samples from seen or unseen classes using only samples from seen classes as training data. During inference, GZSL methods are often biased towards seen classes due to the visibility of seen class samples during training. Most current GZSL methods try to learn an accurate projection function (from visual space to semantic space) to avoid bias and ensure the effectiveness of GZSL methods. However, during inference, the computation of distance will be important when we classify the projection of any sample into its nearest class since we may learn a biased projection function in the model. In our work, we attempt to learn a parameterized Mahalanobis distance within the framework of VAEGAN (Variational Autoencoder \& Generative Adversarial Networks), where the weight matrix depends on the network's output. In particular, we improved the network structure of VAEGAN to leverage the discriminative models of two branches to separately predict the seen samples and the unseen samples generated by this seen one. We proposed a new loss function with two branches to help us learn the optimized Mahalanobis distance representation. Comprehensive evaluation benchmarks on four datasets demonstrate the superiority of our method over the state-of-the-art counterparts. Our codes are available at https://anonymous.4open.science/r/111hxr.
</details>
<details>
<summary>摘要</summary>
通用零shot学习模型（GZSL）目标是使用已知类样本进行训练，并在测试时recognize未知类样本。然而，大多数现有GZSL方法在测试时仍然受到已知类样本的影响，导致模型偏向已知类。为解决这个问题，现有的GZSL方法通常尝试学习一个准确的投影函数（从视觉空间到 semantic空间），以避免偏见和保证GZSL方法的效果。然而，在测试时，计算距离是非常重要的，因为我们可能会学习一个偏向的投影函数。在我们的工作中，我们尝试了在VAEGAN（variational autoencoder & generative adversarial networks）框架中学习一个参数化的马ха拉诺比斯距离。具体来说，我们改进了VAEGAN的网络结构，使其能够利用两个分支的探测模型来分别预测已知样本和由已知样本生成的未知样本。我们提出了一个新的两支loss函数，以帮助我们学习优化的马ха拉诺比斯距离表示。我们在四个数据集上进行了全面的评估，并证明了我们的方法在当前的状态革命性。我们的代码可以在https://anonymous.4open.science/r/111hxr中获取。
</details></li>
</ul>
<hr>
<h2 id="LoRA-like-Calibration-for-Multimodal-Deception-Detection-using-ATSFace-Data"><a href="#LoRA-like-Calibration-for-Multimodal-Deception-Detection-using-ATSFace-Data" class="headerlink" title="LoRA-like Calibration for Multimodal Deception Detection using ATSFace Data"></a>LoRA-like Calibration for Multimodal Deception Detection using ATSFace Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01383">http://arxiv.org/abs/2309.01383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun-Wen Hsiao, Cheng-Yuan Sun</li>
<li>for: 本研究旨在开发一种能够有效地检测人类视频中的谎言，并提供可读性的模型。</li>
<li>methods: 我们提出了一种注意力意识的神经网络模型，该模型通过综合评估视频、音频和文本特征，找到谎言的表征。我们还使用多模态融合策略，提高了准确率。</li>
<li>results: 我们在一个真实的评估数据集上实现了92%的准确率。此外，模型还可以显示视频中的注意力焦点，为检测谎言提供了有价值的信息。<details>
<summary>Abstract</summary>
Recently, deception detection on human videos is an eye-catching techniques and can serve lots applications. AI model in this domain demonstrates the high accuracy, but AI tends to be a non-interpretable black box. We introduce an attention-aware neural network addressing challenges inherent in video data and deception dynamics. This model, through its continuous assessment of visual, audio, and text features, pinpoints deceptive cues. We employ a multimodal fusion strategy that enhances accuracy; our approach yields a 92\% accuracy rate on a real-life trial dataset. Most important of all, the model indicates the attention focus in the videos, providing valuable insights on deception cues. Hence, our method adeptly detects deceit and elucidates the underlying process. We further enriched our study with an experiment involving students answering questions either truthfully or deceitfully, resulting in a new dataset of 309 video clips, named ATSFace. Using this, we also introduced a calibration method, which is inspired by Low-Rank Adaptation (LoRA), to refine individual-based deception detection accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Memory-augment-is-All-You-Need-for-image-restoration"><a href="#Memory-augment-is-All-You-Need-for-image-restoration" class="headerlink" title="Memory augment is All You Need for image restoration"></a>Memory augment is All You Need for image restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01377">http://arxiv.org/abs/2309.01377</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangbaijin/memorynet">https://github.com/zhangbaijin/memorynet</a></li>
<li>paper_authors: Xiao Feng Zhang, Chao Chen Gu, Shan Ying Zhu</li>
<li>for: 本研究旨在提出一种基于三级层次记忆的图像恢复方法，以提高图像恢复性能。</li>
<li>methods: 该方法使用了一种名为MemoryNet的三级层次记忆层和对比学习策略，将样本分为正例、负例和实际三种样本，并通过对比学习来塑造学习的特征。</li>
<li>results: 实验表明，该方法在Derain&#x2F;Deshadow&#x2F;Deblur任务上能够提高图像恢复性能，并且在三个不同类型的质量畸变数据集上获得了显著的PSNR和SSIM提升，这是一种强有力的证明，表明恢复的图像是可见真实的。<details>
<summary>Abstract</summary>
Image restoration is a low-level vision task, most CNN methods are designed as a black box, lacking transparency and internal aesthetics. Although some methods combining traditional optimization algorithms with DNNs have been proposed, they all have some limitations. In this paper, we propose a three-granularity memory layer and contrast learning named MemoryNet, specifically, dividing the samples into positive, negative, and actual three samples for contrastive learning, where the memory layer is able to preserve the deep features of the image and the contrastive learning converges the learned features to balance. Experiments on Derain/Deshadow/Deblur task demonstrate that these methods are effective in improving restoration performance. In addition, this paper's model obtains significant PSNR, SSIM gain on three datasets with different degradation types, which is a strong proof that the recovered images are perceptually realistic. The source code of MemoryNet can be obtained from https://github.com/zhangbaijin/MemoryNet
</details>
<details>
<summary>摘要</summary>
Image restoration 是一个低级视觉任务，大多数 CNN 方法都是黑盒子，缺乏透明度和内部美学。虽然一些将传统优化算法与 DNN 结合的方法有被提议，但它们都有一些限制。在这篇论文中，我们提出了三级别内存层和对比学习名为 MemoryNet，具体来说，将样本分为正样本、负样本和实际三个样本进行对比学习，内存层能够保留图像深度特征，对比学习使得学习的特征进行平衡。实验表明，这些方法可以提高修复性能。此外，这篇论文的模型在三个不同类型的损害数据集上获得了显著的 PSNR、SSIM 提升，这是一个强大的证明，修复的图像是有感知真实的。MemoryNet 的源代码可以从 GitHub 上获取：https://github.com/zhangbaijin/MemoryNet
</details></li>
</ul>
<hr>
<h2 id="ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction"><a href="#ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction" class="headerlink" title="ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction"></a>ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01370">http://arxiv.org/abs/2309.01370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kracr/reonto-relation-extraction">https://github.com/kracr/reonto-relation-extraction</a></li>
<li>paper_authors: Monika Jain, Kuldeep Singh, Raghava Mutharaju</li>
<li>for: 这个研究旨在提高生物医学文本中关系提取 task 的性能，通过使用 neuromorphic 知识来解决生物医学关系的特殊性。</li>
<li>methods: 这种新的技术called ReOnto，使用图 neural network 获得句子表示，并利用公开 accessible  ontology 作为先验知识来识别两个实体之间的句子关系。</li>
<li>results: 实验结果表明，使用符号知识从 ontology 与图 neural network 结合使用，可以超过所有基线（约3%）。<details>
<summary>Abstract</summary>
Relation Extraction (RE) is the task of extracting semantic relationships between entities in a sentence and aligning them to relations defined in a vocabulary, which is generally in the form of a Knowledge Graph (KG) or an ontology. Various approaches have been proposed so far to address this task. However, applying these techniques to biomedical text often yields unsatisfactory results because it is hard to infer relations directly from sentences due to the nature of the biomedical relations. To address these issues, we present a novel technique called ReOnto, that makes use of neuro symbolic knowledge for the RE task. ReOnto employs a graph neural network to acquire the sentence representation and leverages publicly accessible ontologies as prior knowledge to identify the sentential relation between two entities. The approach involves extracting the relation path between the two entities from the ontology. We evaluate the effect of using symbolic knowledge from ontologies with graph neural networks. Experimental results on two public biomedical datasets, BioRel and ADE, show that our method outperforms all the baselines (approximately by 3\%).
</details>
<details>
<summary>摘要</summary>
relation extraction (RE) 是将 sentence 中 entities 之间的 semantic 关系提取出来，并将其与知识图（KG）或ontology 中定义的关系进行对应的任务。目前已经有很多方法提出来了。但是在生物医学文本中应用这些技术时，通常会得到不满足的结果，因为生物医学关系很难直接从句子中提取。为解决这些问题，我们提出了一种新的技术 called ReOnto，它利用 neurosymbolic 知识来进行 RE 任务。ReOnto 使用图ael 神经网络来获取句子表示，并利用公共可访问的 ontology 作为先验知识来确定句子中两个 entit 之间的关系。该方法包括从 ontology 中提取两个 entit 之间的关系路径。我们通过对 symbolic 知识和 graph neural networks 的结合效果进行实验，并在两个公共生物医学数据集（BioRel 和 ADE）上进行了评估。结果表明，我们的方法比所有基eline（约为 3%） superior。
</details></li>
</ul>
<hr>
<h2 id="Refined-Temporal-Pyramidal-Compression-and-Amplification-Transformer-for-3D-Human-Pose-Estimation"><a href="#Refined-Temporal-Pyramidal-Compression-and-Amplification-Transformer-for-3D-Human-Pose-Estimation" class="headerlink" title="Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation"></a>Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01365">http://arxiv.org/abs/2309.01365</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hbing-l/rtpca">https://github.com/hbing-l/rtpca</a></li>
<li>paper_authors: Hanbing Liu, Wangmeng Xiang, Jun-Yan He, Zhi-Qi Cheng, Bin Luo, Yifeng Geng, Xuansong Xie</li>
<li>for: 优化3D人体pose预测器的精度和结构，基于transformer的Refined Temporal Pyramidal Compression-and-Amplification（RTPCA）模型。</li>
<li>methods: 利用时间维度，RTPCA模型通过Temporal Pyramidal Compression-and-Amplification（TPCA）块和 Cross-Layer Refinement（XLR）模块，提高了内块时间模型和 между块特征交互。TPCA块利用时间 pyramid 思想，强化关键和值表示能力，并从运动序列中提取空间 semantics。XLR模块通过不断交互查询、键和值，营养丰富的semantic表示。</li>
<li>results: 在Human3.6M、HumanEva-I和MPI-INF-3DHP测试集上达到了state-of-the-art result，与其他基于transformer的方法相比，具有较少的计算负担。<details>
<summary>Abstract</summary>
Accurately estimating the 3D pose of humans in video sequences requires both accuracy and a well-structured architecture. With the success of transformers, we introduce the Refined Temporal Pyramidal Compression-and-Amplification (RTPCA) transformer. Exploiting the temporal dimension, RTPCA extends intra-block temporal modeling via its Temporal Pyramidal Compression-and-Amplification (TPCA) structure and refines inter-block feature interaction with a Cross-Layer Refinement (XLR) module. In particular, TPCA block exploits a temporal pyramid paradigm, reinforcing key and value representation capabilities and seamlessly extracting spatial semantics from motion sequences. We stitch these TPCA blocks with XLR that promotes rich semantic representation through continuous interaction of queries, keys, and values. This strategy embodies early-stage information with current flows, addressing typical deficits in detail and stability seen in other transformer-based methods. We demonstrate the effectiveness of RTPCA by achieving state-of-the-art results on Human3.6M, HumanEva-I, and MPI-INF-3DHP benchmarks with minimal computational overhead. The source code is available at https://github.com/hbing-l/RTPCA.
</details>
<details>
<summary>摘要</summary>
准确估计视频序列中人体3D姿势需要 Both accuracy和一个良好的架构。在Transformers的成功基础上，我们引入Refined Temporal Pyramidal Compression-and-Amplification（RTPCA）transformer。利用时间维度，RTPCA通过其Temporal Pyramidal Compression-and-Amplification（TPCA）结构进一步发挥 intra-block 时间模型化，并使用 Cross-Layer Refinement（XLR）模块来细化 inter-block 特征互动。特别是，TPCA块采用了时间PYRAMID思想，强化关键和值表示能力，并快速从运动序列中提取空间语义。我们将这些TPCA块与XLR相连，以便通过 queries、keys 和values之间的连续互动，实现丰富的semantic表示。这种策略既保留了早期信息，又与当前流量互动，解决了其他基于Transformer的方法中常见的缺失细节和稳定性问题。我们在Human3.6M、HumanEva-I和MPI-INF-3DHP benchmark上实现了state-of-the-art的结果，而且计算开销很小。代码可以在https://github.com/hbing-l/RTPCA中获取。
</details></li>
</ul>
<hr>
<h2 id="Self-driven-Grounding-Large-Language-Model-Agents-with-Automatical-Language-aligned-Skill-Learning"><a href="#Self-driven-Grounding-Large-Language-Model-Agents-with-Automatical-Language-aligned-Skill-Learning" class="headerlink" title="Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning"></a>Self-driven Grounding: Large Language Model Agents with Automatical Language-aligned Skill Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01352">http://arxiv.org/abs/2309.01352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang, Zikang Tian, Ruizhi Chen, Zidong Du, Qi Guo, Yunji Chen, Ling Li</li>
<li>for: 提高大语言模型在真实环境中的应用能力</li>
<li>methods: 自动提出子目标、与环境互动验证、自适应学习练习技能</li>
<li>results: 在知名的 instruktion following task 上比较出色的表现，与循证学习方法相当，但需要更少的示范数据，证明学习到的技能有效并证明了框架的可行性和效率。<details>
<summary>Abstract</summary>
Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Self-Driven Grounding (SDG) framework to automatically and progressively ground the LLM with self-driven skill learning. SDG first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, SDG can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks which fail to pass the verification phase. Verified in the famous instruction following task set-BabyAI, SDG achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework.
</details>
<details>
<summary>摘要</summary>
SDG first uses the LLM to propose hypotheses of sub-goals to achieve tasks and then verifies the feasibility of these hypotheses by interacting with the underlying environment. Once verified, SDG can learn generalized skills with the guidance of these successfully grounded sub-goals. These skills can be used to accomplish more complex tasks that fail to pass the verification phase. In the famous instruction following task set-BabyAI, SDG achieves comparable performance in the most challenging tasks with millions of demonstrations, demonstrating the effectiveness of learned skills and the feasibility and efficiency of our framework.
</details></li>
</ul>
<hr>
<h2 id="UniSA-Unified-Generative-Framework-for-Sentiment-Analysis"><a href="#UniSA-Unified-Generative-Framework-for-Sentiment-Analysis" class="headerlink" title="UniSA: Unified Generative Framework for Sentiment Analysis"></a>UniSA: Unified Generative Framework for Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01339">http://arxiv.org/abs/2309.01339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dawn0815/saeval-benchmark">https://github.com/dawn0815/saeval-benchmark</a></li>
<li>paper_authors: Zaijing Li, Ting-En Lin, Yuchuan Wu, Meng Liu, Fengxiao Tang, Ming Zhao, Yongbin Li</li>
<li>for: 本研究旨在解决各种情感分析子任务之间的协调问题，提高多模态情感分析的性能。</li>
<li>methods: 该研究提出了一种任务特定提示方法，并 introduce了一种多模态生成框架 named UniSA，以及一个新的情感分析评价标准 benchmark。</li>
<li>results: 实验结果表明，UniSA在各种情感分析子任务中表现 Comparable 于现状况，并且在不同子任务之间具有良好的泛化能力。<details>
<summary>Abstract</summary>
Sentiment analysis is a crucial task that aims to understand people's emotional states and predict emotional categories based on multimodal information. It consists of several subtasks, such as emotion recognition in conversation (ERC), aspect-based sentiment analysis (ABSA), and multimodal sentiment analysis (MSA). However, unifying all subtasks in sentiment analysis presents numerous challenges, including modality alignment, unified input/output forms, and dataset bias. To address these challenges, we propose a Task-Specific Prompt method to jointly model subtasks and introduce a multimodal generative framework called UniSA. Additionally, we organize the benchmark datasets of main subtasks into a new Sentiment Analysis Evaluation benchmark, SAEval. We design novel pre-training tasks and training methods to enable the model to learn generic sentiment knowledge among subtasks to improve the model's multimodal sentiment perception ability. Our experimental results show that UniSA performs comparably to the state-of-the-art on all subtasks and generalizes well to various subtasks in sentiment analysis.
</details>
<details>
<summary>摘要</summary>
（简化中文）情感分析是一项非常重要的任务，旨在理解人们的情感状态并根据多modal信息预测情感类别。它包括多个子任务，如对话中情感识别（ERC）、基于特征的情感分析（ABSA）和多modal情感分析（MSA）。然而，在情感分析中统一所有子任务存在许多挑战，包括模式匹配、统一输入/输出格式和数据集偏见。为了解决这些挑战，我们提出了任务特定提示方法，用于同时模型子任务，并引入了一个多modal生成框架called UniSA。此外，我们还将主要的 benchmark dataset组织成了一个新的情感分析评估 benchmark，称为 SAEval。我们还设计了新的预训练任务和训练方法，以便模型可以从多个子任务中学习通用的情感知识，提高模型的多modal情感感知能力。我们的实验结果显示，UniSA在所有子任务上表现相当于当前状态的顶尖水平，并且在不同的子任务中具有良好的通用性。
</details></li>
</ul>
<hr>
<h2 id="Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach"><a href="#Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach" class="headerlink" title="Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach"></a>Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01336">http://arxiv.org/abs/2309.01336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Dube, Natarajan Gautam, Amarnath Banerjee, Harsha Nagarajan</li>
<li>for: 这篇论文是为了提供一种基于差分bootstrap的日均电力需求Interval估计方法，以便在小聚合负荷setting中更好地管理运营。</li>
<li>methods: 该方法使用机器学习算法获取日均电力需求的点估计值，并使用这些点估计值和相应的差分来生成Interval估计值。具体来说，首先使用一种不supervised learning算法将日均电力需求数据分为类似的日期集合，然后将这些集合用于生成Interval估计值。</li>
<li>results: 该方法在使用实际电力需求数据进行评估时，与其他 bootstrap方法相比，能够更好地保持Interval估计值的准确性和稳定性。具体来说，该方法可以在不同的信任 интер val中提供更加精准的Interval估计值，并且可以避免因点估计值的偏差而导致的误差。<details>
<summary>Abstract</summary>
Accurate predictions of electricity demands are necessary for managing operations in a small aggregation load setting like a Microgrid. Due to low aggregation, the electricity demands can be highly stochastic and point estimates would lead to inflated errors. Interval estimation in this scenario, would provide a range of values within which the future values might lie and helps quantify the errors around the point estimates. This paper introduces a residual bootstrap algorithm to generate interval estimates of day-ahead electricity demand. A machine learning algorithm is used to obtain the point estimates of electricity demand and respective residuals on the training set. The obtained residuals are stored in memory and the memory is further partitioned. Days with similar demand patterns are grouped in clusters using an unsupervised learning algorithm and these clusters are used to partition the memory. The point estimates for test day are used to find the closest cluster of similar days and the residuals are bootstrapped from the chosen cluster. This algorithm is evaluated on the real electricity demand data from EULR(End Use Load Research) and is compared to other bootstrapping methods for varying confidence intervals.
</details>
<details>
<summary>摘要</summary>
准确的电力需求预测是微型电网运营管理中必要的。由于低聚合，电力需求具有高度抽象和点估计会带来膨胀的错误。间隔估计在这种情况下，可以提供未来值的范围，并帮助量化估计错误。本文介绍了剩余 Bootstrap 算法，用于生成间隔估计的日前电力需求。一个机器学习算法用于获取电力需求的点估计和相应的偏差在训练集上。获取的偏差被存储在内存中，并将内存进一步分区。根据类似的需求模式，天数被分组到 clusters 中使用不监督学习算法。测试日点估计用于找到最接近的 cluster，并从选择的 cluster 中 bootstrapping 偏差。这种算法在 EULR（End Use Load Research）实际电力需求数据上进行了评估，并与其他各种各样的启动方法进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Can-I-Trust-Your-Answer-Visually-Grounded-Video-Question-Answering"><a href="#Can-I-Trust-Your-Answer-Visually-Grounded-Video-Question-Answering" class="headerlink" title="Can I Trust Your Answer? Visually Grounded Video Question Answering"></a>Can I Trust Your Answer? Visually Grounded Video Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01327">http://arxiv.org/abs/2309.01327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/doc-doc/next-gqa">https://github.com/doc-doc/next-gqa</a></li>
<li>paper_authors: Junbin Xiao, Angela Yao, Yicong Li, Tat Seng Chua</li>
<li>for: 这个论文旨在探讨利用预处理技术来提高视频语言理解的趋势，具体来说是考虑视频语言模型（VLMs）能够回答问题并同时提供视觉证据，以确定这些技术的预测是否真正受到视频内容的支持，而不是语言或视觉上的偶合关系。</li>
<li>methods: 作者提出了NExT-GQA数据集，用于检验当今最佳的VLMs。通过后期注意力分析，发现这些模型尚未能够坚持回答的根据，这表明这些模型的预测不可靠。为此，作者提出了一种视频定位机制，包括 Gaussian mask 优化和跨模态学习。</li>
<li>results: 作者的实验表明，这种定位机制可以提高视频定位和回答。不同的后端模型的实验结果也表明，这种定位机制可以提高视频定位和回答的可靠性。<details>
<summary>Abstract</summary>
We study visually grounded VideoQA in response to the emerging trends of utilizing pretraining techniques for video-language understanding. Specifically, by forcing vision-language models (VLMs) to answer questions and simultaneously provide visual evidence, we seek to ascertain the extent to which the predictions of such techniques are genuinely anchored in relevant video content, versus spurious correlations from language or irrelevant visual context. Towards this, we construct NExT-GQA -- an extension of NExT-QA with 10.5$K$ temporal grounding (or location) labels tied to the original QA pairs. With NExT-GQA, we scrutinize a variety of state-of-the-art VLMs. Through post-hoc attention analysis, we find that these models are weak in substantiating the answers despite their strong QA performance. This exposes a severe limitation of these models in making reliable predictions. As a remedy, we further explore and suggest a video grounding mechanism via Gaussian mask optimization and cross-modal learning. Experiments with different backbones demonstrate that this grounding mechanism improves both video grounding and QA. Our dataset and code are released. With these efforts, we aim to push towards the reliability of deploying VLMs in VQA systems.
</details>
<details>
<summary>摘要</summary>
我们研究基于视觉的视频问答系统，响应现代技术的趋势，使用预训练技术来理解视频语言。特别是，我们要证明视频语言模型（VLM）的预测是否围绕视频内容进行 anchored，而不是语言或无关的视觉上下文的偶合。为此，我们构建了NExT-GQA数据集，包含10500个时间（或位置）标签，与原始问答对相关。通过对多种现状顶尖VLM进行探究，我们发现这些模型具有强大的问答能力，但却弱于证明答案的能力。这表明这些模型在作出可靠预测时存在严重的限制。为此，我们进一步探讨视频基准机制，包括 Gaussian 掩码优化和跨模态学习。实验表明，这种基准机制可以提高视频基准和问答能力。我们发布了数据集和代码，以便推动VLM在VQA系统中的可靠部署。
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates"><a href="#Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates" class="headerlink" title="Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates"></a>Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03227">http://arxiv.org/abs/2309.03227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysjegal/ysjegal-drug-repositioning">https://github.com/ysjegal/ysjegal-drug-repositioning</a></li>
<li>paper_authors: Yongseung Jegal, Jaewoong Choi, Jiho Lee, Ki-Su Park, Seyoung Lee, Janghyeok Yoon<br>for:This paper aims to present a novel protocol for identifying drug repositioning candidates with both technological potential and scientific evidence.methods:The protocol involves constructing a scientific biomedical knowledge graph (s-BKG) and a patent-informed biomedical knowledge graph (p-BKG), and using a graph embedding protocol to evaluate the relevance scores of potential drug candidates.results:The case study on Alzheimer’s disease demonstrates the efficacy and feasibility of the proposed method, and the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.<details>
<summary>Abstract</summary>
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we developed a graph embedding protocol to ascertain the structure of the p-BKG, thereby calculating the relevance scores of those candidates with target disease-related patents to evaluate their technological potential. Our case study on Alzheimer's disease demonstrates its efficacy and feasibility, while the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.
</details>
<details>
<summary>摘要</summary>
药物重新定位策略，即在现有药物上发现新的治疗用途，在计算科学文献中得到了越来越多的探索。然而，药物重新定位候选者的技术潜力经常被忽视。本研究提出了一种新的协议，用于全面分析各种来源，包括药品专利和生物医学数据库，并从科学角度评估药物重新定位候选者。为此，我们首先构建了一个生物医学知识图（s-BKG），其中包括药物、疾病和基因之间的科学关系，来自生物医学数据库。我们的协议是通过识别具有较少与目标疾病相关的药物，但与s-BKG相互关联的药物作为重新定位候选者。然后，我们将药品专利信息添加到了生物医学知识图（p-BKG）中，并开发了一个图像嵌入协议，以确定p-BKG的结构，从而计算重新定位候选者与疾病相关专利的相互关系。我们的实验案例涉及阿兹海默病，并证明了其可行性和实用性，而量化结果和系统方法即将bridge计算发现和成功应用在药物重新定位研究中的差距。
</details></li>
</ul>
<hr>
<h2 id="Code-Representation-Pre-training-with-Complements-from-Program-Executions"><a href="#Code-Representation-Pre-training-with-Complements-from-Program-Executions" class="headerlink" title="Code Representation Pre-training with Complements from Program Executions"></a>Code Representation Pre-training with Complements from Program Executions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.09980">http://arxiv.org/abs/2309.09980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiabo Huang, Jianyu Zhao, Yuyang Rong, Yiwen Guo, Yifeng He, Hao Chen</li>
<li>for: 提高代码智能的研究，使用大型自然语言处理模型（LLM）。</li>
<li>methods: 使用自定义随机测试工具生成测试用例，并将其用于预训练代码表示。</li>
<li>results: 与其他预训练方法相比，FuzzPretrain在代码搜索中提高了6%以上&#x2F;9%以上的MAP值。<details>
<summary>Abstract</summary>
Large language models (LLMs) for natural language processing have been grafted onto programming language modeling for advancing code intelligence. Although it can be represented in the text format, code is syntactically more rigorous in order to be properly compiled or interpreted to perform a desired set of behaviors given any inputs. In this case, existing works benefit from syntactic representations to learn from code less ambiguously in the forms of abstract syntax tree, control-flow graph, etc. However, programs with the same purpose can be implemented in various ways showing different syntactic representations while the ones with similar implementations can have distinct behaviors. Though trivially demonstrated during executions, such semantics about functionality are challenging to be learned directly from code, especially in an unsupervised manner. Hence, in this paper, we propose FuzzPretrain to explore the dynamic information of programs revealed by their test cases and embed it into the feature representations of code as complements. The test cases are obtained with the assistance of a customized fuzzer and are only required during pre-training. FuzzPretrain yielded more than 6%/9% mAP improvements on code search over its counterparts trained with only source code or AST, respectively. Our extensive experimental results show the benefits of learning discriminative code representations with program executions.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理（LLM）模型已经被应用到程序语言模型中，以提高代码智能。虽然代码可以被表示为文本格式，但它的语法更加严格，以便在输入任意时执行所需的行为。在这种情况下，现有的工作受益于语法表示，以更好地从代码中学习不同的行为。然而，实现相同的目的可以通过不同的方式实现，导致代码的语法表示异常。尽管在执行时可以轻松地证明这些semantics，但在无监督情况下学习代码的Semantics是困难的。因此，在这篇论文中，我们提出了FuzzPretrain，它可以通过测试用例来探索代码中的动态信息，并将其 embedding到代码的特征表示中作为补充。这些测试用例通过自定义的随机测试工具生成，只需在预训练时使用。FuzzPretrain在代码搜索中实现了6%/9%的mAP提升，与只使用源代码或AST训练的对手相比。我们的广泛的实验结果表明了从代码执行中学习特征代码表示的利antages。
</details></li>
</ul>
<hr>
<h2 id="ExMobileViT-Lightweight-Classifier-Extension-for-Mobile-Vision-Transformer"><a href="#ExMobileViT-Lightweight-Classifier-Extension-for-Mobile-Vision-Transformer" class="headerlink" title="ExMobileViT: Lightweight Classifier Extension for Mobile Vision Transformer"></a>ExMobileViT: Lightweight Classifier Extension for Mobile Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01310">http://arxiv.org/abs/2309.01310</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyeongdong Yang, Yungwook Kwon, Hyunjin Kim</li>
<li>for: 提高手机友好的视transformer性能，降低计算负担</li>
<li>methods: 使用均值池化结果来扩展通道数，再利用早期注意力阶段的信息</li>
<li>results: 相比原MobileViT，提高精度，仅增加5%参数量<details>
<summary>Abstract</summary>
The paper proposes an efficient structure for enhancing the performance of mobile-friendly vision transformer with small computational overhead. The vision transformer (ViT) is very attractive in that it reaches outperforming results in image classification, compared to conventional convolutional neural networks (CNNs). Due to its need of high computational resources, MobileNet-based ViT models such as MobileViT-S have been developed. However, their performance cannot reach the original ViT model. The proposed structure relieves the above weakness by storing the information from early attention stages and reusing it in the final classifier. This paper is motivated by the idea that the data itself from early attention stages can have important meaning for the final classification. In order to reuse the early information in attention stages, the average pooling results of various scaled features from early attention stages are used to expand channels in the fully-connected layer of the final classifier. It is expected that the inductive bias introduced by the averaged features can enhance the final performance. Because the proposed structure only needs the average pooling of features from the attention stages and channel expansions in the final classifier, its computational and storage overheads are very small, keeping the benefits of low-cost MobileNet-based ViT (MobileViT). Compared with the original MobileViTs on the ImageNet dataset, the proposed ExMobileViT has noticeable accuracy enhancements, having only about 5% additional parameters.
</details>
<details>
<summary>摘要</summary>
文章提出了一种高效的结构，以提高移动设备友好的视Transformer（ViT）性能，而不需要大量计算资源。由于ViT模型的需求高于常见的卷积神经网络（CNN），因此基于MobileNet的ViT模型如MobileViT-S已经开发。然而，其性能不能达到原始ViT模型的水平。提出的结构解决了上述弱点，通过将早期注意力阶段中的信息存储并重复使用在最终分类器中。这篇论文受到了数据本身在早期注意力阶段的重要性的想法所 inspirited。为了重复使用早期注意力阶段的信息，使用了不同缩放因子的特征的平均池化结果来扩展最终分类器的 Fully Connected（FC）层的通道数。预计通过引入缩放因子引入的预测偏好，可以提高最终性能。由于提出的结构只需要平均池化早期注意力阶段的特征，以及在最终分类器的FC层中进行通道扩展，因此计算和存储开销非常小，保留了低成本的MobileNet基于ViT（MobileViT）的好处。与原始MobileViT在ImageNet dataset上的性能相比，提出的ExMobileViT具有显著的准确性提升，只有约5%的额外参数。
</details></li>
</ul>
<hr>
<h2 id="Partial-Proof-of-a-Conjecture-with-Implications-for-Spectral-Majorization"><a href="#Partial-Proof-of-a-Conjecture-with-Implications-for-Spectral-Majorization" class="headerlink" title="Partial Proof of a Conjecture with Implications for Spectral Majorization"></a>Partial Proof of a Conjecture with Implications for Spectral Majorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01302">http://arxiv.org/abs/2309.01302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeffrey Uhlmann</li>
<li>for: 这项研究探讨了一个关于 $n\times n$ ($n\leq 6$) 正定矩阵的性质的悬念。</li>
<li>methods: 这项研究使用了计算机辅助的和平方方法（SoS）来证明多项式非负性。</li>
<li>results: 研究发现了一个新的矩阵家族，其特点是对角线majorize其 спектrum。此外，这个家族可以通过克로内克组合扩展到 $n&gt;6$ ，保持特殊的majorization property。<details>
<summary>Abstract</summary>
In this paper we report on new results relating to a conjecture regarding properties of $n\times n$, $n\leq 6$, positive definite matrices. The conjecture has been proven for $n\leq 4$ using computer-assisted sum of squares (SoS) methods for proving polynomial nonnegativity. Based on these proven cases, we report on the recent identification of a new family of matrices with the property that their diagonals majorize their spectrum. We then present new results showing that this family can extended via Kronecker composition to $n>6$ while retaining the special majorization property. We conclude with general considerations on the future of computer-assisted and AI-based proofs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们报告了新的结果，与positive definite矩阵($n\times n$, $n\leq 6$)的性质有关。我们已经使用计算机支持的sum of squares（SoS）方法证明了$n\leq 4$的情况。基于已经证明的 случа例，我们报告了一个新的矩阵家族，其特点是主对角线majorize其 спектrum。然后，我们发现了一种可以通过克ро内克组合延伸到$n>6$的方法，保持特殊的majorization性。我们结束于计算机支持和人工智能基于证明的未来考虑。Here's the translation in Traditional Chinese:在这篇论文中，我们报告了新的结果，与positive definite矩阵（$n\times n$, $n\leq 6）的性质有关。我们已经使用计算机支持的sum of squares（SoS）方法证明了$n\leq 4$的情况。基于已经证明的个案例，我们报告了一个新的矩阵家族，其特点是主对角线majorize其 спектrum。然后，我们发现了一种可以通过克ро内克组合延伸到$n>6$的方法，保持特殊的majorization性。我们结束于计算机支持和人工智能基于证明的未来考虑。
</details></li>
</ul>
<hr>
<h2 id="AlphaZero-Gomoku"><a href="#AlphaZero-Gomoku" class="headerlink" title="AlphaZero Gomoku"></a>AlphaZero Gomoku</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01294">http://arxiv.org/abs/2309.01294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suragnair/alpha-zero-general">https://github.com/suragnair/alpha-zero-general</a></li>
<li>paper_authors: Wen Liang, Chao Yu, Brian Whiteaker, Inyoung Huh, Hua Shao, Youzhi Liang</li>
<li>for: 这个论文的目的是探索AlphaZero算法在gomoku棋盘游戏中的表现。</li>
<li>methods: 这个论文使用了AlphaZero算法，具体来说是将深度学习与Monte Carlo搜索结合在一起，以便在gomoku棋盘游戏中实现人工智能的优势。</li>
<li>results: 测试结果表明，AlphaZero算法在gomoku棋盘游戏中表现出了优势，并且能够在不同的游戏环境下保持稳定的高水平。<details>
<summary>Abstract</summary>
In the past few years, AlphaZero's exceptional capability in mastering intricate board games has garnered considerable interest. Initially designed for the game of Go, this revolutionary algorithm merges deep learning techniques with the Monte Carlo tree search (MCTS) to surpass earlier top-tier methods. In our study, we broaden the use of AlphaZero to Gomoku, an age-old tactical board game also referred to as "Five in a Row." Intriguingly, Gomoku has innate challenges due to a bias towards the initial player, who has a theoretical advantage. To add value, we strive for a balanced game-play. Our tests demonstrate AlphaZero's versatility in adapting to games other than Go. MCTS has become a predominant algorithm for decision processes in intricate scenarios, especially board games. MCTS creates a search tree by examining potential future actions and uses random sampling to predict possible results. By leveraging the best of both worlds, the AlphaZero technique fuses deep learning from Reinforcement Learning with the balancing act of MCTS, establishing a fresh standard in game-playing AI. Its triumph is notably evident in board games such as Go, chess, and shogi.
</details>
<details>
<summary>摘要</summary>
Recently, AlphaZero的出色的能力在复杂游戏中精通得到了广泛关注。 alphaZero最初是设计用于围棋游戏，这种革命性的算法将深度学习技术与Monte Carlo Tree Search（MCTS）相结合，超越了之前的顶尖方法。在我们的研究中，我们扩展了AlphaZero的使用范围到了古 mobil游戏（Gomoku），这是一款具有偏好初始玩家的战略游戏。很有趣的是，Gomoku拥有内生的挑战，因为初始玩家有理论上的优势。为了增加价值，我们努力寻求平衡的游戏环境。我们的测试表明AlphaZero在不同于Go的游戏中也有很好的适应能力。MCTS已成为复杂enario中决策过程中的主流算法，特别是板球游戏。MCTS通过评估未来动作的可能性来构建搜索树，并使用随机抽样来预测可能的结果。AlphaZero技术将深度学习从强化学习与MCTS的平衡过程相结合，建立了新的游戏AI标准。其胜利在板球游戏、国际象棋和将棋等游戏中得到了广泛证明。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.AI_2023_09_04/" data-id="clp9qz7yz003dok887jfd2ye0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.CL_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T11:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.CL_2023_09_04/">cs.CL - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation"><a href="#Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation" class="headerlink" title="Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation"></a>Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01860">http://arxiv.org/abs/2309.01860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaber Ibn Abdul Hakim, Rasman Mubtasim Swargo, Muhammad Abdullah Adnan</li>
<li>for: 这个研究的目的是扩展现有的连续手语识别和翻译管道，以包含多modal信息。</li>
<li>methods: 这个研究使用了一种跨modal编码器，将Optical flow信息与RGB图像集成到特征集中，以增强手语识别和翻译的精度。</li>
<li>results: 研究表明，通过包含多modal信息，可以提高手语识别和翻译的结果。在RWTH-PHOENIX-2014数据集上进行手语识别任务，我们的方法可以降低WER值0.9。在RWTH-PHOENIX-2014T数据集上进行翻译任务，我们的方法可以提高大多数BLEU分数值0.6。<details>
<summary>Abstract</summary>
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种方法，用于将多modal信息添加到现有的连续手语识别和翻译管道中。在我们的过程中，我们将光流信息与RGB图像结合以增强特征中的运动相关信息。本研究证明了这种多模态包含的可行性，并使用了一个轻量级的插件，不需要在端到端方式中额外添加新模态的特征提取器。我们对手语识别和翻译进行了应用，并在每个情况下提高了结果。我们在RWTH-PHOENIX-2014 dataset上进行了手语识别任务的评估，并在RWTH-PHOENIX-2014T dataset上进行了翻译任务的评估。在识别任务中，我们的方法降低了WER值0.9，在翻译任务中，我们的方法在测试集上提高了大多数BLEU分数的0.6。
</details></li>
</ul>
<hr>
<h2 id="Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech"><a href="#Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech" class="headerlink" title="Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech"></a>Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02466">http://arxiv.org/abs/2309.02466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Myles Eugenio</li>
<li>for: 这个论文是为了探讨语音演化的经济性的影响，以及这种影响如何使得人们更容易学习语音。</li>
<li>methods: 这篇论文使用了一种基于tensor网络的本地相关模型，这种模型利用了语音中的本地phonetic correlations来促进语音学习。</li>
<li>results: 研究发现，这种模型可以帮助人们更容易学习语音，并且可以生成新的语音单词，这些单词符合目标语言的phonetic规则。此外，模型还可以提供一个 hierarchical 的最likely errors 列表，用于描述在语音行为中可能出现的错误。<details>
<summary>Abstract</summary>
Spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)
</details>
<details>
<summary>摘要</summary>
spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)Here's the translation breakdown:* "spoken language" 口语 (kǒu yǔ)* "evolves" 演化 (biǎn huà)* "constrained" 受限 (shòu jiàn)* "by the economy of speech" 由语言经济 (yǐ yǔ yán jīng jì)* "which depends on factors such as the structure of the human mouth" 即人口结构等因素 (jī rén kǒu jiégòu déng yǐng fāng)* "This gives rise to local phonetic correlations in spoken words" 这会导致 spoken words 中的地方声学相关性 (zhè huì dào cái spoken words zhōng de dì fāng shēng xué xiāng yì)* "Here we demonstrate that these local correlations facilitate the learning of spoken words" 我们在这里示出这些地方声学相关性可以促进 spoken words 的学习 (wǒ men zài zhè lǐ shì chū shēng zhī yì xiǎng xué xí)* "by reducing their information content" 通过减少信息内容 (tōng guò jiǎn shòu xìn xīn nèi zhòng)* "We do this by constructing a locally-connected tensor-network model" 我们使用一种地方连接的tensor网络模型 (wǒ men shǐ yòng yī zhōng dì fāng lián xiǎng de tensor wǎng wǎn mó del)* "inspired by similar variational models used for many-body physics"  Drawing on similar variational models used in many-body physics (fāng yǐn yī xiàng zhī yì zhī shì)* "which exploits these local phonetic correlations to facilitate the learning of spoken words" 这种模型可以利用这些地方声学相关性来促进 spoken words 的学习 (zhè zhōng mó del cóu yì liǎng yòu zhī yì xiǎng yì shì)* "The model is therefore a minimal model of phonetic memory" 这种模型因此成为一种最小的声学记忆模型 (zhè zhōng mó del yìn xiàng zhī yì xiǎng yì)* "where 'learning to pronounce' and 'learning a word' are one and the same" 这种模型中，"学习发音"和"学习一个词"是一样的 (zhè zhōng mó del zhì yì, "xué xí fā yīn" yǔ "xué xí yī gè ci" shì yī yàng de)* "A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language" 这种模型的一个结果是学习出新词，这些词汇在目标语言中是声学合理的 (zhè zhōng mó del yī yì shì, zhè xiē yì shì zhī yì yì bù)* "as well as providing a hierarchy of the most likely errors that could be produced during the action of speech" 这种模型还可以提供一个词汇错误的层次结构 (zhè zhōng mó del yǐn yì shì zhī yì yì bù)* "We test our model against Latin and Turkish words" 我们对 Latin 和 Turkish 词汇进行测试 (wǒ men duì Lati n yǔ Tūrkish yì shì zhì)* "The code is available on GitHub" 代码可以在 GitHub 上找到 (fǎn yì zhī yì zhī shì)
</details></li>
</ul>
<hr>
<h2 id="Into-the-Single-Cell-Multiverse-an-End-to-End-Dataset-for-Procedural-Knowledge-Extraction-in-Biomedical-Texts"><a href="#Into-the-Single-Cell-Multiverse-an-End-to-End-Dataset-for-Procedural-Knowledge-Extraction-in-Biomedical-Texts" class="headerlink" title="Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts"></a>Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01812">http://arxiv.org/abs/2309.01812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ylaboratory/flambe">https://github.com/ylaboratory/flambe</a></li>
<li>paper_authors: Ruth Dannenfelser, Jeffrey Zhong, Ran Zhang, Vicky Yao</li>
<li>for: 这篇论文的主要目的是提供一个用于生物医学领域进行过程知识抽取的数据集，以便进一步开发自然语言处理（NLP）模型。</li>
<li>methods: 该数据集是通过专家 manually 精心纠正的方式从生物医学领域的学术论文中提取出的过程知识，包括名实 recognize 和ambiguation 等任务。</li>
<li>results: 该数据集提供了一个大量的 manually 精心纠正的名实识别和ambiguation 数据集，以便进一步开发NLP模型，同时也有助于提高生物医学研究领域的重复性。<details>
<summary>Abstract</summary>
Many of the most commonly explored natural language processing (NLP) information extraction tasks can be thought of as evaluations of declarative knowledge, or fact-based information extraction. Procedural knowledge extraction, i.e., breaking down a described process into a series of steps, has received much less attention, perhaps in part due to the lack of structured datasets that capture the knowledge extraction process from end-to-end. To address this unmet need, we present FlaMB\'e (Flow annotations for Multiverse Biological entities), a collection of expert-curated datasets across a series of complementary tasks that capture procedural knowledge in biomedical texts. This dataset is inspired by the observation that one ubiquitous source of procedural knowledge that is described as unstructured text is within academic papers describing their methodology. The workflows annotated in FlaMB\'e are from texts in the burgeoning field of single cell research, a research area that has become notorious for the number of software tools and complexity of workflows used. Additionally, FlaMB\'e provides, to our knowledge, the largest manually curated named entity recognition (NER) and disambiguation (NED) datasets for tissue/cell type, a fundamental biological entity that is critical for knowledge extraction in the biomedical research domain. Beyond providing a valuable dataset to enable further development of NLP models for procedural knowledge extraction, automating the process of workflow mining also has important implications for advancing reproducibility in biomedical research.
</details>
<details>
<summary>摘要</summary>
多种常见的自然语言处理（NLP）信息EXTRACTION任务可以被视为评估声明知识或基于事实的信息EXTRACTION。而过程知识EXTRACTION，即从描述过程中提取步骤，则 receiving much less attention，可能是由于缺乏结构化数据集的不足。为解决这一需求，我们提出FlaMB\'e（流动注释 для多元生物实体），这是一系列 complementary tasks 的专家修订 datasets，用于捕捉生物文献中的过程知识。这个数据集得到了学术论文中描述的过程的注释，这些过程来自生物学研究领域的快速发展领域，特别是单细胞研究。此外，FlaMB\'e 还提供了，到我们所知，生物医学研究领域中最大的手动修订名实体识别（NER）和名实体识别（NED）数据集，用于识别基本生物实体，这种实体是生物医学研究领域中知识EXTRACTION的重要基础。除了提供可用于进一步发展NLP模型的价值数据集外，自动化工作流挖掘也有重要的 reproduceability 推动生物医学研究的意义。
</details></li>
</ul>
<hr>
<h2 id="Are-Emergent-Abilities-in-Large-Language-Models-just-In-Context-Learning"><a href="#Are-Emergent-Abilities-in-Large-Language-Models-just-In-Context-Learning" class="headerlink" title="Are Emergent Abilities in Large Language Models just In-Context Learning?"></a>Are Emergent Abilities in Large Language Models just In-Context Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01809">http://arxiv.org/abs/2309.01809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ukplab/on-emergence">https://github.com/ukplab/on-emergence</a></li>
<li>paper_authors: Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych</li>
<li>for: This paper aims to provide a comprehensive examination of the emergent abilities of large language models, specifically looking at the role of in-context learning in their performance.</li>
<li>methods: The authors use a set of 18 models with varying parameters (60 million to 175 billion) and test them on a set of 22 tasks. They conduct over 1,000 experiments to evaluate the models’ performance and determine the underlying mechanisms driving their emergent abilities.</li>
<li>results: The authors find that the emergent abilities of large language models can primarily be attributed to in-context learning, and there is no evidence for the emergence of reasoning abilities. This provides valuable insights into the use of these models and alleviates safety concerns regarding their performance.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是为了对大语言模型的突出能力进行全面的检查，特别是它们在受到不同提示下的性能如何。</li>
<li>methods: 作者使用了18个模型（参数量从60万到175亿），对其进行了22项任务的测试，并进行了超过1000个实验来评估模型的性能。</li>
<li>results: 作者发现，大语言模型的突出能力主要归因于受到提示下的学习，并没有证据表明它们具有推理能力的emergence。这些结论有助于我们更好地理解这些模型的使用，并缓解对其性能的安全性问题。<details>
<summary>Abstract</summary>
Large language models have exhibited emergent abilities, demonstrating exceptional performance across diverse tasks for which they were not explicitly trained, including those that require complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In this study, we provide the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models, encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks. Through an extensive series of over 1,000 experiments, we provide compelling evidence that emergent abilities can primarily be ascribed to in-context learning. We find no evidence for the emergence of reasoning abilities, thus providing valuable insights into the underlying mechanisms driving the observed abilities and thus alleviating safety concerns regarding their use.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports"><a href="#An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports" class="headerlink" title="An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports"></a>An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01740">http://arxiv.org/abs/2309.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Dack, Lorenzo Brigato, Matthew McMurray, Matthias Fontanellaz, Thomas Frauenfelder, Hanno Hoppe, Aristomenis Exadaktylos, Thomas Geiser, Manuela Funke-Chambour, Andreas Christe, Lukas Ebner, Stavroula Mougiakakou</li>
<li>for: 这 paper 是为了研究基于 contrastive visual language learning 的零shot 多标签分类方法，以帮助 radiologist 诊断 COVID-19 和识别详细的 lung 病变。</li>
<li>methods: 这 paper 使用了 unstructured data 和 CT 成像来进行零shot 多标签分类，并与 human expert 合作调节模型。</li>
<li>results: 这 paper 的 empirical analysis 表明，零shot 多标签分类方法可以帮助 radiologist 更好地诊断 COVID-19 和识别详细的 lung 病变。<details>
<summary>Abstract</summary>
The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis community by addressing some challenges associated with unstructured data and fine-grained multi-label classification.
</details>
<details>
<summary>摘要</summary>
pandemic 导致了庞大的不结构数据存储，包括 radiology 报告，由于增加的医疗检查。先前的自动诊断 COVID-19 研究主要集中在 X-ray 图像上，尽管它们的精度比 computed tomography (CT) 扫描低。在这项工作中，我们利用医院的不结构数据和 CT 扫描的细腻特征，实现零shot 多标签分类 based on 对比视觉语言学习。与人类专家合作，我们调查了多种零shot 模型的效果，帮助 radiologist 检测肺动脉塞和识别复杂的肺脉塞，如云彩杂色和聚集。我们的实验分析提供了targeting such fine-grained tasks 的可能解决方案的概述，在医学多Modal 预训练 литературе中被过looked。我们的调查承诺未来医学图像分析社区的发展，解决一些不结构数据和细腻多标签分类的挑战。
</details></li>
</ul>
<hr>
<h2 id="Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction"><a href="#Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction" class="headerlink" title="Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction"></a>Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01715">http://arxiv.org/abs/2309.01715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/20001LastOrder/Taxonomy-GPT">https://github.com/20001LastOrder/Taxonomy-GPT</a></li>
<li>paper_authors: Boqi Chen, Fandi Yi, Dániel Varró</li>
<li>for: 本研究旨在提出一种满足结构约束的自动taxonomy构建框架，以便在不同的软件模型和自然语言处理（NLP）活动中提高taxonomy的效果。</li>
<li>methods: 本研究使用了适当的用户输入（称为提示），将GPT-3等大语言模型（LLMs）引导到多种NLP任务中，而不需要显式（重）训练。</li>
<li>results: 研究发现，无需显式训练，提示方法可以在hypernym taxonomy和计算机科学 taxonomy dataset中对taxonomy进行自动构建，并且在训练集小时，提示方法的性能比 fine-tuning 方法更高。但是， fine-tuning 方法可以轻松地对生成的taxonomy进行后处理，以满足所有约束。<details>
<summary>Abstract</summary>
Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.
</details>
<details>
<summary>摘要</summary>
taxonomies 表示实体之间的层次关系，常用于软件建模和自然语言处理（NLP）活动中。它们通常受到一组结构约束，限制它们的内容。然而，手动构建税onomy可以是时间consuming、不完整和维护成本高。现在的研究表明，适当的用户输入（叫做提示）可以导引大语言模型（LLMs）在多种NLP任务中表现出优秀的性能，无需显式（再）训练。然而，现有的自动税onomy构建方法通常通过调整模型参数来进行细化。在这篇论文中，我们提出一种通用的税onomy构建框架，考虑结构约束。然后，我们进行了系统比较，通过对hypernym税onomy和一个新的计算机科学税onomy数据集进行提示和细化两种方法的性能。我们的结果显示以下：（1）无需显式训练数据集，提示方法比细化方法表现更好，而且当训练数据集较小时，性能差距更大。然而，（2）通过细化方法生成的税onomy可以轻松地进行后期处理，以满足所有约束，而提示方法生成的税onomy处理抵触的问题可以困难。这些评估结果为选择合适的方法提供指导，并高亮了两种方法的可能的改进。
</details></li>
</ul>
<hr>
<h2 id="MathAttack-Attacking-Large-Language-Models-Towards-Math-Solving-Ability"><a href="#MathAttack-Attacking-Large-Language-Models-Towards-Math-Solving-Ability" class="headerlink" title="MathAttack: Attacking Large Language Models Towards Math Solving Ability"></a>MathAttack: Attacking Large Language Models Towards Math Solving Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01686">http://arxiv.org/abs/2309.01686</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang</li>
<li>for: 本研究旨在检测大型自然语言模型（LLMs）在数学问题解决能力方面的安全性。</li>
<li>methods: 我们提出了一种名为MathAttack的模型，用于攻击数学问题样本，以保持原始数学问题的逻辑逻辑。我们首先使用逻辑存在检测来识别逻辑入口，然后使用word-level攻击者对剩下的文本进行攻击。</li>
<li>results: 我们的实验表明，MathAttack可以有效攻击LLMs的数学问题解决能力。我们发现：1）我们的敌意样本从高精度LLMs中生成的样本也能够攻击低精度LLMs（例如，从大到小模型或从多个步骤到零步骤提问中）；2）复杂的数学问题（例如，更多的解决步骤、更长的文本、更多的数字）更容易受到攻击；3）我们可以通过使用我们的反敌样本来提高LLMs的 robustness。<details>
<summary>Abstract</summary>
With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the security of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of security in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. We will release our code and dataset.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLMs）的爆发，解决数学问题（MWP）的研究已经做出了很大的进步。然而，有很少的研究检查LLMs在数学问题解决能力的安全性。而不是通过对提示的使用来进行攻击，我们提议一种名为MathAttack的模型来攻击MWP样本，这些样本更加接近安全性的解决数学问题的本质。相比传统的文本恶意攻击，在攻击MWP时更加重要是保持原始MWP的数学逻辑。为此，我们提出了逻辑实体识别，以冻结逻辑实体。然后，剩下的文本使用word-level攻击者进行攻击。此外，我们提出了一个名为RobustMath的新数据集，用于评估LLMs在数学问题解决能力的Robustness。我们在RobustMath和两个其他数学benchmark数据集GSM8K和MultiAirth上进行了广泛的实验，结果表明MathAttack可以有效攻击LLMs的数学问题解决能力。在实验中，我们注意到以下问题：1.我们从高精度LLMs中生成的恶意样本也可以有效地攻击低精度LLMs（例如，从大到小的LLMs或从多少个提示到零个提示）。2.复杂的MWP（如更多的解决步骤、更长的文本、更多的数字）更容易受到攻击。3.我们可以通过使用我们的恶意样本来提高LLMs的Robustness，特别是在几个提示中。最后，我们希望我们的实践和观察可以作为LLMs在数学问题解决能力的Robustness的重要尝试。我们将发布我们的代码和数据集。
</details></li>
</ul>
<hr>
<h2 id="CRUISE-Screening-Living-Literature-Reviews-Toolbox"><a href="#CRUISE-Screening-Living-Literature-Reviews-Toolbox" class="headerlink" title="CRUISE-Screening: Living Literature Reviews Toolbox"></a>CRUISE-Screening: Living Literature Reviews Toolbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01684">http://arxiv.org/abs/2309.01684</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/projectdossier/cruise-screening">https://github.com/projectdossier/cruise-screening</a></li>
<li>paper_authors: Wojciech Kusa, Petr Knoth, Allan Hanbury</li>
<li>for: 帮助研究人员快速找到相关研究，提高生成Literature Review的效率和有效性。</li>
<li>methods: 使用文本分类和问答模型帮助屏选相关论文，并通过API与多个搜索引擎连接以更新搜索结果。</li>
<li>results: 开发了一款基于Web应用程序，可以实现活动Literature Review的屏选和搜索，可以帮助研究人员避免手动屏选和搜索，提高工作效率。<details>
<summary>Abstract</summary>
Keeping up with research and finding related work is still a time-consuming task for academics. Researchers sift through thousands of studies to identify a few relevant ones. Automation techniques can help by increasing the efficiency and effectiveness of this task. To this end, we developed CRUISE-Screening, a web-based application for conducting living literature reviews - a type of literature review that is continuously updated to reflect the latest research in a particular field. CRUISE-Screening is connected to several search engines via an API, which allows for updating the search results periodically. Moreover, it can facilitate the process of screening for relevant publications by using text classification and question answering models. CRUISE-Screening can be used both by researchers conducting literature reviews and by those working on automating the citation screening process to validate their algorithms. The application is open-source: https://github.com/ProjectDoSSIER/cruise-screening, and a demo is available under this URL: https://citation-screening.ec.tuwien.ac.at. We discuss the limitations of our tool in Appendix A.
</details>
<details>
<summary>摘要</summary>
保持研究的最新信息和找到相关工作仍然是学术人员的时间消耗任务。研究人员需要从千余篇论文中筛选出一些相关的研究，以增加研究效率和效果。自动化技术可以帮助解决这个问题。为此，我们开发了CRUISE-Screening，一个基于网络的应用程序，用于进行生活文献评估 - 一种Periodically更新的文献评估方法，以反映最新的研究领域。CRUISE-Screening通过API与多个搜索引擎连接，以 periodic更新搜索结果。此外，它还可以通过文本分类和问答模型来帮助屏选相关的论文。CRUISE-Screening可以用于由研究人员进行文献评估，以及用于自动化引用屏选过程的验证。该应用程序是开源的，可以在 GitHub 上找到代码：https://github.com/ProjectDoSSIER/cruise-screening，并可以在以下 URL 上查看示例：https://citation-screening.ec.tuwien.ac.at。我们在 Appendix A 中讨论了我们的工具的限制。
</details></li>
</ul>
<hr>
<h2 id="Donkii-Can-Annotation-Error-Detection-Methods-Find-Errors-in-Instruction-Tuning-Datasets"><a href="#Donkii-Can-Annotation-Error-Detection-Methods-Find-Errors-in-Instruction-Tuning-Datasets" class="headerlink" title="Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?"></a>Donkii: Can Annotation Error Detection Methods Find Errors in Instruction-Tuning Datasets?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01669">http://arxiv.org/abs/2309.01669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Weber-Genzel, Robert Litschko, Ekaterina Artemova, Barbara Plank<br>for:这个论文主要针对的是如何在生成 Setting 中应用 Annotation Error Detection (AED) 方法，以提高 Large Language Models (LLMs) 的训练。methods:这篇论文使用了三个 instruction-tuning 数据集，它们都是由专家和 semi-automatic 方法进行了注释。 authors 还提出了四种 AED 基线方法，并对其进行了全面的评估。results:这篇论文发现，选择正确的 AED 方法和模型大小是非常重要，这有助于提高 instruction-tuning 的性能。 authors 还提供了一个首次的案例研究，以了解 instruction-tuning 数据集的质量如何影响下游性能。<details>
<summary>Abstract</summary>
Instruction-tuning has become an integral part of training pipelines for Large Language Models (LLMs) and has been shown to yield strong performance gains. In an orthogonal line of research, Annotation Error Detection (AED) has emerged as a tool for detecting quality issues of gold-standard labels. But so far, the application of AED methods is limited to discriminative settings. It is an open question how well AED methods generalize to generative settings which are becoming widespread via generative LLMs. In this work, we present a first and new benchmark for AED on instruction-tuning data: Donkii. It encompasses three instruction-tuning datasets enriched with annotations by experts and semi-automatic methods. We find that all three datasets contain clear-cut errors that sometimes directly propagate into instruction-tuned LLMs. We propose four AED baselines for the generative setting and evaluate them comprehensively on the newly introduced dataset. Our results demonstrate that choosing the right AED method and model size is indeed crucial, thereby deriving practical recommendations. To gain insights, we provide a first case-study to examine how the quality of the instruction-tuning datasets influences downstream performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>现在，教程调整（Instruction-tuning）已成为大语言模型（LLMs）训练管道的一个重要组成部分，并且已经显示出强大的性能提升。而在平行的研究方向中，标注错误检测（AED）已经出现为检测黄金标准标签质量问题的工具。但目前，AED方法的应用仅限于推理性Setting中。因此，是一个打开的问题，AED方法在生成Setting中的普遍性如何。在这项工作中，我们提出了一个新的和第一个Benchmark дляAED在教程调整数据上：Donkii。它包括三个 instruciton-tuning 数据集，每个数据集都有专家和半自动方法对标注。我们发现所有三个数据集都含有明确的错误，这些错误直接卷入 instruciton-tuned LLMs。我们提出了四个AED基线 для生成Setting，并在新引入的数据集上进行了全面的评估。我们的结果表明，选择正确的AED方法和模型大小是非常重要，从而得到了实用的建议。为了获得更多的洞察，我们提供了一个首次的案例研究，探讨了下游性能如何受到教程调整数据的质量影响。
</details></li>
</ul>
<hr>
<h2 id="Evolving-linguistic-divergence-on-polarizing-social-media"><a href="#Evolving-linguistic-divergence-on-polarizing-social-media" class="headerlink" title="Evolving linguistic divergence on polarizing social media"></a>Evolving linguistic divergence on polarizing social media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01659">http://arxiv.org/abs/2309.01659</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreskarjus/evolving_divergence">https://github.com/andreskarjus/evolving_divergence</a></li>
<li>paper_authors: Andres Karjus, Christine Cuskley</li>
<li>for: 本研究探讨了美国政治各派别之间语言分化的现象，特别是社交媒体平台上的语言使用差异。</li>
<li>methods: 该研究使用了社交媒体数据 mines, lexicostatistics, machine learning 和大语言模型，并采用了系统的人工注释方法，以描述和量化语言分化的现象。</li>
<li>results: 研究发现，美国政治各派别之间存在语言分化的现象，尤其是在话题和话语方面，与之前的研究一致。这些现象表明，美国英语在不断受到政治各派别的影响，可能会导致语言分化。<details>
<summary>Abstract</summary>
Language change is influenced by many factors, but often starts from synchronic variation, where multiple linguistic patterns or forms coexist, or where different speech communities use language in increasingly different ways. Besides regional or economic reasons, communities may form and segregate based on political alignment. The latter, referred to as political polarization, is of growing societal concern across the world. Here we map and quantify linguistic divergence across the partisan left-right divide in the United States, using social media data. We develop a general methodology to delineate (social) media users by their political preference, based on which (potentially biased) news media accounts they do and do not follow on a given platform. Our data consists of 1.5M short posts by 10k users (about 20M words) from the social media platform Twitter (now "X"). Delineating this sample involved mining the platform for the lists of followers (n=422M) of 72 large news media accounts. We quantify divergence in topics of conversation and word frequencies, messaging sentiment, and lexical semantics of words and emoji. We find signs of linguistic divergence across all these aspects, especially in topics and themes of conversation, in line with previous research. While US American English remains largely intelligible within its large speech community, our findings point at areas where miscommunication may eventually arise given ongoing polarization and therefore potential linguistic divergence. Our methodology - combining data mining, lexicostatistics, machine learning, large language models and a systematic human annotation approach - is largely language and platform agnostic. In other words, while we focus here on US political divides and US English, the same approach is applicable to other countries, languages, and social media platforms.
</details>
<details>
<summary>摘要</summary>
language change 由多种因素 influencing，frequently 从同时变化开始，其中多种语言模式或形式同时存在，或者不同的语言社区使用语言在不同的方式。besides regional or economic reasons, communities may form and segregate based on political alignment. the latter, referred to as political polarization, is of growing societal concern across the world. here we map and quantify linguistic divergence across the partisan left-right divide in the united states, using social media data. we develop a general methodology to delineate (social) media users by their political preference, based on which (potentially biased) news media accounts they do and do not follow on a given platform. our data consists of 1.5 million short posts by 10,000 users (about 20 million words) from the social media platform Twitter (now "X"). delineating this sample involved mining the platform for the lists of followers (n=422 million) of 72 large news media accounts. we quantify divergence in topics of conversation and word frequencies, messaging sentiment, and lexical semantics of words and emoji. we find signs of linguistic divergence across all these aspects, especially in topics and themes of conversation, in line with previous research. while US American English remains largely intelligible within its large speech community, our findings point at areas where miscommunication may eventually arise given ongoing polarization and therefore potential linguistic divergence. our methodology - combining data mining, lexicostatistics, machine learning, large language models and a systematic human annotation approach - is largely language and platform agnostic. in other words, while we focus here on US political divides and US English, the same approach is applicable to other countries, languages, and social media platforms.
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-effectiveness-of-ChatGPT-based-feedback-compared-with-teacher-feedback-and-self-feedback-Evidence-from-Chinese-to-English-translation"><a href="#Exploring-the-effectiveness-of-ChatGPT-based-feedback-compared-with-teacher-feedback-and-self-feedback-Evidence-from-Chinese-to-English-translation" class="headerlink" title="Exploring the effectiveness of ChatGPT-based feedback compared with teacher feedback and self-feedback: Evidence from Chinese to English translation"></a>Exploring the effectiveness of ChatGPT-based feedback compared with teacher feedback and self-feedback: Evidence from Chinese to English translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01645">http://arxiv.org/abs/2309.01645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyi Cao, Linping Zhong</li>
<li>for: 本研究是为了比较中国硬件翻译硬件翻译学生在英语作为第二语言中使用ChatGPT反馈的效果，并与教师反馈（TF）和自我反馈（SF）进行比较。</li>
<li>methods: 本研究使用了BLEU分数来衡量翻译质量，以及Coh-Metrix来分析翻译文本中的语言特征。</li>
<li>results: 研究发现，TF和SF带来的翻译文本质量高于ChatGPT反馈，但ChatGPT反馈能够提高翻译文本中的词汇能力和参照相互关系。同时，TF和SF更能够提高翻译文本中的语法能力，特别是正确使用了过去分词。<details>
<summary>Abstract</summary>
ChatGPT,a cutting-edge AI-powered Chatbot,can quickly generate responses on given commands. While it was reported that ChatGPT had the capacity to deliver useful feedback, it is still unclear about its effectiveness compared with conventional feedback approaches,such as teacher feedback (TF) and self-feedback (SF). To address this issue, this study compared the revised Chinese to English translation texts produced by Chinese Master of Translation and Interpretation (MTI) students,who learned English as a Second/Foreign Language (ESL/EFL), based on three feedback types (i.e., ChatGPT-based feedback, TF and SF). The data was analyzed using BLEU score to gauge the overall translation quality as well as Coh-Metrix to examine linguistic features across three dimensions: lexicon, syntax, and cohesion.The findings revealed that TF- and SF-guided translation texts surpassed those with ChatGPT-based feedback, as indicated by the BLEU score. In terms of linguistic features,ChatGPT-based feedback demonstrated superiority, particularly in enhancing lexical capability and referential cohesion in the translation texts. However, TF and SF proved more effective in developing syntax-related skills,as it addressed instances of incorrect usage of the passive voice. These diverse outcomes indicate ChatGPT's potential as a supplementary resource, complementing traditional teacher-led methods in translation practice.
</details>
<details>
<summary>摘要</summary>
chatGPT，一种前沿的人工智能 chatbot，可快速生成对给定命令的回应。尽管报道称 chatGPT 有可能提供有用的反馈，但是它的效果对于传统的反馈方法（如教师反馈和自我反馈）仍然不清楚。为了解决这个问题，本研究比较了由中文翻译和 intérprete 学生（学习英语为第二外语/第二外语）所制定的修改后的中英翻译文本，基于三种反馈类型（即 chatGPT 反馈、教师反馈和自我反馈）。数据分析使用 BLEU 分数来评估翻译质量的总体水平，以及 Coh-Metrix 来检查翻译文本中的三个维度：词汇、 sentence 和 cohesion。研究发现，TF 和 SF 引导的翻译文本在 BLEU 分数上胜过 chatGPT 反馈，而在语言特征方面，chatGPT 反馈表现出优势，特别是在提高翻译文本中的词汇能力和引用共识性。然而，TF 和 SF 更有效地发展了 sentence 结构相关的技能，它解决了 incorrect 使用过去分词的情况。这些多样的结果表明 chatGPT 可以作为辅助资源，与传统的教师带领方法相结合，在翻译实践中发挥作用。
</details></li>
</ul>
<hr>
<h2 id="Critical-Behavioral-Traits-Foster-Peer-Engagement-in-Online-Mental-Health-Communities"><a href="#Critical-Behavioral-Traits-Foster-Peer-Engagement-in-Online-Mental-Health-Communities" class="headerlink" title="Critical Behavioral Traits Foster Peer Engagement in Online Mental Health Communities"></a>Critical Behavioral Traits Foster Peer Engagement in Online Mental Health Communities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01618">http://arxiv.org/abs/2309.01618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aseem Srivastava, Tanya Gupta, Alison Cerezo, Sarah Peregrine, Lord, Md Shad Akhtar, Tanmoy Chakraborty</li>
<li>For: This paper aims to explore the factors that drive peer engagement within counseling threads on online mental health communities, such as Reddit, to enhance our understanding of this critical phenomenon.* Methods: The study uses a novel dataset called BeCOPE, which consists of over 10,000 posts and 58,000 comments from 21 mental health-specific subreddits, and is annotated with three major fine-grained behavior labels (intent, criticism, and readability) and emotion labels.* Results: The study finds that self-criticism is the most prevalent form of criticism expressed by help-seekers, and that individuals who explicitly express their need for help are more likely to receive assistance compared to those who present surveys or engage in rants. Additionally, the study highlights the importance of well-articulated problem descriptions in receiving support.<details>
<summary>Abstract</summary>
Online Mental Health Communities (OMHCs), such as Reddit, have witnessed a surge in popularity as go-to platforms for seeking information and support in managing mental health needs. Platforms like Reddit offer immediate interactions with peers, granting users a vital space for seeking mental health assistance. However, the largely unregulated nature of these platforms introduces intricate challenges for both users and society at large. This study explores the factors that drive peer engagement within counseling threads, aiming to enhance our understanding of this critical phenomenon. We introduce BeCOPE, a novel behavior encoded Peer counseling dataset comprising over 10,118 posts and 58,279 comments sourced from 21 mental health-specific subreddits. The dataset is annotated using three major fine-grained behavior labels: (a) intent, (b) criticism, and (c) readability, along with the emotion labels. Our analysis indicates the prominence of ``self-criticism'' as the most prevalent form of criticism expressed by help-seekers, accounting for a significant 43% of interactions. Intriguingly, we observe that individuals who explicitly express their need for help are 18.01% more likely to receive assistance compared to those who present ``surveys'' or engage in ``rants.'' Furthermore, we highlight the pivotal role of well-articulated problem descriptions, showing that superior readability effectively doubles the likelihood of receiving the sought-after support. Our study emphasizes the essential role of OMHCs in offering personalized guidance and unveils behavior-driven engagement patterns.
</details>
<details>
<summary>摘要</summary>
We introduce BeCOPE, a novel dataset comprising over 10,118 posts and 58,279 comments sourced from 21 mental health-specific subreddits. The dataset is annotated with three major fine-grained behavior labels: (a) intent, (b) criticism, and (c) readability, as well as emotion labels. Our analysis reveals that "self-criticism" is the most prevalent form of criticism expressed by help-seekers, accounting for 43% of interactions. Interestingly, we find that individuals who explicitly express their need for help are 18.01% more likely to receive assistance compared to those who present "surveys" or engage in "rants." Furthermore, we highlight the importance of well-articulated problem descriptions, showing that superior readability effectively doubles the likelihood of receiving the sought-after support.Our study emphasizes the crucial role of OMHCs in offering personalized guidance and unveils behavior-driven engagement patterns. These findings have significant implications for the development of OMHCs and the provision of mental health support online. By understanding the factors that drive peer engagement, we can better tailor these platforms to meet the needs of users and improve the overall quality of mental health support.
</details></li>
</ul>
<hr>
<h2 id="Geo-Encoder-A-Chunk-Argument-Bi-Encoder-Framework-for-Chinese-Geographic-Re-Ranking"><a href="#Geo-Encoder-A-Chunk-Argument-Bi-Encoder-Framework-for-Chinese-Geographic-Re-Ranking" class="headerlink" title="Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking"></a>Geo-Encoder: A Chunk-Argument Bi-Encoder Framework for Chinese Geographic Re-Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01606">http://arxiv.org/abs/2309.01606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Cao, Ruixue Ding, Boli Chen, Xianzhi Li, Min Chen, Daniel Hershcovich, Pengjun Xie, Fei Huang</li>
<li>for: 该论文旨在提高中文地图搜索结果的准确率，以便为地图服务等地理相关应用提供更加有用的结果。</li>
<li>methods: 该论文提出了一种新的框架，即Geo-Encoder，以更好地将中文地理 semantics  интегра到重新排序管道中。该方法首先使用可用的工具将文本与地理 span 相关联，然后提出了一种多任务学习模块，以同时获得一个有效的注意力矩阵，决定chunk的贡献。此外，该论文还提出了一种异步更新机制，以便指导模型更好地关注特定的chunk。</li>
<li>results:  experiments 表明，Geo-Encoder 在两个中文地理搜索数据集上达到了显著提高，相比之前的基eline。特别是，它使得 MGEO-BERT 的 Hit@1 分数从 62.76 提高到 68.98， representing a 6.22% improvement.<details>
<summary>Abstract</summary>
Chinese geographic re-ranking task aims to find the most relevant addresses among retrieved candidates, which is crucial for location-related services such as navigation maps. Unlike the general sentences, geographic contexts are closely intertwined with geographical concepts, from general spans (e.g., province) to specific spans (e.g., road). Given this feature, we propose an innovative framework, namely Geo-Encoder, to more effectively integrate Chinese geographical semantics into re-ranking pipelines. Our methodology begins by employing off-the-shelf tools to associate text with geographical spans, treating them as chunking units. Then, we present a multi-task learning module to simultaneously acquire an effective attention matrix that determines chunk contributions to extra semantic representations. Furthermore, we put forth an asynchronous update mechanism for the proposed addition task, aiming to guide the model capable of effectively focusing on specific chunks. Experiments on two distinct Chinese geographic re-ranking datasets, show that the Geo-Encoder achieves significant improvements when compared to state-of-the-art baselines. Notably, it leads to a substantial improvement in the Hit@1 score of MGEO-BERT, increasing it by 6.22% from 62.76 to 68.98 on the GeoTES dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech"><a href="#A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech" class="headerlink" title="A Comparative Analysis of Pretrained Language Models for Text-to-Speech"></a>A Comparative Analysis of Pretrained Language Models for Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01576">http://arxiv.org/abs/2309.01576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Granero-Moya, Penny Karanasou, Sri Karlapati, Bastian Schnell, Nicole Peinelt, Alexis Moinet, Thomas Drugman</li>
<li>for: 本研究旨在investigate the impact of different pre-trained language models (PLMs) on text-to-speech (TTS) tasks, specifically prosody prediction和pause prediction.</li>
<li>methods: 研究采用了15种不同的PLMs，并对其进行了训练和测试。</li>
<li>results: 发现PLMs的大小和质量之间存在对数关系，并且发现表达和中性表达之间存在显著的性能差异。此外，发现 pause prediction 任务对小型模型的敏感程度较低，并且发现这些语言模型的验证结果和我们的实验结果之间存在强相关性。<details>
<summary>Abstract</summary>
State-of-the-art text-to-speech (TTS) systems have utilized pretrained language models (PLMs) to enhance prosody and create more natural-sounding speech. However, while PLMs have been extensively researched for natural language understanding (NLU), their impact on TTS has been overlooked. In this study, we aim to address this gap by conducting a comparative analysis of different PLMs for two TTS tasks: prosody prediction and pause prediction. Firstly, we trained a prosody prediction model using 15 different PLMs. Our findings revealed a logarithmic relationship between model size and quality, as well as significant performance differences between neutral and expressive prosody. Secondly, we employed PLMs for pause prediction and found that the task was less sensitive to small models. We also identified a strong correlation between our empirical results and the GLUE scores obtained for these language models. To the best of our knowledge, this is the first study of its kind to investigate the impact of different PLMs on TTS.
</details>
<details>
<summary>摘要</summary>
现代文本读取系统（TTS）已经利用预训练语言模型（PLM）提高了语调和创造出更自然的语音。然而，虽然PLM在自然语言理解（NLU）方面得到了广泛的研究，但它们在TTS方面的影响却被忽略了。在这项研究中，我们想要填补这个差距，通过对不同PLM进行比较分析，以探讨它们在两个TTS任务中的表现。首先，我们使用15种不同的PLM进行语调预测模型的训练。我们的发现表明，模型的大小和质量之间存在对数的关系，同时，中性和表达性的语调之间也存在显著的性能差异。其次，我们使用PLM进行停顿预测，发现这个任务对小型模型是更敏感的。我们还发现了这些实验结果和GLUE分数中的语言模型得到的相关性强。根据我们所知，这是第一项研究对TTS中不同PLM的影响的研究。
</details></li>
</ul>
<hr>
<h2 id="What-are-Public-Concerns-about-ChatGPT-A-Novel-Self-Supervised-Neural-Topic-Model-Tells-You"><a href="#What-are-Public-Concerns-about-ChatGPT-A-Novel-Self-Supervised-Neural-Topic-Model-Tells-You" class="headerlink" title="What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You"></a>What are Public Concerns about ChatGPT? A Novel Self-Supervised Neural Topic Model Tells You</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01522">http://arxiv.org/abs/2309.01522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wang, Xing Liu, Yanan Wang, Haiping Huang</li>
<li>for: 本研究的目的是挖掘对 chatGPT 的公共担忧。</li>
<li>methods: 本研究使用了一种名为 Self-Supervised neural Topic Model (SSTM)，它将话题化模型视为表示学习过程。</li>
<li>results: 实验结果显示，提posed方法可以提取高质量的公共担忧，并且具有更好的解释性和多样性，超过了现有的方法的性能。<details>
<summary>Abstract</summary>
The recently released artificial intelligence conversational agent, ChatGPT, has gained significant attention in academia and real life. A multitude of early ChatGPT users eagerly explore its capabilities and share their opinions on it via social media. Both user queries and social media posts express public concerns regarding this advanced dialogue system. To mine public concerns about ChatGPT, a novel Self-Supervised neural Topic Model (SSTM), which formalizes topic modeling as a representation learning procedure, is proposed in this paper. Extensive experiments have been conducted on Twitter posts about ChatGPT and queries asked by ChatGPT users. And experimental results demonstrate that the proposed approach could extract higher quality public concerns with improved interpretability and diversity, surpassing the performance of state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
Recently released artificial intelligence conversational agent ChatGPT 已经吸引了大量学术和实际应用的关注。许多早期 ChatGPT 用户积极探索其能力并分享他们对其的看法 via 社交媒体。用户提问和社交媒体文章表达了公众对 ChatGPT 的担忧。为了挖掘公众对 ChatGPT 的担忧，本文提出了一种新的 Self-Supervised neural Topic Model (SSTM)，它将话题模型化为表示学习过程的形式。对 Twitter 上关于 ChatGPT 的文章和用户提问进行了广泛的实验，并实验结果表明，提出的方法可以提取更高质量的公众担忧，并且可以提高解释性和多样性，超过了现有的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="LLM-and-Infrastructure-as-a-Code-use-case"><a href="#LLM-and-Infrastructure-as-a-Code-use-case" class="headerlink" title="LLM and Infrastructure as a Code use case"></a>LLM and Infrastructure as a Code use case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01456">http://arxiv.org/abs/2309.01456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thibault Chanus, Michael Aubertin</li>
<li>for: This paper aims to explore the use of Generative LLMs (Language Models) to generate and manage Ansible YAML roles and playbooks, with a focus on identifying potential directions and industrial applications.</li>
<li>methods: The paper employs the use of Ansible and YAML, alongside Generative LLMs, to automate systems administration tasks and translate human descriptions into code.</li>
<li>results: The paper outlines the potential of using Generative LLMs in this context, with the potential for improved efficiency and accuracy in generating and managing Ansible YAML roles and playbooks.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文旨在探讨使用生成式LLM（语言模型）来生成和管理 Ansible YAML 角色和执行脚本，注重发现可能的方向和工业应用。</li>
<li>methods: 论文使用 Ansible 和 YAML，并与生成式LLM 结合，自动化系统管理任务，将人类描述转化为代码。</li>
<li>results: 论文强调使用生成式LLM 在这个上下文中的潜在优势，包括提高代码生成和管理 Ansible YAML 角色和执行脚本的效率和准确性。<details>
<summary>Abstract</summary>
Cloud computing and the evolution of management methodologies such as Lean Management or Agile entail a profound transformation in both system construction and maintenance approaches. These practices are encompassed within the term "DevOps." This descriptive approach to an information system or application, alongside the configuration of its constituent components, has necessitated the development of descriptive languages paired with specialized engines for automating systems administration tasks. Among these, the tandem of Ansible (engine) and YAML (descriptive language) stands out as the two most prevalent tools in the market, facing notable competition mainly from Terraform. The current document presents an inquiry into a solution for generating and managing Ansible YAML roles and playbooks, utilizing Generative LLMs (Language Models) to translate human descriptions into code. Our efforts are focused on identifying plausible directions and outlining the potential industrial applications.   Note: For the purpose of this experiment, we have opted against the use of Ansible Lightspeed. This is due to its reliance on an IBM Watson model, for which we have not found any publicly available references. Comprehensive information regarding this remarkable technology can be found directly on our partner RedHat's website, https://www.redhat.com/en/about/press-releases/red-hat-introduces-ansible-lightspeed-ai-driven-it-automation
</details>
<details>
<summary>摘要</summary>
云计算和流程管理方法的发展，如Lean Management或Agile，对系统建构和维护方法进行了深刻的变革。这些实践被称为“DevOps”。这种描述性方法， alongside the configuration of its constituent components, has led to the development of descriptive languages and specialized engines for automating systems administration tasks. Among these, the pairing of Ansible (engine) and YAML (descriptive language) is the most prevalent in the market, facing significant competition from Terraform.本文档讨论了一种解决方案，使用生成型LLM（语言模型）将人类描述翻译成代码，以便生成和管理Ansible YAML角色和执行脚本。我们的努力专注于找到可能的方向和详细描述相关的工业应用。注意：在这个实验中，我们选择不使用Ansible Lightspeed，因为它基于IBM Watson模型，而我们没有找到任何公开可用的参考。如果您想了解更多关于这一技术的信息，请参考我们的合作伙伴Red Hat的官方网站，https://www.redhat.com/en/about/press-releases/red-hat-introduces-ansible-lightspeed-ai-driven-it-automation。
</details></li>
</ul>
<hr>
<h2 id="NumHG-A-Dataset-for-Number-Focused-Headline-Generation"><a href="#NumHG-A-Dataset-for-Number-Focused-Headline-Generation" class="headerlink" title="NumHG: A Dataset for Number-Focused Headline Generation"></a>NumHG: A Dataset for Number-Focused Headline Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01455">http://arxiv.org/abs/2309.01455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jian-Tao Huang, Chung-Chi Chen, Hen-Hsen Huang, Hsin-Hsi Chen</li>
<li>for: 本研究的目的是提高Headline Generation task的 numeral accuracy, 通过引入新的数据集NumHG，并对5种之前的模型进行人工评估。</li>
<li>methods: 本研究使用了Encoder-Decoder模型，并对数据集进行细致的标注，以便更好地学习和评估 numeral generation。</li>
<li>results: 研究发现，现有的模型在numeral generation方面存在缺陷，特别是在数字的准确性方面。 NumHG数据集可以帮助解决这个问题，并且可以驱动进一步的研究和讨论。<details>
<summary>Abstract</summary>
Headline generation, a key task in abstractive summarization, strives to condense a full-length article into a succinct, single line of text. Notably, while contemporary encoder-decoder models excel based on the ROUGE metric, they often falter when it comes to the precise generation of numerals in headlines. We identify the lack of datasets providing fine-grained annotations for accurate numeral generation as a major roadblock. To address this, we introduce a new dataset, the NumHG, and provide over 27,000 annotated numeral-rich news articles for detailed investigation. Further, we evaluate five well-performing models from previous headline generation tasks using human evaluation in terms of numerical accuracy, reasonableness, and readability. Our study reveals a need for improvement in numerical accuracy, demonstrating the potential of the NumHG dataset to drive progress in number-focused headline generation and stimulate further discussions in numeral-focused text generation.
</details>
<details>
<summary>摘要</summary>
摘要生成，摘要文本生成中的一项关键任务，旨在将全文短化为精炼的一行文本。尤其是当今的编码-解码模型在ROUGE指标上表现出色，但它们在精确生成数字的问题上经常困难。我们认为缺乏精细标注数据为准确数字生成带来了重大障碍。为了解决这一问题，我们提出了一个新的数据集，即NumHG，并为其进行了27,000多个精心标注的新闻文章的 исследова。此外，我们使用人类评估来评估五种以前的摘要生成模型，以确定它们在数字准确性、合理性和可读性方面的表现。我们的研究发现，现有的模型在数字准确性方面存在改进的需求，这也证明了NumHG数据集的潜在作用力，以及数字专注的文本生成领域的进一步探讨。
</details></li>
</ul>
<hr>
<h2 id="Open-Sesame-Universal-Black-Box-Jailbreaking-of-Large-Language-Models"><a href="#Open-Sesame-Universal-Black-Box-Jailbreaking-of-Large-Language-Models" class="headerlink" title="Open Sesame! Universal Black Box Jailbreaking of Large Language Models"></a>Open Sesame! Universal Black Box Jailbreaking of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01446">http://arxiv.org/abs/2309.01446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raz Lapid, Ron Langberg, Moshe Sipper</li>
<li>for: 这篇论文旨在提供一种用于 manipulate 大型自然语言模型（LLM）的方法，以便在模型结构和参数不可访问的情况下实现不良目的。</li>
<li>methods: 这篇论文使用了一种基于遗传算法（GA）的方法，通过优化一个通用对抗提示来让模型偏离它的对抗目标，从而导致模型生成不当的输出。</li>
<li>results: 通过广泛的实验，这篇论文证明了这种方法的有效性，从而为负责任AI开发提供了一种诊断工具，用于评估和提高 LLM 的对人意图的Alignment。这是我们所知道的第一个自动化的通用黑盒子监狱攻击。<details>
<summary>Abstract</summary>
Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool for evaluating and enhancing alignment of LLMs with human intent. To our knowledge this is the first automated universal black box jailbreak attack.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通常采用对齐技术来与用户意图和社会准则进行Alignment。 unfortunately，这种对齐可以被黑客利用，以达到不良目的。在这篇论文中，我们介绍了一种新的方法，使用进化算法（GA）来控制LLM，当模型结构和参数不可访问时。GA攻击工作通过优化通用对抗提示，使模型与用户的查询相结合，导致模型的对齐受到干扰，从而导致不良和可能有害的输出。我们的新方法可以系统地揭示模型的局限和漏洞，通过找到模型的响应与预期行为不符的情况。经过广泛的实验，我们证明了我们的技术的有效性，因此贡献到负责AI开发的讨论中，提供了对LLM的对齐评估和提高的 диагности工具。到我们所知，这是第一个自动化的通用黑盒子监狱攻击。
</details></li>
</ul>
<hr>
<h2 id="Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation"><a href="#Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation" class="headerlink" title="Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation"></a>Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02459">http://arxiv.org/abs/2309.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Weinan Tong, Yaoxun Xu, Changhe Song, Zhiyong Wu, Zhao You, Dan Su, Dong Yu, Helen Meng</li>
<li>for: 提高新频谱频率识别（ASR）性能，使用文本数据进行频率适应。</li>
<li>methods: 使用文本数据进行频率适应，通过下采样音频表示来匹配文本表示的长度。</li>
<li>results: 实验结果表明，提出的方法可以更好地学习两个Modalities的统一表示，从而提高ASR性能。<details>
<summary>Abstract</summary>
Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
将两种modalities，语音和文本，映射到共享表示空间是一个研究话题，用文本只数据提高端到端自动语音识别（ASR）性能在新领域。然而，语音表示长度和文本表示长度不一致。遗传方法通过升降样本来匹配语音模式，但可能不匹配预期的实际持续时间。在这篇论文中，我们提出了一种新的匹配策略，通过降低音频表示来匹配文本模式。通过引入一个连续整合和点火（CIF）模块生成匹配语音表示，我们的ASR模型可以更好地从两个modalities中学习统一表示，允许通过文本只数据进行领域适应。实验结果表明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge"><a href="#SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge" class="headerlink" title="SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge"></a>SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01437">http://arxiv.org/abs/2309.01437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Changhe Song, Zhiyong Wu, Helen Meng</li>
<li>for: 提高语音识别的效果，尤其是在域外和长尾数据上</li>
<li>methods: 利用sememe知识来增强语音识别模型</li>
<li>results: 实验表明，sememe知识可以提高语音识别的效果，并且可以提高模型对域外和长尾数据的识别能力<details>
<summary>Abstract</summary>
Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Benchmarking-Large-Language-Models-in-Retrieval-Augmented-Generation"><a href="#Benchmarking-Large-Language-Models-in-Retrieval-Augmented-Generation" class="headerlink" title="Benchmarking Large Language Models in Retrieval-Augmented Generation"></a>Benchmarking Large Language Models in Retrieval-Augmented Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01431">http://arxiv.org/abs/2309.01431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chen700564/RGB">https://github.com/chen700564/RGB</a></li>
<li>paper_authors: Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun</li>
<li>for: This paper aims to evaluate the impact of Retrieval-Augmented Generation (RAG) on large language models (LLMs) and identify potential bottlenecks in their capabilities.</li>
<li>methods: The paper uses a systematic approach to investigate the impact of RAG on LLMs, including the establishment of a new corpus (RGB) and the evaluation of 6 representative LLMs on RGB.</li>
<li>results: The evaluation reveals that while LLMs exhibit some degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information, indicating that there is still a considerable journey ahead to effectively apply RAG to LLMs.<details>
<summary>Abstract</summary>
Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的幻觉可以通过 Retrieval-Augmented Generation（RAG）方法进行缓解。然而，现有的研究缺乏对不同的大量语言模型RAG的精心评估，这使得了解RAG对不同LLM的可能的瓶颈困难。在这篇论文中，我们系统地研究了RAG对大量语言模型的影响。我们分析了不同的大量语言模型在4种基本能力上的表现，包括噪声抵抗、负面排斥、信息集成和Counterfactual Robustness。为此，我们创建了一个新的评估 benchmark，即 Retrieval-Augmented Generation Benchmark（RGB），该benchmark包含了4种分别测试基础能力的测试床。然后，我们评估了6种代表性强的LLM在RGB上，以诊断当前LLM在RAG应用中的挑战。评估结果表明，虽然LLM具有一定的噪声抵抗能力，但仍然在负面排斥、信息集成和面对假信息方面存在显著的困难。上述评估结果表明，在RAG应用中，还有一定的征途需要进行。
</details></li>
</ul>
<hr>
<h2 id="Hateful-Messages-A-Conversational-Data-Set-of-Hate-Speech-produced-by-Adolescents-on-Discord"><a href="#Hateful-Messages-A-Conversational-Data-Set-of-Hate-Speech-produced-by-Adolescents-on-Discord" class="headerlink" title="Hateful Messages: A Conversational Data Set of Hate Speech produced by Adolescents on Discord"></a>Hateful Messages: A Conversational Data Set of Hate Speech produced by Adolescents on Discord</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01413">http://arxiv.org/abs/2309.01413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Fillies, Silvio Peikert, Adrian Paschke</li>
<li>For: The paper aims to address the bias of youth language within hate speech classification and provide a modern and anonymized hate speech youth language data set.* Methods: The research uses a self-developed annotation schema to classify publicly available online messages from the chat platform Discord, with 6.42% of the messages classified as hate speech. The data set includes age annotations for 35,553 messages, with an average author age of under 20 years old.* Results: The paper provides a modern and anonymized hate speech youth language data set consisting of 88,395 annotated chat messages, which can be used to improve the generalizability and performance of automated hate speech classification systems.<details>
<summary>Abstract</summary>
With the rise of social media, a rise of hateful content can be observed. Even though the understanding and definitions of hate speech varies, platforms, communities, and legislature all acknowledge the problem. Therefore, adolescents are a new and active group of social media users. The majority of adolescents experience or witness online hate speech. Research in the field of automated hate speech classification has been on the rise and focuses on aspects such as bias, generalizability, and performance. To increase generalizability and performance, it is important to understand biases within the data. This research addresses the bias of youth language within hate speech classification and contributes by providing a modern and anonymized hate speech youth language data set consisting of 88.395 annotated chat messages. The data set consists of publicly available online messages from the chat platform Discord. ~6,42% of the messages were classified by a self-developed annotation schema as hate speech. For 35.553 messages, the user profiles provided age annotations setting the average author age to under 20 years old.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Zero-shot-information-extraction-from-radiological-reports-using-ChatGPT"><a href="#Zero-shot-information-extraction-from-radiological-reports-using-ChatGPT" class="headerlink" title="Zero-shot information extraction from radiological reports using ChatGPT"></a>Zero-shot information extraction from radiological reports using ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01398">http://arxiv.org/abs/2309.01398</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danqing Hu, Bing Liu, Xiaofeng Zhu, Xudong Lu, Nan Wu</li>
<li>for: 抽象出 radiological report 中有用信息，以便进行第二次分析。</li>
<li>methods: 使用 ChatGPT 大语言模型进行零参数信息提取，不需要注释数据进行模型参数优化。</li>
<li>results: ChatGPT 可以在 847 份 CT 报告中提取有用信息，但还需要进一步改进以提高性能。<details>
<summary>Abstract</summary>
Electronic health records contain an enormous amount of valuable information, but many are recorded in free text. Information extraction is the strategy to transform the sequence of characters into structured data, which can be employed for secondary analysis. However, the traditional information extraction components, such as named entity recognition and relation extraction, require annotated data to optimize the model parameters, which has become one of the major bottlenecks in building information extraction systems. With the large language models achieving good performances on various downstream NLP tasks without parameter tuning, it becomes possible to use large language models for zero-shot information extraction. In this study, we aim to explore whether the most popular large language model, ChatGPT, can extract useful information from the radiological reports. We first design the prompt template for the interested information in the CT reports. Then, we generate the prompts by combining the prompt template with the CT reports as the inputs of ChatGPT to obtain the responses. A post-processing module is developed to transform the responses into structured extraction results. We conducted the experiments with 847 CT reports collected from Peking University Cancer Hospital. The experimental results indicate that ChatGPT can achieve competitive performances for some extraction tasks compared with the baseline information extraction system, but some limitations need to be further improved.
</details>
<details>
<summary>摘要</summary>
电子健康记录包含巨量有价值信息，但许多都记录在自由文本中。信息提取是将字符串序列转换为结构化数据的策略，可以用于次要分析。然而，传统信息提取组件，如命名实体识别和关系EXTRACTION，需要标注数据来优化模型参数，这成为了建立信息提取系统的一个主要瓶颈。随着大语言模型在多个下游NLP任务中达到好表现，不需要参数调整，因此可以使用大语言模型进行零shot信息提取。在这项研究中，我们想要探索是否可以使用最受欢迎的大语言模型ChatGPT提取CT报告中的有用信息。我们首先设计了关注信息的模板Prompt，然后将Prompt与CT报告结合使用ChatGPT来获取响应。我们还开发了一个后处程模块，用于将响应转换为结构化提取结果。我们对847份CT报告进行了实验，结果表明ChatGPT可以与基准信息提取系统相比，在某些提取任务中达到竞争性表现，但还需要进一步改进。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.CL_2023_09_04/" data-id="clp9qz81f00b8ok8801xh4v2a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/55/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/54/">54</a><a class="page-number" href="/page/55/">55</a><span class="page-number current">56</span><a class="page-number" href="/page/57/">57</a><a class="page-number" href="/page/58/">58</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/57/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
