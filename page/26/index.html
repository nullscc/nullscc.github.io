
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/26/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/cs.LG_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T10:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/cs.LG_2023_10_17/">cs.LG - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Balance-Act-Mitigating-Hubness-in-Cross-Modal-Retrieval-with-Query-and-Gallery-Banks"><a href="#Balance-Act-Mitigating-Hubness-in-Cross-Modal-Retrieval-with-Query-and-Gallery-Banks" class="headerlink" title="Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks"></a>Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11612">http://arxiv.org/abs/2310.11612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval">https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval</a></li>
<li>paper_authors: Yimu Wang, Xiangru Jian, Bo Xue</li>
<li>for: 本研究旨在解决跨Modal Retrieval中的干扰问题，即 gallery数据点频繁出现在检索结果中，导致检索性能下降。</li>
<li>methods: 我们提出了一种新的框架，双银行正常化（DBNorm），以及两种新的方法，对比轮径正常化和动态对比轮径正常化，以正常化相似度基于两个银行。这些方法可以减少极值点和查询样本之间的相似度，提高非极值点和查询样本之间的相似度。</li>
<li>results: 我们在多种语言基础 benchmark上进行了广泛的实验，包括文本-图像、文本-视频和文本-音频 benchmark，并证明了我们的方法可以比前方法更好地解决干扰问题，提高检索性能。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval上下载。</a><details>
<summary>Abstract</summary>
In this work, we present a post-processing solution to address the hubness problem in cross-modal retrieval, a phenomenon where a small number of gallery data points are frequently retrieved, resulting in a decline in retrieval performance. We first theoretically demonstrate the necessity of incorporating both the gallery and query data for addressing hubness as hubs always exhibit high similarity with gallery and query data. Second, building on our theoretical results, we propose a novel framework, Dual Bank Normalization (DBNorm). While previous work has attempted to alleviate hubness by only utilizing the query samples, DBNorm leverages two banks constructed from the query and gallery samples to reduce the occurrence of hubs during inference. Next, to complement DBNorm, we introduce two novel methods, dual inverted softmax and dual dynamic inverted softmax, for normalizing similarity based on the two banks. Specifically, our proposed methods reduce the similarity between hubs and queries while improving the similarity between non-hubs and queries. Finally, we present extensive experimental results on diverse language-grounded benchmarks, including text-image, text-video, and text-audio, demonstrating the superior performance of our approaches compared to previous methods in addressing hubness and boosting retrieval performance. Our code is available at https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种后处理解决方案，用于解决跨Modal重现中的枢轴问题，即一些小量的 галеリー数据点频繁地被重现，导致检索性能下降。我们首先理论上证明了在 incorporating both gallery和query数据时，解决枢轴问题的必要性，因为枢轴总是与 gallery和query数据 exhibit high similarity。然后，基于我们的理论结果，我们提出了一种新的框架，双银行Normalization（DBNorm）。在前一些工作中，人们尝试了通过只使用查询样本来缓解枢轴，但DBNorm利用了基于查询和 галеリー样本构建的两个银行来减少在推理中出现的枢轴。此外，为了补充DBNorm，我们提出了两种新的方法，双 inverted softmax和 dual dynamic inverted softmax，用于在两个银行基础上Normalize similarity。specifically，我们的提出的方法可以降低枢轴和查询之间的相似性，而提高非枢轴和查询之间的相似性。最后，我们在多种语言基础 benchmark上进行了广泛的实验，包括文本-图像、文本-视频和文本-声音，并证明了我们的方法比前一些方法在解决枢轴和提高检索性能方面表现更出色。我们的代码可以在 https://github.com/yimuwangcs/Better_Cross_Modal_Retrieval 上获取。
</details></li>
</ul>
<hr>
<h2 id="In-defense-of-parameter-sharing-for-model-compression"><a href="#In-defense-of-parameter-sharing-for-model-compression" class="headerlink" title="In defense of parameter sharing for model-compression"></a>In defense of parameter sharing for model-compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11611">http://arxiv.org/abs/2310.11611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Desai, Anshumali Shrivastava</li>
<li>for: 这篇论文主要目的是对模型减小内存占用的方法进行全面评估，并推广RPS方法在模型压缩中的应用。</li>
<li>methods: 本论文使用了RPS方法、截割技术和建立更小的模型来减少模型的内存占用。</li>
<li>results: 研究发现，RPS方法在压缩范围内 consistently 击败&#x2F;匹配更小的模型和一些中等知识水平的截割策略，特别在高压缩场景下。此外，RPS方法还有较高的鲁棒性和稳定性。<details>
<summary>Abstract</summary>
When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent capacity advantage that RPS enjoys over sparse models. Theoretically, we establish RPS as a superior technique in terms of memory-efficient representation when compared to pruning for linear models. This paper argues in favor of paradigm shift towards RPS based models. During our rigorous evaluation of RPS, we identified issues in the state-of-the-art RPS technique ROAST, specifically regarding stability (ROAST's sensitivity to initialization hyperparameters, often leading to divergence) and Pareto-continuity (ROAST's inability to recover the accuracy of the original model at zero compression). We provably address both of these issues. We refer to the modified RPS, which incorporates our improvements, as STABLE-RPS.
</details>
<details>
<summary>摘要</summary>
当考虑模型建 architecture时，有几种方法可以降低其内存占用量。历史上，流行的方法包括选择更小的 architecture和通过剪裁来减少模型的大小。在这篇论文中，我们系统地评估了减少内存和精度之间的权衡，并对 RPS、剪裁技术和建立更小的模型进行了比较。我们的发现表明，RPS在整个压缩范围内一直表现出优异，特别是在更高的压缩场景下。此外，RPS还比所有中等知识的剪裁策略（如MAG、SNIP、SYNFLOW和GRASP）在整个压缩范围内表现出优异。这种优势在高压缩场景下特别明显。在高压缩场景下，RPS甚至超过了高知识剪裁策略（如Lottery Ticket Rewinding）的性能。这表明RPS在压缩场景下具有内存效率的优势。从理论角度来看，我们证明RPS在线性模型上是一种更佳的压缩技术。这篇论文提倡使用基于RPS的模型。在我们对RPS进行了严格的评估后，我们发现了一些问题，包括ROAST的稳定性和紧张性（ROAST的初始化参数的敏感性，常导致偏转）以及级联稳定性（ROAST无法在零压缩场景下恢复原始模型的精度）。我们解决了这些问题，并提出了一种改进后的RPS，称为稳定RPS（STABLE-RPS）。
</details></li>
</ul>
<hr>
<h2 id="Reflection-Equivariant-Diffusion-for-3D-Structure-Determination-from-Isotopologue-Rotational-Spectra-in-Natural-Abundance"><a href="#Reflection-Equivariant-Diffusion-for-3D-Structure-Determination-from-Isotopologue-Rotational-Spectra-in-Natural-Abundance" class="headerlink" title="Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance"></a>Reflection-Equivariant Diffusion for 3D Structure Determination from Isotopologue Rotational Spectra in Natural Abundance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11609">http://arxiv.org/abs/2310.11609</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aspuru-guzik-group/kreed">https://github.com/aspuru-guzik-group/kreed</a></li>
<li>paper_authors: Austin Cheng, Alston Lo, Santiago Miret, Brooks Pate, Alán Aspuru-Guzik<br>for: 这篇论文的目的是用翻译光谱来确定小分子有机物的三维结构。methods: 这篇论文使用了翻译光谱来获得小分子有机物的精准三维信息，并使用 Краичман分析来确定氢原子的取代坐标。results: 这篇论文开发了一种基于生成 diffusion 模型，可以从分子式、翻译光谱和重元素取代坐标来推断小分子有机物的完整三维结构。这种方法的顶尖预测结果可以在 QM9 和 GEOM 数据集上达到 &gt;98% 的准确率。<details>
<summary>Abstract</summary>
Structure determination is necessary to identify unknown organic molecules, such as those in natural products, forensic samples, the interstellar medium, and laboratory syntheses. Rotational spectroscopy enables structure determination by providing accurate 3D information about small organic molecules via their moments of inertia. Using these moments, Kraitchman analysis determines isotopic substitution coordinates, which are the unsigned $|x|,|y|,|z|$ coordinates of all atoms with natural isotopic abundance, including carbon, nitrogen, and oxygen. While unsigned substitution coordinates can verify guesses of structures, the missing $+/-$ signs make it challenging to determine the actual structure from the substitution coordinates alone. To tackle this inverse problem, we develop KREED (Kraitchman REflection-Equivariant Diffusion), a generative diffusion model that infers a molecule's complete 3D structure from its molecular formula, moments of inertia, and unsigned substitution coordinates of heavy atoms. KREED's top-1 predictions identify the correct 3D structure with >98% accuracy on the QM9 and GEOM datasets when provided with substitution coordinates of all heavy atoms with natural isotopic abundance. When substitution coordinates are restricted to only a subset of carbons, accuracy is retained at 91% on QM9 and 32% on GEOM. On a test set of experimentally measured substitution coordinates gathered from the literature, KREED predicts the correct all-atom 3D structure in 25 of 33 cases, demonstrating experimental applicability for context-free 3D structure determination with rotational spectroscopy.
</details>
<details>
<summary>摘要</summary>
STRUCTURE determination 是必需的，以确定未知的有机分子，如自然产物、刑事样本、 междузвездmedium 和实验室合成。扭转 спектроскопия 可以提供有机分子的准确三维信息，通过其惯性矩来确定结构。使用这些矩，卡迪曼分析可以确定原子的替换坐标，即所有原子的自然同位素含量，包括碳、氮和氧。然而，未签名的替换坐标无法决定实际结构，这是一个逆向问题。为解决这个问题，我们开发了 KREED（卡迪曼反射相对均匀扩散），一种生成扩散模型，可以从分子式、惯性矩和未签名重元素替换坐标中推断出分子的完整三维结构。KREED 的顶峰预测可以在 QM9 和 GEOM 数据集上确定分子的正确三维结构，并且在所有重元素替换坐标中具有 >98% 的准确率。当替换坐标只限于一 subset of 碳时，准确率仍保持在 91% 的水平。在Literature中测试的实验ally measured substitution coordinates中，KREED 预测了正确的所有atom 3D结构，证明了实验可行性。
</details></li>
</ul>
<hr>
<h2 id="TK-KNN-A-Balanced-Distance-Based-Pseudo-Labeling-Approach-for-Semi-Supervised-Intent-Classification"><a href="#TK-KNN-A-Balanced-Distance-Based-Pseudo-Labeling-Approach-for-Semi-Supervised-Intent-Classification" class="headerlink" title="TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification"></a>TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11607">http://arxiv.org/abs/2310.11607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/servicenow/tk-knn">https://github.com/servicenow/tk-knn</a></li>
<li>paper_authors: Nicholas Botzer, David Vasquez, Tim Weninger, Issam Laradji</li>
<li>For: The paper is written for improving the ability to detect intent in dialogue systems, specifically by using semi-supervised learning methods to label unlabeled data.* Methods: The paper proposes a new method called Top-K K-Nearest Neighbor (TK-KNN) that uses a more robust pseudo-labeling approach based on distance in the embedding space, while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach.* Results: The experiments on several datasets show that TK-KNN outperforms existing models, particularly when labeled data is scarce, such as on popular datasets like CLINC150 and Banking77.Here are the three key points in Simplified Chinese text:* For: 这篇论文是为了提高对话系统中的意图检测能力，特别是使用半监督学习方法来标注无标签数据。* Methods: 论文提出了一种新的方法called Top-K K-Nearest Neighbor (TK-KNN)，它使用了更加可靠的 pseudo-labeling 方法基于 embedding 空间的距离，同时保持了类别之间的 pseudo-标签例子具有平衡的分布。* Results: 实验结果表明，TK-KNN 在几个 dataset 上表现出色，特别是在标注数据scarce的情况下，如 CLINC150 和 Banking77 等 популяр的 dataset 上。<details>
<summary>Abstract</summary>
The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudo-labels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However, one particularly perilous consequence of these methods is the risk of picking an imbalanced set of examples across classes, which could lead to poor labels. In the present work, we describe Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling approach based on distance in the embedding space while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach. Experiments on several datasets show that TK-KNN outperforms existing models, particularly when labeled data is scarce on popular datasets such as CLINC150 and Banking77. Code is available at https://github.com/ServiceNow/tk-knn
</details>
<details>
<summary>摘要</summary>
现代技术中探测对话系统中的意图已经变得越来越重要。这些系统经常生成大量未标注数据，并且手动标注这些数据需要很大的人工劳动。半超vised方法试图解决这个问题，使用一些标注的示例来训练模型，然后将 pseudo-标签分配给一部分未标注示例，这些示例的模型预测度高于某个阈值。然而，这些方法存在一个特别危险的后果，即选择类别之间不均衡的示例集，这可能导致 poor labels。在 presente 的工作中，我们描述了 Top-K K-Nearest Neighbor (TK-KNN)，这是一种基于距离 embedding 空间的更加可靠的 pseudo-标签方法，同时保持类别之间的 pseudo-标签示例集均衡。在几个数据集上进行了实验，发现 TK-KNN 超越了现有模型，特别是在 CLINC150 和 Banking77 等Popular数据集上。代码可以在 https://github.com/ServiceNow/tk-knn 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Inferring-Users’-Impressions-of-Robot-Performance-in-Navigation-Scenarios"><a href="#Towards-Inferring-Users’-Impressions-of-Robot-Performance-in-Navigation-Scenarios" class="headerlink" title="Towards Inferring Users’ Impressions of Robot Performance in Navigation Scenarios"></a>Towards Inferring Users’ Impressions of Robot Performance in Navigation Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11590">http://arxiv.org/abs/2310.11590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiping Zhang, Nathan Tsoi, Booyeon Choi, Jie Tan, Hao-Tien Lewis Chiang, Marynel Vázquez</li>
<li>for: 这个论文的目的是研究使用非语言行为特征和机器学习技术来预测人们对机器人表现的印象。</li>
<li>methods: 作者提供了一个名为 SEAN TOGETHER 的数据集，包含人与移动机器人在虚拟现实环境中的互动记录，以及用户对机器人表现的评分。同时，作者还进行了人类和超参量学习模型如何预测人们对机器人表现的印象的分析。</li>
<li>results: 研究发现，人脸表情 alone 可以提供有用的信息关于人们对机器人表现的印象，但在我们测试的导航场景中，空间特征是最critical的信息 для这种推断任务。此外，当评分为二分类而不是多类时，人类预测和机器学习模型的 F1 score 更 чем doublies，表明它们都更好地预测机器人表现的方向性而不是具体评分。<details>
<summary>Abstract</summary>
Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Also, when evaluating results as binary classification (rather than multiclass classification), the F1-Score of human predictions and machine learning models more than doubles, showing that both are better at telling the directionality of robot performance than predicting exact performance ratings. Based on our findings, we provide guidelines for implementing these predictions models in real-world navigation scenarios.
</details>
<details>
<summary>摘要</summary>
人类对机器人性能的印象通常通过调查来衡量。作为一种可扩展和成本效果更高的替代方案，我们研究使用非语言行为特征和机器学习技术预测人类对机器人行为的印象。为此，我们首先提供了SEAN TOGETHER数据集，包括人与移动机器人在虚拟现实环境中的互动记录，以及用户对机器人性能的评分（在5分比例上）。其次，我们分析了人类和监督学习技术如何预测人类对机器人性能的印象，根据不同的观察类型（例如，表情特征、空间特征和地图特征）。我们的结果显示，表情特征alone提供了人类对机器人性能的有用信息；但在我们测试的导航场景中，空间特征是最重要的信息来源。此外，当评估结果为二分类（而不是多类）时，人类预测和机器学习模型的F1分值超过了两倍，表示它们都更好地预测机器人性能的方向性，而不是精确的评分。根据我们的发现，我们提供了实现这些预测模型的指南，用于实际导航场景。
</details></li>
</ul>
<hr>
<h2 id="Partially-Observable-Stochastic-Games-with-Neural-Perception-Mechanisms"><a href="#Partially-Observable-Stochastic-Games-with-Neural-Perception-Mechanisms" class="headerlink" title="Partially Observable Stochastic Games with Neural Perception Mechanisms"></a>Partially Observable Stochastic Games with Neural Perception Mechanisms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11566">http://arxiv.org/abs/2310.11566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Yan, Gabriel Santos, Gethin Norman, David Parker, Marta Kwiatkowska</li>
<li>for:  This paper is written for researchers and practitioners interested in multi-agent decision-making under uncertainty, with a focus on partial observability and data-driven perception.</li>
<li>methods: The paper proposes a new model called neuro-symbolic partially-observable stochastic games (NS-POSGs), which incorporates perception mechanisms and is applicable to one-sided settings with discrete, data-driven observations. The paper also introduces a new point-based method called one-sided NS-HSVI for approximating values of NS-POSGs.</li>
<li>results: The paper presents experimental results demonstrating the practical applicability of the proposed method for neural networks whose preimage is in polyhedral form. The results show that the one-sided NS-HSVI method is effective in approximating values of NS-POSGs and can be used to solve real-world problems involving partial observability and data-driven perception.<details>
<summary>Abstract</summary>
Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it based on the popular particle-based beliefs, showing that it has closed forms for computing values of interest. We provide experimental results to demonstrate the practical applicability of our method for neural networks whose preimage is in polyhedral form.
</details>
<details>
<summary>摘要</summary>
We focus on a one-sided setting where the partially-informed agent has discrete, data-driven observations, while the fully-informed agent has continuous observations. We develop a new point-based method called one-sided NS-HSVI for approximating values of one-sided NS-POSGs, which is based on the popular particle-based beliefs. Our method has closed forms for computing values of interest, and we provide experimental results to demonstrate its practical applicability for neural networks whose preimage is in polyhedral form.
</details></li>
</ul>
<hr>
<h2 id="Online-Algorithms-with-Uncertainty-Quantified-Predictions"><a href="#Online-Algorithms-with-Uncertainty-Quantified-Predictions" class="headerlink" title="Online Algorithms with Uncertainty-Quantified Predictions"></a>Online Algorithms with Uncertainty-Quantified Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11558">http://arxiv.org/abs/2310.11558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Sun, Jerry Huang, Nicolas Christianson, Mohammad Hajiesmaili, Adam Wierman</li>
<li>for: This paper focuses on developing online algorithms that incorporate uncertainty-quantified predictions to achieve high-quality performance guarantees while maintaining bounded worst-case guarantees.</li>
<li>methods: The paper explores the use of uncertainty-quantified predictions in online algorithms, specifically for ski rental and online search problems. The authors propose non-trivial modifications to algorithm design to fully leverage the probabilistic predictions.</li>
<li>results: The paper demonstrates the effectiveness of the proposed methods through theoretical analysis and experimental evaluations. The results show that the algorithms achieve better performance guarantees compared to traditional online algorithms, and the uncertainty-quantified predictions provide valuable information for making optimal decisions in multi-instance settings.<details>
<summary>Abstract</summary>
Online algorithms with predictions have become a trending topic in the field of beyond worst-case analysis of algorithms. These algorithms incorporate predictions about the future to obtain performance guarantees that are of high quality when the predictions are good, while still maintaining bounded worst-case guarantees when predictions are arbitrarily poor. In general, the algorithm is assumed to be unaware of the prediction's quality. However, recent developments in the machine learning literature have studied techniques for providing uncertainty quantification on machine-learned predictions, which describes how certain a model is about its quality. This paper examines the question of how to optimally utilize uncertainty-quantified predictions in the design of online algorithms. In particular, we consider predictions augmented with uncertainty quantification describing the likelihood of the ground truth falling in a certain range, designing online algorithms with these probabilistic predictions for two classic online problems: ski rental and online search. In each case, we demonstrate that non-trivial modifications to algorithm design are needed to fully leverage the probabilistic predictions. Moreover, we consider how to utilize more general forms of uncertainty quantification, proposing a framework based on online learning that learns to exploit uncertainty quantification to make optimal decisions in multi-instance settings.
</details>
<details>
<summary>摘要</summary>
在 beyond worst-case 分析算法领域，在线算法 Predictions 已经成为一个流行的话题。这些算法利用未来的预测来获得高质量的性能保证，当预测准确度很好时，而且仍保持 bounded worst-case 保证，当预测准确度很差时。在总的来说，算法假设不知道预测的质量。然而，现代机器学习文献中的技术已经研究了提供机器学习预测的不确定性评估，这种评估描述了模型对其质量的确定程度。本文考虑了如何优化不确定性评估的预测，并在两个经典的在线问题上进行了实践：滑雪租赁和在线搜索。在每个情况下，我们表明了非常轻量级的修改，以便完全利用预测的不确定性。此外，我们考虑了如何利用更加通用的不确定性评估，提出了基于在线学习的框架，用于在多实例设置中学习利用不确定性评估来做出优化的决策。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Error-Mitigation-in-Software-Generated-Data-An-Advanced-Search-and-Optimization-Framework-Leveraging-Generative-Code-Models"><a href="#Bias-and-Error-Mitigation-in-Software-Generated-Data-An-Advanced-Search-and-Optimization-Framework-Leveraging-Generative-Code-Models" class="headerlink" title="Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models"></a>Bias and Error Mitigation in Software-Generated Data: An Advanced Search and Optimization Framework Leveraging Generative Code Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11546">http://arxiv.org/abs/2310.11546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernesto Giralt Hernández</li>
<li>for:  corrected errors and biases in software systems specializing in data analysis and generation</li>
<li>methods: Solomonoff Induction, Kolmogorov Conditional Complexity, generative models ( LLMS)</li>
<li>results: incrementally improve the quality of output results<details>
<summary>Abstract</summary>
Data generation and analysis is a fundamental aspect of many industries and disciplines, from strategic decision making in business to research in the physical and social sciences. However, data generated using software and algorithms can be subject to biases and errors. These can be due to problems with the original software, default settings that do not align with the specific needs of the situation, or even deeper problems with the underlying theories and models. This paper proposes an advanced search and optimization framework aimed at generating and choosing optimal source code capable of correcting errors and biases from previous versions to address typical problems in software systems specializing in data analysis and generation, especially those in the corporate and data science world. Applying this framework multiple times on the same software system would incrementally improve the quality of the output results. It uses Solomonoff Induction as a sound theoretical basis, extending it with Kolmogorov Conditional Complexity, a novel adaptation, to evaluate a set of candidate programs. We propose the use of generative models for the creation of this set of programs, with special emphasis on the capabilities of Large Language Models (LLMs) to generate high quality code.
</details>
<details>
<summary>摘要</summary>
“数据生成和分析是许多行业和领域的基础方面，从商业战略决策到物理和社会科学研究。但是，由软件和算法生成的数据可能受到偏见和错误的影响。这些问题可能来自原始软件的问题、不适应特定情况的默认设置或更深层次的理论和模型问题。这篇论文提出了一种高级搜索和优化框架，用于生成和选择修正过去版本中的错误和偏见的最佳源代码。通过多次应用这种框架于同一个软件系统，可以逐步提高输出结果的质量。它基于索löмо夫推理为基础，并将其扩展到科尔莫果ров conditional complexity，一种新的适应，以评估候选程序集。我们建议使用生成模型来创建这些候选程序集，尤其是利用大型自然语言模型（LLMs）生成高质量代码。”
</details></li>
</ul>
<hr>
<h2 id="Thin-and-Deep-Gaussian-Processes"><a href="#Thin-and-Deep-Gaussian-Processes" class="headerlink" title="Thin and Deep Gaussian Processes"></a>Thin and Deep Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11527">http://arxiv.org/abs/2310.11527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spectraldani/thindeepgps">https://github.com/spectraldani/thindeepgps</a></li>
<li>paper_authors: Daniel Augusto de Souza, Alexander Nikitin, ST John, Magnus Ross, Mauricio A. Álvarez, Marc Peter Deisenroth, João P. P. Gomes, Diego Mesquita, César Lincoln C. Mattos</li>
<li>for: 这个论文的目的是提出一种新的深度 Gaussian process（TDGP）模型，以解决深度 Gaussian process（DGP）模型中的一些问题，如敏感性和解释性的缺失。</li>
<li>methods: TDGP 模型使用了successively parameterizing kernels with Gaussian process layers，这种方法可以学习输入数据的低维度表示，同时保持 kernel 的 interpretable 性。</li>
<li>results: 研究发现，TDGP 模型可以在标准的 benchmark 数据集上表现出色，并且可以适应增加层数的情况。此外，TDGP 模型可以学习低维度表示，并且不会出现特定的PATHOLOGIES。<details>
<summary>Abstract</summary>
Gaussian processes (GPs) can provide a principled approach to uncertainty quantification with easy-to-interpret kernel hyperparameters, such as the lengthscale, which controls the correlation distance of function values. However, selecting an appropriate kernel can be challenging. Deep GPs avoid manual kernel engineering by successively parameterizing kernels with GP layers, allowing them to learn low-dimensional embeddings of the inputs that explain the output data. Following the architecture of deep neural networks, the most common deep GPs warp the input space layer-by-layer but lose all the interpretability of shallow GPs. An alternative construction is to successively parameterize the lengthscale of a kernel, improving the interpretability but ultimately giving away the notion of learning lower-dimensional embeddings. Unfortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis of both previous approaches: Thin and Deep GP (TDGP). Each TDGP layer defines locally linear transformations of the original input data maintaining the concept of latent embeddings while also retaining the interpretation of lengthscales of a kernel. Moreover, unlike the prior solutions, TDGP induces non-pathological manifolds that admit learning lower-dimensional representations. We show with theoretical and experimental results that i) TDGP is, unlike previous models, tailored to specifically discover lower-dimensional manifolds in the input data, ii) TDGP behaves well when increasing the number of layers, and iii) TDGP performs well in standard benchmark datasets.
</details>
<details>
<summary>摘要</summary>
traducción al chino simplificado:Gaussian processes (GPs) 可以提供一个原理性的方法来评估uncertainty量化，通过容易理解的kernel参数，如lengthscale，控制函数值之间的相关程度。然而，选择合适的kernel可以是困难的。深度GPs 可以通过 successively parameterizing kernels with GP layers 来避免手动kernel工程，从而学习输入数据的低维表示。然而，这些方法通常会失去 shallow GPs 中的解释性。一种alternative construction是 successively parameterize the lengthscale of a kernel，以提高解释性，但是 ultimately give up the notion of learning lower-dimensional embeddings。fortunately, both methods are susceptible to particular pathologies which may hinder fitting and limit their interpretability. This work proposes a novel synthesis of both previous approaches: Thin and Deep GP (TDGP). Each TDGP layer defines locally linear transformations of the original input data maintaining the concept of latent embeddings while also retaining the interpretation of lengthscales of a kernel. Moreover, unlike the prior solutions, TDGP induces non-pathological manifolds that admit learning lower-dimensional representations. We show with theoretical and experimental results that i) TDGP is, unlike previous models, tailored to specifically discover lower-dimensional manifolds in the input data, ii) TDGP behaves well when increasing the number of layers, and iii) TDGP performs well in standard benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="Value-Biased-Maximum-Likelihood-Estimation-for-Model-based-Reinforcement-Learning-in-Discounted-Linear-MDPs"><a href="#Value-Biased-Maximum-Likelihood-Estimation-for-Model-based-Reinforcement-Learning-in-Discounted-Linear-MDPs" class="headerlink" title="Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs"></a>Value-Biased Maximum Likelihood Estimation for Model-based Reinforcement Learning in Discounted Linear MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11515">http://arxiv.org/abs/2310.11515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Heng Hung, Ping-Chun Hsieh, Akshay Mete, P. R. Kumar</li>
<li>for:  linear Markov Decision Processes (MDPs) with infinite horizon and linearly parameterized transition probabilities</li>
<li>methods: Value-Biased Maximum Likelihood Estimation (VBMLE)</li>
<li>results: $\widetilde{O}(d\sqrt{T})$ regret, computationally more efficient than existing regression-based approaches, and a generic convergence result of MLE in linear MDPs through a novel supermartingale construct.Here’s the Chinese translation of the three points:</li>
<li>for:  linear Markov 决策过程 (MDPs)  WITH infinite horizon 和 linearly parameterized transition probabilities</li>
<li>methods: Value-Biased Maximum Likelihood Estimation (VBMLE)</li>
<li>results: $\widetilde{O}(d\sqrt{T})$ regret, computationally more efficient than existing regression-based approaches, AND a generic convergence result of MLE in linear MDPs through a novel supermartingale construct.<details>
<summary>Abstract</summary>
We consider the infinite-horizon linear Markov Decision Processes (MDPs), where the transition probabilities of the dynamic model can be linearly parameterized with the help of a predefined low-dimensional feature mapping. While the existing regression-based approaches have been theoretically shown to achieve nearly-optimal regret, they are computationally rather inefficient due to the need for a large number of optimization runs in each time step, especially when the state and action spaces are large. To address this issue, we propose to solve linear MDPs through the lens of Value-Biased Maximum Likelihood Estimation (VBMLE), which is a classic model-based exploration principle in the adaptive control literature for resolving the well-known closed-loop identification problem of Maximum Likelihood Estimation. We formally show that (i) VBMLE enjoys $\widetilde{O}(d\sqrt{T})$ regret, where $T$ is the time horizon and $d$ is the dimension of the model parameter, and (ii) VBMLE is computationally more efficient as it only requires solving one optimization problem in each time step. In our regret analysis, we offer a generic convergence result of MLE in linear MDPs through a novel supermartingale construct and uncover an interesting connection between linear MDPs and online learning, which could be of independent interest. Finally, the simulation results show that VBMLE significantly outperforms the benchmark method in terms of both empirical regret and computation time.
</details>
<details>
<summary>摘要</summary>
我们考虑无穷远线性Markov决策过程（MDP），其过程概率转移可线性参数化通过一个固定的低维度特征映射。现有的回归方法有理论上可达到近似优劣 regret，但 computationally 较为慢，特别是当状态和动作空间较大时。为解决这个问题，我们提议通过Value-Biased Maximum Likelihood Estimation（VBMLE）解决linear MDPs，VBMLE 是适应控制文献中的一种经典的模型基于探索原理，用于解决Maximum Likelihood Estimation 的关闭loop标定问题。我们正式表明VBMLE 具有 $\widetilde{O}(d\sqrt{T})$ regret，其中 $T$ 是时间悬度，$d$ 是模型参数的维度，并且VBMLE  computationally 更高效，只需在每个时间步骤中解决一个优化问题。在我们的 regret 分析中，我们提供了线性 MDPs 的MLE 的普适减少结果，并发现了线性 MDPs 与在线学习之间的有趣连接，这可能是独立的兴趣。最后，实验结果显示VBMLE 在 empirical regret 和计算时间上明显超过参考方法。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Quantum-Sampling-for-Non-Logconcave-Distributions-and-Estimating-Partition-Functions"><a href="#Stochastic-Quantum-Sampling-for-Non-Logconcave-Distributions-and-Estimating-Partition-Functions" class="headerlink" title="Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions"></a>Stochastic Quantum Sampling for Non-Logconcave Distributions and Estimating Partition Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11445">http://arxiv.org/abs/2310.11445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guneykan Ozgul, Xiantao Li, Mehrdad Mahdavi, Chunhao Wang</li>
<li>for: 这个论文目标是设计一种量子算法来采样非几何均匀概率分布 $\pi(x) \propto \exp(-\beta f(x))$.</li>
<li>methods: 这个方法基于量子模拟热化算法，使用慢变化的马尔可夫链，并使用小批量的梯度诊断来实现量子步进。</li>
<li>results: 这个量子算法在维度和精度两个方面表现出了幂等速度的优化，比最佳known的类型算法更快。<details>
<summary>Abstract</summary>
We present quantum algorithms for sampling from non-logconcave probability distributions in the form of $\pi(x) \propto \exp(-\beta f(x))$. Here, $f$ can be written as a finite sum $f(x):= \frac{1}{N}\sum_{k=1}^N f_k(x)$. Our approach is based on quantum simulated annealing on slowly varying Markov chains derived from unadjusted Langevin algorithms, removing the necessity for function evaluations which can be computationally expensive for large data sets in mixture modeling and multi-stable systems. We also incorporate a stochastic gradient oracle that implements the quantum walk operators inexactly by only using mini-batch gradients. As a result, our stochastic gradient based algorithm only accesses small subsets of data points in implementing the quantum walk. One challenge of quantizing the resulting Markov chains is that they do not satisfy the detailed balance condition in general. Consequently, the mixing time of the algorithm cannot be expressed in terms of the spectral gap of the transition density, making the quantum algorithms nontrivial to analyze. To overcome these challenges, we first build a hypothetical Markov chain that is reversible, and also converges to the target distribution. Then, we quantified the distance between our algorithm's output and the target distribution by using this hypothetical chain as a bridge to establish the total complexity. Our quantum algorithms exhibit polynomial speedups in terms of both dimension and precision dependencies when compared to the best-known classical algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了量子算法用于采样非几何均勋分布，其形式为 $\pi(x) \propto \exp(-\beta f(x))$。其中，$f$ 可以写作 finite sum $f(x):= \frac{1}{N}\sum_{k=1}^N f_k(x)$。我们的方法基于量子模拟热化法，使用慢变化的马尔可夫链，从无调整的勒文算法中 derivation，从而消除了计算成本较高的函数评估，特别是在混合模型和多稳定系统中。我们还使用 Stochastic gradient oracle，通过使用小批量评估来实现量子步进 operator。因此，我们的 Stochastic gradient 基本算法只需访问小 subsets of data points，实现量子步进。一个挑战是量化得到的马尔可夫链不满足细化平衡条件，因此我们无法通过spectral gap 来衡量混合时间。为了突破这些挑战，我们首先建立一个假的马尔可夫链，该链是可逆的，并且 converge 到目标分布。然后，我们使用这个假链作为桥，来衡量我们算法的输出和目标分布之间的距离。我们的量子算法在维度和精度上都 exhibit 对比 classical algorithms 的多项式减速。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Interpretable-Visual-Features-in-Artificial-and-Biological-Neural-Systems"><a href="#Identifying-Interpretable-Visual-Features-in-Artificial-and-Biological-Neural-Systems" class="headerlink" title="Identifying Interpretable Visual Features in Artificial and Biological Neural Systems"></a>Identifying Interpretable Visual Features in Artificial and Biological Neural Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11431">http://arxiv.org/abs/2310.11431</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Klindt, Sophia Sanborn, Francisco Acosta, Frédéric Poitevin, Nina Miolane</li>
<li>for: 这个论文的目的是为了探讨深度神经网络中单个神经元的可解释性，以及是否存在多个无关的特征在同一个神经元中的表现。</li>
<li>methods: 这篇论文使用了一种自动化的可解释性量化方法，该方法基于大量的人类 psychophysics 判断神经元可解释性的数据库，并且还提出了一种在网络活动空间找到有意义的方向的方法。</li>
<li>results: 研究发现，使用这种方法可以在卷积神经网络中找到更加直观的方向，这些方向不同于单个神经元的表现。此外，研究还应用了这种方法于三个最近发表的视觉神经响应数据集，并发现结论大致传递到实际神经数据中，这建议superposition可能被脑部实现。这也提出了关于稳定、高效和分解表示的基本问题，并且与分解有关。<details>
<summary>Abstract</summary>
Single neurons in neural networks are often interpretable in that they represent individual, intuitively meaningful features. However, many neurons exhibit $\textit{mixed selectivity}$, i.e., they represent multiple unrelated features. A recent hypothesis proposes that features in deep networks may be represented in $\textit{superposition}$, i.e., on non-orthogonal axes by multiple neurons, since the number of possible interpretable features in natural data is generally larger than the number of neurons in a given network. Accordingly, we should be able to find meaningful directions in activation space that are not aligned with individual neurons. Here, we propose (1) an automated method for quantifying visual interpretability that is validated against a large database of human psychophysics judgments of neuron interpretability, and (2) an approach for finding meaningful directions in network activation space. We leverage these methods to discover directions in convolutional neural networks that are more intuitively meaningful than individual neurons, as we confirm and investigate in a series of analyses. Moreover, we apply the same method to three recent datasets of visual neural responses in the brain and find that our conclusions largely transfer to real neural data, suggesting that superposition might be deployed by the brain. This also provides a link with disentanglement and raises fundamental questions about robust, efficient and factorized representations in both artificial and biological neural systems.
</details>
<details>
<summary>摘要</summary>
单一神经元在神经网络中经常是可解释的，它们表示单一、直觉的特征。然而，许多神经元会表现出混合选择性，即它们表示多个无关的特征。一个最近的假设提出了，内部特征在深度网络中可能会被表示为组合，即在非正交的轴上由多个神经元表示。由于自然数据中的可解释特征的数量通常大于给定网络中的神经元数量，因此我们应该能够在网络启动空间中找到有意义的方向。我们提出了以下两个方法来进行这些研究：1. 一个自动化的方法来评估视觉可解释性，该方法被验证了一个大量的人类心理学评价神经元可解释性的数据库。2. 一种方法来在网络启动空间中找到有意义的方向，这些方法可以在实际的神经网络中发现更直觉的方向。我们运用这些方法发现，对于某些问题，深度网络中的活动空间中的方向可能更直觉、更有意义，并且在实际的神经网络中发现了这些方向。此外，我们将这些方法应用到了三个最近的视觉神经反应数据中，发现结果大多转移到了实际的神经资料中，这表明了组合可能被脑部使用。这还提供了与分离开来的连结，并提出了基本问题，例如如何实现可靠、高效和分离的表示在人工和生物神经系统中。
</details></li>
</ul>
<hr>
<h2 id="Butterfly-Effects-of-SGD-Noise-Error-Amplification-in-Behavior-Cloning-and-Autoregression"><a href="#Butterfly-Effects-of-SGD-Noise-Error-Amplification-in-Behavior-Cloning-and-Autoregression" class="headerlink" title="Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression"></a>Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11428">http://arxiv.org/abs/2310.11428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Block, Dylan J. Foster, Akshay Krishnamurthy, Max Simchowitz, Cyril Zhang</li>
<li>for: This paper studies the issue of training instability in behavior cloning with deep neural networks, specifically the sharp oscillations in long-horizon rewards that can occur during training.</li>
<li>methods: The authors use minibatch SGD updates to the policy network during training, and empirically disentangle the statistical and computational causes of the oscillations. They also test several standard mitigation techniques and find an exponential moving average (EMA) of iterates to be effective in alleviating the issue.</li>
<li>results: The authors show that GVA is a common phenomenon in both continuous control and autoregressive language generation, and that EMA can effectively mitigate it. They also provide theoretical vignettes to explain the benefits of EMA in alleviating GVA and shed light on the extent to which classical convex models can help in understanding the benefits of iterate averaging in deep learning.<details>
<summary>Abstract</summary>
This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch SGD updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch SGD noise through unstable closed-loop dynamics. While SGD noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification (GVA). We show that many standard mitigation techniques do not alleviate GVA, but find an exponential moving average (EMA) of iterates to be surprisingly effective at doing so. We illustrate the generality of this phenomenon by showing the existence of GVA and its amelioration by EMA in both continuous control and autoregressive language generation. Finally, we provide theoretical vignettes that highlight the benefits of EMA in alleviating GVA and shed light on the extent to which classical convex models can help in understanding the benefits of iterate averaging in deep learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Group-blind-optimal-transport-to-group-parity-and-its-constrained-variants"><a href="#Group-blind-optimal-transport-to-group-parity-and-its-constrained-variants" class="headerlink" title="Group-blind optimal transport to group parity and its constrained variants"></a>Group-blind optimal transport to group parity and its constrained variants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11407">http://arxiv.org/abs/2310.11407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Zhou, Jakub Marecek</li>
<li>for: 这个论文的目的是为了实现不受人类特征（如性别、种族）影响的机器学习模型。</li>
<li>methods: 这个论文使用了一种单组盲投影 map，将源数据中两个组的特征分布都 align，实现（人口）组均点，无需个体样本中的敏感属性值。</li>
<li>results: 作者通过使用真实数据和 sintetic数据进行数值实验，证明了这种方法可以实现不受敏感属性影响的机器学习模型。<details>
<summary>Abstract</summary>
Fairness holds a pivotal role in the realm of machine learning, particularly when it comes to addressing groups categorised by sensitive attributes, e.g., gender, race. Prevailing algorithms in fair learning predominantly hinge on accessibility or estimations of these sensitive attributes, at least in the training process. We design a single group-blind projection map that aligns the feature distributions of both groups in the source data, achieving (demographic) group parity, without requiring values of the protected attribute for individual samples in the computation of the map, as well as its use. Instead, our approach utilises the feature distributions of the privileged and unprivileged groups in a boarder population and the essential assumption that the source data are unbiased representation of the population. We present numerical results on synthetic data and real data.
</details>
<details>
<summary>摘要</summary>
“公平在机器学习中扮演着关键角色，特别是在面临敏感属性分类的群体时。现有的 Fair learning 算法主要基于敏感属性的访问或估计，至少在训练过程中。我们设计了一个单一群体盲目投影Map，使源数据中两个群体的特征分布相互对齐，实现群体平均性，无需个别样本中的敏感属性值，也无需在计算投影Map时和其使用过程中使用敏感属性值。而是我们的方法基于优先群体和受难群体在更大的人口中的特征分布，以及假设源数据是人口的不偏 representations。我们在synthetic数据和实际数据上提供数字结果。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Group-Fairness-in-Online-Settings-Using-Oblique-Decision-Forests"><a href="#Enhancing-Group-Fairness-in-Online-Settings-Using-Oblique-Decision-Forests" class="headerlink" title="Enhancing Group Fairness in Online Settings Using Oblique Decision Forests"></a>Enhancing Group Fairness in Online Settings Using Oblique Decision Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11401">http://arxiv.org/abs/2310.11401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somnath Basu Roy Chowdhury, Nicholas Monath, Ahmad Beirami, Rahul Kidambi, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi</li>
<li>for: 这篇论文的目的是提出一种在线上环境中实现公平的机器学习系统，以确保不同群体之间的公平。</li>
<li>methods: 这篇论文提出了一个名为Aranyani的ensemble of oblique decision trees的方法，可以在线上环境中实现公平的决策。Aranyani使用了树结构，允许在决策时计算公平度量，并且可以快速计算公平度量，不需要额外的储存和前向&#x2F;后向通过。</li>
<li>results: 这篇论文的实验结果显示，Aranyani可以在5个公开的benchmark上 achieves a better accuracy-fairness trade-off compared to baseline approaches。<details>
<summary>Abstract</summary>
Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings. The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes. We also present an efficient framework to train Aranyani and theoretically analyze several of its properties. We conduct empirical evaluations on 5 publicly available benchmarks (including vision and language datasets) to show that Aranyani achieves a better accuracy-fairness trade-off compared to baseline approaches.
</details>
<details>
<summary>摘要</summary>
“公平性，特别是群体公平性，在机器学习系统中是一个重要考虑因素。通常运用的群体公平化技术是在训练过程中使用混合物的公平目标（例如人口平衡）和任务特定目标（例如十字项目）。但在线上数据来临时，实现这些公平目标是有挑战的。具体来说，群体公平目标是根据不同群体的预期预测结果定义的。在线上设置中，algorithm只有单独的实例，估计群体公平目标需要额外的存储和更多的计算（例如前向/后向通过）。在这篇论文中，我们提出Aranyani，一个以梯形树为基础的混合决策树，以确保在线上设置中做出公平的决策。Aranyani的树状架构允许参数隔离和通过先前的决策统计资料来计算公平的梯度，无需额外的存储和前向/后向通过。我们还提供了一个有效的训练框架和理论分析多个性能。我们在5个公开可用的benchmark（包括视觉和语言dataset）进行实验评估，发现Aranyani在精度-公平性贡献中比基准方法更好。”
</details></li>
</ul>
<hr>
<h2 id="Last-One-Standing-A-Comparative-Analysis-of-Security-and-Privacy-of-Soft-Prompt-Tuning-LoRA-and-In-Context-Learning"><a href="#Last-One-Standing-A-Comparative-Analysis-of-Security-and-Privacy-of-Soft-Prompt-Tuning-LoRA-and-In-Context-Learning" class="headerlink" title="Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning"></a>Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11397">http://arxiv.org/abs/2310.11397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem</li>
<li>for: 这篇论文旨在探讨大语言模型（LLM）适用私有数据时的隐私和安全问题。</li>
<li>methods: 论文使用了三种已知技术来适应LLM：Low-Rank Adaptation（LoRA）、Soft Prompt Tuning（SPT）和In-Context Learning（ICL）。</li>
<li>results: 研究发现，无一种适合所有隐私和安全需求的适应技术，每种技术都有不同的优劣点。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are powerful tools for natural language processing, enabling novel applications and user experiences. However, to achieve optimal performance, LLMs often require adaptation with private data, which poses privacy and security challenges. Several techniques have been proposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA), Soft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative privacy and security properties have not been systematically investigated. In this work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL against three types of well-established attacks: membership inference, which exposes data leakage (privacy); backdoor, which injects malicious behavior (security); and model stealing, which can violate intellectual property (privacy and security). Our results show that there is no silver bullet for privacy and security in LLM adaptation and each technique has different strengths and weaknesses.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="VaR-and-CVaR-Estimation-in-a-Markov-Cost-Process-Lower-and-Upper-Bounds"><a href="#VaR-and-CVaR-Estimation-in-a-Markov-Cost-Process-Lower-and-Upper-Bounds" class="headerlink" title="VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds"></a>VaR\ and CVaR Estimation in a Markov Cost Process: Lower and Upper Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11389">http://arxiv.org/abs/2310.11389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanjay Bhat, Prashanth L. A., Gugan Thoppe</li>
<li>for: 这篇论文旨在研究Markov过程中的 Value-at-Risk（VaR）和Conditional Value-at-Risk（CVaR）估计问题。</li>
<li>methods: 该论文首先 derivates a minimax下界为Ω(1&#x2F;√n)，这个下界在预期和 probabilistic上都成立。然后，使用finite-horizon truncation scheme， derivates an upper bound for CVaR estimation error, which matches the lower bound up to constant factors。</li>
<li>results: 该论文的结果表明，在Markovian设定下，our estimation scheme can provide lower and upper bounds on the estimation error for any risk measure, including spectral risk measures and utility-based shortfall risk. Our lower bounds also extend to the infinite-horizon discounted costs’ mean, and improve upon the existing result Ω(1&#x2F;n) [13].<details>
<summary>Abstract</summary>
We tackle the problem of estimating the Value-at-Risk (VaR) and the Conditional Value-at-Risk (CVaR) of the infinite-horizon discounted cost within a Markov cost process. First, we derive a minimax lower bound of $\Omega(1/\sqrt{n})$ that holds both in an expected and in a probabilistic sense. Then, using a finite-horizon truncation scheme, we derive an upper bound for the error in CVaR estimation, which matches our lower bound up to constant factors. Finally, we discuss an extension of our estimation scheme that covers more general risk measures satisfying a certain continuity criterion, e.g., spectral risk measures, utility-based shortfall risk. To the best of our knowledge, our work is the first to provide lower and upper bounds on the estimation error for any risk measure within Markovian settings. We remark that our lower bounds also extend to the infinite-horizon discounted costs' mean. Even in that case, our result $\Omega(1/\sqrt{n}) $ improves upon the existing result $\Omega(1/n)$[13].
</details>
<details>
<summary>摘要</summary>
我们研究了在马尔可夫过程中估计值风险（VaR）和条件值风险（CVaR）的问题。首先，我们 deriv了一个最小最大下界为 $\Omega(1/\sqrt{n})$，这个下界在预期上和概率上都成立。然后，使用一种 finite-horizon  truncation scheme，我们 deriv了 CVaR 估计错误的Upper bound，与我们的下界几乎相同。最后，我们讨论了我们的估计方案的扩展，覆盖更加一般的风险度量，如спектраль风险度量和utilities-based shortfall风险。据我们所知，我们的工作是在马尔可夫 Setting 中提供了任何风险度量的下界和上界。我们的下界还扩展到了无限期折抵费用的mean。甚至在那种情况下，我们的结果 $\Omega(1/\sqrt{n})$ 超越了现有的结果 $\Omega(1/n)$ [13].
</details></li>
</ul>
<hr>
<h2 id="Faster-Algorithms-for-Generalized-Mean-Densest-Subgraph-Problem"><a href="#Faster-Algorithms-for-Generalized-Mean-Densest-Subgraph-Problem" class="headerlink" title="Faster Algorithms for Generalized Mean Densest Subgraph Problem"></a>Faster Algorithms for Generalized Mean Densest Subgraph Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11377">http://arxiv.org/abs/2310.11377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenglin Fan, Ping Li, Hanyu Peng</li>
<li>for: 本研究的目的是解决 $p$-mean densest subgraph问题，即寻找一个 graphs 中的最密集子图，其中 $p $ 是一个非负整数，表示使用 $p $-th 次幂度来衡量子图的密度。</li>
<li>methods: 本研究使用了一种新的通用抽屉算法（GENPEEL），以及一种更快的通用抽屉算法（GENPEEL++），它们可以在 $p \geq 1 $ 时间复杂度为 $O(mn)$，在 $p \in [1, +\infty)$ 时间复杂度为 $O(m(\log n))$，并且具有 $(p+1)^{1&#x2F;p}$ 和 $(2(p+1))^{1&#x2F;p}$ 的近似比率，分别适用于不同的 $p $ 值。</li>
<li>results: 本研究发现，对于 $0 &lt; p &lt; 1 $，标准抽屉算法可以提供 $2^{1&#x2F;p}$ 的近似比率，而不是预期的 $p $ 次幂度。此外，GENPEEL 和 GENPEEL++ 算法可以在 $p \geq 1 $ 和 $p \in [1, +\infty)$ 中提供 $(p+1)^{1&#x2F;p}$ 和 $(2(p+1))^{1&#x2F;p}$ 的近似比率，分别适用于不同的 $p $ 值。<details>
<summary>Abstract</summary>
The densest subgraph of a large graph usually refers to some subgraph with the highest average degree, which has been extended to the family of $p$-means dense subgraph objectives by~\citet{veldt2021generalized}. The $p$-mean densest subgraph problem seeks a subgraph with the highest average $p$-th-power degree, whereas the standard densest subgraph problem seeks a subgraph with a simple highest average degree. It was shown that the standard peeling algorithm can perform arbitrarily poorly on generalized objective when $p>1$ but uncertain when $0<p<1$. In this paper, we are the first to show that a standard peeling algorithm can still yield $2^{1/p}$-approximation for the case $0<p < 1$. (Veldt 2021) proposed a new generalized peeling algorithm (GENPEEL), which for $p \geq 1$ has an approximation guarantee ratio $(p+1)^{1/p}$, and time complexity $O(mn)$, where $m$ and $n$ denote the number of edges and nodes in graph respectively. In terms of algorithmic contributions, we propose a new and faster generalized peeling algorithm (called GENPEEL++ in this paper), which for $p \in [1, +\infty)$ has an approximation guarantee ratio $(2(p+1))^{1/p}$, and time complexity $O(m(\log n))$, where $m$ and $n$ denote the number of edges and nodes in graph, respectively. This approximation ratio converges to 1 as $p \rightarrow \infty$.
</details>
<details>
<summary>摘要</summary>
通常情况下，最密集子图（dense subgraph）指的是一个具有最高平均度的子图。这个概念在$p$-means dense subgraph目标家族中被推广，其中$p$-mean densest subgraph问题 seek一个具有最高$p$-th-power度的子图。与标准的最密集子图问题不同的是，后者仅寻找一个简单的最高平均度的子图。当$p>1$时，标准的剥离算法可能会表现出现 arbitrarily poor performance，而当$0<p<1$时，则 uncertain。在这篇论文中，我们是第一个证明了标准剥离算法可以在$0<p<1$时 still yield $2^{1/p}$-approximation。在这篇论文中，我们提出了一个新的通用剥离算法（GENPEEL），它在$p\geq 1$时有一个 aproximation guarantee ratio $(p+1)^{1/p}$, 并且时间复杂度为$O(mn)$，其中$m$和$n$是图中边和节点的数量。在算法方面，我们提出了一个新的通用剥离算法（GENPEEL++），它在$p\in [1,+\infty)$时有一个 aproximation guarantee ratio $(2(p+1))^{1/p}$, 并且时间复杂度为$O(m(\log n))$,其中$m$和$n$是图中边和节点的数量。这个approximation ratio随着$p$的增长而 convergence to 1。
</details></li>
</ul>
<hr>
<h2 id="Lie-Group-Decompositions-for-Equivariant-Neural-Networks"><a href="#Lie-Group-Decompositions-for-Equivariant-Neural-Networks" class="headerlink" title="Lie Group Decompositions for Equivariant Neural Networks"></a>Lie Group Decompositions for Equivariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11366">http://arxiv.org/abs/2310.11366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mircea Mironenco, Patrick Forré</li>
<li>For: 这个论文的目标是构建具有对称性和同态性的神经网络模型，特别在数据端不充足的情况下。* Methods: 这个论文使用了利群的 Lie 代数和群 exponential 和 logarithm 函数来扩展传统的对称性模型，并使用了 Lie 群的结构和几何来实现对称集合的归一化和全局均衡。* Results: 这个论文通过对比各种已有的对称模型和卷积神经网络模型，证明了其在标准对称检测任务上的性能优越性和对于不同的输入数据的泛化能力。<details>
<summary>Abstract</summary>
Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \text{GL}^{+}(n, \mathbb{R})$ and $G = \text{SL}(n, \mathbb{R})$, as well as their representation as affine transformations $\mathbb{R}^{n} \rtimes G$. Invariant integration as well as a global parametrization is realized by decomposing the `larger` groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals.
</details>
<details>
<summary>摘要</summary>
固有和等变征对几何变换有利，特别是在数据缺乏时。许多研究都集中在 компакт或很小的symmetry group上。现在的工作探索了使用Lie group的方法，包括Lie algebra、组 exponential和logarithm maps。然而，这些方法的应用 scope limited by the fact that the exponential map may not be surjective, and further limitations are encountered when the group of interest $G$ is neither compact nor abelian.我们使用 Lie group的结构和几何特性，提出一个框架，可以让我们在 $G = \text{GL}^{+}(n, \mathbb{R})$ 和 $G = \text{SL}(n, \mathbb{R})$ 上工作，以及它们的表示为抽象变换 $\mathbb{R}^{n} \rtimes G$。我们可以通过将这些 '大' 组织分解成子组织和子抽象变换，并将它们处理一个一个来实现不变 интеграл和全局参数化。在这个框架下，我们可以设计卷积核心来构建对抽象变换具有不变性的模型。我们在标准对称变换分类任务上评估了我们的模型的稳定性和 OUT-OF-distribution泛化能力，并超越了所有均衡变换模型以及所有卷积网络提议。
</details></li>
</ul>
<hr>
<h2 id="Contextualized-Machine-Learning"><a href="#Contextualized-Machine-Learning" class="headerlink" title="Contextualized Machine Learning"></a>Contextualized Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11340">http://arxiv.org/abs/2310.11340</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SAP/contextual-ai">https://github.com/SAP/contextual-ai</a></li>
<li>paper_authors: Benjamin Lengerich, Caleb N. Ellington, Andrea Rubbi, Manolis Kellis, Eric P. Xing</li>
<li>for: 这篇论文旨在探讨Contextualized Machine Learning（ML），一种学习不同和上下文相关的效应的方法。</li>
<li>methods: 该方法使用深度学习来模型meta-关系，将上下文信息翻译成模型参数，实现不同参数的变化。</li>
<li>results: 该方法可以汇集不同的框架，包括带环境分析和年龄模型，并且可以实现非Parametric推断和模型识别性条件。最后，作者提供了一个开源的PyTorch包ContextualizedML。<details>
<summary>Abstract</summary>
We examine Contextualized Machine Learning (ML), a paradigm for learning heterogeneous and context-dependent effects. Contextualized ML estimates heterogeneous functions by applying deep learning to the meta-relationship between contextual information and context-specific parametric models. This is a form of varying-coefficient modeling that unifies existing frameworks including cluster analysis and cohort modeling by introducing two reusable concepts: a context encoder which translates sample context into model parameters, and sample-specific model which operates on sample predictors. We review the process of developing contextualized models, nonparametric inference from contextualized models, and identifiability conditions of contextualized models. Finally, we present the open-source PyTorch package ContextualizedML.
</details>
<details>
<summary>摘要</summary>
我们研究Contextualized Machine Learning（ML），一种学习不同和上下文依赖的效果的方法。Contextualized ML使用深度学习来模型meta关系，即样本上下文信息和样本特定的参数模型之间的关系。这是一种 varying-coefficient modeling，可以统一现有的框架，包括集群分析和团队模型，通过引入两个可重用概念：样本上下文编码器，将样本上下文转换为模型参数，以及样本特定的模型，对样本预测变量进行操作。我们详细介绍了Contextualized模型的开发、非参数推断、和Contextualized模型的可识别条件。最后，我们发布了一个开源的PyTorch包，即ContextualizedML。
</details></li>
</ul>
<hr>
<h2 id="Non-ergodicity-in-reinforcement-learning-robustness-via-ergodicity-transformations"><a href="#Non-ergodicity-in-reinforcement-learning-robustness-via-ergodicity-transformations" class="headerlink" title="Non-ergodicity in reinforcement learning: robustness via ergodicity transformations"></a>Non-ergodicity in reinforcement learning: robustness via ergodicity transformations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11335">http://arxiv.org/abs/2310.11335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Baumann, Erfaun Noorani, James Price, Ole Peters, Colm Connaughton, Thomas B. Schön</li>
<li>for: This paper aims to address the issue of non-robustness in reinforcement learning (RL) algorithms, specifically in real-world applications such as autonomous driving, precision agriculture, and finance.</li>
<li>methods: The authors propose a new algorithm for learning ergodicity transformations from data, which enables the optimization of long-term return for individual agents rather than the average across infinitely many trajectories.</li>
<li>results: The proposed algorithm is demonstrated to be effective in an instructive, non-ergodic environment and on standard RL benchmarks.Here’s the simplified Chinese text:</li>
<li>for: 本研究旨在解决反对式学习（RL）算法在实际应用中的不稳定性问题，特别是在自动驾驶、精准农业和金融等领域。</li>
<li>methods: 作者提出了一种基于数据学习Ergodicity变换的算法，以便优化个体代理的长期返回，而不是权衡无穷多个轨迹的平均返回。</li>
<li>results: 提出的算法在一个教育性的非ergodic环境以及标准RL benchmark上得到了证明。<details>
<summary>Abstract</summary>
Envisioned application areas for reinforcement learning (RL) include autonomous driving, precision agriculture, and finance, which all require RL agents to make decisions in the real world. A significant challenge hindering the adoption of RL methods in these domains is the non-robustness of conventional algorithms. In this paper, we argue that a fundamental issue contributing to this lack of robustness lies in the focus on the expected value of the return as the sole "correct" optimization objective. The expected value is the average over the statistical ensemble of infinitely many trajectories. For non-ergodic returns, this average differs from the average over a single but infinitely long trajectory. Consequently, optimizing the expected value can lead to policies that yield exceptionally high returns with probability zero but almost surely result in catastrophic outcomes. This problem can be circumvented by transforming the time series of collected returns into one with ergodic increments. This transformation enables learning robust policies by optimizing the long-term return for individual agents rather than the average across infinitely many trajectories. We propose an algorithm for learning ergodicity transformations from data and demonstrate its effectiveness in an instructive, non-ergodic environment and on standard RL benchmarks.
</details>
<details>
<summary>摘要</summary>
拟合应用领域 для强化学习（RL）包括自动驾驶、精细农业和金融，这些领域都需要RL代理人做出实际世界中的决策。然而，现有的RL方法在这些领域的应用受到一定的阻碍。在这篇论文中，我们认为RL方法的一个基本问题在于强调预期返回值作为唯一的“正确”优化目标。预期返回值是统计ensemble中的平均值，对于非ergodic返回，这个平均值与单个但是无限长的轨迹的平均值不同。因此，优化预期返回可能导致政策产生极高的返回，但是几乎确定会导致灾难性的结果。这个问题可以通过将收集到的返回时间序列转换成一个ergodic增量来解决。这种转换允许学习 robust政策，而不是优化infinitely多轨迹的平均值。我们提出了一种从数据中学习ergodicity转换的算法，并在一个 instructive、非ergodic环境中和标准RLbenchmark上进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Elucidating-The-Design-Space-of-Classifier-Guided-Diffusion-Generation"><a href="#Elucidating-The-Design-Space-of-Classifier-Guided-Diffusion-Generation" class="headerlink" title="Elucidating The Design Space of Classifier-Guided Diffusion Generation"></a>Elucidating The Design Space of Classifier-Guided Diffusion Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11311">http://arxiv.org/abs/2310.11311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexmaols/elucd">https://github.com/alexmaols/elucd</a></li>
<li>paper_authors: Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun</li>
<li>for: 提高Diffusion模型的样本质量和可控性，通过主流方法和训练自由方法都需要额外的标注数据训练，而训练自由方法的性能尚未得到证明。</li>
<li>methods: 我们通过对设计空间的全面调查，发现可以通过免训练的方式，使用市场上可得的预训练分类器来提高 diffusion 模型的性能，同时兼顾主流方法和训练自由方法的优点。我们提出了几种预处理技术来更好地利用预训练分类器来导向Diffusion生成。</li>
<li>results: 我们通过对 ImageNet 进行了广泛的实验，证明了我们的提posed方法可以提高 state-of-the-art  diffusion 模型（DDPM、EDM、DiT）的性能（最多提高20%），而且几乎不需要额外的计算成本。随着可得到的预训练分类器的普及，我们的提posed方法具有极大的潜力，可以快速扩展到文本到图像生成任务。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/AlexMaOLS/EluCD/tree/main">https://github.com/AlexMaOLS/EluCD/tree/main</a> 获取。<details>
<summary>Abstract</summary>
Guidance in conditional diffusion generation is of great importance for sample quality and controllability. However, existing guidance schemes are to be desired. On one hand, mainstream methods such as classifier guidance and classifier-free guidance both require extra training with labeled data, which is time-consuming and unable to adapt to new conditions. On the other hand, training-free methods such as universal guidance, though more flexible, have yet to demonstrate comparable performance. In this work, through a comprehensive investigation into the design space, we show that it is possible to achieve significant performance improvements over existing guidance schemes by leveraging off-the-shelf classifiers in a training-free fashion, enjoying the best of both worlds. Employing calibration as a general guideline, we propose several pre-conditioning techniques to better exploit pretrained off-the-shelf classifiers for guiding diffusion generation. Extensive experiments on ImageNet validate our proposed method, showing that state-of-the-art diffusion models (DDPM, EDM, DiT) can be further improved (up to 20%) using off-the-shelf classifiers with barely any extra computational cost. With the proliferation of publicly available pretrained classifiers, our proposed approach has great potential and can be readily scaled up to text-to-image generation tasks. The code is available at https://github.com/AlexMaOLS/EluCD/tree/main.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>Diffusion模型的指导是样本质量和可控性方面的关键因素。然而，现有的指导方法尚不够。一方面，主流方法如类型器指导和类型器无指导都需要额外的训练频道标注数据，时间consuming并不适应新条件。另一方面，无需训练的方法如通用指导，虽然更灵活，尚未达到相关性表现。在这种情况下，我们通过对设计空间的全面调查，展示了可以通过利用准备好的类型器来获得显著性能提升，同时兼得到最佳的两个世界。采用准确性为总则，我们提出了一些预处理技巧来更好地利用预训练的类型器来导向扩散生成。广泛的实验 validate我们的提议方法，显示了使用预训练的类型器可以提高状态当前的扩散模型（DDPM、EDM、DiT）的性能（最高提升20%），而且几乎没有额外的计算成本。随着公共预训练类型器的普及，我们的提议方法具有巨大的潜力，可以轻松扩展到文本到图生成任务。代码可以在 GitHub 上获取：https://github.com/AlexMaOLS/EluCD/tree/main。
</details></li>
</ul>
<hr>
<h2 id="An-Automatic-Learning-Rate-Schedule-Algorithm-for-Achieving-Faster-Convergence-and-Steeper-Descent"><a href="#An-Automatic-Learning-Rate-Schedule-Algorithm-for-Achieving-Faster-Convergence-and-Steeper-Descent" class="headerlink" title="An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent"></a>An Automatic Learning Rate Schedule Algorithm for Achieving Faster Convergence and Steeper Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11291">http://arxiv.org/abs/2310.11291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhao Song, Chiwun Yang</li>
<li>for: 这个研究的目的是提高神经网络训练中的优化技术，尤其是解决 mini-batch 优化中的测验问题。</li>
<li>methods: 本研究使用了 delta-bar-delta 算法，并提出了一个新的 RDBD (Regrettable Delta-Bar-Delta) 方法来解决 convergence 问题。</li>
<li>results: 经过广泛的实验评估，RDBD 方法能够增加优化过程的速度和稳定性，并且可以与不同的优化算法整合使用。<details>
<summary>Abstract</summary>
The delta-bar-delta algorithm is recognized as a learning rate adaptation technique that enhances the convergence speed of the training process in optimization by dynamically scheduling the learning rate based on the difference between the current and previous weight updates. While this algorithm has demonstrated strong competitiveness in full data optimization when compared to other state-of-the-art algorithms like Adam and SGD, it may encounter convergence issues in mini-batch optimization scenarios due to the presence of noisy gradients.   In this study, we thoroughly investigate the convergence behavior of the delta-bar-delta algorithm in real-world neural network optimization. To address any potential convergence challenges, we propose a novel approach called RDBD (Regrettable Delta-Bar-Delta). Our approach allows for prompt correction of biased learning rate adjustments and ensures the convergence of the optimization process. Furthermore, we demonstrate that RDBD can be seamlessly integrated with any optimization algorithm and significantly improve the convergence speed.   By conducting extensive experiments and evaluations, we validate the effectiveness and efficiency of our proposed RDBD approach. The results showcase its capability to overcome convergence issues in mini-batch optimization and its potential to enhance the convergence speed of various optimization algorithms. This research contributes to the advancement of optimization techniques in neural network training, providing practitioners with a reliable automatic learning rate scheduler for achieving faster convergence and improved optimization outcomes.
</details>
<details>
<summary>摘要</summary>
delta-bar-delta 算法是一种学习率自适应技术，可以增加训练过程的速度并且在全数据优化中与其他当前标准算法如 Adam 和 SGD 进行比较。然而，在小批量优化场景下，这种算法可能会遇到收敛问题，这是因为梯度具有噪音。在这个研究中，我们对 delta-bar-delta 算法在实际神经网络优化中的收敛行为进行了全面的调查。为了解决任何可能出现的收敛挑战，我们提出了一种新的 Approach，即 RDBD（Regrettable Delta-Bar-Delta）。我们的方法可以快速更正偏导学习率调整，并确保优化过程的收敛。此外，我们证明了 RDBD 可以轻松地与任何优化算法结合使用，并显著提高优化速度。通过进行了广泛的实验和评估，我们证明了 RDBD 的效果和效率。结果表明，RDBD 可以在小批量优化中解决收敛问题，并且有可能在不同的优化算法中提高收敛速度。这项研究对神经网络训练中优化技术的进步做出了贡献，为实践者提供了一个可靠的自动学习率调整器，以实现更快的收敛和优化结果。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Impact-of-Humanitarian-Aid-on-Food-Security"><a href="#Evaluating-the-Impact-of-Humanitarian-Aid-on-Food-Security" class="headerlink" title="Evaluating the Impact of Humanitarian Aid on Food Security"></a>Evaluating the Impact of Humanitarian Aid on Food Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11287">http://arxiv.org/abs/2310.11287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordi Cerdà-Bautista, José María Tárraga, Vasileios Sitokonstantinou, Gustau Camps-Valls</li>
<li>for: 评估气候变化引起的干旱地区食品安全问题，需要紧急人道主义帮助。</li>
<li>methods: 该文章提出了一种 causal inference 框架，用于评估投入到食品危机中的金钱支付政策的影响。文章包括识别食品安全系统中的 causal 关系、完善全面的数据库、和估计人道援助政策对于营养不良的 causal 效应。</li>
<li>results: 研究发现，这些投入没有显著影响，可能因为样本规模太小、数据质量不佳、和 causal 图不完善，即我们对多学科系统食品安全的理解还有限。这说明需要进一步提高数据收集和精细化 causal 模型，以便更有效地进行未来的投入和政策，提高人道援助的透明度和责任感。<details>
<summary>Abstract</summary>
In the face of climate change-induced droughts, vulnerable regions encounter severe threats to food security, demanding urgent humanitarian assistance. This paper introduces a causal inference framework for the Horn of Africa, aiming to assess the impact of cash-based interventions on food crises. Our contributions encompass identifying causal relationships within the food security system, harmonizing a comprehensive database, and estimating the causal effect of humanitarian interventions on malnutrition. Our results revealed no significant effects, likely due to limited sample size, suboptimal data quality, and an imperfect causal graph resulting from our limited understanding of multidisciplinary systems like food security. This underscores the need to enhance data collection and refine causal models with domain experts for more effective future interventions and policies, improving transparency and accountability in humanitarian aid.
</details>
<details>
<summary>摘要</summary>
面对气候变化引起的干旱，抵触地区面临严重的食品安全威胁，需要急需人道主义援助。这篇论文介绍了一种 causal inference 框架，用于评估针对东非的食品危机造成的影响。我们的贡献包括确定食品安全系统中的 causal 关系，融合全面的数据库，并估算人道主义干预对营养不良的影响。我们的结果表明，没有显著的影响， probable 因为样本规模过小、数据质量不佳和我们对多学科系统的理解不够，从而导致 causal 图不准确。这反映了需要增强数据收集和改进 causal 模型，以更有效地应用未来的援助和政策，提高透明度和责任感。
</details></li>
</ul>
<hr>
<h2 id="Self-supervision-meets-kernel-graph-neural-models-From-architecture-to-augmentations"><a href="#Self-supervision-meets-kernel-graph-neural-models-From-architecture-to-augmentations" class="headerlink" title="Self-supervision meets kernel graph neural models: From architecture to augmentations"></a>Self-supervision meets kernel graph neural models: From architecture to augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11281">http://arxiv.org/abs/2310.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawang Dan, Ruofan Wu, Yunpeng Liu, Baokun Wang, Changhua Meng, Tengfei Liu, Tianyi Zhang, Ningtao Wang, Xing Fu, Qi Li, Weiqiang Wang</li>
<li>for: 这 paper 的目的是提高 kernel graph neural networks (KGNNs) 的设计和学习方法，以提高 graph representation learning 的效果。</li>
<li>methods: 本 paper 使用了一种更加灵活的 graph-level similarity定义，以及一种更加简洁的优化目标函数，以解决 MPNNs 中的一些挑战。此外，paper 还提出了一种新的自我监督学习方法 called latent graph augmentation (LGA)，以提高 KGNNs 的表达能力。</li>
<li>results: 实验结果表明，提出的方法可以在 graph classification 任务中达到竞争性的性能，并且在一些比较难的任务中even outperform 现有的状态态-of-the-art 方法。此外，对比其他已有的 graph data augmentation 方法，LGA augmentation scheme 能够更好地捕捉 graph-level 的 semantics。<details>
<summary>Abstract</summary>
Graph representation learning has now become the de facto standard when handling graph-structured data, with the framework of message-passing graph neural networks (MPNN) being the most prevailing algorithmic tool. Despite its popularity, the family of MPNNs suffers from several drawbacks such as transparency and expressivity. Recently, the idea of designing neural models on graphs using the theory of graph kernels has emerged as a more transparent as well as sometimes more expressive alternative to MPNNs known as kernel graph neural networks (KGNNs). Developments on KGNNs are currently a nascent field of research, leaving several challenges from algorithmic design and adaptation to other learning paradigms such as self-supervised learning. In this paper, we improve the design and learning of KGNNs. Firstly, we extend the algorithmic formulation of KGNNs by allowing a more flexible graph-level similarity definition that encompasses former proposals like random walk graph kernel, as well as providing a smoother optimization objective that alleviates the need of introducing combinatorial learning procedures. Secondly, we enhance KGNNs through the lens of self-supervision via developing a novel structure-preserving graph data augmentation method called latent graph augmentation (LGA). Finally, we perform extensive empirical evaluations to demonstrate the efficacy of our proposed mechanisms. Experimental results over benchmark datasets suggest that our proposed model achieves competitive performance that is comparable to or sometimes outperforming state-of-the-art graph representation learning frameworks with or without self-supervision on graph classification tasks. Comparisons against other previously established graph data augmentation methods verify that the proposed LGA augmentation scheme captures better semantics of graph-level invariance.
</details>
<details>
<summary>摘要</summary>
Graph表示学习现在成为了处理图结构数据的标准方法，MPNN框架是最具有影响力的算法工具。然而，MPNN家族受到一些缺点的限制，如透明度和表达能力。近些年，基于图kernels的图神经网络（KGNN）在MPNN的基础上设计图神经网络，被认为是更透明和有时更表达能力的替代方案。KGNN的发展现在是一个有前途的研究领域，还有许多挑战，如算法设计和适应其他学习模式，如无监督学习。在这篇论文中，我们提高了KGNN的设计和学习。首先，我们扩展了KGNN的算法表述，允许更flexible的图级相似性定义，包括过去的提议，如随机步行图kernels，以及提供更平滑的优化目标，以避免引入 combinatorial学习过程。其次，我们通过对KGNN进行自我监督来增强其性能，发展了一种新的结构保持graph数据增强方法，即latent graph augmentation（LGA）。最后，我们进行了广泛的实验评估，以证明我们提出的机制的有效性。实验结果表明，我们的提出的模型在图分类任务上达到了与或超过了现状标准的表现，并且在不含自我监督的情况下也能够达到类似的表现。与其他之前Established graph data增强方法进行比较，我们的LGA增强方案更好地捕捉到图级 semantics。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Sample-Better"><a href="#Learning-to-Sample-Better" class="headerlink" title="Learning to Sample Better"></a>Learning to Sample Better</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11232">http://arxiv.org/abs/2310.11232</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Michael S. Albergo, Eric Vanden-Eijnden</li>
<li>for: 本文介绍了最近的生成模型方法，基于测量传输的动力学，从基本概率分布到目标概率分布的样本映射。</li>
<li>methods: 本文使用了变量学习来学习映射，并使用MC采样技术来提高采样效率。</li>
<li>results: 本文在MCMC和重要采样方面获得了改进的结果。<details>
<summary>Abstract</summary>
These lecture notes provide an introduction to recent advances in generative modeling methods based on the dynamical transportation of measures, by means of which samples from a simple base measure are mapped to samples from a target measure of interest. Special emphasis is put on the applications of these methods to Monte-Carlo (MC) sampling techniques, such as importance sampling and Markov Chain Monte-Carlo (MCMC) schemes. In this context, it is shown how the maps can be learned variationally using data generated by MC sampling, and how they can in turn be used to improve such sampling in a positive feedback loop.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这些讲义 introduce 最近的生成模型方法，基于动态传输度量，从简单的基础探障中映射到目标度量的样本。ocus 在 Monte Carlo (MC) 抽样技术上，如重要抽样和 Markov Chain Monte Carlo (MCMC) 方案。这些讲义显示了如何通过变量学习使得映射，使用 MC 抽样生成的数据，并将其用于改进抽样。
</details></li>
</ul>
<hr>
<h2 id="Zipformer-A-faster-and-better-encoder-for-automatic-speech-recognition"><a href="#Zipformer-A-faster-and-better-encoder-for-automatic-speech-recognition" class="headerlink" title="Zipformer: A faster and better encoder for automatic speech recognition"></a>Zipformer: A faster and better encoder for automatic speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11230">http://arxiv.org/abs/2310.11230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k2-fsa/icefall">https://github.com/k2-fsa/icefall</a></li>
<li>paper_authors: Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan Yang, Zengrui Jin, Long Lin, Daniel Povey</li>
<li>for: 这个论文是为了提出一种更快速、更具有内存效率的 transformer 模型，即 Zipformer，用于自动语音识别（ASR）。</li>
<li>methods: 该模型使用了以下方法：1）U-Net-like Encoder结构，中间堆叠在较低帧率下运行; 2）重新排序块结构，增加更多模块，并在每个模块中重用注意力权重以实现更好的效率; 3）修改了 LayerNorm 为 BiasNorm，以保留一些长度信息; 4）新的激活函数 SwooshR 和 SwooshL 比 Swish 更好。</li>
<li>results: 对 LibriSpeech、Aishell-1 和 WenetSpeech 数据集进行了广泛的实验，并证明了 Zipformer 模型在其他状态码模型中表现更好。<details>
<summary>Abstract</summary>
The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.
</details>
<details>
<summary>摘要</summary>
《充当者》已成为自动语音识别（ASR）最受欢迎的编码器模型。它将卷积模块添加到转换器中，以学习本地和全局依赖关系。在这项工作中，我们描述了一种更快、更有效和性能更高的转换器，即Zipformer。模型变化包括：1. 中堆结构采用U-Net类型，中间堆叠运行速率较低;2. 块结构重新排序，增加更多模块，并在这些模块中重用注意力权重以实现效率;3. 使用修改后的层Normalization，称为BiasNorm，以保留一些长度信息;4. 新的激活函数SwooshR和SwooshL，比Swish更好地工作;5. 我们还提出了一种新的优化器，称为扫描Adam，它可以根据每个tensor的当前尺度缩放更新，以保持相对变化的相同程度，并且显式地学习参数尺度。它在其他状态的ASR模型比Adam更快地 converges和表现更好。我们对LibriSpeech、Aishell-1和WenetSpeech datasets进行了广泛的实验，并证明了我们提出的Zipformer在其他状态的ASR模型之上表现更好。我们的代码可以在https://github.com/k2-fsa/icefall中找到。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-with-Nonvacuous-Generalisation-Bounds"><a href="#Federated-Learning-with-Nonvacuous-Generalisation-Bounds" class="headerlink" title="Federated Learning with Nonvacuous Generalisation Bounds"></a>Federated Learning with Nonvacuous Generalisation Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11203">http://arxiv.org/abs/2310.11203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Jobic, Maxime Haddouche, Benjamin Guedj</li>
<li>for: 本研究旨在采用隐私保护技术来实现 federated learning，每个节点都保持着自己的训练数据私有，而不会泄露给其他节点。</li>
<li>methods: 本研究使用随机生成器来训练 federated learning 模型，每个节点都生成了一个本地隐私predictor，而不把训练数据分享给其他节点。然后，我们建立了一个全局的随机生成器，该随机生成器继承了本地私有predictor的性质，即PAC-Bayesian泛化 bound。</li>
<li>results: 我们通过一系列的数字实验显示，我们的方法可以与批处理方法（其中所有数据集被共享）匹配的预测性能，而不需要共享数据集。此外，我们的方法可以提供 numerically nonvacuous 泛化 bound，保护每个节点的隐私。我们计算了批处理和 federated learning 之间的增量预测性能和泛化 bound，这将为保护隐私而付出的代价。<details>
<summary>Abstract</summary>
We introduce a novel strategy to train randomised predictors in federated learning, where each node of the network aims at preserving its privacy by releasing a local predictor but keeping secret its training dataset with respect to the other nodes. We then build a global randomised predictor which inherits the properties of the local private predictors in the sense of a PAC-Bayesian generalisation bound. We consider the synchronous case where all nodes share the same training objective (derived from a generalisation bound), and the asynchronous case where each node may have its own personalised training objective. We show through a series of numerical experiments that our approach achieves a comparable predictive performance to that of the batch approach where all datasets are shared across nodes. Moreover the predictors are supported by numerically nonvacuous generalisation bounds while preserving privacy for each node. We explicitly compute the increment on predictive performance and generalisation bounds between batch and federated settings, highlighting the price to pay to preserve privacy.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的策略，用于在联合学习中训练随机预测器，每个网络节点都希望保持自己的隐私，通过发布本地预测器，而不把训练数据集与其他节点分享。然后，我们构建了一个全局随机预测器，该预测器继承了本地私有预测器的性质，即PAC-Bayesian泛化约束。我们考虑了同步和异步两种情况，在同步情况下，所有节点共享同一个训练目标（基于一个泛化约束），在异步情况下，每个节点可能有自己的个性化训练目标。我们通过一系列数值实验表示，我们的方法可以与批处理方法（所有数据集在节点间共享）相比，具有相似的预测性能，同时保持隐私性。我们显式计算了批处理和联合学习之间的增量预测性能和泛化约束，强调保护隐私的代价。
</details></li>
</ul>
<hr>
<h2 id="A-Modified-EXP3-and-Its-Adaptive-Variant-in-Adversarial-Bandits-with-Multi-User-Delayed-Feedback"><a href="#A-Modified-EXP3-and-Its-Adaptive-Variant-in-Adversarial-Bandits-with-Multi-User-Delayed-Feedback" class="headerlink" title="A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback"></a>A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11188">http://arxiv.org/abs/2310.11188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chubbro/mud-exp3">https://github.com/chubbro/mud-exp3</a></li>
<li>paper_authors: Yandi Li, Jianxiong Guo</li>
<li>for: 本研究假设了延迟反馈问题中的多用户情况，即每个用户的反馈可能会在不同的延迟时间内提供，而这些延迟时间都是未知的。</li>
<li>methods: 我们采用了修改后EXP3算法，称之为MUD-EXP3算法，它在每个轮次中基于不同用户的重要性权重来做决策。</li>
<li>results: 我们证明了在知道终点轮次索引$T$的情况下，我们的算法的违和为$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$。此外，我们还提出了一种适应算法 named AMUD-EXP3，它在不知道$T$的情况下可以实现SUBLINEAR的违和。最后，我们进行了广泛的实验来证明我们的算法的正确性和有效性。<details>
<summary>Abstract</summary>
For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms.
</details>
<details>
<summary>摘要</summary>
For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms.Here is the translation in Traditional Chinese:For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algorithm named AMUD-EXP3 is proposed with a sublinear regret with respect to $T$. Finally, extensive experiments are conducted to indicate the correctness and effectiveness of our algorithms.
</details></li>
</ul>
<hr>
<h2 id="Efficiently-Visualizing-Large-Graphs"><a href="#Efficiently-Visualizing-Large-Graphs" class="headerlink" title="Efficiently Visualizing Large Graphs"></a>Efficiently Visualizing Large Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11186">http://arxiv.org/abs/2310.11186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/charlie-xiao/embedding-visualization-test">https://github.com/charlie-xiao/embedding-visualization-test</a></li>
<li>paper_authors: Xinyu Li, Yao Xiao, Yuchen Zhou</li>
<li>for: 这个论文旨在提出一种基于维度减少的图像化方法，用于可读性地显示图的结构。</li>
<li>methods: 该方法基于t-SNE算法，但是它采用了邻域结构来降低时间复杂度，从而支持更大的图。此外，该方法还结合了laplacian eigenmaps和最短路算法，以获得高维度的图嵌入。</li>
<li>results: 通过使用这种方法，可以在5分钟内图像化300K个节点和1M个边的图，并且可以达到约10%的视觉质量提升。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/Charlie-XIAO/embedding-visualization-test%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Charlie-XIAO/embedding-visualization-test上获取。</a><details>
<summary>Abstract</summary>
Most existing graph visualization methods based on dimension reduction are limited to relatively small graphs due to performance issues. In this work, we propose a novel dimension reduction method for graph visualization, called t-Distributed Stochastic Graph Neighbor Embedding (t-SGNE). t-SGNE is specifically designed to visualize cluster structures in the graph. As a variant of the standard t-SNE method, t-SGNE avoids the time-consuming computations of pairwise similarity. Instead, it uses the neighbor structures of the graph to reduce the time complexity from quadratic to linear, thus supporting larger graphs. In addition, to suit t-SGNE, we combined Laplacian Eigenmaps with the shortest path algorithm in graphs to form the graph embedding algorithm ShortestPath Laplacian Eigenmaps Embedding (SPLEE). Performing SPLEE to obtain a high-dimensional embedding of the large-scale graph and then using t-SGNE to reduce its dimension for visualization, we are able to visualize graphs with up to 300K nodes and 1M edges within 5 minutes and achieve approximately 10% improvement in visualization quality. Codes and data are available at https://github.com/Charlie-XIAO/embedding-visualization-test.
</details>
<details>
<summary>摘要</summary>
现有的图视化方法基于维度减少通常只能处理相对较小的图，由于性能问题。在这个工作中，我们提出了一种新的维度减少方法 для图视化，即t-Distributed Stochastic Graph Neighbor Embedding（t-SGNE）。t-SGNE专门用于描述图中的集群结构。作为标准t-SNE方法的变体，t-SGNE避免了对对应之间的相似性进行时间消耗的计算，而是使用图中的邻居结构来降低时间复杂度从quadratico至线性，因此可以支持更大的图。此外，为了适应t-SGNE，我们将Laplacian Eigenmaps与图中最短路算法组合成为图嵌入算法ShortestPath Laplacian Eigenmaps Embedding（SPLEE）。通过对大规模图进行SPLEE嵌入，并使用t-SGNE减少其维度进行视化，我们可以在5分钟内视化300K个节点和1M个边的图，并达到约10%的视化质量提升。代码和数据可以在https://github.com/Charlie-XIAO/embedding-visualization-test中找到。
</details></li>
</ul>
<hr>
<h2 id="Serenade-A-Model-for-Human-in-the-loop-Automatic-Chord-Estimation"><a href="#Serenade-A-Model-for-Human-in-the-loop-Automatic-Chord-Estimation" class="headerlink" title="Serenade: A Model for Human-in-the-loop Automatic Chord Estimation"></a>Serenade: A Model for Human-in-the-loop Automatic Chord Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11165">http://arxiv.org/abs/2310.11165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Vincent Koops, Gianluca Micchi, Ilaria Manco, Elio Quinton</li>
<li>for: 这 paper 的目的是提高 Musical Instrument Digital Interface (MIDI) 任务中的自动分 segmentation、 corpus analysis 和自动和声标注 estimation 的精度。</li>
<li>methods: 这 paper 使用了一种新的人类和自动推理模型共同创建和声标注的方法，其中人类在自动生成和声预测时提供精度优化的稀疏标注，而模型则根据人类指导进行修正。</li>
<li>results: 这 paper 在一个流行音乐数据集上进行了评估，并显示了人类和模型共同创建和声标注的方法可以提高和声分析性能，并且人类的贡献被模型的第二次、受限预测所强调。<details>
<summary>Abstract</summary>
Computational harmony analysis is important for MIR tasks such as automatic segmentation, corpus analysis and automatic chord label estimation. However, recent research into the ambiguous nature of musical harmony, causing limited inter-rater agreement, has made apparent that there is a glass ceiling for common metrics such as accuracy. Commonly, these issues are addressed either in the training data itself by creating majority-rule annotations or during the training phase by learning soft targets. We propose a novel alternative approach in which a human and an autoregressive model together co-create a harmonic annotation for an audio track. After automatically generating harmony predictions, a human sparsely annotates parts with low model confidence and the model then adjusts its predictions following human guidance. We evaluate our model on a dataset of popular music and we show that, with this human-in-the-loop approach, harmonic analysis performance improves over a model-only approach. The human contribution is amplified by the second, constrained prediction of the model.
</details>
<details>
<summary>摘要</summary>
计算音乐和谐分析对音乐信息 Retrieval（MIR）任务如自动分割、文献分析和自动和声标注有着重要的作用。然而，最近关于音乐和谐的抽象性的研究，使得限制了通用指标的准确率。通常，这些问题通过创建多数规则约束或在训练阶段学习软目标来解决。我们提出了一种新的人机合作方法，在音频轨道上，人类和自动推理模型共同创建和谐标注。首先，模型自动生成和谐预测，然后人类精选部分低度信任的部分并让模型根据人类指导更新预测。我们对流行音乐数据集进行评估，并显示了人机共同Loop Approach可以提高和谐分析性能，人类贡献被模型第二次预测所增强。
</details></li>
</ul>
<hr>
<h2 id="A-new-high-resolution-indoor-radon-map-for-Germany-using-a-machine-learning-based-probabilistic-exposure-model"><a href="#A-new-high-resolution-indoor-radon-map-for-Germany-using-a-machine-learning-based-probabilistic-exposure-model" class="headerlink" title="A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model"></a>A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11143">http://arxiv.org/abs/2310.11143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Petermann, Peter Bossew, Joachim Kemski, Valeria Gruber, Nils Suhr, Bernd Hoffmann</li>
<li>for: 这个研究是为了更正确地估计室内Radon浓度分布，并且实现高空间分辨率的推估。</li>
<li>methods: 这篇研究使用了一个两阶段的模型方法，包括一个量iles regression forest以环境和建筑资料作为预测器，以及一个可靠的Monte Carlo抽样法来组合和人口权重平均。</li>
<li>results: 研究结果显示，室内Radon浓度的算术平均值为63 Bq&#x2F;m3，几何平均值为41 Bq&#x2F;m3，95 %ile值为180 Bq&#x2F;m3。对100 Bq&#x2F;m3和300 Bq&#x2F;m3的过衡 probabilities是12.5 % (10.5 million people)和2.2 % (1.9 million people)，分别。在大城市地区，个人室内Radon曝露较在乡村地区低，这是因为人口分布不同。<details>
<summary>Abstract</summary>
Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and population weighting of floor-level predictions. In this way, the uncertainty of the individual predictions is effectively propagated into the estimate of variability at the aggregated level. The results give an arithmetic mean of 63 Bq/m3, a geometric mean of 41 Bq/m3 and a 95 %ile of 180 Bq/m3. The exceedance probability for 100 Bq/m3 and 300 Bq/m3 are 12.5 % (10.5 million people) and 2.2 % (1.9 million people), respectively. In large cities, individual indoor radon exposure is generally lower than in rural areas, which is a due to the different distribution of the population on floor levels. The advantages of our approach are 1) an accurate exposure estimation even if the survey was not fully representative with respect to the main controlling factors, and 2) an estimate of the exposure distribution with a much higher spatial resolution than basic descriptive statistics.
</details>
<details>
<summary>摘要</summary>
气体氧化物Radon是一种致癌的放射性气体，可以在室内堆积。室内Radon暴露的国家规模通常通过广泛的测量运动来估算。然而，样本特点与人口特点之间存在许多相关因素，如地源Radon的可用性和地板层。此外，样本大小通常无法实现高空间分辨率的暴露估计。我们提出了一种基于模型的方法，可以更加准确地估计室内Radon分布，并提高空间分辨率。我们采用了两个阶段的模型方法：1. 使用缺陷回归森林来估计室内Radon的分布函数，使用环境和建筑数据作为预测器。2. 使用 probabilistic Monte Carlo sampling technique来组合和人口权重 floor-level 预测。这样，个体预测的uncertainty会被有效地传递到聚合水平的估计中。结果表明， arithmetic mean 为 63 Bq/m3， geometric mean 为 41 Bq/m3，和 95%ile 为 180 Bq/m3。100 Bq/m3和300 Bq/m3的超过 probabilities 分别为 12.5% (10.5 million people) 和 2.2% (1.9 million people)。在大城市地区，室内Radon暴露通常比农村地区低，这是因为人口分布不同。我们的方法的优点包括：1. 即使调查不具有完全反映主要控制因素的 representativeness，也可以准确地估计暴露水平。2. 可以提供高空间分辨率的暴露估计，比基本描述统计数据更加精准。
</details></li>
</ul>
<hr>
<h2 id="Keep-Various-Trajectories-Promoting-Exploration-of-Ensemble-Policies-in-Continuous-Control"><a href="#Keep-Various-Trajectories-Promoting-Exploration-of-Ensemble-Policies-in-Continuous-Control" class="headerlink" title="Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control"></a>Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11138">http://arxiv.org/abs/2310.11138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Li, Chen Gong, Qiang He, Xinwen Hou</li>
<li>for: 这个研究的目的是强化深度强化学习（DRL） ensemble 方法的 empirical 成功，并且提高多模型的统计价值估计和政策的稳定性。</li>
<li>methods: 本研究使用了 Trajectories-awarE Ensemble exploratioN (TEEN) 算法，它的目的是增加多条 trajectory 的丰富性，以提高 ensemble 政策的表现。</li>
<li>results: 实验结果显示，TEEN 可以提高 ensemble 政策的表现，比基eline ensemble DRL 算法高出41%。<details>
<summary>Abstract</summary>
The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \textbf{T}rajectories-awar\textbf{E} \textbf{E}nsemble exploratio\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble policy compared to using sub-policies alone but also improves the performance over ensemble RL algorithms. On average, TEEN outperforms the baseline ensemble DRL algorithms by 41\% in performance on the tested representative environments.
</details>
<details>
<summary>摘要</summary>
深度强化学习（DRL）和集成方法的组合在复杂的顺序决策问题上表现出非常高效。这种成功可以归功于多个模型的使用，增强政策的稳健性和估值函数的准确性。然而，现有的集成RL算法的实证成功仍然受限。我们的新分析发现，前一些集成DRL算法的样本效率可能受到较差的优化策略的限制。驱动于这些发现，我们的研究提出了一种新的集成RL算法，名为天文道-探索-集成探索（TEEN）。TEEN的主要目标是 Maximize the expected return while promoting more diverse trajectories。我们通过广泛的实验表明，TEEN不仅提高了集成政策的样本多样性，还超过了基eline集成DRL算法的性能。在测试环境中，TEEN平均比基eline算法提高41%的性能。
</details></li>
</ul>
<hr>
<h2 id="Non-parametric-Conditional-Independence-Testing-for-Mixed-Continuous-Categorical-Variables-A-Novel-Method-and-Numerical-Evaluation"><a href="#Non-parametric-Conditional-Independence-Testing-for-Mixed-Continuous-Categorical-Variables-A-Novel-Method-and-Numerical-Evaluation" class="headerlink" title="Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation"></a>Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11132">http://arxiv.org/abs/2310.11132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana-Iuliana Popescu, Andreas Gerhardus, Jakob Runge</li>
<li>for: 这篇研究是关于 conditional independence testing (CIT) 的 mixed-type dataset 的应用。</li>
<li>methods: 这篇研究使用了 conditional mutual information (CMI) 估计器，与 local permutation scheme，并比较了两种新的 CMI 估计器：一种是基于 k-nearest-neighbors (k-NN) 方法，另一种是基于 entropy 度量。</li>
<li>results: 研究发现，该 variants 可以更好地检测依赖性，并且可以适应不同的数据分布和预processing 类型。<details>
<summary>Abstract</summary>
Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these estimators and propose a variation of the former approach that does not treat categorical variables as numeric. Our numerical experiments show that our variant detects dependencies more robustly across different data distributions and preprocessing types.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> Conditional independence testing (CIT) 是机器学习中常见的任务之一，例如变量选择，并是约束基于 causal discovery 的主要组成部分。而现实中的大多数 CIT 方法假设所有变量都是数值型或所有变量都是类别型，但是实际应用中经常会出现混合类型的数据集。非 Parametric CIT 可以通过 conditional mutual information (CMI) 估计器和地方排序方案来进行。最近，两种新的 CMI 估计器 для混合类型数据集基于 k-nearest-neighbors (k-NN) 已经被提出。这些估计器都取决于距离度量的定义。一种方法通过一个一键编码的 categorical 变量来计算距离，实际上将 categorical 变量当作数值型处理，而另一种方法通过 entropy 表达来计算 CMI，在 categorical 变量中只有作为条件出现。在这项工作中，我们研究这些估计器，并提出一种不对 categorical 变量进行数值化的变体。我们的数值实验表明，我们的变体在不同的数据分布和预处理类型下能够更加稳定地检测依赖关系。
</details></li>
</ul>
<hr>
<h2 id="FROST-Towards-Energy-efficient-AI-on-5G-Platforms-–-A-GPU-Power-Capping-Evaluation"><a href="#FROST-Towards-Energy-efficient-AI-on-5G-Platforms-–-A-GPU-Power-Capping-Evaluation" class="headerlink" title="FROST: Towards Energy-efficient AI-on-5G Platforms – A GPU Power Capping Evaluation"></a>FROST: Towards Energy-efficient AI-on-5G Platforms – A GPU Power Capping Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11131">http://arxiv.org/abs/2310.11131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Mavromatis, Stefano De Feo, Pietro Carnelli, Robert J. Piechocki, Aftab Khan</li>
<li>for: 该论文targets the Open Radio Access Network (O-RAN) market, which is expected to experience significant growth in the coming years. The paper aims to optimize the energy consumption of Machine Learning (ML) pipelines in O-RAN ecosystems.</li>
<li>methods: 该论文提出了一种名为FROST的解决方案，即Flexible Reconfiguration method with Online System Tuning。FROST可以 Profiling the energy consumption of an ML pipeline and optimizing the hardware accordingly, thereby limiting the power draw.</li>
<li>results: 根据该论文的发现，FROST可以实现energy savings of up to 26.4% without compromising the model’s accuracy or introducing significant time delays.<details>
<summary>Abstract</summary>
The Open Radio Access Network (O-RAN) is a burgeoning market with projected growth in the upcoming years. RAN has the highest CAPEX impact on the network and, most importantly, consumes 73% of its total energy. That makes it an ideal target for optimisation through the integration of Machine Learning (ML). However, the energy consumption of ML is frequently overlooked in such ecosystems. Our work addresses this critical aspect by presenting FROST - Flexible Reconfiguration method with Online System Tuning - a solution for energy-aware ML pipelines that adhere to O-RAN's specifications and principles. FROST is capable of profiling the energy consumption of an ML pipeline and optimising the hardware accordingly, thereby limiting the power draw. Our findings indicate that FROST can achieve energy savings of up to 26.4% without compromising the model's accuracy or introducing significant time delays.
</details>
<details>
<summary>摘要</summary>
openRadio Access Network (O-RAN) 是一个快速发展的市场，未来几年将出现快速增长。RAN 是网络总体的最高CapEx 成本和73% 的能源消耗，因此它成为了优化的目标。然而，机器学习 (ML) 在这些生态系统中的能源消耗frequently 被忽略。我们的工作解决了这个关键问题，提出了FROST - 可变化的重配置方法与在线系统调整 - 一种遵循 O-RAN 规范和原则的能源意识机器学习管道解决方案。FROST 可以对机器学习管道的能源消耗进行 profiling，并根据硬件进行优化，从而限制能源浪费。我们的发现表明，FROST 可以实现能源节约达到 26.4% 而不会 compromise 模型准确性或引入显著的时间延迟。
</details></li>
</ul>
<hr>
<h2 id="Topological-Expressivity-of-ReLU-Neural-Networks"><a href="#Topological-Expressivity-of-ReLU-Neural-Networks" class="headerlink" title="Topological Expressivity of ReLU Neural Networks"></a>Topological Expressivity of ReLU Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11130">http://arxiv.org/abs/2310.11130</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekin Ergen, Moritz Grillo</li>
<li>for: 本研究探讨了ReLU神经网络在二分类问题中的表达能力，从拓扑角度来看。</li>
<li>methods: 本研究使用了Betti数来度量神经网络对数据集的拓扑简化程度。</li>
<li>results: 研究结果表明，深度的ReLU神经网络在二分类问题中的表达能力比浅度的神经网络更强，具体来说是 exponentially more powerful。这提供了一个数学上的正式解释，为什么深度的神经网络能够更好地处理复杂和拓扑 ric 的数据集。<details>
<summary>Abstract</summary>
We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplification. This provides a mathematically rigorous explanation why deeper networks are better equipped to handle complex and topologically rich datasets.
</details>
<details>
<summary>摘要</summary>
我们研究使用ReLU神经网络进行二分类问题的表达能力，从拓扑角度来看。最近的实验证明，神经网络在传递层次时会改变拓扑结构，将复杂的拓扑数据集转化为简单的拓扑结构。这种拓扑简化的度量使用Betti数，它是一种拓扑空间的代数 invariants。我们使用这个度量来确定ReLU神经网络的拓扑简化能力，并提出了对于给定架构的下限和上限。因此，我们对于二分类问题中ReLU神经网络的表达能力做出了更深入的理解，特别是结果表明深度的ReLU神经网络在拓扑简化方面的表达能力是极大的，这提供了一种数学上的正式解释，为复杂和拓扑 ric的数据集进行处理，为何更深度的网络更好。
</details></li>
</ul>
<hr>
<h2 id="On-the-Temperature-of-Bayesian-Graph-Neural-Networks-for-Conformal-Prediction"><a href="#On-the-Temperature-of-Bayesian-Graph-Neural-Networks-for-Conformal-Prediction" class="headerlink" title="On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction"></a>On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11479">http://arxiv.org/abs/2310.11479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seohyeon Cha, Honggu Kang, Joonhyuk Kang</li>
<li>for: 提高Graph Neural Networks（GNNs）中的不确定性评估，尤其在高风险领域where GNNs frequently employed。</li>
<li>methods: 使用Conformal Prediction（CP）框架，提供有效的预测集， garantizing formal probabilistic guarantees that a prediction set contains a true label with a desired probability。</li>
<li>results: 实验表明，可以通过设置温度参数，使得预测集更加有效率。此外，我们还进行了一种分析，以便了解CP性能和模型准确性之间的关系。<details>
<summary>Abstract</summary>
Accurate uncertainty quantification in graph neural networks (GNNs) is essential, especially in high-stakes domains where GNNs are frequently employed. Conformal prediction (CP) offers a promising framework for quantifying uncertainty by providing $\textit{valid}$ prediction sets for any black-box model. CP ensures formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as $\textit{inefficiency}$, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning also provides a credible region based on the estimated posterior distribution, but this region is $\textit{well-calibrated}$ only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration.
</details>
<details>
<summary>摘要</summary>
precisions of uncertainty quantification in graph neural networks (GNNs) is crucial, especially in high-stakes domains where GNNs are widely used. Conformal prediction (CP) provides a promising framework for uncertainty quantification by offering valid prediction sets for any black-box model. CP guarantees formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as inefficiency, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning provides a credible region based on the estimated posterior distribution, but this region is well-calibrated only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration.Here's the translation in Traditional Chinese: precisions of uncertainty quantification in graph neural networks (GNNs) is crucial, especially in high-stakes domains where GNNs are widely used. Conformal prediction (CP) provides a promising framework for uncertainty quantification by offering valid prediction sets for any black-box model. CP guarantees formal probabilistic guarantees that a prediction set contains a true label with a desired probability. However, the size of prediction sets, known as inefficiency, is influenced by the underlying model and data generating process. On the other hand, Bayesian learning provides a credible region based on the estimated posterior distribution, but this region is well-calibrated only when the model is correctly specified. Building on a recent work that introduced a scaling parameter for constructing valid credible regions from posterior estimate, our study explores the advantages of incorporating a temperature parameter into Bayesian GNNs within CP framework. We empirically demonstrate the existence of temperatures that result in more efficient prediction sets. Furthermore, we conduct an analysis to identify the factors contributing to inefficiency and offer valuable insights into the relationship between CP performance and model calibration.
</details></li>
</ul>
<hr>
<h2 id="Sensitivity-Aware-Amortized-Bayesian-Inference"><a href="#Sensitivity-Aware-Amortized-Bayesian-Inference" class="headerlink" title="Sensitivity-Aware Amortized Bayesian Inference"></a>Sensitivity-Aware Amortized Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11122">http://arxiv.org/abs/2310.11122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lasse Elsemüller, Hans Olischläger, Marvin Schmitt, Paul-Christian Bürkner, Ullrich Köthe, Stefan T. Radev</li>
<li>for: 这篇论文的目的是提出一种可以在不同假设下进行 Bayesian 推理的方法，以便更好地了解不同假设下的结果之间的关系。</li>
<li>methods: 这篇论文使用了 neural network 来实现 simulation-based inference，并利用 weight sharing 技术来编码结构相似性。</li>
<li>results: 该方法可以快速地评估不同假设下的结果之间的敏感性，并且可以使用 neural network  ensemble 来评估模型的变化。<details>
<summary>Abstract</summary>
Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly bottleneck of refitting the model(s) for each choice of likelihood, prior, or dataset. Finally, we propose to use neural network ensembles to evaluate variation in results induced by unreliable approximation on unseen data. We demonstrate the effectiveness of our method in applied modeling problems, ranging from the estimation of disease outbreak dynamics and global warming thresholds to the comparison of human decision-making models. Our experiments showcase how our approach enables practitioners to effectively unveil hidden relationships between modeling choices and inferential conclusions.
</details>
<details>
<summary>摘要</summary>
泛bayesian推理是一种强大的推理框架，用于在不纯的情况下做出推理和决策。现代泛bayesian工作流程中的基本选择包括可信度函数和先验分布的规定、 posterior approximator 和数据。每一个选择都会对模型基于推理和后续决策产生重要影响，因此需要敏感分析。在这种工作中，我们提议一种多方面的方法，将敏感分析integrated into amortized Bayesian inference (ABI，即通过神经网络进行 simulations-based inference)。首先，我们利用 weight sharing 将结构相似性编码到替代可信度函数和先验分布中的训练过程中，以 minimize computational overhead。其次，我们利用神经网络的快速推理来评估数据变化或预处理过程对结果的敏感性。与大多数泛bayesian方法不同，这两个步骤都可以避免对模型的重新适应过程中的成本。最后，我们提议使用神经网络集合来评估未知数据上的结果变化。我们在应用模型问题中进行了实验，从疾病爆发动力和全球暖化阈值估计到人类决策模型的比较。我们的实验显示了我们的方法可以帮助实践者更好地揭示模型选择和推理结论之间的隐藏关系。
</details></li>
</ul>
<hr>
<h2 id="Minimally-Informed-Linear-Discriminant-Analysis-training-an-LDA-model-with-unlabelled-data"><a href="#Minimally-Informed-Linear-Discriminant-Analysis-training-an-LDA-model-with-unlabelled-data" class="headerlink" title="Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data"></a>Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11110">http://arxiv.org/abs/2310.11110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Heintz, Tom Francart, Alexander Bertrand</li>
<li>for: 这 paper 是用来解释如何使用 Linear Discriminant Analysis (LDA) 算法来解决无标签数据的分类问题。</li>
<li>methods: 这 paper 使用了一种名为 Minimally Informed Linear Discriminant Analysis (MILDA) 模型，该模型可以在没有标签数据的情况下计算出 LDA 投影向量，只需要一些最小的先验信息。</li>
<li>results: 该 paper 的实验结果表明，MILDA 模型可以准确地模型分类问题，并且可以快速适应非站ARY 数据，这使得它成为一个可靠的 adaptive classifier。<details>
<summary>Abstract</summary>
Linear Discriminant Analysis (LDA) is one of the oldest and most popular linear methods for supervised classification problems. In this paper, we demonstrate that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. More precisely, we show that only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available: (1) the class average of one of the two classes, (2) the difference between both class averages (up to a scaling), or (3) the class covariance matrices (up to a scaling). These theoretical results are validated in numerical experiments, demonstrating that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model. Furthermore, we show that the MILDA projection vector can be computed in a closed form with a computational cost comparable to LDA and is able to quickly adapt to non-stationary data, making it well-suited to use as an adaptive classifier.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>一个类型的类均值（或一个类型的平均值）2. 两个类型之间的差异（几乎可以忽略扩大）3. 两个类型的类 covariance 矩阵（几乎可以忽略扩大）这些理论结果在数学实验中得到了验证，表明了这种只需要最小先验信息的 Linear Discriminant Analysis (MILDA) 模型能够与指导类型的 LDA 模型准确匹配。此外，我们还证明了 MILDA 投影向量可以在关闭形式下计算，计算成本与 LDA 相当，能够快速适应非站点数据，使其成为一种适用于适应类型的批处理器。</details></li>
</ol>
<hr>
<h2 id="Local-Lipschitz-Constant-Computation-of-ReLU-FNNs-Upper-Bound-Computation-with-Exactness-Verification"><a href="#Local-Lipschitz-Constant-Computation-of-ReLU-FNNs-Upper-Bound-Computation-with-Exactness-Verification" class="headerlink" title="Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification"></a>Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11104">http://arxiv.org/abs/2310.11104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshio Ebihara, Xin Dai, Victor Magron, Dimitri Peaucelle, Sophie Tarbouriech</li>
<li>for: 这 paper 关注计算Feedforward Neural Networks (FNNs) 的地方 Lipschitz 常数，其中 activation functions 是 Rectified Linear Units (ReLUs)。地方 Lipschitz 常数 是一个 reasonable measure  для FNNs 的评估量。</li>
<li>methods: 我们首先将 upper bound 计算问题转化为一个 semidefinite programming problem (SDP)，然后引入新的 copositive multipliers 来准确地捕捉 ReLU 的行为。然后，我们通过对 SDP 的 dual 进行分析，提出一种可靠的检验方法来验证 computed upper bound 的准确性。</li>
<li>results: 我们通过数学实验来证明方法的有效性，并通过实验示例来验证方法在实际 FNNs 上的可行性。<details>
<summary>Abstract</summary>
This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original FNN over a neighborhood of the target input. We finally illustrate the effectiveness of the model reduction and exactness verification methods with numerical examples of practical FNNs.
</details>
<details>
<summary>摘要</summary>
We first convert the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP) using multipliers that capture the ReLU behavior. To improve the accuracy of the computation, we introduce new copositive multipliers.Next, we derive a feasibility test for the computed upper bound by considering the dual of the SDP. However, these SDPs are computationally intractable for practical FNNs with hundreds of ReLUs.To address this issue, we propose a method to construct a reduced order model whose input-output property is identical to the original FNN over a neighborhood of the target input. We demonstrate the effectiveness of the model reduction and exactness verification methods with numerical examples of practical FNNs.
</details></li>
</ul>
<hr>
<h2 id="Sparse-DySta-Sparsity-Aware-Dynamic-and-Static-Scheduling-for-Sparse-Multi-DNN-Workloads"><a href="#Sparse-DySta-Sparsity-Aware-Dynamic-and-Static-Scheduling-for-Sparse-Multi-DNN-Workloads" class="headerlink" title="Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads"></a>Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11096">http://arxiv.org/abs/2310.11096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/samsunglabs/sparse-multi-dnn-scheduling">https://github.com/samsunglabs/sparse-multi-dnn-scheduling</a></li>
<li>paper_authors: Hongxiang Fan, Stylianos I. Venieris, Alexandros Kouris, Nicholas D. Lane</li>
<li>for: 本研究旨在探讨多个稀疏深度神经网络（DNN）在 Edge 设备和数据中心之间的平衡执行。</li>
<li>methods: 本文使用了多种简化 Approaches，包括静态稀疏模型和动态稀疏信息，以提高稀疏多DNN 调度。</li>
<li>results: 对于多种部署场景，本文提出了一种新的双级静态和动态调度策略，并通过实验证明其与状态艺术方法相比，可以降低对响应时间的依赖性，并提高平均 норма化响应时间。<details>
<summary>Abstract</summary>
Running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyses the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling. Both static and dynamic components of Dysta are jointly designed at the software and hardware levels, respectively, to improve and refine the scheduling approach. To facilitate future progress in the study of this class of workloads, we construct a public benchmark that contains sparse multi-DNN workloads across different deployment scenarios, spanning from mobile phones and AR/VR wearables to data centers. A comprehensive evaluation on the sparse multi-DNN benchmark demonstrates that our proposed approach outperforms the state-of-the-art methods with up to 10% decrease in latency constraint violation rate and nearly 4X reduction in average normalized turnaround time. Our artifacts and code are publicly available at: https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.
</details>
<details>
<summary>摘要</summary>
running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyzes the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling. Both static and dynamic components of Dysta are jointly designed at the software and hardware levels, respectively, to improve and refine the scheduling approach. To facilitate future progress in the study of this class of workloads, we construct a public benchmark that contains sparse multi-DNN workloads across different deployment scenarios, spanning from mobile phones and AR/VR wearables to data centers. A comprehensive evaluation on the sparse multi-DNN benchmark demonstrates that our proposed approach outperforms the state-of-the-art methods with up to 10% decrease in latency constraint violation rate and nearly 4X reduction in average normalized turnaround time. Our artifacts and code are publicly available at: https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.
</details></li>
</ul>
<hr>
<h2 id="Relearning-Forgotten-Knowledge-on-Forgetting-Overfit-and-Training-Free-Ensembles-of-DNNs"><a href="#Relearning-Forgotten-Knowledge-on-Forgetting-Overfit-and-Training-Free-Ensembles-of-DNNs" class="headerlink" title="Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs"></a>Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11094">http://arxiv.org/abs/2310.11094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uri Stern, Daphna Weinshall</li>
<li>for: 这个论文的目的是解释深度神经网络中的过拟合现象，并提出一种新的评估过拟合方法。</li>
<li>methods: 这篇论文使用了一种新的评估过拟合方法，它基于 validate 数据集中模型忘记率的评估。</li>
<li>results: 该论文的实验结果表明，过拟合可以在模型训练过程中发生，并且可能更为常见 than 先前认为的。此外，该论文还提出了一种基于单个网络训练历史的新ensemble方法，该方法可以提高性能而无需额外的训练时间成本。<details>
<summary>Abstract</summary>
The infrequent occurrence of overfit in deep neural networks is perplexing. On the one hand, theory predicts that as models get larger they should eventually become too specialized for a specific training set, with ensuing decrease in generalization. In contrast, empirical results in image classification indicate that increasing the training time of deep models or using bigger models almost never hurts generalization. Is it because the way we measure overfit is too limited? Here, we introduce a novel score for quantifying overfit, which monitors the forgetting rate of deep models on validation data. Presumably, this score indicates that even while generalization improves overall, there are certain regions of the data space where it deteriorates. When thus measured, we show that overfit can occur with and without a decrease in validation accuracy, and may be more common than previously appreciated. This observation may help to clarify the aforementioned confusing picture. We use our observations to construct a new ensemble method, based solely on the training history of a single network, which provides significant improvement in performance without any additional cost in training time. An extensive empirical evaluation with modern deep models shows our method's utility on multiple datasets, neural networks architectures and training schemes, both when training from scratch and when using pre-trained networks in transfer learning. Notably, our method outperforms comparable methods while being easier to implement and use, and further improves the performance of competitive networks on Imagenet by 1\%.
</details>
<details>
<summary>摘要</summary>
启发性训练深度神经网络中偶尔出现过拟合现象很困惑。一面理论预测，随着模型的大小增加，它们应该逐渐变得特化于具体的训练集，导致泛化性下降。然而，实际研究发现，深度模型的训练时间增加或使用更大的模型在图像分类任务中并没有明显的泛化性下降。我们是否因为量化过拟合的方法有限而导致这种情况呢？在这里，我们引入一种新的过拟合评价指标，可以监测深度模型在验证数据上忘记率。这个指标表明，即使总的泛化性提高，仍然有一些数据空间中的忘记率下降。当如此量化过拟合时，我们发现过拟合可以发生在Validation accuracy下降和不下降的情况下，并且可能更为常见。这一观察可能有助于解释深度神经网络中的困惑场景。我们利用这些观察，构建了一种基于单个网络训练历史的新集成方法，可以在不添加训练时间成本的情况下提供显著性能提升。我们的方法在现代深度模型、不同的 neural network 架构、训练方案和传输学习中都有广泛的实际评估，并且在Imagenet上提高了1%的性能。另外，我们的方法比同类方法更容易实现和使用，并且可以进一步提高竞争力强的网络的性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Drift-Monitoring-for-Log-Anomaly-Detection-Pipelines"><a href="#Data-Drift-Monitoring-for-Log-Anomaly-Detection-Pipelines" class="headerlink" title="Data Drift Monitoring for Log Anomaly Detection Pipelines"></a>Data Drift Monitoring for Log Anomaly Detection Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.14893">http://arxiv.org/abs/2310.14893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dipak Wani, Samuel Ackerman, Eitan Farchi, Xiaotong Liu, Hau-wen Chang, Sarasi Lalithsena</li>
<li>for: 本文主要用于提出了一种基于 bayes factor 的偏移检测方法，用于检测日志活动偏移，并且可以帮助系统可靠工程师（SRE）在系统诊断中获得协助。</li>
<li>methods: 本文使用了 Bayes Factor 来检测日志活动偏移，并且提出了一种基于人工干预的方法来更新 LAD 模型。</li>
<li>results: 本文通过使用真实收集的日志数据和 simulate 的活动序列来证明了该方法的可靠性和有效性。<details>
<summary>Abstract</summary>
Logs enable the monitoring of infrastructure status and the performance of associated applications. Logs are also invaluable for diagnosing the root causes of any problems that may arise. Log Anomaly Detection (LAD) pipelines automate the detection of anomalies in logs, providing assistance to site reliability engineers (SREs) in system diagnosis. Log patterns change over time, necessitating updates to the LAD model defining the `normal' log activity profile. In this paper, we introduce a Bayes Factor-based drift detection method that identifies when intervention, retraining, and updating of the LAD model are required with human involvement. We illustrate our method using sequences of log activity, both from unaltered data, and simulated activity with controlled levels of anomaly contamination, based on real collected log data.
</details>
<details>
<summary>摘要</summary>
日志可以监控基础设施状态和相关应用程序的性能。日志也是解决问题的根本原因的诊断的重要工具。日志异常检测（LAD）管道自动检测日志中的异常情况，为站点可靠工程师（SRE）提供帮助。日志模式随着时间的变化，因此LAD模型需要定期更新。在这篇文章中，我们介绍了基于 bayes 因子的漂移检测方法，可以在人工参与下确定是否需要介入、重新训练和更新 LAD 模型。我们使用了日志活动序列，包括未修改数据和 simulated 活动，以及控制了异常污染的水平，基于实际收集的日志数据来示例我们的方法。
</details></li>
</ul>
<hr>
<h2 id="CSG-Curriculum-Representation-Learning-for-Signed-Graph"><a href="#CSG-Curriculum-Representation-Learning-for-Signed-Graph" class="headerlink" title="CSG: Curriculum Representation Learning for Signed Graph"></a>CSG: Curriculum Representation Learning for Signed Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11083">http://arxiv.org/abs/2310.11083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Zhang, Jiamou Liu, Kaiqi Zhao, Yifei Wang, Pengqian Han, Xianda Zheng, Qiqi Wang, Zijian Zhang</li>
<li>for: 本研究旨在提高Signed Graph Neural Networks（SGNNs）的精度和稳定性，通过设计一种基于课程的训练方法，以便更好地处理复杂的签名图。</li>
<li>methods: 本研究提出了一种基于课程的训练方法，其中样本按照难度从易到复杂进行排序，以便SGNN模型在处理不同难度的样本上进行学习。此外，我们还引入了一种轻量级的机制，以量化图的学习难度。</li>
<li>results: 经验 validate 表明，我们的训练方法可以提高 SGNN 模型的准确率，在链接签记预测（AUC）中提高了23.7%，并且可以显著降低标准差的AUC分布。<details>
<summary>Abstract</summary>
Signed graphs are valuable for modeling complex relationships with positive and negative connections, and Signed Graph Neural Networks (SGNNs) have become crucial tools for their analysis. However, prior to our work, no specific training plan existed for SGNNs, and the conventional random sampling approach did not address varying learning difficulties within the graph's structure. We proposed a curriculum-based training approach, where samples progress from easy to complex, inspired by human learning. To measure learning difficulty, we introduced a lightweight mechanism and created the Curriculum representation learning framework for Signed Graphs (CSG). This framework optimizes the order in which samples are presented to the SGNN model. Empirical validation across six real-world datasets showed impressive results, enhancing SGNN model accuracy by up to 23.7% in link sign prediction (AUC) and significantly improving stability with an up to 8.4 reduction in the standard deviation of AUC scores.
</details>
<details>
<summary>摘要</summary>
签名图是用于模型复杂关系的工具，它们可以表示正有负连接。然而，在我们的工作之前，没有专门的培训计划 для签名图神经网络（SGNN），而常见的随机抽样方法也不能处理图结构中的变化学习困难。我们提出了一种学习级别的培训方法，其中样本从简单到复杂进行排序，这是基于人类学习的启发。为了测量学习困难，我们引入了一种轻量级机制，并创建了签名图表示学习框架（CSG）。这个框架优化了SGNN模型被抽样的顺序。经验 validate 在六个实际数据集上表现出色，SGNN 模型的准确率提高了最多 23.7%（AUC），并有显著提高稳定性，AUC 标准差下降了最多 8.4。
</details></li>
</ul>
<hr>
<h2 id="Resampling-Stochastic-Gradient-Descent-Cheaply-for-Efficient-Uncertainty-Quantification"><a href="#Resampling-Stochastic-Gradient-Descent-Cheaply-for-Efficient-Uncertainty-Quantification" class="headerlink" title="Resampling Stochastic Gradient Descent Cheaply for Efficient Uncertainty Quantification"></a>Resampling Stochastic Gradient Descent Cheaply for Efficient Uncertainty Quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11065">http://arxiv.org/abs/2310.11065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henry Lam, Zitong Wang</li>
<li>for: 本研究旨在分析 Stochastic Gradient Descent（SGD）训练模型和 Stochastic Optimization 中的解决方案，并对其进行uncertainty quantification。</li>
<li>methods: 我们提出了两种 computationally cheap resampling-based方法来构建SGD解决方案的信任区间。其中一种使用多个SGD并行运行，通过从数据中采样而不填充替换，另一种在在线模式下进行。我们的方法可以视为现有批处理方法的改进，同时可以减少计算努力的重复样本需求。</li>
<li>results: 我们采用了一种称为”便宜bootstrap”的新想法和Berry-Esseen-type bound for SGD，以实现这些目标。我们的方法可以减少计算努力，同时可以快速地生成高质量的信任区间。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) or stochastic approximation has been widely used in model training and stochastic optimization. While there is a huge literature on analyzing its convergence, inference on the obtained solutions from SGD has only been recently studied, yet is important due to the growing need for uncertainty quantification. We investigate two computationally cheap resampling-based methods to construct confidence intervals for SGD solutions. One uses multiple, but few, SGDs in parallel via resampling with replacement from the data, and another operates this in an online fashion. Our methods can be regarded as enhancements of established bootstrap schemes to substantially reduce the computation effort in terms of resampling requirements, while at the same time bypassing the intricate mixing conditions in existing batching methods. We achieve these via a recent so-called cheap bootstrap idea and Berry-Esseen-type bound for SGD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Locally-Differentially-Private-Graph-Embedding"><a href="#Locally-Differentially-Private-Graph-Embedding" class="headerlink" title="Locally Differentially Private Graph Embedding"></a>Locally Differentially Private Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11060">http://arxiv.org/abs/2310.11060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zening Li, Rong-Hua Li, Meihao Liao, Fusheng Jin, Guoren Wang</li>
<li>for: 本研究旨在开发一种满足本地散列保护（LDP）的图像抽象算法，以保护图据中敏感信息的隐私。</li>
<li>methods: 本文提出了一种名为LDP-GE的隐私保护图像抽象框架，包括一种LDP机制来隐蔽节点数据，以及使用个性化PageRank来学习节点表示。</li>
<li>results: 对多个真实世界图据集进行了广泛的实验，并证明LDP-GE在节点分类和链接预测任务中达到了有利的隐私-用途质量比。<details>
<summary>Abstract</summary>
Graph embedding has been demonstrated to be a powerful tool for learning latent representations for nodes in a graph. However, despite its superior performance in various graph-based machine learning tasks, learning over graphs can raise significant privacy concerns when graph data involves sensitive information. To address this, in this paper, we investigate the problem of developing graph embedding algorithms that satisfy local differential privacy (LDP). We propose LDP-GE, a novel privacy-preserving graph embedding framework, to protect the privacy of node data. Specifically, we propose an LDP mechanism to obfuscate node data and adopt personalized PageRank as the proximity measure to learn node representations. Then, we theoretically analyze the privacy guarantees and utility of the LDP-GE framework. Extensive experiments conducted over several real-world graph datasets demonstrate that LDP-GE achieves favorable privacy-utility trade-offs and significantly outperforms existing approaches in both node classification and link prediction tasks.
</details>
<details>
<summary>摘要</summary>
“图像插入”已经被证明是一种有力的工具，用于学习图像中节点的隐藏表示。然而，在学习图像时，可能会引起个人隐私问题，特别是当图像数据包含敏感信息时。为了解决这个问题，在这篇论文中，我们调查了在图像上学习隐藏表示的问题，并提出了一种具有地方敏感性（LDP）的图像插入框架。我们提议了一种LDP机制，以隐藏节点数据，并采用个性化PageRank作为距离度量来学习节点表示。然后，我们对LDP-GE框架的隐私保证和实用性进行了理论分析。广泛的实验表明，LDP-GE在节点分类和链接预测任务中具有良好的隐私-实用质量比。
</details></li>
</ul>
<hr>
<h2 id="Causal-Feature-Selection-via-Transfer-Entropy"><a href="#Causal-Feature-Selection-via-Transfer-Entropy" class="headerlink" title="Causal Feature Selection via Transfer Entropy"></a>Causal Feature Selection via Transfer Entropy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11059">http://arxiv.org/abs/2310.11059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paolo Bonetti, Alberto Maria Metelli, Marcello Restelli</li>
<li>for: 本研究旨在提出一种新的方法，旨在Feature Selection和Causal Discovery之间的交叉点上，用于处理时间序列数据。</li>
<li>methods: 我们提出了一种新的 causal feature selection 方法，该方法基于前向和后向 Feature Selection 过程，并利用了传输 entropy 来估计特征之间的 causal 信息流。</li>
<li>results: 我们提供了理论保证，证明在 exact 和 finite-sample 情况下，我们的方法可以实现更好的预测和分类性能。此外，我们还在 synthetic 和实际 regression 问题上进行了数值验证，结果与考虑的基准相当竞争。<details>
<summary>Abstract</summary>
Machine learning algorithms are designed to capture complex relationships between features. In this context, the high dimensionality of data often results in poor model performance, with the risk of overfitting. Feature selection, the process of selecting a subset of relevant and non-redundant features, is, therefore, an essential step to mitigate these issues. However, classical feature selection approaches do not inspect the causal relationship between selected features and target, which can lead to misleading results in real-world applications. Causal discovery, instead, aims to identify causal relationships between features with observational data. In this paper, we propose a novel methodology at the intersection between feature selection and causal discovery, focusing on time series. We introduce a new causal feature selection approach that relies on the forward and backward feature selection procedures and leverages transfer entropy to estimate the causal flow of information from the features to the target in time series. Our approach enables the selection of features not only in terms of mere model performance but also captures the causal information flow. In this context, we provide theoretical guarantees on the regression and classification errors for both the exact and the finite-sample cases. Finally, we present numerical validations on synthetic and real-world regression problems, showing results competitive w.r.t. the considered baselines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Matrix-Compression-via-Randomized-Low-Rank-and-Low-Precision-Factorization"><a href="#Matrix-Compression-via-Randomized-Low-Rank-and-Low-Precision-Factorization" class="headerlink" title="Matrix Compression via Randomized Low Rank and Low Precision Factorization"></a>Matrix Compression via Randomized Low Rank and Low Precision Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11028">http://arxiv.org/abs/2310.11028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajarshi Saha, Varun Srivastava, Mert Pilanci<br>for:这个论文是为了提出一种基于矩阵的压缩算法，以实现矩阵的压缩和处理。methods:该算法利用矩阵的低级结构，通过随机抽象矩阵的列，并对这些列进行量化，以获得一个低级和低精度的矩阵分解。results:该算法可以实现矩阵的压缩，并且可以达到一比特为一的压缩率，同时保持或超过传统压缩技术的性能。<details>
<summary>Abstract</summary>
Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix $\mathbf{A}$ as $\mathbf{A} \approx \mathbf{L}\mathbf{R}$, where $\mathbf{L}$ and $\mathbf{R}$ are the low rank factors. The total number of elements in $\mathbf{L}$ and $\mathbf{R}$ can be significantly less than that in $\mathbf{A}$. Furthermore, the entries of $\mathbf{L}$ and $\mathbf{R}$ are quantized to low precision formats $--$ compressing $\mathbf{A}$ by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of $\mathbf{A}$ by randomly sketching its columns, followed by a quantization of the vectors constituting this basis. It then computes approximate projections of the columns of $\mathbf{A}$ onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements. We empirically demonstrate the efficacy of our algorithm in image compression, nearest neighbor classification of image and text embeddings, and compressing the layers of LlaMa-$7$b. Our results illustrate that we can achieve compression ratios as aggressive as one bit per matrix coordinate, all while surpassing or maintaining the performance of traditional compression techniques.
</details>
<details>
<summary>摘要</summary>
矩阵在不同的领域中非常有用，因为它们可以有效地组织和处理数据。然而，现代矩阵可能包含数百亿个元素，这会导致存储和处理它们的计算资源和内存使用非常高。虽然这些矩阵可能是非常大的，但它们通常是低级别的。我们提出了一个算法，它利用这种结构来获得矩阵 $\mathbf{A}$ 的低级别分解，即 $\mathbf{A} \approx \mathbf{L}\mathbf{R}$，其中 $\mathbf{L}$ 和 $\mathbf{R}$ 是低级别因素。总的来说， $\mathbf{L}$ 和 $\mathbf{R}$ 中的元素数量可以非常少于 $\mathbf{A}$ 中的元素数量。此外， $\mathbf{L}$ 和 $\mathbf{R}$ 的元素可以使用低精度格式进行压缩，从而压缩 $\mathbf{A}$。我们的算法首先计算矩阵 $\mathbf{A}$ 的估计基准的范围空间，然后使用这个基准来压缩 $\mathbf{A}$ 的列。我们then compute approximate projections of the columns of $\mathbf{A}$ onto this quantized basis. We derive upper bounds on the approximation error of our algorithm, and analyze the impact of target rank and quantization bit-budget. The tradeoff between compression ratio and approximation accuracy allows for flexibility in choosing these parameters based on specific application requirements.我们实际实现了我们的算法，并在图像压缩、图像和文本嵌入图像的最近邻紧类фикации以及LLaMa-$7$b层的压缩中进行了实验。我们的结果表明，我们可以达到一个比特为一个矩阵坐标的压缩比，同时超越或保持传统压缩技术的性能。
</details></li>
</ul>
<hr>
<h2 id="SignGT-Signed-Attention-based-Graph-Transformer-for-Graph-Representation-Learning"><a href="#SignGT-Signed-Attention-based-Graph-Transformer-for-Graph-Representation-Learning" class="headerlink" title="SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning"></a>SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11025">http://arxiv.org/abs/2310.11025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinsong Chen, Gaichao Li, John E. Hopcroft, Kun He</li>
<li>for: 这 paper 的目的是提出一种基于签名自注意力的图Transformers，以适应不同图的复杂关系。</li>
<li>methods: 该 paper 使用了自注意力机制，并提出了一种新的签名自注意力机制（SignSA），以便根据节点对的 semantic relevance 生成签名注意力值。此外， paper 还提出了一种结构意识Feed-Forward Network（SFFN），以保留地方topology信息。</li>
<li>results: EXTENSIVE empirical results 表明，SignGT 在 node-level 和 graph-level 任务上表现出色，超过了当前的图Transformers 和高级 GNNs。<details>
<summary>Abstract</summary>
The emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hence, the diverse frequency information between different node pairs could be carefully preserved. Besides, SignGT proposes a structure-aware feed-forward network (SFFN) that introduces the neighborhood bias to preserve the local topology information. In this way, SignGT could learn informative node representations from both long-range dependencies and local topology information. Extensive empirical results on both node-level and graph-level tasks indicate the superiority of SignGT against state-of-the-art graph Transformers as well as advanced GNNs.
</details>
<details>
<summary>摘要</summary>
新出现的图变换器技术已经取得了图表示学习中的出色表现，超过传统的图神经网络（GNNs）。在这项工作中，我们将自注意机制，变换器的核心模块，视为一个完全连接的图上的两步积算操作。由于生成正向注意值的性质，自注意机制等同于对所有节点进行缓和操作，保留低频信息。然而，只capture低频信息可能是学习多样性图上的节点关系不充分的。为此，我们提出了一种签名自注意机制基于图变换器（SignGT），以适应不同图上的多样性频率信息。具体来说，SignGT开发了一种新的签名自注意机制（SignSA），生成签名注意值根据节点对的semantic relevance。因此，不同节点对之间的多样频率信息可以得到细致的保留。此外，SignGT还提出了一种结构意识适应链接网络（SFFN），通过引入邻居偏好来保留本地链接信息。因此，SignGT可以从长距离依赖和本地链接信息中学习有用的节点表示。 empirical研究表明，SignGT在节点级和图级任务上表现出色，超过当前的图变换器和高级GNNs。
</details></li>
</ul>
<hr>
<h2 id="Pure-Exploration-in-Asynchronous-Federated-Bandits"><a href="#Pure-Exploration-in-Asynchronous-Federated-Bandits" class="headerlink" title="Pure Exploration in Asynchronous Federated Bandits"></a>Pure Exploration in Asynchronous Federated Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11015">http://arxiv.org/abs/2310.11015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichen Wang, Chuanhao Li, Chenyu Song, Lianghui Wang, Quanquan Gu, Huazheng Wang</li>
<li>for: 这个论文是为了解决多机器人投掷机制中的联合探索问题，即多个机器人在中央服务器的协作下，找到最佳投掷机制。</li>
<li>methods: 这篇论文提出了首个在异步环境下的联合探索多机器人投掷机制和线性投掷机制算法，以提高实际场景中agent的缺失和延迟的Robustness。</li>
<li>results: 论文的理论分析表明，提议的算法在异步环境下可以 дости到近似优化的样本复杂度和有效的通信成本，并且基于实验数据表明，这些算法在实际场景中具有高效性和通信成本的优势。<details>
<summary>Abstract</summary>
We study the federated pure exploration problem of multi-armed bandits and linear bandits, where $M$ agents cooperatively identify the best arm via communicating with the central server. To enhance the robustness against latency and unavailability of agents that are common in practice, we propose the first federated asynchronous multi-armed bandit and linear bandit algorithms for pure exploration with fixed confidence. Our theoretical analysis shows the proposed algorithms achieve near-optimal sample complexities and efficient communication costs in a fully asynchronous environment. Moreover, experimental results based on synthetic and real-world data empirically elucidate the effectiveness and communication cost-efficiency of the proposed algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究多机构共同探索多臂枪和线性枪问题，其中 $M$ 名代理人共同决定最佳臂via 与中央服务器的通信。为了增强实际中常见的延迟和代理人缺失的响应性，我们提出了首个联邦异步多臂枪和线性枪探索算法，并进行了对这些算法的理论分析。我们的分析结果显示，提案的算法可以实现近乎最佳的样本复杂度和有效的通信成本在完全异步环境中。此外，基于实验数据的实验结果也证明了提案的算法的实际性和通信成本效率。
</details></li>
</ul>
<hr>
<h2 id="Hyperspectral-In-Memory-Computing-with-Optical-Frequency-Combs-and-Programmable-Optical-Memories"><a href="#Hyperspectral-In-Memory-Computing-with-Optical-Frequency-Combs-and-Programmable-Optical-Memories" class="headerlink" title="Hyperspectral In-Memory Computing with Optical Frequency Combs and Programmable Optical Memories"></a>Hyperspectral In-Memory Computing with Optical Frequency Combs and Programmable Optical Memories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11014">http://arxiv.org/abs/2310.11014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Honari Latifpour, Byoung Jun Park, Yoshihisa Yamamoto, Myoung-Gyun Suh</li>
<li>for:  This paper aims to develop a highly parallel, programmable, and scalable optical computing system capable of handling matrix-vector multiplication operations for deep learning and optimization tasks.</li>
<li>methods: The proposed hyperspectral in-memory computing architecture integrates space multiplexing with frequency multiplexing of optical frequency combs and uses spatial light modulators as a programmable optical memory.</li>
<li>results: The authors have experimentally demonstrated multiply-accumulate operations with higher than 4-bit precision in both matrix-vector and matrix-matrix multiplications, which suggests the system’s potential for a wide variety of deep learning and optimization tasks.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文目标是开发一种高并行、可编程、可扩展的光学计算机系统，用于处理深度学习和优化任务中的矩阵-向量乘法操作。</li>
<li>methods: 提议的幽挺色响应计算机架构，结合空间多plexing和频率多plexing的光频率剖面，使用空间光模ulator作为可编程光学记忆。</li>
<li>results: 作者们实验表明，在矩阵-向量和矩阵-矩阵乘法操作中，可以实现高于4比特精度的乘法操作。<details>
<summary>Abstract</summary>
The rapid advancements in machine learning across numerous industries have amplified the demand for extensive matrix-vector multiplication operations, thereby challenging the capacities of traditional von Neumann computing architectures. To address this, researchers are currently exploring alternatives such as in-memory computing systems to develop faster and more energy-efficient hardware. In particular, there is renewed interest in computing systems based on optics, which could potentially handle matrix-vector multiplication in a more energy-efficient way. Despite promising initial results, developing a highly parallel, programmable, and scalable optical computing system capable of rivaling electronic computing hardware still remains elusive. In this context, we propose a hyperspectral in-memory computing architecture that integrates space multiplexing with frequency multiplexing of optical frequency combs and uses spatial light modulators as a programmable optical memory, thereby boosting the computational throughput and the energy efficiency. We have experimentally demonstrated multiply-accumulate operations with higher than 4-bit precision in both matrix-vector and matrix-matrix multiplications, which suggests the system's potential for a wide variety of deep learning and optimization tasks. This system exhibits extraordinary modularity, scalability, and programmability, effectively transcending the traditional limitations of optics-based computing architectures. Our approach demonstrates the potential to scale beyond peta operations per second, marking a significant step towards achieving high-throughput energy-efficient optical computing.
</details>
<details>
<summary>摘要</summary>
快速发展的机器学习技术在各个领域的应用使得大量矩阵-向量乘法操作的需求增加，导致传统的 von Neumann 计算架构的能力受到挑战。为了解决这问题，研究人员正在寻找代替方案，如内存计算系统，以开发更快速、更能效的硬件。特别是，光学计算系统在处理矩阵-向量乘法方面可能存在更高的能效性。虽然初步的结果很有前途，但是开发一个高度并行、可编程、扩展的光学计算系统，能与电子计算硬件竞争仍然很困难。在这种情况下，我们提出了一种快速响应的多光谱内存计算架构，通过空间复用和频率复用光谱镜的技术，使用空间光模拟器作为可编程的光学记忆，从而提高计算通过put和能效性。我们在实验中已经实现了高于4位精度的矩阵-向量和矩阵-矩阵乘法 multiply-accumulate 操作，这表明该系统在深度学习和优化任务中的潜在能力。这种系统具有极高的可组合性、可扩展性和可编程性，实际上跨越了传统光学计算架构的限制。我们的方法可以超过PETA操作每秒，这标志着光学计算技术的高性能、能效的发展。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Pairwise-Encodings-for-Link-Prediction"><a href="#Adaptive-Pairwise-Encodings-for-Link-Prediction" class="headerlink" title="Adaptive Pairwise Encodings for Link Prediction"></a>Adaptive Pairwise Encodings for Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11009">http://arxiv.org/abs/2310.11009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryshomer/lpformer">https://github.com/harryshomer/lpformer</a></li>
<li>paper_authors: Harry Shomer, Yao Ma, Haitao Mao, Juanhui Li, Bo Wu, Jiliang Tang</li>
<li>for: 链接预测任务是图structured数据中常见的任务，有各种应用。过去通常使用手动设计的规则进行预测。</li>
<li>methods: 这些方法使用message-passing神经网络（MPNN）和规则方法的优点，通过对MPNN的输出和“对称编码”（pairwise encoding）进行组合来进行预测。这些方法在许多数据集上达到了强大的表现。但是，现有的对称编码往往带有强大的推导性偏见，使用同一些下面因素来分类所有链接。</li>
<li>results: LPFormer方法可以在许多数据集上达到SOTA表现，同时保持效率。<details>
<summary>Abstract</summary>
Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a new method, LPFormer, which attempts to adaptively learn the pairwise encodings for each link. LPFormer models the link factors via an attention module that learns the pairwise encoding that exists between nodes by modeling multiple factors integral to link prediction. Extensive experiments demonstrate that LPFormer can achieve SOTA performance on numerous datasets while maintaining efficiency.
</details>
<details>
<summary>摘要</summary>
链接预测是图Structured data上常见的任务，它在多个领域应用。过去，人工设计的规则通常用于这种任务。这些规则选择的目的是使其与下面的链接形成因素相吻合。在最近几年里，一种新的方法 emerge，它结合了 message-passing neural networks（MPNN）和规则方法的优点。这些方法通过 MPNN 的输出和候选链接的 "对称编码" 进行预测，其中对于每个链接， captured the relationship between nodes in the candidate link。它们在许多数据集上实现了强的表现。然而，现有的对称编码通常具有强 inductive bias，使用同一些基因来分类所有的链接。这限制了现有方法的能力，以learn how to properly classify a variety of different links that may form from different factors。为了解决这些限制，我们提出了一种新方法，LPFormer，它尝试通过 adaptively learning the pairwise encodings for each link来模型链接因素。LPFormer 通过注意力模块来学习每个链接的对称编码，该编码捕捉了 nodes 之间的多种因素。广泛的实验表明，LPFormer 可以在多个数据集上实现 SOTA 性能，同时保持效率。
</details></li>
</ul>
<hr>
<h2 id="Spatially-resolved-hyperlocal-weather-prediction-and-anomaly-detection-using-IoT-sensor-networks-and-machine-learning-techniques"><a href="#Spatially-resolved-hyperlocal-weather-prediction-and-anomaly-detection-using-IoT-sensor-networks-and-machine-learning-techniques" class="headerlink" title="Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques"></a>Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11001">http://arxiv.org/abs/2310.11001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anita B. Agarwal, Rohit Rajesh, Nitin Arul</li>
<li>for: 本研究旨在提供高精度、快速更新的本地天气预测，以满足各种应用需求，如农业、灾害管理等。</li>
<li>methods: 本研究提议一种新的方法， combining 本地天气预测和异常检测，使用互联网物理传感器网络和高级机器学习技术。该方法利用多个空间分布的、但相对较近的位置和物理传感器数据，创建高分辨率的天气模型，可预测短期、本地化的天气 Conditions。</li>
<li>results: 研究发现，该系统可以增强天气预测的空间分辨率，同时实时检测异常天气情况。此外，该系统还可以通过不监督学习算法，找到异常天气模式，为决策提供时间性信息。<details>
<summary>Abstract</summary>
Accurate and timely hyperlocal weather predictions are essential for various applications, ranging from agriculture to disaster management. In this paper, we propose a novel approach that combines hyperlocal weather prediction and anomaly detection using IoT sensor networks and advanced machine learning techniques. Our approach leverages data from multiple spatially-distributed yet relatively close locations and IoT sensors to create high-resolution weather models capable of predicting short-term, localized weather conditions such as temperature, pressure, and humidity. By monitoring changes in weather parameters across these locations, our system is able to enhance the spatial resolution of predictions and effectively detect anomalies in real-time. Additionally, our system employs unsupervised learning algorithms to identify unusual weather patterns, providing timely alerts. Our findings indicate that this system has the potential to enhance decision-making.
</details>
<details>
<summary>摘要</summary>
准确和及时的本地天气预测非常重要，用于各种应用，从农业到灾害管理。在这篇论文中，我们提出了一种新的方法，它结合了本地天气预测和异常检测，使用互联网器件网络和高级机器学习技术。我们的方法利用多个位于不同地点，但相对较近的位置和互联网器件来创建高分解能力的天气模型，能够预测短期、本地化的天气条件，如温度、压力和湿度。通过监测这些位置之间的天气参数变化，我们的系统可以提高地理分解能力，并实时检测异常。此外，我们的系统使用无监督学习算法来识别异常天气模式，提供实时警报。我们的发现表明，这种系统有可能提高决策。
</details></li>
</ul>
<hr>
<h2 id="Program-Translation-via-Code-Distillation"><a href="#Program-Translation-via-Code-Distillation" class="headerlink" title="Program Translation via Code Distillation"></a>Program Translation via Code Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11476">http://arxiv.org/abs/2310.11476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufan Huang, Mengnan Qi, Yongqiang Yao, Maoquan Wang, Bin Gu, Colin Clement, Neel Sundaresan</li>
<li>for: 这篇论文主要写于如何使用Code Distillation（CoDist）模型进行软件版本迁移和程序翻译。</li>
<li>methods: 这篇论文提出了一种基于Code Distillation（CoDist）模型的方法，该方法可以捕捉代码的semantic和structural等价性，并生成一种语言无关的中间表示。这种中间表示可以作为翻译的准则，从而生成并行的训练数据集，并且可以在任何编程语言上应用。</li>
<li>results: 根据CodeXGLUE和TransCoder GeeksForGeeks翻译测试 benchmark，这种方法可以达到当前最佳性能水平，与TransCoder-ST相比，增加了12.7%的平均绝对提升。<details>
<summary>Abstract</summary>
Software version migration and program translation are an important and costly part of the lifecycle of large codebases. Traditional machine translation relies on parallel corpora for supervised translation, which is not feasible for program translation due to a dearth of aligned data. Recent unsupervised neural machine translation techniques have overcome data limitations by included techniques such as back translation and low level compiler intermediate representations (IR). These methods face significant challenges due to the noise in code snippet alignment and the diversity of IRs respectively. In this paper we propose a novel model called Code Distillation (CoDist) whereby we capture the semantic and structural equivalence of code in a language agnostic intermediate representation. Distilled code serves as a translation pivot for any programming language, leading by construction to parallel corpora which scale to all available source code by simply applying the distillation compiler. We demonstrate that our approach achieves state-of-the-art performance on CodeXGLUE and TransCoder GeeksForGeeks translation benchmarks, with an average absolute increase of 12.7% on the TransCoder GeeksforGeeks translation benchmark compare to TransCoder-ST.
</details>
<details>
<summary>摘要</summary>
软件版本迁移和程序翻译是大型代码库生命周期中的重要和昂贵部分。传统机器翻译依赖平行 corpora 进行监督翻译，但对程序翻译来说不是可行的，因为没有准确的数据对齐。现代无监督神经机器翻译技术已经突破了数据限制，通过包括回翻译和低级编译器中间表示（IR）等技术。然而，这些方法面临着代码片段对齐的噪音和 IR 的多样性的挑战。在这篇论文中，我们提出了一种新的模型 called Code Distillation（CoDist），它可以捕捉代码的 semantics 和结构相似性，并将其转化为语言无关的中间表示。浓缩代码可以作为任何编程语言的翻译轮廓，从而自动生成平行 corpora，并且可以通过 simply 应用浓缩编译器来扩展到所有可用的源代码。我们示示了我们的方法可以在 CodeXGLUE 和 TransCoder GeeksForGeeks 翻译benchmark中达到状态机器翻译的性能水平，与TransCoder-ST 的平均绝对增幅为12.7%。
</details></li>
</ul>
<hr>
<h2 id="Why-Do-Students-Drop-Out-University-Dropout-Prediction-and-Associated-Factor-Analysis-Using-Machine-Learning-Techniques"><a href="#Why-Do-Students-Drop-Out-University-Dropout-Prediction-and-Associated-Factor-Analysis-Using-Machine-Learning-Techniques" class="headerlink" title="Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques"></a>Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10987">http://arxiv.org/abs/2310.10987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Kim, Eliot Yoo, Samuel Kim</li>
<li>for: 本研究旨在预测大学学生的毕业和退学情况，以便帮助教育机构和学生更好地规划教学和学习计划。</li>
<li>methods: 本研究使用学术、民生、社会经济和macro经济数据类型进行大学生毕业和退学预测。同时，我们进行了相关因素分析，以便分析这些数据类型对机器学习模型的表现有多大影响。</li>
<li>results: 我们使用这些特征训练了四个二分类器，以确定学生会毕业或退学。总的来说，这些模型在预测退学状况时的ROC-AUC分数为0.935。对于学术数据类型，模型性能最高，当排除所有学术相关特征时，模型性能下降到0.811。初步结果表明，数据类型和退学状况之间存在相关性。<details>
<summary>Abstract</summary>
Graduation and dropout rates have always been a serious consideration for educational institutions and students. High dropout rates negatively impact both the lives of individual students and institutions. To address this problem, this study examined university dropout prediction using academic, demographic, socioeconomic, and macroeconomic data types. Additionally, we performed associated factor analysis to analyze which type of data would be most influential on the performance of machine learning models in predicting graduation and dropout status. These features were used to train four binary classifiers to determine if students would graduate or drop out. The overall performance of the classifiers in predicting dropout status had an average ROC-AUC score of 0.935. The data type most influential to the model performance was found to be academic data, with the average ROC-AUC score dropping from 0.935 to 0.811 when excluding all academic-related features from the data set. Preliminary results indicate that a correlation does exist between data types and dropout status.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:毕业和退学率一直是教育机构和学生的严重考虑之一。高退学率对个人学生和机构都有负面影响。为了解决这个问题，本研究使用学术、人口、社会经济和 macroeconomic 数据类型进行大学退学预测。此外，我们还进行了相关因素分析，以分析这些数据类型对机器学习模型的执行性能有多大影响。这些特征被用来训练四个二分类器，以确定学生会毕业或退学。总的来说，这些分类器在预测退学状况时的 ROC-AUC 分数为 0.935。数据类型对模型性能最有影响的是学术数据，当排除所有学术相关特征时，模型的 ROC-AUC 分数从 0.935 下降至 0.811。初步结果表明，数据类型和退学状况之间存在相关性。
</details></li>
</ul>
<hr>
<h2 id="Exact-nonlinear-state-estimation"><a href="#Exact-nonlinear-state-estimation" class="headerlink" title="Exact nonlinear state estimation"></a>Exact nonlinear state estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10976">http://arxiv.org/abs/2310.10976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hristo G. Chipilski</li>
<li>for: 提高数据融合方法的准确性和稳定性，尤其是在高维模型中。</li>
<li>methods: 基于生成型人工智能技术的新非线性估计理论，拓展了现有的准 Gaussian 分布假设，提供了更高准确性和稳定性的数据融合方法。</li>
<li>results: 对理想化的统计实验进行了验证，结果表明，在观测错误小于预测不确定性和状态变量存在强非线性相互关系的情况下，ECTF 可以提供更高的准确性和稳定性。<details>
<summary>Abstract</summary>
The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and validated using idealized statistical experiments that feature bounded quantities with non-Gaussian distributions, a prevalent challenge in Earth system models. Results from these experiments indicate that the greatest benefits from ECTF occur when observation errors are small relative to the forecast uncertainty and when state variables exhibit strong nonlinear dependencies. Ultimately, the new filtering theory offers exciting avenues for improving conventional DA algorithms through their principled integration with AI techniques.
</details>
<details>
<summary>摘要</summary>
大多数数据吸收（DA）方法在地球科学中基于 Gaussian 假设。这些假设使得算法效率高，但会导致分析偏误和预测质量下降。非Parametric, 粒子基本的 DA 算法具有更高的准确度，但在高维模型应用中仍存在操作挑战。本文从最近的生成式人工智能（AI）领域启发，提出一种新的非线性估计理论，以尝试桥接现有 DA 方法ología 的空缺。特别是，一种 conjugate transform filter（CTF）被 derivation 和证明能够泛化 kalman 筛到任意非 Gaussian 分布。新筛有多个愉悦性质，如保持先前状态的统计关系和 converge 到高精度观测。一种 ensemble approximation of the new theory（ECTF）也被提出和验证，使用 идеalized 统计实验，这些实验中的量均具有非 Gaussian 分布，是地球系统模型中的普遍问题。实验结果表明，ECTF 在观测Error 小于预测 uncertainty 以及状态变量具有强非线性关系时，具有最大的优势。最后，新的 filtering 理论提供了改进传统 DA 算法的原则性 интеграción with AI 技术的推动力。
</details></li>
</ul>
<hr>
<h2 id="SD-PINN-Deep-Learning-based-Spatially-Dependent-PDEs-Recovery"><a href="#SD-PINN-Deep-Learning-based-Spatially-Dependent-PDEs-Recovery" class="headerlink" title="SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery"></a>SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10970">http://arxiv.org/abs/2310.10970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixian Liu, Peter Gerstoft</li>
<li>for: 这篇论文是为了描述一种能够直接从物理测量数据中恢复部分偏微分方程（PDE）的含义的物理学习神经网络（PINN）的扩展。</li>
<li>methods: 该方法使用一个具有空间依赖性的物理学习神经网络（SD-PINN），可以通过单个神经网络来恢复空间依赖性的PDE含义。该方法还利用物理约束来降低噪声的影响。</li>
<li>results: 该方法可以充分利用物理约束来恢复PDE含义，并且可以在没有测量数据的情况下，通过含义低级假设来恢复PDE含义。<details>
<summary>Abstract</summary>
The physics-informed neural network (PINN) is capable of recovering partial differential equation (PDE) coefficients that remain constant throughout the spatial domain directly from physical measurements. In this work, we propose a spatially dependent physics-informed neural network (SD-PINN), which enables the recovery of coefficients in spatially-dependent PDEs using a single neural network, eliminating the requirement for domain-specific physical expertise. The proposed method exhibits robustness to noise owing to the incorporation of physical constraints. It can also incorporate the low-rank assumption of the spatial variation for the PDE coefficients to recover the coefficients at locations without available measurements.
</details>
<details>
<summary>摘要</summary>
физи学信息泛化神经网络（PINN）可以直接从物理测量中提取常数partial differential equation（PDE）的征量，这些征量在空间领域中保持常数。在这个工作中，我们提议使用空间依赖的 физи学信息泛化神经网络（SD-PINN），它使得可以使用单个神经网络来恢复空间依赖的PDE征量，消除了域pecific的物理专业知识的需求。该方法具有鲁棒性于噪声，并可以 incorporate 空间变化的low-rank假设来恢复征量在测量位置之外的位置。Note: "PINN" and "SD-PINN" in the text are abbreviations for "physics-informed neural network" and "spatially-dependent physics-informed neural network", respectively.
</details></li>
</ul>
<hr>
<h2 id="The-neural-network-models-with-delays-for-solving-absolute-value-equations"><a href="#The-neural-network-models-with-delays-for-solving-absolute-value-equations" class="headerlink" title="The neural network models with delays for solving absolute value equations"></a>The neural network models with delays for solving absolute value equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10965">http://arxiv.org/abs/2310.10965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongmei Yu, Gehao Zhang, Cairong Chen, Deren Han</li>
<li>for: 解决绝值方程 ($Ax - |x| - b &#x3D; 0$)</li>
<li>methods: 使用倒计时间神经网络模型和离散延迟神经网络模型，以及 Lyapunov-Krasovskii 理论和线性矩阵不等式（LMI）方法</li>
<li>results: 能够解决一类绝值方程，其中 $A^{-1}$ 的范数大于 1Here’s the breakdown of each point:1. For: This point states that the paper is written for solving the absolute value equation (AVE).2. Methods: This point lists the methods used in the paper, including the use of inverse-free neural network models with discrete delays, as well as the Lyapunov-Krasovskii theory and LMI method.3. Results: This point states the main result of the paper, which is that the proposed neural network models are exponentially convergent to the solution of the AVE, and can solve a class of AVE with $|A^{-1}|&gt;1$.<details>
<summary>Abstract</summary>
An inverse-free neural network model with mixed delays is proposed for solving the absolute value equation (AVE) $Ax -|x| - b =0$, which includes an inverse-free neural network model with discrete delay as a special case. By using the Lyapunov-Krasovskii theory and the linear matrix inequality (LMI) method, the developed neural network models are proved to be exponentially convergent to the solution of the AVE. Compared with the existing neural network models for solving the AVE, the proposed models feature the ability of solving a class of AVE with $\|A^{-1}\|>1$. Numerical simulations are given to show the effectiveness of the two delayed neural network models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>一种无反函数神经网络模型，包括杂度延迟，被提出来解决绝值方程（AVE） $Ax - |x| - b = 0$。这个模型包括杂度延迟神经网络模型作为特殊情况。通过利用利阿普涅夫-克拉索夫斯基理论和线性矩阵不等式（LMI）方法，我们证明了这些神经网络模型在AVE的解的抽象 convergent。与现有的神经网络模型相比，我们的模型可以解决一类AVE中，$ \|A^{-1}\|>1$。numerical simulations 给出了这两种延迟神经网络模型的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Local-Graph-Limits-Perspective-on-Sampling-Based-GNNs"><a href="#A-Local-Graph-Limits-Perspective-on-Sampling-Based-GNNs" class="headerlink" title="A Local Graph Limits Perspective on Sampling-Based GNNs"></a>A Local Graph Limits Perspective on Sampling-Based GNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10953">http://arxiv.org/abs/2310.10953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeganeh Alimohammadi, Luana Ruiz, Amin Saberi</li>
<li>for: 这项研究旨在提供一种训练图神经网络（GNNs）的理论框架，用于处理大输入图。</li>
<li>methods: 这项研究使用了采样方法，将大输入图分解成小固定大小的子图进行训练。</li>
<li>results: 研究发现，通过采样训练GNNs，可以在减少训练时间和数据量的情况下，保持模型的性能。在一个节点分类任务中，研究发现，使用小型子图进行采样训练，可以达到与直接训练在原始图上的性能相似的水平。<details>
<summary>Abstract</summary>
We propose a theoretical framework for training Graph Neural Networks (GNNs) on large input graphs via training on small, fixed-size sampled subgraphs. This framework is applicable to a wide range of models, including popular sampling-based GNNs, such as GraphSAGE and FastGCN. Leveraging the theory of graph local limits, we prove that, under mild assumptions, parameters learned from training sampling-based GNNs on small samples of a large input graph are within an $\epsilon$-neighborhood of the outcome of training the same architecture on the whole graph. We derive bounds on the number of samples, the size of the graph, and the training steps required as a function of $\epsilon$. Our results give a novel theoretical understanding for using sampling in training GNNs. They also suggest that by training GNNs on small samples of the input graph, practitioners can identify and select the best models, hyperparameters, and sampling algorithms more efficiently. We empirically illustrate our results on a node classification task on large citation graphs, observing that sampling-based GNNs trained on local subgraphs 12$\times$ smaller than the original graph achieve comparable performance to those trained on the input graph.
</details>
<details>
<summary>摘要</summary>
我们提出一种理论框架，用于在大输入图上训练图神经网络（GNNs） via 训练小型、固定大小的采样子图。这个框架适用于各种采样基于GNNs，包括受欢迎的采样GNNs，如GraphSAGE和FastGCN。我们利用图本地限制理论，证明在某些假设下，从训练采样GNNs на小样本上获得的参数与训练同样模型在整个图上的参数在$\epsilon$-邻域内。我们 derive出参数数量、图的大小和训练步骤的上限，作为函数于$\epsilon$。我们的结果为使用采样在训练GNNs提供了新的理论理解，并表明通过训练GNNs于小样本上可以更加快速地确定最佳模型、 гиперпараметры和采样算法。我们在一个节点分类任务上对大量引用图进行了实验，发现采样基于GNNs训练于local子图12$\times$小于原始图的性能与训练于原始图相当。
</details></li>
</ul>
<hr>
<h2 id="Restricted-Tweedie-Stochastic-Block-Models"><a href="#Restricted-Tweedie-Stochastic-Block-Models" class="headerlink" title="Restricted Tweedie Stochastic Block Models"></a>Restricted Tweedie Stochastic Block Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10952">http://arxiv.org/abs/2310.10952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Jian, Mu Zhu, Peijun Sang</li>
<li>for: 本文旨在提出一种基于非正态 Tweedie 分布的随机块模型（SBM），用于社区检测网络中的非负零Inflated 连接量。</li>
<li>methods: 本文提出了一种新的 SBM 模型，使用restricted Tweedie distribution来模型连接量，并考虑了节点信息，如国家之间的地理距离。</li>
<li>results: 在大量的 simulations 和实际国际贸易数据中，本文显示了该模型的效果。特别是，当 nodes 数足够多时，计算最大 likelihood 参数的过程可以独立地计算节点标签。这使得可以开发一种高效的 two-step 算法，将 covariate 效应与其他参数分离。<details>
<summary>Abstract</summary>
The stochastic block model (SBM) is a widely used framework for community detection in networks, where the network structure is typically represented by an adjacency matrix. However, conventional SBMs are not directly applicable to an adjacency matrix that consists of non-negative zero-inflated continuous edge weights. To model the international trading network, where edge weights represent trading values between countries, we propose an innovative SBM based on a restricted Tweedie distribution. Additionally, we incorporate nodal information, such as the geographical distance between countries, and account for its dynamic effect on edge weights. Notably, we show that given a sufficiently large number of nodes, estimating this covariate effect becomes independent of community labels of each node when computing the maximum likelihood estimator of parameters in our model. This result enables the development of an efficient two-step algorithm that separates the estimation of covariate effects from other parameters. We demonstrate the effectiveness of our proposed method through extensive simulation studies and an application to real-world international trading data.
</details>
<details>
<summary>摘要</summary>
Stochastic block model (SBM) 是一种广泛使用的社区探测模型，用于网络结构的表示，通常是一个相对矩阵。然而，传统的 SBM 不直接适用于具有非负零填充连接权重的邻接矩阵。为了模型国际贸易网络，其中边权重表示国家之间贸易值，我们提出了一种创新的 SBM，基于限制的 Tweedie 分布。此外，我们还考虑了节点信息，如国家之间的地理距离，并考虑其动态影响边权重。我们发现，当节点数足够多时，计算每个节点社区标签的最大 LIKELIHOOD 估计器中计算 covariate 效应的结果，是独立的。这一结果允许我们开发一种高效的 two-step 算法，将 covariate 效应与其他参数分离。我们通过广泛的 simulations 研究和实际国际贸易数据应用，证明了我们提出的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Combat-Urban-Congestion-via-Collaboration-Heterogeneous-GNN-based-MARL-for-Coordinated-Platooning-and-Traffic-Signal-Control"><a href="#Combat-Urban-Congestion-via-Collaboration-Heterogeneous-GNN-based-MARL-for-Coordinated-Platooning-and-Traffic-Signal-Control" class="headerlink" title="Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control"></a>Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10948">http://arxiv.org/abs/2310.10948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianyue Peng, Hang Gao, Hao Wang, H. Michael Zhang</li>
<li>for: 提高交通流量和减少拥堵</li>
<li>methods: 使用多智能体学习和交通理论来解决异质性和协调问题，并设计了各自的观察、操作和奖励函数来优化交通流量</li>
<li>results: 通过 SUMO 模拟，实现了交通流量等多个指标的协调结果，并与独立的信号控制或队列控制相比，表现更佳。<details>
<summary>Abstract</summary>
Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale. We evaluate our approach through SUMO simulation, which shows a convergent result in terms of various transportation metrics and better performance over sole signal or platooning control.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow.2. Designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale.We evaluate our approach through SUMO simulation, which shows a convergent result in terms of various transportation metrics and better performance over sole signal or platooning control.Translation notes:* “signal control” and “vehicle platooning” are translated as “信号控制” and “车辆队列”, respectively.* “heterogeneous graph multi-agent reinforcement learning” is translated as “多代理信号控制与车辆队列强化学习”.* “coordination” is translated as “协调”.* “ SUMO simulation” is translated as “SUMO仿真”.</details></li>
</ol>
<hr>
<h2 id="Multi-point-Feedback-of-Bandit-Convex-Optimization-with-Hard-Constraints"><a href="#Multi-point-Feedback-of-Bandit-Convex-Optimization-with-Hard-Constraints" class="headerlink" title="Multi-point Feedback of Bandit Convex Optimization with Hard Constraints"></a>Multi-point Feedback of Bandit Convex Optimization with Hard Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10946">http://arxiv.org/abs/2310.10946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasunari Hikima</li>
<li>for: 本 paper 研究了带约束的带射搜索优化问题，learner 需要在部分损失函数信息的情况下生成一个序列决策，以实现总损失降低和约束违反降低。</li>
<li>methods: 我们采用了累积硬约束违反度作为约束违反度的度量，即 $\sum_{t&#x3D;1}^{T} \max{g_t(\boldsymbol{x}_t), 0}$. 由于最大运算，不可能通过不满足约束的解释取消约束违反的效果，与传统的长期约束违反度不同。我们提出了一种罚款基于的 proximal 梯度下降法，可以在梯度估计中使用两点函数评估。</li>
<li>results: 我们的算法可以实现 $O(d^2T^{\max{c,1-c}})$  regret bounds和 $O(d^2T^{1-\frac{c}{2})$ 约束违反度 bounds，其中 $d$ 是可行区域的维度，$c\in[\frac{1}{2}, 1)$ 是用户决定的参数。我们还扩展了结果到损失函数是强 convex 时的情况，并证明了 regret 和约束违反度 bounds 可以进一步降低。<details>
<summary>Abstract</summary>
This paper studies bandit convex optimization with constraints, where the learner aims to generate a sequence of decisions under partial information of loss functions such that the cumulative loss is reduced as well as the cumulative constraint violation is simultaneously reduced. We adopt the cumulative \textit{hard} constraint violation as the metric of constraint violation, which is defined by $\sum_{t=1}^{T} \max\{g_t(\boldsymbol{x}_t), 0\}$. Owing to the maximum operator, a strictly feasible solution cannot cancel out the effects of violated constraints compared to the conventional metric known as \textit{long-term} constraints violation. We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation, in which the gradient is estimated with a two-point function evaluation. Precisely, our algorithm attains $O(d^2T^{\max\{c,1-c\})$ regret bounds and $O(d^2T^{1-\frac{c}{2})$ cumulative hard constraint violation bounds for convex loss functions and time-varying constraints, where $d$ is the dimensionality of the feasible region and $c\in[\frac{1}{2}, 1)$ is a user-determined parameter. We also extend the result for the case where the loss functions are strongly convex and show that both regret and constraint violation bounds can be further reduced.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reaching-the-Limit-in-Autonomous-Racing-Optimal-Control-versus-Reinforcement-Learning"><a href="#Reaching-the-Limit-in-Autonomous-Racing-Optimal-Control-versus-Reinforcement-Learning" class="headerlink" title="Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning"></a>Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10943">http://arxiv.org/abs/2310.10943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlong Song, Angel Romero, Matthias Mueller, Vladlen Koltun, Davide Scaramuzza</li>
<li>for: This paper aims to design a control system for an agile mobile robot, specifically in the context of autonomous drone racing.</li>
<li>methods: The paper uses reinforcement learning (RL) to train a neural network controller, which outperforms optimal control (OC) methods in this setting.</li>
<li>results: The RL controller achieves superhuman control performance within minutes of training on a standard workstation, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文目标是设计一个适应度高的移动机器人控制系统，具体是在自动驾驶飞机比赛中进行。</li>
<li>methods: 这篇论文使用反馈学习（RL）来训练一个神经网络控制器，RL控制器在这个设定中超过优化控制（OC）方法的性能。</li>
<li>results: RL控制器在标准工作站上训练了几分钟后已经实现了人类控制性能的超越，达到了12倍重力加速度的峰值加速和108公里&#x2F;小时的峰值速度。<details>
<summary>Abstract</summary>
A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain randomization to cope with model uncertainty, allowing the discovery of more robust control responses. Our findings allowed us to push an agile drone to its maximum performance, achieving a peak acceleration greater than 12 times the gravitational acceleration and a peak velocity of 108 kilometers per hour. Our policy achieved superhuman control within minutes of training on a standard workstation. This work presents a milestone in agile robotics and sheds light on the role of RL and OC in robot control.
</details>
<details>
<summary>摘要</summary>
中心问题在 роботике是如何设计一个敏捷移动机器人的控制系统。这篇论文系统地研究这个问题，专注于一个挑战性的设定：自主无人机赛车。我们表明，使用强化学习（RL）训练的神经网络控制器在这个设定中表现得更好于优化控制（OC）方法。然后，我们研究了RL和OC之间的基本因素，发现RL的优势不在于更好地优化目标函数，而是在于优化更好的目标函数。OC将问题分解为规划和控制两个部分，使用显式中间表示（如轨迹）作为控制器的界面，这种分解限制控制器可表达的行为范围，导致面临不Modeled Effects时的控制性下降。相比之下，RL可以直接优化任务级目标函数，并通过随机化预测来快速适应模型不确定性，从而发现更加 Robust control response。我们的发现使我们可以将敏捷无人机 pushed to its maximum performance，达到了12倍重力加速度的峰值加速和108公里/小时的峰值速度。我们的策略在标准工作站上训练仅需几分钟便可以达到超人控制水平。这项工作为敏捷 роботиcs带来了里程碑，也照亮了RL和OC在机器人控制中的角色。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Simple-Spectral-Clustering-in-Theory-and-Practice"><a href="#Fast-and-Simple-Spectral-Clustering-in-Theory-and-Practice" class="headerlink" title="Fast and Simple Spectral Clustering in Theory and Practice"></a>Fast and Simple Spectral Clustering in Theory and Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10939">http://arxiv.org/abs/2310.10939</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pmacg/fast-spectral-clustering">https://github.com/pmacg/fast-spectral-clustering</a></li>
<li>paper_authors: Peter Macgregor</li>
<li>for: 寻找图像中的k个团体</li>
<li>methods: 使用频谱归一化法，使用力量法计算顺序 embed 图像中的顶点</li>
<li>results: 可以快速地找到图像中的团体，并且准确地回归真实的团体结果<details>
<summary>Abstract</summary>
Spectral clustering is a popular and effective algorithm designed to find $k$ clusters in a graph $G$. In the classical spectral clustering algorithm, the vertices of $G$ are embedded into $\mathbb{R}^k$ using $k$ eigenvectors of the graph Laplacian matrix. However, computing this embedding is computationally expensive and dominates the running time of the algorithm. In this paper, we present a simple spectral clustering algorithm based on a vertex embedding with $O(\log(k))$ vectors computed by the power method. The vertex embedding is computed in nearly-linear time with respect to the size of the graph, and the algorithm provably recovers the ground truth clusters under natural assumptions on the input graph. We evaluate the new algorithm on several synthetic and real-world datasets, finding that it is significantly faster than alternative clustering algorithms, while producing results with approximately the same clustering accuracy.
</details>
<details>
<summary>摘要</summary>
spectral clustering 是一种流行的有效算法，用于在图 G 中找到 $k$ 个群。 classical spectral clustering 算法中，图 vertices 被嵌入到 $\mathbb{R}^k$ 中使用 $k$ 个图 Laplacian 矩阵的特征值。然而，计算这个嵌入是计算成本高昂，对算法的运行时间产生很大影响。在本文中，我们提出了一种简单的 spectral clustering 算法，基于一个 $O(\log(k))$ 维的顶点嵌入，通过力方法计算。顶点嵌入的计算时间与图的大小近似线性，而且算法可以证明地回归真实的群集，以下面的自然假设。我们对这种新算法在一些 sintetic 和实际世界数据集上进行了评估，发现它比其他归一化算法更快速，并且生成的结果与实际结果相似。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-in-the-Quantum-Age-Quantum-vs-Classical-Support-Vector-Machines"><a href="#Machine-Learning-in-the-Quantum-Age-Quantum-vs-Classical-Support-Vector-Machines" class="headerlink" title="Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines"></a>Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10910">http://arxiv.org/abs/2310.10910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davut Emre Tasar, Kutan Koruyan, Ceren Ocal Tasar</li>
<li>For: 本研究探讨机器学习算法在经典和量子计算模式下的效率。具体来说，通过强调支持向量机器（SVM），我们研究了使用量子硬件进行分类的量子支持向量机器（QSVM）的性能。* Methods: 本研究采用了一系列实验，使用Qiskit库进行实现，并进行了参数优化。* Results: 发现在某些情况下，QSVM可以与经典SVM匹敌，但是现在的执行时间比较长。此外，我们发现，随着量子计算能力的提高和平行计算的增加，量子机器学习算法的性能可以得到明显改善。这项研究为未来量子机器学习应用提供了有价值的信息。<details>
<summary>Abstract</summary>
This work endeavors to juxtapose the efficacy of machine learning algorithms within classical and quantum computational paradigms. Particularly, by emphasizing on Support Vector Machines (SVM), we scrutinize the classification prowess of classical SVM and Quantum Support Vector Machines (QSVM) operational on quantum hardware over the Iris dataset. The methodology embraced encapsulates an extensive array of experiments orchestrated through the Qiskit library, alongside hyperparameter optimization. The findings unveil that in particular scenarios, QSVMs extend a level of accuracy that can vie with classical SVMs, albeit the execution times are presently protracted. Moreover, we underscore that augmenting quantum computational capacity and the magnitude of parallelism can markedly ameliorate the performance of quantum machine learning algorithms. This inquiry furnishes invaluable insights regarding the extant scenario and future potentiality of machine learning applications in the quantum epoch. Colab: https://t.ly/QKuz0
</details>
<details>
<summary>摘要</summary>
Note:* " classical" and "quantum" are translated as "古典" and "量子" respectively.* "computational paradigms" is translated as "计算框架"* "Support Vector Machines" is translated as "支持向量机"* "Quantum Support Vector Machines" is translated as "量子支持向量机"* "hyperparameter optimization" is translated as "超参数优化"* " execution times" is translated as "执行时间"* "quantum computational capacity" is translated as "量子计算能力"* "magnitude of parallelism" is translated as "并行性的大小"
</details></li>
</ul>
<hr>
<h2 id="Heterogenous-Memory-Augmented-Neural-Networks"><a href="#Heterogenous-Memory-Augmented-Neural-Networks" class="headerlink" title="Heterogenous Memory Augmented Neural Networks"></a>Heterogenous Memory Augmented Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10909">http://arxiv.org/abs/2310.10909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qiuzh20/hma">https://github.com/qiuzh20/hma</a></li>
<li>paper_authors: Zihan Qiu, Zhen Liu, Shuicheng Yan, Shanghang Zhang, Jie Fu</li>
<li>for: 本研究旨在提出一种基于异构记忆扩展的神经网络方法，以提高神经网络在数据稀缺和 OUT-OF-DISTRIBUTION（OOD）场景中的表现。</li>
<li>methods: 该方法通过引入学习记忆标记和注意机制，使得神经网络可以更好地处理大量数据，而无需增加巨大的计算成本。</li>
<li>results: 经过广泛的实验证明，该方法可以与不同的底层模型（MLP、CNN、GNN和Transformer）结合使用，并在图像和图格等任务下显示出竞争性的表现，特别是在数据稀缺和OOD情况下。<details>
<summary>Abstract</summary>
It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios. However, existing semi-parametric methods mostly depend on independent raw data points - this strategy is difficult to scale up due to both high computational costs and the incapacity of current attention mechanisms with a large number of tokens. In this paper, we introduce a novel heterogeneous memory augmentation approach for neural networks which, by introducing learnable memory tokens with attention mechanism, can effectively boost performance without huge computational overhead. Our general-purpose method can be seamlessly combined with various backbones (MLP, CNN, GNN, and Transformer) in a plug-and-play manner. We extensively evaluate our approach on various image and graph-based tasks under both in-distribution (ID) and OOD conditions and show its competitive performance against task-specific state-of-the-art methods. Code is available at \url{https://github.com/qiuzh20/HMA}.
</details>
<details>
<summary>摘要</summary>
研究表明，半 Parametric 方法，将标准神经网络与非 Parametric 组件相结合，如外部记忆模块和数据检索，在数据缺乏和外部分布（OOD）场景中特别有帮助。然而，现有的半 Parametric 方法通常依赖于独立的原始数据点，这种策略难以扩展，因为它们的计算成本高，以及当前的注意机制无法处理大量的 tokens。在这篇文章中，我们介绍了一种新的异 heterogeneous 记忆扩展方法，通过引入学习记忆тоoken和注意机制，可以有效提高性能，而无需巨大的计算负担。我们的通用方法可以与不同的基础结构（MLP、CNN、GNN、Transformer）混合使用，并且可以在插入式方式下运行。我们对各种图像和图Structured 任务进行了广泛的评估，并在ID和OOD条件下显示了与任务特有的状态UNTUK 的竞争性性能。代码可以在 \url{https://github.com/qiuzh20/HMA} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Surrogate-Active-Subspaces-for-Jump-Discontinuous-Functions"><a href="#Surrogate-Active-Subspaces-for-Jump-Discontinuous-Functions" class="headerlink" title="Surrogate Active Subspaces for Jump-Discontinuous Functions"></a>Surrogate Active Subspaces for Jump-Discontinuous Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10907">http://arxiv.org/abs/2310.10907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Wycoff</li>
<li>for: 该研究旨在探讨Surrogate模型和活动子空间在社会科学计算中的应用，特别是对于粒子模型，以及这些技术在处理离散函数时的限制。</li>
<li>methods: 该研究使用了Gaussian процеessed估计active subspace，并对离散函数进行了扩展，以便更好地理解估计中的量。数据进行了比较分析，并在Flee模型中进行了应用，以获得关于8个迁徙危机的新的发现。</li>
<li>results: 研究发现，在离散函数上使用Gaussian процеessed估计active subspace可以提供有用的信息，并且可以帮助理解模型中重要的参数。在Flee模型中，该方法提供了新的发现，对8个迁徙危机进行了分析。<details>
<summary>Abstract</summary>
Surrogate modeling and active subspaces have emerged as powerful paradigms in computational science and engineering. Porting such techniques to computational models in the social sciences brings into sharp relief their limitations in dealing with discontinuous simulators, such as Agent-Based Models, which have discrete outputs. Nevertheless, prior applied work has shown that surrogate estimates of active subspaces for such estimators can yield interesting results. But given that active subspaces are defined by way of gradients, it is not clear what quantity is being estimated when this methodology is applied to a discontinuous simulator. We begin this article by showing some pathologies that can arise when conducting such an analysis. This motivates an extension of active subspaces to discontinuous functions, clarifying what is actually being estimated in such analyses. We also conduct numerical experiments on synthetic test functions to compare Gaussian process estimates of active subspaces on continuous and discontinuous functions. Finally, we deploy our methodology on Flee, an agent-based model of refugee movement, yielding novel insights into which parameters of the simulation are most important across 8 displacement crises in Africa and the Middle East.
</details>
<details>
<summary>摘要</summary>
《代理模型和活跃子空间在计算科学和工程领域已经成为强大的趋势。将这些技术应用到社会科学计算模型上可以使其局限性得到鲜明的表现。然而，先前的应用研究表明，代理估计的活跃子空间可以得到有趣的结果。但是，由于活跃子空间是通过梯度定义的，因此不清楚什么量在这种分析中是被估计的。本文开始通过显示一些在进行这种分析时可能出现的病理来激发扩展。这些病理的出现强化了我们对离散函数的扩展。我们还对 synthetic 测试函数进行了数值实验，比较了 Gaussian 过程的活跃子空间估计在连续和离散函数上。最后，我们将我们的方法应用于 Flee，一个基于代理的难民移动模型，并得到了8个驱逐危机在非洲和中东的新的发现。》Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and widely used in other countries. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Modularity-Maximization-in-Approximation-Heuristic-and-Graph-Neural-Network-Algorithms-for-Community-Detection"><a href="#Analyzing-Modularity-Maximization-in-Approximation-Heuristic-and-Graph-Neural-Network-Algorithms-for-Community-Detection" class="headerlink" title="Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection"></a>Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10898">http://arxiv.org/abs/2310.10898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samin Aref, Mahdi Mostajabdaveh</li>
<li>for: 本研究旨在探讨不同模块化最优化算法在网络Community detection中的性能。</li>
<li>methods: 本研究使用了104个网络，包括了真实世界的例子和Synthetic图的模块结构。研究使用了10种模块最优化算法，包括8种搜索算法、2种变种的图神经网络算法和Bayan Approximation算法的多种变种。</li>
<li>results: 研究发现，大多数常用的模块性最优化算法 rarely produce an optimal partition or a partition resembling an optimal partition，即使网络具有模块结构。如果使用模块性来探讨社群，则应用近似优化算法更加合理。<details>
<summary>Abstract</summary>
Community detection, a fundamental problem in computational sciences, finds applications in various domains. Heuristics are often employed to detect communities through maximizing an objective function, modularity, over partitions of network nodes. Our research delves into the performance of different modularity maximization algorithms in achieving optimal partitions. We use 104 networks, comprising real-world instances from diverse contexts and synthetic graphs with modular structures. We analyze ten inexact modularity-based algorithms against an exact baseline which is an exact integer programming method that globally optimizes modularity. The ten algorithms analyzed include eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm. Our analysis uncovers substantial dissimilarities between the partitions obtained by most commonly used modularity-based methods and any optimal partition of the networks, as indicated by both adjusted and reduced mutual information metrics. Importantly, our results show that near-optimal partitions are often disproportionately dissimilar to any optimal partition. Taken together, our analysis points to a crucial limitation of the commonly used unguaranteed modularity-based methods for discovering communities: they rarely produce an optimal partition or a partition resembling an optimal partition even on networks with modular structures. If modularity is to be used for detecting communities, approximate optimization algorithms are recommendable for a more methodologically sound usage of modularity within its applicability limits.
</details>
<details>
<summary>摘要</summary>
We analyze ten inexact modularity-based algorithms, including eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm, against an exact baseline that globally optimizes modularity using integer programming. Our analysis reveals that the partitions obtained by most commonly used modularity-based methods are often substantially dissimilar to any optimal partition, as indicated by both adjusted and reduced mutual information metrics. Furthermore, we find that near-optimal partitions are often disproportionately dissimilar to any optimal partition.Our results suggest that the commonly used unguaranteed modularity-based methods for discovering communities are rarely able to produce an optimal partition or a partition resembling an optimal partition, even on networks with modular structures. As a result, we recommend using approximate optimization algorithms for a more methodologically sound usage of modularity within its applicability limits.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/cs.LG_2023_10_17/" data-id="clp869u0v00tbk588c2eahbvz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/eess.IV_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T09:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/eess.IV_2023_10_17/">eess.IV - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hybrid-quantum-classical-graph-neural-networks-for-tumor-classification-in-digital-pathology"><a href="#Hybrid-quantum-classical-graph-neural-networks-for-tumor-classification-in-digital-pathology" class="headerlink" title="Hybrid quantum-classical graph neural networks for tumor classification in digital pathology"></a>Hybrid quantum-classical graph neural networks for tumor classification in digital pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11353">http://arxiv.org/abs/2310.11353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anupama Ray, Dhiraj Madan, Srushti Patil, Maria Anna Rapsomaniki, Pushpak Pati</li>
<li>for: 研究用于理解疾病细胞和肿瘤微环境之间的互动，以加速治疗发现。</li>
<li>methods: 组合图解树神经网络（GNN）和量子变量分类器（VQC），以解决抗癌疾病分类任务。</li>
<li>results: hybrid量子神经网络（QNN）与状态当前的类 graph神经网络（GNN）相当，以重量精度、准确率和F1分数来衡量。 Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 随着古典机器学习和单元细胞技术的进步，我们可以更好地理解疾病细胞和肿瘤微环境之间的互动，以加速治疗发现。</li>
<li>methods: 我们采用了组合图解树神经网络（GNN）和量子变量分类器（VQC），以解决抗癌疾病分类任务。</li>
<li>results: 我们发现，hybrid量子神经网络（QNN）与状态当前的类 graph神经网络（GNN）相当，以重量精度、准确率和F1分数来衡量。 另外，我们还发现，通过幂数编码，可以压缩信息，并且在逻辑数量的级别上实现更好的性能。最后，我们发现，结合练习可以超越固定 GNN 参数，并且也略微提高了与 vanilla GNN 的性能。<details>
<summary>Abstract</summary>
Advances in classical machine learning and single-cell technologies have paved the way to understand interactions between disease cells and tumor microenvironments to accelerate therapeutic discovery. However, challenges in these machine learning methods and NP-hard problems in spatial Biology create an opportunity for quantum computing algorithms. We create a hybrid quantum-classical graph neural network (GNN) that combines GNN with a Variational Quantum Classifier (VQC) for classifying binary sub-tasks in breast cancer subtyping. We explore two variants of the same, the first with fixed pretrained GNN parameters and the second with end-to-end training of GNN+VQC. The results demonstrate that the hybrid quantum neural network (QNN) is at par with the state-of-the-art classical graph neural networks (GNN) in terms of weighted precision, recall and F1-score. We also show that by means of amplitude encoding, we can compress information in logarithmic number of qubits and attain better performance than using classical compression (which leads to information loss while keeping the number of qubits required constant in both regimes). Finally, we show that end-to-end training enables to improve over fixed GNN parameters and also slightly improves over vanilla GNN with same number of dimensions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Automatic-Coronary-Artery-Plaque-Quantification-and-CAD-RADS-Prediction-using-Mesh-Priors"><a href="#Automatic-Coronary-Artery-Plaque-Quantification-and-CAD-RADS-Prediction-using-Mesh-Priors" class="headerlink" title="Automatic Coronary Artery Plaque Quantification and CAD-RADS Prediction using Mesh Priors"></a>Automatic Coronary Artery Plaque Quantification and CAD-RADS Prediction using Mesh Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11297">http://arxiv.org/abs/2310.11297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rudolf L. M. van Herten, Nils Hampe, Richard A. P. Takx, Klaas Jan Franssen, Yining Wang, Dominika Suchá, José P. Henriques, Tim Leiner, R. Nils Planken, Ivana Išgum</li>
<li>for: 这个论文是为了评估抗塞性肺动脉疾病（CAD）的风险和治疗方案。</li>
<li>methods: 这个论文使用了直接从中心线约束的方法，将抗塞性肺动脉的血管和瘤直接推导出来，并用其进行下游任务的CAD-RADS分类。</li>
<li>results: 这个论文的结果表明，直接推导抗塞性肺动脉的血管和瘤的方法是可行的，并可以自动预测 Routinely performed CAD-RADS categorization。 lesion-wise volume intraclass correlation coefficients were 0.98, 0.79, and 0.85 for calcified, non-calcified, and total plaque volume respectively. patient-level CAD-RADS categorization achieved linearly weighted kappa（κ）of 0.75.<details>
<summary>Abstract</summary>
Coronary artery disease (CAD) remains the leading cause of death worldwide. Patients with suspected CAD undergo coronary CT angiography (CCTA) to evaluate the risk of cardiovascular events and determine the treatment. Clinical analysis of coronary arteries in CCTA comprises the identification of atherosclerotic plaque, as well as the grading of any coronary artery stenosis typically obtained through the CAD-Reporting and Data System (CAD-RADS). This requires analysis of the coronary lumen and plaque. While voxel-wise segmentation is a commonly used approach in various segmentation tasks, it does not guarantee topologically plausible shapes. To address this, in this work, we propose to directly infer surface meshes for coronary artery lumen and plaque based on a centerline prior and use it in the downstream task of CAD-RADS scoring. The method is developed and evaluated using a total of 2407 CCTA scans. Our method achieved lesion-wise volume intraclass correlation coefficients of 0.98, 0.79, and 0.85 for calcified, non-calcified, and total plaque volume respectively. Patient-level CAD-RADS categorization was evaluated on a representative hold-out test set of 300 scans, for which the achieved linearly weighted kappa ($\kappa$) was 0.75. CAD-RADS categorization on the set of 658 scans from another hospital and scanner led to a $\kappa$ of 0.71. The results demonstrate that direct inference of coronary artery meshes for lumen and plaque is feasible, and allows for the automated prediction of routinely performed CAD-RADS categorization.
</details>
<details>
<summary>摘要</summary>
coronary artery disease (CAD) 仍然是全球最主要的死亡原因。患有可能的 CAD 的患者通常会通过 coronary CT angiography (CCTA) 来评估心血管事件的风险和治疗方案。在 CCTA 中的临床分析中，需要分析 coronary arteries 的病理和病变。而在这个过程中，我们提议直接从中心线约束下直接推算 coronary artery 的血管和病变表面。这种方法可以保证血管和病变的准确性和可靠性。我们在 2407 个 CCTA 扫描数据集上开发和评估了这种方法。我们的方法在不同类型的病变中的体积涂抹相互关系系数为 0.98、0.79 和 0.85。在一个代表样本中，我们对 300 个扫描数据进行了分类，其中的 linearly weighted kappa 值为 0.75。在另一个医院和扫描机器上进行了进一步的评估，其中的 linearly weighted kappa 值为 0.71。这些结果表明，直接从中心线约束下推算 coronary artery 的血管和病变表面是可能的，并且可以自动地预测通常进行的 CAD-RADS 分类。
</details></li>
</ul>
<hr>
<h2 id="MorphFlow-Estimating-Motion-in-In-Situ-Tests-of-Concrete"><a href="#MorphFlow-Estimating-Motion-in-In-Situ-Tests-of-Concrete" class="headerlink" title="MorphFlow: Estimating Motion in In Situ Tests of Concrete"></a>MorphFlow: Estimating Motion in In Situ Tests of Concrete</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11109">http://arxiv.org/abs/2310.11109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tessa Nogatz, Claudia Redenbach, Katja Schladitz</li>
<li>for: 该论文是为了估计从时间序列3D图像中的运动而设计的。</li>
<li>methods: 该算法使用了一种基于多尺度波лет的新方法来估计运动。</li>
<li>results: 该算法可以快速和可靠地处理大规模的室内实验数据，并且可以捕捉到异常的几何变化。两个例子 validate 了该算法的性能，包括一个经典的抗热试验和一个三点弯矩试验。<details>
<summary>Abstract</summary>
We present a novel algorithm explicitly tailored to estimate motion from time series of 3D images of concrete. Such volumetric images are usually acquired by Computed Tomography and can contain for example in situ tests, or more complex procedures like self-healing. Our algorithm is specifically designed to tackle the challenge of large scale in situ investigations of concrete. That means it cannot only cope with big images, but also with discontinuous displacement fields that often occur in in situ tests of concrete. We show the superior performance of our algorithm, especially regarding plausibility and time efficient processing. Core of the algorithm is a novel multiscale representation based on morphological wavelets. We use two examples for validation: A classical in situ test on refractory concrete and and a three point bending test on normal concrete. We show that for both applications structural changes like crack initiation can be already found at low scales -- a central achievement of our algorithm.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法，专门用于从3D图像序列中估算动态。这些三维图像通常由计算 Tomatoes取得，可以包含例如在 situ测试或更复杂的过程，如自适应修复。我们的算法特地设计用于解决大规模在 situ调查咨 concrete 中的挑战。这意味着它不仅可以处理大图像，还可以处理不连续的变位场的问题，这经常发生在 concrete 中的 in situ 测试中。我们展示了我们的算法在真实性和时间效率两个方面的优秀表现。我们的算法核心是一种新的多尺度表示方法，基于 morphological wavelets。我们使用了两个例子进行验证：一个经典的 in situ 测试和一个三点弯曲测试。我们发现，对于两个应用程序，结构变化，如裂隙开始，可以在低尺度上找到，这是我们算法的中心成就。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Clustering-Material-Decomposition-Aided-by-Empirical-Spectral-Correction-for-High-Resolution-Photon-Counting-Detectors-in-Micro-CT"><a href="#Iterative-Clustering-Material-Decomposition-Aided-by-Empirical-Spectral-Correction-for-High-Resolution-Photon-Counting-Detectors-in-Micro-CT" class="headerlink" title="Iterative Clustering Material Decomposition Aided by Empirical Spectral Correction for High-Resolution Photon-Counting Detectors in Micro-CT"></a>Iterative Clustering Material Decomposition Aided by Empirical Spectral Correction for High-Resolution Photon-Counting Detectors in Micro-CT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10913">http://arxiv.org/abs/2310.10913</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan C. R. Luna, Mini Das<br>for: 这项研究旨在提高计算tomography（CT）成像的精度，特别是在用 photon counting detectors（PCDs）进行多能量投射的情况下。methods: 该研究使用了实用的 инструменталь和测量策略，包括Iterative Clustering Material Decomposition（ICMD），以实现在spectral micro CT中量化多种材料的分离。results: 实验结果表明， combining spectral correction和高维数据归一化可以提高分离精度和降低噪声，并可以分解多于三种材料，包括混合物和K-edge材料。<details>
<summary>Abstract</summary>
Photon counting detectors (PCDs) offer promising advancements in computed tomography (CT) imaging by enabling the quantification and 3D imaging of contrast agents and tissue types through multi-energy projections. However, the accuracy of these decomposition methods hinges on precise composite spectral attenuation values that one must reconstruct from spectral micro CT. Factors such as surface defects, local temperature, signal amplification, and impurity levels can cause variations in detector efficiency between pixels, leading to significant quantitative errors. In addition, some inaccuracies such as the charge-sharing effects in PCDs are amplified with a high Z sensor material and also with a smaller detector pixels that are preferred for micro CT. In this work, we propose a comprehensive approach that combines practical instrumentation and measurement strategies leading to the quantitation of multiple materials within an object in a spectral micro CT with a photon counting detector. Our Iterative Clustering Material Decomposition (ICMD) includes an empirical method for detector spectral response corrections, cluster analysis and multi-step iterative material decomposition. Utilizing a CdTe-1mm Medipix detector with a 55$\mu$m pitch, we demonstrate the quantitatively accurate decomposition of several materials in a phantom study, where the sample includes mixtures of material, soft material and K-edge materials. We also show an example of biological sample imaging and separating three distinct types of tissue in mouse: muscle, fat and bone. Our experimental results show that the combination of spectral correction and high-dimensional data clustering enhances decomposition accuracy and reduces noise in micro CT. This ICMD allows for quantitative separation of more than three materials including mixtures and also effectively separates multi-contrast agents.
</details>
<details>
<summary>摘要</summary>
吸收计数器（PCD）在计算tomography（CT）成像中提供了有前途的改进，使得可以量化和三维成像各种 контраст物质和组织类型通过多能量投影。然而，这些分解方法的准确性取决于重建的复合spectralattenuation值，这些值可以从spectral micro CT中提取。因为表面缺陷、地方温度、信号增强和杂质水平等因素会导致每个像素的探测效率存在差异，这会导致重要的量化错误。此外，一些不精准的效应，如charge-sharing效应，会在高Z探测器材料和小像素尺寸下被增强。在这种情况下，我们提出了一种涵盖实用仪器和测量策略的全面方法，以实现spectral micro CT中多种材料的量化。我们的迭代归一化材料分解（ICMD）包括了实际方法、集群分析和多步迭代材料分解。使用CdTe-1mm Medipix探测器，我们在一个phantom研究中展示了高精度的材料分解，包括混合物、软物和K-edge材料。此外，我们还展示了一个生物样本的成像和分离三种不同的组织类型，包括肌肉、脂肪和骨。我们的实验结果表明，将spectral correction和高维数据归一化相结合可以提高分解精度和减少微CT的噪声。ICMD可以量化超过三种材料，包括混合物，并有效地分离多种对比剂。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/eess.IV_2023_10_17/" data-id="clp869u7v01bsk588d4h2bzyv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/17/eess.SP_2023_10_17/" class="article-date">
  <time datetime="2023-10-17T08:00:00.000Z" itemprop="datePublished">2023-10-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/17/eess.SP_2023_10_17/">eess.SP - 2023-10-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="WaveFlex-A-Smart-Surface-for-Private-CBRS-Wireless-Cellular-Networks"><a href="#WaveFlex-A-Smart-Surface-for-Private-CBRS-Wireless-Cellular-Networks" class="headerlink" title="WaveFlex: A Smart Surface for Private CBRS Wireless Cellular Networks"></a>WaveFlex: A Smart Surface for Private CBRS Wireless Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11551">http://arxiv.org/abs/2310.11551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Yi, Kun Woo Cho, Yaxiong Xie, Kyle Jamieson</li>
<li>for: 增强private LTE&#x2F;5G网络在公民广播服务频段下的共享执照框架下运行。</li>
<li>methods: 利用多频多时动态环境下的频率多样性和时间动态性，实现独立于基站和移动用户的自适应响应。</li>
<li>results: 在实际办公室enario中，单个小型基站的均值Signal-to-Noise Ratio提高8.50 dB，单个小型基站的均值吞吐量提高4.36 Mbps，四个小型基站的均值吞吐量提高3.19 Mbps。<details>
<summary>Abstract</summary>
We present the design and implementation of WaveFlex, the first smart surface that enhances Private LTE/5G networks operating under the shared-license framework in the Citizens Broadband Radio Service frequency band. WaveFlex works in the presence of frequency diversity: multiple nearby base stations operating on different frequencies, as dictated by a Spectrum Access System coordinator. It also handles time dynamism: due to the dynamic sharing rules of the band, base stations occasionally switch channels, especially when priority users enter the network. Finally, WaveFlex operates independently of the network itself, not requiring access to nor modification of the base station or mobile users, yet it remain compliant with and effective on prevailing cellular protocols. We have designed and fabricated WaveFlex on a custom multi-layer PCB, software defined radio-based network monitor, and supporting control software and hardware. Our experimental evaluation benchmarks an operational Private LTE network running at full line rate. Results demonstrate an 8.50 dB average SNR gain, and an average throughput gain of 4.36 Mbps for a single small cell, and 3.19 Mbps for four small cells, in a realistic indoor office scenario.
</details>
<details>
<summary>摘要</summary>
我们介绍了waveflex的设计和实现，这是首个在公民宽频服务频率带下的智能表面，用于增强共享许可的LTE/5G网络。waveflex在频率多样性和时间动态性下工作，包括多个附近基站在不同频率上运行，由 спект域访问系统协调员指定。此外，waveflex不需要访问或修改基站或移动用户，却仍然遵循现有的 celullar协议。我们设计了waveflex于自定义多层PCB、基于Software Defined Radio的网络监测器和相应的控制软件和硬件。我们的实验评估表明，一个实际的专用LTE网络在全线速度下运行，得到了8.50 dB的平均噪声比提高和4.36 Mbps的平均吞吐量提高，以及3.19 Mbps的平均吞吐量提高，在一个真实的办公室enario中。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-and-Channel-Estimation-by-Exploiting-Dual-Timescales-for-Delay-Doppler-Alignment-Modulation"><a href="#Integrated-Sensing-and-Channel-Estimation-by-Exploiting-Dual-Timescales-for-Delay-Doppler-Alignment-Modulation" class="headerlink" title="Integrated Sensing and Channel Estimation by Exploiting Dual Timescales for Delay-Doppler Alignment Modulation"></a>Integrated Sensing and Channel Estimation by Exploiting Dual Timescales for Delay-Doppler Alignment Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11326">http://arxiv.org/abs/2310.11326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Xiao, Yong Zeng, Fuxi Wen, Zaichen Zhang, Derrick Wing Kwan Ng</li>
<li>for: This paper proposes a novel ISAC framework that leverages the recently proposed delay-Doppler alignment modulation (DDAM) technique to improve the performance of integrated sensing and communication (ISAC) systems.</li>
<li>methods: The proposed framework uses a novel algorithm called adaptive simultaneously orthogonal matching pursuit with support refinement (ASOMP-SR) for joint environment sensing and PSI estimation, and analyzes the performance of DDAM with imperfectly sensed PSI.</li>
<li>results: Simulation results show that the proposed DDAM-based ISAC can achieve superior spectral efficiency and a reduced peak-to-average power ratio (PAPR) compared to standard orthogonal frequency division multiplexing (OFDM).Here’s the simplified Chinese text:</li>
<li>for: 这篇论文提出了一种基于延迟-Doppler匹配调制（DDAM）技术的新一代 интеGRATED sensing和通信（ISAC）系统。</li>
<li>methods: 该系统使用了一种新的算法 called adaptive simultaneously orthogonal matching pursuit with support refinement（ASOMP-SR）进行环境感知和PSI估计，并分析了受到不准确感知PSI的DDAM性能。</li>
<li>results: 实验结果表明，提出的DDAM-based ISAC可以在spectral efficiency和peak-to-average power ratio（PAPR）方面具有superior性能，比普通的orthogonal frequency division multiplexing（OFDM）更高。<details>
<summary>Abstract</summary>
For integrated sensing and communication (ISAC) systems, the channel information essential for communication and sensing tasks fluctuates across different timescales. Specifically, wireless sensing primarily focuses on acquiring path state information (PSI) (e.g., delay, angle, and Doppler) of individual multi-path components to sense the environment, which usually evolves much more slowly than the composite channel state information (CSI) required for communications. Typically, the CSI is approximately unchanged during the channel coherence time, which characterizes the statistical properties of wireless communication channels. However, this concept is less appropriate for describing that for wireless sensing. To this end, in this paper, we introduce a new timescale to study the variation of the PSI from a channel geometric perspective, termed path invariant time, during which the PSI largely remains constant. Our analysis indicates that the path invariant time considerably exceeds the channel coherence time. Thus, capitalizing on these dual timescales of the wireless channel, in this paper, we propose a novel ISAC framework exploiting the recently proposed delay-Doppler alignment modulation (DDAM) technique. Different from most existing studies on DDAM that assume the availability of perfect PSI, in this work, we propose a novel algorithm, termed as adaptive simultaneously orthogonal matching pursuit with support refinement (ASOMP-SR), for joint environment sensing and PSI estimation. We also analyze the performance of DDAM with imperfectly sensed PSI.Simulation results unveil that the proposed DDAM-based ISAC can achieve superior spectral efficiency and a reduced peak-to-average power ratio (PAPR) compared to standard orthogonal frequency division multiplexing (OFDM).
</details>
<details>
<summary>摘要</summary>
for integrated sensing and communication (ISAC) systems, the channel information that is essential for communication and sensing tasks changes over different time scales. Specifically, wireless sensing primarily focuses on acquiring path state information (PSI) (e.g., delay, angle, and Doppler) of individual multi-path components to sense the environment, which usually evolves much more slowly than the composite channel state information (CSI) required for communications. Typically, the CSI is approximately unchanged during the channel coherence time, which characterizes the statistical properties of wireless communication channels. However, this concept is less appropriate for describing that for wireless sensing. To this end, in this paper, we introduce a new timescale to study the variation of the PSI from a channel geometric perspective, termed path invariant time, during which the PSI largely remains constant. Our analysis indicates that the path invariant time considerably exceeds the channel coherence time. Thus, capitalizing on these dual timescales of the wireless channel, in this paper, we propose a novel ISAC framework exploiting the recently proposed delay-Doppler alignment modulation (DDAM) technique. Different from most existing studies on DDAM that assume the availability of perfect PSI, in this work, we propose a novel algorithm, termed as adaptive simultaneously orthogonal matching pursuit with support refinement (ASOMP-SR), for joint environment sensing and PSI estimation. We also analyze the performance of DDAM with imperfectly sensed PSI.Simulation results unveil that the proposed DDAM-based ISAC can achieve superior spectral efficiency and a reduced peak-to-average power ratio (PAPR) compared to standard orthogonal frequency division multiplexing (OFDM).Here's the word-for-word translation in Simplified Chinese:for 集成感知通信 (ISAC) 系统，通信和感知任务中的通道信息变化在不同的时间尺度上。特别是无线感知主要关注于获取路径状态信息 (PSI)（例如延迟、角度和Doppler）个体多 path 组件来感知环境，这通常比通信通道的Statistical properties 更慢地发展。通常情况下，通道准确性时间内，通信通道的 CSI 保持相对不变。但这个概念对于无线感知来说 less appropriate。为此，在这篇论文中，我们引入了一个新的时间尺度，用于研究 wireless channel 的 PSI 变化，并将其称为 path invariant time， durante el cual la PSI se mantiene prácticamente constante。我们的分析表明，path invariant time 远大于通道准确性时间。因此，基于这两个时间尺度的 wireless channel，在这篇论文中，我们提出了一种新的 ISAC 框架，利用最近提出的 delay-Doppler alignment modulation (DDAM) 技术。与大多数现有研究中的 DDAM 假设完美 PSI 可用，在这种工作中，我们提出了一种新的算法，称为 adaptive simultaneously orthogonal matching pursuit with support refinement (ASOMP-SR)，用于joint 环境感知和 PSI 估计。我们还分析了 DDAM 中的 PSI 估计不准确情况。Simulation results 显示，我们的提议的 DDAM-based ISAC 可以在 spectral efficiency 和 peak-to-average power ratio (PAPR) 两个方面获得更高的性能，相比标准 orthogonal frequency division multiplexing (OFDM)。
</details></li>
</ul>
<hr>
<h2 id="Imaging-of-nonlinear-materials-via-the-Monotonicity-Principle"><a href="#Imaging-of-nonlinear-materials-via-the-Monotonicity-Principle" class="headerlink" title="Imaging of nonlinear materials via the Monotonicity Principle"></a>Imaging of nonlinear materials via the Monotonicity Principle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11234">http://arxiv.org/abs/2310.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vincenzo Mottola, Antonio Corbo Esposito, Gianpaolo Piscitelli, Antonello Tamburrino</li>
<li>for: 本研究旨在解决非线性材料下的 inverse problems，具体是 magnetostatic permeability tomography 问题。</li>
<li>methods: 本研究使用了 Monotonicity Principle 的扩展，开发了首个实时反射方法。</li>
<li>results: 研究提供了一些初步结果，并给出了一些扩展的数值示例。<details>
<summary>Abstract</summary>
The topic of inverse problems, related to Maxwell's equations, in the presence of nonlinear materials is quite new in literature. The lack of contributions in this area can be ascribed to the significant challenges that such problems pose. Retrieving the spatial behaviour of some unknown physical property, starting from boundary measurements, is a nonlinear and highly ill-posed problem even in the presence of linear materials. And the complexity exponentially grows when the focus is on nonlinear material properties. Recently, the Monotonicity Principle has been extended to nonlinear materials under very general assumptions. Starting from the theoretical background given by this extension, we develop a first real-time inversion method for the inverse obstacle problem in the presence of nonlinear materials. The Monotonicity Principle is the foundation of a class of non-iterative algorithms for tomography of linear materials. It has been successfully applied to various problems, governed by different PDEs. In the linear case, MP based inversion methods ensure excellent performances and compatibility with real-time applications. We focus on problems governed by elliptical PDEs and, as an example of application, we treat the Magnetostatic Permeability Tomography problem, in which the aim is to retrieve the spatial behaviour of magnetic permeability through boundary measurements in DC operations. In this paper, we provide some preliminary results giving the foundation of our method and extended numerical examples.
</details>
<details>
<summary>摘要</summary>
topic of inverse problems related to Maxwell's equations in the presence of nonlinear materials is quite new in literature. lack of contributions in this area can be ascribed to the significant challenges that such problems pose. Retrieving the spatial behavior of some unknown physical property starting from boundary measurements is a nonlinear and highly ill-posed problem even in the presence of linear materials. And the complexity exponentially grows when the focus is on nonlinear material properties. Recently, the Monotonicity Principle has been extended to nonlinear materials under very general assumptions. Starting from the theoretical background given by this extension, we develop a first real-time inversion method for the inverse obstacle problem in the presence of nonlinear materials. Monotonicity Principle is the foundation of a class of non-iterative algorithms for tomography of linear materials. It has been successfully applied to various problems, governed by different PDEs. In the linear case, MP-based inversion methods ensure excellent performances and compatibility with real-time applications. We focus on problems governed by elliptical PDEs and, as an example of application, we treat the Magnetostatic Permeability Tomography problem, in which the aim is to retrieve the spatial behavior of magnetic permeability through boundary measurements in DC operations. In this paper, we provide some preliminary results giving the foundation of our method and extended numerical examples.
</details></li>
</ul>
<hr>
<h2 id="Complex-Number-Assignment-in-the-Topology-Method-for-Heartbeat-Interval-Estimation-Using-Millimeter-Wave-Radar"><a href="#Complex-Number-Assignment-in-the-Topology-Method-for-Heartbeat-Interval-Estimation-Using-Millimeter-Wave-Radar" class="headerlink" title="Complex Number Assignment in the Topology Method for Heartbeat Interval Estimation Using Millimeter-Wave Radar"></a>Complex Number Assignment in the Topology Method for Heartbeat Interval Estimation Using Millimeter-Wave Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11149">http://arxiv.org/abs/2310.11149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuji Tanaka, Kimitaka Sumi, Itsuki Iwata, Takuya Sakamoto</li>
<li>for: 用于高精度心跳间隔估计 millimeter 波射频信号</li>
<li>methods: 使用皮肤运动波峰值特征点抽象、赋予每个特征点复杂数字</li>
<li>results: 验证了使用简化的波峰值特征点预测优化复杂数字分配方法的有效性，并使用公共数据集进行了验证。<details>
<summary>Abstract</summary>
The topology method is an algorithm for accurate estimation of instantaneous heartbeat intervals using millimeter-wave radar signals. In this model, feature points are extracted from the skin displacement waveforms generated by heartbeats and a complex number is assigned to each feature point. However, these numbers have been assigned empirically and without solid justification. This study used a simplified model of displacement waveforms to predict the optimal choice of the complex number assignments to feature points corresponding to inflection points, and the validity of these numbers was confirmed using analysis of a publicly available dataset.
</details>
<details>
<summary>摘要</summary>
“扁平方法”是一种用于精确计算心跳间隔的毫米波激光信号中的算法。在这个模型中，从心跳所导致皮肤变形波形中提取特征点，然后将每个特征点分配到复数中。但是这些复数的分配是基于实践和无对Solid的说明。这个研究使用简化的变形波形来预测最佳的复数分配，并使用公共可用数据集进行验证。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Resource-Allocation-for-UAV-Based-Cognitive-NOMA-Networks-An-Active-Inference-Approach"><a href="#Intelligent-Resource-Allocation-for-UAV-Based-Cognitive-NOMA-Networks-An-Active-Inference-Approach" class="headerlink" title="Intelligent Resource Allocation for UAV-Based Cognitive NOMA Networks: An Active Inference Approach"></a>Intelligent Resource Allocation for UAV-Based Cognitive NOMA Networks: An Active Inference Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11070">http://arxiv.org/abs/2310.11070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Obite, Ali Krayani, Atm S. Alam, Lucio Marcenaro, Arumugam Nallanathan, Carlo Regazzoni<br>for:This paper aims to improve the adaptive resource allocation and decision-making of future wireless networks, specifically in the context of uplink UAV-based cognitive NOMA networks.methods:The proposed approach uses an active inference-based learning framework, rooted in cognitive neuroscience, to solve the complex problem of joint subchannel and power allocation. This involves creating a training dataset using random or iterative methods, training a mobile UAV offline to learn a generative model of discrete subchannels and continuous power allocation, and using this model for online inference.results:The proposed approach is validated through numerical simulations, which show efficient performance compared to suboptimal baseline schemes. The approach is able to adapt to non-stationary environments and improve the cumulative sum rate by jointly optimizing the subchannel and power allocation based on the UAV’s mobility at each time step.<details>
<summary>Abstract</summary>
Future wireless networks will need to improve adaptive resource allocation and decision-making to handle the increasing number of intelligent devices. Unmanned aerial vehicles (UAVs) are being explored for their potential in real-time decision-making. Moreover, cognitive non-orthogonal multiple access (Cognitive-NOMA) is envisioned as a remedy to address spectrum scarcity and enable massive connectivity. This paper investigates the design of joint subchannel and power allocation in an uplink UAV-based cognitive NOMA network. We aim to maximize the cumulative sum rate by jointly optimizing the subchannel and power allocation based on the UAV's mobility at each time step. This is often formulated as an optimization problem with random variables. However, conventional optimization algorithms normally introduce significant complexity, and machine learning methods often rely on large but partially representative datasets to build solution models, assuming stationary testing data. Consequently, inference strategies for non stationary events are often overlooked. In this study, we introduce a novel active inference-based learning approach, rooted in cognitive neuroscience, to solve this complex problem. The framework involves creating a training dataset using random or iterative methods to find suboptimal resource allocations. This dataset trains a mobile UAV offline, enabling it to learn a generative model of discrete subchannels and continuous power allocation. The UAV then uses this model for online inference. The method incrementally derives new generative models from training data by identifying dynamic equilibrium conditions between required actions and variables, represented within a unique dynamic Bayesian network. The proposed approach is validated through numerical simulations, showing efficient performance compared to suboptimal baseline schemes.
</details>
<details>
<summary>摘要</summary>
未来无线网络将需要改进适应性资源分配和决策，以满足智能设备的增加。无人机（UAV）正被研究，以其实时决策的潜在优势。此外，认知非对称多接入（Cognitive-NOMA）被视为spectrum scarcity和大规模连接问题的解决方案。本文研究了基于无人机的上行UAV认知多接入网络的共同子频率和功率分配的设计。我们希望通过在每个时间步骤中并行优化子频率和功率分配，以最大化总带宽率。这经常被формализова为随机变量的优化问题。然而，常见的优化算法通常会引入显著的复杂性，而机器学习方法通常需要大量但部分代表性的数据来建立解决方案模型，假设测试数据是静止的。因此，对非静态事件的推理策略通常被忽略。在本研究中，我们提出了一种新的活动推理学习方法，基于认知神经科学，解决这个复杂的问题。该框架包括使用随机或迭代方法创建训练数据集，该数据集用于在线推理。无人机使用该模型在线进行推理，并在训练数据中逐渐 derivation of new generative models from training data by identifying dynamic equilibrium conditions between required actions and variables, represented within a unique dynamic Bayesian network. The proposed approach is validated through numerical simulations, showing efficient performance compared to suboptimal baseline schemes.
</details></li>
</ul>
<hr>
<h2 id="Aerial-Aided-mmWave-VANETs-Using-NOMA-Performance-Analysis-Comparison-and-Insights"><a href="#Aerial-Aided-mmWave-VANETs-Using-NOMA-Performance-Analysis-Comparison-and-Insights" class="headerlink" title="Aerial-Aided mmWave VANETs Using NOMA: Performance Analysis, Comparison, and Insights"></a>Aerial-Aided mmWave VANETs Using NOMA: Performance Analysis, Comparison, and Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11068">http://arxiv.org/abs/2310.11068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Abu Zaid, Baha Eddine Youcef Belmekki, Mohamed-Slim Alouini</li>
<li>for: 这 paper 的目的是研究在协同交通网络 (VANET) 中使用缔结的飞行平台 (NTFP) 来解决城市化导致的问题。</li>
<li>methods: 这 paper 使用 Stochastic Geometry 工具来 derive 缔结平台的停机概率和可以达到的速率表达。</li>
<li>results: 研究结果显示，当 NTFP 作为中继器时，它们在较大的传输距离上表现更好于 traditional roadside units (RSUs)，但是在短距离上，RSUs 表现更好。此外，使用非对称访问 (NOMA) 可以提高spectrum 效率，并且在 millimeter-wave (mmWave) 频率上使用 сектор化扫描模型可以提高数据速率。<details>
<summary>Abstract</summary>
In this paper, we propose the integration of tethered flying platforms in cooperative vehicular ad hoc networks (VANETs) to alleviate the problems of rapid urbanization. In this context, we study the performance of VANETs by deriving approximate outage probability and average achievable rate expressions using tools from stochastic geometry. We compare between the usage of networked tethered flying platforms (NTFPs) and traditional roadside units (RSUs). On the other hand, the rapid increase of smart devices in vehicles and the upcoming urban air mobility (UAM) vision will congest the spectrum and require increased data rates. Hence, we use non-orthogonal multiple access (NOMA) to improve spectral efficiency and compare its performance to orthogonal access schemes. Furthermore, we utilize millimeter-wave (mmWave) frequencies for high data rates and implement a sectored beamforming model. We extensively study the system using three transmission schemes: direct, relay, and hybrid transmission. The results show that when acting as relays, NTFPs outperform RSUs for larger distances between the transmitting and the receiving vehicles, while RSUs outperform NTFPs for short distances. However, NTFPs are the best solution when acting as a source. Moreover, we find that, in most cases, direct transmission is preferred to achieve a high rate compared to other schemes. Finally, the results are summarized in two tables that provide insights into connecting VANETs by selecting the most suitable platform and type of communication for a given set of parameters, configurations, and requirements.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了在合作式自适应网络（VANET）中 integrate 固定飞行平台（NTFP）以解决城市化导致的问题。在这个上下文中，我们研究了VANET的性能，通过Stochastic Geometry工具 derive approximate outage probability 和 average achievable rate 表达。我们比较了使用网络化固定飞行平台（NTFP）和传统路边单元（RSU）。而随着智能设备的增加和未来城市空中交通（UAM）的出现，将导致频率受到压力，需要增加数据速率。因此，我们使用非对称多接入（NOMA）提高频率效率，并与对称访问方案进行比较。此外，我们使用毫米波频率（mmWave）频率获得高数据速率，并实施 сектор化扫描模型。我们广泛研究了系统，使用三种传输方案：直接传输、重复传输和混合传输。结果表明，当NTFP作为中继器时，NTFP在较远的传输和接收车辆之间表现更好，而RSU在短距离之间表现更好。然而，NTFP在源位置时是最佳解决方案。此外，我们发现，在大多数情况下，直接传输是以高速度相比其他方案更好。最后，结果分表两个表格，提供了关于连接VANET的最佳平台和通信方式的准确信息，以便根据不同的参数、配置和需求选择最适合的方案。
</details></li>
</ul>
<hr>
<h2 id="A-Tutorial-on-Near-Field-XL-MIMO-Communications-Towards-6G"><a href="#A-Tutorial-on-Near-Field-XL-MIMO-Communications-Towards-6G" class="headerlink" title="A Tutorial on Near-Field XL-MIMO Communications Towards 6G"></a>A Tutorial on Near-Field XL-MIMO Communications Towards 6G</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11044">http://arxiv.org/abs/2310.11044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haiquan Lu, Yong Zeng, Changsheng You, Yu Han, Jiayi Zhang, Zhe Wang, Zhenjun Dong, Shi Jin, Cheng-Xiang Wang, Tao Jiang, Xiaohu You, Rui Zhang</li>
<li>for: 这篇论文主要旨在为6G移动通信网络的巨大多输入多输出技术（XL-MIMO）提供全面的教程概述，以帮助解决近场通信频率模型、性能分析、通道估计和实际实施中的挑战。</li>
<li>methods: 本论文使用了基于近场模型的XL-MIMO通信技术，包括非均匀球波（NUSW）和空间非站点性的近场模型，以及相关的性能分析和通道估计方法。</li>
<li>results: 本论文通过对XL-MIMO技术的近场模型和性能分析，提出了新的信号噪响比例法则、焊焊范围模式、可达性和度量（DoF）等。此外，论文还详细介绍了各种XL-MIMO设计问题，如近场ibeam代码库、焊焊训练、通道估计和延迟对齐变换（DAM）传输。<details>
<summary>Abstract</summary>
Extremely large-scale multiple-input multiple-output (XL-MIMO) is a promising technology for the sixth-generation (6G) mobile communication networks. By significantly boosting the antenna number or size to at least an order of magnitude beyond current massive MIMO systems, XL-MIMO is expected to unprecedentedly enhance the spectral efficiency and spatial resolution for wireless communication. The evolution from massive MIMO to XL-MIMO is not simply an increase in the array size, but faces new design challenges, in terms of near-field channel modelling, performance analysis, channel estimation, and practical implementation. In this article, we give a comprehensive tutorial overview on near-field XL-MIMO communications, aiming to provide useful guidance for tackling the above challenges. First, the basic near-field modelling for XL-MIMO is established, by considering the new characteristics of non-uniform spherical wave (NUSW) and spatial non-stationarity. Next, based on the near-field modelling, the performance analysis of XL-MIMO is presented, including the near-field signal-to-noise ratio (SNR) scaling laws, beam focusing pattern, achievable rate, and degrees-of-freedom (DoF). Furthermore, various XL-MIMO design issues such as near-field beam codebook, beam training, channel estimation, and delay alignment modulation (DAM) transmission are elaborated. Finally, we point out promising directions to inspire future research on near-field XL-MIMO communications.
</details>
<details>
<summary>摘要</summary>
“极大规模多输入多输出（XL-MIMO）技术是6G移动通信网络的未来技术之一。它通过增加天线数或大小，至少一个量级超过当前庞大MIMO系统，可以无 precedent 地提高spectral efficiency和空间分辨率，以提高无线通信的性能。从庞大MIMO到XL-MIMO的演化不仅是天线数或大小的增加，而且面临新的设计挑战，包括近场通道模型、性能分析、通道估计和实践实现。本文提供了XL-MIMO近场通信的完整教程详细介绍，以便对这些挑战进行有用的指导。首先，我们建立了XL-MIMO近场模型，考虑了新的非均匀球波（NUSW）和空间非站点性。然后，基于近场模型，我们提供了XL-MIMO性能分析，包括近场信噪比（SNR）扩展律、扫描 Pattern、可达率和度量（DoF）。此外，我们还详细介绍了XL-MIMO设计问题，如近场天线代码库、天线训练、通道估计和延迟对齐模ulation（DAM）传输。最后，我们指出了未来研究XL-MIMO近场通信的可能的方向。”
</details></li>
</ul>
<hr>
<h2 id="Channel-Autocorrelation-Estimation-for-IRS-Aided-Wireless-Communications-Based-on-Power-Measurements"><a href="#Channel-Autocorrelation-Estimation-for-IRS-Aided-Wireless-Communications-Based-on-Power-Measurements" class="headerlink" title="Channel Autocorrelation Estimation for IRS-Aided Wireless Communications Based on Power Measurements"></a>Channel Autocorrelation Estimation for IRS-Aided Wireless Communications Based on Power Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11038">http://arxiv.org/abs/2310.11038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ge Yan, Lipeng Zhu, Rui Zhang</li>
<li>for: 增强无线通信系统的性能，通过可编程的信号反射。</li>
<li>methods: 基于接收信号电平的探测和相关矩阵约束问题的解决。</li>
<li>results: 验证了新的通道估计算法的有效性，以及基于估计的IRS投射设计。<details>
<summary>Abstract</summary>
Intelligent reflecting surface (IRS) can bring significant performance enhancement for wireless communication systems by reconfiguring wireless channels via passive signal reflection. However, such performance improvement generally relies on the knowledge of channel state information (CSI) for IRS-associated links. Prior IRS channel estimation strategies mainly estimate IRS-cascaded channels based on the excessive pilot signals received at the users/base station (BS) with time-varying IRS reflections, which, however, are not compatible with the existing channel training/estimation protocol for cellular networks. To address this issue, we propose in this paper a new channel estimation scheme for IRS-assisted communication systems based on the received signal power measured at the user, which is practically attainable without the need of changing the current protocol. Specifically, due to the lack of signal phase information in power measurements, the autocorrelation matrix of the BS-IRS-user cascaded channel is estimated by solving equivalent matrix-rank-minimization problems. Simulation results are provided to verify the effectiveness of the proposed channel estimation algorithm as well as the IRS passive reflection design based on the estimated channel autocorrelation matrix.
</details>
<details>
<summary>摘要</summary>
智能反射表面（IRS）可以带来无线通信系统的性能提升，通过通过pasive signal reflection重新配置无线通道。然而，这种性能提升通常需要IRS相关链路的通道状态信息（CSI）的知识。先前的IRS通道估计策略主要基于用户/基站（BS）接收到的过剩的射频信号来估计IRS-堆叠的通道，这些信号在时间变化IRS反射后是不可靠的。为解决这个问题，我们在这篇论文中提出了一种新的通道估计方案 дляIRS协助通信系统，基于用户接收到的信号功率。具体来说，由于射频信号的相位信息不可获得，我们通过解决相当于矩阵约等减少问题来估计BS-IRS-用户堆叠通道的自相关矩阵。我们提供了估计算法的实验结果，以证明提案的有效性以及基于估计的IRS pasive反射设计。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Efficiency-and-Energy-Efficiency-of-Variable-Length-XP-HARQ"><a href="#Spectral-Efficiency-and-Energy-Efficiency-of-Variable-Length-XP-HARQ" class="headerlink" title="Spectral-Efficiency and Energy-Efficiency of Variable-Length XP-HARQ"></a>Spectral-Efficiency and Energy-Efficiency of Variable-Length XP-HARQ</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10964">http://arxiv.org/abs/2310.10964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahui Feng, Zheng Shi, Yaru Fu, Hong Wang, Guanghua Yang, Shaodan Ma<br>for:* 提高通信的spectral efficiency (SE)和能效率 (EE)methods:* 提出变量长度跨包 hybrid automatic repeat request (VL-XP-HARQ) 技术* 使用 Dinkelbach 变换和successive convex approximation (SCA) 等方法进行优化results:* 提高 SE 和 EE 的Upper bound* 可以通过power allocation来最大化 EE 并保证出错率的要求<details>
<summary>Abstract</summary>
A variable-length cross-packet hybrid automatic repeat request (VL-XP-HARQ) is proposed to boost the spectral efficiency (SE) and the energy efficiency (EE) of communications. The SE is firstly derived in terms of the outage probabilities, with which the SE is proved to be upper bounded by the ergodic capacity (EC). Moreover, to facilitate the maximization of the SE, the asymptotic outage probability is obtained at high signal-to-noise ratio (SNR), with which the SE is maximized by properly choosing the number of new information bits while guaranteeing outage requirement. By applying Dinkelbach's transform, the fractional objective function is transformed into a subtraction form, which can be decomposed into multiple sub-problems through alternating optimization. By noticing that the asymptotic outage probability is a convex function, each sub-problem can be easily relaxed to a convex problem by adopting successive convex approximation (SCA). Besides, the EE of VL-XP-HARQ is also investigated. An upper bound of the EE is found and proved to be attainable. Furthermore, by aiming at maximizing the EE via power allocation while confining outage within a certain constraint, the methods to the maximization of SE are invoked to solve the similar fractional problem. Finally, numerical results are presented for verification.
</details>
<details>
<summary>摘要</summary>
一种变长跨包自动重复请求（VL-XP-HARQ）被提议，以提高通信的spectral efficiency（SE）和能效率（EE）。首先，SE是通过出现概率来 derivation，并证明其Upper bounded by ergodic capacity（EC）。此外，为了最大化SE，高信号噪声比（SNR）下的 asymptotic outage probability 被获得，并通过选择合适的新信息位数来 garantuee outage requirement。通过应用Dinkelbach的变换，目标函数被转换成一个减法表示，可以通过 alternate optimization 分解成多个子问题。由于 asymptotic outage probability 是一个 convex function，每个子问题可以通过Successive convex approximation（SCA）的方式放松到一个convex problem。此外，EE 的Upper bound 也被查找并证明可达。进一步，通过对力分配来最大化EE，并将出现约束在一定范围内，使用SE 的最大化方法来解决相似的分数问题。最后，通过numerical results 进行验证。
</details></li>
</ul>
<hr>
<h2 id="On-the-Performance-of-Near-Field-ISAC"><a href="#On-the-Performance-of-Near-Field-ISAC" class="headerlink" title="On the Performance of Near-Field ISAC"></a>On the Performance of Near-Field ISAC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10917">http://arxiv.org/abs/2310.10917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boqun Zhao, Chongjun Ouyang, Xingqi Zhang, Yuanwei Liu<br>for:* 这个论文是为了研究Integrated Sensing and Communications（ISAC）在靠近场区域中的性能，并提出了一个更加准确的通道模型。methods:* 该论文使用了一种基于效果天线的评估模型，并分析了下降和上升方向的感知和通信性能。results:* 论文显示，随着天线数量的增加，提出的模型的感知率和通信率都会 converges to常数，而传统的TCMs则会无限增长；* ISAC在靠近场区域中可以 achieve 更广泛的速率区域，比传统的频分S&amp;C更好。<details>
<summary>Abstract</summary>
The technical trends for the next-generation wireless network significantly extend the near-field region, necessitating a reevaluation for the performance of integrated sensing and communications (ISAC) to account for the effects introduced by the near field. In this paper, a near-field ISAC framework is proposed with a more accurate channel model than the three conventional models (TCMs): uniform plane wave, uniform spherical wave, and non-uniform spherical wave, in which the effective aperture of the antenna is considered. Based on the proposed model, sensing and communication (S&C) performance in both downlink and uplink scenarios are analyzed. For the downlink case, three distinct designs are studied: the communications-centric (C-C) design, the sensing-centric (S-C) design, and the Pareto optimal design. Regarding the uplink case, the C-C design, the S-C design and the time-sharing strategy are considered. Within each design, sensing rates (SRs) and communication rates (CRs) are derived. To gain further insights, high signal-to-noise ratio slopes and rate scaling laws concerning the number of antennas are also examined. Finally, the attainable SR-CR regions of the near-field ISAC are characterized. Numerical results reveal that 1) as the number of antennas grows, the SRs and CRs of the proposed model converges to constants, while those of the TCMs increase unboundedly; 2) ISAC achieves a more extensive rate region than the conventional frequency-division S&C in both downlink and uplink cases.
</details>
<details>
<summary>摘要</summary>
Next-generation无线网络的技术趋势明显扩展了近场区域，需要重新评估Integrated Sensing and Communications（ISAC）性能，考虑近场效应的影响。本文提出了一种更准确的近场ISAC框架，包括Antenna的有效覆盖面。基于该模型，对下行和上行场景进行了敏感测量和通信性能的分析。在下行场景中，研究了三种设计：通信中心（C-C）设计、探测中心（S-C）设计和Pareto优化设计。在上行场景中，考虑了C-C设计、S-C设计和时间分享策略。对每种设计，计算了探测率（SR）和通信率（CR）。为了更深入地了解，也研究了高信号噪听比斜率和antenna数量下的速率扩展法则。最后，near-field ISAC可达的SR-CR区域的可行性被Characterized。numerical results indicate that: 1) antenna数量增加时，提posed模型中的SR和CR与TCMs相比， converge to constants,而TCMs中的SR和CR无限增长; 2) ISAC在下行和上行场景中都可以获得更广泛的速率区域，比传统频分S&C更好。
</details></li>
</ul>
<hr>
<h2 id="Reuse-Kernels-or-Activations-A-Flexible-Dataflow-for-Low-latency-Spectral-CNN-Acceleration"><a href="#Reuse-Kernels-or-Activations-A-Flexible-Dataflow-for-Low-latency-Spectral-CNN-Acceleration" class="headerlink" title="Reuse Kernels or Activations? A Flexible Dataflow for Low-latency Spectral CNN Acceleration"></a>Reuse Kernels or Activations? A Flexible Dataflow for Low-latency Spectral CNN Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10902">http://arxiv.org/abs/2310.10902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Niu, Rajgopal Kannan, Ajitesh Srivastava, Viktor Prasanna</li>
<li>for: 提高卷积神经网络（CNN）的计算效率和响应时间，解决spectral-domain CNNs的“kernel explosion”问题。</li>
<li>methods: 分析卷积层的带宽-存储费用贸易关系，确定层之间的通信瓶颈，并提出数据流水平优化策略和约束计划算法来最优化数据重复使用和减少外部通信。</li>
<li>results: 在一个现代FPGA平台上，我们的设计可以减少数据传输量42%，使DSP资源利用率达到90%，并实现VGG16模型的推理延迟为9毫秒，比基eline状态则的延迟为68毫秒更快。<details>
<summary>Abstract</summary>
Spectral-domain CNNs have been shown to be more efficient than traditional spatial CNNs in terms of reducing computation complexity. However they come with a `kernel explosion' problem that, even after compression (pruning), imposes a high memory burden and off-chip bandwidth requirement for kernel access. This creates a performance gap between the potential acceleration offered by compression and actual FPGA implementation performance, especially for low-latency CNN inference. In this paper, we develop a principled approach to overcoming this performance gap and designing a low-latency, low-bandwidth, spectral sparse CNN accelerator on FPGAs. First, we analyze the bandwidth-storage tradeoff of sparse convolutional layers and locate communication bottlenecks. We then develop a dataflow for flexibly optimizing data reuse in different layers to minimize off-chip communication. Finally, we propose a novel scheduling algorithm to optimally schedule the on-chip memory access of multiple sparse kernels and minimize read conflicts. On a state-of-the-art FPGA platform, our design reduces data transfers by 42\% with DSP utilization up to 90\% and achieves inference latency of 9 ms for VGG16, compared to the baseline state-of-the-art latency of 68 ms.
</details>
<details>
<summary>摘要</summary>
In this paper, we develop a principled approach to overcoming this performance gap and designing a low-latency, low-bandwidth, spectral sparse CNN accelerator on FPGAs. First, we analyze the bandwidth-storage tradeoff of sparse convolutional layers and locate communication bottlenecks. We then develop a dataflow for flexibly optimizing data reuse in different layers to minimize off-chip communication. Finally, we propose a novel scheduling algorithm to optimally schedule the on-chip memory access of multiple sparse kernels and minimize read conflicts.Our design reduces data transfers by 42% with DSP utilization up to 90% and achieves inference latency of 9 ms for VGG16, compared to the baseline state-of-the-art latency of 68 ms.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/17/eess.SP_2023_10_17/" data-id="clp869u9s01g7k588feje5y4r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.SD_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T15:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.SD_2023_10_16/">cs.SD - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generation-or-Replication-Auscultating-Audio-Latent-Diffusion-Models"><a href="#Generation-or-Replication-Auscultating-Audio-Latent-Diffusion-Models" class="headerlink" title="Generation or Replication: Auscultating Audio Latent Diffusion Models"></a>Generation or Replication: Auscultating Audio Latent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10604">http://arxiv.org/abs/2310.10604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Bralios, Gordon Wichern, François G. Germain, Zexu Pan, Sameer Khurana, Chiori Hori, Jonathan Le Roux</li>
<li>for: 这个研究旨在理解音频扩散模型如何生成真实的声音clip，以及这种技术在音频处理方面的应用前提下，它们是否能够具备生成高质量声音clip的能力。</li>
<li>methods: 这个研究使用文本到音频扩散模型，并系统地分析这些模型在不同训练集大小下的记忆行为。同时，研究还评估了不同的检索指标，以确定哪些指标更能够捕捉训练数据的记忆。</li>
<li>results: 研究发现，使用mel spectrogram Similarity来评估模型的记忆行为是更加稳定和可靠的，而learned embedding vectors则更容易受到训练数据的干扰。此外，研究还发现AudioCaps数据库中存在大量的复制声音clip。<details>
<summary>Abstract</summary>
The introduction of audio latent diffusion models possessing the ability to generate realistic sound clips on demand from a text description has the potential to revolutionize how we work with audio. In this work, we make an initial attempt at understanding the inner workings of audio latent diffusion models by investigating how their audio outputs compare with the training data, similar to how a doctor auscultates a patient by listening to the sounds of their organs. Using text-to-audio latent diffusion models trained on the AudioCaps dataset, we systematically analyze memorization behavior as a function of training set size. We also evaluate different retrieval metrics for evidence of training data memorization, finding the similarity between mel spectrograms to be more robust in detecting matches than learned embedding vectors. In the process of analyzing memorization in audio latent diffusion models, we also discover a large amount of duplicated audio clips within the AudioCaps database.
</details>
<details>
<summary>摘要</summary>
文本描述生成真实的声音clip的能力可能会革命化我们如何处理音频。在这个工作中，我们初步地理解音频干扰模型的内部工作，通过对它们的声音输出与训练数据进行比较，类似于医生 auscultates 病人的器官声音。使用基于 AudioCaps 数据集的文本-声音干扰模型，我们系统地分析了训练集大小的影响，并评估不同的检索指标，发现mel спектрограм相似性更加稳定地检测匹配。在分析声音干扰模型的 memorization 行为的过程中，我们还发现了 AudioCaps 数据库中的大量重复的声音clip。Note: "Simplified Chinese" is a translation of the text into Standard Chinese, which is the official language of China. "Traditional Chinese" is a different writing system used in Taiwan and some other countries.
</details></li>
</ul>
<hr>
<h2 id="BeatDance-A-Beat-Based-Model-Agnostic-Contrastive-Learning-Framework-for-Music-Dance-Retrieval"><a href="#BeatDance-A-Beat-Based-Model-Agnostic-Contrastive-Learning-Framework-for-Music-Dance-Retrieval" class="headerlink" title="BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework for Music-Dance Retrieval"></a>BeatDance: A Beat-Based Model-Agnostic Contrastive Learning Framework for Music-Dance Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10300">http://arxiv.org/abs/2310.10300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaixing Yang, Xukun Zhou, Xulong Tang, Ran Diao, Hongyan Liu, Jun He, Zhaoxin Fan</li>
<li>For: The paper is written for improving dance-music retrieval performance by utilizing the alignment between music beats and dance movements.* Methods: The proposed method, BeatDance, incorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat Blender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval performance.* Results: The experimental results on the Music-Dance (MD) dataset demonstrate the superiority of the proposed method over existing baselines, achieving state-of-the-art performance.Here’s the simplified Chinese version of the three key points:* For: 提高舞蹈音乐 Retrieval 性能，利用音乐拍和舞蹈动作的匹配。* Methods: 提出了 BeatDance 模型无关对比学习框架，包括 Beat-Aware Music-Dance InfoExtractor、Trans-Temporal Beat Blender 和 Beat-Enhanced Hubness Reducer。* Results: 在 Music-Dance（MD）数据集上，实验结果表明提议方法比基eline表现更出色，实现了状态级表现。<details>
<summary>Abstract</summary>
Dance and music are closely related forms of expression, with mutual retrieval between dance videos and music being a fundamental task in various fields like education, art, and sports. However, existing methods often suffer from unnatural generation effects or fail to fully explore the correlation between music and dance. To overcome these challenges, we propose BeatDance, a novel beat-based model-agnostic contrastive learning framework. BeatDance incorporates a Beat-Aware Music-Dance InfoExtractor, a Trans-Temporal Beat Blender, and a Beat-Enhanced Hubness Reducer to improve dance-music retrieval performance by utilizing the alignment between music beats and dance movements. We also introduce the Music-Dance (MD) dataset, a large-scale collection of over 10,000 music-dance video pairs for training and testing. Experimental results on the MD dataset demonstrate the superiority of our method over existing baselines, achieving state-of-the-art performance. The code and dataset will be made public available upon acceptance.
</details>
<details>
<summary>摘要</summary>
文本：舞蹈和音乐是密切相关的表达形式，它们之间存在着很强的相互关联。然而，现有的方法 часто会导致不自然的生成效果，或者完全不利用音乐和舞蹈之间的相互关系。为了解决这些挑战，我们提出了 BeatDance，一种新的 beat-based 模型无关的对比学习框架。 BeatDance 包括一个 Beat-Aware Music-Dance 信息抽取器、一个 Trans-Temporal Beat Blender 和一个 Beat-Enhanced Hubness Reducer，以便通过音乐 beat 和舞蹈动作的协调来提高舞蹈-音乐 retrieve 性能。我们还提出了 Music-Dance（MD）数据集，一个大规模的音乐-舞蹈视频对集，用于训练和测试。实验结果表明，我们的方法在 MD 数据集上表现出优于现有基eline，实现了状态计算机。代码和数据集将在接受后公开。翻译结果：文本：舞蹈和音乐是密切相关的表达形式，它们之间存在着很强的相互关联。然而，现有的方法常常会导致不自然的生成效果，或者完全不利用音乐和舞蹈之间的相互关系。为了解决这些挑战，我们提出了 BeatDance，一种新的 beat-based 模型无关的对比学习框架。 BeatDance 包括一个 Beat-Aware Music-Dance 信息抽取器、一个 Trans-Temporal Beat Blender 和一个 Beat-Enhanced Hubness Reducer，以便通过音乐 beat 和舞蹈动作的协调来提高舞蹈-音乐 retrieve 性能。我们还提出了 Music-Dance（MD）数据集，一个大规模的音乐-舞蹈视频对集，用于训练和测试。实验结果表明，我们的方法在 MD 数据集上表现出优于现有基eline，实现了状态计算机。代码和数据集将在接受后公开。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Audio-Emotion-and-Intent-Recognition-with-Large-Pre-Trained-Models-and-Bayesian-Inference"><a href="#Advancing-Audio-Emotion-and-Intent-Recognition-with-Large-Pre-Trained-Models-and-Bayesian-Inference" class="headerlink" title="Advancing Audio Emotion and Intent Recognition with Large Pre-Trained Models and Bayesian Inference"></a>Advancing Audio Emotion and Intent Recognition with Large Pre-Trained Models and Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10179">http://arxiv.org/abs/2310.10179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dejan Porjazovski, Yaroslav Getman, Tamás Grósz, Mikko Kurimo</li>
<li>for: This paper is written for the ACM Multimedia Computational Paralinguistics Challenge, addressing the Requests and Emotion Share tasks.</li>
<li>methods: The paper employs large pre-trained models and explores audio-only and hybrid solutions leveraging audio and text modalities. The authors also introduce a Bayesian layer as an alternative to the standard linear output layer.</li>
<li>results: The empirical results consistently show the superiority of the hybrid approaches over the audio-only models, with the multimodal fusion approach achieving an 85.4% UAR on HC-Requests and 60.2% on HC-Complaints, and the ensemble model for the Emotion Share task yielding the best rho value of .614. Additionally, the Bayesian wav2vec2 approach allows for easily building ensembles with usable confidence values instead of overconfident posterior probabilities.Here’s the Chinese translation of the three key points:</li>
<li>for: 这篇论文是为了参加 ACM Multimedia Computational Paralinguistics Challenge 的 Requests 和 Emotion Share 任务而写的。</li>
<li>methods: 这篇论文使用了大型预训模型，并 explore 了听音和文本模式的混合解决方案。作者还介绍了一种 Bayesian 层作为标准线性输出层的替代方案。</li>
<li>results: 实验结果表明，混合方案比听音模型更加有优势，并且 multimodal fusion 方法在 HC-Requests 上 achieved 85.4% UAR 和 HC-Complaints 上 achieved 60.2%。此外，作者还介绍了一种 Bayesian wav2vec2 方法，该方法可以轻松地构建集成模型，只需要 fine-tune 一个模型。此外，该方法还可以提供可信度值 instead of 常见的过度信息 posterior probabilities。<details>
<summary>Abstract</summary>
Large pre-trained models are essential in paralinguistic systems, demonstrating effectiveness in tasks like emotion recognition and stuttering detection. In this paper, we employ large pre-trained models for the ACM Multimedia Computational Paralinguistics Challenge, addressing the Requests and Emotion Share tasks. We explore audio-only and hybrid solutions leveraging audio and text modalities. Our empirical results consistently show the superiority of the hybrid approaches over the audio-only models. Moreover, we introduce a Bayesian layer as an alternative to the standard linear output layer. The multimodal fusion approach achieves an 85.4% UAR on HC-Requests and 60.2% on HC-Complaints. The ensemble model for the Emotion Share task yields the best rho value of .614. The Bayesian wav2vec2 approach, explored in this study, allows us to easily build ensembles, at the cost of fine-tuning only one model. Moreover, we can have usable confidence values instead of the usual overconfident posterior probabilities.
</details>
<details>
<summary>摘要</summary>
大型预训模型在para语言系统中扮演着关键角色，在情感识别和偏声检测等任务中显示出了效iveness。在这篇论文中，我们使用大型预训模型参加ACM Multimedia Computational Paralinguistics Challenge的请求和情感分享任务。我们研究了基于音频和文本modalities的混合解决方案，并对audio-only和混合方案进行了比较。我们的实验结果表明，混合方案在HC-Requests和HC-Complaints任务上具有显著的优势。此外，我们还引入了一种 bayesian层作为标准线性输出层的替代方案。我们的Multimodal混合方法在HC-Requests上 achieve 85.4% UAR，在HC-Complaints上 achieve 60.2%。此外，我们还提出了一种 bayesian wav2vec2方法，可以轻松地建立ensemble，只需要 Fine-tuning一个模型。此外，我们可以获得可信度值 instead of  usual overconfident posterior probabilities。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Speech-Enhancement-and-Separation-with-a-Unified-Deep-Neural-Network-for-Single-Dual-Talker-Scenarios"><a href="#Real-time-Speech-Enhancement-and-Separation-with-a-Unified-Deep-Neural-Network-for-Single-Dual-Talker-Scenarios" class="headerlink" title="Real-time Speech Enhancement and Separation with a Unified Deep Neural Network for Single&#x2F;Dual Talker Scenarios"></a>Real-time Speech Enhancement and Separation with a Unified Deep Neural Network for Single&#x2F;Dual Talker Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10026">http://arxiv.org/abs/2310.10026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kashyap Patel, Anton Kovalyov, Issa Panahi</li>
<li>for: 提出了一种实用的方法，利用实时深度学习模型，在单个或双个对话者的输入混合中进行Speech增强和分离。</li>
<li>methods: 使用了时域Signal-to-distortion比（SI-SDR）作为训练指标，并引入了一种轻量级的对话者重叠检测（SOD）模块，通过直接在分离后的面板上操作，而不是直接操作原始混合音频，简化了检测任务。</li>
<li>results: 实验结果表明，提出的训练方法超过了现有的解决方案，并且SOD模块具有高准确性。<details>
<summary>Abstract</summary>
This paper introduces a practical approach for leveraging a real-time deep learning model to alternate between speech enhancement and joint speech enhancement and separation depending on whether the input mixture contains one or two active speakers. Scale-invariant signal-to-distortion ratio (SI-SDR) has shown to be a highly effective training measure in time-domain speech separation. However, the SI-SDR metric is ill-defined for zero-energy target signals, which is a problem when training a speech separation model using utterances with varying numbers of talkers. Unlike existing solutions that focus on modifying the loss function to accommodate zero-energy target signals, the proposed approach circumvents this problem by training the model to extract speech on both its output channels regardless if the input is a single or dual-talker mixture. A lightweight speaker overlap detection (SOD) module is also introduced to differentiate between single and dual-talker segments in real-time. The proposed module takes advantage of the new formulation by operating directly on the separated masks, given by the separation model, instead of the original mixture, thus effectively simplifying the detection task. Experimental results show that the proposed training approach outperforms existing solutions, and the SOD module exhibits high accuracy.
</details>
<details>
<summary>摘要</summary>
Unlike existing solutions that modify the loss function to accommodate zero-energy target signals, the proposed approach trains the model to extract speech on both its output channels regardless of whether the input is a single or dual-talker mixture. Additionally, a lightweight speaker overlap detection (SOD) module is introduced to differentiate between single and dual-talker segments in real-time. The SOD module operates directly on the separated masks, provided by the separation model, rather than the original mixture, making the detection task simpler.Experimental results show that the proposed training approach outperforms existing solutions, and the SOD module exhibits high accuracy.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.SD_2023_10_16/" data-id="clp869u3m010wk5889ejt1ad9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.CV_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T13:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.CV_2023_10_16/">cs.CV - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Filling-the-Holes-on-3D-Heritage-Object-Surface-based-on-Automatic-Segmentation-Algorithm"><a href="#Filling-the-Holes-on-3D-Heritage-Object-Surface-based-on-Automatic-Segmentation-Algorithm" class="headerlink" title="Filling the Holes on 3D Heritage Object Surface based on Automatic Segmentation Algorithm"></a>Filling the Holes on 3D Heritage Object Surface based on Automatic Segmentation Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10875">http://arxiv.org/abs/2310.10875</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sinh Van Nguyen, Son Thanh Le, Minh Khai Tran, Le Thanh Sach</li>
<li>for: 这个论文的目的是提出一种改进的3D物体表面填充方法，以提高计算机图形学、图像处理和计算机视觉等领域中3D对象的重建和处理的精度。</li>
<li>methods: 该论文使用的方法包括计算几何学和深度学习模型，以及基于图像处理的机器学习算法。</li>
<li>results: 相比现有方法，该论文提出的方法可以更高精度地重建3D对象，并且可以适用于多种3D数据类型，包括点云数据和三角形网格数据。<details>
<summary>Abstract</summary>
Reconstructing and processing the 3D objects are popular activities in the research field of computer graphics, image processing and computer vision. The 3D objects are processed based on the methods like geometric modeling, a branch of applied mathematics and computational geometry, or the machine learning algorithms based on image processing. The computation of geometrical objects includes processing the curves and surfaces, subdivision, simplification, meshing, holes filling, reconstructing, and refining the 3D surface objects on both point cloud data and triangular mesh. While the machine learning methods are developed using deep learning models. With the support of 3D laser scan devices and Lidar techniques, the obtained dataset is close to original shape of the real objects. Besides, the photography and its application based on the modern techniques in recent years help us collect data and process the 3D models more precise. This article proposes an improved method for filling holes on the 3D object surface based on an automatic segmentation. Instead of filling the hole directly as the existing methods, we now subdivide the hole before filling it. The hole is first determined and segmented automatically based on computation of its local curvature. It is then filled on each part of the hole to match its local curvature shape. The method can work on both 3D point cloud surfaces and triangular mesh surface. Comparing to the state of the art methods, our proposed method obtained higher accuracy of the reconstructed 3D objects.
</details>
<details>
<summary>摘要</summary>
Computer graphics、图像处理和计算机视觉领域的研究中，重建和处理3D对象是非常流行的活动。这些3D对象通常通过几何模型化或基于图像处理的机器学习算法进行处理。计算几何对象包括处理曲线和表面、分割、简化、网格化、填充洞和改进3D表面对象的点云数据和三角形网格。而机器学习方法则是基于深度学习模型。通过3D激光扫描设备和激光技术获得的数据，我们可以更加准确地重建真实对象的形状。此外，现代技术的应用也有助于我们更加精准地收集数据和处理3D模型。本文提出了一种改进的洞填充方法，通过自动分割洞而不是直接填充洞像现有方法。首先，我们使用计算当地曲线的方法自动分割洞。然后，我们在每个洞部分填充其所对应的本地弯曲形状。这种方法可以在3D点云表面和三角形网格表面上进行应用。与现有方法相比，我们的提议方法能够获得更高的3D对象重建精度。
</details></li>
</ul>
<hr>
<h2 id="Approximation-properties-of-slice-matching-operators"><a href="#Approximation-properties-of-slice-matching-operators" class="headerlink" title="Approximation properties of slice-matching operators"></a>Approximation properties of slice-matching operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10869">http://arxiv.org/abs/2310.10869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiying Li, Caroline Moosmueller</li>
<li>for: This paper is written for the purpose of exploring approximation properties of iterative slice-matching procedures for transferring a source measure to a target measure, particularly in high dimensions.</li>
<li>methods: The paper uses slice-matching operators, which depend on the source and target measures and slicing directions, to examine the approximation properties of iterative slice-matching schemes.</li>
<li>results: The paper demonstrates invariance and equivariance properties of the slice-matching operator with respect to the source and target measures, respectively, and establishes error bounds for approximating the target measure using one step of the slice-matching scheme. Additionally, the paper investigates connections to affine registration problems and extensions to the invariance and equivariance properties of the slice-matching operator.<details>
<summary>Abstract</summary>
Iterative slice-matching procedures are efficient schemes for transferring a source measure to a target measure, especially in high dimensions. These schemes have been successfully used in applications such as color transfer and shape retrieval, and are guaranteed to converge under regularity assumptions. In this paper, we explore approximation properties related to a single step of such iterative schemes by examining an associated slice-matching operator, depending on a source measure, a target measure, and slicing directions. In particular, we demonstrate an invariance property with respect to the source measure, an equivariance property with respect to the target measure, and Lipschitz continuity concerning the slicing directions. We furthermore establish error bounds corresponding to approximating the target measure by one step of the slice-matching scheme and characterize situations in which the slice-matching operator recovers the optimal transport map between two measures. We also investigate connections to affine registration problems with respect to (sliced) Wasserstein distances. These connections can be also be viewed as extensions to the invariance and equivariance properties of the slice-matching operator and illustrate the extent to which slice-matching schemes incorporate affine effects.
</details>
<details>
<summary>摘要</summary>
iterative slice-matching 算法是高维中高效的源度量至目标度量转移方案，尤其在应用中如颜色传输和形状检索中得到了成功。这些算法在Regularity assumptions下是确定的收敛的。在这篇论文中，我们研究了单步iterative slice-matching算法的approximation Properties，包括一个相关的slice-matching运算符，它取决于源度量、目标度量和切割方向。我们证明了对源度量的不变性、对目标度量的对称性和切割方向的 lipschitz连续性。我们还确定了一个基于单步slice-matching算法的目标度量的错误 bound，并 characterize了在 slice-matching算法中可以重建两个度量之间的优化运输图的情况。此外，我们还 investigate了基于水星距离的 affine registration problem 的连接，这些连接可以被视为 slice-matching算法中的 affine 效应的扩展。
</details></li>
</ul>
<hr>
<h2 id="The-Invisible-Map-Visual-Inertial-SLAM-with-Fiducial-Markers-for-Smartphone-based-Indoor-Navigation"><a href="#The-Invisible-Map-Visual-Inertial-SLAM-with-Fiducial-Markers-for-Smartphone-based-Indoor-Navigation" class="headerlink" title="The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation"></a>The Invisible Map: Visual-Inertial SLAM with Fiducial Markers for Smartphone-based Indoor Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10862">http://arxiv.org/abs/2310.10862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Ruvolo, Ayush Chakraborty, Rucha Dave, Richard Li, Duncan Mazza, Xierui Shen, Raiyan Siddique, Krishna Suresh</li>
<li>for: 创建大尺寸、易于导航的3D地图，使用主流智能手机。</li>
<li>methods: 将3D地图问题定义为图像SLAM问题，并估计环境中的建筑物标志（指标）和可导航路径（手机姿态）。</li>
<li>results: 系统可以创建准确的3D地图。此外，我们还提出了选择映射超参数的精细技术，以适应新环境。<details>
<summary>Abstract</summary>
We present a system for creating building-scale, easily navigable 3D maps using mainstream smartphones. In our approach, we formulate the 3D-mapping problem as an instance of Graph SLAM and infer the position of both building landmarks (fiducial markers) and navigable paths through the environment (phone poses). Our results demonstrate the system's ability to create accurate 3D maps. Further, we highlight the importance of careful selection of mapping hyperparameters and provide a novel technique for tuning these hyperparameters to adapt our algorithm to new environments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于主流智能手机的建筑尺度级可探索3D地图创建系统。我们将3D地图问题定义为Instance of Graph SLAM，并通过约束建筑标记（ fiducial markers）和环境中可行路径（手机姿态）来计算位置。我们的结果表明系统可以创建准确的3D地图。此外，我们强调了选择映射超参数的重要性，并提供了一种新的参数调整技术，以适应新环境。
</details></li>
</ul>
<hr>
<h2 id="SoybeanNet-Transformer-Based-Convolutional-Neural-Network-for-Soybean-Pod-Counting-from-Unmanned-Aerial-Vehicle-UAV-Images"><a href="#SoybeanNet-Transformer-Based-Convolutional-Neural-Network-for-Soybean-Pod-Counting-from-Unmanned-Aerial-Vehicle-UAV-Images" class="headerlink" title="SoybeanNet: Transformer-Based Convolutional Neural Network for Soybean Pod Counting from Unmanned Aerial Vehicle (UAV) Images"></a>SoybeanNet: Transformer-Based Convolutional Neural Network for Soybean Pod Counting from Unmanned Aerial Vehicle (UAV) Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10861">http://arxiv.org/abs/2310.10861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiajiali04/soybean-pod-counting-from-uav-images">https://github.com/jiajiali04/soybean-pod-counting-from-uav-images</a></li>
<li>paper_authors: Jiajia Li, Raju Thada Magar, Dong Chen, Feng Lin, Dechun Wang, Xiang Yin, Weichao Zhuang, Zhaojian Li</li>
<li>for: 这个论文的目的是提高豫豢的生产效率，并使用无人机图像来实现豫豢的果实计数。</li>
<li>methods: 这个论文使用了一种新的点基 counting网络，叫做SoybeanNet，使用了强大的变换器核心来同时进行豫豢的果实计数和定位。</li>
<li>results: 该论文在使用实际的无人机图像进行测试时，与五种现有方法进行比较，并取得了84.51%的计数精度。<details>
<summary>Abstract</summary>
Soybeans are a critical source of food, protein and oil, and thus have received extensive research aimed at enhancing their yield, refining cultivation practices, and advancing soybean breeding techniques. Within this context, soybean pod counting plays an essential role in understanding and optimizing production. Despite recent advancements, the development of a robust pod-counting algorithm capable of performing effectively in real-field conditions remains a significant challenge This paper presents a pioneering work of accurate soybean pod counting utilizing unmanned aerial vehicle (UAV) images captured from actual soybean fields in Michigan, USA. Specifically, this paper presents SoybeanNet, a novel point-based counting network that harnesses powerful transformer backbones for simultaneous soybean pod counting and localization with high accuracy. In addition, a new dataset of UAV-acquired images for soybean pod counting was created and open-sourced, consisting of 113 drone images with more than 260k manually annotated soybean pods captured under natural lighting conditions. Through comprehensive evaluations, SoybeanNet demonstrated superior performance over five state-of-the-art approaches when tested on the collected images. Remarkably, SoybeanNet achieved a counting accuracy of $84.51\%$ when tested on the testing dataset, attesting to its efficacy in real-world scenarios. The publication also provides both the source code (\url{https://github.com/JiajiaLi04/Soybean-Pod-Counting-from-UAV-Images}) and the labeled soybean dataset (\url{https://www.kaggle.com/datasets/jiajiali/uav-based-soybean-pod-images}), offering a valuable resource for future research endeavors in soybean pod counting and related fields.
</details>
<details>
<summary>摘要</summary>
soybeans是一种重要的食品、蛋白和油源，因此它们在提高产量、改善栽培方法和进步杂交技术方面 receiving extensive research。在这个 контексте中，豇豆果 counting plays an essential role in understanding and optimizing production. Despite recent advancements, the development of a robust pod-counting algorithm capable of performing effectively in real-field conditions remains a significant challenge.这篇文章提出了一项突破性的豇豆果 counting方法，使用了来自美国密歇根州actual soybean fields的无人机图像。Specifically, this paper presents SoybeanNet, a novel point-based counting network that harnesses powerful transformer backbones for simultaneous soybean pod counting and localization with high accuracy. In addition, a new dataset of UAV-acquired images for soybean pod counting was created and open-sourced, consisting of 113 drone images with more than 260k manually annotated soybean pods captured under natural lighting conditions. Through comprehensive evaluations, SoybeanNet demonstrated superior performance over five state-of-the-art approaches when tested on the collected images. Remarkably, SoybeanNet achieved a counting accuracy of 84.51% when tested on the testing dataset, attesting to its efficacy in real-world scenarios. The publication also provides both the source code (https://github.com/JiajiaLi04/Soybean-Pod-Counting-from-UAV-Images) and the labeled soybean dataset (https://www.kaggle.com/datasets/jiajiali/uav-based-soybean-pod-images), offering a valuable resource for future research endeavors in soybean pod counting and related fields.
</details></li>
</ul>
<hr>
<h2 id="Provable-Probabilistic-Imaging-using-Score-Based-Generative-Priors"><a href="#Provable-Probabilistic-Imaging-using-Score-Based-Generative-Priors" class="headerlink" title="Provable Probabilistic Imaging using Score-Based Generative Priors"></a>Provable Probabilistic Imaging using Score-Based Generative Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10835">http://arxiv.org/abs/2310.10835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Sun, Zihui Wu, Yifan Chen, Berthy T. Feng, Katherine L. Bouman</li>
<li>for: 这篇论文旨在提出一种可靠地估计高质量图像并同时评估其不确定性的权重函数架构。</li>
<li>methods: 该论文提出了一种基于Monte Carlo（MC）的插入式权重函数架构（PMC），可以同时捕捉高质量图像重建和不确定性评估。具体来说，该论文引入了两种PMC算法，可以视为传统插入式质量函数（PnP）和杂化正则化（RED）算法的排除样本分布 аналоги。</li>
<li>results: 对多个代表性的逆问题进行实验，结果表明PMCAlgorithm可以显著提高图像重建质量和高精度不确定性评估。<details>
<summary>Abstract</summary>
Estimating high-quality images while also quantifying their uncertainty are two desired features in an image reconstruction algorithm for solving ill-posed inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as a principled framework for characterizing the space of possible solutions to a general inverse problem. PMC is able to incorporate expressive score-based generative priors for high-quality image reconstruction while also performing uncertainty quantification via posterior sampling. In particular, we introduce two PMC algorithms which can be viewed as the sampling analogues of the traditional plug-and-play priors (PnP) and regularization by denoising (RED) algorithms. We also establish a theoretical analysis for characterizing the convergence of the PMC algorithms. Our analysis provides non-asymptotic stationarity guarantees for both algorithms, even in the presence of non-log-concave likelihoods and imperfect score networks. We demonstrate the performance of the PMC algorithms on multiple representative inverse problems with both linear and nonlinear forward models. Experimental results show that PMC significantly improves reconstruction quality and enables high-fidelity uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>解决具有不整合性 inverse problem 的算法中，估计高质量图像并同时量化其不确定性是两个愿景。在这篇论文中，我们提出了插入式 Monte Carlo (PMC) 作为一种原理性的框架，用于描述解决一般 inverse problem 中可能的解空间。PMC 能够integrate expressive score-based生成模型，以实现高质量图像重建和不确定性量化。具体来说，我们介绍了两种 PMC 算法，可以视为传统的插入式 priors (PnP) 和 regularization by denoising (RED) 算法的抽象。我们还进行了理论分析，用于Characterizing PMC 算法的收敛性。我们的分析提供了不对称站点保证，即使likelihood 不是几何凹形的情况下，PMC 算法仍然能够收敛。我们在多个代表性的 inverse problem 中进行了实验，结果表明，PMC 可以显著提高重建质量和实现高精度的不确定性量化。
</details></li>
</ul>
<hr>
<h2 id="Vision-and-Language-Navigation-in-the-Real-World-via-Online-Visual-Language-Mapping"><a href="#Vision-and-Language-Navigation-in-the-Real-World-via-Online-Visual-Language-Mapping" class="headerlink" title="Vision and Language Navigation in the Real World via Online Visual Language Mapping"></a>Vision and Language Navigation in the Real World via Online Visual Language Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10822">http://arxiv.org/abs/2310.10822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengguang Xu, Hieu T. Nguyen, Christopher Amato, Lawson L. S. Wong</li>
<li>for: 提高移动机器人在未看过的环境中的导航效率，使其能够根据自然语言指令进行导航。</li>
<li>methods: 提出了一个新的导航框架，包括四个关键组件：一个基于LLMs的指令解析器、一个在线视觉语言映射器、一个基于语言索引的地方定位器和一个基于DD-PPO的本地控制器。</li>
<li>results: 在一个未看过的实验室环境中测试了该框架，无需调整，significantly outperformed了现有的VLN基线。<details>
<summary>Abstract</summary>
Navigating in unseen environments is crucial for mobile robots. Enhancing them with the ability to follow instructions in natural language will further improve navigation efficiency in unseen cases. However, state-of-the-art (SOTA) vision-and-language navigation (VLN) methods are mainly evaluated in simulation, neglecting the complex and noisy real world. Directly transferring SOTA navigation policies trained in simulation to the real world is challenging due to the visual domain gap and the absence of prior knowledge about unseen environments. In this work, we propose a novel navigation framework to address the VLN task in the real world. Utilizing the powerful foundation models, the proposed framework includes four key components: (1) an LLMs-based instruction parser that converts the language instruction into a sequence of pre-defined macro-action descriptions, (2) an online visual-language mapper that builds a real-time visual-language map to maintain a spatial and semantic understanding of the unseen environment, (3) a language indexing-based localizer that grounds each macro-action description into a waypoint location on the map, and (4) a DD-PPO-based local controller that predicts the action. We evaluate the proposed pipeline on an Interbotix LoCoBot WX250 in an unseen lab environment. Without any fine-tuning, our pipeline significantly outperforms the SOTA VLN baseline in the real world.
</details>
<details>
<summary>摘要</summary>
naviigating 无法看到环境是机器人 navigation 的关键。增强机器人能够遵循自然语言指令，将会进一步提高无法看到环境中的导航效率。然而，当前的VLN方法（state-of-the-art）主要在模拟环境中进行评估，忽略了实际世界的复杂和噪音。直接将模拟环境中训练的VLN策略传输到实际世界是困难的，因为视觉领域之间的差异和未知环境中的先验知识缺乏。在这种情况下，我们提出了一种新的导航框架，用于解决VLN任务在实际世界中。该框架包括四个关键组件：1. LLMs基础模型based instruction parser，将自然语言指令转换为一系列预定的macro-action描述。2. 在线视觉语言映射，实时建立视觉语言地图，以维护未知环境的空间和Semantic理解。3. 基于语言索引的本地化器，将每个macro-action描述映射到地图上的坐标位置。4. DD-PPO基于本地控制器，预测动作。我们在一个未知的实际环境中使用Interbotix LoCoBot WX250进行评估，无需精细调整，我们的管道在实际世界中显著超越了当前VLN基线。
</details></li>
</ul>
<hr>
<h2 id="Convolutional-Neural-Network-Model-for-Diabetic-Retinopathy-Feature-Extraction-and-Classification"><a href="#Convolutional-Neural-Network-Model-for-Diabetic-Retinopathy-Feature-Extraction-and-Classification" class="headerlink" title="Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification"></a>Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10806">http://arxiv.org/abs/2310.10806</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s21sharan/cnn_dr_detection">https://github.com/s21sharan/cnn_dr_detection</a></li>
<li>paper_authors: Sharan Subramanian, Leilani H. Gilpin</li>
<li>for: 这个研究旨在应用人工智能于医疗领域，尤其是检测无 симtomatic progressing 的疾病，如糖尿病retinopathy (DR)。</li>
<li>methods: 本研究使用了 convolutional Neural Network (CNN) 模型，通过输入照片后，可以正确地识别四种known DR特征，包括 micro-aneurysms、cotton wools、exudates 和 hemorrhages。</li>
<li>results: 本研究获得了97%的敏感度和71%的准确率，表明模型具有高度的可读性和抗过滤性。<details>
<summary>Abstract</summary>
The application of Artificial Intelligence in the medical market brings up increasing concerns but aids in more timely diagnosis of silent progressing diseases like Diabetic Retinopathy. In order to diagnose Diabetic Retinopathy (DR), ophthalmologists use color fundus images, or pictures of the back of the retina, to identify small distinct features through a difficult and time-consuming process. Our work creates a novel CNN model and identifies the severity of DR through fundus image input. We classified 4 known DR features, including micro-aneurysms, cotton wools, exudates, and hemorrhages, through convolutional layers and were able to provide an accurate diagnostic without additional user input. The proposed model is more interpretable and robust to overfitting. We present initial results with a sensitivity of 97% and an accuracy of 71%. Our contribution is an interpretable model with similar accuracy to more complex models. With that, our model advances the field of DR detection and proves to be a key step towards AI-focused medical diagnosis.
</details>
<details>
<summary>摘要</summary>
这个应用人工智能在医疗市场中带来的应用对于不明显进行诊断的疾病，如糖尿病肉眼病（DR），具有增长的担忧。为了诊断DR，医生会使用彩色背部影像（color fundus images），或者是背部 Retina 的照片，以便识别小型的明显特征。我们的工作创造了一个新的 convolutional neural network（CNN）模型，可以通过背部影像的输入来诊断DR的严重程度。我们分类了4种已知的DR特征，包括微型血管、绒毛、渗透物和出血，透过 convolutional layers 进行分类，并能够提供精确的诊断，不需要额外的使用者输入。我们的模型更加可读性和避免过拟合。我们给出了初步的结果，敏感性为97%，准确率为71%。我们的贡献是一个可读性好的模型，与更复杂的模型相比，具有相似的准确性。这个模型对于DR检测具有重要的进步，并且是人工智能在医疗诊断中的一个关键步骤。
</details></li>
</ul>
<hr>
<h2 id="LAMP-Learn-A-Motion-Pattern-for-Few-Shot-Based-Video-Generation"><a href="#LAMP-Learn-A-Motion-Pattern-for-Few-Shot-Based-Video-Generation" class="headerlink" title="LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation"></a>LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10769">http://arxiv.org/abs/2310.10769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RQ-Wu/LAMP">https://github.com/RQ-Wu/LAMP</a></li>
<li>paper_authors: Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, Xiangyu Zhang<br>for:LAMP is designed to learn motion patterns in text-to-video generation, with a focus on few-shot learning and efficient use of resources.methods:The LAMP framework uses a first-frame-conditioned pipeline with an off-the-shelf text-to-image model for content generation, and expands the pretrained 2D convolution layers to temporal-spatial motion learning layers. Shared-noise sampling is used to improve stability and flexibility.results:Extensive experiments show that LAMP can effectively learn motion patterns on limited data and generate high-quality videos, with applications in text-to-image diffusion, real-world image animation, and video editing. The code and models are available online.<details>
<summary>Abstract</summary>
With the impressive progress in diffusion-based text-to-image generation, extending such powerful generative ability to text-to-video raises enormous attention. Existing methods either require large-scale text-video pairs and a large number of training resources or learn motions that are precisely aligned with template videos. It is non-trivial to balance a trade-off between the degree of generation freedom and the resource costs for video generation. In our study, we present a few-shot-based tuning framework, LAMP, which enables text-to-image diffusion model Learn A specific Motion Pattern with 8~16 videos on a single GPU. Specifically, we design a first-frame-conditioned pipeline that uses an off-the-shelf text-to-image model for content generation so that our tuned video diffusion model mainly focuses on motion learning. The well-developed text-to-image techniques can provide visually pleasing and diverse content as generation conditions, which highly improves video quality and generation freedom. To capture the features of temporal dimension, we expand the pretrained 2D convolution layers of the T2I model to our novel temporal-spatial motion learning layers and modify the attention blocks to the temporal level. Additionally, we develop an effective inference trick, shared-noise sampling, which can improve the stability of videos with computational costs. Our method can also be flexibly applied to other tasks, e.g. real-world image animation and video editing. Extensive experiments demonstrate that LAMP can effectively learn the motion pattern on limited data and generate high-quality videos. The code and models are available at https://rq-wu.github.io/projects/LAMP.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>随着文本到图像生成技术的快速进步，扩展这种强大的生成能力到文本到视频生成引发了巨大的关注。现有方法可以通过大量的文本-视频对对和大量的训练资源来学习，但是很难平衡生成自由度和资源成本之间的折衔。在我们的研究中，我们提出了一个几个步骤基于的调整框架，名为LAMP，可以在单个GPU上使用8~16个视频进行调整。 Specifically，我们设计了一个基于首帧的管道，使用商业化的文本到图像模型来生成内容，以便我们的调整视频模型主要集中在动作学习。具有良好的文本到图像技术可以提供辐射的和多样化的生成条件，从而很大地提高视频质量和生成自由度。为了捕捉时间维度的特征，我们扩展了预训练的2D卷积层，并对它们进行我们的新的时空动作学习层，还修改了注意力块到时间层。此外，我们开发了一种有效的推理技术，分享噪声抽样，可以提高视频的稳定性。我们的方法可以适应其他任务，例如真实世界图像动画和视频编辑。广泛的实验证明了LAMP可以有效地学习动作模式，并生成高质量的视频。代码和模型可以在https://rq-wu.github.io/projects/LAMP中获得。
</details></li>
</ul>
<hr>
<h2 id="Deep-Conditional-Shape-Models-for-3D-cardiac-image-segmentation"><a href="#Deep-Conditional-Shape-Models-for-3D-cardiac-image-segmentation" class="headerlink" title="Deep Conditional Shape Models for 3D cardiac image segmentation"></a>Deep Conditional Shape Models for 3D cardiac image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10756">http://arxiv.org/abs/2310.10756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Athira J Jacob, Puneet Sharma, Daniel Ruckert</li>
<li>for: 医疗图像分析的第一步是准确地定义器官结构。</li>
<li>methods: 我们引入了一种新的分割算法，使用深度条件形状模型（DCSM）作为核心组件。该算法使用深度隐式形状表示，学习任何有兴趣的生物结构的模态无关形状模型，并通过自动检测或用户输入的特征点来适应图像。最后，我们添加了一个模态依赖的轻量级细节修正网络，以捕捉图像中没有表示的细节。</li>
<li>results: 我们在各种3D成像Modalities（对比增强CT、非对比CT、3D电子心征图像）中进行心脏左心室（LV）分割，并证明自动DCSM在非对比CT中超过基准，并且在对比CT和3DE中使用细节修正网络时，特别是在 Hausdorff 距离方面获得了显著改进。半自动DCSM使用用户输入的特征点，只在对比CT上训练，可达92%的 dice，对所有Modalities具有Equivalent或更好的性能。<details>
<summary>Abstract</summary>
Delineation of anatomical structures is often the first step of many medical image analysis workflows. While convolutional neural networks achieve high performance, these do not incorporate anatomical shape information. We introduce a novel segmentation algorithm that uses Deep Conditional Shape models (DCSMs) as a core component. Using deep implicit shape representations, the algorithm learns a modality-agnostic shape model that can generate the signed distance functions for any anatomy of interest. To fit the generated shape to the image, the shape model is conditioned on anatomic landmarks that can be automatically detected or provided by the user. Finally, we add a modality-dependent, lightweight refinement network to capture any fine details not represented by the implicit function. The proposed DCSM framework is evaluated on the problem of cardiac left ventricle (LV) segmentation from multiple 3D modalities (contrast-enhanced CT, non-contrasted CT, 3D echocardiography-3DE). We demonstrate that the automatic DCSM outperforms the baseline for non-contrasted CT without the local refinement, and with the refinement for contrasted CT and 3DE, especially with significant improvement in the Hausdorff distance. The semi-automatic DCSM with user-input landmarks, while only trained on contrasted CT, achieves greater than 92% Dice for all modalities. Both automatic DCSM with refinement and semi-automatic DCSM achieve equivalent or better performance compared to inter-user variability for these modalities.
</details>
<details>
<summary>摘要</summary>
医学图像分析工作流程中的解剖结构定义是第一步。卷积神经网络可以达到高性能，但不会包含解剖形态信息。我们介绍了一种新的分割算法，使用深度条件形状模型（DCSM）作为核心组件。使用深度隐式形状表示，算法学习了任意解剖结构的模态独立形状模型，可以生成任意解剖结构的签名距离函数。为了使Shape模型适应图像，Shape模型被conditioned on可自动探测或提供的解剖标志。最后，我们添加了一个模态依赖的轻量级细节修正网络，以捕捉无法由隐式函数表示的细节。我们提出的DCSM框架在cardiac left ventricle（LV）三维模态（对比增强CT、非对比CT和3DE）的分割问题上进行了评估。我们表明，自动DCSM比基线高效，而且与使用本地细节修正有显著改善。用户输入标志的半自动DCSM，即使只在对比CT上训练，可以达到92%的Dice指标或更高。自动DCSM和半自动DCSM都与人类间变化相当或更好，对这些模态来说。
</details></li>
</ul>
<hr>
<h2 id="IDRNet-Intervention-Driven-Relation-Network-for-Semantic-Segmentation"><a href="#IDRNet-Intervention-Driven-Relation-Network-for-Semantic-Segmentation" class="headerlink" title="IDRNet: Intervention-Driven Relation Network for Semantic Segmentation"></a>IDRNet: Intervention-Driven Relation Network for Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10755">http://arxiv.org/abs/2310.10755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/segmentationblwx/sssegmentation">https://github.com/segmentationblwx/sssegmentation</a></li>
<li>paper_authors: Zhenchao Jin, Xiaowei Hu, Lingting Zhu, Luchuan Song, Li Yuan, Lequan Yu</li>
<li>for: 提高 dense prediction 任务中的 contextual information 聚合</li>
<li>methods: 利用 deletion diagnostics 过程模型 contextual relations among different pixels, 并使用 feature enhancement module 进一步提高 distinguishability</li>
<li>results: 对 state-of-the-art segmentation frameworks 带来了一致性的性能提升, 并在各种标准评价 metrics 上达到了竞争性的结果<details>
<summary>Abstract</summary>
Co-occurrent visual patterns suggest that pixel relation modeling facilitates dense prediction tasks, which inspires the development of numerous context modeling paradigms, \emph{e.g.}, multi-scale-driven and similarity-driven context schemes. Despite the impressive results, these existing paradigms often suffer from inadequate or ineffective contextual information aggregation due to reliance on large amounts of predetermined priors. To alleviate the issues, we propose a novel \textbf{I}ntervention-\textbf{D}riven \textbf{R}elation \textbf{Net}work (\textbf{IDRNet}), which leverages a deletion diagnostics procedure to guide the modeling of contextual relations among different pixels. Specifically, we first group pixel-level representations into semantic-level representations with the guidance of pseudo labels and further improve the distinguishability of the grouped representations with a feature enhancement module. Next, a deletion diagnostics procedure is conducted to model relations of these semantic-level representations via perceiving the network outputs and the extracted relations are utilized to guide the semantic-level representations to interact with each other. Finally, the interacted representations are utilized to augment original pixel-level representations for final predictions. Extensive experiments are conducted to validate the effectiveness of IDRNet quantitatively and qualitatively. Notably, our intervention-driven context scheme brings consistent performance improvements to state-of-the-art segmentation frameworks and achieves competitive results on popular benchmark datasets, including ADE20K, COCO-Stuff, PASCAL-Context, LIP, and Cityscapes. Code is available at \url{https://github.com/SegmentationBLWX/sssegmentation}.
</details>
<details>
<summary>摘要</summary>
伴生视觉模式表明，像素关系模型化可以促进紧凑预测任务，这引发了许多上下文模型范文的发展，如多scale-driven和相似性-driven上下文方案。 despite the impressive results, these existing paradigms often suffer from inadequate or ineffective contextual information aggregation due to reliance on large amounts of predetermined priors. To address these issues, we propose a novel \textbf{I}ntervention-\textbf{D}riven \textbf{R}elation \textbf{Net}work (\textbf{IDRNet}), which leverages a deletion diagnostics procedure to guide the modeling of contextual relations among different pixels. Specifically, we first group pixel-level representations into semantic-level representations with the guidance of pseudo labels and further improve the distinguishability of the grouped representations with a feature enhancement module. Next, a deletion diagnostics procedure is conducted to model relations of these semantic-level representations via perceiving the network outputs and the extracted relations are utilized to guide the semantic-level representations to interact with each other. Finally, the interacted representations are utilized to augment original pixel-level representations for final predictions. Extensive experiments are conducted to validate the effectiveness of IDRNet quantitatively and qualitatively. Notably, our intervention-driven context scheme brings consistent performance improvements to state-of-the-art segmentation frameworks and achieves competitive results on popular benchmark datasets, including ADE20K, COCO-Stuff, PASCAL-Context, LIP, and Cityscapes. Code is available at \url{https://github.com/SegmentationBLWX/sssegmentation}.
</details></li>
</ul>
<hr>
<h2 id="HairCLIPv2-Unifying-Hair-Editing-via-Proxy-Feature-Blending"><a href="#HairCLIPv2-Unifying-Hair-Editing-via-Proxy-Feature-Blending" class="headerlink" title="HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending"></a>HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10651">http://arxiv.org/abs/2310.10651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wty-ustc/hairclipv2">https://github.com/wty-ustc/hairclipv2</a></li>
<li>paper_authors: Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Gang Hua, Nenghai Yu</li>
<li>for: 提供一个能够基于文本描述或参考图像进行头发编辑的框架，并且支持多种交互方式，包括文本描述、参考图像和绘制笔触等。</li>
<li>methods: 使用交叉模式模型（如CLIP）将头发编辑转化为头发传送任务，并将编辑条件转化为不同的代理特征。在输入图像上加载编辑效果，通过在头发特征空间中混合相应的代理特征来实现。</li>
<li>results: 对比于原始HairCLIP，HairCLIPv2可以更好地保持无关特征（如人脸特征、背景特征），同时支持未before seen文本描述和不同交互方式。量化和质量实验表明，HairCLIPv2在编辑效果、无关特征保持和视觉自然性等方面具有显著优势。<details>
<summary>Abstract</summary>
Hair editing has made tremendous progress in recent years. Early hair editing methods use well-drawn sketches or masks to specify the editing conditions. Even though they can enable very fine-grained local control, such interaction modes are inefficient for the editing conditions that can be easily specified by language descriptions or reference images. Thanks to the recent breakthrough of cross-modal models (e.g., CLIP), HairCLIP is the first work that enables hair editing based on text descriptions or reference images. However, such text-driven and reference-driven interaction modes make HairCLIP unable to support fine-grained controls specified by sketch or mask. In this paper, we propose HairCLIPv2, aiming to support all the aforementioned interactions with one unified framework. Simultaneously, it improves upon HairCLIP with better irrelevant attributes (e.g., identity, background) preservation and unseen text descriptions support. The key idea is to convert all the hair editing tasks into hair transfer tasks, with editing conditions converted into different proxies accordingly. The editing effects are added upon the input image by blending the corresponding proxy features within the hairstyle or hair color feature spaces. Besides the unprecedented user interaction mode support, quantitative and qualitative experiments demonstrate the superiority of HairCLIPv2 in terms of editing effects, irrelevant attribute preservation and visual naturalness. Our code is available at \url{https://github.com/wty-ustc/HairCLIPv2}.
</details>
<details>
<summary>摘要</summary>
随笔修整技术在最近几年来取得了巨大的进步。早期的修整方法通常使用细致绘制的素描或面Mask来指定修整条件。尽管它们可以实现非常细致的本地控制，但是这些交互方式在基于语言描述或参考图像的修整条件上是不效率的。感谢最近的交互模型技术（如CLIP）的突破，我们的HairCLIP是首个可以基于语言描述或参考图像进行随笔修整的工作。然而，这些基于文本描述或参考图像的交互方式使得HairCLIP无法支持细致的素描或面Mask来指定修整条件。在这篇论文中，我们提出了HairCLIPv2，旨在支持所有的交互方式，同时也提高了不相关特征（如人脸和背景）的保留和未看到文本描述的支持。我们的关键思想是将所有的随笔修整任务转换为随笔传输任务，并将编辑条件转换为不同的代理 accordingly。然后，在输入图像上添加修整效果，通过在额发型或发色特征空间中混合相应的代理特征。除了不同的用户交互方式支持外，我们的HairCLIPv2还在编辑效果、不相关特征保留和视觉自然性方面具有显著优势。我们的代码可以在GitHub上找到：<https://github.com/wty-ustc/HairCLIPv2>。
</details></li>
</ul>
<hr>
<h2 id="TraM-NeRF-Tracing-Mirror-and-Near-Perfect-Specular-Reflections-through-Neural-Radiance-Fields"><a href="#TraM-NeRF-Tracing-Mirror-and-Near-Perfect-Specular-Reflections-through-Neural-Radiance-Fields" class="headerlink" title="TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields"></a>TraM-NeRF: Tracing Mirror and Near-Perfect Specular Reflections through Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10650">http://arxiv.org/abs/2310.10650</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Rubikalubi/TraM-NeRF">https://github.com/Rubikalubi/TraM-NeRF</a></li>
<li>paper_authors: Leif Van Holland, Ruben Bliersbach, Jan U. Müller, Patrick Stotko, Reinhard Klein</li>
<li>for: 实现复杂场景中细节轻松渲染，如镜子等具有偏光反射的物体。</li>
<li>methods: 采用物理可能的材料模型和Monte-Carlo方法在Volume Rendering中厘定反射行为，实现重要抽象和透射计算。</li>
<li>results: 实现了对这些挑战场景的一致射预测和uperior的效果，较前一代方法更好。<details>
<summary>Abstract</summary>
Implicit representations like Neural Radiance Fields (NeRF) showed impressive results for photorealistic rendering of complex scenes with fine details. However, ideal or near-perfectly specular reflecting objects such as mirrors, which are often encountered in various indoor scenes, impose ambiguities and inconsistencies in the representation of the reconstructed scene leading to severe artifacts in the synthesized renderings. In this paper, we present a novel reflection tracing method tailored for the involved volume rendering within NeRF that takes these mirror-like objects into account while avoiding the cost of straightforward but expensive extensions through standard path tracing. By explicitly modeling the reflection behavior using physically plausible materials and estimating the reflected radiance with Monte-Carlo methods within the volume rendering formulation, we derive efficient strategies for importance sampling and the transmittance computation along rays from only few samples. We show that our novel method enables the training of consistent representations of such challenging scenes and achieves superior results in comparison to previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用各种各样的各种方法来描述复杂场景的隐式表示，如神经辐射场（NeRF），已经取得了高度真实的渲染复杂场景的成果。然而，在室内场景中遇到的 идеаль或几乎完美的 espejo 反射物体，如镜子，会导致描述重建场景的表示具有歧义和不一致，从而导致渲染 synthesized 图像中的 artifacts。在这篇论文中，我们提出了一种专门为 NeRF 中 involve 体积渲染中的反射跟踪方法，该方法能够考虑这些 espejo 类型的物体，而不需要 straightforward 而且昂贵的扩展。我们通过物理可能的材料模型和 Monte-Carlo 方法来表示反射行为，从而 deriv 高效的重要抽象策略和传播计算方法。我们示示了我们的新方法可以培养 consistent 的表示这些复杂场景，并且在 compared 到先前的状态的艺术方法上达到了更高的成果。
</details></li>
</ul>
<hr>
<h2 id="TOSS-High-quality-Text-guided-Novel-View-Synthesis-from-a-Single-Image"><a href="#TOSS-High-quality-Text-guided-Novel-View-Synthesis-from-a-Single-Image" class="headerlink" title="TOSS:High-quality Text-guided Novel View Synthesis from a Single Image"></a>TOSS:High-quality Text-guided Novel View Synthesis from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10644">http://arxiv.org/abs/2310.10644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, Heung-Yeung Shum</li>
<li>for: 本研究旨在提出一种基于文本Semantic guidance的novel view synthesis（NVS）方法，以解决单视图NVS问题的不足约束性。</li>
<li>methods: 本方法使用文本作为高级 semantic information来约束NVS解决方案空间，并引入了特定于图像和摄像机pose conditioning的模块，以及专门为pose正确性和细节细节加权训练。</li>
<li>results: 对比Zero-1-to-3，本研究的提议TOSS实现了更可信、控制性和多视图一致的NVS结果，并通过了全面的ablations来证明引入的Semantic guidance和建筑设计的有效性。<details>
<summary>Abstract</summary>
In this paper, we present TOSS, which introduces text to the task of novel view synthesis (NVS) from just a single RGB image. While Zero-1-to-3 has demonstrated impressive zero-shot open-set NVS capability, it treats NVS as a pure image-to-image translation problem. This approach suffers from the challengingly under-constrained nature of single-view NVS: the process lacks means of explicit user control and often results in implausible NVS generations. To address this limitation, TOSS uses text as high-level semantic information to constrain the NVS solution space. TOSS fine-tunes text-to-image Stable Diffusion pre-trained on large-scale text-image pairs and introduces modules specifically tailored to image and camera pose conditioning, as well as dedicated training for pose correctness and preservation of fine details. Comprehensive experiments are conducted with results showing that our proposed TOSS outperforms Zero-1-to-3 with more plausible, controllable and multiview-consistent NVS results. We further support these results with comprehensive ablations that underscore the effectiveness and potential of the introduced semantic guidance and architecture design.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了TOSS，它将文本引入到novel view synthesis（NVS）任务中，只需基于单个RGB图像。 zero-1-to-3 已经表现出了非常出色的零基础开放式 NVS 能力，但是这种方法在单视图 NVS 中存在一些缺乏约束的问题：没有明确的用户控制方式，导致 NVS 生成结果往往不太可能。为了解决这个限制，TOSS 使用文本作为高级Semantic信息来约束 NVS 解决方案空间。TOSS 细致地调整了文本-图像 Stable Diffusion 预训练的大规模文本-图像对，并 introduce了专门为图像和摄像头姿态conditioning设计的模块，以及专门为posecorrectness和细节细节训练。我们进行了全面的实验，结果显示了我们提出的 TOSS 比 zero-1-to-3 更加plausible、可控和多视图一致的 NVS 结果。我们还进行了详细的ablation，以证明引入的semantic导航和建筑设计的效果和潜在。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Photorealistic-Dynamic-Scene-Representation-and-Rendering-with-4D-Gaussian-Splatting"><a href="#Real-time-Photorealistic-Dynamic-Scene-Representation-and-Rendering-with-4D-Gaussian-Splatting" class="headerlink" title="Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting"></a>Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10642">http://arxiv.org/abs/2310.10642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fudan-zvg/4d-gaussian-splatting">https://github.com/fudan-zvg/4d-gaussian-splatting</a></li>
<li>paper_authors: Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, Li Zhang</li>
<li>for: 生成复杂的动态场景3D图像和不同时间点的视图</li>
<li>methods: 使用4D primitives优化approximateunderlying spacetime volume，包括视角 dependent和时间演化的外观</li>
<li>results: 提供了一种简单、灵活、可变长视频和终端培育的方法，能够capture复杂的动态场景运动，并且在实验中达到了较高的视觉质量和效率。<details>
<summary>Abstract</summary>
Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial and temporal structure of dynamic scenes from directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as an entirety and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi-view scenarios, demonstrate our 4DGS model's superior visual quality and efficiency.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified ChineseDynamic 3D scene reconstruction from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, there are still limitations: (i) Inadequate Scene Structure: Existing methods cannot effectively reveal the spatial and temporal structure of dynamic scenes by directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as a whole and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Learning to optimize the 4D primitives enables us to synthesize novel views at any desired time with our tailored rendering routine. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions. Experiments across various benchmarks, including monocular and multi-view scenarios, demonstrate our 4DGS model's superior visual quality and efficiency.Translated by Google Translate
</details></li>
</ul>
<hr>
<h2 id="LLM-Blueprint-Enabling-Text-to-Image-Generation-with-Complex-and-Detailed-Prompts"><a href="#LLM-Blueprint-Enabling-Text-to-Image-Generation-with-Complex-and-Detailed-Prompts" class="headerlink" title="LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts"></a>LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10640">http://arxiv.org/abs/2310.10640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hananshafi/llmblueprint">https://github.com/hananshafi/llmblueprint</a></li>
<li>paper_authors: Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, Peter Wonka</li>
<li>for: 实现文本描述中的复杂场景和多个物件的图像生成</li>
<li>methods: 利用大自然语言模型提取文本描述中的关键元素，包括物件 bounding box 坐标、个别物品的详细描述和背景内容，然后使用这些元素生成图像</li>
<li>results: 与基准扩散模型相比，实现了复杂文本描述中的图像生成，并在用户评估中获得了高度的认可和满意度<details>
<summary>Abstract</summary>
Diffusion-based generative models have significantly advanced text-to-image generation but encounter challenges when processing lengthy and intricate text prompts describing complex scenes with multiple objects. While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs. In response, we present a novel approach leveraging Large Language Models (LLMs) to extract critical components from text prompts, including bounding box coordinates for foreground objects, detailed textual descriptions for individual objects, and a succinct background context. These components form the foundation of our layout-to-image generation model, which operates in two phases. The initial Global Scene Generation utilizes object layouts and background context to create an initial scene but often falls short in faithfully representing object characteristics as specified in the prompts. To address this limitation, we introduce an Iterative Refinement Scheme that iteratively evaluates and refines box-level content to align them with their textual descriptions, recomposing objects as needed to ensure consistency. Our evaluation on complex prompts featuring multiple objects demonstrates a substantial improvement in recall compared to baseline diffusion models. This is further validated by a user study, underscoring the efficacy of our approach in generating coherent and detailed scenes from intricate textual inputs.
</details>
<details>
<summary>摘要</summary>
文本到图像生成技术在进行长度和复杂性增加时遇到了挑战。尽管在简短的单个对象描述中表现出色，但是在处理长度和复杂性更高的文本提示时，这些模型经常遇到困难，不能准确地捕捉文本中的细节。为了解决这问题，我们提出了一种新的方法，利用大型自然语言模型（LLM）来提取文本提示中的关键组成部分，包括主要对象的 bounding box 坐标、对象的详细文本描述和背景 контекст。这些组成部分成为我们的布局到图像生成模型的基础，该模型在两个阶段进行处理。首先，我们使用对象布局和背景 контекст来创建初始场景，但是这些场景经常无法准确地表现出对象的特征。为了解决这个限制，我们提出了一种迭代优化方案，通过评估和修改框架内容，使其与文本描述相符。我们的评估表明，对于包含多个对象的复杂提示，我们的方法可以提高了回归率，并且在用户研究中得到了证明。Translation notes:* "Diffusion-based generative models" is translated as "文本到图像生成技术" (text-to-image generation technology)* "long and intricate text prompts" is translated as "长度和复杂性更高的文本提示" (text prompts with length and complexity)* "nuanced details" is translated as "细节" (details)* "Large Language Models" is translated as "大型自然语言模型" (large language models)* " bounding box coordinates" is translated as " bounding box 坐标" (bounding box coordinates)* "detailed textual descriptions" is translated as "详细文本描述" (detailed textual descriptions)* "succinct background context" is translated as "背景 контекст" (background context)* "Global Scene Generation" is translated as "全球场景生成" (global scene generation)* "Iterative Refinement Scheme" is translated as "迭代优化方案" (iterative refinement scheme)* "box-level content" is translated as "框架内容" (box-level content)* "recomposing objects" is translated as "重新组合对象" (recomposing objects)* "consistency" is translated as "一致性" (consistency)* "user study" is translated as "用户研究" (user study)
</details></li>
</ul>
<hr>
<h2 id="DynVideo-E-Harnessing-Dynamic-NeRF-for-Large-Scale-Motion-and-View-Change-Human-Centric-Video-Editing"><a href="#DynVideo-E-Harnessing-Dynamic-NeRF-for-Large-Scale-Motion-and-View-Change-Human-Centric-Video-Editing" class="headerlink" title="DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing"></a>DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10624">http://arxiv.org/abs/2310.10624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou</li>
<li>for: 这个论文的目的是提出一种基于动态神经辐射场（NeRF）的人称视频编辑方法，以解决现有方法因为视频长度和视角变化而受限的问题。</li>
<li>methods: 这种方法使用动态NeRF作为人称视频表示，并提出了一种基于多视图多姿Score Distillation Sampling（SDS）、图像恢复损失、文本导向地方部超分辨率、风格传递等多种技术的图像三维空间编辑管线。</li>
<li>results: 与比较方法相比，这种方法在两个难度较大的数据集上显示出了大幅提高（50%~95%）的人类喜好度。具体比较结果可以查看到项目页面<a target="_blank" rel="noopener" href="https://showlab.github.io/DynVideo-E/">https://showlab.github.io/DynVideo-E/</a>.<details>
<summary>Abstract</summary>
Despite remarkable research advances in diffusion-based video editing, existing methods are limited to short-length videos due to the contradiction between long-range consistency and frame-wise editing. Recent approaches attempt to tackle this challenge by introducing video-2D representations to degrade video editing to image editing. However, they encounter significant difficulties in handling large-scale motion- and view-change videos especially for human-centric videos. This motivates us to introduce the dynamic Neural Radiance Fields (NeRF) as the human-centric video representation to ease the video editing problem to a 3D space editing task. As such, editing can be performed in the 3D spaces and propagated to the entire video via the deformation field. To provide finer and direct controllable editing, we propose the image-based 3D space editing pipeline with a set of effective designs. These include multi-view multi-pose Score Distillation Sampling (SDS) from both 2D personalized diffusion priors and 3D diffusion priors, reconstruction losses on the reference image, text-guided local parts super-resolution, and style transfer for 3D background space. Extensive experiments demonstrate that our method, dubbed as DynVideo-E, significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% ~ 95% in terms of human preference. Compelling video comparisons are provided in the project page https://showlab.github.io/DynVideo-E/. Our code and data will be released to the community.
</details>
<details>
<summary>摘要</summary>
尽管扩展视频编辑技术已取得了很大的进步，但现有方法仅适用于短视频，因为视频编辑和框架之间存在长距离一致性和帧级编辑之间的矛盾。现有的方法通过引入视频到2D表示来减轻视频编辑到图像编辑。然而，它们在处理大规模运动和视点变化视频，特别是人类中心视频时遇到了重大困难。这个问题驱使我们提出人类中心视频表示——动态神经辐射场（NeRF），以便将视频编辑转化为3D空间编辑任务。在这种情况下，编辑可以在3D空间进行，并通过扭曲场进行对整个视频的广泛传播。为了提供更加精细和直接控制的编辑，我们提议了基于图像的3D空间编辑管线，包括多视图多姿Score Distillation Sampling（SDS）、参考图像的重建损失、文本指导的本地部分超解析和风格转换。我们的方法，即DynVideo-E，在两个挑战性 datasets 上达到了领先的水平，与前一代方法的比较达到了50%~95%的差距。我们在项目页面（https://showlab.github.io/DynVideo-E/）提供了吸引人的视频比较。我们的代码和数据将被公开发布到社区。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Controlling-Vision-Foundation-Models-via-Text-Explanations"><a href="#Interpreting-and-Controlling-Vision-Foundation-Models-via-Text-Explanations" class="headerlink" title="Interpreting and Controlling Vision Foundation Models via Text Explanations"></a>Interpreting and Controlling Vision Foundation Models via Text Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10591">http://arxiv.org/abs/2310.10591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tonychenxyz/vit-interpret">https://github.com/tonychenxyz/vit-interpret</a></li>
<li>paper_authors: Haozhe Chen, Junfeng Yang, Carl Vondrick, Chengzhi Mao</li>
<li>for: 本研究旨在理解和控制CLIP等大规模预训练视觉基础模型的预测结果。</li>
<li>methods: 本研究使用了一种基于自然语言的方法来解释视 transformer 的干 tokens，并通过捕捉最近的文本来进行解释。</li>
<li>results: 本研究可以帮助理解CLIP等模型在视觉任务中的决策过程，并提供了一种控制模型行为的方法，以提高模型的Robustness和减少偏见和偶合关系。<details>
<summary>Abstract</summary>
Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models' predictions and controlling model behaviors have remained open challenges. We present a framework for interpreting vision transformer's latent tokens with natural language. Given a latent token, our framework retains its semantic information to the final layer using transformer's local operations and retrieves the closest text for explanation. Our approach enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, our framework allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations.
</details>
<details>
<summary>摘要</summary>
大规模预训练视觉基础模型，如CLIP，已成为视觉任务的德 факто底层。然而，由于其黑盒模型的性质，理解这些模型预测的下面规则和控制模型行为仍然是开放的挑战。我们提出了一个把视觉转换器的幂谱Token与自然语言相关的框架。给定一个幂谱Token，我们的框架使用转换器的地方运算保留它的含义到最终层，并从文本库中检索最相似的文本来解释。我们的方法可以在不需要额外训练或数据收集的前提下，理解模型的视觉逻辑过程。基于获得的解释，我们的框架允许对模型的编辑，控制模型的逻辑行为，并改善模型免疫偏见和偶极相关性。
</details></li>
</ul>
<hr>
<h2 id="Matching-the-Neuronal-Representations-of-V1-is-Necessary-to-Improve-Robustness-in-CNNs-with-V1-like-Front-ends"><a href="#Matching-the-Neuronal-Representations-of-V1-is-Necessary-to-Improve-Robustness-in-CNNs-with-V1-like-Front-ends" class="headerlink" title="Matching the Neuronal Representations of V1 is Necessary to Improve Robustness in CNNs with V1-like Front-ends"></a>Matching the Neuronal Representations of V1 is Necessary to Improve Robustness in CNNs with V1-like Front-ends</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10575">http://arxiv.org/abs/2310.10575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dicarlolab/vonenet">https://github.com/dicarlolab/vonenet</a></li>
<li>paper_authors: Ruxandra Barbulescu, Tiago Marques, Arlindo L. Oliveira</li>
<li>for: 提高对图像损害的鲁棒性（robustness to image corruptions）</li>
<li>methods: 模拟肉眼初级视觉区域（primate primary visual cortex）的计算，使得模型具有更高的鲁棒性</li>
<li>results: 模型使用生物学发现的电场特性分布（empirical biological distributions） sampling，对图像损害的鲁棒性明显更高（相对差异为8.72%），而同类 neuronal sub-populations 在两个模型中具有相似的响应特性和下游权重学习结果，但下游处理具有不同的影响。<details>
<summary>Abstract</summary>
While some convolutional neural networks (CNNs) have achieved great success in object recognition, they struggle to identify objects in images corrupted with different types of common noise patterns. Recently, it was shown that simulating computations in early visual areas at the front of CNNs leads to improvements in robustness to image corruptions. Here, we further explore this result and show that the neuronal representations that emerge from precisely matching the distribution of RF properties found in primate V1 is key for this improvement in robustness. We built two variants of a model with a front-end modeling the primate primary visual cortex (V1): one sampling RF properties uniformly and the other sampling from empirical biological distributions. The model with the biological sampling has a considerably higher robustness to image corruptions that the uniform variant (relative difference of 8.72%). While similar neuronal sub-populations across the two variants have similar response properties and learn similar downstream weights, the impact on downstream processing is strikingly different. This result sheds light on the origin of the improvements in robustness observed in some biologically-inspired models, pointing to the need of precisely mimicking the neuronal representations found in the primate brain.
</details>
<details>
<summary>摘要</summary>
有些卷积神经网络（CNN）在物体识别方面取得了很大的成功，但它们在受到不同类型的常见噪声损害后仍然难以识别物体。最近，研究人员发现，在CNN的前端模型中 simulate  computations 可以提高对图像损害的Robustness。在这里，我们进一步探索这一结论，并证明了模型在 precisely 匹配了黑眼睛动物V1中的观察者分布 Property 后，会带来更高的Robustness。我们建立了两个模型，其中一个采样RF Property uniform，另一个采样从empirical biological distribution。与uniform variant相比，使用生物分布采样的模型具有更高的Robustness to image corruptions（相对差异为8.72%）。尽管这两个模型中的 neuronal sub-populations 具有相似的response property和downstream weights，但是它们对下游处理的影响却是不同的。这一结论 shed light  onto the origin of the improvements in robustness observed in some biologically-inspired models, pointing to the need of precisely mimicking the neuronal representations found in the primate brain.
</details></li>
</ul>
<hr>
<h2 id="RefConv-Re-parameterized-Refocusing-Convolution-for-Powerful-ConvNets"><a href="#RefConv-Re-parameterized-Refocusing-Convolution-for-Powerful-ConvNets" class="headerlink" title="RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets"></a>RefConv: Re-parameterized Refocusing Convolution for Powerful ConvNets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10563">http://arxiv.org/abs/2310.10563</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aiolus-X/RefConv">https://github.com/Aiolus-X/RefConv</a></li>
<li>paper_authors: Zhicheng Cai, Xiaohan Ding, Qiu Shen, Xun Cao</li>
<li>for: 提高 CNN 模型的性能，无需更改原始模型结构或添加新的推理成本。</li>
<li>methods: 使用可调 Refocusing Transformation 修改基础核函数，使得不同通道的参数之间建立连接，从而提高模型表达能力。</li>
<li>results: 在图像分类、物体检测和 semantic segmentation 等 CNN 模型中，使用 RefConv 可以提高性能（ImageNet 上的顶部一 accuracy 提高至 1.47%），而无需增加推理成本或改变原始模型结构。<details>
<summary>Abstract</summary>
We propose Re-parameterized Refocusing Convolution (RefConv) as a replacement for regular convolutional layers, which is a plug-and-play module to improve the performance without any inference costs. Specifically, given a pre-trained model, RefConv applies a trainable Refocusing Transformation to the basis kernels inherited from the pre-trained model to establish connections among the parameters. For example, a depth-wise RefConv can relate the parameters of a specific channel of convolution kernel to the parameters of the other kernel, i.e., make them refocus on the other parts of the model they have never attended to, rather than focus on the input features only. From another perspective, RefConv augments the priors of existing model structures by utilizing the representations encoded in the pre-trained parameters as the priors and refocusing on them to learn novel representations, thus further enhancing the representational capacity of the pre-trained model. Experimental results validated that RefConv can improve multiple CNN-based models by a clear margin on image classification (up to 1.47% higher top-1 accuracy on ImageNet), object detection and semantic segmentation without introducing any extra inference costs or altering the original model structure. Further studies demonstrated that RefConv can reduce the redundancy of channels and smooth the loss landscape, which explains its effectiveness.
</details>
<details>
<summary>摘要</summary>
我们提议使用Re-parameterized Refocusing Convolution（RefConv）取代常规 convolutional layer，这是一个可插入的模块，可以无需更改预测成本提高性能。具体来说，给定一个预训练模型，RefConv将预训练模型继承的基准kernel应用一个可学习的 Refocusing Transformation，以建立模型参数之间的连接。例如，深度 wise RefConv可以将一个特定通道的 convolution kernel 的参数与其他kernel的参数相关联，例如，使得这些参数强调其他部分的模型，而不是仅仅强调输入特征。从另一个角度来看，RefConv可以利用预训练参数中的代表性作为PRIOR，并强调这些代表性，以学习新的表示，从而进一步提高预训练模型的表达能力。实验结果表明，RefConv可以在图像分类（最高达1.47%的top-1准确率提升在ImageNet）、物体检测和 semantic segmentation 中提高多个CNN基于模型的性能，而无需增加预测成本或改变原始模型结构。进一步的研究还表明，RefConv可以减少通道的重复性和缓和损失函数的曲线，这解释了它的效果。
</details></li>
</ul>
<hr>
<h2 id="InfoGCN-Learning-Representation-by-Predicting-the-Future-for-Online-Human-Skeleton-based-Action-Recognition"><a href="#InfoGCN-Learning-Representation-by-Predicting-the-Future-for-Online-Human-Skeleton-based-Action-Recognition" class="headerlink" title="InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition"></a>InfoGCN++: Learning Representation by Predicting the Future for Online Human Skeleton-based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10547">http://arxiv.org/abs/2310.10547</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stnoah1/sode">https://github.com/stnoah1/sode</a></li>
<li>paper_authors: Seunggeun Chi, Hyung-gun Chi, Qixing Huang, Karthik Ramani</li>
<li>for: online skeleton-based action recognition</li>
<li>methods: InfoGCN++, a novel extension of InfoGCN that enables real-time categorization of action types without complete observation sequences</li>
<li>results: exceptional performance in online action recognition, consistently matching or exceeding existing techniques<details>
<summary>Abstract</summary>
Skeleton-based action recognition has made significant advancements recently, with models like InfoGCN showcasing remarkable accuracy. However, these models exhibit a key limitation: they necessitate complete action observation prior to classification, which constrains their applicability in real-time situations such as surveillance and robotic systems. To overcome this barrier, we introduce InfoGCN++, an innovative extension of InfoGCN, explicitly developed for online skeleton-based action recognition. InfoGCN++ augments the abilities of the original InfoGCN model by allowing real-time categorization of action types, independent of the observation sequence's length. It transcends conventional approaches by learning from current and anticipated future movements, thereby creating a more thorough representation of the entire sequence. Our approach to prediction is managed as an extrapolation issue, grounded on observed actions. To enable this, InfoGCN++ incorporates Neural Ordinary Differential Equations, a concept that lets it effectively model the continuous evolution of hidden states. Following rigorous evaluations on three skeleton-based action recognition benchmarks, InfoGCN++ demonstrates exceptional performance in online action recognition. It consistently equals or exceeds existing techniques, highlighting its significant potential to reshape the landscape of real-time action recognition applications. Consequently, this work represents a major leap forward from InfoGCN, pushing the limits of what's possible in online, skeleton-based action recognition. The code for InfoGCN++ is publicly available at https://github.com/stnoah1/infogcn2 for further exploration and validation.
</details>
<details>
<summary>摘要</summary>
InfoGCN++ 是一种在线动作识别模型，它是 InfoGCN 的一种创新扩展。InfoGCN++ 可以在实时情况下进行动作类型分类，不需要完整的动作观察序列。它超越了传统方法，通过学习当前和预测未来动作的整体表示来提高模型的表示能力。我们采用了神经网络普通微分方程来管理预测问题，以便有效地模型动作的不断演化。经过严格的评估，InfoGCN++ 在三个骨干基于动作识别benchmark上表现出色， consistently 与或超过了现有方法的性能。这成功表明InfoGCN++ 在实时动作识别应用中具有重要的潜力。因此，这种工作代表了 InfoGCN 的一个重要突破，推动了在线骨干基于动作识别领域的发展。InfoGCN++ 的代码可以在 <https://github.com/stnoah1/infogcn2> 上公开下载，以便进一步探索和验证。
</details></li>
</ul>
<hr>
<h2 id="Label-efficient-Segmentation-via-Affinity-Propagation"><a href="#Label-efficient-Segmentation-via-Affinity-Propagation" class="headerlink" title="Label-efficient Segmentation via Affinity Propagation"></a>Label-efficient Segmentation via Affinity Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10533">http://arxiv.org/abs/2310.10533</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/circleradon/apro">https://github.com/circleradon/apro</a></li>
<li>paper_authors: Wentong Li, Yuqian Yuan, Song Wang, Wenyu Liu, Dongqi Tang, Jian Liu, Jianke Zhu, Lei Zhang</li>
<li>for: 降低寸劳的像素精度标注成本</li>
<li>methods: 提出了一种基于对称协议的对应关系建模方法，包括本地和全局对应关系项</li>
<li>results: 在三种标注任务上（包括INSTANCE Segmentation、semantic Segmentation和CLIP-引导Semantic Segmentation）达到了高度的性能提升<details>
<summary>Abstract</summary>
Weakly-supervised segmentation with label-efficient sparse annotations has attracted increasing research attention to reduce the cost of laborious pixel-wise labeling process, while the pairwise affinity modeling techniques play an essential role in this task. Most of the existing approaches focus on using the local appearance kernel to model the neighboring pairwise potentials. However, such a local operation fails to capture the long-range dependencies and ignores the topology of objects. In this work, we formulate the affinity modeling as an affinity propagation process, and propose a local and a global pairwise affinity terms to generate accurate soft pseudo labels. An efficient algorithm is also developed to reduce significantly the computational cost. The proposed approach can be conveniently plugged into existing segmentation networks. Experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmentation, point/scribble-supervised semantic segmentation and CLIP-guided semantic segmentation, demonstrate the superior performance of the proposed approach.
</details>
<details>
<summary>摘要</summary>
弱监督分割的研究已经吸引了越来越多的关注，以减少繁琐的像素精确标注过程的成本。在这个任务中，对 neighboring pairwise 潜在力场的建模技术扮演着关键角色。大多数现有方法都是基于本地外观核函数来建模邻近对的可能性。然而，这种本地操作无法捕捉长距离依赖关系和对象的 topological 结构。在这种工作中，我们将互相关系建模化为一种互相传播过程，并提出了本地和全局对对应潜在力场的两个方法，以生成准确的软精确标签。我们还开发了高效的算法，以减少计算成本。提议的方法可以方便地插入现有的分割网络中。在三种典型的标签有效分割任务中，即盒子监督实例分割、点/scribble监督 semantic segmentation 和 CLIP 引导的 semantic segmentation 中，我们的方法显示出了superior的性能。
</details></li>
</ul>
<hr>
<h2 id="Distribution-prediction-for-image-compression-An-experimental-re-compressor-for-JPEG-images"><a href="#Distribution-prediction-for-image-compression-An-experimental-re-compressor-for-JPEG-images" class="headerlink" title="Distribution prediction for image compression: An experimental re-compressor for JPEG images"></a>Distribution prediction for image compression: An experimental re-compressor for JPEG images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10517">http://arxiv.org/abs/2310.10517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxim Koroteev, Yaroslav Borisov, Pavel Frolov</li>
<li>for: 提高jpg图像压缩率</li>
<li>methods: 使用部分解码算法获取量化的DCT坐标，然后进行更有效的压缩</li>
<li>results: 实现了对jpg图像进行无损压缩<details>
<summary>Abstract</summary>
We propose a new scheme to re-compress JPEG images in a lossless way. Using a JPEG image as an input the algorithm partially decodes the signal to obtain quantized DCT coefficients and then re-compress them in a more effective way.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方案，用于在无损压缩 JPEG 图像。使用 JPEG 图像作为输入，算法部分解码信号，获取量化 DCT 系数，然后在更有效的方式压缩它们。Here's a breakdown of the translation:* "We propose" is translated as "我们提出" (wǒmen tīshì).* "a new scheme" is translated as "一种新的方案" (yī zhǒng xīn de fāng'ān).* "to re-compress" is translated as "重新压缩" (zhòng xīn pīn chā).* "JPEG images" is translated as "JPEG 图像" (JPEG túxiàng).* "in a lossless way" is translated as "在无损压缩的方式" (在无损压缩的方式).* "Using a JPEG image as an input" is translated as "使用 JPEG 图像作为输入" (shǐyòu JPEG túxiàng zhīxīng).* "partially decodes the signal" is translated as "部分解码信号" (bùzhì jiěmǎ xìngxiàng).* "to obtain quantized DCT coefficients" is translated as "获取量化 DCT 系数" (gòuqù liàngpǐ DCT xìshù).* "and then re-compress them" is translated as "然后重新压缩它们" (ránhòu zhòng xīn pīn chā tāmen).I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Unifying-Image-Processing-as-Visual-Prompting-Question-Answering"><a href="#Unifying-Image-Processing-as-Visual-Prompting-Question-Answering" class="headerlink" title="Unifying Image Processing as Visual Prompting Question Answering"></a>Unifying Image Processing as Visual Prompting Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10513">http://arxiv.org/abs/2310.10513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihao Liu, Xiangyu Chen, Xianzheng Ma, Xintao Wang, Jiantao Zhou, Yu Qiao, Chao Dong</li>
<li>for: 提高图像质量和提取视觉特征，替代具体任务模型。</li>
<li>methods: 使用大规模模型预训练和在图像处理任务中进行培 обу，通过视觉提问解决图像处理任务。</li>
<li>results: 提出一个通用的图像处理模型，可以处理多种图像处理任务，包括图像修复、图像增强、图像特征提取等。<details>
<summary>Abstract</summary>
Image processing is a fundamental task in computer vision, which aims at enhancing image quality and extracting essential features for subsequent vision applications. Traditionally, task-specific models are developed for individual tasks and designing such models requires distinct expertise. Building upon the success of large language models (LLMs) in natural language processing (NLP), there is a similar trend in computer vision, which focuses on developing large-scale models through pretraining and in-context learning. This paradigm shift reduces the reliance on task-specific models, yielding a powerful unified model to deal with various tasks. However, these advances have predominantly concentrated on high-level vision tasks, with less attention paid to low-level vision tasks. To address this issue, we propose a universal model for general image processing that covers image restoration, image enhancement, image feature extraction tasks, \textit{etc}. Our proposed framework, named PromptGIP, unifies these diverse image processing tasks within a universal framework. Inspired by NLP question answering (QA) techniques, we employ a visual prompting question answering paradigm. Specifically, we treat the input-output image pair as a structured question-answer sentence, thereby reprogramming the image processing task as a prompting QA problem. PromptGIP can undertake diverse \textbf{cross-domain} tasks using provided visual prompts, eliminating the need for task-specific finetuning. Our methodology offers a universal and adaptive solution to general image processing. While PromptGIP has demonstrated a certain degree of out-of-domain task generalization capability, further research is expected to fully explore its more powerful emergent generalization.
</details>
<details>
<summary>摘要</summary>
计算机视觉中的图像处理是一项基本任务，旨在提高图像质量和提取视觉应用中的关键特征。传统上，图像处理任务需要开发特定任务的模型，并且设计这些模型需要专门的技能。在计算机视觉领域，大型语言模型（LLM）的成功引起了一种类似的趋势，即通过预训练和上下文学习来建立大规模模型。这种思路转移使得图像处理任务可以使用一个强大的通用模型进行处理，而不需要特定任务的模型。然而，这些进步主要集中在高级视觉任务上，对低级视觉任务的关注较少。为了解决这个问题，我们提出了一个通用的图像处理模型，名为PromptGIP。我们的提案旨在将多种图像处理任务集成到一个通用框架中，并采用了视觉提问解答技术来实现。通过将输入输出图像对当做一个结构化的问题和答案句子，我们可以将图像处理任务转化为一个提问解答问题。PromptGIP可以通过提供的视觉提问来完成多个跨领域任务，无需特定任务的训练。我们的方法可以提供一个通用和适应的解决方案 для普通图像处理。虽然PromptGIP已经表现了一定的 OUT-OF-DOMAIN 任务泛化能力，但是进一步的研究可以充分发挥其更强大的 emergent 泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-and-improvement-of-Segment-Anything-Model-for-interactive-histopathology-image-segmentation"><a href="#Evaluation-and-improvement-of-Segment-Anything-Model-for-interactive-histopathology-image-segmentation" class="headerlink" title="Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation"></a>Evaluation and improvement of Segment Anything Model for interactive histopathology image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10493">http://arxiv.org/abs/2310.10493</a></li>
<li>repo_url: None</li>
<li>paper_authors: SeungKyu Kim, Hyun-Jic Oh, Seonghui Min, Won-Ki Jeong</li>
<li>for: This paper focuses on evaluating the performance of the Segment Anything Model (SAM) in interactive segmentation of histopathology data, and comparing it with other state-of-the-art interactive models.</li>
<li>methods: The paper uses the SAM model as a foundational model for image segmentation, and evaluates its performance in zero-shot and fine-tuned scenarios on histopathology data. The authors also propose a modification of SAM’s decoder to improve its local refinement ability and stability.</li>
<li>results: The experimental results show that SAM exhibits weaknesses in segmentation performance compared to other models, but demonstrates relative strengths in inference time and generalization capability. The proposed modification of SAM’s decoder is effective in improving its performance for interactive histology image segmentation.<details>
<summary>Abstract</summary>
With the emergence of the Segment Anything Model (SAM) as a foundational model for image segmentation, its application has been extensively studied across various domains, including the medical field. However, its potential in the context of histopathology data, specifically in region segmentation, has received relatively limited attention. In this paper, we evaluate SAM's performance in zero-shot and fine-tuned scenarios on histopathology data, with a focus on interactive segmentation. Additionally, we compare SAM with other state-of-the-art interactive models to assess its practical potential and evaluate its generalization capability with domain adaptability. In the experimental results, SAM exhibits a weakness in segmentation performance compared to other models while demonstrating relative strengths in terms of inference time and generalization capability. To improve SAM's limited local refinement ability and to enhance prompt stability while preserving its core strengths, we propose a modification of SAM's decoder. The experimental results suggest that the proposed modification is effective to make SAM useful for interactive histology image segmentation. The code is available at \url{https://github.com/hvcl/SAM_Interactive_Histopathology}
</details>
<details>
<summary>摘要</summary>
随着Segment Anything Model（SAM）作为图像分割基本模型的出现，其应用在不同领域得到了广泛的研究，但在 histopathology 数据中的区域分割方面却收到了相对有限的关注。在这篇论文中，我们评估了 SAM 在 zero-shot 和 fine-tuned 场景中对 histopathology 数据的性能，强调交互分割。此外，我们与其他当前领先的交互模型进行比较，以评估 SAM 在实际应用中的实用性和适应性。在实验结果中，SAM 在分割性能方面表现较弱，但在推理时间和适应性方面表现出了相对的优势。为了改进 SAM 的局部精度修正能力并保持其核心优势，我们提议一种修改 SAM 的解码器。实验结果表明，该修改是有效的，使得 SAM 在交互式 histology 图像分割中变得有用。代码可以在 \url{https://github.com/hvcl/SAM_Interactive_Histopathology} 上获取。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transferability-of-Learning-Models-for-Semantic-Segmentation-for-Remote-Sensing-Data"><a href="#On-the-Transferability-of-Learning-Models-for-Semantic-Segmentation-for-Remote-Sensing-Data" class="headerlink" title="On the Transferability of Learning Models for Semantic Segmentation for Remote Sensing Data"></a>On the Transferability of Learning Models for Semantic Segmentation for Remote Sensing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10490">http://arxiv.org/abs/2310.10490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gdaosu/transferability-remote-sensing">https://github.com/gdaosu/transferability-remote-sensing</a></li>
<li>paper_authors: Rongjun Qin, Guixiang Zhang, Yang Tang</li>
<li>for: 本研究旨在investigate remote sensing (RS) semantic segmentation&#x2F;classification任务上的传输性和适应性，以及如何通过领域适应（DA）方法提高深度学习（DL）模型的传输性。</li>
<li>methods: 本研究使用了四个高度不同的RS数据集，并将六个模型在不同的DA策略下进行训练，以量化模型之间的传输性和适应性。此外，我们还提出了一种简单的方法来评估模型在目标领域中的传输性，不需要标签数据。</li>
<li>results: 我们的实验结果显示，DL模型在不同领域之间的传输性较差，而DA策略可以有效地提高DL模型的传输性。此外，我们还发现了一些不常报道的 Raw和适应传输性的观察结果。我们的提出的标签 свобо�评估方法也被证明可以更好地评估模型的传输性。<details>
<summary>Abstract</summary>
Recent deep learning-based methods outperform traditional learning methods on remote sensing (RS) semantic segmentation/classification tasks. However, they require large training datasets and are generally known for lack of transferability due to the highly disparate RS image content across different geographical regions. Yet, there is no comprehensive analysis of their transferability, i.e., to which extent a model trained on a source domain can be readily applicable to a target domain. Therefore, in this paper, we aim to investigate the raw transferability of traditional and deep learning (DL) models, as well as the effectiveness of domain adaptation (DA) approaches in enhancing the transferability of the DL models (adapted transferability). By utilizing four highly diverse RS datasets, we train six models with and without three DA approaches to analyze their transferability between these datasets quantitatively. Furthermore, we developed a straightforward method to quantify the transferability of a model using the spectral indices as a medium and have demonstrated its effectiveness in evaluating the model transferability at the target domain when the labels are unavailable. Our experiments yield several generally important yet not well-reported observations regarding the raw and adapted transferability. Moreover, our proposed label-free transferability assessment method is validated to be better than posterior model confidence. The findings can guide the future development of generalized RS learning models. The trained models are released under this link: https://github.com/GDAOSU/Transferability-Remote-Sensing
</details>
<details>
<summary>摘要</summary>
现代深度学习方法在远程感知（RS）semantic segmentation/分类任务上表现出色，但它们需要大量的训练数据并且通常因为不同地区RS图像内容差异极大而无法转移。然而，没有系统性的分析转移性，即源领域训练的模型可以如何 extent 应用于目标领域。因此，在这篇论文中，我们想要调查传统和深度学习（DL）模型的原生转移性，以及使用领域适应（DA）策略可以提高DL模型的转移性（适应转移性）。通过使用四个高度不同RS数据集，我们训练了六个模型，并使用三种DA策略进行分析其转移性。此外，我们还提出了一种简单的方法来评估模型的转移性，使用spectral indices作为媒介，并在目标领域无标签情况下证明其效果。我们的实验结果表明了一些不常报道的观察结果，包括原生转移性和适应转移性的分析。此外，我们的提出的无标签转移性评估方法被证明为比 posterior model confidence 更有用。这些发现可以指导未来的RS学习模型的发展。我们训练的模型可以在以下链接获取：https://github.com/GDAOSU/Transferability-Remote-Sensing
</details></li>
</ul>
<hr>
<h2 id="Combating-Label-Noise-With-A-General-Surrogate-Model-For-Sample-Selection"><a href="#Combating-Label-Noise-With-A-General-Surrogate-Model-For-Sample-Selection" class="headerlink" title="Combating Label Noise With A General Surrogate Model For Sample Selection"></a>Combating Label Noise With A General Surrogate Model For Sample Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10463">http://arxiv.org/abs/2310.10463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Liang, Linchao Zhu, Humphrey Shi, Yi Yang</li>
<li>for: 减少标签噪音，提高深度学习系统的性能。</li>
<li>methods: 利用CLIPvision-language surrogate模型自动过滤噪音样本，并采用margin适应损失来规范选择偏好。</li>
<li>results: 在实际和Synthetic噪音数据集上实现了显著改进，无需CLIP在推断阶段参与。<details>
<summary>Abstract</summary>
Modern deep learning systems are data-hungry. Learning with web data is one of the feasible solutions, but will introduce label noise inevitably, which can hinder the performance of deep neural networks. Sample selection is an effective way to deal with label noise. The key is to separate clean samples based on some criterion. Previous methods pay more attention to the small loss criterion where small-loss samples are regarded as clean ones. Nevertheless, such a strategy relies on the learning dynamics of each data instance. Some noisy samples are still memorized due to frequently occurring corrupted learning patterns. To tackle this problem, a training-free surrogate model is preferred, freeing from the effect of memorization. In this work, we propose to leverage the vision-language surrogate model CLIP to filter noisy samples automatically. CLIP brings external knowledge to facilitate the selection of clean samples with its ability of text-image alignment. Furthermore, a margin adaptive loss is designed to regularize the selection bias introduced by CLIP, providing robustness to label noise. We validate the effectiveness of our proposed method on both real-world and synthetic noisy datasets. Our method achieves significant improvement without CLIP involved during the inference stage.
</details>
<details>
<summary>摘要</summary>
现代深度学习系统具有巨量数据的需求。使用网络数据进行学习是一个可行的解决方案，但会随机扰动标签，从而影响深度神经网络的性能。样本选择是一个有效的方法来处理标签噪音。以往的方法更多地关注于小损失标准，即将小损失的样本视为干净的样本。然而，这种策略基于每个数据实例的学习动态，一些噪音样本仍然会被记忆由频繁出现的异常学习模式。为解决这个问题，我们提议利用CLIP视觉语言代理模型自动过滤噪音样本。CLIP带来了外部知识，以便通过图文对齐来促进干净样本的选择。此外，我们还设计了一种margin适应损失，以规避CLIP的选择偏见，提供了对标签噪音的Robustness。我们在真实的噪音数据集和静态数据集上验证了我们的提议的有效性，而不需要在推理阶段使用CLIP。
</details></li>
</ul>
<hr>
<h2 id="Model-Selection-of-Anomaly-Detectors-in-the-Absence-of-Labeled-Validation-Data"><a href="#Model-Selection-of-Anomaly-Detectors-in-the-Absence-of-Labeled-Validation-Data" class="headerlink" title="Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data"></a>Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10461">http://arxiv.org/abs/2310.10461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Fung, Chen Qiu, Aodong Li, Maja Rudolph</li>
<li>for: 这个论文旨在提出一种通用的框架，用于评估基于图像的异常检测器。</li>
<li>methods: 该方法假设有一小支持集（support set）的正常图像，通过预训练的扩散模型进行处理，生成了人工异常样本。当混合到正常样本集中时，这些人工异常样本创建了一个适用于异常检测器评估的验证框架。</li>
<li>results: 在广泛的实验研究中，我们发现，使用我们的生成的验证数据可以选择同样的模型和超参数，与使用真实的验证集一样。此外，我们发现，使用我们的方法选择的提示（prompts）在CLIP基于异常检测中表现出色，超过其他所有提示策略，并在挑战性的MVTec-AD数据集上达到最佳检测精度。<details>
<summary>Abstract</summary>
Anomaly detection requires detecting abnormal samples in large unlabeled datasets. While progress in deep learning and the advent of foundation models has produced powerful unsupervised anomaly detection methods, their deployment in practice is often hindered by the lack of labeled data -- without it, the detection accuracy of an anomaly detector cannot be evaluated reliably. In this work, we propose a general-purpose framework for evaluating image-based anomaly detectors with synthetically generated validation data. Our method assumes access to a small support set of normal images which are processed with a pre-trained diffusion model (our proposed method requires no training or fine-tuning) to produce synthetic anomalies. When mixed with normal samples from the support set, the synthetic anomalies create detection tasks that compose a validation framework for anomaly detection evaluation and model selection. In an extensive empirical study, ranging from natural images to industrial applications, we find that our synthetic validation framework selects the same models and hyper-parameters as selection with a ground-truth validation set. In addition, we find that prompts selected by our method for CLIP-based anomaly detection outperforms all other prompt selection strategies, and leads to the overall best detection accuracy, even on the challenging MVTec-AD dataset.
</details>
<details>
<summary>摘要</summary>
异常检测需要检测大量无标签数据中的异常标本。虽然深度学习和基础模型的进步导致了无监督异常检测方法的生成，但它们在实践中的应用受到了无标签数据的缺乏影响，因为无法对异常检测器的准确性进行可靠评估。在这个工作中，我们提出一个通用的框架，用于评估基于图像的异常检测器，使用生成的运动模型来生成异常标本。我们的方法不需要训练或微调。当混合到支持集的正常图像中，生成的异常标本创建了一个异常检测任务，它们组成了一个适用于异常检测评估和模型选择的验证框架。在广泛的实验研究中，我们发现我们的生成验证框架可以选择同样的模型和参数，并且我们的提示选择策略在CLIP基础上的异常检测中表现出色，并且导致了整体最佳的检测精度，甚至在挑战性的MVTec-AD数据集上。
</details></li>
</ul>
<hr>
<h2 id="Object-Detection-in-Aerial-Images-in-Scarce-Data-Regimes"><a href="#Object-Detection-in-Aerial-Images-in-Scarce-Data-Regimes" class="headerlink" title="Object Detection in Aerial Images in Scarce Data Regimes"></a>Object Detection in Aerial Images in Scarce Data Regimes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10433">http://arxiv.org/abs/2310.10433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Le Jeune</li>
<li>for: 这个论文的目的是提高几个shot目标检测的性能，并评估其在不同类型图像上的可迁移性。</li>
<li>methods: 该论文使用了一种特殊的注意力机制来改进小对象的检测性能，以及一种批处理的盒子相似度标准来改进训练和评估。此外，论文还提出了两种不同的metric学习和精度调整方法来提高检测性能。</li>
<li>results: 论文得到了显著的提高在小对象检测上，并在 Cross-Domain FSOD 领域取得了卓越的结果。此外，论文还成功地解决了在 COSE 系统中部署检测模型的工程问题，并在具有超过 100 万像素的图像中进行实时检测。<details>
<summary>Abstract</summary>
Most contributions on Few-Shot Object Detection (FSOD) evaluate their methods on natural images only, yet the transferability of the announced performance is not guaranteed for applications on other kinds of images. We demonstrate this with an in-depth analysis of existing FSOD methods on aerial images and observed a large performance gap compared to natural images. Small objects, more numerous in aerial images, are the cause for the apparent performance gap between natural and aerial images. As a consequence, we improve FSOD performance on small objects with a carefully designed attention mechanism. In addition, we also propose a scale-adaptive box similarity criterion, that improves the training and evaluation of FSOD methods, particularly for small objects. We also contribute to generic FSOD with two distinct approaches based on metric learning and fine-tuning. Impressive results are achieved with the fine-tuning method, which encourages tackling more complex scenarios such as Cross-Domain FSOD. We conduct preliminary experiments in this direction and obtain promising results. Finally, we address the deployment of the detection models inside COSE's systems. Detection must be done in real-time in extremely large images (more than 100 megapixels), with limited computation power. Leveraging existing optimization tools such as TensorRT, we successfully tackle this engineering challenge.
</details>
<details>
<summary>摘要</summary>
多数对几shot对象检测（FSOD）的贡献仅测试在自然图像上，但是这些方法的可传性并不保证在其他类型图像上的应用。我们通过对现有FSOD方法的深入分析在飞行图像上表明，小对象的众多性导致自然图像和飞行图像之间的性能差距。为了解决这个问题，我们采用了特别设计的注意机制来提高小对象的检测性能。此外，我们还提出了可以适应不同大小的盒子相似性标准，以改进FSOD方法的训练和评估。此外，我们还提出了基于度量学习和精度调整的两种不同方法来提高FSOD性能。经过精心调整，我们得到了很好的结果。最后，我们关注在COSE系统中部署检测模型。在具有EXTREMELY大图像（超过100 megapixel）和有限计算资源的情况下，我们成功使用现有的优化工具 such as TensorRT来解决这个工程问题。
</details></li>
</ul>
<hr>
<h2 id="DANAA-Towards-transferable-attacks-with-double-adversarial-neuron-attribution"><a href="#DANAA-Towards-transferable-attacks-with-double-adversarial-neuron-attribution" class="headerlink" title="DANAA: Towards transferable attacks with double adversarial neuron attribution"></a>DANAA: Towards transferable attacks with double adversarial neuron attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10427">http://arxiv.org/abs/2310.10427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Davidjinzb/DANAA">https://github.com/Davidjinzb/DANAA</a></li>
<li>paper_authors: Zhibo Jin, Zhiyu Zhu, Xinyi Wang, Jiayu Zhang, Jun Shen, Huaming Chen</li>
<li>for: 本文旨在提出一种基于中间层的双反抗智能方法，以提高深度神经网络模型中特征重要性的估计结果，并提高模型的抗击性能。</li>
<li>methods: 本文提出了一种基于对抗非线性路径的双反抗神经网络模型，通过计算中间层输出的各个神经元的贡献，以估计特征重要性。</li>
<li>results: 对多个基准数据集进行了广泛的实验，并证明了我们的方法可以达到当今最佳性能。我们的代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Davidjinzb/DANAA">https://github.com/Davidjinzb/DANAA</a><details>
<summary>Abstract</summary>
While deep neural networks have excellent results in many fields, they are susceptible to interference from attacking samples resulting in erroneous judgments. Feature-level attacks are one of the effective attack types, which targets the learnt features in the hidden layers to improve its transferability across different models. Yet it is observed that the transferability has been largely impacted by the neuron importance estimation results. In this paper, a double adversarial neuron attribution attack method, termed `DANAA', is proposed to obtain more accurate feature importance estimation. In our method, the model outputs are attributed to the middle layer based on an adversarial non-linear path. The goal is to measure the weight of individual neurons and retain the features that are more important towards transferability. We have conducted extensive experiments on the benchmark datasets to demonstrate the state-of-the-art performance of our method. Our code is available at: https://github.com/Davidjinzb/DANAA
</details>
<details>
<summary>摘要</summary>
深度神经网络在多个领域取得了出色的成绩，但它们受到攻击样本的干扰，导致评判结果错误。攻击样本是一种有效的攻击类型，targets the learnt features in the hidden layers to improve its transferability across different models。然而，我们发现，通过neuron importance estimation结果，攻击样本的传播性受到了很大的影响。在这篇论文中，我们提出了一种double adversarial neuron attribution attack方法，称为`DANAA'。我们的方法基于一个 adversarial non-linear path，将模型输出归结到中层。我们的目标是测量个体神经元的重要性，保留对传播性更重要的特征。我们在标准 benchmark datasets 上进行了广泛的实验，以示出我们的方法的state-of-the-art性。我们的代码可以在：https://github.com/Davidjinzb/DANAA 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Benchmarking-Paradigm-and-a-Scale-and-Motion-Aware-Model-for-Egocentric-Pedestrian-Trajectory-Prediction"><a href="#A-Novel-Benchmarking-Paradigm-and-a-Scale-and-Motion-Aware-Model-for-Egocentric-Pedestrian-Trajectory-Prediction" class="headerlink" title="A Novel Benchmarking Paradigm and a Scale- and Motion-Aware Model for Egocentric Pedestrian Trajectory Prediction"></a>A Novel Benchmarking Paradigm and a Scale- and Motion-Aware Model for Egocentric Pedestrian Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10424">http://arxiv.org/abs/2310.10424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Rasouli</li>
<li>for: 本研究旨在提高智能驾驶系统中对行人行为预测的精度。</li>
<li>methods: 本文提出了一种新的 egocentric 行人轨迹预测算法评估方法，基于不同的情况下的 contextual information，提取了 meaningful 和系统的 driving scenarios，并提出了一种更有效的 metric 来评估预测模型。</li>
<li>results: 对 existed 模型进行了广泛的 empirical 研究，暴露了不同方法在不同情况下的缺陷和优势，并显示了我们的方法在挑战性情况下可以达到40%的提高。<details>
<summary>Abstract</summary>
Predicting pedestrian behavior is one of the main challenges for intelligent driving systems. In this paper, we present a new paradigm for evaluating egocentric pedestrian trajectory prediction algorithms. Based on various contextual information, we extract driving scenarios for a meaningful and systematic approach to identifying challenges for prediction models. In this regard, we also propose a new metric for more effective ranking within the scenario-based evaluation. We conduct extensive empirical studies of existing models on these scenarios to expose shortcomings and strengths of different approaches. The scenario-based analysis highlights the importance of using multimodal sources of information and challenges caused by inadequate modeling of ego-motion and scale of pedestrians. To this end, we propose a novel egocentric trajectory prediction model that benefits from multimodal sources of data fused in an effective and efficient step-wise hierarchical fashion and two auxiliary tasks designed to learn more robust representation of scene dynamics. We show that our approach achieves significant improvement by up to 40% in challenging scenarios compared to the past arts via empirical evaluation on common benchmark datasets.
</details>
<details>
<summary>摘要</summary>
预测行人行为是智能驾驶系统中的一个主要挑战。在这篇论文中，我们提出了一种新的评估 egocentric 行人轨迹预测算法的新模式。基于多种情境信息，我们提取了 meaningful 和系统的驾驶场景，以便更好地识别预测模型的挑战。在这个 regard，我们也提出了一个更有效的排名 metric。我们对现有模型进行了广泛的实证研究，以暴露不同方法的缺陷和优势。场景基本分析表明，使用多modal 信息和 egocentric 行人的不充分模型和比例会导致预测困难。为此，我们提出了一种新的 egocentric 轨迹预测模型，该模型利用多modal 数据的综合和有效的步骤式层次结构，以及两个辅助任务，以学习更加稳定的Scene 动力学。我们的方法在复杂的场景下比过去的艺术品 achieved 40% 的提升，via 实验评估常见的 benchmark 数据集。
</details></li>
</ul>
<hr>
<h2 id="YOLOv7-for-Mosquito-Breeding-Grounds-Detection-and-Tracking"><a href="#YOLOv7-for-Mosquito-Breeding-Grounds-Detection-and-Tracking" class="headerlink" title="YOLOv7 for Mosquito Breeding Grounds Detection and Tracking"></a>YOLOv7 for Mosquito Breeding Grounds Detection and Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10423">http://arxiv.org/abs/2310.10423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camila Laranjeira, Daniel Andrade, Jefersson A. dos Santos</li>
<li>for: 防止气候变化的威胁，忽略性 tropical diseases 如遗传性疟疾、血吸虫病和生物攻击等可能成为全球问题。</li>
<li>methods: 本文使用 YOLOv7 状态之 искусственный神经网络方法，自动检测和地图蚊子繁殖地点，以便地方机构可以有效 intervene。</li>
<li>results: 我们在 ICIP 2023 大赛中发布的数据集上进行了实验，并示出 YOLOv7 可以直接应用于检测更大的ocus类别，如池塘、车胎和水箱，并且可以通过简单的滤波来实现时间一致性的跟踪过程。<details>
<summary>Abstract</summary>
With the looming threat of climate change, neglected tropical diseases such as dengue, zika, and chikungunya have the potential to become an even greater global concern. Remote sensing technologies can aid in controlling the spread of Aedes Aegypti, the transmission vector of such diseases, by automating the detection and mapping of mosquito breeding sites, such that local entities can properly intervene. In this work, we leverage YOLOv7, a state-of-the-art and computationally efficient detection approach, to localize and track mosquito foci in videos captured by unmanned aerial vehicles. We experiment on a dataset released to the public as part of the ICIP 2023 grand challenge entitled Automatic Detection of Mosquito Breeding Grounds. We show that YOLOv7 can be directly applied to detect larger foci categories such as pools, tires, and water tanks and that a cheap and straightforward aggregation of frame-by-frame detection can incorporate time consistency into the tracking process.
</details>
<details>
<summary>摘要</summary>
With the looming threat of climate change, neglected tropical diseases such as dengue, zika, and chikungunya have the potential to become an even greater global concern. Remote sensing technologies can aid in controlling the spread of Aedes Aegypti, the transmission vector of such diseases, by automating the detection and mapping of mosquito breeding sites, such that local entities can properly intervene. In this work, we leverage YOLOv7, a state-of-the-art and computationally efficient detection approach, to localize and track mosquito foci in videos captured by unmanned aerial vehicles. We experiment on a dataset released to the public as part of the ICIP 2023 grand challenge entitled Automatic Detection of Mosquito Breeding Grounds. We show that YOLOv7 can be directly applied to detect larger foci categories such as pools, tires, and water tanks, and that a cheap and straightforward aggregation of frame-by-frame detection can incorporate time consistency into the tracking process.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="LMT-Longitudinal-Mixing-Training-a-Framework-to-Predict-Disease-Progression-from-a-Single-Image"><a href="#LMT-Longitudinal-Mixing-Training-a-Framework-to-Predict-Disease-Progression-from-a-Single-Image" class="headerlink" title="LMT: Longitudinal Mixing Training, a Framework to Predict Disease Progression from a Single Image"></a>LMT: Longitudinal Mixing Training, a Framework to Predict Disease Progression from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10420">http://arxiv.org/abs/2310.10420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Hugo Le boite, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard</li>
<li>for: 这个论文旨在检测和预测糖尿病Retinopathy (DR) 疾病进程的扩展。</li>
<li>methods: 该论文使用了混合训练和预text任务，并使用了新的神经网络模型 named Neural Ordinary Differential Equation (NODE)。</li>
<li>results: 该论文在使用 Longitudinal Mixing Training (LMT) 框架时，可以更好地检测和预测 DR 疾病进程。在 OPHDIAT 长itudinal retinal Color Fundus Photographs (CFP)  dataset 上，该方法可以预测下一次访问 whether an eye would develop a severe DR，其 AUC 为 0.798，比基线结果高得多。<details>
<summary>Abstract</summary>
Longitudinal imaging is able to capture both static anatomical structures and dynamic changes in disease progression toward earlier and better patient-specific pathology management. However, conventional approaches rarely take advantage of longitudinal information for detection and prediction purposes, especially for Diabetic Retinopathy (DR). In the past years, Mix-up training and pretext tasks with longitudinal context have effectively enhanced DR classification results and captured disease progression. In the meantime, a novel type of neural network named Neural Ordinary Differential Equation (NODE) has been proposed for solving ordinary differential equations, with a neural network treated as a black box. By definition, NODE is well suited for solving time-related problems. In this paper, we propose to combine these three aspects to detect and predict DR progression. Our framework, Longitudinal Mixing Training (LMT), can be considered both as a regularizer and as a pretext task that encodes the disease progression in the latent space. Additionally, we evaluate the trained model weights on a downstream task with a longitudinal context using standard and longitudinal pretext tasks. We introduce a new way to train time-aware models using $t_{mix}$, a weighted average time between two consecutive examinations. We compare our approach to standard mixing training on DR classification using OPHDIAT a longitudinal retinal Color Fundus Photographs (CFP) dataset. We were able to predict whether an eye would develop a severe DR in the following visit using a single image, with an AUC of 0.798 compared to baseline results of 0.641. Our results indicate that our longitudinal pretext task can learn the progression of DR disease and that introducing $t_{mix}$ augmentation is beneficial for time-aware models.
</details>
<details>
<summary>摘要</summary>
长itudinal imaging可以捕捉到稳定的解剖结构以及疾病进程的动态变化，从而提供更早和更好的病理管理。然而，传统方法rarely利用长itudinal信息进行检测和预测，特别是对于肥胖糖尿病（DR）。在过去几年，杂合训练和预文任务with longitudinal context有效提高了DR分类结果，并捕捉了疾病进程。同时，一种新的神经网络模型名为神经常微方程（NODE）已经被提出，可以解决常微方程问题。根据定义，NODE适合解决时间相关的问题。在这篇文章中，我们提议将这三个方面结合，以检测和预测DR疾病进程。我们的框架，长itudinal Mixing Training（LMT），可以被视为一种正则化和预文任务，用于编码疾病进程在幽默空间中。此外，我们评估训练模型的加权因子在下游任务中使用标准和长itudinal预文任务时的性能。我们还介绍了一种新的时间感知训练方法，使用 $t_{mix} $ Weighted average time between two consecutive examinations。我们比较了我们的方法与标准杂合训练在DR分类中的表现，使用 OPHDIAT  longitudinal retinal Color Fundus Photographs（CFP）数据集。我们能够使用单个图像预测眼睛是否在下一次访问中会发展严重的DR，AUC为 0.798，比基eline结果提高了0.641。我们的结果表明，我们的长itudinal预文任务可以学习DR疾病的进程，并且将 $t_{mix} $ 杂合增强是对时间感知模型有利。
</details></li>
</ul>
<hr>
<h2 id="Prior-Free-Continual-Learning-with-Unlabeled-Data-in-the-Wild"><a href="#Prior-Free-Continual-Learning-with-Unlabeled-Data-in-the-Wild" class="headerlink" title="Prior-Free Continual Learning with Unlabeled Data in the Wild"></a>Prior-Free Continual Learning with Unlabeled Data in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10417">http://arxiv.org/abs/2310.10417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visiontao/pfcl">https://github.com/visiontao/pfcl</a></li>
<li>paper_authors: Tao Zhuo, Zhiyong Cheng, Hehe Fan, Mohan Kankanhalli</li>
<li>for: 本研究旨在 Addressing the problem of forgetting in continual learning (CL) when task priors are unknown in real-world applications.</li>
<li>methods: 我们提出了一个 Prior-Free Continual Learning (PFCL) 方法，不需要任务识别或先前的数据。我们透过两个方法来实现这一目标：首先，我们消除了任务识别的需求，以使用固定单头架构中的任务特定出力头选择。其次，我们使用了一种常见性预测策略，以避免在新任务上重访先前的数据。然而，这种方法单独可能在分类增量学习中表现不佳，特别是当任务序列很长时。我们运用了一个辅助数据集来增强模型的一致性，以提高分类精度。</li>
<li>results: 我们的PFCL方法在多个图像分类benchmarkdataset上进行了广泛的实验，结果显示PFCL方法能够对所有三种学习场景进行优化，并且与最近的回温基本方法相比，PFCL方法在竞争率上具有相似的精度。<details>
<summary>Abstract</summary>
Continual Learning (CL) aims to incrementally update a trained model on new tasks without forgetting the acquired knowledge of old ones. Existing CL methods usually reduce forgetting with task priors, \ie using task identity or a subset of previously seen samples for model training. However, these methods would be infeasible when such priors are unknown in real-world applications. To address this fundamental but seldom-studied problem, we propose a Prior-Free Continual Learning (PFCL) method, which learns new tasks without knowing the task identity or any previous data. First, based on a fixed single-head architecture, we eliminate the need for task identity to select the task-specific output head. Second, we employ a regularization-based strategy for consistent predictions between the new and old models, avoiding revisiting previous samples. However, using this strategy alone often performs poorly in class-incremental scenarios, particularly for a long sequence of tasks. By analyzing the effectiveness and limitations of conventional regularization-based methods, we propose enhancing model consistency with an auxiliary unlabeled dataset additionally. Moreover, since some auxiliary data may degrade the performance, we further develop a reliable sample selection strategy to obtain consistent performance improvement. Extensive experiments on multiple image classification benchmark datasets show that our PFCL method significantly mitigates forgetting in all three learning scenarios. Furthermore, when compared to the most recent rehearsal-based methods that replay a limited number of previous samples, PFCL achieves competitive accuracy. Our code is available at: https://github.com/visiontao/pfcl
</details>
<details>
<summary>摘要</summary>
Our approach has two key components. First, we eliminate the need for task identity to select the task-specific output head using a fixed single-head architecture. Second, we employ a regularization-based strategy for consistent predictions between the new and old models, avoiding revisiting previous samples. However, this strategy alone can perform poorly in class-incremental scenarios, so we also use an auxiliary unlabeled dataset to enhance model consistency.To ensure reliable performance improvement, we develop a sample selection strategy to choose the most informative samples from the auxiliary dataset. Our approach significantly mitigates forgetting in all three learning scenarios, and achieves competitive accuracy compared to rehearsal-based methods that replay a limited number of previous samples.Our code is available at: https://github.com/visiontao/pfcl.
</details></li>
</ul>
<hr>
<h2 id="Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings"><a href="#Style-transfer-between-Microscopy-and-Magnetic-Resonance-Imaging-via-Generative-Adversarial-Network-in-small-sample-size-settings" class="headerlink" title="Style transfer between Microscopy and Magnetic Resonance Imaging via Generative Adversarial Network in small sample size settings"></a>Style transfer between Microscopy and Magnetic Resonance Imaging via Generative Adversarial Network in small sample size settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10414">http://arxiv.org/abs/2310.10414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monika Pytlarz, Adrian Onicas, Alessandro Crimi</li>
<li>for: 这个研究的目的是使用 Conditional GAN 架构将 MRI 图像翻译成历史学图像，以便避免侵入性的生物псии检测。</li>
<li>methods: 这个研究使用的方法是使用 Conditional GAN 架构，该架构可以将 MRI 图像翻译成历史学图像。</li>
<li>results: 这个研究表明，使用 Conditional GAN 架构可以可靠地将 MRI 图像翻译成历史学图像，并且可以使用高分辨率的历史学图像和相对较低分辨率的 MRI 图像进行对应。<details>
<summary>Abstract</summary>
Cross-modal augmentation of Magnetic Resonance Imaging (MRI) and microscopic imaging based on the same tissue samples is promising because it can allow histopathological analysis in the absence of an underlying invasive biopsy procedure. Here, we tested a method for generating microscopic histological images from MRI scans of the corpus callosum using conditional generative adversarial network (cGAN) architecture. To our knowledge, this is the first multimodal translation of the brain MRI to histological volumetric representation of the same sample. The technique was assessed by training paired image translation models taking sets of images from MRI scans and microscopy. The use of cGAN for this purpose is challenging because microscopy images are large in size and typically have low sample availability. The current work demonstrates that the framework reliably synthesizes histology images from MRI scans of corpus callosum, emphasizing the network's ability to train on high resolution histologies paired with relatively lower-resolution MRI scans. With the ultimate goal of avoiding biopsies, the proposed tool can be used for educational purposes.
</details>
<details>
<summary>摘要</summary>
通过同一个组织样本的跨模态扩充，核磁共振成像（MRI）和微scopic成像之间的同化是有前途的，因为它可以允许在不基于侵入性生物псиchosurgeries的情况下进行 histopathological 分析。我们在这里测试了一种方法，用于从 MRI 扫描中生成 microscopic  histological 图像。根据我们所知，这是第一种跨modal 翻译 brain MRI 到 histological 三维表示的方法。这种技术被评估了，通过对对应的图像翻译模型进行训练。使用 cGAN 进行这种目的是挑战性的，因为微scopic 图像通常很大，并且 Sample 的可用性很低。当前的研究表明，该框架可靠地将 MRI 扫描中的 corpus callosum 转换为 histology 图像，强调网络的能力在高分辨率 histology 和相对较低分辨率 MRI 扫描之间进行训练。以避免生物псиchosurgeries 为目的，提出的工具可以用于教育用途。
</details></li>
</ul>
<hr>
<h2 id="Image-super-resolution-via-dynamic-network"><a href="#Image-super-resolution-via-dynamic-network" class="headerlink" title="Image super-resolution via dynamic network"></a>Image super-resolution via dynamic network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10413">http://arxiv.org/abs/2310.10413</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellloxiaotian/dsrnet">https://github.com/hellloxiaotian/dsrnet</a></li>
<li>paper_authors: Chunwei Tian, Xuanyu Zhang, Qi Zhang, Mingming Yang, Zhaojie Ju</li>
<li>for: 这篇论文旨在提出一种动态网络 для图像超解像（DSRNet），以提高图像超解像的准确率和复杂场景下的应用性。</li>
<li>methods: 该网络使用了差异增强块、宽增强块、特征细化块和结构块等多种块来提高图像超解像的精度和可靠性。</li>
<li>results: 实验结果表明，与传统方法相比，DSRNet能够更好地处理复杂场景下的图像超解像问题，同时具有较低的计算量和可扩展性，适用于移动设备上的实时应用。<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) depend on deep network architectures to extract accurate information for image super-resolution. However, obtained information of these CNNs cannot completely express predicted high-quality images for complex scenes. In this paper, we present a dynamic network for image super-resolution (DSRNet), which contains a residual enhancement block, wide enhancement block, feature refinement block and construction block. The residual enhancement block is composed of a residual enhanced architecture to facilitate hierarchical features for image super-resolution. To enhance robustness of obtained super-resolution model for complex scenes, a wide enhancement block achieves a dynamic architecture to learn more robust information to enhance applicability of an obtained super-resolution model for varying scenes. To prevent interference of components in a wide enhancement block, a refinement block utilizes a stacked architecture to accurately learn obtained features. Also, a residual learning operation is embedded in the refinement block to prevent long-term dependency problem. Finally, a construction block is responsible for reconstructing high-quality images. Designed heterogeneous architecture can not only facilitate richer structural information, but also be lightweight, which is suitable for mobile digital devices. Experimental results shows that our method is more competitive in terms of performance and recovering time of image super-resolution and complexity. The code of DSRNet can be obtained at https://github.com/hellloxiaotian/DSRNet.
</details>
<details>
<summary>摘要</summary>
convolutional neural networks (CNNs) 依靠深度网络架构来提取图像超分解中的准确信息。然而，这些 CNNs 所获取的信息无法完全表达预测的高质量图像 для复杂场景。在这篇论文中，我们提出了动态网络 для图像超分解 (DSRNet)，它包含了差异增强块、宽增强块、特征细化块和结构块。差异增强块由一个差异增强架构组成，以便在图像超分解中提高层次特征。为了提高获取的超分解模型在复杂场景中的应用 robustness，宽增强块实现了一个动态架构，以学习更加Robust的信息以提高获取的超分解模型的可用性。为了避免各个组件之间的干扰，特征细化块使用了堆叠结构来准确地学习获取的特征。此外，在细化块中还包含了循环学习操作，以避免长期依赖问题。最后，结构块负责重建高质量图像。设计的不同化架构不仅可以提供更加丰富的结构信息，还可以减轻计算负担，适用于移动式数字设备。实验结果表明，我们的方法在性能和图像重建时间方面更加竞争力，同时也更加复杂。DSRNet 的代码可以在 https://github.com/hellloxiaotian/DSRNet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Loci-Segmented-Improving-Scene-Segmentation-Learning"><a href="#Loci-Segmented-Improving-Scene-Segmentation-Learning" class="headerlink" title="Loci-Segmented: Improving Scene Segmentation Learning"></a>Loci-Segmented: Improving Scene Segmentation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10410">http://arxiv.org/abs/2310.10410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CognitiveModeling/Loci-Segmented">https://github.com/CognitiveModeling/Loci-Segmented</a></li>
<li>paper_authors: Manuel Traub, Frederic Becker, Adrian Sauter, Sebastian Otte, Martin V. Butz</li>
<li>for: 本研究旨在提高场景表示的分割能力，并提出了一种基于槽的处理方法。</li>
<li>methods: 本方法使用了一种名为Loci-Segmented（Loci-s）的场景分割神经网络，它基于Loci（Traub等，ICLR 2023）框架，并具有以下三大提升：（1）添加了预训练的动态背景模块；（2）具有对象专注的几何卷积编码模块；（3）采用了级联解码模块，successively生成对象Mask、Masked Depth Maps和Masked, Depth-map-informed RGB重建。</li>
<li>results: 对比于之前的最佳成果，Loci-s在MOVi datasets和另一个Established dataset集合中实现了32%的交集覆盖率（IoU）提升。此外，Loci-s还生成了良好的可解释性 latent representation，这些表示可能作为解释基础模型的可解释基础 для解决下游任务，如语言背景和Context-和Goal-conditioned Event Processing。<details>
<summary>Abstract</summary>
Slot-oriented processing approaches for compositional scene representation have recently undergone a tremendous development. We present Loci-Segmented (Loci-s), an advanced scene segmentation neural network that extends the slot-based location and identity tracking architecture Loci (Traub et al., ICLR 2023). The main advancements are (i) the addition of a pre-trained dynamic background module; (ii) a hyper-convolution encoder module, which enables object-focused bottom-up processing; and (iii) a cascaded decoder module, which successively generates object masks, masked depth maps, and masked, depth-map-informed RGB reconstructions. The background module features the learning of both a foreground identifying module and a background re-generator. We further improve performance via (a) the integration of depth information as well as improved slot assignments via (b) slot-location-entity regularization and (b) a prior segmentation network. Even without these latter improvements, the results reveal superior segmentation performance in the MOVi datasets and in another established dataset collection. With all improvements, Loci-s achieves a 32% better intersection over union (IoU) score in MOVi-E than the previous best. We furthermore show that Loci-s generates well-interpretable latent representations. We believe that these representations may serve as a foundation-model-like interpretable basis for solving downstream tasks, such as grounding language and context- and goal-conditioned event processing.
</details>
<details>
<summary>摘要</summary>
各种槽处理方法在 compositional scene representation 领域受到了非常大的发展。我们现在提出了 Loci-Segmented（Loci-s），这是一种高级的Scene segmentation neural network，它扩展了 Loci（Traub et al., ICLR 2023）槽基 Architecture。主要改进包括：(i) 添加了预训练的动态背景模块；(ii) 使用了对象专注的凹陷 Encoder 模块，以便从底层处进行对象特征提取；(iii) 使用了级联的解码模块，以顺序生成对象面积、掩码depth maps和掩码、 depth-map-informed RGB 重建。背景模块包括学习both foreground 特征和背景重建。我们进一步提高性能通过：(a)  интеграción of depth information以及改进的槽分配via(b) slot-location-entity regularization和(b) 一个前 segmentation network。无论这些改进，Loci-s 在 MOVi 数据集和另一个已知数据集中显示出色的 segmentation 性能。通过所有改进，Loci-s 在 MOVi-E 中实现了与之前最佳的32%的交集 над union（IoU）分数提高。我们进一步表明Loci-s 生成的latent representations是可解释的。我们认为这些表示可以作为基础模型的可解释基础，用于解决下游任务，如语言基础和上下文-和目标conditioned事件处理。
</details></li>
</ul>
<hr>
<h2 id="A-cross-Transformer-for-image-denoising"><a href="#A-cross-Transformer-for-image-denoising" class="headerlink" title="A cross Transformer for image denoising"></a>A cross Transformer for image denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10408">http://arxiv.org/abs/2310.10408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hellloxiaotian/ctnet">https://github.com/hellloxiaotian/ctnet</a></li>
<li>paper_authors: Chunwei Tian, Menghua Zheng, Wangmeng Zuo, Shichao Zhang, Yanning Zhang, Chia-Wen Ling</li>
<li>for: 提高复杂场景中图像的清洁率</li>
<li>methods: 使用交叉 transformer 涨梯网络（CTNet），包括序列块（SB）、平行块（PB）和差分块（RB），以获取有效的结构信息，并通过多种交互来提高适应性。</li>
<li>results: 在实际和synthetic图像杂交中，CTNet表现出色，超过了一些流行的去噪方法。适用于移动设备，如手机。<details>
<summary>Abstract</summary>
Deep convolutional neural networks (CNNs) depend on feedforward and feedback ways to obtain good performance in image denoising. However, how to obtain effective structural information via CNNs to efficiently represent given noisy images is key for complex scenes. In this paper, we propose a cross Transformer denoising CNN (CTNet) with a serial block (SB), a parallel block (PB), and a residual block (RB) to obtain clean images for complex scenes. A SB uses an enhanced residual architecture to deeply search structural information for image denoising. To avoid loss of key information, PB uses three heterogeneous networks to implement multiple interactions of multi-level features to broadly search for extra information for improving the adaptability of an obtained denoiser for complex scenes. Also, to improve denoising performance, Transformer mechanisms are embedded into the SB and PB to extract complementary salient features for effectively removing noise in terms of pixel relations. Finally, a RB is applied to acquire clean images. Experiments illustrate that our CTNet is superior to some popular denoising methods in terms of real and synthetic image denoising. It is suitable to mobile digital devices, i.e., phones. Codes can be obtained at https://github.com/hellloxiaotian/CTNet.
</details>
<details>
<summary>摘要</summary>
深度卷积神经网络 (CNN) 在图像噪声除除针对 feedforward 和反馈方式以获得好的表现。然而，如何通过 CNN 获得有效的结构信息，以有效地表示给定的噪声图像是关键问题。在这篇论文中，我们提出了一种跨Transformer混合卷积神经网络 (CTNet)，包括序列块 (SB)、平行块 (PB) 和差异块 (RB)，以获取复杂场景中的干净图像。SB 使用增强的剩余架构，深入搜索图像噪声除除中的结构信息。为了避免关键信息损失，PB 使用三种不同的网络来实现多种交互，以广泛搜索更多的信息，以提高取得的噪声除除器的适应性。此外，为了提高噪声除除性能，SB 和 PB 中包含了Transformer机制，以EXTRACT complementary salient features，以有效地除除图像层次关系中的噪声。最后，RB 用于获取干净图像。实验表明，我们的 CTNet 在实际和 sintetic 图像噪声除除方面表现优于一些流行的噪声除除方法。它适用于移动设备，如手机。代码可以在 https://github.com/hellloxiaotian/CTNet  obtener。
</details></li>
</ul>
<hr>
<h2 id="LLM4SGG-Large-Language-Model-for-Weakly-Supervised-Scene-Graph-Generation"><a href="#LLM4SGG-Large-Language-Model-for-Weakly-Supervised-Scene-Graph-Generation" class="headerlink" title="LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation"></a>LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10404">http://arxiv.org/abs/2310.10404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rlqja1107/torch-LLM4SGG">https://github.com/rlqja1107/torch-LLM4SGG</a></li>
<li>paper_authors: Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park</li>
<li>for: 本研究旨在提出一种新的、基于语言模型的弱监督Scene Graph生成方法（LLM4SGG），以解决现有WSSGG方法中的两个问题：1）语义过度简化问题，2）低密度场景图问题。</li>
<li>methods: 我们提出一种新的方法，即使用语言模型的语言理解和推理能力来提取caption中的 triplets，并将entity&#x2F; predicate类与目标数据进行对齐。为了更好地利用语言模型，我们采用了链式思维和在Context few-shot learning策略。</li>
<li>results: 我们在Visual Genome和GQA datasets上进行了广泛的实验，并显示了与现有WSSGG方法相比的显著提高，包括Recall@K和mean Recall@K的提高。此外，LLM4SGG还具有数据效率的优势，可以通过小量的训练图像进行效果iveness的模型训练。<details>
<summary>Abstract</summary>
Weakly-Supervised Scene Graph Generation (WSSGG) research has recently emerged as an alternative to the fully-supervised approach that heavily relies on costly annotations. In this regard, studies on WSSGG have utilized image captions to obtain unlocalized triplets while primarily focusing on grounding the unlocalized triplets over image regions. However, they have overlooked the two issues involved in the triplet formation process from the captions: 1) Semantic over-simplification issue arises when extracting triplets from captions, where fine-grained predicates in captions are undesirably converted into coarse-grained predicates, resulting in a long-tailed predicate distribution, and 2) Low-density scene graph issue arises when aligning the triplets in the caption with entity/predicate classes of interest, where many triplets are discarded and not used in training, leading to insufficient supervision. To tackle the two issues, we propose a new approach, i.e., Large Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two issues by leveraging the LLM's in-depth understanding of language and reasoning ability during the extraction of triplets from captions and alignment of entity/predicate classes with target data. To further engage the LLM in these processes, we adopt the idea of Chain-of-Thought and the in-context few-shot learning strategy. To validate the effectiveness of LLM4SGG, we conduct extensive experiments on Visual Genome and GQA datasets, showing significant improvements in both Recall@K and mean Recall@K compared to the state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is data-efficient, enabling effective model training with a small amount of training images.
</details>
<details>
<summary>摘要</summary>
弱监督场景图生成（WSSGG）研究最近几年来得到了更多的关注，它作为完全监督的方法的替代方案，减少了成本的注释。在这个 regard，研究者们通过图文来获取不地址 triplets，主要是将图文中的不地址 triplets与图像区域相对应。然而，他们忽略了图文中 triplet 形成过程中的两个问题：1）语义过于简化问题，图文中的细腻 predicate 被不必要地转换为粗略 predicate，导致 predicate 的分布呈长尾形态；2）图像区域对应问题，在将 triplets 与 interesset class 对应时，多个 triplets 被抛弃，导致训练不充分。为解决这两个问题，我们提出了一种新的方法，即大语言模型 для 弱监督 SGG（LLM4SGG）。我们通过利用 LLM 的深刻语言理解和逻辑能力来缓解这两个问题。为了更好地利用 LLM，我们采用了链条思想和在 Context 中几招学习策略。为验证 LLM4SGG 的有效性，我们对 Visual Genome 和 GQA 数据集进行了广泛的实验，并显示了与州chart 方法相比的显著改善。此外，LLM4SGG 具有数据效率的特点，可以在小量训练图像上进行有效的模型训练。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Edge-Perceptual-Guided-Image-Filtering"><a href="#Enhanced-Edge-Perceptual-Guided-Image-Filtering" class="headerlink" title="Enhanced Edge-Perceptual Guided Image Filtering"></a>Enhanced Edge-Perceptual Guided Image Filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10387">http://arxiv.org/abs/2310.10387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Li</li>
<li>for: 提高图像Edge-preserving能力和计算复杂性的问题</li>
<li>methods: 提出一种基于Explicit first-order edge-protect约束和Explicit residual约束的新导向图像滤波器</li>
<li>results: 在单图像细节增强、多尺度曝光融合和多spectral图像分类等应用中，提出的滤波器能够提高Edge-preserving能力，并且经过理论分析和实验验证<details>
<summary>Abstract</summary>
Due to the powerful edge-preserving ability and low computational complexity, Guided image filter (GIF) and its improved versions has been widely applied in computer vision and image processing. However, all of them are suffered halo artifacts to some degree, as the regularization parameter increase. In the case of inconsistent structure of guidance image and input image, edge-preserving ability degradation will also happen. In this paper, a novel guided image filter is proposed by integrating an explicit first-order edge-protect constraint and an explicit residual constraint which will improve the edge-preserving ability in both cases. To illustrate the efficiency of the proposed filter, the performances are shown in some typical applications, which are single image detail enhancement, multi-scale exposure fusion, hyper spectral images classification. Both theoretical analysis and experimental results prove that the powerful edge-preserving ability of the proposed filter.
</details>
<details>
<summary>摘要</summary>
因为导引图像过滤器（GIF）和其改进版本具有强大的边缘保持能力和低计算复杂性，因此在计算机视觉和图像处理领域得到广泛应用。然而，所有它们都受到一定程度的尘埃畸 artifacts的困扰，随着规则化参数的增加。在指导图像和输入图像的结构不一致的情况下，边缘保持能力也会降低。在这篇论文中，一种新的导引图像过滤器被提出，通过加入显式的第一阶边缘保护约束和显式的差分约束，以提高边缘保持能力。为了证明提案的效果，在一些典型应用中进行了表现，包括单图像细节增强、多比例曝光融合和多spectral图像分类。 Both theoretical analysis and experimental results prove that the proposed filter has powerful edge-preserving ability.
</details></li>
</ul>
<hr>
<h2 id="Looping-LOCI-Developing-Object-Permanence-from-Videos"><a href="#Looping-LOCI-Developing-Object-Permanence-from-Videos" class="headerlink" title="Looping LOCI: Developing Object Permanence from Videos"></a>Looping LOCI: Developing Object Permanence from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10372">http://arxiv.org/abs/2310.10372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Traub, Frederic Becker, Sebastian Otte, Martin V. Butz</li>
<li>for: 本研究旨在提高基于场景表示学习的分割和跟踪方法，使其能够更好地处理部分可见对象和逻辑物理测试。</li>
<li>methods: 本研究使用了Loci-Looped算法，它是一种基于Loci neural network架构的循环型朴素网络，可以自动将像素空间信息与预测结果混合，以获得信息混合活动。Loci-Looped还可以学习对象动态和对象之间交互的 compositional 表示。</li>
<li>results: Loci-Looped可以在对象遮挡时长时间跟踪对象，甚至预测遮挡后对象的重返，无需显式历史缓存。此外，Loci-Looped在ADEPT和CLEVRER数据集上比基于现有模型表现出色，在对象遮挡或感知数据断续时表现更好。这表示Loci-Looped可以在无监督下自适应学习物理概念，包括对象持续性和静止性。<details>
<summary>Abstract</summary>
Recent compositional scene representation learning models have become remarkably good in segmenting and tracking distinct objects within visual scenes. Yet, many of these models require that objects are continuously, at least partially, visible. Moreover, they tend to fail on intuitive physics tests, which infants learn to solve over the first months of their life. Our goal is to advance compositional scene representation algorithms with an embedded algorithm that fosters the progressive learning of intuitive physics, akin to infant development. As a fundamental component for such an algorithm, we introduce Loci-Looped, which advances a recently published unsupervised object location, identification, and tracking neural network architecture (Loci, Traub et al., ICLR 2023) with an internal processing loop. The loop is designed to adaptively blend pixel-space information with anticipations yielding information-fused activities as percepts. Moreover, it is designed to learn compositional representations of both individual object dynamics and between-objects interaction dynamics. We show that Loci-Looped learns to track objects through extended periods of object occlusions, indeed simulating their hidden trajectories and anticipating their reappearance, without the need for an explicit history buffer. We even find that Loci-Looped surpasses state-of-the-art models on the ADEPT and the CLEVRER dataset, when confronted with object occlusions or temporary sensory data interruptions. This indicates that Loci-Looped is able to learn the physical concepts of object permanence and inertia in a fully unsupervised emergent manner. We believe that even further architectural advancements of the internal loop - also in other compositional scene representation learning models - can be developed in the near future.
</details>
<details>
<summary>摘要</summary>
现代场景表示学习模型已经很出色地 segmenting 和跟踪视场中的不同对象。然而，许多这些模型需要对象在视场中保持不间断的可见性。此外，它们在直觉物理测试中表现不佳，这与婴儿在生长过程中学习的直觉物理概念相反。我们的目标是提高场景表示算法，其中包括一个内置的算法，以便逐步学习直觉物理概念，类似于婴儿的发展。为此，我们引入了Loci-Looped，它是一种基于Loci（Traub et al., ICLR 2023）的无监督对象位置、识别和跟踪神经网络架构，并添加了内部处理循环。这个循环通过自适应混合像素空间信息和预测得到的信息混合活动，以便学习对象动态和对象之间的交互动态。我们发现Loci-Looped可以在对象遮挡期间跟踪对象，并且可以预测遮挡物体的重返，无需显式历史缓存。此外，Loci-Looped在ADEPT和CLEVRER数据集上表现出色，even when confronted with object occlusions or temporary sensory data interruptions.这表明Loci-Looped可以在无监督下自适应学习物理概念，包括对象永恒和运动的概念。我们认为，将Loci-Looped的内部循环扩展到其他场景表示学习模型中，可能会在未来得到进一步改进。
</details></li>
</ul>
<hr>
<h2 id="Camera-LiDAR-Fusion-with-Latent-Contact-for-Place-Recognition-in-Challenging-Cross-Scenes"><a href="#Camera-LiDAR-Fusion-with-Latent-Contact-for-Place-Recognition-in-Challenging-Cross-Scenes" class="headerlink" title="Camera-LiDAR Fusion with Latent Contact for Place Recognition in Challenging Cross-Scenes"></a>Camera-LiDAR Fusion with Latent Contact for Place Recognition in Challenging Cross-Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10371">http://arxiv.org/abs/2310.10371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Pan, Jiapeng Xie, Jiajie Wu, Bo Zhou</li>
<li>for: 本文是为了解决在视角变化、季节变化和场景变换等环境下实现地点认知而写的。</li>
<li>methods: 本文使用了一种新的三通道地点描述器，包括图像、点云和融合分支。图像和点云之间的相互关系被利用，以实现信息互动和融合。</li>
<li>results: EXTENSIVE experiments on KITTI、NCLT、USVInland和校园数据集表明，提出的地点描述器为最佳方法，在复杂的场景下表现了 robustness 和通用性。<details>
<summary>Abstract</summary>
Although significant progress has been made, achieving place recognition in environments with perspective changes, seasonal variations, and scene transformations remains challenging. Relying solely on perception information from a single sensor is insufficient to address these issues. Recognizing the complementarity between cameras and LiDAR, multi-modal fusion methods have attracted attention. To address the information waste in existing multi-modal fusion works, this paper introduces a novel three-channel place descriptor, which consists of a cascade of image, point cloud, and fusion branches. Specifically, the fusion-based branch employs a dual-stage pipeline, leveraging the correlation between the two modalities with latent contacts, thereby facilitating information interaction and fusion. Extensive experiments on the KITTI, NCLT, USVInland, and the campus dataset demonstrate that the proposed place descriptor stands as the state-of-the-art approach, confirming its robustness and generality in challenging scenarios.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管已经做出了很大的进步，但是在视角变化、季节变化和场景变换等环境下实现地点认知仍然是一个挑战。仅仅基于单一感知器的信息不够 Address these issues. 识别摄像头和激光仪的补偿性，多感知Modal Fusion方法吸引了关注。为了改进现有多感知融合方法中的信息浪费，本文提出了一种新的三通道地点描述符，包括图像、点云和融合分支。具体来说，融合分支采用了双 stage pipeline，利用两个感知器之间的相互关系，以便信息互动和融合。从 KITTI、NCLT、USVInland 和校园数据集进行了广泛的实验，confirming its robustness and generality in challenging scenarios.
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Object-Query-Initialization-for-3D-Object-Detection"><a href="#Multimodal-Object-Query-Initialization-for-3D-Object-Detection" class="headerlink" title="Multimodal Object Query Initialization for 3D Object Detection"></a>Multimodal Object Query Initialization for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10353">http://arxiv.org/abs/2310.10353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathijs R. van Geerenstein, Felicia Ruppel, Klaus Dietmayer, Dariu M. Gavrila</li>
<li>For: 本研究旨在提高LiDAR和摄像头感知器件之间的对接，以提高3D物体检测模型的性能。* Methods: 我们提出了一种高效、可组合、多Modal的对象查询初始化方法，使得查询可以在多种感知器件输入下进行初始化。* Results: 我们在nuScenesbenchmark上进行了实验，并与现有方法进行比较。结果显示，我们的方法可以在LiDAR-camera输入下达到更高的性能，并且比现有方法更快。此外，我们的方法可以适用于任意感知器件输入组合。<details>
<summary>Abstract</summary>
3D object detection models that exploit both LiDAR and camera sensor features are top performers in large-scale autonomous driving benchmarks. A transformer is a popular network architecture used for this task, in which so-called object queries act as candidate objects. Initializing these object queries based on current sensor inputs is a common practice. For this, existing methods strongly rely on LiDAR data however, and do not fully exploit image features. Besides, they introduce significant latency. To overcome these limitations we propose EfficientQ3M, an efficient, modular, and multimodal solution for object query initialization for transformer-based 3D object detection models. The proposed initialization method is combined with a "modality-balanced" transformer decoder where the queries can access all sensor modalities throughout the decoder. In experiments, we outperform the state of the art in transformer-based LiDAR object detection on the competitive nuScenes benchmark and showcase the benefits of input-dependent multimodal query initialization, while being more efficient than the available alternatives for LiDAR-camera initialization. The proposed method can be applied with any combination of sensor modalities as input, demonstrating its modularity.
</details>
<details>
<summary>摘要</summary>
三维物体探测模型，利用激光和相机感知器件特点，在大规模自动驾驶benchmark中表现出色。 transformer是一种广泛使用的网络架构，在这种情况下，被称为“对象查询”的对象被当作候选对象。现有方法通常基于现有的激光数据进行初始化，但是不充分利用图像特征。此外，它们也会增加显著的延迟。为了解决这些限制，我们提出了高效的EfficientQ3M方法，用于初始化转换器基于三维对象探测模型中的对象查询。我们的初始化方法与“多感器均衡”转换器解码器结合使用，使得查询可以在解码器中访问所有感知模式。在实验中，我们超越了现有的转换器基于LiDAR对象探测模型的状态，在competitive nuScenes benchmark上表现出色，并示出了输入具有multimodal查询初始化的优势，同时更高效于现有的LiDAR-camera初始化方法。该方法可以针对任何感知模式进行输入，表明其模块性。
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Crowd-Counting-with-Contextual-Modeling-Facilitating-Holistic-Understanding-of-Crowd-Scenes"><a href="#Semi-Supervised-Crowd-Counting-with-Contextual-Modeling-Facilitating-Holistic-Understanding-of-Crowd-Scenes" class="headerlink" title="Semi-Supervised Crowd Counting with Contextual Modeling: Facilitating Holistic Understanding of Crowd Scenes"></a>Semi-Supervised Crowd Counting with Contextual Modeling: Facilitating Holistic Understanding of Crowd Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10352">http://arxiv.org/abs/2310.10352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cha15yq/MRC-Crowd">https://github.com/cha15yq/MRC-Crowd</a></li>
<li>paper_authors: Yifei Qian, Xiaopeng Hong, Ognjen Arandjelović, Zhongliang Guo, Carl R. Donovan</li>
<li>for: 增强人群计数模型的可靠性和准确性，提高模型在受限数据量时的泛化能力。</li>
<li>methods: 基于mean teacher框架，对无标签数据进行masking处理，以便模型通过整体特征学习人群场景，模仿人类认知过程。其他方法包括 incorporating fine-grained density classification task，以提高特征学习。</li>
<li>results: 模型在挑战性评价指标上表现出色，胜过之前的方法，且模型具有’subitizing’-like行为，即准确地计算低密度区域，同时 incorporating 地方细节来计算高密度区域。<details>
<summary>Abstract</summary>
To alleviate the heavy annotation burden for training a reliable crowd counting model and thus make the model more practicable and accurate by being able to benefit from more data, this paper presents a new semi-supervised method based on the mean teacher framework. When there is a scarcity of labeled data available, the model is prone to overfit local patches. Within such contexts, the conventional approach of solely improving the accuracy of local patch predictions through unlabeled data proves inadequate. Consequently, we propose a more nuanced approach: fostering the model's intrinsic 'subitizing' capability. This ability allows the model to accurately estimate the count in regions by leveraging its understanding of the crowd scenes, mirroring the human cognitive process. To achieve this goal, we apply masking on unlabeled data, guiding the model to make predictions for these masked patches based on the holistic cues. Furthermore, to help with feature learning, herein we incorporate a fine-grained density classification task. Our method is general and applicable to most existing crowd counting methods as it doesn't have strict structural or loss constraints. In addition, we observe that the model trained with our framework exhibits a 'subitizing'-like behavior. It accurately predicts low-density regions with only a 'glance', while incorporating local details to predict high-density regions. Our method achieves the state-of-the-art performance, surpassing previous approaches by a large margin on challenging benchmarks such as ShanghaiTech A and UCF-QNRF. The code is available at: https://github.com/cha15yq/MRC-Crowd.
</details>
<details>
<summary>摘要</summary>
为了减轻人群计数模型的注释负担，从而使模型更实用和准确，这篇论文提出了一种新的半监督方法基于 Mean Teacher 框架。当缺乏标注数据时，模型容易过拟合本地块。在这种情况下，通过 solely 提高本地块预测的准确性来提高模型的性能是不够的。因此，我们提出了一种更加细化的方法：激发模型的内在 'subitizing' 能力。这种能力使模型可以通过利用人群场景的理解来准确地计算区域的人群数量，这与人类认知过程相似。为了实现这个目标，我们在无标注数据上应用掩码，使模型根据全景册筹的信息进行预测。此外，我们还在模型中添加了细化的浓度分类任务，以帮助特征学习。我们的方法是通用的，可以应用于大多数现有的人群计数方法，不具有严格的结构或损失约束。此外，我们发现模型通过我们的框架进行训练时会展现 'subitizing' 类的行为，即可以准确地计算低密度区域，只需要一个 '察看'，同时 incorporate 本地细节来预测高密度区域。我们的方法在挑战性较高的标准准则 ShanghaiTech A 和 UCF-QNRF 上实现了 estado del arte 的性能，大幅超过了先前的方法。模型代码可以在 GitHub 上找到：https://github.com/cha15yq/MRC-Crowd。
</details></li>
</ul>
<hr>
<h2 id="ConsistNet-Enforcing-3D-Consistency-for-Multi-view-Images-Diffusion"><a href="#ConsistNet-Enforcing-3D-Consistency-for-Multi-view-Images-Diffusion" class="headerlink" title="ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion"></a>ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10343">http://arxiv.org/abs/2310.10343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiayuyang/consistnet">https://github.com/jiayuyang/consistnet</a></li>
<li>paper_authors: Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, Hongdong Li</li>
<li>for: 这篇论文的目的是提出一种能够生成多个不同视角的图像，同时保持3D（多视图）一致性的方法。</li>
<li>methods: 该方法基于一个多视图一致块，该块在多个单视图噪声过程中交换信息，根据多视图几何原理来协调多个单视图特征。</li>
<li>results: 该方法可以轻松地插入预训练的LDMs（卷积神经网络），不需要显式的像素对应关系或深度预测。实验显示，该方法可以在40秒内在单个A100 GPU上生成16个不同视角的图像，并且能够有效地学习3D一致性。<details>
<summary>Abstract</summary>
Given a single image of a 3D object, this paper proposes a novel method (named ConsistNet) that is able to generate multiple images of the same object, as if seen they are captured from different viewpoints, while the 3D (multi-view) consistencies among those multiple generated images are effectively exploited. Central to our method is a multi-view consistency block which enables information exchange across multiple single-view diffusion processes based on the underlying multi-view geometry principles. ConsistNet is an extension to the standard latent diffusion model, and consists of two sub-modules: (a) a view aggregation module that unprojects multi-view features into global 3D volumes and infer consistency, and (b) a ray aggregation module that samples and aggregate 3D consistent features back to each view to enforce consistency. Our approach departs from previous methods in multi-view image generation, in that it can be easily dropped-in pre-trained LDMs without requiring explicit pixel correspondences or depth prediction. Experiments show that our method effectively learns 3D consistency over a frozen Zero123 backbone and can generate 16 surrounding views of the object within 40 seconds on a single A100 GPU. Our code will be made available on https://github.com/JiayuYANG/ConsistNet
</details>
<details>
<summary>摘要</summary>
给定一张3D对象的单张图像，这篇论文提出了一种新的方法（名为ConsistNet），可以生成多张不同视角的图像，同时利用多视图的3D一致性。我们的方法的核心是一个多视图一致块，允许多个单视图的扩散过程之间进行信息交换，基于下面的多视图几何原理。ConsistNet是标准潜在扩散模型的扩展，包括两个子模块：（a）视图聚合模块，将多视图特征映射到全局3D体Volume并评估一致性，以及（b）光束聚合模块，从3D一致的特征样本返回到每个视图，以确保一致性。我们的方法与前一些多视图图像生成方法不同，可以直接使用预训练的LDMs，无需明确的像素匹配或深度预测。实验表明，我们的方法可以在冰zero123架构上学习3D一致性，在单个A100 GPU上生成16个对象周围视图 Within 40秒。我们的代码将在https://github.com/JiayuYANG/ConsistNet上公开。
</details></li>
</ul>
<hr>
<h2 id="Scene-Graph-Conditioning-in-Latent-Diffusion"><a href="#Scene-Graph-Conditioning-in-Latent-Diffusion" class="headerlink" title="Scene Graph Conditioning in Latent Diffusion"></a>Scene Graph Conditioning in Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10338">http://arxiv.org/abs/2310.10338</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/frankfundel/sgcond">https://github.com/frankfundel/sgcond</a></li>
<li>paper_authors: Frank Fundel</li>
<li>for: 这个论文旨在提高基于文本描述的扩展凝聚模型的精细Semantic控制和精准生成图像。</li>
<li>methods: 这篇论文使用了ControlNet和Gated Self-Attention等多种方法来解决大规模凝聚模型的微调和精准生成图像。</li>
<li>results: 研究人员通过使用提出的方法可以生成图像从场景图中得到更高质量的图像，超过了之前的方法。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Diffusion models excel in image generation but lack detailed semantic control using text prompts. Additional techniques have been developed to address this limitation. However, conditioning diffusion models solely on text-based descriptions is challenging due to ambiguity and lack of structure. In contrast, scene graphs offer a more precise representation of image content, making them superior for fine-grained control and accurate synthesis in image generation models. The amount of image and scene-graph data is sparse, which makes fine-tuning large diffusion models challenging. We propose multiple approaches to tackle this problem using ControlNet and Gated Self-Attention. We were able to show that using out proposed methods it is possible to generate images from scene graphs with much higher quality, outperforming previous methods. Our source code is publicly available on https://github.com/FrankFundel/SGCond
</details>
<details>
<summary>摘要</summary>
吸引模型在图像生成方面表现出色，但缺乏文本描述的细腻 semantic控制。为了解决这个限制，有些技术被开发出来。然而，通过 solely 文本描述来conditioning 吸引模型是困难的，因为描述的ambiguity和lack of structure。相比之下，场景图表示图像内容的更加精细，使其成为更好的 Fine-grained control 和图像生成模型的精准合成。然而，图像和场景图数据的量是稀缺的，这使得 fine-tuning 大型吸引模型困难。我们提出了多种方法来解决这个问题，包括ControlNet和Gated Self-Attention。我们能够证明，使用我们的方法可以生成从场景图中的图像，质量远高于之前的方法。我们的源代码在https://github.com/FrankFundel/SGCond 上公开可用。
</details></li>
</ul>
<hr>
<h2 id="Towards-image-compression-with-perfect-realism-at-ultra-low-bitrates"><a href="#Towards-image-compression-with-perfect-realism-at-ultra-low-bitrates" class="headerlink" title="Towards image compression with perfect realism at ultra-low bitrates"></a>Towards image compression with perfect realism at ultra-low bitrates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10325">http://arxiv.org/abs/2310.10325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marlène Careil, Matthew J. Muckley, Jakob Verbeek, Stéphane Lathuilière</li>
<li>for: 提高图像质量，使其不受比特率的影响</li>
<li>methods: 使用迭代扩散模型代替feed-forward推理器，并conditioning模型于vector-quantized图像表示和全文描述</li>
<li>results: 与状态之前的编码器相比，提高图像质量，并在ultra-low比特率下（0.003比特&#x2F;像素）提供高质量图像重建Here’s the breakdown of each point:</li>
<li>for: The paper aims to improve the quality of compressed images and make it less dependent on the bitrate.</li>
<li>methods: The proposed method uses iterative diffusion models instead of feed-forward decoders trained with MSE or LPIPS distortions. Additionally, the model is conditioned on a vector-quantized image representation and a global textual image description to provide additional context.</li>
<li>results: The proposed method outperforms state-of-the-art codecs at ultra-low bitrates (0.003 bits&#x2F;pixel) and provides high-quality image reconstruction with low dependence on the bitrate.<details>
<summary>Abstract</summary>
Image codecs are typically optimized to trade-off bitrate vs, distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality, and to make it less dependent on the bitrate, we propose to decode with iterative diffusion models, instead of feed-forward decoders trained using MSE or LPIPS distortions used in most neural codecs. In addition to conditioning the model on a vector-quantized image representation, we also condition on a global textual image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is an order of magnitude smaller than those considered in most prior work. At this bitrate a 512x768 Kodak image is encoded in less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID, and that the visual quality is less dependent on the bitrate than previous methods.
</details>
<details>
<summary>摘要</summary>
图像编码器通常是进行比特率vs扭曲指标的优化的。在低比特率下，这会导致压缩artefacts，即使在使用感知或敌对损失进行训练。为了改进图像质量并使其不受比特率的影响，我们提议使用迭代扩散模型进行解码，而不是使用MSE或LPIPS损失来训练Feed-forward decoder。此外，我们还conditioning the model on a vector-quantized image representation和global文本描述来提供额外的 контекст。我们称我们的模型为PerCo，用于'感知压缩'，并与当前的编码器进行比较。我们的模型在比特率从0.1下到0.003比特每像素进行比较，其中0.003比特每像素是在大多数先前工作中考虑的一个次数。在这个比特率下，我们可以将512x768像素的Kodak图像编码为 less than 153字节。尽管我们的比特率非常低，但我们的方法可以保持实际的图像重建。我们发现我们的模型可以在FID和KID指标下达到状态泰ometer的视觉质量，并且这种视觉质量与比特率相对较少受到影响。
</details></li>
</ul>
<hr>
<h2 id="Multi-Body-Neural-Scene-Flow"><a href="#Multi-Body-Neural-Scene-Flow" class="headerlink" title="Multi-Body Neural Scene Flow"></a>Multi-Body Neural Scene Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10301">http://arxiv.org/abs/2310.10301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kavisha725/MBNSF">https://github.com/kavisha725/MBNSF</a></li>
<li>paper_authors: Kavisha Vidanapathirana, Shin-Fang Chng, Xueqian Li, Simon Lucey</li>
<li>for: 本研究旨在提高Scene Flow的测试时优化，使其能够更好地处理实际世界数据中的多体刚体运动。</li>
<li>methods: 作者提出了一种基于坐标网络的神经网络优化方法，通过正则化Scene Flow预测中的流体平滑性来捕捉通用运动。此外，作者还引入了一种基于流体尺度的正则项，以便在多体刚体运动中保持流体场的连续性。</li>
<li>results: 作者在实际数据上进行了广泛的实验，并证明了他们的方法能够超过当前最佳的3D Scene Flow和长期点云轨迹预测。 codes available at: \href{<a target="_blank" rel="noopener" href="https://github.com/kavisha725/MBNSF%7D%7Bhttps://github.com/kavisha725/MBNSF%7D">https://github.com/kavisha725/MBNSF}{https://github.com/kavisha725/MBNSF}</a>.<details>
<summary>Abstract</summary>
The test-time optimization of scene flow - using a coordinate network as a neural prior - has gained popularity due to its simplicity, lack of dataset bias, and state-of-the-art performance. We observe, however, that although coordinate networks capture general motions by implicitly regularizing the scene flow predictions to be spatially smooth, the neural prior by itself is unable to identify the underlying multi-body rigid motions present in real-world data. To address this, we show that multi-body rigidity can be achieved without the cumbersome and brittle strategy of constraining the $SE(3)$ parameters of each rigid body as done in previous works. This is achieved by regularizing the scene flow optimization to encourage isometry in flow predictions for rigid bodies. This strategy enables multi-body rigidity in scene flow while maintaining a continuous flow field, hence allowing dense long-term scene flow integration across a sequence of point clouds. We conduct extensive experiments on real-world datasets and demonstrate that our approach outperforms the state-of-the-art in 3D scene flow and long-term point-wise 4D trajectory prediction. The code is available at: \href{https://github.com/kavisha725/MBNSF}{https://github.com/kavisha725/MBNSF}.
</details>
<details>
<summary>摘要</summary>
scene flow 测试时优化 - 使用坐标网络作为神经网络先验 - 在过去几年中变得越来越流行，这是因为它的简单性、不受数据偏见和现在的表现水平都很高。但我们发现，即使坐标网络可以捕捉一般运动的概念，但神经网络本身无法直接捕捉真实世界数据中的多体刚性运动。为解决这个问题，我们表明了一种不需要干扰和脆弱的策略，即在场景流优化中规范化流预测以促进刚性。这种策略允许场景流中的多体刚性，同时保持连续的流场，因此允许长期场景流集成。我们对实际数据进行了广泛的实验，并证明了我们的方法在3D场景流和长期点云轨迹预测中超越了现有的状态艺术。代码可以在：<https://github.com/kavisha725/MBNSF>上下载。
</details></li>
</ul>
<hr>
<h2 id="Effortless-Cross-Platform-Video-Codec-A-Codebook-Based-Method"><a href="#Effortless-Cross-Platform-Video-Codec-A-Codebook-Based-Method" class="headerlink" title="Effortless Cross-Platform Video Codec: A Codebook-Based Method"></a>Effortless Cross-Platform Video Codec: A Codebook-Based Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10292">http://arxiv.org/abs/2310.10292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan Tian, Yonghang Guan, Jinxi Xiang, Jun Zhang, Xiao Han, Wei Yang</li>
<li>for: 提高视频编码器的环境灵活性和计算效率</li>
<li>methods: 基于码ebook的视频编码框架，使用Conditional Cross-Attention模块取得帧之间的上下文</li>
<li>results: 实验结果显示，我们的方法可以超越传统的H.265（中）编码器，无需任何Entropy约束，同时具备跨平台性Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 提高视频编码器的环境灵活性和计算效率</li>
<li>methods: 基于码ebook的视频编码框架，使用Conditional Cross-Attention模块取得帧之间的上下文</li>
<li>results: 实验结果显示，我们的方法可以超越传统的H.265（中）编码器，无需任何Entropy约束，同时具备跨平台性<details>
<summary>Abstract</summary>
Under certain circumstances, advanced neural video codecs can surpass the most complex traditional codecs in their rate-distortion (RD) performance. One of the main reasons for the high performance of existing neural video codecs is the use of the entropy model, which can provide more accurate probability distribution estimations for compressing the latents. This also implies the rigorous requirement that entropy models running on different platforms should use consistent distribution estimations. However, in cross-platform scenarios, entropy models running on different platforms usually yield inconsistent probability distribution estimations due to floating point computation errors that are platform-dependent, which can cause the decoding side to fail in correctly decoding the compressed bitstream sent by the encoding side. In this paper, we propose a cross-platform video compression framework based on codebooks, which avoids autoregressive entropy modeling and achieves video compression by transmitting the index sequence of the codebooks. Moreover, instead of using optical flow for context alignment, we propose to use the conditional cross-attention module to obtain the context between frames. Due to the absence of autoregressive modeling and optical flow alignment, we can design an extremely minimalist framework that can greatly benefit computational efficiency. Importantly, our framework no longer contains any distribution estimation modules for entropy modeling, and thus computations across platforms are not necessarily consistent. Experimental results show that our method can outperform the traditional H.265 (medium) even without any entropy constraints, while achieving the cross-platform property intrinsically.
</details>
<details>
<summary>摘要</summary>
在某些情况下，高级神经视频编码器可以超越最复杂的传统编码器在比特率-损失（RD）性能方面。主要的原因是使用Entropy模型，可以提供更准确的概率分布估计，用于压缩缓冲。然而，在跨平台场景下，运行于不同平台的Entropy模型通常会产生不一致的概率分布估计，因为计算机中的浮点数计算错误是平台相关的，这会导致解码器无法正确地解码编码器发送的压缩位流。在本文中，我们提出了基于codebooks的跨平台视频压缩框架，不使用潮流模型和相关适应模块，而是通过传输编码器序列的index来实现压缩。另外，我们提出了基于条件cross-attention模块来获取帧之间的上下文。由于不使用潮流模型和相关适应模块，我们可以设计一个极其简洁的框架，可以大幅提高计算效率。重要的是，我们的框架不再包含任何分布估计模块，因此在不同平台上的计算是不一致的。实验结果表明，我们的方法可以在不使用Entropy约束下，超越传统H.265（中）的RD性能，同时实现跨平台性特性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-World-Co-Salient-Object-Detection-with-Generative-Uncertainty-aware-Group-Selective-Exchange-Masking"><a href="#Towards-Open-World-Co-Salient-Object-Detection-with-Generative-Uncertainty-aware-Group-Selective-Exchange-Masking" class="headerlink" title="Towards Open-World Co-Salient Object Detection with Generative Uncertainty-aware Group Selective Exchange-Masking"></a>Towards Open-World Co-Salient Object Detection with Generative Uncertainty-aware Group Selective Exchange-Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10264">http://arxiv.org/abs/2310.10264</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wuyang98/CoSOD">https://github.com/wuyang98/CoSOD</a></li>
<li>paper_authors: Yang Wu, Shenglong Hu, Huihui Song, Kaihua Zhang, Bo Liu, Dong Liu</li>
<li>for: 提高CoSOD模型在开放世界场景下的Robustness。</li>
<li>methods: 引入集选择交换掩码（GSEM）方法，使用混合度量选择图像，并使用变量量生成器和CoSOD变换分支模型。</li>
<li>results: 提出了一种基于变量量生成器和CoSOD变换分支模型的Robust CoSOD方法，并在三个开放世界 benchmark dataset上进行了实验，证明了方法的有效性和实用性。<details>
<summary>Abstract</summary>
The traditional definition of co-salient object detection (CoSOD) task is to segment the common salient objects in a group of relevant images. This definition is based on an assumption of group consensus consistency that is not always reasonable in the open-world setting, which results in robustness issue in the model when dealing with irrelevant images in the inputting image group under the open-word scenarios. To tackle this problem, we introduce a group selective exchange-masking (GSEM) approach for enhancing the robustness of the CoSOD model. GSEM takes two groups of images as input, each containing different types of salient objects. Based on the mixed metric we designed, GSEM selects a subset of images from each group using a novel learning-based strategy, then the selected images are exchanged. To simultaneously consider the uncertainty introduced by irrelevant images and the consensus features of the remaining relevant images in the group, we designed a latent variable generator branch and CoSOD transformer branch. The former is composed of a vector quantised-variational autoencoder to generate stochastic global variables that model uncertainty. The latter is designed to capture correlation-based local features that include group consensus. Finally, the outputs of the two branches are merged and passed to a transformer-based decoder to generate robust predictions. Taking into account that there are currently no benchmark datasets specifically designed for open-world scenarios, we constructed three open-world benchmark datasets, namely OWCoSal, OWCoSOD, and OWCoCA, based on existing datasets. By breaking the group-consistency assumption, these datasets provide effective simulations of real-world scenarios and can better evaluate the robustness and practicality of models.
</details>
<details>
<summary>摘要</summary>
传统上，co-salient object detection（CoSOD）任务的定义是将相同的焦点对象在多个相关图像中分割。这个定义基于了群体一致性的假设，这并不总是在开放世界场景下合理的，这会导致模型在处理无关图像时出现Robustness问题。为解决这个问题，我们提出了群选择交换掩码（GSEM）方法，用于增强CoSOD模型的Robustness。GSEM使用两组图像作为输入，每组图像含有不同类型的焦点对象。基于我们定义的混合度量，GSEM选择每组图像的一部分图像，然后将这些图像交换。为同时考虑无关图像引入的不确定性和剩下相关图像的协同特征，我们设计了隐藏变量生成分支和CoSOD变换分支。前者由vector quantized-variational autoencoder组成，用于生成随机全球变量，模型不确定性。后者是为了捕捉协同特征，包括群体一致性。最后，两个分支的输出被 merge，并传递到基于变换器的解码器，以生成Robust的预测。考虑到目前没有特定于开放世界场景的准确数据集，我们构建了三个开放世界数据集，namely OWCoSal、OWCoSOD和OWCoCA，基于现有数据集。由于这些数据集破坏了群体一致性假设，它们可以更好地模拟实际场景，并且可以更好地评估模型的Robustness和实用性。
</details></li>
</ul>
<hr>
<h2 id="Long-term-Dependency-for-3D-Reconstruction-of-Freehand-Ultrasound-Without-External-Tracker"><a href="#Long-term-Dependency-for-3D-Reconstruction-of-Freehand-Ultrasound-Without-External-Tracker" class="headerlink" title="Long-term Dependency for 3D Reconstruction of Freehand Ultrasound Without External Tracker"></a>Long-term Dependency for 3D Reconstruction of Freehand Ultrasound Without External Tracker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10248">http://arxiv.org/abs/2310.10248</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucl-candi/freehand">https://github.com/ucl-candi/freehand</a></li>
<li>paper_authors: Qi Li, Ziyi Shen, Qian Li, Dean C. Barratt, Thomas Dowrick, Matthew J. Clarkson, Tom Vercauteren, Yipeng Hu</li>
<li>For: 定义新的方法来嵌入长期依赖性，并评估其性能。* Methods: 使用序列模型与多个变数预测来编码长期依赖性，并提出两个依赖因子（体部图像内容和扫描协议）以推广精准重建。* Results: 1) 添加长期依赖性可以提高重建精度，并且随序列长度、变数间隔和扫描协议而变化。2) 对于训练中的体部或协议方差的降低，对重建精度产生负面影响。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Objective: Reconstructing freehand ultrasound in 3D without any external tracker has been a long-standing challenge in ultrasound-assisted procedures. We aim to define new ways of parameterising long-term dependencies, and evaluate the performance. Methods: First, long-term dependency is encoded by transformation positions within a frame sequence. This is achieved by combining a sequence model with a multi-transformation prediction. Second, two dependency factors are proposed, anatomical image content and scanning protocol, for contributing towards accurate reconstruction. Each factor is quantified experimentally by reducing respective training variances. Results: 1) The added long-term dependency up to 400 frames at 20 frames per second (fps) indeed improved reconstruction, with an up to 82.4% lowered accumulated error, compared with the baseline performance. The improvement was found to be dependent on sequence length, transformation interval and scanning protocol and, unexpectedly, not on the use of recurrent networks with long-short term modules; 2) Decreasing either anatomical or protocol variance in training led to poorer reconstruction accuracy. Interestingly, greater performance was gained from representative protocol patterns, than from representative anatomical features. Conclusion: The proposed algorithm uses hyperparameter tuning to effectively utilise long-term dependency. The proposed dependency factors are of practical significance in collecting diverse training data, regulating scanning protocols and developing efficient networks. Significance: The proposed new methodology with publicly available volunteer data and code for parametersing the long-term dependency, experimentally shown to be valid sources of performance improvement, which could potentially lead to better model development and practical optimisation of the reconstruction application.
</details>
<details>
<summary>摘要</summary>
目标：无需外部跟踪器，在ultrasound-assisted程序中自由手写三维重建问题已经是长期的挑战。我们想要定义新的方法来parameterize长期依赖关系，并评估其性能。方法：首先，通过将长期依赖关系编码为帧序列中的变换位置，使用序列模型和多变换预测结合。其次，我们提出了两个依赖因素，一是解剖学图像内容，二是扫描协议。每个因素都是通过实验量化训练方差来评估。结果：1）在400帧内的20帧/秒（fps）加入长期依赖关系后，重建精度显著提高，相比基eline性能，下降82.4%的累积错误。这种改进与序列长度、变换间隔和扫描协议有关，不同于使用循环网络long-short term模块。2）在训练中降低解剖学或协议方差可以得到更差的重建精度。意外地，更多的表现是由代表协议模式获得的，而不是由解剖学特征获得。结论：我们的算法使用了适当的hyperparameter调整，以利用长期依赖关系。我们提出的依赖因素对于收集多样化的训练数据、调整扫描协议和开发高效的网络是实际上的有用。意义：我们的新方法ологи在公共可用的志愿者数据和代码中实现了参数化长期依赖关系，实验证明了这些方法的有效性，这可能会导致更好的模型开发和实用优化重建应用。
</details></li>
</ul>
<hr>
<h2 id="Mask-wearing-object-detection-algorithm-based-on-improved-YOLOv5"><a href="#Mask-wearing-object-detection-algorithm-based-on-improved-YOLOv5" class="headerlink" title="Mask wearing object detection algorithm based on improved YOLOv5"></a>Mask wearing object detection algorithm based on improved YOLOv5</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10245">http://arxiv.org/abs/2310.10245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wen, Junhu Zhang, Haitao Li</li>
<li>for: 本研究旨在提出一种基于YOLOv5l的面Mask检测模型，以提高公共场合人员戴Mask的检测精度。</li>
<li>methods: 本研究使用Multi-Head Attentional Self-Convolution和Swin Transformer Block以提高模型的敏捷度和准确率。此外，我们还提出了I-CBAM模块以提高目标检测精度。</li>
<li>results: 在MASK数据集上进行实验，我们的模型比YOLOv5l模型提高了1.1%的mAP(0.5)和1.3%的mAP(0.5:0.95)。这表明我们的提案可以显著提高面Mask检测的精度。<details>
<summary>Abstract</summary>
Wearing a mask is one of the important measures to prevent infectious diseases. However, it is difficult to detect people's mask-wearing situation in public places with high traffic flow. To address the above problem, this paper proposes a mask-wearing face detection model based on YOLOv5l. Firstly, Multi-Head Attentional Self-Convolution not only improves the convergence speed of the model but also enhances the accuracy of the model detection. Secondly, the introduction of Swin Transformer Block is able to extract more useful feature information, enhance the detection ability of small targets, and improve the overall accuracy of the model. Our designed I-CBAM module can improve target detection accuracy. In addition, using enhanced feature fusion enables the model to better adapt to object detection tasks of different scales. In the experimentation on the MASK dataset, the results show that the model proposed in this paper achieved a 1.1% improvement in mAP(0.5) and a 1.3% improvement in mAP(0.5:0.95) compared to the YOLOv5l model. Our proposed method significantly enhances the detection capability of mask-wearing.
</details>
<details>
<summary>摘要</summary>
穿戴口罩是预防感染疾病的一种重要措施。然而，在高流量的公共场所中探测人们穿戴口罩的情况很困难。为解决这个问题，本文提出了基于YOLOv5l的口罩穿戴面部检测模型。首先，多头注意力自适应卷积不仅提高模型的融合速度，也提高了模型的检测精度。其次，将Swin卷积层引入可以提取更多有用的特征信息，提高小目标的检测能力，并提高模型的总精度。我们设计的I-CBAM模块可以提高标的检测精度。此外，使用强化的特征融合可以让模型更好地适应不同的物体检测任务。在MASK dataset上的实验结果显示，提案的模型与YOLOv5l模型相比，在mAP(0.5)和mAP(0.5:0.95)中实现了1.1%和1.3%的提升。我们的提案方法可以优化口罩穿戴的检测能力。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks"><a href="#Generalizing-Medical-Image-Representations-via-Quaternion-Wavelet-Networks" class="headerlink" title="Generalizing Medical Image Representations via Quaternion Wavelet Networks"></a>Generalizing Medical Image Representations via Quaternion Wavelet Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10224">http://arxiv.org/abs/2310.10224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ispamm/QWT">https://github.com/ispamm/QWT</a></li>
<li>paper_authors: Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello</li>
<li>for: 提高医疗图像处理领域的神经网络通用性，适应不同数据来源和任务的研究。</li>
<li>methods: 提出一种新的、通用、数据和任务无关的框架，可以从医疗图像中提取突出的特征。该框架基于四元波峰变换，可以与现有的医疗图像分析或生成任务集成，并且可以与实数、四元数或复数值模型混合使用。</li>
<li>results: 经过广泛的实验评估，包括不同的数据集和任务，如重建、分割和模态翻译等，结果显示提出的框架可以提高网络性能，同时具有广泛适用的通用性。<details>
<summary>Abstract</summary>
Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most representative set of sub-bands to be involved as input to any other neural model for image processing, replacing standard data samples. We conduct an extensive experimental evaluation comprising different datasets, diverse image analysis, and synthesis tasks including reconstruction, segmentation, and modality translation. We also evaluate QUAVE in combination with both real and quaternion-valued models. Results demonstrate the effectiveness and the generalizability of the proposed framework that improves network performance while being flexible to be adopted in manifold scenarios.
</details>
<details>
<summary>摘要</summary>
QUAVE首先使用四元波лет变换提取不同的子带，包括低频/抽象带和高频/细化特征。然后，它对最有代表性的子带进行权重，将其作为任务模型的输入，取代标准数据样本。我们进行了广泛的实验评估，包括不同的数据集和多种图像分析和生成任务，如重建、分割和模式翻译。我们还在QUAVE与实数和四元数值模型结合使用时进行了评估。结果表明我们提出的框架能够提高网络性能，同时具有通用的优势，能够在多种场景中适用。
</details></li>
</ul>
<hr>
<h2 id="RoboLLM-Robotic-Vision-Tasks-Grounded-on-Multimodal-Large-Language-Models"><a href="#RoboLLM-Robotic-Vision-Tasks-Grounded-on-Multimodal-Large-Language-Models" class="headerlink" title="RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models"></a>RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10221">http://arxiv.org/abs/2310.10221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longkukuhi/armbench">https://github.com/longkukuhi/armbench</a></li>
<li>paper_authors: Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</li>
<li>for: This paper is written for robotic vision applications, specifically to address the challenges of object detection, segmentation, and identification in real-world warehouse scenarios.</li>
<li>methods: The paper proposes the use of Multimodal Large Language Models (MLLMs) as a novel backbone for various downstream tasks, leveraging the pre-training capabilities of MLLMs to create a simplified framework and mitigate the need for task-specific encoders.</li>
<li>results: The paper introduces the RoboLLM framework, equipped with a BEiT-3 backbone, which outperforms existing baselines and substantially reduces the engineering burden associated with model selection and tuning, as demonstrated in the ARMBench challenge.<details>
<summary>Abstract</summary>
Robotic vision applications often necessitate a wide range of visual perception tasks, such as object detection, segmentation, and identification. While there have been substantial advances in these individual tasks, integrating specialized models into a unified vision pipeline presents significant engineering challenges and costs. Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. We argue that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.
</details>
<details>
<summary>摘要</summary>
robotic 视觉应用经常需要各种视觉识别任务，如物体检测、分割和识别。 DESPITE 这些任务的 SUBSTANTIAL ADVANCES，整合特殊模型到一个统一的视觉管道中存在 significan engineering challenges 和 Costs。 Recently, Multimodal Large Language Models (MLLMs) have emerged as novel backbones for various downstream tasks. WE ARGUE that leveraging the pre-training capabilities of MLLMs enables the creation of a simplified framework, thus mitigating the need for task-specific encoders. Specifically, the large-scale pretrained knowledge in MLLMs allows for easier fine-tuning to downstream robotic vision tasks and yields superior performance. We introduce the RoboLLM framework, equipped with a BEiT-3 backbone, to address all visual perception tasks in the ARMBench challenge-a large-scale robotic manipulation dataset about real-world warehouse scenarios. RoboLLM not only outperforms existing baselines but also substantially reduces the engineering burden associated with model selection and tuning. The source code is publicly available at https://github.com/longkukuhi/armbench.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Fetal-MRI-3D-Reconstruction-Based-on-Radiation-Diffusion-Generation-Model"><a href="#Self-supervised-Fetal-MRI-3D-Reconstruction-Based-on-Radiation-Diffusion-Generation-Model" class="headerlink" title="Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion Generation Model"></a>Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion Generation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10209">http://arxiv.org/abs/2310.10209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junpeng Tan, Xin Zhang, Yao Lv, Xiangmin Xu, Gang Li</li>
<li>for: 这种论文是为了解决妊娠Magnetic Resonance Imaging(MRI)中的精度恢复问题而写的。</li>
<li>methods: 这种方法使用了基于卷积的射线辐射场(NeRF)和基于超分解的扩展生成器(CINR)等技术来解决区域性灵敏度不均匀和全局一致性问题。</li>
<li>results: 实验结果表明，这种方法可以在实际世界妊娠MRI核心中实现高质量超分解重建。<details>
<summary>Abstract</summary>
Although the use of multiple stacks can handle slice-to-volume motion correction and artifact removal problems, there are still several problems: 1) The slice-to-volume method usually uses slices as input, which cannot solve the problem of uniform intensity distribution and complementarity in regions of different fetal MRI stacks; 2) The integrity of 3D space is not considered, which adversely affects the discrimination and generation of globally consistent information in fetal MRI; 3) Fetal MRI with severe motion artifacts in the real-world cannot achieve high-quality super-resolution reconstruction. To address these issues, we propose a novel fetal brain MRI high-quality volume reconstruction method, called the Radiation Diffusion Generation Model (RDGM). It is a self-supervised generation method, which incorporates the idea of Neural Radiation Field (NeRF) based on the coordinate generation and diffusion model based on super-resolution generation. To solve regional intensity heterogeneity in different directions, we use a pre-trained transformer model for slice registration, and then, a new regionally Consistent Implicit Neural Representation (CINR) network sub-module is proposed. CINR can generate the initial volume by combining a coordinate association map of two different coordinate mapping spaces. To enhance volume global consistency and discrimination, we introduce the Volume Diffusion Super-resolution Generation (VDSG) mechanism. The global intensity discriminant generation from volume-to-volume is carried out using the idea of diffusion generation, and CINR becomes the deviation intensity generation network of the volume-to-volume diffusion model. Finally, the experimental results on real-world fetal brain MRI stacks demonstrate the state-of-the-art performance of our method.
</details>
<details>
<summary>摘要</summary>
although multiple stacks can handle slice-to-volume motion correction and artifact removal problems, there are still several problems: 1) the slice-to-volume method usually uses slices as input, which cannot solve the problem of uniform intensity distribution and complementarity in regions of different fetal MRI stacks; 2) the integrity of 3D space is not considered, which adversely affects the discrimination and generation of globally consistent information in fetal MRI; 3) fetal MRI with severe motion artifacts in the real world cannot achieve high-quality super-resolution reconstruction. to address these issues, we propose a novel fetal brain MRI high-quality volume reconstruction method, called the radiation diffusion generation model (RDGM). it is a self-supervised generation method, which incorporates the idea of neural radiation field (NeRF) based on the coordinate generation and diffusion model based on super-resolution generation. to solve regional intensity heterogeneity in different directions, we use a pre-trained transformer model for slice registration, and then, a new regionally consistent implicit neural representation (CINR) network sub-module is proposed. CINR can generate the initial volume by combining a coordinate association map of two different coordinate mapping spaces. to enhance volume global consistency and discrimination, we introduce the volume diffusion super-resolution generation (VDSG) mechanism. the global intensity discriminant generation from volume-to-volume is carried out using the idea of diffusion generation, and CINR becomes the deviation intensity generation network of the volume-to-volume diffusion model. finally, the experimental results on real-world fetal brain MRI stacks demonstrate the state-of-the-art performance of our method.
</details></li>
</ul>
<hr>
<h2 id="MoConVQ-Unified-Physics-Based-Motion-Control-via-Scalable-Discrete-Representations"><a href="#MoConVQ-Unified-Physics-Based-Motion-Control-via-Scalable-Discrete-Representations" class="headerlink" title="MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations"></a>MoConVQ: Unified Physics-Based Motion Control via Scalable Discrete Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10198">http://arxiv.org/abs/2310.10198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, Libin Liu</li>
<li>for: 本文提出了一种新的物理学基的动作控制框架，即MoConVQ，可以有效地从大量、无结构的动作示例中学习动作嵌入。</li>
<li>methods: 该方法基于量化变换自动编码器（VQ-VAE）和模型基于奖励学习，可以从大量动作示例中学习动作嵌入，并且可以Capture多样化的动作技巧。</li>
<li>results: 研究人员通过多种应用场景来证明MoConVQ的可行性，包括通用跟踪控制、交互式人物控制、物理学基的动作生成等。此外，研究人员还证明了MoConVQ可以与大型语言模型（LLMs）集成，以解决复杂和抽象的任务。<details>
<summary>Abstract</summary>
In this work, we present MoConVQ, a novel unified framework for physics-based motion control leveraging scalable discrete representations. Building upon vector quantized variational autoencoders (VQ-VAE) and model-based reinforcement learning, our approach effectively learns motion embeddings from a large, unstructured dataset spanning tens of hours of motion examples. The resultant motion representation not only captures diverse motion skills but also offers a robust and intuitive interface for various applications. We demonstrate the versatility of MoConVQ through several applications: universal tracking control from various motion sources, interactive character control with latent motion representations using supervised learning, physics-based motion generation from natural language descriptions using the GPT framework, and, most interestingly, seamless integration with large language models (LLMs) with in-context learning to tackle complex and abstract tasks.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们介绍了MoConVQ，一种新的物理基于运动控制框架，利用可扩展的字符串表示法。我们的方法基于vector量化自适应学习（VQ-VAE）和基于模型的奖励学习，从大量、无结构的运动示例中学习出高质量的运动嵌入。这种运动表示不仅捕捉了多样化的运动技巧，还提供了一种稳定和直观的界面，可以应用于多种应用程序。我们在这篇论文中展示了MoConVQ的多种应用，包括从不同运动源的跟踪控制、使用监督学习的潜在运动表示进行交互人物控制、基于自然语言描述的物理运动生成、以及与大语言模型（LLM）集成，以解决复杂和抽象任务。
</details></li>
</ul>
<hr>
<h2 id="The-Road-to-On-board-Change-Detection-A-Lightweight-Patch-Level-Change-Detection-Network-via-Exploring-the-Potential-of-Pruning-and-Pooling"><a href="#The-Road-to-On-board-Change-Detection-A-Lightweight-Patch-Level-Change-Detection-Network-via-Exploring-the-Potential-of-Pruning-and-Pooling" class="headerlink" title="The Road to On-board Change Detection: A Lightweight Patch-Level Change Detection Network via Exploring the Potential of Pruning and Pooling"></a>The Road to On-board Change Detection: A Lightweight Patch-Level Change Detection Network via Exploring the Potential of Pruning and Pooling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10166">http://arxiv.org/abs/2310.10166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihui Xue, Zhihao Wang, Xueqian Wang, Gang Li</li>
<li>for: 这个论文主要是为了提高大规模卫星遥感帧测变检测（CD）方法的效率和可靠性，尤其是在具有限制性的计算和内存资源的edge Computing平台上。</li>
<li>methods: 本论文提出了一个轻量级的帧级CD网络（LPCDNet），以快速除去大量无变帧，以提高后续像素级CD过程的效率和内存成本。LPCDNet使用了一个感度指导的通道剔除方法，删除无重要通道，并建立轻量级的后门网络基于ResNet18网络。此外，本文还提出了一个多层特征压缩（MLFC）模组，用于压缩和融合两个时间点的帧级特征信息。</li>
<li>results: 根据实验结果，LPCDNet在两个CD资料集上可以每秒逐帧检测1000帧以上，比已有方法高得多，而且不会对CD性能造成明显的损失。此外，LPCDNet还可以降低后续像Pixel-level CD过程的内存成本超过60%。<details>
<summary>Abstract</summary>
Existing satellite remote sensing change detection (CD) methods often crop original large-scale bi-temporal image pairs into small patch pairs and then use pixel-level CD methods to fairly process all the patch pairs. However, due to the sparsity of change in large-scale satellite remote sensing images, existing pixel-level CD methods suffer from a waste of computational cost and memory resources on lots of unchanged areas, which reduces the processing efficiency of on-board platform with extremely limited computation and memory resources. To address this issue, we propose a lightweight patch-level CD network (LPCDNet) to rapidly remove lots of unchanged patch pairs in large-scale bi-temporal image pairs. This is helpful to accelerate the subsequent pixel-level CD processing stage and reduce its memory costs. In our LPCDNet, a sensitivity-guided channel pruning method is proposed to remove unimportant channels and construct the lightweight backbone network on basis of ResNet18 network. Then, the multi-layer feature compression (MLFC) module is designed to compress and fuse the multi-level feature information of bi-temporal image patch. The output of MLFC module is fed into the fully-connected decision network to generate the predicted binary label. Finally, a weighted cross-entropy loss is utilized in the training process of network to tackle the change/unchange class imbalance problem. Experiments on two CD datasets demonstrate that our LPCDNet achieves more than 1000 frames per second on an edge computation platform, i.e., NVIDIA Jetson AGX Orin, which is more than 3 times that of the existing methods without noticeable CD performance loss. In addition, our method reduces more than 60% memory costs of the subsequent pixel-level CD processing stage.
</details>
<details>
<summary>摘要</summary>
现有的卫星遥感变化检测（CD）方法 oftentimes crop original large-scale bi-temporal image pairs into small patch pairs and then use pixel-level CD methods to fairly process all the patch pairs. However, due to the sparsity of change in large-scale satellite remote sensing images, existing pixel-level CD methods suffer from a waste of computational cost and memory resources on lots of unchanged areas, which reduces the processing efficiency of on-board platform with extremely limited computation and memory resources. To address this issue, we propose a lightweight patch-level CD network (LPCDNet) to rapidly remove lots of unchanged patch pairs in large-scale bi-temporal image pairs. This is helpful to accelerate the subsequent pixel-level CD processing stage and reduce its memory costs. In our LPCDNet, a sensitivity-guided channel pruning method is proposed to remove unimportant channels and construct the lightweight backbone network on basis of ResNet18 network. Then, the multi-layer feature compression (MLFC) module is designed to compress and fuse the multi-level feature information of bi-temporal image patch. The output of MLFC module is fed into the fully-connected decision network to generate the predicted binary label. Finally, a weighted cross-entropy loss is utilized in the training process of network to tackle the change/unchange class imbalance problem. Experiments on two CD datasets demonstrate that our LPCDNet achieves more than 1000 frames per second on an edge computation platform, i.e., NVIDIA Jetson AGX Orin, which is more than 3 times that of the existing methods without noticeable CD performance loss. In addition, our method reduces more than 60% memory costs of the subsequent pixel-level CD processing stage.
</details></li>
</ul>
<hr>
<h2 id="A-Search-for-Prompts-Generating-Structured-Answers-from-Contracts"><a href="#A-Search-for-Prompts-Generating-Structured-Answers-from-Contracts" class="headerlink" title="A Search for Prompts: Generating Structured Answers from Contracts"></a>A Search for Prompts: Generating Structured Answers from Contracts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10141">http://arxiv.org/abs/2310.10141</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam Roegiest, Radha Chitta, Jonathan Donnelly, Maya Lash, Alexandra Vtyurina, François Longtin</li>
<li>for: 法律问题自动回答，帮助自动化人类审查或标识特定条件（例如，自动续订警示）。</li>
<li>methods: 使用 OpenAI 的 \textit{GPT-3.5-Turbo} 进行不结构化生成问题回答，并对问题回答提供了审查和改进。</li>
<li>results: 相比 semantic matching 方法，我们的模板提问方法更加准确，并且通过Context learning和提问修改，我们的方法可以进一步提高性能。<details>
<summary>Abstract</summary>
In many legal processes being able to action on the concrete implication of a legal question can be valuable to automating human review or signalling certain conditions (e.g., alerts around automatic renewal). To support such tasks, we present a form of legal question answering that seeks to return one (or more) fixed answers for a question about a contract clause. After showing that unstructured generative question answering can have questionable outcomes for such a task, we discuss our exploration methodology for legal question answering prompts using OpenAI's \textit{GPT-3.5-Turbo} and provide a summary of insights.   Using insights gleaned from our qualitative experiences, we compare our proposed template prompts against a common semantic matching approach and find that our prompt templates are far more accurate despite being less reliable in the exact response return. With some additional tweaks to prompts and the use of in-context learning, we are able to further improve the performance of our proposed strategy while maximizing the reliability of responses as best we can.
</details>
<details>
<summary>摘要</summary>
在许多法律程序中，能够对法律问题的具体实施有益于自动化人类审查或标识特定条件（例如，续约提醒）。为支持这些任务，我们提出了一种法律问题回答方法，该方法可以为一个合同条款的问题返回一个或多个固定答案。在显示了无结构生成问题回答可能导致问able的结果后，我们讲述了我们的探索方法ологи，使用OpenAI的GPT-3.5-Turbo进行问题回答提示。我们提供了一个摘要的感想，并与常见Semantic Matching方法进行比较。我们发现，我们的提案的模板提示比Semantic Matching方法更准确，尽管它们可能不那么可靠地返回具体的回答。通过对提示和使用上下文学习进行一些调整，我们能够进一步改进我们的提案的性能，同时最大化回答的可靠性。
</details></li>
</ul>
<hr>
<h2 id="PELA-Learning-Parameter-Efficient-Models-with-Low-Rank-Approximation"><a href="#PELA-Learning-Parameter-Efficient-Models-with-Low-Rank-Approximation" class="headerlink" title="PELA: Learning Parameter-Efficient Models with Low-Rank Approximation"></a>PELA: Learning Parameter-Efficient Models with Low-Rank Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10700">http://arxiv.org/abs/2310.10700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guoyang9/pela">https://github.com/guoyang9/pela</a></li>
<li>paper_authors: Yangyang Guo, Guangzhi Wang, Mohan Kankanhalli</li>
<li>for: 提高预训练模型中参数效率，以适应资源受限的下游任务。</li>
<li>methods: Introducing an intermediate pre-training stage, using low-rank approximation to compress the original large model, and devising a feature distillation module and weight perturbation regularization module to enhance the low-rank model.</li>
<li>results: 提高预训练模型的参数效率，同时保持与基本架构相似的性能水平，减少参数大小一半至二分之一。<details>
<summary>Abstract</summary>
Applying a pre-trained large model to downstream tasks is prohibitive under resource-constrained conditions. Recent dominant approaches for addressing efficiency issues involve adding a few learnable parameters to the fixed backbone model. This strategy, however, leads to more challenges in loading large models for downstream fine-tuning with limited resources. In this paper, we propose a novel method for increasing the parameter efficiency of pre-trained models by introducing an intermediate pre-training stage. To this end, we first employ low-rank approximation to compress the original large model and then devise a feature distillation module and a weight perturbation regularization module. These modules are specifically designed to enhance the low-rank model. Concretely, we update only the low-rank model while freezing the backbone parameters during pre-training. This allows for direct and efficient utilization of the low-rank model for downstream tasks. The proposed method achieves both efficiencies in terms of required parameters and computation time while maintaining comparable results with minimal modifications to the base architecture. Specifically, when applied to three vision-only and one vision-language Transformer models, our approach often demonstrates a $\sim$0.6 point decrease in performance while reducing the original parameter size by 1/3 to 2/3.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="3DYoga90-A-Hierarchical-Video-Dataset-for-Yoga-Pose-Understanding"><a href="#3DYoga90-A-Hierarchical-Video-Dataset-for-Yoga-Pose-Understanding" class="headerlink" title="3DYoga90: A Hierarchical Video Dataset for Yoga Pose Understanding"></a>3DYoga90: A Hierarchical Video Dataset for Yoga Pose Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10131">http://arxiv.org/abs/2310.10131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seonokkim/3dyoga90">https://github.com/seonokkim/3dyoga90</a></li>
<li>paper_authors: Seonok Kim</li>
<li>For: 这个研究是为了开发一个更大、更完整的人工智能训练用运动视频集，具体来说是3D Yoga901数据集，用于习习poses和瑜伽动作识别。* Methods: 这个研究使用了一个专门为这个目标而制作的数据集，其包括90个姿势的RGB视频和3D骨骼序列，同时还有一个三级标签层次结构。* Results: 这个研究创造了一个更大、更完整的公共数据集，包括RGB视频和3D骨骼序列，这对于人工智能训练和瑜伽动作识别具有广泛的应用前景。<details>
<summary>Abstract</summary>
The increasing popularity of exercises including yoga and Pilates has created a greater demand for professional exercise video datasets in the realm of artificial intelligence. In this study, we developed 3DYoga901, which is organized within a three-level label hierarchy. We have expanded the number of poses from an existing state-of-the-art dataset, increasing it from 82 to 90 poses. Our dataset includes meticulously curated RGB yoga pose videos and 3D skeleton sequences. This dataset was created by a dedicated team of six individuals, including yoga instructors. It stands out as one of the most comprehensive open datasets, featuring the largest collection of RGB videos and 3D skeleton sequences among publicly available resources. This contribution has the potential to significantly advance the field of yoga action recognition and pose assessment. Additionally, we conducted experiments to evaluate the practicality of our proposed dataset. We employed three different model variants for benchmarking purposes.
</details>
<details>
<summary>摘要</summary>
随着瑜伽和PILATES等运动的流行，人工智能领域的训练数据需求增加。在这项研究中，我们开发了3DYoga901，这是一个三级标签层次结构下的组织方式。我们将现有状态艺术数据集中的82姿势提高到90姿势。我们的数据集包括仔细挑选的RGB瑜伽姿势视频和3D骨架序列。这个数据集由6名专业人员，包括瑜伽教练，共同创建。它是公共可用资源中最完整的开放数据集，拥有最大的RGB视频和3D骨架序列收集。这一贡献有可能在瑜伽动作识别和姿势评估领域取得重要进步。此外，我们还进行了实验来评估我们的提案的实用性。我们使用了三种不同的模型变体进行比较。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Action-Recognition-with-Captioning-Foundation-Models"><a href="#Few-shot-Action-Recognition-with-Captioning-Foundation-Models" class="headerlink" title="Few-shot Action Recognition with Captioning Foundation Models"></a>Few-shot Action Recognition with Captioning Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10125">http://arxiv.org/abs/2310.10125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Wang, Shiwei Zhang, Hangjie Yuan, Yingya Zhang, Changxin Gao, Deli Zhao, Nong Sang</li>
<li>for: 本研究旨在将预训练的视觉语言知识转移到多个下游任务中，以提高实验效率和准确性。</li>
<li>methods: 本研究使用了一个名为CapFSAR的弹性插件架构，具有自动生成对应的视觉描述和文本嵌入的能力，以扩展预训练的视觉语言知识。</li>
<li>results: 实验结果显示，CapFSAR在多个标准几个阶段训练 benchmark 上表现出色，与现有方法比较，具有更高的准确性和更好的一致性。<details>
<summary>Abstract</summary>
Transferring vision-language knowledge from pretrained multimodal foundation models to various downstream tasks is a promising direction. However, most current few-shot action recognition methods are still limited to a single visual modality input due to the high cost of annotating additional textual descriptions. In this paper, we develop an effective plug-and-play framework called CapFSAR to exploit the knowledge of multimodal models without manually annotating text. To be specific, we first utilize a captioning foundation model (i.e., BLIP) to extract visual features and automatically generate associated captions for input videos. Then, we apply a text encoder to the synthetic captions to obtain representative text embeddings. Finally, a visual-text aggregation module based on Transformer is further designed to incorporate cross-modal spatio-temporal complementary information for reliable few-shot matching. In this way, CapFSAR can benefit from powerful multimodal knowledge of pretrained foundation models, yielding more comprehensive classification in the low-shot regime. Extensive experiments on multiple standard few-shot benchmarks demonstrate that the proposed CapFSAR performs favorably against existing methods and achieves state-of-the-art performance. The code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
<<TRANSLATED TEXT>>使用预训练多modal基础模型的视觉语言知识传递到多个下游任务是一个有前途的方向。然而，当前大多数几 shot动作识别方法仍然只接受单一视觉输入，因为对附加文本描述的标注成本高昂。在这篇论文中，我们开发了一个有效的插件式框架called CapFSAR，以利用预训练多modal模型的知识而不需要手动标注文本。具体来说，我们首先利用一个captioning基础模型（即BLIP）来提取视觉特征并自动生成相关的描述文本 для输入视频。然后，我们应用一个文本编码器来对生成的文本嵌入获得代表性的文本嵌入。最后，我们设计了一个基于Transformer的视TEXT聚合模块，以便在低shot情况下利用多modal视觉语言的补充信息进行可靠的匹配。这样，CapFSAR可以从预训练多modal模型中获得强大的知识，在低shot情况下实现更全面的分类。我们的实验结果表明，提议的CapFSAR在多个标准几 shotbenchmark上表现出色，并达到了状态 искусственный智能的性能。代码将公开。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="AutoDIR-Automatic-All-in-One-Image-Restoration-with-Latent-Diffusion"><a href="#AutoDIR-Automatic-All-in-One-Image-Restoration-with-Latent-Diffusion" class="headerlink" title="AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion"></a>AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10123">http://arxiv.org/abs/2310.10123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yitong Jiang, Zhaoyang Zhang, Tianfan Xue, Jinwei Gu</li>
<li>for:  solves complex real-world image restoration situations with multiple unknown degradations</li>
<li>methods:  uses an all-in-one image restoration framework with latent diffusion, including a Blind Image Quality Assessment Module (BIQA) and an All-in-One Image Refinement (AIR) Module, as well as a Structure Correction Module (SCM)</li>
<li>results:  outperforms state-of-the-art approaches with superior restoration results and supports a wider range of tasks, including real-scenario images with multiple unknown degradations.<details>
<summary>Abstract</summary>
In this paper, we aim to solve complex real-world image restoration situations, in which, one image may have a variety of unknown degradations. To this end, we propose an all-in-one image restoration framework with latent diffusion (AutoDIR), which can automatically detect and address multiple unknown degradations. Our framework first utilizes a Blind Image Quality Assessment Module (BIQA) to automatically detect and identify the unknown dominant image degradation type of the image. Then, an All-in-One Image Refinement (AIR) Module handles multiple kinds of degradation image restoration with the guidance of BIQA. Finally, a Structure Correction Module (SCM) is proposed to recover the image details distorted by AIR. Our comprehensive evaluation demonstrates that AutoDIR outperforms state-of-the-art approaches by achieving superior restoration results while supporting a wider range of tasks. Notably, AutoDIR is also the first method to automatically handle real-scenario images with multiple unknown degradations.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们目标是解决复杂的真实世界图像恢复问题，在这个问题中，一个图像可能具有多种未知的降低效应。为此，我们提议一个整合性图像恢复框架——自适应扩散图像修复（AutoDIR），可以自动检测和解决多种未知降低效应。我们的框架首先利用一个隐藏影像质量评估模块（BIQA）来自动检测和识别图像的未知主要降低类型。然后，一个全面修复（AIR）模块处理多种降低效应的图像修复，以BIQA的指导。最后，我们提出一个结构修复模块（SCM）来恢复图像细节，受到AIR的扭曲影响。我们的全面评估表明，AutoDIR在恢复Result中显示出优于当前方法，同时支持更广泛的任务范围。尤其是，AutoDIR是第一个自动处理真实世界图像中的多种未知降低的方法。
</details></li>
</ul>
<hr>
<h2 id="KAKURENBO-Adaptively-Hiding-Samples-in-Deep-Neural-Network-Training"><a href="#KAKURENBO-Adaptively-Hiding-Samples-in-Deep-Neural-Network-Training" class="headerlink" title="KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training"></a>KAKURENBO: Adaptively Hiding Samples in Deep Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10102">http://arxiv.org/abs/2310.10102</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TruongThaoNguyen/kakurenbo">https://github.com/TruongThaoNguyen/kakurenbo</a></li>
<li>paper_authors: Truong Thao Nguyen, Balazs Gerofi, Edgar Josafat Martinez-Noriega, François Trahay, Mohamed Wahib</li>
<li>for: 提高深度神经网络训练效率，减少训练成本。</li>
<li>methods: 利用训练损失和预测信任度信息， dynamically 排除训练样本，不归一化影响精度。</li>
<li>results: 在多个大规模数据集和模型上，与基eline相比，我们的方法可以降低训练时间，仅带来0.4%的精度下降。可以在<a target="_blank" rel="noopener" href="https://github.com/TruongThaoNguyen/kakurenbo">https://github.com/TruongThaoNguyen/kakurenbo</a> 获取代码。<details>
<summary>Abstract</summary>
This paper proposes a method for hiding the least-important samples during the training of deep neural networks to increase efficiency, i.e., to reduce the cost of training. Using information about the loss and prediction confidence during training, we adaptively find samples to exclude in a given epoch based on their contribution to the overall learning process, without significantly degrading accuracy. We explore the converge properties when accounting for the reduction in the number of SGD updates. Empirical results on various large-scale datasets and models used directly in image classification and segmentation show that while the with-replacement importance sampling algorithm performs poorly on large datasets, our method can reduce total training time by up to 22% impacting accuracy only by 0.4% compared to the baseline. Code available at https://github.com/TruongThaoNguyen/kakurenbo
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Multi-Scale-Spatial-Transformer-U-Net-for-Simultaneously-Automatic-Reorientation-and-Segmentation-of-3D-Nuclear-Cardiac-Images"><a href="#A-Multi-Scale-Spatial-Transformer-U-Net-for-Simultaneously-Automatic-Reorientation-and-Segmentation-of-3D-Nuclear-Cardiac-Images" class="headerlink" title="A Multi-Scale Spatial Transformer U-Net for Simultaneously Automatic Reorientation and Segmentation of 3D Nuclear Cardiac Images"></a>A Multi-Scale Spatial Transformer U-Net for Simultaneously Automatic Reorientation and Segmentation of 3D Nuclear Cardiac Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10095">http://arxiv.org/abs/2310.10095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangfan Ni, Duo Zhang, Gege Ma, Lijun Lu, Zhongke Huang, Wentao Zhu</li>
<li>For: 这个研究旨在提高核心心脏成像中的左心室（LV）重orientation和分割的精度，以便进行多modal的量化分析。* Methods: 该研究提出了一种结合多尺度空间变换网络（MSSTN）和多尺度UNet（MSUNet）模块的端到端模型，用于同时重orientation和分割LV区域从核心心脏成像图像中。* Results: 实验结果表明，该提出的方法可以显著提高重orientation和分割性能。这种结合学习框架可以促进重orientation和分割任务之间的互补性，从而实现高性能和高效的图像处理 workflow。<details>
<summary>Abstract</summary>
Accurate reorientation and segmentation of the left ventricular (LV) is essential for the quantitative analysis of myocardial perfusion imaging (MPI), in which one critical step is to reorient the reconstructed transaxial nuclear cardiac images into standard short-axis slices for subsequent image processing. Small-scale LV myocardium (LV-MY) region detection and the diverse cardiac structures of individual patients pose challenges to LV segmentation operation. To mitigate these issues, we propose an end-to-end model, named as multi-scale spatial transformer UNet (MS-ST-UNet), that involves the multi-scale spatial transformer network (MSSTN) and multi-scale UNet (MSUNet) modules to perform simultaneous reorientation and segmentation of LV region from nuclear cardiac images. The proposed method is trained and tested using two different nuclear cardiac image modalities: 13N-ammonia PET and 99mTc-sestamibi SPECT. We use a multi-scale strategy to generate and extract image features with different scales. Our experimental results demonstrate that the proposed method significantly improves the reorientation and segmentation performance. This joint learning framework promotes mutual enhancement between reorientation and segmentation tasks, leading to cutting edge performance and an efficient image processing workflow. The proposed end-to-end deep network has the potential to reduce the burden of manual delineation for cardiac images, thereby providing multimodal quantitative analysis assistance for physicists.
</details>
<details>
<summary>摘要</summary>
要实现多Modal量子分析的协助，我们提出了一种结合多级空间变换网络（MSSTN）和多级UNet（MSUNet）模块的端到端模型，用于自动将核心心脏图像重定向和分割为标准短轴扁平图像。左心室（LV）区域检测和各个患者的各种心脏结构带来了重orientation和分割任务的挑战。我们的方法通过一种多级strategy来生成和提取图像特征，从而提高重定向和分割性能。我们对13N-氨基酸PET和99mTc-氯丙胺SPECT两种核心心脏图像模式进行训练和测试，并取得了显著提高重定向和分割性能的结果。这种结合学习框架可以减少心脏图像手动分割的劳动，从而为物理学家提供多模态量子分析的协助。
</details></li>
</ul>
<hr>
<h2 id="PUCA-Patch-Unshuffle-and-Channel-Attention-for-Enhanced-Self-Supervised-Image-Denoising"><a href="#PUCA-Patch-Unshuffle-and-Channel-Attention-for-Enhanced-Self-Supervised-Image-Denoising" class="headerlink" title="PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising"></a>PUCA: Patch-Unshuffle and Channel Attention for Enhanced Self-Supervised Image Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10088">http://arxiv.org/abs/2310.10088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HyemiEsme/PUCA">https://github.com/HyemiEsme/PUCA</a></li>
<li>paper_authors: Hyemi Jang, Junsung Park, Dahuin Jung, Jaihyun Lew, Ho Bae, Sungroh Yoon</li>
<li>for: 自动化图像干扰除（image denoising）</li>
<li>methods: 使用自适应卷积（dilated attention blocks）和质patch-unshuffle&#x2F;shuffle来扩大感知场和保持J-不变性</li>
<li>results: 实验结果显示，PUCA在自主学习图像干扰除中实现了状态的最佳性能，超过了现有方法的性能Here’s the simplified Chinese text in the format you requested:</li>
<li>for: 自动化图像干扰除（image denoising）</li>
<li>methods: 使用自适应卷积（dilated attention blocks）和质patch-unshuffle&#x2F;shuffle来扩大感知场和保持J-不变性</li>
<li>results: 实验结果显示，PUCA在自主学习图像干扰除中实现了状态的最佳性能，超过了现有方法的性能<details>
<summary>Abstract</summary>
Although supervised image denoising networks have shown remarkable performance on synthesized noisy images, they often fail in practice due to the difference between real and synthesized noise. Since clean-noisy image pairs from the real world are extremely costly to gather, self-supervised learning, which utilizes noisy input itself as a target, has been studied. To prevent a self-supervised denoising model from learning identical mapping, each output pixel should not be influenced by its corresponding input pixel; This requirement is known as J-invariance. Blind-spot networks (BSNs) have been a prevalent choice to ensure J-invariance in self-supervised image denoising. However, constructing variations of BSNs by injecting additional operations such as downsampling can expose blinded information, thereby violating J-invariance. Consequently, convolutions designed specifically for BSNs have been allowed only, limiting architectural flexibility. To overcome this limitation, we propose PUCA, a novel J-invariant U-Net architecture, for self-supervised denoising. PUCA leverages patch-unshuffle/shuffle to dramatically expand receptive fields while maintaining J-invariance and dilated attention blocks (DABs) for global context incorporation. Experimental results demonstrate that PUCA achieves state-of-the-art performance, outperforming existing methods in self-supervised image denoising.
</details>
<details>
<summary>摘要</summary>
尽管监督的图像噪声网络已经在合成噪声图像上展现出惊人的表现，但在实际应用中它们经常失败，因为实际噪声和合成噪声之间存在差异。由于干净图像和噪声图像的对应对不容易获得，自动学习，即使用噪声输入本身作为目标，已经被研究。为保证自动学习噪声除掉模型不学习同一个映射，每个输出像素不能受到其对应的输入像素的影响，这种要求被称为J-不变性。盲区网络（BSN）在保证J-不变性方面广泛应用。然而，通过在BSN中注射额外操作，如下采样，可能会暴露盲区信息，从而违反J-不变性。因此，特制的BSN材料只能被允许，限制了建筑的创新性。为了突破这一限制，我们提出了PUCA，一种新的J-不变的U-Net架构，用于自动学习噪声除掉。PUCA利用质心不排序/排序来巨大地扩大接收场，同时保持J-不变性，并采用扩展的注意块（DABs）来 incorporate global context。实验结果表明，PUCA可以达到状态之内的表现，比存在的方法在自动学习噪声除掉中高效。
</details></li>
</ul>
<hr>
<h2 id="Expression-Domain-Translation-Network-for-Cross-domain-Head-Reenactment"><a href="#Expression-Domain-Translation-Network-for-Cross-domain-Head-Reenactment" class="headerlink" title="Expression Domain Translation Network for Cross-domain Head Reenactment"></a>Expression Domain Translation Network for Cross-domain Head Reenactment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10073">http://arxiv.org/abs/2310.10073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taewoong Kang, Jeongsik Oh, Jaeseong Lee, Sunghyun Park, Jaegul Choo<br>for:* The paper aims to improve cross-domain head reenactment, specifically transferring human motions to cartoon characters.methods:* The paper introduces a novel expression domain translation network to transform human expressions into anime expressions.* The network uses a 3D geometric-aware loss function to ensure geometric consistency between the input and output expressions.results:* The proposed method outperforms existing methods in both qualitative and quantitative analysis, demonstrating a significant advancement in cross-domain head reenactment.<details>
<summary>Abstract</summary>
Despite the remarkable advancements in head reenactment, the existing methods face challenges in cross-domain head reenactment, which aims to transfer human motions to domains outside the human, including cartoon characters. It is still difficult to extract motion from out-of-domain images due to the distinct appearances, such as large eyes. Recently, previous work introduced a large-scale anime dataset called AnimeCeleb and a cross-domain head reenactment model, including an optimization-based mapping function to translate the human domain's expressions to the anime domain. However, we found that the mapping function, which relies on a subset of expressions, imposes limitations on the mapping of various expressions. To solve this challenge, we introduce a novel expression domain translation network that transforms human expressions into anime expressions. Specifically, to maintain the geometric consistency of expressions between the input and output of the expression domain translation network, we employ a 3D geometric-aware loss function that reduces the distances between the vertices in the 3D mesh of the human and anime. By doing so, it forces high-fidelity and one-to-one mapping with respect to two cross-expression domains. Our method outperforms existing methods in both qualitative and quantitative analysis, marking a significant advancement in the field of cross-domain head reenactment.
</details>
<details>
<summary>摘要</summary>
尽管有很多HEADreenactment的进步，现有的方法在跨domain HEADreenactment中遇到了困难，即将人类动作转移到不同的领域，如漫画人物。因为这些领域的外观有很大的不同，例如大的眼睛，提取动作从不同领域的图像仍然是很困难的。在最近，之前的工作已经介绍了一个大规模的漫画数据集called AnimeCeleb和一种跨领域HEADreenactment模型，包括一种优化基于映射函数，将人类领域的表情翻译到漫画领域。然而，我们发现这种映射函数，它基于一 subset of 表情，强制了表情的限制。为解决这个挑战，我们引入了一种新的表情频率翻译网络，可以将人类表情翻译成漫画表情。在我们的方法中，我们采用了一种3D геометрически感知的损失函数，以保持表情的几何一致性。这使得我们的方法可以具有高精度和一对一的映射性，对于两个跨表情频率的跨领域映射。我们的方法在质量和量度分析中都超过了现有的方法，标志着跨领域HEADreenactment领域的重要进步。
</details></li>
</ul>
<hr>
<h2 id="ZoomTrack-Target-aware-Non-uniform-Resizing-for-Efficient-Visual-Tracking"><a href="#ZoomTrack-Target-aware-Non-uniform-Resizing-for-Efficient-Visual-Tracking" class="headerlink" title="ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking"></a>ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10071">http://arxiv.org/abs/2310.10071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kou-99/zoomtrack">https://github.com/kou-99/zoomtrack</a></li>
<li>paper_authors: Yutong Kou, Jin Gao, Bing Li, Gang Wang, Weiming Hu, Yizheng Wang, Liang Li</li>
<li>for: 本研究旨在实现高速追踪，并获得高性能的tracking results，而不是专注于性能和速度之间的 compromise.</li>
<li>methods: 本文使用非对称resize的方法，将cropped image的input size变小，并保持目标的视觉资讯。这个方法可以通过quadratic programming (QP) efficiently solve，并可以与大多数的crop-based local tracker naturally integrate.</li>
<li>results: 在五个挑战性的dataset上，本文的方法可以获得了consistent improvement，并在speed-oriented版本的OSTrack上even outperform its performance-oriented counterpart by 0.6% AUC on TNL2K，并且在50% faster和save over 55% MACs的情况下实现。<details>
<summary>Abstract</summary>
Recently, the transformer has enabled the speed-oriented trackers to approach state-of-the-art (SOTA) performance with high-speed thanks to the smaller input size or the lighter feature extraction backbone, though they still substantially lag behind their corresponding performance-oriented versions. In this paper, we demonstrate that it is possible to narrow or even close this gap while achieving high tracking speed based on the smaller input size. To this end, we non-uniformly resize the cropped image to have a smaller input size while the resolution of the area where the target is more likely to appear is higher and vice versa. This enables us to solve the dilemma of attending to a larger visual field while retaining more raw information for the target despite a smaller input size. Our formulation for the non-uniform resizing can be efficiently solved through quadratic programming (QP) and naturally integrated into most of the crop-based local trackers. Comprehensive experiments on five challenging datasets based on two kinds of transformer trackers, \ie, OSTrack and TransT, demonstrate consistent improvements over them. In particular, applying our method to the speed-oriented version of OSTrack even outperforms its performance-oriented counterpart by 0.6% AUC on TNL2K, while running 50% faster and saving over 55% MACs. Codes and models are available at https://github.com/Kou-99/ZoomTrack.
</details>
<details>
<summary>摘要</summary>
最近，transformer已经使得速度强调跟踪器可以达到状态之Art（SOTA）性能，并且具有更高的速度，即使使用更小的输入大小或更轻量级的特征提取核心。然而，它们仍然较相对落后于其对应的性能强调版本。在这篇论文中，我们展示了可以减少或甚至消除这个差距，而且可以在更小的输入大小下实现高速跟踪。为此，我们非均匀地缩放cropped图像，使得target的可能出现的区域的分辨率更高，而其他区域的分辨率相对较低。这样可以解决在更小的输入大小下尚可以保留更多的原始信息来搜寻target的问题。我们的非均匀缩放的形式可以通过quadratic programming（QP）有效地解决，并且自然地整合到大多数的crop-based本地跟踪器中。我们在五个复杂的数据集上进行了广泛的实验，并示出了一致的改进。特别是，对于速度强调版本的OSTrack，我们的方法可以在TNL2K上提高0.6%的AUC，并且在50%的速度下运行，并且占用了55%的MACs。代码和模型可以在https://github.com/Kou-99/ZoomTrack上获取。
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Person-Search-on-Open-world-User-Generated-Video-Content"><a href="#Generalizable-Person-Search-on-Open-world-User-Generated-Video-Content" class="headerlink" title="Generalizable Person Search on Open-world User-Generated Video Content"></a>Generalizable Person Search on Open-world User-Generated Video Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10068">http://arxiv.org/abs/2310.10068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Li, Guanshuo Wang, Yichao Yan, Fufu Yu, Qiong Jia, Jie Qin, Shouhong Ding, Xiaokang Yang</li>
<li>for: 实现人寻找任务中的扩展性能，尤其是在不同的摄像头和环境中。</li>
<li>methods: 提出了一个通用框架，包括两种水平的普遍化：对于特征水平，引入多任务抽象型普通批量对顶推对顶推标准差，对于数据水平，则是通过通道宽度ID相关特征装饰策略来实现普遍化。</li>
<li>results: 在两个挑战人寻找测试 bencmarks 上获得了可靠的表现，无需使用任何人工标注或目标领域的样本。<details>
<summary>Abstract</summary>
Person search is a challenging task that involves detecting and retrieving individuals from a large set of un-cropped scene images. Existing person search applications are mostly trained and deployed in the same-origin scenarios. However, collecting and annotating training samples for each scene is often difficult due to the limitation of resources and the labor cost. Moreover, large-scale intra-domain data for training are generally not legally available for common developers, due to the regulation of privacy and public security. Leveraging easily accessible large-scale User Generated Video Contents (\emph{i.e.} UGC videos) to train person search models can fit the open-world distribution, but still suffering a performance gap from the domain difference to surveillance scenes. In this work, we explore enhancing the out-of-domain generalization capabilities of person search models, and propose a generalizable framework on both feature-level and data-level generalization to facilitate downstream tasks in arbitrary scenarios. Specifically, we focus on learning domain-invariant representations for both detection and ReID by introducing a multi-task prototype-based domain-specific batch normalization, and a channel-wise ID-relevant feature decorrelation strategy. We also identify and address typical sources of noise in open-world training frames, including inaccurate bounding boxes, the omission of identity labels, and the absence of cross-camera data. Our framework achieves promising performance on two challenging person search benchmarks without using any human annotation or samples from the target domain.
</details>
<details>
<summary>摘要</summary>
人体搜索是一项复杂的任务，它涉及到从大量未修剪场景图像中检测和检索人体。现有的人体搜索应用程序都是在同一个来源场景中训练和部署的。然而，收集和标注训练样本的成本是非常高，尤其是在资源和劳动力方面。此外，大规模内域数据 для训练通常不可得，因为隐私和公共安全的法规限制。我们利用可达性高的用户生成内容（i.e., UGC视频）来训练人体搜索模型，以适应开放世界分布。然而，这些模型仍然受到频率域不同的问题带来的性能差。在这种情况下，我们提出了一种通用的框架，以便在无法预测的情况下进行下游任务。我们主要关注于学习域外 invariant 表示，包括检测和 ReID 领域的学习。我们引入多任务prototype-based域特定批处理，以及通道级 ID 相关特征修饰策略。我们还识别和解决常见的开放世界训练帧中的噪声源，包括不准确的 bounding box、缺失标签和 cross-camera 数据缺失。我们的框架在两个复杂的人体搜索标准准点上达到了无需使用人类标注或目标域样本的承诺性能。
</details></li>
</ul>
<hr>
<h2 id="A-computational-model-of-serial-and-parallel-processing-in-visual-search"><a href="#A-computational-model-of-serial-and-parallel-processing-in-visual-search" class="headerlink" title="A computational model of serial and parallel processing in visual search"></a>A computational model of serial and parallel processing in visual search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10061">http://arxiv.org/abs/2310.10061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rachelfheaton/CASPER-model">https://github.com/rachelfheaton/CASPER-model</a></li>
<li>paper_authors: Rachel F. Heaton</li>
<li>For: This paper aims to understand the nature of human visual representations and processes through the study of visual search.* Methods: The paper presents a theory of visual search based on empirical findings and instantiated in a computational model called CASPER (Concurrent Attention: Serial and Parallel Evaluation with Relations).* Results: The paper describes seven experiments that test CASPER’s predictions about relational search, and shows that CASPER can account for negative acceleration in search functions for relational stimuli.Here are the three points in Simplified Chinese:</li>
<li>for: 这篇论文旨在通过视觉搜索来理解人类视觉表示和过程的本质。</li>
<li>methods: 这篇论文提出了基于实验发现的视觉搜索理论，并将其实现在一个名为CASPER（并行注意力：序列和平行评估与关系）的计算模型中。</li>
<li>results: 这篇论文描述了七个实验，以测试CASPER模型对关系搜索的预测，并显示了CASPER可以解释视觉系统在关系刺激下的负加速度。<details>
<summary>Abstract</summary>
The following is a dissertation aimed at understanding what the various phenomena in visual search teach us about the nature of human visual representations and processes. I first review some of the major empirical findings in the study of visual search. I next present a theory of visual search in terms of what I believe these findings suggest about the representations and processes underlying ventral visual processing. These principles are instantiated in a computational model called CASPER (Concurrent Attention: Serial and Parallel Evaluation with Relations), originally developed by Hummel, that I have adapted to account for a range of phenomena in visual search. I then describe an extension of the CASPER model to account for our ability to search for visual items defined not simply by the features composing those items but by the spatial relations among those features. Seven experiments (four main experiments and three replications) are described that test CASPER's predictions about relational search. Finally, I evaluate the fit between CASPER's predictions and the empirical findings and show with three additional simulations that CASPER can account for negative acceleration in search functions for relational stimuli if one postulates that the visual system is leveraging an emergent feature that bypasses relational processing.
</details>
<details>
<summary>摘要</summary>
这是一篇关于视觉搜寻的论文，旨在了解人类视觉表示和过程中的本质。我首先介绍了视觉搜寻的一些主要实验发现，然后提出了基于这些发现的视觉搜寻理论。这些原则通过我修改了由哔哔（Hummel）开发的计算模型CASPER（同时注意力：串行和平行评估与关系）来实现。我然后描述了一种扩展CASPER模型，以便解释我们在视觉搜寻中搜寻视觉物体的空间关系。然后，我描述了七个实验（四个主要实验和三个复现），用于测试CASPER模型的预测。最后，我评估了CASPER模型的预测与实验发现的Compatibility，并通过三个额外的仿真显示CASPER模型可以解释视觉系统在搜寻关系性 stimulus 时的负加速。
</details></li>
</ul>
<hr>
<h2 id="EAR-Net-Pursuing-End-to-End-Absolute-Rotations-from-Multi-View-Images"><a href="#EAR-Net-Pursuing-End-to-End-Absolute-Rotations-from-Multi-View-Images" class="headerlink" title="EAR-Net: Pursuing End-to-End Absolute Rotations from Multi-View Images"></a>EAR-Net: Pursuing End-to-End Absolute Rotations from Multi-View Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10051">http://arxiv.org/abs/2310.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhen Liu, Qiulei Dong</li>
<li>for: 提供一种结构为深度神经网络的综合方法，用于从多视图图像中估计绝对旋转。</li>
<li>methods: 使用深度神经网络建立 epipolar confidence graph，并使用 confidence-aware rotation averaging 模块来预测绝对旋转。</li>
<li>results: 在三个公共数据集上，EAR-Net 比现有方法提高了准确性和速度。<details>
<summary>Abstract</summary>
Absolute rotation estimation is an important topic in 3D computer vision. Existing works in literature generally employ a multi-stage (at least two-stage) estimation strategy where multiple independent operations (feature matching, two-view rotation estimation, and rotation averaging) are implemented sequentially. However, such a multi-stage strategy inevitably leads to the accumulation of the errors caused by each involved operation, and degrades its final estimation on global rotations accordingly. To address this problem, we propose an End-to-end method for estimating Absolution Rotations from multi-view images based on deep neural Networks, called EAR-Net. The proposed EAR-Net consists of an epipolar confidence graph construction module and a confidence-aware rotation averaging module. The epipolar confidence graph construction module is explored to simultaneously predict pairwise relative rotations among the input images and their corresponding confidences, resulting in a weighted graph (called epipolar confidence graph). Based on this graph, the confidence-aware rotation averaging module, which is differentiable, is explored to predict the absolute rotations. Thanks to the introduced confidences of the relative rotations, the proposed EAR-Net could effectively handle outlier cases. Experimental results on three public datasets demonstrate that EAR-Net outperforms the state-of-the-art methods by a large margin in terms of accuracy and speed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>三维计算机视觉中的绝对旋转估算是一个重要的话题。现有文献中的方法通常采用多个独立的操作（特征匹配、两视旋转估算和旋转平均）的多阶段（至少两阶段）Strategy，这会导致每个参与的操作的错误积累，从而影响最终的全球旋转估算。为解决这个问题，我们提出了基于深度神经网络的绝对旋转估算方法，called EAR-Net。提案的 EAR-Net 包括 Epipolar 信任图构建模块和信任度权重平均模块。Epipolar 信任图构建模块可以同时预测输入图像之间的对应关系和它们的相对旋转信任度，从而构建一个权重图（称为 Epipolar 信任图）。基于这个图，信任度权重平均模块可以预测绝对旋转。由于引入的相对旋转信任度，提案的 EAR-Net 可以有效地处理异常情况。实验结果表明，EAR-Net 在三个公共数据集上的准确率和速度都高于当前状态的方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Hyperspectral-Image-Fusion-via-Logarithmic-Low-rank-Tensor-Ring-Decomposition"><a href="#Hyperspectral-Image-Fusion-via-Logarithmic-Low-rank-Tensor-Ring-Decomposition" class="headerlink" title="Hyperspectral Image Fusion via Logarithmic Low-rank Tensor Ring Decomposition"></a>Hyperspectral Image Fusion via Logarithmic Low-rank Tensor Ring Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10044">http://arxiv.org/abs/2310.10044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Zhang, Lipeng Zhu, Chao Wang, Shutao Li</li>
<li>for: 本研究旨在提高低分辨率多spectral图像（LR-HSI）与高分辨率多spectral图像（HR-MSI）的混合方法，以获得高分辨率多spectral图像（HR-HSI）。</li>
<li>methods: 本研究使用了tensor环（TR）分解方法，并利用了tensor核内积 regularization（TNN）来保持高维低级结构。</li>
<li>results: 实验结果表明，提出的方法可以提高视觉质量，并超过现有的state-of-the-art混合方法 regarding various quantitative metrics。<details>
<summary>Abstract</summary>
Integrating a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) is recognized as a valid method for acquiring HR-HSI. Among the current fusion approaches, the tensor ring (TR) decomposition-based method has received growing attention owing to its superior performance on preserving the spatial-spectral correlation. Furthermore, the low-rank property in some TR factors has been exploited via the matrix nuclear norm regularization along mode-2. On the other hand, the tensor nuclear norm (TNN)-based approaches have recently demonstrated to be more efficient on keeping high-dimensional low-rank structures in tensor recovery. Here, we study the low-rankness of TR factors from the TNN perspective and consider the mode-2 logarithmic TNN (LTNN) on each TR factor. A novel fusion model is proposed by incorporating this LTNN regularization and the weighted total variation which is to promote the continuity of HR-HSI in the spatial-spectral domain. Meanwhile, we have devised a highly efficient proximal alternating minimization algorithm to solve the proposed model. The experimental results indicate that our method improves the visual quality and exceeds the existing state-of-the-art fusion approaches with respect to various quantitative metrics.
</details>
<details>
<summary>摘要</summary>
Integrating a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) is recognized as a valid method for acquiring HR-HSI. Among the current fusion approaches, the tensor ring (TR) decomposition-based method has received growing attention owing to its superior performance on preserving the spatial-spectral correlation. Furthermore, the low-rank property in some TR factors has been exploited via the matrix nuclear norm regularization along mode-2. On the other hand, the tensor nuclear norm (TNN)-based approaches have recently demonstrated to be more efficient on keeping high-dimensional low-rank structures in tensor recovery. Here, we study the low-rankness of TR factors from the TNN perspective and consider the mode-2 logarithmic TNN (LTNN) on each TR factor. A novel fusion model is proposed by incorporating this LTNN regularization and the weighted total variation which is to promote the continuity of HR-HSI in the spatial-spectral domain. Meanwhile, we have devised a highly efficient proximal alternating minimization algorithm to solve the proposed model. The experimental results indicate that our method improves the visual quality and exceeds the existing state-of-the-art fusion approaches with respect to various quantitative metrics.Translation in Simplified Chinese:合并低分辨率干卷成像（LR-HSI）和高分辨率多spectral成像（HR-MSI）可以获得高分辨率干卷成像（HR-HSI）。当前的融合方法中，基于tensor ring（TR）分解的方法受到了越来越多的关注，因为它能够保留干卷成像的空间-spectral相关性。此外，TR因子中的低级属性也被利用了，通过matrix nuclear norm regularization along mode-2。而tensor nuclear norm（TNN）基于的方法则在tensor recovery中保持高维度低级结构方面表现更高效。在这里，我们从TNN的角度研究TR因子的低级性，并考虑mode-2 logarithmic TNN（LTNN）在每个TR因子上。我们提出了一种新的融合模型，通过加入LTNN regularization和权重Total variation来提高HR-HSI在空间-spectral频域中的连续性。此外，我们还开发了一种高效的 proximal alternating minimization算法来解决提案的模型。实验结果表明，我们的方法可以提高视觉质量，并在各种量化指标上超越现有的融合方法。
</details></li>
</ul>
<hr>
<h2 id="Evading-Detection-Actively-Toward-Anti-Forensics-against-Forgery-Localization"><a href="#Evading-Detection-Actively-Toward-Anti-Forensics-against-Forgery-Localization" class="headerlink" title="Evading Detection Actively: Toward Anti-Forensics against Forgery Localization"></a>Evading Detection Actively: Toward Anti-Forensics against Forgery Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10036">http://arxiv.org/abs/2310.10036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long Zhuo, Shenghai Luo, Shunquan Tan, Han Chen, Bin Li, Jiwu Huang</li>
<li>for: 防止黑客修改图像，使图像检测器判定图像是否修改。</li>
<li>methods: 使用自我指导和对抗学习算法，训练深度学习反诈模型，以逃脱现有的修改检测器。</li>
<li>results: 成功逃脱现有的修改检测器，并在多个 dataset 上实现了高效的修改检测。<details>
<summary>Abstract</summary>
Anti-forensics seeks to eliminate or conceal traces of tampering artifacts. Typically, anti-forensic methods are designed to deceive binary detectors and persuade them to misjudge the authenticity of an image. However, to the best of our knowledge, no attempts have been made to deceive forgery detectors at the pixel level and mis-locate forged regions. Traditional adversarial attack methods cannot be directly used against forgery localization due to the following defects: 1) they tend to just naively induce the target forensic models to flip their pixel-level pristine or forged decisions; 2) their anti-forensics performance tends to be severely degraded when faced with the unseen forensic models; 3) they lose validity once the target forensic models are retrained with the anti-forensics images generated by them. To tackle the three defects, we propose SEAR (Self-supErvised Anti-foRensics), a novel self-supervised and adversarial training algorithm that effectively trains deep-learning anti-forensic models against forgery localization. SEAR sets a pretext task to reconstruct perturbation for self-supervised learning. In adversarial training, SEAR employs a forgery localization model as a supervisor to explore tampering features and constructs a deep-learning concealer to erase corresponding traces. We have conducted largescale experiments across diverse datasets. The experimental results demonstrate that, through the combination of self-supervised learning and adversarial learning, SEAR successfully deceives the state-of-the-art forgery localization methods, as well as tackle the three defects regarding traditional adversarial attack methods mentioned above.
</details>
<details>
<summary>摘要</summary>
反反馈技术目的是消除或隐藏修改 traces。通常，反反馈方法是为了欺骗 binary 检测器，使其错误地评估图像的 authenticity。然而，据我们所知，没有任何尝试使用对 forgery 位置进行欺骗和误导。传统的反对敌方攻击方法无法直接使用对 forgery 位置的攻击，因为以下三点缺陷：1. 它们通常只是简单地让目标伪钞模型变更其像素级的原始或修改的决策;2. 它们对于不同的伪钞模型表现出很差的防御性能;3. 它们在伪钞模型被重新训练后失效。为了解决这些缺陷，我们提出了 SEAR（Self-supErvised Anti-foRensics），一种新的自我超vised 和反对敌方训练算法，用于训练深度学习反反馈模型。SEAR 设置了一个预text task，用于在自我超vised 学习中重建杂音。在反对敌方训练中，SEAR 使用一个 forgery 位置模型作为监视器，以探索修改特征并构建深度学习隐藏器，以消除相应的 traces。我们在多个 dataset 上进行了大规模的实验，实验结果表明，通过将自我超vised 学习和反对敌方训练相结合，SEAR 成功地欺骗了当前最佳的 forgery 位置方法，同时解决了传统反对敌方攻击方法所存在的三个缺陷。
</details></li>
</ul>
<hr>
<h2 id="Deep-Unfolding-Network-for-Image-Compressed-Sensing-by-Content-adaptive-Gradient-Updating-and-Deformation-invariant-Non-local-Modeling"><a href="#Deep-Unfolding-Network-for-Image-Compressed-Sensing-by-Content-adaptive-Gradient-Updating-and-Deformation-invariant-Non-local-Modeling" class="headerlink" title="Deep Unfolding Network for Image Compressed Sensing by Content-adaptive Gradient Updating and Deformation-invariant Non-local Modeling"></a>Deep Unfolding Network for Image Compressed Sensing by Content-adaptive Gradient Updating and Deformation-invariant Non-local Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10033">http://arxiv.org/abs/2310.10033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxue Cui, Xiaopeng Fan, Jian Zhang, Debin Zhao</li>
<li>for: 用于图像压缩感知（CS）领域的深度 unfolding 网络（DUN）的改进。</li>
<li>methods: 提出了一种基于传统的 Proximal Gradient Descent（PGD）算法的 novel DUN 网络（dubbed DUN-CSNet），以解决现有 DUN 中的两个问题：1）大多数超参数是独立于输入内容的，限制了其适应性；2）在每次迭代中使用的普通 convolutional neural network 弱化了更广泛的上下文优先顺序，导致表达能力下降。</li>
<li>results: 经验表明，提出的 DUN-CSNet 在图像压缩感知领域的表现较前者有大幅提升。<details>
<summary>Abstract</summary>
Inspired by certain optimization solvers, the deep unfolding network (DUN) has attracted much attention in recent years for image compressed sensing (CS). However, there still exist the following two issues: 1) In existing DUNs, most hyperparameters are usually content independent, which greatly limits their adaptability for different input contents. 2) In each iteration, a plain convolutional neural network is usually adopted, which weakens the perception of wider context prior and therefore depresses the expressive ability. In this paper, inspired by the traditional Proximal Gradient Descent (PGD) algorithm, a novel DUN for image compressed sensing (dubbed DUN-CSNet) is proposed to solve the above two issues. Specifically, for the first issue, a novel content adaptive gradient descent network is proposed, in which a well-designed step size generation sub-network is developed to dynamically allocate the corresponding step sizes for different textures of input image by generating a content-aware step size map, realizing a content-adaptive gradient updating. For the second issue, considering the fact that many similar patches exist in an image but have undergone a deformation, a novel deformation-invariant non-local proximal mapping network is developed, which can adaptively build the long-range dependencies between the nonlocal patches by deformation-invariant non-local modeling, leading to a wider perception on context priors. Extensive experiments manifest that the proposed DUN-CSNet outperforms existing state-of-the-art CS methods by large margins.
</details>
<details>
<summary>摘要</summary>
traditional Proximal Gradient Descent (PGD) 算法的灵感，一种新的深度 unfolding 网络（DUN）为图像压缩感知（CS）提出了一种新的方法。在这种方法中，存在两个问题：1）在现有的 DUN 中，大多数超参数是独立于输入内容的，这限制了它们的适应性。2）在每个迭代中，通常采用平面卷积神经网络，这弱化了更广泛的上下文先验，从而降低了表达能力。在这篇论文中，我们提出了一种新的 DUN-CSNet，以解决以上两个问题。Specifically，为了解决第一个问题，我们提出了一种新的内容适应的梯度下降网络，其中包括一个 Well-designed 步长生成子网络，通过生成内容快照映射，实现内容适应的梯度更新。为了解决第二个问题，我们发展了一种新的非 lok 的非局部抽象映射网络，该网络可以在不同的扭变下自适应地建立非局部的长距离依赖关系，从而扩大上下文先验的视野。经过广泛的实验，我们发现，提出的 DUN-CSNet 可以舒适性地击败现有的CS方法。
</details></li>
</ul>
<hr>
<h2 id="RoomDesigner-Encoding-Anchor-latents-for-Style-consistent-and-Shape-compatible-Indoor-Scene-Generation"><a href="#RoomDesigner-Encoding-Anchor-latents-for-Style-consistent-and-Shape-compatible-Indoor-Scene-Generation" class="headerlink" title="RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation"></a>RoomDesigner: Encoding Anchor-latents for Style-consistent and Shape-compatible Indoor Scene Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10027">http://arxiv.org/abs/2310.10027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhao-yiqun/roomdesigner">https://github.com/zhao-yiqun/roomdesigner</a></li>
<li>paper_authors: Yiqun Zhao, Zibo Zhao, Jing Li, Sixun Dong, Shenghua Gao</li>
<li>for: 本研究旨在创造具有尺度、样式兼容的室内场景，以便在室内设计和规划中提供更加真实和可信的场景。</li>
<li>methods: 本研究提出了一种两stage模型，首先使用离散 вектор量化来编码家具为anchor-latent，然后利用 transformer 模型预测室内场景。通过 incorporating anchor-latent 表示，我们的生成模型可以生成具有尺度和样式兼容的家具布局。</li>
<li>results: 实验结果表明，我们的方法可以在 3D-Front 数据集上生成更加一致和兼容的室内场景，而无需shape取样。此外，我们还进行了广泛的ablation研究，以验证我们的设计选择在室内场景生成模型中的效果。<details>
<summary>Abstract</summary>
Indoor scene generation aims at creating shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout. However, most existing approaches primarily focus on generating plausible furniture layouts without incorporating specific details related to individual furniture pieces. To address this limitation, we propose a two-stage model integrating shape priors into the indoor scene generation by encoding furniture as anchor latent representations. In the first stage, we employ discrete vector quantization to encode furniture pieces as anchor-latents. Based on the anchor-latents representation, the shape and location information of the furniture was characterized by a concatenation of location, size, orientation, class, and our anchor latent. In the second stage, we leverage a transformer model to predict indoor scenes autoregressively. Thanks to incorporating the proposed anchor-latents representations, our generative model produces shape-compatible and style-consistent furniture arrangements and synthesis furniture in diverse shapes. Furthermore, our method facilitates various human interaction applications, such as style-consistent scene completion, object mismatch correction, and controllable object-level editing. Experimental results on the 3D-Front dataset demonstrate that our approach can generate more consistent and compatible indoor scenes compared to existing methods, even without shape retrieval. Additionally, extensive ablation studies confirm the effectiveness of our design choices in the indoor scene generation model.
</details>
<details>
<summary>摘要</summary>
indoor scene generation aims to create shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout. However, most existing approaches primarily focus on generating plausible furniture layouts without incorporating specific details related to individual furniture pieces. To address this limitation, we propose a two-stage model integrating shape priors into the indoor scene generation by encoding furniture as anchor latent representations. In the first stage, we employ discrete vector quantization to encode furniture pieces as anchor-latents. Based on the anchor-latents representation, the shape and location information of the furniture was characterized by a concatenation of location, size, orientation, class, and our anchor latent. In the second stage, we leverage a transformer model to predict indoor scenes autoregressively. Thanks to incorporating the proposed anchor-latents representations, our generative model produces shape-compatible and style-consistent furniture arrangements and synthesis furniture in diverse shapes. Furthermore, our method facilitates various human interaction applications, such as style-consistent scene completion, object mismatch correction, and controllable object-level editing. Experimental results on the 3D-Front dataset demonstrate that our approach can generate more consistent and compatible indoor scenes compared to existing methods, even without shape retrieval. Additionally, extensive ablation studies confirm the effectiveness of our design choices in the indoor scene generation model.Here's the text with some minor adjustments to make it more idiomatic in Simplified Chinese:indoor scene generation aims to create shape-compatible, style-consistent furniture arrangements within a spatially reasonable layout. However, most existing approaches primarily focus on generating plausible furniture layouts without incorporating specific details related to individual furniture pieces. To address this limitation, we propose a two-stage model integrating shape priors into the indoor scene generation by encoding furniture as anchor latent representations. In the first stage, we employ discrete vector quantization to encode furniture pieces as anchor-latents. Based on the anchor-latents representation, the shape and location information of the furniture was characterized by a concatenation of location, size, orientation, class, and our anchor latent. In the second stage, we leverage a transformer model to predict indoor scenes autoregressively. Thanks to incorporating the proposed anchor-latents representations, our generative model produces shape-compatible and style-consistent furniture arrangements and synthesis furniture in diverse shapes. Furthermore, our method facilitates various human interaction applications, such as style-consistent scene completion, object mismatch correction, and controllable object-level editing. Experimental results on the 3D-Front dataset demonstrate that our approach can generate more consistent and compatible indoor scenes compared to existing methods, even without shape retrieval. Additionally, extensive ablation studies confirm the effectiveness of our design choices in the indoor scene generation model.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Super-resolution-on-Low-resolution-Micro-expression-Recognition"><a href="#An-Empirical-Study-of-Super-resolution-on-Low-resolution-Micro-expression-Recognition" class="headerlink" title="An Empirical Study of Super-resolution on Low-resolution Micro-expression Recognition"></a>An Empirical Study of Super-resolution on Low-resolution Micro-expression Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10022">http://arxiv.org/abs/2310.10022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ling Zhou, Mingpei Wang, Xiaohua Huang, Wenming Zheng, Qirong Mao, Guoying Zhao</li>
<li>for: 本研究旨在提高低分辨率（LR）环境中的微表情识别（MER）精度，特别是在实际应用中的群体MER场景。</li>
<li>methods: 本研究使用了七种最新的状态之册（SOTA）MER技术，并对13种SOTA超分解（SR）技术进行评估，以解决SR助成MER中的问题。</li>
<li>results: 经验研究表明，SR助成MER在LR场景中存在主要的挑战，并提出了改进SR助成MER的方向。<details>
<summary>Abstract</summary>
Micro-expression recognition (MER) in low-resolution (LR) scenarios presents an important and complex challenge, particularly for practical applications such as group MER in crowded environments. Despite considerable advancements in super-resolution techniques for enhancing the quality of LR images and videos, few study has focused on investigate super-resolution for improving LR MER. The scarcity of investigation can be attributed to the inherent difficulty in capturing the subtle motions of micro-expressions, even in original-resolution MER samples, which becomes even more challenging in LR samples due to the loss of distinctive features. Furthermore, a lack of systematic benchmarking and thorough analysis of super-resolution-assisted MER methods has been noted. This paper tackles these issues by conducting a series of benchmark experiments that integrate both super-resolution (SR) and MER methods, guided by an in-depth literature survey. Specifically, we employ seven cutting-edge state-of-the-art (SOTA) MER techniques and evaluate their performance on samples generated from 13 SOTA SR techniques, thereby addressing the problem of super-resolution in MER. Through our empirical study, we uncover the primary challenges associated with SR-assisted MER and identify avenues to tackle these challenges by leveraging recent advancements in both SR and MER methodologies. Our analysis provides insights for progressing toward more efficient SR-assisted MER.
</details>
<details>
<summary>摘要</summary>
低分辨率（LR）环境下的微表达识别（MER）具有重要和复杂的挑战，尤其是在实际应用中，如集体MER在拥挤的环境中。despite considerable advancements in super-resolution techniques for enhancing the quality of LR images and videos, few studies have focused on investigating super-resolution for improving LR MER. The scarcity of investigation can be attributed to the inherent difficulty in capturing the subtle motions of micro-expressions, even in original-resolution MER samples, which becomes even more challenging in LR samples due to the loss of distinctive features. Furthermore, a lack of systematic benchmarking and thorough analysis of super-resolution-assisted MER methods has been noted. This paper tackles these issues by conducting a series of benchmark experiments that integrate both super-resolution (SR) and MER methods, guided by an in-depth literature survey. Specifically, we employ seven cutting-edge state-of-the-art (SOTA) MER techniques and evaluate their performance on samples generated from 13 SOTA SR techniques, thereby addressing the problem of super-resolution in MER. Through our empirical study, we uncover the primary challenges associated with SR-assisted MER and identify avenues to tackle these challenges by leveraging recent advancements in both SR and MER methodologies. Our analysis provides insights for progressing toward more efficient SR-assisted MER.
</details></li>
</ul>
<hr>
<h2 id="Black-box-Targeted-Adversarial-Attack-on-Segment-Anything-SAM"><a href="#Black-box-Targeted-Adversarial-Attack-on-Segment-Anything-SAM" class="headerlink" title="Black-box Targeted Adversarial Attack on Segment Anything (SAM)"></a>Black-box Targeted Adversarial Attack on Segment Anything (SAM)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10010">http://arxiv.org/abs/2310.10010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheng Zheng, Chaoning Zhang</li>
<li>for: 本研究旨在实现对Segment Anything Model（SAM）的targeted adversarial attack（TAA），以便更好地理解SAM在恶意攻击下的Robustness。</li>
<li>methods: 该研究使用了一种简单 yet effective的方法，即只攻击图像Encoder，以解决prompt依赖性。此外，提出了一种新的规范损失来增强cross-model transferability，使攻击图像更有特征Domination。</li>
<li>results: 广泛的实验证明了我们提出的简单技术可以成功地实现黑盒TAA on SAM。<details>
<summary>Abstract</summary>
Deep recognition models are widely vulnerable to adversarial examples, which change the model output by adding quasi-imperceptible perturbation to the image input. Recently, Segment Anything Model (SAM) has emerged to become a popular foundation model in computer vision due to its impressive generalization to unseen data and tasks. Realizing flexible attacks on SAM is beneficial for understanding the robustness of SAM in the adversarial context. To this end, this work aims to achieve a targeted adversarial attack (TAA) on SAM. Specifically, under a certain prompt, the goal is to make the predicted mask of an adversarial example resemble that of a given target image. The task of TAA on SAM has been realized in a recent arXiv work in the white-box setup by assuming access to prompt and model, which is thus less practical. To address the issue of prompt dependence, we propose a simple yet effective approach by only attacking the image encoder. Moreover, we propose a novel regularization loss to enhance the cross-model transferability by increasing the feature dominance of adversarial images over random natural images. Extensive experiments verify the effectiveness of our proposed simple techniques to conduct a successful black-box TAA on SAM.
</details>
<details>
<summary>摘要</summary>
深度识别模型广泛受到敌意例子的攻击，这些攻击通过添加 quasi-不可见的扰动来改变输入图像，从而影响模型的输出。最近，Segment Anything Model（SAM）在计算机视觉领域得到了广泛的应用，因为它在未seen数据和任务上表现出了很好的总体化能力。为了更好地理解SAM在敌意上下文中的稳定性，本工作寻求实现针对SAM的targeted adversarial attack（TAA）。具体来说，在一定的提示下，目标是使针对敌意例子的预测面几乎与给定的目标图像相同。在 white-box 设置下，这项任务在最近的 arXiv 文章中已经实现了，但是假设了对提示和模型的访问，这是不实际的。为解决提示的依赖关系，我们提议一种简单 yet 有效的方法，即只攻击图像Encoder。此外，我们还提议一种新的规范损失，以增强跨模型传输性，通过增加攻击图像的特征主导性，使攻击图像在随机自然图像上占据优势。我们的实验证明了我们提议的简单技术可以成功地实现黑盒 TAA 任务。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Encoder-Decoder-Architectures-for-Robust-Coronary-Artery-Segmentation"><a href="#Assessing-Encoder-Decoder-Architectures-for-Robust-Coronary-Artery-Segmentation" class="headerlink" title="Assessing Encoder-Decoder Architectures for Robust Coronary Artery Segmentation"></a>Assessing Encoder-Decoder Architectures for Robust Coronary Artery Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10002">http://arxiv.org/abs/2310.10002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shisheng Zhang, Ramtin Gharleghi, Sonit Singh, Arcot Sowmya, Susann Beier</li>
<li>for: 避免心血管疾病的诊断延迟，通过精准 coronary artery 分 segmentation，改善病人结果。</li>
<li>methods: 使用 convolutional neural networks (CNN) 和 U-Net 架构，以及 25 个不同的 encoder-decoder 组合。</li>
<li>results: 使用 ASOCA 公共数据集，对 40 个案例进行分析，发现 EfficientNet-LinkNet 组合的 Dice 乘数为 0.882，95% Percentile Hausdorff 距离为 4.753，表明该模型在 MICCAI 2020 挑战中比其他模型表现更出色。<details>
<summary>Abstract</summary>
Coronary artery diseases are among the leading causes of mortality worldwide. Timely and accurate diagnosis, facilitated by precise coronary artery segmentation, is pivotal in changing patient outcomes. In the realm of biomedical imaging, convolutional neural networks, especially the U-Net architecture, have revolutionised segmentation processes. However, one of the primary challenges remains the lack of benchmarking datasets specific to coronary arteries. However through the use of the recently published public dataset ASOCA, the potential of deep learning for accurate coronary segmentation can be improved. This paper delves deep into examining the performance of 25 distinct encoder-decoder combinations. Through analysis of the 40 cases provided to ASOCA participants, it is revealed that the EfficientNet-LinkNet combination, serving as encoder and decoder, stands out. It achieves a Dice coefficient of 0.882 and a 95th percentile Hausdorff distance of 4.753. These findings not only underscore the superiority of our model in comparison to those presented at the MICCAI 2020 challenge but also set the stage for future advancements in coronary artery segmentation, opening doors to enhanced diagnostic and treatment strategies.
</details>
<details>
<summary>摘要</summary>
coronary artery disease 是全球最主要的死亡原因之一，时间和准确的诊断是改善病人结果的关键。在生物医学影像中，对于条形血管的精确分类是非常重要。然而，主要挑战是缺乏特定于条形血管的参考数据集。但是透过使用最近发布的公共数据集ASOCA，可以改善深度学习的精确分类性。本文将进行深入分析25组不同的encoder-decoder组合的表现。通过分析ASOCA参赛者提供的40个档案，发现了EfficientNet-LinkNet组合（作为encoder和decoder）的表现最出色，其Dice系数为0.882，95%的 Hausdorff距离为4.753。这些发现不仅与在MICCAI 2020挑战中提出的模型相比，而且开启了未来条形血管分类的新天地，将来对诊断和治疗策略带来改善。
</details></li>
</ul>
<hr>
<h2 id="SeUNet-Trans-A-Simple-yet-Effective-UNet-Transformer-Model-for-Medical-Image-Segmentation"><a href="#SeUNet-Trans-A-Simple-yet-Effective-UNet-Transformer-Model-for-Medical-Image-Segmentation" class="headerlink" title="SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical Image Segmentation"></a>SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09998">http://arxiv.org/abs/2310.09998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tan-Hanh Pham, Xianqi Li, Kim-Doang Nguyen</li>
<li>for: 这个研究旨在提出一个简单 yet effective的 UNet-Transformer（seUNet-Trans）模型，用于医疗影像分类。</li>
<li>methods: 我们使用 UNet 模型作为特征提取器，将输入影像生成多个特征地图，然后将这些地图转移到一个桥layer中，并使用 Transformer 模型进行自我注意力机制。</li>
<li>results: 我们将模型评估于五个医疗影像分类 dataset上，结果显示 seUNet-Trans 模型在这些dataset上具有较高的性能。<details>
<summary>Abstract</summary>
Automated medical image segmentation is becoming increasingly crucial in modern clinical practice, driven by the growing demand for precise diagnoses, the push towards personalized treatment plans, and advancements in machine learning algorithms, especially the incorporation of deep learning methods. While convolutional neural networks (CNNs) have been prevalent among these methods, the remarkable potential of Transformer-based models for computer vision tasks is gaining more acknowledgment. To harness the advantages of both CNN-based and Transformer-based models, we propose a simple yet effective UNet-Transformer (seUNet-Trans) model for medical image segmentation. In our approach, the UNet model is designed as a feature extractor to generate multiple feature maps from the input images, and these maps are propagated into a bridge layer, which sequentially connects the UNet and the Transformer. In this stage, we employ the pixel-level embedding technique without position embedding vectors to make the model more efficient. Moreover, we applied spatial-reduction attention in the Transformer to reduce the computational/memory overhead. By leveraging the UNet architecture and the self-attention mechanism, our model not only preserves both local and global context information but also captures long-range dependencies between input elements. The proposed model is extensively experimented on five medical image segmentation datasets, including polyp segmentation, to demonstrate its efficacy. A comparison with several state-of-the-art segmentation models on these datasets shows the superior performance of seUNet-Trans.
</details>
<details>
<summary>摘要</summary>
《自动医疗影像分割在现代临床实践中变得越来越重要，这被增长的精准诊断需求、个性化治疗方案推动以及机器学习算法的发展，特别是深度学习方法的应用所驱动。而卷积神经网络（CNN）在这些方法中具有广泛的应用，但是Transformer基本模型在计算机视觉任务中表现出了惊人的潜力。为了利用CNN和Transformer两种模型的优点，我们提出了一种简单 yet effective的UNet-Transformer（seUNet-Trans）模型。在我们的方法中，UNet模型被设计为特征提取器，生成输入图像多个特征地图，然后这些地图被传递到一个桥层，该桥层连接了UNet和Transformer。在这个阶段，我们采用了像素级嵌入技术而不使用位置嵌入向量，以使模型更加高效。此外，我们在Transformer中应用了空间减少注意力，以降低计算/存储占用的开销。通过UNet架构和自注意机制，我们的模型不仅保留了本地和全局上下文信息，还能捕捉输入元素之间的长距离依赖关系。我们对五种医疗影像分割数据集进行了广泛的实验，包括肠肝肿瘤分割，以证明seUNet-Trans模型的高效性。与一些现状顶尖分割模型进行比较，我们的模型在这些数据集上显示出了superior的性能。》
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Graph-and-Attention-Based-Hyperspectral-Image-Classification-Methods-for-Remote-Sensing-Data"><a href="#A-Survey-of-Graph-and-Attention-Based-Hyperspectral-Image-Classification-Methods-for-Remote-Sensing-Data" class="headerlink" title="A Survey of Graph and Attention Based Hyperspectral Image Classification Methods for Remote Sensing Data"></a>A Survey of Graph and Attention Based Hyperspectral Image Classification Methods for Remote Sensing Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09994">http://arxiv.org/abs/2310.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aryan Vats, Manan Suri<br>for: 本研究旨在对频谱成像图像分类中应用深度学习技术，并评估其在远程感知和航空频谱成像图像中的性能。methods: 本研究涉及使用图гра数据结构和注意机制来减少维度，以提高频谱成像图像分类的性能。同时，也探讨了使用图 convolutional Neural Networks 进行频谱成像图像特征提取，以提高分类性能。results: 根据本研究的结果，使用图гра数据结构和注意机制可以提高频谱成像图像分类的性能，并且可以在远程感知和航空频谱成像图像中实现更好的分类结果。<details>
<summary>Abstract</summary>
The use of Deep Learning techniques for classification in Hyperspectral Imaging (HSI) is rapidly growing and achieving improved performances. Due to the nature of the data captured by sensors that produce HSI images, a common issue is the dimensionality of the bands that may or may not contribute to the label class distinction. Due to the widespread nature of class labels, Principal Component Analysis is a common method used for reducing the dimensionality. However,there may exist methods that incorporate all bands of the Hyperspectral image with the help of the Attention mechanism. Furthermore, to yield better spectral spatial feature extraction, recent methods have also explored the usage of Graph Convolution Networks and their unique ability to use node features in prediction, which is akin to the pixel spectral makeup. In this survey we present a comprehensive summary of Graph based and Attention based methods to perform Hyperspectral Image Classification for remote sensing and aerial HSI images. We also summarize relevant datasets on which these techniques have been evaluated and benchmark the processing techniques.
</details>
<details>
<summary>摘要</summary>
使用深度学习技术进行干扰спектраль成像（HSI）的分类正在迅速增长，并达到了改进的性能。由于探测器生成HSI图像的数据特性，一个常见的问题是带宽的维度，这些带可能或可能不会影响类标分布。由于类标的普遍性，常用的方法包括原始特征值分析（PCA）等。然而，可能存在一些方法，它们可以在所有HSI图像带中使用注意力机制，以提高特征特征提取。此外，为了提取更好的 спектral空间特征，现有的方法还在探索使用图像卷积网络，它们可以使用节点特征进行预测，这与像素 спектраль组成类似。在本综述中，我们提供了对图基和注意力基的方法进行干扰спектраль成像图像分类的全面概述，以及相关的数据集和处理技术的比较。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.CV_2023_10_16/" data-id="clp869ty900lhk5889kzl0g0i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.AI_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T12:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.AI_2023_10_16/">cs.AI - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Greedy-Perspectives-Multi-Drone-View-Planning-for-Collaborative-Coverage-in-Cluttered-Environments"><a href="#Greedy-Perspectives-Multi-Drone-View-Planning-for-Collaborative-Coverage-in-Cluttered-Environments" class="headerlink" title="Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments"></a>Greedy Perspectives: Multi-Drone View Planning for Collaborative Coverage in Cluttered Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10863">http://arxiv.org/abs/2310.10863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Suresh, Aditya Rauniyar, Micah Corah, Sebastian Scherer</li>
<li>for: 这篇论文旨在帮助营造大规模的人群拍摄，特别是在团队体育和电影摄影等领域。</li>
<li>methods: 这篇论文使用了序列优化的方法来实现可扩展的Camera View最优化，但是在填充环境中却遇到了协调问题。</li>
<li>results: 作者通过开发了一种多机器人多演员视图规划算法，并对其进行了阻挡和遮挡意识的目标设定，以实现在填充环境中协调多机器人拍摄人群的目的。并且对比formation planner，这种顺序 планинг器在三个场景中 генериру了14%更高的actor view reward，并且在两个场景中与formation planning的性能相似。<details>
<summary>Abstract</summary>
Deployment of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance, we plan in five test environments with complex multiple-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward over the actors for three scenarios and comparable performance to formation planning on two others. We also observe near identical performance of sequential planning both with and without inter-robot collision constraints. Overall, we demonstrate effective coordination of teams of aerial robots for filming groups that may split, merge, or spread apart and in environments cluttered with obstacles that may cause collisions or occlusions.
</details>
<details>
<summary>摘要</summary>
deployments of teams of aerial robots could enable large-scale filming of dynamic groups of people (actors) in complex environments for novel applications in areas such as team sports and cinematography. Toward this end, methods for submodular maximization via sequential greedy planning can be used for scalable optimization of camera views across teams of robots but face challenges with efficient coordination in cluttered environments. Obstacles can produce occlusions and increase chances of inter-robot collision which can violate requirements for near-optimality guarantees. To coordinate teams of aerial robots in filming groups of people in dense environments, a more general view-planning approach is required. We explore how collision and occlusion impact performance in filming applications through the development of a multi-robot multi-actor view planner with an occlusion-aware objective for filming groups of people and compare with a greedy formation planner. To evaluate performance, we plan in five test environments with complex multiple-actor behaviors. Compared with a formation planner, our sequential planner generates 14% greater view reward over the actors for three scenarios and comparable performance to formation planning on two others. We also observe near identical performance of sequential planning both with and without inter-robot collision constraints. Overall, we demonstrate effective coordination of teams of aerial robots for filming groups that may split, merge, or spread apart and in environments cluttered with obstacles that may cause collisions or occlusions.Here's the text with some notes on the translation:* "deployments" is translated as "部署" (bù dào), which is a more general term that can refer to any type of deployment, not just of robots.* "aerial robots" is translated as "空中机器人" (kōng zhōng jī rò bīng), which is a more specific term that refers to robots that operate in the air.* "film" is translated as "拍摄" (pān shè), which is a more general term that can refer to any type of filming or recording.* "applications" is translated as "应用" (yìng yòu), which is a more general term that can refer to any type of use or application.* "team sports" is translated as "团体运动" (tuán tǐ yùn dòng), which is a more specific term that refers to sports that involve teams of players.* "cinematography" is translated as "摄影" (shè yǐng), which is a more specific term that refers to the art and technique of filmmaking.* "submodular maximization" is translated as "互补最大化" (huì chē zhì dà huì), which is a more specific term that refers to a type of optimization problem where the goal is to maximize a submodular function.* "sequential greedy planning" is translated as "顺序贪吃规划" (shù xìa bīng zhèng), which is a more specific term that refers to a type of planning algorithm that uses greedy heuristics to optimize a sequence of decisions.* "obstacles" is translated as "障碍物" (fāng yì wù), which is a more general term that can refer to any type of obstacle or barrier.* "occlusions" is translated as "遮挡" (miǎn zhì), which is a more specific term that refers to the blocking or hiding of objects or viewpoints by other objects or surfaces.* "inter-robot collision" is translated as "机器人间冲突" (jī rò bīng jiān chōng tòu), which is a more specific term that refers to collisions between robots.* "near-optimality guarantees" is translated as "似乎最优化保证" (xiào guī zhì yòu huì huì), which is a more specific term that refers to guarantees that a solution is close to optimal.* "view-planning" is translated as "观察规划" (guān chá zhì huì), which is a more specific term that refers to the planning of views or viewpoints.* "multi-robot multi-actor" is translated as "多机器人多actor" (duō jī rò duō yuǎn), which is a more specific term that refers to systems with multiple robots and multiple actors.* "occlusion-aware objective" is translated as "遮挡目标" (miǎn zhì mù tiǎo), which is a more specific term that refers to objectives that take into account the presence of occlusions.* "formation planner" is translated as "formation规划" (fāng yì zhì huì), which is a more specific term that refers to a type of planner that uses formations or patterns to optimize a sequence of decisions.* "sequential planner" is translated as "顺序规划" (shù xìa zhì huì), which is a more specific term that refers to a type of planner that uses a sequential search algorithm to optimize a sequence of decisions.* "test environments" is translated as "测试环境" (cè shí huán jīng), which is a more general term that can refer to any type of testing or evaluation environment.* "complex multiple-actor behaviors" is translated as "复杂多actor行为" (fāng xìa duō yuǎn xíng wèi), which is a more specific term that refers to systems with multiple actors and complex behaviors.
</details></li>
</ul>
<hr>
<h2 id="Proper-Laplacian-Representation-Learning"><a href="#Proper-Laplacian-Representation-Learning" class="headerlink" title="Proper Laplacian Representation Learning"></a>Proper Laplacian Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10833">http://arxiv.org/abs/2310.10833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Gomez, Michael Bowling, Marlos C. Machado</li>
<li>for: 解决大型反射学习问题，即探索、泛化和传递问题，需要学习好的状态表示。</li>
<li>methods: 使用 Laplacian 表示法，通过寻找矩阵 Laplacian 的特征值和特征向量来实现。</li>
<li>results: 提出了一种 theoretically 有 garantue 的目标函数和优化算法，可以准确地 recuperate  Laplacian 表示，并且在多种环境中进行了实验，证明了其在学习中的稳定性和可靠性。<details>
<summary>Abstract</summary>
The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.
</details>
<details>
<summary>摘要</summary>
“学习良好的状态表示是解决大型回归学习问题的关键，特别是在探索、泛化和传输方面存在挑战。laplacian表示是一种有 Promise的方法，它可以通过时间扩展的动作发现和奖励形成，以及有用的状态编码。但是，为了获得laplacian表示，需要计算图laplacian的eigen系统，这通常是通过兼容深度学习方法的优化目标来实现。这些优化目标 however，依赖于无法效率地调整的超参数，并且会导致优化过程中的旋转矩阵和征值的不准确性。在这篇论文中，我们介绍了一种有理论基础的目标函数和相应的优化算法，可以有效地近似laplacian表示。我们的方法可以自动回归真正的eigen vectors和征值，并消除前一代的优化目标中的超参数依赖性。我们提供了理论保证，并证明了这些结果在多个环境中的实际表现是稳定和可靠的。”
</details></li>
</ul>
<hr>
<h2 id="Detecting-Speech-Abnormalities-with-a-Perceiver-based-Sequence-Classifier-that-Leverages-a-Universal-Speech-Model"><a href="#Detecting-Speech-Abnormalities-with-a-Perceiver-based-Sequence-Classifier-that-Leverages-a-Universal-Speech-Model" class="headerlink" title="Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model"></a>Detecting Speech Abnormalities with a Perceiver-based Sequence Classifier that Leverages a Universal Speech Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13010">http://arxiv.org/abs/2310.13010</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hagen Soltau, Izhak Shafran, Alex Ottenwess, Joseph R. JR Duffy, Rene L. Utianski, Leland R. Barnard, John L. Stricker, Daniela Wiepert, David T. Jones, Hugo Botha</li>
<li>for: 检测speech中的异常现象，尤其是一些神经疾病的表现。</li>
<li>methods: 使用Perceiver-based序列分类器和Universal Speech Model（USM），并将其与12百万小时多样化音频记录进行训练。</li>
<li>results: 提出的模型在Mayo клиника检测集上表现出色，与标准转换器（80.9%）和感知器（81.8%）模型相比，具有更高的准确率（83.1%），并且在有限任务特定数据下，发现预训练是重要的，而且预训练与自动语音识别任务也是有益的。<details>
<summary>Abstract</summary>
We propose a Perceiver-based sequence classifier to detect abnormalities in speech reflective of several neurological disorders. We combine this classifier with a Universal Speech Model (USM) that is trained (unsupervised) on 12 million hours of diverse audio recordings. Our model compresses long sequences into a small set of class-specific latent representations and a factorized projection is used to predict different attributes of the disordered input speech. The benefit of our approach is that it allows us to model different regions of the input for different classes and is at the same time data efficient. We evaluated the proposed model extensively on a curated corpus from the Mayo Clinic. Our model outperforms standard transformer (80.9%) and perceiver (81.8%) models and achieves an average accuracy of 83.1%. With limited task-specific data, we find that pretraining is important and surprisingly pretraining with the unrelated automatic speech recognition (ASR) task is also beneficial. Encodings from the middle layers provide a mix of both acoustic and phonetic information and achieve best prediction results compared to just using the final layer encodings (83.1% vs. 79.6%). The results are promising and with further refinements may help clinicians detect speech abnormalities without needing access to highly specialized speech-language pathologists.
</details>
<details>
<summary>摘要</summary>
我们提议一种基于感知者的序列分类器，用于检测speech中的异常现象，表现为多种神经疾病。我们将这个分类器与一个基于自动语音识别（ASR）任务的通用语音模型（USM）结合，并在1200万小时多样化音频记录上进行无监督训练。我们的模型可以压缩长序列到一小集类特有的归一化表示和一个 факторизовый投影，以预测不同类型的输入异常speech的不同属性。我们的方法的优点在于，它可以为不同类型的输入模型不同的地方，同时具有数据效率的优势。我们对提议模型进行了广泛的评估，并在 mayo临床数据库中验证了模型。我们的模型在标准transformer（80.9%）和感知器（81.8%）模型的基础上提高了性能，并实现了83.1%的平均准确率。我们发现，在有限的任务特定数据上，预训练是重要的，而且预训练使用ASR任务也是有利的。中间层编码器提供了mixture的音频和phonetic信息，并实现了最佳预测结果（83.1% vs. 79.6%）。结果具有潜在的价值，通过进一步的优化，可能帮助临床专业人员检测speech异常性，不需要高度专业的语音学术师。
</details></li>
</ul>
<hr>
<h2 id="If-the-Sources-Could-Talk-Evaluating-Large-Language-Models-for-Research-Assistance-in-History"><a href="#If-the-Sources-Could-Talk-Evaluating-Large-Language-Models-for-Research-Assistance-in-History" class="headerlink" title="If the Sources Could Talk: Evaluating Large Language Models for Research Assistance in History"></a>If the Sources Could Talk: Evaluating Large Language Models for Research Assistance in History</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10808">http://arxiv.org/abs/2310.10808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giselle Gonzalez Garcia, Christian Weilbach</li>
<li>for: 这个论文旨在探讨如何使用大型自然语言模型（LLM）来探索历史记忆（或训练数据），并证明了在增强LLM WITH vector embedding的情况下，可以为历史学家和人文科学研究者提供一种可访问的对话式研究方法。</li>
<li>methods: 这篇论文使用了LLM进行对话式研究，并通过增强LLM WITH vector embedding来提高其对问题的回答和数据EXTRACTION和组织能力。</li>
<li>results: 论文表明，LLM可以在问题解决和数据EXTRACTION和组织等任务中表现出色，并且可以在特定研究项目中应用到大量文本档案中，无需包含在其训练数据中。因此，LLM可以被PRIVATELY queried by researchers，并且可以被用于特定研究项目中。<details>
<summary>Abstract</summary>
The recent advent of powerful Large-Language Models (LLM) provides a new conversational form of inquiry into historical memory (or, training data, in this case). We show that by augmenting such LLMs with vector embeddings from highly specialized academic sources, a conversational methodology can be made accessible to historians and other researchers in the Humanities. Concretely, we evaluate and demonstrate how LLMs have the ability of assisting researchers while they examine a customized corpora of different types of documents, including, but not exclusive to: (1). primary sources, (2). secondary sources written by experts, and (3). the combination of these two. Compared to established search interfaces for digital catalogues, such as metadata and full-text search, we evaluate the richer conversational style of LLMs on the performance of two main types of tasks: (1). question-answering, and (2). extraction and organization of data. We demonstrate that LLMs semantic retrieval and reasoning abilities on problem-specific tasks can be applied to large textual archives that have not been part of the its training data. Therefore, LLMs can be augmented with sources relevant to specific research projects, and can be queried privately by researchers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Demystifying-Poisoning-Backdoor-Attacks-from-a-Statistical-Perspective"><a href="#Demystifying-Poisoning-Backdoor-Attacks-from-a-Statistical-Perspective" class="headerlink" title="Demystifying Poisoning Backdoor Attacks from a Statistical Perspective"></a>Demystifying Poisoning Backdoor Attacks from a Statistical Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10780">http://arxiv.org/abs/2310.10780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ganghua Wang, Xun Xian, Jayanth Srinivasa, Ashish Kundu, Xuan Bi, Mingyi Hong, Jie Ding</li>
<li>for: 本研究旨在评估潜在攻击型机器学习模型的安全性，具体来说是评估含有常量触发器的后门攻击的成功因素和攻击方向。</li>
<li>methods: 本研究使用了定理和实验方法来评估后门攻击的成功因素和攻击方向。</li>
<li>results: 研究发现了一系列关键因素影响后门攻击的成功，包括触发器的类型和位置、模型的类型和训练数据的性质等。此外，研究还发现了一些可能的攻击方向，包括潜在的人为干预和模型的泄漏。<details>
<summary>Abstract</summary>
The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understanding applies to both discriminative and generative models. We also demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>What are the determining factors for a backdoor attack’s success?2. What is the direction of the most effective backdoor attack?3. When will a human-imperceptible trigger succeed?Our findings apply to both discriminative and generative models, and we demonstrate our theory through experiments using benchmark datasets and state-of-the-art backdoor attack scenarios.</details></li>
</ol>
<hr>
<h2 id="BiomedJourney-Counterfactual-Biomedical-Image-Generation-by-Instruction-Learning-from-Multimodal-Patient-Journeys"><a href="#BiomedJourney-Counterfactual-Biomedical-Image-Generation-by-Instruction-Learning-from-Multimodal-Patient-Journeys" class="headerlink" title="BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys"></a>BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10765">http://arxiv.org/abs/2310.10765</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Gu, Jianwei Yang, Naoto Usuyama, Chunyuan Li, Sheng Zhang, Matthew P. Lungren, Jianfeng Gao, Hoifung Poon</li>
<li>for: 这个研究旨在应用自然语言指令学习的技术来生成医疗影像中的counterfactual影像，以分别鉴别 causal structure 和 spurious correlation，并且帮助医生更好地阅读医疗影像进行病程模型化。</li>
<li>methods: 这个研究使用 GPT-4 处理医疗影像报告，生成医疗影像的描述，并且使用这些 triplets (prior image, progression description, new image) 进行 latent diffusion 模型的训练，以生成 counterfactual 医疗影像。</li>
<li>results: 这个研究的结果显示，BiomedJourney 方法可以对医疗影像进行高品质的 counterfactual 生成，并且substantially outperform 先前的 state-of-the-art 方法。<details>
<summary>Abstract</summary>
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion model for counterfactual biomedical image generation. Given the relative scarcity of image time series data, we introduce a two-stage curriculum that first pretrains the denoising network using the much more abundant single image-report pairs (with dummy prior image), and then continues training using the counterfactual triples. Experiments using the standard MIMIC-CXR dataset demonstrate the promise of our method. In a comprehensive battery of tests on counterfactual medical image generation, BiomedJourney substantially outperforms prior state-of-the-art methods in instruction image editing and medical image generation such as InstructPix2Pix and RoentGen. To facilitate future study in counterfactual medical generation, we plan to release our instruction-learning code and pretrained models.
</details>
<details>
<summary>摘要</summary>
快速进步在图像编辑中使用自然语言指令，如InstructPix2Pix，已经取得了显著的成果。在生物医学领域，这些方法可以应用于对比例图像生成，以分解 causal structure 和偶极相关，并且为疾病进程模型提供了更加稳定的图像解释。然而，通用的图像编辑模型在生物医学领域是不适用的，对比例生成图像的研究仍然很少。在这篇论文中，我们提出了 BiomedJourney，一种新的对比例生成方法，通过 instruction-learning 从多modal 患者旅程中学习。给定一个患有两张不同时点的生物医学图像，我们使用 GPT-4 处理相关的医学报告，并生成一个描述疾病进程的自然语言描述。这些 triple（先前图像、进程描述、新图像）然后用于训练一个潜在扩散模型进行对比例生成。由于图像时序数据的缺乏，我们提出了一个两stage 课程，首先使用 much more abundant 的单图像-报告对（与假先前图像）进行预训练，然后继续使用对比例 triple。实验使用标准的 MIMIC-CXR 数据集表明，BiomedJourney 在对比例医学图像生成方面具有明显的优势，substantially outperforming 先前的状态对照方法，如 InstructPix2Pix 和 RoentGen。为便于未来对比例医学生成的研究，我们计划在未来发布我们的 instruction-learning 代码和预训练模型。
</details></li>
</ul>
<hr>
<h2 id="Step-by-Step-Remediation-of-Students’-Mathematical-Mistakes"><a href="#Step-by-Step-Remediation-of-Students’-Mathematical-Mistakes" class="headerlink" title="Step-by-Step Remediation of Students’ Mathematical Mistakes"></a>Step-by-Step Remediation of Students’ Mathematical Mistakes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10648">http://arxiv.org/abs/2310.10648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rosewang2008/remath">https://github.com/rosewang2008/remath</a></li>
<li>paper_authors: Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, Dorottya Demszky</li>
<li>for: 这个论文的目的是探讨大型自然语言模型（LLM）在数学教学中是否能够有效地帮助新手老师更好地纠正学生的错误。</li>
<li>methods: 这个论文使用了一个名为ReMath的benchmark，该benchmark由经验丰富的数学教师共同开发，它包括三个步骤：（1）推断学生错误的类型，（2）确定修正错误的策略，（3）生成一个包含该信息的回答。这个benchmark用于评估当今最好的 instruct-tuned 和对话模型在ReMath上的性能。</li>
<li>results: 研究发现，即使使用最佳模型，模型的回答仍然不能与经验丰富的数学教师相比。提供模型错误类型和策略信息可以提高模型的回答质量，但是这些回答仍然不能达到经验教师的水平。这些结果表明，使用当今的LLM来提供高质量的学习经验，虽然有potential，但还有一定的限制。研究的代码已经公开在GitHub上：<a target="_blank" rel="noopener" href="https://github.com/rosewang2008/remath%E3%80%82">https://github.com/rosewang2008/remath。</a><details>
<summary>Abstract</summary>
Scaling high-quality tutoring is a major challenge in education. Because of the growing demand, many platforms employ novice tutors who, unlike professional educators, struggle to effectively address student mistakes and thus fail to seize prime learning opportunities for students. In this paper, we explore the potential for large language models (LLMs) to assist math tutors in remediating student mistakes. We present ReMath, a benchmark co-developed with experienced math teachers that deconstructs their thought process for remediation. The benchmark consists of three step-by-step tasks: (1) infer the type of student error, (2) determine the strategy to address the error, and (3) generate a response that incorporates that information. We evaluate the performance of state-of-the-art instruct-tuned and dialog models on ReMath. Our findings suggest that although models consistently improve upon original tutor responses, we cannot rely on models alone to remediate mistakes. Providing models with the error type (e.g., the student is guessing) and strategy (e.g., simplify the problem) leads to a 75% improvement in the response quality over models without that information. Nonetheless, despite the improvement, the quality of the best model's responses still falls short of experienced math teachers. Our work sheds light on the potential and limitations of using current LLMs to provide high-quality learning experiences for both tutors and students at scale. Our work is open-sourced at this link: \url{https://github.com/rosewang2008/remath}.
</details>
<details>
<summary>摘要</summary>
增加高质量的帮助是现代教育中的一大挑战。由于需求的增长，许多平台都雇用了不熟悉教育的新教师，与专业教师不同，他们有时无法有效地 corrected学生的错误，因此失去了学生 prime learning opportunities。在这篇论文中，我们探讨了大型自然语言模型（LLM）是否可以帮助数学 tutors  corrected学生的错误。我们提出了一个名为 ReMath 的标准，与经验丰富的数学教师合作开发。ReMath 包括三个步骤任务：（1）推断学生错误的类型，（2）确定修复错误的策略，（3）生成包含该信息的回答。我们对 state-of-the-art 的 instruct-tuned 和对话模型进行评估，我们的发现表明，虽然模型在 ReMath 上表现了进步，但我们无法仅仅通过模型来修复错误。在提供错误类型（如学生假设）和修复策略（如简化问题）的情况下，模型的回答质量提高了75%。然而，即使有这些信息，模型的回答仍然落后于经验丰富的数学教师。我们的工作探讨了当前 LLM 是否可以在大规模上提供高质量的学习经验。我们的工作开源在这里：https://github.com/rosewang2008/remath。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Video-Diffusion-Models"><a href="#A-Survey-on-Video-Diffusion-Models" class="headerlink" title="A Survey on Video Diffusion Models"></a>A Survey on Video Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10647">http://arxiv.org/abs/2310.10647</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChenHsing/Awesome-Video-Diffusion-Models">https://github.com/ChenHsing/Awesome-Video-Diffusion-Models</a></li>
<li>paper_authors: Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, Yu-Gang Jiang</li>
<li>for: 这 paper 主要是为了对 AI 生成内容 (AIGC) 领域中的 video diffusion models 进行了一个系统的综述。</li>
<li>methods: 这 paper 使用了多种方法，包括 diffusion models、GANs 和 auto-regressive Transformers，以探讨 video diffusion models 在不同领域的应用。</li>
<li>results: 这 paper 发现了许多有价值的研究结果，包括 video 生成、编辑、以及其他视频理解任务中的应用。<details>
<summary>Abstract</summary>
The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.
</details>
<details>
<summary>摘要</summary>
最近的人工智能生成内容（AIGC）浪潮中，计算机视觉领域的扩散模型发挥了关键作用。由于它们的出色的生成能力，扩散模型逐渐取代了基于GANs和自适应Transformers的方法，在图像生成和编辑领域以及视频领域的研究中表现出色。然而，现有的评论主要集中在图像生成领域中，对视频领域中的扩散模型的应用有少量最新的评论。为了填补这个空白，本文提供了人工智能生成内容时代的视频扩散模型的全面评论。 Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.
</details></li>
</ul>
<hr>
<h2 id="Interactive-Task-Planning-with-Language-Models"><a href="#Interactive-Task-Planning-with-Language-Models" class="headerlink" title="Interactive Task Planning with Language Models"></a>Interactive Task Planning with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10645">http://arxiv.org/abs/2310.10645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CraftJarvis/MC-Planner">https://github.com/CraftJarvis/MC-Planner</a></li>
<li>paper_authors: Boyi Li, Philipp Wu, Pieter Abbeel, Jitendra Malik</li>
<li>for: 这个paper是为了解决长期任务规划和执行问题，并且可以轻松泛化到不同的目标或任务。</li>
<li>methods: 这个paper使用语言模型来实现交互式任务规划，并且结合高级规划和低级功能执行。</li>
<li>results: 这个paper的系统可以生成新的高级指令来实现未经见过的目标，并且可以轻松地适应不同的任务，只需更改任务指南即可。此外，当用户发送新的请求时，系统可以重新规划根据新的请求、任务指南和之前执行的步骤。<details>
<summary>Abstract</summary>
An interactive robot framework accomplishes long-horizon task planning and can easily generalize to new goals or distinct tasks, even during execution. However, most traditional methods require predefined module design, which makes it hard to generalize to different goals. Recent large language model based approaches can allow for more open-ended planning but often require heavy prompt engineering or domain-specific pretrained models. To tackle this, we propose a simple framework that achieves interactive task planning with language models. Our system incorporates both high-level planning and low-level function execution via language. We verify the robustness of our system in generating novel high-level instructions for unseen objectives and its ease of adaptation to different tasks by merely substituting the task guidelines, without the need for additional complex prompt engineering. Furthermore, when the user sends a new request, our system is able to replan accordingly with precision based on the new request, task guidelines and previously executed steps. Please check more details on our https://wuphilipp.github.io/itp_site and https://youtu.be/TrKLuyv26_g.
</details>
<details>
<summary>摘要</summary>
一个交互式机器人框架实现了长期任务规划，可以轻松泛化到新目标或不同任务，甚至在执行过程中。然而，大多数传统方法需要预定的模块设计，这使得泛化到不同目标变得困难。现有大语言模型基于方法可以允许更开放的规划，但frequently需要重量的提前工程或域特定预训练模型。为解决这个问题，我们提出了一个简单的框架，可以通过语言模型实现交互式任务规划。我们的系统结合高级规划和低级功能执行 via 语言。我们验证了我们的系统在生成未看过目标的新高级指令方面的稳定性和可靠性，以及在不同任务时的扩展性和适应性。具体信息请参考我们的 <https://wuphilipp.github.io/itp_site> 和 <https://youtu.be/TrKLuyv26_g>。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries"><a href="#In-Context-Pretraining-Language-Modeling-Beyond-Document-Boundaries" class="headerlink" title="In-Context Pretraining: Language Modeling Beyond Document Boundaries"></a>In-Context Pretraining: Language Modeling Beyond Document Boundaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10638">http://arxiv.org/abs/2310.10638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Xi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, Mike Lewis</li>
<li>for: 这个论文的目的是提高大语言模型（LMs）的性能，使其能够更好地理解文档之间的关系和Contextual reasoning。</li>
<li>methods: 该论文提出了一种新的预训练方法 called In-Context Pretraining，该方法使用相关的文档序列来显式地鼓励LMs读取和理解文档边界。</li>
<li>results: 实验表明，In-Context Pretraining可以提高LMs的性能，特别是在需要更复杂的文档上下文理解任务中，例如在文档学习 (+8%), 阅读理解 (+15%), 对前Context的忠诚 (+16%), 长文档理解 (+5%), 和检索扩展 (+9%).<details>
<summary>Abstract</summary>
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do this, we introduce approximate algorithms for finding related documents with efficient nearest neighbor search and constructing coherent input contexts with a graph traversal algorithm. Our experiments show In-Context Pretraining offers a simple and scalable approach to significantly enhance LMs'performance: we see notable improvements in tasks that require more complex contextual reasoning, including in-context learning (+8%), reading comprehension (+15%), faithfulness to previous contexts (+16%), long-context reasoning (+5%), and retrieval augmentation (+9%).
</details>
<details>
<summary>摘要</summary>
大型语言模型（LM）目前在预测token的任务上被训练，允许它们直接进行长形生成和提示类型任务，这些任务可以被简化为文档完成。现有的预训管道将LM训练为 concatenating随机的短文档来建立输入 контекст，但是先前的文档提供了无法预测下一个文档的信号。我们则提出了内部预训（In-Context Pretraining），一种新的方法，其中语言模型在相关的文档序列中预训，并且明确地让模型在文档boundaries上读取和理解。我们可以实现内部预训 simply by changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines。但是，这个文档排序问题是具有挑战性的，有 billions of documents，我们希望排序可以最大化文档之间的相似性，而不是重复数据。为了解决这个问题，我们引入了近似算法，用于快速找到相关文档，并使用图 traversal algorithm construct coherent input contexts。我们的实验显示，内部预训可以提供一个简单且扩展的方法，以提高LM的性能：我们在需要更多的contextual reasoning任务中看到了很大的改善（+8%），包括在文档中学习（+15%）、对先前context的忠诚性（+16%）、长形reasoning（+5%）和文档扩展（+9%）。
</details></li>
</ul>
<hr>
<h2 id="Towards-Scenario-based-Safety-Validation-for-Autonomous-Trains-with-Deep-Generative-Models"><a href="#Towards-Scenario-based-Safety-Validation-for-Autonomous-Trains-with-Deep-Generative-Models" class="headerlink" title="Towards Scenario-based Safety Validation for Autonomous Trains with Deep Generative Models"></a>Towards Scenario-based Safety Validation for Autonomous Trains with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10635">http://arxiv.org/abs/2310.10635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Decker, Ananta R. Bhattarai, Michael Lebacher<br>for: 这篇论文是为了探讨如何适当地验证自动驾驶系统的可靠性。methods: 这篇论文使用了深度生成模型来生成数据，以验证自动驾驶系统在不同的照明和天气条件下是否能够正常运行。results: 研究人员通过使用深度生成模型，可以使限量的测试数据更加表示性，并且可以分析自动驾驶系统是否遵循了一般的操作设计域（ODD）要求。特别是在不同的照明和天气条件下，自动驾驶系统是否能够正常运行的问题上，研究人员可以通过深度生成模型来进行分析。<details>
<summary>Abstract</summary>
Modern AI techniques open up ever-increasing possibilities for autonomous vehicles, but how to appropriately verify the reliability of such systems remains unclear. A common approach is to conduct safety validation based on a predefined Operational Design Domain (ODD) describing specific conditions under which a system under test is required to operate properly. However, collecting sufficient realistic test cases to ensure comprehensive ODD coverage is challenging. In this paper, we report our practical experiences regarding the utility of data simulation with deep generative models for scenario-based ODD validation. We consider the specific use case of a camera-based rail-scene segmentation system designed to support autonomous train operation. We demonstrate the capabilities of semantically editing railway scenes with deep generative models to make a limited amount of test data more representative. We also show how our approach helps to analyze the degree to which a system complies with typical ODD requirements. Specifically, we focus on evaluating proper operation under different lighting and weather conditions as well as while transitioning between them.
</details>
<details>
<summary>摘要</summary>
现代人工智能技术为自动驾驶车辆开启了无限可能，但如何正确验证这些系统的可靠性仍然不清楚。一般来说，是通过预先定义的操作设计域（ODD）来确保系统在测试时运行正常。然而，收集足够的实际测试 случа件以确保完整的 ODD 覆盖却是一项具有挑战性的任务。在这篇论文中，我们介绍了使用深度生成模型进行数据simeulation，以验证场景基于 ODD 的安全验证。我们选择了基于摄像头的铁路景象分割系统，用于支持自动列车运行。我们示示了使用深度生成模型编辑铁路场景，以使用有限的测试数据更加 Representative。我们还展示了如何使用我们的方法来分析系统是否符合 Typical ODD 要求。特别是，我们对不同的照明和天气条件下的系统运行正常性进行评估，以及在这些条件之间的过渡中系统的运行情况。
</details></li>
</ul>
<hr>
<h2 id="OpenAgents-An-Open-Platform-for-Language-Agents-in-the-Wild"><a href="#OpenAgents-An-Open-Platform-for-Language-Agents-in-the-Wild" class="headerlink" title="OpenAgents: An Open Platform for Language Agents in the Wild"></a>OpenAgents: An Open Platform for Language Agents in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10634">http://arxiv.org/abs/2310.10634</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xlang-ai/openagents">https://github.com/xlang-ai/openagents</a></li>
<li>paper_authors: Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, Tao Yu</li>
<li>for: 本研究旨在提供一个开源平台，供日常生活中使用语言代理人，并且将语言代理人应用到实际生活中。</li>
<li>methods: 本研究使用了Python&#x2F;SQL和日常API工具来建立三个语言代理人：数据代理人、插件代理人和网页代理人。</li>
<li>results: 本研究实现了一个开源平台，可以让一般用户通过网页使用语言代理人功能，并且提供了一个简单的开发者和研究人员的部署体验，以便实现创新的语言代理人和实际世界中的评估。<details>
<summary>Abstract</summary>
Language agents show potential in being capable of utilizing natural language for varied and intricate tasks in diverse environments, particularly when built upon large language models (LLMs). Current language agent frameworks aim to facilitate the construction of proof-of-concept language agents while neglecting the non-expert user access to agents and paying little attention to application-level designs. We present OpenAgents, an open platform for using and hosting language agents in the wild of everyday life. OpenAgents includes three agents: (1) Data Agent for data analysis with Python/SQL and data tools; (2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web browsing. OpenAgents enables general users to interact with agent functionalities through a web user interface optimized for swift responses and common failures while offering developers and researchers a seamless deployment experience on local setups, providing a foundation for crafting innovative language agents and facilitating real-world evaluations. We elucidate the challenges and opportunities, aspiring to set a foundation for future research and development of real-world language agents.
</details>
<details>
<summary>摘要</summary>
语言代理显示出在使用自然语言完成多样化和复杂任务的潜在能力，特别是在基于大语言模型（LLM）的情况下。现有的语言代理框架主要是为了建立证明性的语言代理，忽略了非专家用户访问代理和应用程序层的设计。我们介绍OpenAgents，一个开放的平台，用于在日常生活中使用和主机语言代理。OpenAgents包括三个代理：（1）数据代理，用于数据分析，使用Python/SQL和数据工具；（2）插件代理，提供200多个日常API工具；（3）网络代理，用于自动化网络浏览。OpenAgents允许一般用户通过网页用户界面进行快速响应和常见失败的交互，同时提供了开发者和研究人员在本地设置上的畅通部署体验，为创造语言代理的未来研究和发展提供了基础。我们详细介绍了挑战和机遇，以便为未来的语言代理研究和发展提供指导。
</details></li>
</ul>
<hr>
<h2 id="BioPlanner-Automatic-Evaluation-of-LLMs-on-Protocol-Planning-in-Biology"><a href="#BioPlanner-Automatic-Evaluation-of-LLMs-on-Protocol-Planning-in-Biology" class="headerlink" title="BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology"></a>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10632">http://arxiv.org/abs/2310.10632</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bioplanner/bioplanner">https://github.com/bioplanner/bioplanner</a></li>
<li>paper_authors: Odhran O’Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, Samuel G Rodriques</li>
<li>for: 本研究旨在开发一种自动生成科学实验协议的能力，以便自动化科学研究。</li>
<li>methods: 本研究使用大型自然语言模型（LLM）来生成科学实验协议，并使用pseudocode表示法来评估模型的性能。</li>
<li>results: 研究发现LLM可以准确地生成科学实验协议，并且可以通过pseudocode表示法来评估模型的性能。此外，研究还发现使用pseudocode表示法可以准确地生成新的实验协议，并且可以在生物实验室中成功完成一个生成的实验协议。<details>
<summary>Abstract</summary>
The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model planning abilities in other areas of science or other areas that lack automatic evaluation.
</details>
<details>
<summary>摘要</summary>
科学实验协议自动生成能力会代表科学自动化的重要一步。大型语言模型（LLM）在各种任务上表现出优异，如问答和文本和代码生成。然而，LLM在多步问题和长期规划方面可能会遇到困难，这些是科学实验的关键。此外，科学实验协议的评估困难，因为实验可以用多种语言描述正确，需要专家知识进行评估，并且通常无法自动执行。我们介绍了一种自动评估框架，用于评估语言模型在计划科学实验协议的能力。我们还提供了生物协议集（BioProt），其包含生物实验协议和对应的伪代码表示。为了衡量语言模型在生成科学协议方面的性能，我们使用一个LLM将自然语言协议转换为伪代码，然后评估LLM是否可以从高级描述和授权伪代码函数中重建伪代码。我们使用GPT-3和GPT-4进行测试，并评估其可靠性。我们还验证了 pseudocode 表示的实验协议的可重复性，并在生物实验室中成功执行了生成的协议。我们的框架可以扩展到其他科学领域或缺乏自动评估的领域中的语言模型规划能力的评估和改进。
</details></li>
</ul>
<hr>
<h2 id="Llemma-An-Open-Language-Model-For-Mathematics"><a href="#Llemma-An-Open-Language-Model-For-Mathematics" class="headerlink" title="Llemma: An Open Language Model For Mathematics"></a>Llemma: An Open Language Model For Mathematics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10631">http://arxiv.org/abs/2310.10631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/EleutherAI/math-lm">https://github.com/EleutherAI/math-lm</a></li>
<li>paper_authors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, Sean Welleck</li>
<li>for: 这篇论文主要是为了描述一种大型语言模型，用于数学领域。</li>
<li>methods: 该论文使用了Code Llama进行预训练，并在Proof-Pile-2 dataset上继续预训练，这个dataset包括科学论文、网络数据和数学代码。</li>
<li>results: 根据MATH benchmark，Llemma模型在开放基础模型中表现出色，并且在相同参数基础上超过了所有已知的开放基础模型和未发布的Minerva模型集。此外，Llemma模型还可以不需要进一步微调来使用工具和正式证明 theorem。<details>
<summary>Abstract</summary>
We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.
</details>
<details>
<summary>摘要</summary>
我团队今天发布了一个大型语言模型，称为Llemma。我们继续预训 Code Llama 在 Proof-Pile-2 上进行预训， Proof-Pile-2 是一个混合科学论文、网络数据和数学代码的杂合，从而得到了 Llemma。在 MATH benchmark 上，Llemma 与所有已知的开放基模型以及未发布的 Minerva 模型集合相比，在参数量为相同的前提下表现出色。此外，Llemma 还可以无需进一步训练地使用工具和正式证明。我们公开发布了所有文件，包括 7 亿和 34 亿参数的模型、Proof-Pile-2 和代码，以便重现我们的实验。
</details></li>
</ul>
<hr>
<h2 id="Factored-Verification-Detecting-and-Reducing-Hallucination-in-Summaries-of-Academic-Papers"><a href="#Factored-Verification-Detecting-and-Reducing-Hallucination-in-Summaries-of-Academic-Papers" class="headerlink" title="Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers"></a>Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10627">http://arxiv.org/abs/2310.10627</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elicit/fave-dataset">https://github.com/elicit/fave-dataset</a></li>
<li>paper_authors: Charlie George, Andreas Stuhlmüller</li>
<li>for: 本研究旨在评估自动生成报告中的幻觉现象，以及使用 Factored Verification 方法检测幻觉。</li>
<li>methods: 本研究使用 Factored Verification 方法对 abstractive 报告进行自动检测幻觉，并在 HaluEval benchmark 上达到了新的 SotA 水平。</li>
<li>results: 研究发现，使用 Factored Critiques 方法自动修正幻觉后，ChatGPT 的幻觉数量降低至 0.49，GPT-4 的幻觉数量降低至 0.46，Claude 2 的幻觉数量降低至 0.95。幻觉的存在导致报告中出现了一些细微的差异。因此，在使用模型生成学报时应该慎重。<details>
<summary>Abstract</summary>
Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers? We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries. This method sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average ChatGPT (16k) summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.
</details>
<details>
<summary>摘要</summary>
投影症也捕捉前沿 LLMS---但是怎么减少它对报告简要的影响？我们评估 Factored Verification，一种简单的自动检测投影症的方法。这种方法在HaluEvalbenchmark中的报告简要任务中设置了新的SotArekord，达到76.2%的准确率。然后，我们使用这种方法来估计语言模型在多篇学术论文总结中的投影症频率，发现了0.62个投影症在ChatGPT（16k）总结中，0.84个投影症在GPT-4总结中，以及1.55个投影症在Claude 2总结中。我们让模型使用Factored Critiques进行自我修复，发现这会降低投影症的数量到0.49个投影症在ChatGPT中，0.46个投影症在GPT-4中，以及0.95个投影症在Claude 2中。我们发现投影症很 часто是柔和的，因此在使用模型Synthesize academic papers时应该保持谨慎。
</details></li>
</ul>
<hr>
<h2 id="Video-Language-Planning"><a href="#Video-Language-Planning" class="headerlink" title="Video Language Planning"></a>Video Language Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10625">http://arxiv.org/abs/2310.10625</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abusufyanvu/6S191_MIT_DeepLearning">https://github.com/abusufyanvu/6S191_MIT_DeepLearning</a></li>
<li>paper_authors: Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, Jonathan Tompson</li>
<li>for: 提高复杂长期任务的视觉规划能力，利用大型生成模型的最新进展。</li>
<li>methods: 训练视语模型和文本到视频模型，并使其服务为策略和价值函数，实现视觉语言规划（VLP）算法。</li>
<li>results: VLP可以根据计算资源的增加，提高完成长期任务的成功率，并可以在不同机器人领域中生成长期视频规划：从多对象重新排序到多摄像头双手巧妙操作。生成的视频规划可以通过目标受控策略转化为真正的机器人行为。实验表明，VLP与先前方法相比，在真实机器人上提高了长期任务成功率。<details>
<summary>Abstract</summary>
We are interested in enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. To this end, we present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP scales with increasing computation budget where more computation time results in improved video plans, and is able to synthesize long-horizon video plans across different robotics domains: from multi-object rearrangement, to multi-camera bi-arm dexterous manipulation. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video. Experiments show that VLP substantially improves long-horizon task success rates compared to prior methods on both simulated and real robots (across 3 hardware platforms).
</details>
<details>
<summary>摘要</summary>
我们有兴趣实现视觉观察计划 для复杂长期任务在生成影像和语言之间，利用最近的大型生成模型。为此，我们提出了视觉语言观察（VLP）算法，它包括树搜索程式，我们在视觉语言模型中训练（i）视觉语言模型作为政策和价值函数，以及（ii）文本视频模型作为动态模型。VLP将长期任务指令和当前影像观察作为输入，将产生详细多模式（影像和语言）的视觉计划，描述如何完成最终任务。VLP随计算预算增加，增加计算时间可以提高视觉计划的质量，并能够适用于不同的机器人领域：从多个物体重新排序到多条臂优雅操作。生成的视觉计划可以转换为真实机器人动作 via 目标受控制政策，每个中途几何视觉计划中的一个条件。实验显示，VLP可以与先前的方法相比，在模拟和真实机器人（三个硬件平台）上提高长期任务成功率。
</details></li>
</ul>
<hr>
<h2 id="Generating-Summaries-with-Controllable-Readability-Levels"><a href="#Generating-Summaries-with-Controllable-Readability-Levels" class="headerlink" title="Generating Summaries with Controllable Readability Levels"></a>Generating Summaries with Controllable Readability Levels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10623">http://arxiv.org/abs/2310.10623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo F. R. Ribeiro, Mohit Bansal, Markus Dreyer</li>
<li>for: 这个论文的目的是控制摘要的可读性水平，以便对不同读者群体进行知识传递。</li>
<li>methods: 这个论文使用了以下三种技术来控制摘要的可读性水平：1）指令式可读性控制，2）使用约束来减少请求和实际可读性差距，3）使用预测器来估算下一步摘要的可读性水平。</li>
<li>results: 这个论文通过对新闻摘要（CNN&#x2F;DM数据集）进行实验，证明了其控制可读性的三种生成技术具有显著的改善作用，从而为可控的可读性 summarization 提供了强有力的基础。<details>
<summary>Abstract</summary>
Readability refers to how easily a reader can understand a written text. Several factors affect the readability level, such as the complexity of the text, its subject matter, and the reader's background knowledge. Generating summaries based on different readability levels is critical for enabling knowledge consumption by diverse audiences. However, current text generation approaches lack refined control, resulting in texts that are not customized to readers' proficiency levels. In this work, we bridge this gap and study techniques to generate summaries at specified readability levels. Unlike previous methods that focus on a specific readability level (e.g., lay summarization), we generate summaries with fine-grained control over their readability. We develop three text generation techniques for controlling readability: (1) instruction-based readability control, (2) reinforcement learning to minimize the gap between requested and observed readability and (3) a decoding approach that uses lookahead to estimate the readability of upcoming decoding steps. We show that our generation methods significantly improve readability control on news summarization (CNN/DM dataset), as measured by various readability metrics and human judgement, establishing strong baselines for controllable readability in summarization.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。</SYS>>可读性指示文本的理解容易程度。不同因素会影响可读性水平，如文本复杂度、主题和读者背景知识。生成基于不同可读性水平的摘要是关键，以便各种读者阅读。然而，当前文本生成方法缺乏细化控制，导致文本不具有读者素途水平定制。在这种情况下，我们尝试填补这个空白，研究生成摘要时控制可读性的技术。与之前的方法不同，我们的生成方法可以在不同的可读性水平上生成摘要，并且可以在不同的读者背景知识下进行细化控制。我们开发了三种文本生成技术来控制可读性：1. 指令式可读性控制2. 使用奖励学习减少请求和实际可读性之间的差距3. 使用预测器来估计下一步的可读性。我们表明，我们的生成方法可以在新闻摘要（CNN/DM数据集）中提高可读性控制，根据不同的可读性指标和人类评价。这些结果设置了可控的可读性基elines。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Assistive-Robustness-Via-the-Natural-Adversarial-Frontier"><a href="#Quantifying-Assistive-Robustness-Via-the-Natural-Adversarial-Frontier" class="headerlink" title="Quantifying Assistive Robustness Via the Natural-Adversarial Frontier"></a>Quantifying Assistive Robustness Via the Natural-Adversarial Frontier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10610">http://arxiv.org/abs/2310.10610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jerry Zhi-Yang He, Zackory Erickson, Daniel S. Brown, Anca D. Dragan</li>
<li>For: The paper aims to build robust policies for robots that assist people, but the challenge is that people can behave unexpectedly and interact with the robot outside of its training distribution, leading to failures.* Methods: The paper proposes a method called RIGID, which constructs the entire natural-adversarial frontier by training adversarial human policies that trade off between minimizing robot reward and acting human-like.* Results: The paper uses RIGID to analyze the performance of standard collaborative Reinforcement Learning and existing methods meant to increase robustness, and compares the frontier identified by RIGID with failures identified in expert adversarial interaction and naturally-occurring failures during user interaction. The results show that RIGID can provide a meaningful measure of robustness predictive of deployment performance and uncover failure cases that are difficult to find manually.Here is the text in Simplified Chinese:* For: 论文目标是建立助人机器人策略，但是人们可能会在测试时表现出意外的行为，导致机器人失败。* Methods: 论文提出了一种方法 called RIGID，它通过培养对应的人类策略来构建整个自然-攻击前景，以确定机器人策略的稳定性。* Results: 论文使用 RIGID 分析了标准合作 reinforcement learning 和现有增强稳定性的方法的性能，并与专家对抗交互中的失败和用户交互中的自然失败进行比较。结果表明，RIGID 可以提供有用的稳定性预测，并揭示了一些难以手动发现的失败案例。<details>
<summary>Abstract</summary>
Our ultimate goal is to build robust policies for robots that assist people. What makes this hard is that people can behave unexpectedly at test time, potentially interacting with the robot outside its training distribution and leading to failures. Even just measuring robustness is a challenge. Adversarial perturbations are the default, but they can paint the wrong picture: they can correspond to human motions that are unlikely to occur during natural interactions with people. A robot policy might fail under small adversarial perturbations but work under large natural perturbations. We propose that capturing robustness in these interactive settings requires constructing and analyzing the entire natural-adversarial frontier: the Pareto-frontier of human policies that are the best trade-offs between naturalness and low robot performance. We introduce RIGID, a method for constructing this frontier by training adversarial human policies that trade off between minimizing robot reward and acting human-like (as measured by a discriminator). On an Assistive Gym task, we use RIGID to analyze the performance of standard collaborative Reinforcement Learning, as well as the performance of existing methods meant to increase robustness. We also compare the frontier RIGID identifies with the failures identified in expert adversarial interaction, and with naturally-occurring failures during user interaction. Overall, we find evidence that RIGID can provide a meaningful measure of robustness predictive of deployment performance, and uncover failure cases in human-robot interaction that are difficult to find manually. https://ood-human.github.io.
</details>
<details>
<summary>摘要</summary>
我们的最终目标是建立Robot assistant的坚强策略。但是，人类在测试时可能会出现意外的行为，导致机器人失败。甚至测试Robot的坚强性也是一个挑战。对于Robot来说，对人类的干预是最大的挑战。我们认为，在这些互动设定下，捕捉Robot的坚强性需要构建和分析人类政策的全面自然针对的前ier：人类政策的Pareto前ier，即在自然性和机器人性能之间寻找最佳的交易。我们提出了RIGID方法，通过在人类政策中培养对减少机器人奖励和人类化行为（由识别器来衡量）进行交易来构建这个前ier。在助手机器人任务上，我们使用RIGID方法分析标准协作学习的性能，以及已有的Robot坚强性增强方法的性能。我们还将这个前ier与专家对机器人互动的攻击、以及用户互动中的自然出现的失败进行比较。总的来说，我们发现RIGID可以提供有意义的坚强性预测，并揭示了人机互动中难以找到的失败案例。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Power-of-Graph-Neural-Networks-in-Solving-Linear-Optimization-Problems"><a href="#Exploring-the-Power-of-Graph-Neural-Networks-in-Solving-Linear-Optimization-Problems" class="headerlink" title="Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems"></a>Exploring the Power of Graph Neural Networks in Solving Linear Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10603">http://arxiv.org/abs/2310.10603</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chendiqian/IPM_MPNN">https://github.com/chendiqian/IPM_MPNN</a></li>
<li>paper_authors: Chendi Qian, Didier Chételat, Christopher Morris</li>
<li>for: 这篇论文主要是为了解释Message-Passing Graph Neural Networks（MPNNs）在增强精确优化算法方面的效果。</li>
<li>methods: 这篇论文使用了MPNNs模仿计算机机械的努力，如强分支，来解决混合整数优化问题。</li>
<li>results: 这篇论文证明了MPNNs可以模拟标准内部点方法来解决线性优化问题，并且可以根据给定问题实例的分布来适应。 Empirical results show that MPNNs can solve LP relaxations of standard combinatorial optimization problems with high accuracy and fast speed, often surpassing conventional solvers and competing approaches.<details>
<summary>Abstract</summary>
Recently, machine learning, particularly message-passing graph neural networks (MPNNs), has gained traction in enhancing exact optimization algorithms. For example, MPNNs speed up solving mixed-integer optimization problems by imitating computational intensive heuristics like strong branching, which entails solving multiple linear optimization problems (LPs). Despite the empirical success, the reasons behind MPNNs' effectiveness in emulating linear optimization remain largely unclear. Here, we show that MPNNs can simulate standard interior-point methods for LPs, explaining their practical success. Furthermore, we highlight how MPNNs can serve as a lightweight proxy for solving LPs, adapting to a given problem instance distribution. Empirically, we show that MPNNs solve LP relaxations of standard combinatorial optimization problems close to optimality, often surpassing conventional solvers and competing approaches in solving time.
</details>
<details>
<summary>摘要</summary>
最近，机器学习技术，特别是消息传递图神经网络（MPNN），在增强精确优化算法方面取得了进展。例如，MPNN可以加速解决杂合整数优化问题，通过模拟计算沉重的规则，如强分支法，解决多个线性优化问题（LP）。虽然实际成功，但MPNN在优化LP的原因 remained largely unclear。在这篇文章中，我们展示MPNN可以模拟标准内部点方法，解释其实际成功。此外，我们指出MPNN可以作为LP的轻量级代理，适应给定问题实例分布。在实验中，我们发现MPNN可以解决标准 combinatorial优化问题的LP relaxation，与优化时间相对较长的传统算法和竞争方法相比，往往达到更高的优化精度。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-neural-wavefields-with-Gabor-basis-functions"><a href="#Physics-informed-neural-wavefields-with-Gabor-basis-functions" class="headerlink" title="Physics-informed neural wavefields with Gabor basis functions"></a>Physics-informed neural wavefields with Gabor basis functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10602">http://arxiv.org/abs/2310.10602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Alkhalifah, Xinquan Huang<br>for: This paper aims to enhance the efficiency and accuracy of neural network wavefield solutions by modeling them as linear combinations of Gabor basis functions that satisfy the wave equation.methods: The proposed approach uses a fully connected neural network with an adaptable Gabor layer as the final hidden layer, employing a weighted summation of Gabor neurons to compute predictions. The weights&#x2F;coefficients of the Gabor functions are learned from previous hidden layers with nonlinear activation functions.results: Realistic assessments showcase the efficacy of this novel implementation compared to the vanilla PINN, particularly in scenarios involving high-frequencies and realistic models that are often challenging for PINNs.<details>
<summary>Abstract</summary>
Recently, Physics-Informed Neural Networks (PINNs) have gained significant attention for their versatile interpolation capabilities in solving partial differential equations (PDEs). Despite their potential, the training can be computationally demanding, especially for intricate functions like wavefields. This is primarily due to the neural-based (learned) basis functions, biased toward low frequencies, as they are dominated by polynomial calculations, which are not inherently wavefield-friendly. In response, we propose an approach to enhance the efficiency and accuracy of neural network wavefield solutions by modeling them as linear combinations of Gabor basis functions that satisfy the wave equation. Specifically, for the Helmholtz equation, we augment the fully connected neural network model with an adaptable Gabor layer constituting the final hidden layer, employing a weighted summation of these Gabor neurons to compute the predictions (output). These weights/coefficients of the Gabor functions are learned from the previous hidden layers that include nonlinear activation functions. To ensure the Gabor layer's utilization across the model space, we incorporate a smaller auxiliary network to forecast the center of each Gabor function based on input coordinates. Realistic assessments showcase the efficacy of this novel implementation compared to the vanilla PINN, particularly in scenarios involving high-frequencies and realistic models that are often challenging for PINNs.
</details>
<details>
<summary>摘要</summary>
近期，物理学 Informed Neural Networks (PINNs) 已经受到了广泛关注，因为它们可以通过解析部分偏微分方程 (PDEs) 来进行多元函数的 interpolating 能力。 despite their potential, the training of PINNs can be computationally demanding, especially for complex functions such as wavefields. This is primarily due to the fact that the neural network-based (learned) basis functions are biased towards low frequencies, as they are dominated by polynomial calculations, which are not inherently wavefield-friendly.为了解决这个问题，我们提出了一种方法，用于提高 PINN 的效率和准确性，通过将它们表示为线性组合的 Gabor 基函数，这些基函数满足波方程。 Specifically, for the Helmholtz equation, we augment the fully connected neural network model with an adaptable Gabor layer constituting the final hidden layer, employing a weighted summation of these Gabor neurons to compute the predictions (output). These weights/coefficients of the Gabor functions are learned from the previous hidden layers that include nonlinear activation functions. To ensure the Gabor layer's utilization across the model space, we incorporate a smaller auxiliary network to forecast the center of each Gabor function based on input coordinates. Realistic assessments showcase the efficacy of this novel implementation compared to the vanilla PINN, particularly in scenarios involving high-frequencies and realistic models that are often challenging for PINNs.
</details></li>
</ul>
<hr>
<h2 id="Automated-Natural-Language-Explanation-of-Deep-Visual-Neurons-with-Large-Models"><a href="#Automated-Natural-Language-Explanation-of-Deep-Visual-Neurons-with-Large-Models" class="headerlink" title="Automated Natural Language Explanation of Deep Visual Neurons with Large Models"></a>Automated Natural Language Explanation of Deep Visual Neurons with Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10708">http://arxiv.org/abs/2310.10708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxu Zhao, Wei Qian, Yucheng Shi, Mengdi Huai, Ninghao Liu</li>
<li>for: 本研究旨在解释深度神经网络中 neuron 的含义，提高神经网络的可解释性和可行性。</li>
<li>methods: 本研究提出了一种基于大型基础模型的后续框架，可自动生成 neuron 的含义，不需要人工干预或专业知识。</li>
<li>results: 实验表明，该方法可以准确地找出 neuron 的含义，并且可以与不同的模型和数据集相结合。<details>
<summary>Abstract</summary>
Deep neural networks have exhibited remarkable performance across a wide range of real-world tasks. However, comprehending the underlying reasons for their effectiveness remains a challenging problem. Interpreting deep neural networks through examining neurons offers distinct advantages when it comes to exploring the inner workings of neural networks. Previous research has indicated that specific neurons within deep vision networks possess semantic meaning and play pivotal roles in model performance. Nonetheless, the current methods for generating neuron semantics heavily rely on human intervention, which hampers their scalability and applicability. To address this limitation, this paper proposes a novel post-hoc framework for generating semantic explanations of neurons with large foundation models, without requiring human intervention or prior knowledge. Our framework is designed to be compatible with various model architectures and datasets, facilitating automated and scalable neuron interpretation. Experiments are conducted with both qualitative and quantitative analysis to verify the effectiveness of our proposed approach.
</details>
<details>
<summary>摘要</summary>
深度神经网络在各种实际任务中表现出色，但理解它们的内在原理仍然是一项复杂的问题。通过分析神经元来解释深度神经网络的工作机制具有明显的优势。前期研究表明，深度视觉网络中的特定神经元具有 semantics 意义，并在模型性能中扮演重要角色。然而，目前用于生成神经元 semantics 的方法仍然高度依赖于人工干预，这限制了其可推广性和应用性。为了解决这一问题，本文提出了一种新的后置框架，用于自动生成大型基础模型中神经元的Semantic解释，不需要人工干预或先验知识。我们的框架适用于多种模型架构和数据集，可以实现自动化和扩展的神经元解释。我们通过对质量和kvantitative分析进行实验来验证我们的提议的有效性。
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Imagenets-of-ML4EDA"><a href="#Towards-the-Imagenets-of-ML4EDA" class="headerlink" title="Towards the Imagenets of ML4EDA"></a>Towards the Imagenets of ML4EDA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10560">http://arxiv.org/abs/2310.10560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Animesh Basak Chowdhury, Shailja Thakur, Hammond Pearce, Ramesh Karri, Siddharth Garg</li>
<li>for: 这篇论文旨在探讨ML导向EDA工具从RTL到GDSII的应用，但是现在没有标准的数据集或评估任务定义于EDA问题领域。</li>
<li>methods: 作者在这篇论文中描述了他们在Verilog代码生成和逻辑合成方面收集和维护了两个大规模、高质量的数据集的经验。</li>
<li>results: 作者在这篇论文中讨论了数据集维护和扩展的挑战，以及数据集质量和安全性问题，并使用专门为硬件领域开发的数据生成工具。<details>
<summary>Abstract</summary>
Despite the growing interest in ML-guided EDA tools from RTL to GDSII, there are no standard datasets or prototypical learning tasks defined for the EDA problem domain. Experience from the computer vision community suggests that such datasets are crucial to spur further progress in ML for EDA. Here we describe our experience curating two large-scale, high-quality datasets for Verilog code generation and logic synthesis. The first, VeriGen, is a dataset of Verilog code collected from GitHub and Verilog textbooks. The second, OpenABC-D, is a large-scale, labeled dataset designed to aid ML for logic synthesis tasks. The dataset consists of 870,000 And-Inverter-Graphs (AIGs) produced from 1500 synthesis runs on a large number of open-source hardware projects. In this paper we will discuss challenges in curating, maintaining and growing the size and scale of these datasets. We will also touch upon questions of dataset quality and security, and the use of novel data augmentation tools that are tailored for the hardware domain.
</details>
<details>
<summary>摘要</summary>
尽管RLT到GDSII之间的ML指导EDA工具的兴趣在增长，但是没有定义了EDA问题领域的标准数据集或范例学习任务。从计算机视觉社区的经验来看，这些数据集是ML进一步发展EDA的关键。我们在这篇文章中描述了我们在Verilog代码生成和逻辑合成方面收集的两个大规模、高质量数据集的经验。第一个数据集是从GitHub和Verilog书籍中收集的Verilog代码集合，称为VeriGen。第二个数据集是一个大规模、标注的数据集，用于ML逻辑合成任务，包括1500次合成运行从开源硬件项目中生成的870,000个和逻辑图（AIGs）。在这篇文章中，我们会讨论数据集维护和扩展的挑战，以及数据集质量和安全性问题，以及适用于硬件领域的特有数据扩展工具。
</details></li>
</ul>
<hr>
<h2 id="Demonstrations-Are-All-You-Need-Advancing-Offensive-Content-Paraphrasing-using-In-Context-Learning"><a href="#Demonstrations-Are-All-You-Need-Advancing-Offensive-Content-Paraphrasing-using-In-Context-Learning" class="headerlink" title="Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning"></a>Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10707">http://arxiv.org/abs/2310.10707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Som, Karan Sikka, Helen Gent, Ajay Divakaran, Andreas Kathol, Dimitra Vergyri</li>
<li>For: 本研究旨在帮助实践者开发可用的妥协译者，通过尝试大语言模型（LLM）中的内在学习（ICL），使用有限的输入标签示例对象引导模型生成特定查询的愿望输出。* Methods: 本研究检查了一些关键因素，如示例数量和顺序，排除提示指导语言，以及减少衡量毒性。我们在三个数据集上进行了原则性的评估，其中包括我们所提出的上下文感知礼貌译 dataset，包含对话式粗鲁言语、礼貌译和附加的对话上下文。* Results: 我们的结果表明，ICL与监督方法在生成质量方面相当，而且在人工评估中提高了25%，并在衡量毒性方面下降了76%。此外，ICL基于的译者只有10%的训练数据 exhibit slight reduction in performance.<details>
<summary>Abstract</summary>
Paraphrasing of offensive content is a better alternative to content removal and helps improve civility in a communication environment. Supervised paraphrasers; however, rely heavily on large quantities of labelled data to help preserve meaning and intent. They also retain a large portion of the offensiveness of the original content, which raises questions on their overall usability. In this paper we aim to assist practitioners in developing usable paraphrasers by exploring In-Context Learning (ICL) with large language models (LLMs), i.e., using a limited number of input-label demonstration pairs to guide the model in generating desired outputs for specific queries. Our study focuses on key factors such as -- number and order of demonstrations, exclusion of prompt instruction, and reduction in measured toxicity. We perform principled evaluation on three datasets, including our proposed Context-Aware Polite Paraphrase dataset, comprising of dialogue-style rude utterances, polite paraphrases, and additional dialogue context. We evaluate our approach using two closed source and one open source LLM. Our results reveal that ICL is comparable to supervised methods in generation quality, while being qualitatively better by 25% on human evaluation and attaining lower toxicity by 76%. Also, ICL-based paraphrasers only show a slight reduction in performance even with just 10% training data.
</details>
<details>
<summary>摘要</summary>
干脆的内容重写是一种更好的代替方案，而不是完全删除内容。这有助于提高通信环境中的文明性。然而，监督重写者依然需要大量标注数据来保持意思和意图。此外，它们还会保留大量的害词，这引发了使用这些重写器的问题。在这篇论文中，我们想帮助实践者开发可用的重写器，通过exploring In-Context Learning（ICL）和大语言模型（LLM）来实现。我们的研究将关注一些关键因素，如数量和顺序的示例，排除提示 instrucion，以及减少测量的害词。我们在三个数据集上进行了原则性的评估，包括我们提出的Context-Aware Polite Paraphrase数据集，这包括对话式的粗鲁言语、文明重写和额外对话背景。我们使用两个关闭源的LLM和一个开源的LLM进行评估。我们的结果表明，ICL与监督方法在生成质量上相当，而且在人工评估中比其提高了25%，并且测量到的害词下降了76%。此外，ICL基本上只有10%的训练数据下降的性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-applied-to-EEG-data-with-different-montages-using-spatial-attention"><a href="#Deep-learning-applied-to-EEG-data-with-different-montages-using-spatial-attention" class="headerlink" title="Deep learning applied to EEG data with different montages using spatial attention"></a>Deep learning applied to EEG data with different montages using spatial attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10550">http://arxiv.org/abs/2310.10550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sccn/deep-channel-harmonization">https://github.com/sccn/deep-channel-harmonization</a></li>
<li>paper_authors: Dung Truong, Muhammad Abdullah Khalid, Arnaud Delorme</li>
<li>for: 本研究旨在使用深度学习处理和提取复杂脑动态信息的EEG raw数据中的信息。</li>
<li>methods: 本研究使用了 espacial attention 对 EEG 电极坐标进行通道协调，以使得可以使用不同的通道组合训练深度学习模型。</li>
<li>results: 研究表明，使用 espacial attention 可以提高模型性能，而且一个使用不同通道组合训练的深度学习模型可以在性别分类任务中表现出色，比Fixed 23-和 128-通道数据 montage 的模型要好。<details>
<summary>Abstract</summary>
The ability of Deep Learning to process and extract relevant information in complex brain dynamics from raw EEG data has been demonstrated in various recent works. Deep learning models, however, have also been shown to perform best on large corpora of data. When processing EEG, a natural approach is to combine EEG datasets from different experiments to train large deep-learning models. However, most EEG experiments use custom channel montages, requiring the data to be transformed into a common space. Previous methods have used the raw EEG signal to extract features of interest and focused on using a common feature space across EEG datasets. While this is a sensible approach, it underexploits the potential richness of EEG raw data. Here, we explore using spatial attention applied to EEG electrode coordinates to perform channel harmonization of raw EEG data, allowing us to train deep learning on EEG data using different montages. We test this model on a gender classification task. We first show that spatial attention increases model performance. Then, we show that a deep learning model trained on data using different channel montages performs significantly better than deep learning models trained on fixed 23- and 128-channel data montages.
</details>
<details>
<summary>摘要</summary>
深度学习可以从Raw EEG数据中提取和处理复杂脑动态信息的能力已经在各种最近的研究中得到证明。然而，深度学习模型也被证明可以在大量数据上表现最佳。在处理 EEG 数据时，自然的方法是将 EEG 数据集合在一起训练大型深度学习模型。然而，大多数 EEG 实验使用自定义通道 montage，需要数据进行变换以达到共同空间。先前的方法使用了 Raw EEG 信号提取关键特征，并将着眼于在 EEG 数据集中共同的特征空间。虽然这是一种合理的方法，但是它忽略了 EEG 原始数据的潜在强大性。在这里，我们探索使用 EEG 电极坐标的空间注意力进行通道协调的 Raw EEG 数据，以便在不同的 montage 上训练深度学习模型。我们在性别分类任务上测试了这种模型，首先显示了空间注意力可以提高模型性能。然后，我们显示了使用不同的 montage 训练深度学习模型可以在性别分类任务中获得显著更好的性能，与固定的 23-和 128-通道数据 montage 相比。
</details></li>
</ul>
<hr>
<h2 id="Use-of-probabilistic-phrases-in-a-coordination-game-human-versus-GPT-4"><a href="#Use-of-probabilistic-phrases-in-a-coordination-game-human-versus-GPT-4" class="headerlink" title="Use of probabilistic phrases in a coordination game: human versus GPT-4"></a>Use of probabilistic phrases in a coordination game: human versus GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10544">http://arxiv.org/abs/2310.10544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laurence T Maloney, Maria F Dal Martello, Vivian Fei, Valerie Ma</li>
<li>for: 这个论文的目的是测试人类和大语言模型GPT4在 probabilistic phrases 上的能力。</li>
<li>methods: 这个论文使用了人类和GPT4在两个不同的上下文中 estimates  probabilistic phrases 的能力。</li>
<li>results: 研究发现人类和GPT4在 probabilistic phrases 上的 estimations 在大多数情况下相互吻合，但人类和GPT4在 ambiguity 上的 estimations 不够一致。 GPT4 重复测试结果表明，它的 estimations 不够稳定。<details>
<summary>Abstract</summary>
English speakers use probabilistic phrases such as likely to communicate information about the probability or likelihood of events. Communication is successful to the extent that the listener grasps what the speaker means to convey and, if communication is successful, two individuals can potentially coordinate their actions based on shared knowledge about uncertainty. We first assessed human ability to estimate the probability and the ambiguity (imprecision) of 23 probabilistic phrases in two different contexts, investment advice and medical advice. We then had GPT4 (OpenAI), a recent Large Language Model, complete the same tasks as the human participants. We found that the median human participant and GPT4 assigned probability estimates that were in good agreement (proportions of variance accounted were close to .90). GPT4's estimates of probability both in the investment and Medical contexts were as close or closer to that of the human participants as the human participants were to one another. Estimates of probability for both the human participants and GPT4 were little affected by context. In contrast, human and GPT4 estimates of ambiguity were not in as good agreement. We repeated some of the GPT4 estimates to assess their stability: does GPT4, if run twice, produce the same or similar estimates? There is some indication that it does not.
</details>
<details>
<summary>摘要</summary>
英语speaker们使用可能性短语来传达事件的可能性或可信度。如果通信成功，两个人可以基于共享不确定性知识协调行动。我们首先评估了人类对23个可能性短语的可能性和不确定性（精度）的能力。然后，我们使用GPT4（OpenAI），一个最近的大语言模型，完成了同样的任务。我们发现 median人参与者和GPT4的可能性估计相差不大（相对变异度占比接近0.90）。GPT4在投资和医疗上的可能性估计与人参与者的估计相似或更相似。人参与者和GPT4对可能性的估计几乎不受 context 的影响。然而，人参与者和GPT4对不确定性的估计不太一致。我们重复了一些GPT4的估计以评估其稳定性：GPT4在两次运行后会产生相同或类似的估计吗？有些证据表明它不一定会。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Dataset-Distillation-through-Alignment-with-Smooth-and-High-Quality-Expert-Trajectories"><a href="#Efficient-Dataset-Distillation-through-Alignment-with-Smooth-and-High-Quality-Expert-Trajectories" class="headerlink" title="Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories"></a>Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10541">http://arxiv.org/abs/2310.10541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiyuan Shen, Wenzhuo Yang, Kwok-Yan Lam</li>
<li>for: 本研究旨在提出一种数据效果的方法，以便在训练大型和 cutting-edge 机器学习模型时，避免使用大量数据。</li>
<li>methods: 该方法基于 expert trajectory 的使用，并引入 clipping loss 和 gradient penalty 来规则参数变化的速率。此外，还提出了代表性初始化、平衡内循环损失和中间匹配损失等优化策略。</li>
<li>results: 实验结果显示，提出的方法在不同的数据集、大小和分辨率上均显著超越先前的方法。<details>
<summary>Abstract</summary>
Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regulate the rate of parameter changes in expert trajectories. Furthermore, in response to the sensitivity exhibited towards randomly initialized variables during distillation, we propose representative initialization for synthetic dataset and balanced inner-loop loss. Finally, we present two enhancement strategies, namely intermediate matching loss and weight perturbation, to mitigate the potential occurrence of cumulative errors. We conduct extensive experiments on datasets of different scales, sizes, and resolutions. The results demonstrate that the proposed method significantly outperforms prior methods.
</details>
<details>
<summary>摘要</summary>
通常，训练大型和当前最佳的机器学习模型需要使用大规模数据集，这会使训练和参数调整过程成为昂贵的时间和资源浪费。一些研究人员尝试将实际世界数据集中的信息简化为小型和紧凑的 sintetic 数据集，同时保持模型训练的能力，这被称为数据减量（DD）。尽管现有的方法已经取得了一定的进步，但现有的方法仍然无法有效替代大规模数据集。在这篇论文中，我们不同于以前的方法，我们认为专家畅通性在使用更强大的专家轨迹时对 DATASET DISTILLATION 的影响是非常重要的。基于这个想法，我们引入了折射损失和梯度罚 penalty 来控制专家轨迹中参数的变化速率。此外，我们还提出了代表性初始化和平衡内循环损失来适应在混合损失中随机初始化的变量的敏感性。最后，我们提出了两种改进策略，即中间匹配损失和重量扰动，以避免可能出现的累累错误。我们在不同的数据集、大小和分辨率上进行了广泛的实验，结果显示，我们的方法在PRIOR METHODS 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Microscaling-Data-Formats-for-Deep-Learning"><a href="#Microscaling-Data-Formats-for-Deep-Learning" class="headerlink" title="Microscaling Data Formats for Deep Learning"></a>Microscaling Data Formats for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10537">http://arxiv.org/abs/2310.10537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/microxcaling">https://github.com/microsoft/microxcaling</a></li>
<li>paper_authors: Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, Stosic Dusan, Venmugil Elango, Maximilian Golub, Alexander Heinecke, Phil James-Roxby, Dharmesh Jani, Gaurav Kolhe, Martin Langhammer, Ada Li, Levi Melnick, Maral Mesmakhosroshahi, Andres Rodriguez, Michael Schulte, Rasoul Shafipour, Lei Shao, Michael Siu, Pradeep Dubey, Paulius Micikevicius, Maxim Naumov, Colin Verrilli, Ralph Wittig, Doug Burger, Eric Chung</li>
<li>for: 降低现代深度学习应用的计算和存储成本</li>
<li>methods: 使用块缩放因子和窄Float和整数类型来组合微规模数据格式</li>
<li>results: 实证结果表明MX数据格式可以作为FP32的Drop-in更新，并且在AI推理和训练中具有低用户阻力，以及可以在训练生成语言模型中使用sub-8位权重、活化和梯度，并且减少了精度损失。<details>
<summary>Abstract</summary>
Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
</details>
<details>
<summary>摘要</summary>
宽度狭小的数据格式是现代深度学习应用中减少计算和存储成本的关键。这篇论文评估了 Microscaling（MX）数据格式，它将每个块缩放因子与窄浮点和整数类型相结合。MX格式均衡硬件效率、模型准确性和用户抵抗。实验结果表明MX格式可以作为FP32基eline的Drop-in取代物，用于AI推理和训练，并且具有低用户抵抗。我们还示出了在低于8位权重、活动和梯度上训练生成语言模型，无需修改训练脚本，且减少了准确性损失。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Parsing-by-Large-Language-Models-for-Intricate-Updating-Strategies-of-Zero-Shot-Dialogue-State-Tracking"><a href="#Semantic-Parsing-by-Large-Language-Models-for-Intricate-Updating-Strategies-of-Zero-Shot-Dialogue-State-Tracking" class="headerlink" title="Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking"></a>Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10520">http://arxiv.org/abs/2310.10520</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ToLightUpTheSky/ParsingDST">https://github.com/ToLightUpTheSky/ParsingDST</a></li>
<li>paper_authors: Yuxiang Wu, Guanting Dong, Weiran Xu</li>
<li>for:  Zero-shot Dialogue State Tracking (DST) aims to address the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly.</li>
<li>methods:  The proposed ParsingDST method leverages powerful Large Language Models (LLMs) and semantic parsing to reformulate the DST task and improve updating strategies in the text-to-JSON process.</li>
<li>results:  Experimental results show that ParsingDST outperforms existing zero-shot DST methods on MultiWOZ, with significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.<details>
<summary>Abstract</summary>
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT Zero-shot Dialogue State Tracking (DST)  Addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="NeMo-Guardrails-A-Toolkit-for-Controllable-and-Safe-LLM-Applications-with-Programmable-Rails"><a href="#NeMo-Guardrails-A-Toolkit-for-Controllable-and-Safe-LLM-Applications-with-Programmable-Rails" class="headerlink" title="NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails"></a>NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10501">http://arxiv.org/abs/2310.10501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, Jonathan Cohen</li>
<li>for: 这个论文主要是为了提供一种开源的工具kit，用于轻松地在基于语言模型（LLM）的对话系统中添加可编程的 guardrails。</li>
<li>methods: 论文使用了一些机制，如模型对齐，来让LLM提供者和开发者在训练时添加到 guardrails。此外，论文还使用了一种运行时灵感自对话管理的方法，允许开发者在运行时添加可编程的 guardrails。</li>
<li>results: 论文的初步结果表明，提出的方法可以与多个LLM提供者合作，开发出可控和安全的LLM应用程序，使用可编程的 guardrails。<details>
<summary>Abstract</summary>
NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.
</details>
<details>
<summary>摘要</summary>
尼莫·卫铁（NeMo Guardrails）是一个开源的工具套件，用于轻松地在基于LLM的对话系统中添加可编程的保护栏（guardrails）。保护栏（rails）是控制LLM输出的一种特定方式，例如不讨论有害话题，遵循预定的对话路径，使用特定的语言风格，等等。它们可以让LLM提供者和开发者在训练时间添加到特定模型中的保护栏，例如使用模型对齐。然而，使用运行时启发自对话管理的NeMo Guardrails，开发者可以添加可编程的rails到LLM应用程序中 - 这些rails是独立于下面LLM的、用户定义的、可解释的。我们的初步结果表明，提议的方法可以与多个LLM提供者合作开发可控和安全的LLM应用程序使用可编程rails。
</details></li>
</ul>
<hr>
<h2 id="LocSelect-Target-Speaker-Localization-with-an-Auditory-Selective-Hearing-Mechanism"><a href="#LocSelect-Target-Speaker-Localization-with-an-Auditory-Selective-Hearing-Mechanism" class="headerlink" title="LocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism"></a>LocSelect: Target Speaker Localization with an Auditory Selective Hearing Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10497">http://arxiv.org/abs/2310.10497</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Chen, Xinyuan Qian, Zexu Pan, Kainan Chen, Haizhou Li</li>
<li>for: 本研究旨在提出一种Selective hearing mechanism的目标说话者定位算法，以提高多说话者场景下的干扰难以听清楚的问题。</li>
<li>methods: 给出一个参考说话者的 Referral speech，首先生成一个基于说话者的 Spectrogram mask，以排除干扰说话者的speech。然后，使用Long short-term memory（LSTM）网络提取目标说话者的位置信息从过滤后的 Spectrogram中。</li>
<li>results: 实验表明，我们提出的方法在不同的 Signal-to-noise ratio（SNR）条件下，与现有算法相比，具有较高的准确率和鲁棒性。Specifically, at SNR &#x3D; -10 dB, our proposed network LocSelect achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.<details>
<summary>Abstract</summary>
The prevailing noise-resistant and reverberation-resistant localization algorithms primarily emphasize separating and providing directional output for each speaker in multi-speaker scenarios, without association with the identity of speakers. In this paper, we present a target speaker localization algorithm with a selective hearing mechanism. Given a reference speech of the target speaker, we first produce a speaker-dependent spectrogram mask to eliminate interfering speakers' speech. Subsequently, a Long short-term memory (LSTM) network is employed to extract the target speaker's location from the filtered spectrogram. Experiments validate the superiority of our proposed method over the existing algorithms for different scale invariant signal-to-noise ratios (SNR) conditions. Specifically, at SNR = -10 dB, our proposed network LocSelect achieves a mean absolute error (MAE) of 3.55 and an accuracy (ACC) of 87.40%.
</details>
<details>
<summary>摘要</summary>
prevailing 听风抵抗和响应抵抗的本地化算法主要强调在多个说话人场景中分离并提供每个说话人的方向性输出，不与说话人标识相关。在这篇论文中，我们提出了一种目标说话人本地化算法，具有选择性听风机制。给定一个参照说话人的参照speech，我们首先生成一个基于说话人的spectrogram掩码，以消除干扰说话人的speech。接着，我们使用Long short-term memory（LSTM）网络提取目标说话人的位置信息从过滤后的spectrogram中。实验证明了我们的提出方法与现有算法在不同的标准信号噪声比（SNR）条件下表现更好。具体来说，在SNR=-10dB的条件下，我们的提出的网络LocSelect的平均绝对误差（MAE）为3.55，准确率（ACC）为87.40%。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Power-of-LLMs-Evaluating-Human-AI-Text-Co-Creation-through-the-Lens-of-News-Headline-Generation"><a href="#Harnessing-the-Power-of-LLMs-Evaluating-Human-AI-Text-Co-Creation-through-the-Lens-of-News-Headline-Generation" class="headerlink" title="Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation"></a>Harnessing the Power of LLMs: Evaluating Human-AI Text Co-Creation through the Lens of News Headline Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10706">http://arxiv.org/abs/2310.10706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jsndg/emnlp23-llm-headline">https://github.com/jsndg/emnlp23-llm-headline</a></li>
<li>paper_authors: Zijian Ding, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, Alejandro Jaimes</li>
<li>for: 这项研究旨在探讨人们如何最佳地利用LLMs进行写作，以及与这些模型交互对于写作过程中的拥有感和信任的影响。</li>
<li>methods: 研究采用了常见的人机交互方式（如导航系统、从系统输出中选择、后期编辑），在LLM协助新闻标题生成上进行了比较。</li>
<li>results: 研究发现，人类控制可以减少LLM输出的不良结果，而且与自由编辑相比，AI协助不会影响参与者对写作过程的感知控制。<details>
<summary>Abstract</summary>
To explore how humans can best leverage LLMs for writing and how interacting with these models affects feelings of ownership and trust in the writing process, we compared common human-AI interaction types (e.g., guiding system, selecting from system outputs, post-editing outputs) in the context of LLM-assisted news headline generation. While LLMs alone can generate satisfactory news headlines, on average, human control is needed to fix undesirable model outputs. Of the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Further, AI assistance did not harm participants' perception of control compared to freeform editing.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)为了了解人类如何最好地利用LLM，并如何与这些模型交互影响写作过程中的所有权和信任感，我们在LLM协助新闻标题生成上比较了不同的人机合作方式（如导引系统、从系统输出选择、后期编辑）。尽管LLM独立可以生成可靠的新闻标题，但平均需要人类控制来修复模型输出的问题。 among the interaction methods, guiding and selecting model output added the most benefit with the lowest cost (in time and effort). Additionally, AI assistance did not harm participants' perception of control compared to freeform editing.
</details></li>
</ul>
<hr>
<h2 id="Type-aware-Decoding-via-Explicitly-Aggregating-Event-Information-for-Document-level-Event-Extraction"><a href="#Type-aware-Decoding-via-Explicitly-Aggregating-Event-Information-for-Document-level-Event-Extraction" class="headerlink" title="Type-aware Decoding via Explicitly Aggregating Event Information for Document-level Event Extraction"></a>Type-aware Decoding via Explicitly Aggregating Event Information for Document-level Event Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10487">http://arxiv.org/abs/2310.10487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gang Zhao, Yidong Shi, Shudong Lu, Xinjie Yang, Guanting Dong, Jian Xu, Xiaocheng Gong, Si Li</li>
<li>for: 本研究旨在解决文档水平事件EXTRACTION（DEE）中的两个主要挑战：事件散布和多事件。 précédentes méthodes have attempted to address these challenges, but they have overlooked the interference of event-unrelated sentences during event detection and neglected the mutual interference of different event roles during argument extraction.</li>
<li>methods: 本研究提出了一种新的Schema-based Explicitly Aggregating（SEA）模型，该模型可以有效地聚合事件信息，并将事件类型和角色信息分别编码为特定的类型和角色表示。通过基于类型的表示来检测每个事件，SEA可以减轻由事件相关信息引起的干扰。此外，SEA可以根据每个角色的表示来提取对应的Arguments，从而减少不同角色之间的互相干扰。</li>
<li>results: 实验结果表明，SEA模型在ChFinAnn和DuEE-fin数据集上的表现优于STATE-OF-THE-ART（SOTA）方法。<details>
<summary>Abstract</summary>
Document-level event extraction (DEE) faces two main challenges: arguments-scattering and multi-event. Although previous methods attempt to address these challenges, they overlook the interference of event-unrelated sentences during event detection and neglect the mutual interference of different event roles during argument extraction. Therefore, this paper proposes a novel Schema-based Explicitly Aggregating~(SEA) model to address these limitations. SEA aggregates event information into event type and role representations, enabling the decoding of event records based on specific type-aware representations. By detecting each event based on its event type representation, SEA mitigates the interference caused by event-unrelated information. Furthermore, SEA extracts arguments for each role based on its role-aware representations, reducing mutual interference between different roles. Experimental results on the ChFinAnn and DuEE-fin datasets show that SEA outperforms the SOTA methods.
</details>
<details>
<summary>摘要</summary>
文档级事件提取（DEE）面临两大挑战：事件散布和多事件。尽管先前的方法尝试解决这些挑战，但它们忽略了事件检测过程中的事件无关句子干扰和对不同角色的事件提取过程中的互相干扰。因此，这篇论文提出了一种新的Schema-based Explicitly Aggregating（SEA）模型，用于解决这些限制。SEA将事件信息聚合到事件类型和角色表示中，使得根据具体的类型意识来解码事件记录。通过根据事件类型表示来检测每个事件，SEA可以减轻由事件无关信息引起的干扰。此外，SEA根据角色意识来提取每个角色的证据，减少不同角色之间的互相干扰。实验结果表明，SEA在ChFinAnn和DuEE-fin数据集上的性能比SOTA方法更高。
</details></li>
</ul>
<hr>
<h2 id="ManyQuadrupeds-Learning-a-Single-Locomotion-Policy-for-Diverse-Quadruped-Robots"><a href="#ManyQuadrupeds-Learning-a-Single-Locomotion-Policy-for-Diverse-Quadruped-Robots" class="headerlink" title="ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots"></a>ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse Quadruped Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10486">http://arxiv.org/abs/2310.10486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milad Shafiee, Guillaume Bellegarda, Auke Ijspeert</li>
<li>for: 这种研究旨在开发一种可以控制多种四足机器人的奔跑策略，而无需重新调整参数和奖励函数。</li>
<li>methods: 研究人员 drew inspiration from animal motor control，并使用了一种模块化的CPG和PF层，以实现不同机器人之间的奔跑策略的共享。</li>
<li>results: 研究人员在不同机器人上测试了这种策略，并观察到了强健的实际到虚拟转移性，甚至在加载15公斤（相当于A1机器人的125% Nominal Mass）时仍然保持了稳定的性能。<details>
<summary>Abstract</summary>
Learning a locomotion policy for quadruped robots has traditionally been constrained to specific robot morphology, mass, and size. The learning process must usually be repeated for every new robot, where hyperparameters and reward function weights must be re-tuned to maximize performance for each new system. Alternatively, attempting to train a single policy to accommodate different robot sizes, while maintaining the same degrees of freedom (DoF) and morphology, requires either complex learning frameworks, or mass, inertia, and dimension randomization, which leads to prolonged training periods. In our study, we show that drawing inspiration from animal motor control allows us to effectively train a single locomotion policy capable of controlling a diverse range of quadruped robots. These differences encompass a variable number of DoFs, (i.e. 12 or 16 joints), three distinct morphologies, a broad mass range spanning from 2 kg to 200 kg, and nominal standing heights ranging from 16 cm to 100 cm. Our policy modulates a representation of the Central Pattern Generator (CPG) in the spinal cord, effectively coordinating both frequencies and amplitudes of the CPG to produce rhythmic output (Rhythm Generation), which is then mapped to a Pattern Formation (PF) layer. Across different robots, the only varying component is the PF layer, which adjusts the scaling parameters for the stride height and length. Subsequently, we evaluate the sim-to-real transfer by testing the single policy on both the Unitree Go1 and A1 robots. Remarkably, we observe robust performance, even when adding a 15 kg load, equivalent to 125% of the A1 robot's nominal mass.
</details>
<details>
<summary>摘要</summary>
学习四肢动物机器人的运动策略传统上受到机器人形态、质量和大小的限制。学习过程通常需要对每个新机器人重新调整超参数和奖励函数权重，以最大化表现。 Alternatively, 尝试使用同一个策略控制不同机器人的不同大小、同样的度度自由（DoF）和形态，需要使用复杂的学习框架，或者质量、抗力和维度随机化，这会导致训练期间过长。在我们的研究中，我们draw inspiration from animal motor control，我们可以有效地训练一个单一的运动策略，可以控制多种不同的四肢动物机器人。这些差异包括变化的DoF数（即12或16关节）、三种不同的形态、机器人质量范围从2公斤到200公斤，和nominal standing heights从16厘米到100厘米。我们的策略调节了中枢pattern generator（CPG）的表达，有效地协调CPG的频率和振荡 amplitudes，并将其映射到Pattern Formation（PF）层。不同的机器人中，唯一变化的是PF层，其调整了步高和步长的缩放参数。我们在Unitree Go1和A1机器人上进行了实验，并观察到了稳定的表现，甚至在加载15公斤的情况下，即A1机器人的125% Nominal mass。
</details></li>
</ul>
<hr>
<h2 id="DemoSG-Demonstration-enhanced-Schema-guided-Generation-for-Low-resource-Event-Extraction"><a href="#DemoSG-Demonstration-enhanced-Schema-guided-Generation-for-Low-resource-Event-Extraction" class="headerlink" title="DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction"></a>DemoSG: Demonstration-enhanced Schema-guided Generation for Low-resource Event Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10481">http://arxiv.org/abs/2310.10481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gang Zhao, Xiaocheng Gong, Xinjie Yang, Guanting Dong, Shudong Lu, Si Li</li>
<li>for: 提高低资源场景中的事件EXTRACTION（EE）效果</li>
<li>methods: 示范学习 paradigm和schema-based prompts</li>
<li>results: 在域 adapted low-resource setting中，对三个数据集进行了广泛的实验，并研究了 DemoSG 的稳定性。结果表明， DemoSG 在低资源场景中明显超过当前方法。Here’s a breakdown of each point:</li>
<li>for: The paper is written to improve the effectiveness of Event Extraction (EE) in low-resource scenarios.</li>
<li>methods: The paper proposes two methods to improve EE in low-resource scenarios: (1) demonstration-based learning paradigm, and (2) schema-based prompts.</li>
<li>results: The paper presents extensive experiments on three datasets in in-domain and domain adaptation low-resource settings, and demonstrates that the proposed DemoSG model significantly outperforms current methods in low-resource scenarios.<details>
<summary>Abstract</summary>
Most current Event Extraction (EE) methods focus on the high-resource scenario, which requires a large amount of annotated data and can hardly be applied to low-resource domains. To address EE more effectively with limited resources, we propose the Demonstration-enhanced Schema-guided Generation (DemoSG) model, which benefits low-resource EE from two aspects: Firstly, we propose the demonstration-based learning paradigm for EE to fully use the annotated data, which transforms them into demonstrations to illustrate the extraction process and help the model learn effectively. Secondly, we formulate EE as a natural language generation task guided by schema-based prompts, thereby leveraging label semantics and promoting knowledge transfer in low-resource scenarios. We conduct extensive experiments under in-domain and domain adaptation low-resource settings on three datasets, and study the robustness of DemoSG. The results show that DemoSG significantly outperforms current methods in low-resource scenarios.
</details>
<details>
<summary>摘要</summary>
现有的事件抽取（EE）方法专注于高资源情况下，需要大量的标注数据并几乎无法应用于低资源领域。为了对EE更有效地应用限制的资源，我们提出了示例增强的结构引导生成（DemoSG）模型，它具有以下两个方面的优点：首先，我们提出了示例学习模式，将标注数据转换为示例，以帮助模型彻底学习。其次，我们将EE视为自然语言生成任务，并透过Schema-based启发词提高标签 semantics，以便在低资源情况下传递知识。我们对三个数据集进行了广泛的实验，包括域内和领域适应低资源情况下的实验，并研究了DemoSG的稳定性。结果显示，DemoSG与现有的方法在低资源情况下具有很大的优势。
</details></li>
</ul>
<hr>
<h2 id="Gaining-Wisdom-from-Setbacks-Aligning-Large-Language-Models-via-Mistake-Analysis"><a href="#Gaining-Wisdom-from-Setbacks-Aligning-Large-Language-Models-via-Mistake-Analysis" class="headerlink" title="Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis"></a>Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10477">http://arxiv.org/abs/2310.10477</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 本研究旨在提高大语言模型（LLM）的安全性和合理性，特别是在面对恶意和毒害语言时。</li>
<li>methods: 本研究提出了一种基于错误分析的新的对齐策略，通过故意暴露LLM于异常输出，然后进行全面的评估，以全面了解内部的原因。</li>
<li>results: 实验结果表明，提出的方法在安全指令遵从方面的性能超过了传统对齐方法，同时保持了高效性。<details>
<summary>Abstract</summary>
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的快速进步带来了机会和挑战，特别是在无意义生成危险和恶意响应方面。传统的Alignment方法努力使LLM towards Desired performance和避免恶意内容，这种研究提出了一种新的Alignment策略，基于 mistake analysis，故意暴露LLM于异常输出，然后进行全面的评估，以全面了解内部原因via自然语言分析。因此，恶意响应可以被转化为调教征集，LLM不仅可以减少生成异常响应，还可以培养自我批判，利用其内置的恶意内容抵制能力。实验结果表明，提出的方法在安全指令遵从方面超过了传统的Alignment技术，同时保持了高效性。
</details></li>
</ul>
<hr>
<h2 id="Stance-Detection-with-Collaborative-Role-Infused-LLM-Based-Agents"><a href="#Stance-Detection-with-Collaborative-Role-Infused-LLM-Based-Agents" class="headerlink" title="Stance Detection with Collaborative Role-Infused LLM-Based Agents"></a>Stance Detection with Collaborative Role-Infused LLM-Based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10467">http://arxiv.org/abs/2310.10467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochong Lan, Chen Gao, Depeng Jin, Yong Li</li>
<li>for: 这篇文章的目的是提出一个三阶段框架，以帮助自然语言处理器（LLM）实现偏见探测。</li>
<li>methods: 这个框架使用了三种不同的LLM，每个LLM都有不同的角色，包括语言专家、领域专家和社交媒体老手。这些LLM共同执行三个阶段，包括多维度文本分析阶段、逻辑增强辩论阶段和结论统一阶段。</li>
<li>results: 这篇文章的结果显示，使用这个框架可以实现高度的偏见探测性能，并且不需要额外的标注资料和模型训练。实验还显示了这个方法的可说明性和可重用性。<details>
<summary>Abstract</summary>
Stance detection automatically detects the stance in a text towards a target, vital for content analysis in web and social media research. Despite their promising capabilities, LLMs encounter challenges when directly applied to stance detection. First, stance detection demands multi-aspect knowledge, from deciphering event-related terminologies to understanding the expression styles in social media platforms. Second, stance detection requires advanced reasoning to infer authors' implicit viewpoints, as stance are often subtly embedded rather than overtly stated in the text. To address these challenges, we design a three-stage framework COLA (short for Collaborative rOle-infused LLM-based Agents) in which LLMs are designated distinct roles, creating a collaborative system where each role contributes uniquely. Initially, in the multidimensional text analysis stage, we configure the LLMs to act as a linguistic expert, a domain specialist, and a social media veteran to get a multifaceted analysis of texts, thus overcoming the first challenge. Next, in the reasoning-enhanced debating stage, for each potential stance, we designate a specific LLM-based agent to advocate for it, guiding the LLM to detect logical connections between text features and stance, tackling the second challenge. Finally, in the stance conclusion stage, a final decision maker agent consolidates prior insights to determine the stance. Our approach avoids extra annotated data and model training and is highly usable. We achieve state-of-the-art performance across multiple datasets. Ablation studies validate the effectiveness of each design role in handling stance detection. Further experiments have demonstrated the explainability and the versatility of our approach. Our approach excels in usability, accuracy, effectiveness, explainability and versatility, highlighting its value.
</details>
<details>
<summary>摘要</summary>
Automatic stance detection可以检测文本中对目标的立场，这对于网络和社交媒体研究是非常重要。然而，深入应用于检测的语言模型（LLMs）会遇到挑战。首先，检测立场需要多方面的知识，包括理解社交媒体平台上的表达方式和解读事件相关的术语。其次，检测立场需要高级的理解，以便推理出作者的潜在观点，因为立场通常不直接在文本中表达。为解决这些挑战，我们设计了一个三个阶段的框架，称为COLA（简称为协作型角色扮演 LLM 代理）。在这个框架中，LLMs被分配为不同的角色，形成一个协作的系统，每个角色具有唯一的贡献。在多维度文本分析阶段，我们配置 LLMS  acted as语言专家、领域专家和社交媒体老手，以获得多方面的分析结果，从而解决第一个挑战。接着，在逻辑批判阶段，我们为每个可能的立场分配了一个特定的 LLM 代理，使得 LLMS 检测文本特征和立场之间的逻辑连接，解决第二个挑战。最后，在立场结论阶段，一个最终的决策者代理将先前的见解集成，以确定立场。我们的方法不需要额外的注释数据和模型训练，具有非常高的可用性。我们在多个数据集上实现了状态的最佳性能。剥离学习 validate了每个设计角色在处理检测立场方面的效果。进一步的实验还表明了我们的方法在可读性、准确性、有效性、可读性和多样性方面的优异。这些结果表明我们的方法具有价值。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Techniques-for-Identifying-the-Defective-Patterns-in-Semiconductor-Wafer-Maps-A-Survey-Empirical-and-Experimental-Evaluations"><a href="#Machine-Learning-Techniques-for-Identifying-the-Defective-Patterns-in-Semiconductor-Wafer-Maps-A-Survey-Empirical-and-Experimental-Evaluations" class="headerlink" title="Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations"></a>Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10705">http://arxiv.org/abs/2310.10705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamal Taha<br>for:This survey paper provides a comprehensive review of machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing, aiming to fill a void in the existing literature and provide an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in this field.methods:The paper employs a four-tier taxonomy to classify ML algorithms into more refined categories and techniques, providing a detailed understanding of the complex relationships between different algorithms and their sub-techniques. The taxonomy includes broad methodology categories, specific sub-techniques, and experimental evaluations to rank the techniques.results:The paper presents a comprehensive empirical evaluation of the techniques based on four criteria and an experimental evaluation that ranks the algorithms employing the same sub-techniques, techniques, sub-categories, and categories. The approach provides a detailed and holistic understanding of ML techniques and algorithms for identifying wafer defects, guiding researchers towards making more informed decisions in their work. The paper also highlights future prospects and opportunities for further research in this field.<details>
<summary>Abstract</summary>
This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous empirical and experimental evaluation to rank these varying techniques. For the empirical evaluation, we assess techniques based on a set of four criteria. The experimental evaluation ranks the algorithms employing the same sub-techniques, techniques, sub-categories, and categories. This integration of a multi-layered taxonomy, empirical evaluations, and comparative experiments provides a detailed and holistic understanding of ML techniques and algorithms for identifying wafer defects. This approach guides researchers towards making more informed decisions in their work. Additionally, the paper illuminates the future prospects of ML techniques for wafer defect identification, underscoring potential advancements and opportunities for further research in this field
</details>
<details>
<summary>摘要</summary>
We present a novel taxonomy of methodologies that categorizes algorithms into more specific categories and techniques. This taxonomy has four tiers, starting with broad methodology categories and ending with specific sub-techniques. This taxonomy helps researchers understand the complex relationships between different algorithms and their techniques.We conduct a rigorous empirical and experimental evaluation of these techniques. For the empirical evaluation, we assess techniques based on a set of four criteria. The experimental evaluation ranks the algorithms using the same sub-techniques, techniques, sub-categories, and categories. This integrated approach provides a comprehensive understanding of ML techniques and algorithms for identifying wafer defects.This paper also highlights the future prospects of ML techniques for wafer defect identification, highlighting potential advancements and opportunities for further research in this field. By providing a detailed and holistic understanding of ML techniques and algorithms, this survey aims to guide researchers in making more informed decisions in their work.翻译结果：这篇研究论文提供了机器学习（ML）技术在半导体制造过程中检测板差的全面回顾。尽管有一些研究证明了ML在板差检测中的效果，但是存在一定的研究杂乱。这篇论文尝试填补这个空白，并提供了一个深入分析的机器学习技术在板差检测中的优点、缺点和应用前景。我们提出了一种新的分类方法，它将机器学习算法分为更加细分的类别和技术。这种分类方法有四层结构，从最高级的方法类别开始，到最低级的特定技术。这种分类方法可以帮助研究人员更好地理解不同的算法和技术之间的复杂关系。我们进行了一项严格的实验和实证评估。对实验来说，我们对不同的技术进行了四个标准的评估标准。这种分类方法可以帮助研究人员在工作中做出更加 Informed 的决策。此外，这篇论文还探讨了机器学习技术在板差检测中的未来前景，并指出了潜在的进步和研究机会。
</details></li>
</ul>
<hr>
<h2 id="On-the-Relevance-of-Temporal-Features-for-Medical-Ultrasound-Video-Recognition"><a href="#On-the-Relevance-of-Temporal-Features-for-Medical-Ultrasound-Video-Recognition" class="headerlink" title="On the Relevance of Temporal Features for Medical Ultrasound Video Recognition"></a>On the Relevance of Temporal Features for Medical Ultrasound Video Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10453">http://arxiv.org/abs/2310.10453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MedAI-Clemson/pda_detection">https://github.com/MedAI-Clemson/pda_detection</a></li>
<li>paper_authors: D. Hudson Smith, John Paul Lineberger, George H. Baker</li>
<li>for: 本研究旨在提高医疗ultrasound视频识别任务的效率，特别是在低数据量情况下。</li>
<li>methods: 本研究提出了一种新的多头注意架构，通过 incorporating 时间特征来提高模型的效率。</li>
<li>results: 对比于效率高的3D CNN视频识别模型，本研究在一些常见的ultrasound任务中表现出优于其，尤其是在训练数据量受限的情况下。<details>
<summary>Abstract</summary>
Many medical ultrasound video recognition tasks involve identifying key anatomical features regardless of when they appear in the video suggesting that modeling such tasks may not benefit from temporal features. Correspondingly, model architectures that exclude temporal features may have better sample efficiency. We propose a novel multi-head attention architecture that incorporates these hypotheses as inductive priors to achieve better sample efficiency on common ultrasound tasks. We compare the performance of our architecture to an efficient 3D CNN video recognition model in two settings: one where we expect not to require temporal features and one where we do. In the former setting, our model outperforms the 3D CNN - especially when we artificially limit the training data. In the latter, the outcome reverses. These results suggest that expressive time-independent models may be more effective than state-of-the-art video recognition models for some common ultrasound tasks in the low-data regime.
</details>
<details>
<summary>摘要</summary>
“许多医疗超音波录影 задачі都涉及到识别关键生物学特征，不论在录影中出现的时间点。这表明模型化这些任务可能不需要时间特征。因此，不包含时间特征的模型架构可能会有更好的样本效率。我们提出了一个新的多头注意架构，将这两个假设作为导引假设，以 achieve better sample efficiency on common ultrasound tasks。我们将比较我们的架构和一个高效的3D CNN录影识别模型在两个设定下的性能：一个情况下，我们不需要时间特征，一个情况下，我们需要时间特征。在前一个情况下，我们的模型比3D CNN更高效，特别是当我们人工限制训练数据时。在后一个情况下，结果逆转。这些结果表明表现出时间独立的表达模型可能比现有的录影识别模型在低数据情况下更有效。”
</details></li>
</ul>
<hr>
<h2 id="Text-Summarization-Using-Large-Language-Models-A-Comparative-Study-of-MPT-7b-instruct-Falcon-7b-instruct-and-OpenAI-Chat-GPT-Models"><a href="#Text-Summarization-Using-Large-Language-Models-A-Comparative-Study-of-MPT-7b-instruct-Falcon-7b-instruct-and-OpenAI-Chat-GPT-Models" class="headerlink" title="Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models"></a>Text Summarization Using Large Language Models: A Comparative Study of MPT-7b-instruct, Falcon-7b-instruct, and OpenAI Chat-GPT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10449">http://arxiv.org/abs/2310.10449</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lbasyal/llms-text-summarization">https://github.com/lbasyal/llms-text-summarization</a></li>
<li>paper_authors: Lochan Basyal, Mihir Sanghvi</li>
<li>For: This paper explores the use of Large Language Models (LLMs) for text summarization, specifically comparing the performance of three different models (MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003) on two datasets (CNN Daily Mail and XSum).* Methods: The paper uses a diverse set of LLMs and evaluates their performance using widely accepted metrics such as BLEU Score, ROUGE Score, and BERT Score. The experiment involves different hyperparameters and aims to provide a comprehensive understanding of the effectiveness of LLMs for text summarization.* Results: According to the experiment, text-davinci-003 outperformed the other two models, demonstrating its effectiveness for text summarization. The paper provides valuable insights for researchers and practitioners within the NLP domain and lays the foundation for the development of advanced Generative AI applications.<details>
<summary>Abstract</summary>
Text summarization is a critical Natural Language Processing (NLP) task with applications ranging from information retrieval to content generation. Leveraging Large Language Models (LLMs) has shown remarkable promise in enhancing summarization techniques. This paper embarks on an exploration of text summarization with a diverse set of LLMs, including MPT-7b-instruct, falcon-7b-instruct, and OpenAI ChatGPT text-davinci-003 models. The experiment was performed with different hyperparameters and evaluated the generated summaries using widely accepted metrics such as the Bilingual Evaluation Understudy (BLEU) Score, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) Score, and Bidirectional Encoder Representations from Transformers (BERT) Score. According to the experiment, text-davinci-003 outperformed the others. This investigation involved two distinct datasets: CNN Daily Mail and XSum. Its primary objective was to provide a comprehensive understanding of the performance of Large Language Models (LLMs) when applied to different datasets. The assessment of these models' effectiveness contributes valuable insights to researchers and practitioners within the NLP domain. This work serves as a resource for those interested in harnessing the potential of LLMs for text summarization and lays the foundation for the development of advanced Generative AI applications aimed at addressing a wide spectrum of business challenges.
</details>
<details>
<summary>摘要</summary>
文本概要是一个重要的自然语言处理（NLP）任务，其应用范围从信息检索到内容生成。利用大语言模型（LLMs）已经显著提高了概要技术。这篇论文展开了使用多种LLMs进行文本概要的研究，包括MPT-7b-instruct、falcon-7b-instruct和OpenAI ChatGPT text-davinci-003模型。实验中使用了不同的超参数，并使用了通用的评价指标如双语评价下study（BLEU）分数、推理引导下的学生评价（ROUGE）分数和Transformers的扩展语言模型（BERT）分数进行评估生成的概要。根据实验结果，text-davinci-003表现最佳。这项研究使用了两个不同的数据集：CNN Daily Mail和XSum。研究的主要目标是为NLP领域的研究者和实践者提供LLMs在不同数据集上的性能评估，以便更好地利用LLMs的潜力。这项工作作为NLP领域的研究资源，并为开发高级生成AI应用程序提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Empowered-Agents-for-Simulating-Macroeconomic-Activities"><a href="#Large-Language-Model-Empowered-Agents-for-Simulating-Macroeconomic-Activities" class="headerlink" title="Large Language Model-Empowered Agents for Simulating Macroeconomic Activities"></a>Large Language Model-Empowered Agents for Simulating Macroeconomic Activities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10436">http://arxiv.org/abs/2310.10436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nian Li, Chen Gao, Yong Li, Qingmin Liao</li>
<li>for: 这篇论文旨在探讨使用语言模型（LLM）在macro经济模拟中的可能性，以解决传统模型中的三大挑战，即代理人差异、macro经济趋势的影响和多方面经济因素的互动。</li>
<li>methods: 该论文提出了一种新的方法，利用LLM来塑造人类决策行为，并通过提问工程来让LLM表现出人类特征，包括感知、反思和决策能力。</li>
<li>results: 在macro经济活动的 simulations中，LLM强化的代理人可以做出更加真实的工作和消费决策，并且可以生成更加合理的macro经济现象。这些结果表明LLM在macro经济模拟中的潜力很大。<details>
<summary>Abstract</summary>
The advent of the Web has brought about a paradigm shift in traditional economics, particularly in the digital economy era, enabling the precise recording and analysis of individual economic behavior. This has led to a growing emphasis on data-driven modeling in macroeconomics. In macroeconomic research, Agent-based modeling (ABM) emerged as an alternative, evolving through rule-based agents, machine learning-enhanced decision-making, and, more recently, advanced AI agents. However, the existing works are suffering from three main challenges when endowing agents with human-like decision-making, including agent heterogeneity, the influence of macroeconomic trends, and multifaceted economic factors. Large language models (LLMs) have recently gained prominence in offering autonomous human-like characteristics. Therefore, leveraging LLMs in macroeconomic simulation presents an opportunity to overcome traditional limitations. In this work, we take an early step in introducing a novel approach that leverages LLMs in macroeconomic simulation. We design prompt-engineering-driven LLM agents to exhibit human-like decision-making and adaptability in the economic environment, with the abilities of perception, reflection, and decision-making to address the abovementioned challenges. Simulation experiments on macroeconomic activities show that LLM-empowered agents can make realistic work and consumption decisions and emerge more reasonable macroeconomic phenomena than existing rule-based or AI agents. Our work demonstrates the promising potential to simulate macroeconomics based on LLM and its human-like characteristics.
</details>
<details>
<summary>摘要</summary>
互联网的出现带来了传统经济学中的 Paradigm shift，特别是在数位经济时代，允许精确地录取和分析个人经济行为。这导致了对数据驱动模型在macroeconomics中的增加强调。在macroeconomic研究中，Agent-based modeling（ABM） emerged as an alternative，通过规则生成的代理人、机器学习增强的决策和、更近期的进步AI代理人。然而，现有的工作受到三大挑战，包括代理人多样性、macroeconomic趋势的影响和多方面的经济因素。 latest Large language models（LLMs）have recently gained prominence in offering autonomous human-like characteristics. Therefore, leveraging LLMs in macroeconomic simulation presents an opportunity to overcome traditional limitations. In this work, we take an early step in introducing a novel approach that leverages LLMs in macroeconomic simulation. We design prompt-engineering-driven LLM agents to exhibit human-like decision-making and adaptability in the economic environment, with the abilities of perception, reflection, and decision-making to address the above-mentioned challenges. Simulation experiments on macroeconomic activities show that LLM-empowered agents can make realistic work and consumption decisions and emerge more reasonable macroeconomic phenomena than existing rule-based or AI agents. Our work demonstrates the promising potential to simulate macroeconomics based on LLM and its human-like characteristics.
</details></li>
</ul>
<hr>
<h2 id="Longitudinal-Self-supervised-Learning-Using-Neural-Ordinary-Differential-Equation"><a href="#Longitudinal-Self-supervised-Learning-Using-Neural-Ordinary-Differential-Equation" class="headerlink" title="Longitudinal Self-supervised Learning Using Neural Ordinary Differential Equation"></a>Longitudinal Self-supervised Learning Using Neural Ordinary Differential Equation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10431">http://arxiv.org/abs/2310.10431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li, Hugo Le Boité, Ramin Tadayoni, Pascal Massin, Béatrice Cochener, Ikram Brahim, Gwenolé Quellec, Mathieu Lamard</li>
<li>for:  investigate the progressive changes in anatomical structures or disease progression over time</li>
<li>methods:  longitudinal self-supervised learning (LSSL) algorithm embedded in an auto-encoder (AE) structure, Siamese-like LSSL, and neural ordinary differential equation (NODE)</li>
<li>results:  demonstration of LSSL without including a reconstruction term, and the potential of incorporating NODE in conjunction with LSSL<details>
<summary>Abstract</summary>
Longitudinal analysis in medical imaging is crucial to investigate the progressive changes in anatomical structures or disease progression over time. In recent years, a novel class of algorithms has emerged with the goal of learning disease progression in a self-supervised manner, using either pairs of consecutive images or time series of images. By capturing temporal patterns without external labels or supervision, longitudinal self-supervised learning (LSSL) has become a promising avenue. To better understand this core method, we explore in this paper the LSSL algorithm under different scenarios. The original LSSL is embedded in an auto-encoder (AE) structure. However, conventional self-supervised strategies are usually implemented in a Siamese-like manner. Therefore, (as a first novelty) in this study, we explore the use of Siamese-like LSSL. Another new core framework named neural ordinary differential equation (NODE). NODE is a neural network architecture that learns the dynamics of ordinary differential equations (ODE) through the use of neural networks. Many temporal systems can be described by ODE, including modeling disease progression. We believe that there is an interesting connection to make between LSSL and NODE. This paper aims at providing a better understanding of those core algorithms for learning the disease progression with the mentioned change. In our different experiments, we employ a longitudinal dataset, named OPHDIAT, targeting diabetic retinopathy (DR) follow-up. Our results demonstrate the application of LSSL without including a reconstruction term, as well as the potential of incorporating NODE in conjunction with LSSL.
</details>
<details>
<summary>摘要</summary>
长itudinal分析在医学成像中是关键性的，用于探索时间序列中结构或疾病进程的变化。近年来，一种新的算法类型出现了，即无supervision的自适应学习疾病进程（LSSL），使用时间序列或邻接图像对。通过捕捉时间模式，LSSL成为了一个有前途的方向。为更好地理解这种核心方法，我们在这篇论文中探讨了LSSL算法在不同情况下的表现。原始LSSL被嵌入了自适应encoder（AE）结构中。然而，传统的自适应策略通常是在Siamese-like的方式实现。因此，在本研究中，我们探索了使用Siamese-like LSSL。另一个新的核心框架是神经ordinary differential equation（NODE）。NODE是一种使用神经网络学习ordinary differential equation（ODE）的神经网络架构。许多时间系统可以通过ODE进行描述，包括疾病进程。我们认为在LSSL和NODE之间存在一种有趣的联系。本文的目标是为了更好地理解这些核心算法，以便更好地学习疾病进程的变化。在不同的实验中，我们使用了名为OPHDIAT的长itudinal数据集，targeting diabetic retinopathy（DR）跟踪。我们的结果表明了不包含重建项的LSSL的应用，以及将NODE与LSSL相结合的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Reading-Books-is-Great-But-Not-if-You-Are-Driving-Visually-Grounded-Reasoning-about-Defeasible-Commonsense-Norms"><a href="#Reading-Books-is-Great-But-Not-if-You-Are-Driving-Visually-Grounded-Reasoning-about-Defeasible-Commonsense-Norms" class="headerlink" title="Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"></a>Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10418">http://arxiv.org/abs/2310.10418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wade3han/normlens">https://github.com/wade3han/normlens</a></li>
<li>paper_authors: Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang, Jiwan Chung, Yejin Son, Yejin Choi, Youngjae Yu</li>
<li>for: 研究视觉封装常识规则，以提高机器人工智能的可理解性和适应能力。</li>
<li>methods: 使用人类评估和自然语言处理技术，构建一个新的多模态测试 benchmark，以评估模型对视觉封装常识规则的适应性和可解性。</li>
<li>results: 发现当前状态的模型判断和解释与人类标注不匹配，并提出一种新的方法，通过借鉴大型自然语言模型中的社会常识知识，改进模型与人类之间的匹配度。<details>
<summary>Abstract</summary>
Commonsense norms are defeasible by context: reading books is usually great, but not when driving a car. While contexts can be explicitly described in language, in embodied scenarios, contexts are often provided visually. This type of visually grounded reasoning about defeasible commonsense norms is generally easy for humans, but (as we show) poses a challenge for machines, as it necessitates both visual understanding and reasoning about commonsense norms. We construct a new multimodal benchmark for studying visual-grounded commonsense norms: NORMLENS. NORMLENS consists of 10K human judgments accompanied by free-form explanations covering 2K multimodal situations, and serves as a probe to address two questions: (1) to what extent can models align with average human judgment? and (2) how well can models explain their predicted judgments? We find that state-of-the-art model judgments and explanations are not well-aligned with human annotation. Additionally, we present a new approach to better align models with humans by distilling social commonsense knowledge from large language models. The data and code are released at https://seungjuhan.me/normlens.
</details>
<details>
<summary>摘要</summary>
通常的规则是可以被上下文所推翻：读书通常是非常好的，但不是在开车时。而在实际情况下，上下文通常是通过视觉提供的。这种基于视觉的常识规则的理解和判断是人类非常容易做，但对机器来说是一种挑战，因为它需要同时具备视觉理解和常识规则的理解。我们构建了一个新的多Modal benchMark，名为NORMLENS，用于研究基于视觉的常识规则。NORMLENS包含10,000个人类判断，以及2,000个多Modal的情况，并用于解决两个问题：（1）模型与人类平均判断是否能够Alignment？（2）模型如何解释其预测的判断？我们发现当前的模型判断和解释都与人类注释不一致。此外，我们还提出了一种新的方法，通过从大语言模型中提取社会常识知识来更好地将模型与人类Alignment。数据和代码在https://seungjuhan.me/normlens上发布。
</details></li>
</ul>
<hr>
<h2 id="Real-Fake-Effective-Training-Data-Synthesis-Through-Distribution-Matching"><a href="#Real-Fake-Effective-Training-Data-Synthesis-Through-Distribution-Matching" class="headerlink" title="Real-Fake: Effective Training Data Synthesis Through Distribution Matching"></a>Real-Fake: Effective Training Data Synthesis Through Distribution Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10402">http://arxiv.org/abs/2310.10402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, Bo Zhao</li>
<li>for: 提高深度学习模型的训练效率和鲁棒性。</li>
<li>methods: 基于分布匹配理论的数据生成方法，包括数据生成和数据筛选。</li>
<li>results: 在多种图像分类任务中， synthetic data 能够取代和补充实际数据，提高模型的鲁棒性和特点外泄能力。<details>
<summary>Abstract</summary>
Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, we analyze the principles underlying training data synthesis for supervised learning and elucidate a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy. Through extensive experiments, we demonstrate the effectiveness of our synthetic data across diverse image classification tasks, both as a replacement for and augmentation to real datasets, while also benefits challenging tasks such as out-of-distribution generalization and privacy preservation.
</details>
<details>
<summary>摘要</summary>
现代深度学习模型的训练数据 synthetic 技术在各种学习任务和场景中得到了广泛应用，具有提高数据量、提高模型性能、隐私保护等优点。然而，现有的synthetic数据生成方法在训练高级深度模型时效率仍然较低，限制其实际应用。为解决这个挑战，我们分析了supervised 学习数据生成的原理，从分布匹配角度出发，探讨生成效果的机制。经过广泛的实验，我们证明了我们的synthetic数据在多种图像分类任务中具有广泛的应用价值，可以替代真实数据，也可以增强模型的性能，同时在难题上如out-of-distribution泛化和隐私保护等方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="Can-Word-Sense-Distribution-Detect-Semantic-Changes-of-Words"><a href="#Can-Word-Sense-Distribution-Detect-Semantic-Changes-of-Words" class="headerlink" title="Can Word Sense Distribution Detect Semantic Changes of Words?"></a>Can Word Sense Distribution Detect Semantic Changes of Words?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10400">http://arxiv.org/abs/2310.10400</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LivNLP/Sense-based-Semantic-Change-Prediction">https://github.com/LivNLP/Sense-based-Semantic-Change-Prediction</a></li>
<li>paper_authors: Xiaohang Tang, Yi Zhou, Taichi Aida, Procheta Sen, Danushka Bollegala</li>
<li>for: 这个研究的目的是为了探索使用时间点数据集来预测字词意思是否有变化。</li>
<li>methods: 这个研究使用预训 слова embeddings 来自动标注目标词的每个出现，然后计算每个词的意思分布。最后，使用不同的分布或距离度量来衡量目标词的意思变化。</li>
<li>results: 实验结果显示，使用时间点数据集可以准确预测英语、德语、瑞典语和拉丁语中字词意思的变化。<details>
<summary>Abstract</summary>
Semantic Change Detection (SCD) of words is an important task for various NLP applications that must make time-sensitive predictions. Some words are used over time in novel ways to express new meanings, and these new meanings establish themselves as novel senses of existing words. On the other hand, Word Sense Disambiguation (WSD) methods associate ambiguous words with sense ids, depending on the context in which they occur. Given this relationship between WSD and SCD, we explore the possibility of predicting whether a target word has its meaning changed between two corpora collected at different time steps, by comparing the distributions of senses of that word in each corpora. For this purpose, we use pretrained static sense embeddings to automatically annotate each occurrence of the target word in a corpus with a sense id. Next, we compute the distribution of sense ids of a target word in a given corpus. Finally, we use different divergence or distance measures to quantify the semantic change of the target word across the two given corpora. Our experimental results on SemEval 2020 Task 1 dataset show that word sense distributions can be accurately used to predict semantic changes of words in English, German, Swedish and Latin.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>word sense distribution的Changesemantic detection（SCD）是各种自然语言处理（NLP）应用程序中的重要任务，这些应用程序需要在时间紧张的情况下进行预测。一些 слова在时间的推移中被用于表达新的意思，这些新意思会成为存在的 слова的新的意思。然而，word sense disambiguation（WSD）方法会将word的意思相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相关的上下文中的word相
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-World-Active-Learning-for-3D-Object-Detection"><a href="#Towards-Open-World-Active-Learning-for-3D-Object-Detection" class="headerlink" title="Towards Open World Active Learning for 3D Object Detection"></a>Towards Open World Active Learning for 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10391">http://arxiv.org/abs/2310.10391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoxiao Chen, Yadan Luo, Zixin Wang, Zijian Wang, Xin Yu, Zi Huang</li>
<li>for: 本研究旨在解决开放世界3D对象检测中存在新类出现的挑战，即效率地选择少量的3D框进行标注，以提高对知类和未知类的检测性能。</li>
<li>methods: 本研究提出了一种名为OpenCRB的简单有效的活动学习策略，通过融合关系约束，选择最有价值的3D框进行标注，以最小化标注成本。</li>
<li>results: 对于开放世界3D对象检测任务，提出了一种名为OpenCRB的简单有效的活动学习策略，可以在很少的标注成本下，达到高效地检测知类和未知类的目标。<details>
<summary>Abstract</summary>
Significant strides have been made in closed world 3D object detection, testing systems in environments with known classes. However, the challenge arises in open world scenarios where new object classes appear. Existing efforts sequentially learn novel classes from streams of labeled data at a significant annotation cost, impeding efficient deployment to the wild. To seek effective solutions, we investigate a more practical yet challenging research task: Open World Active Learning for 3D Object Detection (OWAL-3D), aiming at selecting a small number of 3D boxes to annotate while maximizing detection performance on both known and unknown classes. The core difficulty centers on striking a balance between mining more unknown instances and minimizing the labeling expenses of point clouds. Empirically, our study finds the harmonious and inverse relationship between box quantities and their confidences can help alleviate the dilemma, avoiding the repeated selection of common known instances and focusing on uncertain objects that are potentially unknown. We unify both relational constraints into a simple and effective AL strategy namely OpenCRB, which guides to acquisition of informative point clouds with the least amount of boxes to label. Furthermore, we develop a comprehensive codebase for easy reproducing and future research, supporting 15 baseline methods (i.e., active learning, out-of-distribution detection and open world detection), 2 types of modern 3D detectors (i.e., one-stage SECOND and two-stage PV-RCNN) and 3 benchmark 3D datasets (i.e., KITTI, nuScenes and Waymo). Extensive experiments evidence that the proposed Open-CRB demonstrates superiority and flexibility in recognizing both novel and shared categories with very limited labeling costs, compared to state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>Recent progress has been made in closed-world 3D object detection, testing systems in environments with known classes. However, the challenge arises in open-world scenarios where new object classes appear. Existing efforts sequentially learn novel classes from streams of labeled data at a significant annotation cost, impeding efficient deployment to the wild. To seek effective solutions, we investigate a more practical yet challenging research task: Open World Active Learning for 3D Object Detection (OWAL-3D), aiming at selecting a small number of 3D boxes to annotate while maximizing detection performance on both known and unknown classes. The core difficulty centers on striking a balance between mining more unknown instances and minimizing the labeling expenses of point clouds. Empirically, our study finds the harmonious and inverse relationship between box quantities and their confidences can help alleviate the dilemma, avoiding the repeated selection of common known instances and focusing on uncertain objects that are potentially unknown. We unify both relational constraints into a simple and effective AL strategy named OpenCRB, which guides to acquisition of informative point clouds with the least amount of boxes to label. Furthermore, we develop a comprehensive codebase for easy reproducing and future research, supporting 15 baseline methods (i.e., active learning, out-of-distribution detection, and open-world detection), 2 types of modern 3D detectors (i.e., one-stage SECOND and two-stage PV-RCNN), and 3 benchmark 3D datasets (i.e., KITTI, nuScenes, and Waymo). Extensive experiments evidence that the proposed Open-CRB demonstrates superiority and flexibility in recognizing both novel and shared categories with very limited labeling costs, compared to state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="Cross-Lingual-Consistency-of-Factual-Knowledge-in-Multilingual-Language-Models"><a href="#Cross-Lingual-Consistency-of-Factual-Knowledge-in-Multilingual-Language-Models" class="headerlink" title="Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models"></a>Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10378">http://arxiv.org/abs/2310.10378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Betswish/Cross-Lingual-Consistency">https://github.com/Betswish/Cross-Lingual-Consistency</a></li>
<li>paper_authors: Jirui Qi, Raquel Fernández, Arianna Bisazza</li>
<li>For: The paper aims to study the cross-lingual consistency (CLC) of factual knowledge in various multilingual pre-trained language models (PLMs) and to identify the determining factors for CLC.* Methods: The authors propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. They conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level.* Results: The authors find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. They also conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing, and find that the new piece of knowledge transfers only to languages with which English has a high RankC score.Here are the three points in Simplified Chinese text:* For: 这个论文目的是研究不同语言背景下的多语言大规模预训练语言模型（PLMs）中的知识一致性（CLC），并确定这些因素的影响因素。* Methods: 作者们提出了一种简单的排名基于一致性（RankC）指标，以独立地评估不同语言之间的知识一致性。他们进行了深入的分析，以确定CLC的决定因素，包括模型级别和语言对照级别。* Results: 作者们发现，增加模型大小通常会提高大多数语言中的事实探测精度，但不会提高跨语言一致性。他们还进行了模型编辑后新知识插入的 caso study，发现新知识只转移到与英语有高RankC分数的语言中。<details>
<summary>Abstract</summary>
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.
</details>
<details>
<summary>摘要</summary>
多语言大规模预训练语言模型（PLM）已经显示出很大量的事实知识，但是各语言之间存在大量的变化。为确保用户不同语言背景 obtain consistent feedback from the same model，我们研究了跨语言一致性（CLC）的多语言大规模预训练语言模型中的事实知识。为此，我们提出了一个 Ranking-based Consistency（RankC）度量来评估不同语言之间的知识一致性，不受准确率的影响。使用这个度量，我们进行了详细的分析CLC的决定因素，包括模型级别和语言对级别。 among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.
</details></li>
</ul>
<hr>
<h2 id="GTA-A-Geometry-Aware-Attention-Mechanism-for-Multi-View-Transformers"><a href="#GTA-A-Geometry-Aware-Attention-Mechanism-for-Multi-View-Transformers" class="headerlink" title="GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers"></a>GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10375">http://arxiv.org/abs/2310.10375</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autonomousvision/gta">https://github.com/autonomousvision/gta</a></li>
<li>paper_authors: Takeru Miyato, Bernhard Jaeger, Max Welling, Andreas Geiger</li>
<li>for: 提高3D视觉任务中transformer模型的学习效率和性能，无需额外学习参数，只带有较少的计算开销。</li>
<li>methods: 基于几何关系的相对变换来编码token的几何结构，提出了一种几何意识授益机制（Geometric Transform Attention，GTA）。</li>
<li>results: 在多视图synthesis任务中，GTA提高了state-of-the-art transformer-based NVS模型的学习效率和性能，无需额外学习参数，只带有较少的计算开销。<details>
<summary>Abstract</summary>
As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.
</details>
<details>
<summary>摘要</summary>
transformers是 permutation 的equivariant，因此需要encoding位置信息来进行许多任务。然而，现有的 pozitional 编码方案最初是为 NLP 任务设计的，因此对于视觉任务，它们的适用性是有问题的。我们认为现有的 pozitional 编码方案对于 3D 视觉任务是不佳的，因为它们不尊重它们的下面 3D 结构。基于这个假设，我们提出了一种 geometry-aware 注意力机制，该机制通过 queries 和 key-value 对的几何关系来确定 tokens 的几何变换。我们通过在多视图 synthesis (NVS) 数据集上评估了多个 sparse wide-baseline 多视图设定，并证明了我们的注意力（GTA）可以提高基于 transformer 的 NVS 模型的学习效率和性能，无需额外学习参数，只需要少量的计算开销。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Tuning-for-Multi-View-Graph-Contrastive-Learning"><a href="#Prompt-Tuning-for-Multi-View-Graph-Contrastive-Learning" class="headerlink" title="Prompt Tuning for Multi-View Graph Contrastive Learning"></a>Prompt Tuning for Multi-View Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10362">http://arxiv.org/abs/2310.10362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghua Gong, Xiang Li, Jianxiang Yu, Cheng Yao, Jiaqi Tan, Chengcheng Yu, Dawei Yin</li>
<li>for: 提高 traditional GNN 的问题，如标签依赖和泛化性能，使用 “预训练和精度调整” 方法。</li>
<li>methods: 提出了一种多视图图像对比学习方法作为预tex，并设计了一种启发调整方法来衔接预tex 和下游任务。</li>
<li>results: 通过对多个 benchmark 数据集进行广泛的实验，证明了我们的提议可以有效地提高 GNN 的性能。<details>
<summary>Abstract</summary>
In recent years, "pre-training and fine-tuning" has emerged as a promising approach in addressing the issues of label dependency and poor generalization performance in traditional GNNs. To reduce labeling requirement, the "pre-train, fine-tune" and "pre-train, prompt" paradigms have become increasingly common. In particular, prompt tuning is a popular alternative to "pre-training and fine-tuning" in natural language processing, which is designed to narrow the gap between pre-training and downstream objectives. However, existing study of prompting on graphs is still limited, lacking a framework that can accommodate commonly used graph pre-training methods and downstream tasks. In this paper, we propose a multi-view graph contrastive learning method as pretext and design a prompting tuning for it. Specifically, we first reformulate graph pre-training and downstream tasks into a common format. Second, we construct multi-view contrasts to capture relevant information of graphs by GNN. Third, we design a prompting tuning method for our multi-view graph contrastive learning method to bridge the gap between pretexts and downsteam tasks. Finally, we conduct extensive experiments on benchmark datasets to evaluate and analyze our proposed method.
</details>
<details>
<summary>摘要</summary>
Recently, "预训练和细化" 方法在解决传统GNNS中的标签依赖和泛化性问题上得到了广泛的应用。以减少标签需求为目的，"预训练、细化" 和 "预训练、提示" 两种方法在自然语言处理领域中得到了广泛的应用。特别是，提示练习是自然语言处理中的一种流行的代替方法，旨在减少预训练和下游目标之间的差距。然而，现有的图像提示研究仍然受限，缺乏一个框架可以整合通用的图像预训练方法和下游任务。在这篇论文中，我们提出了一种多视图图像对照学习方法作为预文，并设计了一种提示练习方法来衔接这两者。具体来说，我们首先将图像预训练和下游任务转化为共同的格式。其次，我们使用多视图对照来捕捉图像中重要信息。最后，我们设计了一种提示练习方法来桥接预文和下游任务之间的差距。我们在标准 benchmark 数据集上进行了广泛的实验，以评估和分析我们的提议方法。
</details></li>
</ul>
<hr>
<h2 id="Tabular-Representation-Noisy-Operators-and-Impacts-on-Table-Structure-Understanding-Tasks-in-LLMs"><a href="#Tabular-Representation-Noisy-Operators-and-Impacts-on-Table-Structure-Understanding-Tasks-in-LLMs" class="headerlink" title="Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs"></a>Tabular Representation, Noisy Operators, and Impacts on Table Structure Understanding Tasks in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10358">http://arxiv.org/abs/2310.10358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananya Singha, José Cambronero, Sumit Gulwani, Vu Le, Chris Parnin</li>
<li>for: 这个论文旨在研究大型自然语言模型（LLM）在表格任务中使用上下文学习，并评估不同格式的表格提示表示的影响。</li>
<li>methods: 作者根据先前的工作，生成了一系列自我超vised的结构任务（例如，导航到单元和行；转置表格），并评估不同格式下LLM表现的差异。此外，作者还引入了8种噪音操作，以模拟实际世界中的潦腹数据和敌意输入，并证明这些操作对不同结构理解任务中LLM表现的影响。</li>
<li>results: 研究发现，不同格式下LLM的表现有显著差异，而噪音操作也可以影响LLM的表现。这些结果表明，在选择表格提示格式时，需要考虑表格的结构和噪音特征，以便 optimize LLM的表现。<details>
<summary>Abstract</summary>
Large language models (LLMs) are increasingly applied for tabular tasks using in-context learning. The prompt representation for a table may play a role in the LLMs ability to process the table. Inspired by prior work, we generate a collection of self-supervised structural tasks (e.g. navigate to a cell and row; transpose the table) and evaluate the performance differences when using 8 formats. In contrast to past work, we introduce 8 noise operations inspired by real-world messy data and adversarial inputs, and show that such operations can impact LLM performance across formats for different structural understanding tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在表格任务中使用内容学习，表格提示表现可能影响 LLM 的处理能力。受到先前的工作启发，我们生成了一个自动supervised的结构任务集（例如：前往矩格和行），并评估不同格式的性能差异。相比于过去的工作，我们引入了8种噪音操作，这些操作是根据实际的混乱数据和敌方输入而设计的，并证明这些操作可以影响 LLM 的性能在不同结构理解任务中。
</details></li>
</ul>
<hr>
<h2 id="Compressed-Sensing-of-Generative-Sparse-latent-GSL-Signals"><a href="#Compressed-Sensing-of-Generative-Sparse-latent-GSL-Signals" class="headerlink" title="Compressed Sensing of Generative Sparse-latent (GSL) Signals"></a>Compressed Sensing of Generative Sparse-latent (GSL) Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.15119">http://arxiv.org/abs/2310.15119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Honoré, Anubhab Ghosh, Saikat Chatterjee</li>
<li>for: 该研究旨在使用神经网络生成模型进行环境信号重建，并使用非对称拟合算法实现稀疏化。</li>
<li>methods: 该研究使用了神经网络生成模型，并采用了非对称拟合算法来实现稀疏化。</li>
<li>results: 实验结果表明，使用非对称拟合算法可以实现高质量的环境信号重建。<details>
<summary>Abstract</summary>
We consider reconstruction of an ambient signal in a compressed sensing (CS) setup where the ambient signal has a neural network based generative model. The generative model has a sparse-latent input and we refer to the generated ambient signal as generative sparse-latent signal (GSL). The proposed sparsity inducing reconstruction algorithm is inherently non-convex, and we show that a gradient based search provides a good reconstruction performance. We evaluate our proposed algorithm using simulated data.
</details>
<details>
<summary>摘要</summary>
我们考虑了压缩感知（CS）设置中重建的环境信号，该信号有基于神经网络的生成模型。生成的环境信号我们称为生成稀烈输入信号（GSL）。我们提出的稀烈性引导的重建算法是非几何的，我们表明了使用梯度基本搜索可以获得良好的重建性能。我们使用模拟数据进行评估我们的提议算法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Layerwise-Polynomial-Approximation-for-Efficient-Private-Inference-on-Fully-Homomorphic-Encryption-A-Dynamic-Programming-Approach"><a href="#Optimizing-Layerwise-Polynomial-Approximation-for-Efficient-Private-Inference-on-Fully-Homomorphic-Encryption-A-Dynamic-Programming-Approach" class="headerlink" title="Optimizing Layerwise Polynomial Approximation for Efficient Private Inference on Fully Homomorphic Encryption: A Dynamic Programming Approach"></a>Optimizing Layerwise Polynomial Approximation for Efficient Private Inference on Fully Homomorphic Encryption: A Dynamic Programming Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10349">http://arxiv.org/abs/2310.10349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junghyun Lee, Eunsang Lee, Young-Sik Kim, Yongwoo Lee, Joon-Woo Lee, Yongjune Kim, Jong-Seon No</li>
<li>for: 本研究旨在实现基于完全同质加密的隐私保护深度神经网络，但实际应用受到 prolonged inference times 的限制。这主要归结于使用高度多项式函数approximation，如ReLU函数的高度多项式approximation，占用了大量同质计算资源，导致更慢的推理。</li>
<li>methods: 本研究使用layerwise度优化activation functions的方法，以减少推理时间，保持深度神经网络的分类精度。而不like previoius works，我们不使用最小最大approximation方法，而是使用weighted least squares approximation方法，基于activation functions的输入分布。然后，我们通过 dynamic programming算法获取layerwise优化的度数，考虑每层的扩张错误对深度神经网络的分类精度的影响。此外，我们还提议在ciphertext moduli-chain layerwise进行调整，以更好地减少推理时间。</li>
<li>results: 我们的方法可以将ResNet-20模型和ResNet-32模型的推理时间减少为3.44倍和3.16倍，respectively，相比之前使用均匀度和固定ciphertext modulus的实现。<details>
<summary>Abstract</summary>
Recent research has explored the implementation of privacy-preserving deep neural networks solely using fully homomorphic encryption. However, its practicality has been limited because of prolonged inference times. When using a pre-trained model without retraining, a major factor contributing to these prolonged inference times is the high-degree polynomial approximation of activation functions such as the ReLU function. The high-degree approximation consumes a substantial amount of homomorphic computational resources, resulting in slower inference. Unlike the previous works approximating activation functions uniformly and conservatively, this paper presents a \emph{layerwise} degree optimization of activation functions to aggressively reduce the inference time while maintaining classification accuracy by taking into account the characteristics of each layer. Instead of the minimax approximation commonly used in state-of-the-art private inference models, we employ the weighted least squares approximation method with the input distributions of activation functions. Then, we obtain the layerwise optimized degrees for activation functions through the \emph{dynamic programming} algorithm, considering how each layer's approximation error affects the classification accuracy of the deep neural network. Furthermore, we propose modulating the ciphertext moduli-chain layerwise to reduce the inference time. By these proposed layerwise optimization methods, we can reduce inference times for the ResNet-20 model and the ResNet-32 model by 3.44 times and 3.16 times, respectively, in comparison to the prior implementations employing uniform degree polynomials and a consistent ciphertext modulus.
</details>
<details>
<summary>摘要</summary>
近期研究探讨了使用完全同质加密实现隐私保护深度神经网络。然而，它的实用性受到了 prolonged inference times 的限制。当使用预训练模型无需重新训练时，一个主要的因素是高度 polynomials 的激活函数approximation，如 ReLU 函数。高度的激活函数approximation 需要大量的同质计算资源，导致更慢的推理。 unlike previous works approximating activation functions uniformly and conservatively, this paper presents a layerwise degree optimization of activation functions to aggressively reduce the inference time while maintaining classification accuracy by taking into account the characteristics of each layer。 instead of the minimax approximation commonly used in state-of-the-art private inference models, we employ the weighted least squares approximation method with the input distributions of activation functions。 then, we obtain the layerwise optimized degrees for activation functions through the dynamic programming algorithm, considering how each layer's approximation error affects the classification accuracy of the deep neural network。 furthermore, we propose modulating the ciphertext moduli-chain layerwise to reduce the inference time。 by these proposed layerwise optimization methods, we can reduce inference times for the ResNet-20 model and the ResNet-32 model by 3.44 times and 3.16 times, respectively, in comparison to the prior implementations employing uniform degree polynomials and a consistent ciphertext modulus。
</details></li>
</ul>
<hr>
<h2 id="Attribution-Patching-Outperforms-Automated-Circuit-Discovery"><a href="#Attribution-Patching-Outperforms-Automated-Circuit-Discovery" class="headerlink" title="Attribution Patching Outperforms Automated Circuit Discovery"></a>Attribution Patching Outperforms Automated Circuit Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10348">http://arxiv.org/abs/2310.10348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaquib Syed, Can Rager, Arthur Conmy</li>
<li>for: 这个论文的目的是探讨自动化可读性研究的可能性，以扩展神经网络行为的解释到大型模型。</li>
<li>methods: 这个论文使用了归因覆盖来自动化发现计算子网络（circuit）。它使用了一种简单的方法，基于归因覆盖来估算每个边的重要性，然后使用这个估算来剪枝。</li>
<li>results: 论文表明，使用这种方法可以超越现有的所有方法，只需要两次前向 passes和一次后向 pass。在所有任务中，这种方法的AUC从计算子网络恢复中得到了最高的平均值。<details>
<summary>Abstract</summary>
Automated interpretability research has recently attracted attention as a potential research direction that could scale explanations of neural network behavior to large models. Existing automated circuit discovery work applies activation patching to identify subnetworks responsible for solving specific tasks (circuits). In this work, we show that a simple method based on attribution patching outperforms all existing methods while requiring just two forward passes and a backward pass. We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph. Using this approximation, we prune the least important edges of the network. We survey the performance and limitations of this method, finding that averaged over all tasks our method has greater AUC from circuit recovery than other methods.
</details>
<details>
<summary>摘要</summary>
自动化可读性研究近期吸引了关注，作为可扩展 neural network 行为的解释的潜在研究方向。现有的自动化电路发现工作使用活动贴图来确定解决特定任务（电路）负责的子网络。在这种工作中，我们显示了一种简单的方法，基于归因贴图，超过所有现有方法，只需要两次前进和一次反向传播。我们使用线性近似来估计活动贴图中每个边的重要性。使用这种近似，我们剪枝网络中最不重要的边。我们对这种方法的性能和局限性进行了抽查，发现在所有任务上的均值AUC greater than other methods。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Metasurface-Practicality-for-B5G-Networks-AI-assisted-RIS-Planning"><a href="#Unlocking-Metasurface-Practicality-for-B5G-Networks-AI-assisted-RIS-Planning" class="headerlink" title="Unlocking Metasurface Practicality for B5G Networks: AI-assisted RIS Planning"></a>Unlocking Metasurface Practicality for B5G Networks: AI-assisted RIS Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10330">http://arxiv.org/abs/2310.10330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Encinas-Lago, Antonio Albanese, Vincenzo Sciancalepore, Marco Di Renzo, Xavier Costa-Pérez</li>
<li>for: 本文旨在探讨如何使用可编程智能表面（RIS）来提高无线网络性能，尤其是在 beyond-fifth-generation 网络（B5G）中。</li>
<li>methods: 本文使用深度逆向学习（DRL）算法，训练一个 DRL 代理，以便优化 RIS 的投放。</li>
<li>results: 本文在法国雷恩火车站的indoor场景中进行了实验，并证明了对于无法覆盖的区域，D-RISA 算法可以提供更好的覆盖率（10 dB 的最小信号噪听比提高），同时具有更低的计算时间（下降至 -25%）和更好的扩展性。<details>
<summary>Abstract</summary>
The advent of reconfigurable intelligent surfaces(RISs) brings along significant improvements for wireless technology on the verge of beyond-fifth-generation networks (B5G).The proven flexibility in influencing the propagation environment opens up the possibility of programmatically altering the wireless channel to the advantage of network designers, enabling the exploitation of higher-frequency bands for superior throughput overcoming the challenging electromagnetic (EM) propagation properties at these frequency bands.   However, RISs are not magic bullets. Their employment comes with significant complexity, requiring ad-hoc deployments and management operations to come to fruition. In this paper, we tackle the open problem of bringing RISs to the field, focusing on areas with little or no coverage. In fact, we present a first-of-its-kind deep reinforcement learning (DRL) solution, dubbed as D-RISA, which trains a DRL agent and, in turn, obtain san optimal RIS deployment. We validate our framework in the indoor scenario of the Rennes railway station in France, assessing the performance of our algorithm against state-of-the-art (SOA) approaches. Our benchmarks showcase better coverage, i.e., 10-dB increase in minimum signal-to-noise ratio (SNR), at lower computational time (up to -25 percent) while improving scalability towards denser network deployments.
</details>
<details>
<summary>摘要</summary>
随着智能重新配置表面（RIS）的出现， fifth-generation wireless networks（B5G）的技术将得到显著改进。 RIS 的可变性使得网络设计者可以通过程序控制无线通信环境，从而在高频段上获得更高的吞吐量，并且可以超越高频段的电磁波媒体传输性的挑战。 然而， RIS 不是一个魔术药丸。它的使用需要适当的部署和管理操作，以便实现。在这篇论文中，我们解决了将 RIS 引入到实际应用中的开放问题，特别是在有少量或无覆盖的地区。我们提出了一种首先的深度优化学（DRL）解决方案，称为 D-RISA，它在 RIS 部署方面进行优化。我们在法国雷恩火车站的indoorenario中验证了我们的框架，并与现有的方法进行比较。我们的标准显示了更好的覆盖率，即10dB的最小信号响应比（SNR）的提高，同时减少计算时间（下降至25%），并改善了网络部署的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-and-Exploiting-Functional-Specialization-in-Multi-Head-Attention-under-Multi-task-Learning"><a href="#Interpreting-and-Exploiting-Functional-Specialization-in-Multi-Head-Attention-under-Multi-task-Learning" class="headerlink" title="Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning"></a>Interpreting and Exploiting Functional Specialization in Multi-Head Attention under Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10318">http://arxiv.org/abs/2310.10318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/znlp/functionalspecializationinmha">https://github.com/znlp/functionalspecializationinmha</a></li>
<li>paper_authors: Chong Li, Shaonan Wang, Yunhao Zhang, Jiajun Zhang, Chengqing Zong</li>
<li>for: 这paper aims to investigate the functional specialization of multi-head attention in transformer-based models under multi-task learning.</li>
<li>methods: The authors propose an interpreting method to quantify the degree of functional specialization in multi-head attention and a simple multi-task training method to increase functional specialization and mitigate negative information transfer.</li>
<li>results: Experimental results on seven pre-trained transformer models demonstrate that multi-head attention evolves functional specialization after multi-task training, which is affected by the similarity of tasks. The proposed multi-task training strategy based on functional specialization boosts performance in both multi-task learning and transfer learning without adding any parameters.<details>
<summary>Abstract</summary>
Transformer-based models, even though achieving super-human performance on several downstream tasks, are often regarded as a black box and used as a whole. It is still unclear what mechanisms they have learned, especially their core module: multi-head attention. Inspired by functional specialization in the human brain, which helps to efficiently handle multiple tasks, this work attempts to figure out whether the multi-head attention module will evolve similar function separation under multi-tasking training. If it is, can this mechanism further improve the model performance? To investigate these questions, we introduce an interpreting method to quantify the degree of functional specialization in multi-head attention. We further propose a simple multi-task training method to increase functional specialization and mitigate negative information transfer in multi-task learning. Experimental results on seven pre-trained transformer models have demonstrated that multi-head attention does evolve functional specialization phenomenon after multi-task training which is affected by the similarity of tasks. Moreover, the multi-task training strategy based on functional specialization boosts performance in both multi-task learning and transfer learning without adding any parameters.
</details>
<details>
<summary>摘要</summary>
transformer-based模型，即使达到了人类超常表现，常被视为黑盒子，无法了解它们学习的机制，尤其是核心模块：多头注意力。我们受到人脑功能特化的启发，人脑可以有效地处理多个任务，因此我们想知道多头注意力模块是否会在多任务训练中发展类似的功能分化现象。如果是，那么这种机制可能会进一步提高模型性能吗？为了回答这些问题，我们提出了一种量化多头注意力中函数特化程度的解释方法。此外，我们还提出了一种简单的多任务训练方法，可以增强功能特化并降低多任务学习中的负信息传递。实验结果表明，在七种预训练transformer模型中，多头注意力会在多任务训练后发展功能分化现象，这种现象受到任务相似度的影响。此外，基于功能特化的多任务训练策略可以提高多任务学习和传播学习的性能，无需添加参数。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Offline-Reinforcement-Learning-for-Glycemia-Control"><a href="#End-to-end-Offline-Reinforcement-Learning-for-Glycemia-Control" class="headerlink" title="End-to-end Offline Reinforcement Learning for Glycemia Control"></a>End-to-end Offline Reinforcement Learning for Glycemia Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10312">http://arxiv.org/abs/2310.10312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tristan Beolet, Alice Adenis, Erik Huneker, Maxime Louis</li>
<li>for: 这项研究旨在提高closed-loop系统的糖尿病控制性能，并使其更适应不同情况。</li>
<li>methods: 该研究使用了RL算法，并开发了一个端到端个性化管道，以 removing the need for a simulator while still enabling the estimation of clinically relevant metrics for diabetes。</li>
<li>results: 研究表明，使用RL算法和个性化管道可以提高closed-loop系统的糖尿病控制性能，并减少了风险的存在。<details>
<summary>Abstract</summary>
The development of closed-loop systems for glycemia control in type I diabetes relies heavily on simulated patients. Improving the performances and adaptability of these close-loops raises the risk of over-fitting the simulator. This may have dire consequences, especially in unusual cases which were not faithfully-if at all-captured by the simulator. To address this, we propose to use offline RL agents, trained on real patient data, to perform the glycemia control. To further improve the performances, we propose an end-to-end personalization pipeline, which leverages offline-policy evaluation methods to remove altogether the need of a simulator, while still enabling an estimation of clinically relevant metrics for diabetes.
</details>
<details>
<summary>摘要</summary>
开发闭环系统控制型一 диа베ت斯需要启用模拟患者。提高这些闭环的性能和适应性可能会增加过拟合模拟器的风险，特别是在不常见的情况下。为解决这个问题，我们提议使用线上RL代理，在真实患者数据上训练，来实现血糖控制。此外，我们还提议一个终端个性化管道，利用线上策略评估方法来完全排除模拟器的需求，同时仍能估计临床重要指标。
</details></li>
</ul>
<hr>
<h2 id="Learning-visual-based-deformable-object-rearrangement-with-local-graph-neural-networks"><a href="#Learning-visual-based-deformable-object-rearrangement-with-local-graph-neural-networks" class="headerlink" title="Learning visual-based deformable object rearrangement with local graph neural networks"></a>Learning visual-based deformable object rearrangement with local graph neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10307">http://arxiv.org/abs/2310.10307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dengyh16code/deformable-gnn">https://github.com/dengyh16code/deformable-gnn</a></li>
<li>paper_authors: Yuhong Deng, Xueqian Wang, Lipeng chen</li>
<li>for: 本研究旨在解决机器人对弹性物体（如绳子和布）的重新排序问题，使用只有视觉观察的情况下，将弹性物体转换到预先设定的目标配置。</li>
<li>methods: 本研究提出了一种新的表示策略，可以快速和有效地表示弹性物体的状态，并且可以模型弹性重新排序动态。本研究还提出了一种本地图 neural network（GNN），用于同时学习弹性重新排序动态和掌握最佳抓取和放置动作。</li>
<li>results: 对多种弹性重新排序任务进行了仿真和实验，结果显示，提出的动态图表示方法可以更高效地模型弹性重新排序动态，并且在多任务学习和实际应用中表现出色。<details>
<summary>Abstract</summary>
Goal-conditioned rearrangement of deformable objects (e.g. straightening a rope and folding a cloth) is one of the most common deformable manipulation tasks, where the robot needs to rearrange a deformable object into a prescribed goal configuration with only visual observations. These tasks are typically confronted with two main challenges: the high dimensionality of deformable configuration space and the underlying complexity, nonlinearity and uncertainty inherent in deformable dynamics. To address these challenges, we propose a novel representation strategy that can efficiently model the deformable object states with a set of keypoints and their interactions. We further propose local-graph neural network (GNN), a light local GNN learning to jointly model the deformable rearrangement dynamics and infer the optimal manipulation actions (e.g. pick and place) by constructing and updating two dynamic graphs. Both simulated and real experiments have been conducted to demonstrate that the proposed dynamic graph representation shows superior expressiveness in modeling deformable rearrangement dynamics. Our method reaches much higher success rates on a variety of deformable rearrangement tasks (96.3% on average) than state-of-the-art method in simulation experiments. Besides, our method is much more lighter and has a 60% shorter inference time than state-of-the-art methods. We also demonstrate that our method performs well in the multi-task learning scenario and can be transferred to real-world applications with an average success rate of 95% by solely fine tuning a keypoint detector.
</details>
<details>
<summary>摘要</summary>
goal-conditioned 重新排序的软体 объек� (例如，整 Straightening a rope and folding a cloth) 是最常见的软体 manipulation 任务之一， robot需要根据视觉观察来重新排序软体 объек� 到预定的目标配置中。这些任务通常面临两个主要挑战：一是软体配置空间的维度太高，二是软体动力学的内置复杂性、非线性和不确定性。为了解决这些挑战，我们提出了一种新的表示策略，可以有效地模型软体 объек� 的状态，并且通过建立和更新两个动态图来模型软体重新排序动力学。我们还提出了一种本地图神经网络（GNN），可以同时模型软体重新排序动力学和推理最佳抓取和放置动作。我们在实验中发现，我们的动态图表示方法可以更高效地模型软体重新排序动力学，并且在多种软体重新排序任务上达到96.3%的Success rate（在实验中）。此外，我们的方法比现状态技术更轻量级，并且在推理时间方面具有60%的缩短。此外，我们还证明了我们的方法在多任务学习场景下表现良好，可以通过精细调整一个关键点探测器来转移到实际应用中。
</details></li>
</ul>
<hr>
<h2 id="Forking-Uncertainties-Reliable-Prediction-and-Model-Predictive-Control-with-Sequence-Models-via-Conformal-Risk-Control"><a href="#Forking-Uncertainties-Reliable-Prediction-and-Model-Predictive-Control-with-Sequence-Models-via-Conformal-Risk-Control" class="headerlink" title="Forking Uncertainties: Reliable Prediction and Model Predictive Control with Sequence Models via Conformal Risk Control"></a>Forking Uncertainties: Reliable Prediction and Model Predictive Control with Sequence Models via Conformal Risk Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10299">http://arxiv.org/abs/2310.10299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Zecchin, Sangwoo Park, Osvaldo Simeone</li>
<li>for: 本文旨在提供一种基于 probabilistic implicit or explicit sequence model 的预测 uncertainty 管理方法，以便在具有复杂动力学和分支轨迹的 cyber-physical systems 中提供可靠性和安全性 garanties。</li>
<li>methods: 本文提出了一种基于 ensemble 的多板轨迹预测（PTS-CRC）方法，该方法可以跨多个预测器生成可靠的预测集，以捕捉异常轨迹的不确定性。此外，PTS-CRC 还可以满足非覆盖性定义的可靠性要求，这使得可以实现更加高效的控制策略。</li>
<li>results: 实验结果表明，PTS-CRC 预测器可以提供更加信息归一化的预测集，以及安全性和质量的控制策略，并且在无线网络中的多种任务中都表现出了更高的返回。<details>
<summary>Abstract</summary>
In many real-world problems, predictions are leveraged to monitor and control cyber-physical systems, demanding guarantees on the satisfaction of reliability and safety requirements. However, predictions are inherently uncertain, and managing prediction uncertainty presents significant challenges in environments characterized by complex dynamics and forking trajectories. In this work, we assume access to a pre-designed probabilistic implicit or explicit sequence model, which may have been obtained using model-based or model-free methods. We introduce probabilistic time series-conformal risk prediction (PTS-CRC), a novel post-hoc calibration procedure that operates on the predictions produced by any pre-designed probabilistic forecaster to yield reliable error bars. In contrast to existing art, PTS-CRC produces predictive sets based on an ensemble of multiple prototype trajectories sampled from the sequence model, supporting the efficient representation of forking uncertainties. Furthermore, unlike the state of the art, PTS-CRC can satisfy reliability definitions beyond coverage. This property is leveraged to devise a novel model predictive control (MPC) framework that addresses open-loop and closed-loop control problems under general average constraints on the quality or safety of the control policy. We experimentally validate the performance of PTS-CRC prediction and control by studying a number of use cases in the context of wireless networking. Across all the considered tasks, PTS-CRC predictors are shown to provide more informative predictive sets, as well as safe control policies with larger returns.
</details>
<details>
<summary>摘要</summary>
在许多实际问题中，预测被用来监控和控制电脑物理系统，需要保证可靠性和安全性要求的满足。然而，预测本身具有不确定性，在复杂动态环境中管理预测不确定性呈现出 significanthallenges。在这项工作中，我们假设有一个预先设计的 probabilistic implicit or explicit sequence model，可能通过模型基于或模型自由方法获得。我们介绍了一种新的后期加拟程序，即 probablistic time series-conformal risk prediction (PTS-CRC)，可以在任何预先设计的 probabilistic forecaster 的预测结果上进行后期加拟，以生成可靠的误差范围。与现有艺术 differencely，PTS-CRC 生成基于多个原型轨迹样本集的预测集，支持高效地表示分支不确定性。此外，PTS-CRC 可以满足超过覆盖率的可靠性定义，这种特性被利用来开发一种基于 average constraints 的新的模型预测控制 (MPC) 框架，可以解决一般平均约束下的开 loop 和关 loop 控制问题。我们通过研究无线网络上的一些用例， validate 了 PTS-CRC 预测和控制的性能。在所有考虑的任务中，PTS-CRC 预测器被证明提供更加有用的预测集，以及安全的控制策略与更大的回报。
</details></li>
</ul>
<hr>
<h2 id="Key-phrase-boosted-unsupervised-summary-generation-for-FinTech-organization"><a href="#Key-phrase-boosted-unsupervised-summary-generation-for-FinTech-organization" class="headerlink" title="Key-phrase boosted unsupervised summary generation for FinTech organization"></a>Key-phrase boosted unsupervised summary generation for FinTech organization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10294">http://arxiv.org/abs/2310.10294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aadit Deshpande, Shreya Goyal, Prateek Nagwanshi, Avinash Tripathy</li>
<li>for: 这篇论文的目的是提出一种基于Action-Object对的自动生成社交媒体摘要方法，以帮助金融科技公司更好地利用社交媒体语言数据，并提供一个外部视角来对consumer behavior进行分析。</li>
<li>methods: 这篇论文使用了NLP技术，特别是意向检测、情感分类和文本概要生成等应用，以处理社交媒体语言数据。它还提出了一种基于Action-Object对的自动生成社交媒体摘要方法，以增强对社交媒体语言数据的分析和利用。</li>
<li>results: 该论文通过对Reddit讨论串的社交媒体语言数据进行分析，并对基于Action-Object对的摘要方法进行评估，并证明了该方法的效果。具体来说，该论文在Context Metrics上表现出了显著的优势，包括Unique words、Action-Object对和名称块的数量。<details>
<summary>Abstract</summary>
With the recent advances in social media, the use of NLP techniques in social media data analysis has become an emerging research direction. Business organizations can particularly benefit from such an analysis of social media discourse, providing an external perspective on consumer behavior. Some of the NLP applications such as intent detection, sentiment classification, text summarization can help FinTech organizations to utilize the social media language data to find useful external insights and can be further utilized for downstream NLP tasks. Particularly, a summary which highlights the intents and sentiments of the users can be very useful for these organizations to get an external perspective. This external perspective can help organizations to better manage their products, offers, promotional campaigns, etc. However, certain challenges, such as a lack of labeled domain-specific datasets impede further exploration of these tasks in the FinTech domain. To overcome these challenges, we design an unsupervised phrase-based summary generation from social media data, using 'Action-Object' pairs (intent phrases). We evaluated the proposed method with other key-phrase based summary generation methods in the direction of contextual information of various Reddit discussion threads, available in the different summaries. We introduce certain "Context Metrics" such as the number of Unique words, Action-Object pairs, and Noun chunks to evaluate the contextual information retrieved from the source text in these phrase-based summaries. We demonstrate that our methods significantly outperform the baseline on these metrics, thus providing a qualitative and quantitative measure of their efficacy. Proposed framework has been leveraged as a web utility portal hosted within Amex.
</details>
<details>
<summary>摘要</summary>
One of the key challenges in exploring these tasks in the FinTech domain is the lack of labeled domain-specific datasets. To overcome this challenge, we have designed an unsupervised phrase-based summary generation method from social media data using 'Action-Object' pairs (intent phrases). We evaluated our method against other key-phrase based summary generation methods in terms of contextual information retrieved from the source text.To evaluate the contextual information, we introduced certain "Context Metrics" such as the number of unique words, Action-Object pairs, and noun chunks. Our method significantly outperformed the baseline on these metrics, providing a qualitative and quantitative measure of its efficacy. This framework has been leveraged as a web utility portal hosted within Amex.
</details></li>
</ul>
<hr>
<h2 id="No-Compromise-in-Solution-Quality-Speeding-Up-Belief-dependent-Continuous-POMDPs-via-Adaptive-Multilevel-Simplification"><a href="#No-Compromise-in-Solution-Quality-Speeding-Up-Belief-dependent-Continuous-POMDPs-via-Adaptive-Multilevel-Simplification" class="headerlink" title="No Compromise in Solution Quality: Speeding Up Belief-dependent Continuous POMDPs via Adaptive Multilevel Simplification"></a>No Compromise in Solution Quality: Speeding Up Belief-dependent Continuous POMDPs via Adaptive Multilevel Simplification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10274">http://arxiv.org/abs/2310.10274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrey Zhitnikov, Ori Sztyglic, Vadim Indelman</li>
<li>for: 这篇论文是关于Continuous POMDPs with general belief-dependent rewards的解决方案。</li>
<li>methods: 本论文使用了一种名为“adaptive multilevel simplification”的方法，具体来说是在给定的信念树和MCTS的基础上实现POMDP的线上规划。这种方法可以快速加速POMDP的规划，而不会失去解决方案的质量。</li>
<li>results: 本论文提出了三种算法来加速Continuous POMDP的规划，其中两种算法（SITH-BSP和LAZY-SITH-BSP）可以在任何信念树构建方法上使用，第三种算法（SITH-PFT）是一种可以适应任何探索技术的任何时间MCTS方法。所有这些算法都能够返回与未加速的算法相同的优化的动作。此外，本论文还提出了一种新的信息论 reward的代价计算方法，该方法可以轻松计算，并且可以通过需求的精细化来紧张化。<details>
<summary>Abstract</summary>
Continuous POMDPs with general belief-dependent rewards are notoriously difficult to solve online. In this paper, we present a complete provable theory of adaptive multilevel simplification for the setting of a given externally constructed belief tree and MCTS that constructs the belief tree on the fly using an exploration technique. Our theory allows to accelerate POMDP planning with belief-dependent rewards without any sacrifice in the quality of the obtained solution. We rigorously prove each theoretical claim in the proposed unified theory. Using the general theoretical results, we present three algorithms to accelerate continuous POMDP online planning with belief-dependent rewards. Our two algorithms, SITH-BSP and LAZY-SITH-BSP, can be utilized on top of any method that constructs a belief tree externally. The third algorithm, SITH-PFT, is an anytime MCTS method that permits to plug-in any exploration technique. All our methods are guaranteed to return exactly the same optimal action as their unsimplified equivalents. We replace the costly computation of information-theoretic rewards with novel adaptive upper and lower bounds which we derive in this paper, and are of independent interest. We show that they are easy to calculate and can be tightened by the demand of our algorithms. Our approach is general; namely, any bounds that monotonically converge to the reward can be easily plugged-in to achieve significant speedup without any loss in performance. Our theory and algorithms support the challenging setting of continuous states, actions, and observations. The beliefs can be parametric or general and represented by weighted particles. We demonstrate in simulation a significant speedup in planning compared to baseline approaches with guaranteed identical performance.
</details>
<details>
<summary>摘要</summary>
CONTINUOUS POMDPs WITH GENERAL BELIEF-DEPENDENT REWARDS ARE DIFFICULT TO SOLVE ONLINE. IN THIS PAPER, WE PRESENT A COMPLETE PROVABLE THEORY OF ADAPTIVE MULTILEVEL SIMPLIFICATION FOR THE SETTING OF A GIVEN EXTERNALLY CONSTRUCTED BELIEF TREE AND MCTS THAT CONSTRUCTS THE BELIEF TREE ON THE FLY USING AN EXPLORATION TECHNIQUE. OUR THEORY ALLOWS FOR ACCELERATING POMDP PLANNING WITH BELIEF-DEPENDENT REWARDS WITHOUT ANY SACRIFICE IN THE QUALITY OF THE OBTAINED SOLUTION. WE RIGOROUSLY PROVE EACH THEORETICAL CLAIM IN THE PROPOSED UNIFIED THEORY. USING THE GENERAL THEORETICAL RESULTS, WE PRESENT THREE ALGORITHMS TO ACCELERATE CONTINUOUS POMDP ONLINE PLANNING WITH BELIEF-DEPENDENT REWARDS. OUR TWO ALGORITHMS, SITH-BSP AND LAZY-SITH-BSP, CAN BE UTILIZED ON TOP OF ANY METHOD THAT CONSTRUCTS A BELIEF TREE EXTERNALLY. THE THIRD ALGORITHM, SITH-PFT, IS ANYTIME MCTS METHOD THAT PERMITS TO PLUG-IN ANY EXPLORATION TECHNIQUE. ALL OUR METHODS ARE GUARANTEED TO RETURN THE SAME OPTIMAL ACTION AS THEIR UNSIMPLIFIED EQUIVALENTS. WE REPLACE THE COSTLY COMPUTATION OF INFORMATION-THEORETIC REWARDS WITH NOVEL ADAPTIVE UPPER AND LOWER BOUNDS WHICH WE DERIVE IN THIS PAPER, AND ARE OF INDEPENDENT INTEREST. WE SHOW THAT THEY ARE EASY TO CALCULATE AND CAN BE TIGHTENED BY THE DEMAND OF OUR ALGORITHMS. OUR APPROACH IS GENERAL; NAMELY, ANY BOUNDS THAT MONOTONICALLY CONVERGE TO THE REWARD CAN BE EASILY PLUGGED-IN TO ACHIEVE SIGNIFICANT SPEEDUP WITHOUT ANY LOSS IN PERFORMANCE. OUR THEORY AND ALGORITHMS SUPPORT THE CHALLENGING SETTING OF CONTINUOUS STATES, ACTIONS, AND OBSERVATIONS. THE BELIEFS CAN BE PARAMETRIC OR GENERAL AND REPRESENTED BY WEIGHTED PARTICLES. WE DEMONSTRATE IN SIMULATION A SIGNIFICANT SPEEDUP IN PLANNING COMPARED TO BASELINE APPROACHES WITH GUARANTEED IDENTICAL PERFORMANCE.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Financial-Service-Promotion-With-Hybrid-Recommender-Systems-at-PicPay"><a href="#Rethinking-Financial-Service-Promotion-With-Hybrid-Recommender-Systems-at-PicPay" class="headerlink" title="Rethinking Financial Service Promotion With Hybrid Recommender Systems at PicPay"></a>Rethinking Financial Service Promotion With Hybrid Recommender Systems at PicPay</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10268">http://arxiv.org/abs/2310.10268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Mendonça, Matheus Santos, André Gonçalves, Yan Almeida</li>
<li>for: 这个研究是为了提高PicPay的金融服务推荐效果。</li>
<li>methods: 这个研究使用了两种推荐算法，Switching Hybrid Recommender System，以提高item推荐的效果。</li>
<li>results: 我们的A&#x2F;B测试显示， Switching Hybrid Recommender System可以提高推荐效果，比默认推荐策略提高3.2%。<details>
<summary>Abstract</summary>
The fintech PicPay offers a wide range of financial services to its 30 million monthly active users, with more than 50 thousand items recommended in the PicPay mobile app. In this scenario, promoting specific items that are strategic to the company can be very challenging. In this work, we present a Switching Hybrid Recommender System that combines two algorithms to effectively promote items without negatively impacting the user's experience. The results of our A/B tests show an uplift of up to 3.2\% when compared to a default recommendation strategy.
</details>
<details>
<summary>摘要</summary>
picPay 提供了一个广泛的金融服务，每月活跃用户达3000万人，app中推荐的商品超过50000个。在这种情况下，推荐特定的商品可以非常具有挑战性。在这份工作中，我们提出了一种交换 гибрид推荐系统，将两种算法结合使用，以有效地推荐商品，不对用户体验造成负面影响。我们的A/B测试结果显示，与默认推荐策略相比，我们的推荐策略可以提高用户增长率高达3.2%。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Arabic-Legal-Rulings-using-Large-Language-Models"><a href="#Prediction-of-Arabic-Legal-Rulings-using-Large-Language-Models" class="headerlink" title="Prediction of Arabic Legal Rulings using Large Language Models"></a>Prediction of Arabic Legal Rulings using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10260">http://arxiv.org/abs/2310.10260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Ammar, Anis Koubaa, Bilel Benjdira, Omar Najar, Serry Sibaee</li>
<li>for: 这研究旨在预测阿拉伯语法庭决定，帮助法官做出决策和帮助律师采取更加细化的战略。</li>
<li>methods: 本研究使用了当前state-of-the-art的大语言模型进行预测，包括LLaMA-7b、JAIS-13b和GPT3.5-turbo等三种基础模型，并使用了三种训练方法：零例学习、一例学习和特定精度调整。此外，研究还评估了对原始阿拉伯文输入文本的摘要和&#x2F;或翻译的效果。</li>
<li>results: 研究发现，所有LLaMA模型的性能很差，而GPT-3.5基于模型在所有模型中表现出色，比其他模型的平均分数高出50%。此外，研究还发现，除了人类评估外，其他所有的评估方法都不可靠，无法正确评估大语言模型在法庭决定预测中的性能。<details>
<summary>Abstract</summary>
In the intricate field of legal studies, the analysis of court decisions is a cornerstone for the effective functioning of the judicial system. The ability to predict court outcomes helps judges during the decision-making process and equips lawyers with invaluable insights, enhancing their strategic approaches to cases. Despite its significance, the domain of Arabic court analysis remains under-explored. This paper pioneers a comprehensive predictive analysis of Arabic court decisions on a dataset of 10,813 commercial court real cases, leveraging the advanced capabilities of the current state-of-the-art large language models. Through a systematic exploration, we evaluate three prevalent foundational models (LLaMA-7b, JAIS-13b, and GPT3.5-turbo) and three training paradigms: zero-shot, one-shot, and tailored fine-tuning. Besides, we assess the benefit of summarizing and/or translating the original Arabic input texts. This leads to a spectrum of 14 model variants, for which we offer a granular performance assessment with a series of different metrics (human assessment, GPT evaluation, ROUGE, and BLEU scores). We show that all variants of LLaMA models yield limited performance, whereas GPT-3.5-based models outperform all other models by a wide margin, surpassing the average score of the dedicated Arabic-centric JAIS model by 50%. Furthermore, we show that all scores except human evaluation are inconsistent and unreliable for assessing the performance of large language models on court decision predictions. This study paves the way for future research, bridging the gap between computational linguistics and Arabic legal analytics.
</details>
<details>
<summary>摘要</summary>
在复杂的法律研究领域中，法庭判决分析是司法系统的基础石头。预测法庭结果可以帮助法官做出决策，并让律师获得价值的洞察，提高他们的战略方法。然而，阿拉伯语法庭分析领域仍然未得到足够的探索。这篇论文探索了10813起商业法庭案例的阿拉伯语法庭判决预测，利用当今最先进的大语言模型。通过系统性的探索，我们评估了三种基础模型（LLaMA-7b、JAIS-13b和GPT3.5-turbo）和三种训练方法（零shot、一shot和tailored fine-tuning）。此外，我们还评估了原始阿拉伯语输入文本的摘要和/或翻译是否有利。这导致了14种模型变体，我们为它们提供了细腻的性能评估，包括人类评估、GPT评估、ROUGE和BLEU分数。我们发现所有LLaMA模型的表现很有限，而GPT-3.5基于模型在所有其他模型之上占据了很大优势，超过了特化于阿拉伯语的JAIS模型的平均分数 by 50%。此外，我们发现除人类评估外，所有其他分数都是不可靠和不一致的，这些分数不适用于评估大语言模型在法庭判决预测中的表现。这篇研究为未来的研究提供了桥梁，将计算语言学和阿拉伯语法律分析相连。
</details></li>
</ul>
<hr>
<h2 id="SGOOD-Substructure-enhanced-Graph-Level-Out-of-Distribution-Detection"><a href="#SGOOD-Substructure-enhanced-Graph-Level-Out-of-Distribution-Detection" class="headerlink" title="SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection"></a>SGOOD: Substructure-enhanced Graph-Level Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10237">http://arxiv.org/abs/2310.10237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Ding, Jieming Shi</li>
<li>for: 本研究旨在提高图像分类中的非标准图像检测性能，即在未知数据分布下检测图像是否属于标准分布或者非标准分布。</li>
<li>methods: 本研究提出了一种基于子结构的图像分类方法，包括建立超Graph的子结构、设计两级图像编码管道以及开发三种图像增强技术来增强表达力。</li>
<li>results: 对10种竞争者进行了广泛的实验，常常超过现有方法，并且在许多图像 Dataset 上表现出了显著的优势。<details>
<summary>Abstract</summary>
Graph-level representation learning is important in a wide range of applications. However, existing graph-level models are generally built on i.i.d. assumption for both training and testing graphs, which is not realistic in an open world, where models can encounter out-of-distribution (OOD) testing graphs that are from different distributions unknown during training. A trustworthy model should not only produce accurate predictions for in-distribution (ID) data, but also detect OOD graphs to avoid unreliable prediction. In this paper, we present SGOOD, a novel graph-level OOD detection framework. We find that substructure differences commonly exist between ID and OOD graphs. Hence, SGOOD explicitly utilizes substructures to learn powerful representations to achieve superior performance. Specifically, we build a super graph of substructures for every graph, and design a two-level graph encoding pipeline that works on both original graphs and super graphs to obtain substructure-enhanced graph representations. To further distinguish ID and OOD graphs, we develop three graph augmentation techniques that preserve substructures and increase expressiveness. Extensive experiments against 10 competitors on numerous graph datasets demonstrate the superiority of SGOOD, often surpassing existing methods by a significant margin. The code is available at https://anonymous.4open.science/r/SGOOD-0958.
</details>
<details>
<summary>摘要</summary>
GRAPH-LEVEL REPRESENTATION LEARNING 是在各种应用中非常重要。然而，现有的 GRAPH-LEVEL 模型通常是基于 i.i.d. 假设，即训练和测试 GRAPH 都是同一个分布，这并不是现实世界中的开放世界， где模型可能会遇到不同分布的测试 GRAPH。一个可靠的模型不仅需要在 ID 数据上生成准确的预测，还需要检测 OOD  GRAPH 以避免不可靠的预测。在这篇论文中，我们提出了 SGOOD，一种新的 GRAPH-LEVEL OOD 检测框架。我们发现了 ID 和 OOD  GRAPH 之间的结构差异，因此 SGOOD 使用substructure来学习强大的表示。具体来说，我们建立了每个 GRAPH 的超graph，并设计了两级图编码管道，以便在原始 GRAPH 和超graph 上获得增强的图表示。为了进一步分别 ID 和 OOD GRAPH，我们开发了三种图增强技术，以保持substructure并提高表达能力。我们对10个竞争对手的实验结果表明，SGOOD 常常超过现有方法，准确率高于95%。代码可以在 <https://anonymous.4open.science/r/SGOOD-0958> 获取。
</details></li>
</ul>
<hr>
<h2 id="Using-Global-Land-Cover-Product-as-Prompt-for-Cropland-Mapping-via-Visual-Foundation-Model"><a href="#Using-Global-Land-Cover-Product-as-Prompt-for-Cropland-Mapping-via-Visual-Foundation-Model" class="headerlink" title="Using Global Land Cover Product as Prompt for Cropland Mapping via Visual Foundation Model"></a>Using Global Land Cover Product as Prompt for Cropland Mapping via Visual Foundation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10219">http://arxiv.org/abs/2310.10219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Tao, Aoran Hu, Rong Xiao, Haifeng Li, Yuze Wang</li>
<li>for: 本研究旨在解决受不同场景Attribute和取景条件影响的 cropland 映射问题，通过提出 “Pretrain+Prompting” 方法，以便在模型理解过程中简化领域适应。</li>
<li>methods: 本研究使用了可访问的全球土地覆盖产品，设计了自动提示（APT）方法，通过在模型推理过程中引入各个示例的个性提示，实现了细化的领域适应过程。</li>
<li>results: 根据两个SUB-METER cropland 数据集的实验结果，提出的 “Pretrain+Prompting” 方法在Remote sensing 领域的 cropland 映射问题中表现了比传统supervised learning和精度调整方法更高的性能。<details>
<summary>Abstract</summary>
Data-driven deep learning methods have shown great potential in cropland mapping. However, due to multiple factors such as attributes of cropland (topography, climate, crop type) and imaging conditions (viewing angle, illumination, scale), croplands under different scenes demonstrate a great domain gap. This makes it difficult for models trained in the specific scenes to directly generalize to other scenes. A common way to handle this problem is through the "Pretrain+Fine-tuning" paradigm. Unfortunately, considering the variety of features of cropland that are affected by multiple factors, it is hardly to handle the complex domain gap between pre-trained data and target data using only sparse fine-tuned samples as general constraints. Moreover, as the number of model parameters grows, fine-tuning is no longer an easy and low-cost task. With the emergence of prompt learning via visual foundation models, the "Pretrain+Prompting" paradigm redesigns the optimization target by introducing individual prompts for each single sample. This simplifies the domain adaption from generic to specific scenes during model reasoning processes. Therefore, we introduce the "Pretrain+Prompting" paradigm to interpreting cropland scenes and design the auto-prompting (APT) method based on freely available global land cover product. It can achieve a fine-grained adaptation process from generic scenes to specialized cropland scenes without introducing additional label costs. To our best knowledge, this work pioneers the exploration of the domain adaption problems for cropland mapping under prompt learning perspectives. Our experiments using two sub-meter cropland datasets from southern and northern China demonstrated that the proposed method via visual foundation models outperforms traditional supervised learning and fine-tuning approaches in the field of remote sensing.
</details>
<details>
<summary>摘要</summary>
“数据驱动深度学习方法在耕地地图中表现出了很大的潜力。然而，由于耕地特性（地形、气候、作物种）以及捕获条件（观察角度、照明、比例）的多种因素，耕地不同场景之间存在巨大的领域差异。这使得使用特定场景的训练数据直接适应其他场景变得困难。通常，使用“Pretrain+Fine-tuning”模式来解决这个问题。然而，考虑到耕地特性的多种影响，使用只有稀疏的精度适应样本作为通用约束是不充分的。此外，随着模型参数的增加，精度适应变得不是易于进行的低成本任务。随着视觉基础模型的出现，“Pretrain+Prompting”模式可以重新设定优化目标，通过引入每个样本的个性提示来简化领域适应。因此，我们提出了基于自由可用的全球土地覆盖产品的自动提示（APT）方法，可以实现不受预先标注的场景适应过程。我们的实验使用南方和北方中国的两个半米耕地数据集证明了，与传统的超级学习和精度适应方法相比，我们的方法在远程感知领域中表现出了更好的性能。”
</details></li>
</ul>
<hr>
<h2 id="Large-Models-for-Time-Series-and-Spatio-Temporal-Data-A-Survey-and-Outlook"><a href="#Large-Models-for-Time-Series-and-Spatio-Temporal-Data-A-Survey-and-Outlook" class="headerlink" title="Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"></a>Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10196">http://arxiv.org/abs/2310.10196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qingsongedu/awesome-timeseries-spatiotemporal-lm-llm">https://github.com/qingsongedu/awesome-timeseries-spatiotemporal-lm-llm</a></li>
<li>paper_authors: Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, Shirui Pan, Vincent S. Tseng, Yu Zheng, Lei Chen, Hui Xiong</li>
<li>for: 本研究主要是为了对时间序列和空间时间数据进行分析和挖掘，以便更好地利用这些数据中含的丰富信息，并为各种应用领域提供支持。</li>
<li>methods: 本研究使用大量语言和其他基础模型，对时间序列和空间时间数据进行分析和挖掘，并提供了一个完整的和最新的评论，涵盖四个关键方面：数据类型、模型类别、模型范围和应用领域&#x2F;任务。</li>
<li>results: 本研究提供了一个完整的和最新的评论，涵盖大型模型在时间序列和空间时间数据分析中的应用和发展，并提供了丰富的资源，包括数据集、模型资产和有用工具，以便开发应用和进行进一步的研究。<details>
<summary>Abstract</summary>
Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to equip practitioners with the knowledge to develop applications and further research in this underexplored domain. We primarily categorize the existing literature into two major clusters: large models for time series analysis (LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further classify research based on model scopes (i.e., general vs. domain-specific) and application areas/tasks. We also provide a comprehensive collection of pertinent resources, including datasets, model assets, and useful tools, categorized by mainstream applications. This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.
</details>
<details>
<summary>摘要</summary>
现代数据中，时序数据和空间时序数据具有广泛的应用，它们捕捉了动态系统的测量结果，并由物理和虚拟感知器生成了庞大量数据。分析这些数据类型是利用它们含义的关键，因此在多种下游任务中具有重要意义。最新的大语言和其他基础模型的发展，使得这些模型在时序数据和空间时序数据挖掘中得到广泛的应用。这些方法不仅可以在多种领域中提高模式识别和理解，而且为人工通用智能做好了准备。在本综述中，我们提供了一个完整和最新的时序数据和空间时序数据大模型综述，涵盖四个关键方面：数据类型、模型类别、模型范围和应用领域/任务。我们的目标是为实践者提供开发应用和进一步研究的知识。我们将现有文献分为两个主要群组：时序数据分析大模型（LM4TS）和空间时序数据挖掘大模型（LM4STD）。基于这两个群组，我们进一步分类研究根据模型范围（一般 vs.域特定）和应用领域/任务。此外，我们还提供了一份完整的相关资源，包括数据集、模型资产和有用工具，按照主流应用分类。这篇综述汇集了最新的大模型中心研究的进展，强调了它们的基础、当前进展、实际应用、资源储备和未来研究机遇。
</details></li>
</ul>
<hr>
<h2 id="Battle-of-the-Large-Language-Models-Dolly-vs-LLaMA-vs-Vicuna-vs-Guanaco-vs-Bard-vs-ChatGPT-–-A-Text-to-SQL-Parsing-Comparison"><a href="#Battle-of-the-Large-Language-Models-Dolly-vs-LLaMA-vs-Vicuna-vs-Guanaco-vs-Bard-vs-ChatGPT-–-A-Text-to-SQL-Parsing-Comparison" class="headerlink" title="Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT – A Text-to-SQL Parsing Comparison"></a>Battle of the Large Language Models: Dolly vs LLaMA vs Vicuna vs Guanaco vs Bard vs ChatGPT – A Text-to-SQL Parsing Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10190">http://arxiv.org/abs/2310.10190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Sun, Yuchen Zhang, Jiahuan Yan, Yuze Gao, Donovan Ong, Bin Chen, Jian Su</li>
<li>for: 评估大型自然语言模型（LLM）在文本转换SQL解析方面的表现，以帮助研究人员更好地了解这些模型的实际性能。</li>
<li>methods: 对六种流行的大型自然语言模型进行系统性的评估，使用九个benchmark数据集和五种提示策略进行测试，包括零shot和几shot情况。</li>
<li>results: 发现开源模型在文本转换SQL解析方面的性能落后于关闭源模型如GPT-3.5，表明需要进一步的研究以减少这些模型之间的性能差距。<details>
<summary>Abstract</summary>
The success of ChatGPT has ignited an AI race, with researchers striving to develop new large language models (LLMs) that can match or surpass the language understanding and generation abilities of commercial ones. In recent times, a number of models have emerged, claiming performance near that of GPT-3.5 or GPT-4 through various instruction-tuning methods. As practitioners of Text-to-SQL parsing, we are grateful for their valuable contributions to open-source research. However, it is important to approach these claims with a sense of scrutiny and ascertain the actual effectiveness of these models. Therefore, we pit six popular large language models against each other, systematically evaluating their Text-to-SQL parsing capability on nine benchmark datasets with five different prompting strategies, covering both zero-shot and few-shot scenarios. Regrettably, the open-sourced models fell significantly short of the performance achieved by closed-source models like GPT-3.5, highlighting the need for further work to bridge the performance gap between these models.
</details>
<details>
<summary>摘要</summary>
成功的ChatGPT引燃了一场AI竞赛，研究人员努力开发新的大型自然语言模型（LLMs），以达到或超越商业模型的语言理解和生成能力。最近，一些模型宣称在不同的指导方法下达到GPT-3.5或GPT-4的性能。作为文本转SQL解析的实践者，我们感谢这些开源研究的有价值贡献。然而，我们应该对这些声明进行严格的评估，以确定这些模型的实际效果。因此，我们将六种流行的大型自然语言模型进行比较测试，系统地评估这些模型在九个benchmark datasets上的文本转SQL解析能力，使用五种不同的提示策略，包括零shot和几shot场景。惜亏，开源的模型在closed-source模型GPT-3.5的性能上表现出了明显的差距，这 highlights the need for further work to bridge the performance gap between these models。
</details></li>
</ul>
<hr>
<h2 id="Continual-Generalized-Intent-Discovery-Marching-Towards-Dynamic-and-Open-world-Intent-Recognition"><a href="#Continual-Generalized-Intent-Discovery-Marching-Towards-Dynamic-and-Open-world-Intent-Recognition" class="headerlink" title="Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition"></a>Continual Generalized Intent Discovery: Marching Towards Dynamic and Open-world Intent Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10184">http://arxiv.org/abs/2310.10184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songxiaoshuai/CGID">https://github.com/songxiaoshuai/CGID</a></li>
<li>paper_authors: Xiaoshuai Song, Yutao Mou, Keqing He, Yueyan Qiu, Pei Wang, Weiran Xu</li>
<li>for: 这篇论文目标是解决在不同数据流中进行动态意图发现，以及在开放世界中实现动态意图识别。</li>
<li>methods: 该论文提出了一种新的任务 named Continual Generalized Intent Discovery (CGID), 它通过不断地从动态OOD数据流中发现新意图，然后逐渐将其添加到分类器中，几乎不需要之前的数据。</li>
<li>results: 论文提出了一种名为Prototype-guided Learning with Replay and Distillation (PLRD)的方法，可以实现CGID任务。该方法通过类prototype进行启动新意图发现，并通过数据重播和特征储存来保持新和旧意图的平衡。<details>
<summary>Abstract</summary>
In a practical dialogue system, users may input out-of-domain (OOD) queries. The Generalized Intent Discovery (GID) task aims to discover OOD intents from OOD queries and extend them to the in-domain (IND) classifier. However, GID only considers one stage of OOD learning, and needs to utilize the data in all previous stages for joint training, which limits its wide application in reality. In this paper, we introduce a new task, Continual Generalized Intent Discovery (CGID), which aims to continuously and automatically discover OOD intents from dynamic OOD data streams and then incrementally add them to the classifier with almost no previous data, thus moving towards dynamic intent recognition in an open world. Next, we propose a method called Prototype-guided Learning with Replay and Distillation (PLRD) for CGID, which bootstraps new intent discovery through class prototypes and balances new and old intents through data replay and feature distillation. Finally, we conduct detailed experiments and analysis to verify the effectiveness of PLRD and understand the key challenges of CGID for future research.
</details>
<details>
<summary>摘要</summary>
在实际对话系统中，用户可能输入过域 (OOD) 查询。通用意图发现 (GID) 任务目标是从 OOD 查询中发现 OOD 意图并将其扩展到内域 (IND) 分类器。但 GID 只考虑了一个阶段的 OOD 学习，需要在所有前一阶段的数据上进行联合训练，这限制了其在实际应用中的广泛应用。在本文中，我们介绍了一个新的任务：不断总结化通用意图发现 (CGID)，它目标是从动态 OOD 数据流中不断发现 OOD 意图，然后在几乎没有先前数据的情况下，逐步添加它们到分类器中，从而逐步实现动态意图认知在开放世界中。接着，我们提出了一种方法 called Prototype-guided Learning with Replay and Distillation (PLRD)，它通过类型概念引导新意向发现，并通过数据重播和特征硬化平衡新和旧意图。最后，我们进行了详细的实验和分析，以证明 PLRD 的效果和 CGID 的关键挑战，以便未来研究。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Meet-Open-World-Intent-Discovery-and-Recognition-An-Evaluation-of-ChatGPT"><a href="#Large-Language-Models-Meet-Open-World-Intent-Discovery-and-Recognition-An-Evaluation-of-ChatGPT" class="headerlink" title="Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT"></a>Large Language Models Meet Open-World Intent Discovery and Recognition: An Evaluation of ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10176">http://arxiv.org/abs/2310.10176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songxiaoshuai/OOD-Evaluation">https://github.com/songxiaoshuai/OOD-Evaluation</a></li>
<li>paper_authors: Xiaoshuai Song, Keqing He, Pei Wang, Guanting Dong, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu</li>
<li>for: 本研究旨在评估ChatGPT在非预期意权（OOD）发现和总结扩展（GID）任务中的能力。</li>
<li>methods: 本研究使用了ChatGPT进行OOD意权发现和GID任务，并对其进行了评估。</li>
<li>results: ChatGPT在零shot设定下表现出了一致的优势，但与精心定制的模型相比，其仍然处于劣势。通过一系列的分析实验，本研究揭示了LLMs在扩展OOD意权时所面临的挑战，并提供了未来研究的指导。<details>
<summary>Abstract</summary>
The tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extent OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.
</details>
<details>
<summary>摘要</summary>
Tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extend OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.Here's the translation in Traditional Chinese:Tasks of out-of-domain (OOD) intent discovery and generalized intent discovery (GID) aim to extend a closed intent classifier to open-world intent sets, which is crucial to task-oriented dialogue (TOD) systems. Previous methods address them by fine-tuning discriminative models. Recently, although some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, it is still unclear for the ability of ChatGPT to discover and incrementally extend OOD intents. In this paper, we comprehensively evaluate ChatGPT on OOD intent discovery and GID, and then outline the strengths and weaknesses of ChatGPT. Overall, ChatGPT exhibits consistent advantages under zero-shot settings, but is still at a disadvantage compared to fine-tuned models. More deeply, through a series of analytical experiments, we summarize and discuss the challenges faced by LLMs including clustering, domain-specific understanding, and cross-domain in-context learning scenarios. Finally, we provide empirical guidance for future directions to address these challenges.
</details></li>
</ul>
<hr>
<h2 id="Analyzing-An-After-Sales-Service-Process-Using-Object-Centric-Process-Mining-A-Case-Study"><a href="#Analyzing-An-After-Sales-Service-Process-Using-Object-Centric-Process-Mining-A-Case-Study" class="headerlink" title="Analyzing An After-Sales Service Process Using Object-Centric Process Mining: A Case Study"></a>Analyzing An After-Sales Service Process Using Object-Centric Process Mining: A Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10174">http://arxiv.org/abs/2310.10174</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyunam Park, Sevde Aydin, Cuneyt Ugur, Wil M. P. van der Aalst</li>
<li>for: 本研究旨在探讨对象центри的过程挖掘技术的应用，以帮助实际操作场景中的业务进程优化。</li>
<li>methods: 本研究使用了对象центри的过程挖掘技术，通过对 approximately 65,000 个事件的分析，揭示了这种技术在实际操作场景中的应用前景和优势。</li>
<li>results: 研究发现，对象центри的过程挖掘技术可以更好地捕捉实际操作场景中的企业过程细节，提供更加全面和深入的业务进程nderstandings，帮助企业实现更好的操作优化。<details>
<summary>Abstract</summary>
Process mining, a technique turning event data into business process insights, has traditionally operated on the assumption that each event corresponds to a singular case or object. However, many real-world processes are intertwined with multiple objects, making them object-centric. This paper focuses on the emerging domain of object-centric process mining, highlighting its potential yet underexplored benefits in actual operational scenarios. Through an in-depth case study of Borusan Cat's after-sales service process, this study emphasizes the capability of object-centric process mining to capture entangled business process details. Utilizing an event log of approximately 65,000 events, our analysis underscores the importance of embracing this paradigm for richer business insights and enhanced operational improvements.
</details>
<details>
<summary>摘要</summary>
<TRANSLATE_TEXT> Process mining, a technique turning event data into business process insights, has traditionally operated on the assumption that each event corresponds to a singular case or object. However, many real-world processes are intertwined with multiple objects, making them object-centric. This paper focuses on the emerging domain of object-centric process mining, highlighting its potential yet underexplored benefits in actual operational scenarios. Through an in-depth case study of Borusan Cat's after-sales service process, this study emphasizes the capability of object-centric process mining to capture entangled business process details. Utilizing an event log of approximately 65,000 events, our analysis underscores the importance of embracing this paradigm for richer business insights and enhanced operational improvements. </TRANSLATE_TEXT> traducción al chino simplificado:<SYS><TRANSLATE_TEXT> 过程挖掘，一种将事件数据转化为业务过程智能，传统上假设每个事件对应一个单一的案例或对象。然而，现实世界中许多过程都与多个对象紧密相连，使得它们变成了中心式的。这篇论文关注到在 объекo-中心的过程挖掘领域的出现，强调其在实际运营场景中的可能尚未得到充分利用的优点。通过对博рус安猫后售服务过程的深入探讨，本研究强调了中心式过程挖掘的能力来捕捉互相紧密相连的业务过程细节。通过使用约65,000个事件的日志分析，我们的研究强调了接受这种思想的重要性，以获得更加丰富的商业智能和改进运营效率。 </TRANSLATE_TEXT>
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Knowledge-Distillation-for-Efficient-Deep-Reinforcement-Learning-in-Resource-Constrained-Environments"><a href="#Leveraging-Knowledge-Distillation-for-Efficient-Deep-Reinforcement-Learning-in-Resource-Constrained-Environments" class="headerlink" title="Leveraging Knowledge Distillation for Efficient Deep Reinforcement Learning in Resource-Constrained Environments"></a>Leveraging Knowledge Distillation for Efficient Deep Reinforcement Learning in Resource-Constrained Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10170">http://arxiv.org/abs/2310.10170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paopaolin/papercode/tree/main/MENGGUANLIN_papercode/combine%20V1">https://github.com/paopaolin/papercode/tree/main/MENGGUANLIN_papercode/combine%20V1</a></li>
<li>paper_authors: Guanlin Meng</li>
<li>for: 这篇论文旨在探索深度强化学习（DRL）与知识传播（KD）的组合，以实现将深度模型的计算负载减轻，保持性能。</li>
<li>methods: 这篇论文使用了多种DRL算法的知识传播，并研究了这些知识传播的影响。</li>
<li>results: 这篇论文的研究结果显示，通过将DRL算法与KD技术组合使用，可以开发出较快速、较具有 Computational efficiency 的DRL模型。<details>
<summary>Abstract</summary>
This paper aims to explore the potential of combining Deep Reinforcement Learning (DRL) with Knowledge Distillation (KD) by distilling various DRL algorithms and studying their distillation effects. By doing so, the computational burden of deep models could be reduced while maintaining the performance. The primary objective is to provide a benchmark for evaluating the performance of different DRL algorithms that have been refined using KD techniques. By distilling these algorithms, the goal is to develop efficient and fast DRL models. This research is expected to provide valuable insights that can facilitate further advancements in this promising direction. By exploring the combination of DRL and KD, this work aims to promote the development of models that require fewer GPU resources, learn more quickly, and make faster decisions in complex environments. The results of this research have the capacity to significantly advance the field of DRL and pave the way for the future deployment of resource-efficient, decision-making intelligent systems.
</details>
<details>
<summary>摘要</summary>
本研究旨在探索将深度束缚学习（DRL）与知识传递（KD）相结合，通过传递多种DRL算法，研究其传递效果。这可以减少深度模型的计算负担，保持性能。研究的主要目标是为不同DRL算法提供评估性能的标准准例。通过传递这些算法，目标是开发高效快速的DRL模型。这项研究预期会为这个Promising direction提供有价值的发现，促进DRL领域的进一步发展。通过探索DRL和KD的结合，这项研究期望开发需要 fewer GPU资源、快速学习、在复杂环境中做出快速决策的模型。研究结果具有提高DRL领域的前景，并为未来部署资源有效的决策智能系统铺平道路的潜在性。
</details></li>
</ul>
<hr>
<h2 id="DemoNSF-A-Multi-task-Demonstration-based-Generative-Framework-for-Noisy-Slot-Filling-Task"><a href="#DemoNSF-A-Multi-task-Demonstration-based-Generative-Framework-for-Noisy-Slot-Filling-Task" class="headerlink" title="DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task"></a>DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10169">http://arxiv.org/abs/2310.10169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongguanting/Demo-NSF">https://github.com/dongguanting/Demo-NSF</a></li>
<li>paper_authors: Guanting Dong, Tingfeng Hui, Zhuoma GongQue, Jinxu Zhao, Daichi Guo, Gang Zhao, Keqing He, Weiran Xu</li>
<li>for: 提高生成框架在实际对话场景中的泛化能力，解决它们在输入干扰时的通用化问题。</li>
<li>methods: 提出了多任务示范生成框架，名为DemoNSF，并引入了三种含有干扰信息的辅助任务，即干扰恢复（NR）、随机覆盖（RM）和混合识别（HD），以便在不同粒度上捕捉输入干扰的Semantic结构信息。</li>
<li>results: 在两个标准 benchmark 上， DemoNSF 比所有基eline方法表现出色，并实现了强大的泛化性。进一步的分析提供了生成框架在实践中的指导。<details>
<summary>Abstract</summary>
Recently, prompt-based generative frameworks have shown impressive capabilities in sequence labeling tasks. However, in practical dialogue scenarios, relying solely on simplistic templates and traditional corpora presents a challenge for these methods in generalizing to unknown input perturbations. To address this gap, we propose a multi-task demonstration based generative framework for noisy slot filling, named DemoNSF. Specifically, we introduce three noisy auxiliary tasks, namely noisy recovery (NR), random mask (RM), and hybrid discrimination (HD), to implicitly capture semantic structural information of input perturbations at different granularities. In the downstream main task, we design a noisy demonstration construction strategy for the generative framework, which explicitly incorporates task-specific information and perturbed distribution during training and inference. Experiments on two benchmarks demonstrate that DemoNSF outperforms all baseline methods and achieves strong generalization. Further analysis provides empirical guidance for the practical application of generative frameworks. Our code is released at https://github.com/dongguanting/Demo-NSF.
</details>
<details>
<summary>摘要</summary>
最近，基于提示的生成框架在序列标注任务中表现出了很好的能力。然而，在实际对话场景中，仅仅依靠简单的模板和传统词汇库是生成方法在处理未知输入干扰时的挑战。为解决这个差距，我们提议一种多任务生成框架 для噪声插值，名为 DemoNSF。我们在这个框架中引入了三种噪声辅助任务，即噪声恢复（NR）、随机面（RM）和混合识别（HD），以隐式地捕捉输入干扰的 semantic 结构信息。在下游主任务中，我们设计了一种噪声示例建构策略，用于在训练和推断过程中显式地包含任务特定的信息和干扰分布。实验结果表明， DemoNSF 在两个标准 benchmark 上都高于所有基准方法，并且具有强大的泛化能力。进一步的分析提供了实践应用 generative 框架的指导。我们的代码在 GitHub 上发布，地址为 <https://github.com/dongguanting/Demo-NSF>。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Algorithm-for-Advanced-Level-3-Inverse-Modeling-of-Silicon-Carbide-Power-MOSFET-Devices"><a href="#Deep-Learning-Algorithm-for-Advanced-Level-3-Inverse-Modeling-of-Silicon-Carbide-Power-MOSFET-Devices" class="headerlink" title="Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices"></a>Deep Learning Algorithm for Advanced Level-3 Inverse-Modeling of Silicon-Carbide Power MOSFET Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17657">http://arxiv.org/abs/2310.17657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Massimo Orazio Spata, Sebastiano Battiato, Alessandro Ortis, Francesco Rundo, Michele Calabretta, Carmelo Pino, Angelo Messina</li>
<li>for: 这个论文是为了提取力Field-Effect Transistor (SiC Power MOS)的物理参数而设计的深度学习方法。</li>
<li>methods: 该方法使用深度学习算法来训练Device的参数预测。</li>
<li>results: 实验结果表明，该方法可以有效地重构SiC Power MOS的物理参数，包括晶圆长度。<details>
<summary>Abstract</summary>
Inverse modelling with deep learning algorithms involves training deep architecture to predict device's parameters from its static behaviour. Inverse device modelling is suitable to reconstruct drifted physical parameters of devices temporally degraded or to retrieve physical configuration. There are many variables that can influence the performance of an inverse modelling method. In this work the authors propose a deep learning method trained for retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET (SiC Power MOS). The SiC devices are used in applications where classical silicon devices failed due to high-temperature or high switching capability. The key application of SiC power devices is in the automotive field (i.e. in the field of electrical vehicles). Due to physiological degradation or high-stressing environment, SiC Power MOS shows a significant drift of physical parameters which can be monitored by using inverse modelling. The aim of this work is to provide a possible deep learning-based solution for retrieving physical parameters of the SiC Power MOSFET. Preliminary results based on the retrieving of channel length of the device are reported. Channel length of power MOSFET is a key parameter involved in the static and dynamic behaviour of the device. The experimental results reported in this work confirmed the effectiveness of a multi-layer perceptron designed to retrieve this parameter.
</details>
<details>
<summary>摘要</summary>
倒推模型使用深度学习算法来训练深度架构，以预测设备的参数从其静态行为中。倒推设备模型适用于重构过时的物理参数或恢复物理配置。倒推模型的性能有很多因素的影响。在这项工作中，作者提出了一种深度学习方法，用于重 Retrieving physical parameters of Level-3 model of Power Silicon-Carbide MOSFET (SiC Power MOS). SiC设备在高温或高开关能力应用场景中被广泛使用，因此SiC Power MOS在高温或高压力环境中会显著偏移物理参数，这可以通过倒推模型进行监测。本工作的目标是提供一种可能的深度学习基于解决方案，以重 Retrieving physical parameters of SiC Power MOSFET。初步结果基于设备渠道长度的重构被报告。渠道长度是SiC Power MOS的静态和动态行为中关键参数之一。实验结果表明，使用多层感知器来重构这个参数是有效的。
</details></li>
</ul>
<hr>
<h2 id="Character-LLM-A-Trainable-Agent-for-Role-Playing"><a href="#Character-LLM-A-Trainable-Agent-for-Role-Playing" class="headerlink" title="Character-LLM: A Trainable Agent for Role-Playing"></a>Character-LLM: A Trainable Agent for Role-Playing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10158">http://arxiv.org/abs/2310.10158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/choosewhatulike/trainable-agents">https://github.com/choosewhatulike/trainable-agents</a></li>
<li>paper_authors: Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu</li>
<li>for: 研究是使用大型自然语言模型（LLM）作为代理人类模拟人类行为的能力。</li>
<li>methods: 我们提出了一种方法，即编辑人物profile和经验，以允许LLM模型成为特定人物。</li>
<li>results: 我们在测试场景中训练了agent并评估其能否记忆和表现出人物的特点和经验。实验结果表明了有趣的观察，有助于建立未来的人类模拟。<details>
<summary>Abstract</summary>
Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textit{memorize} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以作为代理人对人类行为进行模拟，因为它具有强大的理解人类指令和生成文本能力。这种能力让我们感到是否可以使LMLM模型模拟出更高一层的人类形式。因此，我们想要将特定人物的资料 Profiling、经验和情感状态传入LMLM模型，以模拟出该人物的行为。在这个研究中，我们提出了Character-LLM，它可以教导LMLM模型以特定人物的形式行为。我们的方法是将特定人物的资料编译为LMLM模型的体验，然后训练这些模型成为特定人物的人工模拟。为了评估我们的方法的有效性，我们建立了一个测试场景，让训练好的代理人回答问题，以判断代理人是否将记忆其角色和体验。实验结果给出了有趣的观察，帮助我们建立未来的人类模拟。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Workload-Distribution-for-Accuracy-aware-DNN-Inference-on-Collaborative-Edge-Platforms"><a href="#Adaptive-Workload-Distribution-for-Accuracy-aware-DNN-Inference-on-Collaborative-Edge-Platforms" class="headerlink" title="Adaptive Workload Distribution for Accuracy-aware DNN Inference on Collaborative Edge Platforms"></a>Adaptive Workload Distribution for Accuracy-aware DNN Inference on Collaborative Edge Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10157">http://arxiv.org/abs/2310.10157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zain Taufique, Antonio Miele, Pasi Liljeberg, Anil Kanduri</li>
<li>for: 加速深度学习模型（DNN）的推理过程，通过分布工作负载到一群协作的边缘节点。</li>
<li>methods: 提议适应性工作负载分布策略，同时考虑边缘设备的差异性和深度学习模型的精度和性能要求。</li>
<li>results: 测试了我们的方法在一个包括Odroid XU4、Raspberry Pi4和Jetson Nano板的边缘集群上，与状态艺术工作负载分布策略相比，实现了平均提高41.52%的性能和5.2%的输出精度。<details>
<summary>Abstract</summary>
DNN inference can be accelerated by distributing the workload among a cluster of collaborative edge nodes. Heterogeneity among edge devices and accuracy-performance trade-offs of DNN models present a complex exploration space while catering to the inference performance requirements. In this work, we propose adaptive workload distribution for DNN inference, jointly considering node-level heterogeneity of edge devices, and application-specific accuracy and performance requirements. Our proposed approach combinatorially optimizes heterogeneity-aware workload partitioning and dynamic accuracy configuration of DNN models to ensure performance and accuracy guarantees. We tested our approach on an edge cluster of Odroid XU4, Raspberry Pi4, and Jetson Nano boards and achieved an average gain of 41.52% in performance and 5.2% in output accuracy as compared to state-of-the-art workload distribution strategies.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Theory-of-Mind-for-Multi-Agent-Collaboration-via-Large-Language-Models"><a href="#Theory-of-Mind-for-Multi-Agent-Collaboration-via-Large-Language-Models" class="headerlink" title="Theory of Mind for Multi-Agent Collaboration via Large Language Models"></a>Theory of Mind for Multi-Agent Collaboration via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10701">http://arxiv.org/abs/2310.10701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara</li>
<li>for: 本研究评估基于大型自然语言模型（LLM）的多智能代理人在多智能协作文本游戏中的表现，并与多智能奖励学习（MARL）和规划基eline进行比较。</li>
<li>methods: 本研究使用LLM来实现多智能协作，并对其表现进行评估。研究还 explore了使用explicit belief state representation来改善LLM的规划优化和任务状态幻觉问题。</li>
<li>results: 研究发现LLM-based agents exhibit emergent collaborative behaviors and high-order Theory of Mind capabilities，但受到长期context管理和任务状态幻觉的限制。使用explicit belief state representation可以提高任务表现和理解能力。<details>
<summary>Abstract</summary>
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在理解和规划方面表现出色，但它们在多代合作中的能力仍然尚未得到充分探索。这个研究评估了基于 LLM 的代理人在多代合作文本游戏中的理论心理推理任务表现，与多代征学习（MARL）和规划基eline相比较。我们发现 LLM 基于代理人在合作行为中发展出了轻度的协力行为和高级理论心理能力。我们的结果显示 LLM 基于代理人在规划优化方面存在长时间前景管理和任务状态幻觉的系统性问题。我们探索了使用明确的信仰状态表示来缓和这些问题，发现这可以提高任务表现和 LLM 基于代理人的理论心理推理精度。
</details></li>
</ul>
<hr>
<h2 id="Recursive-Segmentation-Living-Image-An-eXplainable-AI-XAI-Approach-for-Computing-Structural-Beauty-of-Images-or-the-Livingness-of-Space"><a href="#Recursive-Segmentation-Living-Image-An-eXplainable-AI-XAI-Approach-for-Computing-Structural-Beauty-of-Images-or-the-Livingness-of-Space" class="headerlink" title="Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space"></a>Recursive Segmentation Living Image: An eXplainable AI (XAI) Approach for Computing Structural Beauty of Images or the Livingness of Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10149">http://arxiv.org/abs/2310.10149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Qianxiang, Bin Jiang</li>
<li>for: 这项研究探讨了一种对图像美感评价的 объек Oriented 计算方法，即“结构美”。通过使用 SAM 模型，我们提出了一种基于再嵌套分割的方法，可以更准确地捕捉图像中的更细grained 结构。</li>
<li>methods: 我们使用 SAM 模型进行再嵌套分割，并将结构重建为层次结构，从而获得更加准确的结构量和层次结构。</li>
<li>results: 我们的方法可以准确地分割出图像中的意义 objects，包括树、建筑和窗户等，以及抽象的画作中的子结构。我们的计算结果与人类视觉评价相一致，并且在不同的颜色空间中进行评价时也能够保持一定的一致性。<details>
<summary>Abstract</summary>
This study introduces the concept of "structural beauty" as an objective computational approach for evaluating the aesthetic appeal of images. Through the utilization of the Segment anything model (SAM), we propose a method that leverages recursive segmentation to extract finer-grained substructures. Additionally, by reconstructing the hierarchical structure, we obtain a more accurate representation of substructure quantity and hierarchy. This approach reproduces and extends our previous research, allowing for the simultaneous assessment of Livingness in full-color images without the need for grayscale conversion or separate computations for foreground and background Livingness. Furthermore, the application of our method to the Scenic or Not dataset, a repository of subjective scenic ratings, demonstrates a high degree of consistency with subjective ratings in the 0-6 score range. This underscores that structural beauty is not solely a subjective perception, but a quantifiable attribute accessible through objective computation. Through our case studies, we have arrived at three significant conclusions. 1) our method demonstrates the capability to accurately segment meaningful objects, including trees, buildings, and windows, as well as abstract substructures within paintings. 2) we observed that the clarity of an image impacts our computational results; clearer images tend to yield higher Livingness scores. However, for equally blurry images, Livingness does not exhibit a significant reduction, aligning with human visual perception. 3) our approach fundamentally differs from methods employing Convolutional Neural Networks (CNNs) for predicting image scores. Our method not only provides computational results but also offers transparency and interpretability, positioning it as a novel avenue in the realm of Explainable AI (XAI).
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Our method demonstrates the capability to accurately segment meaningful objects, including trees, buildings, and windows, as well as abstract substructures within paintings.2. We observed that the clarity of an image impacts our computational results; clearer images tend to yield higher Livingness scores. However, for equally blurry images, Livingness does not exhibit a significant reduction, aligning with human visual perception.3. Our approach fundamentally differs from methods employing Convolutional Neural Networks (CNNs) for predicting image scores. Our method not only provides computational results but also offers transparency and interpretability, positioning it as a novel avenue in the realm of Explainable AI (XAI).</details></li>
</ol>
<hr>
<h2 id="LoBaSS-Gauging-Learnability-in-Supervised-Fine-tuning-Data"><a href="#LoBaSS-Gauging-Learnability-in-Supervised-Fine-tuning-Data" class="headerlink" title="LoBaSS: Gauging Learnability in Supervised Fine-tuning Data"></a>LoBaSS: Gauging Learnability in Supervised Fine-tuning Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13008">http://arxiv.org/abs/2310.13008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Zhou, Tingkai Liu, Qianli Ma, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang</li>
<li>for: 本研究的目的是提出一种基于模型学习能力的超级vised fine-tuning数据选择方法，以便在模型精度和学习效率之间寻找优质平衡。</li>
<li>methods: 本研究使用的方法是基于损失函数的SFT数据选择方法（LoBaSS），该方法根据模型在预训练阶段所学习的能力来选择合适的SFT数据，以便提高模型的精度和学习效率。</li>
<li>results: 实验结果表明，使用LoBaSS方法可以在仅6%的全部训练数据量下，超越全数据 Fine-tuning，并在16.7%的数据量下具有同样的精度和学习效率。这表明LoBaSS方法可以在不同领域中协调模型的能力，以达到优质的精度和学习效率。<details>
<summary>Abstract</summary>
Supervised Fine-Tuning (SFT) serves as a crucial phase in aligning Large Language Models (LLMs) to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring optimal compatibility and learning efficiency. In experimental comparisons involving 7B and 13B models, our LoBaSS method is able to surpass full-data fine-tuning at merely 6% of the total training data. When employing 16.7% of the data, LoBaSS harmonizes the model's capabilities across conversational and mathematical domains, proving its efficacy and adaptability.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的超级vised Fine-Tuning（SFT）阶段 serves as a crucial phase in aligning LLMs to specific task prerequisites. The selection of fine-tuning data profoundly influences the model's performance, whose principle is traditionally grounded in data quality and distribution. In this paper, we introduce a new dimension in SFT data selection: learnability. This new dimension is motivated by the intuition that SFT unlocks capabilities acquired by a LLM during the pretraining phase. Given that different pretrained models have disparate capabilities, the SFT data appropriate for one may not suit another. Thus, we introduce the term learnability to define the suitability of data for effective learning by the model. We present the Loss Based SFT Data Selection (LoBaSS) method, utilizing data learnability as the principal criterion for the selection SFT data. This method provides a nuanced approach, allowing the alignment of data selection with inherent model capabilities, ensuring optimal compatibility and learning efficiency. In experimental comparisons involving 7B and 13B models, our LoBaSS method is able to surpass full-data fine-tuning at merely 6% of the total training data. When employing 16.7% of the data, LoBaSS harmonizes the model's capabilities across conversational and mathematical domains, proving its efficacy and adaptability.
</details></li>
</ul>
<hr>
<h2 id="CLIN-A-Continually-Learning-Language-Agent-for-Rapid-Task-Adaptation-and-Generalization"><a href="#CLIN-A-Continually-Learning-Language-Agent-for-Rapid-Task-Adaptation-and-Generalization" class="headerlink" title="CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization"></a>CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10134">http://arxiv.org/abs/2310.10134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, Peter Clark</li>
<li>For: The paper aims to develop a language-based agent that can continually improve over time and perform well in varied environments and tasks.* Methods: The paper proposes a persistent, dynamic, textual memory centered on causal abstractions, which is regularly updated after each trial to gradually learn useful knowledge for new trials.* Results: The proposed approach, called CLIN, outperforms state-of-the-art reflective language agents in the ScienceWorld benchmark, achieves transfer learning to new environments and tasks, and continually improves performance through memory updates.Here are the three points in Simplified Chinese:* For: 这篇论文目的是开发一种可以不断改进并在多个环境和任务中表现出色的语言基于的智能代理。* Methods: 论文提出了一种持续、动态、文本内存，以 causal abstractions 为中心，在每次试验后进行更新，以逐渐学习有用的知识。* Results: CLIN 在 ScienceWorld benchmark 中表现出色，比 state-of-the-art 反射语言代理 Reflexion 高出 23 个绝对分数点，并且在新环境（或新任务）中表现出较好的适应能力和持续改进能力。<details>
<summary>Abstract</summary>
Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory centered on causal abstractions (rather than general "helpful hints") that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.
</details>
<details>
<summary>摘要</summary>
language agents 有能力与外部环境互动，例如虚拟世界 ScienceWorld，完成复杂任务，如培养植物，而无需强化学习的开始成本。然而，虽有零开始能力，这些代理人至今没有持续改进的能力。在这，我们提出了 CLIN，第一个语言基于的代理人，能够在多次尝试中不断改进，包括环境和任务变化时。我们的方法是使用持续、动态、文本中心的 causal abstractions（而不是通用的“帮助提示”）， Regularly update after each trial so that the agent gradually learns useful knowledge for new trials。在 ScienceWorld benchmark 中，CLIN 能够在重复尝试中不断改进同一任务和环境，胜过现状 reflective language agents  like Reflexion 的 23 个绝对分数点。CLIN 还可以转移到新环境（或新任务），提高零开始性能 by 4 个分数点（13 个分数点），并可以通过持续记忆更新，进一步提高性能，增加 17 个分数点（7 个分数点）。这表明了一种基于冻结模型的新架构，可以在不断改进的时间上 continually 和 Rapidly 提高性能。
</details></li>
</ul>
<hr>
<h2 id="A-Non-monotonic-Smooth-Activation-Function"><a href="#A-Non-monotonic-Smooth-Activation-Function" class="headerlink" title="A Non-monotonic Smooth Activation Function"></a>A Non-monotonic Smooth Activation Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10126">http://arxiv.org/abs/2310.10126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Koushik Biswas, Meghana Karri, Ulaş Bağcı</li>
<li>for: The paper is written for proposing a new activation function called Sqish, which is an alternative to existing activation functions in deep learning models.</li>
<li>methods: The paper uses experiments on various tasks such as classification, object detection, segmentation, and adversarial robustness to demonstrate the superiority of the Sqish activation function over existing activation functions such as ReLU.</li>
<li>results: The paper shows that the Sqish activation function achieves better performance than ReLU on several benchmark datasets, including CIFAR100, with an improvement of 8.21% in adversarial robustness and 5.87% in image classification.<details>
<summary>Abstract</summary>
Activation functions are crucial in deep learning models since they introduce non-linearity into the networks, allowing them to learn from errors and make adjustments, which is essential for learning complex patterns. The essential purpose of activation functions is to transform unprocessed input signals into significant output activations, promoting information transmission throughout the neural network. In this study, we propose a new activation function called Sqish, which is a non-monotonic and smooth function and an alternative to existing ones. We showed its superiority in classification, object detection, segmentation tasks, and adversarial robustness experiments. We got an 8.21% improvement over ReLU on the CIFAR100 dataset with the ShuffleNet V2 model in the FGSM adversarial attack. We also got a 5.87% improvement over ReLU on image classification on the CIFAR100 dataset with the ShuffleNet V2 model.
</details>
<details>
<summary>摘要</summary>
translate the given text into Simplified Chinese.Activation functions are crucial in deep learning models, as they introduce non-linearity into the networks, allowing them to learn from errors and make adjustments, which is essential for learning complex patterns. The essential purpose of activation functions is to transform unprocessed input signals into significant output activations, promoting information transmission throughout the neural network. In this study, we propose a new activation function called Sqish, which is a non-monotonic and smooth function and an alternative to existing ones. We showed its superiority in classification, object detection, segmentation tasks, and adversarial robustness experiments. We got an 8.21% improvement over ReLU on the CIFAR100 dataset with the ShuffleNet V2 model in the FGSM adversarial attack. We also got a 5.87% improvement over ReLU on image classification on the CIFAR100 dataset with the ShuffleNet V2 model.中文翻译： activation functions 是深度学习模型中关键的组件，因为它们引入非线性，让模型从错误中学习并进行调整，这是学习复杂模式的关键。 activation functions 的主要目的是将未处理的输入信号转化为有意义的输出活动，促进神经网络中信息的传输。在本研究中，我们提出了一个新的 activation function called Sqish，它是非增长的和平滑的函数，是现有的替代品。我们在类别、物体检测、分割任务和对抗攻击性实验中证明了它的优越性。在 ShuffleNet V2 模型上，我们在 FGSM 对抗攻击中获得了 ReLU 的 8.21% 提升，并在图像分类任务中获得了 ReLU 的 5.87% 提升。
</details></li>
</ul>
<hr>
<h2 id="From-Continuous-Dynamics-to-Graph-Neural-Networks-Neural-Diffusion-and-Beyond"><a href="#From-Continuous-Dynamics-to-Graph-Neural-Networks-Neural-Diffusion-and-Beyond" class="headerlink" title="From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond"></a>From Continuous Dynamics to Graph Neural Networks: Neural Diffusion and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10121">http://arxiv.org/abs/2310.10121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andi Han, Dai Shi, Lequan Lin, Junbin Gao</li>
<li>for: 这篇论文旨在提供关于图 neural network（GNN）的系统性和全面的回顾，尤其是在使用连续动力学方法的研究中。</li>
<li>methods: 这篇论文使用的方法包括message passing机制和连续动力学方法，用于解决图 neural network（GNN）中的各种问题，如过滤和压缩。</li>
<li>results: 该论文提出了一种基于连续动力学方法的GNN设计方法，并对经典GNN的局限性进行了解释和改进。同时，该论文还提供了多个未解决的研究方向，以便进一步探索GNN的可能性。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have demonstrated significant promise in modelling relational data and have been widely applied in various fields of interest. The key mechanism behind GNNs is the so-called message passing where information is being iteratively aggregated to central nodes from their neighbourhood. Such a scheme has been found to be intrinsically linked to a physical process known as heat diffusion, where the propagation of GNNs naturally corresponds to the evolution of heat density. Analogizing the process of message passing to the heat dynamics allows to fundamentally understand the power and pitfalls of GNNs and consequently informs better model design. Recently, there emerges a plethora of works that proposes GNNs inspired from the continuous dynamics formulation, in an attempt to mitigate the known limitations of GNNs, such as oversmoothing and oversquashing. In this survey, we provide the first systematic and comprehensive review of studies that leverage the continuous perspective of GNNs. To this end, we introduce foundational ingredients for adapting continuous dynamics to GNNs, along with a general framework for the design of graph neural dynamics. We then review and categorize existing works based on their driven mechanisms and underlying dynamics. We also summarize how the limitations of classic GNNs can be addressed under the continuous framework. We conclude by identifying multiple open research directions.
</details>
<details>
<summary>摘要</summary>
格图神经网络（GNNs）已经显示出了重要的承诺，可以模型关系数据，并广泛应用于不同的领域。GNNs的关键机制是叫做“消息传递”，信息从邻居传递到中心节点。这种机制与物理过程热扩散有着直接的关系，GNNs的传播 Naturally corresponds to the evolution of heat density。通过对消息传递的过程进行对热动力学的分析，可以更深入地理解GNNs的力量和缺陷，并且可以设计更好的模型。最近，有一些研究借鉴了维持热动力学形式的GNNs，以解决经典GNNs的知名的局限性，如扩散和压缩。在这篇评论中，我们提供了首次系统性和完整性的对Continuous perspective of GNNs的评论。为了实现这一目标，我们介绍了适应维持热动力学的基本成分，并提出了一个总体的框架 для设计图神经动态。然后，我们回顾了现有的研究，并根据他们的驱动机制和下面动力来分类。我们还总结了经典GNNs中的局限性如何通过维持热动力学的方式解决。 Finally, we identify multiple open research directions.
</details></li>
</ul>
<hr>
<h2 id="On-Generative-Agents-in-Recommendation"><a href="#On-Generative-Agents-in-Recommendation" class="headerlink" title="On Generative Agents in Recommendation"></a>On Generative Agents in Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10108">http://arxiv.org/abs/2310.10108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LehengTHU/Agent4Rec">https://github.com/LehengTHU/Agent4Rec</a></li>
<li>paper_authors: An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, Tat-Seng Chua<br>for:* 这个论文旨在提出一种基于大语言模型（LLM）的电影推荐模拟器，帮助解决现有推荐系统中偏差的问题。methods:* 该模拟器使用了LLM-empowered生成代理，每个代理具有用户 profiling、记忆和行为模块，特意为推荐系统设计。results:* 对 Agent4Rec 的广泛和多方面评估表明，LLM-empowered生成代理可以准确地模拟真实自主人类在推荐系统中的行为。I hope this helps! Let me know if you have any further questions or if there’s anything else I can help with.<details>
<summary>Abstract</summary>
Recommender systems are the cornerstone of today's information dissemination, yet a disconnect between offline metrics and online performance greatly hinders their development. Addressing this challenge, we envision a recommendation simulator, capitalizing on recent breakthroughs in human-level intelligence exhibited by Large Language Models (LLMs). We propose Agent4Rec, a novel movie recommendation simulator, leveraging LLM-empowered generative agents equipped with user profile, memory, and actions modules specifically tailored for the recommender system. In particular, these agents' profile modules are initialized using the MovieLens dataset, capturing users' unique tastes and social traits; memory modules log both factual and emotional memories and are integrated with an emotion-driven reflection mechanism; action modules support a wide variety of behaviors, spanning both taste-driven and emotion-driven actions. Each agent interacts with personalized movie recommendations in a page-by-page manner, relying on a pre-implemented collaborative filtering-based recommendation algorithm. We delve into both the capabilities and limitations of Agent4Rec, aiming to explore an essential research question: to what extent can LLM-empowered generative agents faithfully simulate the behavior of real, autonomous humans in recommender systems? Extensive and multi-faceted evaluations of Agent4Rec highlight both the alignment and deviation between agents and user-personalized preferences. Beyond mere performance comparison, we explore insightful experiments, such as emulating the filter bubble effect and discovering the underlying causal relationships in recommendation tasks. Our codes are available at https://github.com/LehengTHU/Agent4Rec.
</details>
<details>
<summary>摘要</summary>
现代推荐系统是信息传递的核心，但是在线和离线指标之间的差距妨碍了其发展。为解决这个挑战，我们提出了一种推荐模拟器，即Agent4Rec，利用最近的人工智能大语言模型（LLM）的突破性。我们的模拟器采用LLM拥有的生成代理，包括用户profile、记忆和行为模块，特地设计用于推荐系统。具体来说，这些代理的profile模块初始化使用MovieLens数据集，捕捉用户独特的味蕾和社交特征；记忆模块记录了事实和情感的记忆，并与情感驱动的反射机制集成；行动模块支持广泛的行为，包括味蕾驱动和情感驱动的行为。每个代理都与个性化电影推荐在页面上进行交互，基于先前实现的共同 filtering 基于推荐算法。我们深入探讨Agent4Rec的能力和局限性，以explore一个关键研究问题：可以使用LLM拥有的生成代理 simulate真实自主人类在推荐系统中的行为到哪 extent?我们进行了广泛和多方面的评估，包括对代理和用户个性化喜好的Alignment和偏差。此外，我们还进行了有趣的实验，如模拟过滤层效应和发现推荐任务中的内在 causal 关系。我们的代码可以在https://github.com/LehengTHU/Agent4Rec 中下载。
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-the-Posterior-Sampling-based-Learning-Algorithm-for-Episodic-POMDPs"><a href="#Regret-Analysis-of-the-Posterior-Sampling-based-Learning-Algorithm-for-Episodic-POMDPs" class="headerlink" title="Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs"></a>Regret Analysis of the Posterior Sampling-based Learning Algorithm for Episodic POMDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10107">http://arxiv.org/abs/2310.10107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dengwang Tang, Rahul Jain, Ashutosh Nayyar, Pierluigi Nuzzo</li>
<li>for: 这 paper 的目的是研究在 partially observable Markov decision processes (POMDPs) 中学习问题。</li>
<li>methods: 这 paper 使用 posterior sampling-based reinforcement learning (PSRL) algorithm for POMDPs，并证明其 bayesian regret 的增长率为 sqrt(eps)。</li>
<li>results: 这 paper 显示，在 POMDPs 中，bayesian regret 随着 horizon length H 的增长而增长 exponentiallly，但在undercomplete和weakly revealing的 condition下，可以得到 polynomial bayesian regret bound，其比 recent result by arXiv:2204.08967 好几个orders of magnitude。<details>
<summary>Abstract</summary>
Compared to Markov Decision Processes (MDPs), learning in Partially Observable Markov Decision Processes (POMDPs) can be significantly harder due to the difficulty of interpreting observations. In this paper, we consider episodic learning problems in POMDPs with unknown transition and observation models. We consider the Posterior Sampling-based Reinforcement Learning (PSRL) algorithm for POMDPs and show that its Bayesian regret scales as the square root of the number of episodes. In general, the regret scales exponentially with the horizon length $H$, and we show that this is inevitable by providing a lower bound. However, under the condition that the POMDP is undercomplete and weakly revealing, we establish a polynomial Bayesian regret bound that improves the regret bound by a factor of $\Omega(H^2\sqrt{SA})$ over the recent result by arXiv:2204.08967.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:与Markov决策过程（MDP）相比，在Partially Observable Markov决策过程（POMDP）中学习可能更加困难，主要是因为观察结果的难以解释。在这篇论文中，我们考虑POMDP中的集集学习问题，其中过渡和观察模型都未知。我们使用Posterior Sampling-based Reinforcement Learning（PSRL）算法，并证明其 bayesian regret的准确性与集数平方根相关。通常情况下， regret scales exponentially with horizon length $H$，并且我们提供了一个下界，证明这是不可避免的。然而，在POMDP是undercomplete和weakly revealing的情况下，我们设立了一个较好的 Bayesian regret bound，它比recent result by arXiv:2204.08967好于$\Omega(H^2\sqrt{SA})$。
</details></li>
</ul>
<hr>
<h2 id="Navigation-with-Large-Language-Models-Semantic-Guesswork-as-a-Heuristic-for-Planning"><a href="#Navigation-with-Large-Language-Models-Semantic-Guesswork-as-a-Heuristic-for-Planning" class="headerlink" title="Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning"></a>Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10103">http://arxiv.org/abs/2310.10103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Michael-Equi/lfg-nav">https://github.com/Michael-Equi/lfg-nav</a></li>
<li>paper_authors: Dhruv Shah, Michael Equi, Blazej Osinski, Fei Xia, Brian Ichter, Sergey Levine</li>
<li>for: 这个论文的目的是为了帮助机器人快速在未知环境中找到目标。</li>
<li>methods: 这个论文使用语言模型来提供启发，并将语言模型作为搜索准则来帮助计划算法探索新环境。</li>
<li>results: 这个论文在实验中表明，使用语言模型可以使机器人在未知环境中更快地找到目标，并且在实际环境和模拟 benchmark 中都能够表现出优于无意义探索和其他语言模型使用方法。<details>
<summary>Abstract</summary>
Navigation in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. Humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics -- e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. Language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. Therefore, in this paper we study how the ``semantic guesswork'' produced by language models can be utilized as a guiding heuristic for planning algorithms. Our method, Language Frontier Guide (LFG), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. We evaluate LFG in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.
</details>
<details>
<summary>摘要</summary>
naviagtion in unfamiliar environments presents a major challenge for robots: while mapping and planning techniques can be used to build up a representation of the world, quickly discovering a path to a desired goal in unfamiliar settings with such methods often requires lengthy mapping and exploration. humans can rapidly navigate new environments, particularly indoor environments that are laid out logically, by leveraging semantics -- e.g., a kitchen often adjoins a living room, an exit sign indicates the way out, and so forth. language models can provide robots with such knowledge, but directly using language models to instruct a robot how to reach some destination can also be impractical: while language models might produce a narrative about how to reach some goal, because they are not grounded in real-world observations, this narrative might be arbitrarily wrong. therefore, in this paper we study how the "semantic guesswork" produced by language models can be utilized as a guiding heuristic for planning algorithms. our method, language frontier guide (lfg), uses the language model to bias exploration of novel real-world environments by incorporating the semantic knowledge stored in language models as a search heuristic for planning with either topological or metric maps. we evaluate lfg in challenging real-world environments and simulated benchmarks, outperforming uninformed exploration and other ways of using language models.
</details></li>
</ul>
<hr>
<h2 id="Reusing-Pretrained-Models-by-Multi-linear-Operators-for-Efficient-Training"><a href="#Reusing-Pretrained-Models-by-Multi-linear-Operators-for-Efficient-Training" class="headerlink" title="Reusing Pretrained Models by Multi-linear Operators for Efficient Training"></a>Reusing Pretrained Models by Multi-linear Operators for Efficient Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10699">http://arxiv.org/abs/2310.10699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 这篇论文目的是提高大型模型的训练速度，使用小型预训练模型来初始化大型模型（称为“目标模型”），并且将这两个模型中的权重 Linearly correlated 以增强加速能力。</li>
<li>methods: 这篇论文使用了一种方法，将每个目标模型的权重与所有预训练模型的权重进行 Linear correlation，以增强加速能力。这篇论文还使用了多元线性算子来降低计算和空间复杂度，使得资源需求可以接受。</li>
<li>results: 实验结果显示，这篇论文的方法可以在DeiT-base上预训练DeiT-small时，节省76%的计算成本，并且在BERT2BERT和LiGO的比较下，提高了12.0%和20.7%的性能。<details>
<summary>Abstract</summary>
Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the ``target model''), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Experiments demonstrate that our method can save 76\% computational costs on DeiT-base transferred from DeiT-small, which outperforms bert2BERT by +12.0\% and LiGO by +20.7\%, respectively.
</details>
<details>
<summary>摘要</summary>
通常，训练大型模型从零开始需要很多资源。为解决这个问题， latest studies such as bert2BERT 和 LiGO  reuse small pre-trained models to initialize a large model（称为“目标模型”），从而大幅提高训练速度。然而，这些前一 Studies only map partial weights,忽略了整个模型中 weights 之间的可能的相互关系。在这篇文章中，我们发现了这些 weights 之间的相互作用和内部相互作用，这意味着只有部分映射可能不能捕捉完整的信息，导致不够的增长。为解决这个问题，我们提议一种方法，使得每个目标模型中的每个Weight 都 Linearly correlated 到整个预训练模型中的所有Weight。我们使用多线性运算符来减少计算和空间复杂度，使得可接受的资源需求。实验表明，我们的方法可以在 DeiT-base 中心从 DeiT-small 中心转移时Save 76% 的计算成本，并且在 bert2BERT 和 LiGO 的比较中，提高 +12.0% 和 +20.7%，分别。
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Uncertainty-Representation-of-Data-Manifold-for-Robust-Long-Tailed-Learning"><a href="#Orthogonal-Uncertainty-Representation-of-Data-Manifold-for-Robust-Long-Tailed-Learning" class="headerlink" title="Orthogonal Uncertainty Representation of Data Manifold for Robust Long-Tailed Learning"></a>Orthogonal Uncertainty Representation of Data Manifold for Robust Long-Tailed Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10090">http://arxiv.org/abs/2310.10090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanbiao Ma, Licheng Jiao, Fang Liu, Shuyuan Yang, Xu Liu, Lingling Li</li>
<li>for: 提高模型在长尾分布下的Robustness</li>
<li>methods: 使用特征嵌入的Orthogonal Uncertainty Representation(OUR)和端到端训练策略</li>
<li>results: 在长尾数据集上进行了全面的评估，OUR方法可以减少模型在长尾分布下的敏感性，并且可以与其他长尾学习方法相结合使用，不需要额外数据生成，快速和高效地训练。<details>
<summary>Abstract</summary>
In scenarios with long-tailed distributions, the model's ability to identify tail classes is limited due to the under-representation of tail samples. Class rebalancing, information augmentation, and other techniques have been proposed to facilitate models to learn the potential distribution of tail classes. The disadvantage is that these methods generally pursue models with balanced class accuracy on the data manifold, while ignoring the ability of the model to resist interference. By constructing noisy data manifold, we found that the robustness of models trained on unbalanced data has a long-tail phenomenon. That is, even if the class accuracy is balanced on the data domain, it still has bias on the noisy data manifold. However, existing methods cannot effectively mitigate the above phenomenon, which makes the model vulnerable in long-tailed scenarios. In this work, we propose an Orthogonal Uncertainty Representation (OUR) of feature embedding and an end-to-end training strategy to improve the long-tail phenomenon of model robustness. As a general enhancement tool, OUR has excellent compatibility with other methods and does not require additional data generation, ensuring fast and efficient training. Comprehensive evaluations on long-tailed datasets show that our method significantly improves the long-tail phenomenon of robustness, bringing consistent performance gains to other long-tailed learning methods.
</details>
<details>
<summary>摘要</summary>
在长尾分布场景下，模型能够识别尾类受限因为尾类样本的下 Representation 不充分。Class重新平衡、信息增强和其他技术已经提出来解决这个问题，但这些方法通常寻求在数据 manifold 上具有平衡的类准确率，而忽略模型对干扰的抵抗能力。我们通过构建噪音数据 manifold 发现，模型在不平衡数据上训练时的 Robustness 存在长尾现象。即，即使在数据Domain 上具有平衡的类准确率，模型在噪音数据 manifold 上仍存在偏见。然而，现有的方法无法有效 Mitigate 这个现象，使得模型在长尾场景中脆弱。在这种情况下，我们提出了一种Orthogonal Uncertainty Representation（OUR）的特征嵌入和端到端训练策略，以改善模型在长尾场景中的Robustness。作为一种通用加强工具，OUR具有优compatibility 性，不需要额外数据生成，保证快速和高效的训练。对长尾数据集进行了全面评估，我们的方法在长尾场景中显著改善了模型的Robustness，并且与其他长尾学习方法相结合，带来了一致的性能提升。
</details></li>
</ul>
<hr>
<h2 id="MOCHA-Real-Time-Motion-Characterization-via-Context-Matching"><a href="#MOCHA-Real-Time-Motion-Characterization-via-Context-Matching" class="headerlink" title="MOCHA: Real-Time Motion Characterization via Context Matching"></a>MOCHA: Real-Time Motion Characterization via Context Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10079">http://arxiv.org/abs/2310.10079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DK-Jang/MOCHA_SIGASIA2023">https://github.com/DK-Jang/MOCHA_SIGASIA2023</a></li>
<li>paper_authors: Deok-Kyeong Jang, Yuting Ye, Jungdam Won, Sung-Hee Lee</li>
<li>for: 这篇论文目的是为了实时转换中性无表情的输入动作为一个知名人物的特有风格。</li>
<li>methods: 这篇论文介绍了一个新的在线动作特征化框架，即MOCHA，可以将目标人物的动作风格和体型特征转移到输入动作上。</li>
<li>results: 该框架可以在实时进行人物动作特征化，并且可以轻松地满足不同应用场景，如只有稀疏输入的人物特征化和实时人物特征化。此外，论文还提供了一个高质量的动作数据集，包括六个不同人物在多种动作中的表现，这可以成为未来研究的优质资源。<details>
<summary>Abstract</summary>
Transforming neutral, characterless input motions to embody the distinct style of a notable character in real time is highly compelling for character animation. This paper introduces MOCHA, a novel online motion characterization framework that transfers both motion styles and body proportions from a target character to an input source motion. MOCHA begins by encoding the input motion into a motion feature that structures the body part topology and captures motion dependencies for effective characterization. Central to our framework is the Neural Context Matcher, which generates a motion feature for the target character with the most similar context to the input motion feature. The conditioned autoregressive model of the Neural Context Matcher can produce temporally coherent character features in each time frame. To generate the final characterized pose, our Characterizer network incorporates the characteristic aspects of the target motion feature into the input motion feature while preserving its context. This is achieved through a transformer model that introduces the adaptive instance normalization and context mapping-based cross-attention, effectively injecting the character feature into the source feature. We validate the performance of our framework through comparisons with prior work and an ablation study. Our framework can easily accommodate various applications, including characterization with only sparse input and real-time characterization. Additionally, we contribute a high-quality motion dataset comprising six different characters performing a range of motions, which can serve as a valuable resource for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入动作的中性化和无特点化的输入动作在实时动画中具有吸引力，这篇论文介绍了MOCHA，一种新的在线动作特征化框架。MOCHA可以同时传递目标人物的动作风格和身体比例到输入动作中。MOCHA开始sBy编码输入动作为一个动作特征，该特征映射体部 topology和捕捉动作依赖关系以便有效地特征化。中心于我们框架的神经Context Matcher生成了目标人物的动作特征，该特征与输入动作特征Context最相似。Conditional autoregressive model of the Neural Context Matcher can produce temporally coherent character features in each time frame. To generate the final characterized pose, our Characterizer network incorporates the characteristic aspects of the target motion feature into the input motion feature while preserving its context. This is achieved through a transformer model that introduces the adaptive instance normalization and context mapping-based cross-attention, effectively injecting the character feature into the source feature. We validate the performance of our framework through comparisons with prior work and an ablation study. Our framework can easily accommodate various applications, including characterization with only sparse input and real-time characterization. Additionally, we contribute a high-quality motion dataset comprising six different characters performing a range of motions, which can serve as a valuable resource for future research.
</details></li>
</ul>
<hr>
<h2 id="Verbosity-Bias-in-Preference-Labeling-by-Large-Language-Models"><a href="#Verbosity-Bias-in-Preference-Labeling-by-Large-Language-Models" class="headerlink" title="Verbosity Bias in Preference Labeling by Large Language Models"></a>Verbosity Bias in Preference Labeling by Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10076">http://arxiv.org/abs/2310.10076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）的性能提升方法，具体来说是通过人工智能反馈学习（RLAIF）取代人类反馈来评估LLMs。</li>
<li>methods: 本研究使用了RLAIF评估GPT-4和人类反馈的方法，并提出了一个量化verbosity bias的指标。</li>
<li>results: 研究发现GPT-4在本研究中偏好 longer answers than humans, and propose a metric to measure this bias.<details>
<summary>Abstract</summary>
In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fine-tuning-ChatGPT-for-Automatic-Scoring"><a href="#Fine-tuning-ChatGPT-for-Automatic-Scoring" class="headerlink" title="Fine-tuning ChatGPT for Automatic Scoring"></a>Fine-tuning ChatGPT for Automatic Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10072">http://arxiv.org/abs/2310.10072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Latif, Xiaoming Zhai</li>
<li>For: This paper demonstrates the potential of fine-tuned ChatGPT (GPT-3.5) for automatically scoring student written constructed responses in science education.* Methods: The paper uses fine-tuned GPT-3.5 on six assessment tasks with a diverse dataset of middle-school and high-school student responses and expert scoring.* Results: The results show that fine-tuned GPT-3.5 achieved a remarkable average increase (9.1%) in automatic scoring accuracy compared to the fine-tuned state-of-the-art Google’s generated language model, BERT, with significant improvements on multi-label tasks and multi-class items.Here are the three points in Simplified Chinese text:* For: 这个研究用于评估学生写的 constructed responses 的自动评分。* Methods: 这个研究使用 fine-tuned GPT-3.5 在 six 个评估任务上，使用多样化的中学和高中生回答和专家评分数据进行 fine-tuning。* Results: 结果显示， fine-tuned GPT-3.5 在 six 个评估任务上达到了9.1%的平均提升率，比 fine-tuned BERT 高，尤其是在多个标签任务和多个类型任务上显示出了显著的提升。<details>
<summary>Abstract</summary>
This study highlights the potential of fine-tuned ChatGPT (GPT-3.5) for automatically scoring student written constructed responses using example assessment tasks in science education. Recent studies on OpenAI's generative model GPT-3.5 proved its superiority in predicting the natural language with high accuracy and human-like responses. GPT-3.5 has been trained over enormous online language materials such as journals and Wikipedia; therefore, more than direct usage of pre-trained GPT-3.5 is required for automatic scoring as students utilize a different language than trained material. These imply that a domain-specific model, fine-tuned over data for specific tasks, can enhance model performance. In this study, we fine-tuned GPT-3.5 on six assessment tasks with a diverse dataset of middle-school and high-school student responses and expert scoring. The six tasks comprise two multi-label and four multi-class assessment tasks. We compare the performance of fine-tuned GPT-3.5 with the fine-tuned state-of-the-art Google's generated language model, BERT. The results show that in-domain training corpora constructed from science questions and responses for BERT achieved average accuracy = 0.838, SD = 0.069. GPT-3.5 shows a remarkable average increase (9.1%) in automatic scoring accuracy (mean = 9.15, SD = 0.042) for the six tasks, p =0.001 < 0.05. Specifically, for multi-label tasks (item 1 with 5 labels; item 2 with 10 labels), GPT-3.5 achieved significantly higher scoring accuracy than BERT across all the labels, with the second item achieving a 7.1% increase. The average scoring increase for the four multi-class items for GPT-3.5 was 10.6% compared to BERT. Our study confirmed the effectiveness of fine-tuned GPT-3.5 for automatic scoring of student responses on domain-specific data in education with high accuracy. We have released fine-tuned models for public use and community engagement.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GreatSplicing-A-Semantically-Rich-Splicing-Dataset"><a href="#GreatSplicing-A-Semantically-Rich-Splicing-Dataset" class="headerlink" title="GreatSplicing: A Semantically Rich Splicing Dataset"></a>GreatSplicing: A Semantically Rich Splicing Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10070">http://arxiv.org/abs/2310.10070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiuli Bi, Jiaming Liang</li>
<li>for: 解决现有拼接质量 dataset 中缺乏 semantic varieties 的问题，提高拼接trace detection的准确率。</li>
<li>methods: 使用 manually created splicing dataset GreatSplicing，包括5000个拼接图像，覆盖335种不同的semantic categories。</li>
<li>results: 模型在 GreatSplicing 上训练后表现出较低的错误率和跨 dataset detection 能力，比现有dataset 更佳。<details>
<summary>Abstract</summary>
In existing splicing forgery datasets, the insufficient semantic varieties of spliced regions cause a problem that trained detection models overfit semantic features rather than splicing traces. Meanwhile, because of the absence of a reasonable dataset, different detection methods proposed cannot reach a consensus on experimental settings. To address these urgent issues, GreatSplicing, a manually created splicing dataset with a considerable amount and high quality, is proposed in this paper. GreatSplicing comprises 5,000 spliced images and covers spliced regions with 335 distinct semantic categories, allowing neural networks to grasp splicing traces better. Extensive experiments demonstrate that models trained on GreatSplicing exhibit minimal misidentification rates and superior cross-dataset detection capabilities compared to existing datasets. Furthermore, GreatSplicing is available for all research purposes and can be downloaded from www.greatsplicing.net.
</details>
<details>
<summary>摘要</summary>
现有的剪辑伪造数据集中，剪辑区域的 semantic variety 不够，导致训练的检测模型更倾向于学习 semantic features 而不是剪辑 traces。另一方面，由于缺乏合理的数据集，不同的检测方法的实际设置不能达成一致。为解决这些紧迫的问题，本文提出了 GreatSplicing，一个手动创建的剪辑数据集，包含 5,000 个剪辑图像，剪辑区域涵盖 335 个不同的 semantic category，使得神经网络更好地捕捉剪辑 traces。广泛的实验表明，基于 GreatSplicing 训练的模型在剪辑检测方面具有较少的误认率和较好的跨数据集检测能力，与现有数据集相比。此外，GreatSplicing 适用于所有研究用途，可以从 www.greatsplicing.net 下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-Graph-Filters-for-Spectral-GNNs-via-Newton-Interpolation"><a href="#Learning-Graph-Filters-for-Spectral-GNNs-via-Newton-Interpolation" class="headerlink" title="Learning Graph Filters for Spectral GNNs via Newton Interpolation"></a>Learning Graph Filters for Spectral GNNs via Newton Interpolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10064">http://arxiv.org/abs/2310.10064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Xu, Enyan Dai, Dongsheng Luo, Xiang Zhang, Suhang Wang</li>
<li>for: 本研究旨在探讨spectral graph neural networks（GNNs）中filter frequency的选择对graph数据的同义度水平（homophily level）的影响，以及如何通过任务指导学习spectral filters来捕捉 graf数据中的关键频率信息。</li>
<li>methods: 本研究采用了 both theoretical and empirical analyses，包括对 existedential GNNs进行分析，以及实验室中的实验。研究发现，low-frequency filters与homophily level之间存在正相关，而高频率 filters则与homophily level之间存在负相关。基于这一结论，研究人员提出了一种shape-aware regularization技术，用于自适应定制polynomial spectral filters，以适应 desired homophily levels。</li>
<li>results: 研究人员通过实验表明，NewtonNet可以成功地实现desired filter shapes，并在homophilous和heterophilous dataset上显示出优秀的性能。<details>
<summary>Abstract</summary>
Spectral Graph Neural Networks (GNNs) are gaining attention because they can surpass the limitations of message-passing GNNs by learning spectral filters that capture essential frequency information in graph data through task supervision. However, previous research suggests that the choice of filter frequency is tied to the graph's homophily level, a connection that hasn't been thoroughly explored in existing spectral GNNs. To address this gap, the study conducts both theoretical and empirical analyses, revealing that low-frequency filters have a positive correlation with homophily, while high-frequency filters have a negative correlation. This leads to the introduction of a shape-aware regularization technique applied to a Newton Interpolation-based spectral filter, enabling the customization of polynomial spectral filters that align with desired homophily levels. Extensive experiments demonstrate that NewtonNet successfully achieves the desired filter shapes and exhibits superior performance on both homophilous and heterophilous datasets.
</details>
<details>
<summary>摘要</summary>
spectral graph neural networks (GNNs) 是受到关注，因为它可以超越消息传递 GNNs 的局限性，通过任务指导学习spectral filters，捕捉图数据中重要的频率信息。然而，前一些研究表明，filter frequency 与图的同化程度（homophily level）之间存在相互关系，这个关系尚未在现有的 spectral GNNs 中得到了充分探讨。为了解决这个差距，这项研究进行了both theoretical和empirical分析，发现低频 filters 与 homophily 之间存在正相关，高频 filters 与 homophily 之间存在负相关。这导致了一种shape-aware regularization技术的引入，用于 Newton Interpolation-based spectral filter，以适应欲要的 homophily 水平。广泛的实验表明，NewtonNet 成功实现了所需的 filter 形状，并在 homophilous 和 heterophilous 数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Evaluation-of-Tool-Assisted-Generation-Strategies"><a href="#A-Comprehensive-Evaluation-of-Tool-Assisted-Generation-Strategies" class="headerlink" title="A Comprehensive Evaluation of Tool-Assisted Generation Strategies"></a>A Comprehensive Evaluation of Tool-Assisted Generation Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10062">http://arxiv.org/abs/2310.10062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva</li>
<li>for:  This paper aims to investigate the effectiveness of various few-shot tool-usage strategies for augmenting language models, and to provide a systematic and fair comparison with strong baselines.</li>
<li>methods:  The paper uses empirical analysis to compare the performance of different few-shot tool-usage strategies, including strategies that refine incorrect outputs with tools and strategies that retrieve relevant information ahead of or during generation.</li>
<li>results:  The paper finds that strong no-tool baselines are competitive to tool-assisted strategies, and that tool-assisted strategies are expensive in terms of the number of tokens they require. The paper emphasizes the need for comprehensive evaluations of future strategies to accurately assess their benefits and costs.<details>
<summary>Abstract</summary>
A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences). Various few-shot tool-usage strategies have been proposed. However, there is no systematic and fair comparison across different strategies, or between these strategies and strong baselines that do not leverage tools. We conduct an extensive empirical analysis, finding that (1) across various datasets, example difficulty levels, and models, strong no-tool baselines are competitive to tool-assisted strategies, implying that effectively using tools with in-context demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval tasks, strategies that *refine* incorrect outputs with tools outperform strategies that retrieve relevant information *ahead of* or *during generation*; (3) tool-assisted strategies are expensive in the number of tokens they require to work -- incurring additional costs by orders of magnitude -- which does not translate into significant improvement in performance. Overall, our findings suggest that few-shot tool integration is still an open challenge, emphasizing the need for comprehensive evaluations of future strategies to accurately assess their *benefits* and *costs*.
</details>
<details>
<summary>摘要</summary>
研究者正在努力增强语言模型，以解决其缺陷（如缺失或错误知识、逻辑推理错误）。各种几招工具使用策略已经被提出，但没有系统化、公平的比较，或与不使用工具的强大基eline进行比较。我们进行了广泛的实验分析，发现：1. 在不同的数据集、示例难度水平和模型上，不使用工具的强大基eline与工具协助策略竞争激烈，表明使用工具进行协助是一个困难的未解决问题。2. 对知识检索任务，使用工具修正错误输出的策略比使用工具预先检索相关信息的策略更高效。3. 使用工具的策略需要更多的字符数，而这些字符数的增加并没有级别提高表现。总之，我们的发现表明几招工具集成仍然是一个开放的挑战，强调未来策略的全面评估，以准确评估其利益和成本。
</details></li>
</ul>
<hr>
<h2 id="Flow-Dynamics-Correction-for-Action-Recognition"><a href="#Flow-Dynamics-Correction-for-Action-Recognition" class="headerlink" title="Flow Dynamics Correction for Action Recognition"></a>Flow Dynamics Correction for Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10059">http://arxiv.org/abs/2310.10059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Wang, Piotr Koniusz</li>
<li>for:  investigate different optical flow and features extracted from these optical flow to improve action recognition performance</li>
<li>methods: power normalization on magnitude component of optical flow for flow dynamics correction, and integrating corrected flow dynamics into popular models through a simple hallucination step</li>
<li>results: performance boosted with corrected optical flow, and new state-of-the-art performance on several benchmarks including HMDB-51, YUP++, fine-grained action recognition on MPII Cooking Activities, and large-scale Charades<details>
<summary>Abstract</summary>
Various research studies indicate that action recognition performance highly depends on the types of motions being extracted and how accurate the human actions are represented. In this paper, we investigate different optical flow, and features extracted from these optical flow that capturing both short-term and long-term motion dynamics. We perform power normalization on the magnitude component of optical flow for flow dynamics correction to boost subtle or dampen sudden motions. We show that existing action recognition models which rely on optical flow are able to get performance boosted with our corrected optical flow. To further improve performance, we integrate our corrected flow dynamics into popular models through a simple hallucination step by selecting only the best performing optical flow features, and we show that by 'translating' the CNN feature maps into these optical flow features with different scales of motions leads to the new state-of-the-art performance on several benchmarks including HMDB-51, YUP++, fine-grained action recognition on MPII Cooking Activities, and large-scale Charades.
</details>
<details>
<summary>摘要</summary>
各种研究表明动作认识性能高度取决于提取的动作类型和表达的准确性。在这篇论文中，我们调查了不同的光流，以及从这些光流中提取的短期和长期动作动力特征。我们对光流的幅组分进行功率正规化，以 corrections for flow dynamics。我们显示了现有的基于光流的动作认识模型可以通过我们修正后的光流获得性能提升。为了进一步提高性能，我们将我们修正后的流动动力集成到了流行的模型中，并通过简单的梦幻步骤选择最佳的光流特征，并显示了通过将CNN特征图转换为不同尺度的光流特征来实现新的状态前瞻性表现。这些表现在HMDB-51、YUP++、细化动作认识在MPII Cooking Activities以及大规模Charades等数据集上达到了新的状态前瞻性。
</details></li>
</ul>
<hr>
<h2 id="NASH-A-Simple-Unified-Framework-of-Structured-Pruning-for-Accelerating-Encoder-Decoder-Language-Models"><a href="#NASH-A-Simple-Unified-Framework-of-Structured-Pruning-for-Accelerating-Encoder-Decoder-Language-Models" class="headerlink" title="NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models"></a>NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10054">http://arxiv.org/abs/2310.10054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jongwooko/nash-pruning-official">https://github.com/jongwooko/nash-pruning-official</a></li>
<li>paper_authors: Jongwoo Ko, Seungjoon Park, Yujin Kim, Sumyeong Ahn, Du-Seong Chang, Euijai Ahn, Se-Young Yun</li>
<li>for: 本研究旨在 investigate encoder-decoder 模型中的结构化剪辑方法，以提高执行速度和生成质量。</li>
<li>methods: 本研究采用了分解式剪辑视角，分别对 encoder 和 decoder 组件进行结构化剪辑。</li>
<li>results: 研究发现，减少 encoder 网络中的层数可以提高执行速度，而减少 decoder 网络中的层数可以提高生成质量。基于这些发现，提出了一种简单有效的框架 NASH，可以快速地适应不同的任务和网络架构。<details>
<summary>Abstract</summary>
Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively less explored compared to encoder-only models. In this study, we investigate the behavior of the structured pruning of the encoder-decoder models in the decoupled pruning perspective of the encoder and decoder component, respectively. Our findings highlight two insights: (1) the number of decoder layers is the dominant factor of inference speed, and (2) low sparsity in the pruned encoder network enhances generation quality. Motivated by these findings, we propose a simple and effective framework, NASH, that narrows the encoder and shortens the decoder networks of encoder-decoder models. Extensive experiments on diverse generation and inference tasks validate the effectiveness of our method in both speedup and output quality.
</details>
<details>
<summary>摘要</summary>
《结构化剪除方法在不同的网络架构中，如转换器，有效地减少模型大小和加速推理速度。尽管encoder-decoder模型在许多自然语言处理任务中表现出了多样性，structured pruning方法在这些模型上 however, relatively less explored compared to encoder-only models. 在这个研究中，我们研究了encoder-decoder模型的结构化剪除在解体预测的encoder和decoder组件中的行为。我们的发现指出了两点：（1）解码层数是推理速度的决定因素，和（2）剪除后encoder网络中的低稀畴性提高了生成质量。这些发现使我们提出了一个简单而有效的框架，称为NASH，该框架缩短了encoder网络和缩短了decoder网络。我们在多种生成和推理任务上进行了广泛的实验，并证明了我们的方法在速度和输出质量上都是有效的。》
</details></li>
</ul>
<hr>
<h2 id="Robust-Collaborative-Filtering-to-Popularity-Distribution-Shift"><a href="#Robust-Collaborative-Filtering-to-Popularity-Distribution-Shift" class="headerlink" title="Robust Collaborative Filtering to Popularity Distribution Shift"></a>Robust Collaborative Filtering to Popularity Distribution Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10696">http://arxiv.org/abs/2310.10696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anzhang314/popgo">https://github.com/anzhang314/popgo</a></li>
<li>paper_authors: An Zhang, Wenchang Ma, Jingnan Zheng, Xiang Wang, Tat-seng Chua</li>
<li>for: 本研究旨在提高collaborative filtering（CF）模型的泛化能力，即使训练数据中的媒体短Circle（popularity shortcut）存在。</li>
<li>methods: 本研究提出了一种简单 yet effective的偏差修正策略，称为PopGo，它可以衡量并降低用户-项目对的交互性wise媒体短Circle。PopGo首先学习一个媒体短Circle模型，然后通过对CF模型的预测进行修正来减少媒体短Circle的影响。</li>
<li>results: 对四个benchmark数据集进行了实验，PopGo可以在ID和OOD测试集上取得显著的提升，比如DICE和MACR等已有的偏差修正策略。<details>
<summary>Abstract</summary>
In leading collaborative filtering (CF) models, representations of users and items are prone to learn popularity bias in the training data as shortcuts. The popularity shortcut tricks are good for in-distribution (ID) performance but poorly generalized to out-of-distribution (OOD) data, i.e., when popularity distribution of test data shifts w.r.t. the training one. To close the gap, debiasing strategies try to assess the shortcut degrees and mitigate them from the representations. However, there exist two deficiencies: (1) when measuring the shortcut degrees, most strategies only use statistical metrics on a single aspect (i.e., item frequency on item and user frequency on user aspect), failing to accommodate the compositional degree of a user-item pair; (2) when mitigating shortcuts, many strategies assume that the test distribution is known in advance. This results in low-quality debiased representations. Worse still, these strategies achieve OOD generalizability with a sacrifice on ID performance. In this work, we present a simple yet effective debiasing strategy, PopGo, which quantifies and reduces the interaction-wise popularity shortcut without any assumptions on the test data. It first learns a shortcut model, which yields a shortcut degree of a user-item pair based on their popularity representations. Then, it trains the CF model by adjusting the predictions with the interaction-wise shortcut degrees. By taking both causal- and information-theoretical looks at PopGo, we can justify why it encourages the CF model to capture the critical popularity-agnostic features while leaving the spurious popularity-relevant patterns out. We use PopGo to debias two high-performing CF models (MF, LightGCN) on four benchmark datasets. On both ID and OOD test sets, PopGo achieves significant gains over the state-of-the-art debiasing strategies (e.g., DICE, MACR).
</details>
<details>
<summary>摘要</summary>
领导合作 filtering（CF）模型中，用户和item的表示可能受到训练数据中的媒体偏袋影响，即媒体偏袋短cut。这些媒体偏袋短cut可以在内部数据（ID）上达到好的性能，但是对于外部数据（OOD）来说，媒体偏袋短cut会导致模型的泛化能力差。为了缓解这 gap，去偏袋策略会评估用户和item的媒体偏袋度并 Mitigate它们从表示中。然而，存在两个缺陷：（1）当衡量媒体偏袋度时，大多数策略只使用单一方面的统计指标（例如，item频次和用户频次），而不考虑用户-item对的compositional度;（2）当缓解媒体偏袋时，许多策略假设测试分布已知。这会导致低质量的去偏袋表示。更糟糕的是，这些策略可以在ID性能的代价下实现OOD泛化性能。在这个工作中，我们提出了一种简单 yet effective的去偏袋策略，即PopGo。PopGo会量化和降低用户-item对的交互媒体偏袋度，不需要测试分布的假设。它首先学习一个媒体偏袋模型，并根据用户和item的媒体偏袋表示计算交互媒体偏袋度。然后，它将CF模型通过对预测进行调整来训练。通过从 causal和信息理论的角度看PopGo，我们可以解释它如何鼓励CF模型捕捉重要的媒体偏袋无关特征，而不捕捉媒体偏袋相关的假特征。我们使用PopGo对两种高性能CF模型（MF、LightGCN）在四个benchmark数据集上进行去偏袋。在ID和OOD测试集上，PopGo实现了与状态 искусственный debiasing策略（例如、DICE、MACR）相比较高的性能提升。
</details></li>
</ul>
<hr>
<h2 id="FATE-LLM-A-Industrial-Grade-Federated-Learning-Framework-for-Large-Language-Models"><a href="#FATE-LLM-A-Industrial-Grade-Federated-Learning-Framework-for-Large-Language-Models" class="headerlink" title="FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models"></a>FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10049">http://arxiv.org/abs/2310.10049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/FederatedAI/FATE-LLM">https://github.com/FederatedAI/FATE-LLM</a></li>
<li>paper_authors: Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang<br>for: 这个论文的目的是提出一个industrial-grade federated learning框架，以便在实际应用中使用大型自然语言模型（LLMs）。methods: 这个框架使用了 parameter-efficient fine-tuning方法来有效地训练大型自然语言模型，并且运用了隐私保护机制来保护知识产权和数据隐私。results: 这个框架能够解决训练大型自然语言模型所需的巨量 Computing资源和高质量数据的问题，并且能够保护知识产权和数据隐私。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research of FedLLM and enable a broad range of industrial applications.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如ChatGPT、LLaMA、GLM和PaLM，在过去的几年中表现出了很好的表现。然而，LLM在实际应用中遇到了两个主要挑战。一个挑战是在小到中型企业的限制计算资源下培训LLM，这限制了LLM的应用。另一个挑战是培训LLM需要大量高质量数据，这些数据经常分散在企业中。为解决这些挑战，我们提出了FATE-LLM，一个工业级联合学习框架 для大型语言模型。FATE-LLM（1）实现联合学习 для大型语言模型（称为FedLLM）；（2）提高FedLLM的高效培训方法；（3）保护LLM的知识产权；（4）在训练和推断过程中保护数据隐私。我们在GitHub上发布了FATE-LLM的代码，以便研究FedLLM和推广各种工业应用。
</details></li>
</ul>
<hr>
<h2 id="TRANSOM-An-Efficient-Fault-Tolerant-System-for-Training-LLMs"><a href="#TRANSOM-An-Efficient-Fault-Tolerant-System-for-Training-LLMs" class="headerlink" title="TRANSOM: An Efficient Fault-Tolerant System for Training LLMs"></a>TRANSOM: An Efficient Fault-Tolerant System for Training LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10046">http://arxiv.org/abs/2310.10046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SenseCore/transom-checkpoint-engine">https://github.com/SenseCore/transom-checkpoint-engine</a></li>
<li>paper_authors: Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li</li>
<li>for: 提高大型语言模型（LLM）的训练效率，解决大规模参数训练过程中的硬件和软件故障问题。</li>
<li>methods: 提出了一个新的故障tolerant LLM 训练系统，包括三个关键子系统：training pipeline automatic fault tolerance and recovery mechanism（TOL）、training task multi-dimensional metric automatic anomaly detection system（TEE）和training checkpoint asynchronous access automatic fault tolerance and recovery technology（TCE）。</li>
<li>results: 实验结果表明， tranSOM 可以显著提高大规模 LLM 训练的效率，Specifically, GPT3-175B 的预训练时间被降低了28%，而 asynchronous checkpoint saving and loading 性能提高了20倍。<details>
<summary>Abstract</summary>
Large language models (LLMs) with hundreds of billions or trillions of parameters, represented by chatGPT, have achieved profound impact on various fields. However, training LLMs with super-large-scale parameters requires large high-performance GPU clusters and long training periods lasting for months. Due to the inevitable hardware and software failures in large-scale clusters, maintaining uninterrupted and long-duration training is extremely challenging. As a result, A substantial amount of training time is devoted to task checkpoint saving and loading, task rescheduling and restart, and task manual anomaly checks, which greatly harms the overall training efficiency. To address these issues, we propose TRANSOM, a novel fault-tolerant LLM training system. In this work, we design three key subsystems: the training pipeline automatic fault tolerance and recovery mechanism named Transom Operator and Launcher (TOL), the training task multi-dimensional metric automatic anomaly detection system named Transom Eagle Eye (TEE), and the training checkpoint asynchronous access automatic fault tolerance and recovery technology named Transom Checkpoint Engine (TCE). Here, TOL manages the lifecycle of training tasks, while TEE is responsible for task monitoring and anomaly reporting. TEE detects training anomalies and reports them to TOL, who automatically enters the fault tolerance strategy to eliminate abnormal nodes and restart the training task. And the asynchronous checkpoint saving and loading functionality provided by TCE greatly shorten the fault tolerance overhead. The experimental results indicate that TRANSOM significantly enhances the efficiency of large-scale LLM training on clusters. Specifically, the pre-training time for GPT3-175B has been reduced by 28%, while checkpoint saving and loading performance have improved by a factor of 20.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如chatGPT，已经在不同领域 achieve 了深见的影响。然而，在超大型参数的训练中，需要大型高性能GPU集群和长时间的训练时间，达到月份甚至更长。由于大型集群中的硬件和软件故障是不可避免的，因此在长时间训练中保持无间断和长时间训练是极其困难。为此，我们提出了 TRANSOM，一种新的 fault-tolerant LLM 训练系统。在这项工作中，我们设计了三个关键子系统：训练管道自动过错tolerance和恢复机制（TOL），任务多维度自动异常检测系统（TEE），以及训练checkpoint异步访问自动过错tolerance和恢复技术（TCE）。在这里，TOL负责训练任务的生命周期管理，而TEE负责任务监控和异常报告。TEE检测训练异常并将其报告给TOL，TOL会自动入口过错策略，消除异常节点并重新启动训练任务。而TCE提供的异步checkpoint存储和加载功能，可以大大减少过错过程的负担。实验结果表明，TRANSOM可以大幅提高大规模 LL M 训练的效率。Specifically，对于GPT3-175B的预训练时间，可以提高28%，而checkpoint存储和加载性能可以提高20倍。
</details></li>
</ul>
<hr>
<h2 id="Smart-City-Transportation-Deep-Learning-Ensemble-Approach-for-Traffic-Accident-Detection"><a href="#Smart-City-Transportation-Deep-Learning-Ensemble-Approach-for-Traffic-Accident-Detection" class="headerlink" title="Smart City Transportation: Deep Learning Ensemble Approach for Traffic Accident Detection"></a>Smart City Transportation: Deep Learning Ensemble Approach for Traffic Accident Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10038">http://arxiv.org/abs/2310.10038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Adewopo, Nelly Elsayed</li>
<li>for: 这篇论文旨在探讨现有的交通事故检测技术，以提高城市智能化交通管理系统中的安全性和效率。</li>
<li>methods: 该论文提出了一种新的I3D-CONVLSTM2D模型架构，该模型结合RGB帧和光流信息，特意为城市智能化交通监测系统设计，并通过实验研究证明了该模型的高效性。</li>
<li>results: 该论文的实验分析表明，I3D-CONVLSTM2D RGB + Optical-Flow (Trainable)模型在交通事故检测方面表现出色，MAP值达到87%。同时，论文还探讨了数据偏置问题，并提出了解决方案。<details>
<summary>Abstract</summary>
The dynamic and unpredictable nature of road traffic necessitates effective accident detection methods for enhancing safety and streamlining traffic management in smart cities. This paper offers a comprehensive exploration study of prevailing accident detection techniques, shedding light on the nuances of other state-of-the-art methodologies while providing a detailed overview of distinct traffic accident types like rear-end collisions, T-bone collisions, and frontal impact accidents. Our novel approach introduces the I3D-CONVLSTM2D model architecture, a lightweight solution tailored explicitly for accident detection in smart city traffic surveillance systems by integrating RGB frames with optical flow information. Our experimental study's empirical analysis underscores our approach's efficacy, with the I3D-CONVLSTM2D RGB + Optical-Flow (Trainable) model outperforming its counterparts, achieving an impressive 87\% Mean Average Precision (MAP). Our findings further elaborate on the challenges posed by data imbalances, particularly when working with a limited number of datasets, road structures, and traffic scenarios. Ultimately, our research illuminates the path towards a sophisticated vision-based accident detection system primed for real-time integration into edge IoT devices within smart urban infrastructures.
</details>
<details>
<summary>摘要</summary>
随着城市智能化的发展，道路交通中的事故检测技术已成为提高安全性和优化交通管理的关键。本文进行了全面的探讨现有事故检测技术，探讨其他现代方法的细节，并提供了不同类型的交通事故的详细概述，如后尾collisions、T-bone collisions和前面Collisions。我们的新方法 introduce了I3D-CONVLSTM2D模型架构，这是一种适应性强的解决方案，通过RGB框架和光流信息来检测事故。我们的实验研究的实证分析表明，我们的I3D-CONVLSTM2D RGB + Optical-Flow（可训练）模型在事故检测方面表现出色，达到了87%的 Mean Average Precision（MAP）。我们的发现还探讨了数据不均衡的挑战，特别是在有限数据集、路径结构和交通场景下。最后，我们的研究阐明了一种基于视觉的事故检测系统，准备好于实时集成到智能城市基础设施中的边缘IoT设备。
</details></li>
</ul>
<hr>
<h2 id="Bootstrap-Your-Own-Skills-Learning-to-Solve-New-Tasks-with-Large-Language-Model-Guidance"><a href="#Bootstrap-Your-Own-Skills-Learning-to-Solve-New-Tasks-with-Large-Language-Model-Guidance" class="headerlink" title="Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance"></a>Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10021">http://arxiv.org/abs/2310.10021</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clvrai/boss">https://github.com/clvrai/boss</a></li>
<li>paper_authors: Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, Joseph J. Lim<br>for:BOSS is designed to solve new long-horizon, complex, and meaningful tasks with minimal supervision.methods:BOSS uses skill bootstrapping, where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. The bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together.results:Agents trained with the LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments.<details>
<summary>Abstract</summary>
We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments. Website at clvrai.com/boss.
</details>
<details>
<summary>摘要</summary>
我们提出了BOSS方法，它可以自动学习解决新的长期、复杂和有意义的任务，只需最小的监督。现有的循环学习方法通常需要专家指导，通过示例或丰富的奖励函数来学习长期任务。相比之下，我们的BOSS方法通过“技能启动”来学习新任务，其中一个已有技能的机器人与环境互动，不接受任务外部的奖励反馈，而是通过大型自然语言模型（LLM）指导，学习新的技能链接。通过这个过程，BOSS可以从基本的原始技能中拓宽许多复杂和有用的行为。我们通过实验表明，使用我们的LLM指导的启动过程训练的机器人在未看过任务和环境的情况下，可以在逻辑家庭环境中比静态训练和先前的无监督技能获取方法表现出更好的成绩。更多信息可以通过官方网站clvrai.com/boss。
</details></li>
</ul>
<hr>
<h2 id="Towards-Unified-and-Effective-Domain-Generalization"><a href="#Towards-Unified-and-Effective-Domain-Generalization" class="headerlink" title="Towards Unified and Effective Domain Generalization"></a>Towards Unified and Effective Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10008">http://arxiv.org/abs/2310.10008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/invictus717/UniDG">https://github.com/invictus717/UniDG</a></li>
<li>paper_authors: Yiyuan Zhang, Kaixiong Gong, Xiaohan Ding, Kaipeng Zhang, Fangrui Lv, Kurt Keutzer, Xiangyu Yue</li>
<li>for: 提高基础模型对不同领域的扩展性和泛化性性能</li>
<li>methods: 基于无监督学习的方法，在推理阶段进行轻量级微调，以避免训练阶段的极端忘却</li>
<li>results: 在12种视觉基础模型上，包括CNN、MLP和Transformer等，平均提高了+5.4%的精度，证明UniDG的多样性和优势。<details>
<summary>Abstract</summary>
We propose $\textbf{UniDG}$, a novel and $\textbf{Uni}$fied framework for $\textbf{D}$omain $\textbf{G}$eneralization that is capable of significantly enhancing the out-of-distribution generalization performance of foundation models regardless of their architectures. The core idea of UniDG is to finetune models during the inference stage, which saves the cost of iterative training. Specifically, we encourage models to learn the distribution of test data in an unsupervised manner and impose a penalty regarding the updating step of model parameters. The penalty term can effectively reduce the catastrophic forgetting issue as we would like to maximally preserve the valuable knowledge in the original model. Empirically, across 12 visual backbones, including CNN-, MLP-, and Transformer-based models, ranging from 1.89M to 303M parameters, UniDG shows an average accuracy improvement of +5.4% on DomainBed. These performance results demonstrate the superiority and versatility of UniDG. The code is publicly available at https://github.com/invictus717/UniDG
</details>
<details>
<summary>摘要</summary>
我们提出了UniDG，一个新的、统一的框架，可以对基础模型的外部泛化性能进行明显改善，不论其架构。UniDG的核心思想是在推断阶段进行调整，这样可以避免迭代训练的成本。具体来说，我们鼓励模型在无监督下学习试验数据的分布，并对模型参数更新的步骤加入一个罚则。这个罚则可以有效减少严重遗忘问题，因为我们希望将原始模型中的有价知识保留到最大程度。实验结果显示，在12种视觉基础模型中，包括CNN、MLP和Transformer等，参数量从1.89M到303M之间，UniDG在DomainBed上平均提高了5.4%的精度。这些表现结果证明UniDG的优越性和多样性。代码可以在https://github.com/invictus717/UniDG上获取。
</details></li>
</ul>
<hr>
<h2 id="Forecaster-Towards-Temporally-Abstract-Tree-Search-Planning-from-Pixels"><a href="#Forecaster-Towards-Temporally-Abstract-Tree-Search-Planning-from-Pixels" class="headerlink" title="Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels"></a>Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09997">http://arxiv.org/abs/2310.09997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Jiralerspong, Flemming Kondrup, Doina Precup, Khimya Khetarpal</li>
<li>for: 本研究旨在提高深度层次优化学习 Agent 的Sample Efficiency，使其在高维状态空间中具有远景见准能力，从而更好地学习和做出决策。</li>
<li>methods: 本研究提出了 Forecaster，一种深度层次优化学习方法，该方法利用高级目标进行规划，并通过模拟环境动力学特性来学习抽象世界模型。</li>
<li>results: 实验表明，Forecaster 可以在 AntMaze domain 中实现更好的 Sample Efficiency，并且可以在新任务下进行普适化学习。<details>
<summary>Abstract</summary>
The ability to plan at many different levels of abstraction enables agents to envision the long-term repercussions of their decisions and thus enables sample-efficient learning. This becomes particularly beneficial in complex environments from high-dimensional state space such as pixels, where the goal is distant and the reward sparse. We introduce Forecaster, a deep hierarchical reinforcement learning approach which plans over high-level goals leveraging a temporally abstract world model. Forecaster learns an abstract model of its environment by modelling the transitions dynamics at an abstract level and training a world model on such transition. It then uses this world model to choose optimal high-level goals through a tree-search planning procedure. It additionally trains a low-level policy that learns to reach those goals. Our method not only captures building world models with longer horizons, but also, planning with such models in downstream tasks. We empirically demonstrate Forecaster's potential in both single-task learning and generalization to new tasks in the AntMaze domain.
</details>
<details>
<summary>摘要</summary>
agent的多级划分能力使其能够预测长期后果，从而实现样本效率学习。这特别有用在高维状态空间如像素的复杂环境中，目标远距离，奖励罕见。我们介绍了Forecaster，一种深层决策学习方法，通过高级目标规划来规划高级目标。Forecaster使用抽象世界模型来模型环境的过程动态，并在 such transition 上训练世界模型。它然后使用这个世界模型来选择优质高级目标，并通过树搜索规划算法来实现。此外，它还训练低级策略，以实现高级目标。我们的方法不仅能够建立更长期的世界模型，还能够在下游任务中使用这些模型进行规划。我们在AntMaze领域进行了实验，证明了Forecaster的潜力。
</details></li>
</ul>
<hr>
<h2 id="Network-Analysis-of-the-iNaturalist-Citizen-Science-Community"><a href="#Network-Analysis-of-the-iNaturalist-Citizen-Science-Community" class="headerlink" title="Network Analysis of the iNaturalist Citizen Science Community"></a>Network Analysis of the iNaturalist Citizen Science Community</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10693">http://arxiv.org/abs/2310.10693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Lu Liu, Thomas Jiralerspong</li>
<li>for: 本研究使用iNaturalist citizen science平台作为案例研究，探讨公民科学项目之间的结构和互动方式。</li>
<li>methods: 我们将iNaturalist数据表示为两类网络，并使用视觉化和已知网络科学技术来获得公民科学项目结构和用户互动的新的视角。</li>
<li>results: 我们提出了一种新的网络准则，使用iNaturalist数据创建一个独特的网络结构，并通过链接预测任务表明这个网络可以用于探讨多种网络科学方法的新思路。<details>
<summary>Abstract</summary>
In recent years, citizen science has become a larger and larger part of the scientific community. Its ability to crowd source data and expertise from thousands of citizen scientists makes it invaluable. Despite the field's growing popularity, the interactions and structure of citizen science projects are still poorly understood and under analyzed. We use the iNaturalist citizen science platform as a case study to analyze the structure of citizen science projects. We frame the data from iNaturalist as a bipartite network and use visualizations as well as established network science techniques to gain insights into the structure and interactions between users in citizen science projects. Finally, we propose a novel unique benchmark for network science research by using the iNaturalist data to create a network which has an unusual structure relative to other common benchmark networks. We demonstrate using a link prediction task that this network can be used to gain novel insights into a variety of network science methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.AI_2023_10_16/" data-id="clp869trt005rk588bf9zaj82" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.CL_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T11:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.CL_2023_10_16/">cs.CL - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="IDEAL-Influence-Driven-Selective-Annotations-Empower-In-Context-Learners-in-Large-Language-Models"><a href="#IDEAL-Influence-Driven-Selective-Annotations-Empower-In-Context-Learners-in-Large-Language-Models" class="headerlink" title="IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models"></a>IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10873">http://arxiv.org/abs/2310.10873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skzhang1/IDEAL">https://github.com/skzhang1/IDEAL</a></li>
<li>paper_authors: Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-Hao Chen, Jiale Liu, Qingyun Wu, Tongliang Liu<br>for: This paper aims to address the challenge of high annotation costs in in-context learning by introducing an influence-driven selective annotation method.methods: The proposed method constructs a directed graph to represent unlabeled data, quantifies the influence of candidate unlabeled subsets using a diffusion process, and selects the most influential subsets using a greedy algorithm.results: The proposed method achieves better performance under lower time consumption during subset selection compared to previous efforts on selective annotations. Experiments confirm the superiority of the proposed method on various benchmarks.Here’s the Chinese translation of the three points:for: 这篇论文目标是解决启动学习中的注释成本高的挑战，提出了一种基于影响的选择性注释方法。methods: 该方法首先构建了一个指向图来表示无标示数据，然后使用扩散过程来衡量候选无标示子集的影响，并使用一种简单 yet effective的批处算法来选择最有影响的子集。results: 该方法在不同的标准 bencmark 上达到了更高的性能，而且在选择子集时间上具有更低的时间投入。实验证明了该方法的优越性。<details>
<summary>Abstract</summary>
In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influence-driven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.
</details>
<details>
<summary>摘要</summary>
内容学习是一种有前途的概念，它利用内容例子作为大型语言模型的预测提示。这些提示是实现强制性的关键，但是因为需要从大量的标注例子中抽取提示，因此找到正确的提示可能会带来高的标注成本。为解决这个挑战，本研究将引入一种影响驱动的选择性标注方法，以降低标注成本而提高内容例子的质量。本方法的核心思想是从大规模的未标注数据池中选择一个关键子集，并将其标注以供后续的提示抽取。首先， constructed 一个导向的图来表示未标注数据。接着， candidate 的未标注子集之间的影响被评估通过一个传播过程。最后，一个简单 yet effective 的对不标注数据选择法是引入，它在每次选择时会选择具有最大 MARGINAL 增长的数据。与先前的选择性标注方法不同，我们的影响驱动方法在端到端方式下进行，避免了一个不可能的明确平衡 между 数据多样性和代表性，并且受到了理论支持。实验确认了我们提出的方法在不同的benchmark上的超越性，在选择subset时间consumption下得到了更好的性能。更多信息可以通过我们的项目页面（https://skzhang1.github.io/IDEAL/）了解。
</details></li>
</ul>
<hr>
<h2 id="Will-the-Prince-Get-True-Love’s-Kiss-On-the-Model-Sensitivity-to-Gender-Perturbation-over-Fairytale-Texts"><a href="#Will-the-Prince-Get-True-Love’s-Kiss-On-the-Model-Sensitivity-to-Gender-Perturbation-over-Fairytale-Texts" class="headerlink" title="Will the Prince Get True Love’s Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts"></a>Will the Prince Get True Love’s Kiss? On the Model Sensitivity to Gender Perturbation over Fairytale Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10865">http://arxiv.org/abs/2310.10865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christina Chance, Da Yin, Dakuo Wang, Kai-Wei Chang</li>
<li>for: 本研究旨在探讨传统童话中存在的性别偏见，以及语言模型学习到的这些偏见是如何影响其性别认知的。</li>
<li>methods: 本研究使用Counterfactual数据增强技术来评估语言模型对性别变化的Robustness。Specifically，我们使用FairytaleQA数据集进行问答任务，并在训练时引入Counterfactual性别刻板印象以降低学习到的偏见。</li>
<li>results: 我们的实验结果显示，模型对性别变化具有敏感性，在原始测试集比较性别偏见的情况下，模型的性能会明显下降。但是，在先进行Counterfactual训练 dataset的 fine-tuning 后，模型对后来引入的Anti-性别刻板文本变得更加敏感。<details>
<summary>Abstract</summary>
Recent studies show that traditional fairytales are rife with harmful gender biases. To help mitigate these gender biases in fairytales, this work aims to assess learned biases of language models by evaluating their robustness against gender perturbations. Specifically, we focus on Question Answering (QA) tasks in fairytales. Using counterfactual data augmentation to the FairytaleQA dataset, we evaluate model robustness against swapped gender character information, and then mitigate learned biases by introducing counterfactual gender stereotypes during training time. We additionally introduce a novel approach that utilizes the massive vocabulary of language models to support text genres beyond fairytales. Our experimental results suggest that models are sensitive to gender perturbations, with significant performance drops compared to the original testing set. However, when first fine-tuned on a counterfactual training dataset, models are less sensitive to the later introduced anti-gender stereotyped text.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CoTFormer-More-Tokens-With-Attention-Make-Up-For-Less-Depth"><a href="#CoTFormer-More-Tokens-With-Attention-Make-Up-For-Less-Depth" class="headerlink" title="CoTFormer: More Tokens With Attention Make Up For Less Depth"></a>CoTFormer: More Tokens With Attention Make Up For Less Depth</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10845">http://arxiv.org/abs/2310.10845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirkeivan Mohtashami, Matteo Pagliardini, Martin Jaggi</li>
<li>for: 本文目的是提出一种基于链式思维（Chain-of-Thought，CoT）机制的 transformer 变体，以实现与更深的模型性能相似的表现。</li>
<li>methods: 本文使用了一种做为链式思维机制的假设，并基于此假设提出了一种名为 CoTFormer 的 transformer 变体。</li>
<li>results: 实验结果表明，CoTFormer 能够与更深的标准 transformer 相比，在多个任务上表现更好。<details>
<summary>Abstract</summary>
The race to continually develop ever larger and deeper foundational models is underway. However, techniques like the Chain-of-Thought (CoT) method continue to play a pivotal role in achieving optimal downstream performance. In this work, we establish an approximate parallel between using chain-of-thought and employing a deeper transformer. Building on this insight, we introduce CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to achieve capacity comparable to a deeper model. Our empirical findings demonstrate the effectiveness of CoTFormers, as they significantly outperform larger standard transformers.
</details>
<details>
<summary>摘要</summary>
“Foundational models”的竞赛不断地进行开发，但“Chain-of-Thought”（CoT）方法仍然扮演着关键的角色，以获得最佳的下游性能。在这个研究中，我们发现使用Chain-of-thought和使用更深的transformer之间存在一种近似的关系。基于这个意识，我们介绍CoTFormer，一种使用隐式CoT-like机制的transformer变体，以获得与更深的模型相同的容量。我们的实验结果显示CoTFormer具有明显的超越性，与标准的transformer模型相比。
</details></li>
</ul>
<hr>
<h2 id="Survey-of-Vulnerabilities-in-Large-Language-Models-Revealed-by-Adversarial-Attacks"><a href="#Survey-of-Vulnerabilities-in-Large-Language-Models-Revealed-by-Adversarial-Attacks" class="headerlink" title="Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"></a>Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10844">http://arxiv.org/abs/2310.10844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh</li>
<li>for: 这篇论文探讨了对大语言模型（LLMs）的敌意攻击的研究，以及如何使AI系统更加可靠。</li>
<li>methods: 论文使用了多种学习结构，包括文本只攻击、多模态攻击和复杂系统特有的攻击方法，以探讨LLMs的安全性问题。</li>
<li>results: 论文提供了LLMs的安全性问题的概述，以及现有研究的总结和可能的防御策略。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在architecture和能力方面快速进步，因此批判它们的安全性特性的必要性也在增加。这篇论文对抗AI系统中的攻击进行了评估，这是一种涉及自然语言处理和安全的新兴领域。先前的研究表明，即使通过 instrucion 调整和人工反馈来实现安全性的LLM也可能受到攻击，这些攻击利用模型的弱点并诱导AI系统出错，例如 chatGPT 和 Bard 上的 "监狱" 攻击。在这篇论文中，我们首先提供了大型语言模型的概述，描述了它们的安全性，然后根据不同的学习结构进行了分类：只有文本攻击、多模态攻击以及特定复杂系统的攻击方法，如联合学习或多代理系统。我们还提供了关于漏洞的基本来源和防御措施的评论。为了让这个领域更加Accessible，我们提供了一个系统性的回顾现有工作，一种结构化的攻击概念 typology，以及其他资源，包括与相关话题的PowerPoint演示在ACL'24年会上。
</details></li>
</ul>
<hr>
<h2 id="Fake-News-in-Sheep’s-Clothing-Robust-Fake-News-Detection-Against-LLM-Empowered-Style-Attacks"><a href="#Fake-News-in-Sheep’s-Clothing-Robust-Fake-News-Detection-Against-LLM-Empowered-Style-Attacks" class="headerlink" title="Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks"></a>Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10830">http://arxiv.org/abs/2310.10830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaying Wu, Bryan Hooi</li>
<li>for: 这篇论文旨在解决大型自然语言模型（LLM）驱动的新闻假消息探测问题，以提高在线新闻环境中自动检测的精度。</li>
<li>methods: 这篇论文提出了一种基于新闻媒体的探测方法，通过使用 style-oriented reframing 技术和内置的大型自然语言模型（LLM），实现对新闻写作风格的适应性。</li>
<li>results: 实验结果表明，这种方法可以在三个 benchmark 数据集上提供显著的改进，并增强对 LLM 驱动的新闻假消息探测的抗性。<details>
<summary>Abstract</summary>
It is commonly perceived that online fake news and reliable news exhibit stark differences in writing styles, such as the use of sensationalist versus objective language. However, we emphasize that style-related features can also be exploited for style-based attacks. Notably, the rise of powerful Large Language Models (LLMs) has enabled malicious users to mimic the style of trustworthy news outlets at minimal cost. Our analysis reveals that LLM-camouflaged fake news content leads to substantial performance degradation of state-of-the-art text-based detectors (up to 38% decrease in F1 Score), posing a significant challenge for automated detection in online ecosystems. To address this, we introduce SheepDog, a style-agnostic fake news detector robust to news writing styles. SheepDog achieves this adaptability through LLM-empowered news reframing, which customizes each article to match different writing styles using style-oriented reframing prompts. By employing style-agnostic training, SheepDog enhances its resilience to stylistic variations by maximizing prediction consistency across these diverse reframings. Furthermore, SheepDog extracts content-focused veracity attributions from LLMs, where the news content is evaluated against a set of fact-checking rationales. These attributions provide supplementary information and potential interpretability that assist veracity prediction. On three benchmark datasets, empirical results show that SheepDog consistently yields significant improvements over competitive baselines and enhances robustness against LLM-empowered style attacks.
</details>
<details>
<summary>摘要</summary>
通常认为在线假新闻和可靠新闻的写作风格有很大差异，如使用感人化语言 versus  объектив语言。然而，我们强调的是风格相关特征也可以被利用于风格基本攻击。尤其是现在强大的大语言模型（LLMs）的出现，使得恶意用户可以轻松地模仿可靠新闻机构的风格，对于自动检测在线环境中具有重大挑战。为了解决这一问题，我们介绍了羊狗（SheepDog），一种不受风格限制的假新闻检测器，可以在不同的新闻风格下保持高度的稳定性。羊狗通过使用 LLMS 进行新闻重 framings，以适应不同的新闻风格，并通过风格无关的训练来增强其对风格变化的抗性。此外，羊狗使用 LLMS 提供的内容相关的真实性评估，对新闻内容进行了实际的真实性评估，以提供可靠的假新闻检测。在三个 benchmark 数据集上，实验结果表明，羊狗可以与竞争对手相比，提供显著的改善，并增强了对 LLMS 风格基本攻击的抗性。
</details></li>
</ul>
<hr>
<h2 id="SD-HuBERT-Self-Distillation-Induces-Syllabic-Organization-in-HuBERT"><a href="#SD-HuBERT-Self-Distillation-Induces-Syllabic-Organization-in-HuBERT" class="headerlink" title="SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT"></a>SD-HuBERT: Self-Distillation Induces Syllabic Organization in HuBERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10803">http://arxiv.org/abs/2310.10803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheol Jun Cho, Abdelrahman Mohamed, Shang-Wen Li, Alan W Black, Gopala K. Anumanchipalli</li>
<li>for: 这篇论文旨在探讨自然语言处理中的自适应学习（SSL）技术，具体来说是检测和分析发音中的句子水平表示。</li>
<li>methods: 作者采用了自我混合对象函数（self-distillation）来练化预训练的HuBERT模型，并使用汇集token来概括整个句子。无需任何监督，模型能够自动从发音中找到定义的边界，并在不同帧中显示出standing的句子结构。</li>
<li>results: 作者的模型在无监督情况下自动找到了发音中的句子结构，并且与实际的句子结构大致匹配。此外，作者还提出了一个新的评价任务——Spoken Speech ABX，用于评估发音中的句子表示。与之前的模型相比，作者的模型在这两个任务中表现出色。<details>
<summary>Abstract</summary>
Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space, limiting the utility of SSL representations. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt "self-distillation" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames show salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.
</details>
<details>
<summary>摘要</summary>
<<SYS>>自动发现单元在自注意力学习（SSL）中的语音处理已经进入了新的时代。然而，发现的单元经常保留在音位空间，这限制了SSL表示的用途。我们在这里示出，在学习 sentence-level 表示的语音中，一种 syllabic 组织structure  emerges。具体来说，我们采用 "self-distillation" 目标来练化预训练 HuBERT 的汇总符号，该符号概括整个句子。无需任何超级视图，得到的模型可以在语音中画定界限，并且在帧中的表示显示出了鲜明的 syllabic 结构。我们示出，这 emergent structure 与真实的 syllables 大致匹配。此外，我们提出了一个新的 benchmark 任务，Spoken Speech ABX，用于评估 sentence-level 表示的语音。与前一代模型相比，我们的模型在无监督 syllable 发现和 sentence-level 表示学习方面表现出色。总之，我们示出了 HuBERT 的自注意力学习可以不依赖于外部标签或模式，并可能提供一种新的数据驱动单元 для spoken language modeling。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Models-of-Speech-Infer-Universal-Articulatory-Kinematics"><a href="#Self-Supervised-Models-of-Speech-Infer-Universal-Articulatory-Kinematics" class="headerlink" title="Self-Supervised Models of Speech Infer Universal Articulatory Kinematics"></a>Self-Supervised Models of Speech Infer Universal Articulatory Kinematics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10788">http://arxiv.org/abs/2310.10788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hermannovski/React">https://github.com/Hermannovski/React</a></li>
<li>paper_authors: Cheol Jun Cho, Abdelrahman Mohamed, Alan W Black, Gopala K. Anumanchipalli</li>
<li>for: 这个论文旨在探讨自动学习（SSL）基于模型在语音识别任务上的表现，以及这些模型内部表征与语音相关的关系。</li>
<li>methods: 这个论文使用了许多现代的探索技术来探索SSL模型的内部表征，包括HuBERT模型。</li>
<li>results: 研究发现，SSL模型具有一种叫做“语音生成动力学”的基本属性，即将语音信号转换为生成语音的动力学过程。此外，这种属性在不同语言训练数据上具有相似性，并且可以通过简单的仿射变换转移到不同的发音者、语言和方言上。这些结果为语音工程领域中SSL模型的性能提供了新的理解和应用前景，同时也为语音科学领域的研究提供了新的可能性。<details>
<summary>Abstract</summary>
Self-Supervised Learning (SSL) based models of speech have shown remarkable performance on a range of downstream tasks. These state-of-the-art models have remained blackboxes, but many recent studies have begun "probing" models like HuBERT, to correlate their internal representations to different aspects of speech. In this paper, we show "inference of articulatory kinematics" as fundamental property of SSL models, i.e., the ability of these models to transform acoustics into the causal articulatory dynamics underlying the speech signal. We also show that this abstraction is largely overlapping across the language of the data used to train the model, with preference to the language with similar phonological system. Furthermore, we show that with simple affine transformations, Acoustic-to-Articulatory inversion (AAI) is transferrable across speakers, even across genders, languages, and dialects, showing the generalizability of this property. Together, these results shed new light on the internals of SSL models that are critical to their superior performance, and open up new avenues into language-agnostic universal models for speech engineering, that are interpretable and grounded in speech science.
</details>
<details>
<summary>摘要</summary>
自顾学学习（SSL）基于模型的语音表现非常出色，但这些顶尖模型一直保持了黑盒模型的状态，许多最近的研究开始使用 HuBERT 等模型进行探测，以 correlate 其内部表示与不同的语音特征。在这篇论文中，我们显示了 SSL 模型中的 "语音生成动态推理" 的基本性质，即将听音信号转化为生成语音的 causal 生成动态。此外，我们还发现这种抽象在训练数据语言上具有很大的 overlap，尤其是在语音系统相似性方面。此外，我们还发现通过简单的仿射变换，可以在不同的说话者、语言和方言之间传递 AAI，这表明这种性质具有普适性。总之，这些结果为 SSL 模型的内部结构提供了新的灯光，并开启了新的语言不受限制的通用模型，这些模型可以解释并基于语音科学。
</details></li>
</ul>
<hr>
<h2 id="BanglaNLP-at-BLP-2023-Task-1-Benchmarking-different-Transformer-Models-for-Violence-Inciting-Text-Detection-in-Bengali"><a href="#BanglaNLP-at-BLP-2023-Task-1-Benchmarking-different-Transformer-Models-for-Violence-Inciting-Text-Detection-in-Bengali" class="headerlink" title="BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali"></a>BanglaNLP at BLP-2023 Task 1: Benchmarking different Transformer Models for Violence Inciting Text Detection in Bengali</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10781">http://arxiv.org/abs/2310.10781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saumajit Saha, Albert Nanda</li>
<li>for: 这个论文是关于推荐一种用于检测孟加拉语挑衅文本的系统。</li>
<li>methods: 这个系统使用了传统和现代方法，以便让模型学习。</li>
<li>results: 我们的提议系统可以判断给定文本是否含有任何威胁。我们对数据增强的影响进行了研究，并对多种转换器-基础模型进行了评估。我们在测试集上 obtained a macro F1 score of 68.11%，在共享任务中排名第23名。<details>
<summary>Abstract</summary>
This paper presents the system that we have developed while solving this shared task on violence inciting text detection in Bangla. We explain both the traditional and the recent approaches that we have used to make our models learn. Our proposed system helps to classify if the given text contains any threat. We studied the impact of data augmentation when there is a limited dataset available. Our quantitative results show that finetuning a multilingual-e5-base model performed the best in our task compared to other transformer-based architectures. We obtained a macro F1 of 68.11\% in the test set and our performance in this shared task is ranked at 23 in the leaderboard.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-reducing-hallucination-in-extracting-information-from-financial-reports-using-Large-Language-Models"><a href="#Towards-reducing-hallucination-in-extracting-information-from-financial-reports-using-Large-Language-Models" class="headerlink" title="Towards reducing hallucination in extracting information from financial reports using Large Language Models"></a>Towards reducing hallucination in extracting information from financial reports using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10760">http://arxiv.org/abs/2310.10760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhaskarjit Sarmah, Tianjie Zhu, Dhagash Mehta, Stefano Pasquali</li>
<li>for: 提高财务报告中问答部分的信息提取效率和准确率，以便更好地进行投资决策和分析。</li>
<li>methods: 使用大语言模型（LLMs）来快速和高精度地提取财务报告 транскрипts中的信息，并通过结合检索增强生成技术和元数据来减少幻觉。</li>
<li>results: 对多种LLMs进行比较，并employs objective metrics for evaluating Q&amp;A systems to demonstrate the superiority of our proposed approach.<details>
<summary>Abstract</summary>
For a financial analyst, the question and answer (Q\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\&A systems, and empirically demonstrate superiority of our method.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Building-Persona-Consistent-Dialogue-Agents-with-Offline-Reinforcement-Learning"><a href="#Building-Persona-Consistent-Dialogue-Agents-with-Offline-Reinforcement-Learning" class="headerlink" title="Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning"></a>Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10735">http://arxiv.org/abs/2310.10735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ryanshea10/personachat_offline_rl">https://github.com/ryanshea10/personachat_offline_rl</a></li>
<li>paper_authors: Ryan Shea, Zhou Yu</li>
<li>for: 提高对话系统的自然语言对话品质和个性化度</li>
<li>methods: 使用离线学习 reinforcement learning 方法，将supervised learning 和 online reinforcement learning 的优点结合在一起，并 introduce 一种减少重要性权重的自适应重要性 sampling 方法</li>
<li>results: 对一个现有的社交聊天机器人进行自动和人类评估，结果显示，该方法可以提高对话系统的自然语言对话品质和个性化度<details>
<summary>Abstract</summary>
Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show that our framework improves both the persona consistency and dialogue quality of a state-of-the-art social chatbot.
</details>
<details>
<summary>摘要</summary>
保持一致的人格是对任何开放领域对话系统的关键质量。现状之 artifical intelligence 系统通常通过经过监督学习或在线强化学习（RL）训练来实现这一目标。然而，通过监督学习训练的系统经常缺乏一致性，因为它们从来没有受到违反的惩罚。额外的 RL 训练可以减轻一些这些问题，但训练过程是昂贵的。因此，我们提出了一个Offline RL框架，以提高对话系统的人格一致性。我们的框架允许我们将supervised learning中的优点与RL中的优点结合起来，并且可以廉价地在现有数据上训练我们的模型。我们还提出了一种简单的重要性抽样方法，以减少偏移重要性抽样的方差，我们称之为“Variance-Reducing MLE-Initialized”（VaRMI）重要性抽样。我们的自动和人类评估表明，我们的框架可以提高一个现有社交聊天机器人的人格一致性和对话质量。
</details></li>
</ul>
<hr>
<h2 id="“Mistakes-Help-Us-Grow”-Facilitating-and-Evaluating-Growth-Mindset-Supportive-Language-in-Classrooms"><a href="#“Mistakes-Help-Us-Grow”-Facilitating-and-Evaluating-Growth-Mindset-Supportive-Language-in-Classrooms" class="headerlink" title="“Mistakes Help Us Grow”: Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms"></a>“Mistakes Help Us Grow”: Facilitating and Evaluating Growth Mindset Supportive Language in Classrooms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10637">http://arxiv.org/abs/2310.10637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Handa, Margaret Clapper, Jessica Boyle, Rose E Wang, Diyi Yang, David S Yeager, Dorottya Demszky</li>
<li>for: 这个论文目的是探讨使用大自然语言模型（LLM）提供自动化、个性化的教师培训，以促进教师的成长心理语言支持（GMSL）。</li>
<li>methods: 这个论文使用了以下方法：（1）建立了一个平行数据集，其中包含GMSL培训的教师重构不支持性语言的示例，并提供了一个批注指南；（2）开发了GMSL提问框架，用于修改教师的不支持性语言；（3）采用了基于心理理论的评价框架，用于评价GMSL的效果。</li>
<li>results: 这个论文的研究结果显示，both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. In addition, model-generated reframings outperform those from the GMSL-trained teachers. These results demonstrate the promise of using LLMs to provide automated GMSL feedback for teachers, and more broadly, the potential of LLMs for supporting students’ learning in the classroom.<details>
<summary>Abstract</summary>
Teachers' growth mindset supportive language (GMSL)--rhetoric emphasizing that one's skills can be improved over time--has been shown to significantly reduce disparities in academic achievement and enhance students' learning outcomes. Although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers' use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers' unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs' potentiality for supporting students' learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains.
</details>
<details>
<summary>摘要</summary>
教师的增长心理支持语言（GMSL）——强调一个人的技能可以逐渐提高——已经显著减少学生学习成绩的差距和提高学生的学习效果。 although teachers espouse growth mindset principles, most find it difficult to adopt GMSL in their practice due to the lack of effective coaching in this area. We explore whether large language models (LLMs) can provide automated, personalized coaching to support teachers' use of GMSL. We establish an effective coaching tool to reframe unsupportive utterances to GMSL by developing (i) a parallel dataset containing GMSL-trained teacher reframings of unsupportive statements with an accompanying annotation guide, (ii) a GMSL prompt framework to revise teachers' unsupportive language, and (iii) an evaluation framework grounded in psychological theory for evaluating GMSL with the help of students and teachers. We conduct a large-scale evaluation involving 174 teachers and 1,006 students, finding that both teachers and students perceive GMSL-trained teacher and model reframings as more effective in fostering a growth mindset and promoting challenge-seeking behavior, among other benefits. We also find that model-generated reframings outperform those from the GMSL-trained teachers. These results show promise for harnessing LLMs to provide automated GMSL feedback for teachers and, more broadly, LLMs' potentiality for supporting students' learning in the classroom. Our findings also demonstrate the benefit of large-scale human evaluations when applying LLMs in educational domains.
</details></li>
</ul>
<hr>
<h2 id="Data-Contamination-Through-the-Lens-of-Time"><a href="#Data-Contamination-Through-the-Lens-of-Time" class="headerlink" title="Data Contamination Through the Lens of Time"></a>Data Contamination Through the Lens of Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10628">http://arxiv.org/abs/2310.10628</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abacusai/to-the-cutoff">https://github.com/abacusai/to-the-cutoff</a></li>
<li>paper_authors: Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, Samuel Dooley</li>
<li>for:  This paper aims to investigate the issue of data contamination in large language models (LLMs) by analyzing the trends in LLM pass rates and their relationship with GitHub popularity and release date.</li>
<li>methods:  The authors use a natural experiment of training cutoffs in GPT models to examine benchmarks released over time, specifically focusing on two code&#x2F;mathematical problem-solving datasets, Codeforces and Project Euler. They employ a longitudinal analysis approach to identify statistically significant trends in LLM pass rates.</li>
<li>results:  The authors find strong evidence of data contamination in LLMs, as reflected in the statistically significant trends in LLM pass rates vs. GitHub popularity and release date. They also open-source their dataset, raw results, and evaluation framework to facilitate rigorous analyses of data contamination in modern models.<details>
<summary>Abstract</summary>
Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. Data contamination remains notoriously challenging to measure and mitigate, even with partial attempts like controlled experimentation of training data, canary strings, or embedding similarities. In this work, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination. By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of LLMs that train on webscale data.
</details>
<details>
<summary>摘要</summary>
In this study, we conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination.By open-sourcing our dataset, raw results, and evaluation framework, our work paves the way for rigorous analyses of data contamination in modern models. We conclude with a discussion of best practices and future steps for publicly releasing benchmarks in the age of LLMs that train on webscale data.
</details></li>
</ul>
<hr>
<h2 id="ForceGen-End-to-end-de-novo-protein-generation-based-on-nonlinear-mechanical-unfolding-responses-using-a-protein-language-diffusion-model"><a href="#ForceGen-End-to-end-de-novo-protein-generation-based-on-nonlinear-mechanical-unfolding-responses-using-a-protein-language-diffusion-model" class="headerlink" title="ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model"></a>ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10605">http://arxiv.org/abs/2310.10605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Ni, David L. Kaplan, Markus J. Buehler</li>
<li>for: 本研究旨在开发一种可预测性强的蛋白质设计模型，以满足复杂非线性机械性质设计目标。</li>
<li>methods: 该模型基于先前训练的蛋白质语言模型，利用蛋白质序列深度知识，将机械 unfolding 响应映射到创造新蛋白质。</li>
<li>results: 通过全原子分子动力学 simulate，证明设计出的蛋白质是新的，并满足目标的机械性质，包括 unfolding energy 和机械强度，以及细致的 unfolding force-separation 曲线。<details>
<summary>Abstract</summary>
Through evolution, nature has presented a set of remarkable protein materials, including elastins, silks, keratins and collagens with superior mechanical performances that play crucial roles in mechanobiology. However, going beyond natural designs to discover proteins that meet specified mechanical properties remains challenging. Here we report a generative model that predicts protein designs to meet complex nonlinear mechanical property-design objectives. Our model leverages deep knowledge on protein sequences from a pre-trained protein language model and maps mechanical unfolding responses to create novel proteins. Via full-atom molecular simulations for direct validation, we demonstrate that the designed proteins are novel, and fulfill the targeted mechanical properties, including unfolding energy and mechanical strength, as well as the detailed unfolding force-separation curves. Our model offers rapid pathways to explore the enormous mechanobiological protein sequence space unconstrained by biological synthesis, using mechanical features as target to enable the discovery of protein materials with superior mechanical properties.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Motion2Language-Unsupervised-learning-of-synchronized-semantic-motion-segmentation"><a href="#Motion2Language-Unsupervised-learning-of-synchronized-semantic-motion-segmentation" class="headerlink" title="Motion2Language, Unsupervised learning of synchronized semantic motion segmentation"></a>Motion2Language, Unsupervised learning of synchronized semantic motion segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10594">http://arxiv.org/abs/2310.10594</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rd20karim/M2T-Segmentation">https://github.com/rd20karim/M2T-Segmentation</a></li>
<li>paper_authors: Karim Radouane, Andon Tchechmedjiev, Sylvie Ranwez, Julien Lagarde</li>
<li>for: 这个论文的目的是建立一种序列到序列架构，用于将动作捕获输入翻译成英语自然语言描述，并同时生成描述和动作的同步。</li>
<li>methods: 论文提出了一种新的循环式注意力形式，适用于同步生成文本，以及一种改进的动作编码器架构，适用于更小的数据集和同步生成。</li>
<li>results: 经过测试，提出的注意力机制和编码器架构都有加成效果，可以提高生成文本的质量（BLEU和Semantic Equivalence）以及同步性。<details>
<summary>Abstract</summary>
In this paper, we investigate building a sequence to sequence architecture for motion to language translation and synchronization. The aim is to translate motion capture inputs into English natural-language descriptions, such that the descriptions are generated synchronously with the actions performed, enabling semantic segmentation as a byproduct, but without requiring synchronized training data. We propose a new recurrent formulation of local attention that is suited for synchronous/live text generation, as well as an improved motion encoder architecture better suited to smaller data and for synchronous generation. We evaluate both contributions in individual experiments, using the standard BLEU4 metric, as well as a simple semantic equivalence measure, on the KIT motion language dataset. In a follow-up experiment, we assess the quality of the synchronization of generated text in our proposed approaches through multiple evaluation metrics. We find that both contributions to the attention mechanism and the encoder architecture additively improve the quality of generated text (BLEU and semantic equivalence), but also of synchronization. Our code will be made available at \url{https://github.com/rd20karim/M2T-Segmentation/tree/main}
</details>
<details>
<summary>摘要</summary>
本文 investigate 建立一种序列到序列架构，用于动作到语言翻译和同步。目标是将动作捕获输入翻译成英语自然语言描述，以便在动作发生时同步生成描述，而无需同步训练数据。我们提出了一种新的循环形式的本地注意力表示，适合同步生成文本，以及一种改进的动作编码建立，更适合小型数据和同步生成。我们在使用标准的BLEU4指标和简单的 semantics 等价度量进行评估，并在 KIT 动作语言数据集上进行单独的实验。在后续实验中，我们评估了我们的提议中的同步生成文本质量，通过多种评价指标。我们发现， both 注意力机制和编码建立增加了生成文本质量（BLEU和semantics），同时也提高了同步生成的质量。我们的代码将在 \url{https://github.com/rd20karim/M2T-Segmentation/tree/main} 上提供。
</details></li>
</ul>
<hr>
<h2 id="Mastering-the-Task-of-Open-Information-Extraction-with-Large-Language-Models-and-Consistent-Reasoning-Environment"><a href="#Mastering-the-Task-of-Open-Information-Extraction-with-Large-Language-Models-and-Consistent-Reasoning-Environment" class="headerlink" title="Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment"></a>Mastering the Task of Open Information Extraction with Large Language Models and Consistent Reasoning Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10590">http://arxiv.org/abs/2310.10590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Qi, Kaixuan Ji, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Lei Hou, Juanzi Li, Bin Xu</li>
<li>for: 解决对自然语言文本中的 объектив结构知识抽取任务的问题，以建立专门的模型。</li>
<li>methods: 使用语言模型进行启发式学习，并提出一种方法来评估语言模型与测试样本之间的语法分布差异，以作为准备证明。</li>
<li>results: 通过在标准 CaRB benchmark上进行 $6$-shot 方法，实现了超过现有监督方法的 $55.3$ $F_1$ 分数，并在 TACRED 和 ACE05 上进行了natural generalization，实现了 $5.7$ 和 $6.8$ $F_1$ 分数的提高。<details>
<summary>Abstract</summary>
Open Information Extraction (OIE) aims to extract objective structured knowledge from natural texts, which has attracted growing attention to build dedicated models with human experience. As the large language models (LLMs) have exhibited remarkable in-context learning capabilities, a question arises as to whether the task of OIE can be effectively tackled with this paradigm? In this paper, we explore solving the OIE problem by constructing an appropriate reasoning environment for LLMs. Specifically, we first propose a method to effectively estimate the discrepancy of syntactic distribution between a LLM and test samples, which can serve as correlation evidence for preparing positive demonstrations. Upon the evidence, we introduce a simple yet effective mechanism to establish the reasoning environment for LLMs on specific tasks. Without bells and whistles, experimental results on the standard CaRB benchmark demonstrate that our $6$-shot approach outperforms state-of-the-art supervised method, achieving an $55.3$ $F_1$ score. Further experiments on TACRED and ACE05 show that our method can naturally generalize to other information extraction tasks, resulting in improvements of $5.7$ and $6.8$ $F_1$ scores, respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BiLL-VTG-Bridging-Large-Language-Models-and-Lightweight-Visual-Tools-for-Video-based-Texts-Generation"><a href="#BiLL-VTG-Bridging-Large-Language-Models-and-Lightweight-Visual-Tools-for-Video-based-Texts-Generation" class="headerlink" title="BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"></a>BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10586">http://arxiv.org/abs/2310.10586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Qi, Kaixuan Ji, Jifan Yu, Duokang Wang, Bin Xu, Lei Hou, Juanzi Li</li>
<li>for: 本文旨在提出一种快速适应性框架，以便基于视频进行文本回答。</li>
<li>methods: 本文使用了大量语言模型（LLM）来进行视频理解和知识推理。 Specifically, 我们发现回答特定指令的关键在于关注相关视频事件，并使用了两种视觉工具：结构化场景图生成和描述性图像标题生成来收集和表示事件信息。 然后，一个搭载了世界知识的 LLM 被用作理解代理，通过多个理解步骤来实现回答。</li>
<li>results: 我们的框架在两个常见的视频文本生成任务上表现出STATE-OF-THE-ART的性能，并且不需要训练。<details>
<summary>Abstract</summary>
Building models that generate textual responses to user instructions for videos is a practical and challenging topic, as it requires both vision understanding and knowledge reasoning. Compared to language and image modalities, training efficiency remains a serious problem as existing studies train models on massive sparse videos aligned with brief descriptions. In this paper, we introduce BiLL-VTG, a fast adaptive framework that leverages large language models (LLMs) to reasoning on videos based on essential lightweight visual tools. Specifically, we reveal the key to response specific instructions is the concentration on relevant video events, and utilize two visual tools of structured scene graph generation and descriptive image caption generation to gather and represent the events information. Thus, a LLM equipped with world knowledge is adopted as the reasoning agent to achieve the response by performing multiple reasoning steps on specified video events.To address the difficulty of specifying events from agent, we further propose an Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the efficient Hungarian matching to localize corresponding video events using linguistic instructions, enabling LLMs to interact with long videos. Extensive experiments on two typical video-based texts generations tasks show that our tuning-free framework outperforms the pre-trained models including Flamingo-80B, to achieve the state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:建立基于视频的模型，以生成用户 instrucion 的文本响应是一个实用和挑战的话题，因为它需要视觉理解和知识推理。相比语言和图像模式，训练效率仍然是一个严重的问题，因为现有的研究通常使用大量稀疏的视频和简短的描述进行训练。在这篇论文中，我们介绍了 BiLL-VTG 框架，该框架利用大型语言模型（LLM）来基于视频中的关键事件进行推理。我们发现关键在于响应特定的 instrucion 是关注相关的视频事件，并使用两种视觉工具：结构化场景图生成和描述性图像标签生成来收集和表示事件信息。然后，一个装备了世界知识的 LLM 作为推理代理来实现响应，通过多个推理步骤来处理指定的视频事件。为了解决指定事件的困难，我们还提出了一种基于有效的匈牙利匹配的 Instruction-oriented Video Events Recognition（InsOVER）算法，以便 LLMS 与长视频进行交互。我们在两个典型的视频基于文本生成任务上进行了广泛的实验，结果显示，我们的自适应框架在与 Flamingo-80B 等预训练模型进行比较时，具有更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Who-Are-All-The-Stochastic-Parrots-Imitating-They-Should-Tell-Us"><a href="#Who-Are-All-The-Stochastic-Parrots-Imitating-They-Should-Tell-Us" class="headerlink" title="Who Are All The Stochastic Parrots Imitating? They Should Tell Us!"></a>Who Are All The Stochastic Parrots Imitating? They Should Tell Us!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10583">http://arxiv.org/abs/2310.10583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagi Shaier, Lawrence E. Hunter, Katharina von der Wense</li>
<li>for: 这篇论文主要是关于语言模型（LM）的可靠性问题。</li>
<li>methods: 作者建议使用LM可以引用其训练数据的方法，以便快速验证LM生成的声明的真实性。</li>
<li>results: 作者认为，当前的LM在重要场景中永远不会被完全信任，并建议一种新的策略来解决这个问题，即建立LM可以引用其训练数据的能力。<details>
<summary>Abstract</summary>
Both standalone language models (LMs) as well as LMs within downstream-task systems have been shown to generate statements which are factually untrue. This problem is especially severe for low-resource languages, where training data is scarce and of worse quality than for high-resource languages. In this opinion piece, we argue that LMs in their current state will never be fully trustworthy in critical settings and suggest a possible novel strategy to handle this issue: by building LMs such that can cite their sources - i.e., point a user to the parts of their training data that back up their outputs. We first discuss which current NLP tasks would or would not benefit from such models. We then highlight the expected benefits such models would bring, e.g., quick verifiability of statements. We end by outlining the individual tasks that would need to be solved on the way to developing LMs with the ability to cite. We hope to start a discussion about the field's current approach to building LMs, especially for low-resource languages, and the role of the training data in explaining model generations.
</details>
<details>
<summary>摘要</summary>
各种自然语言处理（NLP）任务中的语言模型（LM）都有可能生成不准确的陈述，特别是 для低资源语言，训练数据稀缺，质量也较差。在这篇意见文章中，我们 argue that LMs 在当前状态下从不能在重要场景中得到完全信任，并提出一种可能的新策略来解决这个问题：建立LMs 可以指明其所基于的训练数据部分，即用户可以通过点击LMs 的输出来找到相应的训练数据。我们首先讨论了当前NLP任务中哪些任务可以或不可以受益于这种模型，然后描述了这种模型带来的预期优势，例如快速验证陈述的可靠性。最后，我们列出了需要解决的任务，以开发LMs 可以指明其所基于的训练数据部分。我们希望通过这篇文章引发关于当前LMs 建设的讨论，特别是低资源语言的LMs，以及训练数据的角色在解释模型生成中。
</details></li>
</ul>
<hr>
<h2 id="Emerging-Challenges-in-Personalized-Medicine-Assessing-Demographic-Effects-on-Biomedical-Question-Answering-Systems"><a href="#Emerging-Challenges-in-Personalized-Medicine-Assessing-Demographic-Effects-on-Biomedical-Question-Answering-Systems" class="headerlink" title="Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems"></a>Emerging Challenges in Personalized Medicine: Assessing Demographic Effects on Biomedical Question Answering Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10571">http://arxiv.org/abs/2310.10571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagi Shaier, Kevin Bennett, Lawrence Hunter, Katharina von der Wense</li>
<li>for: 本研究旨在检测生物医学问答模型是否受到人群特征影响，以确保医疗公平。</li>
<li>methods: 研究使用了不同类型的问答模型，包括基于知识图（KG）和文本基于的模型，并对它们进行了测试。</li>
<li>results: 研究发现， irrelevant demographic information可以导致问答模型的答案发生变化，变化的比例可达15%（基于知识图）和23%（基于文本）。这些变化可能会影响准确性。<details>
<summary>Abstract</summary>
State-of-the-art question answering (QA) models exhibit a variety of social biases (e.g., with respect to sex or race), generally explained by similar issues in their training data. However, what has been overlooked so far is that in the critical domain of biomedicine, any unjustified change in model output due to patient demographics is problematic: it results in the unfair treatment of patients. Selecting only questions on biomedical topics whose answers do not depend on ethnicity, sex, or sexual orientation, we ask the following research questions: (RQ1) Do the answers of QA models change when being provided with irrelevant demographic information? (RQ2) Does the answer of RQ1 differ between knowledge graph (KG)-grounded and text-based QA systems? We find that irrelevant demographic information change up to 15% of the answers of a KG-grounded system and up to 23% of the answers of a text-based system, including changes that affect accuracy. We conclude that unjustified answer changes caused by patient demographics are a frequent phenomenon, which raises fairness concerns and should be paid more attention to.
</details>
<details>
<summary>摘要</summary>
现代问答（QA）模型表现出多种社会偏见（例如与性别或种族相关），通常可以归因于训练数据中的类似问题。然而，到目前为止忽略了在重要领域生物医学中，任何不当的模型输出变化因为病人特征是问题：它会导致患者不公正地处理。我们选择仅考虑不依赖性别、性别或性 orientation 的生物医学问题，并提出以下研究问题：（RQ1）QA 模型在接受无关的民族信息时是否发生变化？（RQ2）对知识图（KG）基础的 QA 系统和文本基础的 QA 系统而言，RQ1 的答案是否不同？我们发现，无关民族信息可以改变 KG 基础系统的答案，占总答案的 15%，而文本基础系统的答案则占 23%，包括影响准确性的变化。我们 conclude 这种不当的答案变化是常见的，这引发公平问题，需要更多的注意。
</details></li>
</ul>
<hr>
<h2 id="On-Position-Bias-in-Summarization-with-Large-Language-Models"><a href="#On-Position-Bias-in-Summarization-with-Large-Language-Models" class="headerlink" title="On Position Bias in Summarization with Large Language Models"></a>On Position Bias in Summarization with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10570">http://arxiv.org/abs/2310.10570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathieu Ravaut, Shafiq Joty, Aixin Sun, Nancy F. Chen</li>
<li>for: 本研究旨在探讨语言模型在多文档问答 задании中如何利用输入Context，以及这些模型在摘要生成任务中的表现。</li>
<li>methods: 本研究使用了10个数据集、4个语言模型和5个评价指标来分析语言模型在摘要生成任务中如何利用其输入。</li>
<li>results: 研究发现，语言模型倾向于使用 introduce content（以及一定程度的 final content），导致摘要生成性能呈U型曲线。这种偏好对多种多样化的摘要任务提出了挑战。<details>
<summary>Abstract</summary>
Large language models (LLMs) excel in zero-shot abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, surpassing token limits of 32k or more. However, in the realm of multi-document question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization tasks where crucial content may be dispersed throughout the source document(s). This paper presents a comprehensive investigation encompassing 10 datasets, 4 LLMs, and 5 evaluation metrics to analyze how these models leverage their input for abstractive summarization. Our findings reveal a pronounced bias towards the introductory content (and to a lesser extent, the final content), posing challenges for LLM performance across a range of diverse summarization benchmarks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在零shot摘要任务中表现出色，提供流畅和有关的摘要。最近的进步使其能处理长输入上下文，超过32k个Token的限制。然而，在多文档问答任务中，语言模型表现出输入上下文不均匀的问题。它们倾向于初始和 final段，导致摘要性能形成U型曲线，其中答案位于输入中的任何位置。这种偏见存在问题，特别是在摘要任务中，重要的内容可能会分散在源文档中。本文通过10个数据集、4个LLM和5个评价指标进行全面的调查，分析这些模型如何使用其输入进行摘要。我们发现，LLM偏向于引言内容（以及一定 extent的 final content），这会影响LLM在多种多样的摘要benchmark上的表现。
</details></li>
</ul>
<hr>
<h2 id="RegaVAE-A-Retrieval-Augmented-Gaussian-Mixture-Variational-Auto-Encoder-for-Language-Modeling"><a href="#RegaVAE-A-Retrieval-Augmented-Gaussian-Mixture-Variational-Auto-Encoder-for-Language-Modeling" class="headerlink" title="RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling"></a>RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10567">http://arxiv.org/abs/2310.10567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingcheng Deng, Liang Pang, Huawei Shen, Xueqi Cheng</li>
<li>for: 提高语言模型（LM）的表达质量和减少幻觉</li>
<li>methods: 使用检索增强的语言模型（RegaVAE），其基于变量自动编码器（VAE），并在检索和生成过程中使用嵌入空间来捕捉当前和未来文本的信息</li>
<li>results: 在多个 dataset 上实现了显著提高表达质量和幻觉的除去<details>
<summary>Abstract</summary>
Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the latent space and adopt the probabilistic form of the retrieval generation paradigm by expanding the Gaussian prior distribution into a Gaussian mixture distribution. Theoretical analysis provides an optimizable upper bound for RegaVAE. Experimental results on various datasets demonstrate significant improvements in text generation quality and hallucination removal.
</details>
<details>
<summary>摘要</summary>
Translation note:* "outdated information" is translated as "过时信息" (guòshí xīnxiàng)* "hallucinations" is translated as "幻见" (hénjiàn)* "latent variables" is translated as "隐变量" (yǐbiàn yuán)* "compact latent space" is translated as "紧凑的隐藏空间" (jìchōng de yǐnmo yòngkōng)* "raw text" is translated as "原始文本" (yuánshi wén tiān)* "Gaussian prior distribution" is translated as "高斯先验分布" (gāosī xiān yì fāngbù)* "Gaussian mixture distribution" is translated as "高斯混合分布" (gāosī hùn yì fāngbù)* "theoretical analysis" is translated as "理论分析" (lǐlùn fāng'àn)* "upper bound" is translated as "上限" (shàngjìn)
</details></li>
</ul>
<hr>
<h2 id="ViPE-Visualise-Pretty-much-Everything"><a href="#ViPE-Visualise-Pretty-much-Everything" class="headerlink" title="ViPE: Visualise Pretty-much Everything"></a>ViPE: Visualise Pretty-much Everything</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10543">http://arxiv.org/abs/2310.10543</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hazel1994/ViPE-Videos">https://github.com/Hazel1994/ViPE-Videos</a></li>
<li>paper_authors: Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch</li>
<li>for: This paper aims to address the issue of text-to-image models struggling to depict non-literal expressions, by introducing a new method called ViPE.</li>
<li>methods: ViPE uses a series of lightweight and robust language models trained on a large-scale set of lyrics with noisy visual descriptions generated by GPT3.5.</li>
<li>results: ViPE effectively expresses any arbitrary piece of text into a visualisable description, and exhibits an understanding of figurative expressions comparable to human experts. It also provides a powerful and open-source backbone for downstream applications such as music video and caption generation.<details>
<summary>Abstract</summary>
Figurative and non-literal expressions are profoundly integrated in human communication. Visualising such expressions allow us to convey our creative thoughts, and evoke nuanced emotions. Recent text-to-image models like Stable Diffusion, on the other hand, struggle to depict non-literal expressions. Recent works primarily deal with this issue by compiling humanly annotated datasets on a small scale, which not only demands specialised expertise but also proves highly inefficient. To address this issue, we introduce ViPE: Visualise Pretty-much Everything. ViPE offers a series of lightweight and robust language models that have been trained on a large-scale set of lyrics with noisy visual descriptions that represent their implicit meaning. The synthetic visual descriptions are generated by GPT3.5 relying on neither human annotations nor images. ViPE effectively expresses any arbitrary piece of text into a visualisable description, enabling meaningful and high-quality image generation. We provide compelling evidence that ViPE is more robust than GPT3.5 in synthesising visual elaborations. ViPE also exhibits an understanding of figurative expressions comparable to human experts, providing a powerful and open-source backbone to many downstream applications such as music video and caption generation.
</details>
<details>
<summary>摘要</summary>
人类communication中的 figurative 和非Literal 表达是极其深入地融合在一起。Visualizing这些表达可以帮助我们表达创造性的思想，并触发细腻的情感。然而，现有的文本-图像模型，如Stable Diffusion，在描绘非Literal表达方面几乎无法表现出来。现有的工作主要采取了 compile humanly annotated datasets的方法，这不仅需要专业知识，还证明高效率。为解决这个问题，我们引入了 ViPE：Visualize Pretty-much Everything。ViPE 提供了一系列轻量级和可靠的语言模型，这些模型在大规模的歌词中生成了噪音的视觉描述。这些synthetic visual descriptions 由 GPT3.5 生成，不需要人类注释也不需要图像。ViPE 可以将任何文本转换成可视化的描述，从而实现了高质量的图像生成。我们提供了吸引人的证明，表明 ViPE 比 GPT3.5 更加稳定在生成视觉 elaborations 方面。ViPE 还表现出了对 figurative expressions 的理解，与人类专家相当，提供了一个强大且开源的基础结构，可以推动多个下游应用，如音乐视频和caption生成。
</details></li>
</ul>
<hr>
<h2 id="One-For-All-All-For-One-Bypassing-Hyperparameter-Tuning-with-Model-Averaging-For-Cross-Lingual-Transfer"><a href="#One-For-All-All-For-One-Bypassing-Hyperparameter-Tuning-with-Model-Averaging-For-Cross-Lingual-Transfer" class="headerlink" title="One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model Averaging For Cross-Lingual Transfer"></a>One For All &amp; All For One: Bypassing Hyperparameter Tuning with Model Averaging For Cross-Lingual Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10532">http://arxiv.org/abs/2310.10532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fdschmidt93/ofa-xlt">https://github.com/fdschmidt93/ofa-xlt</a></li>
<li>paper_authors: Fabian David Schmidt, Ivan Vulić, Goran Glavaš</li>
<li>for: 这个论文主要探讨了零例转移跨语言传递（ZS-XLT）的效iveness，以及如何选择最佳的模型和 hyperparameter。</li>
<li>methods: 该论文使用了多语言模型，并在不同的语言上进行了针对性的训练和测试。具体来说， authors 使用了不同的 hyperparameter 和模型Snapshot来进行训练和测试，并通过accumulative run-by-run averaging来提高 ZS-XLT 的性能。</li>
<li>results: 研究发现，传统的模型选择方法 based on source-language validation 很快就达到了下降的 ZS-XLT 性能。然而，通过accumulative run-by-run averaging来提高 ZS-XLT 性能，并与 “oracle” ZS-XLT 表现高度相关。<details>
<summary>Abstract</summary>
Multilingual language models enable zero-shot cross-lingual transfer (ZS-XLT): fine-tuned on sizable source-language task data, they perform the task in target languages without labeled instances. The effectiveness of ZS-XLT hinges on the linguistic proximity between languages and the amount of pretraining data for a language. Because of this, model selection based on source-language validation is unreliable: it picks model snapshots with suboptimal target-language performance. As a remedy, some work optimizes ZS-XLT by extensively tuning hyperparameters: the follow-up work then routinely struggles to replicate the original results. Other work searches over narrower hyperparameter grids, reporting substantially lower performance. In this work, we therefore propose an unsupervised evaluation protocol for ZS-XLT that decouples performance maximization from hyperparameter tuning. As a robust and more transparent alternative to extensive hyperparameter tuning, we propose to accumulatively average snapshots from different runs into a single model. We run broad ZS-XLT experiments on both higher-level semantic tasks (NLI, extractive QA) and a lower-level token classification task (NER) and find that conventional model selection based on source-language validation quickly plateaus to suboptimal ZS-XLT performance. On the other hand, our accumulative run-by-run averaging of models trained with different hyperparameters boosts ZS-XLT performance and closely correlates with "oracle" ZS-XLT, i.e., model selection based on target-language validation performance.
</details>
<details>
<summary>摘要</summary>
多语言语模型可以实现零码跨语言传递（ZS-XLT）：经过精心适应源语言任务数据，它们可以在目标语言中完成任务无需标注实例。ZS-XLT的有效性取决于语言之间的语言相似性和语言预训练数据的量。因此，基于源语言验证的模型选择是不可靠的：它可能会选择模型快照中的产生性能不佳的模型。为了解决这个问题，一些研究者们在ZS-XLT中进行了广泛的超参数优化：然而，继续的研究往往难以复制原来的结果。其他研究者们在 narrower 的超参数格上进行了搜索，并报告了较低的性能。在这个研究中，我们因此提出了一种无监督的评估协议，以减少精度优化和超参数优化之间的关系。我们提议通过在不同的run中训练不同的超参数，并将这些run中的模型快照相加，以获得一个更加 robust 和 transparent 的ZS-XLT模型。我们在高级semantic任务（NLI、抽取式问答）和 lower-level 字符串分类任务（NER）上进行了广泛的ZS-XLT实验，并发现了以下结论：在源语言验证中选择模型的方法很快就到达了低效的ZS-XLT性能，而我们的积累run-by-run相加的模型快照则可以提高ZS-XLT性能，并与“oracle” ZS-XLT（基于目标语言验证性能进行选择）高度相关。
</details></li>
</ul>
<hr>
<h2 id="Metric-Ensembles-For-Hallucination-Detection"><a href="#Metric-Ensembles-For-Hallucination-Detection" class="headerlink" title="Metric Ensembles For Hallucination Detection"></a>Metric Ensembles For Hallucination Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10495">http://arxiv.org/abs/2310.10495</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/parthk279/Hallucination-Research">https://github.com/parthk279/Hallucination-Research</a></li>
<li>paper_authors: Grant C. Forbes, Parth Katlana, Zeydy Ortiz</li>
<li>for: 这篇论文主要研究了对摘要的自动生成中减少“幻”信息（不在原始文档中出现的信息）的问题，以及关于这个问题的评估方法。</li>
<li>methods: 该论文使用了许多不同的无监督度量来评估摘要的一致性，并对这些度量之间的相关性和人工评估分数的相关性进行了分析。</li>
<li>results: 研究发现，使用LLM（大型语言模型）基于的方法可以更好地检测摘要中的幻信息，而且 ensemble方法可以进一步提高这些分数。此外，研究还发现，要使ensemble方法有所提高，则需要确保度量在ensemble中具有足够相似的错误率，而不需要完全相同的错误率。<details>
<summary>Abstract</summary>
Abstractive text summarization has garnered increased interest as of late, in part due to the proliferation of large language models (LLMs). One of the most pressing problems related to generation of abstractive summaries is the need to reduce "hallucinations," information that was not included in the document being summarized, and which may be wholly incorrect. Due to this need, a wide array of metrics estimating consistency with the text being summarized have been proposed. We examine in particular a suite of unsupervised metrics for summary consistency, and measure their correlations with each other and with human evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then compare these evaluations to models made from a simple linear ensemble of these metrics. We find that LLM-based methods outperform other unsupervised metrics for hallucination detection. We also find that ensemble methods can improve these scores even further, provided that the metrics in the ensemble have sufficiently similar and uncorrelated error rates. Finally, we present an ensemble method for LLM-based evaluations that we show improves over this previous SOTA.
</details>
<details>
<summary>摘要</summary>
抽象摘要生成技术在最近几年来得到了更多的关注，一部分这是因为大语言模型（LLM）的普及。摘要生成中最大的问题之一是减少“幻觉”，即文档中没有包含的信息，而且可能完全错误。由于这一需求，一系列用于摘要与文档之间的一致性的度量被提出。我们专门研究了这些无监督度量的套件，并测量它们之间的相关性和人工评价分数在wiki_bio_gpt3_hallucination数据集中的相关性。然后，我们比较了这些评价与模型中的其他无监督度量和LLM-based方法的性能。我们发现LLM-based方法在幻觉检测方面表现出色，而且 ensemble方法可以进一步提高这些分数，只要 ensemble中的度量具有相似的错误率。最后，我们提出了一种ensemble方法，可以further improve sobre la última SOTA。
</details></li>
</ul>
<hr>
<h2 id="UNO-DST-Leveraging-Unlabelled-Data-in-Zero-Shot-Dialogue-State-Tracking"><a href="#UNO-DST-Leveraging-Unlabelled-Data-in-Zero-Shot-Dialogue-State-Tracking" class="headerlink" title="UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking"></a>UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10492">http://arxiv.org/abs/2310.10492</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lichuangnus/uno-dst">https://github.com/lichuangnus/uno-dst</a></li>
<li>paper_authors: Chuang Li, Yan Zhang, Min-Yen Kan, Haizhou Li</li>
<li>for: 这篇论文是为了提出一种基于少量数据的零shot对话状态跟踪（DST）方法，以便在目标领域中进行自动标注。</li>
<li>methods: 该方法使用了 auxiliary tasks 生成槽类作为主任务的 inverse prompt，通过联合自我训练来使用无标记数据来增强 DST 模型的训练和精度。</li>
<li>results: 在 MultiWOZ 多语言对话场景中，该方法可以提高平均联合目标任务准确率 by 8%，表明该方法可以有效地提高 DST 模型在零shot 情况下的性能。<details>
<summary>Abstract</summary>
Previous zero-shot dialogue state tracking (DST) methods only apply transfer learning, but ignore unlabelled data in the target domain. We transform zero-shot DST into few-shot DST by utilising such unlabelled data via joint and self-training methods. Our method incorporates auxiliary tasks that generate slot types as inverse prompts for main tasks, creating slot values during joint training. Cycle consistency between these two tasks enables the generation and selection of quality samples in unknown target domains for subsequent fine-tuning. This approach also facilitates automatic label creation, thereby optimizing the training and fine-tuning of DST models. We demonstrate this method's effectiveness on large language models in zero-shot scenarios, improving average joint goal accuracy by $8\%$ across all domains in MultiWOZ.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "zero-shot" is translated as "无需标注的" (wú shí biāo yì)* "few-shot" is translated as "几个shot" (jī gè shòu)* "transfer learning" is translated as "传输学习" (chuán xiū xué xí)* "joint training" is translated as "共同训练" (gòng tóng xiǎo xíng)* "self-training" is translated as "自我训练" (zi wo xiǎo xíng)* "auxiliary tasks" is translated as "辅助任务" (bù zhù zhì gōng)* "slot types" is translated as "槽类型" (shí kè yì)* "inverse prompts" is translated as "反向提示" (fǎn xiàng tím shì)* "main tasks" is translated as "主要任务" (zhǔ yào zhì gōng)* "cycle consistency" is translated as "循环一致" (xún huán yī zhì)* "quality samples" is translated as "高质量的样本" (gāo zhì yàng yī xiǎng)* "unknown target domains" is translated as "未知目标领域" (wèi zhī mù bì yì zhòng)* "subsequent fine-tuning" is translated as "后续精度调整" (hòu xù jīng dù jiǎo yì)* "large language models" is translated as "大型自然语言模型" (dà xíng zì rán yǔ yán mó delì)* "improving" is translated as "提高" (tí gāo)* "average joint goal accuracy" is translated as "平均共同目标准确率" (píng jìn gòng tóng mù zhì jīn yì)Note: The translation is based on the standard Simplified Chinese language and may vary depending on the specific dialect or register used in the target domain.
</details></li>
</ul>
<hr>
<h2 id="xCOMET-Transparent-Machine-Translation-Evaluation-through-Fine-grained-Error-Detection"><a href="#xCOMET-Transparent-Machine-Translation-Evaluation-through-Fine-grained-Error-Detection" class="headerlink" title="xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection"></a>xCOMET: Transparent Machine Translation Evaluation through Fine-grained Error Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10482">http://arxiv.org/abs/2310.10482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, André F. T. Martins</li>
<li>for: 本文旨在bridge sentence-level评估和错误span检测两种方法之间，提供更加细节的翻译评估方法。</li>
<li>methods: 本文提出了一种开源的学习型评估方法xCOMET，可以同时进行 sentence-level评估和错误span检测。</li>
<li>results: xCOMET在所有类型的评估中表现出状元，并能够高亮和分类错误 span，从而增强翻译评估的细节性。<details>
<summary>Abstract</summary>
Widely used learned metrics for machine translation evaluation, such as COMET and BLEURT, estimate the quality of a translation hypothesis by providing a single sentence-level score. As such, they offer little insight into translation errors (e.g., what are the errors and what is their severity). On the other hand, generative large language models (LLMs) are amplifying the adoption of more granular strategies to evaluation, attempting to detail and categorize translation errors. In this work, we introduce xCOMET, an open-source learned metric designed to bridge the gap between these approaches. xCOMET integrates both sentence-level evaluation and error span detection capabilities, exhibiting state-of-the-art performance across all types of evaluation (sentence-level, system-level, and error span detection). Moreover, it does so while highlighting and categorizing error spans, thus enriching the quality assessment. We also provide a robustness analysis with stress tests, and show that xCOMET is largely capable of identifying localized critical errors and hallucinations.
</details>
<details>
<summary>摘要</summary>
Translation (Simplified Chinese):广泛使用的学习型评估指标，如COMET和BLEURT，用单句级分数评估翻译假设，无法提供翻译错误的细节信息（例如，翻译错误的类型和严重程度）。然而，大型自然语言模型（LLMs）正在推广更细化的评估策略，尝试 detail和 categorize 翻译错误。在这种情况下，我们介绍了 xCOMET，一个开源的学习指标，用于bridging这些方法之间的差异。xCOMET integrate了句子级评估和错误异常检测功能，在所有类型的评估中表现出state-of-the-art的性能（句子级、系统级和错误异常检测）。此外，它还可以高亮和 categorize 错误异常，因此可以增加质量评估的深度。我们还提供了一个Robustness分析，使用压力测试，并显示 xCOMET 可以识别和报告局部重要的错误和幻觉。
</details></li>
</ul>
<hr>
<h2 id="G-SPEED-General-SParse-Efficient-Editing-MoDel"><a href="#G-SPEED-General-SParse-Efficient-Editing-MoDel" class="headerlink" title="G-SPEED: General SParse Efficient Editing MoDel"></a>G-SPEED: General SParse Efficient Editing MoDel</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10480">http://arxiv.org/abs/2310.10480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/banner-z/g-speed">https://github.com/banner-z/g-speed</a></li>
<li>paper_authors: Haoke Zhang, Yue Wang, Juntao Li, Xiabing Zhou, Min Zhang</li>
<li>for: 提高工作效率，自动理解人类发出的指令并生成预期的内容。</li>
<li>methods: 提出了一种基于无监督文本编辑数据 clustering 算法的一种新型精简编辑模型建立方法，以及一种使用 sparse 编辑模型架构来缓解小语言模型的学习限制。</li>
<li>results: 对比 LLMS Equipped with 175B parameters，G-SPEED 的508M参数可以超越它们，并且可以满足多种编辑需求。<details>
<summary>Abstract</summary>
Large Language Models~(LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages. Through human-model interactions, LLMs can automatically understand human-issued instructions and output the expected contents, which can significantly increase working efficiency. In various types of real-world demands, editing-oriented tasks account for a considerable proportion, which involves an interactive process that entails the continuous refinement of existing texts to meet specific criteria. Due to the need for multi-round human-model interaction and the generation of complicated editing tasks, there is an emergent need for efficient general editing models. In this paper, we propose \underline{\textbf{G}eneral \underline{\textbf{SP}arse \underline{\textbf{E}fficient \underline{\textbf{E}diting Mo\underline{\textbf{D}el~(\textbf{G-SPEED}), which can fulfill diverse editing requirements through a single model while maintaining low computational costs. Specifically, we first propose a novel unsupervised text editing data clustering algorithm to deal with the data scarcity problem. Subsequently, we introduce a sparse editing model architecture to mitigate the inherently limited learning capabilities of small language models. The experimental outcomes indicate that G-SPEED, with its 508M parameters, can surpass LLMs equipped with 175B parameters. Our code and model checkpoints are available at \url{https://github.com/Banner-Z/G-SPEED}.
</details>
<details>
<summary>摘要</summary>
大型语言模型~(LLMs) 已经表现出了惊人的能力，包括理解、生成和修改语言。通过人机交互，LLMs 可以自动理解人类发布的指令，并输出预期的内容，这可能会提高工作效率。在各种实际应用中，修改任务占了一定的比重，这些任务涉及到人机交互的互动过程，需要不断细化现有的文本，以满足特定的标准。由于需要多轮人机交互和复杂的修改任务，有一种急需高效的通用修改模型。在这篇论文中，我们提出了 \underline{\textbf{G}eneral \underline{\textbf{SP}arse \underline{\textbf{E}fficient \underline{\textbf{E}diting Mo\underline{\textbf{D}el~(\textbf{G-SPEED})，它可以满足多样化的修改需求，而且保持低的计算成本。 Specifically，我们首先提出了一种新的无监督文本修改数据归类算法，以解决数据稀缺问题。然后，我们引入了稀疏修改模型架构，以降低小语言模型的内置学习能力限制。实验结果表明，G-SPEED，具有508M参数，可以超越配备175B参数的LLMs。我们的代码和模型检查点可以在 \url{https://github.com/Banner-Z/G-SPEED} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MechGPT-a-language-based-strategy-for-mechanics-and-materials-modeling-that-connects-knowledge-across-scales-disciplines-and-modalities"><a href="#MechGPT-a-language-based-strategy-for-mechanics-and-materials-modeling-that-connects-knowledge-across-scales-disciplines-and-modalities" class="headerlink" title="MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities"></a>MechGPT, a language-based strategy for mechanics and materials modeling that connects knowledge across scales, disciplines and modalities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10445">http://arxiv.org/abs/2310.10445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus J. Buehler</li>
<li>for: 本研究旨在探索人工智能技术以连接不同领域知识，以便更好地探索多 scales 材料失效的问题。</li>
<li>methods: 本研究使用了一个精度调整的大型自然语言模型（LLM），从 raw 源料中提取问题和答案对，然后使用 LLM 微调。研究还使用了 Ontological Knowledge Graphs 提取结构性信息，以及在不同大小和上下文长度下运行多种计算实验。</li>
<li>results: 研究发现，LLMs 能够提取多 scales 材料失效的结构性信息，并且可以用于新的研究问题框架和可读性图表。三个版本的 MechGPT 被讨论，它们在不同的参数大小和上下文长度下运行，可以实现复杂的检索增强策略和多模态探索。<details>
<summary>Abstract</summary>
For centuries, researchers have sought out ways to connect disparate areas of knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across fields, specialization has taken hold later. With the advent of Artificial Intelligence, we can now explore relationships across areas (e.g., mechanics-biology) or disparate domains (e.g., failure mechanics-art). To achieve this, we use a fine-tuned Large Language Model (LLM), here for a subset of knowledge in multiscale materials failure. The approach includes the use of a general-purpose LLM to distill question-answer pairs from raw sources followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is used in a series of computational experiments to explore its capacity for knowledge retrieval, various language tasks, hypothesis generation, and connecting knowledge across disparate areas. While the model has some ability to recall knowledge from training, we find that LLMs are particularly useful to extract structural insights through Ontological Knowledge Graphs. These interpretable graph structures provide explanatory insights, frameworks for new research questions, and visual representations of knowledge that also can be used in retrieval-augmented generation. Three versions of MechGPT are discussed, featuring different sizes from 13 billion to 70 billion parameters, and reaching context lengths of more than 10,000 tokens. This provides ample capacity for sophisticated retrieval augmented strategies, as well as agent-based modeling where multiple LLMs interact collaboratively and/or adversarially, the incorporation of new data from the literature or web searches, as well as multimodality.
</details>
<details>
<summary>摘要</summary>
Traditionally, researchers have sought to connect diverse areas of knowledge. While early scholars (such as Galileo and da Vinci) were experts across multiple fields, specialization has become more prevalent in recent times. With the advent of Artificial Intelligence, we can now explore relationships between different areas (such as mechanics and biology) or disparate domains (such as failure mechanics and art). To achieve this, we use a fine-tuned Large Language Model (LLM), specifically for a subset of knowledge in multiscale materials failure. The approach involves using a general-purpose LLM to distill question-answer pairs from raw sources, followed by LLM fine-tuning. The resulting MechGPT LLM foundation model is then used in a series of computational experiments to explore its capacity for knowledge retrieval, various language tasks, hypothesis generation, and connecting knowledge across disparate areas. While the model has some ability to recall knowledge from training, we find that LLMs are particularly useful for extracting structural insights through Ontological Knowledge Graphs. These interpretable graph structures provide explanatory insights, frameworks for new research questions, and visual representations of knowledge that can also be used in retrieval-augmented generation. Three versions of MechGPT are discussed, featuring different sizes ranging from 13 billion to 70 billion parameters, and reaching context lengths of more than 10,000 tokens. This provides ample capacity for sophisticated retrieval-augmented strategies, as well as agent-based modeling where multiple LLMs interact collaboratively and/or adversarially, the incorporation of new data from the literature or web searches, as well as multimodality.
</details></li>
</ul>
<hr>
<h2 id="Exploiting-User-Comments-for-Early-Detection-of-Fake-News-Prior-to-Users’-Commenting"><a href="#Exploiting-User-Comments-for-Early-Detection-of-Fake-News-Prior-to-Users’-Commenting" class="headerlink" title="Exploiting User Comments for Early Detection of Fake News Prior to Users’ Commenting"></a>Exploiting User Comments for Early Detection of Fake News Prior to Users’ Commenting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10429">http://arxiv.org/abs/2310.10429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiong Nan, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang, Guang Yang, Jintao Li, Kai Shu</li>
<li>for: 探讨了现有方法中的准确性vs快速性之间的负担，并提出了一种可行但尚未得到广泛研究的解决方案，即利用历史新闻的社交背景（如评论）进行模型训练，并将其应用于新出现的新闻中。</li>
<li>methods: 提出了一种名为Comment Assisted Fake News Detection（CAS-FEND）的方法，该方法利用历史新闻的评论来帮助一个内容只的检测模型提高检测精度。特别是，该方法在训练阶段将有用的知识从教师模型中传递给学生模型，以便在新出现的新闻中进行检测。</li>
<li>results: 实验表明，CAS-FEND学生模型在检测新出现的假新闻方面表现出色，比内容只方法和使用1&#x2F;4的评论作为输入的方法更高效。这示出了CAS-FEND的超越性，并证明了它在早期检测中的优势。<details>
<summary>Abstract</summary>
Both accuracy and timeliness are key factors in detecting fake news on social media. However, most existing methods encounter an accuracy-timeliness dilemma: Content-only methods guarantee timeliness but perform moderately because of limited available information, while social context-based ones generally perform better but inevitably lead to latency because of social context accumulation needs. To break such a dilemma, a feasible but not well-studied solution is to leverage social contexts (e.g., comments) from historical news for training a detection model and apply it to newly emerging news without social contexts. This requires the model to (1) sufficiently learn helpful knowledge from social contexts, and (2) be well compatible with situations that social contexts are available or not. To achieve this goal, we propose to absorb and parameterize useful knowledge from comments in historical news and then inject it into a content-only detection model. Specifically, we design the Comments Assisted Fake News Detection method (CAS-FEND), which transfers useful knowledge from a comments-aware teacher model to a content-only student model during training. The student model is further used to detect newly emerging fake news. Experiments show that the CAS-FEND student model outperforms all content-only methods and even those with 1/4 comments as inputs, demonstrating its superiority for early detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>严谨性和时效性都是社交媒体上检测假新闻的关键因素。然而，现有方法很多时会陷入精度-时效性之间的谍诀：内容仅仅方法可以保证时效性，但是它们的检测能力相对较弱，而基于社交上下文的方法通常可以提供更高的检测精度，但是它们需要较长的时间来积累社交上下文。为了突破这种谍诀，我们可以利用社交上下文（例如评论）来训练检测模型，并将其应用于新出现的新闻。这需要模型可以（1）充分学习社交上下文中的有用知识，并（2）在社交上下文存在或缺失时都能够具有Compatibility。为了实现这个目标，我们提出了注入社交上下文知识（e.g., 评论）到内容仅仅模型中的方法。我们称之为注入社交知识的Comments Assisted Fake News Detection方法（CAS-FEND）。在训练过程中，我们将社交上下文知识由一个师模型转移到内容仅仅模型中，然后使用这个师模型来检测新出现的假新闻。实验结果表明，CAS-FEND学生模型在检测新出现的假新闻方面表现出色，even outperforming those with 1/4 comments as inputs，这说明它在早期检测方面具有优势。
</details></li>
</ul>
<hr>
<h2 id="textit-Swap-and-Predict-–-Predicting-the-Semantic-Changes-in-Words-across-Corpora-by-Context-Swapping"><a href="#textit-Swap-and-Predict-–-Predicting-the-Semantic-Changes-in-Words-across-Corpora-by-Context-Swapping" class="headerlink" title="$\textit{Swap and Predict}$ – Predicting the Semantic Changes in Words across Corpora by Context Swapping"></a>$\textit{Swap and Predict}$ – Predicting the Semantic Changes in Words across Corpora by Context Swapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10397">http://arxiv.org/abs/2310.10397</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a1da4/svp-swap">https://github.com/a1da4/svp-swap</a></li>
<li>paper_authors: Taichi Aida, Danushka Bollegala</li>
<li>For: The paper is written for detecting semantic changes of words in different text corpora.* Methods: The proposed method, Swapping-based Semantic Change Detection (SSCD), uses random context swapping to compare the meaning of a target word in two different text corpora.* Results: The method accurately predicts semantic changes of words in four languages (English, German, Swedish, and Latin) and across different time spans (over 50 years and about five years), and achieves significant performance improvements compared to strong baselines for the English semantic change prediction task.Here are the three key points in Simplified Chinese:* For: 文章目的是检测不同文本集中单个词语的 semantics 是否发生变化。* Methods: 提议的方法是基于随机上下文交换的 Swapping-based Semantic Change Detection (SSCD)，用于比较两个不同文本集中单个词语的含义。* Results: 方法可以准确地检测单个词语在四种语言（英语、德语、瑞典语和拉丁语）和不同时间间隔（超过50年和约5年）中的 semantics 变化，并在英语 semantic change prediction 任务上 achiev 高性能改进。<details>
<summary>Abstract</summary>
Meanings of words change over time and across domains. Detecting the semantic changes of words is an important task for various NLP applications that must make time-sensitive predictions. We consider the problem of predicting whether a given target word, $w$, changes its meaning between two different text corpora, $\mathcal{C}_1$ and $\mathcal{C}_2$. For this purpose, we propose $\textit{Swapping-based Semantic Change Detection}$ (SSCD), an unsupervised method that randomly swaps contexts between $\mathcal{C}_1$ and $\mathcal{C}_2$ where $w$ occurs. We then look at the distribution of contextualised word embeddings of $w$, obtained from a pretrained masked language model (MLM), representing the meaning of $w$ in its occurrence contexts in $\mathcal{C}_1$ and $\mathcal{C}_2$. Intuitively, if the meaning of $w$ does not change between $\mathcal{C}_1$ and $\mathcal{C}_2$, we would expect the distributions of contextualised word embeddings of $w$ to remain the same before and after this random swapping process. Despite its simplicity, we demonstrate that even by using pretrained MLMs without any fine-tuning, our proposed context swapping method accurately predicts the semantic changes of words in four languages (English, German, Swedish, and Latin) and across different time spans (over 50 years and about five years). Moreover, our method achieves significant performance improvements compared to strong baselines for the English semantic change prediction task. Source code is available at https://github.com/a1da4/svp-swap .
</details>
<details>
<summary>摘要</summary>
文字的意思随时间和领域而变化。探测文字的 semantic change 是 NLP 应用中的一项重要任务，需要做到时效预测。我们考虑了 predicting whether a given target word, $w$, changes its meaning between two different text corpora, $\mathcal{C}_1$ and $\mathcal{C}_2$ 的问题。为此，我们提出了 $\textit{Swapping-based Semantic Change Detection}$ (SSCD)，一种无监督的方法， randomly swaps contexts between $\mathcal{C}_1$ and $\mathcal{C}_2$ where $w$ occurs。然后，我们 examine the distribution of contextualised word embeddings of $w$, obtained from a pretrained masked language model (MLM), representing the meaning of $w$ in its occurrence contexts in $\mathcal{C}_1$ and $\mathcal{C}_2$。如果 $w$ 的意思在 $\mathcal{C}_1$ 和 $\mathcal{C}_2$ 中不变，我们就会 expects the distributions of contextualised word embeddings of $w$ to remain the same before and after this random swapping process。尽管其简单，我们示示了使用预训练 MLM 无需 fine-tuning 的我们提posed context swapping method 可以准确地预测英语、德语、瑞典语和拉丁语中文字的 semantic change ，并且在不同的时间间隔（超过 50 年和约 5 年）中具有显著的性能提升。此外，我们的方法在英语 semantic change prediction 任务中也具有显著的性能提升。代码可以在 https://github.com/a1da4/svp-swap 找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Better-Understanding-of-Variations-in-Zero-Shot-Neural-Machine-Translation-Performance"><a href="#Towards-a-Better-Understanding-of-Variations-in-Zero-Shot-Neural-Machine-Translation-Performance" class="headerlink" title="Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance"></a>Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10385">http://arxiv.org/abs/2310.10385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Smu-Tan/ZS-NMT-Variations">https://github.com/Smu-Tan/ZS-NMT-Variations</a></li>
<li>paper_authors: Shaomu Tan, Christof Monz</li>
<li>for: 这个论文旨在探讨多语言神经机器翻译（MNMT）在零shot（ZS）翻译质量方面存在高度变化的原因。</li>
<li>methods: 该论文采用了系统性的实验方法，涵盖了40种语言的1560个翻译方向。通过分析， authors发现了三个关键因素对零shot NMT性能产生高度变化：1）目标语言翻译能力，2）词汇重叠，3）语言特性。</li>
<li>results: 研究发现，目标语言翻译质量是零shot NMT性能的最大影响因素，词汇重叠一直影响翻译质量。此外，语言属性，如语言家族和书写系统，对小型模型来说也具有一定的影响。此外， authors还发现了零shot翻译挑战不仅是 Off-target 问题，更是 beyond Off-target 问题。<details>
<summary>Abstract</summary>
Multilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low ZS performance, our work introduces a fresh perspective: the presence of high variations in ZS performance. This suggests that MNMT does not uniformly exhibit poor ZS capability; instead, certain translation directions yield reasonable results. Through systematic experimentation involving 1,560 language directions spanning 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target side translation capability 2) vocabulary overlap 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting ZS performance. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furthermore, we suggest that the off-target issue is a symptom of inadequate ZS performance, emphasizing that zero-shot translation challenges extend beyond addressing the off-target problem. We release the data and models serving as a benchmark to study zero-shot for future research at https://github.com/Smu-Tan/ZS-NMT-Variations
</details>
<details>
<summary>摘要</summary>
多语言神经机器翻译（MNMT）促进知识共享，但经常受到零上下文（ZS）翻译质量的劣化影响。尽管先前的工作已经探讨过总体低ZS性能的原因，我们的工作引入了一个新的视角：ZS翻译方向中的高变化性。这表示MNMT不uniformmente具有差的ZS能力；相反，某些翻译方向实际上可以得到不错的结果。通过对40种语言、1560个语言方向进行系统性的实验，我们确定了三个关键因素对ZS NMT性能的高变化：1）目标语言翻译能力2）词汇重叠3）语言特性。我们的发现表明目标语言翻译质量是最重要的因素，词汇重叠一直影响ZS性能。此外，语言家庭和书写系统等语言特性也在一定程度上影响ZS性能，特别是使用较小的模型时。此外，我们认为偏离问题是ZS翻译挑战的一部分，强调零上下文翻译挑战不仅是解决偏离问题而已。我们在github上发布了数据和模型，用于未来研究零上下文翻译，请参考https://github.com/Smu-Tan/ZS-NMT-Variations。
</details></li>
</ul>
<hr>
<h2 id="Privacy-in-Large-Language-Models-Attacks-Defenses-and-Future-Directions"><a href="#Privacy-in-Large-Language-Models-Attacks-Defenses-and-Future-Directions" class="headerlink" title="Privacy in Large Language Models: Attacks, Defenses and Future Directions"></a>Privacy in Large Language Models: Attacks, Defenses and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10383">http://arxiv.org/abs/2310.10383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi Hu, Chunkit Chan, Yangqiu Song</li>
<li>for: This paper aims to provide a comprehensive analysis of privacy attacks targeting large language models (LLMs) and to identify potential vulnerabilities in these models.</li>
<li>methods: The paper uses a categorization of privacy attacks based on the adversary’s assumed capabilities to shed light on the potential vulnerabilities present in LLMs. It also presents a detailed overview of prominent defense strategies that have been developed to counter these privacy attacks.</li>
<li>results: The paper identifies upcoming privacy concerns as LLMs evolve and points out several potential avenues for future exploration.<details>
<summary>Abstract</summary>
The advancement of large language models (LLMs) has significantly enhanced the ability to effectively tackle various downstream NLP tasks and unify these tasks into generative pipelines. On the one hand, powerful language models, trained on massive textual data, have brought unparalleled accessibility and usability for both models and users. On the other hand, unrestricted access to these models can also introduce potential malicious and unintentional privacy risks. Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved. In this paper, we provide a comprehensive analysis of the current privacy attacks targeting LLMs and categorize them according to the adversary's assumed capabilities to shed light on the potential vulnerabilities present in LLMs. Then, we present a detailed overview of prominent defense strategies that have been developed to counter these privacy attacks. Beyond existing works, we identify upcoming privacy concerns as LLMs evolve. Lastly, we point out several potential avenues for future exploration.
</details>
<details>
<summary>摘要</summary>
LLMs 的进步significantly 提高了解决不同下游 NLP 任务的能力，并将这些任务集成成生成管道。一方面，强大的语言模型，通过庞大的文本数据进行训练，带来了无 precedent的可用性和使用性，对于模型和用户来说。然而，不受限制的访问这些模型也可能 introduce 恶意和无意的隐私风险。虽然持续努力解决 LLMS 中的安全和隐私问题，但问题仍未得到解决。本文提供了 LLMS 中隐私攻击的全面分析，根据敌对者假设的能力，将隐私攻击分为不同类别，以透视 LLMS 中的可能性隐私漏洞。然后，我们提供了一个详细的防御策略的概述，以响应这些隐私攻击。此外，我们还标识了 LLMS 的未来隐私问题。最后，我们指出了未来探索的一些可能性。
</details></li>
</ul>
<hr>
<h2 id="Contextual-Data-Augmentation-for-Task-Oriented-Dialog-Systems"><a href="#Contextual-Data-Augmentation-for-Task-Oriented-Dialog-Systems" class="headerlink" title="Contextual Data Augmentation for Task-Oriented Dialog Systems"></a>Contextual Data Augmentation for Task-Oriented Dialog Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10380">http://arxiv.org/abs/2310.10380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dustin Axman, Avik Ray, Shubham Garg, Jing Huang</li>
<li>for: 增强当前对话系统的训练任务。</li>
<li>methods: 使用对话上下文 Conditional 生成用户回复，并通过新的提示设计和输出重新排序来生成对话。</li>
<li>results: 在多种 benchmark 数据集上，我们的对话增强模型可以生成高质量的对话，提高对话成功率达到 $8%$ 的提高。Here’s the full text in Simplified Chinese:</li>
<li>for: 本文主要用于增强当前对话系统的训练任务。</li>
<li>methods: 我们提出了一种基于对话上下文 Conditional 生成用户回复的对话增强模型，并通过新的提示设计和输出重新排序来生成对话。</li>
<li>results: 在多种 benchmark 数据集上，我们的对话增强模型可以生成高质量的对话，提高对话成功率达到 $8%$ 的提高。<details>
<summary>Abstract</summary>
Collection of annotated dialogs for training task-oriented dialog systems have been one of the key bottlenecks in improving current models. While dialog response generation has been widely studied on the agent side, it is not evident if similar generative models can be used to generate a large variety of, and often unexpected, user inputs that real dialog systems encounter in practice. Existing data augmentation techniques such as paraphrase generation do not take the dialog context into consideration. In this paper, we develop a novel dialog augmentation model that generates a user turn, conditioning on full dialog context. Additionally, with a new prompt design for language model, and output re-ranking, the dialogs generated from our model can be directly used to train downstream dialog systems. On common benchmark datasets MultiWoZ and SGD, we show that our dialog augmentation model generates high quality dialogs and improves dialog success rate by as much as $8\%$ over baseline.
</details>
<details>
<summary>摘要</summary>
“对话系统训练 Task-oriented 对话系统的集成 annotation 对话集成是一个关键瓶颈，目前模型的改进。虽然对话回复生成已经广泛研究，但是不清楚是否可以使用类似的生成模型来生成实际对话系统遇到的多样化和意外的用户输入。现有的数据增强技术，如重叠生成，不考虑对话上下文。在本文中，我们开发了一种基于对话上下文的对话增强模型，可以生成用户转折，并且通过新的语言模型提示和输出重新排序，生成的对话可以直接用于下游对话系统训练。在 MultiWoZ 和 SGD 等常用数据集上，我们展示了我们的对话增强模型可以生成高质量对话，提高对话成功率达到 $8\%$ 。”
</details></li>
</ul>
<hr>
<h2 id="Legal-NLP-Meets-MiCAR-Advancing-the-Analysis-of-Crypto-White-Papers"><a href="#Legal-NLP-Meets-MiCAR-Advancing-the-Analysis-of-Crypto-White-Papers" class="headerlink" title="Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers"></a>Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10333">http://arxiv.org/abs/2310.10333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina Camassa</li>
<li>for: 这个论文是为了探讨欧盟Markets in Crypto-Assets Regulation（MiCAR）对不ikel进行规范的影响，以及在这个领域中文本分析的应用。</li>
<li>methods: 本论文使用自然语言处理（NLP）技术来分析不ikel白皮书，并探讨在MiCAR规范下如何integrate NLP。</li>
<li>results: 本论文发现了不ikel白皮书的文本分析应用存在一些研究漏洞，并对MiCAR规范的影响进行了分析，从而为规范机构、投资者和私有货币发行人提供了可能的研究方向。<details>
<summary>Abstract</summary>
In the rapidly evolving field of crypto assets, white papers are essential documents for investor guidance, and are now subject to unprecedented content requirements under the European Union's Markets in Crypto-Assets Regulation (MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for both analyzing these documents and assisting in regulatory compliance. This paper delivers two contributions to the topic. First, we survey existing applications of textual analysis to unregulated crypto asset white papers, uncovering a research gap that could be bridged with interdisciplinary collaboration. We then conduct an analysis of the changes introduced by MiCAR, highlighting the opportunities and challenges of integrating NLP within the new regulatory framework. The findings set the stage for further research, with the potential to benefit regulators, crypto asset issuers, and investors.
</details>
<details>
<summary>摘要</summary>
在迅速发展的区块链资产领域，白皮书是投资者指导的重要文件，现在欧盟市场区块链资产管理法规（MiCAR）下面面临无前例的内容要求。自然语言处理（NLP）可以作为分析这些文件并协助合规遵守的强大工具。这篇论文在这个主题上做出了两项贡献。首先，我们对未经规范的区块链资产白皮书的文本分析应用进行了调查，揭示出了一个研究差距，这可以通过交叉领域合作bridged。然后，我们对MiCAR引入的变化进行了分析， highlighting the opportunities and challenges of integrating NLP within the new regulatory framework。这些发现可以为 regulators、区块链资产发行人和投资者带来 beneficial。
</details></li>
</ul>
<hr>
<h2 id="Optimized-Tokenization-for-Transcribed-Error-Correction"><a href="#Optimized-Tokenization-for-Transcribed-Error-Correction" class="headerlink" title="Optimized Tokenization for Transcribed Error Correction"></a>Optimized Tokenization for Transcribed Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10704">http://arxiv.org/abs/2310.10704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomer Wullach, Shlomo E. Chazan</li>
<li>for: 提高 speech recognition 系统的精度和可靠性</li>
<li>methods: 使用生成的错误分布和语言特定的 vocabulary 调整</li>
<li>results: 证明使用生成的错误分布和语言特定的 vocabulary 可以提高 correction 模型的性能，并且可以在多种语言和speech recognition 系统中应用<details>
<summary>Abstract</summary>
The challenges facing speech recognition systems, such as variations in pronunciations, adverse audio conditions, and the scarcity of labeled data, emphasize the necessity for a post-processing step that corrects recurring errors. Previous research has shown the advantages of employing dedicated error correction models, yet training such models requires large amounts of labeled data which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often utilized, however, bridging the distribution gap between transcribed errors and synthetic noise is not trivial. In this paper, we demonstrate that the performance of correction models can be significantly increased by training solely using synthetic data. Specifically, we empirically show that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocabulary of a BPE tokenizer strike a balance between adapting to unseen distributions and retaining knowledge of transcribed errors. We showcase the benefits of these key observations, and evaluate our approach using multiple languages, speech recognition systems and prominent speech recognition datasets.
</details>
<details>
<summary>摘要</summary>
Speech recognition systems face many challenges, such as differences in pronunciation, poor audio quality, and a lack of labeled data. To address these challenges, researchers have found that using dedicated error correction models can be effective, but these models require large amounts of labeled data, which is not easily obtained. To overcome this limitation, synthetic transcribed-like data is often used, but it can be difficult to bridge the gap between the distribution of transcribed errors and the synthetic noise. In this paper, we show that the performance of correction models can be significantly improved by training solely using synthetic data. Specifically, we find that: (1) synthetic data generated using the error distribution derived from a set of transcribed data outperforms the common approach of applying random perturbations; (2) applying language-specific adjustments to the vocabulary of a BPE tokenizer can strike a balance between adapting to unseen distributions and retaining knowledge of transcribed errors. We demonstrate the benefits of these key observations using multiple languages, speech recognition systems, and prominent speech recognition datasets.
</details></li>
</ul>
<hr>
<h2 id="Untying-the-Reversal-Curse-via-Bidirectional-Language-Model-Editing"><a href="#Untying-the-Reversal-Curse-via-Bidirectional-Language-Model-Editing" class="headerlink" title="Untying the Reversal Curse via Bidirectional Language Model Editing"></a>Untying the Reversal Curse via Bidirectional Language Model Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10322">http://arxiv.org/abs/2310.10322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mjy1111/BAKE">https://github.com/mjy1111/BAKE</a></li>
<li>paper_authors: Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, Cong Liu</li>
<li>for: 本研究旨在提供一种 bidirectional language model editing 的评估方法，以评估编辑后模型是否可以在反向方向上撤回知识。</li>
<li>methods: 本研究提出了一种 bidirectional assessment for knowledge editing (BAKE) 的benchmark，以评估编辑后模型的反向可逆性。此外，研究还提出了一种名为 bidirectionally inversible relationship modeling (BIRD) 的方法，用于 Mitigating the reversal curse。</li>
<li>results: 实验显示，BIRD 可以通过更新模型参数来提高四种不同大小的 LLM 的表现，并且可以在问答和判断任务中提高模型的表现。<details>
<summary>Abstract</summary>
Recent studies have demonstrated that large language models (LLMs) store massive factual knowledge within their parameters. But existing LLMs are prone to hallucinate unintended text due to false or outdated knowledge. Since retraining LLMs is resource intensive, there has been a growing interest in the concept of model editing. Despite the emergence of benchmarks and approaches, these unidirectional editing and evaluation have failed to explore the reversal curse. Intuitively, if "The capital of France is" is edited to be a counterfact "London" within a model, then it should be able to naturally reason and recall the reverse fact, i.e., "London is the capital of" followed by "France" instead of "England". In this paper, we study bidirectional language model editing, aiming to provide rigorous model editing evaluation to assess if edited LLMs can recall the editing knowledge bidirectionally. A new evaluation metric of reversibility is introduced, and a benchmark dubbed as Bidirectional Assessment for Knowledge Editing (BAKE) is constructed to evaluate the reversibility of edited models in recalling knowledge in the reverse direction of editing. We surprisingly observe that while current editing methods and LLMs can effectively recall editing facts in the direction of editing, they suffer serious deficiencies when evaluated in the reverse direction. To mitigate the reversal curse, a method named Bidirectionally Inversible Relationship moDeling (BIRD) is proposed. A set of editing objectives that incorporate bidirectional relationships between subject and object into the updated model weights are designed. Experiments show that BIRD improves the performance of four representative LLMs of different sizes via question answering and judgement.
</details>
<details>
<summary>摘要</summary>
研究者最近发现，大型语言模型（LLM）中含有巨量的事实知识。然而，现有的LLM容易产生假或过时的知识，导致模型产生假信息。由于重新训练LLM是资源占用的，因此对模型编辑的概念产生了增加的兴趣。虽然有了 benchmarcks 和方法，但这些单向编辑和评估未能探索反转咒。在这篇文章中，我们研究了对向语言模型编辑，以提供对编辑后模型的精确评估，以确定编辑后模型是否可以在反向方向上恢复编辑知识。我们引入了一种新的评估指标——反向可逆性指标，并构建了一个名为“ bidirectional Assessment for Knowledge Editing”（BAKE）的benchmarcks，以评估编辑后模型在反向方向上的知识恢复能力。我们意外发现，当前的编辑方法和LLM可以很好地在编辑方向上恢复编辑知识，但在反向方向上表现异常差。为了 Mitigate the reversal curse，我们提出了一种名为“ bidirectionally Inversible Relationship moDeling”（BIRD）的方法。我们设计了一组编辑目标，将对象和主题之间的双向关系 integrate 到更新后的模型参数中。实验表明，BIRD 可以提高四种不同大小的 LLM 的表现，通过问答和判断。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Bias-in-Multilingual-Language-Models-Cross-Lingual-Transfer-of-Debiasing-Techniques"><a href="#Investigating-Bias-in-Multilingual-Language-Models-Cross-Lingual-Transfer-of-Debiasing-Techniques" class="headerlink" title="Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques"></a>Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10310">http://arxiv.org/abs/2310.10310</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manon-reusens/multilingual_bias">https://github.com/manon-reusens/multilingual_bias</a></li>
<li>paper_authors: Manon Reusens, Philipp Borchert, Margot Mieskes, Jochen De Weerdt, Bart Baesens</li>
<li>for: 本研究探讨了多语言模型中偏见纠正技术的跨语言传递性。我们对英文、法语、德语和荷语进行了研究。</li>
<li>methods: 我们使用多语言BERT（mBERT）来检验跨语言纠正技术的可行性，并发现这些技术可以跨语言传递，并且在不同语言上表现良好。</li>
<li>results: 我们发现，对非英语语言应用这些技术不会带来性能下降。使用CrowS-Pairs数据集的翻译，我们发现 SentenceDebias 是所有语言中最佳的纠正技术，可以在 mBERT 中减少偏见约13%。此外，我们发现在各种语言上追加预训练可以提高跨语言效果，特别是在低资源语言中。<details>
<summary>Abstract</summary>
This paper investigates the transferability of debiasing techniques across different languages within multilingual models. We examine the applicability of these techniques in English, French, German, and Dutch. Using multilingual BERT (mBERT), we demonstrate that cross-lingual transfer of debiasing techniques is not only feasible but also yields promising results. Surprisingly, our findings reveal no performance disadvantages when applying these techniques to non-English languages. Using translations of the CrowS-Pairs dataset, our analysis identifies SentenceDebias as the best technique across different languages, reducing bias in mBERT by an average of 13%. We also find that debiasing techniques with additional pretraining exhibit enhanced cross-lingual effectiveness for the languages included in the analyses, particularly in lower-resource languages. These novel insights contribute to a deeper understanding of bias mitigation in multilingual language models and provide practical guidance for debiasing techniques in different language contexts.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文研究了多语言模型中的偏见纠正技术的传递性。我们对英语、法语、德语和荷语进行了研究，使用多语言BERT（mBERT）来示范了跨语言传递的偏见纠正技术的可行性和效果。我们的结果表明，对非英语语言应用这些技术并不会带来性能下降，而且使用翻译的 CrowS-Pairs 数据集，我们的分析发现，在不同语言上，SentenceDebias 是最有效的技术，可以减少 mBERT 中的偏见程度。此外，我们还发现，对于不同语言的语言模型，额外的预训练可以提高跨语言效果，特别是对于低资源语言。这些发现对偏见纠正在多语言语言模型中的深入理解和实践指导提供了有价值的贡献。
</details></li>
</ul>
<hr>
<h2 id="Multi-Stage-Pre-training-Enhanced-by-ChatGPT-for-Multi-Scenario-Multi-Domain-Dialogue-Summarization"><a href="#Multi-Stage-Pre-training-Enhanced-by-ChatGPT-for-Multi-Scenario-Multi-Domain-Dialogue-Summarization" class="headerlink" title="Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization"></a>Multi-Stage Pre-training Enhanced by ChatGPT for Multi-Scenario Multi-Domain Dialogue Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10285">http://arxiv.org/abs/2310.10285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhouweixiao/mp4">https://github.com/zhouweixiao/mp4</a></li>
<li>paper_authors: Weixiao Zhou, Gengyao Li, Xianfu Cheng, Xinnian Liang, Junnan Zhu, Feifei Zhai, Zhoujun Li</li>
<li>for: 本研究针对多scene多domain的对话摘要进行了新的预训练模型设计，以增强预训练模型的适应性和对话摘要能力。</li>
<li>methods: 本研究使用了一种多stage预训练策略，通过将各个预训练目标调整为预训练模型的核心部分，以减少预训练模型与精革模型之间的差距。具体来说，我们首先进行了域对预训练，使用大量多scene多domain的对话资料，以增强我们的预训练模型的适应性。然后，我们进行了任务对预训练，使用大量多scene多domain的 “对话摘要” 平行数据，由ChatGPT进行标注，以增强我们的预训练模型的对话摘要能力。</li>
<li>results: 实验结果显示，我们的预训练模型在全域 fine-tuning、zero-shot 和几少shot设定中均有着重要的进步，与先前的状态艺术模型相比，具有更高的准确率和更好的一致性。<details>
<summary>Abstract</summary>
Dialogue summarization involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a multi-stage pre-training strategy to reduce the gap between the pre-training objective and fine-tuning objective. Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario multi-domain dialogue data to enhance the adaptability of our pre-trained model. Then, we conduct task-oriented pre-training using large-scale multi-scenario multi-domain "dialogue-summary" parallel data annotated by ChatGPT to enhance the dialogue summarization ability of our pre-trained model. Experimental results on three dialogue summarization datasets from different scenarios and domains indicate that our pre-trained model significantly outperforms previous state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.
</details>
<details>
<summary>摘要</summary>
对话概要化 involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a multi-stage pre-training strategy to reduce the gap between the pre-training objective and fine-tuning objective. Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario multi-domain dialogue data to enhance the adaptability of our pre-trained model. Then, we conduct task-oriented pre-training using large-scale multi-scenario multi-domain "dialogue-summary" parallel data annotated by ChatGPT to enhance the dialogue summarization ability of our pre-trained model. Experimental results on three dialogue summarization datasets from different scenarios and domains indicate that our pre-trained model significantly outperforms previous state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.Here's the translation in Traditional Chinese:对话概要化 involves a wide range of scenarios and domains. However, existing methods generally only apply to specific scenarios or domains. In this study, we propose a new pre-trained model specifically designed for multi-scenario multi-domain dialogue summarization. It adopts a multi-stage pre-training strategy to reduce the gap between the pre-training objective and fine-tuning objective. Specifically, we first conduct domain-aware pre-training using large-scale multi-scenario multi-domain dialogue data to enhance the adaptability of our pre-trained model. Then, we conduct task-oriented pre-training using large-scale multi-scenario multi-domain "dialogue-summary" parallel data annotated by ChatGPT to enhance the dialogue summarization ability of our pre-trained model. Experimental results on three dialogue summarization datasets from different scenarios and domains indicate that our pre-trained model significantly outperforms previous state-of-the-art models in full fine-tuning, zero-shot, and few-shot settings.
</details></li>
</ul>
<hr>
<h2 id="Generative-Calibration-for-In-context-Learning"><a href="#Generative-Calibration-for-In-context-Learning" class="headerlink" title="Generative Calibration for In-context Learning"></a>Generative Calibration for In-context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10266">http://arxiv.org/abs/2310.10266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changmenseng/generative_calibration">https://github.com/changmenseng/generative_calibration</a></li>
<li>paper_authors: Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, Kang Liu</li>
<li>for: 本研究目的是解释LLMs中的受欢迎特性——即场景学习，并提出一种基于生成抽象的约束方法来改进其性能。</li>
<li>methods: 本研究使用了 тео리тиче分析和实验方法来解释受欢迎特性的问题，并提出了一种基于生成抽象的约束方法来改进性能。</li>
<li>results: 研究发现，通过调整 labels 的分布，可以提高受欢迎特性的性能，并且这种方法可以在不同的 prompt 配置下保持稳定性。实验结果显示，提出的方法可以大幅提高受欢迎特性的性能，相比于 ICAL 和现有的准则方法，提高了27%的粗略率。<details>
<summary>Abstract</summary>
As one of the most exciting features of large language models (LLMs), in-context learning is a mixed blessing. While it allows users to fast-prototype a task solver with only a few training examples, the performance is generally sensitive to various configurations of the prompt such as the choice or order of the training examples. In this paper, we for the first time theoretically and empirically identify that such a paradox is mainly due to the label shift of the in-context model to the data distribution, in which LLMs shift the label marginal $p(y)$ while having a good label conditional $p(x|y)$. With this understanding, we can simply calibrate the in-context predictive distribution by adjusting the label marginal, which is estimated via Monte-Carlo sampling over the in-context model, i.e., generation of LLMs. We call our approach as generative calibration. We conduct exhaustive experiments with 12 text classification tasks and 12 LLMs scaling from 774M to 33B, generally find that the proposed method greatly and consistently outperforms the ICL as well as state-of-the-art calibration methods, by up to 27% absolute in macro-F1. Meanwhile, the proposed method is also stable under different prompt configurations.
</details>
<details>
<summary>摘要</summary>
一个 LLM 中最吸引人的特点之一是内容学习（in-context learning），它允许用户快速批量任务解决器，只需要几个训练示例。然而，这种特点同时带来了一些问题，例如prompt的选择和顺序对性能的敏感性。在这篇论文中，我们首次 theoretically和empirically发现，这种парадок斯主要是由 LLMS 的数据分布 Label Shift 引起的， LLMS 会将标签梯度 $p(y)$ Shift，而保持标签条件 $p(x|y)$ 良好。通过这种理解，我们可以简单地调整受 Context 预测分布，这是通过 Monte-Carlo 采样来Estimate  LLMS 的标签梯度。我们称之为生成 Calibration。我们进行了12种文本分类任务和12种 LLMS 的探索性实验，发现我们的方法可以大幅提高 IC 和当前最佳化方法的表现，最高提高27%的绝对值。此外，我们的方法也在不同的 prompt 配置下保持稳定。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Interpretability-using-Human-Similarity-Judgements-to-Prune-Word-Embeddings"><a href="#Enhancing-Interpretability-using-Human-Similarity-Judgements-to-Prune-Word-Embeddings" class="headerlink" title="Enhancing Interpretability using Human Similarity Judgements to Prune Word Embeddings"></a>Enhancing Interpretability using Human Similarity Judgements to Prune Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10262">http://arxiv.org/abs/2310.10262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natalia Flechas Manrique, Wanqian Bao, Aurelie Herbelot, Uri Hasson</li>
<li>for: 这个论文的目的是提供一种可读性方法，以便理解自然语言处理（NLP）系统中的 semantics。</li>
<li>methods: 这种方法使用supervised learning，并为给定的领域（如运动、职业）标识一 subset of 模型特征，以提高人类相似性判断的预测。这种方法只保留20-40%的原始特征，并且在8个独立的semantic domain中都有不同的特征集。</li>
<li>results: 这种方法可以帮助理解NLP系统中的 semantics，并且可以用来解释 humans 对不同领域的分类。例如， humans 在分类运动时会 differentiate  based on how gender-inclusive和international they are。此外，这种方法还可以用来预测words的Semantic dimensions，例如 cognitive、emotional 和social dimensions。<details>
<summary>Abstract</summary>
Interpretability methods in NLP aim to provide insights into the semantics underlying specific system architectures. Focusing on word embeddings, we present a supervised-learning method that, for a given domain (e.g., sports, professions), identifies a subset of model features that strongly improve prediction of human similarity judgments. We show this method keeps only 20-40% of the original embeddings, for 8 independent semantic domains, and that it retains different feature sets across domains. We then present two approaches for interpreting the semantics of the retained features. The first obtains the scores of the domain words (co-hyponyms) on the first principal component of the retained embeddings, and extracts terms whose co-occurrence with the co-hyponyms tracks these scores' profile. This analysis reveals that humans differentiate e.g. sports based on how gender-inclusive and international they are. The second approach uses the retained sets as variables in a probing task that predicts values along 65 semantically annotated dimensions for a dataset of 535 words. The features retained for professions are best at predicting cognitive, emotional and social dimensions, whereas features retained for fruits or vegetables best predict the gustation (taste) dimension. We discuss implications for alignment between AI systems and human knowledge.
</details>
<details>
<summary>摘要</summary>
《NLプロセッシングにおける可読性方法の探索》目的：提供NLプロセッシング中の具体的システム构造の下でのSemanticsの问题解釈を提供する方法。方法：1. 给定のドメイン（例如、スポーツ、职业）に対し、predict human similarity judgmentsのためのsupervised learning方法を提供する。2. この方法では、原始の埋め込みに対して、20-40%の削减を行い、8つの独立したセマンティック ドメインでのpredictionの改善を目指す。3. この方法でRetained featuresのSemanticsを解釈するために、two approachesを提供する。第一方法：1. Retained embeddingsの初期Componentのスコアを计算し、これらのスコアに対応するドメインワード（co-hyponyms）のスコアを测定する。2. この分析により、人々はスポーツなどをどのように区别しているかを理解することができる。第二方法：1. Retained setsを使用して、65を超えるsemantically annotated dimensionに対する予测タスクを実行する。2. この方法では、职业に対するRetained featuresは、cognitive、emotional、socialdimensionsに最も优れていることが分かり、フルーツや野菜に対するRetained featuresは、味（gustation）dimensionに最も优れていることが分かる。结论：これらの方法により、AIシステムと人间の知识のAlignmentを改善することができる。
</details></li>
</ul>
<hr>
<h2 id="Repetition-In-Repetition-Out-Towards-Understanding-Neural-Text-Degeneration-from-the-Data-Perspective"><a href="#Repetition-In-Repetition-Out-Towards-Understanding-Neural-Text-Degeneration-from-the-Data-Perspective" class="headerlink" title="Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective"></a>Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10226">http://arxiv.org/abs/2310.10226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmftbygmftby/rep-dropout">https://github.com/gmftbygmftby/rep-dropout</a></li>
<li>paper_authors: Huayang Li, Tian Lan, Zihao Fu, Deng Cai, Lemao Liu, Nigel Collier, Taro Watanabe, Yixuan Su</li>
<li>for: 本研究旨在解释文本神经网络垃圾问题的根本原因，从数据角度出发，提出了一个简单的解释。</li>
<li>methods: 我们采用了随机抽样和注意力抑制等方法来调查这个问题，并进行了实验 validate our findings。</li>
<li>results: 我们的实验结果表明，训练数据中的重复元素与神经网络垃圾问题之间存在强相关关系，避免训练数据中的重复元素可以大幅减少垃圾问题的出现。此外，我们发现，对于不同的方法，包括高流入词、可能性目标和自我强化现象，都可以通过对训练数据中的重复元素进行罚金来解释其效果。<details>
<summary>Abstract</summary>
There are a number of diverging hypotheses about the neural text degeneration problem, i.e., generating repetitive and dull loops, which makes this problem both interesting and confusing. In this work, we aim to advance our understanding by presenting a straightforward and fundamental explanation from the data perspective. Our preliminary investigation reveals a strong correlation between the degeneration issue and the presence of repetitions in training data. Subsequent experiments also demonstrate that by selectively dropping out the attention to repetitive words in training data, degeneration can be significantly minimized. Furthermore, our empirical analysis illustrates that prior works addressing the degeneration issue from various standpoints, such as the high-inflow words, the likelihood objective, and the self-reinforcement phenomenon, can be interpreted by one simple explanation. That is, penalizing the repetitions in training data is a common and fundamental factor for their effectiveness. Moreover, our experiments reveal that penalizing the repetitions in training data remains critical even when considering larger model sizes and instruction tuning.
</details>
<details>
<summary>摘要</summary>
有很多关于神经文本衰退问题的不同假设，即生成循环和极端的循环，使得这个问题同时具有诱人性和混乱性。在这项工作中，我们希望通过数据角度提供直接和基本的解释，以进一步深化我们对这个问题的理解。我们的初步调查发现，衰退问题与训练数据中的重复的强相关性存在很强的关系。后续的实验也表明，在训练数据中 selectively dropping out 重复的注意力可以明显减少衰退。此外，我们的实验分析表明，先前关于衰退问题的不同方法，如高流入词、可能性目标和自我强化现象，都可以通过一个简单的解释：即在训练数据中 penalty 重复。此外，我们的实验还表明，即使考虑更大的模型大小和指导调整，penalizing 训练数据中的重复仍然是关键的。
</details></li>
</ul>
<hr>
<h2 id="AdaLomo-Low-memory-Optimization-with-Adaptive-Learning-Rate"><a href="#AdaLomo-Low-memory-Optimization-with-Adaptive-Learning-Rate" class="headerlink" title="AdaLomo: Low-memory Optimization with Adaptive Learning Rate"></a>AdaLomo: Low-memory Optimization with Adaptive Learning Rate</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10195">http://arxiv.org/abs/2310.10195</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlmlab/lomo">https://github.com/openlmlab/lomo</a></li>
<li>paper_authors: Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu</li>
<li>for: 降低大语言模型训练的硬件门槛</li>
<li>methods: 利用非负矩阵分解估算二阶均值，采用分组更新正则化稳定收敛</li>
<li>results: 与AdamW相当的性能，同时减少训练内存占用<details>
<summary>Abstract</summary>
Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经取得了非常出色的成功，但它们的庞大参数大小需要很大的内存进行训练，从而设置了高度的门槛。而最近提出的低内存优化（LOMO）可以降低内存占用量，但是它的优化技术，类似于随机梯度下降，对于hyper参数敏感，而且 converge 性不如 AdamW 优化器，fail to match the performance of the prevailing optimizer for large language models。经验表明，与滑动 average 相比，适应式学习率更是关键性的 bridging 因素。基于这一点，我们提出了low-memory optimization with adaptive learning rate（AdaLomo），它在每个参数上提供了适应式学习率。为保持内存效率，我们使用非负矩阵因子分解来Estimate 第二个矩阵积分。此外，我们建议使用 grouped update normalization来稳定收敛。我们的实验表明，AdaLomo 可以与 AdamW 的性能相当，同时具有 significanly 降低内存需求，从而降低训练大语言模型的硬件阻碍。
</details></li>
</ul>
<hr>
<h2 id="VIBE-Topic-Driven-Temporal-Adaptation-for-Twitter-Classification"><a href="#VIBE-Topic-Driven-Temporal-Adaptation-for-Twitter-Classification" class="headerlink" title="VIBE: Topic-Driven Temporal Adaptation for Twitter Classification"></a>VIBE: Topic-Driven Temporal Adaptation for Twitter Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10191">http://arxiv.org/abs/2310.10191</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CelestineZYJ/VIBE-Temporal-Adaptation">https://github.com/CelestineZYJ/VIBE-Temporal-Adaptation</a></li>
<li>paper_authors: Yuji Zhang, Jing Li, Wenjie Li</li>
<li>for: address the challenge of deteriorating text classification performance in real-world social media due to language evolution</li>
<li>methods: 使用变量信息瓶颈（IB）正则化模型 latent topic evolution 进行时间适应，并通过多任务训练来使用时间戳和类别标签预测</li>
<li>results: 在 Twitter 上进行三种分类任务，与前一个状态的继续预处理方法相比，只使用3%的数据，显著提高了模型的性能<details>
<summary>Abstract</summary>
Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and class label prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our model, with only 3% of data, significantly outperforms previous state-of-the-art continued-pretraining methods.
</details>
<details>
<summary>摘要</summary>
<SYS>语言特征在现实世界社交媒体上发展，导致文本分类的性能下降。为Address这个挑战，我们研究时间适应，即使模型在过去数据上训练后，在未来数据上进行测试。大多数前期工作都集中在继续预训练或知识更新上，这可能会 compromise 社交媒体数据的性能。为解决这个问题，我们通过模拟 latent topic evolution 来反射特征变化，并提出了一种新的模型，namely VIBE：Variational Information Bottleneck for Evolutions。具体来说，我们首先使用两个 Information Bottleneck（IB）正则化来分辨过去和未来话题。然后，这些分辨出来的话题被用作 adaptive features，通过多任务训练时间戳和类别标签预测。在adaptive learning中，VIBE 利用了 posterior 于训练数据时间创建的在线流量中检索到的无标签数据，以进行学习。在 Twitter 上进行了三种分类任务的实验，我们发现，使用只有 3% 的数据，我们的模型可以明显超越先前的继续预训练方法。</SYS>Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="TRIGO-Benchmarking-Formal-Mathematical-Proof-Reduction-for-Generative-Language-Models"><a href="#TRIGO-Benchmarking-Formal-Mathematical-Proof-Reduction-for-Generative-Language-Models" class="headerlink" title="TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models"></a>TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10180">http://arxiv.org/abs/2310.10180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/menik1126/TRIGO">https://github.com/menik1126/TRIGO</a></li>
<li>paper_authors: Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, Chuanyang Zheng, Xiaodan Liang, Ming Zhang, Qun Liu</li>
<li>for: 检验高级生成语言模型的逻辑能力和数学逻辑能力。</li>
<li>methods: 提出了一个基于Lean formal语言系统的ATP benchmark，评估模型在式子和数学表达中的推理能力和 manipulate、分组、因数化能力。</li>
<li>results: 对高级生成语言模型进行了广泛的实验，发现TRIGO benchmark可以挑战高级模型，包括GPT-4，并提供一个新的工具来研究高级模型在正式逻辑和数学逻辑方面的能力。<details>
<summary>Abstract</summary>
Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proofs but also evaluates a generative LM's reasoning ability on formulas and its capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from the web, annotate the simplification process manually, and translate it into the Lean formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we develop an automatic generator based on Lean-Gym to create dataset splits of varying difficulties and distributions in order to thoroughly analyze the model's generalization ability. Our extensive experiments show our proposed TRIGO poses a new challenge for advanced generative LM's including GPT-4 which is pre-trained on a considerable amount of open-source formal theorem-proving language data, and provide a new tool to study the generative LM's ability on both formal and mathematical reasoning.
</details>
<details>
<summary>摘要</summary>
自动证明 theorem (ATP) 已成为一个吸引人的领域，以探索最新的成功生成语言模型的逻辑能力。然而，当前的 ATP 标准 mainly focuses on 符号逻辑推理，很少涉及复杂的数学运算理解。在这种工作中，我们提出了 TRIGO，一个 ATP 标准，需要模型将 trigonometric 表达式简化为步骤证明，并评估生成LM的逻辑能力，包括数学表达式的排序、分组和因数化。我们从网络上收集了 trigonometric 表达式和简化过程，并 manually 鉴定了这些简化过程。然后，我们使用 Lean 正式语言系统来翻译这些样例，并自动生成了更多的样例来扩大数据集。此外，我们开发了基于 Lean-Gym 的自动生成器，以创建不同难度和分布的数据集，以全面分析模型的总体化能力。我们的广泛实验表明，我们的提出的 TRIGO 对高级的生成LM，包括 GPT-4，具有新的挑战，并提供了一个新的工具来研究生成LM的 both formal 和数学逻辑能力。
</details></li>
</ul>
<hr>
<h2 id="Joint-Music-and-Language-Attention-Models-for-Zero-shot-Music-Tagging"><a href="#Joint-Music-and-Language-Attention-Models-for-Zero-shot-Music-Tagging" class="headerlink" title="Joint Music and Language Attention Models for Zero-shot Music Tagging"></a>Joint Music and Language Attention Models for Zero-shot Music Tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10159">http://arxiv.org/abs/2310.10159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingjian Du, Zhesong Yu, Jiaju Lin, Bilei Zhu, Qiuqiang Kong</li>
<li>for: 这个论文目的是提出一种针对开放集音乐标签问题的零shot音乐标签系统。</li>
<li>methods: 该系统使用一种联合音乐和语言注意力（JMLA）模型，包括一个预训练的masked autoencoder音频编码器和一个Falcon7B干扰器。我们还引入了preceiver resampler将任意长度音频转换为固定长度表示。在编码器和解码器层之间添加了紧密的注意力连接，以改进编码器和解码器层之间的信息流。</li>
<li>results: 我们使用了互联网上收集的大规模音乐和描述数据集来训练JMLA模型。我们使用ChatGPT将原始描述转换为正规化和多样化的描述，以训练JMLA模型。我们的提议的JMLA系统在GTZAN数据集上实现了零shot音乐标签准确率为64.82%，超过了前一个零shot系统的性能，并与前一个系统在FMA和MagnaTagATune数据集上的性能相似。<details>
<summary>Abstract</summary>
Music tagging is a task to predict the tags of music recordings. However, previous music tagging research primarily focuses on close-set music tagging tasks which can not be generalized to new tags. In this work, we propose a zero-shot music tagging system modeled by a joint music and language attention (JMLA) model to address the open-set music tagging problem. The JMLA model consists of an audio encoder modeled by a pretrained masked autoencoder and a decoder modeled by a Falcon7B. We introduce preceiver resampler to convert arbitrary length audio into fixed length embeddings. We introduce dense attention connections between encoder and decoder layers to improve the information flow between the encoder and decoder layers. We collect a large-scale music and description dataset from the internet. We propose to use ChatGPT to convert the raw descriptions into formalized and diverse descriptions to train the JMLA models. Our proposed JMLA system achieves a zero-shot audio tagging accuracy of $ 64.82\% $ on the GTZAN dataset, outperforming previous zero-shot systems and achieves comparable results to previous systems on the FMA and the MagnaTagATune datasets.
</details>
<details>
<summary>摘要</summary>
音乐标注是一项任务，旨在预测音乐录音的标签。然而，过去的音乐标注研究主要集中在靠近音乐标注任务上，这些任务无法泛化到新的标签。在这项工作中，我们提出了一种基于共同音乐和语言注意力（JMLA）模型的零批学习音乐标注系统，以解决开放集音乐标注问题。JMLA模型包括一个预训练的masked autoencoder音频编码器和一个Falcon7B decoder。我们引入了preceiver resampler将任意长度音频转换为固定长度嵌入。我们引入了 dense attention连接 между编码器和解码器层，以改进编码器和解码器之间的信息流。我们收集了互联网上大规模的音乐和描述数据集。我们提议使用ChatGPT将Raw描述转换为正式化和多样化的描述，以训练JMLA模型。我们提出的JMLA系统在GTZAN数据集上实现了零批学习音乐标注精度为64.82%，超过了前一代零批系统的性能，并与前一代系统在FMA和MagnaTagATune数据集上实现了相似的结果。
</details></li>
</ul>
<hr>
<h2 id="DNA-Denoised-Neighborhood-Aggregation-for-Fine-grained-Category-Discovery"><a href="#DNA-Denoised-Neighborhood-Aggregation-for-Fine-grained-Category-Discovery" class="headerlink" title="DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery"></a>DNA: Denoised Neighborhood Aggregation for Fine-grained Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10151">http://arxiv.org/abs/2310.10151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Lackel/DNA">https://github.com/Lackel/DNA</a></li>
<li>paper_authors: Wenbin An, Feng Tian, Wenkai Shi, Yan Chen, Qinghua Zheng, QianYing Wang, Ping Chen</li>
<li>for:  bridging the gap between fine-grained analysis and high annotation cost</li>
<li>methods:  self-supervised framework that encodes semantic structures of data into the embedding space, with three principles to filter out false neighbors</li>
<li>results:  retrieves more accurate neighbors and outperforms state-of-the-art models by a large margin (average 9.96% improvement on three metrics)<details>
<summary>Abstract</summary>
Discovering fine-grained categories from coarsely labeled data is a practical and challenging task, which can bridge the gap between the demand for fine-grained analysis and the high annotation cost. Previous works mainly focus on instance-level discrimination to learn low-level features, but ignore semantic similarities between data, which may prevent these models learning compact cluster representations. In this paper, we propose Denoised Neighborhood Aggregation (DNA), a self-supervised framework that encodes semantic structures of data into the embedding space. Specifically, we retrieve k-nearest neighbors of a query as its positive keys to capture semantic similarities between data and then aggregate information from the neighbors to learn compact cluster representations, which can make fine-grained categories more separatable. However, the retrieved neighbors can be noisy and contain many false-positive keys, which can degrade the quality of learned embeddings. To cope with this challenge, we propose three principles to filter out these false neighbors for better representation learning. Furthermore, we theoretically justify that the learning objective of our framework is equivalent to a clustering loss, which can capture semantic similarities between data to form compact fine-grained clusters. Extensive experiments on three benchmark datasets show that our method can retrieve more accurate neighbors (21.31% accuracy improvement) and outperform state-of-the-art models by a large margin (average 9.96% improvement on three metrics). Our code and data are available at https://github.com/Lackel/DNA.
</details>
<details>
<summary>摘要</summary>
发现细化类别从宽域标注数据是一个实用和挑战性的任务，可以bridging the gap между需求细化分析和高标注成本。先前的工作主要关注实例级别的 отличия来学习低级特征，但忽略数据之间的 semantic similarity，这可能会使这些模型学习不够紧凑的集群表示。在这篇论文中，我们提出了 Denoised Neighborhood Aggregation（DNA），一种无监督的框架，它可以将数据的 semantic structure编码到嵌入空间中。具体来说，我们在查询时检索 k 个最近邻居作为它的正确键，以捕捉数据之间的semantic similarity，然后将邻居中的信息聚合以学习紧凑的集群表示。但是，检索到的邻居可能含有很多假阳键，这会下降学习得到的嵌入的质量。为了解决这个挑战，我们提出了三个原则来筛选假阳键，以便更好地学习嵌入。此外，我们也证明了我们的学习目标等价于一种聚类损失函数，可以捕捉数据之间的semantic similarity，以形成细化的集群。我们在三个标准数据集上进行了广泛的实验，得到了更高准确的邻居（21.31%的准确率提高）和超过当前领先模型（平均9.96%的提高）。我们的代码和数据可以在https://github.com/Lackel/DNA中找到。
</details></li>
</ul>
<hr>
<h2 id="Node-based-Knowledge-Graph-Contrastive-Learning-for-Medical-Relationship-Prediction"><a href="#Node-based-Knowledge-Graph-Contrastive-Learning-for-Medical-Relationship-Prediction" class="headerlink" title="Node-based Knowledge Graph Contrastive Learning for Medical Relationship Prediction"></a>Node-based Knowledge Graph Contrastive Learning for Medical Relationship Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10138">http://arxiv.org/abs/2310.10138</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhi520/nc-kge">https://github.com/zhi520/nc-kge</a></li>
<li>paper_authors: Zhiguang Fan, Yuedong Yang, Mingyuan Xu, Hongming Chen</li>
<li>For: The paper is written for enhancing the distinctiveness of knowledge graph embeddings (KGEs) and improving the performance of downstream tasks such as predicting drug combinations and reasoning disease-drug relationships.* Methods: The paper proposes a novel node-based contrastive learning method for KGE, called NC-KGE, which constructs appropriate contrastive node pairs on knowledge graphs (KGs) and integrates a relation-aware attention mechanism to focus on semantic relationships and node interactions.* Results: The paper shows that NC-KGE performs competitively with state-of-the-art models on public datasets and outperforms all baselines in predicting biomedical relationship predictions tasks, especially in predicting drug combination relationships.<details>
<summary>Abstract</summary>
The embedding of Biomedical Knowledge Graphs (BKGs) generates robust representations, valuable for a variety of artificial intelligence applications, including predicting drug combinations and reasoning disease-drug relationships. Meanwhile, contrastive learning (CL) is widely employed to enhance the distinctiveness of these representations. However, constructing suitable contrastive pairs for CL, especially within Knowledge Graphs (KGs), has been challenging. In this paper, we proposed a novel node-based contrastive learning method for knowledge graph embedding, NC-KGE. NC-KGE enhances knowledge extraction in embeddings and speeds up training convergence by constructing appropriate contrastive node pairs on KGs. This scheme can be easily integrated with other knowledge graph embedding (KGE) methods. For downstream task such as biochemical relationship prediction, we have incorporated a relation-aware attention mechanism into NC-KGE, focusing on the semantic relationships and node interactions. Extensive experiments show that NC-KGE performs competitively with state-of-the-art models on public datasets like FB15k-237 and WN18RR. Particularly in biomedical relationship prediction tasks, NC-KGE outperforms all baselines on datasets such as PharmKG8k-28, DRKG17k-21, and BioKG72k-14, especially in predicting drug combination relationships. We release our code at https://github.com/zhi520/NC-KGE.
</details>
<details>
<summary>摘要</summary>
biomedical知识图（BKG）的嵌入生成了可靠的表示，对于许多人工智能应用有益，如预测药物组合和理解疾病药物关系。然而，在知识图（KG）中构建适当的对照对是一项挑战。在这篇论文中，我们提出了一种新的节点基于对照学习方法 для知识图嵌入（NC-KGE）。NC-KGE在KG中构建适当的对照节点对，从而提高了知识EXTRACTION在嵌入中的效果，并加速了训练的收敛。此方法可以轻松地与其他知识图嵌入（KGE）方法结合使用。在下游任务中，我们将一种关系意识注意力机制 incorporated into NC-KGE，这种机制会话焦点在知识图中的semantic关系和节点互动。我们在公共数据集FB15k-237和WN18RR进行了广泛的实验，结果显示NC-KGE与状态之前模型相当竞争。特别是在生物医学关系预测任务中，NC-KGE在PharmKG8k-28、DRKG17k-21和BioKG72k-14等数据集上表现出色，尤其是预测药物组合关系。我们在https://github.com/zhi520/NC-KGE中发布了代码。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Rank-Context-for-Named-Entity-Recognition-Using-a-Synthetic-Dataset"><a href="#Learning-to-Rank-Context-for-Named-Entity-Recognition-Using-a-Synthetic-Dataset" class="headerlink" title="Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset"></a>Learning to Rank Context for Named Entity Recognition Using a Synthetic Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10118">http://arxiv.org/abs/2310.10118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Amalvy, Vincent Labatut, Richard Dufour</li>
<li>for: 提高 named entity recognition (NER) 的准确率，特别是在长文档中。</li>
<li>methods: 使用 Alpaca  instrucituned large language model (LLM) 生成一个 sintethic context retrieval 训练数据集，然后使用 BERT 模型进行 neural context retriever 训练。</li>
<li>results: 在英文文学数据集中（包括 40 本第一章），我们的方法比几种 retrieval 基准方法高效，提高 NER 任务的准确率。<details>
<summary>Abstract</summary>
While recent pre-trained transformer-based models can perform named entity recognition (NER) with great accuracy, their limited range remains an issue when applied to long documents such as whole novels. To alleviate this issue, a solution is to retrieve relevant context at the document level. Unfortunately, the lack of supervision for such a task means one has to settle for unsupervised approaches. Instead, we propose to generate a synthetic context retrieval training dataset using Alpaca, an instructiontuned large language model (LLM). Using this dataset, we train a neural context retriever based on a BERT model that is able to find relevant context for NER. We show that our method outperforms several retrieval baselines for the NER task on an English literary dataset composed of the first chapter of 40 books.
</details>
<details>
<summary>摘要</summary>
Recent pre-trained transformer-based models can perform named entity recognition (NER) with high accuracy, but their limited range is a problem when applied to long documents like whole novels. To address this issue, we propose to retrieve relevant context at the document level. However, due to the lack of supervision, we must rely on unsupervised approaches. We use Alpaca, an instruction-tuned large language model (LLM), to generate a synthetic context retrieval training dataset, and then train a neural context retriever based on a BERT model to find relevant context for NER. Our method outperforms several retrieval baselines for the NER task on an English literary dataset composed of the first chapter of 40 books.Here's the word-for-word translation of the text into Simplified Chinese:现代预训练变换器模型可以实现命名实体识别（NER）的高精度，但它们的限制范围是一个问题，应用于整个小说等长文档时。为解决这个问题，我们提议在文档级别上提取相关的上下文。然而，由于缺乏监督，我们必须采用无监督方法。我们使用Alpaca，一个指导调整的大型自然语言模型（LLM），生成一个假数据集，并使用这个数据集来训练一个基于BERT模型的神经网络上下文检索器。我们表明，我们的方法在英文文学 dataset 上（由40本第一章组成） outperform 多个检索基准点 для NER 任务。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Multichannel-Speaker-Attributed-ASR-Speaker-Guided-Decoder-and-Input-Feature-Analysis"><a href="#End-to-end-Multichannel-Speaker-Attributed-ASR-Speaker-Guided-Decoder-and-Input-Feature-Analysis" class="headerlink" title="End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder and Input Feature Analysis"></a>End-to-end Multichannel Speaker-Attributed ASR: Speaker Guided Decoder and Input Feature Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10106">http://arxiv.org/abs/2310.10106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent</li>
<li>for: 本研究旨在开发一个综合式多频道自动语音识别（MC-SA-ASR）系统，该系统结合Conformer编码器和 speaker-attributed Transformer编码器，并且可以有效地结合语音识别和发音识别模块在多频道设置下。</li>
<li>methods: 本研究使用了Conformer编码器和 speaker-attributed Transformer编码器，并且在多频道设置下使用了多框精度注意力和发音识别模块。</li>
<li>results: 在对LibriSpeech数据的模拟混合数据进行测试时，本研究可以 reduves the word error rate（WER）by up to 12% and 16% compared to previous single-channel and multichannel approaches，respectively。此外，本研究还 investigate了不同的输入特征对ASR性能的影响。最后，我们的实验表明，本系统在真实世界的多频道会议记录中具有有效性。<details>
<summary>Abstract</summary>
We present an end-to-end multichannel speaker-attributed automatic speech recognition (MC-SA-ASR) system that combines a Conformer-based encoder with multi-frame crosschannel attention and a speaker-attributed Transformer-based decoder. To the best of our knowledge, this is the first model that efficiently integrates ASR and speaker identification modules in a multichannel setting. On simulated mixtures of LibriSpeech data, our system reduces the word error rate (WER) by up to 12% and 16% relative compared to previously proposed single-channel and multichannel approaches, respectively. Furthermore, we investigate the impact of different input features, including multichannel magnitude and phase information, on the ASR performance. Finally, our experiments on the AMI corpus confirm the effectiveness of our system for real-world multichannel meeting transcription.
</details>
<details>
<summary>摘要</summary>
我们提出了一个综合式多通道自动语音识别（MC-SA-ASR）系统，该系统使用Conformer编码器和Speaker-attributed Transformer编码器。我们认为这是首个在多通道设定下集成ASR和speaker认知模块的模型。在对LibriSpeech数据的模拟混合物中，我们的系统可以降低单个通道和多个通道方法相比，Word Error Rate（WER）下降至12%和16%。此外，我们还研究了不同的输入特征，包括多通道幅度和频率信息，对ASR性能的影响。最后，我们在AMI corpus上进行了实验，证明了我们的系统在实际多通道会议记录中的有效性。Note: "Simplified Chinese" is used to refer to the standardized form of Chinese used in mainland China, which is different from "Traditional Chinese" used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="Decomposed-Prompt-Tuning-via-Low-Rank-Reparameterization"><a href="#Decomposed-Prompt-Tuning-via-Low-Rank-Reparameterization" class="headerlink" title="Decomposed Prompt Tuning via Low-Rank Reparameterization"></a>Decomposed Prompt Tuning via Low-Rank Reparameterization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10094">http://arxiv.org/abs/2310.10094</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyaoooo/dpt">https://github.com/xyaoooo/dpt</a></li>
<li>paper_authors: Yao Xiao, Lu Xu, Jiaxi Li, Wei Lu, Xiaoli Li</li>
<li>for: 提高Prompt Tuning的效率和精度</li>
<li>methods: 使用低级别矩阵初始化软提问</li>
<li>results: 在高资源和低资源情况下，实验结果表明提议方法具有效果<details>
<summary>Abstract</summary>
While prompt tuning approaches have achieved competitive performance with high efficiency, we observe that they invariably employ the same initialization process, wherein the soft prompt is either randomly initialized or derived from an existing embedding vocabulary. In contrast to these conventional methods, this study aims to investigate an alternative way to derive soft prompt. Our empirical studies show that the soft prompt typically exhibits a low intrinsic rank characteristic. With such observations, we propose decomposed prompt tuning, a novel approach that utilizes low-rank matrices to initialize the soft prompt. Through the low-rank reparameterization, our method significantly reduces the number of trainable parameters while maintaining effectiveness. Experimental results on the SuperGLUE benchmark in both high-resource and low-resource scenarios demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
而Prompt调整方法已经实现了高效率的竞争性表现，但我们发现这些传统方法 invariably使用相同的初始化过程，即软提示是随机初始化或者基于现有的Embedding词汇。相比之下，这一研究旨在调查一种不同的软提示 derive的方法。我们的实验研究表明，软提示通常具有低内在矩阵特征。基于这些观察，我们提议了分解Prompt调整，一种使用低级数矩阵初始化软提示的新方法。通过低级数重parameter化，我们的方法可以减少训练参数的数量，同时保持效果。SuperGLUEbenchmark上的实验结果表明，我们的方法在高资源和低资源情况下都具有显著的效果。
</details></li>
</ul>
<hr>
<h2 id="JMedLoRA-Medical-Domain-Adaptation-on-Japanese-Large-Language-Models-using-Instruction-tuning"><a href="#JMedLoRA-Medical-Domain-Adaptation-on-Japanese-Large-Language-Models-using-Instruction-tuning" class="headerlink" title="JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning"></a>JMedLoRA:Medical Domain Adaptation on Japanese Large Language Models using Instruction-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10083">http://arxiv.org/abs/2310.10083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Issey Sukeda, Masahiro Suzuki, Hiroki Sakaji, Satoshi Kodera</li>
<li>for: 本研究旨在探讨如何适应医疗领域的大语言模型（LLMs），以及如何通过领域适应来提高模型的性能。</li>
<li>methods: 本研究使用了LoRA基于的指令调整方法来调整LLMs，以吸收医疗领域特定的知识。</li>
<li>results: 研究发现，通过LoRA基于的指令调整方法，可以部分地将医疗领域特定的知识integrated到LLMs中，大型模型表现更加明显。此外，研究还发现，可以通过适应英语中心模型来进行日本应用领域的适应，同时也 highlighted了日本中心模型的局限性。这些发现可以帮助医疗机构 fine-tune和运行模型，不需要依赖于外部服务。<details>
<summary>Abstract</summary>
In the ongoing wave of impact driven by large language models (LLMs) like ChatGPT, the adaptation of LLMs to medical domain has emerged as a crucial research frontier. Since mainstream LLMs tend to be designed for general-purpose applications, constructing a medical LLM through domain adaptation is a huge challenge. While instruction-tuning is used to fine-tune some LLMs, its precise roles in domain adaptation remain unknown. Here we show the contribution of LoRA-based instruction-tuning to performance in Japanese medical question-answering tasks. In doing so, we employ a multifaceted evaluation for multiple-choice questions, including scoring based on "Exact match" and "Gestalt distance" in addition to the conventional accuracy. Our findings suggest that LoRA-based instruction-tuning can partially incorporate domain-specific knowledge into LLMs, with larger models demonstrating more pronounced effects. Furthermore, our results underscore the potential of adapting English-centric models for Japanese applications in domain adaptation, while also highlighting the persisting limitations of Japanese-centric models. This initiative represents a pioneering effort in enabling medical institutions to fine-tune and operate models without relying on external services.
</details>
<details>
<summary>摘要</summary>
在现代语言模型（LLM）如ChatGPT的浪潮中，适应医疗领域的LLM研究已成为一个关键的前沿领域。由于主流LLM通常是设计为通用应用程序，因此在医疗领域中构建一个LLM通过领域适应是一项巨大的挑战。而在某些LLM上进行了 instrucion-tuning，其precise roles在领域适应仍然未知。在这里，我们展示了LoRA基于的instruction-tuning对于日本医学问答任务的贡献。为此，我们采用了多方面的评估方法，包括基于"精确匹配"和"格式距离"的分数，以及传统的准确率。我们的发现表明，LoRA基于的instruction-tuning可以部分地将领域特定知识引入LLM，大型模型表现更加明显。此外，我们的结果还指出了将英语中心模型适应日本应用的潜在优势，同时也高亮了日本中心模型的限制。这个实验代表了医疗机构可以通过自主定制和操作模型而不需要依赖于外部服务的先驱性努力。
</details></li>
</ul>
<hr>
<h2 id="Let’s-reward-step-by-step-Step-Level-reward-model-as-the-Navigators-for-Reasoning"><a href="#Let’s-reward-step-by-step-Step-Level-reward-model-as-the-Navigators-for-Reasoning" class="headerlink" title="Let’s reward step by step: Step-Level reward model as the Navigators for Reasoning"></a>Let’s reward step by step: Step-Level reward model as the Navigators for Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10080">http://arxiv.org/abs/2310.10080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang</li>
<li>for: 本研究旨在探讨用Large Language Models（LLM）进行多步逻辑时，是否可以通过在推理过程中提供反馈或搜索机制来提高推理准确性。</li>
<li>methods: 本研究使用了Process-Supervised Reward Model（PRM），在训练阶段为LLM提供步骤级别的反馈，类似于Proximal Policy Optimization（PPO）或拒绝抽样。我们还提出了一种启发式搜索算法，使用PRM的步骤级别反馈来优化LLM在多步任务中推理的路径。</li>
<li>results: 我们的研究显示，使用修改后的PRM在数学 benchmark 上（GSM8K和MATH）得到了更好的结果，并且在代码生成任务中也得到了类似的改进。此外，我们还开发了一种自动生成步骤级别奖励数据的方法，用于探讨代码生成任务中的不同路径。这些结果表明，我们的奖励模型基于的方法在推理任务中具有良好的robust性。<details>
<summary>Abstract</summary>
Recent years have seen considerable advancements in multi-step reasoning with Large Language Models (LLMs). The previous studies have elucidated the merits of integrating feedback or search mechanisms during model inference to improve the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically furnishes LLMs with step-by-step feedback during the training phase, akin to Proximal Policy Optimization (PPO) or reject sampling. Our objective is to examine the efficacy of PRM in the inference phase to help discern the optimal solution paths for multi-step tasks such as mathematical reasoning and code generation. To this end, we propose a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs. This tailored PRM demonstrated enhanced results compared to the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH. Additionally, to explore the versatility of our approach, we develop a novel method to automatically generate step-level reward dataset for coding tasks and observed similar improved performance in the code generation tasks. Thus highlighting the robust nature of our reward-model-based approach to inference for reasoning tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prompt-Packer-Deceiving-LLMs-through-Compositional-Instruction-with-Hidden-Attacks"><a href="#Prompt-Packer-Deceiving-LLMs-through-Compositional-Instruction-with-Hidden-Attacks" class="headerlink" title="Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks"></a>Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10077">http://arxiv.org/abs/2310.10077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuyu Jiang, Xingshu Chen, Rui Tang</li>
<li>for: This paper aims to reveal the vulnerability of large language models (LLMs) to compositional instruction attacks that can elicit harmful content, despite current approaches that focus on detecting and training against harmful prompts.</li>
<li>methods: The paper introduces an innovative technique called Compositional Instruction Attacks (CIA), which combines and encapsulates multiple instructions to hide harmful prompts within harmless ones. Two transformation methods, T-CIA and W-CIA, are also proposed to disguise harmful instructions as talking or writing tasks.</li>
<li>results: The paper achieves an attack success rate of 95%+ on safety assessment datasets and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed), and 91%+ for ChatGLM2 on harmful prompt datasets, demonstrating the effectiveness of CIA in eliciting harmful content from LLMs.<details>
<summary>Abstract</summary>
Recently, Large language models (LLMs) with powerful general capabilities have been increasingly integrated into various Web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. Unfortunately, they remain the risk of generating harmful content like hate speech and criminal activities in practical applications. Current approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. However, they typically focused on the "superficial" harmful prompts with a solitary intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios. In this paper, we introduce an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combination and encapsulation of multiple instructions. CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs. We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieves an attack success rate of 95%+ on safety assessment datasets, and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed) and ChatGLM2-6B on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. Warning: this paper may contain offensive or upsetting content!
</details>
<details>
<summary>摘要</summary>
Currently, large language models (LLMs) with strong overall capabilities have been increasingly integrated into various web applications, while undergoing alignment training to ensure that the generated content aligns with user intent and ethics. However, they still face the risk of generating harmful content such as hate speech and criminal activities in practical applications. Existing approaches primarily rely on detecting, collecting, and training against harmful prompts to prevent such risks. However, they typically focus on the "superficial" harmful prompts with a single intent, ignoring composite attack instructions with multiple intentions that can easily elicit harmful content in real-world scenarios.In this paper, we propose an innovative technique for obfuscating harmful instructions: Compositional Instruction Attacks (CIA), which refers to attacking by combining and encapsulating multiple instructions. CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify the underlying malicious intentions. Furthermore, we implement two transformation methods, known as T-CIA and W-CIA, to automatically disguise harmful instructions as talking or writing tasks, making them appear harmless to LLMs.We evaluated CIA on GPT-4, ChatGPT, and ChatGLM2 with two safety assessment datasets and two harmful prompt datasets. It achieved an attack success rate of 95%+ on safety assessment datasets and 83%+ for GPT-4, 91%+ for ChatGPT (gpt-3.5-turbo backed), and 91%+ for ChatGLM2 on harmful prompt datasets. Our approach reveals the vulnerability of LLMs to such compositional instruction attacks that harbor underlying harmful intentions, contributing significantly to LLM security development. Warning: this paper may contain offensive or upsetting content!
</details></li>
</ul>
<hr>
<h2 id="Bridging-Code-Semantic-and-LLMs-Semantic-Chain-of-Thought-Prompting-for-Code-Generation"><a href="#Bridging-Code-Semantic-and-LLMs-Semantic-Chain-of-Thought-Prompting-for-Code-Generation" class="headerlink" title="Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation"></a>Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10698">http://arxiv.org/abs/2310.10698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingwei Ma, Yue Yu, Shanshan Li, Yu Jiang, Yong Guo, Yuanliang Zhang, Yutao Xie, Xiangke Liao</li>
<li>for: 提高自动代码生成的准确率，充分利用大语言模型（LLM）的含义映射能力。</li>
<li>methods: 提出“含义链条”（SeCoT）方法，通过LLM自动学习源代码的含义信息（如数据流和控制流），提高代码生成的精度。</li>
<li>results: 在三个DL benchmark上实现了状态之准确率提高，证明SeCoT可以帮助大型LLM实现更高精度的代码生成。<details>
<summary>Abstract</summary>
Large language models (LLMs) have showcased remarkable prowess in code generation. However, automated code generation is still challenging since it requires a high-level semantic mapping between natural language requirements and codes. Most existing LLMs-based approaches for code generation rely on decoder-only causal language models often treate codes merely as plain text tokens, i.e., feeding the requirements as a prompt input, and outputing code as flat sequence of tokens, potentially missing the rich semantic features inherent in source code. To bridge this gap, this paper proposes the "Semantic Chain-of-Thought" approach to intruduce semantic information of code, named SeCoT. Our motivation is that the semantic information of the source code (\eg data flow and control flow) describes more precise program execution behavior, intention and function. By guiding LLM consider and integrate semantic information, we can achieve a more granular understanding and representation of code, enhancing code generation accuracy. Meanwhile, while traditional techniques leveraging such semantic information require complex static or dynamic code analysis to obtain features such as data flow and control flow, SeCoT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (i.e., in-context learning), while being generalizable and applicable to challenging domains. While SeCoT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: ChatGPT(close-source model) and WizardCoder(open-source model). The experimental study on three popular DL benchmarks (i.e., HumanEval, HumanEval-ET and MBPP) shows that SeCoT can achieves state-of-the-art performance, greatly improving the potential for large models and code generation.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经展示了很好的代码生成能力。然而，自动化代码生成仍然是一个挑战，因为它需要高级别的 semantic mapping  zwischen自然语言要求和代码。现有的 LLMs-based 方法 для代码生成都是基于 causal 语言模型，通常将代码当作平面文本符号，即通过提供要求作为输入，并将代码输出为平面序列符号。这可能会遗漏代码中的较为复杂的 semantics 特征。为了bridging这个差距，本文提出了“semantic chain-of-thought” 方法，以帮助 LL M 考虑和integrate semantic information，从而提高代码生成的准确性。我们的动机是，源代码中的semantic信息（例如数据流和控制流）可以描述更加精确的程序执行行为、意图和功能。通过引导 LL M 考虑这些semantic信息，我们可以实现代码生成更加精准和智能。而传统的方法需要复杂的静态或动态代码分析，以获取such as data flow和control flow的特征。然而，SeCoT 示出了这个过程可以通过 LL Ms 的内在能力（即在场景学习）自动化，并且可以普遍应用于复杂的领域。SeCoT 可以与不同的 LL Ms 结合使用，本文主要采用了强大的 GPT-style 模型：ChatGPT（关闭源代码模型）和WizardCoder（开源模型）。我们对三个Popular DL  bencmarks（即HumanEval、HumanEval-ET和MBPP）进行了实验，结果表明，SeCoT 可以实现状态机器人的表现，大幅提高可能性。
</details></li>
</ul>
<hr>
<h2 id="EfficientOCR-An-Extensible-Open-Source-Package-for-Efficiently-Digitizing-World-Knowledge"><a href="#EfficientOCR-An-Extensible-Open-Source-Package-for-Efficiently-Digitizing-World-Knowledge" class="headerlink" title="EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge"></a>EfficientOCR: An Extensible, Open-Source Package for Efficiently Digitizing World Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10050">http://arxiv.org/abs/2310.10050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Bryan, Jacob Carlson, Abhishek Arora, Melissa Dell</li>
<li>for:  liberating public domain texts at scale</li>
<li>methods:  EffOCR (EfficientOCR), a novel open-source OCR package that uses a character or word-level image retrieval approach, is accurate and sample efficient to train and deploy</li>
<li>results:  EffOCR was used to digitize 20 million historical U.S. newspaper scans with high accuracy, and achieved zero-shot performance on randomly selected documents from the U.S. National Archives, as well as accurately digitizing Japanese documents that other OCR solutions failed on.<details>
<summary>Abstract</summary>
Billions of public domain documents remain trapped in hard copy or lack an accurate digitization. Modern natural language processing methods cannot be used to index, retrieve, and summarize their texts; conduct computational textual analyses; or extract information for statistical analyses, and these texts cannot be incorporated into language model training. Given the diversity and sheer quantity of public domain texts, liberating them at scale requires optical character recognition (OCR) that is accurate, extremely cheap to deploy, and sample-efficient to customize to novel collections, languages, and character sets. Existing OCR engines, largely designed for small-scale commercial applications in high resource languages, often fall short of these requirements. EffOCR (EfficientOCR), a novel open-source OCR package, meets both the computational and sample efficiency requirements for liberating texts at scale by abandoning the sequence-to-sequence architecture typically used for OCR, which takes representations from a learned vision model as inputs to a learned language model. Instead, EffOCR models OCR as a character or word-level image retrieval problem. EffOCR is cheap and sample efficient to train, as the model only needs to learn characters' visual appearance and not how they are used in sequence to form language. Models in the EffOCR model zoo can be deployed off-the-shelf with only a few lines of code. Importantly, EffOCR also allows for easy, sample efficient customization with a simple model training interface and minimal labeling requirements due to its sample efficiency. We illustrate the utility of EffOCR by cheaply and accurately digitizing 20 million historical U.S. newspaper scans, evaluating zero-shot performance on randomly selected documents from the U.S. National Archives, and accurately digitizing Japanese documents for which all other OCR solutions failed.
</details>
<details>
<summary>摘要</summary>
亿量公共领域文档尚未被整合到数字化，或者缺乏准确的数字化。现代自然语言处理技术无法对这些文档进行索引、检索和概要分析，或者提取信息进行统计分析，这些文档也无法被包含在语言模型训练中。由于公共领域文档的多样性和庞大量， liberating them at scale requires an accurate, extremely cheap, and sample-efficient optical character recognition (OCR) technology. Existing OCR engines, primarily designed for small-scale commercial applications in high-resource languages, often fall short of these requirements.EffOCR（EfficientOCR）是一个新的开源 OCR 包，它满足了计算机和样本效率的要求，以便大规模解放文档。而不是使用常见的序列到序列架构，EffOCR 将 OCR 视为字符或单词级图像检索问题。EffOCR 具有低成本和样本效率的训练，因为模型只需学习字符的视觉特征，而不是字符串如何在语言中sequenced使用。EffOCR 的模型集可以通过几行代码部署，并且支持轻松、样本效率地自定义。此外，EffOCR 还具有简单的模型训练接口和最小的标注要求，因此可以轻松地进行随机选择的文档评估和日本文档的数字化，而其他 OCR 解决方案都失败了。
</details></li>
</ul>
<hr>
<h2 id="Improving-Large-Language-Model-Fine-tuning-for-Solving-Math-Problems"><a href="#Improving-Large-Language-Model-Fine-tuning-for-Solving-Math-Problems" class="headerlink" title="Improving Large Language Model Fine-tuning for Solving Math Problems"></a>Improving Large Language Model Fine-tuning for Solving Math Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10047">http://arxiv.org/abs/2310.10047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, Peter J. Liu</li>
<li>for: 解决大语言模型（LLMs）在数学问题解决方面的成本高、精度低问题。</li>
<li>methods:  investigate three fine-tuning strategies：(1) solution fine-tuning，(2) solution-cluster re-ranking，(3) multi-task sequential fine-tuning。</li>
<li>results: 使用MATH dataset，对PaLM 2模型进行了三种精度调整策略的研究，并发现：(1) 使用精度调整的步骤解释可以对模型性能产生显著影响；(2) 筛选和多数投票可以单独使用以提高模型性能，同时使用两者可以叠加提高性能；(3) 将生成和评估任务分别进行多任务并行调整可以比基eline更高的性能。<details>
<summary>Abstract</summary>
Despite their success in many natural language tasks, solving math problems remains a significant challenge for large language models (LLMs). A large gap exists between LLMs' pass-at-one and pass-at-N performance in solving math problems, suggesting LLMs might be close to finding correct solutions, motivating our exploration of fine-tuning methods to unlock LLMs' performance. Using the challenging MATH dataset, we investigate three fine-tuning strategies: (1) solution fine-tuning, where we fine-tune to generate a detailed solution for a given math problem; (2) solution-cluster re-ranking, where the LLM is fine-tuned as a solution verifier/evaluator to choose among generated candidate solution clusters; (3) multi-task sequential fine-tuning, which integrates both solution generation and evaluation tasks together efficiently to enhance the LLM performance. With these methods, we present a thorough empirical study on a series of PaLM 2 models and find: (1) The quality and style of the step-by-step solutions used for fine-tuning can make a significant impact on the model performance; (2) While solution re-ranking and majority voting are both effective for improving the model performance when used separately, they can also be used together for an even greater performance boost; (3) Multi-task fine-tuning that sequentially separates the solution generation and evaluation tasks can offer improved performance compared with the solution fine-tuning baseline. Guided by these insights, we design a fine-tuning recipe that yields approximately 58.8% accuracy on the MATH dataset with fine-tuned PaLM 2-L models, an 11.2% accuracy improvement over the few-shot performance of pre-trained PaLM 2-L model with majority voting.
</details>
<details>
<summary>摘要</summary>
尽管大型自然语言模型（LLM）在许多自然语言任务上表现出色，但解决数学问题仍然是它们的主要挑战。LLM的通过一次和通过N次性能在解决数学问题上存在很大的差距，这表明LLM可能在解决数学问题的过程中很近于发现正确的解决方案，因此我们对LLM的 fine-tuning 方法进行了探索。使用具有挑战性的 MATH 数据集，我们 investigate了三种 fine-tuning 策略：（1）解决 fine-tuning，我们将 LLM  fine-tune 为生成一个给定数学问题的详细解决方案；（2）解决集 cluster 重新排名，我们将 LLM  fine-tune 为一个解决方案验证器/评估器，以选择生成的候选解决方案集；（3）多任务顺序 fine-tuning，它将解决方案生成和评估任务集成起来，以提高 LLM 性能。通过这些方法，我们在 PaLM 2 模型上进行了一系列实验，并发现：（1）用于 fine-tuning 的步骤解决方案质量和风格可以对模型性能产生重要影响；（2）解决重新排名和多数投票都是可以提高模型性能的有效方法，但是它们可以同时使用以实现更大的性能提升；（3）将解决生成和评估任务分开并进行多任务顺序 fine-tuning 可以比基于解决 fine-tuning 的基eline提供更好的性能。根据这些发现，我们设计了一种 fine-tuning 配方，通过这种配方，我们在 MATH 数据集上使用 fine-tuned PaLM 2-L 模型，实现了 Approximately 58.8% 的准确率，与未经 fine-tuning 的 PaLM 2-L 模型的多shot 性能相比，提高了约 11.2%。
</details></li>
</ul>
<hr>
<h2 id="Empirical-Study-of-Zero-Shot-NER-with-ChatGPT"><a href="#Empirical-Study-of-Zero-Shot-NER-with-ChatGPT" class="headerlink" title="Empirical Study of Zero-Shot NER with ChatGPT"></a>Empirical Study of Zero-Shot NER with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10035">http://arxiv.org/abs/2310.10035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emma1066/zero-shot-ner-with-chatgpt">https://github.com/emma1066/zero-shot-ner-with-chatgpt</a></li>
<li>paper_authors: Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Zuozhu Liu, Hongwei Wang</li>
<li>for: 本研究探讨了大型自然语言模型（LLM）在零shot信息EXTRACTION任务中的表现，尤其是在ChatGPT和命名实体识别（NER）任务中。</li>
<li>methods: 我们采用了启发于LLM的卓越逻辑能力的方法，并对NER任务进行了修改和适应。我们提出了分解问题解决方案，将NER任务分解成更加简单的互相关联问题，并通过语法提高和工具增强等方法来促进模型的中间思考。此外，我们还采用了自身一致性来优化NER任务。</li>
<li>results: 我们的方法在七个benchmark上实现了零shotNER任务的很好表现，包括中文和英文 dataset，以及域特定和通用领域场景。此外，我们还进行了错误分析和优化建议。此外，我们还证明了我们的方法在几个shot设置和其他LLM中的效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) exhibited powerful capability in various natural language processing tasks. This work focuses on exploring LLM performance on zero-shot information extraction, with a focus on the ChatGPT and named entity recognition (NER) task. Inspired by the remarkable reasoning capability of LLM on symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods to NER and propose reasoning strategies tailored for NER. First, we explore a decomposed question-answering paradigm by breaking down the NER task into simpler subproblems by labels. Second, we propose syntactic augmentation to stimulate the model's intermediate thinking in two ways: syntactic prompting, which encourages the model to analyze the syntactic structure itself, and tool augmentation, which provides the model with the syntactic information generated by a parsing tool. Besides, we adapt self-consistency to NER by proposing a two-stage majority voting strategy, which first votes for the most consistent mentions, then the most consistent types. The proposed methods achieve remarkable improvements for zero-shot NER across seven benchmarks, including Chinese and English datasets, and on both domain-specific and general-domain scenarios. In addition, we present a comprehensive analysis of the error types with suggestions for optimization directions. We also verify the effectiveness of the proposed methods on the few-shot setting and other LLMs.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在各种自然语言处理任务中表现出了强大的能力。本研究将关注LLM在零式信息提取任务中的表现，特别是关注ChatGPT和命名实体识别（NER）任务。受到LLM在符号逻辑和加算逻辑中的卓越逻辑能力的启发，我们采用了现有的逻辑方法，并对NER任务进行了修改和定制。首先，我们探索了一种分解问题解决方案，将NER任务分解成 simpler subproblems by labels。其次，我们提出了语法增强的方法，通过语法提示和工具增强来让模型在语法结构本身进行分析。此外，我们采用了自适应性来NER，提出了两个阶段多数投票策略，首先投票最符合的提及，然后投票最符合的类型。提出的方法在零式NER中实现了显著的提升，在七个benchmark上，包括中文和英文数据集，以及域特定和通用领域场景中。此外，我们还提供了错误类型的完整分析和优化方向。此外，我们还证明了提出的方法在几个ew-shot设定和其他LLMs中的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.CL_2023_10_16/" data-id="clp869tu300dik588geaz4lca" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/cs.LG_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T10:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/cs.LG_2023_10_16/">cs.LG - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Learning-Framework-for-Cost-Effective-TCR-Epitope-Binding-Affinity-Prediction"><a href="#Active-Learning-Framework-for-Cost-Effective-TCR-Epitope-Binding-Affinity-Prediction" class="headerlink" title="Active Learning Framework for Cost-Effective TCR-Epitope Binding Affinity Prediction"></a>Active Learning Framework for Cost-Effective TCR-Epitope Binding Affinity Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10893">http://arxiv.org/abs/2310.10893</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lee-cbg/activetcr">https://github.com/lee-cbg/activetcr</a></li>
<li>paper_authors: Pengfei Zhang, Seojin Bang, Heewook Lee</li>
<li>for: 这个研究的目的是为了提高TCR与复装序列之间的紧密互动，以便更好地预测TCR对复装序列的认知。</li>
<li>methods: 这个研究使用了活动学习和机器学习技术，将TCR与复装序列之间的紧密互动预测模型与训练数据集成一体。</li>
<li>results: 这个研究发现，使用活动学习可以大幅降低验证成本，并且可以提高预测模型的性能。此外，提供真实的TCR与复装序列对的标签可以帮助减少更多的重复数据，无需增加训练数据量。<details>
<summary>Abstract</summary>
T cell receptors (TCRs) are critical components of adaptive immune systems, responsible for responding to threats by recognizing epitope sequences presented on host cell surface. Computational prediction of binding affinity between TCRs and epitope sequences using machine/deep learning has attracted intense attention recently. However, its success is hindered by the lack of large collections of annotated TCR-epitope pairs. Annotating their binding affinity requires expensive and time-consuming wet-lab evaluation. To reduce annotation cost, we present ActiveTCR, a framework that incorporates active learning and TCR-epitope binding affinity prediction models. Starting with a small set of labeled training pairs, ActiveTCR iteratively searches for unlabeled TCR-epitope pairs that are ''worth'' for annotation. It aims to maximize performance gains while minimizing the cost of annotation. We compared four query strategies with a random sampling baseline and demonstrated that ActiveTCR reduces annotation costs by approximately 40%. Furthermore, we showed that providing ground truth labels of TCR-epitope pairs to query strategies can help identify and reduce more than 40% redundancy among already annotated pairs without compromising model performance, enabling users to train equally powerful prediction models with less training data. Our work is the first systematic investigation of data optimization for TCR-epitope binding affinity prediction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Calysto-Scheme-Project"><a href="#The-Calysto-Scheme-Project" class="headerlink" title="The Calysto Scheme Project"></a>The Calysto Scheme Project</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10886">http://arxiv.org/abs/2310.10886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/calysto/calysto_scheme">https://github.com/calysto/calysto_scheme</a></li>
<li>paper_authors: Douglas S. Blank, James B. Marshall</li>
<li>for: 这篇论文主要是用来介绍一种名为Calysto Scheme的编程语言，以及它在Python基础上实现的一系列可行性和可读性优化。</li>
<li>methods: 这篇论文使用了Continuation-Passing Style和一系列正确性保持的编程变换来将Scheme语言转换成Python语言。它支持标准Scheme功能，包括call&#x2F;cc，以及一些扩展功能，如非探测运算符、自动回tracking和Python交互。</li>
<li>results: 这篇论文的研究结果表明，Calysto Scheme可以在教学和实际应用中使用，具有简单易用的特点和可以快速安装。它已经在Jupyter Notebook生态系统中集成，并在教学中使用了一些有趣和独特的措施。<details>
<summary>Abstract</summary>
Calysto Scheme is written in Scheme in Continuation-Passing Style, and converted through a series of correctness-preserving program transformations into Python. It has support for standard Scheme functionality, including call/cc, as well as syntactic extensions, a nondeterministic operator for automatic backtracking, and many extensions to allow Python interoperation. Because of its Python foundation, it can take advantage of modern Python libraries, including those for machine learning and other pedagogical contexts. Although Calysto Scheme was developed with educational purposes in mind, it has proven to be generally useful due to its simplicity and ease of installation. It has been integrated into the Jupyter Notebook ecosystem and used in the classroom to teach introductory Programming Languages with some interesting and unique twists.
</details>
<details>
<summary>摘要</summary>
加利斯托计划（Calysto Scheme）是一种使用Scheme语言编写的continuation-passing style编程语言，并通过一系列正确性保持的程序转换成Python语言。它支持标准Scheme功能，包括call/cc，以及一些语法扩展和不确定运算符，用于自动回tracking。由于其基于Python语言，因此可以利用现代Python库，包括机器学习和其他教学上的其他库。虽然加利斯托计划是为教育目的设计的，但由于其简单易用，因此在其他场景中也有广泛的应用。它已经与Jupyter Notebook生态系统集成，并在课堂上使用，以教授初级编程语言。
</details></li>
</ul>
<hr>
<h2 id="BLoad-Enhancing-Neural-Network-Training-with-Efficient-Sequential-Data-Handling"><a href="#BLoad-Enhancing-Neural-Network-Training-with-Efficient-Sequential-Data-Handling" class="headerlink" title="BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling"></a>BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10879">http://arxiv.org/abs/2310.10879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Ruschel, A. S. M. Iftekhar, B. S. Manjunath, Suya You</li>
<li>for: 提高现代深度学习模型的优化和扩展数据集的训练效率</li>
<li>methods: 提出了一种基于分布式数据并行训练的新训练方案，可以有效地处理不同长度的序列，无需添加过多的padding</li>
<li>results: 通过该方案，可以在训练时间和准确率两个方面提高表现，在实验中比较了100倍以上的减少padding量而不删除任何帧<details>
<summary>Abstract</summary>
The increasing complexity of modern deep neural network models and the expanding sizes of datasets necessitate the development of optimized and scalable training methods. In this white paper, we addressed the challenge of efficiently training neural network models using sequences of varying sizes. To address this challenge, we propose a novel training scheme that enables efficient distributed data-parallel training on sequences of different sizes with minimal overhead. By using this scheme we were able to reduce the padding amount by more than 100$x$ while not deleting a single frame, resulting in an overall increased performance on both training time and Recall in our experiments.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络模型的复杂度和数据集的规模在不断增长，因此需要开发优化和可扩展的训练方法。在这份白皮书中，我们解决了神经网络模型使用不同长度序列训练的挑战。我们提议一种新的训练方案，可以允许高效分布式数据并行训练，无需较大的过载。通过这种方法，我们可以在不删帧的情况下，将padding减少超过100倍，从而提高了训练时间和准确率的表现。
</details></li>
</ul>
<hr>
<h2 id="Eco-Driving-Control-of-Connected-and-Automated-Vehicles-using-Neural-Network-based-Rollout"><a href="#Eco-Driving-Control-of-Connected-and-Automated-Vehicles-using-Neural-Network-based-Rollout" class="headerlink" title="Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout"></a>Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10878">http://arxiv.org/abs/2310.10878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Paugh, Zhaoxuan Zhu, Shobhit Gupta, Marcello Canova, Stephanie Stockar</li>
<li>for: 提高connected和自主汽车的能源消耗效率，通过在行驶中使用 Vehicle-to-Everything信息优化车辆速度和动力系统。</li>
<li>methods: 使用层次多时间预报法，通过神经网络学习全路价值函数，并用来 aproximate终端成本在减少范围优化中。</li>
<li>results: 在实际道路上的模拟中，提议的方法可以与使用强化学习获得的 Stochastic 优化解决方案相当，而无需 слож的训练方法和内存占用。<details>
<summary>Abstract</summary>
Connected and autonomous vehicles have the potential to minimize energy consumption by optimizing the vehicle velocity and powertrain dynamics with Vehicle-to-Everything info en route. Existing deterministic and stochastic methods created to solve the eco-driving problem generally suffer from high computational and memory requirements, which makes online implementation challenging.   This work proposes a hierarchical multi-horizon optimization framework implemented via a neural network. The neural network learns a full-route value function to account for the variability in route information and is then used to approximate the terminal cost in a receding horizon optimization. Simulations over real-world routes demonstrate that the proposed approach achieves comparable performance to a stochastic optimization solution obtained via reinforcement learning, while requiring no sophisticated training paradigm and negligible on-board memory.
</details>
<details>
<summary>摘要</summary>
自适应和连接的汽车可以减少能源消耗，通过优化车辆速度和动力系统，基于车辆到所有事物（V2X）信息在路线上进行优化。现有的决策方法通常具有高计算和存储需求，在线实现具有挑战性。本工作提出了层次多 horizons 优化框架，通过神经网络学习全路价值函数，并用于缩小往返极限优化。通过实验示例，我们证明了提议的方法可以与基于强化学习的随机优化方法相当，而不需要复杂的训练方法和车辆上的快速存储。
</details></li>
</ul>
<hr>
<h2 id="Religious-Affiliation-in-the-Twenty-First-Century-A-Machine-Learning-Perspective-on-the-World-Value-Survey"><a href="#Religious-Affiliation-in-the-Twenty-First-Century-A-Machine-Learning-Perspective-on-the-World-Value-Survey" class="headerlink" title="Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey"></a>Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10874">http://arxiv.org/abs/2310.10874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaheh Jafarigol, William Keely, Tess Hartog, Tom Welborn, Peyman Hekmatpour, Theodore B. Trafalis</li>
<li>for: 这个研究是使用全球数据收集的世界价值调查数据进行量化分析，以研究社会中个体的宗教信仰、价值观和行为的变化趋势。</li>
<li>methods: 该研究使用随机森林算法来标识宗教性的关键因素，并使用重抽样技术来平衡数据并改善偏袋学习性能指标。</li>
<li>results: 变量重要性分析结果显示，年龄和收入在大多数国家中是最重要的变量，这与社会学基本理论中关于宗教和人类行为的概念有直接关系。<details>
<summary>Abstract</summary>
This paper is a quantitative analysis of the data collected globally by the World Value Survey. The data is used to study the trajectories of change in individuals' religious beliefs, values, and behaviors in societies. Utilizing random forest, we aim to identify the key factors of religiosity and classify respondents of the survey as religious and non religious using country level data. We use resampling techniques to balance the data and improve imbalanced learning performance metrics. The results of the variable importance analysis suggest that Age and Income are the most important variables in the majority of countries. The results are discussed with fundamental sociological theories regarding religion and human behavior. This study is an application of machine learning in identifying the underlying patterns in the data of 30 countries participating in the World Value Survey. The results from variable importance analysis and classification of imbalanced data provide valuable insights beneficial to theoreticians and researchers of social sciences.
</details>
<details>
<summary>摘要</summary>
这个论文是一项量化分析全球World Value Survey所收集的数据。数据用于研究社会中个体信仰、价值观和行为的变化轨迹。使用Random Forest算法，我们希望寻找 religiosity 的关键因素并使用国家级数据将响应者分类为宗教和非宗教。我们使用重样技术来协议数据，以改善偏袋学习表现指标。结果变量重要性分析表明，年龄和收入在大多数国家中是最重要的变量。结果与基本社会学理论相关于宗教和人类行为进行了讨论。这项研究是机器学习在World Value Survey数据中寻找下面的 patrón 的应用。变量重要性分析和响应者分类提供了有价值的发现，对社会科学研究人员有所帮助。
</details></li>
</ul>
<hr>
<h2 id="Joint-Optimization-of-Traffic-Signal-Control-and-Vehicle-Routing-in-Signalized-Road-Networks-using-Multi-Agent-Deep-Reinforcement-Learning"><a href="#Joint-Optimization-of-Traffic-Signal-Control-and-Vehicle-Routing-in-Signalized-Road-Networks-using-Multi-Agent-Deep-Reinforcement-Learning" class="headerlink" title="Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning"></a>Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10856">http://arxiv.org/abs/2310.10856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xianyue Peng, Hang Gao, Gengyue Han, Hao Wang, Michael Zhang</li>
<li>for: 解决现代道路网络中的城市干道堵塞问题，提高交通效率。</li>
<li>methods: 提出了一种联合优化方法，通过同时控制信号时间和车辆路径选择，使用多智能拟似人类学习（MADRL）。</li>
<li>results: 经过数学实验表明，我们的策略可以在修改后的哈佛环境中提高交通效率，并且比只控制信号时间或车辆路径选择 alone 更高。<details>
<summary>Abstract</summary>
Urban traffic congestion is a critical predicament that plagues modern road networks. To alleviate this issue and enhance traffic efficiency, traffic signal control and vehicle routing have proven to be effective measures. In this paper, we propose a joint optimization approach for traffic signal control and vehicle routing in signalized road networks. The objective is to enhance network performance by simultaneously controlling signal timings and route choices using Multi-Agent Deep Reinforcement Learning (MADRL). Signal control agents (SAs) are employed to establish signal timings at intersections, whereas vehicle routing agents (RAs) are responsible for selecting vehicle routes. By establishing relevance between agents and enabling them to share observations and rewards, interaction and cooperation among agents are fostered, which enhances individual training. The Multi-Agent Advantage Actor-Critic algorithm is used to handle multi-agent environments, and Deep Neural Network (DNN) structures are designed to facilitate the algorithm's convergence. Notably, our work is the first to utilize MADRL in determining the optimal joint policy for signal control and vehicle routing. Numerical experiments conducted on the modified Sioux network demonstrate that our integration of signal control and vehicle routing outperforms controlling signal timings or vehicles' routes alone in enhancing traffic efficiency.
</details>
<details>
<summary>摘要</summary>
现代城市交通堵塞是一个严重的问题，对现代道路网络造成了很大的影响。为了解决这个问题并提高交通效率，交通信号控制和车辆 routing 已经被证明是有效的解决方案。在这篇论文中，我们提出了一种联合优化方法，将交通信号控制和车辆 routing 联合优化。我们使用多 Agent Deep Reinforcement Learning（MADRL）来同时控制信号时间和车辆路径选择。信号控制代理（SAs）负责在交叉口设置信号时间，而车辆路径选择代理（RAs）负责选择车辆路径。通过在代理之间建立相关性，并让代理们共享观察和奖励，这会促进代理之间的交互和合作，从而提高每个代理的训练效果。我们使用多 Agent Advantage Actor-Critic算法来处理多 Agent 环境，并使用深度神经网络（DNN）结构来促进算法的收敛。值得注意的是，我们的工作是首次在确定合适的共同策略方面利用 MADRL。我们在修改过 Sioux 网络进行数值实验，结果显示，我们将信号控制和车辆 routing 联合优化的策略与单独控制信号时间或车辆路径的策略相比，在提高交通效率方面具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Classification-by-Density-Estimation-Using-Gaussian-Mixture-Model-and-Masked-Autoregressive-Flow"><a href="#Probabilistic-Classification-by-Density-Estimation-Using-Gaussian-Mixture-Model-and-Masked-Autoregressive-Flow" class="headerlink" title="Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow"></a>Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10843">http://arxiv.org/abs/2310.10843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bghojogh/density-based-classifiers">https://github.com/bghojogh/density-based-classifiers</a></li>
<li>paper_authors: Benyamin Ghojogh, Milad Amir Toutounchian</li>
<li>for: 这篇论文主要用于提出一种基于密度估计的分类方法，而密度估计通常用于数据分布估计而不是分类。</li>
<li>methods: 该论文使用了两种密度估计方法： Gaussian Mixture Model (GMM) 和 Masked Autoregressive Flow (MAF)。GMM 是一种预测最大化的密度估计方法，而 MAF 则是一种基于普通化流和自适应网络的生成模型。</li>
<li>results: 该论文的实验结果显示，使用 GMM 和 MAF 进行密度估计可以超过简单的线性分类器，如线性激发分析。此外，该论文还开启了研究者们可以根据密度估计来提出其他可能的概率分类器的研究方向。<details>
<summary>Abstract</summary>
Density estimation, which estimates the distribution of data, is an important category of probabilistic machine learning. A family of density estimators is mixture models, such as Gaussian Mixture Model (GMM) by expectation maximization. Another family of density estimators is the generative models which generate data from input latent variables. One of the generative models is the Masked Autoregressive Flow (MAF) which makes use of normalizing flows and autoregressive networks. In this paper, we use the density estimators for classification, although they are often used for estimating the distribution of data. We model the likelihood of classes of data by density estimation, specifically using GMM and MAF. The proposed classifiers outperform simpler classifiers such as linear discriminant analysis which model the likelihood using only a single Gaussian distribution. This work opens the research door for proposing other probabilistic classifiers based on joint density estimation.
</details>
<details>
<summary>摘要</summary>
density 估计是机器学习中一种重要的分布估计类别。一个家族的density 估计器是混合模型，如 Gaussian Mixture Model (GMM) 使用期望最大化。另一个家族的density 估计器是生成模型，它们可以将输入的latent variable generate 为数据。本文使用density 估计器进行分类，尽管它们通常用于估计数据的分布。我们使用GMM和MAF来模型数据的类别概率。提议的分类器比 simpler 分类器，如线性混合分析，which 模型只使用单个 Gaussian 分布来模型类别概率。这项工作打开了研究机会，提出其他基于共同分布估计的概率分类器的建议。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-based-Algorithm-for-Automated-Detection-of-Frequency-based-Events-in-Recorded-Time-Series-of-Sensor-Data"><a href="#A-Machine-Learning-based-Algorithm-for-Automated-Detection-of-Frequency-based-Events-in-Recorded-Time-Series-of-Sensor-Data" class="headerlink" title="A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data"></a>A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10841">http://arxiv.org/abs/2310.10841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahareh Medghalchi, Andreas Vogel</li>
<li>for: 本研究旨在提出一种新的自动事件探测方法，用于检测时间序列数据中的事件。</li>
<li>methods: 该方法首先将时间序列数据映射到时间频域的表示中，然后对表示进行筛选和对象检测模型的训练，以检测期望的事件对象在表示中。</li>
<li>results: 该方法在未seen的数据集上测试，准确率为0.97，能够准确地检测事件的时间间隔，提高了自动事件检测的精度和可靠性。<details>
<summary>Abstract</summary>
Automated event detection has emerged as one of the fundamental practices to monitor the behavior of technical systems by means of sensor data. In the automotive industry, these methods are in high demand for tracing events in time series data. For assessing the active vehicle safety systems, a diverse range of driving scenarios is conducted. These scenarios involve the recording of the vehicle's behavior using external sensors, enabling the evaluation of operational performance. In such setting, automated detection methods not only accelerate but also standardize and objectify the evaluation by avoiding subjective, human-based appraisals in the data inspection. This work proposes a novel event detection method that allows to identify frequency-based events in time series data. To this aim, the time series data is mapped to representations in the time-frequency domain, known as scalograms. After filtering scalograms to enhance relevant parts of the signal, an object detection model is trained to detect the desired event objects in the scalograms. For the analysis of unseen time series data, events can be detected in their scalograms with the trained object detection model and are thereafter mapped back to the time series data to mark the corresponding time interval. The algorithm, evaluated on unseen datasets, achieves a precision rate of 0.97 in event detection, providing sharp time interval boundaries whose accurate indication by human visual inspection is challenging. Incorporating this method into the vehicle development process enhances the accuracy and reliability of event detection, which holds major importance for rapid testing analysis.
</details>
<details>
<summary>摘要</summary>
自动化事件检测已经成为监测技术系统行为的基本实践，通过感知器数据。在汽车业，这些方法受到时间序列数据跟踪事件的高需求。为评估活动汽车安全系统，进行了多种驾驶场景测试。这些场景包括记录汽车的行为使用外部感知器，以便评估运作性能。在这种设置下，自动检测方法不仅加速了，还标准化和对象化了评估，通过避免人类基于数据检查的主观评估。本研究提出了一种新的事件检测方法，可以在时间序列数据中检测频率基于事件。为此，将时间序列数据映射到时间频率域的表示，称为scalogram。然后，对scalogram进行过滤，以增强有关信号的部分。然后，将对象检测模型训练以检测想要的事件对象在scalogram中。对于未看过的时间序列数据分析，可以使用训练好的对象检测模型在scalogram中检测事件，并将其映射回时间序列数据，以标识相应的时间间隔。这种算法，在未经过测试的数据上进行评估，具有0.97的准确率，提供了准确的时间间隔边界，人工视觉检查具有挑战性。通过将这种方法 integrate到汽车开发过程中，提高了事件检测的准确性和可靠性，这些特性对于快速分析具有重要性。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Two-Layer-Feedforward-Networks-for-Efficient-Transformers"><a href="#Approximating-Two-Layer-Feedforward-Networks-for-Efficient-Transformers" class="headerlink" title="Approximating Two-Layer Feedforward Networks for Efficient Transformers"></a>Approximating Two-Layer Feedforward Networks for Efficient Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10837">http://arxiv.org/abs/2310.10837</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robertcsordas/moe">https://github.com/robertcsordas/moe</a></li>
<li>paper_authors: Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber</li>
<li>for: 降低神经网络（NN）计算和内存需求，而不是牺牲性能。</li>
<li>methods: 使用稀疏混合专家（MoE）建立资源高效的大语言模型（LM）。</li>
<li>results: 在 WikiText-103 和 enwiki8 数据集上，与 dense Transformer-XL 相比，我们的 MoE 在两个不同的尺度上具有相当的竞争力，而且具有许多资源的灵活性。<details>
<summary>Abstract</summary>
How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.
</details>
<details>
<summary>摘要</summary>
如何减少神经网络（NN）的计算和内存需求而不 sacrificing性能？许多latest works使用稀疏混合专家（MoE）来建立资源高效的大语言模型（LM）。我们介绍了MoE的一些新的视角，提出了一个通用的框架，可以 aproximate two-layer NNs（例如Transformers的Feedforward块），包括产品密钥记忆（PKM）。通过这个框架的洞察，我们提出了MoE和PKM的改进方法。与先前的工作不同，我们的评估条件是参数平等（parameter-equal），这是评估LMs的关键。我们的MoE在WikiText-103和enwiki8 dataset上与 dense Transformer-XL 在两个不同的 scales 上具有相似的竞争力，而且具有许多资源的约束。这表明MoE不仅适用于极大规模的LM，还适用于任何规模的资源高效LM。我们的代码public。
</details></li>
</ul>
<hr>
<h2 id="Gaussian-processes-based-data-augmentation-and-expected-signature-for-time-series-classification"><a href="#Gaussian-processes-based-data-augmentation-and-expected-signature-for-time-series-classification" class="headerlink" title="Gaussian processes based data augmentation and expected signature for time series classification"></a>Gaussian processes based data augmentation and expected signature for time series classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10836">http://arxiv.org/abs/2310.10836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Romito, Francesco Triggiano</li>
<li>for: 这篇论文旨在提出一种基于预期签名的时间序列特征提取模型，用于预测时间序列的统计特征。</li>
<li>methods: 该模型使用 Gaussian processes 数据增强来计算预期签名，并通过一个监督任务来学习最佳的特征提取。</li>
<li>results: 模型可以学习出优化的特征提取，并且可以用于预测时间序列的统计特征。<details>
<summary>Abstract</summary>
The signature is a fundamental object that describes paths (that is, continuous functions from an interval to a Euclidean space). Likewise, the expected signature provides a statistical description of the law of stochastic processes. We propose a feature extraction model for time series built upon the expected signature. This is computed through a Gaussian processes based data augmentation. One of the main features is that an optimal feature extraction is learnt through the supervised task that uses the model.
</details>
<details>
<summary>摘要</summary>
《签名》是一个基本对象，描述了路径（即连续函数从一个时间interval到一个欧几里得空间）。类似地，预期签名提供了一个统计描述，描述了随机过程的法律。我们提议一种基于预期签名的特征提取模型，用于时间序列。这个模型通过基于 Gaussian 过程的数据增强来计算预期签名。其中一个主要特点是通过一个监督任务来学习优化的特征提取。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Data-Driven-Surrogates-of-Dynamical-Systems-for-Forward-Propagation-of-Uncertainty"><a href="#Accurate-Data-Driven-Surrogates-of-Dynamical-Systems-for-Forward-Propagation-of-Uncertainty" class="headerlink" title="Accurate Data-Driven Surrogates of Dynamical Systems for Forward Propagation of Uncertainty"></a>Accurate Data-Driven Surrogates of Dynamical Systems for Forward Propagation of Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10831">http://arxiv.org/abs/2310.10831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saibal De, Reese E. Jones, Hemanth Kolla</li>
<li>for: 这篇研究的目的是为了提出一种新的非侵入式方法来建立模型uncertainty quantification中的代理模型。</li>
<li>methods: 这篇研究使用了Stochastic collocation（SC）方法，并与Data-driven sparse identification of nonlinear dynamics（SINDy）框架结合，以建立动态模型的代理模型。</li>
<li>results: 研究发现，使用SC-over-dynamics框架可以降低错误，包括系统轨道的描述和模型状态分布的描述。三个测试问题中的两个问题（一个ordinary differential equation和一个partial differential equation）的数据表明，这种方法可以提供更好的结果。<details>
<summary>Abstract</summary>
Stochastic collocation (SC) is a well-known non-intrusive method of constructing surrogate models for uncertainty quantification. In dynamical systems, SC is especially suited for full-field uncertainty propagation that characterizes the distributions of the high-dimensional primary solution fields of a model with stochastic input parameters. However, due to the highly nonlinear nature of the parameter-to-solution map in even the simplest dynamical systems, the constructed SC surrogates are often inaccurate. This work presents an alternative approach, where we apply the SC approximation over the dynamics of the model, rather than the solution. By combining the data-driven sparse identification of nonlinear dynamics (SINDy) framework with SC, we construct dynamics surrogates and integrate them through time to construct the surrogate solutions. We demonstrate that the SC-over-dynamics framework leads to smaller errors, both in terms of the approximated system trajectories as well as the model state distributions, when compared against full-field SC applied to the solutions directly. We present numerical evidence of this improvement using three test problems: a chaotic ordinary differential equation, and two partial differential equations from solid mechanics.
</details>
<details>
<summary>摘要</summary>
这项工作提出了一种alternative方法，在这里我们通过SC 近似方法来 Approximate the dynamics of the model, rather than the solution。通过将数据驱动的稀疏标识非线性动力（SINDy）框架与SC 结合，我们构建了动力准模型，并将其通过时间集成以构建准确的解决方案。我们发现，使用SC  над动力框架，相比拟合解决方案直接使用SC，能够减少误差，包括系统轨迹的近似值以及模型状态分布的误差。我们通过三个测试问题来提供数字证据：一个混沌的常微分方程，以及两个固体力学中的部分偏微分方程。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-aware-transfer-across-tasks-using-hybrid-model-based-successor-feature-reinforcement-learning"><a href="#Uncertainty-aware-transfer-across-tasks-using-hybrid-model-based-successor-feature-reinforcement-learning" class="headerlink" title="Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning"></a>Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10818">http://arxiv.org/abs/2310.10818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parvin Malekzadeh, Ming Hou, Konstantinos N. Plataniotis</li>
<li>For: 该论文主要针对复杂大规模决策问题的实用强化学习（RL）问题进行研究，旨在提高样本效率。* Methods: 该论文提出了一种将模型基于（MB）方法与继承特征（SF）算法结合的方法，以及一种基于卡尔曼滤波器（KF）的多模型适应估计来实现不确定性感知探索。* Results: 该论文的实验结果显示，该算法可以在不同的转移动力学中转移知识，对下游任务进行有效的探索和学习，并在样本量方面比起现有基elines要少得多。<details>
<summary>Abstract</summary>
Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this paper, the uncertainty of the value of each action is approximated by a Kalman filter (KF)-based multiple-model adaptive estimation. This KF-based framework treats the parameters of a model as random variables. To the best of our knowledge, this is the first attempt at formulating a hybrid MB-SF algorithm capable of generalizing knowledge across large or continuous state space tasks with various transition dynamics while requiring less computation at decision time than MB methods. The number of samples required to learn the tasks was compared to recent SF and MB baselines. The results show that our algorithm generalizes its knowledge across different transition dynamics, learns downstream tasks with significantly fewer samples than starting from scratch, and outperforms existing approaches.
</details>
<details>
<summary>摘要</summary>
sample efficiency是RL中央的一个重要问题，它可以提高RL的实用性和可扩展性。在不同的任务中，通过转移和总结之前的经验可以提高sample efficiency。现有研究表明，Successor Feature（SF）RL算法可以在不同的奖励下产生相同的转移动力学中转移知识。此外，uncertainty-aware探索被广泛认为是一个有appeal的方法，可以提高sample efficiency。将两个想法MB-SF和uncertainty结合起来，可以解决在不同的转移动力学或奖励函数下的样本效率不稳定问题。在这篇论文中，我们使用Kalman缓冲（KF）基于多模型适应的方法来估计每个行为的值的不确定性。这是我们知道的第一个能够在大或连续状态空间任务上广泛适用的hybrid MB-SF算法。我们对SF和MB基线进行比较，并发现我们的算法可以在不同的转移动力学下转移知识，在不同的任务上学习要少样本更多，并且超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Multi-Agent-Reinforcement-Learning-via-Adversarial-Regularization-Theoretical-Foundation-and-Stable-Algorithms"><a href="#Robust-Multi-Agent-Reinforcement-Learning-via-Adversarial-Regularization-Theoretical-Foundation-and-Stable-Algorithms" class="headerlink" title="Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms"></a>Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10810">http://arxiv.org/abs/2310.10810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abukharin3/ernie">https://github.com/abukharin3/ernie</a></li>
<li>paper_authors: Alexander Bukharin, Yan Li, Yue Yu, Qingru Zhang, Zhehui Chen, Simiao Zuo, Chao Zhang, Songan Zhang, Tuo Zhao</li>
<li>For: The paper aims to improve the robustness of multi-agent reinforcement learning (MARL) policies by controlling the Lipschitz constant of the policies and using adversarial regularization to promote continuity with respect to state observations and actions.* Methods: The proposed framework, called ERNIE, uses adversarial regularization to promote the Lipschitz continuity of policies, and the authors reformulate adversarial regularization as a Stackelberg game to reduce training instability.* Results: The authors demonstrate the effectiveness of the proposed framework with extensive experiments in traffic light control and particle environments, and show that the ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. Additionally, the authors extend ERNIE to mean-field MARL with a formulation based on distributionally robust optimization that outperforms its non-robust counterpart and is of independent interest.<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To reduce this instability, we reformulate adversarial regularization as a Stackelberg game. We demonstrate the effectiveness of the proposed framework with extensive experiments in traffic light control and particle environments. In addition, we extend ERNIE to mean-field MARL with a formulation based on distributionally robust optimization that outperforms its non-robust counterpart and is of independent interest. Our code is available at https://github.com/abukharin3/ERNIE.
</details>
<details>
<summary>摘要</summary>
多智能体学习（MARL）已经在多个领域展示了承诺的成绩。然而，MARL策略通常缺乏鲁棒性，因此容易受到环境小变化的影响。这对实际世界中部署MARL算法提出了严重的问题，因为测试环境可能与训练环境有所不同。在这种情况下，我们展示了通过控制策略的 lipschitz常量来增加鲁棒性的可能性，并在某些条件下证明存在 lipschitz和准确策略的存在。基于这些发现，我们提出了一个新的鲁棒MARL框架，称为ERNIE，它通过对策略的状态观测和行动进行对抗规范化来提高策略的鲁棒性。ERNIE框架可以抵抗噪声观测、变化的转移动力和代理者的恶意行动。然而，ERNIE的对抗规范化可能会导致训练不稳定。为了减少这种不稳定，我们将对抗规范化转换为一个Stackelberg游戏。我们通过广泛的实验证明了提议的框架的效果，包括交通灯控制和粒子环境。此外，我们将ERNIE扩展到了 Mean-field MARL，使其超越其不鲁棒的对比，并且这种扩展是独立有价值的。我们的代码可以在https://github.com/abukharin3/ERNIE上获取。
</details></li>
</ul>
<hr>
<h2 id="Regularization-properties-of-adversarially-trained-linear-regression"><a href="#Regularization-properties-of-adversarially-trained-linear-regression" class="headerlink" title="Regularization properties of adversarially-trained linear regression"></a>Regularization properties of adversarially-trained linear regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10807">http://arxiv.org/abs/2310.10807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/antonior92/advtrain-linreg">https://github.com/antonior92/advtrain-linreg</a></li>
<li>paper_authors: Antônio H. Ribeiro, Dave Zachariah, Francis Bach, Thomas B. Schön</li>
<li>For: The paper is focused on studying the effectiveness of adversarial training in defending against input perturbations in linear models, and exploring the relationship between adversarial training and other regularization methods.* Methods: The paper uses a min-max formulation of adversarial training to search for the best solution when the training data are corrupted by worst-case attacks. The authors also compare the solution of adversarial training with other regularization methods, such as ridge regression and Lasso.* Results: The main findings of the paper include that adversarial training yields the minimum-norm interpolating solution in the overparameterized regime, and that adversarial training can be equivalent to parameter shrinking methods in the underparametrized region. Additionally, the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. The authors confirm their theoretical findings with numerical examples.<details>
<summary>Abstract</summary>
State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training -- as in square-root Lasso -- the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.
</details>
<details>
<summary>摘要</summary>
现代机器学习模型可能受到非常小的输入干扰，这些干扰是利用攻击性的构造的。对抗攻击是一种有效的防御策略。作为一个最小化问题，它搜索最佳解决方案，当训练数据被攻击最坏的攻击时。线性模型是我们研究的简单模型，在这种情况下，对抗训练转化为一个凸优化问题，可以通过最小化一个有限和来解决。我们提供了对抗训练在线性回归和其他规化方法之间的比较分析。我们的主要发现是：(A) 对抗训练在过参数化 régime（更多参数 чем数据）下给出最小欧式 interpolator，只要攻击干扰半径小于一个阈值。而且，对抗训练的解决方案与最小欧式 interpolator相同。(B) 对抗训练可以与参数缩放方法（ridge regression和lasso）等同。这发生在下参数化 régime，对应的攻击干扰半径和零均匀分布的covariate是合适的。(C) 对 $\ell_\infty$-对抗训练（如平方lasso）中选择的攻击干扰半径对最佳 bound 没有依赖于加法噪声Variance。我们通过数学示例证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Neural-Tangent-Kernels-Motivate-Graph-Neural-Networks-with-Cross-Covariance-Graphs"><a href="#Neural-Tangent-Kernels-Motivate-Graph-Neural-Networks-with-Cross-Covariance-Graphs" class="headerlink" title="Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs"></a>Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10791">http://arxiv.org/abs/2310.10791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shervin Khalafi, Saurabh Sihag, Alejandro Ribeiro</li>
<li>for: 这篇论文探讨了用神经网络学习推论和泛化行为，特别是在图 нейрон网络（GNN）中。</li>
<li>methods: 该论文使用了神经 tangent 函数（NTK）来分析神经网络的学习和泛化行为。</li>
<li>results: 研究发现，在 GNN 中，优化对齐关系（alignment）可以优化图表示或图变换函数，并且有理论保证对齐是最佳的。实验结果表明，使用 cross-covariance 作为图 shift 函数可以超过只使用输入数据 covariance matrix 的 GNN。<details>
<summary>Abstract</summary>
Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK kernel and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept, we investigate NTKs and alignment in the context of graph neural networks (GNNs), where our analysis reveals that optimizing alignment translates to optimizing the graph representation or the graph shift operator in a GNN. Our results further establish the theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experiments focused on a multi-variate time series prediction task for a publicly available dataset. Specifically, they demonstrate that GNNs with cross-covariance as the graph shift operator indeed outperform those that operate on the covariance matrix from only the input data.
</details>
<details>
<summary>摘要</summary>
neural tangent kernels (NTKs) 提供了一个理论 régime 来分析过 parametrized  нейрон网络在学习和泛化行为的条件。在一个监督学习任务中，NTK kernel 的 eigenvectors 和资料之间的关联（一个称为“alignment”的概念）可以控制梯度下降的速度，以及对未见到的资料的泛化。在这篇文章中，我们对 GNN （图形神经网络）中的 NTK 和配置进行了研究，我们的分析表明，对 GNN 进行配置的最佳化将导致图形表现或图形移动操作的最佳化。我们的结果还证明了在 GNN 中对 alignment 的最佳化具有理论保证，这些保证是由图形移动操作的cross-covariance 和输入资料的covariance matrix 所决定。我们的实验还证明了 GNN 使用 cross-covariance 作为图形移动操作实际上会比使用仅从输入资料的covariance matrix 来操作更好。
</details></li>
</ul>
<hr>
<h2 id="Correcting-model-misspecification-in-physics-informed-neural-networks-PINNs"><a href="#Correcting-model-misspecification-in-physics-informed-neural-networks-PINNs" class="headerlink" title="Correcting model misspecification in physics-informed neural networks (PINNs)"></a>Correcting model misspecification in physics-informed neural networks (PINNs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10776">http://arxiv.org/abs/2310.10776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zongren Zou, Xuhui Meng, George Em Karniadakis<br>methods: 这 paper 使用的方法包括 physics-informed neural networks (PINNs) 和其他深度神经网络 (DNNs)。results: 这 paper 的结果表明，通过使用 DNNs 来模型不完全的物理模型，可以将计算错误减少，并且可以使 PINNs 在复杂系统中应用。此外，这 paper 还使用 Bayesian PINNs (B-PINNs) 和&#x2F;或 ensemble PINNs 来评估不确定性。<details>
<summary>Abstract</summary>
Data-driven discovery of governing equations in computational science has emerged as a new paradigm for obtaining accurate physical models and as a possible alternative to theoretical derivations. The recently developed physics-informed neural networks (PINNs) have also been employed to learn governing equations given data across diverse scientific disciplines. Despite the effectiveness of PINNs for discovering governing equations, the physical models encoded in PINNs may be misspecified in complex systems as some of the physical processes may not be fully understood, leading to the poor accuracy of PINN predictions. In this work, we present a general approach to correct the misspecified physical models in PINNs for discovering governing equations, given some sparse and/or noisy data. Specifically, we first encode the assumed physical models, which may be misspecified, then employ other deep neural networks (DNNs) to model the discrepancy between the imperfect models and the observational data. Due to the expressivity of DNNs, the proposed method is capable of reducing the computational errors caused by the model misspecification and thus enables the applications of PINNs in complex systems where the physical processes are not exactly known. Furthermore, we utilize the Bayesian PINNs (B-PINNs) and/or ensemble PINNs to quantify uncertainties arising from noisy and/or gappy data in the discovered governing equations. A series of numerical examples including non-Newtonian channel and cavity flows demonstrate that the added DNNs are capable of correcting the model misspecification in PINNs and thus reduce the discrepancy between the physical models and the observational data. We envision that the proposed approach will extend the applications of PINNs for discovering governing equations in problems where the physico-chemical or biological processes are not well understood.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用数据驱动的方法发现计算机科学中的管理方程式已经成为一种新的方法，以获取准确的物理模型，也可能是理论 derivations的可能的替代方法。最近开发的物理学泛化神经网络（PINNs）已经在多个科学领域中使用来学习管理方程式。 despite the effectiveness of PINNs for discovering governing equations, the physical models encoded in PINNs may be misspecified in complex systems as some of the physical processes may not be fully understood, leading to poor accuracy of PINN predictions. In this work, we present a general approach to correct the misspecified physical models in PINNs for discovering governing equations, given some sparse and/or noisy data. Specifically, we first encode the assumed physical models, which may be misspecified, then employ other deep neural networks (DNNs) to model the discrepancy between the imperfect models and the observational data. Due to the expressivity of DNNs, the proposed method is capable of reducing the computational errors caused by the model misspecification and thus enables the applications of PINNs in complex systems where the physical processes are not exactly known. Furthermore, we utilize the Bayesian PINNs (B-PINNs) and/or ensemble PINNs to quantify uncertainties arising from noisy and/or gappy data in the discovered governing equations. A series of numerical examples including non-Newtonian channel and cavity flows demonstrate that the added DNNs are capable of correcting the model misspecification in PINNs and thus reduce the discrepancy between the physical models and the observational data. We envision that the proposed approach will extend the applications of PINNs for discovering governing equations in problems where the physico-chemical or biological processes are not well understood.
</details></li>
</ul>
<hr>
<h2 id="Gotta-be-SAFE-A-New-Framework-for-Molecular-Design"><a href="#Gotta-be-SAFE-A-New-Framework-for-Molecular-Design" class="headerlink" title="Gotta be SAFE: A New Framework for Molecular Design"></a>Gotta be SAFE: A New Framework for Molecular Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10773">http://arxiv.org/abs/2310.10773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emmanuel Noutahi, Cristian Gabellini, Michael Craig, Jonathan S. C Lim, Prudencio Tossou</li>
<li>for: 用于AI驱动的分子设计</li>
<li>methods: 使用Sequential Attachment-based Fragment Embedding（SAFE），一种新的线notation для化学结构</li>
<li>results: 通过训练一个87亿参数的GPT2-like模型，实现了多种优秀的优化性能，打开了新的化学空间探索的可能性，对AI驱动的分子设计具有广泛的应用前景<details>
<summary>Abstract</summary>
Traditional molecular string representations, such as SMILES, often pose challenges for AI-driven molecular design due to their non-sequential depiction of molecular substructures. To address this issue, we introduce Sequential Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical structures. SAFE reimagines SMILES strings as an unordered sequence of interconnected fragment blocks while maintaining full compatibility with existing SMILES parsers. It streamlines complex generative tasks, including scaffold decoration, fragment linking, polymer generation, and scaffold hopping, while facilitating autoregressive generation for fragment-constrained design, thereby eliminating the need for intricate decoding or graph-based models. We demonstrate the effectiveness of SAFE by training an 87-million-parameter GPT2-like model on a dataset containing 1.1 billion SAFE representations. Through extensive experimentation, we show that our SAFE-GPT model exhibits versatile and robust optimization performance. SAFE opens up new avenues for the rapid exploration of chemical space under various constraints, promising breakthroughs in AI-driven molecular design.
</details>
<details>
<summary>摘要</summary>
传统分子串表示方式，如SMILES，经常对AI驱动分子设计带来挑战，因为它们不能正确地表示分子子结构的顺序。为解决这个问题，我们介绍Sequential Attachment-based Fragment Embedding（SAFE），一种新的化学结构表示方式。SAFE将SMILES字符串重新表示为一个无序的连接式块序列，同时保持与现有SMILES解析器的兼容性。这种方法可以简化复杂的生成任务，如架构饰 ornamentation、分子连接、聚合物生成和架构跳跃，并且可以支持束缚生成，从而消除需要复杂的解码或图形模型。我们在一个8700万参数的GPT2-like模型上训练了SAFE模型，并通过广泛的实验表明了我们的SAFE-GPT模型在不同的约束下进行优化表现灵活和强大。SAFE打开了新的化学空间探索的可能性，承诺AI驱动分子设计中的突破。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Lead-Sheet-Generation-via-Semantic-Compression"><a href="#Unsupervised-Lead-Sheet-Generation-via-Semantic-Compression" class="headerlink" title="Unsupervised Lead Sheet Generation via Semantic Compression"></a>Unsupervised Lead Sheet Generation via Semantic Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10772">http://arxiv.org/abs/2310.10772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zacharynovack/lead-ae">https://github.com/zacharynovack/lead-ae</a></li>
<li>paper_authors: Zachary Novack, Nikita Srivatsan, Taylor Berg-Kirkpatrick, Julian McAuley</li>
<li>for: 本研究旨在提高生成音乐的效率和质量，通过生成与原始乐谱版本相对应的简化后的乐谱（lead sheet）。</li>
<li>methods: 我们提出了一种新的模型 called Lead-AE，它使用可控的局部稀缺约束来模型乐谱，并使用可导的top-k算法来实现简化后的乐谱可控。</li>
<li>results: 我们的方法比已有的决定性基线要好，可以生成具有准确性和完整性的简化后乐谱，并且在人工评价中也得到了良好的评价。<details>
<summary>Abstract</summary>
Lead sheets have become commonplace in generative music research, being used as an initial compressed representation for downstream tasks like multitrack music generation and automatic arrangement. Despite this, researchers have often fallen back on deterministic reduction methods (such as the skyline algorithm) to generate lead sheets when seeking paired lead sheets and full scores, with little attention being paid toward the quality of the lead sheets themselves and how they accurately reflect their orchestrated counterparts. To address these issues, we propose the problem of conditional lead sheet generation (i.e. generating a lead sheet given its full score version), and show that this task can be formulated as an unsupervised music compression task, where the lead sheet represents a compressed latent version of the score. We introduce a novel model, called Lead-AE, that models the lead sheets as a discrete subselection of the original sequence, using a differentiable top-k operator to allow for controllable local sparsity constraints. Across both automatic proxy tasks and direct human evaluations, we find that our method improves upon the established deterministic baseline and produces coherent reductions of large multitrack scores.
</details>
<details>
<summary>摘要</summary>
乐谱简纸在生成音乐研究中变得普遍，用作下游任务的初始压缩表示，如多轨音乐生成和自动编排。尽管如此，研究人员通常会回归到决定性减少方法（如天空线算法）来生成乐谱简纸，未受到乐谱简纸本身质量和如何准确反映其管弦乐谱的关注。为解决这些问题，我们提出了 conditional lead sheet generation 问题（即根据全音乐谱版本生成乐谱简纸），并证明这可以视为一种无监督的音乐压缩任务，其中乐谱简纸表示一个压缩的 latent 序列。我们提出了一种新的模型， называ为 Lead-AE，该模型将乐谱简纸视为原始序列的一个离散子选择，使用可微 differentiable top-k 算子来实现可控的地方缺失约束。在自动代理任务和直接人类评估中，我们发现我们的方法比已有的决定性基线更好，并生成了大量多轨音乐谱的准确压缩。
</details></li>
</ul>
<hr>
<h2 id="Wide-Neural-Networks-as-Gaussian-Processes-Lessons-from-Deep-Equilibrium-Models"><a href="#Wide-Neural-Networks-as-Gaussian-Processes-Lessons-from-Deep-Equilibrium-Models" class="headerlink" title="Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models"></a>Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10767">http://arxiv.org/abs/2310.10767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianxiang Gao, Xiaokai Huo, Hailiang Liu, Hongyang Gao</li>
<li>for: 本研究探讨了深度平衡模型（DEQ），即无限层 neural network 的普适性和训练特性。</li>
<li>methods: 本研究使用了 neural ordinary differential equations（ODEs）和 deep equilibrium models（DEQs）来探讨深度平衡模型的性质。</li>
<li>results: 研究发现，当 DEQ 层宽度趋于无穷时，它会 converge to a Gaussian process，并且这种整合在深度和宽度之间进行交换时不会出现。此外，研究还发现，相关的 Gaussian vector 在任意两个不同输入数据对之间保持非零最小特征值，这使得 NNGP kernel 具有稳定性。这些发现对 DEQ 的训练和泛化做出了基础性的研究。<details>
<summary>Abstract</summary>
Neural networks with wide layers have attracted significant attention due to their equivalence to Gaussian processes, enabling perfect fitting of training data while maintaining generalization performance, known as benign overfitting. However, existing results mainly focus on shallow or finite-depth networks, necessitating a comprehensive analysis of wide neural networks with infinite-depth layers, such as neural ordinary differential equations (ODEs) and deep equilibrium models (DEQs). In this paper, we specifically investigate the deep equilibrium model (DEQ), an infinite-depth neural network with shared weight matrices across layers. Our analysis reveals that as the width of DEQ layers approaches infinity, it converges to a Gaussian process, establishing what is known as the Neural Network and Gaussian Process (NNGP) correspondence. Remarkably, this convergence holds even when the limits of depth and width are interchanged, which is not observed in typical infinite-depth Multilayer Perceptron (MLP) networks. Furthermore, we demonstrate that the associated Gaussian vector remains non-degenerate for any pairwise distinct input data, ensuring a strictly positive smallest eigenvalue of the corresponding kernel matrix using the NNGP kernel. These findings serve as fundamental elements for studying the training and generalization of DEQs, laying the groundwork for future research in this area.
</details>
<details>
<summary>摘要</summary>
神经网络with宽层有吸引了广泛关注，因为它们相当于 Gaussian 过程，可以完美适应训练数据而且保持泛化性能，称为恰好的过拟合。然而，现有的研究主要集中在浅或固定深度的神经网络上，需要进行广泛的抽象深度神经网络的研究，如神经ordinary differential equations（ODEs）和deep equilibrium models（DEQs）。在这篇论文中，我们专门研究深度平衡模型（DEQ），这是一个无限深度神经网络，具有共享权重矩阵的层。我们的分析表明，当 DEQ 层的宽度接近无穷大时，它会 converge to a Gaussian process，确立了称为神经网络和Gaussian过程（NNGP）匹配。很Remarkably，这种convergence 随着深度和宽度的限制的交换，不同于 typical 无限深度多层感知（MLP）网络。此外，我们证明了相关的 Gaussian vector 在任意不同输入数据对之间保持非零特征值，确保 smallest eigenvalue  of the corresponding kernel matrix  strictly positive using the NNGP kernel。这些发现对 DEQ 的训练和泛化提供了基本的元素，为将来在这个领域的研究奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Exploring-hyperelastic-material-model-discovery-for-human-brain-cortex-multivariate-analysis-vs-artificial-neural-network-approaches"><a href="#Exploring-hyperelastic-material-model-discovery-for-human-brain-cortex-multivariate-analysis-vs-artificial-neural-network-approaches" class="headerlink" title="Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches"></a>Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10762">http://arxiv.org/abs/2310.10762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jixin Hou, Nicholas Filla, Xianyan Chen, Mir Jalil Razavi, Tianming Liu, Xianqiao Wang</li>
<li>for: 这个研究的目的是找到最适合人脑组织的 constitutive material model.</li>
<li>methods: 这个研究使用人工神经网络和多元回归方法来自动找到合适的 constitutive material model.</li>
<li>results: 研究发现，人工神经网络可以自动地找到准确的 constitutive material model，但是五个参数和两个参数神经网络模型在单模和多模载 scenarios下被发现是不优的，可以further simplifies into two-term和单term模型。这些发现 highlights the importance of hyperparameters for artificial neural network和emphasize the necessity for detailed cross-validations of regularization parameters to ensure optimal selection at a global level.<details>
<summary>Abstract</summary>
Traditional computational methods, such as the finite element analysis, have provided valuable insights into uncovering the underlying mechanisms of brain physical behaviors. However, precise predictions of brain physics require effective constitutive models to represent the intricate mechanical properties of brain tissue. In this study, we aimed to identify the most favorable constitutive material model for human brain tissue. To achieve this, we applied artificial neural network and multiple regression methods to a generalization of widely accepted classic models, and compared the results obtained from these two approaches. To evaluate the applicability and efficacy of the model, all setups were kept consistent across both methods, except for the approach to prevent potential overfitting. Our results demonstrate that artificial neural networks are capable of automatically identifying accurate constitutive models from given admissible estimators. Nonetheless, the five-term and two-term neural network models trained under single-mode and multi-mode loading scenarios, were found to be suboptimal and could be further simplified into two-term and single-term, respectively, with higher accuracy using multiple regression. Our findings highlight the importance of hyperparameters for the artificial neural network and emphasize the necessity for detailed cross-validations of regularization parameters to ensure optimal selection at a global level in the development of material constitutive models. This study validates the applicability and accuracy of artificial neural network to automatically discover constitutive material models with proper regularization as well as the benefits in model simplification without compromising accuracy for traditional multivariable regression.
</details>
<details>
<summary>摘要</summary>
传统计算方法，如finite element分析，已经提供了许多关键的发现，揭示了大脑物理行为的下面机制。然而，精确预测大脑物理需要有效的 constitutive 模型来表示大脑组织的复杂机械性质。在本研究中，我们想要找到最佳的 constitutive 材料模型 для人类大脑组织。为了实现这一目标，我们使用人工神经网络和多重回归方法，并对这两种方法进行比较。为了评估模型的适用性和效果，所有的设置都保持了一致，除非是避免过拟合。我们的结果表明，人工神经网络可以自动地从给定的可接受的估计器中提取精确的 constitutive 模型。然而，在单模式和多模式荷载场景下，五项和二项神经网络模型被发现为不优化，可以进一步简化为二项和单项模型，具有更高的准确率。我们的发现强调了人工神经网络中的hyperparameter的重要性，并提醒了在开发物理模型时需要进行详细的交叉验证，以确保优选的全局范围内的正则化参数。本研究证明了人工神经网络可以自动地找到符合正则化的 constitutive 材料模型，并且可以避免过拟合而不会产生准确性下降。
</details></li>
</ul>
<hr>
<h2 id="Statistical-Barriers-to-Affine-equivariant-Estimation"><a href="#Statistical-Barriers-to-Affine-equivariant-Estimation" class="headerlink" title="Statistical Barriers to Affine-equivariant Estimation"></a>Statistical Barriers to Affine-equivariant Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10758">http://arxiv.org/abs/2310.10758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Chen, Yeshwanth Cherapanamjeri</li>
<li>for:  Robust mean estimation in high-dimensional datasets with affine-invariant properties.</li>
<li>methods: Affine-equivariant estimators, lower bounds, and a new estimator based on a high-dimensional median.</li>
<li>results: Strict degradation in recovery error with quantitative rates degrading by a factor of $\sqrt{d}$ under two outlier models, and a new affine-equivariant estimator that nearly matches the lower bound.<details>
<summary>Abstract</summary>
We investigate the quantitative performance of affine-equivariant estimators for robust mean estimation. As a natural stability requirement, the construction of such affine-equivariant estimators has been extensively studied in the statistics literature. We quantitatively evaluate these estimators under two outlier models which have been the subject of much recent work: the heavy-tailed and adversarial corruption settings. We establish lower bounds which show that affine-equivariance induces a strict degradation in recovery error with quantitative rates degrading by a factor of $\sqrt{d}$ in both settings. We find that classical estimators such as the Tukey median (Tukey '75) and Stahel-Donoho estimator (Stahel '81 and Donoho '82) are either quantitatively sub-optimal even within the class of affine-equivariant estimators or lack any quantitative guarantees. On the other hand, recent estimators with strong quantitative guarantees are not affine-equivariant or require additional distributional assumptions to achieve it. We remedy this by constructing a new affine-equivariant estimator which nearly matches our lower bound. Our estimator is based on a novel notion of a high-dimensional median which may be of independent interest. Notably, our results are applicable more broadly to any estimator whose performance is evaluated in the Mahalanobis norm which, for affine-equivariant estimators, corresponds to an evaluation in Euclidean norm on isotropic distributions.
</details>
<details>
<summary>摘要</summary>
我们研究了不变式性的估计器在鲁棒均值估计中的量化性能。作为自然的稳定要求，建构这类不变式估计器在统计学Literature中得到了广泛的研究。我们量化地评估这些估计器在两种噪声模型下：重 tailed 和 adversarial corruption 设定下。我们建立了下限，显示不变式性导致了减少Recovery error的精度下限，具体是在两个设定下的 $\sqrt{d}$ 因子下降。我们发现经典估计器，如Tukey median（Tukey '75）和Stahel-Donoho estimator（Stahel '81和Donoho '82）在不变式估计器中是量化上不优或者没有量化保证。然而，现有的估计器具有强量化保证的，但是它们不是不变式的或者需要额外的分布假设来实现不变式性。我们提供了一种新的不变式估计器，它几乎与我们的下限匹配。我们的估计器基于一种新的高维度中位数据，这可能是独立的兴趣。值得注意的是，我们的结果适用于任何在Mahalanobis 距离上评估的估计器，这对于不变式估计器来说相当于在几何均勋度上评估。
</details></li>
</ul>
<hr>
<h2 id="Mori-Zwanzig-latent-space-Koopman-closure-for-nonlinear-autoencoder"><a href="#Mori-Zwanzig-latent-space-Koopman-closure-for-nonlinear-autoencoder" class="headerlink" title="Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder"></a>Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10745">http://arxiv.org/abs/2310.10745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Priyam Gupta, Peter J. Schmid, Denis Sipp, Taraneh Sayadi, Georgios Rigas</li>
<li>for: 该研究旨在提高数据驱动方法的精度和稳定性，以便更好地理解和预测复杂非线性系统的动态。</li>
<li>methods: 该研究提出了一种新的Morzy-Zwanzig自适应器（MZ-AE），通过非线性自适应器提取关键观察量，并通过Morzy-Zwanzigormalism来修正非马洛夫矩阵，实现了closed的动态表示。</li>
<li>results: 实验表明，MZ-AE可以准确地捕捉圆柱体流动中的模式转移，并提供了低维度的预测模型，对恒定 Kuramoto-Sivashinsky 系统 exhibit promising short-term predictability和robust long-term statistical performance。<details>
<summary>Abstract</summary>
The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and stability of the Koopman operator approximation. Demonstrations showcase the technique's ability to capture regime transitions in the flow around a circular cylinder. It also provided a low dimensional approximation for chaotic Kuramoto-Sivashinsky with promising short-term predictability and robust long-term statistical performance. By bridging the gap between data-driven techniques and the mathematical foundations of Koopman theory, MZ-AE offers a promising avenue for improved understanding and prediction of complex nonlinear dynamics.
</details>
<details>
<summary>摘要</summary>
科普曼运算符提供了一种globally linearization的方法，使得非线性系统的理解得以简化。虽然数据驱动的方法在 aproximate Koopman operator 方面表现出了承诺，但它们还需要解决一些挑战，例如选择合适的观察量、维度减少和准确预测复杂系统行为。这种研究提出了一种新的方法，即Mori-Zwanzig autoencoder (MZ-AE)，以稳定地 aproximate Koopman operator 在低维空间中。该方法利用非线性自适应神经网络提取关键观察量，并通过Morin Zwanzig 正则进行修正。因此，该方法可以在非线性自适应神经网络的 latent manifold 中 closure 动力学，从而提高 Koopman operator 的准确性和稳定性。示例显示该方法可以在圆柱体流动中捕捉转态。此外，它还为混沌 Kuramoto-Sivashinsky 提供了一种低维度的近似，并且在短期预测和长期统计性能方面具有承诺。通过将数据驱动技术与 Koopman 理论的数学基础相连接，MZ-AE 提供了一条可能的通路，以提高复杂非线性动力学的理解和预测。
</details></li>
</ul>
<hr>
<h2 id="Fast-Adversarial-Label-Flipping-Attack-on-Tabular-Data"><a href="#Fast-Adversarial-Label-Flipping-Attack-on-Tabular-Data" class="headerlink" title="Fast Adversarial Label-Flipping Attack on Tabular Data"></a>Fast Adversarial Label-Flipping Attack on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10744">http://arxiv.org/abs/2310.10744</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinglong Chang, Gillian Dobbie, Jörg Wicker</li>
<li>for: 这篇研究旨在阐述机器学习模型在需要高可靠性的领域中受到攻击的问题，以及这些攻击的威胁。</li>
<li>methods: 本研究提出了一种新的快速攻击方法，即 Fast Adversarial Label-Flipping Attack (FALFA)，用于游戏机器学习模型。FALFA基于对敌人目标的变数转换，并使用线性程式来降低计算Complexity。</li>
<li>results: 使用了10个真实的条形数据集，研究发现FALFA具有高度的攻击潜力，显示了需要robust的防御措施。<details>
<summary>Abstract</summary>
Machine learning models are increasingly used in fields that require high reliability such as cybersecurity. However, these models remain vulnerable to various attacks, among which the adversarial label-flipping attack poses significant threats. In label-flipping attacks, the adversary maliciously flips a portion of training labels to compromise the machine learning model. This paper raises significant concerns as these attacks can camouflage a highly skewed dataset as an easily solvable classification problem, often misleading machine learning practitioners into lower defenses and miscalculations of potential risks. This concern amplifies in tabular data settings, where identifying true labels requires expertise, allowing malicious label-flipping attacks to easily slip under the radar. To demonstrate this risk is inherited in the adversary's objective, we propose FALFA (Fast Adversarial Label-Flipping Attack), a novel efficient attack for crafting adversarial labels. FALFA is based on transforming the adversary's objective and employs linear programming to reduce computational complexity. Using ten real-world tabular datasets, we demonstrate FALFA's superior attack potential, highlighting the need for robust defenses against such threats.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MOFDiff-Coarse-grained-Diffusion-for-Metal-Organic-Framework-Design"><a href="#MOFDiff-Coarse-grained-Diffusion-for-Metal-Organic-Framework-Design" class="headerlink" title="MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design"></a>MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10732">http://arxiv.org/abs/2310.10732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Fu, Tian Xie, Andrew S. Rosen, Tommi Jaakkola, Jake Smith</li>
<li>for: 这项研究旨在开发一种基于推卷模型的金属有机框架（MOF）结构生成器，以便为碳捕集应用提供高性能的MOF材料。</li>
<li>methods: 该研究使用了一种基于等距离推卷模型的diffusion模型，通过对分子组分部件坐标和identities进行杜磊推卷过程，生成高精度的MOF结构。然后，通过一种新型组装算法，确定了全原子MOF结构。</li>
<li>results: 研究人员通过分子仿真实验，证明了该模型可以生成有效和新型的MOF结构，并且可以有效地设计出standing MOFOaterials for carbon capture应用。<details>
<summary>Abstract</summary>
Metal-organic frameworks (MOFs) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry. Their modular nature has enabled the use of template-based methods to generate hypothetical MOFs by combining molecular building blocks in accordance with known network topologies. However, the ability of these methods to identify top-performing MOFs is often hindered by the limited diversity of the resulting chemical space. In this work, we propose MOFDiff: a coarse-grained (CG) diffusion model that generates CG MOF structures through a denoising diffusion process over the coordinates and identities of the building blocks. The all-atom MOF structure is then determined through a novel assembly algorithm. Equivariant graph neural networks are used for the diffusion model to respect the permutational and roto-translational symmetries. We comprehensively evaluate our model's capability to generate valid and novel MOF structures and its effectiveness in designing outstanding MOF materials for carbon capture applications with molecular simulations.
</details>
<details>
<summary>摘要</summary>
金属有机框架（MOF）在应用于气体存储和碳捕集等领域具有极高的利用价值，这主要归功于它们的非常的孔隙和可调化化学结构。MOF的模块性质使得可以通过模板基本方法生成 гипотетическихMOF结构，这些结构是通过将分子结构块组合在已知网络结构中来实现的。然而，这些方法的选择性往往受到生成化学空间的局限性的影响。在这种情况下，我们提出了MOFDiff：一种粗粒度（CG）扩散模型，该模型通过CG结构块坐标和分子标识的杜瓦扩散过程来生成CG MOF结构。然后，我们使用一种新的组装算法来确定全原子MOF结构。我们使用对称图 Néural networks来实现扩散模型，以保持分子的卷积和旋转平移 symmetries。我们对我们的模型的有效性进行了广泛的评估，并通过分子价值计算来评估MOF材料在碳捕集应用中的性能。
</details></li>
</ul>
<hr>
<h2 id="A-representation-learning-approach-to-probe-for-dynamical-dark-energy-in-matter-power-spectra"><a href="#A-representation-learning-approach-to-probe-for-dynamical-dark-energy-in-matter-power-spectra" class="headerlink" title="A representation learning approach to probe for dynamical dark energy in matter power spectra"></a>A representation learning approach to probe for dynamical dark energy in matter power spectra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10717">http://arxiv.org/abs/2310.10717</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Piras, Lucas Lombriser</li>
<li>for:  searched for a compressed representation of dynamical dark energy models in observational studies of the cosmic large-scale structure.</li>
<li>methods:  trained a variational autoencoder (VAE) architecture, DE-VAE, on matter power spectra boosts generated at different redshift values and wavenumbers, and used a neural network to compress and reconstruct the boosts.</li>
<li>results:  found that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra within $1\sigma$ ($2\sigma$) of a Gaussian error, and the three variables can be linked together with an explicit equation through symbolic regression.<details>
<summary>Abstract</summary>
We present DE-VAE, a variational autoencoder (VAE) architecture to search for a compressed representation of dynamical dark energy (DE) models in observational studies of the cosmic large-scale structure. DE-VAE is trained on matter power spectra boosts generated at wavenumbers $k\in(0.01-2.5) \ h/\rm{Mpc}$ and at four redshift values $z\in(0.1,0.48,0.78,1.5)$ for the most typical dynamical DE parametrization with two extra parameters describing an evolving DE equation of state. The boosts are compressed to a lower-dimensional representation, which is concatenated with standard cold dark matter (CDM) parameters and then mapped back to reconstructed boosts; both the compression and the reconstruction components are parametrized as neural networks. Remarkably, we find that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra generated over a broad range of cosmological parameters within $1\sigma$ ($2\sigma$) of a Gaussian error which includes cosmic variance, shot noise and systematic effects for a Stage IV-like survey. This single parameter shows a high mutual information with the two DE parameters, and these three variables can be linked together with an explicit equation through symbolic regression. Considering a model with two latent variables only marginally improves the accuracy of the predictions, and adding a third latent variable has no significant impact on the model's performance. We discuss how the DE-VAE architecture can be extended from a proof of concept to a general framework to be employed in the search for a common lower-dimensional parametrization of a wide range of beyond-$\Lambda$CDM models and for different cosmological datasets. Such a framework could then both inform the development of cosmological surveys by targeting optimal probes, and provide theoretical insight into the common phenomenological aspects of beyond-$\Lambda$CDM models.
</details>
<details>
<summary>摘要</summary>
我们提出了DE-VAE，一种简化自动抽象（VAE）架构，用于在观测宇宙大规模结构的观测学中寻找压缩表现。DE-VAE 被训练在物质能谱强化器中，这些强化器在几何常数 $k\in(0.01-2.5) \ h/\rm{Mpc}$ 和四个红shift值 $z\in(0.1,0.48,0.78,1.5)$ 上生成了最常见的动态暗能（DE）模型的两个额外参数。这些强化器被压缩到较低维度的表现，并与标准冷黑物质（CDM）参数一起 concatenated，然后将其映射回重建的强化器；压缩和重建都是用神经网 parametrized。我们发现，仅具一个潜在参数可以预测95%（99%）的DE强化器生成的广泛范围的 cosmological 参数之间的误差，包括cosmic variance、shot noise和系统效应。这个单一参数与 DE 两个参数之间存在高的共同信息，这三个变数可以通过symbolic regression 连接起来。仅具二个潜在参数的情况只有marginally 提高了预测的精度，而添加第三个潜在参数没有显著影响模型的性能。我们讨论了DE-VAE 架构如何从证明理论中扩展到一个通用的架构，以便在不同的 cosmological 资料集上寻找一致的下dimensional parametrization。这个架构可以帮助发展 cosmological 调查，targeting 最佳探针，并提供理论上的共同现象描述。
</details></li>
</ul>
<hr>
<h2 id="A-Computational-Framework-for-Solving-Wasserstein-Lagrangian-Flows"><a href="#A-Computational-Framework-for-Solving-Wasserstein-Lagrangian-Flows" class="headerlink" title="A Computational Framework for Solving Wasserstein Lagrangian Flows"></a>A Computational Framework for Solving Wasserstein Lagrangian Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10649">http://arxiv.org/abs/2310.10649</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/necludov/wl-mechanics">https://github.com/necludov/wl-mechanics</a></li>
<li>paper_authors: Kirill Neklyudov, Rob Brekelmans, Alexander Tong, Lazar Atanackovic, Qiang Liu, Alireza Makhzani</li>
<li>for: 这篇论文主要针对单元细胞动力学问题进行优化运输问题的扩展，包括不同的可能性空间（kinetic energy）和权重函数（potential energy）的组合，以及这些组合所导致的多种优化运输问题，如契德桥、不均习运输、物理约束等。</li>
<li>methods: 该论文提出了一种基于深度学习的框架，可以处理这些优化运输问题的复杂计算。该框架不需要直接 simulate 或 backpropagate learned dynamics，也不需要优化couplings。</li>
<li>results: 作者们在单元细胞动力学问题中展示了该框架的灵活性和高效性，并比 précédentes 方法（含 incorporating prior knowledge into the dynamics）取得了更好的结果。<details>
<summary>Abstract</summary>
The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperforming previous approaches for the single-cell trajectory inference, where incorporating prior knowledge into the dynamics is crucial for correct predictions.
</details>
<details>
<summary>摘要</summary>
“Optimal transport问题的动力学表述可以通过不同的下层结构（动能）和扩散函数（potential energy）的选择扩展。这些组合导致了不同的变量问题（Lagrangians），包括舒得桥、不均衡优化运输、物理约束优化运输等。总的来说，优化的扩散路径未知，解决这些变量问题可能会 computationally challenging。我们基于对准形式的方法提出了一种新的深度学习框架，该框架不需要 simulate或backpropagate通过学习的动力学轨迹，也不需要对优化的扩散函数进行访问。我们展示了该框架的多样性，在单个细胞轨迹推断中超过了先前的方法。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Efficacy-of-Dual-Encoders-for-Extreme-Multi-Label-Classification"><a href="#Efficacy-of-Dual-Encoders-for-Extreme-Multi-Label-Classification" class="headerlink" title="Efficacy of Dual-Encoders for Extreme Multi-Label Classification"></a>Efficacy of Dual-Encoders for Extreme Multi-Label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10636">http://arxiv.org/abs/2310.10636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Gupta, Devvrit Khatri, Ankit S Rawat, Srinadh Bhojanapalli, Prateek Jain, Inderjit S Dhillon</li>
<li>for: 这个论文主要针对的是多类分类问题（Extreme Multi-Label Classification，XMC），具体来说是使用 dual-encoder 模型来解决这类问题。</li>
<li>methods: 这篇论文使用了 dual-encoder 模型，并且提出了一种新的损失函数来优化 Recall@k 纪录。</li>
<li>results: 论文的实验结果表明，使用标准的 dual-encoder 模型可以与现有的 SOTA 多类分类方法匹配或超越，即使是在最大的 XMC 数据集上。此外，论文还提出了一种可微的 topk 错误基于损失函数，可以专门优化 Recall@k 纪录。<details>
<summary>Abstract</summary>
Dual-encoder models have demonstrated significant success in dense retrieval tasks for open-domain question answering that mostly involves zero-shot and few-shot scenarios. However, their performance in many-shot retrieval problems where training data is abundant, such as extreme multi-label classification (XMC), remains under-explored. Existing empirical evidence suggests that, for such problems, the dual-encoder method's accuracies lag behind the performance of state-of-the-art (SOTA) extreme classification methods that grow the number of learnable parameters linearly with the number of classes. As a result, some recent extreme classification techniques use a combination of dual-encoders and a learnable classification head for each class to excel on these tasks. In this paper, we investigate the potential of "pure" DE models in XMC tasks. Our findings reveal that when trained correctly standard dual-encoders can match or outperform SOTA extreme classification methods by up to 2% at Precision@1 even on the largest XMC datasets while being 20x smaller in terms of the number of trainable parameters. We further propose a differentiable topk error-based loss function, which can be used to specifically optimize for Recall@k metrics. We include our PyTorch implementation along with other resources for reproducing the results in the supplementary material.
</details>
<details>
<summary>摘要</summary>
dual-encoder 模型在开放问题 answering 中的 dense retrieval 任务中表现出了重要的成功，特别是在零shot 和几shot 场景下。然而，它们在有很多training data的 many-shot retrieval 问题中，如极多标签分类 (XMC)，的性能还未得到了充分的探索。现有的实际证据表明，对于这些任务， dual-encoder 方法的准确率落后于 state-of-the-art (SOTA) 极分类方法的性能，后者通过将学习参数的数量与类数直线上增加来提高性能。因此，一些最新的极分类技术使用了 dual-encoder 和每个类别上的学习权重的组合来进行优化。在这篇论文中，我们调查了 "纯" dual-encoder 模型在 XMC 任务中的潜力。我们发现，当正确地训练标准 dual-encoder 模型时，它们可以与 SOTA 极分类方法相当或超越它们，在最大 XMC 数据集上的精度@1 指标上提高至2%，而且只需20倍的学习参数数量。我们还提出了一种可微的 topk 错误基于损失函数，可以专门优化 Recall@k 指标。我们在辅料中包含了 PyTorch 实现以及其他用于重现结果的资源。
</details></li>
</ul>
<hr>
<h2 id="Certainty-In-Certainty-Out-REVQCs-for-Quantum-Machine-Learning"><a href="#Certainty-In-Certainty-Out-REVQCs-for-Quantum-Machine-Learning" class="headerlink" title="Certainty In, Certainty Out: REVQCs for Quantum Machine Learning"></a>Certainty In, Certainty Out: REVQCs for Quantum Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10629">http://arxiv.org/abs/2310.10629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hannah Helgesen, Michael Felsberg, Jan-Åke Larsson</li>
<li>for: 这篇论文的目的是提出高单个样本准确率作为主要目标，并提出一种逆向培训方法以实现此目标。</li>
<li>methods: 该论文使用统计理论和反向培训方法来实现高准确率单个样本推断。</li>
<li>results: 论文通过评估多种有效的变换量量计划（VQC）在随机二进制子集上进行单个样本推断时，实现了10-15%的提升。<details>
<summary>Abstract</summary>
The field of Quantum Machine Learning (QML) has emerged recently in the hopes of finding new machine learning protocols or exponential speedups for classical ones. Apart from problems with vanishing gradients and efficient encoding methods, these speedups are hard to find because the sampling nature of quantum computers promotes either simulating computations classically or running them many times on quantum computers in order to use approximate expectation values in gradient calculations. In this paper, we make a case for setting high single-sample accuracy as a primary goal. We discuss the statistical theory which enables highly accurate and precise sample inference, and propose a method of reversed training towards this end. We show the effectiveness of this training method by assessing several effective variational quantum circuits (VQCs), trained in both the standard and reversed directions, on random binary subsets of the MNIST and MNIST Fashion datasets, on which our method provides an increase of $10-15\%$ in single-sample inference accuracy.
</details>
<details>
<summary>摘要</summary>
quantum机器学习（QML）领域在最近才出现，旨在找到新的机器学习协议或类比速度。不过，由于混合度难以计算和有效编码方法，这些增速很难找。在这篇论文中，我们提出了设置高单个样本准确率为主要目标的观点。我们讨论了 Statistical Theory，它使得高准确和精确的样本推测成为可能，并提出了反向训练方法。我们通过评估多种有效的量子征值逻辑环（VQC），在标准和反向方向上进行训练，在随机二进制subset of MNIST和MNIST Fashion数据集上显示了10-15%的单个样本推测准确率提高。
</details></li>
</ul>
<hr>
<h2 id="How-Do-Transformers-Learn-In-Context-Beyond-Simple-Functions-A-Case-Study-on-Learning-with-Representations"><a href="#How-Do-Transformers-Learn-In-Context-Beyond-Simple-Functions-A-Case-Study-on-Learning-with-Representations" class="headerlink" title="How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations"></a>How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10616">http://arxiv.org/abs/2310.10616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, Yu Bai</li>
<li>for: 这篇论文探讨了基于转换器架构的大语言模型在更复杂的情况下的启发式学习（ICL）能力，以及这种能力的理论和机制。</li>
<li>methods: 作者构造了一系列基于复杂表达函数的synthetic ICLe学习问题，并证明了存在特定的转换器可以近似地实现这些算法，只需要较少的深度和大小。在实验中，作者发现训练过的转换器能够在这些设定下达到近似optimal ICL性能，并展示了层次结构的分解，其中下层层transforms the dataset，而上层进行线性ICL。</li>
<li>results: 作者通过广泛的探索和一种新的粘贴实验发现了许多在训练过的转换器中的机制，如输入和表示的具体复制行为，上层线性ICL能力，以及在更加复杂的混合 Setting中的后ICL表示选择机制。这些观察到的机制与作者的理论相符，可能有助于理解转换器在更真实的场景中的ICL能力。<details>
<summary>Abstract</summary>
While large language models based on the transformer architecture have demonstrated remarkable in-context learning (ICL) capabilities, understandings of such capabilities are still in an early stage, where existing theory and mechanistic understanding focus mostly on simple scenarios such as learning simple function classes. This paper takes initial steps on understanding ICL in more complex scenarios, by studying learning with representations. Concretely, we construct synthetic in-context learning problems with a compositional structure, where the label depends on the input through a possibly complex but fixed representation function, composed with a linear function that differs in each instance. By construction, the optimal ICL algorithm first transforms the inputs by the representation function, and then performs linear ICL on top of the transformed dataset. We show theoretically the existence of transformers that approximately implement such algorithms with mild depth and size. Empirically, we find trained transformers consistently achieve near-optimal ICL performance in this setting, and exhibit the desired dissection where lower layers transforms the dataset and upper layers perform linear ICL. Through extensive probing and a new pasting experiment, we further reveal several mechanisms within the trained transformers, such as concrete copying behaviors on both the inputs and the representations, linear ICL capability of the upper layers alone, and a post-ICL representation selection mechanism in a harder mixture setting. These observed mechanisms align well with our theory and may shed light on how transformers perform ICL in more realistic scenarios.
</details>
<details>
<summary>摘要</summary>
大型语言模型基于变换器架构已经展示了很出色的上下文学习（ICL）能力，但对这些能力的理解仍然处于早期阶段，现有的理论和机制理解主要集中在简单的情景下，如学习简单的函数类。这篇论文从更复杂的情景出发，研究学习表示法。具体来说，我们构造了一些具有复合结构的培成式上下文学习问题，其中标签取决于输入的表示函数，这个函数可能是复杂的但固定的。因此，最佳的ICL算法首先将输入转化为表示函数的输出，然后在这些输出上进行线性ICL。我们证明了在某种程度上，存在可以近似实现这种算法的变换器，只需要较少的深度和大小。Empirically，我们发现训练后的变换器在这种设定下具有近乎最佳的ICL性能，并且展现出了预期的分割，下层层次将输入数据转化，而上层层次进行线性ICL。通过广泛的探索和一种新的粘贴实验，我们还发现了许多内部机制，如输入和表示的具体复制行为，上层层次的线性ICL能力，以及在更复杂的混合 Setting下的后ICL表示选择机制。这些观察到的机制与我们的理论相吻合，可能为ICL在更实际的情景中的研究提供了灵感。
</details></li>
</ul>
<hr>
<h2 id="IW-GAE-Importance-weighted-group-accuracy-estimation-for-improved-calibration-and-model-selection-in-unsupervised-domain-adaptation"><a href="#IW-GAE-Importance-weighted-group-accuracy-estimation-for-improved-calibration-and-model-selection-in-unsupervised-domain-adaptation" class="headerlink" title="IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation"></a>IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10611">http://arxiv.org/abs/2310.10611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taejong Joo, Diego Klabjan</li>
<li>for: 这篇论文旨在解决机器学习中的分布偏移问题，即在不具备标签的情况下，在分布偏移后的预测 зада务中保持高度的准确性。</li>
<li>methods: 该论文提出了一种重要性权重Weighted group accuracy estimator，通过提出一个优化问题，找到一个能够在分布偏移后的预测任务中准确地估计分组准确率的重要性权重。同时，该论文也进行了理论分析。</li>
<li>results: 经过广泛的实验，论文证明了该重要性权重Weighted group accuracy estimator的效果，可以帮助解决不supervised domain adaptation问题中的模型校准和模型选择问题。同时，该论文还提出了一种新的改进方向，即通过提高分组准确率来提高模型的转移率。<details>
<summary>Abstract</summary>
Reasoning about a model's accuracy on a test sample from its confidence is a central problem in machine learning, being connected to important applications such as uncertainty representation, model selection, and exploration. While these connections have been well-studied in the i.i.d. settings, distribution shifts pose significant challenges to the traditional methods. Therefore, model calibration and model selection remain challenging in the unsupervised domain adaptation problem--a scenario where the goal is to perform well in a distribution shifted domain without labels. In this work, we tackle difficulties coming from distribution shifts by developing a novel importance weighted group accuracy estimator. Specifically, we formulate an optimization problem for finding an importance weight that leads to an accurate group accuracy estimation in the distribution shifted domain with theoretical analyses. Extensive experiments show the effectiveness of group accuracy estimation on model calibration and model selection. Our results emphasize the significance of group accuracy estimation for addressing challenges in unsupervised domain adaptation, as an orthogonal improvement direction with improving transferability of accuracy.
</details>
<details>
<summary>摘要</summary>
machine learning 中，关于模型在测试样本上的准确性的推理是一个中心问题，与重要应用领域如不确定性表示、模型选择和探索相连。然而，在不同分布下 poses significant challenges to traditional methods。因此，模型准确性和模型选择在无supervised domain adaptation问题中仍然是挑战。在这种情况下，我们通过开发一种重要性加权组准精度估计器来解决分布shift的困难。specifically，我们提出了一个优化问题，找到一个导致在分布shifted domain中准确的组准精度估计器，并进行了理论分析。我们的实验表明，组准精度估计器对模型准确性和模型选择具有重要的作用，并且是一个对照 Transferability of accuracy的正交改进方向。
</details></li>
</ul>
<hr>
<h2 id="BayRnTune-Adaptive-Bayesian-Domain-Randomization-via-Strategic-Fine-tuning"><a href="#BayRnTune-Adaptive-Bayesian-Domain-Randomization-via-Strategic-Fine-tuning" class="headerlink" title="BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning"></a>BayRnTune: Adaptive Bayesian Domain Randomization via Strategic Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10606">http://arxiv.org/abs/2310.10606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianle Huang, Nitish Sontakke, K. Niranjan Kumar, Irfan Essa, Stefanos Nikolaidis, Dennis W. Hong, Sehoon Ha</li>
<li>for: 降低 sim2real 距离</li>
<li>methods: 自适应域随机化 + 精细调整</li>
<li>results: 比 vanilla DR 或 Bayesian DR 更高的奖励得分，同样的时间步数内Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to address the issue of careful tuning of randomization parameters in domain randomization (DR) methods, and to propose a new method called Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune) that can significantly accelerate the learning process by fine-tuning from previously learned policy.</li>
<li>methods: The proposed BayRnTune method inherits the spirit of Bayesian DR but with a key difference - it uses strategic fine-tuning of the previous policy to adapt to new environments. The method is evaluated in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments.</li>
<li>results: The results show that BayRnTune yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR. This suggests that the proposed method can significantly accelerate the learning process and improve the performance of DR in robotics.<details>
<summary>Abstract</summary>
Domain randomization (DR), which entails training a policy with randomized dynamics, has proven to be a simple yet effective algorithm for reducing the gap between simulation and the real world. However, DR often requires careful tuning of randomization parameters. Methods like Bayesian Domain Randomization (Bayesian DR) and Active Domain Randomization (Adaptive DR) address this issue by automating parameter range selection using real-world experience. While effective, these algorithms often require long computation time, as a new policy is trained from scratch every iteration. In this work, we propose Adaptive Bayesian Domain Randomization via Strategic Fine-tuning (BayRnTune), which inherits the spirit of BayRn but aims to significantly accelerate the learning processes by fine-tuning from previously learned policy. This idea leads to a critical question: which previous policy should we use as a prior during fine-tuning? We investigated four different fine-tuning strategies and compared them against baseline algorithms in five simulated environments, ranging from simple benchmark tasks to more complex legged robot environments. Our analysis demonstrates that our method yields better rewards in the same amount of timesteps compared to vanilla domain randomization or Bayesian DR.
</details>
<details>
<summary>摘要</summary>
域随机化（DR），即在训练策略时随机使用不同的动力学，已经证明是一种简单 yet effective的算法，可以减少实际世界和模拟之间的差距。然而，DR通常需要仔细调整随机参数。例如， bayesian domain randomization（Bayesian DR）和活动域随机化（Adaptive DR）可以自动选择随机参数的范围，使用实际世界经验。尽管有效，这些算法通常需要长时间的计算，因为每轮训练都需要从头开始训练一个新的策略。在这种情况下，我们提出了 adaptive bayesian domain randomization via strategic fine-tuning（BayRnTune），它继承了 BayRn 的精神，但是强调快速学习过程，通过对之前学习的策略进行细化来加速学习。这个想法引出了一个关键的问题：我们在细化过程中应该使用哪个先前学习的策略作为先前？我们 investigate了四种不同的细化策略，并与基线算法进行比较在五个模拟环境中，这些环境从简单的benchmark任务到更复杂的四肢机器人环境。我们的分析表明，我们的方法可以在同样的时间步骤内达到更高的奖励。
</details></li>
</ul>
<hr>
<h2 id="Pareto-Optimization-to-Accelerate-Multi-Objective-Virtual-Screening"><a href="#Pareto-Optimization-to-Accelerate-Multi-Objective-Virtual-Screening" class="headerlink" title="Pareto Optimization to Accelerate Multi-Objective Virtual Screening"></a>Pareto Optimization to Accelerate Multi-Objective Virtual Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10598">http://arxiv.org/abs/2310.10598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jenna C. Fromer, David E. Graff, Connor W. Coley</li>
<li>for: 本研究旨在透过多属性算法来快速找到具有强烈结合性、最小化副作用和适当的药物性特性的药物分子。</li>
<li>methods: 本研究使用多属性贝叶斯搜寻来减少虚拟实验成本，并运用这种方法在确定蛋白质和副标的对应的选择性抑制剂中找到适当的药物分子。</li>
<li>results: 本研究发现，使用多属性贝叶斯搜寻可以快速找到具有强烈结合性、最小化副作用和适当的药物性特性的药物分子，并且可以实现对虚拟实验中的药物分子库进行高效的搜寻和范畴化。<details>
<summary>Abstract</summary>
The discovery of therapeutic molecules is fundamentally a multi-objective optimization problem. One formulation of the problem is to identify molecules that simultaneously exhibit strong binding affinity for a target protein, minimal off-target interactions, and suitable pharmacokinetic properties. Inspired by prior work that uses active learning to accelerate the identification of strong binders, we implement multi-objective Bayesian optimization to reduce the computational cost of multi-property virtual screening and apply it to the identification of ligands predicted to be selective based on docking scores to on- and off-targets. We demonstrate the superiority of Pareto optimization over scalarization across three case studies. Further, we use the developed optimization tool to search a virtual library of over 4M molecules for those predicted to be selective dual inhibitors of EGFR and IGF1R, acquiring 100% of the molecules that form the library's Pareto front after exploring only 8% of the library. This workflow and associated open source software can reduce the screening burden of molecular design projects and is complementary to research aiming to improve the accuracy of binding predictions and other molecular properties.
</details>
<details>
<summary>摘要</summary>
发现治疗分子是一个多目标优化问题的基本问题。一种形ulation的问题是通过同时具有高绑定亲和力、最小的偶折受影响和合适的药物生物学性 Properties 来认定分子。取得了先前工作使用活动学习加速绑定分子的识别的灵感，我们实现了多属性权重优化来降低虚拟屏选中计算成本，并应用于预测绑定分子的药物设计中。我们在三个案例中证明了对比权重优化的优势，并使用开发的优化工具来搜索虚拟库中的可选性双抑制剂。通过探索虚拟库的8% only，我们收获了虚拟库的极值 front 上的100%分子。这种工作流和相关的开源软件可以减轻分子设计项目的屏选负担，并且与尝试提高绑定预测和其他分子性质的研究相 complementary。
</details></li>
</ul>
<hr>
<h2 id="HelmSim-Learning-Helmholtz-Dynamics-for-Interpretable-Fluid-Simulation"><a href="#HelmSim-Learning-Helmholtz-Dynamics-for-Interpretable-Fluid-Simulation" class="headerlink" title="HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation"></a>HelmSim: Learning Helmholtz Dynamics for Interpretable Fluid Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10565">http://arxiv.org/abs/2310.10565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lanxiang Xing, Haixu Wu, Yuezhou Ma, Jianmin Wang, Mingsheng Long</li>
<li>for: 这个论文旨在提出一种准确且可解释的流体模拟器，即HelmSim，以解决流体动力学的长期挑战。</li>
<li>methods: 该论文提出了一种基于Helmholtz定理的HelmDynamic块，该块将流体动力学分解为更容易解决的curl-free和divergence-free部分，物理相应于流体的潜potential和流体流函数。该块被 embedding到一个多尺度 интеграцион网络中，以 интеGRATE temporal维度上的多个空间尺度的Helmholtz动力学。</li>
<li>results: 对比previoius velocity estimating方法，HelmSim具有 faithful derived from Helmholtz theorem和Physically interpretable evidence，并在 numerically simulated和实际观测的标准准样中实现了一致的state-of-the-art表现，即使在复杂的边界条件下。<details>
<summary>Abstract</summary>
Fluid simulation is a long-standing challenge due to the intrinsic high-dimensional non-linear dynamics. Previous methods usually utilize the non-linear modeling capability of deep models to directly estimate velocity fields for future prediction. However, skipping over inherent physical properties but directly learning superficial velocity fields will overwhelm the model from generating precise or physics-reliable results. In this paper, we propose the HelmSim toward an accurate and interpretable simulator for fluid. Inspired by the Helmholtz theorem, we design a HelmDynamic block to learn the Helmholtz dynamics, which decomposes fluid dynamics into more solvable curl-free and divergence-free parts, physically corresponding to potential and stream functions of fluid. By embedding the HelmDynamic block into a Multiscale Integration Network, HelmSim can integrate learned Helmholtz dynamics along temporal dimension in multiple spatial scales to yield future fluid. Comparing with previous velocity estimating methods, HelmSim is faithfully derived from Helmholtz theorem and ravels out complex fluid dynamics with physically interpretable evidence. Experimentally, our proposed HelmSim achieves the consistent state-of-the-art in both numerical simulated and real-world observed benchmarks, even for scenarios with complex boundaries.
</details>
<details>
<summary>摘要</summary>
fluid 模拟是一个长期的挑战，因为它的自然高维非线性动力学性。以前的方法通常使用深度模型的非线性建模能力直接估算未来的速度场，但是跳过了内在物理属性，直接学习 superficies 的速度场将会让模型生成精度不高或者物理可靠的结果。在这篇论文中，我们提出了 HelmSim，一种准确和可解释的流体模拟器。受 helmholtz 定理启发，我们设计了 HelmDynamic 块，用于学习 helmholtz 动力学，它将流体动力学分解为更可解决的 curl-free 和 divergence-free 部分，物理相应于流体的潜在函数和流函数。通过在多尺度练习网络中嵌入 HelmDynamic 块，HelmSim 可以在多个空间尺度上将学习的 helmholtz 动力学集成到时间维度上，以生成未来的流体。相比之前的速度估计方法，HelmSim 是准确地从 helmholtz 定理中派生出来，并且可以揭示出复杂的流体动力学性，并且具有物理可解的证据。实验表明，我们提出的 HelmSim 在数值 simulate 和实际观测的标准准确，即使场景具有复杂的边界。
</details></li>
</ul>
<hr>
<h2 id="Causal-Dynamic-Variational-Autoencoder-for-Counterfactual-Regression-in-Longitudinal-Data"><a href="#Causal-Dynamic-Variational-Autoencoder-for-Counterfactual-Regression-in-Longitudinal-Data" class="headerlink" title="Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data"></a>Causal Dynamic Variational Autoencoder for Counterfactual Regression in Longitudinal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10559">http://arxiv.org/abs/2310.10559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Cournède</li>
<li>for: 这篇论文的目的是用来估计治疗效果的变化趋势，特别是在精准医学、epidemiology、经济和市场营销等领域。</li>
<li>methods: 这篇论文使用了一种新的方法，即假设存在不观察到的风险因素（也称为调整变量），这些风险因素只影响短期内的结果。</li>
<li>results: 论文的实验结果显示，这种新方法可以准确地估计个体治疗效果，并能够捕捉长期内治疗响应中的不观察到风险因素。<details>
<summary>Abstract</summary>
Estimating treatment effects over time is relevant in many real-world applications, such as precision medicine, epidemiology, economy, and marketing. Many state-of-the-art methods either assume the observations of all confounders or seek to infer the unobserved ones. We take a different perspective by assuming unobserved risk factors, i.e., adjustment variables that affect only the sequence of outcomes. Under unconfoundedness, we target the Individual Treatment Effect (ITE) estimation with unobserved heterogeneity in the treatment response due to missing risk factors. We address the challenges posed by time-varying effects and unobserved adjustment variables. Led by theoretical results over the validity of the learned adjustment variables and generalization bounds over the treatment effect, we devise Causal DVAE (CDVAE). This model combines a Dynamic Variational Autoencoder (DVAE) framework with a weighting strategy using propensity scores to estimate counterfactual responses. The CDVAE model allows for accurate estimation of ITE and captures the underlying heterogeneity in longitudinal data. Evaluations of our model show superior performance over state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
在许多实际应用中，如精准医学、 Epidemiology、经济和市场营销中，估计治疗效果的演化是非常重要的。许多现代方法都是假设所有干扰因素的观察，或者尝试推断未观察到的干扰因素。我们采取了一种不同的视角，假设存在未观察到的风险因素，即调整变量，这些变量只影响结果序列。在干扰性下，我们target个人治疗效果（ITE）估计，带有未观察到的多变性。我们解决了时间变化的效果和未观察到的调整变量的挑战。通过理论结果的有效性和权重分配策略使用可能性分数来估计对应响应，我们设计了 causal DVAE（CDVAE）模型。这个模型结合了动态变量自动编码器（DVAE）框架和一种利用可能性分数进行权重分配的策略，以估计对应响应。 CDVAE 模型允许精准地估计 ITE，并捕捉了长期数据中的下降多变性。我们对我们的模型进行评估，并证明它们在现有模型中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Preference-Based-Nonparametric-Off-Policy-Evaluation-with-Deep-Networks"><a href="#Sample-Complexity-of-Preference-Based-Nonparametric-Off-Policy-Evaluation-with-Deep-Networks" class="headerlink" title="Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks"></a>Sample Complexity of Preference-Based Nonparametric Off-Policy Evaluation with Deep Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10556">http://arxiv.org/abs/2310.10556</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Li, Xiang Ji, Minshuo Chen, Mengdi Wang</li>
<li>for:  solves reinforcement learning problems with human preference data</li>
<li>methods:  uses actor-critic methods and fitted-Q-evaluation with a deep neural network</li>
<li>results:  establishes a sample-efficient estimator for off-policy evaluation with high reward smoothness, and almost aligns with classical OPE results with observable reward data.<details>
<summary>Abstract</summary>
A recently popular approach to solving reinforcement learning is with data from human preferences. In fact, human preference data are now used with classic reinforcement learning algorithms such as actor-critic methods, which involve evaluating an intermediate policy over a reward learned from human preference data with distribution shift, known as off-policy evaluation (OPE). Such algorithm includes (i) learning reward function from human preference dataset, and (ii) learning expected cumulative reward of a target policy. Despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any low-dimensional manifold structure in the Markov decision process and obtain a sample-efficient estimator without suffering from the curse of high data ambient dimensionality. Under the assumption of high reward smoothness, our results \textit{almost align with the classical OPE results with observable reward data}. To the best of our knowledge, this is the first result that establishes a \textit{provably efficient} guarantee for off-policy evaluation with RLHF.
</details>
<details>
<summary>摘要</summary>
一种最近受欢迎的解决方案是使用人类偏好数据来解决强化学习问题。实际上，人类偏好数据现在与 классиical 强化学习算法（如actor-critic方法）结合使用，这些算法包括（i）从人类偏好数据集中学习奖励函数，和（ii）使用learned reward的Off-Policy Evaluation（OPE）来评估目标策略的预期总奖励。 despite the huge empirical success, existing OPE methods with preference data often lack theoretical understanding and rely heavily on heuristics. In this paper, we study the sample efficiency of OPE with human preference and establish a statistical guarantee for it. Specifically, we approach OPE by learning the value function by fitted-Q-evaluation with a deep neural network. By appropriately selecting the size of a ReLU network, we show that one can leverage any low-dimensional manifold structure in the Markov decision process and obtain a sample-efficient estimator without suffering from the curse of high data ambient dimensionality. Under the assumption of high reward smoothness, our results almost align with the classical OPE results with observable reward data. To the best of our knowledge, this is the first result that establishes a provably efficient guarantee for off-policy evaluation with RLHF.
</details></li>
</ul>
<hr>
<h2 id="Population-based-wind-farm-monitoring-based-on-a-spatial-autoregressive-approach"><a href="#Population-based-wind-farm-monitoring-based-on-a-spatial-autoregressive-approach" class="headerlink" title="Population-based wind farm monitoring based on a spatial autoregressive approach"></a>Population-based wind farm monitoring based on a spatial autoregressive approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10555">http://arxiv.org/abs/2310.10555</a></li>
<li>repo_url: None</li>
<li>paper_authors: W. Lin, K. Worden, E. J. Cross</li>
<li>for: 降低风力电站运行和维护成本</li>
<li>methods: 使用人口基于的结构健康监测系统，并利用多个结构（i.e.~风机）共享数据来提高结构行为预测</li>
<li>results: 提出了一种基于 Gaussian process 的空间自回归模型（GP-SPARX 模型），可以正确捕捉风机群的空间和时间相关性，并且可以用于健康监测系统的实现。<details>
<summary>Abstract</summary>
An important challenge faced by wind farm operators is to reduce operation and maintenance cost. Structural health monitoring provides a means of cost reduction through minimising unnecessary maintenance trips as well as prolonging turbine service life. Population-based structural health monitoring can further reduce the cost of health monitoring systems by implementing one system for multiple structures (i.e.~turbines). At the same time, shared data within a population of structures may improve the predictions of structural behaviour. To monitor turbine performance at a population/farm level, an important initial step is to construct a model that describes the behaviour of all turbines under normal conditions. This paper proposes a population-level model that explicitly captures the spatial and temporal correlations (between turbines) induced by the wake effect. The proposed model is a Gaussian process-based spatial autoregressive model, named here a GP-SPARX model. This approach is developed since (a) it reflects our physical understanding of the wake effect, and (b) it benefits from a stochastic data-based learner. A case study is provided to demonstrate the capability of the GP-SPARX model in capturing spatial and temporal variations as well as its potential applicability in a health monitoring system.
</details>
<details>
<summary>摘要</summary>
operator of wind farms 面临一个重要挑战是减少运营和维护成本。人口基本的结构健康监测可以通过最小化无必要的维护旅行以及提高机顺服务寿命，从而减少健康监测系统的成本。同时，在多个结构之间共享数据可以提高结构行为预测的准确性。为监测风 турbin的性能，一个重要的初始步骤是建立一个描述所有风 турbin在正常情况下行为的模型。这篇文章提出了一种人口级别的模型，该模型由 Gaussian 过程基本的空间自相关模型（GP-SPARX）组成，这种方法因其体现了风阻效应的物理理解，同时受益于数据驱动的随机学习。一个案例研究证明了 GP-SPARX 模型能够 capture 空间和时间变化，以及其可能应用于健康监测系统。
</details></li>
</ul>
<hr>
<h2 id="TacticAI-an-AI-assistant-for-football-tactics"><a href="#TacticAI-an-AI-assistant-for-football-tactics" class="headerlink" title="TacticAI: an AI assistant for football tactics"></a>TacticAI: an AI assistant for football tactics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10553">http://arxiv.org/abs/2310.10553</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Wang, Petar Veličković, Daniel Hennes, Nenad Tomašev, Laurel Prince, Michael Kaisers, Yoram Bachrach, Romuald Elie, Li Kevin Wenliang, Federico Piccinini, William Spearman, Ian Graham, Jerome Connor, Yi Yang, Adrià Recasens, Mina Khan, Nathalie Beauguerlange, Pablo Sprechmann, Pol Moreno, Nicolas Heess, Michael Bowling, Demis Hassabis, Karl Tuyls</li>
<li>for: 这篇论文是为了开发一种基于人工智能的足球战术助手（TacticAI），帮助教练分析对手队伍的战术模式，并提供有效的回应策略。</li>
<li>methods: 这篇论文使用了预测和生成两部分的算法，允许教练通过样本和探索不同的球员布局来评估不同的角球模式，并选择最有可能性 succeed 的设置。</li>
<li>results: 研究人员通过对一些有关的 benchmark task 进行验证，证明 TacticAI 的模型建议不仅与实际战术无法分辨，而且在 90% 的时间上超过现有战术。 另外，TacticAI 还提供了一个有效的角球检索系统。<details>
<summary>Abstract</summary>
Identifying key patterns of tactics implemented by rival teams, and developing effective responses, lies at the heart of modern football. However, doing so algorithmically remains an open research challenge. To address this unmet need, we propose TacticAI, an AI football tactics assistant developed and evaluated in close collaboration with domain experts from Liverpool FC. We focus on analysing corner kicks, as they offer coaches the most direct opportunities for interventions and improvements. TacticAI incorporates both a predictive and a generative component, allowing the coaches to effectively sample and explore alternative player setups for each corner kick routine and to select those with the highest predicted likelihood of success. We validate TacticAI on a number of relevant benchmark tasks: predicting receivers and shot attempts and recommending player position adjustments. The utility of TacticAI is validated by a qualitative study conducted with football domain experts at Liverpool FC. We show that TacticAI's model suggestions are not only indistinguishable from real tactics, but also favoured over existing tactics 90% of the time, and that TacticAI offers an effective corner kick retrieval system. TacticAI achieves these results despite the limited availability of gold-standard data, achieving data efficiency through geometric deep learning.
</details>
<details>
<summary>摘要</summary>
现代足球中，认识对手队伍实施的战术模式，并开发有效应对策略，是核心问题。然而，这种算法化研究仍然是一个开放的研究挑战。为解决这个需求，我们提出了TacticAI，一个基于人工智能的足球战术助手。我们与足球领域专家合作开发并评估了TacticAI，专注于分析角球机会，因为这些机会提供了教练最直接的改进和优化机会。TacticAI包含预测和生成两个组成部分，允许教练通过采样和探索不同的玩家设置来寻找最有可能成功的角球机会。我们在多个相关的 bencmark任务上验证了TacticAI：预测接收者和射击尝试，并建议玩家位置调整。我们通过对足球领域专家进行质量调研，证明TacticAI的模型建议与实际战术无法分辨，并且90%的时间 prefer TacticAI的建议。此外，TacticAI还提供了有效的角球检索系统。TacticAI达到了这些结果，尽管数据的可用性受限，通过几何深度学习实现了数据效率。
</details></li>
</ul>
<hr>
<h2 id="Optimal-vintage-factor-analysis-with-deflation-varimax"><a href="#Optimal-vintage-factor-analysis-with-deflation-varimax" class="headerlink" title="Optimal vintage factor analysis with deflation varimax"></a>Optimal vintage factor analysis with deflation varimax</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10545">http://arxiv.org/abs/2310.10545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Bing, Dian Jin, Yuqian Zhang</li>
<li>for: This paper proposes a new method for vintage factor analysis, which aims to find a low-dimensional representation of the original data and then seek a rotation that is scientifically meaningful.</li>
<li>methods: The proposed method uses a deflation varimax procedure that solves each row of an orthogonal matrix sequentially, which has a net computational gain and flexibility.</li>
<li>results: The proposed method is able to fully establish theoretical guarantees for the proposed procedure in a broad context, and it is shown to be optimal in all SNR regimes. Additionally, the method is valid for finite sample and allows the number of the latent factors to grow with the sample size.Here is the Chinese translation of the three points:</li>
<li>for: 这个论文提出了一种新的维度分析方法，目的是找到原始数据的低维度表示，然后寻找科学意义的旋转。</li>
<li>methods: 该方法使用了一种减法varimax过程，解决每个正交矩阵的每行问题，具有计算效益和灵活性。</li>
<li>results: 该方法能够在广泛的情况下提供完整的理论保证，并且在所有的噪声范围内都是优化的。此外，方法是有限样本的有效性和因子数量可以随样本大小和维度的增长而增长。<details>
<summary>Abstract</summary>
Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. Perhaps the most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.   In this paper, we propose a deflation varimax procedure that solves each row of an orthogonal matrix sequentially. In addition to its net computational gain and flexibility, we are able to fully establish theoretical guarantees for the proposed procedure in a broad context.   Adopting this new varimax approach as the second step after PCA, we further analyze this two step procedure under a general class of factor models. Our results show that it estimates the factor loading matrix in the optimal rate when the signal-to-noise-ratio (SNR) is moderate or large. In the low SNR regime, we offer possible improvement over using PCA and the deflation procedure when the additive noise under the factor model is structured. The modified procedure is shown to be optimal in all SNR regimes. Our theory is valid for finite sample and allows the number of the latent factors to grow with the sample size as well as the ambient dimension to grow with, or even exceed, the sample size.   Extensive simulation and real data analysis further corroborate our theoretical findings.
</details>
<details>
<summary>摘要</summary>
古典因素分析是一种重要的因素分析方法，旨在首先找到原始数据的低维度表示，然后寻找一种可靠的旋转，使得旋转后的低维度表示具有科学意义。最广泛使用的古典因素分析方法是主Component分析（PCA）followed by varimax旋转。尽管它受欢迎，但是可以提供的理论保证很少，因为varimax旋转需要解决非核心化优化问题。  在这篇论文中，我们提出了一种减少varimax过程中的计算量和灵活性的方法，并且可以在广泛的Context下提供完整的理论保证。我们采用这种新的varimax方法作为PCA之后的第二步，然后对这两步进程进行了广泛的分析。我们的结果表明，这种两步过程在中等或大的信号噪声比（SNR）下能够优化因子加载矩阵。在噪声比较低的情况下，我们提供了可能的改进方案，其中添加的噪声在因子模型下是结构化的。我们的修改过程在所有SNR régime下是优化的。我们的理论是有限样本和因子数量可以随样本大小和环境维度增长。我们的实验和实际数据分析进一步证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Comparators-in-Generalization-Bounds"><a href="#Comparing-Comparators-in-Generalization-Bounds" class="headerlink" title="Comparing Comparators in Generalization Bounds"></a>Comparing Comparators in Generalization Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10534">http://arxiv.org/abs/2310.10534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fredrik Hellström, Benjamin Guedj</li>
<li>for: 本研究的目的是提出一种基于信息理论和PAC-搜索概率的通用泛化约束，用于评估机器学习模型的泛化性能。</li>
<li>methods: 本文使用了信息理论和PAC-搜索概率来 derive一些泛化约束，并证明了这些约束的优化性。</li>
<li>results: 本文的研究结果表明，使用这些泛化约束可以获得更加优化的泛化性能，并且可以在不同的维度上进行泛化。<details>
<summary>Abstract</summary>
We derive generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training and population loss. The bounds hold under the assumption that the cumulant-generating function (CGF) of the comparator is upper-bounded by the corresponding CGF within a family of bounding distributions. We show that the tightest possible bound is obtained with the comparator being the convex conjugate of the CGF of the bounding distribution, also known as the Cram\'er function. This conclusion applies more broadly to generalization bounds with a similar structure. This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses and leads to novel bounds under other bounding distributions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-optimal-integration-of-spatial-and-temporal-information-in-noisy-chemotaxis"><a href="#Learning-optimal-integration-of-spatial-and-temporal-information-in-noisy-chemotaxis" class="headerlink" title="Learning optimal integration of spatial and temporal information in noisy chemotaxis"></a>Learning optimal integration of spatial and temporal information in noisy chemotaxis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10531">http://arxiv.org/abs/2310.10531</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kirkegaardlab/chemoxrl">https://github.com/kirkegaardlab/chemoxrl</a></li>
<li>paper_authors: Albert Alonso, Julius B. Kirkegaard</li>
<li>for: 研究 chemotaxis 驱动 by spatial 和 temporal 估计的边界</li>
<li>methods: 使用 deep reinforcement learning 研究可以在不受限制的方式集成 spatial 和 temporal 信息</li>
<li>results: 发现 transition between regimes 是连续的，combined strategy 在过渡区域表现更好，并且 policy 听取的 gradient 信息是非rivial的组合。Is there anything else I can help with?<details>
<summary>Abstract</summary>
We investigate the boundary between chemotaxis driven by spatial estimation of gradients and chemotaxis driven by temporal estimation. While it is well known that spatial chemotaxis becomes disadvantageous for small organisms at high noise levels, it is unclear whether there is a discontinuous switch of optimal strategies or a continuous transition exists. Here, we employ deep reinforcement learning to study the possible integration of spatial and temporal information in an a priori unconstrained manner. We parameterize such a combined chemotactic policy by a recurrent neural network and evaluate it using a minimal theoretical model of a chemotactic cell. By comparing with constrained variants of the policy, we show that it converges to purely temporal and spatial strategies at small and large cell sizes, respectively. We find that the transition between the regimes is continuous, with the combined strategy outperforming in the transition region both the constrained variants as well as models that explicitly integrate spatial and temporal information. Finally, by utilizing the attribution method of integrated gradients, we show that the policy relies on a non-trivial combination of spatially and temporally derived gradient information in a ratio that varies dynamically during the chemotactic trajectories.
</details>
<details>
<summary>摘要</summary>
Translation into Simplified Chinese:我们研究 chemotaxis 驱动 by 空间估计 gradient 和 temporal 估计之间的边界。然而，已知的是，随着噪声水平的提高，小organism 中的 spatial chemotaxis 变得不利。但是，是否存在突然的优化策略转换，或者是一个连续的转换，这还未得到了解。我们使用深度学习来研究可能的空间和时间信息的集成，不受任何假设或限制。我们使用 recurrent neural network 来参数化这种合并的 chemotactic 政策，并使用一个最小的化学吸引细胞模型来评估它。我们比较了这种政策与受限的变种，发现它在小和大细胞尺度之间分别转换为纯 temporal 和空间策略，并且在这两个策略之间存在一个连续的转换。此外，我们还发现，在转换区域，合并策略比受限变种和显式地集成空间和时间信息的模型都更高效。最后，我们使用 integrated gradients 的归因方法，发现政策在化学追踪过程中动态变化的空间和时间信息的权重组合是非常复杂的。
</details></li>
</ul>
<hr>
<h2 id="From-Spectral-Theorem-to-Statistical-Independence-with-Application-to-System-Identification"><a href="#From-Spectral-Theorem-to-Statistical-Independence-with-Application-to-System-Identification" class="headerlink" title="From Spectral Theorem to Statistical Independence with Application to System Identification"></a>From Spectral Theorem to Statistical Independence with Application to System Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10523">http://arxiv.org/abs/2310.10523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Abdullah Naeem, Amir Khazraei, Miroslav Pajic</li>
<li>for: 这个论文是关于高维Random Dynamical Systems的研究，具体来说是研究这些系统的identification问题。</li>
<li>methods: 作者使用spectral theorem for non-Hermitian operators来研究系统的特征向量，并通过分析eigenvalues和eigenvectors来描述系统的特性。</li>
<li>results: 作者发现，当系统是稳定的时，系统的特征向量可以分解为多个lower dimensional Random Dynamical Systems，这些系统之间是独立的。此外，作者还发现，在这种情况下，covariates可能会受到维度的干扰，导致error的增加。<details>
<summary>Abstract</summary>
High dimensional random dynamical systems are ubiquitous, including -- but not limited to -- cyber-physical systems, daily return on different stocks of S&P 1500 and velocity profile of interacting particle systems around McKeanVlasov limit. Mathematically, underlying phenomenon can be captured via a stable $n$-dimensional linear transformation `$A$' and additive randomness. System identification aims at extracting useful information about underlying dynamical system, given a length $N$ trajectory from it (corresponds to an $n \times N$ dimensional data matrix). We use spectral theorem for non-Hermitian operators to show that spatio-temperal correlations are dictated by the discrepancy between algebraic and geometric multiplicity of distinct eigenvalues corresponding to state transition matrix. Small discrepancies imply that original trajectory essentially comprises of multiple lower dimensional random dynamical systems living on $A$ invariant subspaces and are statistically independent of each other. In the process, we provide first quantitative handle on decay rate of finite powers of state transition matrix $\|A^{k}\|$ . It is shown that when a stable dynamical system has only one distinct eigenvalue and discrepancy of $n-1$: $\|A\|$ has a dependence on $n$, resulting dynamics are spatially inseparable and consequently there exist at least one row with covariates of typical size $\Theta\big(\sqrt{N-n+1}$ $e^{n}\big)$ i.e., even under stability assumption, covariates can suffer from curse of dimensionality. In the light of these findings we set the stage for non-asymptotic error analysis in estimation of state transition matrix $A$ via least squares regression on observed trajectory by showing that element-wise error is essentially a variant of well-know Littlewood-Offord problem.
</details>
<details>
<summary>摘要</summary>
高维Random动力系统广泛存在，包括但不限于Cyber-Physical Systems、每天不同股票S&P 1500的回报和Interacting Particle Systems around McKeanVlasov limit的速度 Profile。数学上，下面的现象可以通过一个稳定的$n$-维线性变换'$A$'和随机性来捕捉。系统识别目标是从这个系统中提取有用的信息，了解下面的动力系统。我们使用非 hermitian 算子的特征定理来证明，在空间-时间 correlations 中，存在一些独特的多个低维Random dynamical systems 在 $A$  invariable subspaces 中生活，这些系统是独立的。在这个过程中，我们提供了第一个量化的把握，以及 $\|A^{k}\|$ 的衰减率。当一个稳定的动力系统只有一个独特的征值，并且差值为 $n-1$，则 $\|A\|$ 具有对 $n$ 的依赖关系，结果的动力系统是无法分离的。因此，存在至少一行具有特点大小 $\Theta\big(\sqrt{N-n+1}$ $e^{n}\big)$ 的covariates，即，even under stability assumption，covariates 可能会受到维度约束。在这些发现的基础上，我们设置了非对数学术的错误分析在 $A$ 的最小二乘回归中，并证明了元素级别的错误是一种变种的 Littlewood-Offord 问题。
</details></li>
</ul>
<hr>
<h2 id="Reproducing-Bayesian-Posterior-Distributions-for-Exoplanet-Atmospheric-Parameter-Retrievals-with-a-Machine-Learning-Surrogate-Model"><a href="#Reproducing-Bayesian-Posterior-Distributions-for-Exoplanet-Atmospheric-Parameter-Retrievals-with-a-Machine-Learning-Surrogate-Model" class="headerlink" title="Reproducing Bayesian Posterior Distributions for Exoplanet Atmospheric Parameter Retrievals with a Machine Learning Surrogate Model"></a>Reproducing Bayesian Posterior Distributions for Exoplanet Atmospheric Parameter Retrievals with a Machine Learning Surrogate Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10521">http://arxiv.org/abs/2310.10521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyup B. Unlu, Roy T. Forestano, Konstantin T. Matchev, Katia Matcheva</li>
<li>for: 这个论文是为了实现基于机器学习的 posterior 分布模型，用于重现通过掩蔽行星的谱 spectra 获得的外层星球大气参数的 Bayesian  posterior distributions。</li>
<li>methods: 该模型使用了适应性学习和半监督学习，以便利用大量的无标注训练数据。它还进行了领域适应的特征处理，以提高模型性能。</li>
<li>results: 该模型在2023年 Ariel 机器学习数据挑战中获得了优胜解决方案。<details>
<summary>Abstract</summary>
We describe a machine-learning-based surrogate model for reproducing the Bayesian posterior distributions for exoplanet atmospheric parameters derived from transmission spectra of transiting planets with typical retrieval software such as TauRex. The model is trained on ground truth distributions for seven parameters: the planet radius, the atmospheric temperature, and the mixing ratios for five common absorbers: $H_2O$, $CH_4$, $NH_3$, $CO$ and $CO_2$. The model performance is enhanced by domain-inspired preprocessing of the features and the use of semi-supervised learning in order to leverage the large amount of unlabelled training data available. The model was among the winning solutions in the 2023 Ariel Machine Learning Data Challenge.
</details>
<details>
<summary>摘要</summary>
我们描述了一种基于机器学习的代理模型，用于重现吸收 спектроскопии中探测到的外层星球大气参数的 bayesian posterior distribution。该模型使用了常用的恢复软件 such as TauRex，并在七个参数上进行了训练：星球半径、大气温度以及五种常见吸收物的混合率：$H_2O$, $CH_4$, $NH_3$, $CO$ 和 $CO_2$。通过域名预处理和使用半监督学习，我们提高了模型的性能，并利用了大量的无标注训练数据。该模型在2023年的Ariel机器学习数据挑战中获得了奖励。
</details></li>
</ul>
<hr>
<h2 id="ReMax-A-Simple-Effective-and-Efficient-Reinforcement-Learning-Method-for-Aligning-Large-Language-Models"><a href="#ReMax-A-Simple-Effective-and-Efficient-Reinforcement-Learning-Method-for-Aligning-Large-Language-Models" class="headerlink" title="ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models"></a>ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10505">http://arxiv.org/abs/2310.10505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liziniu/ReMax">https://github.com/liziniu/ReMax</a></li>
<li>paper_authors: Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo</li>
<li>for: 本研究旨在提高RLHF任务中的训练效率，并解决PPO算法的计算效率问题。</li>
<li>methods: 本研究提出了一种新的RLHF算法 called ReMax，基于REINFORCE算法，并具有一种新的减少方差技术。</li>
<li>results: ReMax比PPO具有三大优点：首先，ReMax简单实现，消除了多个超参数，减少了训练时间和精度优化的努力。其次，ReMax减少了50%的内存使用量，可以在8xA100-40GB GPU上训练Llama2（7B）模型。最后，ReMax比PPO快2倍，不降低性能。<details>
<summary>Abstract</summary>
Alignment is of critical importance for training large language models (LLMs). The predominant strategy to address this is through Reinforcement Learning from Human Feedback (RLHF), where PPO serves as the de-facto algorithm. Yet, PPO is known to suffer from computational inefficiency, which is a challenge that this paper aims to address. We identify three important properties in RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on such observations, we develop a new algorithm tailored for RLHF, called ReMax. The algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is equipped with a new variance-reduction technique.   Our method has three-fold advantages over PPO: first, ReMax is simple to implement and removes many hyper-parameters in PPO, which are scale-sensitive and laborious to tune. Second, ReMax saves about 50% memory usage in principle. As a result, PPO runs out-of-memory when fine-tuning a Llama2 (7B) model on 8xA100-40GB GPUs, whereas ReMax can afford training. This memory improvement is achieved by removing the value model in PPO. Third, based on our calculations, we find that even assuming PPO can afford the training of Llama2 (7B), it would still run about 2x slower than ReMax. This is due to the computational overhead of the value model, which does not exist in ReMax. Importantly, the above computational improvements do not sacrifice the performance. We hypothesize these advantages can be maintained in larger-scaled models. Our implementation of ReMax is available at https://github.com/liziniu/ReMax
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified Chinese<</SYS>>大量语言模型（LLM）的训练需要Alignment是关键。现有的主流策略是通过人类反馈学习（RLHF），其中PPO serves as the de-facto algorithm。然而，PPO知道 suffer from computational inefficiency，这是这篇文章的目标。我们确定了RLHF任务中的三个重要特性：快速的模拟，决定性的转移和轨迹级别的奖励，这些特性在PPO中未被利用。基于这些观察，我们开发了一种适合RLHF的新算法，called ReMax。ReMax的算法设计基于celebrated algorithm REINFORCE，但具有一种新的减少偏移技术。  我们的方法有三个优势：首先，ReMax简单实现，消除了PPO中许多参数，这些参数是敏感度和耗时consuming。其次，ReMax将减少约50%的内存使用量。这使得PPO在精度级别的模型（7B）上的8xA100-40GB GPU上进行精度级别的模型（7B）上进行精度级别的训练时出现内存不足问题，而ReMax可以进行训练。这种内存改进是通过 removing the value model in PPO 来实现的。第三，基于我们的计算，即使PPO可以训练Llama2（7B），它仍然会比ReMax约2倍 slower。这是因为值模型在PPO中的计算 overhead，不存在在ReMax中。重要的是，上述计算改进不会减少性能。我们认为这些优势可以在更大的模型 scale 中被维持。我们的 ReMax 实现可以在 <https://github.com/liziniu/ReMax> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-Learning-Patterns-in-Financial-Time-Series-for-Trend-Following-Strategies"><a href="#Few-Shot-Learning-Patterns-in-Financial-Time-Series-for-Trend-Following-Strategies" class="headerlink" title="Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies"></a>Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10500">http://arxiv.org/abs/2310.10500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kieran Wood, Samuel Kessler, Stephen J. Roberts, Stefan Zohren</li>
<li>for: 这个论文是为了提出一种能够快速适应金融市场变化的时间序列趋势预测模型，以避免在金融市场突然变化时出现的损失。</li>
<li>methods: 该模型使用了深度学习的最新进展，特别是几拟学习，以及时间序列趋势预测模型。</li>
<li>results: 该模型在2018-2023年的紧张市场期间，相比 neural forecaster 和时间序列势力策略，提高了18.9%的谭瑞比，并且在 COVID-19 下落期间， doubles 快速恢复。此外，该模型还可以对新的金融资产进行零 shot 位置，并且与 neural time-series trend forecaster 相比，在同一时间期内提高了5倍的谭瑞比。<details>
<summary>Abstract</summary>
Forecasting models for systematic trading strategies do not adapt quickly when financial market conditions change, as was seen in the advent of the COVID-19 pandemic in 2020, when market conditions changed dramatically causing many forecasting models to take loss-making positions. To deal with such situations, we propose a novel time-series trend-following forecaster that is able to quickly adapt to new market conditions, referred to as regimes. We leverage recent developments from the deep learning community and use few-shot learning. We propose the Cross Attentive Time-Series Trend Network - X-Trend - which takes positions attending over a context set of financial time-series regimes. X-Trend transfers trends from similar patterns in the context set to make predictions and take positions for a new distinct target regime. X-Trend is able to quickly adapt to new financial regimes with a Sharpe ratio increase of 18.9% over a neural forecaster and 10-fold over a conventional Time-series Momentum strategy during the turbulent market period from 2018 to 2023. Our strategy recovers twice as quickly from the COVID-19 drawdown compared to the neural-forecaster. X-Trend can also take zero-shot positions on novel unseen financial assets obtaining a 5-fold Sharpe ratio increase versus a neural time-series trend forecaster over the same period. X-Trend both forecasts next-day prices and outputs a trading signal. Furthermore, the cross-attention mechanism allows us to interpret the relationship between forecasts and patterns in the context set.
</details>
<details>
<summary>摘要</summary>
预测模型 для系统性交易策略不快适应金融市场条件变化，例如2020年COVID-19大流行期间，市场条件快速变化，许多预测模型亏损。为解决这种情况，我们提出了一种新的时间序列趋势预测器，可以快速适应新的市场条件，称为“ régime”。我们利用了最新的深度学习社区的进展，并使用几何学学习。我们提出了跨注意力时间序列趋势网络（X-Trend），它在一个上下文集中注意力分配位置，并将趋势从类似的模式传递到新目标 régime 中进行预测和交易。X-Trend 能快速适应新的金融 régime，其肖特比（Sharpe ratio）提高18.9%于神经预测器和10倍于传统时间序列势力策略在2018-2023年的混乱市场期间。我们的策略在COVID-19下滑期间复制两倍于神经预测器。X-Trend 还可以在未看到的金融资产上出现零shot位置，其肖特比提高5倍于神经时间序列趋势预测器在同一时间期。X-Trend 同时预测下一天的价格和输出交易信号。此外，跨注意力机制允许我们解释预测和上下文集中的模式之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Passive-Inference-Attacks-on-Split-Learning-via-Adversarial-Regularization"><a href="#Passive-Inference-Attacks-on-Split-Learning-via-Adversarial-Regularization" class="headerlink" title="Passive Inference Attacks on Split Learning via Adversarial Regularization"></a>Passive Inference Attacks on Split Learning via Adversarial Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10483">http://arxiv.org/abs/2310.10483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaochen Zhu, Xinjian Luo, Yuncheng Wu, Yangfan Jiang, Xiaokui Xiao, Beng Chin Ooi</li>
<li>for: 这个研究旨在攻击 Split Learning (SL) 的实际和有效替代方案。</li>
<li>methods: 这个研究引入了一个名为 SDAR 的攻击框架，这个框架使用辅助数据和敌对调整来学习一个可以实时重建客户端私人模型的可靠模拟器。</li>
<li>results: 实验结果显示，在实际且实用的攻击enario中，SDAR 能够实时重建客户端私人数据，并在 U-shaped SL 中重建数据和标签。在 CIFAR-10 上，在深度分割水平 7 下，SDAR 能够实现私人数据重建的 mean squared error 小于 0.025，并在 U-shaped SL 中 дости得标签推论精度高于 98%。<details>
<summary>Abstract</summary>
Split Learning (SL) has emerged as a practical and efficient alternative to traditional federated learning. While previous attempts to attack SL have often relied on overly strong assumptions or targeted easily exploitable models, we seek to develop more practical attacks. We introduce SDAR, a novel attack framework against SL with an honest-but-curious server. SDAR leverages auxiliary data and adversarial regularization to learn a decodable simulator of the client's private model, which can effectively infer the client's private features under the vanilla SL, and both features and labels under the U-shaped SL. We perform extensive experiments in both configurations to validate the effectiveness of our proposed attacks. Notably, in challenging but practical scenarios where existing passive attacks struggle to reconstruct the client's private data effectively, SDAR consistently achieves attack performance comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR achieves private feature reconstruction with less than 0.025 mean squared error in both the vanilla and the U-shaped SL, and attains a label inference accuracy of over 98% in the U-shaped setting, while existing attacks fail to produce non-trivial results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adaptive-Neural-Ranking-Framework-Toward-Maximized-Business-Goal-for-Cascade-Ranking-Systems"><a href="#Adaptive-Neural-Ranking-Framework-Toward-Maximized-Business-Goal-for-Cascade-Ranking-Systems" class="headerlink" title="Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems"></a>Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10462">http://arxiv.org/abs/2310.10462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunli Wang, Zhiqiang Wang, Jian Yang, Shiyang Wen, Dongying Kong, Han Li, Kun Gai</li>
<li>for: 这个论文主要针对大规模top-k选择问题中的涨幅排序系统优化，具体来说是通过学习排序来优化模型。</li>
<li>methods: 该论文提出了一种基于多任务学习框架的 Adaptive Neural Ranking Framework，通过将relaxed和完整的目标优化并 combinely，使得优化目标适应不同数据复杂度和模型能力。</li>
<li>results: 实验结果表明，该方法在4个公共和商业benchmark上表现出色，并且在线上实验中具有显著的应用价值。<details>
<summary>Abstract</summary>
Cascade ranking is widely used for large-scale top-k selection problems in online advertising and recommendation systems, and learning-to-rank is an important way to optimize the models in cascade ranking systems. Previous works on learning-to-rank usually focus on letting the model learn the complete order or pay more attention to the order of top materials, and adopt the corresponding rank metrics as optimization targets. However, these optimization targets can not adapt to various cascade ranking scenarios with varying data complexities and model capabilities; and the existing metric-driven methods such as the Lambda framework can only optimize a rough upper bound of the metric, potentially resulting in performance misalignment. To address these issues, we first propose a novel perspective on optimizing cascade ranking systems by highlighting the adaptability of optimization targets to data complexities and model capabilities. Concretely, we employ multi-task learning framework to adaptively combine the optimization of relaxed and full targets, which refers to metrics Recall@m@k and OAP respectively. Then we introduce a permutation matrix to represent the rank metrics and employ differentiable sorting techniques to obtain a relaxed permutation matrix with controllable approximate error bound. This enables us to optimize both the relaxed and full targets directly and more appropriately using the proposed surrogate losses within the deep learning framework. We named this method as Adaptive Neural Ranking Framework. We use the NeuralSort method to obtain the relaxed permutation matrix and draw on the uncertainty weight method in multi-task learning to optimize the proposed losses jointly. Experiments on a total of 4 public and industrial benchmarks show the effectiveness and generalization of our method, and online experiment shows that our method has significant application value.
</details>
<details>
<summary>摘要</summary>
cascade ranking 广泛应用于大规模 top-k 选择问题中，学习 rank 是一种重要的优化方法。前一些工作通常是让模型学习完整的排序或更加注重 top Materials 的排序，采用相应的rank metric作为优化目标。然而，这些优化目标无法适应不同的排序场景中的数据复杂性和模型能力；而现有的 metric-driven 方法，如Lambda框架，只能优化一个粗略的上界，可能导致性能不符。为解决这些问题，我们首先提出一种新的视角，即优化 cascade ranking 系统的可适应性。具体来说，我们使用多任务学习框架来适应性地组合优化 relaxed 和 full 目标。relaxed 目标指的是 recall@m@k 和 OAP metric，而 full 目标则是完整的排序。然后，我们引入排序矩阵来表示排序 metric，并使用可微排序技术来获得一个可控的相对误差 bound。这使得我们可以直接优化 relaxed 和 full 目标，并更加合适地使用我们提出的代理损失函数在深度学习框架中进行优化。我们称这种方法为 Adaptive Neural Ranking Framework。我们使用 NeuralSort 方法来获得 relaxed 排序矩阵，并在多任务学习中使用不确定性权重来优化我们的提出的损失函数。实验结果显示，我们的方法在四个公共和工业标准准中表现出色，并且在实际应用中具有显著的价值。
</details></li>
</ul>
<hr>
<h2 id="A-Geometric-Insight-into-Equivariant-Message-Passing-Neural-Networks-on-Riemannian-Manifolds"><a href="#A-Geometric-Insight-into-Equivariant-Message-Passing-Neural-Networks-on-Riemannian-Manifolds" class="headerlink" title="A Geometric Insight into Equivariant Message Passing Neural Networks on Riemannian Manifolds"></a>A Geometric Insight into Equivariant Message Passing Neural Networks on Riemannian Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10448">http://arxiv.org/abs/2310.10448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilyes Batatia</li>
<li>for: 本文提出了一种 geometric 的思路，用于解释 equivariant message passing 在 Riemannian manifold 上的实现。</li>
<li>methods: 作者使用 coordinate-independent feature fields 表示数据的 numerical features，并将其映射到主bundle 上的 equivariant embedding 中。然后，他们提出一种优化 Polyakov action 的方法，以确保 embedding 中的 metric 与原始 metric 相似。</li>
<li>results: 作者提出了一种基于 equivariant diffusion process 的 message passing scheme，可以在 manifold 上实现。此外，他们还提出了一种基于高阶 equivariant diffusion process 的新的一般化 GNN 模型，可以扩展 ACE 和 MACE  formalism 到 Riemannian manifold 上的数据。<details>
<summary>Abstract</summary>
This work proposes a geometric insight into equivariant message passing on Riemannian manifolds. As previously proposed, numerical features on Riemannian manifolds are represented as coordinate-independent feature fields on the manifold. To any coordinate-independent feature field on a manifold comes attached an equivariant embedding of the principal bundle to the space of numerical features. We argue that the metric this embedding induces on the numerical feature space should optimally preserve the principal bundle's original metric. This optimality criterion leads to the minimization of a twisted form of the Polyakov action with respect to the graph of this embedding, yielding an equivariant diffusion process on the associated vector bundle. We obtain a message passing scheme on the manifold by discretizing the diffusion equation flow for a fixed time step. We propose a higher-order equivariant diffusion process equivalent to diffusion on the cartesian product of the base manifold. The discretization of the higher-order diffusion process on a graph yields a new general class of equivariant GNN, generalizing the ACE and MACE formalism to data on Riemannian manifolds.
</details>
<details>
<summary>摘要</summary>
这个工作提出了一种几何视角来理解在里曼尼投影上的平衡消息传递。在先前的提议中，数字特征在里曼尼投影上是作为独立坐标的特征场表示的。为任何独立特征场在投影上来说，有一个对称嵌入主 bundle 到特征空间的 equivariant 嵌入。我们 argue 这个嵌入应该保持原始主 bundle 的 metric 的最佳方式，这个标准导致了对 twisted 形式的 Polyakov 动作的最小化，从而获得一个 equivariant 扩散过程在关联的向量bundle 上。我们可以通过粘束扩散方程的离散来获得一个消息传递方案在投影上。我们提出了一种高阶不变扩散过程，与 cartesian product 的基 manifold 相等。离散这种高阶扩散过程在图上得到一个新的一般类型的不变 GNN，扩展了数据在里曼尼投影上的 ACE 和 MACE  formalism。
</details></li>
</ul>
<hr>
<h2 id="Taming-the-Sigmoid-Bottleneck-Provably-Argmaxable-Sparse-Multi-Label-Classification"><a href="#Taming-the-Sigmoid-Bottleneck-Provably-Argmaxable-Sparse-Multi-Label-Classification" class="headerlink" title="Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification"></a>Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10443">http://arxiv.org/abs/2310.10443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreasgrv/sigmoid-bottleneck">https://github.com/andreasgrv/sigmoid-bottleneck</a></li>
<li>paper_authors: Andreas Grivas, Antonio Vergari, Adam Lopez</li>
<li>for: 这篇论文是关于多标签分类任务中的sigmoid输出层，其中每个输入可以获得多个标签。</li>
<li>methods: 这篇论文使用了Discrete Fourier Transform（DFT）输出层，以确保所有稀疏的标签组合都是可arginmax的。</li>
<li>results: 论文表明，sigmoid输出层在多标签分类任务中会导致无法argmax的输出，并且可以通过使用DFT输出层来避免这种情况。DFT输出层比sigmoid输出层更快速地训练，并且具有更好的参数效率。<details>
<summary>Abstract</summary>
Sigmoid output layers are widely used in multi-label classification (MLC) tasks, in which multiple labels can be assigned to any input. In many practical MLC tasks, the number of possible labels is in the thousands, often exceeding the number of input features and resulting in a low-rank output layer. In multi-class classification, it is known that such a low-rank output layer is a bottleneck that can result in unargmaxable classes: classes which cannot be predicted for any input. In this paper, we show that for MLC tasks, the analogous sigmoid bottleneck results in exponentially many unargmaxable label combinations. We explain how to detect these unargmaxable outputs and demonstrate their presence in three widely used MLC datasets. We then show that they can be prevented in practice by introducing a Discrete Fourier Transform (DFT) output layer, which guarantees that all sparse label combinations with up to $k$ active labels are argmaxable. Our DFT layer trains faster and is more parameter efficient, matching the F1@k score of a sigmoid layer while using up to 50% fewer trainable parameters. Our code is publicly available at https://github.com/andreasgrv/sigmoid-bottleneck.
</details>
<details>
<summary>摘要</summary>
希格迪输出层在多标签分类(MLC)任务中广泛使用，在任务中任何输入都可以获得多个标签。在实际应用中，可能有数천个可能的标签，常常超过输入特征的数量，导致输出层的低级排名。在多类分类中，这种低级输出层会导致不可预测的类：无法预测的类。在这篇论文中，我们表明MLC任务中的希格迪瓶颈会导致无数多个不可预测的标签组合。我们解释了如何检测这些不可预测的输出和三个常用的MLC数据集中其存在。然后我们表明可以通过引入离散傅里叶变换(DFT)输出层来避免这些不可预测的输出。我们的DFT层在训练时更快，并且使用更少的可训练参数，与希格迪层的F1@k分数相同，而使用的参数数量可以减少到50%。我们的代码可以在https://github.com/andreasgrv/sigmoid-bottleneck上获取。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Matrix-Function-Neural-Networks"><a href="#Equivariant-Matrix-Function-Neural-Networks" class="headerlink" title="Equivariant Matrix Function Neural Networks"></a>Equivariant Matrix Function Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10434">http://arxiv.org/abs/2310.10434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilyes Batatia, Lars L. Schaaf, Huajie Chen, Gábor Csányi, Christoph Ortner, Felix A. Faber</li>
<li>for:  This paper aims to address the challenges of modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials using Graph Neural Networks (GNNs) and traditional neural networks.</li>
<li>methods:  The paper introduces a novel architecture called Matrix Function Neural Networks (MFNs), which parameterizes non-local interactions through analytic matrix equivariant functions. The MFN architecture uses resolvent expansions for a straightforward implementation and the potential for linear scaling with system size.</li>
<li>results:  The MFN architecture achieves state-of-the-art performance in standard graph benchmarks, such as the ZINC and TU datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials. Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The MFN architecture achieves state-of-the-art performance in standard graph benchmarks, such as the ZINC and TU datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields.
</details>
<details>
<summary>摘要</summary>
图形神经网络（GNNs），特别是消息传递神经网络（MPNNs），在不同应用场景中显示出了强大的架构能力。然而，MPNNs在大 conjugated molecules、金属和归一化材料等系统中模型非本地交互时面临挑战。虽然spectral GNNs和传统神经网络如回归神经网络和transformers可以减轻这些挑战，但它们经常缺乏广泛性、适应性、普适性、计算效率或失去数据中的细致结构关系或对称性。为解决这些问题，我们介绍了矩阵函数神经网络（MFNs），一种新的架构，该参数非本地交互通过矩阵对偶变换函数。使用resolvent expansions的实现可以提供一种简单的实现方式，并且可能实现系统大小的线性扩展。MFN架构在标准图形数据集上达到了state-of-the-art性能，如ZINC和TU数据集，并能够捕捉到量子系统中的复杂非本地交互，为新的state-of-the-art力场开创道路。
</details></li>
</ul>
<hr>
<h2 id="Continuously-Adapting-Random-Sampling-CARS-for-Power-Electronics-Parameter-Design"><a href="#Continuously-Adapting-Random-Sampling-CARS-for-Power-Electronics-Parameter-Design" class="headerlink" title="Continuously Adapting Random Sampling (CARS) for Power Electronics Parameter Design"></a>Continuously Adapting Random Sampling (CARS) for Power Electronics Parameter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10425">http://arxiv.org/abs/2310.10425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Happel, Philipp Brendel, Andreas Rosskopf, Stefan Ditze</li>
<li>for: 这个论文主要针对的是电子能源参数设计任务的优化问题，通常使用详细的优化方法或者笨拙的搜索方法来解决。</li>
<li>methods: 该论文提出了一种新的方法 named “Continuously Adapting Random Sampling” (CARS)，它提供了一种连续的方法，位于详细优化方法和笨拙搜索方法之间。这种方法可以快速地进行大量的 simulations，同时逐渐增加关注最有前途的参数范围。这个方法 Draws inspiration from multi-armed bandit research and leads to prioritized sampling of sub-domains in one high-dimensional parameter tensor。</li>
<li>results: 该论文对三个例子的电子能源使用情况进行了评估，得到的设计与遗传算法相当竞争力，同时具有高度并行化的 simulate 特点和不断进行探索和利用设置之间的融合。<details>
<summary>Abstract</summary>
To date, power electronics parameter design tasks are usually tackled using detailed optimization approaches with detailed simulations or using brute force grid search grid search with very fast simulations. A new method, named "Continuously Adapting Random Sampling" (CARS) is proposed, which provides a continuous method in between. This allows for very fast, and / or large amounts of simulations, but increasingly focuses on the most promising parameter ranges. Inspirations are drawn from multi-armed bandit research and lead to prioritized sampling of sub-domains in one high-dimensional parameter tensor. Performance has been evaluated on three exemplary power electronic use-cases, where resulting designs appear competitive to genetic algorithms, but additionally allow for highly parallelizable simulation, as well as continuous progression between explorative and exploitative settings.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：迄今，电力电子参数设计任务通常使用详细优化方法或使用劳顿搜索法，均采用详细的simulation。一种新的方法，名为“连续适应随机抽样”（CARS）被提议，它提供了一种连续的方法，位于详细优化和劳顿搜索之间。这使得可以很快、或者进行大量的simulation，但是逐渐关注最有前途的参数范围。 draw inspirations from multi-armed bandit research and lead to prioritized sampling of sub-domains in one high-dimensional parameter tensor。 performance has been evaluated on three exemplary power electronic use-cases, where resulting designs appear competitive to genetic algorithms, but additionally allow for highly parallelizable simulation, as well as continuous progression between explorative and exploitative settings。</SYS>Here is the translation of the text into Simplified Chinese:迄今，电力电子参数设计任务通常使用详细优化方法或使用劳顿搜索法，均采用详细的simulation。一种新的方法，名为“连续适应随机抽样”（CARS）被提议，它提供了一种连续的方法，位于详细优化和劳顿搜索之间。这使得可以很快、或者进行大量的simulation，但是逐渐关注最有前途的参数范围。 draw inspirations from multi-armed bandit research and lead to prioritized sampling of sub-domains in one high-dimensional parameter tensor。 performance has been evaluated on three exemplary power electronic use-cases, where resulting designs appear competitive to genetic algorithms, but additionally allow for highly parallelizable simulation, as well as continuous progression between explorative and exploitative settings。
</details></li>
</ul>
<hr>
<h2 id="Towards-Fair-and-Calibrated-Models"><a href="#Towards-Fair-and-Calibrated-Models" class="headerlink" title="Towards Fair and Calibrated Models"></a>Towards Fair and Calibrated Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10399">http://arxiv.org/abs/2310.10399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Vipul Rathore, Mausam, Parag Singla</li>
<li>for: 建立不偏袋化和准确的机器学习模型</li>
<li>methods: 使用特定定义的公平性、抽象和解释性，并提出了一种基于温度 scaling的简单预处理技术和修改现有抽象损失来实现公平和准确的模型</li>
<li>results: 通过对多种数据集进行广泛实验，发现这些技术可以实现公平和准确的模型，并提供了对模型的解释和分析。<details>
<summary>Abstract</summary>
Recent literature has seen a significant focus on building machine learning models with specific properties such as fairness, i.e., being non-biased with respect to a given set of attributes, calibration i.e., model confidence being aligned with its predictive accuracy, and explainability, i.e., ability to be understandable to humans. While there has been work focusing on each of these aspects individually, researchers have shied away from simultaneously addressing more than one of these dimensions. In this work, we address the problem of building models which are both fair and calibrated. We work with a specific definition of fairness, which closely matches [Biswas et. al. 2019], and has the nice property that Bayes optimal classifier has the maximum possible fairness under our definition. We show that an existing negative result towards achieving a fair and calibrated model [Kleinberg et. al. 2017] does not hold for our definition of fairness. Further, we show that ensuring group-wise calibration with respect to the sensitive attributes automatically results in a fair model under our definition. Using this result, we provide a first cut approach for achieving fair and calibrated models, via a simple post-processing technique based on temperature scaling. We then propose modifications of existing calibration losses to perform group-wise calibration, as a way of achieving fair and calibrated models in a variety of settings. Finally, we perform extensive experimentation of these techniques on a diverse benchmark of datasets, and present insights on the pareto-optimality of the resulting solutions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Revisiting-Logistic-softmax-Likelihood-in-Bayesian-Meta-Learning-for-Few-Shot-Classification"><a href="#Revisiting-Logistic-softmax-Likelihood-in-Bayesian-Meta-Learning-for-Few-Shot-Classification" class="headerlink" title="Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification"></a>Revisiting Logistic-softmax Likelihood in Bayesian Meta-Learning for Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10379">http://arxiv.org/abs/2310.10379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/keanson/revisit-logistic-softmax">https://github.com/keanson/revisit-logistic-softmax</a></li>
<li>paper_authors: Tianjun Ke, Haoqun Cao, Zenan Ling, Feng Zhou</li>
<li>for: 这个论文主要研究了如何使用逻辑-软泛函数来提高几个shot分类（FSC）中的不确定性评估和性能。</li>
<li>methods: 该论文使用了 bayesian 方法来 caracterize  uncertainty in FSC，并使用了修改后的逻辑-软泛函数来控制先前不确定性的问题。</li>
<li>results: 该论文通过 theoretically 和 empirically 表明，修改后的逻辑-软泛函数可以提高 uncertainty 估计的准确性和性能，并且可以在标准 benchmark 数据集上达到或超过相同水平。<details>
<summary>Abstract</summary>
Meta-learning has demonstrated promising results in few-shot classification (FSC) by learning to solve new problems using prior knowledge. Bayesian methods are effective at characterizing uncertainty in FSC, which is crucial in high-risk fields. In this context, the logistic-softmax likelihood is often employed as an alternative to the softmax likelihood in multi-class Gaussian process classification due to its conditional conjugacy property. However, the theoretical property of logistic-softmax is not clear and previous research indicated that the inherent uncertainty of logistic-softmax leads to suboptimal performance. To mitigate these issues, we revisit and redesign the logistic-softmax likelihood, which enables control of the \textit{a priori} confidence level through a temperature parameter. Furthermore, we theoretically and empirically show that softmax can be viewed as a special case of logistic-softmax and logistic-softmax induces a larger family of data distribution than softmax. Utilizing modified logistic-softmax, we integrate the data augmentation technique into the deep kernel based Gaussian process meta-learning framework, and derive an analytical mean-field approximation for task-specific updates. Our approach yields well-calibrated uncertainty estimates and achieves comparable or superior results on standard benchmark datasets. Code is publicly available at \url{https://github.com/keanson/revisit-logistic-softmax}.
</details>
<details>
<summary>摘要</summary>
<SYS>使用适应学习的方法可以在几个批处理（Few-shot Classification，FSC）中表现出色，因为它可以通过之前的知识来解决新的问题。 bayesian方法可以准确地描述 FSC 中的uncertainty，这对于高风险领域非常重要。在这种情况下，通常使用Logistic-softmax概率 Distribution来取代Softmax概率 Distribution，因为它们具有 conditional conjugacy 性质。然而，Logistic-softmax的理论性不够清楚，而且先前的研究表明，Logistic-softmax的内在不确定性会导致表现下降。为了解决这些问题，我们重新访问和重新设计Logistic-softmax概率 Distribution，这使得可以通过温度参数控制 \textit{a priori} 信任水平。此外，我们还证明了Softmax可以视为Logistic-softmax的特殊情况，Logistic-softmax可以生成更大的数据分布Family。通过修改Logistic-softmax，我们将数据扩展技术集成到深度kernel基于Gaussian Process meta-学习框架中，并 derive了analytical mean-field Approximation for task-specific updates。我们的方法可以提供Well-calibrated uncertainty estimates，并在标准 benchmark datasets上实现了相对或superior的Results。相关代码可以在 <https://github.com/keanson/revisit-logistic-softmax> 上获取。</SYS>I hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Multi-Factor-Spatio-Temporal-Prediction-based-on-Graph-Decomposition-Learning"><a href="#Multi-Factor-Spatio-Temporal-Prediction-based-on-Graph-Decomposition-Learning" class="headerlink" title="Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition Learning"></a>Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10374">http://arxiv.org/abs/2310.10374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahao Ji, Jingyuan Wang, Yu Mou, Cheng Long</li>
<li>for: 本文提出了一种多因素空间时间预测任务，用于预测不同因素的空间时间数据的发展趋势。</li>
<li>methods: 本文提出了一种基于层次分解策略的 theoretically 有效的方法，以及一种名为空间时间图分解学习（STGDL）的模型无关框架。STGDL 包括两个主要组成部分：自动图分解模块和分解学习网络。</li>
<li>results: 对四个实际的空间时间数据集进行了广泛的实验，结果显示，使用本文提出的方法可以significantly 降低不同模型的预测错误率，最高降低到35.36%。此外，一个案例研究也表明了本方法的可解释性潜力。<details>
<summary>Abstract</summary>
Spatio-temporal (ST) prediction is an important and widely used technique in data mining and analytics, especially for ST data in urban systems such as transportation data. In practice, the ST data generation is usually influenced by various latent factors tied to natural phenomena or human socioeconomic activities, impacting specific spatial areas selectively. However, existing ST prediction methods usually do not refine the impacts of different factors, but directly model the entangled impacts of multiple factors. This amplifies the modeling complexity of ST data and compromises model interpretability. To this end, we propose a multi-factor ST prediction task that predicts partial ST data evolution under different factors, and combines them for a final prediction. We make two contributions to this task: an effective theoretical solution and a portable instantiation framework. Specifically, we first propose a theoretical solution called decomposed prediction strategy and prove its effectiveness from the perspective of information entropy theory. On top of that, we instantiate a novel model-agnostic framework, named spatio-temporal graph decomposition learning (STGDL), for multi-factor ST prediction. The framework consists of two main components: an automatic graph decomposition module that decomposes the original graph structure inherent in ST data into subgraphs corresponding to different factors, and a decomposed learning network that learns the partial ST data on each subgraph separately and integrates them for the final prediction. We conduct extensive experiments on four real-world ST datasets of two types of graphs, i.e., grid graph and network graph. Results show that our framework significantly reduces prediction errors of various ST models by 9.41% on average (35.36% at most). Furthermore, a case study reveals the interpretability potential of our framework.
</details>
<details>
<summary>摘要</summary>
这是一个很重要的数据探索和分析技术，尤其是在城市系统中的交通数据。在实践中，这些数据通常受到自然现象或人类社会经济活动的多种隐藏因素影响，这些因素影响特定的空间区域选择性地。然而，现有的这些预测方法通常不会细分这些因素的影响，而是直接模型这些杂糅的影响。这会增加这些数据的预测复杂性和模型解释性。为了解决这个问题，我们提出了一个多因素预测任务，预测不同因素的部分预测结果，然后结合它们进行最终预测。我们做出了两个贡献：一个有效的理论解决方案和一个可携的实现框架。具体来说，我们首先提出了一个名为分解预测策略的理论解决方案，并证明其有效性从信息熵理论的角度。而在这个基础上，我们实现了一个名为类型-独立预测架构（STGDL）的新模型独立框架，这个框架包括两个主要 ком成分：一个自动对应图解析模组，将原始的图структуре组织体内的ST数据分解为不同因素的子图，以及一个分解学网络，这个学网络在每个子图上进行分解预测，然后将它们结合进行最终预测。我们对四个真实世界的ST数据集进行了广泛的实验，结果显示，我们的框架可以对不同的ST模型进行预测，将预测错误量降低了9.41%的平均值（最高到35.36%）。此外，一个实验显示了我们的框架的解释能力。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-in-physics-a-short-guide"><a href="#Machine-learning-in-physics-a-short-guide" class="headerlink" title="Machine learning in physics: a short guide"></a>Machine learning in physics: a short guide</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10368">http://arxiv.org/abs/2310.10368</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/franciscorodrigues-usp/MLP">https://github.com/franciscorodrigues-usp/MLP</a></li>
<li>paper_authors: Francisco A. Rodrigues</li>
<li>for: Physics field （物理领域）</li>
<li>methods: Machine learning（机器学习）</li>
<li>results: Causal inference, symbolic regression, deep learning（因果推理、符号回归、深度学习）Here’s a more detailed explanation of each point:1. for: The paper is written for the field of physics, specifically focusing on the applications of machine learning in physics.2. methods: The paper covers the main concepts of machine learning, including supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning.3. results: The paper discusses some of the principal applications of machine learning in physics and highlights the associated challenges and perspectives.I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Machine learning is a rapidly growing field with the potential to revolutionize many areas of science, including physics. This review provides a brief overview of machine learning in physics, covering the main concepts of supervised, unsupervised, and reinforcement learning, as well as more specialized topics such as causal inference, symbolic regression, and deep learning. We present some of the principal applications of machine learning in physics and discuss the associated challenges and perspectives.
</details>
<details>
<summary>摘要</summary>
机器学习是一个迅速成长的领域，拥有可能改革多个科学领域的潜力，包括物理学。本篇文章提供了物理学中机器学习的简要总览，涵盖主要概念的监督学习、无监督学习和强化学习，以及更特殊的主题，如 causal inference、符号回传和深度学习。我们介绍了物理学中机器学习的主要应用和相关挑战，以及未来的展望。
</details></li>
</ul>
<hr>
<h2 id="Advantages-of-Machine-Learning-in-Bus-Transport-Analysis"><a href="#Advantages-of-Machine-Learning-in-Bus-Transport-Analysis" class="headerlink" title="Advantages of Machine Learning in Bus Transport Analysis"></a>Advantages of Machine Learning in Bus Transport Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19810">http://arxiv.org/abs/2310.19810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirsadegh Roshanzamir</li>
<li>for: 这个研究旨在使用指导学习算法分析特拉特当地公共汽车系统的准时性。</li>
<li>methods: 该研究使用了各种指导学习算法，包括Python的Sci Kit Learn和Stats Models库，以建立准确的模型，能够预测任何一天是否会遵循公共汽车路线的时间标准。</li>
<li>results: 研究发现，指导学习算法最重要的考虑因素是公共汽车路线的效率，这对于改善公共汽车系统的性能提供了重要的洞察。<details>
<summary>Abstract</summary>
Supervised Machine Learning is an innovative method that aims to mimic human learning by using past experiences. In this study, we utilize supervised machine learning algorithms to analyze the factors that contribute to the punctuality of Tehran BRT bus system. We gather publicly available datasets of 2020 to 2022 from Municipality of Tehran to train and test our models. By employing various algorithms and leveraging Python's Sci Kit Learn and Stats Models libraries, we construct accurate models capable of predicting whether a bus route will meet the prescribed standards for on-time performance on any given day. Furthermore, we delve deeper into the decision-making process of each algorithm to determine the most influential factor it considers. This investigation allows us to uncover the key feature that significantly impacts the effectiveness of bus routes, providing valuable insights for improving their performance.
</details>
<details>
<summary>摘要</summary>
超vised机器学习是一种创新的方法，旨在模仿人类学习的方式，使用过去的经验。在这个研究中，我们使用超vised机器学习算法来分析特拉ن布特公共汽车系统的准时性因素。我们使用2020年至2022年公共数据集，来训练和测试我们的模型。通过使用不同的算法和利用Python的Sci Kit Learn和Stats Models库，我们构建了准确的模型，能够预测任何一天会否遵循指定的准时性标准。此外，我们还探究每个算法的决策过程，以确定它最重要的考虑因素。这些调查可以帮助我们找到影响公共汽车线路效果的关键特征，提供有价值的反馈，以提高其性能。
</details></li>
</ul>
<hr>
<h2 id="MgNO-Efficient-Parameterization-of-Linear-Operators-via-Multigrid"><a href="#MgNO-Efficient-Parameterization-of-Linear-Operators-via-Multigrid" class="headerlink" title="MgNO: Efficient Parameterization of Linear Operators via Multigrid"></a>MgNO: Efficient Parameterization of Linear Operators via Multigrid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19809">http://arxiv.org/abs/2310.19809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncai He, Xinliang Liu, Jinchao Xu</li>
<li>for: 这个论文旨在提出一种简洁的神经网络架构，用于学习运算。</li>
<li>methods: 该方法使用了神经网络中的非线性运算层，其输出可以表示为 $\mathcal O_i(u) &#x3D; \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$。在这里，$\mathcal W_{ij}$ 是将 $j $- 个输入神经元连接到 $i $- 个输出神经元的半bounded线性算子，而偏置 $\mathcal B_{ij}$ 是一个函数而不是整数。</li>
<li>results: 该方法可以准确地解决不同类型的偏微分方程（PDEs），并且在训练时显示出了更高的易学性和更低的抗抑阻性。<details>
<summary>Abstract</summary>
In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting operators typically required in previous neural operators. Moreover, it seamlessly accommodates diverse boundary conditions. Our empirical observations reveal that MgNO exhibits superior ease of training compared to other CNN-based models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators. We demonstrate the efficiency and accuracy of our method with consistently state-of-the-art performance on different types of partial differential equations (PDEs).
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种简洁神经操作架构，用于神经网络学习。我们将神经操作定义为：输出第i个神经元的非线性操作层的输出为： $\mathcal O_i(u) = \sigma\left(\sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. 其中， $\mathcal W_{ij}$ 表示连接第j个输入神经元到第i个输出神经元的稍尺度的线性操作，而偏置 $\mathcal B_{ij}$ 是一个函数而非整数。由于这个新的通用近似性质，神经操作中的稍尺度线性操作之间的效率参数化（Banach空间）扮演了关键的角色。因此，我们引入MgNO，利用多重格struktur来参数这些线性操作。这种方法具有数学上的准确性和实际上的表达力。此外，MgNO可以自然地满足多种边界条件。我们的实验观察表明，MgNO比其他CNN基于模型更易于训练，同时也具有较少的折衔强度。我们通过不同类型的偏微分方程（PDEs）的实验表明了我们的方法的效率和准确性。
</details></li>
</ul>
<hr>
<h2 id="An-Anytime-Algorithm-for-Good-Arm-Identification"><a href="#An-Anytime-Algorithm-for-Good-Arm-Identification" class="headerlink" title="An Anytime Algorithm for Good Arm Identification"></a>An Anytime Algorithm for Good Arm Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10359">http://arxiv.org/abs/2310.10359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc Jourdan, Clémence Réda</li>
<li>for: 这 paper 的目的是解决在固定预算和时间限制下的好臂标识问题（GAI）。</li>
<li>methods: 这 paper 提出了一种无参数和时间自适应的采样规则，称为 APGAI，可以在固定信度和预算设置下使用。</li>
<li>results: 作者提供了关于 APGAI 的Upper bound 的概率错误和预测采样复杂性的证明，以及实验结果表明 APGAI 在 synthetic 和实际数据上具有良好的表现。<details>
<summary>Abstract</summary>
In good arm identification (GAI), the goal is to identify one arm whose average performance exceeds a given threshold, referred to as good arm, if it exists. Few works have studied GAI in the fixed-budget setting, when the sampling budget is fixed beforehand, or the anytime setting, when a recommendation can be asked at any time. We propose APGAI, an anytime and parameter-free sampling rule for GAI in stochastic bandits. APGAI can be straightforwardly used in fixed-confidence and fixed-budget settings. First, we derive upper bounds on its probability of error at any time. They show that adaptive strategies are more efficient in detecting the absence of good arms than uniform sampling. Second, when APGAI is combined with a stopping rule, we prove upper bounds on the expected sampling complexity, holding at any confidence level. Finally, we show good empirical performance of APGAI on synthetic and real-world data. Our work offers an extensive overview of the GAI problem in all settings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate into Simplified ChineseIn good arm identification (GAI), the goal is to identify one arm whose average performance exceeds a given threshold, referred to as good arm, if it exists. Few works have studied GAI in the fixed-budget setting, when the sampling budget is fixed beforehand, or the anytime setting, when a recommendation can be asked at any time. We propose APGAI, an anytime and parameter-free sampling rule for GAI in stochastic bandits. APGAI can be straightforwardly used in fixed-confidence and fixed-budget settings. First, we derive upper bounds on its probability of error at any time. They show that adaptive strategies are more efficient in detecting the absence of good arms than uniform sampling. Second, when APGAI is combined with a stopping rule, we prove upper bounds on the expected sampling complexity, holding at any confidence level. Finally, we show good empirical performance of APGAI on synthetic and real-world data. Our work offers an extensive overview of the GAI problem in all settings.中文简体版：在好臂标识（GAI）中，目标是找到一个臂的平均性能超过给定的阈值的臂，如果存在。已有相对少的研究对GAI进行了固定预算设定或任何时间设定。我们提出了APGAI，一种无参数和任何时间 sampling 规则。APGAI可以直接在固定信度和固定预算设定下使用。我们首先 deriv 了APGAI在任何时间的错误概率的Upper bound。这些结果显示了适应策略在缺乏好臂时更有效率地检测。其次，当APGAI与停止规则结合使用时，我们证明了预期的样本复杂度的Upper bound，保持任何信度水平。最后，我们在 sintetic 和实际数据上显示了APGAI的良好实际表现。我们的工作对GAI问题在所有设定中进行了全面的概述。
</details></li>
</ul>
<hr>
<h2 id="Hamming-Encoder-Mining-Discriminative-k-mers-for-Discrete-Sequence-Classification"><a href="#Hamming-Encoder-Mining-Discriminative-k-mers-for-Discrete-Sequence-Classification" class="headerlink" title="Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence Classification"></a>Hamming Encoder: Mining Discriminative k-mers for Discrete Sequence Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10321">http://arxiv.org/abs/2310.10321</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Dong, Mudi Jiang, Lianyu Hu, Zengyou He</li>
<li>for: 该论文的目的是提出一种新的序列分类方法，以解决现有方法中的一些挑战，如缺乏特征组合的探索和精度下降。</li>
<li>methods: 该方法基于1D卷积神经网络（1DCNN）架构，并采用哈明距离基于相似度度量来确保特征挖掘和分类过程中的一致性。具体来说，该方法首先训练一个可解释的CNNEncoder对序列数据进行学习，然后通过梯度下降方式搜索出高度探索的k-mer组合。</li>
<li>results: 实验结果表明，该方法在分类精度方面比现有的状态作法更高。<details>
<summary>Abstract</summary>
Sequence classification has numerous applications in various fields. Despite extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. Existing pattern-based methods measure the discriminative power of each feature individually during the mining process, leading to the result of missing some combinations of features with discriminative power. Furthermore, it is difficult to ensure the overall discriminative performance after converting sequences into feature vectors. To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer combinations. Experiments show that the Hamming Encoder method proposed in this paper outperforms existing state-of-the-art methods in terms of classification accuracy.
</details>
<details>
<summary>摘要</summary>
Sequence 分类有很多应用场景，尤其是在不同领域。 DESPITE  extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. Existing pattern-based methods measure the discriminative power of each feature individually during the mining process, leading to the result of missing some combinations of features with discriminative power. Furthermore, it is difficult to ensure the overall discriminative performance after converting sequences into feature vectors. To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer combinations. Experiments show that the Hamming Encoder method proposed in this paper outperforms existing state-of-the-art methods in terms of classification accuracy.Here's the word-for-word translation:序列分类有很多应用场景，尤其是在不同领域。 DESPITE  extensive studies in the last decades, many challenges still exist, particularly in pattern-based methods. 现有的 pattern-based methods 在挖掘过程中对每个特征进行分解能力的测量，导致漏掉一些特征组合的分解能力。 更重要的是，将序列转换为特征向量后，保证总的分解性能是一个大问题。 To address these challenges, we propose a novel approach called Hamming Encoder, which utilizes a binarized 1D-convolutional neural network (1DCNN) architecture to mine discriminative k-mer sets. In particular, we adopt a Hamming distance-based similarity measure to ensure consistency in the feature mining and classification procedure. Our method involves training an interpretable CNN encoder for sequential data and performing a gradient-based search for discriminative k-mer combinations. Experiments show that the Hamming Encoder method proposed in this paper outperforms existing state-of-the-art methods in terms of classification accuracy.
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Quantum-Machine-Learning-Current-Trends-Challenges-Opportunities-and-the-Road-Ahead"><a href="#A-Survey-on-Quantum-Machine-Learning-Current-Trends-Challenges-Opportunities-and-the-Road-Ahead" class="headerlink" title="A Survey on Quantum Machine Learning: Current Trends, Challenges, Opportunities, and the Road Ahead"></a>A Survey on Quantum Machine Learning: Current Trends, Challenges, Opportunities, and the Road Ahead</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10315">http://arxiv.org/abs/2310.10315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kamila Zaman, Alberto Marchisio, Muhammad Abdullah Hanif, Muhammad Shafique</li>
<li>for: 这篇论文主要是为了提供一个全面的Quantum Machine Learning（QML）领域的审视，并对不同的QML算法、量子数据集、硬件技术、软件工具、模拟器和应用场景进行了详细的介绍。</li>
<li>methods: 本论文使用了多种方法，包括论述基础概念、对класситиче计算的比较、介绍不同的QML算法和其适用领域、描述量子数据集和硬件技术的发展，以及介绍软件工具和模拟器。</li>
<li>results: 本论文提供了大量有价值的信息和资源，可以帮助读者快速入门到当前QML领域的state-of-the-art技术。<details>
<summary>Abstract</summary>
Quantum Computing (QC) claims to improve the efficiency of solving complex problems, compared to classical computing. When QC is applied to Machine Learning (ML) applications, it forms a Quantum Machine Learning (QML) system. After discussing the basic concepts of QC and its advantages over classical computing, this paper reviews the key aspects of QML in a comprehensive manner. We discuss different QML algorithms and their domain applicability, quantum datasets, hardware technologies, software tools, simulators, and applications. In this survey, we provide valuable information and resources for readers to jumpstart into the current state-of-the-art techniques in the QML field.
</details>
<details>
<summary>摘要</summary>
量子计算（QC）宣称可以提高解决复杂问题的效率，相比于经典计算。当QC应用于机器学习（ML）应用时，它形成了量子机器学习（QML）系统。本文详细介绍了QML的关键方面，包括不同的QML算法和它们的领域应用、量子数据集、硬件技术、软件工具、模拟器和应用。本文提供了读者们进入现有技术领域的价值信息和资源，以便他们可以快速掌握当前领域的最新技术。
</details></li>
</ul>
<hr>
<h2 id="Transparent-Anomaly-Detection-via-Concept-based-Explanations"><a href="#Transparent-Anomaly-Detection-via-Concept-based-Explanations" class="headerlink" title="Transparent Anomaly Detection via Concept-based Explanations"></a>Transparent Anomaly Detection via Concept-based Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10702">http://arxiv.org/abs/2310.10702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laya Rafiee Sevyeri, Ivaxi Sheth, Farhood Farahnak, Shirin Abbasinejad Enger</li>
<li>for: 本文提出了一种可解释的异常检测方法，以提高异常检测的可读性和人类可解释性。</li>
<li>methods: 本文使用了一种基于概念学习的异常检测方法，可以提供人类可解释的概念解释。此外，本文还提出了一种可与其他分类型异常检测方法集成的概念学习方法。</li>
<li>results: 本文通过三个实际数据集的实验表明，ACE方法可以提供高或相当于黑色盒模型的准确率，同时具有人类可解释的优势。<details>
<summary>Abstract</summary>
Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic datasets - bird classification on CUB-200-2011, challenging histopathology slide image classification on TIL-WSI-TCGA, and gender classification on CelebA. We further demonstrate that our concept learning paradigm can be seamlessly integrated with other classification-based AD methods.
</details>
<details>
<summary>摘要</summary>
深度学习技术的进步使得异常检测性能得到了提高。然而，实际应用中需要更进一步的透明度和理解，而不仅仅是精度。异常检测任务的目标是判断给定样本是否遵循学习的分布。现有方法缺乏对结果的解释能力。因此，我们提出了透明异常检测概念解释（ACE）。ACE可以提供人类可读解释，并且与异常预测一起提供概念。根据我们所知，这是第一篇提出可解释的异常检测方法。此外，ACE还允许人机交互，从而提高了异常检测的效iveness。我们的提议的模型在三个实际数据集上进行验证：鸟类分类在CUB-200-2011上， histopathology slice image分类在TIL-WSI-TCGA上，以及性别分类在CelebA上。此外，我们还证明了我们的概念学习方法可以与其他分类型异常检测方法一起兼容。
</details></li>
</ul>
<hr>
<h2 id="Time-integration-schemes-based-on-neural-networks-for-solving-partial-differential-equations-on-coarse-grids"><a href="#Time-integration-schemes-based-on-neural-networks-for-solving-partial-differential-equations-on-coarse-grids" class="headerlink" title="Time integration schemes based on neural networks for solving partial differential equations on coarse grids"></a>Time integration schemes based on neural networks for solving partial differential equations on coarse grids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10308">http://arxiv.org/abs/2310.10308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinxin Yan, Zhideng Zhou, Xiaohan Cheng, Xiaolei Yang</li>
<li>for: 本研究旨在提出一种基于神经网络的时间步长学习方法，以满足不同数学条件的需求。</li>
<li>methods: 本研究使用神经网络学习3步线性多步法，并应用到了三个模拟问题中，即一维热方程、一维波方程和一维吸引方程。</li>
<li>results: 结果显示，学习的完全约束方法的预测误差与Runge-Kutta方法和Adams-Bashforth方法的预测误差几乎相同。相比传统方法，学习的无约束和半约束方法在粗网格上显著减少预测误差，特别是对一维热方程的温度预测有显著改善。在4倍粗网格上，一些热方程的 casos 的 Mean Square Error 可以减少一个数量级，而波方程的预测相比传统方法具有明显的改善。在32倍粗网格上，Burgers方程的 Mean Square Error 可以减少35%-40%。<details>
<summary>Abstract</summary>
The accuracy of solving partial differential equations (PDEs) on coarse grids is greatly affected by the choice of discretization schemes. In this work, we propose to learn time integration schemes based on neural networks which satisfy three distinct sets of mathematical constraints, i.e., unconstrained, semi-constrained with the root condition, and fully-constrained with both root and consistency conditions. We focus on the learning of 3-step linear multistep methods, which we subsequently applied to solve three model PDEs, i.e., the one-dimensional heat equation, the one-dimensional wave equation, and the one-dimensional Burgers' equation. The results show that the prediction error of the learned fully-constrained scheme is close to that of the Runge-Kutta method and Adams-Bashforth method. Compared to the traditional methods, the learned unconstrained and semi-constrained schemes significantly reduce the prediction error on coarse grids. On a grid that is 4 times coarser than the reference grid, the mean square error shows a reduction of up to an order of magnitude for some of the heat equation cases, and a substantial improvement in phase prediction for the wave equation. On a 32 times coarser grid, the mean square error for the Burgers' equation can be reduced by up to 35% to 40%.
</details>
<details>
<summary>摘要</summary>
“对于半精簇方程（PDEs）的粗糙网格解决方法，选择精度方法的选择对准精度有着很大的影响。在这项工作中，我们提出了基于神经网络的时间拟合方法，满足三种不同的数学约束，即无约束、半约束（根条件）和完全约束（根条件和一致性条件）。我们主要关注了学习3步线性多步法，并将其应用于解决三个模型PDE中的一维热方程、一维波方程和一维布尔格方程。结果表明，学习的完全约束方法的预测误差与Runge-Kutta方法和Adams-Bashforth方法几乎相同。相比传统方法，学习无约束和半约束方法在粗糙网格上显著减少预测误差。在参照网格4倍粗的情况下，一些热方程的 случа在下面可以减少至次之 Magnitude 的误差，而波方程的预测也有显著改善。在32倍粗网格上，布尔格方程的误差可以减少35%-40%。”
</details></li>
</ul>
<hr>
<h2 id="Mimicking-the-Maestro-Exploring-the-Efficacy-of-a-Virtual-AI-Teacher-in-Fine-Motor-Skill-Acquisition"><a href="#Mimicking-the-Maestro-Exploring-the-Efficacy-of-a-Virtual-AI-Teacher-in-Fine-Motor-Skill-Acquisition" class="headerlink" title="Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in Fine Motor Skill Acquisition"></a>Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in Fine Motor Skill Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10280">http://arxiv.org/abs/2310.10280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadar Mulian, Segev Shlomov, Lior Limonad</li>
<li>for: 这项研究旨在探讨人工智能教师模型在促进细动技能学习中的潜在优势，以提高学习效率和学习结果的一致性。</li>
<li>methods: 该研究采用了人工智能学习和仿真学习方法，通过模拟教师与学生之间的互动来评估人工智能教师模型的效果。</li>
<li>results: 研究发现，使用人工智能教师模型可以提高学习效率和学习结果的一致性，并且可以适应不同的学生和学习环境。<details>
<summary>Abstract</summary>
Motor skills, especially fine motor skills like handwriting, play an essential role in academic pursuits and everyday life. Traditional methods to teach these skills, although effective, can be time-consuming and inconsistent. With the rise of advanced technologies like robotics and artificial intelligence, there is increasing interest in automating such teaching processes using these technologies, via human-robot and human-computer interactions. In this study, we examine the potential of a virtual AI teacher in emulating the techniques of human educators for motor skill acquisition. We introduce an AI teacher model that captures the distinct characteristics of human instructors. Using a Reinforcement Learning environment tailored to mimic teacher-learner interactions, we tested our AI model against four guiding hypotheses, emphasizing improved learner performance, enhanced rate of skill acquisition, and reduced variability in learning outcomes. Our findings, validated on synthetic learners, revealed significant improvements across all tested hypotheses. Notably, our model showcased robustness across different learners and settings and demonstrated adaptability to handwriting. This research underscores the potential of integrating Reinforcement Learning and Imitation Learning models with robotics in revolutionizing the teaching of critical motor skills.
</details>
<details>
<summary>摘要</summary>
motor skills, especially fine motor skills like handwriting, play a crucial role in academic pursuits and everyday life. traditional methods to teach these skills, although effective, can be time-consuming and inconsistent. with the rise of advanced technologies like robotics and artificial intelligence, there is increasing interest in automating such teaching processes using these technologies, via human-robot and human-computer interactions. in this study, we examine the potential of a virtual AI teacher in emulating the techniques of human educators for motor skill acquisition. we introduce an AI teacher model that captures the distinct characteristics of human instructors. using a reinforcement learning environment tailored to mimic teacher-learner interactions, we tested our AI model against four guiding hypotheses, emphasizing improved learner performance, enhanced rate of skill acquisition, and reduced variability in learning outcomes. our findings, validated on synthetic learners, revealed significant improvements across all tested hypotheses. notably, our model showcased robustness across different learners and settings and demonstrated adaptability to handwriting. this research underscores the potential of integrating reinforcement learning and imitation learning models with robotics in revolutionizing the teaching of critical motor skills.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-heterogeneous-spillover-effects-in-maximizing-contextual-bandit-rewards"><a href="#Leveraging-heterogeneous-spillover-effects-in-maximizing-contextual-bandit-rewards" class="headerlink" title="Leveraging heterogeneous spillover effects in maximizing contextual bandit rewards"></a>Leveraging heterogeneous spillover effects in maximizing contextual bandit rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10259">http://arxiv.org/abs/2310.10259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Sayeed Faruk, Elena Zheleva</li>
<li>for: 提高个性化推荐的相关性和准确性</li>
<li>methods: 利用多重环境抽象和个性化投资策略考虑用户之间的协同影响</li>
<li>results: 比现有方法高得多，可以更好地满足用户的需求和期望Here’s a brief explanation of each point:</li>
<li>for: The paper aims to improve the relevance and accuracy of personalized recommendations by taking into account the interdependent relationships between users.</li>
<li>methods: The proposed method uses a multi-armed bandit framework to model the interactions between users and the items they interact with, and incorporates heterogeneous spillover effects to better capture the impact of one user’s actions on others.</li>
<li>results: The proposed method outperforms existing approaches that ignore spillover effects, achieving significantly higher rewards in several real-world datasets.<details>
<summary>Abstract</summary>
Recommender systems relying on contextual multi-armed bandits continuously improve relevant item recommendations by taking into account the contextual information. The objective of these bandit algorithms is to learn the best arm (i.e., best item to recommend) for each user and thus maximize the cumulative rewards from user engagement with the recommendations. However, current approaches ignore potential spillover between interacting users, where the action of one user can impact the actions and rewards of other users. Moreover, spillover may vary for different people based on their preferences and the closeness of ties to other users. This leads to heterogeneity in the spillover effects, i.e., the extent to which the action of one user can impact the action of another. Here, we propose a framework that allows contextual multi-armed bandits to account for such heterogeneous spillovers when choosing the best arm for each user. By experimenting on several real-world datasets using prominent linear and non-linear contextual bandit algorithms, we observe that our proposed method leads to significantly higher rewards than existing solutions that ignore spillover.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: recommender systems 使用 contextual multi-armed bandits 不断提高相关的 item 推荐，通过考虑 contextual 信息来学习每个用户最佳的 arm (即最佳推荐)，以达到用户参与推荐的累积奖励的最大化。然而，当前的方法忽略了用户之间的互动副作用，即一个用户的行为会影响另一个用户的行为和奖励。此外，这种副作用可能因用户的偏好和与其他用户之间的关系而异常，即副作用的强度不同。为此，我们提出了一个框架，使得 contextual multi-armed bandits 能够考虑这种异常的副作用，以便为每个用户选择最佳的 arm。通过在一些真实世界数据上使用许多知名的线性和非线性 contextual bandit 算法进行实验，我们发现，我们提出的方法可以与忽略副作用的方法相比，获得更高的奖励。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Topological-Maps-in-Deep-Reinforcement-Learning-for-Multi-Object-Navigation"><a href="#Leveraging-Topological-Maps-in-Deep-Reinforcement-Learning-for-Multi-Object-Navigation" class="headerlink" title="Leveraging Topological Maps in Deep Reinforcement Learning for Multi-Object Navigation"></a>Leveraging Topological Maps in Deep Reinforcement Learning for Multi-Object Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10250">http://arxiv.org/abs/2310.10250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Hakenes, Tobias Glasmachers</li>
<li>for: 解决扩展空间 navigate 极少奖励问题</li>
<li>methods: 使用 topological maps 提升 elementary actions 到 object-oriented macro actions</li>
<li>results: 使用 DQN agent 解决 otherwise 不可能的环境<details>
<summary>Abstract</summary>
This work addresses the challenge of navigating expansive spaces with sparse rewards through Reinforcement Learning (RL). Using topological maps, we elevate elementary actions to object-oriented macro actions, enabling a simple Deep Q-Network (DQN) agent to solve otherwise practically impossible environments.
</details>
<details>
<summary>摘要</summary>
这个工作面临了在广阔空间中缺乏奖励的挑战，通过再增 learning (RL) 方法解决。我们使用 topological maps，将基本的动作提升到对象层次的macro动作，使得简单的深度Q网络 (DQN)  Agent 能够解决 otherwise 不可能的环境。
</details></li>
</ul>
<hr>
<h2 id="The-Mixtures-and-the-Neural-Critics-On-the-Pointwise-Mutual-Information-Profiles-of-Fine-Distributions"><a href="#The-Mixtures-and-the-Neural-Critics-On-the-Pointwise-Mutual-Information-Profiles-of-Fine-Distributions" class="headerlink" title="The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions"></a>The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10240">http://arxiv.org/abs/2310.10240</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cbg-ethz/bmi">https://github.com/cbg-ethz/bmi</a></li>
<li>paper_authors: Paweł Czyż, Frederic Grabowski, Julia E. Vogt, Niko Beerenwinkel, Alexander Marx</li>
<li>for: 这个论文研究了点wise矩阵相互信息的profile，这是矩阵相互信息的一种扩展，它保持了 diffeomorphisms 的变换不变性。</li>
<li>methods: 论文使用了 Monte Carlo 方法来近似 multivariate normal distributions 的profile，并 introduce了 fine distributions 家族，可以用来研究现有的矩阵相互信息估计器的局限性，以及 neural critics 在variational estimators 中的行为。</li>
<li>results: 论文显示了 fine distributions 可以用来研究矩阵相互信息估计器的局限性，以及 neural critics 的行为，并可以用来获得model-based Bayesian 矩阵相互信息估计，适用于具有可用的领域专业知识的问题，在哪里 uncertainty quantification 是必要的。<details>
<summary>Abstract</summary>
Mutual information quantifies the dependence between two random variables and remains invariant under diffeomorphisms. In this paper, we explore the pointwise mutual information profile, an extension of mutual information that maintains this invariance. We analytically describe the profiles of multivariate normal distributions and introduce the family of fine distributions, for which the profile can be accurately approximated using Monte Carlo methods. We then show how fine distributions can be used to study the limitations of existing mutual information estimators, investigate the behavior of neural critics used in variational estimators, and understand the effect of experimental outliers on mutual information estimation. Finally, we show how fine distributions can be used to obtain model-based Bayesian estimates of mutual information, suitable for problems with available domain expertise in which uncertainty quantification is necessary.
</details>
<details>
<summary>摘要</summary>
互信息量量化了两个随机变量之间的依赖关系，并保持不变于 diffeomorphisms。在这篇文章中，我们探讨了点 wise 互信息 Profile，是互信息的扩展，保持这种不变性。我们 analytically 描述了多变量正态分布的 Profile，并引入了 fine 分布家族，其中 profile 可以使用 Monte Carlo 方法准确地 approximation。然后，我们示示了 fine 分布可以用来研究现有互信息估计器的限制，调查变量批评器在variational estimator中的行为，并理解试验异常点对互信息估计的影响。最后，我们示示了 fine 分布可以用来获得基于模型的 Bayesian 估计，适用于具有可用的领域专业知识的问题，在uncertainty quantification中需要。
</details></li>
</ul>
<hr>
<h2 id="Structural-transfer-learning-of-non-Gaussian-DAG"><a href="#Structural-transfer-learning-of-non-Gaussian-DAG" class="headerlink" title="Structural transfer learning of non-Gaussian DAG"></a>Structural transfer learning of non-Gaussian DAG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10239">http://arxiv.org/abs/2310.10239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyang Ren, Xin He, Junhui Wang</li>
<li>for:  targets to improve the reconstruction of directional relationships among nodes in a directed acyclic graph (DAG) using heterogeneous data from multiple studies.</li>
<li>methods:  proposes a novel set of structural similarity measures for DAG and a transfer DAG learning framework that leverages information from auxiliary DAGs of different levels of similarities.</li>
<li>results:  substantial improvement in DAG reconstruction in the target study, even when no auxiliary DAG is overall similar to the target DAG, and supported by extensive numerical experiments on both synthetic data and multi-site brain functional connectivity network data.Here’s the full translation in Simplified Chinese:</li>
<li>for: 本研究目的是提高基于多个研究中收集的不同数据的指向关系图（DAG）的重建精度。</li>
<li>methods: 提出了一种新的结构相似度测量方法，并提出了一种基于不同相似度水平的转移DAG学习框架，以有效地利用 auxiliary DAGs 中的信息。</li>
<li>results: 对目标研究中的 DAGC 重建具有重要提高，即使 auxiliary DAG 与目标 DAGC 无法总体相似，而且通过对 sintetic 数据和多地点大脑功能连接网络数据进行广泛的数值实验支持。<details>
<summary>Abstract</summary>
Directed acyclic graph (DAG) has been widely employed to represent directional relationships among a set of collected nodes. Yet, the available data in one single study is often limited for accurate DAG reconstruction, whereas heterogeneous data may be collected from multiple relevant studies. It remains an open question how to pool the heterogeneous data together for better DAG structure reconstruction in the target study. In this paper, we first introduce a novel set of structural similarity measures for DAG and then present a transfer DAG learning framework by effectively leveraging information from auxiliary DAGs of different levels of similarities. Our theoretical analysis shows substantial improvement in terms of DAG reconstruction in the target study, even when no auxiliary DAG is overall similar to the target DAG, which is in sharp contrast to most existing transfer learning methods. The advantage of the proposed transfer DAG learning is also supported by extensive numerical experiments on both synthetic data and multi-site brain functional connectivity network data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GEVO-ML-Optimizing-Machine-Learning-Code-with-Evolutionary-Computation"><a href="#GEVO-ML-Optimizing-Machine-Learning-Code-with-Evolutionary-Computation" class="headerlink" title="GEVO-ML: Optimizing Machine Learning Code with Evolutionary Computation"></a>GEVO-ML: Optimizing Machine Learning Code with Evolutionary Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10211">http://arxiv.org/abs/2310.10211</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jhe-Yu Liou, Stephanie Forrest, Carole-Jean Wu</li>
<li>for: 本研究旨在提高大规模机器学习（ML）应用中的并行加速器（如GPU）的性能，但 ML 模型开发者通常缺乏关于下游系统架构的详细知识，而系统编程者则通常没有高级别的 ML 模型的理解。</li>
<li>methods: 本研究提出了 GEVO-ML，一种自动发现优化机会并调整 ML kernels性能的工具，其中 ML 模型和训练&#x2F;预测过程都是通过单一高级表示语言（多层次中间表示语言，MLIR）表示的。GEVO-ML 使用多目标进化搜索发现 MLIR 代码中的修改（突变），以提高 Desired riteria 的性能，保留必要的功能。</li>
<li>results: 对两个不同的 ML 工作负荷进行了训练和预测。GEVO-ML 在这两个模型中发现了显著的 Pareto 提升，提高了模型精度的误差率从 2% 下降到 90.43%，并在训练工作负荷中提高了模型精度从 91% 到 96%，无需牺牲训练或测试速度。分析表明，GEVO-ML 的关键突变包括多种code修改，尽管可能不familiar with human developers，但它们实现了类似于人类开发者在模型设计中进行的改进，例如更改学习率或者修剪不必要的层参数。<details>
<summary>Abstract</summary>
Parallel accelerators, such as GPUs, are key enablers for large-scale Machine Learning (ML) applications. However, ML model developers often lack detailed knowledge of the underlying system architectures, while system programmers usually do not have a high-level understanding of the ML model that runs on the specific system. To mitigate this gap between two relevant aspects of domain knowledge, this paper proposes GEVO-ML, a tool for automatically discovering optimization opportunities and tuning the performance of ML kernels, where the model and training/prediction processes are uniformly represented in a single intermediate language, the Multiple-Layer Intermediate Representation (MLIR). GEVO-ML uses multi-objective evolutionary search to find edits (mutations) to MLIR code that ultimately runs on GPUs, improving performance on desired criteria while retaining required functionality.   We demonstrate GEVO-ML on two different ML workloads for both model training and prediction. GEVO-ML finds significant Pareto improvements for these models, achieving 90.43% performance improvement when model accuracy is relaxed by 2%, from 91.2% to 89.3%. For the training workloads, GEVO-ML finds a 4.88% improvement in model accuracy, from 91% to 96%, without sacrificing training or testing speed. Our analysis of key GEVO-ML mutations reveals diverse code modifications, while might be foreign to human developers, achieving similar effects with how human developers improve model design, for example, by changing learning rates or pruning non-essential layer parameters.
</details>
<details>
<summary>摘要</summary>
高级加速器，如图形处理器（GPU），是大规模机器学习（ML）应用的关键驱动器。然而，ML模型开发者经常缺乏深入的系统架构知识，而系统编程者通常没有高级的ML模型的具体知识。为了 bridge这两个领域的知识差距，这篇论文提出了 GEVO-ML，一种自动发现优化机会并调整 ML kernels的工具。GEVO-ML 使用多目标进化搜索来找到 MLIR 代码中的修改（突变），以提高 Desired 特性的性能，保留必要的功能。我们在两个不同的 ML 任务上运行 GEVO-ML，包括模型训练和预测。GEVO-ML 在这些模型上发现了显著的 pareto 改进，将模型精度从 91.2% 下降到 89.3%，同时提高了性能。对于训练任务，GEVO-ML 提高了模型精度从 91% 到 96%，而无需牺牲训练或测试速度。我们分析了 GEVO-ML 中关键的突变，发现这些突变可能 foreign 于人类开发者，但具有类似的效果，例如更改学习率或减少不必要的层参数。
</details></li>
</ul>
<hr>
<h2 id="Bongard-OpenWorld-Few-Shot-Reasoning-for-Free-form-Visual-Concepts-in-the-Real-World"><a href="#Bongard-OpenWorld-Few-Shot-Reasoning-for-Free-form-Visual-Concepts-in-the-Real-World" class="headerlink" title="Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World"></a>Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10207">http://arxiv.org/abs/2310.10207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joyjayng/Bongard-OpenWorld">https://github.com/joyjayng/Bongard-OpenWorld</a></li>
<li>paper_authors: Rujie Wu, Xiaojian Ma, Qing Li, Wei Wang, Zhenliang Zhang, Song-Chun Zhu, Yizhou Wang</li>
<li>for: 评估现实世界中的几何shot理解能力，即通过几何shot的图像训练模型可以在新的图像中理解和分类图像。</li>
<li>methods: 使用经典的Bongard问题（BPs）作为基础，并添加两种新的挑战：1）开放世界自由形容符，即图像概念由开放词汇中的图像特征和概念组成，2）使用实际世界图像而非 sintetic 图像。</li>
<li>results: 研究发现，当前的几何shot理解算法面临 significiant 挑战，而且even irectly  probing VLMs 和 combining VLMs 和 LLMs 在交互理解方案中，无法距离人类的问题解决能力（64% 准确率，而人类参与者可以达到 91%）。<details>
<summary>Abstract</summary>
We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even designed a neuro-symbolic reasoning approach that reconciles LLMs & VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.
</details>
<details>
<summary>摘要</summary>
我们介绍了一个新的评价实际世界几何reasoning的benchmark，即Bongard-OpenWorld。它基于经典的Bongard问题（BP），要求模型通过引入视觉概念，将查询图像分配到正确的集合中。我们的benchmark继承了原BP的几何概念引入，并添加了两个新的挑战：1）开放世界自由形态概念，视觉概念在Bongard-OpenWorld中是独特的词汇库中的组合，范围从物体类到抽象视觉特征和常识知识; 2）实际图像，而不是许多同类的synthetic图像。在我们的探索中，Bongard-OpenWorld已经对当前几何reasoning算法带来了 significativetranslation challenges。我们进一步调查了current Large Language Models (LLMs)和Vision-Language Models (VLMs)是否可以解决我们的任务，直接考试VLMs，并将VLMs和LLMs结合在互动理解方案中。我们甚至设计了一种神经符号逻辑 reasoningapproach，将LLMs & VLMs与逻辑逻辑 reasoning相结合，以便模拟人类问题解决过程。然而， none of these approaches manage to close the human-machine gap，best learner的准确率只有64%，而人类参与者容易达到91%。我们希望Bongard-OpenWorld可以帮助我们更好地理解当前视觉智能的局限性，并促进未来的视觉代理人with stronger few-shot visual reasoning能力的研究。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Predictive-Models-to-Understand-Risk-Factors-for-Maternal-and-Fetal-Outcomes"><a href="#Interpretable-Predictive-Models-to-Understand-Risk-Factors-for-Maternal-and-Fetal-Outcomes" class="headerlink" title="Interpretable Predictive Models to Understand Risk Factors for Maternal and Fetal Outcomes"></a>Interpretable Predictive Models to Understand Risk Factors for Maternal and Fetal Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10203">http://arxiv.org/abs/2310.10203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomas M. Bosschieter, Zifei Xu, Hui Lan, Benjamin J. Lengerich, Harsha Nori, Ian Painter, Vivienne Souter, Rich Caruana</li>
<li>for: 这个论文旨在提高妈妈和婴儿的健康，通过更好地理解风险因素，加强高风险患者的监测，及时采取有效措施，以便妈妈医生能够提供更好的照料。</li>
<li>methods: 这篇论文使用了可解释扩展机器学习方法（EBM）进行预测和重要风险因素的 indentification。EBM具有高准确率和可解释性，并且在验证和稳定性分析中证明了其可靠性。</li>
<li>results: 研究发现，EBM模型可以准确预测四种妈妈和婴儿的病情，并且可以提供有价值的风险因素。例如， maternal height 是Shoulder dystocia 的第二重要风险因素。这些结果表明，EBM模型在预测和预防妈妈和婴儿的严重病情中具有优秀的性能和可解释性。<details>
<summary>Abstract</summary>
Although most pregnancies result in a good outcome, complications are not uncommon and can be associated with serious implications for mothers and babies. Predictive modeling has the potential to improve outcomes through better understanding of risk factors, heightened surveillance for high risk patients, and more timely and appropriate interventions, thereby helping obstetricians deliver better care. We identify and study the most important risk factors for four types of pregnancy complications: (i) severe maternal morbidity, (ii) shoulder dystocia, (iii) preterm preeclampsia, and (iv) antepartum stillbirth. We use an Explainable Boosting Machine (EBM), a high-accuracy glass-box learning method, for prediction and identification of important risk factors. We undertake external validation and perform an extensive robustness analysis of the EBM models. EBMs match the accuracy of other black-box ML methods such as deep neural networks and random forests, and outperform logistic regression, while being more interpretable. EBMs prove to be robust. The interpretability of the EBM models reveals surprising insights into the features contributing to risk (e.g. maternal height is the second most important feature for shoulder dystocia) and may have potential for clinical application in the prediction and prevention of serious complications in pregnancy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Interpretable-Deep-Learning-Framework-for-Predicting-Hospital-Readmissions-From-Electronic-Health-Records"><a href="#An-Interpretable-Deep-Learning-Framework-for-Predicting-Hospital-Readmissions-From-Electronic-Health-Records" class="headerlink" title="An Interpretable Deep-Learning Framework for Predicting Hospital Readmissions From Electronic Health Records"></a>An Interpretable Deep-Learning Framework for Predicting Hospital Readmissions From Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10187">http://arxiv.org/abs/2310.10187</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Azzalini, Tommaso Dolci, Marco Vagaggini</li>
<li>for: 预测医院复 admit 的风险，以降低医疗成本并提高患者健康状况。</li>
<li>methods: 提出了一种新的、可解释的深度学习框架，基于 NLP 发现 word embeddings 和 ConvLSTM 神经网络模型，以更好地处理时间数据。</li>
<li>results: 对医院复 admit 的预测任务进行了 validate，并 introduce 了一种模型依赖的技术来使结果更容易被医疗 personnels 理解。结果比传统基于机器学习的模型提供更好的性能，同时也提供了更加可解释的结果。<details>
<summary>Abstract</summary>
With the increasing availability of patients' data, modern medicine is shifting towards prospective healthcare. Electronic health records contain a variety of information useful for clinical patient description and can be exploited for the construction of predictive models, given that similar medical histories will likely lead to similar progressions. One example is unplanned hospital readmission prediction, an essential task for reducing hospital costs and improving patient health. Despite predictive models showing very good performances especially with deep-learning models, they are often criticized for the poor interpretability of their results, a fundamental characteristic in the medical field, where incorrect predictions might have serious consequences for the patient health. In this paper we propose a novel, interpretable deep-learning framework for predicting unplanned hospital readmissions, supported by NLP findings on word embeddings and by neural-network models (ConvLSTM) for better handling temporal data. We validate our system on the two predictive tasks of hospital readmission within 30 and 180 days, using real-world data. In addition, we introduce and test a model-dependent technique to make the representation of results easily interpretable by the medical staff. Our solution achieves better performances compared to traditional models based on machine learning, while providing at the same time more interpretable results.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "prospective healthcare" is translated as "前瞻医疗" (pre-emptive healthcare)* "electronic health records" is translated as "电子健康记录" (electronic health records)* "clinical patient description" is translated as "临床患者描述" (clinical patient description)* "unplanned hospital readmission" is translated as "不计划入院" (unplanned hospital readmission)* "predictive models" is translated as "预测模型" (predictive models)* "word embeddings" is translated as "词嵌入" (word embeddings)* "ConvLSTM" is translated as "卷积LSTM" (ConvLSTM)* "temporal data" is translated as "时间数据" (temporal data)* "medical staff" is translated as "医疗人员" (medical staff)
</details></li>
</ul>
<hr>
<h2 id="Hypergraph-Echo-State-Network"><a href="#Hypergraph-Echo-State-Network" class="headerlink" title="Hypergraph Echo State Network"></a>Hypergraph Echo State Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10177">http://arxiv.org/abs/2310.10177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Lien</li>
<li>for: 这篇文章是用于描述一种基于几何网络的对数据进行有效处理的网络模型，并且提出了一个基于几何网络的对数状态网络（HypergraphESN）的设计。</li>
<li>methods: 这篇文章使用了一种基于几何网络的对数状态网络（HypergraphESN），并且提出了这个方法的算法和稳定性条件。</li>
<li>results: 数据实验显示，HypergraphESN在处理几何网络结构的数据时，能够与传统的几何网络模型（GraphESN）相比，获得更高的准确率。具体来说，HypergraphESN在处理高阶相互作用的数据时，能够更好地处理非线性特征，并且可以实现更高的准确率。<details>
<summary>Abstract</summary>
A hypergraph as a generalization of graphs records higher-order interactions among nodes, yields a more flexible network model, and allows non-linear features for a group of nodes. In this article, we propose a hypergraph echo state network (HypergraphESN) as a generalization of graph echo state network (GraphESN) designed for efficient processing of hypergraph-structured data, derive convergence conditions for the algorithm, and discuss its versatility in comparison to GraphESN. The numerical experiments on the binary classification tasks demonstrate that HypergraphESN exhibits comparable or superior accuracy performance to GraphESN for hypergraph-structured data, and accuracy increases if more higher-order interactions in a network are identified.
</details>
<details>
<summary>摘要</summary>
一种超графи（hypergraph）是图的扩展，用于记录高阶交互 among nodes，具有更灵活的网络模型，并允许非线性特征 для一组节点。在这篇文章中，我们提议一种基于超графи的响应状态网络（HypergraphESN）作为图响应状态网络（GraphESN）的扩展，用于高效处理超графи结构数据， derivation of convergence conditions for the algorithm, and discussion of its versatility compared to GraphESN. 数值实验表明，对于二分类任务，HypergraphESN可以与GraphESN具有相同或更高的准确率表现，并且如果在网络中更多的高阶交互被标识， то准确率会进一步提高。
</details></li>
</ul>
<hr>
<h2 id="On-permutation-symmetries-in-Bayesian-neural-network-posteriors-a-variational-perspective"><a href="#On-permutation-symmetries-in-Bayesian-neural-network-posteriors-a-variational-perspective" class="headerlink" title="On permutation symmetries in Bayesian neural network posteriors: a variational perspective"></a>On permutation symmetries in Bayesian neural network posteriors: a variational perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10171">http://arxiv.org/abs/2310.10171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Rossi, Ankit Singh, Thomas Hannagan</li>
<li>for: 这项研究旨在理解神经网络中梯度下降优化的困难性, 以及 Bayesian neural networks（BNNs）中 approximate inference 的问题。</li>
<li>methods: 这篇论文使用了 marginalized loss barrier 和 solution interpolation 的扩展 formalism, 以及一种匹配算法来搜索线性连接的解。</li>
<li>results: 实验结果表明, 对于多种架构和数据集, linearly connected solutions 的 marginalized loss barrier 几乎为零。<details>
<summary>Abstract</summary>
The elusive nature of gradient-based optimization in neural networks is tied to their loss landscape geometry, which is poorly understood. However recent work has brought solid evidence that there is essentially no loss barrier between the local solutions of gradient descent, once accounting for weight-permutations that leave the network's computation unchanged. This raises questions for approximate inference in Bayesian neural networks (BNNs), where we are interested in marginalizing over multiple points in the loss landscape. In this work, we first extend the formalism of marginalized loss barrier and solution interpolation to BNNs, before proposing a matching algorithm to search for linearly connected solutions. This is achieved by aligning the distributions of two independent approximate Bayesian solutions with respect to permutation matrices. We build on the results of Ainsworth et al. (2023), reframing the problem as a combinatorial optimization one, using an approximation to the sum of bilinear assignment problem. We then experiment on a variety of architectures and datasets, finding nearly zero marginalized loss barriers for linearly connected solutions.
</details>
<details>
<summary>摘要</summary>
“神经网络中梯度基本优化的难易程度与其损失函数 geometry 存在深刻的关系。然而，最近的研究表明，在考虑权重 Permutation 后，梯度 descend 的本地解决方案之间存在无损函数梯度。这引发了对 approximate inference 在 Bayesian neural networks （BNNs）中进行 marginalization 的问题。在这个工作中，我们首先扩展了 marginalized loss barrier 和 solution interpolation 的形式主义，然后提出了一种匹配算法，用于搜索 linearly connected solutions。这是通过对两个独立的approximate Bayesian解决方案的分布进行对齐，以实现对 permutation matrices 的Alignment。我们基于 Ainsworth et al. (2023) 的结果，重新定义问题为一个 combinatorial optimization 问题，使用一种 Approximation 来计算 bilinear assignment problem 的和。然后我们在不同的架构和数据集上进行了实验，发现linearly connected solutions 的 marginalized loss barrier 几乎为零。”Note: The translation is in Simplified Chinese, which is the standardized form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Simplicial-Representation-Learning-with-Wasserstein-Distance"><a href="#An-Empirical-Study-of-Simplicial-Representation-Learning-with-Wasserstein-Distance" class="headerlink" title="An Empirical Study of Simplicial Representation Learning with Wasserstein Distance"></a>An Empirical Study of Simplicial Representation Learning with Wasserstein Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10143">http://arxiv.org/abs/2310.10143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Makoto Yamada, Yuki Takezawa, Guillaume Houry, Kira Michaela Dusterwald, Deborah Sulem, Han Zhao, Yao-Hung Hubert Tsai</li>
<li>for: 本研究探讨了使用树结构上的1- Wasserstein距离（Tree-Wasserstein distance，TWD）来学习 simplicial 表示，TWD 是两个树嵌入向量之间的L1距离。</li>
<li>methods: 本研究使用了一种基于自动采样的自监学习方法，使用 TWD 作为相似度度量，并提出了一种简单 yet effective的 Jeffrey divergence 基于正则化方法来稳定优化。</li>
<li>results: 通过对 STL10、CIFAR10、CIFAR100 和 SVHN 等数据集进行实验，研究发现，将 softmax 函数和 TWD 组合使用可以获得较低的结果，而且模型性能取决于 TWD 和 simplicial 模型的组合，并且 Jeffrey divergence 正则化通常能够稳定模型训练。最终，研究人员发现了选择合适的 TWD 和 simplicial 模型的组合可以超越cosine similarity 基于表示学习。<details>
<summary>Abstract</summary>
In this paper, we delve into the problem of simplicial representation learning utilizing the 1-Wasserstein distance on a tree structure (a.k.a., Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance between two tree-embedded vectors. Specifically, we consider a framework for simplicial representation estimation employing a self-supervised learning approach based on SimCLR with a negative TWD as a similarity measure. In SimCLR, the cosine similarity with real-vector embeddings is often utilized; however, it has not been well studied utilizing L1-based measures with simplicial embeddings. A key challenge is that training the L1 distance is numerically challenging and often yields unsatisfactory outcomes, and there are numerous choices for probability models. Thus, this study empirically investigates a strategy for optimizing self-supervised learning with TWD and find a stable training procedure. More specifically, we evaluate the combination of two types of TWD (total variation and ClusterTree) and several simplicial models including the softmax function, the ArcFace probability model, and simplicial embedding. Moreover, we propose a simple yet effective Jeffrey divergence-based regularization method to stabilize the optimization. Through empirical experiments on STL10, CIFAR10, CIFAR100, and SVHN, we first found that the simple combination of softmax function and TWD can obtain significantly lower results than the standard SimCLR (non-simplicial model and cosine similarity). We found that the model performance depends on the combination of TWD and the simplicial model, and the Jeffrey divergence regularization usually helps model training. Finally, we inferred that the appropriate choice of combination of TWD and simplicial models outperformed cosine similarity based representation learning.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了使用树结构上的一 Wasserstein 距离（TWD）来学习 simplicial 表示，其中 TWD 定义为两个树嵌入向量之间的 L1 距离。我们考虑了一种基于自适应学习的 simplicial 表示估计方法，使用 SimCLR 自适应学习框架，并使用 TWD 作为相似度量。在 SimCLR 中，通常使用 cosine 相似性来衡量实际向量嵌入，但是使用 L1 基于的度量尚未得到广泛研究。主要挑战在于训练 L1 距离是数值上困难，而且存在多种概率模型选择。因此，我们在这篇论文中进行了实验性的研究，以便在 TWD 和 simplicial 模型之间找到稳定的训练过程。具体来说，我们评估了两种类型的 TWD（总变量和 ClusterTree）以及多种 simplicial 模型，包括软max 函数、ArcFace 概率模型和 simplicial 嵌入。此外，我们还提出了一种简单 yet 有效的 Jeffrey 分布基本规范 regularization 方法，以稳定优化。通过对 STL10、CIFAR10、CIFAR100 和 SVHN 等数据集进行实验，我们发现了以下结论：1. 将 softmax 函数和 TWD 结合使用可以获得显著更好的结果，与标准 SimCLR（非 simplicial 模型和 cosine 相似性）相比。2. 模型性能取决于 TWD 和 simplicial 模型的组合，而 Jeffrey 分布基本规范常常帮助模型训练。3. 选择合适的 TWD 和 simplicial 模型的组合，通常会超越 cosine 相似性基于的表示学习。总之，我们的研究表明，使用 TWD 和 simplicial 模型可以提高表示学习的性能，并且可以选择合适的组合来超越 cosine 相似性基于的表示学习。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-Privacy-Risks-in-Curriculum-Learning"><a href="#A-Comprehensive-Study-of-Privacy-Risks-in-Curriculum-Learning" class="headerlink" title="A Comprehensive Study of Privacy Risks in Curriculum Learning"></a>A Comprehensive Study of Privacy Risks in Curriculum Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10124">http://arxiv.org/abs/2310.10124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joann Qiongna Chen, Xinlei He, Zheng Li, Yang Zhang, Zhou Li</li>
<li>for: 本研究旨在探讨curriculum learning（CL）对机器学习的隐私影响，以填补现有的知识空白。</li>
<li>methods: 我们使用了membership inference attack（MIA）和attribute inference attack（AIA）两种方法来衡量CL对隐私的泄露。</li>
<li>results: 我们的evalution结果显示，MIA在CL下变得slightly more effective，但它的影响尤其明显于difficult sample subset。AIA对CL下的模型比MIA更加敏感，而exististing defense techniques仍然有效。此外，我们还提出了一种新的MIA方法，称为Diff-Cali，它基于difficulty scores进行结果准确化。<details>
<summary>Abstract</summary>
Training a machine learning model with data following a meaningful order, i.e., from easy to hard, has been proven to be effective in accelerating the training process and achieving better model performance. The key enabling technique is curriculum learning (CL), which has seen great success and has been deployed in areas like image and text classification. Yet, how CL affects the privacy of machine learning is unclear. Given that CL changes the way a model memorizes the training data, its influence on data privacy needs to be thoroughly evaluated. To fill this knowledge gap, we perform the first study and leverage membership inference attack (MIA) and attribute inference attack (AIA) as two vectors to quantify the privacy leakage caused by CL.   Our evaluation of nine real-world datasets with attack methods (NN-based, metric-based, label-only MIA, and NN-based AIA) revealed new insights about CL. First, MIA becomes slightly more effective when CL is applied, but the impact is much more prominent to a subset of training samples ranked as difficult. Second, a model trained under CL is less vulnerable under AIA, compared to MIA. Third, the existing defense techniques like DP-SGD, MemGuard, and MixupMMD are still effective under CL, though DP-SGD has a significant impact on target model accuracy. Finally, based on our insights into CL, we propose a new MIA, termed Diff-Cali, which exploits the difficulty scores for result calibration and is demonstrated to be effective against all CL methods and the normal training method. With this study, we hope to draw the community's attention to the unintended privacy risks of emerging machine-learning techniques and develop new attack benchmarks and defense solutions.
</details>
<details>
<summary>摘要</summary>
通过训练机器学习模型使用meaningful order的数据，即从易到难，已经证明可以加速训练过程并提高模型性能。关键技术是curriculum learning（CL），已经在图像和文本分类等领域取得了很大成功。然而，CL对机器学习的隐私影响是不清楚。因为CL改变了模型对训练数据的记忆方式，因此其对隐私的影响需要进行仔细评估。为了填补这个知识空白，我们进行了第一个研究，并利用成员推理攻击（MIA）和特征推理攻击（AIA）作为两种量度CL对隐私的泄露的方法。我们对九个实际 datasets进行了评估，并使用NN-based、metric-based、label-only MIA和NN-based AIA等方法进行攻击。我们发现了以下新的发现：1. MIA在CL应用后变得略微更加有效，但对于一些训练样本 ranked as difficult 的影响更加明显。2. 一个CL训练的模型对AIA更加抵触，相比于MIA。3. 现有的防御技术如DP-SGD、MemGuard和MixupMMD仍然有效于CL，尽管DP-SGD对目标模型准确率有显著影响。4. 基于我们对CL的发现，我们提出了一种新的MIA，称为Diff-Cali，它利用难度分数进行结果准确性的调整，并证明可以有效地对CL方法和常规训练方法进行攻击。通过这项研究，我们希望能吸引社区关注机器学习领域的意外隐私风险，并开发新的攻击 benchmark和防御解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-proximal-augmented-Lagrangian-based-algorithm-for-federated-learning-with-global-and-local-convex-conic-constraints"><a href="#A-proximal-augmented-Lagrangian-based-algorithm-for-federated-learning-with-global-and-local-convex-conic-constraints" class="headerlink" title="A proximal augmented Lagrangian based algorithm for federated learning with global and local convex conic constraints"></a>A proximal augmented Lagrangian based algorithm for federated learning with global and local convex conic constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10117">http://arxiv.org/abs/2310.10117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuan He, Le Peng, Ju Sun</li>
<li>for: 本研究针对 federated learning (FL) with constraints 进行研究，实现了在中央服务器和所有本地客户端之间收集数据，并实现了模型训练。</li>
<li>methods: 本研究提出了一个基于 proximal augmented Lagrangian (AL) 的 federated learning 框架，并使用了各种对数方法来解决问题。</li>
<li>results: 本研究的实验结果显示了这个方法在 Neyman-Pearson 分类和模型公平性方面的实际优势。 另外，本研究还提出了一个新的 federated learning 框架，具有全球和本地凸对数约束的特点。<details>
<summary>Abstract</summary>
This paper considers federated learning (FL) with constraints, where the central server and all local clients collectively minimize a sum of convex local objective functions subject to global and local convex conic constraints. To train the model without moving local data from clients to the central server, we propose an FL framework in which each local client performs multiple updates using the local objective and local constraint, while the central server handles the global constraint and performs aggregation based on the updated local models. In particular, we develop a proximal augmented Lagrangian (AL) based algorithm for FL with global and local convex conic constraints. The subproblems arising in this algorithm are solved by an inexact alternating direction method of multipliers (ADMM) in a federated fashion. Under a local Lipschitz condition and mild assumptions, we establish the worst-case complexity bounds of the proposed algorithm for finding an approximate KKT solution. To the best of our knowledge, this work proposes the first algorithm for FL with global and local constraints. Our numerical experiments demonstrate the practical advantages of our algorithm in performing Neyman-Pearson classification and enhancing model fairness in the context of FL.
</details>
<details>
<summary>摘要</summary>
We develop a proximal augmented Lagrangian (AL) based algorithm for FL with global and local convex conic constraints. The subproblems arising in this algorithm are solved using an inexact alternating direction method of multipliers (ADMM) in a federated fashion. Under local Lipschitz conditions and mild assumptions, we establish the worst-case complexity bounds of the proposed algorithm for finding an approximate KKT solution.To the best of our knowledge, this work proposes the first algorithm for FL with global and local constraints. Our numerical experiments demonstrate the practical advantages of our algorithm in performing Neyman-Pearson classification and enhancing model fairness in the context of FL.Here's the Simplified Chinese translation:这篇论文研究了基于约束的联合学习（Federated Learning，FL），其中中央服务器和所有本地客户端共同减少一个拥有 convex 本地目标函数的总和，同时遵循全局和本地 convex 凹陷约束。为了不让本地数据从客户端传输到中央服务器，我们提议了一种基于 FL 的框架，其中每个本地客户端可以多次使用本地目标和约束进行更新，而中央服务器则负责全局约束并基于更新后的本地模型进行聚合。我们开发了一种基于 proximal augmented Lagrangian（AL）的算法，用于解决 FL 中的全局和本地 convex 凹陷约束问题。这些子问题在我们的算法中使用了一种不精确的 alternating direction method of multipliers（ADMM）来解决。在本地 Lipschitz 条件和某些假设下，我们确定了我们提议的算法的最坏情况复杂性 bound。根据我们所知，这是第一个基于 FL 的全局和本地约束算法。我们的数据实验表明，我们的算法在 Neyman-Pearson 分类和 Federation Learning 中的实际优势。
</details></li>
</ul>
<hr>
<h2 id="PAC-Learning-Linear-Thresholds-from-Label-Proportions"><a href="#PAC-Learning-Linear-Thresholds-from-Label-Proportions" class="headerlink" title="PAC Learning Linear Thresholds from Label Proportions"></a>PAC Learning Linear Thresholds from Label Proportions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10098">http://arxiv.org/abs/2310.10098</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Rishi Saket, Aravindan Raghuveer</li>
<li>for: 本研究旨在学习从标签分布中提取信息，特别是在标签分布中存在噪声和不确定性的情况下。</li>
<li>methods: 本研究使用了一种基于 Gaussian distribution 的方法，使用随机抽样来估算标签分布的方差矩阵，并使用这个矩阵来定义一个特征向量空间中的正常向量。然后，使用这个正常向量来定义一个线性阈值函数（LTF），并使用这个 LTF 来学习实例分类器。</li>
<li>results: 本研究表明，使用这种方法可以高效地学习 LTF，并且可以在标签分布中存在噪声和不确定性的情况下提取有用的信息。此外，研究还提供了一些总体错误 bounds 和特性分布 bounds，以确保学习的准确性和稳定性。实验评估表明，本方法可以与 [Saket’21, Saket’22] 等方法相比，并且在某些特殊情况下可以提供更高的准确性。<details>
<summary>Abstract</summary>
Learning from label proportions (LLP) is a generalization of supervised learning in which the training data is available as sets or bags of feature-vectors (instances) along with the average instance-label of each bag. The goal is to train a good instance classifier. While most previous works on LLP have focused on training models on such training data, computational learnability of LLP was only recently explored by [Saket'21, Saket'22] who showed worst case intractability of properly learning linear threshold functions (LTFs) from label proportions. However, their work did not rule out efficient algorithms for this problem on natural distributions.   In this work we show that it is indeed possible to efficiently learn LTFs using LTFs when given access to random bags of some label proportion in which feature-vectors are, conditioned on their labels, independently sampled from a Gaussian distribution $N(\mathbf{\mu}, \mathbf{\Sigma})$. Our work shows that a certain matrix -- formed using covariances of the differences of feature-vectors sampled from the bags with and without replacement -- necessarily has its principal component, after a transformation, in the direction of the normal vector of the LTF. Our algorithm estimates the means and covariance matrices using subgaussian concentration bounds which we show can be applied to efficiently sample bags for approximating the normal direction. Using this in conjunction with novel generalization error bounds in the bag setting, we show that a low error hypothesis LTF can be identified. For some special cases of the $N(\mathbf{0}, \mathbf{I})$ distribution we provide a simpler mean estimation based algorithm. We include an experimental evaluation of our learning algorithms along with a comparison with those of [Saket'21, Saket'22] and random LTFs, demonstrating the effectiveness of our techniques.
</details>
<details>
<summary>摘要</summary>
学习从标签比例（LLP）是一种泛化超级vised学习，其训练数据为特征向量集或包中的平均实例标签。目标是训练一个好的实例分类器。而大多数之前的LLP研究都集中在模型的训练上，而Computational learnability of LLP只在[Saket'21, Saket'22]中被研究过，他们表明了线性阈值函数（LTF）的合理学习是最坏情况不可能的。然而，他们的工作没有排除了自然分布下的有效算法。在这个工作中，我们证明了可以高效地学习LTF，只要给出Random Bag of Label Proportions（RBLP）中的特征向量，并且这些特征向量是Conditioned on their labels，独立地从 Gaussian 分布 $N(\mathbf{\mu}, \mathbf{\Sigma})$ 中随机抽取。我们的工作表明，一个特定的矩阵，由RBLP中带有和无置换的特征向量的差异的covariances形成，然后经过一种变换，必然有其主成分在正常方向上。我们的算法使用Subgaussian散射约束来估算均值和 covariance 矩阵，然后使用这些矩阵来随机抽取包来估算正常方向。使用这种方法，我们可以高效地分类LTF。对于 $N(\mathbf{0}, \mathbf{I})$ 分布的特殊情况，我们还提供了一个简单的均值估计基于算法。我们在实验评估了我们的学习算法，并与 [Saket'21, Saket'22] 和随机 LTF 进行了比较，demonstrating the effectiveness of our techniques。
</details></li>
</ul>
<hr>
<h2 id="LLP-Bench-A-Large-Scale-Tabular-Benchmark-for-Learning-from-Label-Proportions"><a href="#LLP-Bench-A-Large-Scale-Tabular-Benchmark-for-Learning-from-Label-Proportions" class="headerlink" title="LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions"></a>LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label Proportions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10096">http://arxiv.org/abs/2310.10096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Mohith Pokala, Rishi Saket, Aravindan Raghuveer</li>
<li>for: 本文提出了一个大规模的标注量学习（LLP）数据集，用于 Addressing the lack of a open, large scale tabular benchmark。</li>
<li>methods: 本文提出了一个名为 LLP-Bench 的数据集，包含 56 个 LLP 数据集（52 个特征袋和 4 个随机袋数据集），它们都是从 Criteo CTR 预测数据集中的 45 万个实例中构建的。此外，本文还提出了四种度量量学习数据集的困难程度。</li>
<li>results: 本文通过使用这四种度量量学习数据集的困难程度，进行了深入的分析。此外，本文还通过使用 9 种 state-of-the-art 和受欢迎的标注量学习技术，对所有 56 个数据集进行了性能测试。根据本文的描述，这是文献中最广泛的标注量学习技术测试。<details>
<summary>Abstract</summary>
In the task of Learning from Label Proportions (LLP), a model is trained on groups (a.k.a bags) of instances and their corresponding label proportions to predict labels for individual instances. LLP has been applied pre-dominantly on two types of datasets - image and tabular. In image LLP, bags of fixed size are created by randomly sampling instances from an underlying dataset. Bags created via this methodology are called random bags. Experimentation on Image LLP has been mostly on random bags on CIFAR-* and MNIST datasets. Despite being a very crucial task in privacy sensitive applications, tabular LLP does not yet have a open, large scale LLP benchmark. One of the unique properties of tabular LLP is the ability to create feature bags where all the instances in a bag have the same value for a given feature. It has been shown in prior research that feature bags are very common in practical, real world applications [Chen et. al '23, Saket et. al. '22].   In this paper, we address the lack of a open, large scale tabular benchmark. First we propose LLP-Bench, a suite of 56 LLP datasets (52 feature bag and 4 random bag datasets) created from the Criteo CTR prediction dataset consisting of 45 million instances. The 56 datasets represent diverse ways in which bags can be constructed from underlying tabular data. To the best of our knowledge, LLP-Bench is the first large scale tabular LLP benchmark with an extensive diversity in constituent datasets. Second, we propose four metrics that characterize and quantify the hardness of a LLP dataset. Using these four metrics we present deep analysis of the 56 datasets in LLP-Bench. Finally we present the performance of 9 SOTA and popular tabular LLP techniques on all the 56 datasets. To the best of our knowledge, our study consisting of more than 2500 experiments is the most extensive study of popular tabular LLP techniques in literature.
</details>
<details>
<summary>摘要</summary>
在学习从标签比例（LLP）任务中，一个模型被训练在实例组（即袋）和其对应的标签比例上，以预测个体实例的标签。 LLG 已经主要应用于图像和表格数据集。在图像 LLG 中，实例组通常通过随机抽样实例从下面数据集创建。这种方法创建的袋被称为随机袋。在 CIFAR-* 和 MNIST 数据集上进行了大量实验。虽然图像 LLG 是一个非常重要的任务，但是表格 LLG 尚未有一个开放、大规模的 LLG  bencmark。表格 LLG 的一个独特性是可以创建特征袋，其中所有实例在袋中都有相同的特征值。在先前的研究中已经证明了特征袋在实际应用中很常见。在这篇文章中，我们解决表格 LLG 缺乏一个开放、大规模的 bencmark 问题。我们提出了 LLP-Bench，一个包含 56 个 LLG 数据集（52 个特征袋数据集和 4 个随机袋数据集）的集合，这些数据集是从 Criteo CTR 预测数据集中的 45 万个实例中创建的。这些 56 个数据集表示了从下面表格数据中构建袋的多种方法。我们知道，LLP-Bench 是首先开放、大规模的表格 LLG bencmark，并且具有广泛的数据集多样性。其次，我们提出了四种度量 LLG 数据集的困难程度。使用这四种度量，我们进行了深入的分析 LLP-Bench 中的 56 个数据集。最后，我们在所有 56 个数据集上运行了 9 种 state-of-the-art 和流行的表格 LLG 技术，并进行了 более чем 2500 个实验。到目前为止，我们的研究是文献中最广泛的表格 LLG 技术研究。
</details></li>
</ul>
<hr>
<h2 id="Label-Differential-Privacy-via-Aggregation"><a href="#Label-Differential-Privacy-via-Aggregation" class="headerlink" title="Label Differential Privacy via Aggregation"></a>Label Differential Privacy via Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10092">http://arxiv.org/abs/2310.10092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Brahmbhatt, Rishi Saket, Shreyas Havaldar, Anshul Nasery, Aravindan Raghuveer</li>
<li>for: 保护敏感训练标签的隐私</li>
<li>methods: 使用 randomly weighted aggregation 和 additive noise 保护隐私</li>
<li>results: 可以实现 label-DP 保护，无需或少量的添加噪声，并且 preserved 训练任务的效果<details>
<summary>Abstract</summary>
In many real-world applications, in particular due to recent developments in the privacy landscape, training data may be aggregated to preserve the privacy of sensitive training labels. In the learning from label proportions (LLP) framework, the dataset is partitioned into bags of feature-vectors which are available only with the sum of the labels per bag. A further restriction, which we call learning from bag aggregates (LBA) is where instead of individual feature-vectors, only the (possibly weighted) sum of the feature-vectors per bag is available. We study whether such aggregation techniques can provide privacy guarantees under the notion of label differential privacy (label-DP) previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22].   It is easily seen that naive LBA and LLP do not provide label-DP. Our main result however, shows that weighted LBA using iid Gaussian weights with $m$ randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon, \delta)$-label-DP for any $\varepsilon > 0$ with $\delta \approx \exp(-\Omega(\sqrt{k}))$ assuming a lower bound on the linear-mse regression loss. Further, this preserves the optimum over linear mse-regressors of bounded norm to within $(1 \pm o(1))$-factor w.p. $\approx 1 - \exp(-\Omega(m))$. We emphasize that no additive label noise is required.   The analogous weighted-LLP does not however admit label-DP. Nevertheless, we show that if additive $N(0, 1)$ noise can be added to any constant fraction of the instance labels, then the noisy weighted-LLP admits similar label-DP guarantees without assumptions on the dataset, while preserving the utility of Lipschitz-bounded neural mse-regression tasks.   Our work is the first to demonstrate that label-DP can be achieved by randomly weighted aggregation for regression tasks, using no or little additive noise.
</details>
<details>
<summary>摘要</summary>
在许多实际应用中，特别是due to recent developments in privacy landscape，training data可能会被聚合以保护敏感训练标签的隐私。在学习从标签聚合（LLP）框架中，数据集被分解成具有特征向量的袋子，但这些特征向量只有每个袋子的标签总和。我们称这种约束为学习从袋子聚合（LBA）。我们研究了这种聚合技术是否可以提供隐私保证，并且我们发现这种保证是可行的。我们的主要结果表明，使用独立的 Gaussian 权重，将 $m$ 个不同大小的 $k$-个袋子Randomly sampled，并使用加权 LBA，可以实现 $( \varepsilon, \delta)$-标签隐私（label-DP），其中 $\delta \approx \exp(-\Omega(\sqrt{k}))$。此外，这种方法可以保持最佳的线性mse回归损失，即 $(1 \pm o(1))$-factor w.p. $\approx 1 - \exp(-\Omega(m))$.这意味着没有添加标签噪声。虽然加权 LLP 不能实现标签隐私，但我们发现，如果将任意一部分的实例标签添加 $N(0, 1)$ 噪声，那么这种噪声化的加权 LLP 可以实现类似的标签隐私保证，不需要对数据集进行任何假设。此外，这种方法可以保持 Lipschitz-bounded 神经网络 mse-regression 任务的实用性。我们的工作是首次示出，通过Randomly weighted aggregation可以实现标签隐私，无需或只需少量的添加噪声。
</details></li>
</ul>
<hr>
<h2 id="Over-the-Air-Federated-Learning-and-Optimization"><a href="#Over-the-Air-Federated-Learning-and-Optimization" class="headerlink" title="Over-the-Air Federated Learning and Optimization"></a>Over-the-Air Federated Learning and Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10089">http://arxiv.org/abs/2310.10089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyang Zhu, Yuanming Shi, Yong Zhou, Chunxiao Jiang, Wei Chen, Khaled B. Letaief</li>
<li>For: 这篇论文关注于 federated learning (FL) 中的 over-the-air computation (AirComp)，以减少无线网络上的通信负担，但是会增加模型聚合错误引起的学习性能下降。* Methods: 该论文首先进行了 AirComp-based FedAvg 算法的完整的研究，包括在强型凸和非凸设定下的常数和减少学习率下的渐进分析，以及在数据不同性下的影响分析。* Results: 该论文通过渐进分析和启发性分析，描述了模型聚合错误对渐进级别的影响，并提供了系统设计的准确性保证。此外，该论文还探讨了不同类型的本地更新（模型、梯度和模型差异）在 AirFedAvg 算法中的影响。<details>
<summary>Abstract</summary>
Federated learning (FL), as an emerging distributed machine learning paradigm, allows a mass of edge devices to collaboratively train a global model while preserving privacy. In this tutorial, we focus on FL via over-the-air computation (AirComp), which is proposed to reduce the communication overhead for FL over wireless networks at the cost of compromising in the learning performance due to model aggregation error arising from channel fading and noise. We first provide a comprehensive study on the convergence of AirComp-based FedAvg (AirFedAvg) algorithms under both strongly convex and non-convex settings with constant and diminishing learning rates in the presence of data heterogeneity. Through convergence and asymptotic analysis, we characterize the impact of aggregation error on the convergence bound and provide insights for system design with convergence guarantees. Then we derive convergence rates for AirFedAvg algorithms for strongly convex and non-convex objectives. For different types of local updates that can be transmitted by edge devices (i.e., local model, gradient, and model difference), we reveal that transmitting local model in AirFedAvg may cause divergence in the training procedure. In addition, we consider more practical signal processing schemes to improve the communication efficiency and further extend the convergence analysis to different forms of model aggregation error caused by these signal processing schemes. Extensive simulation results under different settings of objective functions, transmitted local information, and communication schemes verify the theoretical conclusions.
</details>
<details>
<summary>摘要</summary>
federated learning（FL）是一种新的分布式机器学习模式，允许Edge设备之间的大量设备共同训练全球模型，保持隐私。在这个教程中，我们关注FL通过过空 computation（AirComp）来降低由无线网络传输的交流负担，但是由于通道抖动和噪声而导致模型聚合错误，因此降低了学习性能。我们首先对AirComp-based FedAvg（AirFedAvg）算法进行了全面的研究，包括强度凸和非凸设置下的常数和减少学习率，并在数据不同性下进行了与界分析。我们通过收敛和极限分析来描述聚合错误对收敛 bound 的影响，并提供了系统设计的准确性保证。然后，我们 derivated convergence rates for AirFedAvg algorithms for strongly convex and non-convex objectives. 对于不同的本地更新可以在 Edge devices 上传输（即本地模型、梯度和模型差异），我们发现在 AirFedAvg 中传输本地模型可能会导致训练过程中的偏转。此外，我们考虑了更实际的通信减少技术，并将这些技术应用于不同的聚合错误类型，以进一步推广收敛分析。我们的实验结果在不同的目标函数、传输的本地信息和通信方案下都验证了我们的理论结论。
</details></li>
</ul>
<hr>
<h2 id="A-simple-uniformly-optimal-method-without-line-search-for-convex-optimization"><a href="#A-simple-uniformly-optimal-method-without-line-search-for-convex-optimization" class="headerlink" title="A simple uniformly optimal method without line search for convex optimization"></a>A simple uniformly optimal method without line search for convex optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10082">http://arxiv.org/abs/2310.10082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianjiao Li, Guanghui Lan</li>
<li>for:  solves convex optimization problems with unknown problem parameters (e.g., Lipschitz constant) without the need for line search procedures.</li>
<li>methods:  presents a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that achieves an optimal $\mathcal{O}(1&#x2F;k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant.</li>
<li>results:  demonstrates the advantages of AC-FGM over the previously developed parameter-free methods for convex optimization through numerical results.<details>
<summary>Abstract</summary>
Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal $\mathcal{O}(1/k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with H\"{o}lder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Finally, we report some encouraging numerical results that demonstrate the advantages of AC-FGM over the previously developed parameter-free methods for convex optimization.
</details>
<details>
<summary>摘要</summary>
《线搜索（或回溯）过程已广泛应用于首领方法中解决凸优化问题，特别是当问题参数未知（例如 lipschitz常数）。在这篇论文中，我们证明线搜索是不必要的，以实现凸优化问题的最佳$\mathcal{O}(1/k^2)$趋势速度。我们 THEN present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM)，可以在凸优化问题中不需要global lipschitz常数或线搜索过程来实现最佳趋势速度。我们然后扩展AC-FGM来解决凸优化问题，并证明它自动实现了最佳趋势速度，并且可以在所有问题类型中实现最佳趋势速度，只需要输入所需的精度。最后，我们report some encouraging numerical results that demonstrate the advantages of AC-FGM over the previously developed parameter-free methods for convex optimization。》Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="SoTTA-Robust-Test-Time-Adaptation-on-Noisy-Data-Streams"><a href="#SoTTA-Robust-Test-Time-Adaptation-on-Noisy-Data-Streams" class="headerlink" title="SoTTA: Robust Test-Time Adaptation on Noisy Data Streams"></a>SoTTA: Robust Test-Time Adaptation on Noisy Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10074">http://arxiv.org/abs/2310.10074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taeckyung/SoTTA">https://github.com/taeckyung/SoTTA</a></li>
<li>paper_authors: Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, Sung-Ju Lee</li>
<li>for: 这个论文旨在Addressing distributional shifts between training and testing data using only unlabeled test data streams for continual model adaptation.</li>
<li>methods: 这个方法使用 two-fold enablers: (i) input-wise robustness via high-confidence uniform-class sampling, and (ii) parameter-wise robustness via entropy-sharpness minimization.</li>
<li>results: 比较先前的TTA方法，这个方法在存在噪音样本的情况下实现了比较好的性能，并且在没有噪音样本的情况下实现了相当的性能。<details>
<summary>Abstract</summary>
Test-time adaptation (TTA) aims to address distributional shifts between training and testing data using only unlabeled test data streams for continual model adaptation. However, most TTA methods assume benign test streams, while test samples could be unexpectedly diverse in the wild. For instance, an unseen object or noise could appear in autonomous driving. This leads to a new threat to existing TTA algorithms; we found that prior TTA algorithms suffer from those noisy test samples as they blindly adapt to incoming samples. To address this problem, we present Screening-out Test-Time Adaptation (SoTTA), a novel TTA algorithm that is robust to noisy samples. The key enabler of SoTTA is two-fold: (i) input-wise robustness via high-confidence uniform-class sampling that effectively filters out the impact of noisy samples and (ii) parameter-wise robustness via entropy-sharpness minimization that improves the robustness of model parameters against large gradients from noisy samples. Our evaluation with standard TTA benchmarks with various noisy scenarios shows that our method outperforms state-of-the-art TTA methods under the presence of noisy samples and achieves comparable accuracy to those methods without noisy samples. The source code is available at https://github.com/taeckyung/SoTTA .
</details>
<details>
<summary>摘要</summary>
(i) input-wise robustness via high-confidence uniform-class sampling that effectively filters out the impact of noisy samples;(ii) parameter-wise robustness via entropy-sharpness minimization that improves the robustness of model parameters against large gradients from noisy samples.Our evaluation with standard TTA benchmarks with various noisy scenarios shows that our method outperforms state-of-the-art TTA methods under the presence of noisy samples and achieves comparable accuracy to those methods without noisy samples. The source code is available at <https://github.com/taeckyung/SoTTA>.
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentation-for-Time-Series-Classification-An-Extensive-Empirical-Study-and-Comprehensive-Survey"><a href="#Data-Augmentation-for-Time-Series-Classification-An-Extensive-Empirical-Study-and-Comprehensive-Survey" class="headerlink" title="Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey"></a>Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10060">http://arxiv.org/abs/2310.10060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zijun Gao, Lingbo Li, Tianhua Xu<br>for:* The paper aims to provide a comprehensive review of data augmentation (DA) techniques for time series classification (TSC) and to develop a novel taxonomy for categorizing these techniques.methods:* The paper uses an extensive literature review and a rigorous analysis of over 100 scholarly articles to identify and categorize more than 60 unique DA techniques for TSC.* The paper also employs an all-encompassing empirical assessment using 8 UCR time-series datasets and ResNet to evaluate the performance of various DA strategies.results:* The paper reports a benchmark accuracy of 88.94 +- 11.83% using a multi-faceted evaluation paradigm that includes Accuracy, Method Ranking, and Residual Analysis.* The paper highlights the inconsistent efficacies of DA techniques for TSC and underscores the need for a robust navigational aid for scholars to select appropriate methods.<details>
<summary>Abstract</summary>
Data Augmentation (DA) has emerged as an indispensable strategy in Time Series Classification (TSC), primarily due to its capacity to amplify training samples, thereby bolstering model robustness, diversifying datasets, and curtailing overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible, user-oriented tools. In light of these challenges, this study embarks on an exhaustive dissection of DA methodologies within the TSC realm. Our initial approach involved an extensive literature review spanning a decade, revealing that contemporary surveys scarcely capture the breadth of advancements in DA for TSC, prompting us to meticulously analyze over 100 scholarly articles to distill more than 60 unique DA techniques. This rigorous analysis precipitated the formulation of a novel taxonomy, purpose-built for the intricacies of DA in TSC, categorizing techniques into five principal echelons: Transformation-Based, Pattern-Based, Generative, Decomposition-Based, and Automated Data Augmentation. Our taxonomy promises to serve as a robust navigational aid for scholars, offering clarity and direction in method selection. Addressing the conspicuous absence of holistic evaluations for prevalent DA techniques, we executed an all-encompassing empirical assessment, wherein upwards of 15 DA strategies were subjected to scrutiny across 8 UCR time-series datasets, employing ResNet and a multi-faceted evaluation paradigm encompassing Accuracy, Method Ranking, and Residual Analysis, yielding a benchmark accuracy of 88.94 +- 11.83%. Our investigation underscored the inconsistent efficacies of DA techniques, with...
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>时序分类（TSC）中的数据扩展（DA）已经成为一种不可或缺的策略，主要是因为它可以增加训练样本数量，从而增强模型的鲁棒性，维护数据集的多样性，并避免过拟合。然而，现有的DA在TSC领域的文献 landscape 受到了 Fragmented 的文献回顾，混乱的方法分类、不够的评价标准和Accessible 的用户工具的缺乏，这些挑战使得这项研究发起了一项极其详细的DA方法分析。我们的初始方法是进行了一项广泛的文献回顾，覆盖了一个 décennial 的时间范围，发现当前的 contemporary 文献几乎不能涵盖DA在TSC领域的全面发展，因此我们仔细分析了超过 100 篇学术文章，提取了超过 60 种Unique DA技术。这项精心的分析导致了我们提出了一个专门为TSC领域的DA方法分类的新分类法，将技术分为五个主要层次：转换基于、模式基于、生成器、分解基于和自动化数据扩展。我们的分类法承诺成为学术界的一个robust navigational aid，为执行者提供了方法选择的明确性和方向性。为了弥补DA技术的普遍存在的评价问题，我们实施了一项总面的实验室评价，对超过 15 种DA策略进行了8个UCSD时序数据集的评估，使用了ResNet和一种多方面的评价方案，包括准确率、方法排名和剩余分析，实现了基准准确率88.94±11.83%。我们的调查表明，DA技术的不同策略在不同的时序数据集上的表现存在差异，...
</details></li>
</ul>
<hr>
<h2 id="Latent-Conservative-Objective-Models-for-Data-Driven-Crystal-Structure-Prediction"><a href="#Latent-Conservative-Objective-Models-for-Data-Driven-Crystal-Structure-Prediction" class="headerlink" title="Latent Conservative Objective Models for Data-Driven Crystal Structure Prediction"></a>Latent Conservative Objective Models for Data-Driven Crystal Structure Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10056">http://arxiv.org/abs/2310.10056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Qi, Xinyang Geng, Stefano Rando, Iku Ohama, Aviral Kumar, Sergey Levine</li>
<li>for: 这 paper 的目的是提出一种数据驱动的晶体结构预测方法，以优化晶体结构的计算成本。</li>
<li>methods: 这 paper 使用了一种名为 LCOMs (latent conservative objective models) 的方法，它利用一个状态环境 autoencoder 将晶体结构转换为一个矢量空间中的搜索空间，然后优化一个保守的晶体能量模型。</li>
<li>results: 这 paper 的结果表明，LCOMs 方法可以与现有最佳方法相比，成功率相似，而计算成本减少了多少。<details>
<summary>Abstract</summary>
In computational chemistry, crystal structure prediction (CSP) is an optimization problem that involves discovering the lowest energy stable crystal structure for a given chemical formula. This problem is challenging as it requires discovering globally optimal designs with the lowest energies on complex manifolds. One approach to tackle this problem involves building simulators based on density functional theory (DFT) followed by running search in simulation, but these simulators are painfully slow. In this paper, we study present and study an alternate, data-driven approach to crystal structure prediction: instead of directly searching for the most stable structures in simulation, we train a surrogate model of the crystal formation energy from a database of existing crystal structures, and then optimize this model with respect to the parameters of the crystal structure. This surrogate model is trained to be conservative so as to prevent exploitation of its errors by the optimizer. To handle optimization in the non-Euclidean space of crystal structures, we first utilize a state-of-the-art graph diffusion auto-encoder (CD-VAE) to convert a crystal structure into a vector-based search space and then optimize a conservative surrogate model of the crystal energy, trained on top of this vector representation. We show that our approach, dubbed LCOMs (latent conservative objective models), performs comparably to the best current approaches in terms of success rate of structure prediction, while also drastically reducing computational cost.
</details>
<details>
<summary>摘要</summary>
在计算化学中，晶体结构预测（CSP）是一个优化问题，涉及到找到给定化学式的最低能量稳定晶体结构。这个问题是复杂的，因为需要找到最低能量的全局优化设计在复杂的拟合上。一种方法是建立基于密度函数理论（DFT）的模拟器，然后通过搜索在模拟中进行优化，但这些模拟器很慢。在这篇论文中，我们研究了一种 alternate，数据驱动的晶体结构预测方法：而不是直接在模拟中搜索最稳定的结构，我们将训练一个晶体形成能量的模拟器，该模拟器在数据库中的已知晶体结构基础上被训练，然后对晶体结构参数进行优化。这个模拟器被设计为保守的，以避免其错误被优化器利用。为处理晶体结构的非几何空间优化问题，我们首先使用当前最佳的图像扩散自动encoder（CD-VAE）将晶体结构转换为一个矢量基本搜索空间，然后对这个矢量表示的晶体能量模拟器进行保守的优化。我们发现，我们的方法，称为LCOMs（幽默保守目标模型），与当前最佳方法相比，在结构预测成功率方面表现相似，同时也很快地减少计算成本。
</details></li>
</ul>
<hr>
<h2 id="Symmetrical-SyncMap-for-Imbalanced-General-Chunking-Problems"><a href="#Symmetrical-SyncMap-for-Imbalanced-General-Chunking-Problems" class="headerlink" title="Symmetrical SyncMap for Imbalanced General Chunking Problems"></a>Symmetrical SyncMap for Imbalanced General Chunking Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10045">http://arxiv.org/abs/2310.10045</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Zhang, Danilo Vasconcellos Vargas</li>
<li>for: 本研究旨在学习从序列中检索复杂结构，并适应任何结构变化。</li>
<li>methods: 本研究使用非线性动力学方程， inspirited by neuron group behaviors，而不使用损失函数。</li>
<li>results: 我们的算法在12个不均衡CGCP中表现出色，超过或与其他无监督状态元既达到同等水平。在实际应用场景中，我们的方法在3个场景中表现出优异，表明对时间数据中的トポлогиcal结构和层次结构具有探索性。<details>
<summary>Abstract</summary>
Recently, SyncMap pioneered an approach to learn complex structures from sequences as well as adapt to any changes in underlying structures. This is achieved by using only nonlinear dynamical equations inspired by neuron group behaviors, i.e., without loss functions. Here we propose Symmetrical SyncMap that goes beyond the original work to show how to create dynamical equations and attractor-repeller points which are stable over the long run, even dealing with imbalanced continual general chunking problems (CGCPs). The main idea is to apply equal updates from negative and positive feedback loops by symmetrical activation. We then introduce the concept of memory window to allow for more positive updates. Our algorithm surpasses or ties other unsupervised state-of-the-art baselines in all 12 imbalanced CGCPs with various difficulties, including dynamically changing ones. To verify its performance in real-world scenarios, we conduct experiments on several well-studied structure learning problems. The proposed method surpasses substantially other methods in 3 out of 4 scenarios, suggesting that symmetrical activation plays a critical role in uncovering topological structures and even hierarchies encoded in temporal data.
</details>
<details>
<summary>摘要</summary>
最近，SyncMap开创了一种从序列学习复杂结构的方法，同时适应下面结构的变化。这是通过使用非线性动力学方程， inspirited by neuron group behaviors，而不使用损失函数。在这个研究中，我们提出了对称的SyncMap，超越原始工作，并显示了如何创建动力学方程和吸引器-抵抗点，这些点在长期内是稳定的，甚至在不均衡的CGCP中进行总览。我们的算法在12个不均衡CGCP中至少与其他无监督状态艺术基elines一样好，包括动态变化的CGCP。为了证明它在实际情况下的表现，我们在several well-studied structure learning问题上进行了实验。提出的方法在3个问题中大幅超过其他方法，表明对称活动在捕捉时间数据中的 topological结构和层次结构具有关键作用。
</details></li>
</ul>
<hr>
<h2 id="TpopT-Efficient-Trainable-Template-Optimization-on-Low-Dimensional-Manifolds"><a href="#TpopT-Efficient-Trainable-Template-Optimization-on-Low-Dimensional-Manifolds" class="headerlink" title="TpopT: Efficient Trainable Template Optimization on Low-Dimensional Manifolds"></a>TpopT: Efficient Trainable Template Optimization on Low-Dimensional Manifolds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10039">http://arxiv.org/abs/2310.10039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingkai Yan, Shiyu Wang, Xinyu Rain Wei, Jimmy Wang, Zsuzsanna Márka, Szabolcs Márka, John Wright</li>
<li>for: 检测低维度信号家族</li>
<li>methods: 使用 TemPlate OPTimization 框架，combined with embedding和kernel interpolation，提高计算效率</li>
<li>results: 在 gravitational wave detection 和手写数据上显示了明显的性能改善，并且可以替换现有的 matched filtering 方法<details>
<summary>Abstract</summary>
In scientific and engineering scenarios, a recurring task is the detection of low-dimensional families of signals or patterns. A classic family of approaches, exemplified by template matching, aims to cover the search space with a dense template bank. While simple and highly interpretable, it suffers from poor computational efficiency due to unfavorable scaling in the signal space dimensionality. In this work, we study TpopT (TemPlate OPTimization) as an alternative scalable framework for detecting low-dimensional families of signals which maintains high interpretability. We provide a theoretical analysis of the convergence of Riemannian gradient descent for TpopT, and prove that it has a superior dimension scaling to covering. We also propose a practical TpopT framework for nonparametric signal sets, which incorporates techniques of embedding and kernel interpolation, and is further configurable into a trainable network architecture by unrolled optimization. The proposed trainable TpopT exhibits significantly improved efficiency-accuracy tradeoffs for gravitational wave detection, where matched filtering is currently a method of choice. We further illustrate the general applicability of this approach with experiments on handwritten digit data.
</details>
<details>
<summary>摘要</summary>
在科学和工程应用中，检测低维度信号家族是一项常复现的任务。经典的方法之一是模板匹配，它在搜索空间使用密集的模板银行，但它的计算效率受到信号空间维度的不利影响。在这项工作中，我们研究TpopT（模板优化）作为一种可扩展的搜索框架，可以快速检测低维度信号家族，同时保持高度可读性。我们提供了TpopT的理论分析，证明它在维度上有更好的缩放性。此外，我们还提出了一种实用的TpopT框架，用于非参数式信号集，该框架包括投影和核函数 interpolate 技术，并可以通过不断的优化来转化为可训练的网络结构。我们的可训练TpopT在探测 gravitational wave 方面表现出了明显的效率-准确性融合优势，现在matched filtering 是选择的方法。此外，我们还通过对手写数据进行实验，证明了这种方法的通用性。
</details></li>
</ul>
<hr>
<h2 id="Unraveling-Fundamental-Properties-of-Power-System-Resilience-Curves-using-Unsupervised-Machine-Learning"><a href="#Unraveling-Fundamental-Properties-of-Power-System-Resilience-Curves-using-Unsupervised-Machine-Learning" class="headerlink" title="Unraveling Fundamental Properties of Power System Resilience Curves using Unsupervised Machine Learning"></a>Unraveling Fundamental Properties of Power System Resilience Curves using Unsupervised Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10030">http://arxiv.org/abs/2310.10030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Li, Ali Mostafavi</li>
<li>for: 这个研究旨在描述和量化基础设施的鲜敏性特点。</li>
<li>methods: 这个研究使用无监督机器学习分析了超过200个关于三次极端天气事件的停电情况下的鲜敏性曲线。</li>
<li>results: 研究发现了两种基础设施鲜敏性曲线模型：三角形曲线和梯形曲线。三角形曲线基于 three critical functionality threshold、critical functionality recovery rate 和 recovery pivot point。梯形曲线则基于停电持续时间和平均恢复率。停电持续时间越长，恢复率就越慢。这些发现可以帮助我们更好地理解和预测基础设施的鲜敏性表现。<details>
<summary>Abstract</summary>
The standard model of infrastructure resilience, the resilience triangle, has been the primary way of characterizing and quantifying infrastructure resilience. However, the theoretical model merely provides a one-size-fits-all framework for all infrastructure systems. Most of the existing studies examine the characteristics of infrastructure resilience curves based on analytical models constructed upon simulated system performance. Limited empirical studies hindered our ability to fully understand and predict resilience characteristics in infrastructure systems. To address this gap, this study examined over 200 resilience curves related to power outages in three major extreme weather events. Using unsupervised machine learning, we examined different curve archetypes, as well as the fundamental properties of each resilience curve archetype. The results show two primary archetypes for power system resilience curves, triangular, and trapezoidal curves. Triangular curves characterize resilience behavior based on 1. critical functionality threshold, 2. critical functionality recovery rate, and 3. recovery pivot point. Trapezoidal archetypes explain resilience curves based on 1. duration of sustained function loss and 2. constant recovery rate. The longer the duration of sustained function loss, the slower the constant rate of recovery. The findings of this study provide novel perspectives enabling better understanding and prediction of resilience performance of power system infrastructures.
</details>
<details>
<summary>摘要</summary>
现代基础设施鲜度模型，即鲜度三角形模型，已成为基础设施鲜度的主要方法。然而，这种理论模型只能为所有基础设施系统提供一个一大 Familiar framework。大多数现有研究都是基于对基础设施系统性能的分析建模。有限的实证研究限制了我们理解和预测基础设施系统鲜度的能力。为了解决这个差距，本研究对三次极端天气事件中的电力停机事件进行了200多个鲜度曲线的研究。使用无监督机器学习方法，我们研究了不同的鲜度曲线范型，以及每个鲜度曲线范型的基本性质。结果显示，电力系统鲜度曲线有两种主要范型：三角形曲线和梯形曲线。三角形曲线表示鲜度行为的三个关键指标：极限功能阈值、极限功能恢复率和恢复枢轴点。梯形曲线则解释鲜度曲线的两个指标：持续功能损失的时间长度和恢复率。即使持续功能损失的时间长度越长，恢复率也越慢。这些发现为电力系统基础设施的鲜度性能提供了新的视角，帮助更好地理解和预测鲜度性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Driven-Score-Based-Models-for-Generating-Stable-Structures-with-Adaptive-Crystal-Cells"><a href="#Data-Driven-Score-Based-Models-for-Generating-Stable-Structures-with-Adaptive-Crystal-Cells" class="headerlink" title="Data-Driven Score-Based Models for Generating Stable Structures with Adaptive Crystal Cells"></a>Data-Driven Score-Based Models for Generating Stable Structures with Adaptive Crystal Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10695">http://arxiv.org/abs/2310.10695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/findooshka/diffusion-atoms">https://github.com/findooshka/diffusion-atoms</a></li>
<li>paper_authors: Arsen Sultanov, Jean-Claude Crivello, Tabea Rebafka, Nataliya Sokolovska</li>
<li>for: 本研究旨在通过机器学习生成模型，找到新的功能性和稳定性的材料。</li>
<li>methods: 该研究使用了分布式朴素迭代随机动力学模型，在训练过程中学习了晶格的各个参数，并在生成新的化学结构时使用了两个杂谱处理来生成晶格和原子位置。</li>
<li>results: 研究人员通过对不同化学系统和晶体群进行比较，表明了他们的模型能够在不需要额外训练的情况下，生成新的候选结构。<details>
<summary>Abstract</summary>
The discovery of new functional and stable materials is a big challenge due to its complexity. This work aims at the generation of new crystal structures with desired properties, such as chemical stability and specified chemical composition, by using machine learning generative models. Compared to the generation of molecules, crystal structures pose new difficulties arising from the periodic nature of the crystal and from the specific symmetry constraints related to the space group. In this work, score-based probabilistic models based on annealed Langevin dynamics, which have shown excellent performance in various applications, are adapted to the task of crystal generation. The novelty of the presented approach resides in the fact that the lattice of the crystal cell is not fixed. During the training of the model, the lattice is learned from the available data, whereas during the sampling of a new chemical structure, two denoising processes are used in parallel to generate the lattice along the generation of the atomic positions. A multigraph crystal representation is introduced that respects symmetry constraints, yielding computational advantages and a better quality of the sampled structures. We show that our model is capable of generating new candidate structures in any chosen chemical system and crystal group without any additional training. To illustrate the functionality of the proposed method, a comparison of our model to other recent generative models, based on descriptor-based metrics, is provided.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>新型功能稳定材料的发现是一个大的挑战，因为它的复杂性。这项工作的目标是通过机器学习生成模型生成新的晶体结构，其具有指定的化学稳定性和某些化学成分。与分子生成不同，晶体结构受到晶体 периоди性和空间群特殊约束的限制，这些约束使得晶体生成增加了新的挑战。在这项工作中，我们使用了Score-based潜在随机模型，这种模型在多种应用中表现出色。我们的新方法在训练模型时不 fix 晶体维度，而是在数据available时学习晶体矩阵，并在生成原子位置时使用了两个杂化过程。我们引入了多граф晶体表示，该表示符合Symmetry约束，从而获得计算优势和更高质量的样本结构。我们显示了我们的模型可以在任选的化学系统和晶体组中生成新的候选结构，无需额外训练。为证明我们的方法的可行性，我们对其与其他最近的生成模型进行了比较，并通过描述符 metric 进行评估。
</details></li>
</ul>
<hr>
<h2 id="Riemannian-Residual-Neural-Networks"><a href="#Riemannian-Residual-Neural-Networks" class="headerlink" title="Riemannian Residual Neural Networks"></a>Riemannian Residual Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10013">http://arxiv.org/abs/2310.10013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isay Katsman, Eric Ming Chen, Sidhanth Holalkere, Anna Asch, Aaron Lou, Ser-Nam Lim, Christopher De Sa</li>
<li>for: 这种研究旨在扩展常见的欧几丁素神经网络（ResNet）到整体几何抽象空间中，以便在自然科学中遇到的拓扑空间数据上进行学习。</li>
<li>methods: 这篇论文使用了几何神经网络的扩展，以便在拓扑空间上进行学习。这种扩展基于几何抽象空间的原则，并且可以覆盖整体几何抽象空间中的任何点。</li>
<li>results: 论文的实验结果表明，使用这种几何神经网络可以在拓扑空间上进行更好的学习，并且在相关的测试指标上表现更好，比如训练律动和测试结果。<details>
<summary>Abstract</summary>
Recent methods in geometric deep learning have introduced various neural networks to operate over data that lie on Riemannian manifolds. Such networks are often necessary to learn well over graphs with a hierarchical structure or to learn over manifold-valued data encountered in the natural sciences. These networks are often inspired by and directly generalize standard Euclidean neural networks. However, extending Euclidean networks is difficult and has only been done for a select few manifolds. In this work, we examine the residual neural network (ResNet) and show how to extend this construction to general Riemannian manifolds in a geometrically principled manner. Originally introduced to help solve the vanishing gradient problem, ResNets have become ubiquitous in machine learning due to their beneficial learning properties, excellent empirical results, and easy-to-incorporate nature when building varied neural networks. We find that our Riemannian ResNets mirror these desirable properties: when compared to existing manifold neural networks designed to learn over hyperbolic space and the manifold of symmetric positive definite matrices, we outperform both kinds of networks in terms of relevant testing metrics and training dynamics.
</details>
<details>
<summary>摘要</summary>
现代几何深度学习方法已经引入了许多神经网络操作于偏射抽象空间上的数据。这些神经网络经常用于学习具有层次结构的图或者学习自然科学中遇到的拟合空间上的数据。这些神经网络通常是基于标准欧几何网络的扩展，但扩展到普通的欧几何空间是困难的，只有对一些特殊的欧几何空间进行了扩展。在这个工作中，我们研究了剩余神经网络（ResNet）的扩展，并证明了这种扩展方法可以在一般的偏射抽象空间上进行地理emetric的扩展。原本是解决减速问题的概念，ResNet在机器学习中得到了广泛的应用，因为它具有良好的学习性、优秀的实验性和容易整合到不同神经网络中的特点。我们发现，我们的偏射抽象空间上的ResNet和已有的欧几何空间上的神经网络相比，在相关的测试指标和训练动态上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Ring-A-Bell-How-Reliable-are-Concept-Removal-Methods-for-Diffusion-Models"><a href="#Ring-A-Bell-How-Reliable-are-Concept-Removal-Methods-for-Diffusion-Models" class="headerlink" title="Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?"></a>Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10012">http://arxiv.org/abs/2310.10012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang</li>
<li>for: 本研究旨在调查 diffusion models 的安全机制，以确保它们不会生成不适或有害内容。</li>
<li>methods: 我们提出了一种新的概念检索算法，可以评估 diffusion models 的安全性。该算法首先提取敏感或不适的概念，然后使用这些概念来自动标识 diffusion models 中可能生成不适内容的提问。</li>
<li>results: 我们的研究表明， Ring-A-Bell 可以 manipulate 安全提问 benchmarks，使得原本被视为安全的提问可以逃脱现有的安全机制，并生成不适或有害内容。这表明现有的安全机制并不够，需要进一步改进。<details>
<summary>Abstract</summary>
Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online services such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents.
</details>
<details>
<summary>摘要</summary>
Diffusion模型 для文本到图像（T2I）合成，如稳定扩散（SD），最近已经展示出了高质量内容的生成能力。然而，这种进步也引起了许多关于可能的不当使用的担忧，特别是在生成版权、禁止或限制的内容，或者NSFW（不适合工作）图像。虽有尝试了对这些问题进行缓解，例如在评估阶段实施安全筛选或者 Fine-tune模型以消除不жела的概念或风格，但是这些安全措施在各种提示下的效果仍然未经充分探索。在这项工作中，我们目的是调查这些安全机制。我们提出了一种新的概念检索算法，用于评估T2I扩散模型的安全性。我们称之为“铃铛”（Ring-A-Bell），它是一种无关模型的红Team工具，可以在提前准备的情况下完全无需先知Target模型来进行评估。具体来说，“铃铛”首先从敏感和不适合内容中提取概念，然后利用提取到的概念来自动识别扩散模型中的问题提示，并生成相应的不适合内容。这样，用户可以评估 deployed safety mechanisms的可靠性。最后，我们经验 validate我们的方法，测试在线服务如midjourney和不同的概念 removalfrom。我们的结果表明，“铃铛”可以通过修改安全提示benchmark，将原本被视为安全的提示转变为扩散模型生成不适合内容，因此揭示了现有的安全机制的缺陷，这些缺陷可能导致生成危害内容。
</details></li>
</ul>
<hr>
<h2 id="Implicit-regularization-via-soft-ascent-descent"><a href="#Implicit-regularization-via-soft-ascent-descent" class="headerlink" title="Implicit regularization via soft ascent-descent"></a>Implicit regularization via soft ascent-descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10006">http://arxiv.org/abs/2310.10006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feedbackward/bdd-flood">https://github.com/feedbackward/bdd-flood</a></li>
<li>paper_authors: Matthew J. Holland, Kosuke Nakatani</li>
<li>for: 提高机器学习过程中的OFF-sample泛化性能，避免过多的试错和重复。</li>
<li>methods: 使用Gradient Regularization的softened、点 wise机制，以降低边缘点的影响和抑制异常值的影响。</li>
<li>results: 与SAM和Flooding相比，SoftAD可以实现类比的分类精度，同时具有远小的损失泛化差和模型评价。<details>
<summary>Abstract</summary>
As models grow larger and more complex, achieving better off-sample generalization with minimal trial-and-error is critical to the reliability and economy of machine learning workflows. As a proxy for the well-studied heuristic of seeking "flat" local minima, gradient regularization is a natural avenue, and first-order approximations such as Flooding and sharpness-aware minimization (SAM) have received significant attention, but their performance depends critically on hyperparameters (flood threshold and neighborhood radius, respectively) that are non-trivial to specify in advance. In order to develop a procedure which is more resilient to misspecified hyperparameters, with the hard-threshold "ascent-descent" switching device used in Flooding as motivation, we propose a softened, pointwise mechanism called SoftAD that downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. We contrast formal stationarity guarantees with those for Flooding, and empirically demonstrate how SoftAD can realize classification accuracy competitive with SAM and Flooding while maintaining a much smaller loss generalization gap and model norm. Our empirical tests range from simple binary classification on the plane to image classification using neural networks with millions of parameters; the key trends are observed across all datasets and models studied, and suggest a potential new approach to implicit regularization.
</details>
<details>
<summary>摘要</summary>
To overcome this limitation, we propose a softened, pointwise mechanism called SoftAD, which downweights points on the borderline, limits the effects of outliers, and retains the ascent-descent effect. By comparing the formal stationarity guarantees of SoftAD with those of Flooding, we demonstrate that SoftAD can achieve classification accuracy competitive with SAM and Flooding while maintaining a much smaller loss generalization gap and model norm.Our empirical tests cover a range of datasets and models, from simple binary classification on the plane to image classification using neural networks with millions of parameters. The key trends observed across all datasets and models suggest a potential new approach to implicit regularization.
</details></li>
</ul>
<hr>
<h2 id="Conformal-Contextual-Robust-Optimization"><a href="#Conformal-Contextual-Robust-Optimization" class="headerlink" title="Conformal Contextual Robust Optimization"></a>Conformal Contextual Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10003">http://arxiv.org/abs/2310.10003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Patel, Sahana Rayan, Ambuj Tewari</li>
<li>for: 这个论文是为了解决决策问题，具体来说是使用数据驱动方法来避免对不确定性范围的误差，从而提高决策的优化。</li>
<li>methods: 这个论文使用的方法是基于Conditional Generative Model的高维空间中的非 conjugate 预测区域，这些预测区域具有 Desired distribution-free coverage guarantees。</li>
<li>results: 研究人员通过在一系列的 simulations-based inference benchmark tasks和基于气象预测的交通路径规划问题来展示 CPO 框架的效果，并提供了semantically meaningful的视觉总结来解释决策的优化。<details>
<summary>Abstract</summary>
Data-driven approaches to predict-then-optimize decision-making problems seek to mitigate the risk of uncertainty region misspecification in safety-critical settings. Current approaches, however, suffer from considering overly conservative uncertainty regions, often resulting in suboptimal decisionmaking. To this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for leveraging highly informative, nonconvex conformal prediction regions over high-dimensional spaces based on conditional generative models, which have the desired distribution-free coverage guarantees. Despite guaranteeing robustness, such black-box optimization procedures alone inspire little confidence owing to the lack of explanation of why a particular decision was found to be optimal. We, therefore, augment CPO to additionally provide semantically meaningful visual summaries of the uncertainty regions to give qualitative intuition for the optimal decision. We highlight the CPO framework by demonstrating results on a suite of simulation-based inference benchmark tasks and a vehicle routing task based on probabilistic weather prediction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用数据驱动的方法来解决决策问题，以减少不确定性区域的误差，在安全关键的场景中非常重要。现有的方法frequently suffer from considering overly conservative uncertainty regions, often resulting in suboptimal decision-making. To address this, we propose Conformal-Predict-Then-Optimize (CPO), a framework that leverages highly informative, nonconvex conformal prediction regions over high-dimensional spaces based on conditional generative models, which provide desired distribution-free coverage guarantees. Despite providing robustness, such black-box optimization procedures alone may lack confidence due to the lack of explanation of why a particular decision was found to be optimal. We, therefore, augment CPO with additional provision of semantically meaningful visual summaries of the uncertainty regions to provide qualitative intuition for the optimal decision. We demonstrate the effectiveness of the CPO framework through results on a suite of simulation-based inference benchmark tasks and a vehicle routing task based on probabilistic weather prediction.</SYS>>Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and widely used in other countries as well. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Outlier-Detection-Using-Generative-Models-with-Theoretical-Performance-Guarantees"><a href="#Outlier-Detection-Using-Generative-Models-with-Theoretical-Performance-Guarantees" class="headerlink" title="Outlier Detection Using Generative Models with Theoretical Performance Guarantees"></a>Outlier Detection Using Generative Models with Theoretical Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09999">http://arxiv.org/abs/2310.09999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jirong Yi, Jingchao Gao, Tianming Wang, Xiaodong Wu, Weiyu Xu</li>
<li>for: 这篇论文考虑了模拟器模型中的信号恢复问题，具体来说是在线性测量中受到稀疏异常的情况下恢复原始信号。</li>
<li>methods: 我们提出了一种异常检测方法，可以在模拟器模型下恢复原始信号，并且我们提供了有关信号恢复的理论保证。</li>
<li>results: 我们的实验结果表明，使用我们的方法可以成功恢复信号，即使在稀疏异常的情况下。我们的方法比传统的lasso和平方$\ell_2$最小化方法更高效。<details>
<summary>Abstract</summary>
This paper considers the problem of recovering signals modeled by generative models from linear measurements contaminated with sparse outliers. We propose an outlier detection approach for reconstructing the ground-truth signals modeled by generative models under sparse outliers. We establish theoretical recovery guarantees for reconstruction of signals using generative models in the presence of outliers, giving lower bounds on the number of correctable outliers. Our results are applicable to both linear generator neural networks and the nonlinear generator neural networks with an arbitrary number of layers. We propose an iterative alternating direction method of multipliers (ADMM) algorithm for solving the outlier detection problem via $\ell_1$ norm minimization, and a gradient descent algorithm for solving the outlier detection problem via squared $\ell_1$ norm minimization. We conduct extensive experiments using variational auto-encoder and deep convolutional generative adversarial networks, and the experimental results show that the signals can be successfully reconstructed under outliers using our approach. Our approach outperforms the traditional Lasso and $\ell_2$ minimization approach.
</details>
<details>
<summary>摘要</summary>
We propose two algorithms to solve the outlier detection problem: an iterative alternating direction method of multipliers (ADMM) algorithm that minimizes the $\ell_1$ norm, and a gradient descent algorithm that minimizes the squared $\ell_1$ norm. We conduct extensive experiments using variational auto-encoders and deep convolutional generative adversarial networks, and the results show that our approach can successfully reconstruct the signals even under outliers. Our approach outperforms traditional Lasso and $\ell_2$ minimization methods.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Machine-Learning-in-Biopharmaceutical-Process-Development-and-Manufacturing-Current-Trends-Challenges-and-Opportunities"><a href="#Applications-of-Machine-Learning-in-Biopharmaceutical-Process-Development-and-Manufacturing-Current-Trends-Challenges-and-Opportunities" class="headerlink" title="Applications of Machine Learning in Biopharmaceutical Process Development and Manufacturing: Current Trends, Challenges, and Opportunities"></a>Applications of Machine Learning in Biopharmaceutical Process Development and Manufacturing: Current Trends, Challenges, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09991">http://arxiv.org/abs/2310.09991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanh Tung Khuat, Robert Bassett, Ellen Otte, Alistair Grevis-James, Bogdan Gabrys</li>
<li>for: 本研究旨在提供一个全面的机器学习（ML）解决方案在生物医药领域的应用现状，包括生物产品设计、监测、控制和优化的过程中的应用。</li>
<li>methods: 本研究使用的方法包括机器学习模型的采用，以提高生物医药生产过程中的分析、监测和控制能力。</li>
<li>results: 本研究结果表明，机器学习模型在生物医药生产过程中的应用可以提高生产效率、产品质量和生产可靠性等方面的表现。同时，本研究还揭示了生物医药过程数据的复杂性和多维性，以及机器学习模型在生物医药过程中的挑战和限制。<details>
<summary>Abstract</summary>
While machine learning (ML) has made significant contributions to the biopharmaceutical field, its applications are still in the early stages in terms of providing direct support for quality-by-design based development and manufacturing of biopharmaceuticals, hindering the enormous potential for bioprocesses automation from their development to manufacturing. However, the adoption of ML-based models instead of conventional multivariate data analysis methods is significantly increasing due to the accumulation of large-scale production data. This trend is primarily driven by the real-time monitoring of process variables and quality attributes of biopharmaceutical products through the implementation of advanced process analytical technologies. Given the complexity and multidimensionality of a bioproduct design, bioprocess development, and product manufacturing data, ML-based approaches are increasingly being employed to achieve accurate, flexible, and high-performing predictive models to address the problems of analytics, monitoring, and control within the biopharma field. This paper aims to provide a comprehensive review of the current applications of ML solutions in a bioproduct design, monitoring, control, and optimisation of upstream, downstream, and product formulation processes. Finally, this paper thoroughly discusses the main challenges related to the bioprocesses themselves, process data, and the use of machine learning models in biopharmaceutical process development and manufacturing. Moreover, it offers further insights into the adoption of innovative machine learning methods and novel trends in the development of new digital biopharma solutions.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在生物医药领域已经做出了重要贡献，但是其应用还处于初期阶段，对生物医药生产的质量设计和生产进行直接支持的应用还尚未发挥出大量潜力。然而，由于生产数据的积累，ML模型的应用正在不断增加，取代传统的多变量数据分析方法。这种趋势主要归功于实时监测生产过程中变量和产品质量特征的实施，以及高级进程分析技术的普及。由于生物产品设计、生产和加工数据的复杂性和多维性，ML方法在解决生物过程数据分析、监测和控制方面提供了高精度、灵活性和高性能的预测模型。本文旨在为读者提供生物产品设计、监测、控制和优化过程中机器学习解决方案的全面审视。此外，本文还详细讨论了生物过程本身、数据和机器学习模型在生物医药过程开发和生产中的主要挑战，以及采用创新的机器学习方法和新趋势在生物医药领域的发展。
</details></li>
</ul>
<hr>
<h2 id="Personalization-of-CTC-based-End-to-End-Speech-Recognition-Using-Pronunciation-Driven-Subword-Tokenization"><a href="#Personalization-of-CTC-based-End-to-End-Speech-Recognition-Using-Pronunciation-Driven-Subword-Tokenization" class="headerlink" title="Personalization of CTC-based End-to-End Speech Recognition Using Pronunciation-Driven Subword Tokenization"></a>Personalization of CTC-based End-to-End Speech Recognition Using Pronunciation-Driven Subword Tokenization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09988">http://arxiv.org/abs/2310.09988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihong Lei, Ernest Pusateri, Shiyi Han, Leo Liu, Mingbin Xu, Tim Ng, Ruchir Travadi, Youyuan Zhang, Mirko Hannemann, Man-Hung Siu, Zhen Huang</li>
<li>for: 这个论文旨在提高端到端语音识别系统的个性化性，使其能够更准确地识别个人内容，如联系人姓名。</li>
<li>methods: 该论文基于连接主义时间分类的技术，提出了一种生成个人实体唤起的新的子词tokenization方法。此外，该论文还使用了两种已知技术：上下文偏移和词段均衡。</li>
<li>results: 根据论文的表述，使用这些技术组合后，个人名实体识别精度与一个竞争性hybrid系统相当。<details>
<summary>Abstract</summary>
Recent advances in deep learning and automatic speech recognition have improved the accuracy of end-to-end speech recognition systems, but recognition of personal content such as contact names remains a challenge. In this work, we describe our personalization solution for an end-to-end speech recognition system based on connectionist temporal classification. Building on previous work, we present a novel method for generating additional subword tokenizations for personal entities from their pronunciations. We show that using this technique in combination with two established techniques, contextual biasing and wordpiece prior normalization, we are able to achieve personal named entity accuracy on par with a competitive hybrid system.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的深度学习和自动语音识别技术的进步，已经提高了端到端语音识别系统的准确率，但是个人内容such as contact names仍然是一个挑战。在这项工作中，我们描述了基于连接主义时间分类的个人化解决方案。基于之前的工作，我们提出了一种新的方法，通过个人实体的发音来生成额外的子字符串拼接。我们表明，使用这种技术与两种已知技术，Contextual biasing和wordpiece prior normalization，可以达到与竞争性混合系统相同的个人名实体准确率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/cs.LG_2023_10_16/" data-id="clp869u0w00tdk5882ck2fyfd" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/eess.IV_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T09:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/eess.IV_2023_10_16/">eess.IV - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Overcoming-the-Rayleigh-limit-in-extremely-low-SNR"><a href="#Overcoming-the-Rayleigh-limit-in-extremely-low-SNR" class="headerlink" title="Overcoming the Rayleigh limit in extremely low SNR"></a>Overcoming the Rayleigh limit in extremely low SNR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10633">http://arxiv.org/abs/2310.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunsoo Choi, Seungman Choi, Peter Menart, Angshuman Deka, Zubin Jacob<br>for:* 这个论文的目的是开发一种新的随机子衰减图像重构算法（SSRI），以优化低信号响应率（SNR）和分辨率较低的光学成像系统。methods:* 该算法利用了常见的成像设备，使其在实际应用中易于适应。results:* 对于多种挑战性的场景，如非常低的SNR水平和较大的相对亮度比，SSRI算法表现出色，超过了已知的理德兹逊-卢西（Richardson-Lucy）减 convolution和CLEAN算法。* SSRI算法在实验图像中成功地估计了点源的位置、亮度和数量，并且在SNR水平低于1.2和子衰减范围内表现出80%-40%的成功率，位偏误在2.5像素以下。<details>
<summary>Abstract</summary>
Overcoming the diffraction limit and addressing low Signal-to-Noise Ratio (SNR) scenarios have posed significant challenges to optical imaging systems in applications such as medical diagnosis, remote sensing, and astronomical observations. In this study, we introduce a novel Stochastic Sub-Rayleigh Imaging (SSRI) algorithm capable of localizing point sources and estimating their positions, brightness, and number in low SNR conditions and within the diffraction limit. The SSRI algorithm utilizes conventional imaging devices, facilitating practical and adaptable solutions for real-world applications. Through extensive experimentation, we demonstrate that our proposed method outperforms established algorithms, such as Richardson-Lucy deconvolution and CLEAN, in various challenging scenarios, including extremely low SNR conditions and large relative brightness ratios. We achieved between 40% and 80% success rate in estimating the number of point sources in experimental images with SNR less than 1.2 and sub-Rayleigh separations, with mean position errors less than 2.5 pixels. In the same conditions, the Richardson-Lucy and CLEAN algorithms correctly estimated the number of sources between 0% and 10% of the time, with mean position errors greater than 5 pixels. Notably, SSRI consistently performs well even in the sub-Rayleigh region, offering a benchmark for assessing future quantum superresolution techniques. In conclusion, the SSRI algorithm presents a significant advance in overcoming diffraction limitations in optical imaging systems, particularly under low SNR conditions, with potential widespread impact across multiple fields like biomedical microscopy and astronomical imaging.
</details>
<details>
<summary>摘要</summary>
超过 diffraction limit 和低信号噪比 (SNR) 场景下，光学成像系统在医疗诊断、远程探测和天文观测等领域中受到了重大挑战。在这种研究中，我们介绍了一种新的 Stochastic Sub-Rayleigh Imaging（SSRI）算法，能够在低 SNR 条件下和 diffraction limit 内 Localize 点源并估计其位置、亮度和数量。SSRI 算法可以使用普通的成像设备，提供了实用和适应的解决方案。经过广泛的实验，我们证明了我们的提posed方法在各种挑战性enario中都能够超过Richardson-Lucy 混合和 CLEAN 算法，包括 extremely low SNR 条件下和大relative brightness ratio。我们在实验图像中成功地估计了40%到80%的点源数量，位置误差在2.5 pix 左右，而Richardson-Lucy 和 CLEAN 算法只能在0%到10%的时间内正确地估计点源数量，位置误差大于5 pix。特别是，SSRI 在 sub-Rayleigh 区域中表现良好，为未来 quantum superresolution 技术的评估提供了标准。综上所述，SSRI 算法在光学成像系统中超过 diffraction limit 的能力，特别是在低 SNR 条件下，具有广泛的应用前景，如生物微scopy 和天文成像。
</details></li>
</ul>
<hr>
<h2 id="NeuroQuantify-–-An-Image-Analysis-Software-for-Detection-and-Quantification-of-Neurons-and-Neurites-using-Deep-Learning"><a href="#NeuroQuantify-–-An-Image-Analysis-Software-for-Detection-and-Quantification-of-Neurons-and-Neurites-using-Deep-Learning" class="headerlink" title="NeuroQuantify – An Image Analysis Software for Detection and Quantification of Neurons and Neurites using Deep Learning"></a>NeuroQuantify – An Image Analysis Software for Detection and Quantification of Neurons and Neurites using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10978">http://arxiv.org/abs/2310.10978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/StanleyZ0528/neural-image-segmentation">https://github.com/StanleyZ0528/neural-image-segmentation</a></li>
<li>paper_authors: Ka My Dang, Yi Jia Zhang, Tianchen Zhang, Chao Wang, Anton Sinner, Piero Coronica, Joyce K. S. Poon</li>
<li>for: 研究neuronal networks的发展和neuron growth的量化信息</li>
<li>methods: 使用深度学习自动分类 cells和neurites</li>
<li>results: 可以快速和高效地分类 cells和neurites，并提供neurite length和orientation的量化信息<details>
<summary>Abstract</summary>
The segmentation of cells and neurites in microscopy images of neuronal networks provides valuable quantitative information about neuron growth and neuronal differentiation, including the number of cells, neurites, neurite length and neurite orientation. This information is essential for assessing the development of neuronal networks in response to extracellular stimuli, which is useful for studying neuronal structures, for example, the study of neurodegenerative diseases and pharmaceuticals. However, automatic and accurate analysis of neuronal structures from phase contrast images has remained challenging. To address this, we have developed NeuroQuantify, an open-source software that uses deep learning to efficiently and quickly segment cells and neurites in phase contrast microscopy images. NeuroQuantify offers several key features: (i) automatic detection of cells and neurites; (ii) post-processing of the images for the quantitative neurite length measurement based on segmentation of phase contrast microscopy images, and (iii) identification of neurite orientations. The user-friendly NeuroQuantify software can be installed and freely downloaded from GitHub https://github.com/StanleyZ0528/neural-image-segmentation.
</details>
<details>
<summary>摘要</summary>
segmenation of cells and neurites in microscopy images of neuronal networks provides valuable quantitative information about neuron growth and neuronal differentiation, including the number of cells, neurites, neurite length and neurite orientation. This information is essential for assessing the development of neuronal networks in response to extracellular stimuli, which is useful for studying neuronal structures, for example, the study of neurodegenerative diseases and pharmaceuticals. However, automatic and accurate analysis of neuronal structures from phase contrast images has remained challenging. To address this, we have developed NeuroQuantify, an open-source software that uses deep learning to efficiently and quickly segment cells and neurites in phase contrast microscopy images. NeuroQuantify offers several key features: (i) automatic detection of cells and neurites; (ii) post-processing of the images for the quantitative neurite length measurement based on segmentation of phase contrast microscopy images, and (iii) identification of neurite orientations. The user-friendly NeuroQuantify software can be installed and freely downloaded from GitHub https://github.com/StanleyZ0528/neural-image-segmentation.Here's the word-for-word translation of the text into Simplified Chinese: cells 和 neurites 的分 segmentation 在 neuronal networks 的 microscopy 图像中提供了有价值的量化信息，包括细胞数量、辐化长度、辐化方向等。这些信息对于研究 neuronal networks 的发展响应 extracellular stimuli 非常重要，这些信息可以用于研究 neuronal structures，例如研究 neuodegenerative diseases 和 pharmaceuticals。然而，从 phase contrast 图像中自动和准确地分析 neuronal structures 一直是一个挑战。为解决这个问题，我们已经开发了 NeuroQuantify，一个开源的软件，使用深度学习来快速和高效地分 segmentation 细胞和 neurites 在 phase contrast microscopy 图像中。NeuroQuantify 提供了多个关键特性： (i) 自动检测细胞和辐化； (ii) 根据 segmentation 的图像进行后处理，以获取辐化长度的量化测量；以及 (iii) 辐化方向的识别。用户友好的 NeuroQuantify 软件可以在 GitHub 上免费下载 https://github.com/StanleyZ0528/neural-image-segmentation。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Data-Synthesis-Strategies-for-the-Classification-of-Craniosynostosis"><a href="#Impact-of-Data-Synthesis-Strategies-for-the-Classification-of-Craniosynostosis" class="headerlink" title="Impact of Data Synthesis Strategies for the Classification of Craniosynostosis"></a>Impact of Data Synthesis Strategies for the Classification of Craniosynostosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10199">http://arxiv.org/abs/2310.10199</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kit-ibt/craniosource-gan-pca-ssm">https://github.com/kit-ibt/craniosource-gan-pca-ssm</a></li>
<li>paper_authors: Matthias Schaufelberger, Reinald Peter Kühle, Andreas Wachter, Frederic Weichel, Niclas Hagen, Friedemann Ringwald, Urs Eisenmann, Jürgen Hoffmann, Michael Engel, Christian Freudlsperger, Werner Nahm</li>
<li>for: 用于评估和分类颅部凹陷症。</li>
<li>methods: 使用三种不同的人工数据源：统计学形态模型（SSM）、生成敌对网络（GAN）和图像基本主成分分析，为一种基于 convolutional neural network（CNN）的颅部凹陷症分类。CNN 只在人工数据上训练，但 Validate 和测试在临床数据上。</li>
<li>results: 组合 SSM 和 GAN 达到了高于 0.96 的准确率和高于 0.95 的 F1 分数在未看到的测试集上。与训练在临床数据上的差异小于 0.01。包括第二个图像模式可以提高分类性能。So the three key points are:1. The paper is written to assess and classify craniosynostosis using photogrammetric surface scans.2. The methods used include three different synthetic data sources: a statistical shape model, a generative adversarial network, and image-based principal component analysis.3. The results show that a combination of these synthetic data sources can achieve high accuracy and F1 score (over 0.95 and 0.96 respectively) on unseen test sets, with little difference between training on synthetic data and clinical data.<details>
<summary>Abstract</summary>
Introduction: Photogrammetric surface scans provide a radiation-free option to assess and classify craniosynostosis. Due to the low prevalence of craniosynostosis and high patient restrictions, clinical data is rare. Synthetic data could support or even replace clinical data for the classification of craniosynostosis, but this has never been studied systematically. Methods: We test the combinations of three different synthetic data sources: a statistical shape model (SSM), a generative adversarial network (GAN), and image-based principal component analysis for a convolutional neural network (CNN)-based classification of craniosynostosis. The CNN is trained only on synthetic data, but validated and tested on clinical data. Results: The combination of a SSM and a GAN achieved an accuracy of more than 0.96 and a F1-score of more than 0.95 on the unseen test set. The difference to training on clinical data was smaller than 0.01. Including a second image modality improved classification performance for all data sources. Conclusion: Without a single clinical training sample, a CNN was able to classify head deformities as accurate as if it was trained on clinical data. Using multiple data sources was key for a good classification based on synthetic data alone. Synthetic data might play an important future role in the assessment of craniosynostosis.
</details>
<details>
<summary>摘要</summary>
方法：我们测试了三种不同的生成数据源：统计学形态模型（SSM）、生成对抗网络（GAN）和图像基于主成分分析（PCA），用于基于 convolutional neural network（CNN）的颅部缺陷分类。CNN只在生成数据上训练，但VALIDATE和测试在临床数据上进行验证。结果：SSM和GAN的组合实现了在未看到的测试集上的准确率高于0.96和F1分数高于0.95。与训练在临床数据上的差异小于0.01。包括第二个图像特征提高了所有数据源的分类性能。结论：没有任何临床训练样本，CNN仍可以准确地将头形缺陷分类为临床数据。使用多种数据源是针对synthetic数据alone的分类的关键。synthetic数据可能在未来对颅部缺陷的评估中扮演一个重要的角色。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/eess.IV_2023_10_16/" data-id="clp869u7u01bok588dn8o2boz" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/16/eess.SP_2023_10_16/" class="article-date">
  <time datetime="2023-10-16T08:00:00.000Z" itemprop="datePublished">2023-10-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/16/eess.SP_2023_10_16/">eess.SP - 2023-10-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Rapid-Non-cartesian-Reconstruction-Using-an-Implicit-Representation-of-GROG-Kernels"><a href="#Rapid-Non-cartesian-Reconstruction-Using-an-Implicit-Representation-of-GROG-Kernels" class="headerlink" title="Rapid Non-cartesian Reconstruction Using an Implicit Representation of GROG Kernels"></a>Rapid Non-cartesian Reconstruction Using an Implicit Representation of GROG Kernels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10823">http://arxiv.org/abs/2310.10823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Abraham, Mark Nishimura, Xiaozhi Cao, Congyu Liao, Kawin Setsompop</li>
<li>for: 提高MR图像成像的速度和效率，使非 carteesian sampling更广泛应用</li>
<li>methods: 使用iGROG方法将非 carteesian数据转换为cartesian数据，以便更加简单和快速地进行重建</li>
<li>results: 提高了MR图像成像的速度和效率，并且可以更好地抵消运动 artifacts<details>
<summary>Abstract</summary>
MRI data is acquired in Fourier space. Data acquisition is typically performed on a Cartesian grid in this space to enable the use of a fast Fourier transform algorithm to achieve fast and efficient reconstruction. However, it has been shown that for multiple applications, non-Cartesian data acquisition can improve the performance of MR imaging by providing fast and more efficient data acquisition, and improving motion robustness. Nonetheless, the image reconstruction process of non-Cartesian data is more involved and can be time-consuming, even through the use of efficient algorithms such as non-uniform FFT (NUFFT). This work provides an efficient approach (iGROG) to transform the non-Cartesian data into Cartesian data, to achieve simpler and faster reconstruction which should help enable non-Cartesian data sampling to be performed more widely in MRI.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Constant-Modulus-Waveform-Design-with-Block-Level-Interference-Exploitation-for-DFRC-Systems"><a href="#Constant-Modulus-Waveform-Design-with-Block-Level-Interference-Exploitation-for-DFRC-Systems" class="headerlink" title="Constant Modulus Waveform Design with Block-Level Interference Exploitation for DFRC Systems"></a>Constant Modulus Waveform Design with Block-Level Interference Exploitation for DFRC Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10804">http://arxiv.org/abs/2310.10804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Byunghyun Lee, Anindya Bijoy Das, David J. Love, Christopher G. Brinton, James V. Krogmeier</li>
<li>for: 这篇论文旨在设计具有双 функ数 radar-通信（DFRC）系统的常数模ulus波形。</li>
<li>methods: 本文使用了相互干扰基于封页水平 precoding（CI-BLP）来利用多用户和雷达传输所带来的歪曲。我们还提出了一个基于主要化-最小化（MM）的解决方案，并使用了一个改进的主要化函数，充分利用了一个新的 діагональ矩阵结构。</li>
<li>results: 透过严谨的 simulations，我们证明了提案的方法和主要化函数的效果。<details>
<summary>Abstract</summary>
Dual-functional radar-communication (DFRC) is a promising technology where radar and communication functions operate on the same spectrum and hardware. In this paper, we propose an algorithm for designing constant modulus waveforms for DFRC systems. Particularly, we jointly optimize the correlation properties and the spatial beam pattern. For communication, we employ constructive interference-based block-level precoding (CI-BLP) to exploit distortion due to multi-user and radar transmission. We propose a majorization-minimization (MM)-based solution to the formulated problem. To accelerate convergence, we propose an improved majorizing function that leverages a novel diagonal matrix structure. We then evaluate the performance of the proposed algorithm through rigorous simulations. Simulation results demonstrate the effectiveness of the proposed approach and the proposed majorizer.
</details>
<details>
<summary>摘要</summary>
双功能雷达通信（DFRC）技术是一种有前途的技术，雷达和通信功能都运行在同一频谱和硬件上。在这篇论文中，我们提出了一种常数模式波形设计算法，特别是同时优化相关性和空间扫描方式。在通信方面，我们使用基于构建性干扰的块级预编码（CI-BLP）来利用多用户和雷达传输所导致的扭曲。我们提出了一种基于主要化-最小化（MM）的解决方案，并提出了一种改进的主要化函数，利用了一个新的对角矩阵结构。然后，我们通过严格的仿真测试评估了提案的性能和提案的主要化函数。Here's the text with some additional information about the Simplified Chinese translation:Simplified Chinese is a written form of Chinese that uses simpler characters and grammar than Traditional Chinese. It is commonly used in mainland China and Singapore.In this translation, I have used Simplified Chinese characters and grammar to translate the text. However, I have kept the original English sentence structure and phrasing to ensure that the meaning of the text is preserved.Note that some technical terms and jargon may have different translations in Simplified Chinese, depending on the context and the specific field of study. However, I have tried my best to provide an accurate and natural-sounding translation based on my knowledge of Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Neuromorphic-Place-Cells"><a href="#Neuromorphic-Place-Cells" class="headerlink" title="Neuromorphic Place Cells"></a>Neuromorphic Place Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10790">http://arxiv.org/abs/2310.10790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoqi Chen, Ralph Etienne-Cummings</li>
<li>for: 这个脑机模型系统可能比传统系统更加高效实现。</li>
<li>methods: 我们实现了混合模式的空间编码神经元，包括theta细胞、vector细胞和place细胞。这些神经元组成了生物学可能的网络，可以重produce地方Cells的localization功能。</li>
<li>results: 我们的模型在Analog Circuit变化时的Robustness得到了实验 validate。我们提供了动态脑机SLAM系统的实现基础和生物学形成空间细胞的灵感。<details>
<summary>Abstract</summary>
A neuromorphic SLAM system shows potential for more efficient implementation than its traditional counterpart. We demonstrate a mixed-mode implementation for spatial encoding neurons including theta cells, vector cells and place cells. Together, they form a biologically plausible network that could reproduce the localization functionality of place cells. Experimental results validate the robustness of our model when suffering from variations of analog circuits. We provide a foundation for implementing dynamic neuromorphic SLAM systems and inspirations for the formation of spatial cells in biology.
</details>
<details>
<summary>摘要</summary>
一种神经模拟SLAM系统显示了更高效的实现可能性，相比传统系统。我们实现了混合模式的空间编码神经元，包括theta细胞、向量细胞和位置细胞。这些神经元共同组成了生物学上可能的网络，可以重现位置细胞的地方化功能。实验结果证明我们的模型在 анаóg逻circuit变化时的稳定性。我们提供了神经模拟SLAM系统的实现基础和生物学形成空间细胞的灵感。Note: "Simplified Chinese" is also known as "Mandarin Chinese" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Indoor-Wireless-Signal-Modeling-with-Smooth-Surface-Diffraction-Effects"><a href="#Indoor-Wireless-Signal-Modeling-with-Smooth-Surface-Diffraction-Effects" class="headerlink" title="Indoor Wireless Signal Modeling with Smooth Surface Diffraction Effects"></a>Indoor Wireless Signal Modeling with Smooth Surface Diffraction Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10578">http://arxiv.org/abs/2310.10578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruichen Wang, Samuel Audia, Dinesh Manocha</li>
<li>for: 提高室内电磁场模拟的准确性，包括表面散射的影响</li>
<li>methods: 使用统一几何理论 Of Diffraction (UTD) 表面散射，并提出了精炼表面 UTD 和高效计算射线路的技术</li>
<li>results: 提高阴影区预测功率约 5dB，并能够捕捉到阴影区之外的复杂场效应，并且在不同室内场景下表现出60%更快的计算速度。<details>
<summary>Abstract</summary>
We present a novel algorithm that enhances the accuracy of electromagnetic field simulations in indoor environments by incorporating the Uniform Geometrical Theory of Diffraction (UTD) for surface diffraction. This additional diffraction phenomenology is important for the design of modern wireless systems and allows us to capture the effects of more complex scene geometries. Central to our methodology is the Dynamic Coherence-Based EM Ray Tracing Simulator (DCEM), and we augment that formulation with smooth surface UTD and present techniques to efficiently compute the ray paths. We validate our additions by comparing them to analytical solutions of a sphere, method of moments solutions from FEKO, and ray-traced indoor scenes from WinProp. Our algorithm improves shadow region predicted powers by about 5dB compared to our previous work, and captures nuanced field effects beyond shadow boundaries. We highlight the performance on different indoor scenes and observe 60% faster computation time over WinProp.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的算法，用于提高室内电磁场 simulations 的准确性，通过包含表面折射理论（UTD）的各向异性折射现象。这种额外的折射现象对现代无线系统设计非常重要，允许我们捕捉更复杂的场景几何。我们的方法中心是动态几何相关性基于EM射线追踪模拟器（DCEM），并在该形式ulation中添加了光滑表面UTD。我们还提供了有效计算射线路的技术。我们的添加与 Analytical solutions of a sphere、method of moments solutions from FEKO和WinProp的射线追踪场景进行比较。我们的算法可以在不同的室内场景中提高阴影区域预测功率约5dB，并捕捉到场效应 beyond shadow boundaries。我们还证明了我们的算法在不同的室内场景中的性能，并发现其计算时间比WinProp快60%。
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Distributed-Machine-Learning-for-the-Internet-of-Things-A-Comprehensive-Survey"><a href="#Applications-of-Distributed-Machine-Learning-for-the-Internet-of-Things-A-Comprehensive-Survey" class="headerlink" title="Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey"></a>Applications of Distributed Machine Learning for the Internet-of-Things: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10549">http://arxiv.org/abs/2310.10549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mai Le, Thien Huynh-The, Tan Do-Duy, Thai-Hoc Vu, Won-Joo Hwang, Quoc-Viet Pham</li>
<li>for: 提高 emerging 无线网络（如 beyond 5G 和 6G）中的服务和应用程序的质量，通过在互联网物联网（IoT）中使用人工智能（AI）。</li>
<li>methods: 分布式机器学习（distributed learning）方法，包括联邦学习、多代理奖励学习和分布式推理。</li>
<li>results: 对 IoT 服务和应用程序的重要提高，包括数据共享和计算卸载、定位、移动 Crowdsensing 和安全隐私。<details>
<summary>Abstract</summary>
The emergence of new services and applications in emerging wireless networks (e.g., beyond 5G and 6G) has shown a growing demand for the usage of artificial intelligence (AI) in the Internet of Things (IoT). However, the proliferation of massive IoT connections and the availability of computing resources distributed across future IoT systems have strongly demanded the development of distributed AI for better IoT services and applications. Therefore, existing AI-enabled IoT systems can be enhanced by implementing distributed machine learning (aka distributed learning) approaches. This work aims to provide a comprehensive survey on distributed learning for IoT services and applications in emerging networks. In particular, we first provide a background of machine learning and present a preliminary to typical distributed learning approaches, such as federated learning, multi-agent reinforcement learning, and distributed inference. Then, we provide an extensive review of distributed learning for critical IoT services (e.g., data sharing and computation offloading, localization, mobile crowdsensing, and security and privacy) and IoT applications (e.g., smart healthcare, smart grid, autonomous vehicle, aerial IoT networks, and smart industry). From the reviewed literature, we also present critical challenges of distributed learning for IoT and propose several promising solutions and research directions in this emerging area.
</details>
<details>
<summary>摘要</summary>
随着新的服务和应用程序在 развивающихся无线网络（例如 beyond 5G 和 6G）的出现，人们对艺ificial intelligence（AI）在互联网物联网（IoT）中的使用的需求在增加。然而，质量的巨大 IoT 连接和未来 IoT 系统中的计算资源的分布强烈要求开发分布式 AI，以提供更好的 IoT 服务和应用程序。因此，现有的 AI 启用 IoT 系统可以通过实施分布式机器学习（分布式学习）方法进行增强。本工作的目的是为提供分布式学习在 IoT 服务和应用程序方面的全面的评价。特别是，我们首先提供机器学习的背景，然后介绍一些常见的分布式学习方法，如联合学习、多代理人奖励学习和分布式推理。然后，我们对分布式学习在关键的 IoT 服务（例如数据分享和计算卸载、位置定位、移动 Crowdsensing 和安全隐私）和 IoT 应用程序（例如智能医疗、智能电网、自动驾驶、空中 IoT 网络和智能工业）进行了广泛的评审。从 Literature 中，我们还提出了分布式学习在 IoT 中的主要挑战和一些可能的解决方案和研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Tutorial-on-Chirp-Spread-Spectrum-for-LoRaWAN-Basics-and-Key-Advances"><a href="#A-Tutorial-on-Chirp-Spread-Spectrum-for-LoRaWAN-Basics-and-Key-Advances" class="headerlink" title="A Tutorial on Chirp Spread Spectrum for LoRaWAN: Basics and Key Advances"></a>A Tutorial on Chirp Spread Spectrum for LoRaWAN: Basics and Key Advances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10503">http://arxiv.org/abs/2310.10503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Maleki, Ha H. Nguyen, Ebrahim Bedeer, Robert Barton</li>
<li>for: 本研究旨在提供一个全面的CSS模ulation在LoRaWAN应用中的教程，包括信号生成、检测、错误性表现和频率特性等方面的分析。</li>
<li>methods: 本研究使用了LoRa特有的CSS模ulation，并对其在IoT网络中的应用进行了深入的检查和分析。</li>
<li>results: 研究发现CSS模ulation在LoRaWAN应用中具有优秀的错误性和spectral caracteristics，并提出了一些适用于IoT网络的CSS模ulation应用的新技术和算法。<details>
<summary>Abstract</summary>
Chirps spread spectrum (CSS) modulation is the heart of long-range (LoRa) modulation used in the context of long-range wide area network (LoRaWAN) in internet of things (IoT) scenarios. Despite being a proprietary technology owned by Semtech Corp., LoRa modulation has drawn much attention from the research and industry communities in recent years. However, to the best of our knowledge, a comprehensive tutorial, investigating the CSS modulation in the LoRaWAN application, is missing in the literature. Therefore, in the first part of this paper, we provide a thorough analysis and tutorial of CSS modulation modified by LoRa specifications, discussing various aspects such as signal generation, detection, error performance, and spectral characteristics. Moreover, a summary of key recent advances in the context of CSS modulation applications in IoT networks is presented in the second part of this paper under four main categories of transceiver configuration and design, data rate improvement, interference modeling, and synchronization algorithms.
</details>
<details>
<summary>摘要</summary>
射频扩散模ulation (CSS) 是LoRa射频模ulation的核心，用于长距离宽频网络（LoRaWAN）应用场景中的物联网（IoT）。尽管LoRa模ulation是Semtech Corp.拥有的专有技术，但在过去几年中，研究和业界社区对其吸引了很多关注。然而，根据我们所知，Literature中没有一篇全面的教程，探讨CSS模ulation在LoRaWAN应用中的各个方面，包括信号生成、检测、错误性能和频谱特性。因此，在本文的第一部分中，我们提供了CSS模ulation在LoRaWAN应用中的全面分析和教程，讨论了各种方面。此外，在文章的第二部分中，我们还提供了针对CSS模ulation在IoT网络应用中的四个主要类别的扩散配置和设计、数据速率改进、干扰模型和同步算法的最新进展。
</details></li>
</ul>
<hr>
<h2 id="Performance-Analysis-of-a-Low-Complexity-OTFS-Integrated-Sensing-and-Communication-System"><a href="#Performance-Analysis-of-a-Low-Complexity-OTFS-Integrated-Sensing-and-Communication-System" class="headerlink" title="Performance Analysis of a Low-Complexity OTFS Integrated Sensing and Communication System"></a>Performance Analysis of a Low-Complexity OTFS Integrated Sensing and Communication System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10476">http://arxiv.org/abs/2310.10476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Bacchielli, Lorenzo Pucci, Enrico Paolini, Andrea Giorgetti</li>
<li>for: 该论文提出了一种低复杂度估计方法，用于 ortfs 基于集成感知通信（isac）系统。</li>
<li>methods: 我们首先定义了四个低维度矩阵，用于计算通道矩阵通过简单的代数手动操作。然后，我们建立了一个独立系统参数的分析标准，用于 Identify the most informative elements within these derived matrices，利用 Dirichlet kernel 的性质。这使得我们可以简化这些矩阵，保留只有关键的元素，从而实现高效、低复杂度的感知接收器。</li>
<li>results: 数字结果表明，提出的近似技术可以高效地保持感知性能， measured in terms of root mean square error (RMSE) of the range and velocity estimation， 同时减少计算努力 enormously。<details>
<summary>Abstract</summary>
This work proposes a low-complexity estimation approach for an orthogonal time frequency space (OTFS)-based integrated sensing and communication (ISAC) system. In particular, we first define four low-dimensional matrices used to compute the channel matrix through simple algebraic manipulations. Secondly, we establish an analytical criterion, independent of system parameters, to identify the most informative elements within these derived matrices, leveraging the properties of the Dirichlet kernel. This allows the distilling of such matrices, keeping only those entries that are essential for detection, resulting in an efficient, low-complexity implementation of the sensing receiver. Numerical results, which refer to a vehicular scenario, demonstrate that the proposed approximation technique effectively preserves the sensing performance, evaluated in terms of root mean square error (RMSE) of the range and velocity estimation, while concurrently reducing the computational effort enormously.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The traditional Chinese writing system is also widely used, especially in Taiwan and Hong Kong. If you prefer the traditional Chinese writing system, please let me know and I can provide the translation accordingly.
</details></li>
</ul>
<hr>
<h2 id="Flag-Sequence-Set-Design-for-Low-Complexity-Delay-Doppler-Estimation"><a href="#Flag-Sequence-Set-Design-for-Low-Complexity-Delay-Doppler-Estimation" class="headerlink" title="Flag Sequence Set Design for Low-Complexity Delay-Doppler Estimation"></a>Flag Sequence Set Design for Low-Complexity Delay-Doppler Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10457">http://arxiv.org/abs/2310.10457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingsheng Meng, Yong Liang Guan, Yao Ge, Zilong Liu</li>
<li>for: 该 paper 探讨了用 Flag 序列实现低复杂度延迟-多普勒估计，通过利用 Flag 序列的特殊峰柜ambiguity函数（AF）。不同于现有的 Flag 序列设计，我们的设计不受 prime 长度和 periodic auto-AF 的限制，而是设计了 Flag 序列集合的任意长度和低（非极） periodic&#x2F;aperiodic auto-和cross-AF。</li>
<li>methods: 我们首先investigated Zone-based Curtain sequence sets of arbitrary lengths的代数设计。我们的提议的设计导致了新的 Curtain sequence sets，其具有理想的毯幕自动ambiguity函数（AF）和低&#x2F;zero cross-AF在延迟-多普勒频率范围内。使用这些 Curtain sequence sets，我们提出了两个优化问题，以最小化 Flag sequence set的总成本weighted integrated sidelobe level（SCWISL）。我们还提出了一种加速Parallel Partially Majorization-Minimization Algorithm，用于同时优化发射 Flag sequence和与其匹配&#x2F;不匹配的参照序列。</li>
<li>results: 我们的实验结果表明，我们的提议 Flag sequences 比现有 Flag sequences 具有更好的 SCWISL 和自定义峰-侧噪比。此外，我们的 Flag sequences under Flag method 的 Mean Squared Errors 逐渐接近 Cramer-Rao Lower Bound 和 Sampling Bound，当信号噪声比高 enough 时。<details>
<summary>Abstract</summary>
This paper studies Flag sequences for lowcomplexity delay-Doppler estimation by exploiting their distinctive peak-curtain ambiguity functions (AFs). Unlike the existing Flag sequence designs that are limited to prime lengths and periodic auto-AFs, we aim to design Flag sequence sets of arbitrary lengths and with low (nontrivial) periodic/aperiodic auto- and cross-AFs. Since every Flag sequence consists of a Curtain sequence and a Peak sequence, we first investigate the algebraic design of zone-based Curtain sequence sets of arbitrary lengths. Our proposed design gives rise to novel Curtain sequence sets with ideal curtain auto-AFs and low/zero cross-AFs within the delay-Doppler zone of interest. Leveraging these Curtain sequence sets, two optimization problems are formulated to minimize the summed customized weighted integrated sidelobe level (SCWISL) of the Flag sequence set. Accelerated Parallel Partially Majorization-Minimization Algorithms are proposed to jointly optimize the transmit Flag sequences and matched/mismatched reference sequences stored in the receiver. Simulations demonstrate that our proposed Flag sequences lead to improved SCWISL and customized peak-to-max-sidelobe ratio compared with the existing Flag sequences. Additionally, our Flag sequences under Flag method exhibit Mean Squared Errors that approach the Cramer-Rao Lower Bound and the Sampling Bound at high signal-to-noise power ratios.
</details>
<details>
<summary>摘要</summary>
Since every Flag sequence consists of a Curtain sequence and a Peak sequence, we first investigate the algebraic design of zone-based Curtain sequence sets of arbitrary lengths. Our proposed design yields novel Curtain sequence sets with ideal curtain auto-AFs and low/zero cross-AFs within the delay-Doppler zone of interest.Leveraging these Curtain sequence sets, two optimization problems are formulated to minimize the summed customized weighted integrated sidelobe level (SCWISL) of the Flag sequence set. Accelerated Parallel Partially Majorization-Minimization Algorithms are proposed to jointly optimize the transmit Flag sequences and matched/mismatched reference sequences stored in the receiver.Simulations show that our proposed Flag sequences lead to improved SCWISL and customized peak-to-max-sidelobe ratio compared with existing Flag sequences. Additionally, our Flag sequences under the Flag method exhibit Mean Squared Errors that approach the Cramer-Rao Lower Bound and the Sampling Bound at high signal-to-noise power ratios.
</details></li>
</ul>
<hr>
<h2 id="Soft-Demodulator-for-Symbol-Level-Precoding-in-Coded-Multiuser-MISO-Systems"><a href="#Soft-Demodulator-for-Symbol-Level-Precoding-in-Coded-Multiuser-MISO-Systems" class="headerlink" title="Soft Demodulator for Symbol-Level Precoding in Coded Multiuser MISO Systems"></a>Soft Demodulator for Symbol-Level Precoding in Coded Multiuser MISO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10296">http://arxiv.org/abs/2310.10296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yafei Wang, Hongwei Hou, Wenjin Wang, Xinping Yi, Shi Jin</li>
<li>for: 本文研究了Symbol-level precoding (SLP)在channel-coded多用户多输入单输出（MISO）系统中的应用。</li>
<li>methods: 本文提出了一种新的软解调器设计方法，用于处理不符合 Gaussian 分布的 SLP 信号。</li>
<li>results: 实验结果表明，提议的软解调器可以减少现有 SLP 系统的通信 overhead 和计算复杂度，同时提高了传输率。<details>
<summary>Abstract</summary>
In this paper, we consider symbol-level precoding (SLP) in channel-coded multiuser multi-input single-output (MISO) systems. It is observed that the received SLP signals do not always follow Gaussian distribution, rendering the conventional soft demodulation with the Gaussian assumption unsuitable for the coded SLP systems. It, therefore, calls for novel soft demodulator designs for non-Gaussian distributed SLP signals with accurate log-likelihood ratio (LLR) calculation. To this end, we first investigate the non-Gaussian characteristics of both phase-shift keying (PSK) and quadrature amplitude modulation (QAM) received signals with existing SLP schemes and categorize the signals into two distinct types. The first type exhibits an approximate-Gaussian distribution with the outliers extending along the constructive interference region (CIR). In contrast, the second type follows some distribution that significantly deviates from the Gaussian distribution. To obtain accurate LLR, we propose the modified Gaussian soft demodulator and Gaussian mixture model (GMM) soft demodulators to deal with two types of signals respectively. Subsequently, to further reduce the computational complexity and pilot overhead, we put forward a novel neural soft demodulator, named pilot feature extraction network (PFEN), leveraging the transformer mechanism in deep learning. Simulation results show that the proposed soft demodulators dramatically improve the throughput of existing SLPs for both PSK and QAM transmission in coded systems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了符号级precoding（SLP）在多用户多输入单出力（MISO）系统中。我们发现接收的SLP信号不总是follow Gaussian分布，这使得传统的软模解器不适用于编码SLP系统。因此，我们需要设计新的软模解器，以便在非Gaussian分布下计算准确的log-likelihood ratio（LLR）。为此，我们首先研究了现有SLP方案中PSK和QAM接收信号的非Gaussian特征，并将信号分类为两种类型。第一种类型表现出近似Gaussian分布，其异常点分布在构建性干扰区（CIR）上。然而，第二种类型的信号具有显著不同于Gaussian分布的特征。为了获得准确的LLR，我们提议使用修改后Gaussian软模解器和Gaussian混合模型（GMM）软模解器来处理这两种信号。然后，为了进一步减少计算复杂性和导航点负担，我们提出了一种新的神经软模解器，即预测特征提取网络（PFEN），利用深度学习中的转换机制。实验结果表明，我们提出的软模解器可以很大程度提高现有SLP的传输能力。
</details></li>
</ul>
<hr>
<h2 id="A-Low-Complexity-Block-oriented-Functional-Link-Adaptive-Filtering-Algorithm"><a href="#A-Low-Complexity-Block-oriented-Functional-Link-Adaptive-Filtering-Algorithm" class="headerlink" title="A Low Complexity Block-oriented Functional Link Adaptive Filtering Algorithm"></a>A Low Complexity Block-oriented Functional Link Adaptive Filtering Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10276">http://arxiv.org/abs/2310.10276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavankumar Ganjimala, Subrahmanyam Mula</li>
<li>for: 模型无线非线性系统</li>
<li>methods: 使用块归一化功能链适应 Filter (BO-FLAF) 和 Hammersen BO trigonometric FLAF (HBO-TFLAF)</li>
<li>results: 对比原始 TFLAF，HBO-TFLAF 具有47%  fewer multiplications，并且 exhibits 更快的 convergence rate 和 3-5 dB 更好的稳态平均方差 (MSE) 表现。<details>
<summary>Abstract</summary>
The high computation complexity of nonlinear adaptive filtering algorithms poses significant challenges at the hardware implementation level. In order to tackle the computational complexity problem, this paper proposes a novel block-oriented functional link adaptive filter (BO-FLAF) to model memoryless nonlinear systems. Through theoretical complexity analysis, we show that the proposed Hammerstein BO trigonometric FLAF (HBO-TFLAF) has 47% lesser multiplications than the original TFLAF for a filter order of 1024. Moreover, the HBO-TFLAF exhibits a faster convergence rate and achieved 3-5 dB lesser steady-state mean square error (MSE) compared to the original TFLAF for a memoryless nonlinear system identification task.
</details>
<details>
<summary>摘要</summary>
高度计算复杂性的非线性适应滤波算法在硬件实现方面带来了重要的挑战。为了解决计算复杂性问题，这篇论文提议了一种新的块 oriented 函数链适应滤波器（BO-FLAF），用于模型无记忆非线性系统。通过理论复杂性分析，我们表明了提案的汽olinski BO  trigonometric FLAF（HBO-TFLAF）在缓存大小为 1024 的情况下，相比原始 TFLAF 减少了 47% 的 multiply 操作数。此外，HBO-TFLAF 还表现出更快的收敛速率和对非线性系统识别任务中的稳态平均幂二分量（MSE）减少了 3-5 dB。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-MTC-User-Activity-Detection-and-Channel-Estimation-with-Unknown-Spatial-Covariance"><a href="#Hierarchical-MTC-User-Activity-Detection-and-Channel-Estimation-with-Unknown-Spatial-Covariance" class="headerlink" title="Hierarchical MTC User Activity Detection and Channel Estimation with Unknown Spatial Covariance"></a>Hierarchical MTC User Activity Detection and Channel Estimation with Unknown Spatial Covariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.10204">http://arxiv.org/abs/2310.10204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Djelouat, Mikko J. Sillanpää, Markus Leinonen, Markku Juntti</li>
<li>for: 这篇论文解决了机器型通信中的共同用户标识和通道估计（JUICE）问题，采用实际的空间相关通道模型和未知 covariance 矩阵。</li>
<li>methods: 作者首先利用了强级先验的概念，并提出了层次稀突减模矩阵来模型结构化稀突活动模式。然后，他们 derivated了一种 bayesian 推理方案，将 expectation propagation（EP）算法和 expectation maximization（EM）框架结合起来。</li>
<li>results: 作者通过对 JUICE 问题进行最大 posteriori（MAP）估计，并提出了一种基于 alternating direction method of multipliers（ADMM）的计算效率高的解决方案。数据结果表明，提出的算法具有显著性能提升和具有不同用户稀突活动行为假设的Robustness。<details>
<summary>Abstract</summary>
This paper addresses the joint user identification and channel estimation (JUICE) problem in machine-type communications under the practical spatially correlated channels model with unknown covariance matrices. Furthermore, we consider an MTC network with hierarchical user activity patterns following an event-triggered traffic mode. Therein the users are distributed over clusters with a structured sporadic activity behaviour that exhibits both cluster-level and intra-cluster sparsity patterns. To solve the JUICE problem, we first leverage the concept of strong priors and propose a hierarchical-sparsity-inducing spike-and-slab prior to model the structured sparse activity pattern. Subsequently, we derive a Bayesian inference scheme by coupling the expectation propagation (EP) algorithm with the expectation maximization (EM) framework. Second, we reformulate the JUICE as a maximum a posteriori (MAP) estimation problem and propose a computationally-efficient solution based on the alternating direction method of multipliers (ADMM). More precisely, we relax the strong spike-and-slab prior with a cluster-sparsity-promoting prior based on the long-sum penalty. We then derive an ADMM algorithm that solves the MAP problem through a sequence of closed-form updates. Numerical results highlight the significant performance significant gains obtained by the proposed algorithms, as well as their robustness against various assumptions on the users sparse activity behaviour.
</details>
<details>
<summary>摘要</summary>
Second, we reformulate the JUICE as a maximum a posteriori (MAP) estimation problem and propose a computationally-efficient solution based on the alternating direction method of multipliers (ADMM). More precisely, we relax the strong spike-and-slab prior with a cluster-sparsity-promoting prior based on the long-sum penalty. We then derive an ADMM algorithm that solves the MAP problem through a sequence of closed-form updates. Numerical results highlight the significant performance gains obtained by the proposed algorithms, as well as their robustness against various assumptions on the users' sparse activity behavior.Translated into Simplified Chinese:这篇论文研究了机器类通信中的用户标识和通道估计（JUICE）问题，在实际的空间相关的通道模型下，并且假设用户活动模式遵循事件触发的交通模式。用户被分布在归一化的集群中，并且表现出了结构化零散的活动模式，这种模式包括集群水平和内部零散的特征。为解决JUICE问题，我们首先利用强级先验的概念，并提出了一种层次含拥权的钉板准则，以模型结构化零散的活动模式。然后，我们 deriv了一种 Bayesian 推理方案，通过将期望传播算法和期望最大化算法结合在一起。其次，我们将JUICE问题转换为最大 posteriori（MAP）估计问题，并提出了一种计算效率高的解决方案基于 alternating direction method of multipliers（ADMM）。更具体地，我们将强级钉板准则松弛为一种层次含拥权的长SUM penalty，然后 deriv 了一种 ADMM 算法来解决 MAP 问题。数据结果表明，我们提出的算法具有显著的性能提升和各种假设用户的稀疏活动行为下的稳定性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/16/eess.SP_2023_10_16/" data-id="clp869u9l01fxk5887h415wta" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/25/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="page-number" href="/page/25/">25</a><span class="page-number current">26</span><a class="page-number" href="/page/27/">27</a><a class="page-number" href="/page/28/">28</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/27/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
