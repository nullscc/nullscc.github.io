
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/35/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/eess.IV_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T09:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/eess.IV_2023_09_21/">eess.IV - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ISLAND-Informing-Brightness-and-Surface-Temperature-Through-a-Land-Cover-based-Interpolator"><a href="#ISLAND-Informing-Brightness-and-Surface-Temperature-Through-a-Land-Cover-based-Interpolator" class="headerlink" title="ISLAND: Informing Brightness and Surface Temperature Through a Land Cover-based Interpolator"></a>ISLAND: Informing Brightness and Surface Temperature Through a Land Cover-based Interpolator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12416">http://arxiv.org/abs/2309.12416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Liu, Pranavesh Panakkal, Sylvia Dee, Guha Balakrishnan, Jamie Padgett, Ashok Veeraraghavan</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†è§£å†³äº‘å¹•å¹²æ‰°remote sensing thermal imagingä¸­çš„é—®é¢˜è€Œå†™çš„ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³ISLANDæ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨äº†å…°è¾¾ç‰¹8å·å«æ˜Ÿçš„çƒ­çº¢å¤–å›¾åƒå’ŒNLCDåœ°é¢è¦†ç›–æ•°æ®ï¼Œé€šè¿‡ä¸€ç³»åˆ—çš„ç©ºé—´-æ—¶é—´æ»¤æ³¢æ¥é¢„æµ‹äº‘å¹•å¹²æ‰°æ‰€å¹²æ‰°çš„çƒ­åº¦å’Œåœ°é¢æ¸©åº¦ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡è´¨é‡å’Œé‡åº¦åˆ†æè¡¨æ˜ï¼ŒISLANDæ–¹æ³•åœ¨ä¸åŒçš„äº‘å¹•å¹²æ‰°å’Œåœ°é¢è¦†ç›–æ¡ä»¶ä¸‹å…·æœ‰è‰¯å¥½çš„é‡å»ºæ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰é«˜ç©ºé—´-æ—¶é—´åˆ†è¾¨ç‡ã€‚ authorsè¿˜æä¾›äº†20ä¸ªç¾å›½åŸå¸‚çš„å…¬å…±æ•°æ®é›†ï¼Œä»¥ä¾¿ç”¨äºè¯æ˜ISLANDæ–¹æ³•çš„å¯è¡Œæ€§å’Œåº”ç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
Cloud occlusion is a common problem in the field of remote sensing, particularly for thermal infrared imaging. Remote sensing thermal instruments onboard operational satellites are supposed to enable frequent and high-resolution observations over land; unfortunately, clouds adversely affect thermal signals by blocking outgoing longwave radiation emission from Earth's surface, interfering with the retrieved ground emission temperature. Such cloud contamination severely reduces the set of serviceable thermal images for downstream applications, making it impractical to perform intricate time-series analysis of land surface temperature (LST). In this paper, we introduce a novel method to remove cloud occlusions from Landsat 8 LST images. We call our method ISLAND, an acronym for Informing Brightness and Surface Temperature Through a Land Cover-based Interpolator. Our approach uses thermal infrared images from Landsat 8 (at 30 m resolution with 16-day revisit cycles) and the NLCD land cover dataset. Inspired by Tobler's first law of Geography, ISLAND predicts occluded brightness temperature and LST through a set of spatio-temporal filters that perform distance-weighted spatio-temporal interpolation. A critical feature of ISLAND is that the filters are land cover-class aware, making it particularly advantageous in complex urban settings with heterogeneous land cover types and distributions. Through qualitative and quantitative analysis, we show that ISLAND achieves robust reconstruction performance across a variety of cloud occlusion and surface land cover conditions, and with a high spatio-temporal resolution. We provide a public dataset of 20 U.S. cities with pre-computed ISLAND thermal infrared and LST outputs. Using several case studies, we demonstrate that ISLAND opens the door to a multitude of high-impact urban and environmental applications across the continental United States.
</details>
<details>
<summary>æ‘˜è¦</summary>
äº‘å±‚é®æŒ¡æ˜¯è¿œç¨‹æ„ŸçŸ¥é¢†åŸŸä¸­å¸¸è§çš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¯¹äºthermal infraredæˆåƒã€‚è¿œç¨‹æ„ŸçŸ¥thermalä»ªå™¨è£…è½½åœ¨è¿è¡Œçš„å«æ˜Ÿä¸Šï¼Œæ—¨åœ¨å®ç°é¢‘ç¹å’Œé«˜åˆ†è¾¨ç‡çš„åœ°è¡¨è§‚æµ‹;ç„¶è€Œï¼Œäº‘å±‚ä¼šé˜»æŒ¡åœ°è¡¨å‘å°„çš„é•¿æ³¢è¾å°„ï¼Œä½¿å¾—æŠ½å–åœ°è¡¨æ¸©åº¦çš„çƒ­æˆåƒå—åˆ°æŠ‘åˆ¶ï¼Œä»è€Œå‡å°‘å¯ç”¨çš„çƒ­æˆåƒæ•°æ®ï¼Œä½¿å¾—æ— æ³•è¿›è¡Œå¤æ‚çš„æ—¶é—´åºåˆ†æã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„äº‘å±‚é®æŒ¡é™¤å»æ–¹æ³•ï¼Œç§°ä¸ºISLANDï¼ˆè¡¨ç¤ºåœ°è¡¨æ¸©åº¦å’Œè¾å°„é€šè¿‡åœ°åŸŸæ¶‚æŠ¹ interpolatorï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨å«æ˜Ÿ8çš„çƒ­çº¢å¤–æˆåƒï¼ˆåˆ†è¾¨ç‡30ç±³ï¼Œå¤æ‚å‘¨æœŸ16å¤©ï¼‰å’ŒNLCDåœ°è¡¨è¦†ç›–æ•°æ®ã€‚å—åˆ° Tobler's first law of Geography çš„æ¿€å‘ï¼ŒISLAND é¢„æµ‹äº‘å±‚é®æŒ¡çš„æ˜äº®æ¸©åº¦å’Œåœ°è¡¨æ¸©åº¦é€šè¿‡ä¸€ç³»åˆ—çš„ç©ºé—´æ—¶é—´æ»¤æ³¢æ¥å®ç°ã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„ä¸€ä¸ªå…³é”®ç‰¹ç‚¹æ˜¯æ»¤æ³¢æ˜¯æ ¹æ®åœ°è¡¨è¦†ç›–ç±»å‹è¿›è¡Œåœ°åŸŸæ¶‚æŠ¹ï¼Œè¿™ä½¿å¾—å®ƒåœ¨å¤æ‚çš„åŸå¸‚ç¯å¢ƒä¸­å…·æœ‰ä¼˜åŠ¿ã€‚é€šè¿‡è´¨é‡å’Œé‡åŒ–åˆ†æï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†ISLAND åœ¨å¤šç§äº‘å±‚é®æŒ¡å’Œåœ°è¡¨è¦†ç›–æ¡ä»¶ä¸‹å…·æœ‰å¼ºå¥çš„é‡å»ºæ€§ï¼Œå¹¶ä¸”å…·æœ‰é«˜ç©ºé—´æ—¶é—´åˆ†è¾¨ç‡ã€‚æˆ‘ä»¬æä¾›äº†20ä¸ªç¾å›½åŸå¸‚çš„å‰è®¡ç®—ISLAND çƒ­çº¢å¤–æˆåƒå’Œåœ°è¡¨æ¸©åº¦è¾“å‡ºæ•°æ®ã€‚é€šè¿‡å¤šä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œæˆ‘ä»¬ç¤ºå‡ºäº†ISLAND å¯ä»¥å¼€å¯è®¸å¤šé«˜å½±å“çš„åŸå¸‚å’Œç¯å¢ƒåº”ç”¨ç¨‹åºï¼Œè¦†ç›–æ•´ä¸ªåŒ—ç¾å¤§é™†ã€‚
</details></li>
</ul>
<hr>
<h2 id="Bloch-Equation-Enables-Physics-informed-Neural-Network-in-Parametric-Magnetic-Resonance-Imaging"><a href="#Bloch-Equation-Enables-Physics-informed-Neural-Network-in-Parametric-Magnetic-Resonance-Imaging" class="headerlink" title="Bloch Equation Enables Physics-informed Neural Network in Parametric Magnetic Resonance Imaging"></a>Bloch Equation Enables Physics-informed Neural Network in Parametric Magnetic Resonance Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11763">http://arxiv.org/abs/2309.11763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingrui Cai, Liuhong Zhu, Jianjun Zhou, Chen Qian, Di Guo, Xiaobo Qu</li>
<li>for: ç”¨äºéä¾µå…¥æ€§çš„è¯Šæ–­ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ä¸­çš„é‡è¦æˆåƒæ–¹æ³•ä¹‹ä¸€ï¼Œå³æ ¸ç£å…±æŒ¯æˆåƒï¼ˆMRIï¼‰ã€‚</li>
<li>methods: æå‡ºä½¿ç”¨ç‰©ç†è§„åˆ™embedded into the loss of physics-informed neural networkï¼ˆPINNï¼‰æ¥å­¦ä¹  Bloch equationï¼Œå¹¶ä¸”é€šè¿‡è¿™ç§æ–¹æ³•æ¥ä¼°ç®—T2å‚æ•°å’Œç”Ÿæˆphysically synthetic dataã€‚</li>
<li>results: åœ¨phantomå’Œcardiac imagingä¸­è¿›è¡Œäº†å®éªŒï¼Œå¹¶å¾—åˆ°äº†è¿™ç§æ–¹æ³•çš„æ½œåœ¨åº”ç”¨äºé‡åŒ–MRIä¸­çš„å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) is an important non-invasive imaging method in clinical diagnosis. Beyond the common image structures, parametric imaging can provide the intrinsic tissue property thus could be used in quantitative evaluation. The emerging deep learning approach provides fast and accurate parameter estimation but still encounters the lack of network interpretation and enough training data. Even with a large amount of training data, the mismatch between the training and target data may introduce errors. Here, we propose one way that solely relies on the target scanned data and does not need a pre-defined training database. We provide a proof-of-concept that embeds the physical rule of MRI, the Bloch equation, into the loss of physics-informed neural network (PINN). PINN enables learning the Bloch equation, estimating the T2 parameter, and generating a series of physically synthetic data. Experimental results are conducted on phantom and cardiac imaging to demonstrate its potential in quantitative MRI.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/eess.IV_2023_09_21/" data-id="clopawo1i016fag887be21jay" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_21" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/21/eess.SP_2023_09_21/" class="article-date">
  <time datetime="2023-09-21T08:00:00.000Z" itemprop="datePublished">2023-09-21</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/21/eess.SP_2023_09_21/">eess.SP - 2023-09-21</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Distributed-CSMA-CA-MAC-Protocol-for-RIS-Assisted-Networks"><a href="#Distributed-CSMA-CA-MAC-Protocol-for-RIS-Assisted-Networks" class="headerlink" title="Distributed CSMA&#x2F;CA MAC Protocol for RIS-Assisted Networks"></a>Distributed CSMA&#x2F;CA MAC Protocol for RIS-Assisted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12526">http://arxiv.org/abs/2309.12526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhou Zhang, Saman Atapattu, Yizhu Wang, Marco Di Renzo</li>
<li>for: æé«˜åˆ†å¸ƒå¼ç½‘ç»œä¸­å¤šç”¨æˆ·é€šé“è®¿é—®çš„ä¼˜åŒ–</li>
<li>methods: åŸºäºå¯é…ç½®æ™ºèƒ½é¢(RIS)çš„åˆ†å¸ƒå¼CSMA&#x2F;CAç­–ç•¥ï¼ŒåŒ…æ‹¬æœºä¼šæ£€æµ‹å’Œé¿å…å†²çª</li>
<li>results: æå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„åˆ†å¸ƒå¼CSMA&#x2F;CAç­–ç•¥ï¼Œå¯ä»¥ maximize ç³»ç»Ÿååé‡ï¼Œå¹¶ä¸”åœ¨æ•°æ®åˆ†æå’Œå®éªŒéªŒè¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”è¡¨ç°æ›´ä¼˜ã€‚<details>
<summary>Abstract</summary>
This paper focuses on achieving optimal multi-user channel access in distributed networks using a reconfigurable intelligent surface (RIS). The network includes wireless channels with direct links between users and RIS links connecting users to the RIS. To maximize average system throughput, an optimal channel access strategy is proposed, considering the trade-off between exploiting spatial diversity gain with RIS assistance and the overhead of channel probing. The paper proposes an optimal distributed Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) strategy with opportunistic RIS assistance, based on statistics theory of optimal sequential observation planned decision. Each source-destination pair makes decisions regarding the use of direct links and/or probing source-RIS-destination links. Channel access occurs in a distributed manner after successful channel contention. The optimality of the strategy is rigorously derived using multiple-level pure thresholds. A distributed algorithm, which achieves significantly lower online complexity at $O(1)$, is developed to implement the proposed strategy. Numerical simulations verify the theoretical results and demonstrate the superior performance compared to existing approaches.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation:è¿™ç¯‡è®ºæ–‡å…³æ³¨äº†ä½¿ç”¨åˆ†å¸ƒå¼ç½‘ç»œä¸­çš„å¯é…ç½®æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰å®ç°æœ€ä½³å¤šç”¨æˆ·é€šé“è®¿é—®ã€‚ç½‘ç»œåŒ…æ‹¬æ— çº¿é€šé“å’ŒRISè¿æ¥ç”¨æˆ·å’ŒRISä¹‹é—´çš„è¿æ¥ã€‚ä¸ºäº†æœ€å¤§åŒ–ç³»ç»Ÿååé‡ï¼Œæå‡ºäº†ä¸€ç§æœ€ä½³çš„å¤šç”¨æˆ·é€šé“è®¿é—®ç­–ç•¥ï¼Œè€ƒè™‘äº†RISååŠ©ä¸‹çš„ç©ºé—´å¤šæ™®é€šè€—å’Œæ‰«æè¿‡ç¨‹çš„å¼€é”€ã€‚æå‡ºäº†åŸºäºç»Ÿè®¡å­¦ç†è®ºçš„æœ€ä½³åˆ†å¸ƒå¼CSMA/CAç­–ç•¥ï¼Œæ¯ä¸ªæº-ç›®çš„å¯¹è±¡å¯¹ä½¿ç”¨ç›´æ¥é“¾æ¥å’Œ/æˆ–æ¢æµ‹æº-RIS-ç›®çš„é“¾æ¥è¿›è¡Œå†³ç­–ã€‚é€šé“è®¿é—®å‘ç”Ÿåœ¨åˆ†å¸ƒå¼æ–¹å¼ä¸‹ï¼Œå¹¶ä¸”åœ¨æˆåŠŸæ‰«æåè¿›è¡Œé€šé“ç«äº‰ã€‚æå‡ºçš„ç­–ç•¥çš„ä¼˜åŒ–æ€§åŸºäºå¤šçº§çº¯é˜ˆå€¼ç†è®ºã€‚å¼€å‘äº†ä¸€ç§å®ç°è¯¥ç­–ç•¥çš„åˆ†å¸ƒå¼ç®—æ³•ï¼Œå…·æœ‰è¾ƒä½çš„åœ¨çº¿å¤æ‚åº¦($O(1)$)ã€‚numerical simulations verify the theoretical results and demonstrate the superior performance compared to existing approaches.
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Backscatter-Communications-Augmenting-Intelligence-in-Future-Internet-of-Things"><a href="#Deep-Reinforcement-Learning-for-Backscatter-Communications-Augmenting-Intelligence-in-Future-Internet-of-Things" class="headerlink" title="Deep Reinforcement Learning for Backscatter Communications: Augmenting Intelligence in Future Internet of Things"></a>Deep Reinforcement Learning for Backscatter Communications: Augmenting Intelligence in Future Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12507">http://arxiv.org/abs/2309.12507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wali Ullah Khan, Eva Lagunas, Zain Ali, Asad Mahmood, Chandan Kumar Sheemar, Manzoor Ahmed, Symeon Chatzinotas, BjÃ¶rn Ottersten</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰å’Œå›å°„é€šä¿¡ï¼ˆBCï¼‰æŠ€æœ¯åœ¨ä¸‹ä¸€ä»£äº’è”ç½‘å¤§æ•°æ®ç½‘ç»œä¸­çš„åº”ç”¨ã€‚</li>
<li>methods: æœ¬æ–‡é¦–å…ˆä»‹ç»äº†BCç³»ç»Ÿçš„åŸºæœ¬åŸç†ï¼Œç„¶åè¯¦ç»†ä»‹ç»äº†å¤šç§DRLæŠ€æœ¯å’Œå…¶å®ç°æ–¹å¼ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼ŒDRLå¯ä»¥å¸®åŠ©BCç³»ç»Ÿæé«˜æ€§èƒ½å’Œå¯é æ€§ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥å‡å°‘èƒ½è€—ã€‚ä¸€ä¸ªä½¿ç”¨RISå¢å¼ºéå¯¹ç§°å¤šaccess BCç³»ç»Ÿçš„å®è·µæ¡ˆä¾‹ä¹Ÿè¢«è¯¦ç»†æ¢è®¨ï¼Œä»¥ highlight its potentialã€‚<details>
<summary>Abstract</summary>
Backscatter communication (BC) technology offers sustainable solutions for next-generation Internet-of-Things (IoT) networks, where devices can transmit data by reflecting and adjusting incident radio frequency signals. In parallel to BC, deep reinforcement learning (DRL) has recently emerged as a promising tool to augment intelligence and optimize low-powered IoT devices. This article commences by elucidating the foundational principles underpinning BC systems, subsequently delving into the diverse array of DRL techniques and their respective practical implementations. Subsequently, it investigates potential domains and presents recent advancements in the realm of DRL-BC systems. A use case of RIS-aided non-orthogonal multiple access BC systems leveraging DRL is meticulously examined to highlight its potential. Lastly, this study identifies and investigates salient challenges and proffers prospective avenues for future research endeavors.
</details>
<details>
<summary>æ‘˜è¦</summary>
ğŸ‡¨ğŸ‡³ å¤‡å—å…³æ³¨çš„æŠ€æœ¯ï¼šåé€’å°„é€šä¿¡ï¼ˆBCï¼‰æŠ€æœ¯å¯ä»¥ä¸ºä¸‹ä¸€ä»£äº’è”ç½‘å…³é”®è®¾å¤‡ï¼ˆIoTï¼‰ç½‘ç»œæä¾›å¯æŒç»­çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­è®¾å¤‡å¯ä»¥é€šè¿‡åå°„å’Œè°ƒæ•´ incident æ— çº¿ç”µé¢‘ä¿¡å·æ¥ä¼ è¾“æ•°æ®ã€‚åŒæ—¶ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æŠ€æœ¯åœ¨æœ€è¿‘å‡ å¹´å†… emerge ä¸ºä¼˜åŒ–ä½åŠŸè€— IoT è®¾å¤‡çš„å·¥å…·ã€‚æœ¬æ–‡ä» BC ç³»ç»Ÿçš„åŸºç¡€åŸç†å‡ºå‘ï¼Œç„¶åä»‹ç»äº†å¤šç§ DRL æŠ€æœ¯å’Œå…¶å®è·µã€‚æ¥ç€ï¼Œå®ƒ investigate äº† BC-DRL ç³»ç»Ÿåœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨å‰æ™¯ï¼Œå¹¶ analyze äº†ä¸€äº›æœ€æ–°çš„è¿›å±•ã€‚æœ€åï¼Œæœ¬æ–‡è¯¦ç»†ä»‹ç»äº† RIS-assisted éå¯¹ç§°å¤šæ¥å…¥ BC ç³»ç»Ÿçš„åº”ç”¨ï¼Œä»¥ illustrate å…¶æ½œåœ¨çš„ä¼˜åŠ¿ã€‚æ€»ä¹‹ï¼Œæœ¬æ–‡æ¦‚æ‹¬äº† BC æŠ€æœ¯å’Œ DRL æŠ€æœ¯çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶ analyze äº†å®ƒä»¬åœ¨ IoT ç½‘ç»œä¸­çš„åº”ç”¨å‰æ™¯ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æå‡ºäº†æœªæ¥ç ”ç©¶çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Secure-Degree-of-Freedom-of-Wireless-Networks-Using-Collaborative-Pilots"><a href="#Secure-Degree-of-Freedom-of-Wireless-Networks-Using-Collaborative-Pilots" class="headerlink" title="Secure Degree of Freedom of Wireless Networks Using Collaborative Pilots"></a>Secure Degree of Freedom of Wireless Networks Using Collaborative Pilots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12502">http://arxiv.org/abs/2309.12502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingbo Hua, Qingpeng Liang, Md Saydur Rahman</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨ç ”ç©¶ä¸€ç§åŸºäºåä½œé¢‘é“ä¼°è®¡ï¼ˆANECEï¼‰çš„æ— çº¿ç½‘ç»œï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹éƒ½ä½¿ç”¨åŒæ ·çš„æ•°é‡çš„å¤©çº¿è¿›è¡Œå‘é€å’Œæ¥æ”¶ã€‚è¿™ç§ç½‘ç»œå¯ä»¥åœ¨æ— çº¿ç½‘ç»œä¸­æä¾›å®‰å…¨çš„åº¦é‡ï¼ˆSDoFï¼‰ï¼Œæ— è®ºä¾¦æµ‹è€…å¯èƒ½æ‹¥æœ‰å¤šå°‘å¤©çº¿ã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨ç§˜å¯†é”®å®¹é‡ï¼ˆSKCï¼‰æ¥åˆ†ææ¯å¯¹èŠ‚ç‚¹çš„å®‰å…¨åº¦é‡ã€‚æ¯ä¸ªä¼ è¾“ä¼šè¯ä¸­æœ‰ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”¨äºæµ‹è¯•ï¼Œç¬¬äºŒé˜¶æ®µç”¨äºéšæœºç¬¦å·ã€‚è¿™å¯¼è‡´äº†ä¸¤éƒ¨åˆ†çš„SDoFã€‚</li>
<li>results: ç ”ç©¶å‘ç°äº†ä¸€äº›é‡è¦çš„ç»“è®ºï¼ŒåŒ…æ‹¬ï¼ša) é˜¶æ®µ1çš„SDoFç›¸åŒäºå¤šç”¨æˆ·ANECEå’Œå¯¹ç­‰ANECEï¼Œä½†å‰è€…å¯èƒ½éœ€è¦è¾ƒå°‘çš„æ—¶é—´æ§½æ•°ï¼›b) ä¸‰ä¸ªèŠ‚ç‚¹ç½‘ç»œä¸­çš„é˜¶æ®µ2SDoFé€šå¸¸æ¯”å¯¹ç­‰ANECEæ›´å¤§ï¼›c) ä¸¤èŠ‚ç‚¹ç½‘ç»œä¸­ä½¿ç”¨ä¿®æ”¹åçš„ANECEï¼Œä½¿ç”¨å—å½¢éé›¶å¹‚é¢‘é“Matrixï¼Œå¯ä»¥æé«˜æ€»çš„SDoFã€‚è¿™äº›å¤šç”¨æˆ·ANECEå’Œä¿®æ”¹åçš„ä¸¤èŠ‚ç‚¹ANECEåœ¨å®‰å…¨åº¦é‡æ–¹é¢ä¸æ¯ä¸ªèŠ‚ç‚¹ä½¿ç”¨ç»™å®šæ•°é‡çš„å¤©çº¿è¿›è¡Œå‘é€å’Œæ¥æ”¶æ˜¯ä»Šå¤©å·²çŸ¥æœ€ä½³çš„å…¨åŒå·¥åè®®ã€‚<details>
<summary>Abstract</summary>
A wireless network of full-duplex nodes/users, using anti-eavesdropping channel estimation (ANECE) based on collaborative pilots, can yield a positive secure degree-of-freedom (SDoF) regardless of the number of antennas an eavesdropper may have. This paper presents novel results on SDoF of ANECE by analyzing secret-key capacity (SKC) of each pair of nodes in a network of multiple collaborative nodes per channel coherence period. Each transmission session of ANECE has two phases: phase 1 is used for pilots, and phase 2 is used for random symbols. This results in two parts of SDoF of ANECE. Both lower and upper bounds on the SDoF of ANECE for any number of users are shown, and the conditions for the two bounds to meet are given. This leads to important discoveries, including: a) The phase-1 SDoF is the same for both multi-user ANECE and pair-wise ANECE while the former may require only a fraction of the number of time slots needed by the latter; b) For a three-user network, the phase-2 SDoF of all-user ANECE is generally larger than that of pair-wise ANECE; c) For a two-user network, a modified ANECE deploying square-shaped nonsingular pilot matrices yields a higher total SDoF than the original ANECE. The multi-user ANECE and the modified two-user ANECE shown in this paper appear to be the best full-duplex schemes known today in terms of SDoF subject to each node using a given number of antennas for both transmitting and receiving.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸€ä¸ªæ— çº¿ç½‘ç»œï¼Œç”±å…¨åŒå·¥èŠ‚ç‚¹/ç”¨æˆ·ç»„æˆï¼Œä½¿ç”¨åå¬æŠ“å–æ¸ é“ä¼°è®¡ï¼ˆANECEï¼‰ï¼Œå¯ä»¥è·å¾—ä¸€å®šçš„å®‰å…¨åº¦é‡ï¼ˆSDoFï¼‰ï¼Œæ— è®ºæŠ“å–è€…å…·æœ‰å¤šå°‘å¤©çº¿ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†æ–°çš„SDoFç»“æœï¼Œé€šè¿‡åˆ†ææ¯å¯¹èŠ‚ç‚¹çš„ç§˜å¯†é”®å®¹é‡ï¼ˆSKCï¼‰ï¼Œå¹¶åˆ†ææ¯ä¸ªé€šä¿¡ä¼šè¯çš„ä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µç”¨äºæµ‹è¯•ï¼Œç¬¬äºŒé˜¶æ®µç”¨äºéšæœºç¬¦å·ã€‚è¿™å¯¼è‡´äº†ä¸¤ä¸ªSDoFçš„éƒ¨åˆ†ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¯ç¬¬ä¸€é˜¶æ®µçš„SDoFï¼Œå¦ä¸€ä¸ªæ˜¯ç¬¬äºŒé˜¶æ®µçš„SDoFã€‚è¿™ç¯‡è®ºæ–‡è¿˜æä¾›äº†å¯¹SDoFçš„ä¸‹ç•Œå’Œä¸Šç•Œï¼Œä»¥åŠè¿™ä¸¤ä¸ªç•Œé™ä¹‹é—´çš„æ¡ä»¶ã€‚è¿™äº›ç»“æœåŒ…æ‹¬ï¼ša) ç¬¬ä¸€é˜¶æ®µSDoFåœ¨å¤šç”¨æˆ·ANECEå’Œå¯¹åº”çš„å¯¹æŠ—å¼ANECEä¸­æ˜¯ç›¸åŒçš„ï¼Œè€Œåè€…å¯èƒ½éœ€è¦æ›´å°‘çš„æ—¶é—´æ§½æ•°ï¼›b) å¯¹äºä¸‰ä¸ªç”¨æˆ·ç½‘ç»œï¼Œç¬¬äºŒé˜¶æ®µSDoFçš„å…¨ç”¨æˆ·ANECEé€šå¸¸å¤§äºå¯¹æŠ—å¼ANECEçš„SDoFï¼›c) å¯¹äºä¸¤ä¸ªç”¨æˆ·ç½‘ç»œï¼Œä½¿ç”¨æ–¹å½¢éé›¶å¹‚æµ‹è¯•çŸ©é˜µçš„ä¿®æ”¹åANECEå¯ä»¥è·å¾—æ›´é«˜çš„æ€»SDoFï¼Œæ¯”åŸå§‹ANECEæ›´é«˜ã€‚è¿™äº›å¤šç”¨æˆ·ANECEå’Œä¿®æ”¹åçš„ä¸¤ç”¨æˆ·ANECEåœ¨ä»Šå¤©å¯èƒ½æ˜¯ä½¿ç”¨ç»™å®šæ•°é‡å¤©çº¿çš„æœ€ä½³å…¨åŒå·¥æ–¹æ¡ˆï¼Œä»SDoFçš„è§’åº¦æ¥çœ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Near-Field-Optimization-Algorithm-for-Reconfigurable-Intelligent-Surface"><a href="#Near-Field-Optimization-Algorithm-for-Reconfigurable-Intelligent-Surface" class="headerlink" title="Near Field Optimization Algorithm for Reconfigurable Intelligent Surface"></a>Near Field Optimization Algorithm for Reconfigurable Intelligent Surface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12448">http://arxiv.org/abs/2309.12448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emanuel Colella, Luca Bastianelli, Franco Moglie, Valter Mariani Primiani</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§å¯é‡æ–°é…ç½®çš„æ™ºèƒ½è¡¨é¢æŠ€æœ¯ï¼Œä»¥ä¼˜åŒ–æ— çº¿é€šä¿¡åœºæ™¯ä¸­ä¿¡å·ä¼ é€’çš„æ€§èƒ½ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å¤šç»´åº¦ä¼˜åŒ–ç®—æ³•ï¼ŒåŸºäºGNUç§‘å­¦åº“çš„Multidimensional optimizationç®—æ³•ï¼Œæ¥é‡æ–°é…ç½®æ™ºèƒ½è¡¨é¢ã€‚</li>
<li>results: é€šè¿‡ç”µç£åŠ¨åŠ›å­¦ simulationsï¼Œç ”ç©¶äººå‘˜å‘ç°è¯¥ç®—æ³•å¯ä»¥å¾ˆæœ‰æ•ˆåœ°é‡æ–°é…ç½®æ™ºèƒ½è¡¨é¢ï¼Œä½¿ç”µç£æ³¢èƒ½å¤Ÿå¼ºåˆ¶æ–¹å‘æ€§åœ°ä¼ é€’åˆ°ç‚¹ interestsã€‚<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is a type of wireless communication technology that uses a reconfigurable surface, such as a wall or building that is able to adjust its properties by an integrated optimization algorithm in order to optimize the signal propagation for a given communication scenario. As a reconfiguration algorithm the multidimensional optimization of the GNU scientific library was analyzed to evaluate the performance of the smart surface in the quality of signal reception. This analysis took place by means of electrodynamic simulations based on the finite difference time domain method. Through these simulations it was possible to observe the efficiency of the algorithm in the reconfiguration of the RIS, managing to focus the electromagnetic waves in a remarkable way towards the point of interest.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ™ºèƒ½è¡¨é¢é‡é…ç½®æŠ€æœ¯ (RIS) æ˜¯ä¸€ç§æ— çº¿é€šä¿¡æŠ€æœ¯ï¼Œä½¿ç”¨å¯é‡é…ç½®çš„è¡¨é¢ï¼Œå¦‚å¢™æˆ–å»ºç­‘ç‰©ï¼Œé€šè¿‡å†…ç½®ä¼˜åŒ–ç®—æ³•æ¥è°ƒæ•´å…¶å±æ€§ï¼Œä»¥ä¼˜åŒ–ç»™å®šé€šä¿¡åœºæ™¯ä¸­ä¿¡å·åè®®çš„ä¼ æ’­ã€‚ä½œä¸ºé‡é…ç½®ç®—æ³•ï¼Œå¤šç»´åº¦ä¼˜åŒ– GNU ç§‘å­¦åº“çš„åˆ†æè¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥è¯„ä¼°æ™ºèƒ½è¡¨é¢åœ¨ä¿¡å·æ¥æ”¶è´¨é‡æ–¹é¢çš„æ€§èƒ½ã€‚è¿™ç§åˆ†æé€šè¿‡åŸºäº Finite Difference Time Domain æ–¹æ³•çš„ç”µç£åŠ¨åŠ›å­¦æ¨¡æ‹Ÿæ¥è¿›è¡Œã€‚é€šè¿‡è¿™äº›æ¨¡æ‹Ÿï¼Œå¯ä»¥è§‚å¯Ÿæ™ºèƒ½è¡¨é¢é‡é…ç½®ç®—æ³•çš„æ•ˆç‡ï¼Œå¹¶èƒ½å¤Ÿå¾ˆæœ‰æ•ˆåœ°å°†ç”µç£æ³¢é›†ä¸­åˆ° interess pointã€‚
</details></li>
</ul>
<hr>
<h2 id="RadYOLOLet-Radar-Detection-and-Parameter-Estimation-Using-YOLO-and-WaveLet"><a href="#RadYOLOLet-Radar-Detection-and-Parameter-Estimation-Using-YOLO-and-WaveLet" class="headerlink" title="RadYOLOLet: Radar Detection and Parameter Estimation Using YOLO and WaveLet"></a>RadYOLOLet: Radar Detection and Parameter Estimation Using YOLO and WaveLet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12094">http://arxiv.org/abs/2309.12094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shamik Sarkar, Dongning Guo, Danijela Cabric</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æ¢è®¨ä¸€ç§æ— åŠ© partir radar ä¿¡å·æ¢æµ‹æ–¹æ³•ï¼Œä»¥æ»¡è¶³æœªæ¥çš„å…±äº«é¢‘ç‡æ— çº¿ç½‘ç»œï¼Œå¦‚å…¬æ°‘å¹¿æ’­ç”µæ³¢æœåŠ¡ (CBRS) çš„éœ€æ±‚ã€‚</li>
<li>methods: æœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å‡ ä½•å­¦ deep learning åŸºç¡€çš„spectrum sensingæ–¹æ³•ï¼Œåä¸º RadYOLOLetï¼Œå¯ä»¥æ¢æµ‹ä½åŠŸç‡ radar ä¿¡å·åœ¨å¹²æ‰°çš„æƒ…å†µä¸‹ï¼Œå¹¶ä¸”å¯ä»¥ä¼°ç®— radar ä¿¡å·çš„å‚æ•°ã€‚RadYOLOLet çš„æ ¸å¿ƒæ˜¯ä¸¤ä¸ªä¸åŒçš„å·ç§¯ç¥ç»ç½‘ (CNN)ï¼Œåä¸º RadYOLO å’Œ Wavelet-CNNï¼Œå®ƒä»¬æ˜¯ç‹¬ç«‹åœ°è®­ç»ƒã€‚RadYOLO è¿ä½œåœ¨spectrograms ä¸Šï¼Œæä¾›äº† RadYOLOLet çš„å¤§éƒ¨åˆ†åŠŸèƒ½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨ä½ä¿¡å·è¾“å…¥æ¯”ä¾‹ (SNR)  regime çš„æƒ…å†µä¸‹å…·æœ‰ä½é€‚åº”ç‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº† Wavelet-CNNï¼Œå®ƒè¿ä½œåœ¨ç‹¬ç«‹çš„å¹²æ‰°å˜æ¢ (Wavelet) ä¸Šï¼Œå¹¶ä¸”ä»…åœ¨ RadYOLO æ— æ³•æ¢æµ‹ä»»ä½• radar ä¿¡å·æ—¶ä½¿ç”¨ã€‚</li>
<li>results: æ ¹æ®æˆ‘ä»¬çš„è¯„ä¼°ï¼ŒRadYOLOLet å¯ä»¥åœ¨ä¸åŒçš„å®éªŒä¸­ï¼Œå®ç° 100% çš„ radar æ¢æµ‹ç²¾åº¦ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å¹²æ‰°è¾“å…¥æ¯”ä¾‹ (SINR)  up to 16 dB ä¸‹è¿ä½œæ­£ç¡®ã€‚<details>
<summary>Abstract</summary>
Detection of radar signals without assistance from the radar transmitter is a crucial requirement for emerging and future shared-spectrum wireless networks like Citizens Broadband Radio Service (CBRS). In this paper, we propose a supervised deep learning-based spectrum sensing approach called RadYOLOLet that can detect low-power radar signals in the presence of interference and estimate the radar signal parameters. The core of RadYOLOLet is two different convolutional neural networks (CNN), RadYOLO and Wavelet-CNN, that are trained independently. RadYOLO operates on spectrograms and provides most of the capabilities of RadYOLOLet. However, it suffers from low radar detection accuracy in the low signal-to-noise ratio (SNR) regime. We develop Wavelet-CNN specifically to deal with this limitation of RadYOLO. Wavelet-CNN operates on continuous Wavelet transform of the captured signals, and we use it only when RadYOLO fails to detect any radar signal. We thoroughly evaluate RadYOLOLet using different experiments corresponding to different types of interference signals. Based on our evaluations, we find that RadYOLOLet can achieve 100% radar detection accuracy for our considered radar types up to 16 dB SNR, which cannot be guaranteed by other comparable methods. RadYOLOLet can also function accurately under interference up to 16 dB SINR.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ¢æµ‹æ— åŠ©è€…çš„æ¿€å…‰è®¯å·æ˜¯æœªæ¥å…±äº«é¢‘ç‡æ— çº¿ç½‘ç»œçš„é‡è¦éœ€æ±‚ï¼Œå¦‚å…¬æ°‘å¹¿æ’­ç”µå°æœåŠ¡ï¼ˆCBRSï¼‰ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç›‘ç£å­¦ä¹ åŸºäºçš„å¯¹åº”æ–¹æ³•ï¼Œåä¸ºRadYOLOLetï¼Œå¯ä»¥æ¢æµ‹ä½åŠŸç‡æ¿€å…‰è®¯å·åœ¨å¹²æ‰°ä¸‹çš„å­˜åœ¨ï¼Œå¹¶ä¸”ä¼°ç®—æ¿€å…‰è®¯å·çš„å‚æ•°ã€‚RadYOLOLetçš„æ ¸å¿ƒæ˜¯ä¸¤ä¸ªä¸åŒçš„å·ç§¯ç¥ç»ç½‘ï¼ˆCNNï¼‰ï¼šRadYOLOå’Œæµªæ½®-CNNã€‚è¿™ä¸¤ä¸ªç¥ç»ç½‘åœ¨ç‹¬ç«‹åœ°è®­ç»ƒã€‚RadYOLOåœ¨spectrogramä¸­è¿ä½œï¼Œå®ƒæä¾›äº†RadYOLOLetçš„å¤§éƒ¨åˆ†åŠŸèƒ½ã€‚ç„¶è€Œï¼Œå®ƒåœ¨ä½ä¿¡å·è½½æ³¢æ¯”ä¾‹ï¼ˆSNRï¼‰ regimeä¸‹çš„æ¿€å…‰æ¢æµ‹ç²¾åº¦è¾ƒä½ã€‚æˆ‘ä»¬ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ç‰¹åˆ«çš„æµªæ½®-CNNï¼Œå®ƒåœ¨æ•æ‰åˆ°çš„ä¿¡å·ä¸­ä½¿ç”¨æµªæ½®å˜æ¢ï¼Œå¹¶ä¸”ä»…åœ¨RadYOLOå¤±è´¥æ¢æµ‹ä»»ä½•æ¿€å…‰è®¯å·æ—¶ä½¿ç”¨ã€‚æˆ‘ä»¬å¯¹RadYOLOLetè¿›è¡Œäº†ä¸åŒç±»å‹çš„å®éªŒï¼ŒåŒ…æ‹¬ä¸åŒç±»å‹çš„å¹²æ‰°ä¿¡å·ã€‚æ ¹æ®æˆ‘ä»¬çš„è¯„ä¼°ï¼ŒRadYOLOLetå¯ä»¥åœ¨è€ƒè™‘çš„æ¿€å…‰å‹åˆ«ä¸Šè¾¾åˆ°100%çš„æ¢æµ‹ç²¾åº¦ï¼Œå¹¶ä¸”åœ¨å¹²æ‰°è¾ƒé«˜çš„16 dB SINRä¸‹è¿˜èƒ½æ­£ç¡®è¿ä½œã€‚â€
</details></li>
</ul>
<hr>
<h2 id="UAV-Swarm-Deployment-and-Trajectory-for-3D-Area-Coverage-via-Reinforcement-Learning"><a href="#UAV-Swarm-Deployment-and-Trajectory-for-3D-Area-Coverage-via-Reinforcement-Learning" class="headerlink" title="UAV Swarm Deployment and Trajectory for 3D Area Coverage via Reinforcement Learning"></a>UAV Swarm Deployment and Trajectory for 3D Area Coverage via Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11992">http://arxiv.org/abs/2309.11992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia He, Ziye Jia, Chao Dong, Junyu Liu, Qihui Wu, Jingxian Liu</li>
<li>For: æœ¬æ–‡æ—¨åœ¨ç ”ç©¶æ— äººé£è¡Œå™¨ç¾¤ï¼ˆUAVç¾¤ï¼‰çš„æŠ•æ”¾å’Œè½¨è¿¹è®¡åˆ’ï¼Œä»¥æ»¡è¶³ä¸‰ç»´ï¼ˆ3Dï¼‰enarioä¸­çš„æ— çº¿é€šä¿¡æœåŠ¡ã€‚* Methods: æœ¬æ–‡æå‡ºäº†å±‚æ¬¡ç¾¤ç»„ç»‡æœºåˆ¶ï¼Œä»¥æœ‰æ•ˆåœ°æœåŠ¡å¤§é¢ç§¯ç”¨æˆ·ã€‚é—®é¢˜è½¬åŒ–ä¸ºæœ€å°åŒ–UAVç¾¤çš„æ€»è½¨è¿¹æŸå¤±ã€‚ä½†é—®é¢˜å…·æœ‰éæ‰˜Formatter propertyï¼Œå› æ­¤å°†å…¶æ‹†åˆ†ä¸ºç”¨æˆ·å·ç§¯ã€UAVç¾¤åœç•™ç‚¹é€‰æ‹©å’Œç¾¤ trajectory ç¡®å®šã€‚* Results: æœ¬æ–‡é‡‡ç”¨Qå­¦ä¹ ç®—æ³•åŠ é€Ÿè§£å†³æ•ˆç‡ã€‚ç»è¿‡å¹¿æ³›çš„ simulationsï¼Œæå‡ºçš„æœºåˆ¶å’Œç®—æ³•è¢«è¯æ˜è¶…è¿‡å…¶ä»–ç›¸å…³æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Unmanned aerial vehicles (UAVs) are recognized as promising technologies for area coverage due to the flexibility and adaptability. However, the ability of a single UAV is limited, and as for the large-scale three-dimensional (3D) scenario, UAV swarms can establish seamless wireless communication services. Hence, in this work, we consider a scenario of UAV swarm deployment and trajectory to satisfy 3D coverage considering the effects of obstacles. In detail, we propose a hierarchical swarm framework to efficiently serve the large-area users. Then, the problem is formulated to minimize the total trajectory loss of the UAV swarm. However, the problem is intractable due to the non-convex property, and we decompose it into smaller issues of users clustering, UAV swarm hovering points selection, and swarm trajectory determination. Moreover, we design a Q-learning based algorithm to accelerate the solution efficiency. Finally, we conduct extensive simulations to verify the proposed mechanisms, and the designed algorithm outperforms other referred methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰è¢«è®¤ä¸ºæ˜¯å¹¿æ³›è¦†ç›–åŒºåŸŸçš„æœ‰æœ›æŠ€æœ¯ï¼Œç”±äºå®ƒä»¬çš„çµæ´»å’Œé€‚åº”èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå•ä¸ªUAVçš„èƒ½åŠ›æœ‰é™ï¼Œè€Œåœ¨å¤§è§„æ¨¡ä¸‰ç»´ï¼ˆ3Dï¼‰åœºæ™¯ä¸­ï¼ŒUAVç¾¤å¯ä»¥å»ºç«‹æ— ç¼æ— çº¿é€šä¿¡æœåŠ¡ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†UAVç¾¤çš„éƒ¨ç½²å’Œè½¨è¿¹ï¼Œä»¥æ»¡è¶³3Dè¦†ç›–çš„éœ€æ±‚ï¼Œå¹¶è€ƒè™‘äº†éšœç¢ç‰©çš„å½±å“ã€‚åœ¨è¯¦ç»†çš„æè¿°ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å±‚æ¬¡ç¾¤ç»„ç»‡ï¼Œä»¥é«˜æ•ˆåœ°æœåŠ¡äºå¤§é¢ç§¯ç”¨æˆ·ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†é—®é¢˜å®šä¹‰ä¸ºæœ€å°åŒ–UAVç¾¤çš„æ€»è½¨è¿¹æŸå¤±ã€‚ç„¶è€Œï¼Œé—®é¢˜çš„éæ ¸å¿ƒæ€§ä½¿å¾—å®ƒä¸å¯è§£ï¼Œæˆ‘ä»¬å°†å…¶åˆ†è§£ä¸ºç”¨æˆ·åˆ’åˆ†ã€UAVç¾¤åœç•™ç‚¹é€‰æ‹©å’Œç¾¤è½¨è¿¹å†³å®šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†Qå­¦ä¹ ç®—æ³•ï¼Œä»¥åŠ é€Ÿè§£å†³æ•ˆç‡ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„ simulate æ¥éªŒè¯æˆ‘ä»¬çš„æœºåˆ¶ï¼Œå¹¶å‘ç°æˆ‘ä»¬çš„è®¾è®¡ç®—æ³•æ¯”å…¶ä»–å·²çŸ¥æ–¹æ³•æ›´é«˜æ•ˆã€‚
</details></li>
</ul>
<hr>
<h2 id="Alteration-of-skeletal-muscle-energy-metabolism-assessed-by-31P-MRS-in-clinical-routine-part-2-Clinical-application"><a href="#Alteration-of-skeletal-muscle-energy-metabolism-assessed-by-31P-MRS-in-clinical-routine-part-2-Clinical-application" class="headerlink" title="Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 2: Clinical application"></a>Alteration of skeletal muscle energy metabolism assessed by 31P MRS in clinical routine, part 2: Clinical application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11934">http://arxiv.org/abs/2309.11934</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine NaÃ«gel, HÃ©lÃ¨ne Ratiney, Jabrane Karkouri, Djahid Kennouche, Nicolas Royer, Jill M Slade, JÃ©rÃ´me Morel, Pierre Croisille, Magalie Viallon</li>
<li>For: This study aimed to evaluate the impact of an advanced quality control pipeline on dynamic 31P-MRS studies of two patient populations with different types of fatigue, COVID-19 and multiple sclerosis (MS).* Methods: The study used 31P-MRS on a 3T clinical MRI to collect data from 19 COVID-19 patients, 38 MS patients, and 40 healthy controls. The advanced quality control pipeline was applied to the selected patient cohorts to investigate its impact on clinical outcomes.* Results: The application of the quality control pipeline resulted in increased statistical power, changed the values of several outcome measures, and reduced variability. Significant differences were found between the two patient populations and healthy controls for several metabolite concentrations, including T1PCr and T1Pi for MS patients, and resting [PCr], [Pi], [ADP], [H2PO4-], and pH for COVID-19 patients. Additionally, the use of a fixed correction factor led to systematically higher estimated concentrations of PCr and Pi than when using individually corrected factors.<details>
<summary>Abstract</summary>
Background: In this second part of a two-part paper, we intend to demonstrate the impact of the previously proposed advanced quality control pipeline. To understand its benefit and challenge the proposed methodology in a real scenario, we chose to compare the outcome when applying it to the analysis of two patient populations with a significant but highly different types of fatigue: COVID19 and multiple sclerosis (MS). Experimental: 31P-MRS was performed on a 3T clinical MRI, in 19 COVID19 patients, 38 MS patients, and 40 matched healthy controls. Dynamic acquisitions using an MR-compatible ergometer ran over a rest(40s), exercise(2min), and a recovery phase(6min). Long and short TR acquisitions were also made at rest for T1 correction. The advanced data quality control pipeline presented in part 1 is applied to the selected patient cohorts to investigate its impact on clinical outcomes. We first used power and sample size analysis to estimate objectively the impact of adding QCS. Then, comparisons between patients and healthy control groups using validated QCS were performed using unpaired T-tests or Mann-Whitney tests (p<0.05).Results: The application of the QCS resulted in increased statistical power, changed the values of several outcome measures, and reduced variability (SD). A significant difference was found between the T1PCr and T1Pi of MS patients and healthy controls. Furthermore, the use of a fixed correction factor led to systematically higher estimated concentrations of PCr and Pi than when using individually corrected factors. We observed significant differences between the two patient populations and healthy controls for resting [PCr] -- MS only, [Pi], [ADP], [H2PO4-] and pH -- COVID19 only, and post-exercise [PCr],[Pi] and [H2PO4-] - MS only. The dynamic indicators $\tau$PCr, $\tau$Pi, ViPCr and Vmax were reduced for COVID19 and MS patients compared to controls. Conclusion: Our results show that QCS in dynamic 31P-MRS studies results in smaller data variability and therefore impacts study sample size and power. Although QCS resulted in discarded data and therefore reduced the acceptable data and subject numbers, this rigorous and unbiased approach allowed for proper assessment of muscle metabolites and metabolism in patient populations. The outcomes include an increased metabolite T1, which directly affect the T1 correction factor applied to the amplitudes of the metabolite, and a prolonged $\tau$PCr indicating reduced muscle oxidative capacity for patients with MS and COVID19.
</details>
<details>
<summary>æ‘˜è¦</summary>
Background: åœ¨è¿™ç¯‡ä¸¤éƒ¨åˆ†æ–‡ç« çš„ç¬¬äºŒéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦è¯æ˜å…ˆå‰æå‡ºçš„é«˜çº§è´¨é‡æ§åˆ¶ç®¡é“çš„å½±å“ã€‚ä¸ºäº†äº†è§£å…¶æ•ˆæœå’ŒæŒ‘æˆ˜ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å¯¹ä¸¤ç§ä¸åŒç±»å‹çš„ç–²åŠ³ patient populationè¿›è¡Œæ¯”è¾ƒï¼šCOVID-19å’Œå¤šå‘æ€§ç¡¬åŒ–ç—…ï¼ˆMSï¼‰ã€‚Experimental: æˆ‘ä»¬ä½¿ç”¨3Tä¸´åºŠMRIè®¾å¤‡è¿›è¡Œ31P-MRSæµ‹é‡ï¼Œå…±æœ‰19ä¾‹COVID-19æ‚£è€…ã€38ä¾‹MSæ‚£è€…å’Œ40ä¾‹å¥åº·æ§åˆ¶ç¾¤ã€‚åŠ¨æ€è·å–ä½¿ç”¨MRç›¸å®¹çš„è€åŠ›æµ‹è¯•å™¨åœ¨ä¼‘æ¯ï¼ˆ40ç§’ï¼‰ã€è¿åŠ¨ï¼ˆ2åˆ†ï¼‰å’Œæ¢å¤é˜¶æ®µï¼ˆ6åˆ†ï¼‰è¿›è¡Œæµ‹é‡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†é•¿TRå’ŒçŸ­TRçš„è·å–ï¼Œä»¥ä¾¿å¯¹T1çš„ä¿®æ­£ã€‚æˆ‘ä»¬å¯¹é€‰æ‹©çš„æ‚£è€…ç¾¤è¿›è¡Œäº†é«˜çº§æ•°æ®è´¨é‡æ§åˆ¶ç®¡é“çš„åº”ç”¨ï¼Œä»¥è°ƒæŸ¥å…¶å½±å“ä¸´åºŠç»“æœã€‚æˆ‘ä»¬é¦–å…ˆä½¿ç”¨åŠ›å’Œæ ·æœ¬å¤§å°åˆ†ææ¥å¯¹æ·»åŠ QCSçš„å½±å“è¿›è¡Œ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²è¯„ä¼°ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æ— å¯¹ç…§ç»„Tæ£€æµ‹æˆ–æ›¼æ©-æ€€ç‰¹è¯„ä¼°æµ‹è¯•ï¼ˆp<0.05ï¼‰æ¥æ¯”è¾ƒæ‚£è€…å’Œå¥åº·æ§åˆ¶ç¾¤ã€‚Results: QCSçš„åº”ç”¨å¯¼è‡´æ•°æ®å˜é‡å‡å°‘ï¼Œæé«˜äº†ç»Ÿè®¡åŠ›ï¼Œå¹¶æ”¹å˜äº†ä¸€äº›ç»“æœæ¢ç´¢ç»“æœã€‚COVID-19å’ŒMSæ‚£è€…çš„T1PCrå’ŒT1Piä¸å¥åº·æ§åˆ¶ç¾¤ç›¸æ¯”æ˜¾è‘—ä¸åŒã€‚æ­¤å¤–ï¼Œä½¿ç”¨å›ºå®šä¿®æ­£å› å­å¯¼è‡´PCrå’ŒPiçš„ä¼°è®¡å€¼é«˜äºä½¿ç”¨ä¸ªä½“ä¿®æ­£å› å­ã€‚æˆ‘ä»¬å‘ç°COVID-19å’ŒMSæ‚£è€…åœ¨ä¼‘æ¯æœŸçš„PCrã€Piã€ADPã€H2PO4-å’ŒpHä¸­å…·æœ‰æ˜¾è‘—å·®å¼‚ã€‚åœ¨è¿åŠ¨åï¼ŒCOVID-19å’ŒMSæ‚£è€…çš„PCrã€Piå’ŒH2PO4-ä¸­å…·æœ‰æ˜¾è‘—å·®å¼‚ã€‚åŠ¨æ€æŒ‡æ ‡$\tau$PCrã€$\tau$Piã€ViPCrå’ŒVmaxåœ¨COVID-19å’ŒMSæ‚£è€…ä¸­ç›¸æ¯”äºæ§åˆ¶ç¾¤è¡¨ç°ä¸ºä¸‹é™ã€‚Conclusion: æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨åŠ¨æ€31P-MRSç ”ç©¶ä¸­åº”ç”¨QCSä¼šå‡å°‘æ•°æ®å˜é‡ï¼Œå› æ­¤å½±å“ç ”ç©¶æ ·æœ¬å¤§å°å’Œ statistically powerã€‚è™½ç„¶QCSå¯¼è‡´äº†æŠ›å¼ƒæ•°æ®ï¼Œå› æ­¤å‡å°‘äº†å¯æ¥å—çš„æ•°æ®å’Œè¯•éªŒè€…æ•°é‡ï¼Œä½†è¿™ç§ä¸åè¢‹ä¸­å’Œä¸åéšœçš„æ–¹æ³•å…è®¸æˆ‘ä»¬å¯¹æ‚£è€…ç¾¤è¿›è¡Œæ­£ç¡®çš„ Ğ¼ĞµÑ‚Ğ°Ğ±Ğ¾Ù„Ğ¸Ñ‚å’Œä»£è°¢è¯„ä¼°ã€‚ç»“æœåŒ…æ‹¬T1çš„å¢åŠ ï¼Œç›´æ¥å½±å“äº†åº”ç”¨äºæ¿€å‘ç‰©è´¨çš„T1ä¿®æ­£å› å­ï¼Œä»¥åŠCOVID-19å’ŒMSæ‚£è€…çš„ prolonged $\tau$PCrï¼Œ indicating reduced muscle oxidative capacity.
</details></li>
</ul>
<hr>
<h2 id="Index-Modulation-based-Information-Harvesting-for-Far-Field-RF-Power-Transfer"><a href="#Index-Modulation-based-Information-Harvesting-for-Far-Field-RF-Power-Transfer" class="headerlink" title="Index Modulation-based Information Harvesting for Far-Field RF Power Transfer"></a>Index Modulation-based Information Harvesting for Far-Field RF Power Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11929">http://arxiv.org/abs/2309.11929</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Ertug Pihtili, Mehmet C. Ilter, Ertugrul Basar, Risto Wichman, Jyri HÃ¤mÃ¤lÃ¤inen</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨ä½¿ç”¨ç°æœ‰çš„è¿œåœºèƒ½é‡ä¼ è¾“ç³»ç»Ÿè¿›è¡Œæ•°æ®ä¼ è¾“ï¼Œä»¥å®ç°æ— ç”µæ± è®¾å¤‡çš„ battery-less é€šä¿¡æŠ€æœ¯ã€‚</li>
<li>methods: è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç´¢å¼•ä¿®æ”¹ï¼ˆIMï¼‰æŠ€æœ¯çš„ä¿¡æ¯æ”¶é›†ï¼ˆIHï¼‰åè®®ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒçš„æ€§èƒ½æŒ‡æ ‡ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡åœ¨ç°æœ‰çš„è¿œåœºèƒ½é‡ä¼ è¾“ç³»ç»Ÿä¸­åº”ç”¨IMæŠ€æœ¯ï¼Œå¯ä»¥å®ç°æ•°æ®ä¼ è¾“ï¼Œç‰¹åˆ«åœ¨ä¸‹ä¸€ä»£ç‰©è”ç½‘æ— çº¿ç½‘ç»œä¸­è¡¨ç°å‡ºäº†æ˜æ˜¾çš„æ½œåŠ›ã€‚<details>
<summary>Abstract</summary>
While wireless information transmission (WIT) is evolving into its sixth generation (6G), maintaining terminal operations that rely on limited battery capacities has become one of the most paramount challenges for Internet-of-Things (IoT) platforms. In this respect, there exists a growing interest in energy harvesting technology from ambient resources, and wireless power transfer (WPT) can be the key solution towards enabling battery-less infrastructures referred to as zero-power communication technology. Indeed, eclectic integration approaches between WPT and WIT mechanisms are becoming a vital necessity to limit the need for replacing batteries. Beyond the conventional separation between data and power components of the emitted waveforms, as in simultaneous wireless information and power transfer (SWIPT) mechanisms, a novel protocol referred to as information harvesting (IH) has recently emerged. IH leverages existing WPT mechanisms for data communication by incorporating index modulation (IM) techniques on top of the existing far-field power transfer mechanism. In this paper, a unified framework for the IM-based IH mechanisms has been presented where the feasibility of various IM techniques are evaluated based on different performance metrics. The presented results demonstrate the substantial potential to enable data communication within existing far-field WPT systems, particularly in the context of next-generation IoT wireless networks.
</details>
<details>
<summary>æ‘˜è¦</summary>
sixth generation æ— çº¿ä¿¡æ¯ä¼ è¾“ (6G) çš„å‘å±•ï¼Œä½¿å¾—äº’è”ç½‘ç‰©è”ç½‘ (IoT) å¹³å°ä¸Šçš„ç»ˆç«¯è®¾å¤‡é ç”µæ± èƒ½åŠ›æœ‰é™çš„é—®é¢˜å˜å¾—éå¸¸ç´§è¿«ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œèƒ½é‡æ”¶é›†æŠ€æœ¯ä»å‘¨å›´ç¯å¢ƒçš„èµ„æºæˆä¸ºäº†ä¸€ç§ä¸å¯æˆ–ç¼ºçš„è§£å†³æ–¹æ¡ˆã€‚æ— çº¿ç”µåŠ›ä¼ è¾“ (WPT) å¯ä»¥æ˜¯é’ˆå¯¹æ— ç”µæ± åŸºç¡€è®¾æ–½çš„å…³é”®è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œå°† WPT å’Œæ— çº¿ä¿¡æ¯ä¼ è¾“ (WIT) æœºåˆ¶ç»“åˆåœ¨ä¸€èµ·ï¼Œä»¥é™åˆ¶æ›´æ¢ç”µæ± çš„éœ€æ±‚ã€‚åœ¨ä¼ ç»Ÿçš„æ•°æ®å’Œç”µåŠ›ä¸¤éƒ¨åˆ†åˆ†ç¦»çš„æƒ…å†µä¸‹ï¼Œæ–°çš„åè®®è¢«ç§°ä¸ºä¿¡æ¯æ”¶é›† (IH)ï¼Œåˆ©ç”¨ç°æœ‰çš„ WPT æœºåˆ¶æ¥å®ç°æ•°æ®ä¼ è¾“ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä¸€ç§åŸºäº Ğ¸Ğ½Ğ´ĞµĞºÑä¿®æ”¹ (IM) æŠ€æœ¯çš„ IH æœºåˆ¶çš„ä¸€ä½“åŒ–æ¡†æ¶è¢«æå‡ºï¼Œå¹¶å¯¹ä¸åŒçš„æ€§èƒ½æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚è·å¾—çš„ç»“æœè¡¨æ˜ï¼Œå¯ä»¥åœ¨ç°æœ‰çš„è¿œåœº WPT ç³»ç»Ÿä¸­å®ç°æ•°æ®ä¼ è¾“ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸‹ä¸€ä»£ IoT æ— çº¿ç½‘ç»œä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multi-Passive-Active-IRS-Enhanced-Wireless-Coverage-Deployment-Optimization-and-Cost-Performance-Trade-off"><a href="#Multi-Passive-Active-IRS-Enhanced-Wireless-Coverage-Deployment-Optimization-and-Cost-Performance-Trade-off" class="headerlink" title="Multi-Passive&#x2F;Active-IRS Enhanced Wireless Coverage: Deployment Optimization and Cost-Performance Trade-off"></a>Multi-Passive&#x2F;Active-IRS Enhanced Wireless Coverage: Deployment Optimization and Cost-Performance Trade-off</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11918">http://arxiv.org/abs/2309.11918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min Fu, Weidong Mei, Rui Zhang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¢å¼ºæ— çº¿ç½‘ç»œè¦†ç›–ç‡ï¼Œé€šè¿‡åˆ›å»ºå¤šä¸ªéšœç¢ç‰©freeçš„å‚çº¿æ€§è¿æ¥ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨å¤šä¸ªPassive&#x2F;ActiveIRSï¼ˆPIRS&#x2F;AIRSï¼‰å’Œå¤šantennaåŸºç«™ï¼ˆBSï¼‰ï¼Œå¹¶ç ”ç©¶äº†å¤šä¸ªéé‡å çš„ç»†åˆ†åŒºåŸŸï¼Œæ¯ä¸ªç»†åˆ†åŒºåŸŸå¯èƒ½ä¼šæœ‰ä¸€ä¸ªå€™é€‰ä½ç½®å¯ä»¥éƒ¨ç½²PIRSæˆ–AIRSã€‚</li>
<li>results: ç ”ç©¶äººå‘˜é€šè¿‡è°ƒæ•´PIRS&#x2F;AIRSçš„æ•°é‡å’Œéƒ¨ç½²ä½ç½®ï¼Œä»¥å®ç°ç»™å®šçš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ç›®æ ‡ï¼ŒåŒæ—¶å°½é‡é™ä½æ€»éƒ¨ç½²æˆæœ¬ã€‚ simulationç»“æœè¡¨æ˜ï¼Œæè®®çš„ç®—æ³•å¯ä»¥åœ¨å›°éš¾çš„ combinatorial optimization é—®é¢˜ä¸­åšå‡ºä¼˜åŒ–çš„é€‰æ‹©ï¼Œå¹¶ä¸”åœ¨cost-performance trade-offä¸­è¡¨ç°æ›´å¥½ã€‚<details>
<summary>Abstract</summary>
Both passive and active intelligent reflecting surfaces (IRSs) can be deployed in complex environments to enhance wireless network coverage by creating multiple blockage-free cascaded line-of-sight (LoS) links. In this paper, we study a multi-passive/active-IRS (PIRS/AIRS) aided wireless network with a multi-antenna base station (BS) in a given region. First, we divide the region into multiple non-overlapping cells, each of which may contain one candidate location that can be deployed with a single PIRS or AIRS. Then, we show several trade-offs between minimizing the total IRS deployment cost and enhancing the signal-to-noise ratio (SNR) performance over all cells via direct/cascaded LoS transmission with the BS. To reconcile these trade-offs, we formulate a joint multi-PIRS/AIRS deployment problem to select an optimal subset of all candidate locations for deploying IRS and also optimize the number of passive/active reflecting elements deployed at each selected location to satisfy a given SNR target over all cells, such that the total deployment cost is minimized. However, due to the combinatorial optimization involved, the formulated problem is difficult to be solved optimally. To tackle this difficulty, we first optimize the reflecting element numbers with given PIRS/AIRS deployed locations via sequential refinement, followed by a partial enumeration to determine the PIRS/AIRS locations. Simulation results show that our proposed algorithm achieves better cost-performance trade-offs than other baseline deployment strategies.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>TRANSLATE_TEXT</SYS>> Both passive and active intelligent reflecting surfaces (IRSs) can be deployed in complex environments to enhance wireless network coverage by creating multiple blockage-free cascaded line-of-sight (LoS) links. In this paper, we study a multi-passive/active-IRS (PIRS/AIRS) aided wireless network with a multi-antenna base station (BS) in a given region. First, we divide the region into multiple non-overlapping cells, each of which may contain one candidate location that can be deployed with a single PIRS or AIRS. Then, we show several trade-offs between minimizing the total IRS deployment cost and enhancing the signal-to-noise ratio (SNR) performance over all cells via direct/cascaded LoS transmission with the BS. To reconcile these trade-offs, we formulate a joint multi-PIRS/AIRS deployment problem to select an optimal subset of all candidate locations for deploying IRS and also optimize the number of passive/active reflecting elements deployed at each selected location to satisfy a given SNR target over all cells, such that the total deployment cost is minimized. However, due to the combinatorial optimization involved, the formulated problem is difficult to be solved optimally. To tackle this difficulty, we first optimize the reflecting element numbers with given PIRS/AIRS deployed locations via sequential refinement, followed by a partial enumeration to determine the PIRS/AIRS locations. Simulation results show that our proposed algorithm achieves better cost-performance trade-offs than other baseline deployment strategies.Translated by Google Translate
</details></li>
</ul>
<hr>
<h2 id="REM-U-net-Deep-Learning-Based-Agile-REM-Prediction-with-Energy-Efficient-Cell-Free-Use-Case"><a href="#REM-U-net-Deep-Learning-Based-Agile-REM-Prediction-with-Energy-Efficient-Cell-Free-Use-Case" class="headerlink" title="REM-U-net: Deep Learning Based Agile REM Prediction with Energy-Efficient Cell-Free Use Case"></a>REM-U-net: Deep Learning Based Agile REM Prediction with Energy-Efficient Cell-Free Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11898">http://arxiv.org/abs/2309.11898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hazem Sallouha, Shamik Sarkar, Enes Krijestorac, Danijela Cabric<br>for:è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§å¿«é€Ÿã€å‡†ç¡®åœ°é¢„æµ‹Radioç¯å¢ƒåœ°å›¾ï¼ˆREMï¼‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œä»¥ä¾¿ä¼˜åŒ–æ— çº¿ç½‘ç»œéƒ¨ç½²ã€æé«˜ç½‘ç»œæ€§èƒ½å’Œæœ‰æ•ˆåœ°ç®¡ç†é¢‘ç‡èµ„æºã€‚methods:è¯¥è®ºæ–‡ä½¿ç”¨äº†u-netç½‘ç»œï¼Œå¹¶åœ¨å¤§è§„æ¨¡3Dåœ°å›¾ datasetä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æå‡ºäº†æ•°æ®å¤„ç†æ­¥éª¤æ¥è¿›ä¸€æ­¥æ”¹è¿› REM é¢„æµ‹ç²¾åº¦ã€‚results:è®ºæ–‡åœ¨2023å¹´IEEE ICASSP Signal Processing Grand Challengeä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œå¾—åˆ°äº†0.045çš„ normalized root-mean-square errorï¼ˆRMSEï¼‰å’Œ14æ¯«ç§’çš„å¹³å‡è¿è¡Œæ—¶é—´ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜ç¤ºå‡ºäº†åœ¨CF-mMIMOç½‘ç»œä¸­é¢„æµ‹ REM çš„ç²¾åº¦å¯ä»¥ä»£æ›¿å¤§è§„æ¨¡çš„æŠ˜å°„æµ‹é‡ï¼Œä»è€Œå‡å°‘èƒ½æºæ¶ˆè€—ã€‚<details>
<summary>Abstract</summary>
Radio environment maps (REMs) hold a central role in optimizing wireless network deployment, enhancing network performance, and ensuring effective spectrum management. Conventional REM prediction methods are either excessively time-consuming, e.g., ray tracing, or inaccurate, e.g., statistical models, limiting their adoption in modern inherently dynamic wireless networks. Deep-learning-based REM prediction has recently attracted considerable attention as an appealing, accurate, and time-efficient alternative. However, existing works on REM prediction using deep learning are either confined to 2D maps or use a limited dataset. In this paper, we introduce a runtime-efficient REM prediction framework based on u-nets, trained on a large-scale 3D maps dataset. In addition, data preprocessing steps are investigated to further refine the REM prediction accuracy. The proposed u-net framework, along with preprocessing steps, are evaluated in the context of the 2023 IEEE ICASSP Signal Processing Grand Challenge, namely, the First Pathloss Radio Map Prediction Challenge. The evaluation results demonstrate that the proposed method achieves an average normalized root-mean-square error (RMSE) of 0.045 with an average of 14 milliseconds (ms) runtime. Finally, we position our achieved REM prediction accuracy in the context of a relevant cell-free massive multiple-input multiple-output (CF-mMIMO) use case. We demonstrate that one can obviate consuming energy on large-scale fading measurements and rely on predicted REM instead to decide on which sleep access points (APs) to switch on in a CF-mMIMO network that adopts a minimum propagation loss AP switch ON/OFF strategy.
</details>
<details>
<summary>æ‘˜è¦</summary>
Radio ç¯å¢ƒåœ°å›¾ (REM) åœ¨æ— çº¿ç½‘ç»œéƒ¨ç½²ã€æé«˜ç½‘ç»œæ€§èƒ½å’Œæœ‰æ•ˆspectrumç®¡ç†ä¸­æ‰®æ¼”ä¸­å¿ƒè§’è‰²ã€‚ä¼ ç»Ÿçš„ REM é¢„æµ‹æ–¹æ³•æ˜¯ either è¿‡æ—¶ consume æ—¶é—´ (å¦‚å°„çº¿è¿½è¸ª) æˆ–è€…ä¸å‡†ç¡® (å¦‚ç»Ÿè®¡æ¨¡å‹)ï¼Œè¿™é™åˆ¶äº†å®ƒä»¬åœ¨ç°ä»£è‡ªç„¶åŠ¨æ€æ— çº¿ç½‘ç»œä¸­çš„é‡‡ç”¨ã€‚æ·±åº¦å­¦ä¹ åŸºäºçš„ REM é¢„æµ‹åœ¨æœ€è¿‘å¸å¼•äº†å¤§é‡å…³æ³¨ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä¸€ç§å¸å¼•äººçš„ã€å‡†ç¡®çš„å’Œé«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œç°æœ‰çš„ REM é¢„æµ‹ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„å·¥ä½œéƒ½æ˜¯ confined to 2D åœ°å›¾æˆ–è€…ä½¿ç”¨æœ‰é™çš„æ•°æ®é›†ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ REM é¢„æµ‹æ¡†æ¶ï¼ŒåŸºäº u-netsï¼Œåœ¨å¤§è§„æ¨¡ 3D åœ°å›¾æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿ investigate äº†æ•°æ®é¢„å¤„ç†æ­¥éª¤ï¼Œä»¥è¿›ä¸€æ­¥ç²¾ç»†åŒ– REM é¢„æµ‹ç²¾åº¦ã€‚æˆ‘ä»¬çš„æå‡ºçš„ u-net æ¡†æ¶ã€é¢„å¤„ç†æ­¥éª¤å’Œè¯„ä¼°ç»“æœåœ¨ 2023 IEEE ICASSP Signal Processing Grand Challenge ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨normalized root-mean-square error (RMSE) æ–¹é¢ achieve å¹³å‡å€¼ä¸º 0.045ï¼Œå¹¶ä¸”å¹³å‡è¿è¡Œæ—¶é—´ä¸º 14 æ¯«ç§’ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬å®ç°çš„ REM é¢„æµ‹ç²¾åº¦ä¸ç›¸å…³çš„ cell-free massive multiple-input multiple-output (CF-mMIMO) åº”ç”¨åœºæ™¯è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¯ä»¥ä¸æ¶ˆè€—å¤§é‡çš„èƒ½æºè¿›è¡Œå¤§è§„æ¨¡çš„æŠ˜å°„æŸå¤±æµ‹é‡ï¼Œè€Œæ˜¯å¯ä»¥ä¾é é¢„æµ‹çš„ REM æ¥å†³å®šåœ¨ CF-mMIMO ç½‘ç»œä¸­ Switch ON/OFF çš„å¤§é‡ç¡çœ Access Points (APs)ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-the-Performance-Analysis-of-RIS-Empowered-Communications-Over-Nakagami-m-Fading"><a href="#On-the-Performance-Analysis-of-RIS-Empowered-Communications-Over-Nakagami-m-Fading" class="headerlink" title="On the Performance Analysis of RIS-Empowered Communications Over Nakagami-m Fading"></a>On the Performance Analysis of RIS-Empowered Communications Over Nakagami-m Fading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11893">http://arxiv.org/abs/2309.11893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitris Selimis, Kostas P. Peppas, George C. Alexandropoulos, Fotis I. Lazarakis</li>
<li>for: ç ”ç©¶äº†æ— çº¿é€šä¿¡é€è¿‡å…·å¤‡è‡ªé€‚åº”æ™ºèƒ½é¢ï¼ˆRISsï¼‰çš„ nakagami-m è°ƒé¢‘é€šé“æ€§èƒ½ã€‚</li>
<li>methods: è€ƒè™‘äº†ä¸¤ç§é˜¶æ®µé…ç½®è®¾è®¡ Ğ´Ğ»Ñ RISï¼šä¸€ä¸ªéšæœºçš„å’Œå¦ä¸€ä¸ªåŸºäºåè°ƒé¢‘ç‡è°ƒæ•´ã€‚</li>
<li>results: æ˜¾ç¤ºäº†å¯¹ binary è°ƒå˜æ–¹æ¡ˆçš„åœæœºæ¦‚ç‡ã€é”™è¯¯ç‡å’Œå‡è´¨è´¨é‡çš„å•çº¯ç§¯åˆ†è¡¨è¾¾ï¼Œå¹¶æå‡ºäº†ç²¾ç¡®çš„å…³é”®æ•°æ®è¡¨ç¤ºã€‚<details>
<summary>Abstract</summary>
In this paper, we study the performance of wireless communications empowered by Reconfigurable Intelligent Surface (RISs) over Nakagami-m fading channels. We consider two phase configuration designs for the RIS, one random and another one based on coherent phase shifting. For both phase configuration cases, we present single-integral expressions for the outage probability and the bit error rate of binary modulation schemes, which can be efficiently evaluated numerically. In addition, we propose accurate closed-form approximations for the ergodic capacity of the considered system. For all considered metrics, we have also derived simple analytical expressions that become tight for large numbers of RIS reflecting elements. Numerically evaluated results compared with Monte Carlo simulations are presented in order to verify the correctness of the proposed analysis and showcase the impact of various system settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŸºäºå¯ configurableæ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰çš„æ— çº¿é€šä¿¡ç³»ç»Ÿåœ¨ nakagami-m æŠ˜å°„é€šé“ä¸Šçš„æ€§èƒ½ã€‚æˆ‘ä»¬è€ƒè™‘äº†ä¸¤ç§é˜¶æ®µé…ç½®è®¾è®¡ Ğ´Ğ»Ñ RISï¼Œä¸€ä¸ªæ˜¯éšæœºçš„ï¼Œå¦ä¸€ä¸ªæ˜¯åŸºäº coherent ç›¸ä½è°ƒåˆ¶ã€‚å¯¹äºä¸¤ç§é˜¶æ®µé…ç½®æƒ…å†µï¼Œæˆ‘ä»¬æä¾›äº†å•ä¸€ç§¯åˆ†è¡¨è¾¾å¼ï¼Œå¯ä»¥é«˜æ•ˆåœ°è¯„ä¼° numericallyã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†å‡†ç¡®çš„é—­å¼è¡¨è¾¾å¼ï¼Œç”¨äºè¯„ä¼°ç³»ç»Ÿçš„å¹³å‡å®¹é‡ã€‚å¯¹æ‰€æœ‰è€ƒè™‘çš„æŒ‡æ ‡ï¼Œæˆ‘ä»¬è¿˜ deriväº†ç®€å•çš„åˆ†æè¡¨è¾¾å¼ï¼Œè¿™äº›è¡¨è¾¾å¼åœ¨å¤§é‡ RIS åå°„å…ƒä»¶æ—¶å˜å¾—ç´§å¼ ã€‚æˆ‘ä»¬é€šè¿‡ä¸ Monte Carlo ä»¿çœŸç»“æœè¿›è¡Œæ¯”è¾ƒï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„åˆ†æçš„æ­£ç¡®æ€§ï¼Œå¹¶æ˜¾ç¤ºäº†ä¸åŒç³»ç»Ÿè®¾ç½®å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Near-Field-Beam-Training-Joint-Angle-and-Range-Estimation-with-DFT-Codebook"><a href="#Near-Field-Beam-Training-Joint-Angle-and-Range-Estimation-with-DFT-Codebook" class="headerlink" title="Near-Field Beam Training: Joint Angle and Range Estimation with DFT Codebook"></a>Near-Field Beam Training: Joint Angle and Range Estimation with DFT Codebook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11872">http://arxiv.org/abs/2309.11872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xun Wu, Changsheng You, Jiapeng Li, Yunpu Zhang</li>
<li>for: æé«˜near-field beam trainingçš„æ•ˆç‡å’Œå‡†ç¡®æ€§</li>
<li>methods: ä½¿ç”¨DFT codebookè¿›è¡Œoff-gridèŒƒå›´ä¼°è®¡ï¼Œå¹¶æå‡ºäº†ä¸¤ç§jointlyä¼°è®¡ç”¨æˆ·è§’åº¦å’ŒèŒƒå›´çš„æ–¹æ³•</li>
<li>results: ç»è¿‡numerical simulationsè¡¨æ˜ï¼Œæè®®æ–¹æ³•å¯ä»¥å¤§å¹…é™ä½near-field beam trainingçš„è®­ç»ƒå¼€é”€å’Œæé«˜èŒƒå›´ä¼°è®¡ç²¾åº¦ï¼Œä¸various benchmark schemesç›¸æ¯”æœ‰æ˜¾è‘—ä¼˜åŠ¿<details>
<summary>Abstract</summary>
Prior works on near-field beam training have mostly assumed dedicated polar-domain codebook and on-grid range estimation, which, however, may suffer long training overhead and degraded estimation accuracy. To address these issues, we propose in this paper new and efficient beam training schemes with off-grid range estimation by using conventional discrete Fourier transform (DFT) codebook. Specifically, we first analyze the received beam pattern at the user when far-field beamforming vectors are used for beam scanning, and show an interesting result that this beam pattern contains useful user angle and range information. Then, we propose two efficient schemes to jointly estimate the user angle and range with the DFT codebook. The first scheme estimates the user angle based on a defined angular support and resolves the user range by leveraging an approximated angular support width, while the second scheme estimates the user range by minimizing a power ratio mean square error (MSE) to improve the range estimation accuracy. Finally, numerical simulations show that our proposed schemes greatly reduce the near-field beam training overhead and improve the range estimation accuracy as compared to various benchmark schemes.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…ˆå‰çš„è¿œè¿‘åœºåŸŸè®­ç»ƒç ”ç©¶å¤šåšå‡ºäº†ä¸“é—¨çš„æåŸŸç¼–ç ebookå’Œåœ¨ç½‘æ ¼ä¸Šçš„è·ç¦»ä¼°è®¡ï¼Œä½†è¿™äº›æ–¹æ³•å¯èƒ½ä¼šå¸¦æ¥é•¿æ—¶é—´çš„è®­ç»ƒå¼€é”€å’Œä¼°è®¡ç²¾åº¦ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€äº›æ–°çš„å’Œé«˜æ•ˆçš„è¿œè¿‘åœºåŸŸè®­ç»ƒæ–¹æ¡ˆï¼Œä½¿ç”¨å¸¸è§çš„å¿«é€Ÿå‚…ç«‹å¶å˜æ¢ï¼ˆDFTï¼‰ç¼–ç ebookã€‚ Specifically, we first analyze the received beam pattern at the user when far-field beamforming vectors are used for beam scanning, and show an interesting result that this beam pattern contains useful user angle and range information. Then, we propose two efficient schemes to jointly estimate the user angle and range with the DFT codebook. The first scheme estimates the user angle based on a defined angular support and resolves the user range by leveraging an approximated angular support width, while the second scheme estimates the user range by minimizing a power ratio mean square error (MSE) to improve the range estimation accuracy. Finally, numerical simulations show that our proposed schemes greatly reduce the near-field beam training overhead and improve the range estimation accuracy as compared to various benchmark schemes.
</details></li>
</ul>
<hr>
<h2 id="Joint-Beamforming-for-RIS-Aided-Full-Duplex-Integrated-Sensing-and-Uplink-Communication"><a href="#Joint-Beamforming-for-RIS-Aided-Full-Duplex-Integrated-Sensing-and-Uplink-Communication" class="headerlink" title="Joint Beamforming for RIS Aided Full-Duplex Integrated Sensing and Uplink Communication"></a>Joint Beamforming for RIS Aided Full-Duplex Integrated Sensing and Uplink Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11850">http://arxiv.org/abs/2309.11850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Guo, Yang Liu, Qingqing Wu, Xin Zeng, Qingjiang Shi</li>
<li>For: This paper studies the integrated sensing and communication (ISAC) technology in a full-duplex (FD) uplink communication system, with the aim of improving the uninterrupted target sensing and reducing self-interference (SI).* Methods: The paper employs reconfigurable intelligent surface (RIS) technology to improve the SI suppression and signal processing gain, and develops an iterative solution using convex optimization techniques such as majorization-minimization (MM) and penalty-dual-decomposition (PDD) to optimize all variables.* Results: Numerical results demonstrate the effectiveness of the proposed solution and the great benefit of employing RIS in the FD ISAC system.<details>
<summary>Abstract</summary>
This paper studies integrated sensing and communication (ISAC) technology in a full-duplex (FD) uplink communication system. As opposed to the half-duplex system, where sensing is conducted in a first-emit-then-listen manner, FD ISAC system emits and listens simultaneously and hence conducts uninterrupted target sensing. Besides, impressed by the recently emerging reconfigurable intelligent surface (RIS) technology, we also employ RIS to improve the self-interference (SI) suppression and signal processing gain. As will be seen, the joint beamforming, RIS configuration and mobile users' power allocation is a difficult optimization problem. To resolve this challenge, via leveraging the cutting-the-edge majorization-minimization (MM) and penalty-dual-decomposition (PDD) methods, we develop an iterative solution that optimizes all variables via using convex optimization techniques. Numerical results demonstrate the effectiveness of our proposed solution and the great benefit of employing RIS in the FD ISAC system.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Semi-Supervised-Variational-Inference-over-Nonlinear-Channels"><a href="#Semi-Supervised-Variational-Inference-over-Nonlinear-Channels" class="headerlink" title="Semi-Supervised Variational Inference over Nonlinear Channels"></a>Semi-Supervised Variational Inference over Nonlinear Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11841">http://arxiv.org/abs/2309.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Burshtein, Eli Bery</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸“é—¨é’ˆå¯¹æœªçŸ¥éçº¿æ€§é€šä¿¡ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ¼è¿›è¡Œæ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç ”ç©¶ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº† semi-supervised learning æ–¹æ³•ï¼ŒåŒ…æ‹¬ Monte Carlo expectation maximization å’Œ variational autoencoderï¼Œä»¥è§£ç æœªçŸ¥éçº¿æ€§é€šä¿¡ ĞºĞ°Ğ½Ğ°nlã€‚</li>
<li>results: è¿™äº›æ–¹æ³•å¯ä»¥å……åˆ†åˆ©ç”¨å°‘é‡çš„è¯•éªŒç¬¦å·å’Œæ•°æ®payloadï¼Œå¹¶ä¸”åœ¨å……åˆ†å¤šçš„æ•°æ®payloadæƒ…å†µä¸‹ï¼Œvariational autoencoder ä¹Ÿå¯ä»¥å®ç°æ›´ä½çš„é”™è¯¯ç‡ï¼Œæ¯” meta learning ä½¿ç”¨å½“å‰å’Œå‰ä¸€ä¸ªä¼ è¾“å—çš„è¯•éªŒç¬¦å·ã€‚<details>
<summary>Abstract</summary>
Deep learning methods for communications over unknown nonlinear channels have attracted considerable interest recently. In this paper, we consider semi-supervised learning methods, which are based on variational inference, for decoding unknown nonlinear channels. These methods, which include Monte Carlo expectation maximization and a variational autoencoder, make efficient use of few pilot symbols and the payload data. The best semi-supervised learning results are achieved with a variational autoencoder. For sufficiently many payload symbols, the variational autoencoder also has lower error rate compared to meta learning that uses the pilot data of the present as well as previous transmission blocks.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æœªçŸ¥éçº¿æ€§é€šé“ä¸Šè¿›è¡Œé€šä¿¡å·²ç»å¸å¼•äº†ç›¸å½“å¤šçš„å…³æ³¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘ä½¿ç”¨å˜é‡æ¨ç†çš„åŠç›‘ç£å­¦ä¹ æ–¹æ³•æ¥è§£ç æœªçŸ¥éçº¿æ€§é€šé“ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬Monte Carloé¢„æœŸæœ€å¤§åŒ–å’Œå˜é‡è‡ªé€‚åº”å™¨ï¼Œå®ƒä»¬å¯ä»¥å……åˆ†åˆ©ç”¨å‡ ä¸ªç¤ºä¾‹ç¬¦å·å’Œæ•°æ® payloadã€‚å˜é‡è‡ªé€‚åº”å™¨åœ¨å…·æœ‰è¶³å¤Ÿå¤špayloadç¬¦å·æ—¶å®ç°æœ€ä½³åŠç›‘ç£å­¦ä¹ ç»“æœï¼Œå¹¶ä¸”åœ¨ä½¿ç”¨å½“å‰å’Œå‰ä¸€ä¸ªä¼ è¾“å—çš„Metaå­¦ä¹ æ—¶ä¹Ÿå…·æœ‰è¾ƒä½çš„é”™è¯¯ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Study-of-PAPR-Reduction-Techniques-for-Deep-Joint-Source-Channel-Coding-in-OFDM-Systems"><a href="#A-Comprehensive-Study-of-PAPR-Reduction-Techniques-for-Deep-Joint-Source-Channel-Coding-in-OFDM-Systems" class="headerlink" title="A Comprehensive Study of PAPR Reduction Techniques for Deep Joint Source Channel Coding in OFDM Systems"></a>A Comprehensive Study of PAPR Reduction Techniques for Deep Joint Source Channel Coding in OFDM Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11803">http://arxiv.org/abs/2309.11803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maolin Liu, Wei Chen, Jialong Xu, Bo Ai</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯æ·±åº¦è”åˆæºæ¸ é“ç¼–ç ï¼ˆDJSCCï¼‰ç³»ç»Ÿä¸­çš„å¹²æ‰°ç‡ï¼ˆPAPRï¼‰é—®é¢˜ã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨äº†å¤šç§OFDMå¹²æ‰°ç‡å‡å°‘æŠ€æœ¯ï¼ŒåŒ…æ‹¬ä¼ ç»ŸæŠ€æœ¯such as clippingã€compandingã€SLMå’ŒPTSï¼Œä»¥åŠæ·±åº¦å­¦ä¹ åŸºäºçš„PAPRå‡å°‘æŠ€æœ¯such as PAPRæŸå¤±å’Œclipping with retrainingã€‚</li>
<li>results: æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œè™½ç„¶ä¼ ç»Ÿçš„PAPRå‡å°‘æŠ€æœ¯å¯ä»¥åº”ç”¨äºDJSCCï¼Œä½†å…¶æ€§èƒ½ä¸ä¼ ç»Ÿçš„åˆ†æºæ¸ é“ç¼–ç ä¸åŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œå¯¹ä¿¡å·æŸå®³PAPRå‡å°‘æŠ€æœ¯ï¼Œclipping with retrainingå¯ä»¥åœ¨DJSCCä¸­å®ç°æœ€å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸ä¼šå¯¹ä¿¡å·é‡å»ºç‡äº§ç”Ÿè´Ÿé¢å½±å“ã€‚åŒæ—¶ï¼Œå¯¹ä¿¡å·éæŸå®³PAPRå‡å°‘æŠ€æœ¯å¯ä»¥æˆåŠŸåœ°å‡å°‘DJSCCä¸­çš„PAPRï¼Œä¸ä¼šå½±å“ä¿¡å·é‡å»ºç‡ã€‚<details>
<summary>Abstract</summary>
Recently, deep joint source channel coding (DJSCC) techniques have been extensively studied and have shown significant performance with limited bandwidth and low signal to noise ratio. Most DJSCC work considers discrete-time analog transmission, while combining it with orthogonal frequency division multiplexing (OFDM) creates serious high peak-to-average power ratio (PAPR) problem. This paper conducts a comprehensive analysis on the use of various OFDM PAPR reduction techniques in the DJSCC system, including both conventional techniques such as clipping, companding, SLM and PTS, and deep learning-based PAPR reduction techniques such as PAPR loss and clipping with retraining. Our investigation shows that although conventional PAPR reduction techniques can be applied to DJSCC, their performance in DJSCC is different from the conventional split source channel coding. Moreover, we observe that for signal distortion PAPR reduction techniques, clipping with retraining achieves the best performance in terms of both PAPR reduction and recovery accuracy. It is also noticed that signal non-distortion PAPR reduction techniques can successfully reduce the PAPR in DJSCC without compromise to signal reconstruction.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æ¥ï¼Œæ·±åº¦è”åˆæºæ¸ é“ç¼–ç ï¼ˆDJSCCï¼‰æŠ€æœ¯å·²ç»å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶å’Œåº”ç”¨ï¼Œå®ƒå¯ä»¥åœ¨å…·æœ‰æœ‰é™å¸¦å®½å’Œä½ä¿¡å™ªæ¯”çš„æƒ…å†µä¸‹æ˜¾ç¤ºå‡ºè¾ƒé«˜çš„æ€§èƒ½ã€‚å¤§å¤šæ•°DJSCCå·¥ä½œéƒ½æ˜¯å¯¹ç¦»æ•£æ—¶é—´åˆ†æä¼ è¾“è¿›è¡Œç ”ç©¶ï¼Œè€Œå°†OFDMåˆ†é…å¤šè°±åˆ†å¤šå±‚ï¼ˆPAPRï¼‰é—®é¢˜å¼•å…¥åˆ°DJSCCç³»ç»Ÿä¸­ä¼šäº§ç”Ÿä¸¥é‡çš„é«˜å³°å€¼è‡³å¹³å‡åŠŸç‡æ¯”ï¼ˆPAPRï¼‰é—®é¢˜ã€‚æœ¬æ–‡å¯¹DJSCCç³»ç»Ÿä¸­OFDM PAPRå‡å°‘æŠ€æœ¯çš„ä½¿ç”¨è¿›è¡Œäº†å…¨é¢çš„åˆ†æï¼ŒåŒ…æ‹¬ä¼ ç»ŸæŠ€æœ¯such as clippingã€compandingã€SLMå’ŒPTSï¼Œä»¥åŠæ·±åº¦å­¦ä¹ åŸºäºPAPRå‡å°‘æŠ€æœ¯such as PAPRæŸå¤±å’Œclipping with retrainingã€‚æˆ‘ä»¬çš„è°ƒæŸ¥è¡¨æ˜ï¼Œè™½ç„¶ä¼ ç»ŸPAPRå‡å°‘æŠ€æœ¯å¯ä»¥åº”ç”¨äºDJSCCï¼Œä½†å®ƒä»¬åœ¨DJSCCä¸­çš„æ€§èƒ½ä¸ä¼ ç»Ÿåˆ†Split source channel codingä¸åŒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°åœ¨ä¿¡å·æŸå®³PAPRå‡å°‘æŠ€æœ¯ä¸­ï¼Œclipping with retrainingå¯ä»¥åœ¨PAPRå‡å°‘å’Œé‡å»ºç²¾åº¦æ–¹é¢è¾¾åˆ°æœ€ä½³æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†åœ¨éæŸä¿¡å·PAPRå‡å°‘æŠ€æœ¯ä¸­ï¼Œå¯ä»¥æˆåŠŸåœ°å‡å°‘DJSCCä¸­çš„PAPRï¼Œè€Œæ— éœ€ç‰ºç‰²ä¿¡å·é‡å»ºç²¾åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="Quantum-Circuits-for-Stabilizer-Error-Correcting-Codes-A-Tutorial"><a href="#Quantum-Circuits-for-Stabilizer-Error-Correcting-Codes-A-Tutorial" class="headerlink" title="Quantum Circuits for Stabilizer Error Correcting Codes: A Tutorial"></a>Quantum Circuits for Stabilizer Error Correcting Codes: A Tutorial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11793">http://arxiv.org/abs/2309.11793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Mondal, Keshab K. Parhi</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†ä»‹ç»è®¾è®¡å’Œæ¨¡æ‹Ÿé‡å­ç¼–ç å™¨å’Œè§£ç å™¨ç”µè·¯ï¼Œä»¥åŠéªŒè¯è¿™äº›ç”µè·¯çš„æ­£ç¡®æ€§ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†é‡å­ç¼–ç å’Œè§£ç çš„æ–¹æ³•ï¼Œå¹¶æä¾›äº†äº”ä¸ªé‡å­æ¯”ç‰¹çš„ç¼–ç å’Œè§£ç ç”µè·¯ï¼Œä»¥åŠé€‚ç”¨äºäº”ä¸ªé‡å­æ¯”ç‰¹çš„æœ€è¿‘é‚»å±…å…¼å®¹çš„ç¼–ç å’Œè§£ç ç”µè·¯ã€‚</li>
<li>results: è®ºæ–‡éªŒè¯äº†è¿™äº›ç”µè·¯çš„æ­£ç¡®æ€§ï¼Œå¹¶æä¾›äº†ä½¿ç”¨IBM Qiskitè¿›è¡ŒéªŒè¯çš„æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Quantum computers have the potential to provide exponential speedups over their classical counterparts. Quantum principles are being applied to fields such as communications, information processing, and artificial intelligence to achieve quantum advantage. However, quantum bits are extremely noisy and prone to decoherence. Thus, keeping the qubits error free is extremely important toward reliable quantum computing. Quantum error correcting codes have been studied for several decades and methods have been proposed to import classical error correcting codes to the quantum domain. However, circuits for such encoders and decoders haven't been explored in depth. This paper serves as a tutorial on designing and simulating quantum encoder and decoder circuits for stabilizer codes. We present encoding and decoding circuits for five-qubit code and Steane code, along with verification of these circuits using IBM Qiskit. We also provide nearest neighbour compliant encoder and decoder circuits for the five-qubit code.
</details>
<details>
<summary>æ‘˜è¦</summary>
é‡å­è®¡ç®—æœºæœ‰å¯èƒ½æä¾›æŒ‡æ•°å¢é€Ÿäºå…¶ç»å…¸å¯¹æ‰‹ã€‚é‡å­åŸç†åœ¨é€šä¿¡ã€ä¿¡æ¯å¤„ç†å’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸåº”ç”¨ä»¥å®ç°é‡å­ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œé‡å­æ¯”ç‰¹éå¸¸æ˜“å—å™ªå£°å’Œé™è§£çš„å½±å“ã€‚å› æ­¤ï¼Œä¿æŒé‡å­æ¯”ç‰¹é”™è¯¯è‡ªç”±éå¸¸é‡è¦äºå¯é çš„é‡å­è®¡ç®—ã€‚é‡å­é”™è¯¯ä¿®å¤ä»£ç å·²ç»åœ¨å‡ åå¹´å†…ç ”ç©¶ï¼Œå¹¶æå‡ºäº†å°†ç»å…¸é”™è¯¯ä¿®å¤ä»£ç å¼•å…¥é‡å­é¢†åŸŸçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›åœˆå®šå™¨å’Œè§£ç å™¨ç”µè·¯çš„è®¾è®¡å’Œä»¿çœŸè¿˜æ²¡æœ‰å¾—åˆ°æ·±å…¥ç ”ç©¶ã€‚è¿™ç¯‡è®ºæ–‡ä½œä¸ºé‡å­ç¼–ç å’Œè§£ç ç”µè·¯è®¾è®¡å’Œä»¿çœŸçš„æ•™ç¨‹ï¼Œæˆ‘ä»¬æå‡ºäº†äº”ä¸ªé‡å­æ¯”ç‰¹ç¼–ç å’Œæ–¯ç‰¹æ©ä»£ç çš„ç¼–ç å’Œè§£ç ç”µè·¯ï¼Œå¹¶ä½¿ç”¨IBM Qiskitè¿›è¡ŒéªŒè¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†æœ€è¿‘é‚»å±…å…¼å®¹çš„ç¼–ç å’Œè§£ç ç”µè·¯ Ğ´Ğ»Ñäº”ä¸ªé‡å­æ¯”ç‰¹ç¼–ç ã€‚
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Fault-Identification-Reconstruction-in-Multi-Agent-Systems"><a href="#Collaborative-Fault-Identification-Reconstruction-in-Multi-Agent-Systems" class="headerlink" title="Collaborative Fault-Identification &amp; Reconstruction in Multi-Agent Systems"></a>Collaborative Fault-Identification &amp; Reconstruction in Multi-Agent Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11784">http://arxiv.org/abs/2309.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiraz Khan, Inseok Hwang</li>
<li>for: æä¾›äº†ä¸€ç§é«˜æ•ˆçš„åˆ†å¸ƒå¼FDIRæœºåˆ¶ï¼Œé€‚ç”¨äºå¤š Agent åº”ç”¨ã€‚</li>
<li>methods: åŸºäºsequential convex programming (SCP) å’Œ alternating direction method of multipliers (ADMM) ä¼˜åŒ–æ–¹æ³•ï¼Œå®ç°åˆ†å¸ƒå¼å¤š Agent FDIRç®—æ³•ã€‚</li>
<li>results: å¯ä»¥å¤„ç†å¤š Agent é—´æµ‹é‡ï¼ˆåŒ…æ‹¬è·ç¦»ã€æ–¹å‘ã€ç›¸å¯¹é€Ÿåº¦å’Œå¤¹è§’ï¼‰ï¼Œç¡®å®šfaulty Agent å’Œé‡å»ºå…¶çœŸå®çŠ¶æ€ã€‚<details>
<summary>Abstract</summary>
The conventional solutions for fault-detection, identification, and reconstruction (FDIR) require centralized decision-making mechanisms which are typically combinatorial in their nature, necessitating the design of an efficient distributed FDIR mechanism that is suitable for multi-agent applications. To this end, we develop a general framework for efficiently reconstructing a sparse vector being observed over a sensor network via nonlinear measurements. The proposed framework is used to design a distributed multi-agent FDIR algorithm based on a combination of the sequential convex programming (SCP) and the alternating direction method of multipliers (ADMM) optimization approaches. The proposed distributed FDIR algorithm can process a variety of inter-agent measurements (including distances, bearings, relative velocities, and subtended angles between agents) to identify the faulty agents and recover their true states. The effectiveness of the proposed distributed multi-agent FDIR approach is demonstrated by considering a numerical example in which the inter-agent distances are used to identify the faulty agents in a multi-agent configuration, as well as reconstruct their error vectors.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼ ç»Ÿçš„ç‘•ç‚¹æ£€æµ‹ã€è¯†åˆ«å’Œé‡å»ºï¼ˆFDIRï¼‰è§£å†³æ–¹æ¡ˆé€šå¸¸éœ€è¦ä¸­å¤®å†³ç­–æœºåˆ¶ï¼Œè¿™äº›æœºåˆ¶é€šå¸¸æ˜¯ combinatorial çš„æ€§è´¨ï¼Œéœ€è¦è®¾è®¡ä¸€ç§é«˜æ•ˆçš„åˆ†å¸ƒå¼ FDIR æœºåˆ¶ï¼Œé€‚ç”¨äºå¤šæœºå™¨äººåº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§é«˜æ•ˆåœ°é‡å»º sparse vector åœ¨æ„ŸçŸ¥ç½‘ç»œä¸Šè¢«è§‚å¯Ÿçš„æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŸºäº sequential convex programming (SCP) å’Œ alternating direction method of multipliers (ADMM) ä¼˜åŒ–æ–¹æ³•æ¥è®¾è®¡åˆ†å¸ƒå¼å¤šæœºå™¨äºº FDIR ç®—æ³•ã€‚è¯¥ç®—æ³•å¯ä»¥å¤„ç†å¤šæœºå™¨äººä¹‹é—´çš„å„ç§æµ‹é‡æ•°æ®ï¼ˆåŒ…æ‹¬è·ç¦»ã€æ–¹å‘ã€ç›¸å¯¹é€Ÿåº¦å’Œ agents ä¹‹é—´çš„å¤¹è§’ï¼‰æ¥è¯†åˆ«ç‘•ç‚¹æœºå™¨äººå¹¶é‡å»ºå…¶çœŸå®çŠ¶æ€ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæ•°å­¦ç¤ºä¾‹æ¥è¯æ˜æå‡ºçš„åˆ†å¸ƒå¼å¤šæœºå™¨äºº FDIR æ–¹æ³•çš„æ•ˆæœï¼Œåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œåˆ©ç”¨äº†æœºå™¨äººä¹‹é—´çš„è·ç¦»æµ‹é‡æ¥è¯†åˆ«ç‘•ç‚¹æœºå™¨äººå’Œé‡å»ºå…¶é”™è¯¯å‘é‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-SEFDM-Performance-in-High-Doppler-Channels"><a href="#Enhancing-the-SEFDM-Performance-in-High-Doppler-Channels" class="headerlink" title="Enhancing the SEFDM Performance in High-Doppler Channels"></a>Enhancing the SEFDM Performance in High-Doppler Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11774">http://arxiv.org/abs/2309.11774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahdi Shamsi, Farokh Marvasti</li>
<li>for: è¯¥ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§åŸºäº Spectrally Efficient Frequency Division Multiplexing (SEFDM) æŠ€æœ¯çš„é«˜æ•ˆå¤„ç†ç§»åŠ¨é€šä¿¡é¢‘ç‡å»¶è¿Ÿå’ŒDoppleråç§»çš„æ–¹æ³•ï¼Œå¹¶é‡‡ç”¨ Frequency Domain Cyclic Prefix (FDCP) å’Œ Modified Non-Linear (MNL) åŠ é€ŸæŠ€æœ¯ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº† SEFDM æŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨äº† FDCP å’Œ MNL åŠ é€ŸæŠ€æœ¯æ¥å¤„ç†ç§»åŠ¨é€šä¿¡é¢‘ç‡å»¶è¿Ÿå’ŒDoppleråç§»çš„å½±å“ã€‚</li>
<li>results: è¯¥ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ SEFDM æŠ€æœ¯å¯ä»¥åœ¨ç§»åŠ¨é€šä¿¡é¢‘ç‡å»¶è¿Ÿå’ŒDoppleråç§»çš„ç¯å¢ƒä¸­å®ç°å¯é å’Œé«˜è´¨é‡çš„é€šä¿¡ï¼Œå¹¶ä¸”å¯ä»¥ä¿æŒä¼ ç»Ÿé€šä¿¡ç³»ç»Ÿçš„ spectral efficiencyã€‚<details>
<summary>Abstract</summary>
In this paper, we propose the use of Spectrally Efficient Frequency Division Multiplexing (SEFDM) with additional techniques such as Frequency Domain Cyclic Prefix (FDCP) and Modified Non-Linear (MNL) acceleration for efficient handling of the impact of delay and Doppler shift in mobile communication channels. Our approach exhibits superior performance and spectral efficiency in comparison to traditional communication systems, while maintaining low computational cost. We study a model of the SEFDM communication system and investigate the impact of MNL acceleration with soft and hard decision Inverse System on the performance of SEFDM detection in the AWGN channel. We also analyze the effectiveness of FDCP in compensating for the impact of Doppler shift, and report BER detection figures using Regularized Sphere Decoding in various simulation scenarios. Our simulations demonstrate that it is possible to achieve acceptable performance in Doppler channels while maintaining the superiority of SEFDM over OFDM in terms of spectral efficiency. The results suggest that our proposed approach can tackle the effects of delay and Doppler shift in mobile communication networks, guaranteeing dependable and high-quality communication even in extremely challenging environments.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨å…·æœ‰é¢‘ç‡åˆ†é…å¤šæ™®é›·æ–¯ç‰¹ï¼ˆSEFDMï¼‰çš„spectrally efficient frequency division multiplexingæŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨é¢‘åŸŸå¾ªç¯ prefixï¼ˆFDCPï¼‰å’Œä¿®æ”¹éçº¿æ€§ï¼ˆMNLï¼‰åŠ é€ŸæŠ€æœ¯æ¥æœ‰æ•ˆåœ°å¤„ç†ç§»åŠ¨é€šä¿¡é¢‘é“ä¸­çš„å»¶è¿Ÿå’ŒDoppleråç§»çš„å½±å“ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯”è¾ƒ tradicional communication systemsçš„æƒ…å†µä¸‹è¡¨ç°å‡ºè¾ƒé«˜çš„æ€§èƒ½å’Œé¢‘ç‡æ•ˆç‡ï¼ŒåŒæ—¶ä¿æŒä½çš„è®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬ç ”ç©¶äº†SEFDMé€šä¿¡ç³»ç»Ÿçš„æ¨¡å‹ï¼Œå¹¶investigate MNLåŠ é€Ÿå™¨åœ¨SOFTå’ŒHARD decision inverse systemä¸­çš„å½±å“ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†FDCPåœ¨è¡¥åšDoppleråç§»çš„æ•ˆæœï¼Œå¹¶reportäº†åœ¨ä¸åŒçš„ simulate scenarioä¸­çš„BERæ£€æµ‹æ•°æ®ã€‚æˆ‘ä»¬çš„Simulationsè¡¨æ˜ï¼Œå¯ä»¥åœ¨Doppleré¢‘é“ä¸­å®ç°å¯æ¥å—çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒSEFDMåœ¨OFDMæ–¹é¢çš„ä¼˜åŠ¿ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æè®®çš„æ–¹æ³•å¯ä»¥åœ¨ç§»åŠ¨é€šä¿¡ç½‘ç»œä¸­æŠµå¾¡å»¶è¿Ÿå’ŒDoppleråç§»çš„å½±å“ï¼Œä¿è¯é«˜è´¨é‡å’Œå¯é çš„é€šä¿¡ï¼Œeven in extremely challenging environmentsã€‚
</details></li>
</ul>
<hr>
<h2 id="Symbol-Detection-for-Coarsely-Quantized-OTFS"><a href="#Symbol-Detection-for-Coarsely-Quantized-OTFS" class="headerlink" title="Symbol Detection for Coarsely Quantized OTFS"></a>Symbol Detection for Coarsely Quantized OTFS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11759">http://arxiv.org/abs/2309.11759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junwei He, Haochuan Zhang, Chao Dong, Huimin Zhu</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸“é—¨é’ˆå¯¹å—é™åˆ¶çš„é‡åŒ– Communication system ä¸­çš„orthogonal time frequency space (OTFS) æŠ€æœ¯ï¼Œä»¥å®ç°æˆæœ¬å’ŒåŠŸç‡çš„æœ€ä¼˜åŒ–ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†coarse quantization å’Œ signal recovery ç®—æ³•ï¼ŒåŒ…æ‹¬åŸå§‹çš„approximate message passing (AMP) å’Œ generalized expectation consistent for signal recovery (GEC-SR)ã€‚</li>
<li>results: è®ºæ–‡æå‡ºäº†ä¸€ç§ä½å¤æ‚åº¦çš„ç®—æ³•ï¼Œå³å°† GEC-SR ç®—æ³•ä¸å¿«é€Ÿå½’ä¸€åŒ–çš„ quasi-banded matrices ç»“åˆï¼Œä»è€Œé™ä½äº†è®¡ç®—å¤æ‚åº¦ä»ç«‹æ–¹ä½“ç§¯åˆ°çº¿æ€§ç§¯ï¼Œä¿æŒäº†æ€§èƒ½æ°´å¹³ã€‚<details>
<summary>Abstract</summary>
This paper explicitly models a coarse and noisy quantization in a communication system empowered by orthogonal time frequency space (OTFS) for cost and power efficiency. We first point out, with coarse quantization, the effective channel is imbalanced and thus no longer able to circularly shift the transmitted symbols along the delay-Doppler domain. Meanwhile, the effective channel is non-isotropic, which imposes a significant loss to symbol detection algorithms like the original approximate message passing (AMP). Although the algorithm of generalized expectation consistent for signal recovery (GEC-SR) can mitigate this loss, the complexity in computation is prohibitively high, mainly due to an dramatic increase in the matrix size of OTFS. In this context, we propose a low-complexity algorithm that incorporates into the GEC-SR a quick inversion of quasi-banded matrices, reducing the complexity from a cubic order to a linear order while keeping the performance at the same level.
</details>
<details>
<summary>æ‘˜è¦</summary>
In the system, the effective channel is imbalanced and non-isotropic due to coarse quantization, which leads to a significant loss in symbol detection algorithms such as the original approximate message passing (AMP). The GEC-SR algorithm can mitigate this loss, but the high computational complexity prohibits its use. The proposed algorithm addresses this issue by reducing the computational complexity while maintaining the performance.The key idea of the proposed algorithm is to incorporate a quick inversion of quasi-banded matrices into the GEC-SR method. This allows for a significant reduction in computational complexity, from a cubic order to a linear order, while maintaining the same performance. The proposed algorithm is designed to address the issues of coarse and noisy quantization in OTFS-based communication systems, and it has important implications for cost and power efficiency.
</details></li>
</ul>
<hr>
<h2 id="Systematic-Design-and-Optimization-of-Quantum-Circuits-for-Stabilizer-Codes"><a href="#Systematic-Design-and-Optimization-of-Quantum-Circuits-for-Stabilizer-Codes" class="headerlink" title="Systematic Design and Optimization of Quantum Circuits for Stabilizer Codes"></a>Systematic Design and Optimization of Quantum Circuits for Stabilizer Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12373">http://arxiv.org/abs/2309.12373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Mondal, Keshab K. Parhi</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æ„å»ºä¸€ç§ç³»ç»Ÿçš„æ‰©å±•Builderçš„æ‰©å±•Builderï¼Œä»¥å®ç°ç¨³å®šçš„é‡å­è®¡ç®—ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§å½¢å¼åŒ–çš„ç®—æ³•ï¼Œä»¥ç³»ç»Ÿåœ°æ„å»ºæ‰©å±•Builderï¼Œå¹¶ä¸”æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ï¼Œä»¥é™ä½æ‰©å±•Builderçš„é—¨æ•°ã€‚</li>
<li>results: æœ¬æ–‡é€šè¿‡ä½¿ç”¨IBM Qiskitè¿›è¡ŒéªŒè¯ï¼Œæå‡ºäº†ä¸€ç§ä¼˜åŒ–çš„å…«ä¸ªé‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰ç¼–ç å™¨ï¼Œå…¶ä¸­ä½¿ç”¨äº†18ä¸ªCNOTé—¨å’Œ4ä¸ª Hadamardé—¨ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨å…ˆå‰çš„å·¥ä½œä¸­åªç”¨äº†14ä¸ªå•é‡å­é—¨ã€33ä¸ªäºŒé‡å­é—¨å’Œ6ä¸ªCCNOTé—¨ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¼˜åŒ–çš„æ–¯å¦å†…ç ç¼–ç å™¨å’Œ13ä¸ªé‡å­æ¯”ç‰¹ç¼–ç å™¨ï¼Œä»¥é™ä½é—¨æ•°ã€‚<details>
<summary>Abstract</summary>
Quantum computing is an emerging technology that has the potential to achieve exponential speedups over their classical counterparts. To achieve quantum advantage, quantum principles are being applied to fields such as communications, information processing, and artificial intelligence. However, quantum computers face a fundamental issue since quantum bits are extremely noisy and prone to decoherence. Keeping qubits error free is one of the most important steps towards reliable quantum computing. Different stabilizer codes for quantum error correction have been proposed in past decades and several methods have been proposed to import classical error correcting codes to the quantum domain. However, formal approaches towards the design and optimization of circuits for these quantum encoders and decoders have so far not been proposed. In this paper, we propose a formal algorithm for systematic construction of encoding circuits for general stabilizer codes. This algorithm is used to design encoding and decoding circuits for an eight-qubit code. Next, we propose a systematic method for the optimization of the encoder circuit thus designed. Using the proposed method, we optimize the encoding circuit in terms of the number of 2-qubit gates used. The proposed optimized eight-qubit encoder uses 18 CNOT gates and 4 Hadamard gates, as compared to 14 single qubit gates, 33 2-qubit gates, and 6 CCNOT gates in a prior work. The encoder and decoder circuits are verified using IBM Qiskit. We also present optimized encoder circuits for Steane code and a 13-qubit code in terms of the number of gates used.
</details>
<details>
<summary>æ‘˜è¦</summary>
é‡å­è®¡ç®—æ˜¯ä¸€ç§emergingæŠ€æœ¯ï¼Œå®ƒå¯ä»¥å®ç°å¯¹äºç±»ä¼ ç»Ÿè®¡ç®—æœºçš„å¿«é€Ÿå¢é•¿ã€‚ä¸ºäº†å®ç°é‡å­ä¼˜åŠ¿ï¼Œé‡å­åŸç†è¢«åº”ç”¨åˆ°é€šä¿¡ã€ä¿¡æ¯å¤„ç†å’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚ç„¶è€Œï¼Œé‡å­è®¡ç®—æœºé¢ä¸´ä¸€ä¸ªfundamentalé—®é¢˜ï¼Œé‚£å°±æ˜¯é‡å­æ¯”ç‰¹ï¼ˆqubitsï¼‰å…·æœ‰æé«˜çš„å™ªå£°å’Œå¤±å»ç¨³å®šæ€§ã€‚ä¿æŒqubitsé”™è¯¯è‡ªç”±æ˜¯é‡å­è®¡ç®—çš„é‡è¦æ­¥éª¤ã€‚è¿‡å»å‡ åå¹´ï¼Œæœ‰å¤šç§ç¨³å®šç ä¸ºé‡å­é”™è¯¯ corrections proposedï¼Œä½† formalæ–¹æ³• towards the design and optimization of circuits for these quantum encoders and decoders have not been proposed.åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„å»ºæ„æ–¹æ³• Ğ´Ğ»Ñæ™®é€šçš„ç¨³å®šç ç¼–ç ç”µè·¯ã€‚è¿™ç§æ–¹æ³•ç”¨äºè®¾è®¡ç¼–ç å’Œè§£ç ç”µè·¯ Ğ´Ğ»Ñå…«ä¸ªé‡å­æ¯”ç‰¹çš„ä»£ç ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç³»ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–æ‰€è®¾è®¡çš„ç¼–ç ç”µè·¯ã€‚ä½¿ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†ç¼–ç ç”µè·¯ï¼Œä½¿å…¶ä½¿ç”¨çš„ä¸¤ä¸ªé‡å­æ¯”ç‰¹é—¨çš„æ•°é‡å‡å°‘ä¸º18ä¸ªCNOTé—¨å’Œ4ä¸ª Hadamardé—¨ï¼Œä¸ä¹‹å‰çš„14ä¸ªå•é‡å­æ¯”ç‰¹é—¨ã€33ä¸ªäºŒé‡å­æ¯”ç‰¹é—¨å’Œ6ä¸ªCCNOTé—¨ç›¸æ¯”ã€‚æˆ‘ä»¬ä½¿ç”¨IBM QiskitéªŒè¯äº†ç¼–ç å’Œè§£ç ç”µè·¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¼˜åŒ–åçš„å…«ä¸ªé‡å­æ¯”ç‰¹ç¼–ç ç”µè·¯ã€Steaneä»£ç å’Œ13ä¸ªé‡å­æ¯”ç‰¹ä»£ç çš„ä¼˜åŒ–ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Meets-Swarm-Intelligence-for-UAV-Assisted-IoT-Coverage-in-Massive-MIMO"><a href="#Deep-Learning-Meets-Swarm-Intelligence-for-UAV-Assisted-IoT-Coverage-in-Massive-MIMO" class="headerlink" title="Deep Learning Meets Swarm Intelligence for UAV-Assisted IoT Coverage in Massive MIMO"></a>Deep Learning Meets Swarm Intelligence for UAV-Assisted IoT Coverage in Massive MIMO</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11748">http://arxiv.org/abs/2309.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mobeen Mahmood, MohammadMahdi Ghadaksaz, Asil Koc, Tho Le-Ngoc</li>
<li>for: This study is written for UAV-assisted multi-user massive multiple-input multiple-output (MU-mMIMO) systems, specifically for Internet-of-Things (IoT) users.</li>
<li>methods: The study uses a joint optimization problem of hybrid beamforming (HBF), UAV relay positioning, and power allocation (PA) to maximize the total achievable rate (AR) for multiple IoT users. The study also adopts a geometry-based millimeter-wave (mmWave) channel model for both links and proposes three different swarm intelligence (SI)-based algorithmic solutions to optimize.</li>
<li>results: The study shows that the proposed algorithmic solutions can attain higher capacity and reduce average delay for delay-constrained transmissions in a UAV-assisted MU-mMIMO IoT systems. Additionally, the proposed J-HBF-DLLPA can closely approach the optimal capacity while significantly reducing the runtime by 99%, which makes the DL-based solution a promising implementation for real-time online applications in UAV-assisted MU-mMIMO IoT systems.Here is the result in Simplified Chinese text:</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†UAVååŠ©å¤šç”¨æˆ·å¤§è§„æ¨¡å¤šè¾“å…¥å¤šè¾“å‡ºï¼ˆMU-mMIMOï¼‰ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯äº’è”ç½‘ç‰©è”ç½‘ï¼ˆIoTï¼‰ç”¨æˆ·ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªå…±åŒä¼˜åŒ–é—®é¢˜ï¼ŒåŒ…æ‹¬Hybrid beamformingï¼ˆHBFï¼‰ã€UAV relayä½ç½®å†³å®šå’ŒåŠŸç‡åˆ†é…ï¼ˆPAï¼‰ï¼Œä»¥æœ€å¤§åŒ–å¤šä¸ªIoTç”¨æˆ·çš„æ€»å¯å®ç°ç‡ï¼ˆARï¼‰ã€‚ç ”ç©¶è¿˜é‡‡ç”¨äº†ä¸€ä¸ªåŸºäºå‡ ä½•å­¦çš„æ¯«ç±³æ³¢ï¼ˆmmWaveï¼‰é€šé“æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸‰ç§ä¸åŒçš„ç¾¤æ™ºèƒ½ç®—æ³•è§£å†³æ–¹æ¡ˆã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œæå‡ºçš„ç®—æ³•è§£å†³æ–¹æ¡ˆå¯ä»¥åœ¨UAVååŠ©MU-mMIMO IoTç³»ç»Ÿä¸­å®ç°æ›´é«˜çš„å®¹é‡å’Œå‡å°‘å»¶è¿Ÿã€‚æ­¤å¤–ï¼Œæå‡ºçš„J-HBF-DLLPAå¯ä»¥å‡†ç¡®åœ°é¢„æµ‹UAVçš„ä½ç½®å’Œä¼˜åŒ–çš„åŠŸç‡å€¼ï¼Œä»¥å®ç°æœ€å¤§åŒ–ARã€‚<details>
<summary>Abstract</summary>
This study considers a UAV-assisted multi-user massive multiple-input multiple-output (MU-mMIMO) systems, where a decode-and-forward (DF) relay in the form of an unmanned aerial vehicle (UAV) facilitates the transmission of multiple data streams from a base station (BS) to multiple Internet-of-Things (IoT) users. A joint optimization problem of hybrid beamforming (HBF), UAV relay positioning, and power allocation (PA) to multiple IoT users to maximize the total achievable rate (AR) is investigated. The study adopts a geometry-based millimeter-wave (mmWave) channel model for both links and proposes three different swarm intelligence (SI)-based algorithmic solutions to optimize: 1) UAV location with equal PA; 2) PA with fixed UAV location; and 3) joint PA with UAV deployment. The radio frequency (RF) stages are designed to reduce the number of RF chains based on the slow time-varying angular information, while the baseband (BB) stages are designed using the reduced-dimension effective channel matrices. Then, a novel deep learning (DL)-based low-complexity joint hybrid beamforming, UAV location and power allocation optimization scheme (J-HBF-DLLPA) is proposed via fully-connected deep neural network (DNN), consisting of an offline training phase, and an online prediction of UAV location and optimal power values for maximizing the AR. The illustrative results show that the proposed algorithmic solutions can attain higher capacity and reduce average delay for delay-constrained transmissions in a UAV-assisted MU-mMIMO IoT systems. Additionally, the proposed J-HBF-DLLPA can closely approach the optimal capacity while significantly reducing the runtime by 99%, which makes the DL-based solution a promising implementation for real-time online applications in UAV-assisted MU-mMIMO IoT systems.
</details>
<details>
<summary>æ‘˜è¦</summary>
To solve this optimization problem, the study proposes three different swarm intelligence (SI)-based algorithmic solutions:1. UAV location with equal PA2. PA with fixed UAV location3. Joint PA with UAV deploymentThe radio frequency (RF) stages are designed to reduce the number of RF chains based on slow time-varying angular information, while the baseband (BB) stages are designed using reduced-dimension effective channel matrices.Furthermore, a novel deep learning (DL)-based low-complexity joint hybrid beamforming, UAV location, and power allocation optimization scheme (J-HBF-DLLPA) is proposed. This scheme consists of an offline training phase and an online prediction of UAV location and optimal power values to maximize AR.The illustrative results show that the proposed algorithmic solutions can achieve higher capacity and reduce average delay for delay-constrained transmissions in a UAV-assisted MU-mMIMO IoT system. Additionally, the proposed J-HBF-DLLPA can closely approach the optimal capacity while significantly reducing the runtime by 99%, making it a promising implementation for real-time online applications in UAV-assisted MU-mMIMO IoT systems.
</details></li>
</ul>
<hr>
<h2 id="Resource-Allocation-for-Semantic-Aware-Mobile-Edge-Computing-Systems"><a href="#Resource-Allocation-for-Semantic-Aware-Mobile-Edge-Computing-Systems" class="headerlink" title="Resource Allocation for Semantic-Aware Mobile Edge Computing Systems"></a>Resource Allocation for Semantic-Aware Mobile Edge Computing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11736">http://arxiv.org/abs/2309.11736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Cang, Ming Chen, Zhaohui Yang, Yuntao Hu, Yinlu Wang, Zhaoyang Zhang, Kai-Kit Wong</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§åŸºäºç§»åŠ¨è¾¹ç¼˜è®¡ç®—ï¼ˆMECï¼‰ç³»ç»Ÿçš„è¯­ä¹‰æ„è¯†æ„ŸçŸ¥é€šä¿¡å’Œè®¡ç®—èµ„æºåˆ†é…æ¡†æ¶ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†è¯­ä¹‰æ„ŸçŸ¥æŠ€æœ¯æ¥å‡å°‘ä¼ è¾“è´Ÿæ‹…ï¼Œæ¯ä¸ªç»ˆç«¯è®¾å¤‡ï¼ˆTDï¼‰å°†å°å‹EXTRACTED semantic informationå‘é€åˆ°æœåŠ¡å™¨ï¼Œè€Œä¸æ˜¯å¤§é‡çš„åŸå§‹æ•°æ®ã€‚ä¸€ä¸ªä¼˜åŒ–é—®é¢˜çš„ JOINT semantic-awareåˆ†é…å› å­ã€é€šä¿¡å’Œè®¡ç®—èµ„æºç®¡ç†é—®é¢˜æ˜¯å½¢ulatedï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–æ‰€æœ‰TDçš„æ‰§è¡Œå»¶è¿Ÿï¼ŒåŒæ—¶æ»¡è¶³èƒ½é‡æ¶ˆè€—é™åˆ¶ã€‚</li>
<li>results: é€šè¿‡å¯¹éå¯¹ç§°çš„åŸå§‹é—®é¢˜è¿›è¡Œå‡ ä½•ç¼–ç¨‹å˜æ¢ï¼Œå¹¶ä½¿ç”¨äº¤äº’ä¼˜åŒ–ç®—æ³•è§£å†³ï¼Œå¾—åˆ°äº†æœ€ä¼˜è§£ã€‚æ­¤å¤–ï¼Œclosed-formçš„è¯­ä¹‰æå–å› å­çš„ä¼˜åŒ–è§£ä¹Ÿæ˜¯ deriveã€‚å¯¹æ¯” benchmark algorithm without semantic-aware allocationï¼Œæå‡ºçš„ç®—æ³•å¯ä»¥å‡å°‘æœ€å¤§æ‰§è¡Œå»¶è¿Ÿè¾¾37.10%ã€‚åŒæ—¶ï¼Œåœ¨å¤§ä»»åŠ¡å¤§å°å’ŒPoor channel conditionä¸‹ï¼Œå°è¯­ä¹‰æå–å› å­è¢«é¦–é€‰ã€‚<details>
<summary>Abstract</summary>
In this paper, a semantic-aware joint communication and computation resource allocation framework is proposed for mobile edge computing (MEC) systems. In the considered system, each terminal device (TD) has a computation task, which needs to be executed by offloading to the MEC server. To further decrease the transmission burden, each TD sends the small-size extracted semantic information of tasks to the server instead of the large-size raw data. An optimization problem of joint semantic-aware division factor, communication and computation resource management is formulated. The problem aims to minimize the maximum execution delay of all TDs while satisfying energy consumption constraints. The original non-convex problem is transformed into a convex one based on the geometric programming and the optimal solution is obtained by the alternating optimization algorithm. Moreover, the closed-form optimal solution of the semantic extraction factor is derived. Simulation results show that the proposed algorithm yields up to 37.10% delay reduction compared with the benchmark algorithm without semantic-aware allocation. Furthermore, small semantic extraction factors are preferred in the case of large task sizes and poor channel conditions.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æœ¬æ–‡ä¸­ï¼Œä¸€ç§åŸºäº semantics çš„é›†æˆé€šä¿¡å’Œè®¡ç®—èµ„æºåˆ†é…æ¡†æ¶è¢«æå‡ºç”¨äºç§»åŠ¨è¾¹ç¼˜è®¡ç®—ï¼ˆMECï¼‰ç³»ç»Ÿã€‚ç³»ç»Ÿä¸­æ¯ä¸ªç»ˆç«¯è®¾å¤‡ï¼ˆTDï¼‰éƒ½æœ‰ä¸€ä¸ªè®¡ç®—ä»»åŠ¡ï¼Œéœ€è¦é€šè¿‡å¸è½½åˆ° MEC æœåŠ¡å™¨è¿›è¡Œæ‰§è¡Œã€‚ä¸ºäº†è¿›ä¸€æ­¥å‡å°‘ä¼ è¾“è´Ÿæ‹…ï¼Œæ¯ä¸ª TD å°†å°å‹çš„æŠ½å– semantic ä¿¡æ¯å‘é€åˆ°æœåŠ¡å™¨ï¼Œè€Œä¸æ˜¯å¤§é‡çš„åŸå§‹æ•°æ®ã€‚ä¸€ä¸ªåè°ƒsemantic-awareåˆ†é…å› å­ã€é€šä¿¡å’Œè®¡ç®—èµ„æºç®¡ç†çš„ä¼˜åŒ–é—®é¢˜è¢«å½¢ulatedã€‚è¯¥é—®é¢˜çš„ç›®æ ‡æ˜¯ minimize æ‰€æœ‰ TD çš„æ‰§è¡Œå»¶è¿Ÿæœ€å¤§å€¼ï¼ŒåŒæ—¶æ»¡è¶³èƒ½é‡æ¶ˆè€—é™åˆ¶ã€‚åŸå§‹çš„éæ³›åˆå‡½æ•°é—®é¢˜è¢«è½¬åŒ–ä¸ºä¸€ä¸ªå·ç§¯å‡½æ•°é—®é¢˜ï¼Œå¹¶é€šè¿‡å·ç§¯ç¼–ç¨‹å¾—åˆ°äº†ä¼˜åŒ–è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œclosed-form ä¼˜åŒ–è§£å†³æ–¹æ¡ˆçš„ semantic æŠ½å–å› å­ä¹Ÿè¢« derivationã€‚ simulation ç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ç®—æ³•å¯ä»¥å‡å°‘æœ€å¤š 37.10% çš„å»¶è¿Ÿï¼Œç›¸æ¯” Without semantic-aware åˆ†é…ç®—æ³•ã€‚æ­¤å¤–ï¼Œå°çš„ semantic æŠ½å–å› å­åœ¨å¤§ä»»åŠ¡å¤§å°å’Œå·®annels æ¡ä»¶ä¸‹è¢«é¦–é€‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-class-weighted-supervised-contrastive-learning-long-tailed-bearing-fault-diagnosis-approach-using-quadratic-neural-network"><a href="#A-class-weighted-supervised-contrastive-learning-long-tailed-bearing-fault-diagnosis-approach-using-quadratic-neural-network" class="headerlink" title="A class-weighted supervised contrastive learning long-tailed bearing fault diagnosis approach using quadratic neural network"></a>A class-weighted supervised contrastive learning long-tailed bearing fault diagnosis approach using quadratic neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11717">http://arxiv.org/abs/2309.11717</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweien1120/CCQNet">https://github.com/yuweien1120/CCQNet</a></li>
<li>paper_authors: Wei-En Yu, Jinwei Sun, Shiping Zhang, Xiaoge Zhang, Jing-Xiao Liao<br>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æé«˜æ·±åº¦å­¦ä¹ æ–¹æ³•å¯¹æ•…éšœè¯Šæ–­ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢ä¸´é«˜åº¦ä¸å‡è¡¡æˆ–é•¿å°¾æ•°æ®æ—¶ã€‚methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç›‘ç£å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨ç±» weights å¯¹å†³ç­–å‡½æ•°è¿›è¡Œè°ƒæ•´ï¼Œä»è€Œæé«˜ç¥ç»ç½‘ç»œå¯¹æ•…éšœè¯Šæ–­çš„ç‰¹å¾æå–èƒ½åŠ›ã€‚results: å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ State-of-the-Art æ–¹æ³•ç›¸æ¯”ï¼ŒCCQNet åœ¨é¢ä¸´é«˜åº¦ä¸å‡è¡¡æˆ–é•¿å°¾æ•°æ®æ—¶è¡¨ç°æ˜æ˜¾æ›´å¥½ï¼Œå¯ä»¥æ›´å¥½åœ°è¯†åˆ«æ•…éšœã€‚<details>
<summary>Abstract</summary>
Deep learning has achieved remarkable success in bearing fault diagnosis. However, its performance oftentimes deteriorates when dealing with highly imbalanced or long-tailed data, while such cases are prevalent in industrial settings because fault is a rare event that occurs with an extremely low probability. Conventional data augmentation methods face fundamental limitations due to the scarcity of samples pertaining to the minority class. In this paper, we propose a supervised contrastive learning approach with a class-aware loss function to enhance the feature extraction capability of neural networks for fault diagnosis. The developed class-weighted contrastive learning quadratic network (CCQNet) consists of a quadratic convolutional residual network backbone, a contrastive learning branch utilizing a class-weighted contrastive loss, and a classifier branch employing logit-adjusted cross-entropy loss. By utilizing class-weighted contrastive loss and logit-adjusted cross-entropy loss, our approach encourages equidistant representation of class features, thereby inducing equal attention on all the classes. We further analyze the superior feature extraction ability of quadratic network by establishing the connection between quadratic neurons and autocorrelation in signal processing. Experimental results on public and proprietary datasets are used to validate the effectiveness of CCQNet, and computational results reveal that CCQNet outperforms SOTA methods in handling extremely imbalanced data substantially.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ åœ¨æ»¤æ³¢å™¨ç–¾ç—…è¯Šæ–­ä¸­å®ç°äº†å¾ˆå¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼Œå®ƒåœ¨é¢ä¸´é«˜åº¦ä¸å‡è¡¡æˆ–é•¿å°¾æ•°æ®æ—¶è¡¨ç°ä¸ä½³ï¼Œè¿™äº›æƒ…å†µåœ¨å·¥ä¸šåœºæ™¯ä¸­å´å¾ˆæ™®éï¼Œå› ä¸ºç–¾ç—…æ˜¯ä¸€ç§éå¸¸ç½•è§çš„äº‹ä»¶ï¼Œå‘ç”Ÿæ¦‚ç‡éå¸¸ä½ã€‚ä¼ ç»Ÿçš„æ•°æ®æ‰©å±•æ–¹æ³•å—åˆ°ç½•è§ç±»æ ·æœ¬çš„ç¼ºä¹çš„é™åˆ¶ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Supervised Contrastive Learningæ–¹æ³•ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œåœ¨ç–¾ç—…è¯Šæ–­ä¸­æé«˜ç‰¹å¾æå–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªquadratic convolutional residual networkåº•å±‚ã€ä¸€ä¸ªä½¿ç”¨ç±»Weighted Contrastive Lossçš„å¯¹æ¯”å­¦åˆ†æ”¯ã€ä»¥åŠä¸€ä¸ªä½¿ç”¨Logit-adjusted Cross-Entropy Lossçš„åˆ†ç±»åˆ†æ”¯ã€‚é€šè¿‡ä½¿ç”¨ç±»Weighted Contrastive Losså’ŒLogit-adjusted Cross-Entropy Lossï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¿ƒè¿›äº†ç±»åˆ«ç‰¹å¾ä¹‹é—´çš„ç­‰è·è€¦åˆï¼Œä»è€Œä½¿ç¥ç»ç½‘ç»œå¯¹æ‰€æœ‰ç±»å‹çš„ç‰¹å¾å…·æœ‰å¹³ç­‰çš„æ³¨æ„åŠ›ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†quadratic neuronçš„ç‰¹ç‚¹ï¼Œå¹¶å°†å…¶ä¸è‡ªç›¸å…³å‡½æ•°çš„åº”ç”¨ç›¸è¿æ¥ï¼Œä»¥è¯æ˜quadratic neuronåœ¨ä¿¡å·å¤„ç†ä¸­çš„ä¼˜åŠ¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCCQNetåœ¨é¢ä¸´é«˜åº¦ä¸å‡è¡¡æ•°æ®æ—¶è¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä¸SOTAæ–¹æ³•ç›¸æ¯”ï¼ŒCCQNetåœ¨æ‰§è¡Œæ»¤æ³¢å™¨ç–¾ç—…è¯Šæ–­æ–¹é¢å…·æœ‰æ˜¾è‘—çš„æ”¹è¿›ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/21/eess.SP_2023_09_21/" data-id="clopawo33019xag88572dew3t" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.SD_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T15:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.SD_2023_09_20/">cs.SD - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Joint-Minimum-Processing-Beamforming-and-Near-end-Listening-Enhancement"><a href="#Joint-Minimum-Processing-Beamforming-and-Near-end-Listening-Enhancement" class="headerlink" title="Joint Minimum Processing Beamforming and Near-end Listening Enhancement"></a>Joint Minimum Processing Beamforming and Near-end Listening Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11243">http://arxiv.org/abs/2309.11243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas J. Fuglsig, Jesper Jensen, Zheng-Hua Tan, Lars S. Bertelsen, Jens Christian Lindof, Jan Ã˜stergaard</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜å—åˆ°å™ªéŸ³ç¯å¢ƒå½±å“çš„è¯­éŸ³è¯†åˆ«åº¦ï¼Œå¹¶é™åˆ¶å™ªéŸ³å¤„ç†é‡ï¼Œä»¥ç¡®ä¿è¯­éŸ³è´¨é‡ä¸ä¼šè¿‡åº¦å—æŸã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†æœ€å°å¤„ç†æ¡†æ¶ï¼Œä»¥å‡å°‘å™ªéŸ³æˆ–å¢å¼ºå¬åŠ›ï¼Œå¹¶ä¿è¯è¯­éŸ³è´¨é‡ä¸ä¼šè¿‡åº¦å—æŸã€‚</li>
<li>results: ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œ JOINT æœ€å°å¤„ç†æ¡†æ¶å¯ä»¥æé«˜è¯­éŸ³è¯†åˆ«åº¦ï¼Œå¹¶é™åˆ¶å™ªéŸ³å¤„ç†é‡ï¼Œå¯¹äºæœ‰åˆ©çš„å™ªéŸ³æƒ…å†µä¸‹ï¼Œè¯­éŸ³è´¨é‡ä¸ä¼šè¿‡åº¦å—æŸã€‚<details>
<summary>Abstract</summary>
We consider speech enhancement for signals picked up in one noisy environment that must be rendered to a listener in another noisy environment. For both far-end noise reduction and near-end listening enhancement, it has been shown that excessive focus on noise suppression or intelligibility maximization may lead to excessive speech distortions and quality degradations in favorable noise conditions, where intelligibility is already at ceiling level. Recently [1,2] propose to remedy this with a minimum processing framework that either reduces noise or enhances listening a minimum amount given that a certain intelligibility criterion is still satisfied. Additionally, it has been shown that joint consideration of both environments improves speech enhancement performance. In this paper, we formulate a joint far- and near-end minimum processing framework, that improves intelligibility while limiting speech distortions in favorable noise conditions. We provide closed-form solutions to specific boundary scenarios and investigate performance for the general case using numerical optimization. We also show that concatenating existing minimum processing far- and near-end enhancement methods preserves the effects of the initial methods. Results show that the joint optimization can further improve performance compared to the concatenated approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è€ƒè™‘ speech å¢å¼ºå™¨åœ¨ä¸€ä¸ªå™ªéŸ³ç¯å¢ƒä¸­æ•æ‰çš„è®¯å·ï¼Œéœ€è¦åœ¨å¦ä¸€ä¸ªå™ªéŸ³ç¯å¢ƒä¸­å‘ˆç°ç»™å¬è€…ã€‚å¯¹äºè·ç¦»ç«¯å™ªéŸ³æŠ‘åˆ¶å’Œè¿‘ç«¯å¬åŠ›å¢å¼ºè€Œè¨€ï¼Œè¿‡åº¦å¼ºè°ƒå™ªéŸ³æŠ‘åˆ¶æˆ–æ™ºèƒ½åŒ–æœ€å¤§åŒ–å¯èƒ½ä¼šå¯¼è‡´å¯¹äºæœ‰åˆ©çš„å™ªéŸ³æƒ…å†µä¸‹çš„è¯è¯­å˜åŒ–å’Œè´¨é‡ä¸‹é™ã€‚æœ€è¿‘ï¼Œ[1,2] æå‡ºäº†ä¸€ä¸ªæœ€å°å¤„ç†æ¡†æ¶ï¼Œå¯ä»¥åœ¨ä¿æŒæ™ºèƒ½åŒ–æ°´å¹³ä¸‹æœ€å°åŒ–å™ªéŸ³æˆ–å¢å¼ºå¬åŠ›ã€‚æ­¤å¤–ï¼Œjointly è€ƒè™‘ä¸¤ä¸ªç¯å¢ƒå¯ä»¥æé«˜è¯è¯­å¢å¼ºè¡¨ç°ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªå…±åŒè·ç¦»å’Œè¿‘ç«¯æœ€å°å¤„ç†æ¡†æ¶ï¼Œå¯ä»¥åœ¨æœ‰åˆ©å™ªéŸ³æƒ…å†µä¸‹æé«˜æ™ºèƒ½åŒ–æ°´å¹³ï¼Œå¹¶é™åˆ¶è¯è¯­å˜åŒ–ã€‚æˆ‘ä»¬æä¾›äº†å…³é—­å¼è§£çš„å…·ä½“æƒ…å†µï¼Œå¹¶é€šè¿‡æ•°å€¼ä¼˜åŒ–è¿›è¡Œæ¢ç´¢ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº† concatenating ç°æœ‰çš„æœ€å°å¤„ç†è·ç¦»å’Œè¿‘ç«¯å¢å¼ºæ–¹æ³•å¯ä»¥ä¿æŒåˆå§‹æ–¹æ³•çš„æ•ˆæœã€‚ç»“æœæ˜¾ç¤ºï¼Œå…±åŒä¼˜åŒ–å¯ä»¥è¿›ä¸€æ­¥æé«˜è¡¨ç°ï¼Œæ¯” concatenated æ–¹æ³•æ›´å¥½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Deep-Complex-U-Net-with-Conformer-for-Audio-Visual-Speech-Enhancement"><a href="#Deep-Complex-U-Net-with-Conformer-for-Audio-Visual-Speech-Enhancement" class="headerlink" title="Deep Complex U-Net with Conformer for Audio-Visual Speech Enhancement"></a>Deep Complex U-Net with Conformer for Audio-Visual Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11059">http://arxiv.org/abs/2309.11059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shafique Ahmed, Chia-Wei Chen, Wenze Ren, Chin-Jou Li, Ernie Chu, Jun-Cheng Chen, Amir Hussain, Hsin-Min Wang, Yu Tsao, Jen-Cheng Hou</li>
<li>for: æé«˜è¯­éŸ³å¢å¼ºç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†è¯­éŸ³å’Œè§†é¢‘æ•°æ®ã€‚</li>
<li>methods: ä½¿ç”¨æ·±åº¦å¤æ‚U-Netæ¶æ„ï¼Œå¹¶å°†éŸ³é¢‘å’Œè§†é¢‘ä¿¡å·è¿›è¡Œå¤æ‚ç¼–ç å’Œè§£ç ï¼Œä»¥åŠä½¿ç”¨åµŒå…¥è‡ªæˆ‘æ³¨æ„æœºåˆ¶å’Œå·ç§¯æ“ä½œæ¥æ•æ‰è¯­éŸ³å’Œè§†é¢‘æ•°æ®çš„ç›¸äº’å…³ç³»ã€‚</li>
<li>results: åœ¨COG-MHEAR AVSE Challenge 2023 çš„åŸºå‡†æ¨¡å‹ä¸Šè¡¨ç°å‡ºä¼˜äº0.14çš„æå‡ï¼Œå¹¶åœ¨å°æ¹¾å®˜è¯è¯­éŸ³è§†é¢‘æ•°æ®é›†ï¼ˆTMSVï¼‰ä¸Šä¸çŠ¶æ€çº§æ¨¡å‹ç›¸å½“ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰æ¯”è¾ƒæ¨¡å‹ä¸­è¡¨ç°å‡ºæœ€ä½³resultã€‚<details>
<summary>Abstract</summary>
Recent studies have increasingly acknowledged the advantages of incorporating visual data into speech enhancement (SE) systems. In this paper, we introduce a novel audio-visual SE approach, termed DCUC-Net (deep complex U-Net with conformer network). The proposed DCUC-Net leverages complex domain features and a stack of conformer blocks. The encoder and decoder of DCUC-Net are designed using a complex U-Net-based framework. The audio and visual signals are processed using a complex encoder and a ResNet-18 model, respectively. These processed signals are then fused using the conformer blocks and transformed into enhanced speech waveforms via a complex decoder. The conformer blocks consist of a combination of self-attention mechanisms and convolutional operations, enabling DCUC-Net to effectively capture both global and local audio-visual dependencies. Our experimental results demonstrate the effectiveness of DCUC-Net, as it outperforms the baseline model from the COG-MHEAR AVSE Challenge 2023 by a notable margin of 0.14 in terms of PESQ. Additionally, the proposed DCUC-Net performs comparably to a state-of-the-art model and outperforms all other compared models on the Taiwan Mandarin speech with video (TMSV) dataset.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´ç ”ç©¶å‡è®¤å¯äº†å°†è§†è§‰æ•°æ® integrate åˆ°è¯­éŸ³æå‡ï¼ˆSEï¼‰ç³»ç»Ÿä¸­çš„ä¼˜åŠ¿ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„åµŒå…¥å¼éŸ³è§†é¢‘SEæ–¹æ³•ï¼Œç§°ä¸ºDCUC-Netï¼ˆæ·±åº¦å¤æ‚U-Net with å‡†ç¡®ç½‘ç»œï¼‰ã€‚æˆ‘ä»¬çš„DCUC-Netåˆ©ç”¨å¤æ‚Domainç‰¹å¾å’Œä¸€ä¸ªå †æ ˆçš„å‡†ç¡®å—ã€‚ç¼–ç å™¨å’Œè§£ç å™¨éƒ½é‡‡ç”¨äº†å¤æ‚U-Netçš„æ¡†æ¶ã€‚éŸ³é¢‘å’Œè§†é¢‘ä¿¡å·åˆ†åˆ«é€šè¿‡å¤æ‚ç¼–ç å™¨å’ŒResNet-18æ¨¡å‹å¤„ç†ï¼Œç„¶åé€šè¿‡å‡†ç¡®å—è¿›è¡Œæ‹¼æ¥ï¼Œå¹¶è½¬åŒ–ä¸ºæå‡åçš„è¯­éŸ³æ³¢å½¢ã€‚å‡†ç¡®å—åŒ…æ‹¬è‡ªæ³¨æ„æœºåˆ¶å’Œå·ç§¯æ“ä½œï¼Œä½¿DCUC-Netèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰å…¨å±€å’Œå±€éƒ¨éŸ³è§†é¢‘ç›¸äº’ä¾èµ–å…³ç³»ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDCUC-Netæ¯”åŸºçº¿æ¨¡å‹åœ¨COG-MHEAR AVSE Challenge 2023ä¸­è¡¨ç°å‡ºäº†æ˜æ˜¾çš„æå‡ï¼ˆ0.14ï¼‰ï¼Œå¹¶ä¸”ä¸å½“å‰çš„çŠ¶æ€è‰ºæ¨¡å‹ç›¸å½“ï¼Œåœ¨å°æ¹¾å®˜è¯è¯­éŸ³è§†é¢‘ï¼ˆTMSVï¼‰æ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†æœ€é«˜çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Multilingual-Pre-Trained-Models-for-Predicting-Multi-Label-Regression-Emotion-Share-from-Speech"><a href="#Ensembling-Multilingual-Pre-Trained-Models-for-Predicting-Multi-Label-Regression-Emotion-Share-from-Speech" class="headerlink" title="Ensembling Multilingual Pre-Trained Models for Predicting Multi-Label Regression Emotion Share from Speech"></a>Ensembling Multilingual Pre-Trained Models for Predicting Multi-Label Regression Emotion Share from Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11014">http://arxiv.org/abs/2309.11014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bagus Tris Atmaja, Akira Sasou</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯speech emotion recognitioné¢†åŸŸçš„ç ”ç©¶å’Œå®è·µåº”ç”¨ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ensemble learningæ–¹æ³•ï¼Œå°†å¤šç§é¢„è®­ç»ƒæ¨¡å‹çš„ç»“æœè¿›è¡Œèåˆï¼Œä»¥æé«˜speech emotion recognitionçš„æ€§èƒ½ã€‚</li>
<li>results: æµ‹è¯•é›†çš„spearman correlation coefficientä¸º0.537ï¼Œå¼€å‘é›†çš„spearman correlation coefficientä¸º0.524ï¼Œä¸¤è€…éƒ½é«˜äºä¹‹å‰åŸºäºå•è¯­è¨€æ•°æ®çš„èåˆæ–¹æ³•çš„ç ”ç©¶ç»“æœï¼ˆtesté›†çš„spearman correlation coefficientä¸º0.476ï¼Œå¼€å‘é›†çš„spearman correlation coefficientä¸º0.470ï¼‰ã€‚<details>
<summary>Abstract</summary>
Speech emotion recognition has evolved from research to practical applications. Previous studies of emotion recognition from speech have focused on developing models on certain datasets like IEMOCAP. The lack of data in the domain of emotion modeling emerges as a challenge to evaluate models in the other dataset, as well as to evaluate speech emotion recognition models that work in a multilingual setting. This paper proposes an ensemble learning to fuse results of pre-trained models for emotion share recognition from speech. The models were chosen to accommodate multilingual data from English and Spanish. The results show that ensemble learning can improve the performance of the baseline model with a single model and the previous best model from the late fusion. The performance is measured using the Spearman rank correlation coefficient since the task is a regression problem with ranking values. A Spearman rank correlation coefficient of 0.537 is reported for the test set, while for the development set, the score is 0.524. These scores are higher than the previous study of a fusion method from monolingual data, which achieved scores of 0.476 for the test and 0.470 for the development.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç ”ç©¶è€…ä»¬åœ¨æ¼”è®²æƒ…æ„Ÿè¯†åˆ«æ–¹é¢ä»ç ”ç©¶é˜¶æ®µé€æ¸æ¼”åŒ–åˆ°å®é™…åº”ç”¨ã€‚ previous studies on speech emotion recognition have focused on developing models on specific datasets such as IEMOCAP. However, the lack of data in the domain of emotion modeling poses a challenge to evaluate models on other datasets and to evaluate speech emotion recognition models that work in a multilingual setting. This paper proposes an ensemble learning approach to fuse the results of pre-trained models for speech emotion recognition. The models chosen accommodate multilingual data from English and Spanish. The results show that ensemble learning can improve the performance of the baseline model and the previous best model from late fusion. The performance is measured using the Spearman rank correlation coefficient, as the task is a regression problem with ranking values. The reported Spearman rank correlation coefficient for the test set is 0.537, while for the development set, the score is 0.524. These scores are higher than the previous study of a fusion method from monolingual data, which achieved scores of 0.476 for the test and 0.470 for the development.
</details></li>
</ul>
<hr>
<h2 id="Directional-Source-Separation-for-Robust-Speech-Recognition-on-Smart-Glasses"><a href="#Directional-Source-Separation-for-Robust-Speech-Recognition-on-Smart-Glasses" class="headerlink" title="Directional Source Separation for Robust Speech Recognition on Smart Glasses"></a>Directional Source Separation for Robust Speech Recognition on Smart Glasses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10993">http://arxiv.org/abs/2309.10993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tiantian Feng, Ju Lin, Yiteng Huang, Weipeng He, Kaustubh Kalgaonkar, Niko Moritz, Li Wan, Xin Lei, Ming Sun, Frank Seide</li>
<li>for: æé«˜æ—¥å¸¸äº¤æµä¸­è¯­éŸ³è¯†åˆ«ç‡å’Œè¯´è¯è€…æ£€æµ‹ç²¾åº¦</li>
<li>methods: ä½¿ç”¨é«˜çº§éŸ³é¢‘æ„ŸçŸ¥å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯å®ç°å®æ—¶è½¬å½•å’Œå­—å¹•æœåŠ¡ï¼Œå¹¶åˆ©ç”¨å¤šæéº¦å…‹é£çµæ´»æé«˜è¯­éŸ³è¯†åˆ«ç²¾åº¦</li>
<li>results: irectional source separation å¯ä»¥æé«˜è¯­éŸ³è¯†åˆ«ç‡å’Œè¯´è¯è€…æ£€æµ‹ç²¾åº¦ï¼Œä½†æ˜¯å¯¹å¯¹è¯ä¼™ä¼´æ— æ•ˆã€‚ joint training Directional source separation å’Œ ASR æ¨¡å‹å¯ä»¥ achieve the best overall ASR performance.<details>
<summary>Abstract</summary>
Modern smart glasses leverage advanced audio sensing and machine learning technologies to offer real-time transcribing and captioning services, considerably enriching human experiences in daily communications. However, such systems frequently encounter challenges related to environmental noises, resulting in degradation to speech recognition and speaker change detection. To improve voice quality, this work investigates directional source separation using the multi-microphone array. We first explore multiple beamformers to assist source separation modeling by strengthening the directional properties of speech signals. In addition to relying on predetermined beamformers, we investigate neural beamforming in multi-channel source separation, demonstrating that automatic learning directional characteristics effectively improves separation quality. We further compare the ASR performance leveraging separated outputs to noisy inputs. Our results show that directional source separation benefits ASR for the wearer but not for the conversation partner. Lastly, we perform the joint training of the directional source separation and ASR model, achieving the best overall ASR performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£æ™ºèƒ½çœ¼é•œåˆ©ç”¨å…ˆè¿›çš„éŸ³é¢‘æ„ŸçŸ¥å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œåœ¨å®æ—¶è½¬å½•å’Œå­—å¹•æœåŠ¡æ–¹é¢æä¾›äº†å¾ˆå¤§çš„ä¾¿åˆ©ï¼Œå¯¹æ—¥å¸¸äº¤æµä¸­çš„äººç±»ä½“éªŒå¸¦æ¥äº†å¾ˆå¤§çš„æ”¹å–„ã€‚ç„¶è€Œï¼Œè¿™äº›ç³»ç»Ÿç»å¸¸é‡åˆ°ç¯å¢ƒå™ªéŸ³çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´è¯­éŸ³è¯†åˆ«å’Œå‘è¨€è€…å˜æ¢çš„å¹²æ‰°ã€‚ä¸ºäº†æé«˜éŸ³è´¨ï¼Œæœ¬å·¥ä½œç ”ç©¶äº†å¤šé¢‘é“æºåˆ†ç¦»ã€‚æˆ‘ä»¬é¦–å…ˆæ¢è®¨äº†å¤šç§æ‰©å£°å™¨ï¼Œä»¥å¢å¼ºå¯¹è¯è¯­éŸ³çš„æ–¹å‘æ€§ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ investigateäº†åŸºäºè‡ªåŠ¨å­¦ä¹ çš„ç¥ç»æ‰©å£°å™¨åœ¨å¤šä¸ªé€šé“æºåˆ†ç¦»ä¸­çš„åº”ç”¨ï¼Œå¹¶è¯æ˜äº†è‡ªåŠ¨å­¦ä¹ æ–¹å‘ç‰¹æ€§å¯ä»¥æœ‰æ•ˆæé«˜åˆ†ç¦»è´¨é‡ã€‚æœ€åï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åˆ©ç”¨åˆ†ç¦»è¾“å‡ºè¿›è¡ŒASRçš„æ€§èƒ½å’Œç›´æ¥ä½¿ç”¨å™ªéŸ³è¾“å…¥è¿›è¡ŒASRçš„æ€§èƒ½ï¼Œç»“æœè¡¨æ˜irectional source separationå¯¹ASRæœ‰åˆ©ï¼Œä½†å¯¹å¯¹è¯ä¼™ä¼´æ— æ•ˆã€‚æœ€åï¼Œæˆ‘ä»¬å®ç°äº†irectional source separationå’ŒASRæ¨¡å‹çš„å…±åŒè®­ç»ƒï¼Œè¾¾åˆ°äº†æœ€ä½³çš„æ€»ASRæ€§èƒ½ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.SD_2023_09_20/" data-id="clopawnxe00wcag88hd0ef33m" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/eess.AS_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T14:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/eess.AS_2023_09_20/">eess.AS - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Neural-TTS-System-with-Parallel-Prosody-Transfer-from-Unseen-Speakers"><a href="#A-Neural-TTS-System-with-Parallel-Prosody-Transfer-from-Unseen-Speakers" class="headerlink" title="A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers"></a>A Neural TTS System with Parallel Prosody Transfer from Unseen Speakers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11487">http://arxiv.org/abs/2309.11487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Slava Shechtman, Raul Fernandez</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯å¼€å‘ä¸€ç§å¯ä»¥ä» parallel text recording ä¸­æå–é«˜çº§åˆ«çš„è¯­éŸ³ç‰¹å¾ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸åŒçš„ TTS  voz ä¸­ï¼Œä»¥å®ç°æ›´åŠ è‡ªç„¶å’Œè¡¨æƒ…å……æ²›çš„è¯­éŸ³è¯»å–ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„ TTS ç³»ç»Ÿï¼Œå¹¶å°†å…¶ equiped  avec prosody-control åŠŸèƒ½ï¼Œä»¥ä¾¿åœ¨æ¨ç†æ—¶é—´å¯¹è¯­éŸ³è¾“å‡ºè¿›è¡Œæ›´directçš„Shapeã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿå¯ä»¥å‡†ç¡®åœ°ä»æ–°çš„è¯´è¯è€…çš„ parallel text recording ä¸­æå–è¯­éŸ³ç‰¹å¾ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸åŒçš„ TTS voz ä¸­ï¼Œæ— è´¨é‡ä¸‹é™ï¼ŒåŒæ—¶ä¿æŒç›®æ ‡ TTS voz çš„identidadï¼Œæ ¹æ®ä¸€ç³»åˆ—ä¸»è§‚å¬åŠ›å®éªŒçš„è¯„ä¼°ã€‚<details>
<summary>Abstract</summary>
Modern neural TTS systems are capable of generating natural and expressive speech when provided with sufficient amounts of training data. Such systems can be equipped with prosody-control functionality, allowing for more direct shaping of the speech output at inference time. In some TTS applications, it may be desirable to have an option that guides the TTS system with an ad-hoc speech recording exemplar to impose an implicit fine-grained, user-preferred prosodic realization for certain input prompts. In this work we present a first-of-its-kind neural TTS system equipped with such functionality to transfer the prosody from a parallel text recording from an unseen speaker. We demonstrate that the proposed system can precisely transfer the speech prosody from novel speakers to various trained TTS voices with no quality degradation, while preserving the target TTS speakers' identity, as evaluated by a set of subjective listening experiments.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£ç¥ç»ç½‘ç»œText-to-Speechç³»ç»Ÿå¯ä»¥ä»å……è¶³çš„è®­ç»ƒæ•°æ®ä¸­ç”Ÿæˆè‡ªç„¶å’Œè¡¨è¾¾åŠ›å¼ºçš„è¯­éŸ³ã€‚è¿™äº›ç³»ç»Ÿå¯ä»¥æ­è½½å—æ§æ‹å±‚åŠŸèƒ½ï¼Œä»¥æ›´ç›´æ¥åœ¨æ¨ç†æ—¶è°ƒèŠ‚è¯­éŸ³è¾“å‡ºã€‚åœ¨æŸäº›TTSåº”ç”¨ç¨‹åºä¸­ï¼Œå¯èƒ½æ„¿æ„æœ‰ä¸€ä¸ªé€‰é¡¹ï¼Œä½¿TTSç³»ç»Ÿé€šè¿‡é¢å¤–çš„å³æ—¶ç¤ºä¾‹æ¥å¼ºåˆ¶æŸäº›è¾“å…¥æç¤ºçš„ç»†è…»ã€ç”¨æˆ·é¦–é€‰çš„è¯­éŸ³è¡¨ç°ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é¦–æ¬¡å®ç°çš„ç¥ç»ç½‘ç»œTTSç³»ç»Ÿï¼Œå¯ä»¥å°†æ¥è‡ªæœªè§çš„è¯´è¯äººçš„è¯­éŸ³ç‰¹å¾ç²¾ç¡®åœ°ä¼ é€’åˆ°ä¸åŒçš„è®­ç»ƒè¿‡çš„TTSvoiceä¸­ï¼Œè€Œæ— æŸè´¨é‡ï¼ŒåŒæ—¶ä¿æŒç›®æ ‡TTS speakerçš„èº«ä»½ï¼Œæ ¹æ®ä¸€ç»„ä¸»è§‚å¬åŠ›è¯•éªŒçš„è¯„ä»·ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/eess.AS_2023_09_20/" data-id="clopawnz9010uag88aca7hyi0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.CV_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T13:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.CV_2023_09_20/">cs.CV - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Understanding-Pose-and-Appearance-Disentanglement-in-3D-Human-Pose-Estimation"><a href="#Understanding-Pose-and-Appearance-Disentanglement-in-3D-Human-Pose-Estimation" class="headerlink" title="Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation"></a>Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11667">http://arxiv.org/abs/2309.11667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Krishna Kanth Nakka, Mathieu Salzmann</li>
<li>for: æœ¬ç ”ç©¶ç›®çš„æ˜¯åˆ†æå½“å‰é¢†åŸŸå†…æœ€æ–°çš„è‡ªç„¶è¯­è¨€æè¿°å­¦ä¹ æ–¹æ³•æ˜¯å¦èƒ½å¤ŸçœŸæ­£åˆ†ç¦» pose ä¿¡æ¯å’Œ appearance ä¿¡æ¯ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸‰ç§å½“å‰é¢†åŸŸå†…æœ€æ–°çš„è‡ªç„¶è¯­è¨€æè¿°å­¦ä¹ æ–¹æ³•è¿›è¡Œåˆ†æï¼Œå³ DenseCap, DensePose, å’Œ H3DNetã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè¿™ä¸‰ç§æ–¹æ³•ä¸­çš„ pose ä»£ç å«æœ‰æ˜¾è‘—çš„ appearance ä¿¡æ¯ï¼Œè€Œä¸”è¿™äº›æ–¹æ³•çš„åˆ†ç¦»æ•ˆæœå¹¶ä¸å¤Ÿå®Œå–„ã€‚<details>
<summary>Abstract</summary>
As 3D human pose estimation can now be achieved with very high accuracy in the supervised learning scenario, tackling the case where 3D pose annotations are not available has received increasing attention. In particular, several methods have proposed to learn image representations in a self-supervised fashion so as to disentangle the appearance information from the pose one. The methods then only need a small amount of supervised data to train a pose regressor using the pose-related latent vector as input, as it should be free of appearance information. In this paper, we carry out in-depth analysis to understand to what degree the state-of-the-art disentangled representation learning methods truly separate the appearance information from the pose one. First, we study disentanglement from the perspective of the self-supervised network, via diverse image synthesis experiments. Second, we investigate disentanglement with respect to the 3D pose regressor following an adversarial attack perspective. Specifically, we design an adversarial strategy focusing on generating natural appearance changes of the subject, and against which we could expect a disentangled network to be robust. Altogether, our analyses show that disentanglement in the three state-of-the-art disentangled representation learning frameworks if far from complete, and that their pose codes contain significant appearance information. We believe that our approach provides a valuable testbed to evaluate the degree of disentanglement of pose from appearance in self-supervised 3D human pose estimation.
</details>
<details>
<summary>æ‘˜è¦</summary>
As 3D human pose estimation å¯ä»¥åœ¨è¶…çº§vised learning scenario ä¸­å®ç°éå¸¸é«˜çš„å‡†ç¡®ç‡ï¼Œå› æ­¤å¤„ç†æ²¡æœ‰3D pose annotationsçš„æƒ…å†µ receiving increasing attention. ç‰¹åˆ«æ˜¯ï¼Œseveral methods have proposed to learn image representations in a self-supervised fashion so as to disentangle the appearance information from the pose one. The methods then only need a small amount of supervised data to train a pose regressor using the pose-related latent vector as input, as it should be free of appearance information.In this paper, we carry out in-depth analysis to understand to what degree the state-of-the-art disentangled representation learning methods truly separate the appearance information from the pose one. First, we study disentanglement from the perspective of the self-supervised network, via diverse image synthesis experiments. Second, we investigate disentanglement with respect to the 3D pose regressor following an adversarial attack perspective. Specifically, we design an adversarial strategy focusing on generating natural appearance changes of the subject, and against which we could expect a disentangled network to be robust.Altogether, our analyses show that disentanglement in the three state-of-the-art disentangled representation learning frameworks is far from complete, and that their pose codes contain significant appearance information. We believe that our approach provides a valuable testbed to evaluate the degree of disentanglement of pose from appearance in self-supervised 3D human pose estimation.
</details></li>
</ul>
<hr>
<h2 id="Neural-Image-Compression-Using-Masked-Sparse-Visual-Representation"><a href="#Neural-Image-Compression-Using-Masked-Sparse-Visual-Representation" class="headerlink" title="Neural Image Compression Using Masked Sparse Visual Representation"></a>Neural Image Compression Using Masked Sparse Visual Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11661">http://arxiv.org/abs/2309.11661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Wei Wang, Yue Chen</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦ç ”ç©¶äº†åŸºäºç¨€ç–è§†è§‰è¡¨ç¤ºï¼ˆSVRï¼‰çš„ç¥ç»å›¾åƒå‹ç¼©ï¼Œç›®çš„æ˜¯æé«˜å‹ç¼©ç‡å’Œå‹ç¼©åå›¾åƒè´¨é‡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºSVRçš„å‹ç¼©æ–¹æ³•ï¼Œå…¶ä¸­å›¾åƒè¢«åµŒå…¥åˆ°ä¸€ä¸ªç¦»æ•£çš„ latent space ä¸­ï¼Œå¹¶ä½¿ç”¨äº†å­¦ä¹ çš„è§†è§‰codebookæ¥è¡¨ç¤ºå›¾åƒã€‚åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´å…±äº« codebookï¼Œç¼–ç å™¨å°†å›¾åƒè½¬æ¢ä¸º integer ä»£ç word indicesï¼Œå¹¶å°†è¿™äº›æŒ‡æ ‡ä¼ è¾“ç»™è§£ç å™¨è¿›è¡Œé‡å»ºã€‚è¿™ç§æ–¹æ³•æå‡ºäº†ä¸€ç§åä¸º Masked Adaptive Codebook çš„å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥åœ¨bitrateå’Œé‡å»ºè´¨é‡ä¹‹é—´è¿›è¡Œè´Ÿæƒè¡¥å¿ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒM-AdaCode æ–¹æ³•å¯ä»¥åœ¨ JPEG-AI æ ‡å‡†æ•°æ®é›†ä¸Šå®ç°æ›´é«˜çš„å‹ç¼©ç‡å’Œæ›´é«˜çš„é‡å»ºè´¨é‡ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„ä¼ è¾“æ¯”ç‰¹ç‡ä¸‹è¿›è¡Œè´Ÿæƒè¡¥å¿ã€‚<details>
<summary>Abstract</summary>
We study neural image compression based on the Sparse Visual Representation (SVR), where images are embedded into a discrete latent space spanned by learned visual codebooks. By sharing codebooks with the decoder, the encoder transfers integer codeword indices that are efficient and cross-platform robust, and the decoder retrieves the embedded latent feature using the indices for reconstruction. Previous SVR-based compression lacks effective mechanism for rate-distortion tradeoffs, where one can only pursue either high reconstruction quality or low transmission bitrate. We propose a Masked Adaptive Codebook learning (M-AdaCode) method that applies masks to the latent feature subspace to balance bitrate and reconstruction quality. A set of semantic-class-dependent basis codebooks are learned, which are weighted combined to generate a rich latent feature for high-quality reconstruction. The combining weights are adaptively derived from each input image, providing fidelity information with additional transmission costs. By masking out unimportant weights in the encoder and recovering them in the decoder, we can trade off reconstruction quality for transmission bits, and the masking rate controls the balance between bitrate and distortion. Experiments over the standard JPEG-AI dataset demonstrate the effectiveness of our M-AdaCode approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶åŸºäºç¨€ç–è§†è§‰è¡¨ç¤ºï¼ˆSVRï¼‰çš„ç¥ç»ç½‘ç»œå›¾åƒå‹ç¼©ï¼Œå›¾åƒè¢«åµŒå…¥åˆ°å­¦ä¹ çš„è§†è§‰ç åº“ä¸­çš„ç¦»æ•£ç‰¹å¾ç©ºé—´ä¸­ã€‚é€šè¿‡åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´å…±äº«ç åº“ï¼Œç¼–ç å™¨å°†è½¬åŒ–ä¸ºæ•´æ•°ç¼–ç å­—ç¬¦ä¸²ï¼Œè¿™äº›ç¼–ç å­—ç¬¦ä¸²æ˜¯é«˜æ•ˆç©¿æ¢­å¹³å°å¼ºçš„å’Œå¯é çš„ï¼Œè€Œè§£ç å™¨é€šè¿‡è¿™äº›ç¼–ç å­—ç¬¦ä¸²æ¥é‡å»ºå›¾åƒã€‚ prÃ©cÃ©dente çš„ SVR åŸºäºå‹ç¼©ç¼ºä¹æœ‰æ•ˆçš„Rate-Distortion è´¨é‡è¡¡é‡æœºåˆ¶ï¼Œåªèƒ½è¿½æ±‚é«˜é‡å»ºè´¨é‡æˆ–ä½ä¼ è¾“æ¯”ç‰¹ç‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰æ©ç ï¼ˆMaskï¼‰çš„è‡ªé€‚åº”ç åº“å­¦ä¹ ï¼ˆM-AdaCodeï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ©ç åœ¨å¹²æ‰°ç‰¹å¾ç©ºé—´ä¸­è¿›è¡Œæƒé‡è°ƒæ•´ï¼Œä»¥å®ç°Rate-Distortion è´¨é‡è¡¡é‡æœºåˆ¶ã€‚æˆ‘ä»¬å­¦ä¹ äº†åŸºäºè¾“å…¥å›¾åƒçš„semanticç±»åˆ«çš„åŸºç¡€ç åº“ï¼Œå¹¶å°†è¿™äº›åŸºç¡€ç åº“Weightedly ç»„åˆï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡é‡å»ºçš„ç»¼åˆç‰¹å¾ã€‚ç¼–ç å™¨ä¸­çš„æ©ç å°†æ©è”½ä¸é‡è¦çš„æƒé‡ï¼Œè€Œè§£ç å™¨ä¸­çš„æ©ç å°†é‡æ–°è¿˜åŸè¿™äº›æ©ç ï¼Œä»¥å®ç°Rate-Distortion è´¨é‡è¡¡é‡æœºåˆ¶ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ M-AdaCode æ–¹æ³•åœ¨æ ‡å‡† JPEG-AI æ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ã€‚
</details></li>
</ul>
<hr>
<h2 id="GenLayNeRF-Generalizable-Layered-Representations-with-3D-Model-Alignment-for-Multi-Human-View-Synthesis"><a href="#GenLayNeRF-Generalizable-Layered-Representations-with-3D-Model-Alignment-for-Multi-Human-View-Synthesis" class="headerlink" title="GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Multi-Human View Synthesis"></a>GenLayNeRF: Generalizable Layered Representations with 3D Model Alignment for Multi-Human View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11627">http://arxiv.org/abs/2309.11627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Abdelkareem, Shady Shehata, Fakhri Karray</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†è§£å†³å¤šäººSceneä¸­çš„å¤æ‚äººç‰© occlusion é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªé€šç”¨çš„ layered representation æ¥æ•æ‰å¤šäººScene çš„å†…å®¹ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º GenLayNeRF çš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªå¤šå±‚æ¶æ„æ¥åˆ†è§£Sceneï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªæ–°çš„å¯¹ç­–æœºåˆ¶æ¥è¿›è¡Œé€‚åº”å™¨è°ƒæ•´å’Œå¤šè§‚å¯Ÿç‰¹å¾èåˆï¼Œä»¥ç¡®ä¿ pixel-level çš„ä½“æ¨¡å‹ä¸è¾“å…¥è§†é‡çš„åŒæ­¥ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨ NVS ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸é€šç”¨ NeRF æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒèƒ½å¤Ÿåœ¨å‡ ä¹æ²¡æœ‰é¢„æœŸä¼˜åŒ–çš„æƒ…å†µä¸‹æä¾›é«˜å“è´¨çš„å†…å®¹ç”Ÿæˆã€‚è€Œä¸å±‚åŒ– per-scene NeRF æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒèƒ½å¤Ÿåœ¨å‡ ä¹æ²¡æœ‰æµ‹è¯•æ—¶é—´ä¼˜åŒ–çš„æƒ…å†µä¸‹æä¾›ç›¸ä¼¼æˆ–æ›´å¥½çš„è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
Novel view synthesis (NVS) of multi-human scenes imposes challenges due to the complex inter-human occlusions. Layered representations handle the complexities by dividing the scene into multi-layered radiance fields, however, they are mainly constrained to per-scene optimization making them inefficient. Generalizable human view synthesis methods combine the pre-fitted 3D human meshes with image features to reach generalization, yet they are mainly designed to operate on single-human scenes. Another drawback is the reliance on multi-step optimization techniques for parametric pre-fitting of the 3D body models that suffer from misalignment with the images in sparse view settings causing hallucinations in synthesized views. In this work, we propose, GenLayNeRF, a generalizable layered scene representation for free-viewpoint rendering of multiple human subjects which requires no per-scene optimization and very sparse views as input. We divide the scene into multi-human layers anchored by the 3D body meshes. We then ensure pixel-level alignment of the body models with the input views through a novel end-to-end trainable module that carries out iterative parametric correction coupled with multi-view feature fusion to produce aligned 3D models. For NVS, we extract point-wise image-aligned and human-anchored features which are correlated and fused using self-attention and cross-attention modules. We augment low-level RGB values into the features with an attention-based RGB fusion module. To evaluate our approach, we construct two multi-human view synthesis datasets; DeepMultiSyn and ZJU-MultiHuman. The results indicate that our proposed approach outperforms generalizable and non-human per-scene NeRF methods while performing at par with layered per-scene methods without test time optimization.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Š Novel View Synthesis of Multi-Human Scenes with Generalizable Layered Scene Representationã€‹ Multi-human scene novel view synthesis ï¼ˆNVSï¼‰é¢ä¸´è®¸å¤šæŒ‘æˆ˜ï¼Œä¸»è¦æ˜¯å› ä¸ºäººä½“ occlusion å¤æ‚ã€‚å±‚æ¬¡è¡¨ç¤ºå¤„ç†è¿™äº›å¤æ‚æ€§ï¼Œé€šè¿‡å°†åœºæ™¯åˆ†è§£ä¸ºå¤šå±‚Radiance Fieldsï¼Œä½†æ˜¯å®ƒä»¬ä¸»è¦æ˜¯åŸºäºåœºæ™¯ä¼˜åŒ–ï¼Œå› æ­¤æ•ˆç‡ä½ã€‚é€šç”¨äººä½“è§†å›¾åˆæˆæ–¹æ³•å°†é¢„å…ˆé€‚åº”çš„3Däººä½“æ¨¡å‹ä¸å›¾åƒç‰¹å¾ç»“åˆèµ·æ¥ï¼Œä½†æ˜¯å®ƒä»¬ä¸»è¦æ˜¯é’ˆå¯¹å•ä¸ªäººä½“åœºæ™¯è®¾è®¡ã€‚å¦ä¸€ä¸ªç¼ºç‚¹æ˜¯åœ¨ç¼ºè§†è®¾å®šä¸‹ï¼Œä½¿ç”¨å¤šæ­¥ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œå‚æ•°é¢„å®šçš„3Däººä½“æ¨¡å‹ä¼šå¯¼è‡´æŠ•å½±å¹»è§‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†GenLayNeRFï¼Œä¸€ç§é€šç”¨å±‚æ¬¡åœºæ™¯è¡¨ç¤ºï¼Œç”¨äºæ— éœ€åœºæ™¯ä¼˜åŒ–å’Œéå¸¸ç½•è§çš„è§†å›¾è¾“å…¥è¿›è¡Œè‡ªç”±è§†è§’æ¸²æŸ“å¤šä¸ªäººä½“ä¸»é¢˜ã€‚æˆ‘ä»¬å°†åœºæ™¯åˆ†è§£æˆå¤šä¸ªäººä½“å±‚ï¼Œç”±3Däººä½“æ¨¡å‹anchorã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ç§æ–°çš„ç»ˆç«¯å¯è°ƒæ¨¡å—ï¼Œé€šè¿‡iterative parametric correctionå’Œå¤šè§†å›¾ç‰¹å¾èåˆæ¥ç¡®ä¿åƒç´ çº§åŒ¹é…3Dæ¨¡å‹ä¸è¾“å…¥è§†å›¾ã€‚ Ğ´Ğ»Ñ NVSï¼Œæˆ‘ä»¬æå–äººä½“åµŒå…¥å’Œå›¾åƒå¯¹é½çš„ç‚¹çº§ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›æ¨¡å—è¿›è¡Œç›¸å…³å’Œèåˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†ä½çº§RGBå€¼åŠ å…¥ç‰¹å¾ä¸­ï¼Œä½¿ç”¨æ³¨æ„åŠ›åŸºäºRGBèåˆæ¨¡å—ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªå¤šä¸ªäººä½“è§†å›¾åˆæˆæ•°æ®é›†ï¼šDeepMultiSynå’ŒZJU-MultiHumanã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æå‡ºæ–¹æ³•åœ¨æ¯”è¾ƒé€šç”¨å’Œéäººä½“åœºæ™¯NeRFæ–¹æ³•çš„åŒæ—¶ï¼Œèƒ½å¤Ÿè¾¾åˆ°ç›¸åŒçš„æ€§èƒ½æ°´å¹³ï¼Œè€Œä¸éœ€è¦æµ‹è¯•æ—¶ä¼˜åŒ–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Sentence-Attention-Blocks-for-Answer-Grounding"><a href="#Sentence-Attention-Blocks-for-Answer-Grounding" class="headerlink" title="Sentence Attention Blocks for Answer Grounding"></a>Sentence Attention Blocks for Answer Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11593">http://arxiv.org/abs/2309.11593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedalireza Khoshsirat, Chandra Kambhamettu</li>
<li>for: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å»ºç­‘å—ï¼Œå³ Sentence Attention Blockï¼Œä»¥è§£å†³æ–‡æœ¬æè¿°ç­”æ¡ˆé—®é¢˜ä¸­çš„é—®é¢˜ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§å·²çŸ¥çš„æ³¨æ„åŠ›æ–¹æ³•ï¼Œå¹¶é€šè¿‡å°æ”¹è¿›ï¼Œæé«˜äº†ç»“æœã€‚</li>
<li>results: æœ¬æ–‡åœ¨ TextVQA-Xã€VQSã€VQA-X å’Œ VizWiz-VQA-Grounding æ•°æ®é›†ä¸Šè¾¾åˆ°äº†çŠ¶æ€çš„æœ€ä½³å‡†ç¡®ç‡ã€‚<details>
<summary>Abstract</summary>
Answer grounding is the task of locating relevant visual evidence for the Visual Question Answering task. While a wide variety of attention methods have been introduced for this task, they suffer from the following three problems: designs that do not allow the usage of pre-trained networks and do not benefit from large data pre-training, custom designs that are not based on well-grounded previous designs, therefore limiting the learning power of the network, or complicated designs that make it challenging to re-implement or improve them. In this paper, we propose a novel architectural block, which we term Sentence Attention Block, to solve these problems. The proposed block re-calibrates channel-wise image feature-maps by explicitly modeling inter-dependencies between the image feature-maps and sentence embedding. We visually demonstrate how this block filters out irrelevant feature-maps channels based on sentence embedding. We start our design with a well-known attention method, and by making minor modifications, we improve the results to achieve state-of-the-art accuracy. The flexibility of our method makes it easy to use different pre-trained backbone networks, and its simplicity makes it easy to understand and be re-implemented. We demonstrate the effectiveness of our method on the TextVQA-X, VQS, VQA-X, and VizWiz-VQA-Grounding datasets. We perform multiple ablation studies to show the effectiveness of our design choices.
</details>
<details>
<summary>æ‘˜è¦</summary>
Answer grounding ä»»åŠ¡æ˜¯ä¸ºVisual Question Answering ä»»åŠ¡ä¸­æ‰¾åˆ°ç›¸å…³çš„è§†è§‰è¯æ®ã€‚è™½ç„¶è¿‡å»çš„å¾ˆå¤šæ³¨æ„åŠ›æ–¹æ³•è¢«æå‡ºï¼Œä½†å®ƒä»¬å—åˆ°ä»¥ä¸‹ä¸‰ä¸ªé—®é¢˜çš„é™åˆ¶ï¼šä¸å…è®¸ä½¿ç”¨é¢„è®­ç»ƒç½‘ç»œï¼Œä¸èƒ½å……åˆ†åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®ï¼Œæˆ–è€…è‡ªå®šä¹‰çš„è®¾è®¡ä¸åŸºäºå·²æœ‰çš„å›ºå®šè®¾è®¡ï¼Œå› æ­¤é™åˆ¶äº†ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å»ºç­‘å—ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå¥å­æ³¨æ„åŠ›å—ï¼ˆSentence Attention Blockï¼‰ï¼Œä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚æˆ‘ä»¬çš„å—é€šè¿‡æ˜¾å¼åœ°æ¨¡å‹å›¾åƒç‰¹å¾åœ°å›¾å’Œå¥å­åµŒå…¥çš„é—´æ¥å…³ç³»æ¥é‡æ–°å‡†ç¡®åŒ–é€šé“ wise å›¾åƒç‰¹å¾åœ°å›¾ã€‚æˆ‘ä»¬å¯è§†ç¤ºäº†è¯¥å—å¦‚ä½•åŸºäºå¥å­åµŒå…¥æ¥è¿‡æ»¤ä¸ç›¸å…³çš„é€šé“ wise å›¾åƒç‰¹å¾åœ°å›¾ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªå·²çŸ¥çš„æ³¨æ„åŠ›æ–¹æ³•å¼€å§‹ï¼Œé€šè¿‡å°é‡ä¿®æ”¹ï¼Œæˆ‘ä»¬æé«˜äº†ç»“æœï¼Œè¾¾åˆ°äº†çŠ¶æ€ä¹‹Art accuracyã€‚æˆ‘ä»¬çš„æ–¹æ³•çš„çµæ´»æ€§ä½¿å¾—å¯ä»¥ä½¿ç”¨ä¸åŒçš„é¢„è®­ç»ƒåå°ç½‘ç»œï¼Œå…¶ç®€æ´æ€§ä½¿å¾—å®¹æ˜“ç†è§£å’Œé‡æ–°å®ç°ã€‚æˆ‘ä»¬åœ¨TextVQA-Xã€VQSã€VQA-X å’Œ VizWiz-VQA-Grounding æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¤šä¸ªç¼ºçœç ”ç©¶ï¼Œä»¥è¯æ˜æˆ‘ä»¬çš„è®¾è®¡é€‰æ‹©çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Continuous-Levels-of-Detail-for-Light-Field-Networks"><a href="#Continuous-Levels-of-Detail-for-Light-Field-Networks" class="headerlink" title="Continuous Levels of Detail for Light Field Networks"></a>Continuous Levels of Detail for Light Field Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11591">http://arxiv.org/abs/2309.11591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AugmentariumLab/continuous-lfn">https://github.com/AugmentariumLab/continuous-lfn</a></li>
<li>paper_authors: David Li, Brandon Y. Feng, Amitabh Varshney</li>
<li>for: æé«˜renderingæ•ˆæœå’Œèµ„æºåˆ©ç”¨ç‡ï¼Œé€šè¿‡ä½¿ç”¨è¿ç»­å¤šçº§è¯¦ç»†åº¦ï¼ˆLODsï¼‰ç”Ÿæˆç¥ç»è¡¨ç¤ºã€‚</li>
<li>methods: ä½¿ç”¨æƒé‡æ¢¯åº¦æ»¤æ³¢å’Œé‡è¦æ€§ sampling æŠ€æœ¯ï¼Œå®ç°ç²¾ç»†æ§åˆ¶è¯¦ç»†åº¦çš„è°ƒæ•´ï¼Œä»¥é€‚åº”ä¸åŒçš„ rendering æ¡ä»¶ã€‚</li>
<li>results: æå‡ºä¸€ç§åŸºäºè¿ç»­ LODs çš„ç¥ç»ç½‘ç»œè¡¨ç¤ºæ–¹æ³•ï¼Œå¯ä»¥å®ç°è¿›åº¦å¼æµå¼ç¥ç»ç½‘ç»œè¡¨ç¤ºï¼Œé™ä½æ¸²æŸ“å»¶è¿Ÿå’Œèµ„æºä½¿ç”¨ç‡ã€‚<details>
<summary>Abstract</summary>
Recently, several approaches have emerged for generating neural representations with multiple levels of detail (LODs). LODs can improve the rendering by using lower resolutions and smaller model sizes when appropriate. However, existing methods generally focus on a few discrete LODs which suffer from aliasing and flicker artifacts as details are changed and limit their granularity for adapting to resource limitations. In this paper, we propose a method to encode light field networks with continuous LODs, allowing for finely tuned adaptations to rendering conditions. Our training procedure uses summed-area table filtering allowing efficient and continuous filtering at various LODs. Furthermore, we use saliency-based importance sampling which enables our light field networks to distribute their capacity, particularly limited at lower LODs, towards representing the details viewers are most likely to focus on. Incorporating continuous LODs into neural representations enables progressive streaming of neural representations, decreasing the latency and resource utilization for rendering.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘äº›å¹´ï¼Œå¤šçº§ç»†èŠ‚ï¼ˆLODï¼‰ç”Ÿæˆç¥ç»è¡¨ç¤ºæ–¹æ³•å¾—åˆ°äº†ä¸€äº›çªç ´ã€‚LODå¯ä»¥é€šè¿‡ä½¿ç”¨è¾ƒä½çš„åˆ†è¾¨ç‡å’Œå°å‹æ¨¡å‹æ¥æé«˜æ¸²æŸ“ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•é€šå¸¸åªå…³æ³¨ä¸€äº›ç²¾ç¡®çš„LODï¼Œè¿™ä¼šå¯¼è‡´æŠ–æŠ–å’Œé—ªçƒartifactsï¼Œé™åˆ¶å…¶ç»†èŠ‚é€‚åº”èµ„æºçš„å˜åŒ–ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨è¿ç»­LODç¼–ç å…‰åœºç½‘ç»œæ–¹æ³•ï¼Œå…è®¸ä¸ºæ¸²æŸ“æ¡ä»¶è¿›è¡Œç»†åŒ–é€‚åº”ã€‚æˆ‘ä»¬çš„è®­ç»ƒè¿‡ç¨‹ä½¿ç”¨æ€»é¢ç§¯è¡¨ filteringï¼Œä»¥å®ç°é«˜æ•ˆçš„è¿ç»­filteringåœ¨ä¸åŒLODsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨å…³æ³¨åº¦åŸºäºçš„é‡è¦æ€§é‡‡æ ·ï¼Œä½¿æˆ‘ä»¬çš„å…‰åœºç½‘ç»œèƒ½å¤Ÿæ›´å¥½åœ°åˆ†é…å…¶å®¹é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾ƒä½çš„LODsã€‚å°†è¿ç»­LODintegrated intoç¥ç»è¡¨ç¤ºå…è®¸è¿›è¡Œè¿›ç¨‹å¼æµåŠ¨ç¥ç»è¡¨ç¤ºï¼Œé™ä½æ¸²æŸ“çš„å»¶è¿Ÿå’Œèµ„æºåˆ©ç”¨ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Kernel-Temporal-Segmentation-as-an-Adaptive-Tokenizer-for-Long-form-Video-Understanding"><a href="#Revisiting-Kernel-Temporal-Segmentation-as-an-Adaptive-Tokenizer-for-Long-form-Video-Understanding" class="headerlink" title="Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding"></a>Revisiting Kernel Temporal Segmentation as an Adaptive Tokenizer for Long-form Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11569">http://arxiv.org/abs/2309.11569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Afham, Satya Narayan Shukla, Omid Poursaeed, Pengchuan Zhang, Ashish Shah, Sernam Lim</li>
<li>for: æé«˜é•¿è§†é¢‘ç†è§£çš„æ•ˆæœï¼Œé€‚åº”å®é™…è§†é¢‘ä¸­çš„ semantic consistencyã€‚</li>
<li>methods: åŸºäº Kernel Temporal Segmentation (KTS) çš„é€‚åº” sampling å’Œ tokenization æ–¹æ³•ï¼Œä¸éœ€è¦ä»»åŠ¡ç‰¹å®šçš„supervisionæˆ–å›ºå®šé•¿åº¦çš„clipã€‚</li>
<li>results: åœ¨è§†é¢‘åˆ†ç±»å’Œ temporal action localization ä»»åŠ¡ä¸Šå®ç°äº†consistent improvementï¼Œå¹¶è¾¾åˆ°äº†é•¿è§†é¢‘æ¨¡å‹çš„state-of-the-artè¡¨ç°ã€‚<details>
<summary>Abstract</summary>
While most modern video understanding models operate on short-range clips, real-world videos are often several minutes long with semantically consistent segments of variable length. A common approach to process long videos is applying a short-form video model over uniformly sampled clips of fixed temporal length and aggregating the outputs. This approach neglects the underlying nature of long videos since fixed-length clips are often redundant or uninformative. In this paper, we aim to provide a generic and adaptive sampling approach for long-form videos in lieu of the de facto uniform sampling. Viewing videos as semantically consistent segments, we formulate a task-agnostic, unsupervised, and scalable approach based on Kernel Temporal Segmentation (KTS) for sampling and tokenizing long videos. We evaluate our method on long-form video understanding tasks such as video classification and temporal action localization, showing consistent gains over existing approaches and achieving state-of-the-art performance on long-form video modeling.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“ä»Šå¤§å¤šæ•°è§†é¢‘ç†è§£æ¨¡å‹éƒ½æ˜¯åœ¨çŸ­èŒƒå›´clipä¸Šè¿è¡Œï¼Œä½†å®é™…ä¸–ç•Œä¸­çš„è§†é¢‘å¾€å¾€æ˜¯æ•°åˆ†é’Ÿé•¿ï¼Œå¹¶ä¸”æœ‰semantically consistentçš„åˆ†å‰²æ®µã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•å¤„ç†é•¿è§†é¢‘æ˜¯ï¼Œå°†çŸ­è§†é¢‘æ¨¡å‹åº”ç”¨äºå›ºå®š temporal lengthçš„clipä¸Šï¼Œå¹¶å°†è¾“å‡ºé›†æˆã€‚è¿™ç§æ–¹æ³•å¿½ç•¥äº†é•¿è§†é¢‘çš„æœ¬è´¨ï¼Œå› ä¸ºå›ºå®šé•¿clipç»å¸¸æ˜¯ redundancy or uninformativeã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç›®çš„æ˜¯æä¾›ä¸€ç§é€šç”¨å’Œé€‚åº”æ€§çš„æŠ½æ ·æ–¹æ³•ï¼Œä»¥ä»£æ›¿ç°æœ‰çš„å›ºå®šæŠ½æ ·ã€‚è§†é¢‘è¢«è§†ä¸ºsemantically consistentçš„åˆ†å‰²æ®µï¼Œæˆ‘ä»¬åŸºäºKernel Temporal Segmentationï¼ˆKTSï¼‰æå‡ºäº†ä¸€ç§ä»»åŠ¡æ— å…³ã€æ— ç›‘ç£å’Œå¯æ‰©å±•çš„æ–¹æ³•ï¼Œç”¨äºæŠ½å–å’Œ tokenize é•¿è§†é¢‘ã€‚æˆ‘ä»¬å¯¹é•¿è§†é¢‘ç†è§£ä»»åŠ¡ï¼Œå¦‚è§†é¢‘åˆ†ç±»å’Œ temporal action localizationï¼Œè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºäº†ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”çš„consistentæå‡ï¼Œå¹¶å®ç°äº†é•¿è§†é¢‘æ¨¡å‹çš„å·é™…æ€§è¡¨ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Large-scale-Dataset-for-Audio-Language-Representation-Learning"><a href="#A-Large-scale-Dataset-for-Audio-Language-Representation-Learning" class="headerlink" title="A Large-scale Dataset for Audio-Language Representation Learning"></a>A Large-scale Dataset for Audio-Language Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11500">http://arxiv.org/abs/2309.11500</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ä¸ªæ–°çš„è‡ªåŠ¨éŸ³é¢‘æè¿°ç”Ÿæˆç®¡é“ï¼Œä»¥åŠæ„å»ºä¸€ä¸ªå¤§è§„æ¨¡ã€é«˜è´¨é‡çš„éŸ³é¢‘è¯­è¨€æ•°æ®é›†ï¼ˆAuto-ACDï¼‰ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç³»åˆ—å…¬å…±å·¥å…·æˆ– APIï¼Œè‡ªåŠ¨ç”Ÿæˆäº†å¤§é‡çš„éŸ³é¢‘æè¿°æ–‡æœ¬ã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡åœ¨ä¸åŒä¸‹æ¸¸ä»»åŠ¡ä¸Šè®­ç»ƒ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†å¯¹ Audio-Language Retrievalã€Audio Captioning å’Œç¯å¢ƒåˆ†ç±»ç­‰ä»»åŠ¡çš„æ€§èƒ½æ”¹è¿›ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„æµ‹è¯•é›†ï¼Œå¹¶ä¸ºéŸ³é¢‘è¯­è¨€ä»»åŠ¡æä¾›äº†ä¸€ä¸ª referential å¹³å°ã€‚<details>
<summary>Abstract</summary>
The AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, in the audio representation learning community, the present audio-language datasets suffer from limitations such as insufficient volume, simplistic content, and arduous collection procedures. To tackle these challenges, we present an innovative and automatic audio caption generation pipeline based on a series of public tools or APIs, and construct a large-scale, high-quality, audio-language dataset, named as Auto-ACD, comprising over 1.9M audio-text pairs. To demonstrate the effectiveness of the proposed dataset, we train popular models on our dataset and show performance improvement on various downstream tasks, namely, audio-language retrieval, audio captioning, environment classification. In addition, we establish a novel test set and provide a benchmark for audio-text tasks. The proposed dataset will be released at https://auto-acd.github.io/.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šäººå·¥æ™ºèƒ½ç¤¾åŒºåœ¨å¼€å‘å¼ºå¤§åŸºç¡€æ¨¡å‹æ–¹é¢å·²ç»åšå‡ºäº† significiant è¿›æ­¥ï¼Œè¿™äº›åŸºç¡€æ¨¡å‹å¾—ç›Šäºå¤§è§„æ¨¡å¤šmodalæ•°æ®é©±åŠ¨ã€‚ç„¶è€Œï¼Œåœ¨éŸ³é¢‘è¡¨ç¤ºå­¦æœ¯ç¤¾åŒºä¸­ï¼Œç°æœ‰çš„éŸ³é¢‘è¯­è¨€æ•°æ®é›†å—åˆ°ä¸€äº›é™åˆ¶ï¼Œå¦‚æ•°æ®é‡ä¸è¶³ã€å†…å®¹è¿‡äºç®€å•ã€æ”¶é›†è¿‡ç¨‹è¾ƒä¸ºç¹çã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„è‡ªåŠ¨éŸ³é¢‘captionç”Ÿæˆç®¡é“ï¼ŒåŸºäºä¸€ç³»åˆ—å…¬å…±å·¥å…·æˆ–APIï¼Œå¹¶æ„å»ºäº†å¤§è§„æ¨¡ã€é«˜è´¨é‡çš„éŸ³é¢‘è¯­è¨€æ•°æ®é›†ï¼Œåä¸ºAuto-ACDï¼ŒåŒ…å«è¶…è¿‡190ä¸‡ä¸ªéŸ³é¢‘æ–‡æœ¬å¯¹ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„æ•°æ®é›†çš„æ•ˆivenessï¼Œæˆ‘ä»¬åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šè®­ç»ƒäº†popularæ¨¡å‹ï¼Œå¹¶åœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šæ˜¾ç¤ºäº†æ€§èƒ½æ”¹è¿›ï¼ŒåŒ…æ‹¬éŸ³é¢‘è¯­è¨€æ£€ç´¢ã€éŸ³é¢‘captioningã€ç¯å¢ƒåˆ†ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾ç«‹äº†ä¸€ä¸ªæ–°çš„æµ‹è¯•é›†ï¼Œå¹¶æä¾›äº†éŸ³é¢‘æ–‡æœ¬ä»»åŠ¡çš„benchmarkã€‚æˆ‘ä»¬è®¡åˆ’åœ¨https://auto-acd.github.io/ä¸Šå‘å¸ƒæˆ‘ä»¬çš„æ•°æ®é›†ã€‚ã€‹Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="FreeU-Free-Lunch-in-Diffusion-U-Net"><a href="#FreeU-Free-Lunch-in-Diffusion-U-Net" class="headerlink" title="FreeU: Free Lunch in Diffusion U-Net"></a>FreeU: Free Lunch in Diffusion U-Net</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11497">http://arxiv.org/abs/2309.11497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ChenyangSi/FreeU">https://github.com/ChenyangSi/FreeU</a></li>
<li>paper_authors: Chenyang Si, Ziqi Huang, Yuming Jiang, Ziwei Liu</li>
<li>For: æé«˜ diffusion U-Net ç”Ÿæˆè´¨é‡ï¼Œæ— éœ€é¢å¤–è®­ç»ƒæˆ–è°ƒæ•´ã€‚* Methods: åˆ©ç”¨ U-Net æ¶æ„çš„ skip connections å’Œ backbone feature mapsï¼Œé€šè¿‡é‡æ–°æƒé‡åˆ†é…æ¥æé«˜ç”Ÿæˆè´¨é‡ã€‚* Results: åœ¨å›¾åƒå’Œè§†é¢‘ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæå‡ºäº†ä¸€ç§ç®€å• yet effective çš„æ–¹æ³• FreeUï¼Œå¯ä»¥è½»æ¾åœ°ä¸ç°æœ‰çš„ diffusion æ¨¡å‹ç»“åˆä½¿ç”¨ï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚<details>
<summary>Abstract</summary>
In this paper, we uncover the untapped potential of diffusion U-Net, which serves as a "free lunch" that substantially improves the generation quality on the fly. We initially investigate the key contributions of the U-Net architecture to the denoising process and identify that its main backbone primarily contributes to denoising, whereas its skip connections mainly introduce high-frequency features into the decoder module, causing the network to overlook the backbone semantics. Capitalizing on this discovery, we propose a simple yet effective method-termed "FreeU" - that enhances generation quality without additional training or finetuning. Our key insight is to strategically re-weight the contributions sourced from the U-Net's skip connections and backbone feature maps, to leverage the strengths of both components of the U-Net architecture. Promising results on image and video generation tasks demonstrate that our FreeU can be readily integrated to existing diffusion models, e.g., Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: https://chenyangsi.top/FreeU/.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†æ‰©æ•£U-Netçš„æœªåˆ©ç”¨æ½œåŠ›ï¼Œå®ƒä½œä¸ºä¸€ç§"å…è´¹çš„åˆé¤"ï¼Œå¯ä»¥åœ¨é£è¡Œä¸­æ˜¾è‘—æé«˜ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬é¦–å…ˆè°ƒæŸ¥æ‰©æ•£U-Netçš„å»ºç­‘å‡è¡¡å¯¹å‡å™ªè¿‡ç¨‹çš„å…³é”®è´¡çŒ®ï¼Œå¹¶å‘ç°å…¶ä¸»è¦è„Šæ¢ä¸»è¦åšå‡å™ªï¼Œè€Œè·³è½¬è¿æ¥ä¸»è¦å°†é«˜é¢‘ç‰¹å¾å¼•å…¥åˆ°è§£ç æ¨¡å—ï¼Œä½¿ç½‘ç»œå¿½ç•¥è„Šæ¢ semanticsã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å• yet effectiveçš„æ–¹æ³•â€”â€”FreeUï¼Œå¯ä»¥æ— éœ€é¢å¤–è®­ç»ƒæˆ–å¾®è°ƒï¼Œæé«˜ç”Ÿæˆè´¨é‡ã€‚æˆ‘ä»¬å…³é”®çš„æ€è·¯æ˜¯åœ¨æ‰©æ•£U-Netçš„è·³è½¬è¿æ¥å’Œè„Šæ¢ç‰¹å¾å›¾ä¹‹é—´è¿›è¡Œæƒé‡è°ƒæ•´ï¼Œä»¥åˆ©ç”¨æ‰©æ•£U-Netçš„ä¸¤ä¸ªç»„ä»¶ä¹‹é—´çš„ä¼˜åŠ¿ã€‚ promising results on image and video generation tasks show that our FreeU can be easily integrated into existing diffusion models, such as Stable Diffusion, DreamBooth, ModelScope, Rerender and ReVersion, to improve the generation quality with only a few lines of code. All you need is to adjust two scaling factors during inference. Project page: <https://chenyangsi.top/FreeU/>.
</details></li>
</ul>
<hr>
<h2 id="Budget-Aware-Pruning-Handling-Multiple-Domains-with-Less-Parameters"><a href="#Budget-Aware-Pruning-Handling-Multiple-Domains-with-Less-Parameters" class="headerlink" title="Budget-Aware Pruning: Handling Multiple Domains with Less Parameters"></a>Budget-Aware Pruning: Handling Multiple Domains with Less Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11464">http://arxiv.org/abs/2309.11464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Felipe dos Santos, Rodrigo Berriel, Thiago Oliveira-Santos, Nicu Sebe, Jurandy Almeida</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯å®ç°å¤šå…ƒé¢†åŸŸå­¦ä¹ ï¼ˆMulti-Domain Learningï¼‰ï¼Œå³è®©æ¨¡å‹åœ¨å¤šä¸ªé¢†åŸŸä¸­è¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”é™ä½è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å¤§å°ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†å‰Šé™¤ç­–ç•¥æ¥å®ç°æ¨¡å‹ç¼©å‡ï¼Œå³é¼“åŠ±æ‰€æœ‰é¢†åŸŸä½¿ç”¨ç›¸åŒçš„å­é›† filters æ¥æ„æˆæ¨¡å‹ï¼Œå¹¶å°†ä¸ä½¿ç”¨çš„ filters å‰Šé™¤ã€‚</li>
<li>results: ç ”ç©¶è·å¾—äº†ä¸åŸºå‡†æ¨¡å‹ç›¸ä¼¼çš„åˆ†ç±»æ€§èƒ½ï¼Œå¹¶ä¸”é™ä½äº†è®¡ç®—æˆæœ¬å’Œæ¨¡å‹å¤§å°ã€‚å¦å¤–ï¼Œè¿™ä¸ªæ–¹æ³•åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šä¹Ÿèƒ½å¤Ÿæ›´å¥½åœ°è¿è¡Œã€‚<details>
<summary>Abstract</summary>
Deep learning has achieved state-of-the-art performance on several computer vision tasks and domains. Nevertheless, it still has a high computational cost and demands a significant amount of parameters. Such requirements hinder the use in resource-limited environments and demand both software and hardware optimization. Another limitation is that deep models are usually specialized into a single domain or task, requiring them to learn and store new parameters for each new one. Multi-Domain Learning (MDL) attempts to solve this problem by learning a single model that is capable of performing well in multiple domains. Nevertheless, the models are usually larger than the baseline for a single domain. This work tackles both of these problems: our objective is to prune models capable of handling multiple domains according to a user-defined budget, making them more computationally affordable while keeping a similar classification performance. We achieve this by encouraging all domains to use a similar subset of filters from the baseline model, up to the amount defined by the user's budget. Then, filters that are not used by any domain are pruned from the network. The proposed approach innovates by better adapting to resource-limited devices while, to our knowledge, being the only work that handles multiple domains at test time with fewer parameters and lower computational complexity than the baseline model for a single domain.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ å·²ç»åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡å’Œé¢†åŸŸä¸Šè¾¾åˆ°äº†çŠ¶æ€å¯¹æŠ—æ€§ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶å…·æœ‰é«˜çš„è®¡ç®—æˆæœ¬å’Œéœ€è¦è¾ƒå¤šçš„å‚æ•°ã€‚è¿™äº›é™åˆ¶ä½¿å¾—åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­ä½¿ç”¨å®ƒä»¬å˜å¾—å›°éš¾ï¼Œéœ€è¦è½¯ä»¶å’Œç¡¬ä»¶ä¼˜åŒ–ã€‚å¦å¤–ï¼Œæ·±åº¦æ¨¡å‹é€šå¸¸æ˜¯ä¸“é—¨ä¸ºå•ä¸ªé¢†åŸŸæˆ–ä»»åŠ¡è®¾è®¡çš„ï¼Œå› æ­¤å®ƒä»¬éœ€è¦å­¦ä¹ å’Œå­˜å‚¨æ¯ä¸ªæ–°é¢†åŸŸæˆ–ä»»åŠ¡çš„æ–°å‚æ•°ã€‚å¤šä¸ªé¢†åŸŸå­¦ä¹ ï¼ˆMDLï¼‰å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡å­¦ä¹ ä¸€ä¸ªèƒ½å¤Ÿåœ¨å¤šä¸ªé¢†åŸŸä¸­è¡¨ç°å¥½çš„å•ä¸€æ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸æ¯”åŸºelineæ¨¡å‹æ›´å¤§ã€‚æœ¬å·¥ä½œè§£å†³äº†è¿™ä¸¤ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨ç”¨æˆ·å®šä¹‰çš„é¢„ç®—æ¥é‡‡æ ·å’Œè£å‰ªæ¨¡å‹ï¼Œä½¿å…¶åœ¨è®¡ç®—ä¸Šæ›´åŠ å¯æŒé¢„ç®—è€Œä»ä¿æŒç›¸ä¼¼çš„åˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬å®ç°äº†è¿™ä¸€ç‚¹é€šè¿‡ä¼˜åŒ–æ‰€æœ‰é¢†åŸŸä½¿ç”¨åŸºelineæ¨¡å‹çš„ç›¸ä¼¼subset of filtersï¼Œå¹¶ä¸”ä¸ç”¨äºä»»ä½•é¢†åŸŸçš„ç­›å­è¢«è£å‰ªå‡ºå»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ›æ–°åœ¨èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šæ›´å¥½åœ°é€‚åº”ï¼Œå¹¶ä¸”ï¼Œè‡³äºæˆ‘ä»¬æ‰€çŸ¥é“çš„ï¼Œæ˜¯å”¯ä¸€ä¸€ä¸ªåœ¨æµ‹è¯•æ—¶å¤„ç†å¤šä¸ªé¢†åŸŸçš„æ–¹æ³•ï¼Œä½¿ç”¨ fewer parameters å’Œæ›´ä½çš„è®¡ç®—å¤æ‚åº¦æ¥æ¯”åŸºelineæ¨¡å‹åœ¨å•ä¸ªé¢†åŸŸä¸­è¡¨ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Weight-Averaging-Improves-Knowledge-Distillation-under-Domain-Shift"><a href="#Weight-Averaging-Improves-Knowledge-Distillation-under-Domain-Shift" class="headerlink" title="Weight Averaging Improves Knowledge Distillation under Domain Shift"></a>Weight Averaging Improves Knowledge Distillation under Domain Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11446">http://arxiv.org/abs/2309.11446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vorobeevich/distillation-in-dg">https://github.com/vorobeevich/distillation-in-dg</a></li>
<li>paper_authors: Valeriy Berezovskiy, Nikita Morozov</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†çŸ¥è¯†å¡‘åŒ–ï¼ˆKDï¼‰æŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸæ•°æ®ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å­¦ä¹  teacher ç½‘ç»œå’Œå­¦ç”Ÿç½‘ç»œï¼Œå¹¶å¯¹å­¦ç”Ÿç½‘ç»œè¿›è¡Œäº†æƒé‡å¹³å‡æŠ€æœ¯ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œæƒé‡å¹³å‡æŠ€æœ¯å¯ä»¥æé«˜çŸ¥è¯†å¡‘åŒ–åœ¨ä¸åŒé¢†åŸŸæ•°æ®ä¸Šçš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§ç®€å•çš„æƒé‡å¹³å‡ç­–ç•¥ï¼Œä¸éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°éªŒè¯æ•°æ®ï¼Œå¹¶è¯æ˜å…¶ä¸SWADå’ŒSMAç›¸å½“ã€‚<details>
<summary>Abstract</summary>
Knowledge distillation (KD) is a powerful model compression technique broadly used in practical deep learning applications. It is focused on training a small student network to mimic a larger teacher network. While it is widely known that KD can offer an improvement to student generalization in i.i.d setting, its performance under domain shift, i.e. the performance of student networks on data from domains unseen during training, has received little attention in the literature. In this paper we make a step towards bridging the research fields of knowledge distillation and domain generalization. We show that weight averaging techniques proposed in domain generalization literature, such as SWAD and SMA, also improve the performance of knowledge distillation under domain shift. In addition, we propose a simplistic weight averaging strategy that does not require evaluation on validation data during training and show that it performs on par with SWAD and SMA when applied to KD. We name our final distillation approach Weight-Averaged Knowledge Distillation (WAKD).
</details>
<details>
<summary>æ‘˜è¦</summary>
çŸ¥è¯†å¡‘åŒ–ï¼ˆKDï¼‰æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨åœ¨æ·±åº¦å­¦ä¹ å®è·µä¸­çš„æ¨¡å‹å‹ç¼©æŠ€æœ¯ã€‚å®ƒå…³æ³¨è®­ç»ƒä¸€ä¸ªå°å­¦ç”Ÿç½‘ç»œï¼Œä»¥æ¨¡ä»¿ä¸€ä¸ªæ›´å¤§çš„æ•™å¸ˆç½‘ç»œã€‚è™½ç„¶å¹¿æ³›è®¤çŸ¥KDå¯ä»¥æé«˜å­¦ç”Ÿç½‘ç»œåœ¨åŒä¸€ä¸ªåˆ†å¸ƒä¸‹çš„æ³›åŒ–æ€§èƒ½ï¼Œä½†å®ƒåœ¨é¢†åŸŸè½¬ç§»æƒ…å†µä¸‹çš„æ€§èƒ½å°šæœªå¾—åˆ°äº†æ–‡çŒ®çš„å……åˆ†å…³æ³¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°è¯•å°†çŸ¥è¯†å¡‘åŒ–å’Œé¢†åŸŸæ€»ç»“ä¸¤ä¸ªé¢†åŸŸè”ç³»èµ·æ¥ã€‚æˆ‘ä»¬è¡¨æ˜äº†åœ¨é¢†åŸŸè½¬ç§»æƒ…å†µä¸‹ä½¿ç”¨Weight averagingæŠ€æœ¯ï¼Œå¦‚SWADå’ŒSMAï¼Œå¯ä»¥æé«˜çŸ¥è¯†å¡‘åŒ–çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„Weight averagingç­–ç•¥ï¼Œä¸éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°éªŒè¯æ•°æ®ï¼Œå¹¶è¯æ˜å®ƒä¸SWADå’ŒSMAåœ¨KDä¸­å…·æœ‰ç›¸åŒçš„æ€§èƒ½ã€‚æˆ‘ä»¬å°†è¿™ç§æœ€ç»ˆå¡‘åŒ–æ–¹æ³•ç§°ä¸ºWeight-Averaged Knowledge Distillationï¼ˆWAKDï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="SkeleTR-Towrads-Skeleton-based-Action-Recognition-in-the-Wild"><a href="#SkeleTR-Towrads-Skeleton-based-Action-Recognition-in-the-Wild" class="headerlink" title="SkeleTR: Towrads Skeleton-based Action Recognition in the Wild"></a>SkeleTR: Towrads Skeleton-based Action Recognition in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11445">http://arxiv.org/abs/2309.11445</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodong Duan, Mingze Xu, Bing Shuai, Davide Modolo, Zhuowen Tu, Joseph Tighe, Alessandro Bergamo</li>
<li>for: æœ¬æ–‡ targets more general scenarios of action recognition, such as variable number of people and various forms of interaction.</li>
<li>methods: æ–¹æ³•ä½¿ç”¨ two-stage paradigmï¼Œé¦–å…ˆä½¿ç”¨å›¾ convolutions æ¨¡å‹æ¯ä¸ªäººçš„å†…éƒ¨åŠ¨ä½œåŠ¨æ€ï¼Œç„¶åä½¿ç”¨å †å¼ transformer encoder æ•æ‰äººä¹‹é—´çš„äº¤äº’ã€‚</li>
<li>results: å¯¹å¤šç§ skeleton-based action recognition ä»»åŠ¡è¿›è¡Œäº†å…¨é¢çš„è§£å†³ï¼ŒåŒ…æ‹¬è§†é¢‘çº§åŠ¨ä½œåˆ†ç±»ã€å®ä¾‹çº§åŠ¨ä½œæ£€æµ‹å’Œç¾¤ä½“æ´»åŠ¨è¯†åˆ«ã€‚å®ç°äº† transfer learning å’Œå…±åŒè®­ç»ƒ across different action tasks and datasetsï¼Œå¹¶ä¸”åœ¨å¤šä¸ª benchmark ä¸Šè¾¾åˆ°äº† state-of-the-art æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
We present SkeleTR, a new framework for skeleton-based action recognition. In contrast to prior work, which focuses mainly on controlled environments, we target more general scenarios that typically involve a variable number of people and various forms of interaction between people. SkeleTR works with a two-stage paradigm. It first models the intra-person skeleton dynamics for each skeleton sequence with graph convolutions, and then uses stacked Transformer encoders to capture person interactions that are important for action recognition in general scenarios. To mitigate the negative impact of inaccurate skeleton associations, SkeleTR takes relative short skeleton sequences as input and increases the number of sequences. As a unified solution, SkeleTR can be directly applied to multiple skeleton-based action tasks, including video-level action classification, instance-level action detection, and group-level activity recognition. It also enables transfer learning and joint training across different action tasks and datasets, which result in performance improvement. When evaluated on various skeleton-based action recognition benchmarks, SkeleTR achieves the state-of-the-art performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†SkeleTRï¼Œä¸€ä¸ªæ–°çš„éª¨æ¶åŸºäºåŠ¨ä½œè¯†åˆ«æ¡†æ¶ã€‚ä¸å…ˆå‰çš„å·¥ä½œä¸åŒï¼ŒSkeleTRé’ˆå¯¹æ›´åŠ ä¸€èˆ¬çš„åœºæ™¯ï¼Œé€šå¸¸åŒ…æ‹¬å˜é‡æ•°é‡çš„äººå‘˜å’Œäººå‘˜ä¹‹é—´å¤šç§äº’åŠ¨ã€‚SkeleTRé‡‡ç”¨ä¸¤stageæ¶æ„ï¼Œé¦–å…ˆä½¿ç”¨å›¾ convolution æ¨¡å‹æ¯ä¸ªéª¨sequencesçš„å†…éƒ¨åŠ¨ä½œåŠ¨æ€ï¼Œç„¶åä½¿ç”¨å †å¼ transformer ç¼–ç å™¨æ•æ‰äººå‘˜ä¹‹é—´é‡è¦çš„åŠ¨ä½œè¯†åˆ«ã€‚ä¸ºäº†å‡è½»ä¸å‡†ç¡®çš„éª¨ association çš„å½±å“ï¼ŒSkeleTR ä½¿ç”¨çŸ­skeleton sequence ä½œä¸ºè¾“å…¥ï¼Œå¹¶å¢åŠ è¾“å…¥åºåˆ—çš„æ•°é‡ã€‚ä½œä¸ºä¸€ä¸ªé€šç”¨è§£å†³æ–¹æ¡ˆï¼ŒSkeleTR å¯ä»¥ç›´æ¥åº”ç”¨äºå¤šç§éª¨åŸºäºåŠ¨ä½œä»»åŠ¡ï¼ŒåŒ…æ‹¬è§†é¢‘çº§åŠ¨ä½œåˆ†ç±»ã€å®ä¾‹çº§åŠ¨ä½œæ£€æµ‹å’Œç¾¤ä½“æ´»åŠ¨è¯†åˆ«ã€‚å®ƒè¿˜å…è®¸è½¬ç§»å­¦ä¹ å’Œå…±åŒè®­ç»ƒä¸åŒçš„åŠ¨ä½œä»»åŠ¡å’Œæ•°æ®é›†ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚åœ¨å¤šç§éª¨åŸºäºåŠ¨ä½œè¯†åˆ« benchmark ä¸Šè¯„ä¼°ï¼ŒSkeleTR å®ç°äº†çŠ¶æ€çš„æä½³è¡¨ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Signature-Activation-A-Sparse-Signal-View-for-Holistic-Saliency"><a href="#Signature-Activation-A-Sparse-Signal-View-for-Holistic-Saliency" class="headerlink" title="Signature Activation: A Sparse Signal View for Holistic Saliency"></a>Signature Activation: A Sparse Signal View for Holistic Saliency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11443">http://arxiv.org/abs/2309.11443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dtak/signature-activation">https://github.com/dtak/signature-activation</a></li>
<li>paper_authors: Jose Roberto Tello Ayala, Akl C. Fahed, Weiwei Pan, Eugene V. Pomerantsev, Patrick T. Ellinor, Anthony Philippakis, Finale Doshi-Velez</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºMachine Learningçš„åŒ»ç–—å›¾åƒå¤„ç†æ–¹æ³•ï¼Œä»¥æé«˜åŒ»ç–—å›¾åƒè¯†åˆ«çš„é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‚</li>
<li>methods: æœ¬æ–‡å¼•å…¥äº†Signature Activationï¼Œä¸€ç§å¯é æ€§æ–¹æ³•ï¼Œå®ƒå¯ä»¥å¯¹äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„è¾“å‡ºç”Ÿæˆæ•´ä½“å’Œæ— å…³å¯¹è±¡çš„è§£é‡Šã€‚æœ¬æ–¹æ³•åŸºäºåŒ»ç–—å›¾åƒä¸­çš„å‰æ™¯å’ŒèƒŒæ™¯ç‰©ä»¶ä¹‹é—´çš„æ˜¾è‘—å·®å¼‚ã€‚</li>
<li>results: æœ¬æ–‡é€è¿‡è¯„ä¼° coronary angiogram ä¸­çš„ç—…å˜æ£€æµ‹ï¼Œè¯æ˜äº† Signature Activation çš„å¯é æ€§å’Œæœ‰ç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
The adoption of machine learning in healthcare calls for model transparency and explainability. In this work, we introduce Signature Activation, a saliency method that generates holistic and class-agnostic explanations for Convolutional Neural Network (CNN) outputs. Our method exploits the fact that certain kinds of medical images, such as angiograms, have clear foreground and background objects. We give theoretical explanation to justify our methods. We show the potential use of our method in clinical settings through evaluating its efficacy for aiding the detection of lesions in coronary angiograms.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šæœºå™¨å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨éœ€è¦æ¨¡å‹çš„é€æ˜åº¦å’Œè§£é‡Šæ€§ã€‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ã€Šç­¾åæ´»åŒ–ã€‹ï¼Œä¸€ç§å¯ä»¥ç”Ÿæˆæ•´ä½“å’Œæ— ç±»åˆ«çš„è§£é‡Šæ–¹æ³•ï¼Œç”¨äº convolutional neural networkï¼ˆCNNï¼‰è¾“å‡ºã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†æŸäº›åŒ»ç–—å›¾åƒï¼Œå¦‚è¡€ç®¡agramï¼Œå…·æœ‰æ˜ç¡®çš„å‰æ™¯å’ŒèƒŒæ™¯å¯¹è±¡ã€‚æˆ‘ä»¬ç»™å‡ºäº†ç†è®ºè§£é‡Šï¼Œä»¥ä¾¿è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡è¯„ä¼°å…¶åœ¨ coronary angiogram ä¸­çš„å¯ç”¨æ€§ï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸´åºŠåº”ç”¨ä¸­çš„æ½œåœ¨ä»·å€¼ã€‚
</details></li>
</ul>
<hr>
<h2 id="CalibFPA-A-Focal-Plane-Array-Imaging-System-based-on-Online-Deep-Learning-Calibration"><a href="#CalibFPA-A-Focal-Plane-Array-Imaging-System-based-on-Online-Deep-Learning-Calibration" class="headerlink" title="CalibFPA: A Focal Plane Array Imaging System based on Online Deep-Learning Calibration"></a>CalibFPA: A Focal Plane Array Imaging System based on Online Deep-Learning Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11421">http://arxiv.org/abs/2309.11421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alper GÃ¼ngÃ¶r, M. Umut Bahceci, Yasin Ergen, Ahmet SÃ¶zak, O. Oner Ekiz, Tolga Yelboga, Tolga Ã‡ukur</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å‹ç¼©é•œå¤´æ•°ç»„ç³»ç»Ÿï¼ˆCalibFPAï¼‰ï¼Œä»¥å®ç°é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰æˆåƒï¼Œå¹¶ä¸”ä¸éœ€è¦çº¿ä¸Šå‡†å¤‡ã€‚</li>
<li>methods: è¯¥ç³»ç»Ÿä½¿ç”¨äº†ç”µå­æ§åˆ¶çš„ç©ºé—´å…‰æ¨¡ulatorsï¼ˆSLMï¼‰è¿›è¡Œå¤šé‡ç¼–ç ï¼Œå¹¶ä½¿ç”¨äº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ ç½‘ç»œæ¥åœ¨å¤šä¸ªLRæµ‹é‡ä¸­ correect ç³»ç»Ÿä¸è‰¯çš„å½±å“ã€‚</li>
<li>results: åœ¨æ¨¡æ‹Ÿå’Œå®éªŒæ•°æ®ä¸Šï¼ŒCalibFPAçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„å‹ç¼©é•œå¤´æ•°ç»„æ–¹æ³•ï¼Œå¹¶ä¸”è¿›è¡Œäº†ç³»ç»Ÿå…ƒç´ çš„åˆ†æå’Œè®¡ç®—å¤æ‚åº¦çš„è¯„ä¼°ã€‚<details>
<summary>Abstract</summary>
Compressive focal plane arrays (FPA) enable cost-effective high-resolution (HR) imaging by acquisition of several multiplexed measurements on a low-resolution (LR) sensor. Multiplexed encoding of the visual scene is typically performed via electronically controllable spatial light modulators (SLM). An HR image is then reconstructed from the encoded measurements by solving an inverse problem that involves the forward model of the imaging system. To capture system non-idealities such as optical aberrations, a mainstream approach is to conduct an offline calibration scan to measure the system response for a point source at each spatial location on the imaging grid. However, it is challenging to run calibration scans when using structured SLMs as they cannot encode individual grid locations. In this study, we propose a novel compressive FPA system based on online deep-learning calibration of multiplexed LR measurements (CalibFPA). We introduce a piezo-stage that locomotes a pre-printed fixed coded aperture. A deep neural network is then leveraged to correct for the influences of system non-idealities in multiplexed measurements without the need for offline calibration scans. Finally, a deep plug-and-play algorithm is used to reconstruct images from corrected measurements. On simulated and experimental datasets, we demonstrate that CalibFPA outperforms state-of-the-art compressive FPA methods. We also report analyses to validate the design elements in CalibFPA and assess computational complexity.
</details>
<details>
<summary>æ‘˜è¦</summary>
é«˜åº¦å‹ç¼©çš„æŠ•å½±å¹³é¢é˜µåˆ—ï¼ˆFPAï¼‰å¯ä»¥å®ç°ä½æˆæœ¬é«˜åˆ†è¾¨ç‡ï¼ˆHRï¼‰æˆåƒï¼Œé€šè¿‡å¤šä¸ªå¤šæ ·åŒ–æµ‹é‡åœ¨ä½åˆ†è¾¨ç‡ï¼ˆLRï¼‰æ„ŸçŸ¥å™¨ä¸Šã€‚å¤šæ ·åŒ–ç¼–ç è§†åœºé€šå¸¸é€šè¿‡ç”µå­æ§åˆ¶å¯å˜å…‰å­¦æ¨¡æ‹Ÿå™¨ï¼ˆSLMï¼‰è¿›è¡Œã€‚ç„¶åï¼Œä»ç¼–ç æµ‹é‡ä¸­é‡å»ºHRå›¾åƒï¼Œé€šè¿‡è§£å†³ä¸€ä¸ªåå°„é—®é¢˜ï¼Œè¯¥é—®é¢˜æ¶‰åŠåˆ°æˆåƒç³»ç»Ÿçš„å‰å‘æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨ç»“æ„åŒ–SLMæ—¶éš¾ä»¥è¿›è¡Œçº¿ä¸Šå‡†å¤‡æ‰«æï¼Œä»¥ä¾¿æµ‹é‡ç³»ç»Ÿå“åº”ç‚¹æºåœ¨æ¯ä¸ªç©ºé—´ä½ç½®ä¸Šã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å‹ç¼©FPAç³»ç»Ÿï¼ŒåŸºäºåœ¨çº¿æ·±åº¦å­¦ä¹ å‡†å¤‡å¤šæ ·åŒ–LRæµ‹é‡ï¼ˆCalibFPAï¼‰ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª piezo é˜¶æ®µï¼Œä½¿å¾—é¢„åˆ¶å°åˆ·çš„å›ºå®šç¼–ç çª—å£åœ¨ä¸åŒçš„ç©ºé—´ä½ç½®ä¸Šç§»åŠ¨ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œæ¥çº æ­£å¤šæ ·åŒ–æµ‹é‡ä¸­ç³»ç»Ÿéç†æƒ³çš„å½±å“ï¼Œæ— éœ€è¿›è¡Œçº¿ä¸Šå‡†å¤‡æ‰«æã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ·±åº¦æ’ä»¶æ’­å®¢ç®—æ³•æ¥é‡å»ºå›¾åƒã€‚åœ¨æ¨¡æ‹Ÿå’Œå®éªŒæ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†CalibFPAçš„æ€§èƒ½æ¯”ç°æœ‰å‹ç¼©FPAæ–¹æ³•æ›´é«˜ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†åˆ†æï¼Œä»¥éªŒè¯è®¾è®¡å…ƒç´ çš„åˆç†æ€§å’Œè®¡ç®—å¤æ‚æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="CNNs-for-JPEGs-A-Study-in-Computational-Cost"><a href="#CNNs-for-JPEGs-A-Study-in-Computational-Cost" class="headerlink" title="CNNs for JPEGs: A Study in Computational Cost"></a>CNNs for JPEGs: A Study in Computational Cost</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11417">http://arxiv.org/abs/2309.11417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Felipe dos Santos, Nicu Sebe, Jurandy Almeida</li>
<li>for: æœ¬æ–‡æ—¨åœ¨ç ”ç©¶é¢‘åŸŸé¢„å¤„ç†åçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¼˜åŒ–è®¡ç®—æˆæœ¬å’Œå‚æ•°æ•°é‡ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†DCTé¢‘åŸŸè¡¨ç¤ºæ³•ï¼Œå¹¶å¯¹ä¼ ç»Ÿ CNN æ¶æ„è¿›è¡Œäº†ä¿®æ”¹ï¼Œä»¥é€‚åº”é¢‘åŸŸæ•°æ®ã€‚</li>
<li>results: æœ¬æ–‡æå‡ºäº†ä¸€äº›æ‰‹åŠ¨å’Œæ•°æ®é©±åŠ¨çš„æŠ€æœ¯æ¥é™ä½è®¡ç®—æˆæœ¬å’Œå‚æ•°æ•°é‡ï¼Œä»¥å®ç°é«˜æ•ˆä¸”ç²¾å‡†çš„é¢‘åŸŸæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Convolutional neural networks (CNNs) have achieved astonishing advances over the past decade, defining state-of-the-art in several computer vision tasks. CNNs are capable of learning robust representations of the data directly from the RGB pixels. However, most image data are usually available in compressed format, from which the JPEG is the most widely used due to transmission and storage purposes demanding a preliminary decoding process that have a high computational load and memory usage. For this reason, deep learning methods capable of learning directly from the compressed domain have been gaining attention in recent years. Those methods usually extract a frequency domain representation of the image, like DCT, by a partial decoding, and then make adaptation to typical CNNs architectures to work with them. One limitation of these current works is that, in order to accommodate the frequency domain data, the modifications made to the original model increase significantly their amount of parameters and computational complexity. On one hand, the methods have faster preprocessing, since the cost of fully decoding the images is avoided, but on the other hand, the cost of passing the images though the model is increased, mitigating the possible upside of accelerating the method. In this paper, we propose a further study of the computational cost of deep models designed for the frequency domain, evaluating the cost of decoding and passing the images through the network. We also propose handcrafted and data-driven techniques for reducing the computational complexity and the number of parameters for these models in order to keep them similar to their RGB baselines, leading to efficient models with a better trade off between computational cost and accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨è¿‡å»çš„ä¸€ä»£æ—¶é—´å†…å–å¾—äº†éå¸¸çš„è¿›æ­¥ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å®šä¹‰äº†çŠ¶æ€çš„è‰ºæœ¯ã€‚CNNå¯ä»¥ç›´æ¥ä»RGBåƒç´ ä¸Šå­¦ä¹ åšå®çš„æ•°æ®è¡¨ç¤ºã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å›¾åƒæ•°æ®é€šå¸¸æ˜¯å‹ç¼©å½¢å¼ï¼ŒJPEGæ˜¯æœ€å¸¸ç”¨çš„ï¼Œå› ä¸ºä¼ è¾“å’Œå­˜å‚¨ç›®çš„éœ€è¦é«˜è®¡ç®—è´Ÿæ‹…å’Œå†…å­˜ä½¿ç”¨ã€‚ä¸ºæ­¤ï¼Œå¯ä»¥ç›´æ¥ä»å‹ç¼©é¢†åŸŸå­¦ä¹ æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨è¿‡å»å‡ å¹´å†…å¾—åˆ°äº†å…³æ³¨ã€‚è¿™äº›æ–¹æ³•é€šå¸¸æå–å›¾åƒçš„é¢‘ç‡é¢‘è°±è¡¨ç¤ºï¼Œä¾‹å¦‚DCTï¼Œé€šè¿‡éƒ¨åˆ†è§£ç ï¼Œç„¶åå°†å…¶ä¸ä¼ ç»Ÿçš„CNNæ¶æ„è¿›è¡Œé€‚åº”ã€‚ç°æœ‰çš„æ–¹æ³•çš„ä¸€ä¸ªé™åˆ¶æ˜¯ï¼Œä¸ºäº†é€‚åº”é¢‘ç‡é¢‘è°±æ•°æ®ï¼Œæ¨¡å‹çš„ä¿®æ”¹ä¼šå¢åŠ æ˜¾è‘—ã€‚ä¸€æ–¹é¢ï¼Œé¢„å¤„ç†æ›´å¿«ï¼Œå› ä¸ºå®Œå…¨è§£ç å›¾åƒçš„æˆæœ¬è¢«é¿å…äº†ï¼Œä½†å¦ä¸€æ–¹é¢ï¼Œé€šè¿‡ç½‘ç»œä¼ è¾“å›¾åƒçš„æˆæœ¬å¢åŠ ï¼Œè¿™å¯èƒ½å¯¼è‡´åŠ é€Ÿæ–¹æ³•çš„å¯èƒ½æ€§å‡é€€ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¿›ä¸€æ­¥ç ”ç©¶æ·±åº¦æ¨¡å‹åœ¨é¢‘ç‡é¢‘è°±é¢‘è°±ä¸Šçš„è®¡ç®—æˆæœ¬ï¼Œä»¥åŠå›¾åƒä¼ è¾“å’Œç½‘ç»œä¼ è¾“çš„æˆæœ¬ã€‚æˆ‘ä»¬è¿˜å°†æå‡ºæ‰‹å·¥å’Œæ•°æ®é©±åŠ¨çš„æŠ€æœ¯ï¼Œä»¥å‡å°‘æ¨¡å‹çš„è®¡ç®—å¤æ‚æ€§å’Œå‚æ•°æ•°é‡ï¼Œä»¥ä¿æŒä¸RGBåŸºelineç›¸ä¼¼çš„æ•ˆç‡ï¼Œä»è€Œå®ç°æ›´å¥½çš„è®¡ç®—æˆæœ¬å’Œå‡†ç¡®æ€§çš„è´Ÿæ‹…å¹³è¡¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-motion-trajectory-segmentation-of-rigid-bodies-using-a-novel-screw-based-trajectory-shape-representation"><a href="#Enhancing-motion-trajectory-segmentation-of-rigid-bodies-using-a-novel-screw-based-trajectory-shape-representation" class="headerlink" title="Enhancing motion trajectory segmentation of rigid bodies using a novel screw-based trajectory-shape representation"></a>Enhancing motion trajectory segmentation of rigid bodies using a novel screw-based trajectory-shape representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11413">http://arxiv.org/abs/2309.11413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arno Verduyn, Maxim Vochten, Joris De Schutter</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹3Då›ºä½“è¿åŠ¨çš„è½¨è¿¹åˆ†å‰²ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è½¨è¿¹è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒåŒ…æ‹¬ä¸€ä¸ª Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹è¿›åº¦ç‡å’Œä¸€ä¸ªç¬¬ä¸‰é˜¶è½¨è¿¹å½¢æ€æè¿°å™¨ï¼Œå¹¶ä¸”å…·æœ‰ä¸€äº›æƒ¯æ€§æ€§å’Œå‚è€ƒç‚¹æ— å…³æ€§çš„ç‰¹ç‚¹ã€‚</li>
<li>results: è¯¥è®ºæ–‡ä½¿ç”¨è‡ªæˆ‘ç›‘ç£åˆ†å‰²æ–¹æ³•è¿›è¡ŒéªŒè¯ï¼Œåœ¨å®éªŒå’ŒçœŸå®çš„äººç±»æ–Ÿ Pouring åŠ¨ä½œè®°å½•ä¸­è¡¨ç°å‡ºæ›´åŠ ç¨³å®šå’Œä¸€è‡´çš„åˆ†å‰²ç»“æœï¼Œä¸ä¼ ç»Ÿè¡¨ç¤ºæ–¹æ³•ç›¸æ¯”ã€‚<details>
<summary>Abstract</summary>
Trajectory segmentation refers to dividing a trajectory into meaningful consecutive sub-trajectories. This paper focuses on trajectory segmentation for 3D rigid-body motions. Most segmentation approaches in the literature represent the body's trajectory as a point trajectory, considering only its translation and neglecting its rotation. We propose a novel trajectory representation for rigid-body motions that incorporates both translation and rotation, and additionally exhibits several invariant properties. This representation consists of a geometric progress rate and a third-order trajectory-shape descriptor. Concepts from screw theory were used to make this representation time-invariant and also invariant to the choice of body reference point. This new representation is validated for a self-supervised segmentation approach, both in simulation and using real recordings of human-demonstrated pouring motions. The results show a more robust detection of consecutive submotions with distinct features and a more consistent segmentation compared to conventional representations. We believe that other existing segmentation methods may benefit from using this trajectory representation to improve their invariance.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‡†ç¡®åœ°æè¿°è¡Œèµ°è¿‡ç¨‹çš„åˆ†æ®µæ˜¯æŒ‡å°†è¡Œèµ°è¿‡ç¨‹åˆ†è§£æˆæœ‰æ„ä¹‰çš„è¿ç»­å­è¿‡ç¨‹ã€‚è¿™ç¯‡è®ºæ–‡å…³æ³¨äºä¸‰ç»´å›ºå®šä½“è¿åŠ¨çš„è½¨è¿¹åˆ†æ®µã€‚å¤§å¤šæ•°æ–‡çŒ®ä¸­çš„åˆ†æ®µæ–¹æ³•åªè€ƒè™‘ä½“çš„ç¿»è¯‘å’Œå¿½ç•¥å…¶æ—‹è½¬ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è½¨è¿¹è¡¨ç¤ºæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ª Ğ³ĞµĞ¾Ğ¼ĞµÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹è¿›åº¦ç‡å’Œä¸€ä¸ªç¬¬ä¸‰é˜¶è½¨è¿¹å½¢æ€æè¿°å™¨ã€‚æˆ‘ä»¬ä½¿ç”¨äº†æ»šç­’ç†è®ºæ¥ä½¿è¿™ç§è¡¨ç¤ºæ–¹æ³•æ—¶é—´ä¸å˜å’Œå‚ç…§ç‚¹æ— å…³ã€‚è¿™ç§æ–°çš„è¡¨ç¤ºæ–¹æ³•åœ¨è‡ªä¸»ç›‘ç£åˆ†æ®µæ–¹æ³•ä¸­å¾—åˆ°éªŒè¯ï¼ŒåŒ…æ‹¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®çš„äººç±»å€’ Pouring åŠ¨ä½œè®°å½•ä¸­ã€‚ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è¿™ç§è½¨è¿¹è¡¨ç¤ºæ–¹æ³•å¯ä»¥æ›´å¥½åœ°æ£€æµ‹å‡ºä¸åŒç‰¹å¾çš„è¿ç»­å­è¿‡ç¨‹ï¼Œå¹¶ä¸”æ¯”ä¼ ç»Ÿè¡¨ç¤ºæ–¹æ³•æ›´åŠ ä¸€è‡´ã€‚æˆ‘ä»¬è®¤ä¸ºå…¶ä»–ç°æœ‰çš„åˆ†æ®µæ–¹æ³•å¯èƒ½ä¼šä»è¿™ç§è½¨è¿¹è¡¨ç¤ºæ–¹æ³•ä¸­å—ç›Šï¼Œä»¥æé«˜å…¶å¯¹ç§°æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-learning-unveils-change-in-urban-housing-from-street-level-images"><a href="#Self-supervised-learning-unveils-change-in-urban-housing-from-street-level-images" class="headerlink" title="Self-supervised learning unveils change in urban housing from street-level images"></a>Self-supervised learning unveils change in urban housing from street-level images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11354">http://arxiv.org/abs/2309.11354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steven Stalder, Michele Volpi, Nicolas BÃ¼ttner, Stephen Law, Kenneth Harttgen, Esra Suel</li>
<li>for:  tracks progress in urban housing, specifically in Londonâ€™s housing supply</li>
<li>methods:  uses deep learning-based computer vision methods and self-supervised techniques to measure change in street-level images</li>
<li>results:  successfully identified point-level change in Londonâ€™s housing supply and distinguished between major and minor change, providing timely information for urban planning and policy decisions.<details>
<summary>Abstract</summary>
Cities around the world face a critical shortage of affordable and decent housing. Despite its critical importance for policy, our ability to effectively monitor and track progress in urban housing is limited. Deep learning-based computer vision methods applied to street-level images have been successful in the measurement of socioeconomic and environmental inequalities but did not fully utilize temporal images to track urban change as time-varying labels are often unavailable. We used self-supervised methods to measure change in London using 15 million street images taken between 2008 and 2021. Our novel adaptation of Barlow Twins, Street2Vec, embeds urban structure while being invariant to seasonal and daily changes without manual annotations. It outperformed generic embeddings, successfully identified point-level change in London's housing supply from street-level images, and distinguished between major and minor change. This capability can provide timely information for urban planning and policy decisions toward more liveable, equitable, and sustainable cities.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…¨çƒå„åœ°åŸå¸‚é¢ä¸´ç€ä¾›åº”å……è¶³ã€å®‰å…¨ã€å¥åº·çš„ä½æˆ¿çš„ç´§è¿«éœ€æ±‚ã€‚å°½ç®¡åŸå¸‚ä½æˆ¿é—®é¢˜çš„æ”¿ç­–é‡è¦æ€§ä¸è¨€è€Œå–»ï¼Œä½†æˆ‘ä»¬å¯¹åŸå¸‚å˜åŒ–çš„è¿½è¸ªå’Œç›‘æµ‹èƒ½åŠ›å´å—åˆ°é™åˆ¶ã€‚ä½¿ç”¨æ·±åº¦å­¦ä¹ è®¡ç®—æœºè§†è§‰æ–¹æ³•å¯¹è¡—é“çº§å›¾åƒè¿›è¡Œåˆ†æï¼Œå¯ä»¥æˆåŠŸåœ°è¡¡é‡ç¤¾ä¼šç»æµå’Œç¯å¢ƒä¸å¹³ç­‰ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•åˆ©ç”¨æ—¶é—´å˜åŒ–æ¥è¿½è¸ªåŸå¸‚å˜åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨è‡ªåŠ¨å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨2008å¹´è‡³2021å¹´ä¹‹é—´çš„1500ä¸‡ä¸ªè¡—é“çº§å›¾åƒï¼Œåœ¨ä¼¦æ•¦å¸‚è¿›è¡Œäº†æ—¶é—´å˜åŒ–çš„è¿½è¸ªã€‚æˆ‘ä»¬å¯¹Barlow Twinsè¿›è¡Œäº†æ”¹è¿›ï¼Œç§°ä¹‹ä¸ºStreet2Vecï¼Œå®ƒå¯ä»¥åµŒå…¥åŸå¸‚ç»“æ„ï¼ŒåŒæ—¶å…·æœ‰å­£èŠ‚å’Œæ—¥æœŸå˜åŒ–çš„æŠ—è¾å°„æ€§ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡æ³¨ã€‚Street2Vecåœ¨ä¼¦æ•¦å¸‚çš„ä½æˆ¿ä¾›åº”å˜åŒ–è¿½è¸ªä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥æä¾›å®æ—¶çš„åŸå¸‚è§„åˆ’å’Œæ”¿ç­–å†³ç­–ä¿¡æ¯ï¼Œä»¥å»ºç«‹æ›´åŠ äººå±…ä½ã€å…¬å¹³ã€å¯æŒç»­çš„åŸå¸‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="You-can-have-your-ensemble-and-run-it-too-â€“-Deep-Ensembles-Spread-Over-Time"><a href="#You-can-have-your-ensemble-and-run-it-too-â€“-Deep-Ensembles-Spread-Over-Time" class="headerlink" title="You can have your ensemble and run it too â€“ Deep Ensembles Spread Over Time"></a>You can have your ensemble and run it too â€“ Deep Ensembles Spread Over Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11333">http://arxiv.org/abs/2309.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isak Meding, Alexander Bodin, Adam Tonderski, Joakim Johnander, Christoffer Petersson, Lennart Svensson</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æ¢è®¨æ·±åº¦ Ensemble å¯ä»¥åœ¨æ—¶é—´åºåˆ—ä¸Šæ‰©å±•ä»¥æé«˜é¢„æµ‹æ€§å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å¯èƒ½æ€§ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº† Deep Ensembles Spread Over Time (DESOT) æ–¹æ³•ï¼Œå°†å•ä¸€çš„ Ensemble  member åº”ç”¨åˆ°æ¯ä¸ªæ•°æ®ç‚¹ä¸Šï¼Œå¹¶èåˆå¤šä¸ªæ•°æ®ç‚¹çš„é¢„æµ‹ã€‚</li>
<li>results: DESOT å¯ä»¥è·å¾—æ·±åº¦ Ensemble çš„ä¼˜åŒ–æ€§å’Œä¸ç¡®å®šæ€§ä¼°è®¡æ€§ï¼Œè€Œä¸éœ€è¦é¢å¤–çš„è®¡ç®—æˆæœ¬å¢åŠ ã€‚ DESOT ä¹Ÿç®€å•å®ç°ï¼Œä¸éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ—¶é—´åºåˆ—ã€‚ æœ€åï¼Œæˆ‘ä»¬å‘ç° DESOT å’Œæ·±åº¦ Ensemble éƒ½èƒ½åœ¨éæ ‡å‡†æ•°æ®ä¸Šè¿›è¡Œé¢„æµ‹å’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚<details>
<summary>Abstract</summary>
Ensembles of independently trained deep neural networks yield uncertainty estimates that rival Bayesian networks in performance. They also offer sizable improvements in terms of predictive performance over single models. However, deep ensembles are not commonly used in environments with limited computational budget -- such as autonomous driving -- since the complexity grows linearly with the number of ensemble members. An important observation that can be made for robotics applications, such as autonomous driving, is that data is typically sequential. For instance, when an object is to be recognized, an autonomous vehicle typically observes a sequence of images, rather than a single image. This raises the question, could the deep ensemble be spread over time?   In this work, we propose and analyze Deep Ensembles Spread Over Time (DESOT). The idea is to apply only a single ensemble member to each data point in the sequence, and fuse the predictions over a sequence of data points. We implement and experiment with DESOT for traffic sign classification, where sequences of tracked image patches are to be classified. We find that DESOT obtains the benefits of deep ensembles, in terms of predictive and uncertainty estimation performance, while avoiding the added computational cost. Moreover, DESOT is simple to implement and does not require sequences during training. Finally, we find that DESOT, like deep ensembles, outperform single models for out-of-distribution detection.
</details>
<details>
<summary>æ‘˜è¦</summary>
ensemble of independently trained deep neural networkså¯ä»¥æä¾›ä¸ bayesian networksç›¸å½“çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚ä½†æ˜¯ï¼Œæ·±åº¦ ensembleåœ¨è®¡ç®—budgetæœ‰é™çš„ç¯å¢ƒä¸­å¹¶ä¸å¾ˆå¸¸è§ï¼Œå› ä¸ºensembleçš„å¤æ‚åº¦éšç€æˆå‘˜å¢åŠ è€Œå¢åŠ ã€‚åœ¨Ñ€Ğ¾Ğ±Ğ¾ç‰¹åº”ç”¨ï¼Œå¦‚è‡ªåŠ¨é©¾é©¶ï¼Œå‘ç°æ•°æ®é€šå¸¸æ˜¯é¡ºåºçš„ã€‚ä¾‹å¦‚ï¼Œå½“éœ€è¦è¯†åˆ«ä¸€ä¸ªç‰©ä½“æ—¶ï¼Œä¸€è¾†è‡ªåŠ¨é©¾é©¶è½¦é€šå¸¸ä¼šè§‚å¯Ÿä¸€ä¸²å›¾åƒï¼Œè€Œä¸æ˜¯å•ä¸ªå›¾åƒã€‚è¿™å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šå¯ä»¥å°†æ·±åº¦ ensembleæ¨å¹¿åˆ°æ—¶é—´å—ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦ ensembleæ¨å¹¿åˆ°æ—¶é—´ï¼ˆDESOTï¼‰çš„æƒ³æ³•ã€‚æˆ‘ä»¬åªåº”ç”¨ä¸€ä¸ª ensemble member åˆ°æ¯ä¸ªæ•°æ®ç‚¹çš„åºåˆ—ä¸­ï¼Œå¹¶å°†é¢„æµ‹ç»“æœè¿›è¡Œèåˆã€‚æˆ‘ä»¬å®ç°å¹¶å¯¹ traffic sign classification è¿›è¡Œå®éªŒï¼ŒSequence of tracked image patches éœ€è¦è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å‘ç° DESOT å¯ä»¥è·å¾—æ·±åº¦ ensemble çš„ä¼˜ç‚¹ï¼Œå³é¢„æµ‹æ€§èƒ½å’Œä¸ç¡®å®šæ€§ä¼°è®¡çš„å¥½å¤„ï¼Œè€Œä¸éœ€è¦æ·»åŠ è®¡ç®—æˆæœ¬ã€‚æ­¤å¤–ï¼ŒDESOT ç®€å•æ˜“å®ç°ï¼Œä¸éœ€è¦åœ¨è®­ç»ƒæ—¶åºåˆ—ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç° DESOT ä¹Ÿå¯ä»¥è¶…è¿‡å•ä¸ªæ¨¡å‹çš„è¡¨ç°ï¼Œå¯¹äºéæ ‡å‡†èŒƒå›´æ£€æµ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="How-to-turn-your-camera-into-a-perfect-pinhole-model"><a href="#How-to-turn-your-camera-into-a-perfect-pinhole-model" class="headerlink" title="How to turn your camera into a perfect pinhole model"></a>How to turn your camera into a perfect pinhole model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11326">http://arxiv.org/abs/2309.11326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan De Boi, Stuti Pathak, Marina Oliveira, Rudi Penne</li>
<li>for: æé«˜è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­çš„ç›¸æœºå‡†å¤‡ç¯å¢ƒï¼Œæä¾›ä¸€ç§å¯ä»¥å¤„ç†å¤šç§æ‰­æ›²æºçš„æ–°æ–¹æ³•ã€‚</li>
<li>methods: ä½¿ç”¨ Gaussian processes æ¥å»é™¤å›¾åƒä¸­çš„æ‰­æ›²å’Œç›¸æœºç¼ºé™·ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ç†æƒ³æŠ•å°„ç›¸æœºï¼Œåªéœ€ä¸€å¼ æ­£æ–¹å½¢ç½‘æ ¼æ£€æŸ¥æ¨¡å¼å›¾åƒã€‚</li>
<li>results: æé«˜äº†è®¸å¤šè®¡ç®—æœºè§†è§‰ç®—æ³•å’Œåº”ç”¨çš„æ€§èƒ½ï¼Œæ¶ˆé™¤äº†æ‰­æ›²å‚æ•°å’Œè¿­ä»£ä¼˜åŒ–ã€‚  Validated by synthetic data and real-world images.<details>
<summary>Abstract</summary>
Camera calibration is a first and fundamental step in various computer vision applications. Despite being an active field of research, Zhang's method remains widely used for camera calibration due to its implementation in popular toolboxes. However, this method initially assumes a pinhole model with oversimplified distortion models. In this work, we propose a novel approach that involves a pre-processing step to remove distortions from images by means of Gaussian processes. Our method does not need to assume any distortion model and can be applied to severely warped images, even in the case of multiple distortion sources, e.g., a fisheye image of a curved mirror reflection. The Gaussian processes capture all distortions and camera imperfections, resulting in virtual images as though taken by an ideal pinhole camera with square pixels. Furthermore, this ideal GP-camera only needs one image of a square grid calibration pattern. This model allows for a serious upgrade of many algorithms and applications that are designed in a pure projective geometry setting but with a performance that is very sensitive to nonlinear lens distortions. We demonstrate the effectiveness of our method by simplifying Zhang's calibration method, reducing the number of parameters and getting rid of the distortion parameters and iterative optimization. We validate by means of synthetic data and real world images. The contributions of this work include the construction of a virtual ideal pinhole camera using Gaussian processes, a simplified calibration method and lens distortion removal.
</details>
<details>
<summary>æ‘˜è¦</summary>
Camera å¡åˆ©ibration æ˜¯ computer vision åº”ç”¨ä¸­çš„ç¬¬ä¸€æ­¥å’ŒåŸºç¡€æ­¥éª¤ã€‚å°½ç®¡æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œå¼ çš„æ–¹æ³•ä»ç„¶å¹¿æ³›ä½¿ç”¨äº camera å¡åˆ©ibration due to its implementation in popular toolboxesã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•åˆå§‹åŒ–å‡è®¾äº†ç¼©å½±æ¨¡å‹ï¼Œå¿½ç•¥äº†çœŸå®çš„æ‰­æ›²æ¨¡å‹ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡ Gaussian processes æ¥ä»å›¾åƒä¸­é™¤æ‰­æ›²ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦ä»»ä½•æ‰­æ›²æ¨¡å‹ï¼Œå¯ä»¥åº”ç”¨äºä¸¥é‡æ‰­æ›²çš„å›¾åƒï¼Œç”šè‡³åœ¨å¤šä¸ªæ‰­æ›²æºçš„æƒ…å†µä¸‹ï¼Œå¦‚ fisheye å›¾åƒ reflection çš„å¼¯æ›²é•œã€‚ Gaussian processes æ•æ‰äº†æ‰€æœ‰çš„æ‰­æ›²å’Œç›¸æœºç¼ºé™·ï¼Œä»è€Œç”Ÿæˆè™šæ‹Ÿçš„ ideal pinhole camera å›¾åƒï¼Œå¦‚quare pixelsã€‚æ­¤å¤–ï¼Œè¿™ä¸ª ideal GP-camera åªéœ€ä¸€ä¸ªå¹³æ–¹æ ¼ calibration pattern å›¾åƒã€‚è¿™ç§æ¨¡å‹å…è®¸è®¸å¤šç®—æ³•å’Œåº”ç”¨ç¨‹åºï¼Œå…¶ä¸­ä¸€äº›æ˜¯åœ¨çº¯ proyective geometry è®¾å®šä¸‹è®¾è®¡ï¼Œä½†æ˜¯æ€§èƒ½å—åˆ°éçº¿æ€§é•œå¤´æ‰­æ›²çš„å½±å“ã€‚æˆ‘ä»¬é€šè¿‡ç®€åŒ–å¼ çš„å¡åˆ©ibration æ–¹æ³•ï¼Œå‡å°‘å‚æ•°çš„æ•°é‡ï¼Œæ¶ˆé™¤æ‰­æ›²å‚æ•°å’Œè¿­ä»£ä¼˜åŒ–æ¥è¯æ˜æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬éªŒè¯äº†è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§é€šè¿‡ synthetic æ•°æ®å’Œå®é™…å›¾åƒã€‚æœ¬ç ”ç©¶çš„è´¡çŒ®åŒ…æ‹¬ï¼šåœ¨ Gaussian processes ä¸­æ„å»ºè™šæ‹Ÿçš„ ideal pinhole cameraï¼Œç®€åŒ–å¡åˆ©ibration æ–¹æ³•å’Œé•œå¤´æ‰­æ›²é™¤é™¤ã€‚
</details></li>
</ul>
<hr>
<h2 id="Face-Aging-via-Diffusion-based-Editing"><a href="#Face-Aging-via-Diffusion-based-Editing" class="headerlink" title="Face Aging via Diffusion-based Editing"></a>Face Aging via Diffusion-based Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11321">http://arxiv.org/abs/2309.11321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MunchkinChen/FADING">https://github.com/MunchkinChen/FADING</a></li>
<li>paper_authors: Xiangyi Chen, StÃ©phane LathuiliÃ¨re</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³é¢éƒ¨å¹´è½»åŒ–é—®é¢˜ï¼Œç”Ÿæˆé¢éƒ¨å›¾åƒçš„è¿‡å»æˆ–æœªæ¥å›¾åƒï¼Œé€šè¿‡å¢åŠ å¹´é¾„ç›¸å…³çš„å˜åŒ–ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³FADINGï¼Œåˆ©ç”¨è¯­è¨€-å›¾åƒæ‰©æ•£æ¨¡å‹çš„ä¸°å¯Œå‰æï¼Œæ¥è§£å†³é¢éƒ¨å¹´è½»åŒ–é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆç‰¹åŒ–ä¸€ä¸ªé¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶æ›´é€‚åº”é¢éƒ¨å¹´è½»åŒ–ä»»åŠ¡ï¼Œç„¶åå¯¹è¾“å…¥å›¾åƒè¿›è¡Œå€’æ•£ã€è·å–ä¼˜åŒ–çš„Nullå™ªéŸ³åµŒå…¥ï¼Œæœ€åé€šè¿‡æ–‡æœ¬å¼•å¯¼çš„åœ°æ–¹å¹´è½»ç¼–è¾‘ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å¹´è½»ç²¾åº¦ã€ç‰¹å¾ä¿ç•™å’Œå¹´è½»è´¨é‡ç­‰æ–¹é¢å…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
In this paper, we address the problem of face aging: generating past or future facial images by incorporating age-related changes to the given face. Previous aging methods rely solely on human facial image datasets and are thus constrained by their inherent scale and bias. This restricts their application to a limited generatable age range and the inability to handle large age gaps. We propose FADING, a novel approach to address Face Aging via DIffusion-based editiNG. We go beyond existing methods by leveraging the rich prior of large-scale language-image diffusion models. First, we specialize a pre-trained diffusion model for the task of face age editing by using an age-aware fine-tuning scheme. Next, we invert the input image to latent noise and obtain optimized null text embeddings. Finally, we perform text-guided local age editing via attention control. The quantitative and qualitative analyses demonstrate that our method outperforms existing approaches with respect to aging accuracy, attribute preservation, and aging quality.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†äººè„¸å¹´é¾„åŒ–é—®é¢˜ï¼šé€šè¿‡ incorporating å¹´é¾„ç›¸å…³å˜åŒ–æ¥ç”Ÿæˆè¿‡å»æˆ–æœªæ¥çš„è„¸éƒ¨å›¾åƒã€‚å…ˆå‰çš„å¹´é¾„æ–¹æ³•ä»…ä»…åŸºäºäººç±»è„¸éƒ¨å›¾åƒé›†åˆï¼Œå› æ­¤å—åˆ°å…¶å†…ç½®çš„å°ºåº¦å’Œåè§çš„é™åˆ¶ï¼Œåªèƒ½ç”Ÿæˆæœ‰é™çš„å¹´é¾„èŒƒå›´å†…çš„å›¾åƒï¼Œå¹¶ä¸”æ— æ³•å¤„ç†å¤§çš„å¹´é¾„å·®ã€‚æˆ‘ä»¬æå‡ºäº† FADINGï¼Œä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£å†³äººè„¸å¹´é¾„åŒ–é—®é¢˜ï¼Œé€šè¿‡è¯­è¨€-å›¾åƒæ‰©æ•£æ¨¡å‹çš„è´¨é‡ä¸°å¯Œçš„å…ˆå¤©çŸ¥è¯†æ¥è¶…è¶Šç°æœ‰æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç‰¹åŒ–äº†é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶æ›´é€‚åº”äººè„¸å¹´é¾„ç¼–è¾‘ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨å¹´é¾„æ„è¯† fine-tuning æ–¹æ¡ˆè¿›è¡Œç‰¹åŒ–ã€‚æ¥ç€ï¼Œæˆ‘ä»¬å°†è¾“å…¥å›¾åƒåè½¬ä¸ºå¹²æ‰°å™ª embeddingï¼Œå¹¶è·å¾—ä¼˜åŒ–çš„ null text embeddingã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ–‡æœ¬å¼•å¯¼çš„æœ¬åœ°å¹´é¾„ç¼–è¾‘æ¥è¿›è¡Œæ§åˆ¶ã€‚é‡åŒ–å’Œè´¨é‡åˆ†æè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹´é¾„å‡†ç¡®æ€§ã€ç‰¹å¾ä¿æŒå’Œå¹´é¾„è´¨é‡ç­‰æ–¹é¢éƒ½è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Uncovering-the-effects-of-model-initialization-on-deep-model-generalization-A-study-with-adult-and-pediatric-Chest-X-ray-images"><a href="#Uncovering-the-effects-of-model-initialization-on-deep-model-generalization-A-study-with-adult-and-pediatric-Chest-X-ray-images" class="headerlink" title="Uncovering the effects of model initialization on deep model generalization: A study with adult and pediatric Chest X-ray images"></a>Uncovering the effects of model initialization on deep model generalization: A study with adult and pediatric Chest X-ray images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11318">http://arxiv.org/abs/2309.11318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sivaramakrishnan Rajaraman, Ghada Zamzmi, Feng Yang, Zhaohui Liang, Zhiyun Xue, Sameer Antani<br>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯é æ€§ã€‚è€Œå…³äºåŒ»ç–—å›¾åƒï¼ˆç‰¹åˆ«æ˜¯èƒ¸éƒ¨Xå°„çº¿å›¾åƒï¼‰çš„å½±å“åˆ™æ›´å°‘äº†è§£ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†ä¸‰ç§æ·±åº¦æ¨¡å‹åˆå§‹åŒ–æŠ€æœ¯ï¼šå†·å¯åŠ¨ã€æš–å¯åŠ¨å’Œç¼©å°å’Œæ‰°åŠ¨startï¼Œå¯¹æˆäººå’Œå„¿ç«¥ä¸¤ä¸ªäººå£è¿›è¡Œäº†è¯„ä¼°ã€‚methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸‰ç§æ·±åº¦æ¨¡å‹åˆå§‹åŒ–æŠ€æœ¯ï¼šå†·å¯åŠ¨ã€æš–å¯åŠ¨å’Œç¼©å°å’Œæ‰°åŠ¨startã€‚è¿™äº›æŠ€æœ¯åœ¨åŒ»ç–—å›¾åƒçš„æ‰¹å¤„ç†è®­ç»ƒåœºæ™¯ä¸‹è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥é€‚åº”å®é™…ä¸–ç•Œä¸­æ•°æ®ä¸æ–­æ¥ä¸´å’Œæ¨¡å‹æ›´æ–°çš„éœ€æ±‚ã€‚results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–çš„æ¨¡å‹åœ¨æˆäººå’Œå„¿ç«¥ä¸¤ä¸ªäººå£ä¸­çš„æ€»ä½“åŒ–èƒ½åŠ›è¾ƒé«˜ï¼Œè¶…è¿‡éšæœºåˆå§‹åŒ–çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼ŒImageNeté¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸åŒè®­ç»ƒåœºæ™¯ä¸‹çš„å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•ä¸­éƒ½è¡¨ç°äº†ç¨³å®šçš„æ€§èƒ½ã€‚weightçº§ ensembleæ–¹æ³•ä¹Ÿæ˜¾ç¤ºäº†æ˜æ˜¾çš„æé«˜ï¼ˆp&lt;0.05ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ‹è¯•é˜¶æ®µã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶å¼ºè°ƒäº†ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–çš„å¥½å¤„ï¼Œå°¤å…¶æ˜¯åœ¨weightçº§ ensembleæ–¹æ³•ä¸‹ï¼Œä¸ºåˆ›å»ºå¯é å’Œæ€»ä½“åŒ–çš„æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚<details>
<summary>Abstract</summary>
Model initialization techniques are vital for improving the performance and reliability of deep learning models in medical computer vision applications. While much literature exists on non-medical images, the impacts on medical images, particularly chest X-rays (CXRs) are less understood. Addressing this gap, our study explores three deep model initialization techniques: Cold-start, Warm-start, and Shrink and Perturb start, focusing on adult and pediatric populations. We specifically focus on scenarios with periodically arriving data for training, thereby embracing the real-world scenarios of ongoing data influx and the need for model updates. We evaluate these models for generalizability against external adult and pediatric CXR datasets. We also propose novel ensemble methods: F-score-weighted Sequential Least-Squares Quadratic Programming (F-SLSQP) and Attention-Guided Ensembles with Learnable Fuzzy Softmax to aggregate weight parameters from multiple models to capitalize on their collective knowledge and complementary representations. We perform statistical significance tests with 95% confidence intervals and p-values to analyze model performance. Our evaluations indicate models initialized with ImageNet-pre-trained weights demonstrate superior generalizability over randomly initialized counterparts, contradicting some findings for non-medical images. Notably, ImageNet-pretrained models exhibit consistent performance during internal and external testing across different training scenarios. Weight-level ensembles of these models show significantly higher recall (p<0.05) during testing compared to individual models. Thus, our study accentuates the benefits of ImageNet-pretrained weight initialization, especially when used with weight-level ensembles, for creating robust and generalizable deep learning solutions.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ¨¡å‹åˆå§‹åŒ–æŠ€æœ¯å¯¹æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨åŒ»ç–—è®¡ç®—æœºè§†è§‰åº”ç”¨ä¸­çš„æ€§èƒ½å’Œå¯é æ€§æœ‰ç€é‡è¦çš„å½±å“ã€‚è™½ç„¶å…³äºéåŒ»å­¦å›¾åƒçš„ç ”ç©¶å·²ç»å……åˆ†ï¼Œä½†å¯¹åŒ»å­¦å›¾åƒï¼Œç‰¹åˆ«æ˜¯èƒ¸éƒ¨Xå°„å½±ï¼ˆCXRï¼‰çš„å½±å“è¿˜æœªå¾—åˆ°å……åˆ†äº†è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¢è®¨äº†ä¸‰ç§æ·±åº¦æ¨¡å‹åˆå§‹åŒ–æŠ€æœ¯ï¼šå†·å¯åŠ¨ã€æ¸©å¯åŠ¨å’Œç¼©æ”¾å’Œæ‰°åŠ¨å¯åŠ¨ï¼Œå¯¹äºæˆäººå’Œå„¿ç«¥ä¸¤ä¸ªäººå£è¿›è¡Œäº†ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒåœ¨è¿›è¡Œè®­ç»ƒæ—¶periodically arriving dataçš„æƒ…å†µä¸‹ï¼Œä»¥æ»¡è¶³å®é™…ä¸–ç•Œä¸­æ•°æ®ä¸æ–­æ¥ä¸´å’Œæ¨¡å‹æ›´æ–°çš„éœ€æ±‚ã€‚æˆ‘ä»¬ä½¿ç”¨F-score-weighted Sequential Least-Squares Quadratic Programmingï¼ˆF-SLSQPï¼‰å’ŒAttention-Guided Ensembles with Learnable Fuzzy Softmaxæ¥æƒè¡¡å¤šä¸ªæ¨¡å‹çš„å‚æ•°ï¼Œä»¥ä¾¿å……åˆ†åˆ©ç”¨å®ƒä»¬çš„å…±åŒçŸ¥è¯†å’Œè¡¥å……è¡¨ç¤ºã€‚æˆ‘ä»¬å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œäº†ç»Ÿè®¡å­¦ significativity æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–çš„æ¨¡å‹åœ¨æ€»ä½“æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„è®­ç»ƒåœºæ™¯ä¸‹ä¿æŒäº†ä¸€è‡´çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œå¯¹è¿™äº›æ¨¡å‹è¿›è¡Œæƒé‡çº§åˆ«çš„åˆå¹¶ä¹Ÿè¡¨ç°å‡ºäº†æ˜æ˜¾çš„æå‡ï¼ˆp<0.05ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¯æ˜äº†ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡åˆå§‹åŒ–çš„æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨æƒé‡çº§åˆ«çš„åˆå¹¶ä¸‹ï¼Œå¯ä»¥åˆ›å»ºå¯é å’Œæ€»ä½“æ€§èƒ½ä¼˜ç§€çš„æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Generalizing-Across-Domains-in-Diabetic-Retinopathy-via-Variational-Autoencoders"><a href="#Generalizing-Across-Domains-in-Diabetic-Retinopathy-via-Variational-Autoencoders" class="headerlink" title="Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders"></a>Generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11301">http://arxiv.org/abs/2309.11301</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sharonchokuwa/VAE-DG">https://github.com/sharonchokuwa/VAE-DG</a></li>
<li>paper_authors: Sharon Chokuwa, Muhammad H. Khan</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨Variational Autoencoderï¼ˆVAï¼‰æ˜¯å¦èƒ½å¤Ÿå®ç°ç±»å‹æ™®éåŒ–ï¼Œä»¥å¯¹æŠ—DRé¢„æµ‹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ñ–ä¸­çš„é¢†åŸŸè½¬ç§»ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨Variational Autoencoderï¼ˆVAï¼‰æ¥åˆ†æçœ¼åº•ç…§ç‰‡çš„latent spaceï¼Œä»¥è·å¾—ä¸€ä¸ªæ›´åŠ çµæ´»å’Œé€‚åº”çš„é¢†åŸŸä¸å¯¹ç§°è¡¨ç¤ºï¼Œä»¥åº”å¯¹DRæ•°æ®é›†ä¸­çš„é¢†åŸŸè½¬ç§»ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡æ˜¾ç¤ºï¼Œä½¿ç”¨VAçš„ç®€å•æ–¹æ³•å¯ä»¥è¶…è¶Šç°æœ‰çš„å·é™…é¡¶å¯¹åº”æ–¹æ³•ï¼Œå¹¶åœ¨å…¬å¼€å¯ç”¨çš„æ•°æ®é›†ä¸Šè¾¾åˆ°æ›´é«˜çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœæ˜¾ç¤ºï¼Œç®€å•çš„æ–¹æ³•å¯ä»¥åœ¨åŒ»ç–—å›¾åƒé¢†åŸŸä¸­å®ç°æ›´å¥½çš„é¢†åŸŸæ™®éåŒ–ï¼Œè€Œä¸æ˜¯ä»…ä»…é èµ–é«˜åº¦å¤æ‚çš„æŠ€æœ¯ã€‚<details>
<summary>Abstract</summary>
Domain generalization for Diabetic Retinopathy (DR) classification allows a model to adeptly classify retinal images from previously unseen domains with various imaging conditions and patient demographics, thereby enhancing its applicability in a wide range of clinical environments. In this study, we explore the inherent capacity of variational autoencoders to disentangle the latent space of fundus images, with an aim to obtain a more robust and adaptable domain-invariant representation that effectively tackles the domain shift encountered in DR datasets. Despite the simplicity of our approach, we explore the efficacy of this classical method and demonstrate its ability to outperform contemporary state-of-the-art approaches for this task using publicly available datasets. Our findings challenge the prevailing assumption that highly sophisticated methods for DR classification are inherently superior for domain generalization. This highlights the importance of considering simple methods and adapting them to the challenging task of generalizing medical images, rather than solely relying on advanced techniques.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŸŸ generale åŒ– Ğ´Ğ»Ñ è¯Šæ–­ç³–å°¿ç—… Retinopathy (DR) è®©æ¨¡å‹èƒ½å¤Ÿefficacious åˆ†ç±» retinal å›¾åƒä»ä»¥å‰æœªç»è§åˆ°çš„åŸŸä¸ä¸åŒçš„æ‹æ‘„æ¡ä»¶å’Œæ‚£è€…ç‰¹å¾ä¸‹ï¼Œä»è€Œæé«˜å…¶åœ¨å„ç§ä¸´åºŠç¯å¢ƒä¸­çš„åº”ç”¨æ€§ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å˜é‡è‡ªåŠ¨ç¼–ç å™¨å†…ç½®çš„latent spaceçš„åˆ†è§£èƒ½åŠ›ï¼Œä»¥è·å¾—æ›´åŠ ç¨³å®šå’Œé€‚åº”çš„åŸŸä¸å¯¹ç§°è¡¨ç¤ºï¼Œä»¥æ›´å¥½åœ°è§£å†³DRæ•°æ®é›†ä¸­çš„åŸŸè½¬ç§»é—®é¢˜ã€‚è™½ç„¶æˆ‘ä»¬çš„æ–¹æ³•ç®€å•ï¼Œä½†æˆ‘ä»¬å‘ç°è¿™ç§ç»å…¸æ–¹æ³•çš„æ•ˆæœå¯ä»¥è¶…è¿‡å½“ä»Šçš„çŠ¶æ€å¯¹DRåˆ†ç±»ä»»åŠ¡çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„å‘ç°è¯æ˜äº†ä¸è¦ä»…ä»…ä¾èµ–äºé«˜åº¦å¤æ‚çš„æ–¹æ³•ï¼Œè€Œæ˜¯åº”è¯¥è€ƒè™‘ç®€å•çš„æ–¹æ³•å¹¶é€‚åº”å®ƒä»¬æ¥æ™®éåŒ–åŒ»ç–—å›¾åƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Language-driven-Object-Fusion-into-Neural-Radiance-Fields-with-Pose-Conditioned-Dataset-Updates"><a href="#Language-driven-Object-Fusion-into-Neural-Radiance-Fields-with-Pose-Conditioned-Dataset-Updates" class="headerlink" title="Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates"></a>Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11281">http://arxiv.org/abs/2309.11281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kcshum/pose-conditioned-NeRF-object-fusion">https://github.com/kcshum/pose-conditioned-NeRF-object-fusion</a></li>
<li>paper_authors: Ka Chun Shum, Jaeyeon Kim, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung</li>
<li>for: è¿™ paper æ˜¯ç”¨äºæè¿°ä¸€ç§åŸºäºç¥ç»è¾å°„åœºçš„å›¾åƒæ¸²æŸ“æ–¹æ³•ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å¤šè§†å›¾ä¸€è‡´çš„å›¾åƒã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº†ä¸€ç§åŸºäºæ–‡æœ¬æ‰©å±•çš„æ–¹æ³•æ¥å®ç°å¯¹ neural radiance field ä¸­çš„å¯¹è±¡çš„æ“ä½œï¼ŒåŒ…æ‹¬æ’å…¥æ–°èƒŒæ™¯å’Œ removing å·²æœ‰å¯¹è±¡ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ paper çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ¸²æŸ“å›¾åƒï¼Œå¹¶ä¸”åœ¨ 3D é‡å»ºå’Œç¥ç»è¾å°„åœºèåˆæ–¹é¢è¶…è¿‡äº†ç°æœ‰çš„æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Neural radiance field is an emerging rendering method that generates high-quality multi-view consistent images from a neural scene representation and volume rendering. Although neural radiance field-based techniques are robust for scene reconstruction, their ability to add or remove objects remains limited. This paper proposes a new language-driven approach for object manipulation with neural radiance fields through dataset updates. Specifically, to insert a new foreground object represented by a set of multi-view images into a background radiance field, we use a text-to-image diffusion model to learn and generate combined images that fuse the object of interest into the given background across views. These combined images are then used for refining the background radiance field so that we can render view-consistent images containing both the object and the background. To ensure view consistency, we propose a dataset updates strategy that prioritizes radiance field training with camera views close to the already-trained views prior to propagating the training to remaining views. We show that under the same dataset updates strategy, we can easily adapt our method for object insertion using data from text-to-3D models as well as object removal. Experimental results show that our method generates photorealistic images of the edited scenes, and outperforms state-of-the-art methods in 3D reconstruction and neural radiance field blending.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç¥ç»è¾å°„åœºæ˜¯ä¸€ç§å‡ºç°åœ¨æ¸²æŸ“æ–¹æ³•ä¸­çš„æ–°æŠ€æœ¯ï¼Œå®ƒå¯ä»¥ç”Ÿæˆé«˜è´¨é‡ã€å¤šè§†å›¾ä¸€è‡´çš„å›¾åƒä»ç¥ç»åœºæ™¯è¡¨ç¤ºå’Œä½“ç§¯æ¸²æŸ“ã€‚ although neural radiance field-based techniques are robust for scene reconstruction, their ability to add or remove objects remains limited. This paper proposes a new language-driven approach for object manipulation with neural radiance fields through dataset updates. Specifically, to insert a new foreground object represented by a set of multi-view images into a background radiance field, we use a text-to-image diffusion model to learn and generate combined images that fuse the object of interest into the given background across views. These combined images are then used for refining the background radiance field so that we can render view-consistent images containing both the object and the background. To ensure view consistency, we propose a dataset updates strategy that prioritizes radiance field training with camera views close to the already-trained views prior to propagating the training to remaining views. We show that under the same dataset updates strategy, we can easily adapt our method for object insertion using data from text-to-3D models as well as object removal. Experimental results show that our method generates photorealistic images of the edited scenes, and outperforms state-of-the-art methods in 3D reconstruction and neural radiance field blending.
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-Time-Neural-Video-Codec-for-Cross-Platform-Application-Using-Calibration-Information"><a href="#Towards-Real-Time-Neural-Video-Codec-for-Cross-Platform-Application-Using-Calibration-Information" class="headerlink" title="Towards Real-Time Neural Video Codec for Cross-Platform Application Using Calibration Information"></a>Towards Real-Time Neural Video Codec for Cross-Platform Application Using Calibration Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11276">http://arxiv.org/abs/2309.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuan Tian, Yonghang Guan, Jinxi Xiang, Jun Zhang, Xiao Han, Wei Yang</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§å®æ—¶è·¨å¹³å°ç¥ç»è§†é¢‘ç¼–ç å™¨ï¼Œä»¥è§£å†³ç°æœ‰ç¥ç»ç½‘ç»œç¼–ç å™¨åœ¨å®é™…åº”ç”¨ä¸­çš„ä¸¤å¤§æŒ‘æˆ˜ã€‚</li>
<li>methods: ä½œè€…ä½¿ç”¨äº†ä¸€ç§åè°ƒä¼ è¾“ç³»ç»Ÿæ¥ä¿è¯ç¼–ç å’Œè§£ç è¿‡ç¨‹ä¸­çš„ç»Ÿä¸€åŒ–é‡åŒ–å‚æ•°ï¼Œå¹¶ä½¿ç”¨äº†ä¸€ç§å°ºåº¦çº¦æŸæ¥ä¿®å¤åˆ†å¸ƒ Entropy å‚æ•°çš„ä¸å‡åŒ€æ€§ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½œè€…çš„æ¨¡å‹å¯ä»¥åœ¨ NVIDIA RTX 2080 GPU ä¸Šå®ç° 25 FPS çš„è§£ç é€Ÿåº¦ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å¦ä¸€ä¸ªå¹³å°ä¸Šç¼–ç çš„ 720P è§†é¢‘è¿›è¡Œå®æ—¶è§£ç ã€‚æ­¤å¤–ï¼Œå®æ—¶æ¨¡å‹å¯ä»¥æä¾›æœ€é«˜ 24.2% BD-rate æ”¹å–„ï¼Œç›¸æ¯” anchor H.265ã€‚<details>
<summary>Abstract</summary>
The state-of-the-art neural video codecs have outperformed the most sophisticated traditional codecs in terms of RD performance in certain cases. However, utilizing them for practical applications is still challenging for two major reasons. 1) Cross-platform computational errors resulting from floating point operations can lead to inaccurate decoding of the bitstream. 2) The high computational complexity of the encoding and decoding process poses a challenge in achieving real-time performance. In this paper, we propose a real-time cross-platform neural video codec, which is capable of efficiently decoding of 720P video bitstream from other encoding platforms on a consumer-grade GPU. First, to solve the problem of inconsistency of codec caused by the uncertainty of floating point calculations across platforms, we design a calibration transmitting system to guarantee the consistent quantization of entropy parameters between the encoding and decoding stages. The parameters that may have transboundary quantization between encoding and decoding are identified in the encoding stage, and their coordinates will be delivered by auxiliary transmitted bitstream. By doing so, these inconsistent parameters can be processed properly in the decoding stage. Furthermore, to reduce the bitrate of the auxiliary bitstream, we rectify the distribution of entropy parameters using a piecewise Gaussian constraint. Second, to match the computational limitations on the decoding side for real-time video codec, we design a lightweight model. A series of efficiency techniques enable our model to achieve 25 FPS decoding speed on NVIDIA RTX 2080 GPU. Experimental results demonstrate that our model can achieve real-time decoding of 720P videos while encoding on another platform. Furthermore, the real-time model brings up to a maximum of 24.2\% BD-rate improvement from the perspective of PSNR with the anchor H.265.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£ç¥ç»è§†é¢‘ç¼–ç å™¨åœ¨æŸäº›æƒ…å†µä¸‹å·²ç»è¶…è¶Šäº†æœ€å¤æ‚çš„ä¼ ç»Ÿç¼–ç å™¨ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶å­˜åœ¨ä¸¤å¤§æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œç”±æµ®ç‚¹è¿ç®—å¼•èµ·çš„å¹³å°é—´è®¡ç®—é”™è¯¯å¯èƒ½å¯¼è‡´é”™è¯¯è§£ç bitstreamã€‚å…¶æ¬¡ï¼Œç¼–ç å’Œè§£ç è¿‡ç¨‹çš„è®¡ç®—å¤æ‚æ€§ä½¿å¾—å®æ—¶æ€§å¾ˆéš¾å®ç°ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®æ—¶å¯é çš„cross-platformç¥ç»è§†é¢‘ç¼–ç å™¨ï¼Œå¯ä»¥åœ¨consumer-grade GPUä¸Šé«˜é€Ÿè§£ç 720Pè§†é¢‘bitstreamã€‚é¦–å…ˆï¼Œä¸ºäº†è§£å†³ç”±ä¸ç¡®å®šçš„æµ®ç‚¹è®¡ç®—æ‰€å¼•èµ·çš„ç¼–ç å™¨ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œæˆ‘ä»¬è®¾è®¡äº†å¡åˆ©ibration transmittingç³»ç»Ÿï¼Œä»¥ garantuee the consistent quantization of entropy parameters between the encoding and decoding stagesã€‚åœ¨ç¼–ç é˜¶æ®µï¼Œæˆ‘ä»¬æ ‡è¯†å‡ºå¯èƒ½å­˜åœ¨è·¨ç•Œé‡è¯‘å‚æ•°çš„é—®é¢˜ï¼Œå¹¶å°†å…¶åæ ‡ä¼ è¾“ç»™ä¸‹æ¸¸ç¼–ç å™¨ã€‚è¿™æ ·ï¼Œåœ¨è§£ç é˜¶æ®µå¯ä»¥æ­£ç¡®å¤„ç†è¿™äº›ä¸ä¸€è‡´çš„å‚æ•°ã€‚å…¶æ¬¡ï¼Œä¸ºäº†é™ä½auxiliary bitstreamçš„æ¯”ç‰¹ç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨piecewise Gaussian constraintæ¥ä¿®æ­£å‚æ•°çš„åˆ†å¸ƒã€‚å…¶æ¬¡ï¼Œä¸ºäº†åœ¨è§£ç å™¨ç«¯å®ç°å®æ—¶æ€§ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§è½»é‡çº§æ¨¡å‹ã€‚æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç³»åˆ—çš„æ•ˆç‡æŠ€å·§ï¼Œä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹åœ¨NVIDIA RTX 2080 GPUä¸Šå¯ä»¥è¾¾åˆ°25å¸§/ç§’çš„è§£ç é€Ÿåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å®æ—¶è§£ç 720Pè§†é¢‘ï¼Œè€Œencoded on another platformã€‚æ­¤å¤–ï¼Œå®æ—¶æ¨¡å‹å¯ä»¥æé«˜æœ€å¤š24.2%çš„BD-rateï¼Œç›¸æ¯” anchor H.265ã€‚
</details></li>
</ul>
<hr>
<h2 id="StructChart-Perception-Structuring-Reasoning-for-Visual-Chart-Understanding"><a href="#StructChart-Perception-Structuring-Reasoning-for-Visual-Chart-Understanding" class="headerlink" title="StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding"></a>StructChart: Perception, Structuring, Reasoning for Visual Chart Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11268">http://arxiv.org/abs/2309.11268</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renqiu Xia, Bo Zhang, Haoyang Peng, Ning Liao, Peng Ye, Botian Shi, Junchi Yan, Yu Qiao</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯å»ºç«‹ä¸€ç§ç»Ÿä¸€çš„å­¦ä¹ æ¨¡å¼ï¼Œèƒ½å¤ŸåŒæ—¶å®Œæˆå›¾è¡¨æ„ŸçŸ¥å’Œç†è§£ä»»åŠ¡ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åä¸ºStructured Triplet Representationsï¼ˆSTRï¼‰çš„æ–°çš„è¡¨ç¤ºæ–¹å¼ï¼Œä»¥åŠä¸€ç§åä¸ºStructuring Chart-oriented Representation Metricï¼ˆSCRMï¼‰çš„è¡¨ç°è¯„ä»·æ–¹æ³•ï¼Œæ¥æé«˜å›¾è¡¨ç†è§£èƒ½åŠ›ã€‚</li>
<li>results: ç»è¿‡å¹¿æ³›çš„å®éªŒï¼Œè®ºæ–‡å‘ç°è¿™ç§ç»Ÿä¸€çš„å­¦ä¹ æ¨¡å¼èƒ½å¤Ÿåœ¨ä¸åŒçš„å›¾è¡¨ä»»åŠ¡ä¸Šè¾¾åˆ°æé«˜çš„è¡¨ç°ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ‰©å¤§å›¾è¡¨æ•°æ®é›†ï¼Œä»¥æé«˜å›¾è¡¨ç†è§£èƒ½åŠ›ã€‚<details>
<summary>Abstract</summary>
Charts are common in literature across different scientific fields, conveying rich information easily accessible to readers. Current chart-related tasks focus on either chart perception which refers to extracting information from the visual charts, or performing reasoning given the extracted data, e.g. in a tabular form. In this paper, we aim to establish a unified and label-efficient learning paradigm for joint perception and reasoning tasks, which can be generally applicable to different downstream tasks, beyond the question-answering task as specifically studied in peer works. Specifically, StructChart first reformulates the chart information from the popular tubular form (specifically linearized CSV) to the proposed Structured Triplet Representations (STR), which is more friendly for reducing the task gap between chart perception and reasoning due to the employed structured information extraction for charts. We then propose a Structuring Chart-oriented Representation Metric (SCRM) to quantitatively evaluate the performance for the chart perception task. To enrich the dataset for training, we further explore the possibility of leveraging the Large Language Model (LLM), enhancing the chart diversity in terms of both chart visual style and its statistical information. Extensive experiments are conducted on various chart-related tasks, demonstrating the effectiveness and promising potential for a unified chart perception-reasoning paradigm to push the frontier of chart understanding.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›¾è¡¨æ˜¯ç§‘å­¦æ–‡çŒ®ä¸­å¸¸è§çš„æ•°æ®å¯è§†åŒ–æ–¹å¼ï¼Œèƒ½å¤Ÿå¿«é€Ÿä¼ é€’ä¸°å¯Œçš„ä¿¡æ¯ç»™è¯»è€…ã€‚ç›®å‰çš„å›¾è¡¨ç›¸å…³ä»»åŠ¡ä¸»è¦é›†ä¸­åœ¨å›¾è¡¨è¯†åˆ«å’ŒåŸºäºEXTRACTEDæ•°æ®çš„é€»è¾‘æ€ç»´ä¸¤ä¸ªæ–¹é¢ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å»ºç«‹ä¸€ç§ç»Ÿä¸€çš„å’Œæ ‡ç­¾æœ‰æ•ˆçš„å­¦ä¹  Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼ï¼Œèƒ½å¤Ÿæ™®é€‚åº”ç”¨äºä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œè€Œä¸ä»…ä»…æ˜¯ç‰¹å®šçš„é—®ç­”ä»»åŠ¡ï¼Œå¦‚åœ¨åŒç­‰ä½œè€…çš„è®ºæ–‡ä¸­æ‰€ç ”ç©¶ã€‚ Specifically, StructCharté¦–å…ˆå°†æµè¡Œçš„ tubular å½¢å¼ï¼ˆå…·ä½“æ˜¯çº¿æ€§åŒ– CSVï¼‰ä¸­çš„å›¾è¡¨ä¿¡æ¯é‡æ–°è¡¨è¿°ä¸ºæˆ‘ä»¬æå‡ºçš„ç»“æ„åŒ– triplet è¡¨ç¤ºï¼ˆSTRï¼‰ï¼Œè¿™ç§ç»“æ„åŒ–ä¿¡æ¯æå–æŠ€æœ¯ä½¿å¾—å›¾è¡¨è¯†åˆ«å’Œé€»è¾‘æ€ç»´ä¹‹é—´çš„ä»»åŠ¡å·®è·æ›´å°ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ Chart-oriented Representation Metricï¼ˆSCRMï¼‰æ¥è¡¡é‡å›¾è¡¨è¯†åˆ«ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºäº†è®©è®­ç»ƒé›†æ›´åŠ ä¸°å¯Œï¼Œæˆ‘ä»¬è¿˜æ¢ç´¢äº†ä½¿ç”¨ Large Language Modelï¼ˆLLMï¼‰ï¼Œé€šè¿‡æ‰©å±•å›¾è¡¨çš„è§†è§‰é£æ ¼å’Œç»Ÿè®¡ä¿¡æ¯ï¼Œæé«˜å›¾è¡¨çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„å›¾è¡¨ç›¸å…³ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†è¿™ç§ç»Ÿä¸€çš„å›¾è¡¨è¯†åˆ«å’Œé€»è¾‘æ€ç»´æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåœ¨çš„å‰iersã€‚
</details></li>
</ul>
<hr>
<h2 id="From-Classification-to-Segmentation-with-Explainable-AI-A-Study-on-Crack-Detection-and-Growth-Monitoring"><a href="#From-Classification-to-Segmentation-with-Explainable-AI-A-Study-on-Crack-Detection-and-Growth-Monitoring" class="headerlink" title="From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring"></a>From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11267">http://arxiv.org/abs/2309.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Forest, Hugo Porta, Devis Tuia, Olga Fink</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ automatization åŸºç¡€è®¾æ–½ä¸­çš„è¡¨é¢è£‚éš™ç›‘æµ‹ï¼Œä»¥å®ç°ç»“æ„å¥åº·ç›‘æµ‹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä½†éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®è¿›è¡Œè¶…vised è®­ç»ƒã€‚è€Œonce a crack is detectedï¼Œ monitoring its severity é€šå¸¸éœ€è¦ç²¾å‡†çš„åƒç´ çº§åˆ«åˆ†å‰²ã€‚ç„¶è€Œï¼Œå¯¹äºæ¯ä¸ªå›¾åƒè¿›è¡Œåƒç´ çº§åˆ«åˆ†å‰²çš„æ ‡æ³¨æ˜¯åŠ³åŠ¨å¯†é›†çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬ç ”ç©¶æè®®ä½¿ç”¨å¯è§£é‡Šçš„äººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰æ–¹æ³•ï¼Œä»ç±»ifierçš„è§£é‡Šä¸­ derivate åˆ†å‰²ï¼Œåªéœ€è¦å¼±å‹å›¾åƒçº§åˆ«çš„ç›‘ç£ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨XAIæ–¹æ³•å¯ä»¥ç”Ÿæˆæœ‰æ„ä¹‰çš„åˆ†å‰²é¢æ©æ¨¡ï¼Œå³ä½¿æ— éœ€å¤§é‡çš„æ ‡æ³¨æ•°æ®ã€‚Results reveal that while the resulting segmentation masks may exhibit lower quality than those produced by supervised methods, they remain meaningful and enable severity monitoring, thus reducing substantial labeling costs.<details>
<summary>Abstract</summary>
Monitoring surface cracks in infrastructure is crucial for structural health monitoring. Automatic visual inspection offers an effective solution, especially in hard-to-reach areas. Machine learning approaches have proven their effectiveness but typically require large annotated datasets for supervised training. Once a crack is detected, monitoring its severity often demands precise segmentation of the damage. However, pixel-level annotation of images for segmentation is labor-intensive. To mitigate this cost, one can leverage explainable artificial intelligence (XAI) to derive segmentations from the explanations of a classifier, requiring only weak image-level supervision. This paper proposes applying this methodology to segment and monitor surface cracks. We evaluate the performance of various XAI methods and examine how this approach facilitates severity quantification and growth monitoring. Results reveal that while the resulting segmentation masks may exhibit lower quality than those produced by supervised methods, they remain meaningful and enable severity monitoring, thus reducing substantial labeling costs.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç›‘æµ‹åŸºç¡€è®¾æ–½è¡¨é¢è£‚éš™æ˜¯ç»“æ„å¥åº·ç›‘æµ‹çš„å…³é”®ã€‚è‡ªåŠ¨è§†è§æ£€æµ‹æä¾›äº†ä¸€ä¸ªæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å›°éš¾ accessed çš„åœ°æ–¹ã€‚æœºå™¨å­¦ä¹ æ–¹æ³•å·²ç»è¯æ˜å…¶æ•ˆæœï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡çš„æ³¨é‡ŠåŒ–æ•°æ®é›† Ğ´Ğ»Ñsupervised è®­ç»ƒã€‚ä¸€æ—¦è£‚éš™è¢«æ£€æµ‹å‡ºæ¥ï¼Œåˆ™éœ€è¦ç²¾ç¡®åœ°åˆ†ç±»æŸå®³ã€‚ç„¶è€Œï¼Œåƒç´ çº§æ³¨é‡Šå›¾åƒ Ğ´Ğ»Ñåˆ†ç±»æ˜¯æ—¶é—´consumingã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™ç¯‡è®ºæ–‡æè®®ä½¿ç”¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ derive åˆ†ç±»å™¨çš„è§£é‡Šï¼Œåªéœ€å¼±å‹å›¾åƒçº§æŒ‡å¯¼ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©å®ç°è£‚éš™åˆ†ç±»å’Œä¸¥é‡æ€§è¯„ä¼°ï¼Œå¹¶ä¸”å¯ä»¥é™ä½å¤§é‡çš„æ ‡æ³¨æˆæœ¬ã€‚æˆ‘ä»¬è¯„ä¼°äº†ä¸åŒçš„XAIæ–¹æ³•çš„æ€§èƒ½ï¼Œå¹¶ç ”ç©¶äº†è¿™ç§æ–¹æ³•æ˜¯å¦å¯ä»¥å®ç°ä¸¥é‡æ€§è¯„ä¼°å’Œç”Ÿé•¿ç›‘æµ‹ã€‚ç»“æœè¡¨æ˜ï¼Œå°½ç®¡ç”Ÿæˆçš„åˆ†ç±»å™¨åˆ†å‰²é¢å¯èƒ½ä¸å¦‚supervised æ–¹æ³•ç”Ÿæˆçš„åˆ†å‰²é¢è´¨é‡é«˜ï¼Œä½†å®ƒä»¬ä»ç„¶æœ‰æ„ä¹‰ï¼Œå¹¶ä¸”å¯ä»¥å®ç°ä¸¥é‡æ€§è¯„ä¼°å’Œç”Ÿé•¿ç›‘æµ‹ï¼Œä»è€Œå‡å°‘æ ‡æ³¨æˆæœ¬ã€‚
</details></li>
</ul>
<hr>
<h2 id="TwinTex-Geometry-aware-Texture-Generation-for-Abstracted-3D-Architectural-Models"><a href="#TwinTex-Geometry-aware-Texture-Generation-for-Abstracted-3D-Architectural-Models" class="headerlink" title="TwinTex: Geometry-aware Texture Generation for Abstracted 3D Architectural Models"></a>TwinTex: Geometry-aware Texture Generation for Abstracted 3D Architectural Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11258">http://arxiv.org/abs/2309.11258</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ligo04/TwinTex">https://github.com/Ligo04/TwinTex</a></li>
<li>paper_authors: Weidan Xiong, Hongqian Zhang, Botao Peng, Ziyu Hu, Yongli Wu, Jianwei Guo, Hui Huang</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†ç”Ÿæˆä¸€ä¸ªç²¾ç»†çš„åŸå¸‚ Digital Twin ä¸­çš„å»ºç­‘ç‰©å’Œæ™¯è§‚çš„å›¾åƒTexture mappingã€‚</li>
<li>methods: è¿™ä¸ªæ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æ–°çš„è‡ªåŠ¨åŒ–æ–‡æœ¬æ˜ å°„æ–¹æ³•ï¼ŒåŒ…æ‹¬é€‰æ‹©é«˜è´¨é‡ç…§ç‰‡ï¼Œæå–LoLç‰¹å¾ï¼Œå¯¹ç…§ç‰‡å’Œgeometryè¿›è¡Œå¯¹é½ï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªæ–°çš„æ‰©å±•æ•°æ®é›†å’Œæ»¤æ³¢æ¨¡å‹æ¥å®Œå–„ç¼ºå¤±åŒºåŸŸã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥é«˜æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬æ˜ å°„ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„å»ºç­‘ç‰©å’Œæ™¯è§‚ä¸­å®ç°äººå·¥ä¸“å®¶æ°´å¹³çš„æ•ˆæœï¼Œè€Œä¸éœ€è¦å¤ªå¤šçš„å·¥ä½œã€‚<details>
<summary>Abstract</summary>
Coarse architectural models are often generated at scales ranging from individual buildings to scenes for downstream applications such as Digital Twin City, Metaverse, LODs, etc. Such piece-wise planar models can be abstracted as twins from 3D dense reconstructions. However, these models typically lack realistic texture relative to the real building or scene, making them unsuitable for vivid display or direct reference. In this paper, we present TwinTex, the first automatic texture mapping framework to generate a photo-realistic texture for a piece-wise planar proxy. Our method addresses most challenges occurring in such twin texture generation. Specifically, for each primitive plane, we first select a small set of photos with greedy heuristics considering photometric quality, perspective quality and facade texture completeness. Then, different levels of line features (LoLs) are extracted from the set of selected photos to generate guidance for later steps. With LoLs, we employ optimization algorithms to align texture with geometry from local to global. Finally, we fine-tune a diffusion model with a multi-mask initialization component and a new dataset to inpaint the missing region. Experimental results on many buildings, indoor scenes and man-made objects of varying complexity demonstrate the generalization ability of our algorithm. Our approach surpasses state-of-the-art texture mapping methods in terms of high-fidelity quality and reaches a human-expert production level with much less effort. Project page: https://vcc.tech/research/2023/TwinTex.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>æ–‡æœ¬ç¿»è¯‘æˆç®€åŒ–ä¸­æ–‡ã€‚</SYS>>å»ºç­‘æ¨¡å‹ç»å¸¸åœ¨å¤§è§„æ¨¡ç”Ÿæˆï¼Œä»ä¸ªåˆ«å»ºç­‘åˆ°åœºæ™¯ï¼Œç”¨äºä¸‹æ¸¸åº”ç”¨ç¨‹åºï¼Œå¦‚æ•°å­—åŸå¸‚ã€Metaverseã€LODsç­‰ã€‚è¿™äº›å—çŠ¶å¹³é¢æ¨¡å‹å¯ä»¥è¢«æŠ½è±¡ä¸ºçœŸå®å»ºç­‘æˆ–åœºæ™¯çš„å­ªç”Ÿã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ç¼ºä¹çœŸå®å»ºç­‘æˆ–åœºæ™¯çš„ç²¾ç‚¼æ–‡åŒ–ï¼Œä½¿å…¶ä¸é€‚åˆç²¾å½©æ˜¾ç¤ºæˆ–ç›´æ¥å‚è€ƒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† TwinTexï¼Œé¦–ä¸ªè‡ªåŠ¨Texture mappingæ¡†æ¶ï¼Œç”¨äºç”Ÿæˆå…·æœ‰é«˜ç²¾ç‚¼åº¦çš„Texture Ğ´Ğ»Ñå—çŠ¶å¹³é¢ä»£ç†ã€‚æˆ‘ä»¬çš„æ–¹æ³•è§£å†³äº†è¿™ç±»å­ªç”ŸTextureç”Ÿæˆä¸­çš„ä¸»è¦æŒ‘æˆ˜ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äºæ¯ä¸ªåŸºæœ¬å¹³é¢ï¼Œæˆ‘ä»¬é¦–å…ˆé€‰æ‹©ä¸€å°é›†æ•°æ®ï¼Œä½¿ç”¨å–„æ„çš„è§„åˆ™æ¥è€ƒè™‘å…‰å­¦è´¨é‡ã€è§†è§’è´¨é‡å’Œå»ºç­‘é¢æ–™å®Œæ•´æ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»è¿™äº›é€‰æ‹©çš„æ•°æ®ä¸­æå–ä¸åŒçº§åˆ«çš„çº¿æ¡ç‰¹å¾ï¼ˆLoLsï¼‰ï¼Œä»¥ä¾›åç»­æ­¥éª¤çš„å¼•å¯¼ã€‚ä½¿ç”¨LoLsï¼Œæˆ‘ä»¬è¿ç”¨ä¼˜åŒ–ç®—æ³•å°†Textureä¸Geometryè¿›è¡Œå¯¹é½ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰©å±•æ¨¡å‹ï¼Œå¹¶åœ¨æ–°çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¡«å……ç¼ºå¤±åŒºåŸŸã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•å¯ä»¥åœ¨è®¸å¤šä¸åŒå¤æ‚åº¦çš„å»ºç­‘ã€å®¤å†…åœºæ™¯å’Œäººå·¥åˆ¶å“ä¸Šå®ç°é«˜ç²¾ç‚¼åº¦çš„Texture mappingï¼Œå¹¶ä¸”è¶…è¿‡äº†å½“å‰çŠ¶æ€è‰ºçš„Texture mappingæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡å°‘å¾ˆå¤šåŠ³åŠ¨åŠ›ï¼Œè¾¾åˆ°äººå·¥ä¸“å®¶æ°´å¹³ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://vcc.tech/research/2023/TwinTexã€‚
</details></li>
</ul>
<hr>
<h2 id="Box2Poly-Memory-Efficient-Polygon-Prediction-of-Arbitrarily-Shaped-and-Rotated-Text"><a href="#Box2Poly-Memory-Efficient-Polygon-Prediction-of-Arbitrarily-Shaped-and-Rotated-Text" class="headerlink" title="Box2Poly: Memory-Efficient Polygon Prediction of Arbitrarily Shaped and Rotated Text"></a>Box2Poly: Memory-Efficient Polygon Prediction of Arbitrarily Shaped and Rotated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11248">http://arxiv.org/abs/2309.11248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuyang Chen, Dong Wang, Konrad Schindler, Mingwei Sun, Yongliang Wang, Nicolo Savioli, Liqiu Meng</li>
<li>For: æé«˜æ–‡æœ¬æ£€æµ‹çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œå°¤å…¶æ˜¯å¯¹äºä¸è§„åˆ™çš„æ–‡æœ¬å¸ƒå±€ã€‚* Methods: åŸºäºSparse R-CNNçš„åè°ƒè§£ç ç®¡é“ï¼Œé€šè¿‡é€æ¬¡ç²¾åº¦è°ƒæ•´å¤šè¾¹å½¢é¢„æµ‹ï¼Œä½¿ç”¨å•ä¸ªç‰¹å¾å‘é‡å¯¼å¼•å¤šè¾¹å½¢å®ä¾‹å‡†å¤‡ã€‚* Results: æ¯”è¾ƒDPText-DETRæ–¹æ³•ï¼Œå…·æœ‰æ›´é«˜çš„å†…å­˜æ•ˆç‡ï¼ˆ&gt;50%ï¼‰å’Œæ¨ç†é€Ÿåº¦ï¼ˆ&gt;40%ï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†åŸºå‡†æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æ°´å¹³ã€‚<details>
<summary>Abstract</summary>
Recently, Transformer-based text detection techniques have sought to predict polygons by encoding the coordinates of individual boundary vertices using distinct query features. However, this approach incurs a significant memory overhead and struggles to effectively capture the intricate relationships between vertices belonging to the same instance. Consequently, irregular text layouts often lead to the prediction of outlined vertices, diminishing the quality of results. To address these challenges, we present an innovative approach rooted in Sparse R-CNN: a cascade decoding pipeline for polygon prediction. Our method ensures precision by iteratively refining polygon predictions, considering both the scale and location of preceding results. Leveraging this stabilized regression pipeline, even employing just a single feature vector to guide polygon instance regression yields promising detection results. Simultaneously, the leverage of instance-level feature proposal substantially enhances memory efficiency (>50% less vs. the state-of-the-art method DPText-DETR) and reduces inference speed (>40% less vs. DPText-DETR) with minor performance drop on benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
traducciÃ³n al chino simplificado:ç°åœ¨ï¼ŒåŸºäºTransformerçš„æ–‡æœ¬æ£€æµ‹æŠ€æœ¯å°è¯•é¢„æµ‹å¤šè¾¹å½¢ï¼Œé€šè¿‡å¯¹å„ä¸ªè¾¹ç•Œé¡¶ç‚¹çš„åæ ‡ä½¿ç”¨ç‰¹å®šçš„æŸ¥è¯¢ç‰¹å¾è¿›è¡Œç¼–ç ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¸¦æ¥äº†æ˜¾è‘—çš„å†…å­˜å¼€é”€ï¼Œå¹¶ä¸”å¾ˆéš¾å‡†ç¡®åœ°æ•æ‰åŒä¸€ä¸ªå®ä¾‹ä¸­çš„é€»è¾‘å…³ç³»ã€‚å› æ­¤ï¼Œä¸è§„åˆ™çš„æ–‡æœ¬å¸ƒå±€ç»å¸¸å¯¼è‡´é¢„æµ‹çš„è¾¹ç•Œé¡¶ç‚¹å˜ä¸ºå›´æ é¡¶ç‚¹ï¼Œè¿™ä¼šå¯¼è‡´ç»“æœçš„è´¨é‡ä¸‹é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼ŒåŸºäºSparse R-CNNï¼šä¸€ä¸ªé€»è¾‘æ‹“å±•ç®¡é“ Ğ´Ğ»Ñå¤šè¾¹å½¢é¢„æµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¿è¯å‡†ç¡®æ€§ï¼Œé€šè¿‡è¿­ä»£åœ°çº æ­£å¤šè¾¹å½¢é¢„æµ‹ç»“æœï¼Œè€ƒè™‘å¤šè¾¹å½¢çš„ç¼©æ”¾å’Œä½ç½®ã€‚é€šè¿‡è¿™ä¸ªç¨³å®šçš„å›å½’ç®¡é“ï¼Œç”šè‡³åªä½¿ç”¨ä¸€ä¸ªç‰¹å¾å‘é‡æ¥å¼•å¯¼å¤šè¾¹å½¢å®ä¾‹å›å½’ï¼Œä¹Ÿå¯ä»¥è·å¾—äº†æœ‰å‰é€”çš„æ£€æµ‹ç»“æœã€‚åŒæ—¶ï¼Œé€šè¿‡å®ä¾‹çº§åˆ«çš„ç‰¹å¾ææ¡£ï¼Œå¯ä»¥å¤§å¹…æé«˜å†…å­˜æ•ˆç‡ï¼ˆ>50%æ¯”DPText-DETRæ›´é«˜ï¼‰ï¼Œå¹¶ä¸”é™ä½æ¨ç†é€Ÿåº¦ï¼ˆ>40%æ¯”DPText-DETRæ›´ä½ï¼‰ï¼Œè€Œæ— éœ€åšå‡ºé‡è¦çš„æ€§èƒ½ä¸‹é™ã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-Robust-Few-shot-Point-Cloud-Semantic-Segmentation"><a href="#Towards-Robust-Few-shot-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Towards Robust Few-shot Point Cloud Semantic Segmentation"></a>Towards Robust Few-shot Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11228">http://arxiv.org/abs/2309.11228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Pixie8888/R3DFSSeg">https://github.com/Pixie8888/R3DFSSeg</a></li>
<li>paper_authors: Yating Xu, Na Zhao, Gim Hee Lee</li>
<li>for: æé«˜å‡ ä½•ç‚¹äº‘Semantic segmentationçš„é²æ£’æ€§ï¼Œä½¿å…¶åœ¨å®é™…ä¸–ç•Œä¸­å¿«é€Ÿé€‚åº”æ–°çš„æœªçŸ¥ç±»å‹ï¼Œåªéœ€å‡ ä¸ªæ”¯æŒé›†æ ·æœ¬ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§Component-level Clean Noise Separationï¼ˆCCNSï¼‰è¡¨ç¤ºå­¦ä¹ ï¼Œä»¥å­¦ä¹ ç»†åˆ†targetç±»çš„å‡€æ ·æœ¬ä¸å™ªå£°æ ·æœ¬ä¹‹é—´çš„åˆ†åŒ–ç‰¹å¾è¡¨ç¤ºã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Multi-scale Degree-based Noise Suppressionï¼ˆMDNSï¼‰æ–¹æ¡ˆï¼Œä»¥æ¶ˆé™¤æ”¯æŒé›†ä¸­çš„å™ªå£°æ ·æœ¬ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨ä¸åŒå™ªå£°è®¾å®šä¸‹è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºCCNSå’ŒMDNSçš„ç»„åˆæ˜¾è‘—æé«˜äº†æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Few-shot point cloud semantic segmentation aims to train a model to quickly adapt to new unseen classes with only a handful of support set samples. However, the noise-free assumption in the support set can be easily violated in many practical real-world settings. In this paper, we focus on improving the robustness of few-shot point cloud segmentation under the detrimental influence of noisy support sets during testing time. To this end, we first propose a Component-level Clean Noise Separation (CCNS) representation learning to learn discriminative feature representations that separates the clean samples of the target classes from the noisy samples. Leveraging the well separated clean and noisy support samples from our CCNS, we further propose a Multi-scale Degree-based Noise Suppression (MDNS) scheme to remove the noisy shots from the support set. We conduct extensive experiments on various noise settings on two benchmark datasets. Our results show that the combination of CCNS and MDNS significantly improves the performance. Our code is available at https://github.com/Pixie8888/R3DFSSeg.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡æœ¬ï¼šå‡ ä¸ªç±»åˆ«ç‚¹äº‘ semantic segmentation ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹å¿«é€Ÿé€‚åº”æ–°æœªè§ç±»åˆ«ï¼Œä»…ä»…éœ€è¦ä¸€äº›æ”¯æŒé›†æ ·æœ¬ã€‚ç„¶è€Œï¼Œå®é™…ä¸–ç•Œä¸­çš„å®é™…è®¾å®šä¸­å¯èƒ½ä¼šè½»æ¾è¿åæ— å™ªè®¾å®šã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¢å¼ºå‡ ä¸ªç±»åˆ«ç‚¹äº‘ semantic segmentation çš„Robustnessï¼Œåœ¨æµ‹è¯•æ—¶testingæ—¶çš„æ¶åŠ£å½±å“ä¸‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†Component-level Clean Noise Separation (CCNS) è¡¨ç¤ºå­¦ä¹ ï¼Œä»¥å­¦ä¹ åˆ†ç±»ç‰¹å¾è¡¨ç°ï¼Œå°†ç›®æ ‡ç±»åˆ«çš„æ¸…æ´æ ·æœ¬ä¸å™ªéŸ³æ ·æœ¬åˆ†ç¦»ã€‚ç„¶åï¼Œæˆ‘ä»¬æ›´è¿›ä¸€æ­¥æå‡ºäº†Multi-scale Degree-based Noise Suppression (MDNS) æ–¹æ¡ˆï¼Œä»¥ç§»é™¤æµ‹è¯•æ—¶çš„å™ªéŸ³æ ·æœ¬ã€‚æˆ‘ä»¬å¯¹ä¸åŒå™ªéŸ³è®¾å®šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼ŒCCNS å’Œ MDNS çš„ç»„åˆå¯ä»¥æ˜æ˜¾æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ <https://github.com/Pixie8888/R3DFSSeg> ä¸­æ‰¾åˆ°ã€‚ç¿»è¯‘ç»“æœï¼šæ–‡æœ¬ï¼šå‡ ä¸ªç±»åˆ«ç‚¹äº‘ semantic segmentation ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹å¿«é€Ÿé€‚åº”æ–°æœªè§ç±»åˆ«ï¼Œä»…ä»…éœ€è¦ä¸€äº›æ”¯æŒé›†æ ·æœ¬ã€‚ç„¶è€Œï¼Œå®é™…ä¸–ç•Œä¸­çš„å®é™…è®¾å®šä¸­å¯èƒ½ä¼šè½»æ¾è¿åæ— å™ªè®¾å®šã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºå¢å¼ºå‡ ä¸ªç±»åˆ«ç‚¹äº‘ semantic segmentation çš„Robustnessï¼Œåœ¨æµ‹è¯•æ—¶testingæ—¶çš„æ¶åŠ£å½±å“ä¸‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†Component-level Clean Noise Separation (CCNS) è¡¨ç¤ºå­¦ä¹ ï¼Œä»¥å­¦ä¹ åˆ†ç±»ç‰¹å¾è¡¨ç°ï¼Œå°†ç›®æ ‡ç±»åˆ«çš„æ¸…æ´æ ·æœ¬ä¸å™ªéŸ³æ ·æœ¬åˆ†ç¦»ã€‚ç„¶åï¼Œæˆ‘ä»¬æ›´è¿›ä¸€æ­¥æå‡ºäº†Multi-scale Degree-based Noise Suppression (MDNS) æ–¹æ¡ˆï¼Œä»¥ç§»é™¤æµ‹è¯•æ—¶çš„å™ªéŸ³æ ·æœ¬ã€‚æˆ‘ä»¬å¯¹ä¸åŒå™ªéŸ³è®¾å®šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœæ˜¾ç¤ºï¼ŒCCNS å’Œ MDNS çš„ç»„åˆå¯ä»¥æ˜æ˜¾æé«˜æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ <https://github.com/Pixie8888/R3DFSSeg> ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Generalized-Few-Shot-Point-Cloud-Segmentation-Via-Geometric-Words"><a href="#Generalized-Few-Shot-Point-Cloud-Segmentation-Via-Geometric-Words" class="headerlink" title="Generalized Few-Shot Point Cloud Segmentation Via Geometric Words"></a>Generalized Few-Shot Point Cloud Segmentation Via Geometric Words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11222">http://arxiv.org/abs/2309.11222</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Pixie8888/GFS-3DSeg_GWs">https://github.com/Pixie8888/GFS-3DSeg_GWs</a></li>
<li>paper_authors: Yating Xu, Conghui Hu, Na Zhao, Gim Hee Lee</li>
<li>For: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ›´å®ç”¨çš„æ™®é€šå¤šå°‘shotç‚¹äº‘åˆ†å‰²æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ–°ç±»å‡ºç°æ—¶é€šè¿‡å‡ ä¸ªæ”¯æŒç‚¹äº‘æ¥æ³›åŒ–åˆ°æ–°ç±»ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€ç±»çš„åˆ†å‰²ç²¾åº¦ã€‚* Methods: è¯¥æ–¹æ³•ä½¿ç”¨çš„æ˜¯ geometric words æ¥è¡¨ç¤ºåŸºç¡€å’Œæ–°ç±»ä¹‹é—´çš„ geometric å…±åŒéƒ¨åˆ†ï¼Œå¹¶å°†å…¶ incorporated åˆ°ä¸€ç§æ–°çš„ geometric-aware semantic representation ä¸­ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ³›åŒ–åˆ°æ–°ç±»è€Œä¸å¿˜è®°åŸºç¡€ç±»ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¼•å…¥ geometric prototypes æ¥å¯¼å¼•åˆ†å‰²ï¼Œä½¿ç”¨ geometric prior knowledgeã€‚* Results:  compared withåŸºelineæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ S3DIS å’Œ ScanNet ä¸Šçš„å®éªŒè¡¨ç°å‡ºè‰²ï¼Œæ˜¾ç¤ºäº†æ›´é«˜çš„æ€§èƒ½ã€‚I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Existing fully-supervised point cloud segmentation methods suffer in the dynamic testing environment with emerging new classes. Few-shot point cloud segmentation algorithms address this problem by learning to adapt to new classes at the sacrifice of segmentation accuracy for the base classes, which severely impedes its practicality. This largely motivates us to present the first attempt at a more practical paradigm of generalized few-shot point cloud segmentation, which requires the model to generalize to new categories with only a few support point clouds and simultaneously retain the capability to segment base classes. We propose the geometric words to represent geometric components shared between the base and novel classes, and incorporate them into a novel geometric-aware semantic representation to facilitate better generalization to the new classes without forgetting the old ones. Moreover, we introduce geometric prototypes to guide the segmentation with geometric prior knowledge. Extensive experiments on S3DIS and ScanNet consistently illustrate the superior performance of our method over baseline methods. Our code is available at: https://github.com/Pixie8888/GFS-3DSeg_GWs.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°æœ‰çš„å®Œå…¨ç›‘ç£çš„ç‚¹äº‘åˆ†å‰²æ–¹æ³•åœ¨æ–°ç±»å‡ºç°çš„åŠ¨æ€æµ‹è¯•ç¯å¢ƒä¸­è¡¨ç°ä¸ä½³ï¼Œè¿™æ˜¯å› ä¸ºè¿™äº›æ–¹æ³•åœ¨å­¦ä¹ æ–°ç±»æ—¶ä¼šå·ç§¯åˆ°åŸºç¡€ç±»çš„ç²¾åº¦ï¼Œè¿™å¤§å¤§é™åˆ¶äº†å…¶å®ç”¨æ€§ã€‚è¿™ç§æƒ…å†µæ¿€åŠ±æˆ‘ä»¬æå‡ºä¸€ç§æ›´å®ç”¨çš„é€šç”¨å‡ shotç‚¹äº‘åˆ†å‰²æ–¹æ³•ï¼Œè¦æ±‚æ¨¡å‹èƒ½å¤Ÿé€šè¿‡å‡ ä¸ªæ”¯æŒç‚¹äº‘æ¥æ‰©å±•åˆ°æ–°ç±»ï¼ŒåŒæ—¶ä¿æŒåŸºç¡€ç±»çš„åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨â€œgeometry wordsâ€æ¥è¡¨ç¤ºåŸºç¡€å’Œæ–°ç±»ä¹‹é—´çš„å‡ ä½•å…±åŒéƒ¨åˆ†ï¼Œå¹¶å°†å…¶ integrate into a novel geometric-aware semantic representationï¼Œä»¥ä¾¿æ›´å¥½åœ°é€‚åº”æ–°ç±»è€Œæ— éœ€å¿˜è®°æ—§ç±»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥å‡ ä½•è§„èŒƒæ¥å¯¼èˆªåˆ†å‰²ï¼Œä»¥åˆ©ç”¨å‡ ä½•çŸ¥è¯†æ¥æé«˜åˆ†å‰²ç²¾åº¦ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨S3DISå’ŒScanNetä¸Šçš„æ‰©å±•æ€§å’Œç¨³å®šæ€§éƒ½æ˜¾è‘—æé«˜ã€‚ä»£ç å¯ä»¥åœ¨ï¼šhttps://github.com/Pixie8888/GFS-3DSeg_GWs ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Automatic-Bat-Call-Classification-using-Transformer-Networks"><a href="#Automatic-Bat-Call-Classification-using-Transformer-Networks" class="headerlink" title="Automatic Bat Call Classification using Transformer Networks"></a>Automatic Bat Call Classification using Transformer Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11218">http://arxiv.org/abs/2309.11218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Fundel, Daniel A. Braun, Sebastian Gottwald</li>
<li>for:  automatic bat call identification</li>
<li>methods:  Transformer architecture for multi-label classification</li>
<li>results:  single species accuracy of 88.92% (F1-score of 84.23%), multi species macro F1-score of 74.40%<details>
<summary>Abstract</summary>
Automatically identifying bat species from their echolocation calls is a difficult but important task for monitoring bats and the ecosystem they live in. Major challenges in automatic bat call identification are high call variability, similarities between species, interfering calls and lack of annotated data. Many currently available models suffer from relatively poor performance on real-life data due to being trained on single call datasets and, moreover, are often too slow for real-time classification. Here, we propose a Transformer architecture for multi-label classification with potential applications in real-time classification scenarios. We train our model on synthetically generated multi-species recordings by merging multiple bats calls into a single recording with multiple simultaneous calls. Our approach achieves a single species accuracy of 88.92% (F1-score of 84.23%) and a multi species macro F1-score of 74.40% on our test set. In comparison to three other tools on the independent and publicly available dataset ChiroVox, our model achieves at least 25.82% better accuracy for single species classification and at least 6.9% better macro F1-score for multi species classification.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨è¯†åˆ«è™è ç§ç±»ä»å‘¼å«å£°ä¸­æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§å’Œé‡è¦æ€§çš„ä»»åŠ¡ï¼Œç”¨äºç›‘æµ‹è™è å’Œå®ƒä»¬æ‰€å¤„ç”Ÿæ€ç³»ç»Ÿã€‚ä¸»è¦æŒ‘æˆ˜åœ¨è‡ªåŠ¨è™è å‘¼å«è¯†åˆ«ä¸­æ˜¯å‘¼å«å£°çš„é«˜åº¦å˜åŒ–ã€ç§ç±»ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€å¹²æ‰°å£°å’Œç¼ºä¹æ ‡æ³¨æ•°æ®ã€‚ç°æœ‰çš„è®¸å¤šæ¨¡å‹åœ¨å®é™…æ•°æ®ä¸Šè¡¨ç°è¾ƒå·®ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒä»¬åœ¨å•ä¸ªå‘¼å«æ•°æ®é›†ä¸Šè®­ç»ƒã€‚æˆ‘ä»¬æå‡ºä¸€ç§Transformeræ¶æ„ï¼Œç”¨äºå¤šç±»åˆ«åˆ†ç±»ï¼Œå…·æœ‰å®æ—¶åˆ†ç±»åœºæ™¯çš„åº”ç”¨ potentialã€‚æˆ‘ä»¬åœ¨åˆæˆç”Ÿæˆçš„å¤šç§ recordingä¸­è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªè®°å½•åŒ…å«å¤šä¸ªåŒæ—¶å‘ç”Ÿçš„å‘¼å«ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†å•ç§å‘¼å«ç²¾åº¦88.92%ï¼ˆF1-scoreä¸º84.23%ï¼‰å’Œå¤šç§macro F1-score74.40%ã€‚ä¸ä¸‰ä¸ªå…¶ä»–å·¥å…·åœ¨ç‹¬ç«‹å…¬å…±çš„æ•°æ®é›†ChiroVoxä¸Šè¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬çš„æ¨¡å‹è‡³å°‘25.82%æ›´é«˜çš„å•ç§å‘¼å«ç²¾åº¦å’Œ6.9%æ›´é«˜çš„å¤šç§ macro F1-scoreã€‚
</details></li>
</ul>
<hr>
<h2 id="EPTQ-Enhanced-Post-Training-Quantization-via-Label-Free-Hessian"><a href="#EPTQ-Enhanced-Post-Training-Quantization-via-Label-Free-Hessian" class="headerlink" title="EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian"></a>EPTQ: Enhanced Post-Training Quantization via Label-Free Hessian</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11531">http://arxiv.org/abs/2309.11531</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ssi-research/eptq">https://github.com/ssi-research/eptq</a></li>
<li>paper_authors: Ofir Gordon, Hai Victor Habi, Arnon Netzer</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„å¢å¼ºåæœŸé‡åŒ–æ–¹æ³•ï¼ˆEPTQï¼‰ï¼Œä»¥æé«˜æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„åµŒå…¥ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†çŸ¥è¯†ä¼ æ’­ï¼ˆknowledge distillationï¼‰å’Œè‡ªé€‚åº”å±‚é‡å¤ï¼ˆadaptive weighting of layersï¼‰æ¥å®ç°å¢å¼ºåæœŸé‡åŒ–ã€‚å¦å¤–ï¼Œè®ºæ–‡è¿˜å¼•å…¥äº†ä¸€ç§æ— æ ‡ç­¾æŠ€æœ¯æ¥è¿‘ä¼¼ä»»åŠ¡æŸå¤±çš„å¸Œè€¶æ•°ï¼ˆLabel-Free Hessianï¼‰ï¼Œä»¥é™¤å»éœ€è¦æ ‡ç­¾æ•°æ®é›†çš„éœ€æ±‚ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œé€šè¿‡ä½¿ç”¨EPTQï¼Œå¯ä»¥åœ¨å„ç§æ¨¡å‹ã€ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šå–å¾—æœ€ä½³çš„ç»“æœï¼ŒåŒ…æ‹¬ImageNetåˆ†ç±»ã€COCOç‰©ä»¶æ£€æµ‹å’ŒPascal-VOC semantic segmentationã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯æ˜äº†EPTQçš„å¯è¡Œæ€§å’Œå¯æ›¿ä»£æ€§ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ¶æ„ä¸Šè¿›è¡Œå®ç°ï¼ŒåŒ…æ‹¬CNNsã€Transformersã€æ··åˆå’ŒMLP-onlyæ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Quantization of deep neural networks (DNN) has become a key element in the efforts of embedding such networks on end-user devices. However, current quantization methods usually suffer from costly accuracy degradation. In this paper, we propose a new method for Enhanced Post Training Quantization named EPTQ. The method is based on knowledge distillation with an adaptive weighting of layers. In addition, we introduce a new label-free technique for approximating the Hessian trace of the task loss, named Label-Free Hessian. This technique removes the requirement of a labeled dataset for computing the Hessian. The adaptive knowledge distillation uses the Label-Free Hessian technique to give greater attention to the sensitive parts of the model while performing the optimization. Empirically, by employing EPTQ we achieve state-of-the-art results on a wide variety of models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation. We demonstrate the performance and compatibility of EPTQ on an extended set of architectures, including CNNs, Transformers, hybrid, and MLP-only models.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰çš„é‡åŒ–å·²æˆä¸ºåµŒå…¥è¿™äº›ç½‘ç»œåœ¨ç”¨æˆ·ç«¯è®¾å¤‡çš„å…³é”®å…ƒç´ ã€‚ç„¶è€Œï¼Œå½“å‰çš„é‡åŒ–æ–¹æ³•é€šå¸¸ä¼šå¯¼è‡´ç²¾åº¦ä¸‹é™ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¢å¼ºåæœŸé‡åŒ–æ–¹æ³•ï¼Œç§°ä¸ºå¢å¼ºåæœŸé‡åŒ–ï¼ˆEPTQï¼‰ã€‚è¯¥æ–¹æ³•åŸºäºçŸ¥è¯†ä¼ æ‰¿ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”å±‚æƒé‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§æ–°çš„æ— æ ‡ç­¾æŠ€æœ¯ï¼Œç”¨äºä¼°è®¡ä»»åŠ¡æŸå¤±çš„å¸Œå°”ä¼¯ç‰¹ç‰¹å¾ï¼Œç§°ä¸ºæ— æ ‡ç­¾å¸Œå°”ä¼¯ç‰¹ç‰¹å¾ï¼ˆLabel-Free Hessianï¼‰ã€‚è¿™ç§æŠ€æœ¯æ¶ˆé™¤äº†éœ€è¦æ ‡æ³¨æ•°æ®é›†æ¥è®¡ç®—å¸Œå°”ä¼¯ç‰¹ç‰¹å¾çš„éœ€æ±‚ã€‚é€‚åº”çŸ¥è¯†ä¼ æ‰¿ä½¿ç”¨æ— æ ‡ç­¾å¸Œå°”ä¼¯ç‰¹ç‰¹å¾æ¥å¢åŠ å¯¹æ¨¡å‹æ•æ„Ÿéƒ¨åˆ†çš„æ³¨æ„åŠ›ï¼Œè¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨EPTQï¼Œæˆ‘ä»¬åœ¨å„ç§æ¨¡å‹ã€ä»»åŠ¡å’Œæ•°æ®é›†ä¸Šè¾¾åˆ°äº†çŠ¶æ€å¯¹çš„ç»“æœï¼ŒåŒ…æ‹¬ImageNetåˆ†ç±»ã€COCOç‰©ä½“æ£€æµ‹å’ŒPascal-VOC semantics segmentationã€‚æˆ‘ä»¬ä¹Ÿè¯æ˜äº†EPTQåœ¨æ‰©å±•çš„é›†æˆä½“ç³»ä¸­çš„æ€§èƒ½å’Œå…¼å®¹æ€§ï¼ŒåŒ…æ‹¬CNNsã€Transformersã€æ··åˆå’ŒMLP-onlyæ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Partition-A-Medical-Image-Extracting-Multiple-Representative-Sub-regions-for-Few-shot-Medical-Image-Segmentation"><a href="#Partition-A-Medical-Image-Extracting-Multiple-Representative-Sub-regions-for-Few-shot-Medical-Image-Segmentation" class="headerlink" title="Partition-A-Medical-Image: Extracting Multiple Representative Sub-regions for Few-shot Medical Image Segmentation"></a>Partition-A-Medical-Image: Extracting Multiple Representative Sub-regions for Few-shot Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11172">http://arxiv.org/abs/2309.11172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/PAMI">https://github.com/YazhouZhu19/PAMI</a></li>
<li>paper_authors: Yazhou Zhu, Shidong Wang, Tong Xin, Zheng Zhang, Haofeng Zhang</li>
<li>for: è¿™åˆ™ç ”ç©¶targetsåŒ»ç–—å½±åƒåˆ†ç±»ä»»åŠ¡ï¼Œæ—¨åœ¨æä¾›æ›´æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºåŒ»ç–—å½±åƒåˆ†ç±»ä»»åŠ¡ä¸­é«˜è´¨é‡çš„æ ‡ç­¾æ˜¯è‡ªç„¶ç½•è§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨Regional Prototypical Learning (RPL)æ¨¡å—å°†æ”¯æŒå½±åƒçš„å‰æ™¯ decomposed into distinct regionsï¼Œç„¶åä½¿ç”¨è¿™äº›åŒºåŸŸæ¥ derivation region-level representationsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ä¸ªæ–°çš„Prototypical Representation Debiasing (PRD)æ¨¡å—ï¼Œç”¨äºæŠ‘åˆ¶åŒºåŸŸè¡¨ç¤ºçš„å¹²æ‰°ã€‚</li>
<li>results: ç»è¿‡å¹¿æ³›çš„å®éªŒè¯æ˜ï¼Œæœ¬ç ”ç©¶åœ¨ä¸‰ä¸ªå…¬å¼€ accessible medical imaging datasetsä¸Šå®ç°äº†ä¸ä¸»æµ FSMIS æ–¹æ³•ç›¸æ¯”çš„ç¨³å®šæ”¹è¿›ã€‚å¹¶ä¸”æä¾›äº†ä¸€ä¸ªå¯ç”¨çš„æºä»£ç ï¼ˆ<a target="_blank" rel="noopener" href="https://github.com/YazhouZhu19/PAMI%EF%BC%89%E3%80%82">https://github.com/YazhouZhu19/PAMIï¼‰ã€‚</a><details>
<summary>Abstract</summary>
Few-shot Medical Image Segmentation (FSMIS) is a more promising solution for medical image segmentation tasks where high-quality annotations are naturally scarce. However, current mainstream methods primarily focus on extracting holistic representations from support images with large intra-class variations in appearance and background, and encounter difficulties in adapting to query images. In this work, we present an approach to extract multiple representative sub-regions from a given support medical image, enabling fine-grained selection over the generated image regions. Specifically, the foreground of the support image is decomposed into distinct regions, which are subsequently used to derive region-level representations via a designed Regional Prototypical Learning (RPL) module. We then introduce a novel Prototypical Representation Debiasing (PRD) module based on a two-way elimination mechanism which suppresses the disturbance of regional representations by a self-support, Multi-direction Self-debiasing (MS) block, and a support-query, Interactive Debiasing (ID) block. Finally, an Assembled Prediction (AP) module is devised to balance and integrate predictions of multiple prototypical representations learned using stacked PRD modules. Results obtained through extensive experiments on three publicly accessible medical imaging datasets demonstrate consistent improvements over the leading FSMIS methods. The source code is available at https://github.com/YazhouZhu19/PAMI.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¾›å°‘åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆFSMISï¼‰æ˜¯ä¸€ç§æ›´æœ‰å‰é€”çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œé«˜è´¨é‡æ ‡æ³¨å¾ˆéš¾è·å¾—ã€‚ç„¶è€Œï¼Œå½“å‰ä¸»æµæ–¹æ³•ä¸»è¦æ˜¯æå–æ”¯æŒå›¾åƒä¸­å·¨é‡çš„å†…éƒ¨å˜åŒ–çš„æ•´ä½“è¡¨ç¤ºï¼Œå¹¶é‡åˆ°åœ¨æŸ¥è¯¢å›¾åƒä¸Šé€‚åº”çš„å›°éš¾ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥ä»æ”¯æŒåŒ»å­¦å›¾åƒä¸­æå–å¤šä¸ªä»£è¡¨æ€§å­åŒºåŸŸï¼Œä»¥ä¾¿ç²¾ç»†åœ°é€‰æ‹©ç”Ÿæˆçš„å›¾åƒåŒºåŸŸã€‚å…·ä½“æ¥è¯´ï¼Œæ”¯æŒå›¾åƒçš„å‰æ™¯è¢«åˆ†è§£æˆä¸åŒçš„åŒºåŸŸï¼Œç„¶åé€šè¿‡æˆ‘ä»¬è®¾è®¡çš„åŒºåŸŸå±‚å­¦ä¹ ï¼ˆRPLï¼‰æ¨¡å—æ¥ derivation region-levelè¡¨ç¤ºã€‚æˆ‘ä»¬ç„¶åå¼•å…¥äº†ä¸€ç§æ–°çš„è¡¨ç¤ºåå¯¼ï¼ˆPRDï¼‰æ¨¡å—ï¼ŒåŸºäºä¸¤ç§æ’é™¤æœºåˆ¶ï¼Œå³è‡ªæˆ‘æ”¯æŒçš„å¤šå‘æ’é™¤ï¼ˆMSï¼‰å—å’Œæ”¯æŒ-æŸ¥è¯¢çš„äº’åŠ¨æ’é™¤ï¼ˆIDï¼‰å—ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé›†æˆé¢„æµ‹ï¼ˆAPï¼‰æ¨¡å—ï¼Œå¯ä»¥å¹³è¡¡å’Œé›†æˆå¤šä¸ªè¡¨ç¤ºå­¦ä¹ çš„PRDæ¨¡å—ä¸­çš„é¢„æµ‹ã€‚ç»è¿‡äº†å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬åœ¨ä¸‰ä¸ªå…¬å…± accessibleçš„åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸Šè·å¾—äº†ä¸€è‡´çš„æ”¹è¿›ã€‚æºä»£ç å¯ä»¥åœ¨https://github.com/YazhouZhu19/PAMIä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="AutoSynth-Learning-to-Generate-3D-Training-Data-for-Object-Point-Cloud-Registration"><a href="#AutoSynth-Learning-to-Generate-3D-Training-Data-for-Object-Point-Cloud-Registration" class="headerlink" title="AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration"></a>AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11170">http://arxiv.org/abs/2309.11170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Dang, Mathieu Salzmann</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ç§è‡ªåŠ¨ç”Ÿæˆ3Dè®­ç»ƒæ•°æ®çš„æ–¹æ³•ï¼Œä»¥æé«˜3Då¯¹è±¡æ³¨å†Œä»»åŠ¡çš„è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„3Dæ•°æ®é›†ï¼Œé€šè¿‡ç­›é€‰æœç´¢ç©ºé—´ä¸­çš„ä¼˜ç§€æ•°æ®é›†ï¼Œä»¥ä¾¿åœ¨ä½æˆæœ¬ä¸‹è·å¾—ä¼˜è´¨çš„3Dè®­ç»ƒæ•°æ®ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨TUD-Lã€LINEMODå’ŒOccluded-LINEMODç­‰ä»»åŠ¡ä¸Šå®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œæ¯”å¦‚ModelNet40æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„ç‚¹äº‘æ³¨å†Œç½‘ç»œä¸Šå®ç°æ›´å¥½çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
In the current deep learning paradigm, the amount and quality of training data are as critical as the network architecture and its training details. However, collecting, processing, and annotating real data at scale is difficult, expensive, and time-consuming, particularly for tasks such as 3D object registration. While synthetic datasets can be created, they require expertise to design and include a limited number of categories. In this paper, we introduce a new approach called AutoSynth, which automatically generates 3D training data for point cloud registration. Specifically, AutoSynth automatically curates an optimal dataset by exploring a search space encompassing millions of potential datasets with diverse 3D shapes at a low cost.To achieve this, we generate synthetic 3D datasets by assembling shape primitives, and develop a meta-learning strategy to search for the best training data for 3D registration on real point clouds. For this search to remain tractable, we replace the point cloud registration network with a much smaller surrogate network, leading to a $4056.43$ times speedup. We demonstrate the generality of our approach by implementing it with two different point cloud registration networks, BPNet and IDAM. Our results on TUD-L, LINEMOD and Occluded-LINEMOD evidence that a neural network trained on our searched dataset yields consistently better performance than the same one trained on the widely used ModelNet40 dataset.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœ¨çš„æ·±åº¦å­¦ä¹  paradigmaä¸­ï¼Œè®­ç»ƒæ•°æ®çš„é‡å’Œè´¨é‡æ˜¯ç½‘ç»œæ¶æ„å’Œè®­ç»ƒç»†èŠ‚çš„ equally important factorsã€‚ç„¶è€Œï¼Œæ”¶é›†ã€å¤„ç†å’Œæ ‡æ³¨å®é™…æ•°æ®åœ¨å¤§è§„æ¨¡ä¸Šæ˜¯å›°éš¾ã€æ˜‚è´µå’Œæ—¶é—´consumingçš„ï¼Œç‰¹åˆ«æ˜¯ Ğ´Ğ»Ñ tasks such as 3D object registrationã€‚ although synthetic datasets can be created, they require expertise to design and have a limited number of categories. In this paper, we introduce a new approach called AutoSynth, which automatically generates 3D training data for point cloud registration. Specifically, AutoSynth automatically curates an optimal dataset by exploring a search space encompassing millions of potential datasets with diverse 3D shapes at a low cost.To achieve this, we generate synthetic 3D datasets by assembling shape primitives, and develop a meta-learning strategy to search for the best training data for 3D registration on real point clouds. For this search to remain tractable, we replace the point cloud registration network with a much smaller surrogate network, leading to a $4056.43$ times speedup. We demonstrate the generality of our approach by implementing it with two different point cloud registration networks, BPNet and IDAM. Our results on TUD-L, LINEMOD and Occluded-LINEMOD evidence that a neural network trained on our searched dataset yields consistently better performance than the same one trained on the widely used ModelNet40 dataset.
</details></li>
</ul>
<hr>
<h2 id="Multi-grained-Temporal-Prototype-Learning-for-Few-shot-Video-Object-Segmentation"><a href="#Multi-grained-Temporal-Prototype-Learning-for-Few-shot-Video-Object-Segmentation" class="headerlink" title="Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation"></a>Multi-grained Temporal Prototype Learning for Few-shot Video Object Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11160">http://arxiv.org/abs/2309.11160</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nankepan/VIPMT">https://github.com/nankepan/VIPMT</a></li>
<li>paper_authors: Nian Liu, Kepan Nan, Wangbo Zhao, Yuanwei Liu, Xiwen Yao, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Junwei Han, Fahad Shahbaz Khan</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨ç”¨å°‘é‡æ ‡æ³¨å›¾åƒæ”¯æŒè¿›è¡Œè§†é¢‘å¯¹è±¡åˆ†å‰²ï¼Œä»¥ä¾¿åœ¨è§†é¢‘æ•°æ®ä¸­åˆ†å‰²åŒä¸€ç±»ç›®æ ‡å¯¹è±¡ã€‚</li>
<li>methods: è¯¥æ–¹æ³•åŸºäºIPMTï¼Œä¸€ç§ç°æœ‰çš„å°‘é‡å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå¹¶å°†å¤šé‡å±‚æ¬¡æ—¶é—´å¼•å¯¼ä¿¡æ¯å¼•å…¥è§†é¢‘æ•°æ®å¤„ç†ä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒæŸ¥è¯¢è§†é¢‘ä¿¡æ¯è¢«åˆ†è§£æˆclipå‹prototypeå’Œè®°å¿†å‹prototypeï¼Œä»¥æ•æ‰å½“åœ°å’Œé•¿æœŸå†…éƒ¨æ—¶é—´å¼•å¯¼ä¿¡æ¯ã€‚æ¯å¸§ç‹¬ç«‹ä½¿ç”¨æ¡†å‹ prototype å¤„ç†ç»†è‡´åŒ–é€‚åº”å¼•å¯¼ï¼Œå¹¶å®ç°äº†åŒå‘clip-frame prototype äº¤æµã€‚æ­¤å¤–ï¼Œä¸ºå‡å°‘å™ªéŸ³è®°å¿†çš„å½±å“ï¼Œæå‡ºäº†åŸºäºç»“æ„ç›¸ä¼¼å…³ç³»çš„æ”¯æŒé€‰æ‹©å¯é è®°å¿†å¸§ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²æŸå¤±ï¼Œä»¥æé«˜å­¦ä¹  prototype çš„ç±»åˆ«å¯è¯†åˆ«åº¦ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„è§†é¢‘ IPMT æ¨¡å‹åœ¨ä¸¤ä¸ªæ ‡å‡†æµ‹è¯•é›†ä¸Šæ˜¾è‘—è¶…è¿‡äº†ä¹‹å‰çš„æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Few-Shot Video Object Segmentation (FSVOS) aims to segment objects in a query video with the same category defined by a few annotated support images. However, this task was seldom explored. In this work, based on IPMT, a state-of-the-art few-shot image segmentation method that combines external support guidance information with adaptive query guidance cues, we propose to leverage multi-grained temporal guidance information for handling the temporal correlation nature of video data. We decompose the query video information into a clip prototype and a memory prototype for capturing local and long-term internal temporal guidance, respectively. Frame prototypes are further used for each frame independently to handle fine-grained adaptive guidance and enable bidirectional clip-frame prototype communication. To reduce the influence of noisy memory, we propose to leverage the structural similarity relation among different predicted regions and the support for selecting reliable memory frames. Furthermore, a new segmentation loss is also proposed to enhance the category discriminability of the learned prototypes. Experimental results demonstrate that our proposed video IPMT model significantly outperforms previous models on two benchmark datasets. Code is available at https://github.com/nankepan/VIPMT.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‡ ä¸ªè§†é¢‘å¯¹è±¡åˆ†å‰²ï¼ˆFSVOSï¼‰ç›®æ ‡æ˜¯ä½¿ç”¨ä¸€äº›å®šä¹‰åŒä¸€ç±»ç›®çš„æ”¯æŒå›¾åƒæ¥åˆ†å‰²æŸ¥è¯¢è§†é¢‘ä¸­çš„å¯¹è±¡ã€‚ç„¶è€Œï¼Œè¿™ä¸ªä»»åŠ¡å‡ ä¹æ²¡æœ‰è¢«ç ”ç©¶ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬åŸºäºIPMTï¼Œä¸€ç§ç°æœ‰çš„å°‘é‡å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œé€šè¿‡ Ğ²Ğ½Ğµéƒ¨æ”¯æŒå¯¼èˆªä¿¡æ¯å’Œé€‚åº”æŸ¥è¯¢å¯¼èˆªå¾æ–™æ¥æ‹“å±•æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†æŸ¥è¯¢è§†é¢‘ä¿¡æ¯åˆ†è§£æˆä¸€ä¸ªclipåŸå‹å’Œä¸€ä¸ªè®°å¿†åŸå‹ï¼Œä»¥æ•æ‰æœ¬åœ°å’Œé•¿æœŸå†…éƒ¨ temporalå¯¼èˆªä¿¡æ¯ã€‚æ¯å¸§prototypeè¢«ä½¿ç”¨ï¼Œä»¥ç‹¬ç«‹å¤„ç†ç»†è…»çš„é€‚åº”å¯¼èˆªå’Œä¸¤ä¸ªæ–¹å‘clip-frame prototypeé€šä¿¡ã€‚ä¸ºäº†å‡å°‘å¹²æ‰°çš„å†…å­˜ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨ä¸åŒé¢„æµ‹åŒºåŸŸä¹‹é—´çš„ç»“æ„ç›¸ä¼¼å…³ç³»å’Œæ”¯æŒé€‰æ‹©å¯é çš„è®°å¿†å¸§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„åˆ†å‰²æŸå¤±ï¼Œä»¥æé«˜å­¦ä¹ çš„ç±»åˆ«å¯è¯†åˆ«åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æå‡ºçš„è§†é¢‘IPMTæ¨¡å‹åœ¨ä¸¤ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¹‹å‰çš„æ¨¡å‹ã€‚ä»£ç å¯ä»¥åœ¨https://github.com/nankepan/VIPMTä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-Deformable-3D-Graph-Similarity-to-Track-Plant-Cells-in-Unregistered-Time-Lapse-Images"><a href="#Learning-Deformable-3D-Graph-Similarity-to-Track-Plant-Cells-in-Unregistered-Time-Lapse-Images" class="headerlink" title="Learning Deformable 3D Graph Similarity to Track Plant Cells in Unregistered Time Lapse Images"></a>Learning Deformable 3D Graph Similarity to Track Plant Cells in Unregistered Time Lapse Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11157">http://arxiv.org/abs/2309.11157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Shazid Islam, Arindam Dutta, Calvin-Khang Ta, Kevin Rodriguez, Christian Michael, Mark Alber, G. Venugopala Reddy, Amit K. Roy-Chowdhury</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œç”¨äºå‡†ç¡®åœ°è·Ÿè¸ªæ¤ç‰©ç»†èƒå›¾åƒä¸­çš„ç»†èƒã€‚</li>
<li>methods: è¯¥æ–¹æ³•åˆ©ç”¨æ¤ç‰©ç»†èƒçš„ç´§å¯†æ’åˆ—ä¸‰ç»´ç»“æ„ï¼Œåˆ›å»ºä¸‰ç»´å›¾åº“ï¼Œä»¥å®ç°å‡†ç¡®çš„ç»†èƒè·Ÿè¸ªã€‚å¦å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æå‡ºäº†æ–°çš„ç»†èƒåˆ†è£‚æ£€æµ‹ç®—æ³•å’Œé«˜æ•ˆä¸‰ç»´å¯¹ align ç®—æ³•ã€‚</li>
<li>results: è¯¥è®ºæ–‡åœ¨ä¸€ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜äº†è¯¥æ–¹æ³•çš„è·Ÿè¸ªç²¾åº¦å’Œæœç´¢æ—¶é—´çš„ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
Tracking of plant cells in images obtained by microscope is a challenging problem due to biological phenomena such as large number of cells, non-uniform growth of different layers of the tightly packed plant cells and cell division. Moreover, images in deeper layers of the tissue being noisy and unavoidable systemic errors inherent in the imaging process further complicates the problem. In this paper, we propose a novel learning-based method that exploits the tightly packed three-dimensional cell structure of plant cells to create a three-dimensional graph in order to perform accurate cell tracking. We further propose novel algorithms for cell division detection and effective three-dimensional registration, which improve upon the state-of-the-art algorithms. We demonstrate the efficacy of our algorithm in terms of tracking accuracy and inference-time on a benchmark dataset.
</details>
<details>
<summary>æ‘˜è¦</summary>
track plant cells in microscope images æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œå› ä¸ºç”Ÿç‰©ç°è±¡å¦‚å¤§é‡ç»†èƒã€ä¸å‡ç”Ÿé•¿çš„ä¸åŒå±‚æ¬¡ç´§å¯†æ’åˆ—çš„æ¤ç‰©ç»†èƒï¼Œä»¥åŠç»†èƒåˆ†è£‚ã€‚æ­¤å¤–ï¼Œæ·±å±‚ç»„ç»‡å›¾åƒä¸­çš„å™ªå£°å’Œä¸å¯é¿å…çš„å›¾åƒæ•æ‰è¿‡ç¨‹ä¸­çš„ç³»ç»Ÿæ€§é”™è¯¯æ›´åŠ å¤æ‚äº†é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨æ¤ç‰©ç»†èƒç´§å¯†ä¸‰ç»´ç»“æ„æ¥åˆ›å»ºä¸‰ç»´å›¾è¡¨ï¼Œä»¥è¿›è¡Œå‡†ç¡®çš„ç»†èƒè·Ÿè¸ªã€‚æˆ‘ä»¬è¿˜æå‡ºäº†æ–°çš„ç»†èƒåˆ†è£‚æ£€æµ‹ç®—æ³•å’Œæœ‰æ•ˆçš„ä¸‰ç»´å¯¹æ¥ç®—æ³•ï¼Œè¿™äº›ç®—æ³•éƒ½è¶…è¿‡äº†å½“å‰çŠ¶æ€çš„ç®—æ³•ã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¸€ä¸ªæ ‡å‡†æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ç®—æ³•çš„å‡†ç¡®æ€§å’Œæ¨ç†æ—¶é—´ã€‚
</details></li>
</ul>
<hr>
<h2 id="CNN-based-local-features-for-navigation-near-an-asteroid"><a href="#CNN-based-local-features-for-navigation-near-an-asteroid" class="headerlink" title="CNN-based local features for navigation near an asteroid"></a>CNN-based local features for navigation near an asteroid</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11156">http://arxiv.org/abs/2309.11156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olli Knuuttila, Antti KestilÃ¤, Esa Kallio</li>
<li>for:  asteroid exploration missions and on-orbit servicing</li>
<li>methods:  lightweight feature extractor specifically tailored for asteroid proximity navigation, designed to be robust to illumination changes and affine transformations</li>
<li>results:  effective navigation and localization, with incremental improvements over existing methods and a trained feature extractor<details>
<summary>Abstract</summary>
This article addresses the challenge of vision-based proximity navigation in asteroid exploration missions and on-orbit servicing. Traditional feature extraction methods struggle with the significant appearance variations of asteroids due to limited scattered light. To overcome this, we propose a lightweight feature extractor specifically tailored for asteroid proximity navigation, designed to be robust to illumination changes and affine transformations. We compare and evaluate state-of-the-art feature extraction networks and three lightweight network architectures in the asteroid context. Our proposed feature extractors and their evaluation leverages both synthetic images and real-world data from missions such as NEAR Shoemaker, Hayabusa, Rosetta, and OSIRIS-REx. Our contributions include a trained feature extractor, incremental improvements over existing methods, and a pipeline for training domain-specific feature extractors. Experimental results demonstrate the effectiveness of our approach in achieving accurate navigation and localization. This work aims to advance the field of asteroid navigation and provides insights for future research in this domain.
</details>
<details>
<summary>æ‘˜è¦</summary>
(Simplified Chinese translation)è¿™ç¯‡æ–‡ç« å…³æ³¨ asteroid æ¢æµ‹å’Œå¤„ç†ä»»åŠ¡ä¸­çš„è§†è§‰é è¿‘å¯¼èˆªæŒ‘æˆ˜ï¼Œä¼ ç»Ÿçš„ç‰¹å¾æå–æ–¹æ³•ç”±äº asteroid çš„é™åˆ¶æ•£å°„å…‰å¯¼è‡´è¡¨ç°å˜åŒ–å¼ºå¤§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä¸€ç§é€‚åº” asteroid é è¿‘å¯¼èˆªçš„è½»é‡çº§ç‰¹å¾æå–å™¨ï¼Œå¯ä»¥æŠ—æŠ—ç…§æ˜å˜åŒ–å’ŒæŠ½è±¡å˜æ¢ã€‚æˆ‘ä»¬æ¯”è¾ƒå’Œè¯„ä¼°äº†ç°æœ‰çš„ç‰¹å¾æå–ç½‘ç»œå’Œä¸‰ç§è½»é‡çº§ç½‘ç»œä½“ç³»ï¼Œå¹¶åœ¨ asteroid ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„ææ¡ˆåŒ…æ‹¬ä¸€ä¸ªå·²ç»è®­ç»ƒå¥½çš„ç‰¹å¾æå–å™¨ï¼Œä»¥åŠå¯¹ç°æœ‰æ–¹æ³•è¿›è¡Œäº†æ”¹è¿›ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°é«˜ç²¾åº¦çš„å¯¼èˆªå’Œåœ°å€ç¡®å®šã€‚è¿™é¡¹å·¥ä½œå¸Œæœ›å¯ä»¥æ¨åŠ¨ asteroid å¯¼èˆªé¢†åŸŸçš„è¿›æ­¥ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æ–°çš„æ€è·¯å’Œçµæ„Ÿã€‚
</details></li>
</ul>
<hr>
<h2 id="Online-Calibration-of-a-Single-Track-Ground-Vehicle-Dynamics-Model-by-Tight-Fusion-with-Visual-Inertial-Odometry"><a href="#Online-Calibration-of-a-Single-Track-Ground-Vehicle-Dynamics-Model-by-Tight-Fusion-with-Visual-Inertial-Odometry" class="headerlink" title="Online Calibration of a Single-Track Ground Vehicle Dynamics Model by Tight Fusion with Visual-Inertial Odometry"></a>Online Calibration of a Single-Track Ground Vehicle Dynamics Model by Tight Fusion with Visual-Inertial Odometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11148">http://arxiv.org/abs/2309.11148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolong Li, Joerg Stueckler</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æä¾›ä¸€ç§åŸºäºè§†è§‰é¥æ„Ÿå’ŒåŠ¨åŠ›å­¦æ¨¡å‹çš„å•è½¦è·‘åŠ¨ä¼°è®¡æ–¹æ³•ï¼Œç”¨äº Navigation Planningã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†å•è½¦åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œä¸è§†è§‰é¥æ„Ÿï¼ˆVIOï¼‰ç›¸ç»“åˆï¼Œåœ¨çº¿è¿›è¡Œæ¨¡å‹Parameters calibrationå’Œé€‚åº”ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„ç¯å¢ƒä¸‹ï¼ˆå®¤å†…å’Œå¤–ï¼‰ï¼Œé€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œå¹¶ä¸”å¯ä»¥å‡†ç¡®åœ°é¢„æµ‹æœªæ¥æ§åˆ¶è¾“å…¥çš„æ•ˆæœã€‚åŒæ—¶ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥æé«˜è·Ÿè¸ªç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
Wheeled mobile robots need the ability to estimate their motion and the effect of their control actions for navigation planning. In this paper, we present ST-VIO, a novel approach which tightly fuses a single-track dynamics model for wheeled ground vehicles with visual inertial odometry. Our method calibrates and adapts the dynamics model online and facilitates accurate forward prediction conditioned on future control inputs. The single-track dynamics model approximates wheeled vehicle motion under specific control inputs on flat ground using ordinary differential equations. We use a singularity-free and differentiable variant of the single-track model to enable seamless integration as dynamics factor into VIO and to optimize the model parameters online together with the VIO state variables. We validate our method with real-world data in both indoor and outdoor environments with different terrain types and wheels. In our experiments, we demonstrate that our ST-VIO can not only adapt to the change of the environments and achieve accurate prediction under new control inputs, but even improves the tracking accuracy. Supplementary video: https://youtu.be/BuGY1L1FRa4.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨ç§»åŠ¨æœºå™¨äººéœ€è¦ä¼°ç®—å…¶è¿åŠ¨å’Œæ§åˆ¶åŠ¨ä½œçš„å½±å“ä»¥å®ç°å¯¼èˆªè§„åˆ’ã€‚æœ¬æ–‡æå‡ºäº†ST-VIOï¼Œä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå®ƒå°†å•è½¦è¾†åŠ¨åŠ›å­¦æ¨¡å‹ç´§å¯†èåˆè§†è§‰é™€èºä»ªå®šä½ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨çº¿æŠ•å…¥å’Œè°ƒæ•´åŠ¨åŠ›å­¦æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æœªæ¥æ§åˆ¶è¾“å…¥çš„å‰æä¸‹è¿›è¡Œé«˜ç²¾åº¦é¢„æµ‹ã€‚å•è½¦è¾†åŠ¨åŠ›å­¦æ¨¡å‹æ˜¯åœ¨ç‰¹å®šçš„æ§åˆ¶è¾“å…¥ä¸‹ï¼Œåœ¨å¹³åœ°ä¸Šä½¿ç”¨æ™®é€šå¾®åˆ†æ–¹ç¨‹æè¿°è½¦è¾†çš„è¿åŠ¨ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸å«ç‰¹å¾ç‚¹å’Œå¯å¾®åˆ†çš„å•è½¦è¾†æ¨¡å‹ï¼Œä»¥ä¾¿è½»æ¾åœ°å°†åŠ¨åŠ›å­¦æ¨¡å‹çº³å…¥VIĞä¸­ï¼Œå¹¶åœ¨VIĞçŠ¶æ€å˜é‡ä¸Šçº¿ä¸Šè°ƒæ•´æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„ST-VIOå¯ä»¥ä¸ä»…é€‚åº”ç¯å¢ƒå˜åŒ–ï¼Œå¹¶åœ¨æ–°çš„æ§åˆ¶è¾“å…¥ä¸‹å®ç°é«˜ç²¾åº¦è·Ÿè¸ªã€‚è¡¥å……è§†é¢‘ï¼šhttps://youtu.be/BuGY1L1FRa4ã€‚
</details></li>
</ul>
<hr>
<h2 id="GraphEcho-Graph-Driven-Unsupervised-Domain-Adaptation-for-Echocardiogram-Video-Segmentation"><a href="#GraphEcho-Graph-Driven-Unsupervised-Domain-Adaptation-for-Echocardiogram-Video-Segmentation" class="headerlink" title="GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation"></a>GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11145">http://arxiv.org/abs/2309.11145</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GraphEcho">https://github.com/xmed-lab/GraphEcho</a></li>
<li>paper_authors: Jiewen Yang, Xinpeng Ding, Ziyang Zheng, Xiaowei Xu, Xiaomeng Li</li>
<li>For: è¿™ä¸ªè®ºæ–‡ç ”ç©¶äº†éç›‘ç£é¢†åŸŸé€‚åº”ï¼ˆUnsupervised Domain Adaptationï¼ŒUDAï¼‰åœ¨echocardiogramè§†é¢‘åˆ†å‰²æ–¹é¢ï¼Œç›®çš„æ˜¯å°†æ¥è‡ªæºé¢‘è°±åŸŸçš„æ¨¡å‹æ³›åŒ–åˆ°å…¶ä»–æœªæ ‡æ³¨ç›®æ ‡é¢‘è°±åŸŸã€‚* Methods: æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„CardiacUDAæ•°æ®é›†å’Œä¸€ç§åä¸ºGraphEchoçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªåˆ›æ–°æ¨¡å—ï¼šç©ºé—´åŸŸé¢‘è°±åŒ¹é…ï¼ˆSCGMï¼‰å’Œå¿ƒè·³å‘¨æœŸä¸€è‡´æ€§ï¼ˆTCCï¼‰æ¨¡å—ã€‚è¿™ä¸¤ä¸ªæ¨¡å—å¯ä»¥æ›´å¥½åœ°å¯¹globalå’Œlocalç‰¹å¾ä»æºå’Œç›®æ ‡é¢‘è°±åŸŸè¿›è¡Œå¯¹é½ï¼Œä»è€Œæé«˜UDAåˆ†å‰²ç»“æœã€‚* Results: æˆ‘ä»¬çš„GraphEchoæ–¹æ³•åœ¨å¯¹æ¯” existedçŠ¶æ€çš„æ¨èUDAåˆ†å‰²æ–¹æ³•æ—¶è¡¨ç°å‡ºè‰²ï¼Œå®éªŒç»“æœè¡¨æ˜ã€‚æˆ‘ä»¬çš„CardiacUDAæ•°æ®é›†å’Œä»£ç å°†åœ¨æ¥å—åå…¬å¼€å‘å¸ƒï¼Œè¿™é¡¹å·¥ä½œå°†ä¸ºå¿ƒè„ç»“æ„åˆ†å‰²ä»echocardiogramè§†é¢‘ä¸­å¥ å®šæ–°çš„ã€åšå®çš„åŸºç¡€ã€‚ä»£ç å’Œæ•°æ®é›†å¯ä»¥é€šè¿‡<a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GraphEcho%E8%AE%BF%E9%97%AE%E3%80%82">https://github.com/xmed-lab/GraphEchoè®¿é—®ã€‚</a><details>
<summary>Abstract</summary>
Echocardiogram video segmentation plays an important role in cardiac disease diagnosis. This paper studies the unsupervised domain adaption (UDA) for echocardiogram video segmentation, where the goal is to generalize the model trained on the source domain to other unlabelled target domains. Existing UDA segmentation methods are not suitable for this task because they do not model local information and the cyclical consistency of heartbeat. In this paper, we introduce a newly collected CardiacUDA dataset and a novel GraphEcho method for cardiac structure segmentation. Our GraphEcho comprises two innovative modules, the Spatial-wise Cross-domain Graph Matching (SCGM) and the Temporal Cycle Consistency (TCC) module, which utilize prior knowledge of echocardiogram videos, i.e., consistent cardiac structure across patients and centers and the heartbeat cyclical consistency, respectively. These two modules can better align global and local features from source and target domains, improving UDA segmentation results. Experimental results showed that our GraphEcho outperforms existing state-of-the-art UDA segmentation methods. Our collected dataset and code will be publicly released upon acceptance. This work will lay a new and solid cornerstone for cardiac structure segmentation from echocardiogram videos. Code and dataset are available at: https://github.com/xmed-lab/GraphEcho
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šechocardiogramè§†é¢‘åˆ†å‰² plays an important role in cardiac disease diagnosisã€‚This paper studies the unsupervised domain adaptionï¼ˆUDAï¼‰for echocardiogramè§†é¢‘åˆ†å‰²ï¼Œwhere the goal is to generalize the model trained on the source domain to other unlabelled target domainsã€‚Existing UDA segmentation methods are not suitable for this task because they do not model local information and the cyclical consistency of heartbeatã€‚In this paper, we introduce a newly collected CardiacUDA dataset and a novel GraphEcho method for cardiac structure segmentationã€‚Our GraphEcho comprises two innovative modulesï¼Œthe Spatial-wise Cross-domain Graph Matchingï¼ˆSCGMï¼‰and the Temporal Cycle Consistencyï¼ˆTCCï¼‰moduleï¼Œwhich utilize prior knowledge of echocardiogram videosï¼Œi.e., consistent cardiac structure across patients and centers and the heartbeat cyclical consistencyï¼Œrespectivelyã€‚These two modules can better align global and local features from source and target domainsï¼Œimproving UDA segmentation resultsã€‚Experimental results showed that our GraphEcho outperforms existing state-of-the-art UDA segmentation methodsã€‚Our collected dataset and code will be publicly released upon acceptanceã€‚This work will lay a new and solid cornerstone for cardiac structure segmentation from echocardiogram videosã€‚Code and dataset are available atï¼šhttps://github.com/xmed-lab/GraphEchoã€‚ã€‹Note that Simplified Chinese is the official writing system used in mainland China, and it may be different from Traditional Chinese, which is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="GL-Fusion-Global-Local-Fusion-Network-for-Multi-view-Echocardiogram-Video-Segmentation"><a href="#GL-Fusion-Global-Local-Fusion-Network-for-Multi-view-Echocardiogram-Video-Segmentation" class="headerlink" title="GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation"></a>GL-Fusion: Global-Local Fusion Network for Multi-view Echocardiogram Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11144">http://arxiv.org/abs/2309.11144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/GL-Fusion">https://github.com/xmed-lab/GL-Fusion</a></li>
<li>paper_authors: Ziyang Zheng, Jiewen Yang, Xinpeng Ding, Xiaowei Xu, Xiaomeng Li</li>
<li>for: è¿™ç§ç ”ç©¶æ—¨åœ¨æé«˜è‡ªåŠ¨åˆ†ç±»echocardiogramè§†é¢‘ä¸­çš„å¿ƒè„ç»“æ„åˆ†å‰²ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>methods: è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„å…¨çƒ-æœ¬åœ°èåˆç½‘ç»œï¼ˆGL-Fusionï¼‰ï¼Œç”¨äºåŒæ—¶åˆ©ç”¨å¤šè§†å›¾ä¿¡æ¯çš„å…¨çƒå’Œæœ¬åœ°ç‰¹å¾ï¼Œä»¥æé«˜echocardiogramåˆ†æçš„å‡†ç¡®æ€§ã€‚</li>
<li>results: è¯¥ç ”ç©¶é€šè¿‡ä½¿ç”¨MvEVDæ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼Œå‘ç°GL-Fusionæ–¹æ³•å¯ä»¥æé«˜echocardiogramåˆ†æçš„å‡†ç¡®æ€§ï¼Œä¸åŸºelineæ–¹æ³•ç›¸æ¯”æé«˜äº†7.83%ã€‚æ­¤å¤–ï¼ŒGL-Fusionæ–¹æ³•è¿˜è¶…è¿‡äº†ç°æœ‰çš„çŠ¶æ€ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹æ™ºèƒ½æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Cardiac structure segmentation from echocardiogram videos plays a crucial role in diagnosing heart disease. The combination of multi-view echocardiogram data is essential to enhance the accuracy and robustness of automated methods. However, due to the visual disparity of the data, deriving cross-view context information remains a challenging task, and unsophisticated fusion strategies can even lower performance. In this study, we propose a novel Gobal-Local fusion (GL-Fusion) network to jointly utilize multi-view information globally and locally that improve the accuracy of echocardiogram analysis. Specifically, a Multi-view Global-based Fusion Module (MGFM) is proposed to extract global context information and to explore the cyclic relationship of different heartbeat cycles in an echocardiogram video. Additionally, a Multi-view Local-based Fusion Module (MLFM) is designed to extract correlations of cardiac structures from different views. Furthermore, we collect a multi-view echocardiogram video dataset (MvEVD) to evaluate our method. Our method achieves an 82.29% average dice score, which demonstrates a 7.83% improvement over the baseline method, and outperforms other existing state-of-the-art methods. To our knowledge, this is the first exploration of a multi-view method for echocardiogram video segmentation. Code available at: https://github.com/xmed-lab/GL-Fusion
</details>
<details>
<summary>æ‘˜è¦</summary>
å¡ç¬¬äºšç»“æ„åˆ†å‰²è‡ªechocardiogramè§†é¢‘ä¸­æ‰®æ¼”é‡è¦çš„è§’è‰²ï¼Œç”¨äºè¯Šæ–­å¿ƒè¡€ç®¡ç–¾ç—…ã€‚å¤šè§†å›¾echocardiogramæ•°æ®çš„ç»„åˆæ˜¯æé«˜è‡ªåŠ¨æ–¹æ³•çš„å‡†ç¡®æ€§å’Œå¯é æ€§çš„å…³é”®ã€‚ç„¶è€Œï¼Œç”±äºè§†è§‰å·®å¼‚ï¼Œ derivation of cross-view context information remains a challenging task, and unsophisticated fusion strategies can even lower performance. åœ¨è¿™ç§ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„å…¨çƒ-æœ¬åœ°æ··åˆï¼ˆGL-Fusionï¼‰ç½‘ç»œï¼Œç”¨äºåŒæ—¶åˆ©ç”¨å¤šè§†å›¾ä¿¡æ¯çš„å…¨çƒå’Œæœ¬åœ°ä¿¡æ¯ï¼Œä»¥æé«˜echocardiogramåˆ†æçš„å‡†ç¡®æ€§ã€‚ Specifically, a Multi-view Global-based Fusion Module (MGFM) is proposed to extract global context information and to explore the cyclic relationship of different heartbeat cycles in an echocardiogram video. Additionally, a Multi-view Local-based Fusion Module (MLFM) is designed to extract correlations of cardiac structures from different views. Furthermore, we collect a multi-view echocardiogram video dataset (MvEVD) to evaluate our method. Our method achieves an 82.29% average dice score, which demonstrates a 7.83% improvement over the baseline method, and outperforms other existing state-of-the-art methods. To our knowledge, this is the first exploration of a multi-view method for echocardiogram video segmentation. å¯ä»¥åœ¨https://github.com/xmed-lab/GL-Fusionæ‰¾åˆ°æˆ‘ä»¬çš„ä»£ç ã€‚
</details></li>
</ul>
<hr>
<h2 id="More-complex-encoder-is-not-all-you-need"><a href="#More-complex-encoder-is-not-all-you-need" class="headerlink" title="More complex encoder is not all you need"></a>More complex encoder is not all you need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11139">http://arxiv.org/abs/2309.11139</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aitechlabcn/neUNet">https://github.com/aitechlabcn/neUNet</a></li>
<li>paper_authors: Weibin Yang, Longwei Xu, Pengwei Wang, Dehua Geng, Yusong Li, Mingyuan Xu, Zhiqi Dong</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦ç”¨äºæé«˜åŒ»ç–—å½±åƒåˆ†ç±»çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬ï¼šU-Netå’Œå…¶å˜ä½“ï¼Œå¹¶å°†æ³¨æ„åŠ›é›†ä¸­åœ¨å¢å¼ºè§£oderçš„éƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯å¢å¼ºupsamplingéƒ¨åˆ†ä»¥æé«˜åˆ†ç±»ç»“æœã€‚</li>
<li>results: æœ¬æ–‡çš„ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨äº†æ–°çš„Sub-pixel Convolutionå’Œå¤šæ¡æ°”å¹³é¢è¾“å…¥æ¨¡å—ï¼Œå¯ä»¥æé«˜åˆ†ç±»ç»“æœçš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œå¹¶ä¸”åœ¨Synapseå’ŒACDC datasetsä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†å…¶ä»–ç°æœ‰çš„æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
U-Net and its variants have been widely used in medical image segmentation. However, most current U-Net variants confine their improvement strategies to building more complex encoder, while leaving the decoder unchanged or adopting a simple symmetric structure. These approaches overlook the true functionality of the decoder: receiving low-resolution feature maps from the encoder and restoring feature map resolution and lost information through upsampling. As a result, the decoder, especially its upsampling component, plays a crucial role in enhancing segmentation outcomes. However, in 3D medical image segmentation, the commonly used transposed convolution can result in visual artifacts. This issue stems from the absence of direct relationship between adjacent pixels in the output feature map. Furthermore, plain encoder has already possessed sufficient feature extraction capability because downsampling operation leads to the gradual expansion of the receptive field, but the loss of information during downsampling process is unignorable. To address the gap in relevant research, we extend our focus beyond the encoder and introduce neU-Net (i.e., not complex encoder U-Net), which incorporates a novel Sub-pixel Convolution for upsampling to construct a powerful decoder. Additionally, we introduce multi-scale wavelet inputs module on the encoder side to provide additional information. Our model design achieves excellent results, surpassing other state-of-the-art methods on both the Synapse and ACDC datasets.
</details>
<details>
<summary>æ‘˜è¦</summary>
U-Netå’Œå…¶å˜ç§åœ¨åŒ»å­¦å½±åƒåˆ†å‰²ä¸­å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œç°æœ‰çš„U-Netå˜ç§é€šå¸¸æ˜¯é€šè¿‡å»ºç«‹æ›´å¤æ‚çš„ç¼–ç å™¨æ¥æé«˜æ€§èƒ½ï¼Œè€Œå¿½ç•¥äº†è§£ç å™¨çš„çœŸæ­£åŠŸèƒ½ï¼šæ¥æ”¶ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾å¹¶å°†å…¶ä¿®å¤åˆ°åŸå§‹åˆ†è¾¨ç‡å’Œä¸¢å¤±ä¿¡æ¯ã€‚è¿™äº›æ–¹æ³•å¿½ç•¥äº†è§£ç å™¨ä¸­çš„upsamplingç»„ä»¶çš„é‡è¦ä½œç”¨ï¼Œè¿™ä½¿å¾—åˆ†å‰²ç»“æœå—åˆ°é™åˆ¶ã€‚å°¤å…¶åœ¨3DåŒ»å­¦å½±åƒåˆ†å‰²ä¸­ï¼Œé€šå¸¸ä½¿ç”¨çš„æ‹¼æ¥ convolution å¯èƒ½ä¼šå¯¼è‡´è§†è§‰artefactsã€‚è¿™ç§é—®é¢˜çš„åŸå› åœ¨äºè¾“å‡ºç‰¹å¾å›¾ä¸­ä¸å­˜åœ¨ç›´æ¥ç›¸é‚»åƒç´ çš„ç›´æ¥å…³ç³»ã€‚æ­¤å¤–ï¼Œç®€å•çš„ç¼–ç å™¨å·²ç»æ‹¥æœ‰äº†å……è¶³çš„ç‰¹å¾æå–èƒ½åŠ›ï¼Œå› ä¸ºä¸‹é™æ“ä½œå¯¼è‡´æ•æ‰åŒºåŸŸçš„æ‰©å±•ï¼Œä½†æ˜¯ä¸‹é™æ“ä½œä¸­ä¸¢å¤±çš„ä¿¡æ¯æ˜¯ä¸å¯å¿½ç•¥çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªç ”ç©¶æ¼æ´ï¼Œæˆ‘ä»¬æ‰©å±•äº†æˆ‘ä»¬çš„å…³æ³¨èŒƒå›´ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„Sub-pixel Convolution Ğ´Ğ»Ñ upsamplingï¼Œä»¥å»ºç«‹ä¸€ä¸ªå¼ºå¤§çš„è§£ç å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†å¤šå°ºåº¦waveletè¾“å…¥æ¨¡å—åœ¨ç¼–ç å™¨Sideæ¥æä¾›é¢å¤–ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ¨¡å‹è®¾è®¡å®ç°äº†å‡ºè‰²çš„ç»“æœï¼Œè¶…è¿‡äº†å…¶ä»–çŠ¶æ€å¯¹çš„æ–¹æ³•åœ¨Synapseå’ŒACDCæ•°æ®é›†ä¸Šã€‚
</details></li>
</ul>
<hr>
<h2 id="Shape-Anchor-Guided-Holistic-Indoor-Scene-Understanding"><a href="#Shape-Anchor-Guided-Holistic-Indoor-Scene-Understanding" class="headerlink" title="Shape Anchor Guided Holistic Indoor Scene Understanding"></a>Shape Anchor Guided Holistic Indoor Scene Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11133">http://arxiv.org/abs/2309.11133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Geo-Tell/AncRec">https://github.com/Geo-Tell/AncRec</a></li>
<li>paper_authors: Mingyue Dong, Linxi Huan, Hanjiang Xiong, Shuhan Shen, Xianwei Zheng</li>
<li>for: æå‡ºäº†ä¸€ç§åŸºäºå½¢æ€é”šç‚¹çš„å­¦ä¹ ç­–ç•¥ï¼ˆAncLearnï¼‰ï¼Œç”¨äºå®ç°å®¤å†…Sceneç†è§£çš„ç¨³å®šå’Œå‡†ç¡®æ€§ã€‚</li>
<li>methods: åˆ©ç”¨å½¢æ€é”šç‚¹ç”Ÿæˆanchorsï¼Œä»¥ä¾¿åœ¨æœç´¢ç©ºé—´ä¸­æå–å®é™…å­˜åœ¨çš„å¯¹è±¡è¡¨ç¤ºï¼Œå¹¶ä¸”é€šè¿‡å¯¹å™ªå£°å’Œç›®æ ‡ç›¸å…³ç‰¹å¾è¿›è¡Œåˆ†ç¦»ï¼Œæä¾›å¯é çš„æè®®ã€‚åœ¨é‡å»ºé˜¶æ®µï¼Œé€šè¿‡å‡å°‘å¼‚å¸¸å€¼ï¼Œæä¾›é«˜è´¨é‡çš„å¯¹è±¡ç‚¹æŠ½è±¡ã€‚</li>
<li>results: åœ¨ScanNetv2 datasetä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶å–å¾—äº†åœ¨3Då¯¹è±¡æ£€æµ‹ã€å¸ƒå±€ä¼°è®¡å’Œå½¢æ€é‡å»ºæ–¹é¢çš„çŠ¶æ€ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹æ™ºèƒ½æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
This paper proposes a shape anchor guided learning strategy (AncLearn) for robust holistic indoor scene understanding. We observe that the search space constructed by current methods for proposal feature grouping and instance point sampling often introduces massive noise to instance detection and mesh reconstruction. Accordingly, we develop AncLearn to generate anchors that dynamically fit instance surfaces to (i) unmix noise and target-related features for offering reliable proposals at the detection stage, and (ii) reduce outliers in object point sampling for directly providing well-structured geometry priors without segmentation during reconstruction. We embed AncLearn into a reconstruction-from-detection learning system (AncRec) to generate high-quality semantic scene models in a purely instance-oriented manner. Experiments conducted on the challenging ScanNetv2 dataset demonstrate that our shape anchor-based method consistently achieves state-of-the-art performance in terms of 3D object detection, layout estimation, and shape reconstruction. The code will be available at https://github.com/Geo-Tell/AncRec.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Locate-and-Verify-A-Two-Stream-Network-for-Improved-Deepfake-Detection"><a href="#Locate-and-Verify-A-Two-Stream-Network-for-Improved-Deepfake-Detection" class="headerlink" title="Locate and Verify: A Two-Stream Network for Improved Deepfake Detection"></a>Locate and Verify: A Two-Stream Network for Improved Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11131">http://arxiv.org/abs/2309.11131</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sccsok/Locate-and-Verify">https://github.com/sccsok/Locate-and-Verify</a></li>
<li>paper_authors: Chao Shuai, Jieming Zhong, Shuang Wu, Feng Lin, Zhibo Wang, Zhongjie Ba, Zhenguang Liu, Lorenzo Cavallaro, Kui Ren</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ·±ä¼ªæ£€æµ‹æ–¹æ³•çš„ä¸€èˆ¬åŒ–èƒ½åŠ›å’Œç‰¹å®š forgery åŒºåŸŸæ¢æµ‹èƒ½åŠ›ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸‰ä¸ªæ–¹æ³•æ¥è§£å†³ç°æœ‰æ–¹æ³•çš„ç¼ºé™·ï¼šä¸€ä¸ªåˆ›æ–°çš„ä¸¤æ¡æµç½‘ç»œï¼Œä¸‰ä¸ªåŠŸèƒ½æ¨¡ç»„ï¼Œä»¥åŠä¸€ä¸ªåŠsupervised Patch Similarity Learningç­–ç•¥ã€‚</li>
<li>results: æœ¬æ–‡çš„æ–¹æ³•åœ¨å…­ä¸ªbenchmarkä¸Šä¸ç°æœ‰æ–¹æ³•æ¯”è¾ƒï¼Œè¡¨ç°å‡ºäº†significantly improvedçš„ä¸€èˆ¬åŒ–å’Œç‰¹å®š forgery åŒºåŸŸæ¢æµ‹èƒ½åŠ›ï¼ŒåŒ…æ‹¬Frame-level AUCåœ¨Deepfake Detection Challenge preview datasetä¸Šä»0.797æé«˜åˆ°0.835ï¼Œä»¥åŠVideo-level AUCåœ¨CelebDF$_$v1 datasetä¸Šä»0.811æé«˜åˆ°0.847ã€‚<details>
<summary>Abstract</summary>
Deepfake has taken the world by storm, triggering a trust crisis. Current deepfake detection methods are typically inadequate in generalizability, with a tendency to overfit to image contents such as the background, which are frequently occurring but relatively unimportant in the training dataset. Furthermore, current methods heavily rely on a few dominant forgery regions and may ignore other equally important regions, leading to inadequate uncovering of forgery cues. In this paper, we strive to address these shortcomings from three aspects: (1) We propose an innovative two-stream network that effectively enlarges the potential regions from which the model extracts forgery evidence. (2) We devise three functional modules to handle the multi-stream and multi-scale features in a collaborative learning scheme. (3) Confronted with the challenge of obtaining forgery annotations, we propose a Semi-supervised Patch Similarity Learning strategy to estimate patch-level forged location annotations. Empirically, our method demonstrates significantly improved robustness and generalizability, outperforming previous methods on six benchmarks, and improving the frame-level AUC on Deepfake Detection Challenge preview dataset from 0.797 to 0.835 and video-level AUC on CelebDF$\_$v1 dataset from 0.811 to 0.847. Our implementation is available at https://github.com/sccsok/Locate-and-Verify.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åˆ»çš„å‡åŠ¨ä½œï¼ˆDeepfakeï¼‰å·²ç»åœ¨ä¸–ç•Œä¸Šå¼•å‘äº†ä¸€åœºä¿¡ä»»å±æœºã€‚ç›®å‰çš„å‡åŠ¨ä½œæ£€æµ‹æ–¹æ³•é€šå¸¸æ— æ³•æ™®éåŒ–ï¼Œå¾€å¾€å¯¹èƒŒæ™¯è¿›è¡Œè¿‡æ»¤ï¼Œè¿™äº›èƒŒæ™¯è™½ç„¶å¸¸è§ä½†ç›¸å¯¹ speaking ä¸é‡è¦ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æ–¹æ³•å€¾å‘äºä»…å¯¹ä¸€äº›ä¸»å¯¼çš„ä¼ªé€ åŒºåŸŸè¿›è¡Œè¿‡æ»¤ï¼Œå¯èƒ½ä¼šå¿½ç•¥å…¶ä»–Equally important regionsï¼Œä»è€Œå¯¼è‡´ä¼ªé€ è®¯å·çš„ä¸å……åˆ†æ¢æµ‹ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°è¯•è§£å†³è¿™äº›ç¼ºé™·è‡ªä¸‰ä¸ªæ–¹é¢ï¼š1. æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„ä¸¤æ¡æµç½‘ç»œï¼Œå®é™…åœ°æ‰©å¤§äº†æ¨¡å‹ä»ä¸­æå–ä¼ªé€ è¯æ®çš„å¯èƒ½åŒºåŸŸã€‚2. æˆ‘ä»¬è®¾è®¡äº†ä¸‰ä¸ªåŠŸèƒ½æ¨¡ç»„ï¼Œä»¥å®ç°å¤šæ¡æµå’Œå¤šä¸ªæ ‡å‡†ä¹‹é—´çš„åˆä½œå­¦ä¹ ã€‚3. é¢å¯¹ä¼ªé€ æ ‡æ³¨çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŠsupervised Patch Similarity Learningç­–ç•¥ï¼Œä»¥ä¼°è®¡ä¼ªé€ åŒºåŸŸæ ‡æ³¨ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…­ä¸ªbenchmarkä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸å‰ä¸€ä»£æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´å¥½çš„ Robustness å’Œæ™®éåŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å®ç°å¯ä»¥åœ¨https://github.com/sccsok/Locate-and-Verifyä¸Šæ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="PSDiff-Diffusion-Model-for-Person-Search-with-Iterative-and-Collaborative-Refinement"><a href="#PSDiff-Diffusion-Model-for-Person-Search-with-Iterative-and-Collaborative-Refinement" class="headerlink" title="PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement"></a>PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11125">http://arxiv.org/abs/2309.11125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang, Jingdong Wang, Qinghua Zheng</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„äººå‘˜æœç´¢æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰æ–¹æ³•ä¸­çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š1ï¼‰æ¢æµ‹é˜¶æ®µæ¨¡å—ä¸é€‚åˆäººè„¸è¯†åˆ«ä»»åŠ¡ï¼›2ï¼‰ä¸¤ä¸ªå­ä»»åŠ¡ä¹‹é—´çš„åä½œè¢«å¿½ç•¥ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†Diffusionæ¨¡å‹ï¼Œå°†äººå‘˜æœç´¢è½¬åŒ–ä¸ºä¸¤ä¸ªé˜¶æ®µçš„åŒæ‚åŒ–è¿‡ç¨‹ï¼Œä»å™ªå£°æ¡†å’Œäººè„¸åµŒå…¥è½¬åŒ–ä¸ºå®é™…æƒ…å†µã€‚ä¸ä¼ ç»Ÿçš„æ¢æµ‹åˆ°äººè„¸è¯†åˆ«çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ‚åŒ–æ–¹æ³•å¯ä»¥æ¶ˆé™¤æ¢æµ‹é˜¶æ®µæ¨¡å—ï¼Œä»è€Œé¿å…äººè„¸è¯†åˆ«ä»»åŠ¡çš„åœ°æ–¹æœ€ä¼˜ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æ–°çš„ååŒæ‚åŒ–å±‚ï¼Œä»¥ä¾¿åœ¨æ¢æµ‹å’Œäººè„¸è¯†åˆ«ä¸¤ä¸ªå­ä»»åŠ¡ä¹‹é—´è¿›è¡ŒèåˆååŒï¼Œä½¿ä¸¤ä¸ªå­ä»»åŠ¡äº’ç›¸å¸®åŠ©ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒPSDiffåœ¨æ ‡å‡†æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†å½“å‰æœ€ä½³æ€§èƒ½ï¼Œå…·æœ‰è¾ƒå°‘çš„å‚æ•°å’Œçµæ´»è®¡ç®—è´Ÿæ‹…ã€‚<details>
<summary>Abstract</summary>
Dominant Person Search methods aim to localize and recognize query persons in a unified network, which jointly optimizes two sub-tasks, \ie, detection and Re-IDentification (ReID). Despite significant progress, two major challenges remain: 1) Detection-prior modules in previous methods are suboptimal for the ReID task. 2) The collaboration between two sub-tasks is ignored. To alleviate these issues, we present a novel Person Search framework based on the Diffusion model, PSDiff. PSDiff formulates the person search as a dual denoising process from noisy boxes and ReID embeddings to ground truths. Unlike existing methods that follow the Detection-to-ReID paradigm, our denoising paradigm eliminates detection-prior modules to avoid the local-optimum of the ReID task. Following the new paradigm, we further design a new Collaborative Denoising Layer (CDL) to optimize detection and ReID sub-tasks in an iterative and collaborative way, which makes two sub-tasks mutually beneficial. Extensive experiments on the standard benchmarks show that PSDiff achieves state-of-the-art performance with fewer parameters and elastic computing overhead.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸»æµäººä½“æœç´¢æ–¹æ³•ç›®æ ‡æ˜¯åœ¨ä¸€ä¸ªç»Ÿä¸€ç½‘ç»œä¸­æœ¬åœ°åŒ–å’Œè¯†åˆ«æŸ¥è¯¢äººä½“ï¼ŒåŒæ—¶ä¼˜åŒ–ä¸¤ä¸ªå­ä»»åŠ¡ï¼Œå³æ¢æµ‹å’ŒReIDï¼ˆäººä½“è¯†åˆ«ï¼‰ã€‚å°½ç®¡æœ‰äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ï¼š1ï¼‰æ¢æµ‹ä¼˜å…ˆæ¨¡å—åœ¨å…ˆå‰çš„æ–¹æ³•ä¸­æ˜¯ä¸ä½³çš„ReIDä»»åŠ¡ã€‚2ï¼‰ä¸¤ä¸ªå­ä»»åŠ¡ä¹‹é—´çš„åˆä½œè¢«å¿½è§†ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºDiffusionæ¨¡å‹çš„äººä½“æœç´¢æ¡†æ¶ï¼Œç§°ä¸ºPSDiffã€‚PSDiffå°†äººä½“æœç´¢è½¬åŒ–ä¸ºä¸€ä¸ªåŒæ–¹å‡å™ªè¿‡ç¨‹ï¼Œä»å™ªå£°æ¡†å’ŒReIDåµŒå…¥è½¬åŒ–åˆ°å®é™…å€¼ã€‚ä¸å…ˆå‰çš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„å‡å™ªæ–¹æ³•ä¸éœ€è¦æ¢æµ‹ä¼˜å…ˆæ¨¡å—ï¼Œä»¥é¿å…æ¢æµ‹ä»»åŠ¡çš„æœ¬åœ°æœ€ä½³ç‚¹ã€‚åœ¨æ–°çš„ paradigmaä¸‹ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è®¾è®¡äº†ä¸€ä¸ªæ–°çš„åˆä½œå‡å™ªå±‚ï¼ˆCDLï¼‰ï¼Œä»¥ä¾¿åœ¨è¿­ä»£å’ŒååŒçš„æ–¹å¼ä¼˜åŒ–æ¢æµ‹å’ŒReIDå­ä»»åŠ¡ï¼Œä½¿ä¸¤ä¸ªå­ä»»åŠ¡äº’ç›¸æœ‰åˆ©ã€‚ç»éªŒè¡¨æ˜ï¼ŒPSDiffåœ¨æ ‡å‡†æµ‹è¯•å‡†åˆ™ä¸Šè¾¾åˆ°äº†çŠ¶æ€çš„ç²¾åº¦æ€§è¡¨ç°ï¼Œå¹¶ä¸”å…·æœ‰ fewer å‚æ•°å’Œçµæ´»è®¡ç®—è´Ÿæ‹…ã€‚
</details></li>
</ul>
<hr>
<h2 id="Hyperspectral-Benchmark-Bridging-the-Gap-between-HSI-Applications-through-Comprehensive-Dataset-and-Pretraining"><a href="#Hyperspectral-Benchmark-Bridging-the-Gap-between-HSI-Applications-through-Comprehensive-Dataset-and-Pretraining" class="headerlink" title="Hyperspectral Benchmark: Bridging the Gap between HSI Applications through Comprehensive Dataset and Pretraining"></a>Hyperspectral Benchmark: Bridging the Gap between HSI Applications through Comprehensive Dataset and Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11122">http://arxiv.org/abs/2309.11122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogsys-tuebingen/hsi_benchmark">https://github.com/cogsys-tuebingen/hsi_benchmark</a></li>
<li>paper_authors: Hannah Frank, Leon Amadeus Varga, Andreas Zell</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢çš„ä¸“é—¨åº”ç”¨äºå‡ ä½•pectralå®éªŒï¼ˆHSIï¼‰çš„benchmark datasetï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯„ä¼°å‡ ä½•spectralæ¨¡å‹çš„èƒ½åŠ›ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„benchmark datasetï¼ŒåŒ…æ‹¬ä¸‰ä¸ªä¸åŒçš„HSIåº”ç”¨ï¼šé£Ÿå“æ£€æŸ¥ã€è¿œç¨‹æ„ŸçŸ¥å’Œå›æ”¶ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ä¸ªé¢„è®­ç®¡é“ï¼Œä»¥æé«˜ä¸“é—¨çš„è®­ç»ƒè¿‡ç¨‹ç¨³å®šæ€§ã€‚</li>
<li>results: æœ¬ç ”ç©¶çš„ç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸ªbenchmark datasetå¯ä»¥æ›´å¥½åœ°è¯„ä¼°ä¸“é—¨çš„HSIæ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥æ¨å¹¿ç°æœ‰çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œé¢„è®­ç®¡é“å¯ä»¥æé«˜ä¸“é—¨çš„è®­ç»ƒè¿‡ç¨‹ç¨³å®šæ€§ã€‚<details>
<summary>Abstract</summary>
Hyperspectral Imaging (HSI) serves as a non-destructive spatial spectroscopy technique with a multitude of potential applications. However, a recurring challenge lies in the limited size of the target datasets, impeding exhaustive architecture search. Consequently, when venturing into novel applications, reliance on established methodologies becomes commonplace, in the hope that they exhibit favorable generalization characteristics. Regrettably, this optimism is often unfounded due to the fine-tuned nature of models tailored to specific HSI contexts.   To address this predicament, this study introduces an innovative benchmark dataset encompassing three markedly distinct HSI applications: food inspection, remote sensing, and recycling. This comprehensive dataset affords a finer assessment of hyperspectral model capabilities. Moreover, this benchmark facilitates an incisive examination of prevailing state-of-the-art techniques, consequently fostering the evolution of superior methodologies.   Furthermore, the enhanced diversity inherent in the benchmark dataset underpins the establishment of a pretraining pipeline for HSI. This pretraining regimen serves to enhance the stability of training processes for larger models. Additionally, a procedural framework is delineated, offering insights into the handling of applications afflicted by limited target dataset sizes.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¹²elespectral Imagingï¼ˆHSIï¼‰æ˜¯ä¸€ç§ä¸ destrucciÃ³nçš„ç©ºé—´ÑĞ¿ĞµĞºÑ‚Ñ€Ğ¾ÑĞºĞ¾Ğ¿Ğ¸æŠ€æœ¯ï¼Œå…·æœ‰å„ç§åº”ç”¨å‰æ™¯ã€‚ç„¶è€Œï¼Œä¸€ä¸ªå¸¸ recursçš„æŒ‘æˆ˜æ˜¯ç›®æ ‡æ•°æ®é›†çš„æœ‰é™å¤§å°ï¼Œå¯¼è‡´äº†è¾ƒå°‘çš„æ¨¡å‹æœç´¢ç©ºé—´ã€‚å› æ­¤ï¼Œåœ¨æ¢ç´¢æ–°åº”ç”¨åœºæ™¯æ—¶ï¼Œé€šå¸¸ä¼šä¾é å·²æœ‰çš„æ–¹æ³•ï¼Œå¸Œæœ›å®ƒä»¬åœ¨ä¸åŒçš„HSIä¸Šèƒ½å¤Ÿå±•ç°è‰¯å¥½çš„æ³›åŒ–ç‰¹æ€§ã€‚ç„¶è€Œï¼Œè¿™ç§optimismé€šå¸¸æ˜¯ä¸ç¬¦çš„ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ˜¯ä¸ºç‰¹å®šHSIä¸Šç²¾å¿ƒå®šåˆ¶çš„ã€‚ä¸ºè§£å†³è¿™ä¸ªå›°å¢ƒï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„æ ‡å‡†æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ˜ç¡®ä¸åŒçš„HSIåº”ç”¨ï¼šé£Ÿå“æ£€æŸ¥ã€è¿œç¨‹æ„ŸçŸ¥å’Œå›æ”¶ã€‚è¿™ä¸ªå…¨é¢çš„æ•°æ®é›†ä¸ºå¹²elespectralæ¨¡å‹çš„èƒ½åŠ›è¿›è¡Œæ›´åŠ ç»†è‡´çš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªæ ‡å‡†æ•°æ®é›†è¿˜æ”¯æŒç°æœ‰çš„state-of-the-artæŠ€æœ¯çš„å‡†ç¡®æ€§çš„å‡å¼±ï¼Œä»è€Œä¿ƒè¿›äº†æ›´é«˜æ°´å¹³çš„æ–¹æ³•çš„è¿›åŒ–ã€‚æ­¤å¤–ï¼Œå¢å¼ºçš„æ•°æ®é›†å¤šæ ·æ€§ä¸ºHSIé¢„è®­ç»ƒç®¡é“æä¾›äº†åŸºç¡€ã€‚è¿™ä¸ªé¢„è®­ç»ƒç®¡é“å¯ä»¥å¢å¼ºå¤§å‹æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§æ‰‹åŠ¨æ¡†æ¶ï¼Œç”¨äºå¤„ç†å—æœ‰é™targetæ•°æ®é›†å¤§å°çš„åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="BroadBEV-Collaborative-LiDAR-camera-Fusion-for-Broad-sighted-Birdâ€™s-Eye-View-Map-Construction"><a href="#BroadBEV-Collaborative-LiDAR-camera-Fusion-for-Broad-sighted-Birdâ€™s-Eye-View-Map-Construction" class="headerlink" title="BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Birdâ€™s Eye View Map Construction"></a>BroadBEV: Collaborative LiDAR-camera Fusion for Broad-sighted Birdâ€™s Eye View Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11119">http://arxiv.org/abs/2309.11119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsu Kim, Giseop Kim, Kyong Hwan Jin, Sunwook Choi</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ¿€å…‰æ‘„åƒæœºï¼ˆLiDARï¼‰å’Œæ‘„åƒæœºï¼ˆcameraï¼‰çš„ Birdâ€™s Eye Viewï¼ˆBEVï¼‰ç©ºé—´èåˆï¼Œä»¥å®ç°æ›´å¹¿æ³›çš„è§†åœºå’Œé«˜ç²¾åº¦çš„åœ°é¢æ£€æµ‹ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¹¿æ³›çš„ BEVèåˆç­–ç•¥ï¼ˆBroadBEVï¼‰ï¼ŒåŒ…æ‹¬ç‚¹æ•£å‘ï¼ˆPoint-scatteringï¼‰å’Œè‡ªæ³¨é‡æƒé‡ï¼ˆColFusionï¼‰ä¸¤ä¸ªéƒ¨åˆ†ã€‚ç‚¹æ•£å‘æ–¹æ³•ä½¿å¾—LiDAR BEVåˆ†å¸ƒæ•£å°„åˆ°æ‘„åƒæœºæ·±åº¦åˆ†å¸ƒä¸­ï¼Œä»¥æé«˜æ‘„åƒæœºåˆ†æ”¯çš„æ·±åº¦ä¼°è®¡å’Œç²¾åº¦ã€‚è‡ªæ³¨é‡æƒé‡æ–¹æ³•åœ¨LiDARå’Œæ‘„åƒæœº BEVç‰¹å¾ä¹‹é—´åº”ç”¨è‡ªæ³¨é‡æƒé‡ï¼Œä»¥å®ç°æœ‰æ•ˆçš„ BEVèåˆã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒBroadBEVå¯ä»¥æä¾›å¹¿æ³›çš„ BEVè§†åœºï¼Œå¹¶ä¸”æœ‰è¾ƒé«˜çš„æ€§èƒ½æå‡ã€‚<details>
<summary>Abstract</summary>
A recent sensor fusion in a Bird's Eye View (BEV) space has shown its utility in various tasks such as 3D detection, map segmentation, etc. However, the approach struggles with inaccurate camera BEV estimation, and a perception of distant areas due to the sparsity of LiDAR points. In this paper, we propose a broad BEV fusion (BroadBEV) that addresses the problems with a spatial synchronization approach of cross-modality. Our strategy aims to enhance camera BEV estimation for a broad-sighted perception while simultaneously improving the completion of LiDAR's sparsity in the entire BEV space. Toward that end, we devise Point-scattering that scatters LiDAR BEV distribution to camera depth distribution. The method boosts the learning of depth estimation of the camera branch and induces accurate location of dense camera features in BEV space. For an effective BEV fusion between the spatially synchronized features, we suggest ColFusion that applies self-attention weights of LiDAR and camera BEV features to each other. Our extensive experiments demonstrate that BroadBEV provides a broad-sighted BEV perception with remarkable performance gains.
</details>
<details>
<summary>æ‘˜è¦</summary>
Recently, a sensor fusion in a bird's eye view (BEV) space has shown its potential in various tasks such as 3D detection and map segmentation. However, the approach is limited by inaccurate camera BEV estimation and a lack of information on distant areas due to the sparsity of LiDAR points. In this paper, we propose a broad BEV fusion (BroadBEV) that addresses these problems using a cross-modality spatial synchronization approach. Our method aims to improve camera BEV estimation for a broad-sighted perception while simultaneously enhancing the completion of LiDAR's sparsity in the entire BEV space. To achieve this, we use Point-scattering to scatter LiDAR BEV distribution to camera depth distribution, which boosts the learning of depth estimation of the camera branch and accurately locates dense camera features in BEV space. Additionally, we propose ColFusion, which applies self-attention weights of LiDAR and camera BEV features to each other for effective BEV fusion. Our extensive experiments show that BroadBEV provides a broad-sighted BEV perception with significant performance gains.
</details></li>
</ul>
<hr>
<h2 id="PRAT-PRofiling-Adversarial-aTtacks"><a href="#PRAT-PRofiling-Adversarial-aTtacks" class="headerlink" title="PRAT: PRofiling Adversarial aTtacks"></a>PRAT: PRofiling Adversarial aTtacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11111">http://arxiv.org/abs/2309.11111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rahulambati/PRAT">https://github.com/rahulambati/PRAT</a></li>
<li>paper_authors: Rahul Ambati, Naveed Akhtar, Ajmal Mian, Yogesh Singh Rawat</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯ä¸ºäº†æ£€æµ‹å’Œè¯†åˆ«æ·±åº¦å­¦ä¹ æ¨¡å‹é¢å¯¹æ”»å‡»æ—¶æ‰€äº§ç”Ÿçš„æ”»å‡»æ–¹æ³•ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„æ¶æ„ï¼Œå«åšGLOFï¼ˆGlobal-LOcal Featureï¼‰æ¨¡ç»„ï¼Œå®ƒå¯ä»¥å°†æ”»å‡»ç¤ºä¾‹ä¸­çš„ç‰¹å¾æå–å‡ºæ¥ï¼Œå¹¶ä¸”ç”¨äºè¯†åˆ«æ”»å‡»çš„æ–¹æ³•ã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªå¤§é‡çš„æ”»å‡»è¯†åˆ«æ•°æ®é›†ï¼ˆAIDï¼‰ï¼ŒåŒ…å«äº†180,000ä¸ªæ”»å‡»ç¤ºä¾‹ï¼Œå¹¶é€šè¿‡ä½¿ç”¨GLOFæ¨¡ç»„è¿›è¡Œæ”»å‡»è¯†åˆ«ï¼Œè·å¾—äº†å¤šä¸ªæœ‰è¶£çš„æ¯”è¾ƒç»“æœã€‚<details>
<summary>Abstract</summary>
Intrinsic susceptibility of deep learning to adversarial examples has led to a plethora of attack techniques with a broad common objective of fooling deep models. However, we find slight compositional differences between the algorithms achieving this objective. These differences leave traces that provide important clues for attacker profiling in real-life scenarios. Inspired by this, we introduce a novel problem of PRofiling Adversarial aTtacks (PRAT). Given an adversarial example, the objective of PRAT is to identify the attack used to generate it. Under this perspective, we can systematically group existing attacks into different families, leading to the sub-problem of attack family identification, which we also study. To enable PRAT analysis, we introduce a large Adversarial Identification Dataset (AID), comprising over 180k adversarial samples generated with 13 popular attacks for image specific/agnostic white/black box setups. We use AID to devise a novel framework for the PRAT objective. Our framework utilizes a Transformer based Global-LOcal Feature (GLOF) module to extract an approximate signature of the adversarial attack, which in turn is used for the identification of the attack. Using AID and our framework, we provide multiple interesting benchmark results for the PRAT problem.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ å†…ç½®çš„æ”»å‡»ä¾‹å­æ„Ÿå—æ€§é—®é¢˜ï¼Œå¯¼è‡´äº†è®¸å¤šæ”»å‡»æŠ€æœ¯çš„å‡ºç°ï¼Œå®ƒä»¬çš„å…±åŒç›®æ ‡éƒ½æ˜¯æ¬ºéª—æ·±åº¦æ¨¡å‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ”»å‡»æŠ€æœ¯ä¹‹é—´å­˜åœ¨è½»å¾®çš„ç»„åˆå·®å¼‚ï¼Œè¿™äº›å·®å¼‚ç•™ä¸‹äº†é‡è¦çš„æ”»å‡»è€…è¿½è¸ª tracesã€‚ inspirited by thisï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼šPRofiling Adversarial aTtacksï¼ˆPRATï¼‰ã€‚ç»™å®šä¸€ä¸ªæ”»å‡»ä¾‹å­ï¼ŒPRAT çš„ç›®æ ‡æ˜¯ç¡®å®šæ”»å‡»è¯¥ä¾‹å­çš„æ”»å‡»æ–¹æ³•ã€‚åŸºäºè¿™ç§è§†è§’ï¼Œæˆ‘ä»¬å¯ä»¥ç³»ç»Ÿåœ°å°†ç°æœ‰çš„æ”»å‡»åˆ†ä¸ºä¸åŒçš„å®¶æ—ï¼Œå¯¼è‡´äº†æ”»å‡»å®¶æ—è¯†åˆ«é—®é¢˜çš„ç ”ç©¶ï¼Œæˆ‘ä»¬ä¹Ÿè¿›è¡Œäº†è¿™ç§ç ”ç©¶ã€‚ä¸ºäº†å¯ç”¨ PRAT åˆ†æï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤§å‹çš„æ”»å‡»æ ‡è¯†æ•°æ®é›†ï¼ˆAIDï¼‰ï¼ŒåŒ…å«äº†180kå¤šä¸ªç”Ÿæˆäº†13ç§æµè¡Œçš„æ”»å‡»çš„æ”»å‡»ç¤ºä¾‹ï¼Œç”¨äºé»‘è‰²/ç™½è‰²ç›’å­è®¾ç½®ã€‚æˆ‘ä»¬ä½¿ç”¨ AID å’Œæˆ‘ä»¬çš„æ¡†æ¶ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶æ¥å®ç° PRAT ç›®æ ‡ã€‚æˆ‘ä»¬çš„æ¡†æ¶ä½¿ç”¨ Transformer åŸºäºçš„å…¨å±€-æœ¬åœ°ç‰¹å¾ï¼ˆGLOFï¼‰æ¨¡å—ï¼Œå°†æ”»å‡»ä¾‹å­ä¸­çš„æ”»å‡»ç‰¹å¾æå–å‡ºæ¥ï¼Œå¹¶ç”¨äºæ”»å‡»çš„è¯†åˆ«ã€‚ä½¿ç”¨ AID å’Œæˆ‘ä»¬çš„æ¡†æ¶ï¼Œæˆ‘ä»¬æä¾›äº†å¤šä¸ªæœ‰è¶£çš„ PRAT é—®é¢˜çš„ benchmark ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-Domain-agnostic-Domain-Adaptation-for-Satellite-Images"><a href="#Self-supervised-Domain-agnostic-Domain-Adaptation-for-Satellite-Images" class="headerlink" title="Self-supervised Domain-agnostic Domain Adaptation for Satellite Images"></a>Self-supervised Domain-agnostic Domain Adaptation for Satellite Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11109">http://arxiv.org/abs/2309.11109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fahong Zhang, Yilei Shi, Xiao Xiang Zhu</li>
<li>for:  Addressing the domain shift issue in machine learning for global scale satellite image processing.</li>
<li>methods:  Proposed an self-supervised domain-agnostic domain adaptation (SS(DA)2) method, which uses a contrastive generative adversarial loss to train a generative network for image-to-image translation, and improves the generalizability of downstream models by augmenting the training data with different testing spectral characteristics.</li>
<li>results:  Experimental results on public benchmarks verified the effectiveness of SS(DA)2.<details>
<summary>Abstract</summary>
Domain shift caused by, e.g., different geographical regions or acquisition conditions is a common issue in machine learning for global scale satellite image processing. A promising method to address this problem is domain adaptation, where the training and the testing datasets are split into two or multiple domains according to their distributions, and an adaptation method is applied to improve the generalizability of the model on the testing dataset. However, defining the domain to which each satellite image belongs is not trivial, especially under large-scale multi-temporal and multi-sensory scenarios, where a single image mosaic could be generated from multiple data sources. In this paper, we propose an self-supervised domain-agnostic domain adaptation (SS(DA)2) method to perform domain adaptation without such a domain definition. To achieve this, we first design a contrastive generative adversarial loss to train a generative network to perform image-to-image translation between any two satellite image patches. Then, we improve the generalizability of the downstream models by augmenting the training data with different testing spectral characteristics. The experimental results on public benchmarks verify the effectiveness of SS(DA)2.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŸŸå¤–è½¬ç§»é—®é¢˜ï¼Œå¦‚ä¸åŒåœ°ç†åŒºåŸŸæˆ–è·å–æ¡ä»¶ï¼Œæ˜¯æœºå™¨å­¦ä¹ åœ¨å…¨çƒèŒƒå›´å«æ˜Ÿå›¾åƒå¤„ç†ä¸­çš„å¸¸è§é—®é¢˜ã€‚ä¸€ç§æœ‰å‰é€”çš„æ–¹æ³•æ˜¯é¢†åŸŸé€‚åº”ï¼Œå…¶ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¢«åˆ†æˆä¸¤ä¸ªæˆ–å¤šä¸ªé¢†åŸŸï¼Œå¹¶åº”ç”¨é€‚åº”æ–¹æ³•ä»¥æé«˜æµ‹è¯•é›†æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚ç„¶è€Œï¼Œå®šä¹‰å„å«æ˜Ÿå›¾åƒå½’å±çš„é¢†åŸŸå¹¶ä¸æ˜¯æ˜“äº‹ï¼Œå°¤å…¶åœ¨å¤§è§„æ¨¡å¤šæ—¶é—´å’Œå¤šæ„Ÿå™¨åœºæ™¯ä¸‹ï¼Œä¸€ä¸ªå«æ˜Ÿå›¾åƒèåˆå¯èƒ½æ¥è‡ªå¤šä¸ªæ•°æ®æºã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªä¸»é€‚åº”é¢†åŸŸæ— å…³çš„è‡ªåŠ¨é€‚åº”ï¼ˆSS(DA)2ï¼‰æ–¹æ³•ï¼Œæ— éœ€å®šä¹‰å„å«æ˜Ÿå›¾åƒçš„é¢†åŸŸã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè®¾è®¡äº†ä¸€ç§å¯¹æ¯”ç”Ÿæˆéšè—å±‚çš„æŒ‘æˆ˜æ¨èæŸå¤±ï¼Œä»¥è®­ç»ƒç”Ÿæˆç½‘ç»œè¿›è¡Œå«æ˜Ÿå›¾åƒå—ä¹‹é—´çš„è‡ªåŠ¨ç¿»è¯‘ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å¢åŠ ä¸åŒæµ‹è¯•spectralç‰¹å¾æ¥æé«˜ä¸‹æ¸¸æ¨¡å‹çš„æ³›åŒ–æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSS(DA)2æœ‰æ•ˆåœ°è§£å†³äº†åŸŸå¤–è½¬ç§»é—®é¢˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Forgery-aware-Adaptive-Vision-Transformer-for-Face-Forgery-Detection"><a href="#Forgery-aware-Adaptive-Vision-Transformer-for-Face-Forgery-Detection" class="headerlink" title="Forgery-aware Adaptive Vision Transformer for Face Forgery Detection"></a>Forgery-aware Adaptive Vision Transformer for Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11092">http://arxiv.org/abs/2309.11092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anwei Luo, Rizhao Cai, Chenqi Kong, Xiangui Kang, Jiwu Huang, Alex C. Kot</li>
<li>for: ä¿æŠ¤ authentication å®Œæ•´æ€§ï¼Œé˜²æ­¢ face ä¼ªé€ æ”»å‡»ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§ Novel Forgery-aware Adaptive Vision Transformer (FA-ViT)ï¼Œå…·æœ‰å†»ç»“ vanilla ViT çš„å‚æ•°ï¼Œå¹¶é‡‡ç”¨ Local-aware Forgery Injector (LFI) å’Œ Global-aware Forgery Adaptor (GFA) ä¸¤ç§ç‰¹æ®Šç»„ä»¶ï¼Œä»¥é€‚åº”ä¼ªé€ ç›¸å…³çš„çŸ¥è¯†ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ FA-ViT åœ¨ cross-dataset è¯„ä¼°å’Œ cross- manipulate åœºæ™¯ä¸­è¾¾åˆ°äº†çŠ¶æ€æœºå™¨äººçš„æ€§èƒ½ï¼Œå¹¶æé«˜äº†å¯¹æœªç»çœ‹åˆ°çš„å¹²æ‰°çš„Robustnessã€‚<details>
<summary>Abstract</summary>
With the advancement in face manipulation technologies, the importance of face forgery detection in protecting authentication integrity becomes increasingly evident. Previous Vision Transformer (ViT)-based detectors have demonstrated subpar performance in cross-database evaluations, primarily because fully fine-tuning with limited Deepfake data often leads to forgetting pre-trained knowledge and over-fitting to data-specific ones. To circumvent these issues, we propose a novel Forgery-aware Adaptive Vision Transformer (FA-ViT). In FA-ViT, the vanilla ViT's parameters are frozen to preserve its pre-trained knowledge, while two specially designed components, the Local-aware Forgery Injector (LFI) and the Global-aware Forgery Adaptor (GFA), are employed to adapt forgery-related knowledge. our proposed FA-ViT effectively combines these two different types of knowledge to form the general forgery features for detecting Deepfakes. Specifically, LFI captures local discriminative information and incorporates these information into ViT via Neighborhood-Preserving Cross Attention (NPCA). Simultaneously, GFA learns adaptive knowledge in the self-attention layer, bridging the gap between the two different domain. Furthermore, we design a novel Single Domain Pairwise Learning (SDPL) to facilitate fine-grained information learning in FA-ViT. The extensive experiments demonstrate that our FA-ViT achieves state-of-the-art performance in cross-dataset evaluation and cross-manipulation scenarios, and improves the robustness against unseen perturbations.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€äººè„¸æœæ’°æŠ€æœ¯çš„å‘å±•ï¼Œä¿æŠ¤èº«ä»½éªŒè¯çš„ authenticty integrity æˆä¸ºè¶Šæ¥è¶Šé‡è¦çš„ã€‚å…ˆå‰çš„ Vision Transformer (ViT) åŸºäºçš„æ£€æµ‹å™¨åœ¨è·¨æ•°æ®åº“è¯„ä¼°ä¸­è¡¨ç°ä¸ä½³ï¼Œä¸»è¦å› ä¸ºå®Œå…¨ç²¾åº¦è°ƒæ•´ WITH æœ‰é™çš„ Deepfake æ•°æ®é€šå¸¸ä¼šå¯¼è‡´å¿˜è®°é¢„è®­ç»ƒçŸ¥è¯†å¹¶è¿‡æ‹Ÿåˆæ•°æ®åº“specific çš„çŸ¥è¯†ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ novel Forgery-aware Adaptive Vision Transformer (FA-ViT)ã€‚åœ¨ FA-ViT ä¸­ï¼Œvanilla ViT çš„å‚æ•°è¢«å†»ç»“ï¼Œä»¥ä¿æŒå…¶é¢„è®­ç»ƒçš„çŸ¥è¯†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ä¸ªç‰¹åˆ¶çš„ç»„ä»¶ï¼šLocal-aware Forgery Injector (LFI) å’Œ Global-aware Forgery Adaptor (GFA)ã€‚LFI æ•æ‰äº†åœ°æ–¹ç‰¹å¾ä¿¡æ¯ï¼Œå¹¶å°†è¿™äº›ä¿¡æ¯ä¸ Neighborhood-Preserving Cross Attention (NPCA) ç»“åˆï¼Œä»¥ä¾¿åœ¨ ViT ä¸­æ•æ‰åˆ°åœ°æ–¹ç‰¹å¾ã€‚è€Œ GFA åœ¨è‡ªæ³¨æ„å±‚ä¸­å­¦ä¹ äº†é€‚åº”æ€§çŸ¥è¯†ï¼Œ bridging the gap  Ğ¼ĞµĞ¶Ğ´Ñƒä¸¤ç§ä¸åŒçš„é¢†åŸŸã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§ novel Single Domain Pairwise Learning (SDPL)ï¼Œä»¥ä¾¿åœ¨ FA-ViT ä¸­è¿›è¡Œç»†åŒ–ä¿¡æ¯å­¦ä¹ ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ FA-ViT åœ¨è·¨æ•°æ®åº“è¯„ä¼°å’Œè·¨æœæ’°åœºæ™¯ä¸­è¡¨ç°å‡ºäº† state-of-the-art çš„æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¯¹æœªç»è§æœæ’°çš„æ”»å‡»è¿›è¡Œé²æ£’åŒ–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-Segment-Similarity-and-Alignment-in-Large-Scale-Content-Based-Video-Retrieval"><a href="#Learning-Segment-Similarity-and-Alignment-in-Large-Scale-Content-Based-Video-Retrieval" class="headerlink" title="Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval"></a>Learning Segment Similarity and Alignment in Large-Scale Content Based Video Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11091">http://arxiv.org/abs/2309.11091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiang, Kaiming Huang, Sifeng He, Xudong Yang, Wei Zhang, Xiaobo Zhang, Yuan Cheng, Lei Yang, Qing Wang, Furong Xu, Tan Pan, Wei Chu</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ—¨åœ¨æé«˜å†…å®¹åŸºäºè§†é¢‘æ£€ç´¢ï¼ˆCBVRï¼‰çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œå°¤å…¶æ˜¯åœ¨é•¿è§†é¢‘åœºæ™¯ä¸‹ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè‡ªåŠ©å­¦ä¹ çš„ Segment Similarity and Alignment Network (SSAN)ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–°æå‡ºçš„æ¨¡å—ï¼š(1) é«˜æ•ˆçš„è‡ªåŠ¨ç”Ÿæˆå…³é”®å¸§EXTractionï¼ˆSKEï¼‰æ¨¡å—ï¼Œ(2) ç¨³å®šçš„ Similarity Pattern Detectionï¼ˆSPDï¼‰æ¨¡å—ã€‚</li>
<li>results: å¯¹äºå…¬å…±æ•°æ®é›†çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSSANå¯ä»¥è·å¾—æ›´é«˜çš„Alignmentç²¾åº¦ï¼ŒåŒæ—¶å‡å°‘å­˜å‚¨å’Œåœ¨çº¿æŸ¥è¯¢è®¡ç®—æˆæœ¬ï¼Œæ¯”æ—¢æœ‰æ–¹æ³•æ›´é«˜ã€‚<details>
<summary>Abstract</summary>
With the explosive growth of web videos in recent years, large-scale Content-Based Video Retrieval (CBVR) becomes increasingly essential in video filtering, recommendation, and copyright protection. Segment-level CBVR (S-CBVR) locates the start and end time of similar segments in finer granularity, which is beneficial for user browsing efficiency and infringement detection especially in long video scenarios. The challenge of S-CBVR task is how to achieve high temporal alignment accuracy with efficient computation and low storage consumption. In this paper, we propose a Segment Similarity and Alignment Network (SSAN) in dealing with the challenge which is firstly trained end-to-end in S-CBVR. SSAN is based on two newly proposed modules in video retrieval: (1) An efficient Self-supervised Keyframe Extraction (SKE) module to reduce redundant frame features, (2) A robust Similarity Pattern Detection (SPD) module for temporal alignment. In comparison with uniform frame extraction, SKE not only saves feature storage and search time, but also introduces comparable accuracy and limited extra computation time. In terms of temporal alignment, SPD localizes similar segments with higher accuracy and efficiency than existing deep learning methods. Furthermore, we jointly train SSAN with SKE and SPD and achieve an end-to-end improvement. Meanwhile, the two key modules SKE and SPD can also be effectively inserted into other video retrieval pipelines and gain considerable performance improvements. Experimental results on public datasets show that SSAN can obtain higher alignment accuracy while saving storage and online query computational cost compared to existing methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€ç½‘ç»œè§†é¢‘çš„å¿«é€Ÿå¢é•¿ï¼Œå¤§è§„æ¨¡çš„å†…å®¹åŸºäºè§†é¢‘æ£€ç´¢ï¼ˆCBVRï¼‰åœ¨è§†é¢‘ç­›é€‰ã€æ¨èå’Œç‰ˆæƒä¿æŠ¤ä¸­å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚segmentçº§CBVRï¼ˆS-CBVRï¼‰å¯ä»¥åœ¨æ›´ç»†ç²’åº¦ä¸Šå®šä½ç›¸ä¼¼çš„åˆ†å‰²æ—¶é—´ï¼Œè¿™å¯¹ç”¨æˆ·æµè§ˆæ•ˆç‡å’Œä¾µæƒæ£€æµ‹å°¤ä¸ºé‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿è§†é¢‘åœºæ™¯ä¸‹ã€‚S-CBVRä»»åŠ¡çš„æŒ‘æˆ˜æ˜¯å¦‚ä½•å®ç°é«˜ç²¾åº¦æ—¶é—´å¯¹å¯¹åº”å’Œé«˜æ•ˆè®¡ç®—ä¸”å¿«é€Ÿå­˜å‚¨æ¶ˆè€—ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Segment Similarity and Alignment Networkï¼ˆSSANï¼‰æ¥è§£å†³è¿™ä¸ªæŒ‘æˆ˜ã€‚SSANåŸºäºä¸¤ä¸ªæ–°æå‡ºçš„æ¨¡å—ï¼šï¼ˆ1ï¼‰é«˜æ•ˆçš„è‡ªåŠ¨å­¦ä¹ é”®å¸§EXTRACTIONï¼ˆSKEï¼‰æ¨¡å—ï¼Œä»¥å‡å°‘ç¼“å­˜å’Œæœç´¢æ—¶é—´ï¼ŒåŒæ—¶ä¿æŒç›¸ä¼¼æ€§å’Œç²¾åº¦ï¼›ï¼ˆ2ï¼‰Robustçš„åŒæ—¶é—´æ¨¡å¼æ£€æµ‹ï¼ˆSPDï¼‰æ¨¡å—ï¼Œç”¨äºæ—¶é—´å¯¹å¯¹åº”ã€‚ç›¸æ¯”äºå›ºå®šå¸§EXTRACTIONï¼ŒSKEä¸ä»…å‡å°‘äº†ç‰¹å¾å­˜å‚¨å’Œæœç´¢æ—¶é—´ï¼Œè¿˜å¼•å…¥äº†ç›¸ä¼¼çš„å‡†ç¡®æ€§å’Œæœ‰é™çš„é¢å¤–è®¡ç®—æ—¶é—´ã€‚åœ¨æ—¶é—´å¯¹å¯¹åº”æ–¹é¢ï¼ŒSPDå¯ä»¥æ›´é«˜ç²¾åº¦åœ°localåŒ–ç›¸ä¼¼åˆ†å‰²ï¼Œè€Œä¸”æ›´é«˜æ•ˆ thanç°æœ‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†SSANã€SKEå’ŒSPDè”åˆè®­ç»ƒï¼Œå®ç°äº†ç«¯åˆ°ç«¯æå‡ã€‚æ­¤å¤–ï¼Œè¿™ä¸¤ä¸ªå…³é”®æ¨¡å—ä¹Ÿå¯ä»¥åœ¨å…¶ä»–è§†é¢‘æ£€ç´¢ç®¡é“ä¸­æ’å…¥ï¼Œå¹¶è·å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSSANå¯ä»¥åœ¨å…¬å…±æ•°æ®é›†ä¸Šè·å¾—æ›´é«˜çš„å¯¹åº”ç²¾åº¦ï¼ŒåŒæ—¶å‡å°‘å­˜å‚¨å’Œåœ¨çº¿æŸ¥è¯¢è®¡ç®—æˆæœ¬ã€‚
</details></li>
</ul>
<hr>
<h2 id="Dense-2D-3D-Indoor-Prediction-with-Sound-via-Aligned-Cross-Modal-Distillation"><a href="#Dense-2D-3D-Indoor-Prediction-with-Sound-via-Aligned-Cross-Modal-Distillation" class="headerlink" title="Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation"></a>Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11081">http://arxiv.org/abs/2309.11081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heeseung Yun, Joonil Na, Gunhee Kim</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨å¦‚ä½•ä½¿æ·±åº¦ç½‘ç»œæ‹¥æœ‰ç©ºé—´é€»è¾‘èƒ½åŠ›ï¼Œä»¥ä¾¿åœ¨æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­æ›´å¥½åœ°åˆ©ç”¨å£°éŸ³ä¿¡æ¯ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œåŒ¹é…å¼•å¯¼â€ï¼ˆSAMï¼‰çš„çŸ¥è¯†å¡«å……æ¡†æ¶ï¼Œç”¨äºåœ¨è§†è§‰çŸ¥è¯†ä¼ è¾“ä¸­åŒ¹é…åœ°å€é—®é¢˜ã€‚è¯¥æ¡†æ¶å°†å¬éŸ³ç‰¹å¾ä¸è§†è§‰å‡†ç¡®çš„å­¦ä¹ ç©ºé—´åµŒå…¥ç»“åˆèµ·æ¥ï¼Œä»¥è§£å†³å¤šå±‚å­¦ç”Ÿæ¨¡å‹ä¸­çš„ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡ä¸€ä¸ªæ–°åˆ›å»ºçš„å°é—­é¢„æµ‹æ•°æ®é›†ï¼ˆDAPSï¼‰ï¼ŒæˆåŠŸåœ°è§£å†³äº†indoor dense predictioné—®é¢˜ï¼ŒåŒ…æ‹¬å£°éŸ³åŸºç¡€ depth estimationã€semantic segmentationå’Œ3Dåœºæ™¯é‡å»ºç­‰é—®é¢˜ã€‚åœ¨ä¸åŒçš„metricå’Œåå¤„ç†æ¶æ„ä¸‹ï¼Œè¯¥distillationæ¡†æ¶ä¸€è‡´åœ°å®ç°äº†çŠ¶æ€çš„æœ€ä½³æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Sound can convey significant information for spatial reasoning in our daily lives. To endow deep networks with such ability, we address the challenge of dense indoor prediction with sound in both 2D and 3D via cross-modal knowledge distillation. In this work, we propose a Spatial Alignment via Matching (SAM) distillation framework that elicits local correspondence between the two modalities in vision-to-audio knowledge transfer. SAM integrates audio features with visually coherent learnable spatial embeddings to resolve inconsistencies in multiple layers of a student model. Our approach does not rely on a specific input representation, allowing for flexibility in the input shapes or dimensions without performance degradation. With a newly curated benchmark named Dense Auditory Prediction of Surroundings (DAPS), we are the first to tackle dense indoor prediction of omnidirectional surroundings in both 2D and 3D with audio observations. Specifically, for audio-based depth estimation, semantic segmentation, and challenging 3D scene reconstruction, the proposed distillation framework consistently achieves state-of-the-art performance across various metrics and backbone architectures.
</details>
<details>
<summary>æ‘˜è¦</summary>
å£°éŸ³å¯ä»¥ä¼ é€’é‡è¦çš„ä¿¡æ¯æ¥å¸®åŠ©æˆ‘ä»¬æ—¥å¸¸å‡†å¤‡ç©ºé—´ç†è§£ã€‚ä¸ºäº†è®©æ·±åº¦ç½‘ç»œå…·å¤‡è¿™ç§èƒ½åŠ›ï¼Œæˆ‘ä»¬åœ¨è§†AudioçŸ¥è¯†ä¼ é€’ä¸­å¤„ç†ç´§å‡‘çš„å®¤å†…é¢„æµ‹é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸ºåŒ¹é…ï¼ˆSAMï¼‰çŸ¥è¯†ä¼ é€’æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨è§†AudioçŸ¥è¯†ä¼ é€’ä¸­æ‰¾åˆ°æœ¬åœ°åŒ¹é…ç‚¹ï¼Œä»¥è§£å†³å¤šå±‚å­¦ä¹ æ¨¡å‹ä¸­çš„ä¸ä¸€è‡´ã€‚SAMå°†éŸ³é¢‘ç‰¹å¾ä¸è§†è§‰ä¸€è‡´çš„å­¦ä¹ å¯å˜çš„ç©ºé—´åµŒå…¥ç»“åˆèµ·æ¥ï¼Œä»¥è§£å†³å¤šå±‚å­¦ä¹ æ¨¡å‹ä¸­çš„ä¸ä¸€è‡´ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ä¾èµ–ç‰¹å®šçš„è¾“å…¥è¡¨ç¤ºï¼Œå› æ­¤å¯ä»¥åœ¨è¾“å…¥å½¢çŠ¶æˆ–ç»´åº¦ä¸Šè¿›è¡Œçµæ´»çš„è°ƒæ•´æ— è®ºå½±å“æ€§ã€‚æˆ‘ä»¬æ–°ç¼–åˆ¶äº†ä¸€ä¸ªåä¸ºç¯å¢ƒé¢„æµ‹ï¼ˆDAPSï¼‰çš„æƒå¨æ•°æ®é›†ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªåœ¨2Då’Œ3Då®¤å†…ç¯å¢ƒé¢„æµ‹ä¸­ä½¿ç”¨éŸ³é¢‘è§‚å¯Ÿç»“æœè¿›è¡Œå¯†é›†é¢„æµ‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨éŸ³é¢‘åŸºäºæ·±åº¦ä¼°è®¡ã€è¯­ä¹‰åˆ†å‰²å’Œå¤æ‚3Dåœºæ™¯é‡å»ºç­‰æ–¹é¢å‡å®ç°äº†çŠ¶æ€çš„æœ€ä½³æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Visual-Question-Answering-in-the-Medical-Domain"><a href="#Visual-Question-Answering-in-the-Medical-Domain" class="headerlink" title="Visual Question Answering in the Medical Domain"></a>Visual Question Answering in the Medical Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11080">http://arxiv.org/abs/2309.11080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abachaa/VQA-Med-2019">https://github.com/abachaa/VQA-Med-2019</a></li>
<li>paper_authors: Louisa Canepa, Sonit Singh, Arcot Sowmya</li>
<li>for: è¿™ç¯‡ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ä¸ªé€‚åº”åŒ»ç–—å›¾åƒé—®é¢˜çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥è§£ç­”åŸºäº givent medical images çš„è‡ªç„¶è¯­è¨€é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸“é—¨çš„é¢†åŸŸé¢„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸€ç§æ–°çš„å¯¹ç§°å­¦ä¹ é¢„è®­æ–¹æ³•ï¼Œä»¥å‡å°‘å°è§„æ¨¡ dataset çš„é—®é¢˜ã€‚</li>
<li>results: æˆ‘ä»¬çš„ææ¡ˆæ¨¡å‹åœ¨ VQA-Med 2019 æµ‹è¯•é›†ä¸Šè·å¾—äº†60%çš„å‡†ç¡®ç‡ï¼Œä¸å…¶ä»–å·OF-the-art Med-VQA æ¨¡å‹ç›¸å½“ã€‚<details>
<summary>Abstract</summary>
Medical visual question answering (Med-VQA) is a machine learning task that aims to create a system that can answer natural language questions based on given medical images. Although there has been rapid progress on the general VQA task, less progress has been made on Med-VQA due to the lack of large-scale annotated datasets. In this paper, we present domain-specific pre-training strategies, including a novel contrastive learning pretraining method, to mitigate the problem of small datasets for the Med-VQA task. We find that the model benefits from components that use fewer parameters. We also evaluate and discuss the model's visual reasoning using evidence verification techniques. Our proposed model obtained an accuracy of 60% on the VQA-Med 2019 test set, giving comparable results to other state-of-the-art Med-VQA models.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆMed-VQAï¼‰æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œæ—¨åœ¨åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿæ ¹æ®ç»™å®šåŒ»å­¦å›¾åƒå›ç­”è‡ªç„¶è¯­è¨€é—®é¢˜çš„ç³»ç»Ÿã€‚è™½ç„¶æ€»ä½“VQAä»»åŠ¡ä¸Šæœ‰äº†å¿«é€Ÿçš„è¿›æ­¥ï¼Œä½†Med-VQAä»»åŠ¡ä¸Šçš„è¿›æ­¥è¾ƒå°‘ï¼Œè¿™ä¸»è¦å½’ç»“äºåŒ»å­¦å›¾åƒæ•°æ®çš„å°è§„æ¨¡ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸŸç‰¹å®šé¢„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ä¸€ç§æ–°çš„å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒæ–¹æ³•ï¼Œä»¥è§£å†³Med-VQAä»»åŠ¡çš„æ•°æ®å°è§„æ¨¡é—®é¢˜ã€‚æˆ‘ä»¬å‘ç°æ¨¡å‹å—åˆ°å‚æ•°æ•°é‡çš„é™åˆ¶å…·æœ‰å¥½å¤„ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°å’Œè®¨è®ºæ¨¡å‹çš„è§†è§‰é€»è¾‘ä½¿ç”¨è¯æ˜æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æè®®çš„æ¨¡å‹åœ¨VQA-Med 2019æµ‹è¯•é›†ä¸Šå–å¾—äº†60%çš„å‡†ç¡®ç‡ï¼Œä¸å…¶ä»–çŠ¶æ€ä¹‹å‰çš„Med-VQAæ¨¡å‹ç›¸å½“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Score-Mismatching-for-Generative-Modeling"><a href="#Score-Mismatching-for-Generative-Modeling" class="headerlink" title="Score Mismatching for Generative Modeling"></a>Score Mismatching for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11043">http://arxiv.org/abs/2309.11043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/senmaoy/Score-Mismatching">https://github.com/senmaoy/Score-Mismatching</a></li>
<li>paper_authors: Senmao Ye, Fei Liu</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ–°çš„åˆ†æ•°åŸºæœ¬æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå›¾åƒã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€æ­¥é‡‡æ ·æ–¹æ³•ï¼Œå–ä»£äº†ä¹‹å‰çš„è¿­ä»£é‡‡æ ·æ–¹æ³•ã€‚åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œä¸€ä¸ªç‹¬ç«‹çš„ç”Ÿæˆå™¨å°†æ‰€æœ‰çš„æ—¶é—´æ­¥é‡‡æ ·å‹ç¼©åˆ°äº†æ¢¯åº¦åpropagationæ¥è‡ªåˆ†æ•°ç½‘ç»œã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„æ¨¡å‹åœ¨CIFAR-10æ•°æ®é›†ä¸Šæ¯”Consistency Modelå’ŒDenoising Score Matchingæ›´é«˜æ•ˆï¼Œè¿™è¡¨æ˜äº†è¿™ç§æ¡†æ¶çš„æ½œåœ¨åŠ›é‡ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜åœ¨MINISTå’ŒLSUNæ•°æ®é›†ä¸Šè¿›è¡Œäº†æ›´å¤šçš„ç¤ºä¾‹ã€‚ä»£ç å¯ä»¥åœ¨GitHubä¸Šä¸‹è½½ã€‚<details>
<summary>Abstract</summary>
We propose a new score-based model with one-step sampling. Previously, score-based models were burdened with heavy computations due to iterative sampling. For substituting the iterative process, we train a standalone generator to compress all the time steps with the gradient backpropagated from the score network. In order to produce meaningful gradients for the generator, the score network is trained to simultaneously match the real data distribution and mismatch the fake data distribution. This model has the following advantages: 1) For sampling, it generates a fake image with only one step forward. 2) For training, it only needs 10 diffusion steps.3) Compared with consistency model, it is free of the ill-posed problem caused by consistency loss. On the popular CIFAR-10 dataset, our model outperforms Consistency Model and Denoising Score Matching, which demonstrates the potential of the framework. We further provide more examples on the MINIST and LSUN datasets. The code is available on GitHub.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†æ•°åŸºäºæ¨¡å‹ï¼Œä½¿ç”¨å•æ­¥é‡‡æ ·ã€‚åœ¨è¿‡å»ï¼Œåˆ†æ•°åŸºäºæ¨¡å‹å—åˆ°è¿­ä»£é‡‡æ ·çš„è®¡ç®—å‹åŠ›ã€‚ä¸ºäº†æ›¿ä»£è¿­ä»£è¿‡ç¨‹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç‹¬ç«‹çš„ç”Ÿæˆå™¨ï¼Œä½¿å…¶åœ¨åˆ†æ•°ç½‘ç»œçš„æ¢¯åº¦å½’æ•´ä¸‹å‹ç¼©æ‰€æœ‰æ—¶é—´æ­¥ã€‚ä¸ºäº†ç”Ÿæˆæœ‰æ„ä¹‰çš„æ¢¯åº¦ï¼Œåˆ†æ•°ç½‘ç»œéœ€è¦åŒæ—¶åŒ¹é…çœŸå®æ•°æ®åˆ†å¸ƒå’Œå‡æ•°æ®åˆ†å¸ƒã€‚è¿™ä¸ªæ¨¡å‹å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š1ï¼‰é‡‡æ ·æ—¶åªéœ€ä¸€æ­¥å‰è¿›ã€‚2ï¼‰è®­ç»ƒæ—¶åªéœ€10æ­¥æ‰©æ•£ã€‚3ï¼‰ä¸ä¸€è‡´æ€§æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒå…å—ä¸€è‡´æ€§æŸå¤±å¯¼è‡´çš„ç³Ÿç³•é—®é¢˜ã€‚åœ¨æµè¡Œçš„ CIFAR-10 æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¶…è¶Šäº†ä¸€è‡´æ€§æ¨¡å‹å’Œæ‚å™ªåˆ†åŒ¹é…æ¨¡å‹ï¼Œè¿™è¡¨æ˜äº†è¯¥æ¡†æ¶çš„æ½œåŠ›ã€‚æˆ‘ä»¬è¿˜æä¾›äº†æ›´å¤šçš„ä¾‹å­åœ¨ MINIST å’Œ LSUN æ•°æ®é›†ä¸Šã€‚ä»£ç å¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="CaveSeg-Deep-Semantic-Segmentation-and-Scene-Parsing-for-Autonomous-Underwater-Cave-Exploration"><a href="#CaveSeg-Deep-Semantic-Segmentation-and-Scene-Parsing-for-Autonomous-Underwater-Cave-Exploration" class="headerlink" title="CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous Underwater Cave Exploration"></a>CaveSeg: Deep Semantic Segmentation and Scene Parsing for Autonomous Underwater Cave Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11038">http://arxiv.org/abs/2309.11038</a></li>
<li>repo_url: None</li>
<li>paper_authors: A. Abdullah, T. Barua, R. Tibbetts, Z. Chen, M. J. Islam, I. Rekleitis</li>
<li>for: æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªç”¨äºæ½œæ°´æ´ç©´æ¢ç´¢å’Œåœ°å›¾åˆ›å»ºçš„è‡ªä¸»æ½œæ°´å™¨è§†è§‰å­¦ä¹ ç®¡çº¿ï¼ŒååŠ©AUVåœ¨æ½œæ°´æ´ç©´ç¯å¢ƒä¸­å¿«é€Ÿå®Œæˆ semantic segmentation å’Œ scene parsingã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªå…·æœ‰å…¨é¢æ€§çš„æ•°æ®é›†ï¼Œä»¥ä¾¿å¯¹æ½œæ°´æ´ç©´åœºæ™¯è¿›è¡Œsemantic segmentationï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªåŸºäº transformer çš„è§†è§‰æ¨¡å‹ï¼Œå…·æœ‰å¿«é€Ÿæ‰§è¡Œå’Œä½ computational complexityã€‚</li>
<li>results: æœ¬ç ”ç©¶é€šè¿‡åœ¨ç¾å›½ã€å¢¨è¥¿å“¥å’Œè¥¿ç­ç‰™çš„æ´ç©´ç³»ç»Ÿè¿›è¡Œäº† comprehensive benchmark åˆ†æï¼Œè¯æ˜äº†å¯ä»¥é€è¿‡ CaveSeg å‘å±•å‡ºé«˜æ€§èƒ½çš„æ·±åº¦è§†è§‰æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨å®é™…åº”ç”¨ä¸­å®ç°äº†å¿«é€Ÿçš„ semantic scene parsingã€‚<details>
<summary>Abstract</summary>
In this paper, we present CaveSeg - the first visual learning pipeline for semantic segmentation and scene parsing for AUV navigation inside underwater caves. We address the problem of scarce annotated training data by preparing a comprehensive dataset for semantic segmentation of underwater cave scenes. It contains pixel annotations for important navigation markers (e.g. caveline, arrows), obstacles (e.g. ground plain and overhead layers), scuba divers, and open areas for servoing. Through comprehensive benchmark analyses on cave systems in USA, Mexico, and Spain locations, we demonstrate that robust deep visual models can be developed based on CaveSeg for fast semantic scene parsing of underwater cave environments. In particular, we formulate a novel transformer-based model that is computationally light and offers near real-time execution in addition to achieving state-of-the-art performance. Finally, we explore the design choices and implications of semantic segmentation for visual servoing by AUVs inside underwater caves. The proposed model and benchmark dataset open up promising opportunities for future research in autonomous underwater cave exploration and mapping.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†CaveSegï¼Œé¦–ä¸ªç”¨äºsemantic segmentationå’Œåœºæ™¯åˆ†è§£çš„AUVå†…æ°´æ´ç¯å¢ƒè§†è§‰å­¦ä¹ ç®¡é“ã€‚æˆ‘ä»¬è§£å†³äº†ç½•è§çš„æ³¨é‡ŠåŸ¹è®­æ•°æ®çš„é—®é¢˜ï¼Œprepareäº†åŒ…å«é‡è¦å¯¼èˆªæ ‡è®°ï¼ˆä¾‹å¦‚ï¼Œ cave lineã€ç®­å¤´ï¼‰ã€éšœç¢ç‰©ï¼ˆä¾‹å¦‚ï¼Œåœ°é¢å±‚å’Œå¤©èŠ±æ¿å±‚ï¼‰ã€æ½œæ°´å‘˜å’Œå¼€æ”¾åŒºåŸŸçš„åƒç´ æ³¨é‡Šã€‚é€šè¿‡å¯¹ç¾å›½ã€å¢¨è¥¿å“¥å’Œè¥¿ç­ç‰™ç­‰åœ°æ°´æ´ç³»ç»Ÿè¿›è¡Œäº†å…¨é¢çš„æ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†å¯ä»¥åŸºäºCaveSegæ„å»ºRobustçš„æ·±åº¦è§†è§‰æ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿsemantic scene parsingæ°´æ´ç¯å¢ƒã€‚å°¤å…¶æ˜¯ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ transformer-based æ¨¡å‹ï¼Œå…·æœ‰è¾ƒå°‘è®¡ç®—é‡å’Œå®æ—¶æ‰§è¡Œèƒ½åŠ›ï¼ŒåŒæ—¶ä¹Ÿè¾¾åˆ°äº†çŠ¶æ€å®éªŒå®¤çš„æ€§èƒ½ã€‚æœ€åï¼Œæˆ‘ä»¬æ¢è®¨äº†semantic segmentationå¯¹AUVå†…æ°´æ´ç¯å¢ƒçš„è§†ervokingçš„è®¾è®¡é€‰æ‹©å’Œæ„ä¹‰ã€‚æå‡ºçš„æ¨¡å‹å’Œæ•°æ®é›†å¼€ upäº†æœªæ¥æ°´æ´explorationå’Œ mapping çš„å¯èƒ½æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Light-Field-Diffusion-for-Single-View-Novel-View-Synthesis"><a href="#Light-Field-Diffusion-for-Single-View-Novel-View-Synthesis" class="headerlink" title="Light Field Diffusion for Single-View Novel View Synthesis"></a>Light Field Diffusion for Single-View Novel View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11525">http://arxiv.org/abs/2309.11525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifeng Xiong, Haoyu Ma, Shanlin Sun, Kun Han, Xiaohui Xie</li>
<li>for: å•è§†å›¾æ–°è§†è§’åˆæˆï¼Œç”ŸæˆåŸºäºå•ä¸ªå‚è€ƒå›¾åƒçš„å›¾åƒï¼Œæ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­ä¸€é¡¹é‡è¦ä½†å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨Light Field Diffusionï¼ˆLFDï¼‰æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºæ‰©æ•£çš„å¢å¼ºæ¨¡å‹ï¼Œåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­å°†æ‘„åƒå¤´è§†è§’ä¿¡æ¯è½¬æ¢ä¸ºå…‰åœºç¼–ç ï¼Œå¹¶ä¸å‚è€ƒå›¾åƒç›¸ç»“åˆã€‚è¿™ç§è®¾è®¡å¼•å…¥äº†æœ¬åœ°åƒç´ çº§åˆ«çš„çº¦æŸï¼Œä»è€Œä¿ƒè¿›äº†å¤šè§†å›¾ä¸€è‡´æ€§ã€‚</li>
<li>results: æˆ‘ä»¬çš„LFDå¯ä»¥é«˜æ•ˆåœ°ç”Ÿæˆé«˜è´¨é‡å›¾åƒï¼Œå¹¶åœ¨å¤æ‚çš„åŒºåŸŸä¸­ä¿æŒæ›´å¥½çš„3Dä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸NeRF-basedæ¨¡å‹ç›¸æ¯”ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„æ¨¡å‹è§„æ¨¡åªæ˜¯NeRF-basedæ¨¡å‹çš„ä¸€åŠã€‚<details>
<summary>Abstract</summary>
Single-view novel view synthesis, the task of generating images from new viewpoints based on a single reference image, is an important but challenging task in computer vision. Recently, Denoising Diffusion Probabilistic Model (DDPM) has become popular in this area due to its strong ability to generate high-fidelity images. However, current diffusion-based methods directly rely on camera pose matrices as viewing conditions, globally and implicitly introducing 3D constraints. These methods may suffer from inconsistency among generated images from different perspectives, especially in regions with intricate textures and structures. In this work, we present Light Field Diffusion (LFD), a conditional diffusion-based model for single-view novel view synthesis. Unlike previous methods that employ camera pose matrices, LFD transforms the camera view information into light field encoding and combines it with the reference image. This design introduces local pixel-wise constraints within the diffusion models, thereby encouraging better multi-view consistency. Experiments on several datasets show that our LFD can efficiently generate high-fidelity images and maintain better 3D consistency even in intricate regions. Our method can generate images with higher quality than NeRF-based models, and we obtain sample quality similar to other diffusion-based models but with only one-third of the model size.
</details>
<details>
<summary>æ‘˜è¦</summary>
å•è§†å›¾novelè§†è§‰åˆæˆé—®é¢˜ï¼Œå³åŸºäºå•ä¸ªå‚è€ƒå›¾åƒç”Ÿæˆæ–°è§†ç‚¹å›¾åƒï¼Œæ˜¯è®¡ç®—æœºè§†è§‰ä¸­é‡è¦ä½†å›°éš¾çš„ä»»åŠ¡ã€‚æœ€è¿‘ï¼ŒDenosing Diffusion Probabilistic Model (DDPM) åœ¨è¿™ä¸ªé¢†åŸŸä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥ç”Ÿæˆé«˜å“è´¨å›¾åƒã€‚ç„¶è€Œï¼Œå½“å‰çš„æ‰©æ•£åŸºæœ¬æ–¹æ³•ç›´æ¥ä½¿ç”¨æ‘„åƒæœºposeçŸ©é˜µä½œä¸ºè§†å›¾æ¡ä»¶ï¼Œå…¨å±€å’Œå¼ºåˆ¶æ€§åœ°å¼•å…¥3Dçº¦æŸã€‚è¿™äº›æ–¹æ³•å¯èƒ½åœ¨ä¸åŒè§†ç‚¹å›¾åƒä¸­ç”Ÿæˆçš„å›¾åƒä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰å¤æ‚ Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ğ°å’Œç»“æ„çš„åŒºåŸŸä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†Light Field Diffusion (LFD)ï¼Œä¸€ç§åŸºäºæ¡ä»¶æ‰©æ•£çš„å•è§†å›¾novelè§†è§‰åˆæˆæ¨¡å‹ã€‚ä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒï¼ŒLFDå°†æ‘„åƒæœºè§†è§’ä¿¡æ¯è½¬æ¢ä¸ºå…‰åœºç¼–ç ï¼Œå¹¶å°†å…¶ä¸å‚è€ƒå›¾åƒç›¸ç»“åˆã€‚è¿™ç§è®¾è®¡å¼•å…¥äº†æœ¬åœ°åƒç´ çº§åˆ«çš„æ‰©æ•£æ¨¡å‹ä¸­çš„çº¦æŸï¼Œä»è€Œé¼“åŠ±æ›´å¥½çš„å¤šè§†å›¾ä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„LFDå¯ä»¥é«˜æ•ˆåœ°ç”Ÿæˆé«˜å“è´¨å›¾åƒï¼Œå¹¶åœ¨å¤æ‚åŒºåŸŸä¸­ä¿æŒæ›´å¥½çš„3Dä¸€è‡´æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç”Ÿæˆå›¾åƒè´¨é‡é«˜äºNeRF-basedæ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ¨¡å‹å¤§å°æ–¹é¢ä¸å…¶ä»–æ‰©æ•£åŸºæœ¬æ–¹æ³•ç›¸å½“ï¼Œä½†åªéœ€ä¸€åŠçš„æ¨¡å‹å¤§å°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Conformalized-Multimodal-Uncertainty-Regression-and-Reasoning"><a href="#Conformalized-Multimodal-Uncertainty-Regression-and-Reasoning" class="headerlink" title="Conformalized Multimodal Uncertainty Regression and Reasoning"></a>Conformalized Multimodal Uncertainty Regression and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11018">http://arxiv.org/abs/2309.11018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Domenico Parente, Nastaran Darabi, Alex C. Stutts, Theja Tulabandhula, Amit Ranjan Trivedi</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨ä¸€ç§è½»é‡çº§çš„ä¸ç¡®å®šåº¦ä¼°è®¡å™¨ï¼Œå¯ä»¥é¢„æµ‹å¤šmodalï¼ˆåˆ†ç¦»ï¼‰çš„ä¸ç¡®å®šåº¦ boundï¼Œé€šè¿‡å°†ĞºĞ¾Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°ãƒ«é¢„æµ‹ä¸æ·±åº¦å­¦ä¹ å›æ¨å™¨ç»“åˆèµ·æ¥ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å°†ĞºĞ¾Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°ãƒ«é¢„æµ‹ä¸æ·±åº¦å­¦ä¹ å›æ¨å™¨ç»“åˆèµ·æ¥ï¼Œä»¥é¢„æµ‹å¤šmodalï¼ˆåˆ†ç¦»ï¼‰çš„ä¸ç¡®å®šåº¦ boundã€‚</li>
<li>results:  simulations ç»“æœæ˜¾ç¤ºï¼Œåœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œä¸ç¡®å®šåº¦ä¼°è®¡å™¨é€‚åº”äº†å…·æœ‰ä¸¥é‡å™ªéŸ³ã€æœ‰é™è®­ç»ƒæ•°æ®å’Œæœ‰é™é¢„æµ‹æ¨¡å‹å¤§å°çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç†è§£æ¡†æ¶ï¼Œåˆ©ç”¨è¿™äº›å¯é çš„ä¸ç¡®å®šåº¦ä¼°è®¡å™¨ï¼Œå¹¶ä¸å…‰æµåŸºäºçš„ç†è§£æ¥æé«˜é¢„æµ‹ç²¾åº¦ã€‚å› æ­¤ï¼Œé€šè¿‡é€‚å½“åœ°è€ƒè™‘æ•°æ®é©±åŠ¨å­¦ä¹ ä¸­çš„é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œå¹¶é€è¿‡è§„å¾‹åŸºäºçš„ç†è§£æ¥å…³é—­é¢„æµ‹æ¨¡å‹çš„ä¼°è®¡loopï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰è¿™äº›é—®é¢˜ä¸Šæ˜¾è‘—è¶…è¶Šäº†ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå®é™…ä¸Šé™ä½é¢„æµ‹é”™è¯¯çš„æ¯”ä¾‹ä¸º2-3å€ã€‚<details>
<summary>Abstract</summary>
This paper introduces a lightweight uncertainty estimator capable of predicting multimodal (disjoint) uncertainty bounds by integrating conformal prediction with a deep-learning regressor. We specifically discuss its application for visual odometry (VO), where environmental features such as flying domain symmetries and sensor measurements under ambiguities and occlusion can result in multimodal uncertainties. Our simulation results show that uncertainty estimates in our framework adapt sample-wise against challenging operating conditions such as pronounced noise, limited training data, and limited parametric size of the prediction model. We also develop a reasoning framework that leverages these robust uncertainty estimates and incorporates optical flow-based reasoning to improve prediction prediction accuracy. Thus, by appropriately accounting for predictive uncertainties of data-driven learning and closing their estimation loop via rule-based reasoning, our methodology consistently surpasses conventional deep learning approaches on all these challenging scenarios--pronounced noise, limited training data, and limited model size-reducing the prediction error by 2-3x.
</details>
<details>
<summary>æ‘˜è¦</summary>
Here is the text in Simplified Chinese:è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§è½»é‡çº§çš„ä¸ç¡®å®šæ€§ä¼°è®¡å™¨ï¼Œå¯ä»¥é€šè¿‡å°† ĞºĞ¾Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹é¢„æµ‹ä¸æ·±åº¦å­¦ä¹ å›å½’å™¨ç»“åˆæ¥é¢„æµ‹å¤šModalä¸ç¡®å®šæ€§ boundã€‚æˆ‘ä»¬ç‰¹åˆ«æ¢è®¨äº†å®ƒåœ¨è§†è§‰è¿åŠ¨ï¼ˆVOï¼‰ä¸­çš„åº”ç”¨ï¼Œ Ğ³Ğ´Ğµ environmental featureså’Œæ„ŸçŸ¥æµ‹é‡åœ¨å¼‚å¸¸å’Œé®æŒ¡ä¸‹å¯èƒ½å¯¼è‡´å¤šModalä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬çš„ simulations è¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡é€‚åº”æ ·æœ¬æ‰€å¯¹æŠ—å¤æ‚çš„è¿è¡Œæ¡ä»¶ï¼Œå¦‚å¼ºåº¦çš„å™ªéŸ³ã€æœ‰é™çš„è®­ç»ƒæ•°æ®å’Œæœ‰é™çš„é¢„æµ‹æ¨¡å‹å¤§å°ã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ç§ä½¿ç”¨è¿™äº›ç¨³å¥çš„ä¸ç¡®å®šæ€§ä¼°è®¡å’ŒåŸºäºæ¨Flowçš„reasoning Frameworkæ¥æé«˜é¢„æµ‹å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œé€šè¿‡åˆç†åœ°è€ƒè™‘æ•°æ®é©±åŠ¨å­¦ä¹ çš„é¢„æµ‹ä¸ç¡®å®šæ€§å’Œå…³é—­å…¶ä¼°è®¡å¾ªç¯ via è§„åˆ™åŸºäºçš„reasoningï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰è¿™äº›å¤æ‚çš„ scenariosä¸­ä¸€ç›´èµ¶åœ¨æ·±åº¦å­¦ä¹ æ–¹æ³•ä¹‹å‰ï¼Œå‡å°‘é¢„æµ‹é”™è¯¯ by 2-3å€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Controllable-Dynamic-Appearance-for-Neural-3D-Portraits"><a href="#Controllable-Dynamic-Appearance-for-Neural-3D-Portraits" class="headerlink" title="Controllable Dynamic Appearance for Neural 3D Portraits"></a>Controllable Dynamic Appearance for Neural 3D Portraits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11009">http://arxiv.org/abs/2309.11009</a></li>
<li>repo_url: None</li>
<li>paper_authors: ShahRukh Athar, Zhixin Shu, Zexiang Xu, Fujun Luan, Sai Bi, Kalyan Sunkavalli, Dimitris Samaras</li>
<li>for: åˆ›å»ºå®Œå…¨å¯æ§çš„3Däººç‰©å¤´åƒï¼Œåœ¨çœŸå®æ•æ‰ç¯å¢ƒä¸­ã€‚</li>
<li>methods: ä½¿ç”¨NeRFæŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€å‡ºç°æ¨¡å‹æ¥ aproximateç…§æ˜ä¾èµ–çš„æ•ˆæœï¼Œå¹¶é€šè¿‡é¢æ³•å¯¼å‘æ¥å‡†ç¡®é¢„æµ‹è¡¨ë©´æ³•å‘é‡ã€‚</li>
<li>results: ä½¿ç”¨çŸ­è§†é¢‘ captured with smartphoneï¼Œåœ¨ä¸åŒçš„å¤´éƒ¨å§¿åŠ¿å’Œè¡¨æƒ…æ§åˆ¶ä¸‹å®ç°äº†é«˜è´¨é‡çš„è‡ªç”±è§† sintesisæ•ˆæœï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¨¡æ‹ŸçœŸå®çš„ç…§æ˜æ•ˆæœã€‚<details>
<summary>Abstract</summary>
Recent advances in Neural Radiance Fields (NeRFs) have made it possible to reconstruct and reanimate dynamic portrait scenes with control over head-pose, facial expressions and viewing direction. However, training such models assumes photometric consistency over the deformed region e.g. the face must be evenly lit as it deforms with changing head-pose and facial expression. Such photometric consistency across frames of a video is hard to maintain, even in studio environments, thus making the created reanimatable neural portraits prone to artifacts during reanimation. In this work, we propose CoDyNeRF, a system that enables the creation of fully controllable 3D portraits in real-world capture conditions. CoDyNeRF learns to approximate illumination dependent effects via a dynamic appearance model in the canonical space that is conditioned on predicted surface normals and the facial expressions and head-pose deformations. The surface normals prediction is guided using 3DMM normals that act as a coarse prior for the normals of the human head, where direct prediction of normals is hard due to rigid and non-rigid deformations induced by head-pose and facial expression changes. Using only a smartphone-captured short video of a subject for training, we demonstrate the effectiveness of our method on free view synthesis of a portrait scene with explicit head pose and expression controls, and realistic lighting effects. The project page can be found here: http://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰æŠ€æœ¯çªç ´ï¼Œä½¿å¾—å¯ä»¥é‡å»ºå’Œå¤æ´»åŠ¨æ€è‚–åƒåœºæ™¯ï¼ŒåŒ…æ‹¬å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…çš„æ§åˆ¶ã€‚ç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹æ—¶éœ€è¦å…‰ometric consistency over the deformed regionï¼Œä¾‹å¦‚è„¸éƒ¨å¿…é¡»åœ¨ä¸åŒçš„å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…å˜åŒ–ä¸­ä¿æŒå…‰åº¦çš„å‡åŒ€æ€§ã€‚è¿™ç§å…‰åº¦ä¸€è‡´æ€§åœ¨è§†é¢‘å¸§ä¸­å¾ˆéš¾ä¿æŒï¼Œå³ä½¿åœ¨studio environmentä¸­ï¼Œå› æ­¤åˆ›å»ºçš„å¯æ§3Dè‚–åƒå®¹æ˜“å‡ºç°artifacts during reanimationã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†CoDyNeRFç³»ç»Ÿï¼Œå¯ä»¥åœ¨çœŸå®çš„æ•æ‰æ¡ä»¶ä¸‹åˆ›å»ºå®Œå…¨å¯æ§çš„3Dè‚–åƒã€‚CoDyNeRFé€šè¿‡learns to approximate illumination dependent effects via a dynamic appearance model in the canonical space that is conditioned on predicted surface normals and the facial expressions and head-pose deformationsæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ surface normals predictionæ˜¯é€šè¿‡3DMM normalsä½œä¸ºä¸€ä¸ªç²—ç•¥çš„ä¼°è®¡å™¨æ¥å¼•å¯¼çš„ï¼Œå› ä¸ºdirect prediction of normalsæ˜¯ç”±äºå¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…å˜åŒ–inducedçš„å›ºå®šå’Œéå›ºå®šæ‰­æ›²è€Œå›°éš¾ã€‚é€šè¿‡åªä½¿ç”¨çŸ­è§†é¢‘ captureçš„æ™ºèƒ½æ‰‹æœºè®­ç»ƒï¼Œæˆ‘ä»¬ç¤ºç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨free view synthesis of a portrait scene with explicit head pose and expression controls, and realistic lighting effectsã€‚ç›¸å…³é¡¹ç›®é¡µé¢å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼šhttp://shahrukhathar.github.io/2023/08/22/CoDyNeRF.html
</details></li>
</ul>
<hr>
<h2 id="STARNet-Sensor-Trustworthiness-and-Anomaly-Recognition-via-Approximated-Likelihood-Regret-for-Robust-Edge-Autonomy"><a href="#STARNet-Sensor-Trustworthiness-and-Anomaly-Recognition-via-Approximated-Likelihood-Regret-for-Robust-Edge-Autonomy" class="headerlink" title="STARNet: Sensor Trustworthiness and Anomaly Recognition via Approximated Likelihood Regret for Robust Edge Autonomy"></a>STARNet: Sensor Trustworthiness and Anomaly Recognition via Approximated Likelihood Regret for Robust Edge Autonomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11006">http://arxiv.org/abs/2309.11006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sinatayebati/STARNet">https://github.com/sinatayebati/STARNet</a></li>
<li>paper_authors: Nastaran Darabi, Sina Tayebati, Sureshkumar S., Sathya Ravi, Theja Tulabandhula, Amit R. Trivedi</li>
<li>for: This paper is written to address the reliability concerns of complex sensors such as LiDAR and camera sensors in autonomous robotics, and to improve the prediction accuracy of deep learning models by detecting untrustworthy sensor streams.</li>
<li>methods: STARNet, a Sensor Trustworthiness and Anomaly Recognition Network, is used to detect untrustworthy sensor streams. STARNet employs the concept of approximated likelihood regret, a gradient-free framework tailored for low-complexity hardware.</li>
<li>results: STARNet enhances prediction accuracy by approximately 10% by filtering out untrustworthy sensor streams in unimodal and multimodal settings, especially in addressing internal sensor failures such as cross-sensor interference and crosstalk.<details>
<summary>Abstract</summary>
Complex sensors such as LiDAR, RADAR, and event cameras have proliferated in autonomous robotics to enhance perception and understanding of the environment. Meanwhile, these sensors are also vulnerable to diverse failure mechanisms that can intricately interact with their operation environment. In parallel, the limited availability of training data on complex sensors also affects the reliability of their deep learning-based prediction flow, where their prediction models can fail to generalize to environments not adequately captured in the training set. To address these reliability concerns, this paper introduces STARNet, a Sensor Trustworthiness and Anomaly Recognition Network designed to detect untrustworthy sensor streams that may arise from sensor malfunctions and/or challenging environments. We specifically benchmark STARNet on LiDAR and camera data. STARNet employs the concept of approximated likelihood regret, a gradient-free framework tailored for low-complexity hardware, especially those with only fixed-point precision capabilities. Through extensive simulations, we demonstrate the efficacy of STARNet in detecting untrustworthy sensor streams in unimodal and multimodal settings. In particular, the network shows superior performance in addressing internal sensor failures, such as cross-sensor interference and crosstalk. In diverse test scenarios involving adverse weather and sensor malfunctions, we show that STARNet enhances prediction accuracy by approximately 10% by filtering out untrustworthy sensor streams. STARNet is publicly available at \url{https://github.com/sinatayebati/STARNet}.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤æ‚çš„æ„ŸçŸ¥å™¨å¦‚LiDARã€RADARå’Œäº‹ä»¶æ‘„åƒå¤´åœ¨è‡ªä¸» Ñ€Ğ¾Ğ±Ğ¾æ‰®ä¸­å¹¿æ³›åº”ç”¨ï¼Œä»¥æé«˜ç¯å¢ƒçš„æ„ŸçŸ¥å’Œç†è§£ã€‚ç„¶è€Œï¼Œè¿™äº›æ„ŸçŸ¥å™¨ä¹Ÿé¢ä¸´ç€å¤šç§å¤±æ•ˆæœºåˆ¶ï¼Œè¿™äº›å¤±æ•ˆæœºåˆ¶å¯èƒ½ä¸å…¶è¿è¡Œç¯å¢ƒäº’ç›¸å¤æ‚äº¤äº’ã€‚åŒæ—¶ï¼Œå¯¹äºå¤æ‚çš„æ„ŸçŸ¥å™¨ï¼Œæœ‰é™çš„è®­ç»ƒæ•°æ®ä¹Ÿä¼šå½±å“å…¶æ·±åº¦å­¦ä¹ åŸºäºé¢„æµ‹æµçš„å¯é æ€§ï¼Œå…¶é¢„æµ‹æ¨¡å‹å¯èƒ½æ— æ³•æ³›åŒ–åˆ°ä¸å……åˆ† captured çš„ç¯å¢ƒä¸­ã€‚ä¸ºè§£å†³è¿™äº›å¯é æ€§é—®é¢˜ï¼Œæœ¬æ–‡ä»‹ç»äº† STARNetï¼Œä¸€ç§æ„ŸçŸ¥å™¨å¯é æ€§å’Œå¼‚å¸¸æ£€æµ‹ç½‘ç»œï¼Œå¯ä»¥æ£€æµ‹ä¸å¯é çš„æ„ŸçŸ¥æµï¼Œè¿™äº›æ„ŸçŸ¥æµå¯èƒ½ç”±æ„ŸçŸ¥å™¨æ•…éšœå’Œ/æˆ–æŒ‘æˆ˜ç¯å¢ƒå¼•èµ·ã€‚æˆ‘ä»¬ especifically å¯¹ LiDAR å’Œæ‘„åƒå¤´æ•°æ®è¿›è¡Œäº† benchmarkã€‚STARNet é‡‡ç”¨äº†approximated likelihood regretï¼Œä¸€ç§é€‚ç”¨äºä½å¤æ‚åº¦ç¡¬ä»¶çš„æ¢¯åº¦è‡ªç”±æ¡†æ¶ã€‚é€šè¿‡å¹¿æ³›çš„ simulationsï¼Œæˆ‘ä»¬å±•ç¤ºäº† STARNet åœ¨å•æ¨¡æ€å’Œå¤šæ¨¡æ€è®¾ç½®ä¸‹çš„æ•ˆæœã€‚å°¤å…¶æ˜¯ï¼Œç½‘ç»œåœ¨å†…éƒ¨æ„ŸçŸ¥å™¨æ•…éšœæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¦‚äº¤å‰æ„ŸçŸ¥å’Œç”µç£å¹²æ‰°ã€‚åœ¨å¤šç§æµ‹è¯•enarioä¸­ï¼ŒåŒ…æ‹¬ä¸è‰¯å¤©æ°”å’Œæ„ŸçŸ¥å™¨æ•…éšœï¼Œæˆ‘ä»¬è¡¨æ˜ STARNet å¯ä»¥æé«˜é¢„æµ‹ç²¾åº¦çº¦ 10%ï¼Œé€šè¿‡ç­›é€‰ä¸å¯é çš„æ„ŸçŸ¥æµã€‚STARNet å…¬å…±å¯ç”¨äº \url{https://github.com/sinatayebati/STARNet}.
</details></li>
</ul>
<hr>
<h2 id="PPD-A-New-Valet-Parking-Pedestrian-Fisheye-Dataset-for-Autonomous-Driving"><a href="#PPD-A-New-Valet-Parking-Pedestrian-Fisheye-Dataset-for-Autonomous-Driving" class="headerlink" title="PPD: A New Valet Parking Pedestrian Fisheye Dataset for Autonomous Driving"></a>PPD: A New Valet Parking Pedestrian Fisheye Dataset for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11002">http://arxiv.org/abs/2309.11002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zizhang Wu, Xinyuan Chen, Fan Song, Yuanzhu Gan, Tianhao Xu, Jian Pu, Rui Tang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ä¸ªå¤§è§„æ¨¡çš„ fisheye æ•°æ®é›†ï¼Œä»¥æ”¯æŒå¯¹å®é™…ä¸–ç•Œä¸­çš„æ­¥è¡Œäººè¿›è¡Œç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹²æ‰°å’Œå¤šç§å§¿åŠ¿ä¸‹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨ fisheye æ‘„åƒå¤´æ•æ‰äº†å¤šç§ç±»å‹çš„æ­¥è¡Œäººï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ•°æ®å¢å¼ºæŠ€æœ¯æ¥æé«˜åŸºelineã€‚</li>
<li>results: å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•çš„æ•ˆæœï¼Œå¹¶è¯æ˜äº†æ•°æ®é›†çš„éå¸¸æ™®éåŒ–ã€‚<details>
<summary>Abstract</summary>
Pedestrian detection under valet parking scenarios is fundamental for autonomous driving. However, the presence of pedestrians can be manifested in a variety of ways and postures under imperfect ambient conditions, which can adversely affect detection performance. Furthermore, models trained on publicdatasets that include pedestrians generally provide suboptimal outcomes for these valet parking scenarios. In this paper, wepresent the Parking Pedestrian Dataset (PPD), a large-scale fisheye dataset to support research dealing with real-world pedestrians, especially with occlusions and diverse postures. PPD consists of several distinctive types of pedestrians captured with fisheye cameras. Additionally, we present a pedestrian detection baseline on PPD dataset, and introduce two data augmentation techniques to improve the baseline by enhancing the diversity ofthe original dataset. Extensive experiments validate the effectiveness of our novel data augmentation approaches over baselinesand the dataset's exceptional generalizability.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨é©¾é©¶ä¸­çš„äººè¡Œæ£€æµ‹åœ¨åœè½¦åœºæ™¯ä¸‹æ˜¯åŸºæœ¬çš„ã€‚ç„¶è€Œï¼Œäººè¡Œå¯ä»¥åœ¨ä¸åŒçš„ç¯å¢ƒæ¡ä»¶ä¸‹è¡¨ç°å‡ºå¤šç§å½¢å¼å’Œå§¿åŠ¿ï¼Œè¿™ä¼š adversely affect æ£€æµ‹æ€§èƒ½ã€‚å°¤å…¶æ˜¯æ¨¡å‹é€šå¸¸åœ¨å…¬å…±æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¿™äº›æ•°æ®é›†ä¸­çš„äººè¡Œé€šå¸¸ä¸é€‚åˆåœè½¦åœºæ™¯ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åœè½¦åœºæ™¯äººè¡Œæ•°æ®é›†ï¼ˆPPDï¼‰ï¼Œä¸€ä¸ªå¤§è§„æ¨¡çš„é±¼çœ¼æ•°æ®é›†ï¼Œä»¥æ”¯æŒå®é™…ä¸–ç•Œä¸­çš„äººè¡Œæ£€æµ‹ï¼Œç‰¹åˆ«æ˜¯å¹²æ‰°å’Œå¤šç§å§¿åŠ¿ã€‚PPD åŒ…æ‹¬å¤šç§ç‰¹å¾çš„äººè¡Œï¼Œé€šè¿‡é±¼çœ¼æ‘„åƒå¤´æ•æ‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†äººè¡Œæ£€æµ‹åŸºçº¿åœ¨ PPD æ•°æ®é›†ä¸Šï¼Œå¹¶ä»‹ç»äº†ä¸¤ç§æ•°æ®å¢å¼ºæŠ€æœ¯æ¥æé«˜åŸºçº¿ï¼Œä»¥æé«˜æ•°æ®é›†çš„å¤šæ ·æ€§ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–°çš„æ•°æ®å¢å¼ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä»¥åŠæ•°æ®é›†çš„å‡ºè‰²çš„æ™®é€‚æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="COSE-A-Consistency-Sensitivity-Metric-for-Saliency-on-Image-Classification"><a href="#COSE-A-Consistency-Sensitivity-Metric-for-Saliency-on-Image-Classification" class="headerlink" title="COSE: A Consistency-Sensitivity Metric for Saliency on Image Classification"></a>COSE: A Consistency-Sensitivity Metric for Saliency on Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10989">http://arxiv.org/abs/2309.10989</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cvl-umass/COSE">https://github.com/cvl-umass/COSE</a></li>
<li>paper_authors: Rangel Daroya, Aaron Sun, Subhransu Maji</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€å¥—åŸºäºè§†è§‰ä¼˜å…ˆçš„è¡¨ç°è¯„ä¼°æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šç§è§†è§‰ç„¦ç‚¹æ˜ å°„æ–¹æ³•ï¼ŒåŒ…æ‹¬GradCAMã€Guided Backpropagationï¼ˆGBPï¼‰å’ŒDeepLIFTï¼ˆDLIFTï¼‰ç­‰ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè™½ç„¶å¤šç§ç„¦ç‚¹æ˜ å°„æ–¹æ³•éƒ½èƒ½å¤Ÿè§£é‡Šæ¨¡å‹å†³ç­–ï¼Œä½†æ˜¯transformeræ¨¡å‹æ¯” convolutionalæ¨¡å‹æ›´éš¾è¢«è¿™äº›æ–¹æ³•è§£é‡Šã€‚æ­¤å¤–ï¼ŒGradCAMè¡¨ç°æœ€ä½³ï¼Œä½†æ˜¯å®ƒåœ¨ç»†èŠ‚åŒ–æ•°æ®é›†ä¸Šç¼ºä¹å¤šæ ·æ€§ã€‚é€šè¿‡å¯¹å‡†åˆ™å’Œæ•æ„Ÿåº¦è¿›è¡Œå¹³è¡¡ï¼Œå¯ä»¥è·å¾—ä¸€ä¸ªå‡†ç¡®åœ°è¡¨ç¤ºæ¨¡å‹è¡Œä¸ºçš„ç„¦ç‚¹æ˜ å°„ã€‚<details>
<summary>Abstract</summary>
We present a set of metrics that utilize vision priors to effectively assess the performance of saliency methods on image classification tasks. To understand behavior in deep learning models, many methods provide visual saliency maps emphasizing image regions that most contribute to a model prediction. However, there is limited work on analyzing the reliability of saliency methods in explaining model decisions. We propose the metric COnsistency-SEnsitivity (COSE) that quantifies the equivariant and invariant properties of visual model explanations using simple data augmentations. Through our metrics, we show that although saliency methods are thought to be architecture-independent, most methods could better explain transformer-based models over convolutional-based models. In addition, GradCAM was found to outperform other methods in terms of COSE but was shown to have limitations such as lack of variability for fine-grained datasets. The duality between consistency and sensitivity allow the analysis of saliency methods from different angles. Ultimately, we find that it is important to balance these two metrics for a saliency map to faithfully show model behavior.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç»„ç»´åº¦åº¦é‡ï¼Œä½¿ç”¨è§†è§‰ä¼˜å…ˆæ¥è¯„ä¼°é’ˆå¯¹å›¾åƒåˆ†ç±»ä»»åŠ¡çš„ç²¾åº¦æ–¹æ³•çš„è¡¨ç°ã€‚åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œè®¸å¤šæ–¹æ³•æä¾›è§†è§‰ç²¾åº¦åœ°å›¾ï¼Œå¼ºè°ƒå›¾åƒåŒºåŸŸå¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®ã€‚ç„¶è€Œï¼Œå¯¹äºåˆ†ææ·±åº¦å­¦ä¹ æ¨¡å‹å†³ç­–çš„å¯é æ€§çš„å·¥ä½œå‡ ä¹ç¼ºä¹ã€‚æˆ‘ä»¬æå‡ºäº†COnsistency-SEnsitivityï¼ˆCOSEï¼‰åº¦é‡ï¼Œç”¨äºè¡¡é‡è§†è§‰æ¨¡å‹è§£é‡Šçš„ç­‰å˜å’Œä¸å˜æ€§ã€‚é€šè¿‡æˆ‘ä»¬çš„åº¦é‡ï¼Œæˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶è®¸å¤šæ–¹æ³•è¢«è®¤ä¸ºæ˜¯æ— å…³äºæ¨¡å‹ç»“æ„çš„ï¼Œä½†å¤§å¤šæ•°æ–¹æ³•åœ¨åŸºäºè½¬æ¢å™¨æ¨¡å‹æ—¶è¡¨ç°è¾ƒå¥½ã€‚æ­¤å¤–ï¼ŒGradCAMåœ¨COSEæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒåœ¨ç»†è…»æ•°æ®ä¸Šç¼ºä¹å˜åŒ–ã€‚è¿™ç§å¯¹ç…§æ€§Allowæˆ‘ä»¬ä»ä¸åŒè§’åº¦åˆ†æç²¾åº¦æ–¹æ³•ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬å‘ç°ï¼Œä¸ºäº†è®©ç²¾åº¦åœ°å›¾å‡†ç¡®åæ˜ æ¨¡å‹è¡Œä¸ºï¼Œéœ€è¦å¹³è¡¡è¿™ä¸¤ä¸ªåº¦é‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="RMT-Retentive-Networks-Meet-Vision-Transformers"><a href="#RMT-Retentive-Networks-Meet-Vision-Transformers" class="headerlink" title="RMT: Retentive Networks Meet Vision Transformers"></a>RMT: Retentive Networks Meet Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11523">http://arxiv.org/abs/2309.11523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qihang Fan, Huaibo Huang, Mingrui Chen, Hongmin Liu, Ran He</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æ¢è®¨å°†Retrieval Networkï¼ˆRetNetï¼‰çš„æ€æƒ³åº”ç”¨äºè®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œä»¥æé«˜è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ€§èƒ½ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»„åˆRetNetå’ŒTransformerçš„æ¨¡å‹ï¼Œç§°ä¸ºRMTã€‚è¯¥æ¨¡å‹å¼•å…¥äº†æ˜ç¡®çš„è¡°å‡å…ƒç´ ï¼Œä»¥å¸®åŠ©è®¡ç®—æœºè§†è§‰æ¨¡å‹æ›´å¥½åœ°æ§åˆ¶å„ä¸ªTokençš„èŒƒå›´ã€‚æ­¤å¤–ï¼Œä¸ºäº†é™ä½å…¨æ¨¡å‹çš„è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åˆ†è§£æˆä¸¤ä¸ªåæ ‡è½´ä¸Šçš„åˆ†å‰²ã€‚</li>
<li>results: æˆ‘ä»¬çš„RMTåœ¨å¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚åœ¨ImageNet-1kä¸Šè¾¾åˆ°84.1%çš„Top1-accï¼Œä½¿ç”¨äº†ä»…4.5G FLOPsã€‚æ­¤å¤–ï¼ŒRMTåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¦‚ç‰©ä½“æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œsemantic segmentationä¸­ä¹Ÿè¡¨ç°å‡ºä¼˜å¼‚ã€‚<details>
<summary>Abstract</summary>
Transformer first appears in the field of natural language processing and is later migrated to the computer vision domain, where it demonstrates excellent performance in vision tasks. However, recently, Retentive Network (RetNet) has emerged as an architecture with the potential to replace Transformer, attracting widespread attention in the NLP community. Therefore, we raise the question of whether transferring RetNet's idea to vision can also bring outstanding performance to vision tasks. To address this, we combine RetNet and Transformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay into the vision backbone, bringing prior knowledge related to spatial distances to the vision model. This distance-related spatial prior allows for explicit control of the range of tokens that each token can attend to. Additionally, to reduce the computational cost of global modeling, we decompose this modeling process along the two coordinate axes of the image. Abundant experiments have demonstrated that our RMT exhibits exceptional performance across various computer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k using merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT achieves the highest Top1-acc when models are of similar size and trained with the same strategy. Moreover, RMT significantly outperforms existing vision backbones in downstream tasks such as object detection, instance segmentation, and semantic segmentation. Our work is still in progress.
</details>
<details>
<summary>æ‘˜è¦</summary>
transformer æœ€åˆå‡ºç°åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œåæ¥è¿ç§»åˆ°è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œåœ¨è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼Œæœ€è¿‘ï¼ŒRetentive Networkï¼ˆRetNetï¼‰ Architecture å‡ºç°ï¼Œå¸å¼•äº†è‡ªç„¶è¯­è¨€ç¤¾åŒºçš„å¹¿æ³›å…³æ³¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å°† RetNet çš„æƒ³æ³•åº”ç”¨äºè§†è§‰é¢†åŸŸï¼Œä»¥æé«˜è§†è§‰ä»»åŠ¡çš„è¡¨ç°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°† RetNet å’Œ transformer ç»“åˆï¼Œæå‡ºäº† RMTã€‚ RetNet ä¸­å¼•å…¥äº†æ˜¾å¼è¡°å‡ï¼Œä½¿è§†è§‰æ¨¡å‹å—åˆ°ç›¸å¯¹è·ç¦»çš„çŸ¥è¯†ã€‚è¿™ç§è·ç¦»ç›¸å…³çš„ç©ºé—´å…ˆéªŒä½¿æ¯ä¸ªtokenå¯ä»¥æ˜¾å¼æ§åˆ¶æ‰€èƒ½attendçš„tokenèŒƒå›´ã€‚æ­¤å¤–ï¼Œä¸ºé™ä½å…¨å±€æ¨¡å‹çš„è®¡ç®—æˆæœ¬ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åŒ–è¿‡ç¨‹åˆ†è§£æˆä¸¤ä¸ªåæ ‡è½´çš„å›¾åƒã€‚æˆ‘ä»¬çš„ RMT åœ¨å¤šä¸ªè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¾‹å¦‚åœ¨ ImageNet-1k ä¸­ achiev 84.1% Top1-acc ä½¿ç”¨ä»… 4.5G FLOPsã€‚æˆ‘ä»¬çŸ¥é“ï¼Œåœ¨åŒæ ·å¤§å°çš„æ¨¡å‹å’ŒåŒæ ·ç­–ç•¥ä¸‹ï¼ŒRMT åœ¨æ‰€æœ‰æ¨¡å‹ä¸­å…·æœ‰æœ€é«˜çš„ Top1-accã€‚æ­¤å¤–ï¼ŒRMT åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œå¦‚ç‰©ä½“æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œ semantics åˆ†å‰²ï¼Œä¹Ÿè¡¨ç°å‡ºäº†æ˜¾è‘—çš„ä¼˜å¼‚ã€‚æˆ‘ä»¬çš„å·¥ä½œä»åœ¨è¿›è¡Œä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="SEMPART-Self-supervised-Multi-resolution-Partitioning-of-Image-Semantics"><a href="#SEMPART-Self-supervised-Multi-resolution-Partitioning-of-Image-Semantics" class="headerlink" title="SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics"></a>SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10972">http://arxiv.org/abs/2309.10972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sriram Ravindran, Debraj Basu</li>
<li>for: æœ¬æ–‡ä¸ºäº†è§£å†³åŸºäºå›¾åƒæ•°æ®çš„ç¨€ç¼ºæ—¶ï¼Œå‡†ç¡®åœ°å®šä¹‰å›¾åƒä¸­é‡è¦çš„åŒºåŸŸè€Œä½œå‡ºäº†è´¡çŒ®ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†åŸºäºDINOçš„è‡ªåŠ¨ç¼–å†™æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨å›¾åƒ semantic graphä¸­çš„å«ä¹‰æ¥æ‰¾åˆ°å‰æ™¯ç‰©ä½“ã€‚</li>
<li>results: æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºSEMPARTçš„æ–¹æ³•ï¼Œå¯ä»¥åŒæ—¶ç¡®å®šå›¾åƒçš„ç²—ç»†åˆ†å‰²å’Œç»†åˆ†å‰²ï¼Œå¹¶ä¸”å¯ä»¥å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„maskã€‚<details>
<summary>Abstract</summary>
Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç²¾ç¡®åœ°å®šä¹‰å›¾åƒä¸­é‡è¦åŒºåŸŸæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå°¤å…¶å½“æ ‡æ³¨æ•°æ®ç¨€ç¼ºæ—¶ã€‚åŸºäºDINOçš„è‡ªåŠ¨å­¦ä¹ æ–¹æ³•æœ€è¿‘åœ¨æ•æ‰å›¾åƒä¸­å…·æœ‰æ„ä¹‰çš„Semantic Featureä¸­æ‰¾åˆ°äº†å‰æ™¯å¯¹è±¡ã€‚ç°æœ‰æ–¹æ³•è¿˜å°†ç›´è§‰çº¦æŸ incorporated åˆ°äº†æ— ç›‘ç£æ–¹æ³•ä¸­ï¼Œå¹¶åœ¨å¯¹è±¡åˆ†å‰²æ–¹é¢è¡¨ç°å‡ºäº†ä»·å€¼ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æè®®äº† SEMPARTï¼Œå®ƒåŒæ—¶åˆ†è§£å›¾åƒçš„DINOåŸºäºsemantic graphçš„ç²—ç»†åˆ†å‰²ç»“æœã€‚æ­¤å¤–ï¼ŒSEMPARTè¿˜ä½¿ç”¨å›¾åƒé©±åŠ¨çš„æ­£åˆ™åŒ–æ¥ä¿æŒç»†èŠ‚ï¼Œå¹¶æˆåŠŸåœ°å‚¨å­˜ç²—ç»†åˆ†å‰²ç»“æœã€‚æˆ‘ä»¬çš„ç²¾ç¡®å¯¹è±¡æ£€æµ‹å’Œå•ä¸ªå¯¹è±¡Localizationç»“æœè¡¨æ˜ï¼ŒSEMPARTå¯ä»¥å¿«é€Ÿç”Ÿæˆé«˜è´¨é‡çš„Maskï¼Œæ— éœ€é¢å¤–å¤„ç†ï¼Œå¹¶ä¸”å—ç›Šäºç²—ç»†åˆ†æ”¯çš„å…±åŒä¼˜åŒ–ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.CV_2023_09_20/" data-id="clopawnsl00ioag887a5c92p8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.AI_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T12:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.AI_2023_09_20/">cs.AI - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="RAI4IoE-Responsible-AI-for-Enabling-the-Internet-of-Energy"><a href="#RAI4IoE-Responsible-AI-for-Enabling-the-Internet-of-Energy" class="headerlink" title="RAI4IoE: Responsible AI for Enabling the Internet of Energy"></a>RAI4IoE: Responsible AI for Enabling the Internet of Energy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11691">http://arxiv.org/abs/2309.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minhui Xue, Surya Nepal, Ling Liu, Subbu Sethuvenkatraman, Xingliang Yuan, Carsten Rudolph, Ruoxi Sun, Greg Eisenhauer</li>
<li>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªå…¬å¹³ä¸”è´Ÿè´£ä»»çš„AIæ¡†æ¶ï¼Œä»¥ä¾¿åœ¨äº’è”ç½‘èƒ½æºï¼ˆIoEï¼‰ä¸­å®ç°å¯é çš„èƒ½æºåˆ†å¸ƒã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†å…ˆè¿›çš„5G-6Gç½‘ç»œå’ŒAIæŠ€æœ¯ï¼Œä»¥è¿æ¥å’Œ Ğ¸Ğ½Ñ‚Ğµæ ¼åŠ›åŒ–å¯å†ç”Ÿåˆ†å¸ƒèƒ½æºèµ„æºï¼ˆDERsï¼‰ï¼Œå¦‚ç”µåŠ¨è½¦ã€å­˜å‚¨ç”µæ± ã€é£åŠ›å‘ç”µå’Œå¤ªé˜³èƒ½ç”µæ± ã€‚è¿™ä½¿å¾—DERæ‰€æœ‰è€…ä½œä¸ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…ï¼ˆprosumersï¼‰å¯ä»¥å‚ä¸èƒ½æºå¸‚åœºï¼Œå¹¶ä»ä¸­è·å¾—ç»æµæ”¶ç›Šã€‚</li>
<li>results: è¯¥ç ”ç©¶çš„ç›®æ ‡æ˜¯ç¡®ä¿ç¤¾åŒºæˆå‘˜çš„å…¬å¹³å‚ä¸ï¼Œå¹¶è´Ÿè´£ä½¿ç”¨ä»–ä»¬çš„æ•°æ®ï¼Œä»¥ä¾¿åœ¨IoEä¸­æä¾›å®‰å…¨ã€å¯é å’Œå¯å†ç”Ÿçš„èƒ½æºæœåŠ¡ã€‚<details>
<summary>Abstract</summary>
This paper plans to develop an Equitable and Responsible AI framework with enabling techniques and algorithms for the Internet of Energy (IoE), in short, RAI4IoE. The energy sector is going through substantial changes fueled by two key drivers: building a zero-carbon energy sector and the digital transformation of the energy infrastructure. We expect to see the convergence of these two drivers resulting in the IoE, where renewable distributed energy resources (DERs), such as electric cars, storage batteries, wind turbines and photovoltaics (PV), can be connected and integrated for reliable energy distribution by leveraging advanced 5G-6G networks and AI technology. This allows DER owners as prosumers to participate in the energy market and derive economic incentives. DERs are inherently asset-driven and face equitable challenges (i.e., fair, diverse and inclusive). Without equitable access, privileged individuals, groups and organizations can participate and benefit at the cost of disadvantaged groups. The real-time management of DER resources not only brings out the equity problem to the IoE, it also collects highly sensitive location, time, activity dependent data, which requires to be handled responsibly (e.g., privacy, security and safety), for AI-enhanced predictions, optimization and prioritization services, and automated management of flexible resources. The vision of our project is to ensure equitable participation of the community members and responsible use of their data in IoE so that it could reap the benefits of advances in AI to provide safe, reliable and sustainable energy services.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä»½ç ”ç©¶æŠ¥å‘Šè®¡åˆ’å¼€å‘ä¸€ä¸ªå…¬å¹³å’Œè´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼ˆRAI4IoEï¼‰ï¼Œç”¨äºäº’è”ç½‘èƒ½æºï¼ˆIoEï¼‰é¢†åŸŸã€‚èƒ½æºé¢†åŸŸæ­£åœ¨ç»å†é‡å¤§å˜é©ï¼Œè¿™ä¸¤ä¸ªå…³é”®é©±åŠ¨å› ç´ ï¼šå»ºç«‹é›¶ç¢³ç´ èƒ½æºäº§ä¸šå’Œèƒ½æºåŸºç¡€è®¾æ–½çš„æ•°å­—å˜é©ã€‚æˆ‘ä»¬é¢„è®¡è¿™ä¸¤ä¸ªé©±åŠ¨å› ç´ ä¼šç›¸äº’äº¤é›†ï¼Œå¯¼è‡´IoEçš„å‡ºç°ï¼Œå…¶ä¸­å¯å†ç”Ÿåˆ†å¸ƒå¼èƒ½æºèµ„æºï¼ˆDERsï¼‰ï¼Œå¦‚ç”µåŠ¨è½¦ã€å­˜å‚¨ç”µæ± ã€é£åŠ›å‘ç”µå’Œå¤ªé˜³èƒ½ç”µæ± ï¼ˆPVï¼‰ï¼Œå¯ä»¥ç›¸äº’è¿æ¥å’Œé›†æˆï¼Œä»¥å®ç°å¯é çš„èƒ½æºåˆ†å¸ƒï¼Œé€šè¿‡åˆ©ç”¨å…ˆè¿›çš„5G-6Gç½‘ç»œå’Œäººå·¥æ™ºèƒ½æŠ€æœ¯ã€‚è¿™å…è®¸DERæ‰€æœ‰è€…ä½œä¸ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…ï¼ˆprosumersï¼‰å‚ä¸èƒ½æºå¸‚åœºï¼Œä»è€Œè·å¾—ç»æµæ”¶ç›Šã€‚DERsæœ¬èº«å…·æœ‰èµ„äº§é©±åŠ¨çš„ç‰¹ç‚¹ï¼Œé¢ä¸´å…¬å¹³æŒ‘æˆ˜ï¼ˆä¾‹å¦‚ï¼Œå…¬å¹³ã€å¤šæ ·åŒ–å’ŒåŒ…å®¹ï¼‰ã€‚å¦‚æœæ²¡æœ‰å…¬å¹³è®¿é—®ï¼Œç‰¹æƒä¸ªäººã€ç»„ç»‡å’Œé›†å›¢å¯ä»¥å‚ä¸å’Œè·å¾—åˆ©ç›Šï¼Œè€Œå—æŠ˜ç£¨çš„ç¾¤ä½“åˆ™è¢«æ’é™¤åœ¨å¤–ã€‚IoEå®æ—¶ç®¡ç†DERèµ„æºä¸ä»…æŠ›å‡ºäº†å…¬å¹³é—®é¢˜ï¼Œè¿˜æ”¶é›†äº†é«˜åº¦æ•æ„Ÿçš„åœ°ç‚¹ã€æ—¶é—´ã€æ´»åŠ¨ä¾èµ–æ•°æ®ï¼Œéœ€è¦è´Ÿè´£ä»»åœ°å¤„ç†ï¼ˆä¾‹å¦‚ï¼Œéšç§ã€å®‰å…¨å’Œå®‰å…¨ï¼‰ï¼Œä»¥ä¾¿é€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯æä¾›äº†é¢„æµ‹ã€ä¼˜åŒ–å’Œä¼˜å…ˆçº§æœåŠ¡ï¼Œè‡ªåŠ¨ç®¡ç†çµæ´»èµ„æºã€‚æˆ‘ä»¬çš„é¡¹ç›®è§†å›¾æ˜¯ç¡®ä¿ç¤¾åŒºæˆå‘˜å…¬å¹³å‚ä¸IoEï¼Œå¹¶è´Ÿè´£ä½¿ç”¨ä»–ä»¬çš„æ•°æ®ï¼Œä»¥ä¾¿IoEå¯ä»¥é€šè¿‡äººå·¥æ™ºèƒ½æŠ€æœ¯çš„è¿›æ­¥è·å¾—å®‰å…¨ã€å¯é å’Œå¯å†ç”Ÿçš„èƒ½æºæœåŠ¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="LLM-Guided-Inductive-Inference-for-Solving-Compositional-Problems"><a href="#LLM-Guided-Inductive-Inference-for-Solving-Compositional-Problems" class="headerlink" title="LLM Guided Inductive Inference for Solving Compositional Problems"></a>LLM Guided Inductive Inference for Solving Compositional Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11688">http://arxiv.org/abs/2309.11688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhigya Sodani, Lauren Moos, Matthew Mirman</li>
<li>for: è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ˜¯å®ƒä»¬çš„è¡¨ç°å—é™äºé—®é¢˜ä¸­ä¸åŒ…å«åœ¨æ¨¡å‹è®­ç»ƒæ•°æ®ä¸­çš„çŸ¥è¯†ï¼Œéœ€è¦é€šè¿‡ç›´æ¥è§‚å¯Ÿæˆ–ä¸å®é™…ä¸–ç•Œäº¤äº’æ¥è·å¾—ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå³ Recursion based extensible LLMï¼ˆREBELï¼‰ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨ç†è§£æŠ€æœ¯å¦‚åŠ¨æ€è§„åˆ’å’Œå‰è¿›é“¾æ¥ç­–ç•¥æ¥å¤„ç†å¼€æ”¾ä¸–ç•Œã€æ·±åº¦ç†è§£ä»»åŠ¡ã€‚REBELä½¿ç”¨è‡ªç„¶è¯­è¨€æè¿°æ¥æŒ‡å®šå·¥å…·ï¼Œå¹¶ä½¿ç”¨è¿™äº›å·¥å…·è¿›è¡Œé€’å½’é—®é¢˜åˆ†è§£å’Œå¤–éƒ¨å·¥å…·ä½¿ç”¨ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨ä¸€ç»„éœ€è¦æ·±åº¦åµŒå¥—ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„é—®é¢˜ä¸Šç¤ºå‡ºäº†REBELçš„èƒ½åŠ›ï¼Œå¹¶åœ¨ä¸€ä¸ªç»„åˆå’Œå¯¹è¯æ€§çš„ Settingä¸­è¿›è¡Œäº†è¯æ˜ã€‚<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated impressive performance in question-answering tasks, their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world. Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks. We introduce a method, Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by employing automated reasoning techniques like dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason via recursive problem decomposition and utilization of external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational setting.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„è¡¨ç°å—åˆ°è®­ç»ƒæ•°æ®ä¸­ä¸åŒ…å«çš„çŸ¥è¯†çš„é™åˆ¶ã€‚ç°æœ‰çš„æ–¹æ³•é€šè¿‡è¿è¡Œæ¨¡ç»„æ¥ decomposing æ¨ç†ä»»åŠ¡ï¼Œé™åˆ¶å®ƒä»¬ Answer deep reasoning tasks. We propose a method called Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by using automated reasoning techniques such as dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason through recursive problem decomposition and utilize external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL's capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational setting.
</details></li>
</ul>
<hr>
<h2 id="Dr-FERMI-A-Stochastic-Distributionally-Robust-Fair-Empirical-Risk-Minimization-Framework"><a href="#Dr-FERMI-A-Stochastic-Distributionally-Robust-Fair-Empirical-Risk-Minimization-Framework" class="headerlink" title="Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework"></a>Dr. FERMI: A Stochastic Distributionally Robust Fair Empirical Risk Minimization Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11682">http://arxiv.org/abs/2309.11682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sina Baharlouei, Meisam Razaviyayn</li>
<li>for: This paper aims to address the issue of fair machine learning models behaving unfairly on test data due to distribution shifts.</li>
<li>methods: The proposed method is based on distributionally robust optimization under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. The method does not require knowledge of the causal graph and can be implemented in a stochastic fashion.</li>
<li>results: The proposed framework has been evaluated through extensive experiments on real datasets consisting of distribution shifts, and the results show that it performs well in terms of fairness and efficiency.<details>
<summary>Abstract</summary>
While training fair machine learning models has been studied extensively in recent years, most developed methods rely on the assumption that the training and test data have similar distributions. In the presence of distribution shifts, fair models may behave unfairly on test data. There have been some developments for fair learning robust to distribution shifts to address this shortcoming. However, most proposed solutions are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. More specifically, we formulate the fair inference in the presence of the distribution shift as a distributionally robust optimization problem under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. We then discuss how the proposed method can be implemented in a stochastic fashion. We have evaluated the presented framework's performance and efficiency through extensive experiments on real datasets consisting of distribution shifts.
</details>
<details>
<summary>æ‘˜è¦</summary>
traditional machine learning models have been extensively studied in recent years, but most of these methods rely on the assumption that the training and test data have similar distributions. However, in the presence of distribution shifts, fair models may behave unfairly on test data. To address this shortcoming, there have been some developments in fair learning that are robust to distribution shifts, but these methods are based on the assumption of having access to the causal graph describing the interaction of different features. Moreover, existing algorithms require full access to data and cannot be used when small batches are used (stochastic/batch implementation). This paper proposes the first stochastic distributionally robust fairness framework with convergence guarantees that do not require knowledge of the causal graph. Specifically, we formulate the fair inference in the presence of distribution shift as a distributionally robust optimization problem under $L_p$ norm uncertainty sets with respect to the Exponential Renyi Mutual Information (ERMI) as the measure of fairness violation. We then discuss how the proposed method can be implemented in a stochastic fashion. We have evaluated the presented framework's performance and efficiency through extensive experiments on real datasets consisting of distribution shifts.Here's the translation in Traditional Chinese:ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æœ€è¿‘çš„å¹´ä»½å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†å¤§å¤šæ•°è¿™äº›æ–¹æ³•å‡è®¾è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„åˆ†å¸ƒç›¸ä¼¼ã€‚ç„¶è€Œï¼Œåœ¨åˆ†å¸ƒshiftæƒ…å†µä¸‹ï¼Œå…¬å¹³çš„æ¨¡å‹å¯èƒ½ä¼šåœ¨æµ‹è¯•æ•°æ®ä¸Šä¸å…¬å¹³ã€‚ä¸ºäº†è§£å†³è¿™é—®é¢˜ï¼Œæœ‰äº›å¼€å‘äº†ä¸åŒçš„å…¬å¹³å­¦ä¹ æ–¹æ³•ï¼Œä½†è¿™äº›æ–¹æ³•å‡è®¾æœ‰å­˜åœ¨ causal graph æè¿°ä¸åŒç‰¹å¾ä¹‹é—´çš„äº’åŠ¨ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„ç®—æ³•éœ€è¦å®Œæ•´çš„æ•°æ®å­˜å–ï¼Œå¹¶ä¸”æ— æ³•åœ¨å°æ‰¹é‡ä¸­ä½¿ç”¨ï¼ˆstochastic/batchå®ç°ï¼‰ã€‚æœ¬æ–‡æå‡ºäº†é¦–ä¸ªå¯é çš„åˆ†å¸ƒrobustå…¬å¹³æ€§æ¡†æ¶ï¼Œä¸éœ€è¦çŸ¥é“ causal graphã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°† fair inference åœ¨åˆ†å¸ƒshiftæƒ…å†µä¸‹å½¢å¼åŒ–ä¸ºåˆ†å¸ƒrobustä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä½¿ç”¨ $L_p$  Ğ½Ğ¾Ñ€ uncertainty set æ¥åº¦é‡å…¬å¹³è¿åã€‚æˆ‘ä»¬ç„¶åè®¨è®ºäº†å¦‚ä½•å®ç°è¿™ä¸ªæ–¹æ³•åœ¨æŠ½è±¡çš„æ–¹å¼ä¸Šã€‚æˆ‘ä»¬é€šè¿‡å®é™…çš„å®éªŒï¼Œè¯„ä¼°äº†æå‡ºçš„æ¡†æ¶çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œä»¥åŠå®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-with-Neural-Graphical-Models"><a href="#Federated-Learning-with-Neural-Graphical-Models" class="headerlink" title="Federated Learning with Neural Graphical Models"></a>Federated Learning with Neural Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11680">http://arxiv.org/abs/2309.11680</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Urszula Chajewska, Harsh Shrivastava</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨åˆ›å»ºåŸºäºä¸“æœ‰æ•°æ®çš„æ¨¡å‹ï¼Œä»¥ä¾¿å¤šä¸ªå®¢æˆ·ä¿ç•™ä¸“æœ‰æ•°æ®æ§åˆ¶æƒï¼ŒåŒæ—¶é€šè¿‡å…±äº«èµ„æºæé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„è”é‚¦å­¦ä¹ æ¡†æ¶ï¼ˆFedNGMsï¼‰ï¼Œä½¿ç”¨ probabilistic graphical modelsï¼ˆNGMsï¼‰å­¦ä¹ å¤æ‚çš„è¾“å…¥ç‰¹å¾ä¹‹é—´çš„éçº¿æ€§å…³ç³»ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„ FedNGMs æ¡†æ¶å¯ä»¥é¿å… neuron matching æ¡†æ¶å¦‚ Federated Matched Averaging çš„ç¼ºç‚¹ï¼Œå¹¶ä¸”å¯ä»¥é€‚åº”æ•°æ®ä¸å‡è¡¡ã€å¤šä¸ªå‚ä¸è€…å’Œlimited communication bandwidth ç­‰é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Federated Learning (FL) addresses the need to create models based on proprietary data in such a way that multiple clients retain exclusive control over their data, while all benefit from improved model accuracy due to pooled resources. Recently proposed Neural Graphical Models (NGMs) are Probabilistic Graphical models that utilize the expressive power of neural networks to learn complex non-linear dependencies between the input features. They learn to capture the underlying data distribution and have efficient algorithms for inference and sampling. We develop a FL framework which maintains a global NGM model that learns the averaged information from the local NGM models while keeping the training data within the client's environment. Our design, FedNGMs, avoids the pitfalls and shortcomings of neuron matching frameworks like Federated Matched Averaging that suffers from model parameter explosion. Our global model size remains constant throughout the process. In the cases where clients have local variables that are not part of the combined global distribution, we propose a `Stitching' algorithm, which personalizes the global NGM models by merging the additional variables using the client's data. FedNGM is robust to data heterogeneity, large number of participants, and limited communication bandwidth.
</details>
<details>
<summary>æ‘˜è¦</summary>
Federated Learning (FL) è§£å†³äº†åŸºäºä¸“æœ‰æ•°æ®çš„æ¨¡å‹åˆ›å»ºçš„éœ€æ±‚ï¼Œä»¥ä¾¿å¤šä¸ªå®¢æˆ·ç«¯ä¿ç•™ä¸“æœ‰æ•°æ®æ§åˆ¶æƒï¼Œè€ŒåŒæ—¶å„è‡ªå—ç›Šäºå…±äº«èµ„æºçš„æé«˜æ¨¡å‹ç²¾åº¦ã€‚æœ€è¿‘æå‡ºçš„ç¥ç»å›¾æ¨¡å‹ï¼ˆNGMï¼‰æ˜¯ä¸€ç§æ¦‚ç‡å›¾æ¨¡å‹ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›æ¥å­¦ä¹ è¾“å…¥ç‰¹å¾ä¹‹é—´çš„å¤æ‚éçº¿æ€§å…³ç³»ã€‚å®ƒä»¬å­¦ä¹ ä¸‹é¢æ•°æ®åˆ†å¸ƒï¼Œå¹¶æœ‰æ•ˆçš„æ¨ç†å’Œé‡‡æ ·ç®—æ³•ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŸºäºFLçš„æ¡†æ¶ï¼Œç§°ä¹‹ä¸ºFedNGMsï¼Œè¯¥æ¡†æ¶åœ¨å®¢æˆ·ç«¯ç¯å¢ƒä¸­ä¿æŒglobal NGMæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å­¦ä¹ å®¢æˆ·ç«¯çš„local NGMæ¨¡å‹ä¸­çš„å‡å€¼ä¿¡æ¯ï¼Œè€Œä¸éœ€è¦å°†è®­ç»ƒæ•°æ®ä¼ è¾“åˆ°å®¢æˆ·ç«¯ã€‚æˆ‘ä»¬çš„è®¾è®¡é¿å…äº†ç¥ç»ç½‘ç»œåŒ¹é…æ¡†æ¶å¦‚è”é‚¦åŒ¹é…å¹³å‡çš„ç¼ºç‚¹ï¼Œä¾‹å¦‚æ¨¡å‹å‚æ•°çˆ†ç‚¸ã€‚æˆ‘ä»¬çš„å…¨çƒæ¨¡å‹å¤§å°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿æŒä¸å˜ã€‚åœ¨å®¢æˆ·ç«¯æœ‰æœ¬åœ°å˜é‡ï¼Œè¿™äº›å˜é‡ä¸æ˜¯å…¨å±€å…±äº«çš„å…±åŒåˆ†å¸ƒä¸­çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨"ç¼åˆ"ç®—æ³•ï¼Œå°†è¿™äº›å˜é‡ä¸å…¨å±€NGMæ¨¡å‹è¿›è¡Œä¸ªæ€§åŒ–ç»“åˆã€‚FedNGMå…·æœ‰å¯¹æ•°æ®ä¸ä¸€è‡´ã€å¤§é‡å‚ä¸è€…å’Œæœ‰é™é€šä¿¡å¸¦å®½çš„ Robustnessã€‚
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-in-Mafia-like-Game-Simulation"><a href="#Generative-AI-in-Mafia-like-Game-Simulation" class="headerlink" title="Generative AI in Mafia-like Game Simulation"></a>Generative AI in Mafia-like Game Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11672">http://arxiv.org/abs/2309.11672</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MunyeongKim/Gen-AI-in-Mafia-like-Game">https://github.com/MunyeongKim/Gen-AI-in-Mafia-like-Game</a></li>
<li>paper_authors: Munyeong Kim, Sungsu Kim</li>
<li>for: è¿™é¡¹ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹åœ¨è§’è‰²æ‰®æ¼” simulations ä¸­çš„å¯èƒ½æ€§å’Œæ½œåŠ›ï¼Œä»¥æ¸¸æˆ Spyfall ä¸ºä¾‹ã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨ GPT-4 çš„é«˜çº§åŠŸèƒ½ï¼Œé€šè¿‡å¯¹æ¸¸æˆåœºæ™¯çš„ç†è§£ã€å†³ç­–å’Œäº’åŠ¨è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œä»¥è¯æ˜æ¨¡å‹åœ¨è¿™äº›æ–¹é¢çš„æ½œåŠ›ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒGPT-4 åœ¨æ¸¸æˆç¯å¢ƒä¸­çš„é€‚åº”æ€§æœ‰æ˜¾è‘—æé«˜ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æé—®å’Œå‘è¡¨äººç±»åŒ–çš„å›ç­”ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨éª—å–å’Œé¢„æµ‹å¯¹æ‰‹è¡ŒåŠ¨æ–¹é¢å­˜åœ¨é™åˆ¶ã€‚ç ”ç©¶è¿˜è®¨è®ºäº†æ¸¸æˆå¼€å‘ã€è´¢æ”¿é™åˆ¶å’Œéè¯­è¨€é™åˆ¶çš„é—®é¢˜ã€‚ç»“æœè¡¨æ˜ï¼Œè™½ç„¶ GPT-4 è¡¨ç°å‡ºäº†è¾ƒæ—©æ¨¡å‹çš„è¿›æ­¥ï¼Œä½†è¿˜æœ‰æ›´å¤šçš„å‘å±•ç©ºé—´ï¼Œå°¤å…¶æ˜¯åœ¨å¡‘é€ æ›´äººç±»åŒ–çš„ AI æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
In this research, we explore the efficacy and potential of Generative AI models, specifically focusing on their application in role-playing simulations exemplified through Spyfall, a renowned mafia-style game. By leveraging GPT-4's advanced capabilities, the study aimed to showcase the model's potential in understanding, decision-making, and interaction during game scenarios. Comparative analyses between GPT-4 and its predecessor, GPT-3.5-turbo, demonstrated GPT-4's enhanced adaptability to the game environment, with significant improvements in posing relevant questions and forming human-like responses. However, challenges such as the model;s limitations in bluffing and predicting opponent moves emerged. Reflections on game development, financial constraints, and non-verbal limitations of the study were also discussed. The findings suggest that while GPT-4 exhibits promising advancements over earlier models, there remains potential for further development, especially in instilling more human-like attributes in AI.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢äº†ç”ŸæˆAIæ¨¡å‹çš„æ•ˆæœå’Œæ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¸¸æˆè§’è‰²æ‰®æ¼” simulationsä¸­çš„åº”ç”¨ã€‚é€šè¿‡åˆ©ç”¨GPT-4çš„é«˜çº§åŠŸèƒ½ï¼Œç ”ç©¶æ—¨åœ¨è¡¨æ˜æ¨¡å‹åœ¨æ¸¸æˆåœºæ™¯ä¸­çš„ç†è§£ã€å†³ç­–å’Œäº’åŠ¨çš„æ½œåŠ›ã€‚å¯¹æ¯”GPT-4å’Œå…¶å‰ä¸€ä»£GPT-3.5-turboï¼Œç ”ç©¶å‘ç°GPT-4åœ¨æ¸¸æˆç¯å¢ƒä¸­çš„é€‚åº”æ€§å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æé—®å’Œè¡¨è¾¾äººç±»åŒ–çš„é—®é¢˜æ–¹é¢ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨è°è¨€å’Œé¢„æµ‹å¯¹æ‰‹è¡ŒåŠ¨æ–¹é¢å­˜åœ¨é™åˆ¶ã€‚ç ”ç©¶è¿˜è®¨è®ºäº†æ¸¸æˆå¼€å‘ã€è´¢åŠ¡é™åˆ¶å’Œéè¯­è¨€é™åˆ¶çš„é—®é¢˜ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè™½ç„¶GPT-4åœ¨å‰ä¸€ä»£æ¨¡å‹ä¹‹ä¸Šå…·æœ‰æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†è¿˜æœ‰å¯èƒ½è¿›ä¸€æ­¥å‘å±•ï¼Œå°¤å…¶æ˜¯åœ¨å…·å¤‡æ›´å¤šäººç±»ç‰¹å¾çš„AIæ–¹é¢ã€‚
</details></li>
</ul>
<hr>
<h2 id="â€œItâ€™s-a-Fair-Gameâ€™â€™-or-Is-It-Examining-How-Users-Navigate-Disclosure-Risks-and-Benefits-When-Using-LLM-Based-Conversational-Agents"><a href="#â€œItâ€™s-a-Fair-Gameâ€™â€™-or-Is-It-Examining-How-Users-Navigate-Disclosure-Risks-and-Benefits-When-Using-LLM-Based-Conversational-Agents" class="headerlink" title="â€œItâ€™s a Fair Gameâ€™â€™, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents"></a>â€œItâ€™s a Fair Gameâ€™â€™, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11653">http://arxiv.org/abs/2309.11653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiping Zhang, Michelle Jia, Hao-Ping, Lee, Bingsheng Yao, Sauvik Das, Ada Lerner, Dakuo Wang, Tianshi Li</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¸®åŠ©å»ºç«‹ä¼˜å…ˆè€ƒè™‘ç”¨æˆ·éšç§çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºå¯¹è¯ä»£ç†ï¼ˆCAï¼‰ï¼Œä»¥è§£å†³ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨æ¨¡å‹æ–¹é¢ï¼Œå¿½ç•¥ç”¨æˆ·è§†è§’çš„é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶é€šè¿‡åˆ†æå®é™…çš„ChatGPTå¯¹è¯å’Œå¯¹19åLLMåŸºäºCAç”¨æˆ·è¿›è¡Œsemiç»“æ„åŒ–é‡‡è®¿ï¼Œå‘ç°ç”¨æˆ·åœ¨ä½¿ç”¨LLMåŸºäºCAæ—¶ç»å¸¸é¢ä¸´privacyã€utilitieså’Œä¾¿åˆ©æ€§ä¹‹é—´çš„æƒè¡¡å†³ç­–ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ç”¨æˆ·çš„é”™è¯¯å¿ƒç†æ¨¡å‹å’Œç³»ç»Ÿè®¾è®¡ä¸­çš„é»‘æš— Patternsé™åˆ¶äº†ä»–ä»¬å¯¹éšç§é£é™©çš„è®¤è¯†å’Œç†è§£ï¼ŒåŒæ—¶äººå·¥æ™ºèƒ½åŒ–çš„äº¤äº’ä½¿ç”¨è€…æ›´å®¹æ˜“å¯¹è‡ªå·±çš„æ•æ„Ÿä¿¡æ¯è¿›è¡ŒæŠ«éœ²ï¼Œä½¿ç”¨è€…åœ¨å†³ç­–ä¸­å—åˆ°å¢åŠ çš„å›°éš¾ã€‚<details>
<summary>Abstract</summary>
The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users' perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users' erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users' ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigmatic shifts to protect the privacy of LLM-based CA users.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¹¿æ³›ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºäºå¯¹è¯ä»£ç†ï¼ˆCAï¼‰ï¼Œç‰¹åˆ«åœ¨é«˜é£é™©é¢†åŸŸï¼Œå¼•å‘äº†è®¸å¤šéšç§é—®é¢˜ã€‚å»ºç«‹å°Šé‡ç”¨æˆ·éšç§çš„LLMåŸºäºCAéœ€è¦æ·±å…¥äº†è§£ç”¨æˆ·å…³å¿ƒçš„éšç§é£é™©ã€‚ç„¶è€Œï¼Œç°æœ‰ç ”ç©¶ä¸»è¦å…³æ³¨æ¨¡å‹ï¼Œæœªèƒ½æä¾›ç”¨æˆ·è§†è§’çš„æ·±å…¥ç†è§£ã€‚ä¸ºäº†è¡¥å¼ºè¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬åˆ†æäº†å®é™…çš„ChatGPTå¯¹è¯ä¸­çš„æ•æ„Ÿæ³„éœ²ï¼Œå¹¶è¿›è¡Œäº†19åLLMåŸºäºCAç”¨æˆ·çš„semiç»“æ„åŒ–é‡‡è®¿ã€‚æˆ‘ä»¬å‘ç°ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨LLMåŸºäºCAæ—¶ç»å¸¸é¢ä¸´privacyã€åŠŸèƒ½å’Œä¾¿åˆ©æ€§ä¹‹é—´çš„æƒè¡¡ã€‚ç„¶è€Œï¼Œç”¨æˆ·çš„é”™è¯¯çš„è®¤çŸ¥æ¨¡å‹å’Œç³»ç»Ÿè®¾è®¡ä¸­çš„é»‘æš—Patternsé™åˆ¶äº†ä»–ä»¬å¯¹éšç§é£é™©çš„è®¤è¯†å’Œç†è§£ã€‚æ­¤å¤–ï¼Œäººç±»åŒ–çš„äº’åŠ¨æ›´åŠ é¼“åŠ±ç”¨æˆ·æä¾›æ›´å¤šçš„æ•æ„Ÿä¿¡æ¯ï¼Œä½¿ç”¨æˆ·æ›´éš¾avigateæƒè¡¡ã€‚æˆ‘ä»¬è®¨è®ºäº†å®ç”¨çš„è®¾è®¡æŒ‡å—å’Œä¿æŠ¤LLMåŸºäºCAç”¨æˆ·éšç§çš„éœ€æ±‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="Orbital-AI-based-Autonomous-Refuelling-Solution"><a href="#Orbital-AI-based-Autonomous-Refuelling-Solution" class="headerlink" title="Orbital AI-based Autonomous Refuelling Solution"></a>Orbital AI-based Autonomous Refuelling Solution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11648">http://arxiv.org/abs/2309.11648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duarte Rondao, Lei He, Nabil Aouf</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨ä½¿ç”¨æ‘„åƒå¤´è¿›è¡Œå¤ªç©ºå¯¹æ¥å’Œè½¨é“æœåŠ¡ï¼ˆOOSï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¥å°†æ‘„åƒå¤´å˜æˆä¸»è¦æ„ŸçŸ¥å™¨ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†è®¸å¤š convolutional neural networkï¼ˆCNNï¼‰Backboneæ¶æ„ï¼Œ benchmarked on synthetically generated docking manoeuvres with the International Space Stationï¼ˆISSï¼‰ï¼Œä»¥è·å¾—positionå’Œæ€åº¦ä¼°ç®—ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨AIå¯ä»¥å°†relative navigation solutionæ‰©å±•åˆ°å¤šç§enarioï¼Œä¾‹å¦‚targetsæˆ–ç…§æ˜æ¡ä»¶ï¼Œå¹¶ä¸”å¯ä»¥å®ç°positionå’Œæ€åº¦ä¼°ç®—çš„é«˜ç²¾åº¦ã€‚å®é™…ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥å¤§å¤§å‡å°‘äº†éœ€è¦çš„å·¥ç¨‹å¸ˆæ—¶é—´å’Œèµ„æºã€‚<details>
<summary>Abstract</summary>
Cameras are rapidly becoming the choice for on-board sensors towards space rendezvous due to their small form factor and inexpensive power, mass, and volume costs. When it comes to docking, however, they typically serve a secondary role, whereas the main work is done by active sensors such as lidar. This paper documents the development of a proposed AI-based (artificial intelligence) navigation algorithm intending to mature the use of on-board visible wavelength cameras as a main sensor for docking and on-orbit servicing (OOS), reducing the dependency on lidar and greatly reducing costs. Specifically, the use of AI enables the expansion of the relative navigation solution towards multiple classes of scenarios, e.g., in terms of targets or illumination conditions, which would otherwise have to be crafted on a case-by-case manner using classical image processing methods. Multiple convolutional neural network (CNN) backbone architectures are benchmarked on synthetically generated data of docking manoeuvres with the International Space Station (ISS), achieving position and attitude estimates close to 1% range-normalised and 1 deg, respectively. The integration of the solution with a physical prototype of the refuelling mechanism is validated in laboratory using a robotic arm to simulate a berthing procedure.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€èˆªå¤©å™¨çš„å‘å±•ï¼Œé•œå¤´åœ¨èˆªå¤©å™¨ä¸Šçš„åº”ç”¨ä¹Ÿåœ¨ä¸æ–­æ‰©å¤§ã€‚é•œå¤´çš„å°å‹åŒ–å’Œä½åŠŸè€—ã€è´¨é‡å’Œä½“ç§¯æˆæœ¬ä½¿å…¶æˆä¸ºèˆªå¤©å™¨ä¸Šçš„é¦–é€‰æ„ŸçŸ¥å™¨ã€‚ç„¶è€Œï¼Œåœ¨å docking è¿‡ç¨‹ä¸­ï¼Œé•œå¤´é€šå¸¸æ‰®æ¼”ç€æ¬¡è¦è§’è‰²ï¼Œä¸»è¦å·¥ä½œç”±æ´»åŠ¨æ„ŸçŸ¥å™¨ such as lidar å®Œæˆã€‚è¿™ç¯‡è®ºæ–‡æè¿°äº†ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¯¼èˆªç®—æ³•çš„å¼€å‘ï¼Œæ—¨åœ¨é€šè¿‡ä½¿ç”¨é•œå¤´æ¥æé«˜å docking å’Œç©ºé—´æœåŠ¡ï¼ˆOOSï¼‰ä¸­çš„ç²¾åº¦å’Œå¯é æ€§ã€‚ä½¿ç”¨ AI å¯ä»¥æ‰©å±•ç›¸å¯¹å¯¼èˆªè§£å†³æ–¹æ¡ˆåˆ°å¤šç§åœºæ™¯ï¼Œå¦‚ç›®æ ‡æˆ–ç…§æ˜æ¡ä»¶ï¼Œè€Œè¿™äº›åœºæ™¯ä¹‹å‰åªèƒ½é€šè¿‡ç»å…¸å›¾åƒå¤„ç†æ–¹æ³•æ¥æ‰‹åŠ¨è®¾è®¡ã€‚æœ¬æ–‡ä½¿ç”¨å¤šç§å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åå¤„ç†å™¨ï¼Œå¯¹äººå·¥ç”Ÿæˆçš„å docking æ¼”ç¤ºæ•°æ®è¿›è¡Œäº†æµ‹è¯•ï¼Œå®ç°äº† Position å’Œ Attitude ä¼°è®¡çš„å‡†ç¡®ç‡æ¥è¿‘ 1% èŒƒå›´å†…å’Œ 1 åº¦ã€‚æ­¤å¤–ï¼Œå°†è§£å†³æ–¹æ¡ˆä¸å®é™…å‚¨å­˜æœºåˆ¶çš„ç‰©ç† Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ‚Ğ¸Ğ¿ç»“åˆï¼Œåœ¨å®éªŒå®¤ä¸­ä½¿ç”¨ Ñ€Ğ¾Ğ±Ğ¾è‡‚æ¨¡æ‹Ÿå docking è¿‡ç¨‹è¿›è¡ŒéªŒè¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Attentive-VQ-VAE"><a href="#Attentive-VQ-VAE" class="headerlink" title="Attentive VQ-VAE"></a>Attentive VQ-VAE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11641">http://arxiv.org/abs/2309.11641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariano Rivera, Angello Hoyos</li>
<li>for: æé«˜ VQVAE æ¨¡å‹çš„èƒ½åŠ›ï¼Œä¿æŒå®ç”¨å‚æ•°æ°´å¹³</li>
<li>methods:  integrate Attentive Residual Encoder (AREN) å’Œ Residual Pixel Attention layerï¼Œä½¿ç”¨å¤šçº§ç¼–ç å™¨ï¼Œå¹¶é‡‡ç”¨å†…éƒ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æœ‰æ•ˆåœ°æ•æ‰å’Œåˆ©ç”¨ Contextual information</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œææ¡ˆçš„ä¿®æ”¹å¯ä»¥æ˜æ˜¾æé«˜æ•°æ®è¡¨ç¤ºå’Œç”Ÿæˆèƒ½åŠ›ï¼Œä½¿ VQVAEs æ›´é€‚åˆå„ç§åº”ç”¨ã€‚<details>
<summary>Abstract</summary>
We present a novel approach to enhance the capabilities of VQVAE models through the integration of an Attentive Residual Encoder (AREN) and a Residual Pixel Attention layer. The objective of our research is to improve the performance of VQVAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQVAEs even more suitable for a wide range of applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡ç»“åˆAttentive Residual Encoderï¼ˆARENï¼‰å’ŒResidual Pixel Attentionå±‚ï¼Œä»¥æé«˜VQVAEæ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç›®æ ‡æ˜¯æé«˜VQVAEè¡¨ç°ï¼ŒåŒæ—¶ä¿æŒå®é™…å‚æ•°æ°´å¹³ã€‚ARENç¼–ç å™¨è®¾è®¡å¯ä»¥åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šè¿è¡Œï¼Œé€‚åº”ä¸åŒçš„å»ºç­‘å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„å…³é”®åˆ›æ–°æ˜¯å°†Inter-pixelè‡ªåŠ¨æ³¨æ„æœºåˆ¶integrated into ARENç¼–ç å™¨ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ•ˆç‡åœ°æ•æ‰å¹¶åˆ©ç”¨ latent vectorä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜ä½¿ç”¨äº†å¤šä¸ªç¼–ç å±‚ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾åŠ›ã€‚æˆ‘ä»¬çš„æ³¨æ„å±‚é‡‡ç”¨äº†æœ€å°å‚æ•°çš„æ–¹æ³•ï¼Œç¡®ä¿latent vectoråªæœ‰å½“å…¶ä»–åƒç´ ä¸­æœ‰pertinent informationæ—¶æ‰ä¼šè¢«ä¿®æ”¹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ä¿®æ”¹å¯¼è‡´äº†æ•°æ®è¡¨ç¤ºå’Œç”Ÿæˆçš„æ˜¾è‘—æ”¹è¿›ï¼Œä½¿VQVAEsæ›´é€‚åˆå„ç§åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-the-semantics-of-sequential-patterns-with-negation"><a href="#A-survey-on-the-semantics-of-sequential-patterns-with-negation" class="headerlink" title="A survey on the semantics of sequential patterns with negation"></a>A survey on the semantics of sequential patterns with negation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11638">http://arxiv.org/abs/2309.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Guyet</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æ¢è®¨ç”¨æˆ·å¯¹é€»è¾‘ä¸åŒçš„æ—¶é—´åºåˆ—æ¨¡å¼å…·æœ‰ä½•ç§ç›´è§‚æ€§ï¼Ÿ</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä»½é—®å·æ¥æ¢è®¨ç”¨æˆ·å¯¹ä¸åŒ semantics çš„ç›´è§‚æ€§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ç”¨æˆ·å¯¹ä¸¤ç§ semantics å…·æœ‰ç›´è§‚æ€§ï¼Œä½†è¿™ä¸¤ç§ semantics å¹¶ä¸ä¸ç°æœ‰çš„ä¸»æµç®—æ³• semantics ä¸€è‡´ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€äº›å»ºè®®ï¼Œä»¥ä¾¿æ›´å¥½åœ°è€ƒè™‘è¿™äº›å·®å¼‚ã€‚<details>
<summary>Abstract</summary>
A sequential pattern with negation, or negative sequential pattern, takes the form of a sequential pattern for which the negation symbol may be used in front of some of the pattern's itemsets. Intuitively, such a pattern occurs in a sequence if negated itemsets are absent in the sequence. Recent work has shown that different semantics can be attributed to these pattern forms, and that state-of-the-art algorithms do not extract the same sets of patterns. This raises the important question of the interpretability of sequential pattern with negation. In this study, our focus is on exploring how potential users perceive negation in sequential patterns. Our aim is to determine whether specific semantics are more "intuitive" than others and whether these align with the semantics employed by one or more state-of-the-art algorithms. To achieve this, we designed a questionnaire to reveal the semantics' intuition of each user. This article presents both the design of the questionnaire and an in-depth analysis of the 124 responses obtained. The outcomes indicate that two of the semantics are predominantly intuitive; however, neither of them aligns with the semantics of the primary state-of-the-art algorithms. As a result, we provide recommendations to account for this disparity in the conclusions drawn.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸€ç§é¡ºåºæ¨¡å¼ WITH negationï¼Œæˆ–è´Ÿé¡ºåºæ¨¡å¼ï¼Œçš„å½¢å¼æ˜¯ä¸€ç§é¡ºåºæ¨¡å¼ï¼Œå…¶ä¸­å¯ä»¥åœ¨ä¸€äº›æ¨¡å¼itemsetå‰é¢ä½¿ç”¨å¦å®šç¬¦ã€‚Intuitivelyï¼Œè¿™ç§æ¨¡å¼åœ¨åºåˆ—ä¸­å‡ºç°ï¼Œå½“è´Ÿå¦å®šitemsetç¼ºå¤±åœ¨åºåˆ—ä¸­ã€‚ recent work has shown that different semantics can be attributed to these pattern forms, and that state-of-the-art algorithms do not extract the same sets of patterns. This raises the important question of the interpretability of sequential pattern with negation. In this study, our focus is on exploring how potential users perceive negation in sequential patterns. Our aim is to determine whether specific semantics are more "intuitive" than others and whether these align with the semantics employed by one or more state-of-the-art algorithms. To achieve this, we designed a questionnaire to reveal the semantics' intuition of each user. This article presents both the design of the questionnaire and an in-depth analysis of the 124 responses obtained. The outcomes indicate that two of the semantics are predominantly intuitive; however, neither of them aligns with the semantics of the primary state-of-the-art algorithms. As a result, we provide recommendations to account for this disparity in the conclusions drawn.Note: The word "WITH" in the original text is not translated as it is not a word in Simplified Chinese. Instead, the phrase "é¡ºåºæ¨¡å¼ WITH negation" is translated as "é¡ºåºæ¨¡å¼ WITH å¦å®š" (sequential pattern with negation).
</details></li>
</ul>
<hr>
<h2 id="Cloud-Based-Hierarchical-Imitation-Learning-for-Scalable-Transfer-of-Construction-Skills-from-Human-Workers-to-Assisting-Robots"><a href="#Cloud-Based-Hierarchical-Imitation-Learning-for-Scalable-Transfer-of-Construction-Skills-from-Human-Workers-to-Assisting-Robots" class="headerlink" title="Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots"></a>Cloud-Based Hierarchical Imitation Learning for Scalable Transfer of Construction Skills from Human Workers to Assisting Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11619">http://arxiv.org/abs/2309.11619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongrui Yu, Vineet R. Kamat, Carol C. Menassa</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨å°†å»ºç­‘å·¥ç¨‹ä¸­çš„é‡é‡å’Œphysically-demandingä»»åŠ¡äº¤ç»™æœºå™¨äººï¼Œä»¥é™ä½äººå·¥ä¼¤å®³ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨Imitation Learningï¼ˆILï¼‰æŠ€æœ¯å°†èŒäººçš„æ‰‹è‰ºæŠ€èƒ½è½¬ç§»åˆ°æœºå™¨äººèº«ä¸Šï¼Œä»¥æˆåŠŸå§”æ‰˜å»ºç­‘å·¥ç¨‹ä»»åŠ¡å’Œè·å¾—é«˜å“è´¨æœºå™¨äººåˆ¶æˆçš„æˆæœã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶æå‡ºäº†ä¸€ä¸ªå…·æœ‰å®éªŒå­¦ä¹ ï¼ˆHILï¼‰æ¨¡å‹å’Œäº‘ roboticsæŠ€æœ¯çš„è™šæ‹Ÿç¤ºèŒƒæ¡†æ¶ï¼Œå¯ä»¥å¸®åŠ©å°†èŒäººçš„æ‰‹è‰ºæŠ€èƒ½è½¬ç§»åˆ°æœºå™¨äººèº«ä¸Šï¼Œå¹¶ä¸”å¯ä»¥é‡å¤ä½¿ç”¨è¿™äº›ç¤ºèŒƒï¼Œä»¥å‡å°‘äººå·¥ç¤ºèŒƒçš„éœ€æ±‚ã€‚è¿™ä¸ªæ¡†æ¶å¯ä»¥å¸®åŠ©æé«˜å»ºç­‘å·¥ç¨‹ä¸­çš„é›‡å‘˜å¤šæ ·æ€§å’Œæ•™è‚²èƒŒæ™¯ã€‚<details>
<summary>Abstract</summary>
Assigning repetitive and physically-demanding construction tasks to robots can alleviate human workers's exposure to occupational injuries. Transferring necessary dexterous and adaptive artisanal construction craft skills from workers to robots is crucial for the successful delegation of construction tasks and achieving high-quality robot-constructed work. Predefined motion planning scripts tend to generate rigid and collision-prone robotic behaviors in unstructured construction site environments. In contrast, Imitation Learning (IL) offers a more robust and flexible skill transfer scheme. However, the majority of IL algorithms rely on human workers to repeatedly demonstrate task performance at full scale, which can be counterproductive and infeasible in the case of construction work. To address this concern, this paper proposes an immersive, cloud robotics-based virtual demonstration framework that serves two primary purposes. First, it digitalizes the demonstration process, eliminating the need for repetitive physical manipulation of heavy construction objects. Second, it employs a federated collection of reusable demonstrations that are transferable for similar tasks in the future and can thus reduce the requirement for repetitive illustration of tasks by human agents. Additionally, to enhance the trustworthiness, explainability, and ethical soundness of the robot training, this framework utilizes a Hierarchical Imitation Learning (HIL) model to decompose human manipulation skills into sequential and reactive sub-skills. These two layers of skills are represented by deep generative models, enabling adaptive control of robot actions. By delegating the physical strains of construction work to human-trained robots, this framework promotes the inclusion of workers with diverse physical capabilities and educational backgrounds within the construction industry.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å‘ç°ç»™å®šæ–‡æœ¬çš„ç®€åŒ–ä¸­æ–‡ç¿»è¯‘ã€‚<</SYS>>å§”æ‰˜ repetitive å’Œ physically-demanding çš„å»ºç­‘ä»»åŠ¡ç»™æœºå™¨äººï¼Œå¯ä»¥å‡è½»äººå·¥å·¥ä½œè€…çš„èŒä¸šå±å®³é£é™©ã€‚å°†å¿…è¦çš„çµæ´»å’Œé€‚åº”çš„è‰ºæœ¯å·¥è‰ºæŠ€èƒ½ä»å·¥ä½œè€…ä¼ é€’åˆ°æœºå™¨äººæ˜¯æˆåŠŸå§”æ‰˜å»ºç­‘ä»»åŠ¡å’Œè·å¾—é«˜è´¨é‡æœºå™¨äººæ„å»ºçš„å…³é”®ã€‚é¢„å®šçš„è¿åŠ¨è§„åˆ’è„šæœ¬é€šå¸¸åœ¨æ— ç»“æ„çš„å»ºç­‘ç°åœºç¯å¢ƒä¸­ç”ŸæˆåƒµåŒ–å’Œç¢°æ’çš„æœºå™¨äººè¡Œä¸ºã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå­¦ä¹ æ¨¡å¼ï¼ˆILï¼‰æä¾›äº†æ›´åŠ ç¨³å®šå’Œçµæ´»çš„æŠ€èƒ½ä¼ é€’æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œå¤§å¤šæ•° IL ç®—æ³•éœ€è¦äººå·¥å·¥ä½œè€…é‡å¤åœ°å±•ç¤ºä»»åŠ¡å®Œæˆï¼Œè¿™å¯èƒ½æ˜¯ä¸å¯èƒ½çš„å’Œä¸å¯é¢„æœŸçš„åœ¨å»ºç­‘å·¥ä½œä¸­ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ª immerse äº‘ robotics åŸºç¡€è®¾æ–½ï¼Œå®ƒæ‹¥æœ‰ä»¥ä¸‹ä¸¤ä¸ªä¸»è¦ç›®çš„ï¼šé¦–å…ˆï¼Œå®ƒå°†ç¤ºä¾‹è¿‡ç¨‹æ•°å­—åŒ–ï¼Œä»è€Œæ¶ˆé™¤é‡å¤åœ°Physical æ‰§è¡Œé‡æ„å»ºç­‘ç‰©å“çš„éœ€è¦ã€‚å…¶æ¬¡ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ª Federated é›†åˆçš„å¯é‡ç”¨ç¤ºä¾‹ï¼Œä»¥ä¾¿åœ¨æœªæ¥å¯¹ç±»ä¼¼ä»»åŠ¡è¿›è¡Œå¿«é€Ÿåè°ƒã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºæœºå™¨äººåŸ¹è®­çš„å¯é æ€§ã€å¯è§£é‡Šæ€§å’Œä¼¦ç†åˆç†æ€§ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨ Hierarchical Imitation Learningï¼ˆHILï¼‰æ¨¡å‹ï¼Œå°†äººç±»æŠ“å–æŠ€èƒ½ decomposed æˆSequential å’Œ reactive ä¸¤å±‚ã€‚è¿™ä¸¤å±‚æŠ€èƒ½è¢«è¡¨ç¤ºä¸ºæ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨æœºå™¨äººè¡Œä¸ºä¸­è¿›è¡Œé€‚åº”æ§åˆ¶ã€‚é€šè¿‡å§”æ‰˜å»ºç­‘å·¥ä½œç»™äººç±»åŸ¹è®­çš„æœºå™¨äººï¼Œè¿™ä¸ªæ¡†æ¶æ¨å¹¿äº†å»ºç­‘ä¸šä¸­ä¸åŒçš„èº«ä½“èƒ½åŠ›å’Œæ•™è‚²èƒŒæ™¯çš„äººå‘˜çš„åŒ…å®¹æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Hand-Gesture-Recognition-with-Two-Stage-Approach-Using-Transfer-Learning-and-Deep-Ensemble-Learning"><a href="#Hand-Gesture-Recognition-with-Two-Stage-Approach-Using-Transfer-Learning-and-Deep-Ensemble-Learning" class="headerlink" title="Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning"></a>Hand Gesture Recognition with Two Stage Approach Using Transfer Learning and Deep Ensemble Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11610">http://arxiv.org/abs/2309.11610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serkan SavaÅŸ, Atilla ErgÃ¼zen</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯å¯¹äººå·¥æ™ºèƒ½ä¸ Computing è¿›è¡Œæ”¹è¿›ï¼Œä»¥æé«˜å…¶æ€§èƒ½ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶åº”ç”¨äºè¯†åˆ«æ‰‹åŠ¿ã€‚</li>
<li>results: ç ”ç©¶è·å¾—äº†98.88%çš„å‡†ç¡®ç‡ï¼Œè¿™è¡¨æ˜äº†æ·±åº¦ensembleå­¦ä¹ æŠ€æœ¯åœ¨äººå·¥æ™ºèƒ½ä¸Computing ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚<details>
<summary>Abstract</summary>
Human-Computer Interaction (HCI) has been the subject of research for many years, and recent studies have focused on improving its performance through various techniques. In the past decade, deep learning studies have shown high performance in various research areas, leading researchers to explore their application to HCI. Convolutional neural networks can be used to recognize hand gestures from images using deep architectures. In this study, we evaluated pre-trained high-performance deep architectures on the HG14 dataset, which consists of 14 different hand gesture classes. Among 22 different models, versions of the VGGNet and MobileNet models attained the highest accuracy rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of 94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand gesture recognition on the dataset using an ensemble learning technique, which combined the four most successful models. By utilizing these models as base learners and applying the Dirichlet ensemble technique, we achieved an accuracy rate of 98.88%. These results demonstrate the effectiveness of the deep ensemble learning technique for HCI and its potential applications in areas such as augmented reality, virtual reality, and game technologies.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººæœºäº¤äº’ï¼ˆHCIï¼‰å·²ç»æ˜¯å¤šå¹´çš„ç ”ç©¶ä¸»é¢˜ï¼Œè€Œæœ€è¿‘çš„ç ”ç©¶å¼ºè°ƒæé«˜å…¶æ€§èƒ½é€šè¿‡ä¸åŒçš„æŠ€æœ¯ã€‚è¿‡å»åå¹´ï¼Œæ·±åº¦å­¦ä¹ ç ”ç©¶åœ¨å„ä¸ªé¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œå¯¼è‡´ç ”ç©¶è€…æƒ³è¦æŠŠå®ƒä»¬åº”ç”¨äºHCIã€‚é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œè¯†åˆ«æ‰‹åŠ¿å›¾åƒï¼Œå¯ä»¥ä½¿ç”¨æ·±åº¦å»ºç­‘ã€‚æœ¬ç ”ç©¶åœ¨HG14æ•°æ®é›†ä¸Šè¯„ä¼°äº†22ç§ä¸åŒçš„æ¨¡å‹ï¼Œå…¶ä¸­åŒ…æ‹¬VGGNetå’ŒMobileNetæ¨¡å‹çš„å¤šç§ç‰ˆæœ¬ã€‚ç»“æœå‘ç°ï¼ŒVGG16å’ŒVGG19æ¨¡å‹çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º94.64%å’Œ94.36%ï¼Œè€ŒMobileNetå’ŒMobileNetV2æ¨¡å‹çš„å‡†ç¡®ç‡åˆ†åˆ«ä¸º96.79%å’Œ94.43%ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ensembleå­¦ä¹ æŠ€æœ¯ï¼Œå°†è¿™äº›æ¨¡å‹ä½œä¸ºåŸºç¡€å­¦ä¹ å™¨ï¼Œå¹¶åº”ç”¨Dirichlet ensembleæŠ€æœ¯ï¼Œè¾¾åˆ°äº†98.88%çš„å‡†ç¡®ç‡ã€‚è¿™äº›ç»“æœè¡¨æ˜æ·±åº¦ensembleå­¦ä¹ æŠ€æœ¯åœ¨HCIä¸­çš„æ•ˆivenessï¼Œå¹¶åœ¨è™šæ‹Ÿç°å®ã€æ‰©å±•ç°å®å’Œæ¸¸æˆæŠ€æœ¯ç­‰é¢†åŸŸæœ‰æ½œåŠ›åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Dataset-Factory-A-Toolchain-For-Generative-Computer-Vision-Datasets"><a href="#Dataset-Factory-A-Toolchain-For-Generative-Computer-Vision-Datasets" class="headerlink" title="Dataset Factory: A Toolchain For Generative Computer Vision Datasets"></a>Dataset Factory: A Toolchain For Generative Computer Vision Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11608">http://arxiv.org/abs/2309.11608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kharitonov, Ryan Turner</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨è§£å†³ç”ŸæˆAIå·¥ä½œæµç¨‹ä¸­æ•°æ®å¤„ç†é—®é¢˜ï¼Œæé«˜æ•°æ®å¤„ç†æ•ˆç‡å’Œå¯é‡ç”¨æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§â€œæ•°æ®å·¥å‚â€æ–¹æ³•ï¼Œå°†æ ·æœ¬å­˜å‚¨å’Œå¤„ç†åˆ†ç¦»äºå…ƒæ•°æ®ï¼Œå¹¶å…è®¸æ•°æ®é©±åŠ¨æ“ä½œè¿›è¡Œæ‰¹å¤„ç†ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨â€œæ•°æ®å·¥å‚â€æ–¹æ³•å¯ä»¥æé«˜ç”ŸæˆAIå·¥ä½œæµç¨‹çš„æ•°æ®å¤„ç†æ•ˆç‡å’Œå¯é‡ç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
Generative AI workflows heavily rely on data-centric tasks - such as filtering samples by annotation fields, vector distances, or scores produced by custom classifiers. At the same time, computer vision datasets are quickly approaching petabyte volumes, rendering data wrangling difficult. In addition, the iterative nature of data preparation necessitates robust dataset sharing and versioning mechanisms, both of which are hard to implement ad-hoc. To solve these challenges, we propose a "dataset factory" approach that separates the storage and processing of samples from metadata and enables data-centric operations at scale for machine learning teams and individual researchers.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="CATS-Conditional-Adversarial-Trajectory-Synthesis-for-Privacy-Preserving-Trajectory-Data-Publication-Using-Deep-Learning-Approaches"><a href="#CATS-Conditional-Adversarial-Trajectory-Synthesis-for-Privacy-Preserving-Trajectory-Data-Publication-Using-Deep-Learning-Approaches" class="headerlink" title="CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches"></a>CATS: Conditional Adversarial Trajectory Synthesis for Privacy-Preserving Trajectory Data Publication Using Deep Learning Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11587">http://arxiv.org/abs/2309.11587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geods/cats">https://github.com/geods/cats</a></li>
<li>paper_authors: Jinmeng Rao, Song Gao, Sijia Zhu</li>
<li>for: æœ¬ç ”ç©¶ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ä¿æŠ¤äººå‘˜æµåŠ¨æ•°æ®éšç§ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„äººå‘˜æµåŠ¨æ•°æ®ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨K-anonymityä¿è¯äººå‘˜æµåŠ¨æ•°æ®çš„åˆ†å¸ƒæ°´å¹³éšç§ï¼Œå¹¶ä½¿ç”¨æ¡ä»¶å¯¹æŠ—è®­ç»ƒã€äººæµç¯å¢ƒå­¦ä¹ å’Œç›¸é‚»è½¨è¿¹ç‚¹åŒ¹é…æ¥é‡å»ºè½¨è¿¹ topologyã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éšç§ä¿æŠ¤ã€ç©ºé—´æ—¶é—´ç‰¹å¾ä¿æŒå’Œä¸‹æ¸¸å®ç”¨æ€§æ–¹é¢æ¯”åŸºçº¿æ–¹æ³•è¡¨ç°æ›´å¥½ï¼Œä¸ºäººæµåŠ¨æ•°æ®éšç§ç ”ç©¶Usingç”ŸæˆAIæŠ€æœ¯å’Œæ•°æ®ä¼¦ç†é—®é¢˜æä¾›æ–°çš„è§†è§’ã€‚<details>
<summary>Abstract</summary>
The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-level synthetic trajectory data, which can serve as supplements or alternatives to raw data for privacy-preserving trajectory data publication. The experiment results on over 90k GPS trajectories show that our method has a better performance in privacy preservation, spatiotemporal characteristic preservation, and downstream utility compared with baseline methods, which brings new insights into privacy-preserving human mobility research using generative AI techniques and explores data ethics issues in GIScience.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œç°ä»£ç¤¾ä¼šä¸­æ™®éå­˜åœ¨ ubique ä½ç½®æ„è¯†è®¾å¤‡å’Œç§»åŠ¨äº’è”ç½‘ï¼Œæˆ‘ä»¬å¯ä»¥ä»ç”¨æˆ·æ”¶é›†å·¨å¤§çš„ä¸ªäººåŒ–è½¨è¿¹æ•°æ®é›†ã€‚è¿™äº›è½¨è¿¹å¤§æ•°æ®ä¸ºäººç±»æ´»åŠ¨ç ”ç©¶å¸¦æ¥äº†æ–°çš„æœºä¼šï¼Œä½†ä¹Ÿå¼•èµ·äº†äººä»¬å…³äºä½ç½®éšç§çš„æ‹…å¿§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº† Conditional Adversarial Trajectory Synthesisï¼ˆCATSï¼‰ï¼Œä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„GeoAIæ–¹æ³•æ¡†æ¶ï¼Œç”¨äºéšç§ä¿æŠ¤çš„è½¨è¿¹æ•°æ®ç”Ÿæˆå’Œå‘å¸ƒã€‚CATSé€šè¿‡å¯¹äººç±»æ´»åŠ¨çš„ä¸‹å±‚ç©ºé—´æ—¶é—´åˆ†å¸ƒè¿›è¡ŒK-anonimityå¤„ç†ï¼Œæä¾›äº†å¼ºçš„éšç§ä¿è¯ã€‚é€šè¿‡ä½¿ç”¨å—æ¡ä»¶ adversarial è®­ç»ƒçš„äººç±»æ´»åŠ¨çŸ©é˜µï¼Œæ²¿ç€é‚»è¿‘è½¨è¿¹ç‚¹çš„å¾ªç¯åŒå‘å›¾åŒ¹é…ï¼Œä»¥åŠä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œè½¨è¿¹å…¨çƒä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒCATSå¯ä»¥ä»å—æ¡ä»¶é‡‡æ ·çš„ä½ç½®ä¸­é‡å»ºè½¨è¿¹æ‹“æ‰‘ï¼Œå¹¶ç”Ÿæˆé«˜è´¨é‡çš„ä¸ªäººåŒ– sintetic è½¨è¿¹æ•°æ®ï¼Œå¯ä»¥ä½œä¸ºéšç§ä¿æŠ¤ä¸‹çš„è½¨è¿¹æ•°æ®å‘å¸ƒçš„è¡¥å……æˆ–æ›¿ä»£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨éšç§ä¿æŠ¤ã€ç©ºé—´æ—¶é—´ç‰¹å¾ä¿æŒå’Œä¸‹æ¸¸å®ç”¨æ€§æ–¹é¢è¡¨ç°æ›´å¥½äºåŸºelineæ–¹æ³•ï¼Œè¿™å¸¦æ¥äº†æ–°çš„æ€è·¯ Ğ´Ğ»Ñéšç§ä¿æŠ¤çš„äººç±»æ´»åŠ¨ç ”ç©¶ï¼Œå¹¶æ¢è®¨äº†GIScienceä¸­çš„æ•°æ®ä¼¦ç†é—®é¢˜ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Distilling-Adversarial-Prompts-from-Safety-Benchmarks-Report-for-the-Adversarial-Nibbler-Challenge"><a href="#Distilling-Adversarial-Prompts-from-Safety-Benchmarks-Report-for-the-Adversarial-Nibbler-Challenge" class="headerlink" title="Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge"></a>Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11575">http://arxiv.org/abs/2309.11575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manuel Brack, Patrick Schramowski, Kristian Kersting</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æŒ‘æˆ˜ç°æœ‰çš„ç”Ÿæˆå›¾åƒæ¨¡å‹å®‰å…¨æ€§é—®é¢˜ï¼Œé€šè¿‡åˆ†ææ½œåœ¨çš„æ”»å‡»è¾“å…¥æ¥æ£€æµ‹æ¨¡å‹çš„è„†å¼±æ€§ã€‚</li>
<li>methods: ç ”ç©¶äººå‘˜ä½¿ç”¨äº†ç°æœ‰çš„å®‰å…¨å‡†åˆ™æ¥ç”Ÿæˆå¤§é‡çš„æ”»å‡»è¾“å…¥ï¼Œå¹¶å¯¹è¿™äº›è¾“å…¥å’Œç›¸åº”çš„å›¾åƒè¿›è¡Œåˆ†æï¼Œä»¥æ¢è®¨å½“å‰ç”Ÿæˆå›¾åƒæ¨¡å‹ä¸­çš„å®‰å…¨é—®é¢˜ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜å‘ç°äº†è®¸å¤šæ½œåœ¨çš„å®‰å…¨é—®é¢˜ï¼ŒåŒ…æ‹¬è¾“å…¥ç­›é€‰å™¨çš„è„†å¼±æ€§å’Œç³»ç»Ÿæ€§çš„å®‰å…¨é—®é¢˜ï¼Œè¿™äº›é—®é¢˜å¯èƒ½ä¼šå½±å“ç”Ÿæˆå›¾åƒæ¨¡å‹çš„å®‰å…¨æ€§ã€‚<details>
<summary>Abstract</summary>
Text-conditioned image generation models have recently achieved astonishing image quality and alignment results. Consequently, they are employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the web, they also produce unsafe content. As a contribution to the Adversarial Nibbler challenge, we distill a large set of over 1,000 potential adversarial inputs from existing safety benchmarks. Our analysis of the gathered prompts and corresponding images demonstrates the fragility of input filters and provides further insights into systematic safety issues in current generative image models.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="BTLM-3B-8K-7B-Parameter-Performance-in-a-3B-Parameter-Model"><a href="#BTLM-3B-8K-7B-Parameter-Performance-in-a-3B-Parameter-Model" class="headerlink" title="BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model"></a>BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11568">http://arxiv.org/abs/2309.11568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nolan Dey, Daria Soboleva, Faisal Al-Khateeb, Bowen Yang, Ribhu Pathria, Hemant Khachane, Shaheer Muhammad, Zhiming, Chen, Robert Myers, Jacob Robert Steeves, Natalia Vassilieva, Marvin Tom, Joel Hestness</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯ä»‹ç»ä¸€ç§æ–°çš„è¯­è¨€æ¨¡å‹ï¼Œå³BTLM-3B-8Kï¼Œè¯¥æ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å…·æœ‰è¾ƒå°çš„å‚æ•°å¤§å°å’Œè®¡ç®—èµ„æºå ç”¨ã€‚</li>
<li>methods: è¯¥æ¨¡å‹ä½¿ç”¨äº†ä¸€äº›ç°æœ‰çš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬ALiBiä½åµŒå…¥å’ŒSwiGLUéçº¿æ€§å‡½æ•°ï¼Œå¹¶ä¸”é€šè¿‡ä¼˜åŒ–Hyperparameterå’Œå­¦ä¹ ç¯å¢ƒæ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>results: ç›¸æ¯”å…¶ä»–3Bå‚æ•°æ¨¡å‹ï¼ŒBTLM-3B-8Kåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡º2-5.5%çš„æå‡ï¼Œè€Œä¸”åœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºä¼˜ç§€çš„è¡¨ç°ï¼Œæ¯”å¦‚MPT-7B-8Kå’ŒXGen-7B-8Kã€‚æ­¤å¤–ï¼ŒBTLM-3B-8Kçš„è®¡ç®—èµ„æºå ç”¨ç›¸å¯¹è¾ƒå°‘ï¼Œåªéœ€3GBçš„å†…å­˜å’Œ2.5å€çš„è®¡ç®—èµ„æºã€‚<details>
<summary>Abstract</summary>
We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.   On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: https://huggingface.co/cerebras/btlm-3b-8k-base.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»â€œBTLM-3B-8Kâ€è¯­è¨€æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªæ–°çš„å·é™…ä¹‹å† å¼€æºè¯­è¨€æ¨¡å‹ï¼Œæ‹¥æœ‰30äº¿ä¸ªå‚æ•°ã€‚BTLM-3B-8Kåœ¨627äº¿ä¸ªTokençš„SlimPajamaæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä½¿ç”¨2048å’Œ8192çš„ä¸Šä¸‹æ–‡é•¿åº¦æ··åˆè®­ç»ƒã€‚ç›¸æ¯”äºç°æœ‰çš„30äº¿ä¸ªå‚æ•°æ¨¡å‹ï¼ŒBTLM-3B-8Kåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°å‡º2-5.5%çš„æå‡ã€‚æ­¤å¤–ï¼ŒBTLM-3B-8Kåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ¯”MPT-7B-8Kå’ŒXGen-7B-8Kæ›´é«˜ã€‚æˆ‘ä»¬åœ¨ç²¾ç®€å’Œåˆ é™¤äº†SlimPajamaæ•°æ®é›†ä¸Šè®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸¥æ ¼åœ°è°ƒæ•´äº†Î¼På‚æ•°å’Œæ—¶é—´è¡¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº†ALiBiä½åµŒå…¥å’ŒSwiGLUéçº¿æ€§ã€‚åœ¨Hugging Faceä¸Šï¼Œæœ€å—æ¬¢è¿çš„æ¨¡å‹éƒ½æœ‰70äº¿ä¸ªå‚æ•°ï¼Œè¿™è¡¨æ˜ç”¨æˆ·å¯¹70äº¿ä¸ªå‚æ•°æ¨¡å‹çš„è´¨é‡-å¤§å°æ¯”ä¾‹æ„Ÿå…´è¶£ã€‚å°†70äº¿ä¸ªå‚æ•°æ¨¡å‹ç¼©å‡åˆ°30äº¿ä¸ªå‚æ•°ï¼Œå‡ ä¹æ²¡æœ‰å½±å“æ€§èƒ½ï¼Œæ˜¯ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ã€‚BTLM-3B-8Kåªéœ€è¦3GBçš„å†…å­˜å’Œ4ä½å‡†ç¡®ï¼Œåœ¨æµ‹è¯•è¿‡ç¨‹ä¸­è€—ç”¨2.5å€çš„è®¡ç®—èµ„æºï¼Œå¸®åŠ©å¼€è¾Ÿäº†ä¸€ä¸ªå…·æœ‰å¼ºå¤§è¯­è¨€æ¨¡å‹çš„é—¨æ§›ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šè¿è¡Œã€‚BTLM-3B-8Kåœ¨Hugging Faceä¸Šå¯ä»¥å…è´¹ä¸‹è½½ï¼šhttps://huggingface.co/cerebras/btlm-3b-8k-baseã€‚
</details></li>
</ul>
<hr>
<h2 id="Limitations-in-odour-recognition-and-generalisation-in-a-neuromorphic-olfactory-circuit"><a href="#Limitations-in-odour-recognition-and-generalisation-in-a-neuromorphic-olfactory-circuit" class="headerlink" title="Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit"></a>Limitations in odour recognition and generalisation in a neuromorphic olfactory circuit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11555">http://arxiv.org/abs/2309.11555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nik Dennler, AndrÃ© van Schaik, Michael Schmuker</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨ç ”ç©¶ä¸€ç§åŸºäºç¥ç»omorphic computingçš„èŠ³é¦™å­¦ä¹ ç®—æ³•ï¼Œä»¥åŠè¯¥ç®—æ³•åœ¨è¯†åˆ«æ°”ä½“èŠ³é¦™çš„èƒ½åŠ›ã€‚</li>
<li>methods: è¯¥ç®—æ³•ä½¿ç”¨äº†ä¸€ç§åŸºäºèŠ³é¦™ç»†èƒç½‘ç»œçš„ç¥ç»omorphicæ¶æ„ï¼Œå¹¶ä½¿ç”¨äº†ä¸€äº›ç¡¬ä»¶åŠ é€ŸæŠ€æœ¯æ¥åŠ é€Ÿè®¡ç®—ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè¯¥ç®—æ³•åœ¨è¯†åˆ«ä¸åŒæ°”ä½“èŠ³é¦™çš„èƒ½åŠ›è¾ƒå¼ºï¼Œä½†æ˜¯åœ¨é‡å¤ presentaion çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰é™ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°äº†ä¸€äº›é™åˆ¶ï¼Œå¯¼è‡´éƒ¨åˆ†ç»“è®ºéœ€è¦è¿›ä¸€æ­¥éªŒè¯ã€‚<details>
<summary>Abstract</summary>
Neuromorphic computing is one of the few current approaches that have the potential to significantly reduce power consumption in Machine Learning and Artificial Intelligence. Imam & Cleland presented an odour-learning algorithm that runs on a neuromorphic architecture and is inspired by circuits described in the mammalian olfactory bulb. They assess the algorithm's performance in "rapid online learning and identification" of gaseous odorants and odorless gases (short "gases") using a set of gas sensor recordings of different odour presentations and corrupting them by impulse noise. We replicated parts of the study and discovered limitations that affect some of the conclusions drawn. First, the dataset used suffers from sensor drift and a non-randomised measurement protocol, rendering it of limited use for odour identification benchmarks. Second, we found that the model is restricted in its ability to generalise over repeated presentations of the same gas. We demonstrate that the task the study refers to can be solved with a simple hash table approach, matching or exceeding the reported results in accuracy and runtime. Therefore, a validation of the model that goes beyond restoring a learned data sample remains to be shown, in particular its suitability to odour identification tasks.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Chain-of-Verification-Reduces-Hallucination-in-Large-Language-Models"><a href="#Chain-of-Verification-Reduces-Hallucination-in-Large-Language-Models" class="headerlink" title="Chain-of-Verification Reduces Hallucination in Large Language Models"></a>Chain-of-Verification Reduces Hallucination in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11495">http://arxiv.org/abs/2309.11495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason Weston</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å‡ä¿¡æ¯ç”Ÿæˆé—®é¢˜ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†é“¾å¼éªŒè¯ï¼ˆCoVeï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é¦–å…ˆï¼ˆiï¼‰ç”Ÿæˆåˆå§‹å›ç­”ï¼Œç„¶åï¼ˆiiï¼‰è§„åˆ’éªŒè¯é—®é¢˜ï¼Œï¼ˆiiiï¼‰ç‹¬ç«‹å›ç­”éªŒè¯é—®é¢˜ï¼Œå¹¶ï¼ˆivï¼‰ç”Ÿæˆæœ€ç»ˆéªŒè¯åçš„å›ç­”ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨å„ç§ä»»åŠ¡ä¸Šï¼ˆå¦‚Wikidataåˆ—è¡¨é—®é¢˜ã€å…³é—­ä¹¦MultiSpanQAå’Œé•¿æ–‡æœ¬ç”Ÿæˆï¼‰å®éªŒè¡¨æ˜ï¼ŒCoVeå¯ä»¥å‡å°‘å‡ä¿¡æ¯çš„å‘ç”Ÿã€‚<details>
<summary>Abstract</summary>
Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»æƒ³ï¼ˆhallucinationï¼‰é—®é¢˜ä»æœªå¾—åˆ°è§£å†³ã€‚æˆ‘ä»¬ç ”ç©¶è¯­è¨€æ¨¡å‹æ˜¯å¦å¯ä»¥å¯¹å…¶å›ç­”è¿›è¡Œæ£€æŸ¥å’Œæ›´æ­£ã€‚æˆ‘ä»¬å¼€å‘äº†é“¾å¼éªŒè¯ï¼ˆChain-of-Verificationï¼ŒCoVeï¼‰æ–¹æ³•ï¼Œå®ƒåŒ…æ‹¬ä»¥ä¸‹å››ä¸ªæ­¥éª¤ï¼š1. æ¨¡å‹é¦–å…ˆæå‡ºä¸€ä¸ªåˆæ­¥ç­”æ¡ˆï¼ˆdraftï¼‰ï¼›2. ç„¶åï¼Œæ¨¡å‹è®¡åˆ’ä¸€ç³»åˆ—çš„éªŒè¯é—®é¢˜ï¼Œä»¥éªŒè¯å…¶åˆæ­¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼›3. æ¨¡å‹ç‹¬ç«‹åœ°å›ç­”è¿™äº›éªŒè¯é—®é¢˜ï¼Œä»¥é¿å…å—å…¶ä»–ç­”æ¡ˆçš„å½±å“ï¼›4. æœ€åï¼Œæ¨¡å‹ç”Ÿæˆä¸€ä¸ªç»éªŒéªŒè¯çš„ç­”æ¡ˆã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°CoVeå¯ä»¥åœ¨å¤šç§ä»»åŠ¡ä¸Šå‡å°‘å¹»æƒ³ï¼ŒåŒ…æ‹¬åŸºäºWikidataçš„åˆ—è¡¨é—®é¢˜ã€å…³é—­ä¹¦MultiSpanQAå’Œé•¿æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Text2Reward-Automated-Dense-Reward-Function-Generation-for-Reinforcement-Learning"><a href="#Text2Reward-Automated-Dense-Reward-Function-Generation-for-Reinforcement-Learning" class="headerlink" title="Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning"></a>Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11489">http://arxiv.org/abs/2309.11489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, Tao Yu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ç§æ•°æ®è‡ªç”±çš„æ¸¸æˆå¥–åŠ±å‡½æ•°ç”Ÿæˆæ¡†æ¶ï¼Œå¸®åŠ©è§£å†³ç°æœ‰RLä¸­ç‰¹æ®ŠçŸ¥è¯†æˆ–åŸŸæ•°æ®çš„éœ€æ±‚ï¼Œä»è€Œé™ä½å¼€å‘æˆæœ¬ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è‡ªåŠ¨ç”Ÿæˆ dense reward functionsï¼Œå¹¶å°†å¥–åŠ±å‡½æ•°è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„ç¨‹åºï¼Œä»¥æ»¡è¶³ä¸åŒä»»åŠ¡çš„éœ€æ±‚ã€‚</li>
<li>results: åœ¨ä¸¤ä¸ªæœºå™¨äººæ“ä½œbenchmarkï¼ˆManiSkill2ã€MetaWorldï¼‰å’Œä¸¤ä¸ªmujocoçš„ locomotiveç¯å¢ƒä¸­ï¼Œä½¿ç”¨ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°è®©ç­–ç•¥å–å¾—äº†13é¡¹17ä¸ªä»»åŠ¡çš„æˆåŠŸç‡å’Œé€Ÿåº¦ä¸ä¸“å®¶å†™çš„å¥–åŠ±å‡½æ•°ç›¸å½“æˆ–æ›´é«˜ï¼Œå¹¶ä¸”åœ¨å…­ä¸ªæ–°çš„ locomotiveè¡Œä¸ºä¸­å–å¾—äº†94%ä»¥ä¸Šçš„æˆåŠŸç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®é™…ä¸–ç•Œä¸­éƒ¨ç½²çš„ç­–ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡äººå·¥åé¦ˆæ¥è¿›ä¸€æ­¥æ”¹è¿›ç­–ç•¥çš„å¥–åŠ±å‡½æ•°ã€‚è§†é¢‘ç»“æœå¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://text-to-reward.github.ioæŸ¥çœ‹./">https://text-to-reward.github.ioæŸ¥çœ‹ã€‚</a><details>
<summary>Abstract</summary>
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io
</details>
<details>
<summary>æ‘˜è¦</summary>
ï¿½ï¿½utes2ï¿½ï¿½wardæ˜¯ä¸€ç§æŠ½è±¡çš„æ¸¸æˆæœºåˆ¶ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå¡«å……çš„å¥–åŠ±å‡½æ•°ï¼Œä¸éœ€è¦ç‰¹æ®Šçš„çŸ¥è¯†æˆ–åŸŸæ•°æ®ï¼Œä»è€Œé™ä½å¼€å‘æˆæœ¬ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥Text2Rewardï¼Œä¸€ç§æ•°æ®è‡ªç”±æ¡†æ¶ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå¡«å……çš„å¥–åŠ±å‡½æ•°ï¼ŒåŸºäºå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚ç»™å‡ºä¸€ä¸ªç”¨è‡ªç„¶è¯­è¨€æè¿°çš„ç›®æ ‡ï¼ŒText2Rewardå¯ä»¥ç”Ÿæˆå¡«å……çš„å¥–åŠ±å‡½æ•°ï¼Œä½œä¸ºå¯æ‰§è¡Œçš„ç¨‹åºï¼Œå¹¶å°†å…¶ä¸ç¯å¢ƒçš„å‡å°‘è¡¨ç¤ºç›¸å…³è”ã€‚ä¸åŒäºåå‘RLå’Œæœ€è¿‘çš„å·¥ä½œï¼Œä½¿ç”¨LLMå†™ç¨€ç–å¥–åŠ±ä»£ç ï¼ŒText2Rewardç”Ÿæˆçš„å¥–åŠ±ä»£ç å¯è¯»æ€§å¥½ï¼Œå¯ä»¥è¦†ç›–å„ç§ä»»åŠ¡ï¼Œä½¿ç”¨ç°æœ‰åŒ…ï¼Œå¹¶å…è®¸è¿­ä»£åé¦ˆã€‚æˆ‘ä»¬åœ¨ManiSkill2å’ŒMetaWorldä¸¤ä¸ªæœºå™¨äºº manipulate æµ‹è¯•ç¯å¢ƒä¸­è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥åŠMuJoCoä¸¤ä¸ªæ¶‚æŠ¹ç¯å¢ƒã€‚åœ¨13ä¸ªæœºå™¨äºº manipulate ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨ç”Ÿæˆçš„å¥–åŠ±å‡½æ•°è®­ç»ƒçš„ç­–ç•¥çš„ä»»åŠ¡æˆåŠŸç‡å’Œé€Ÿåº¦ä¸ä¸“å®¶å†™çš„å¥–åŠ±å‡½æ•°ç›¸å½“æˆ–æ›´å¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å­¦ä¹ äº†6ç§æ–°çš„è¡Œèµ°è¡Œä¸ºï¼Œå…¶æˆåŠŸç‡è¶…è¿‡94%ã€‚æœ€åï¼Œæˆ‘ä»¬è¡¨æ˜ä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®é™…ä¸–ç•Œä¸­è®­ç»ƒçš„ç­–ç•¥å¯ä»¥åœ¨çœŸå®ä¸–ç•Œä¸­éƒ¨ç½²ã€‚ Text2Reward è¿˜å¯ä»¥é€šè¿‡äººç±»åé¦ˆæ¥è¿›ä¸€æ­¥æ”¹è¿›ç­–ç•¥çš„å¥–åŠ±å‡½æ•°ã€‚è§†é¢‘ç»“æœå¯ä»¥åœ¨<https://text-to-reward.github.io> æŸ¥çœ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Fictional-Worlds-Real-Connections-Developing-Community-Storytelling-Social-Chatbots-through-LLMs"><a href="#Fictional-Worlds-Real-Connections-Developing-Community-Storytelling-Social-Chatbots-through-LLMs" class="headerlink" title="Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs"></a>Fictional Worlds, Real Connections: Developing Community Storytelling Social Chatbots through LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11478">http://arxiv.org/abs/2309.11478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Sun, Hanyi Wang, Pok Man Chan, Morteza Tabibi, Yan Zhang, Huan Lu, Yuheng Chen, Chang Hee Lee, Ali Asadipour</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘ç¤¾åŒºä¸­å¯ä»¥ä¸äººäº’åŠ¨çš„ç¤¾äº¤è™šæ‹ŸåŠ©æ‰‹ï¼ˆSCï¼‰ï¼Œé€šè¿‡æ•…äº‹çš„ç”¨é€”æ¥å¢å¼ºç¤¾äº¤äº’åŠ¨çš„å¯ä¿¡åº¦å’Œè¶£å‘³æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†è¯­è¨€æ¨¡å‹GPT-3é©±åŠ¨çš„æ•…äº‹ç¤¾äº¤è™šæ‹ŸåŠ©æ‰‹(â€œDavidâ€å’Œâ€Catherineâ€)ï¼Œå¹¶åœ¨åœ¨çº¿æ¸¸æˆç¤¾åŒºâ€DE (Alias)â€ä¸Šçš„Discordè¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>results: è¯¥ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ•…äº‹çš„ä½¿ç”¨å¯ä»¥å¢å¼ºç¤¾äº¤è™šæ‹ŸåŠ©æ‰‹åœ¨ç¤¾åŒº settingä¸­çš„å‚ä¸åº¦å’Œå¯ä¿¡åº¦ã€‚<details>
<summary>Abstract</summary>
We address the integration of storytelling and Large Language Models (LLMs) to develop engaging and believable Social Chatbots (SCs) in community settings. Motivated by the potential of fictional characters to enhance social interactions, we introduce Storytelling Social Chatbots (SSCs) and the concept of story engineering to transform fictional game characters into "live" social entities within player communities. Our story engineering process includes three steps: (1) Character and story creation, defining the SC's personality and worldview, (2) Presenting Live Stories to the Community, allowing the SC to recount challenges and seek suggestions, and (3) Communication with community members, enabling interaction between the SC and users. We employed the LLM GPT-3 to drive our SSC prototypes, "David" and "Catherine," and evaluated their performance in an online gaming community, "DE (Alias)," on Discord. Our mixed-method analysis, based on questionnaires (N=15) and interviews (N=8) with community members, reveals that storytelling significantly enhances the engagement and believability of SCs in community settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶å°†æ•…äº‹ä¸å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼Œä»¥å¼€å‘åœ¨ç¤¾åŒºä¸­å¼•äººå…¥æ¥å’Œ credible çš„ç¤¾äº¤èŠå¤©æœºå™¨äººï¼ˆSCï¼‰ã€‚æˆ‘ä»¬è¢«å¯å‘äº†è™šæ„äººç‰©å¯ä»¥å¢å¼ºç¤¾äº¤äº’åŠ¨çš„æ½œåŠ›ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥äº† Storytelling Social Chatbotsï¼ˆSSCsï¼‰å’Œæ•…äº‹å·¥ç¨‹æŠ€æœ¯ï¼Œå°†è™šæ„æ¸¸æˆè§’è‰²è½¬åŒ–ä¸ºç¤¾åŒºä¸­çš„ "live" ç¤¾äº¤å®ä½“ã€‚æˆ‘ä»¬çš„æ•…äº‹å·¥ç¨‹è¿‡ç¨‹åŒ…æ‹¬ä¸‰ä¸ªæ­¥éª¤ï¼šï¼ˆ1ï¼‰äººç‰©å’Œæ•…äº‹åˆ›ä½œï¼Œå®šä¹‰ SC çš„ä¸ªæ€§å’Œè§‚ç‚¹ï¼Œï¼ˆ2ï¼‰å‘ç¤¾åŒºæˆå‘˜å±•ç¤ºLive Storyï¼Œè®© SC æè¿°æŒ‘æˆ˜å’Œå¯»æ±‚å»ºè®®ï¼Œï¼ˆ3ï¼‰ä¸ç¤¾åŒºæˆå‘˜äº¤æµï¼Œå…è®¸ SC ä¸ç”¨æˆ·äº’åŠ¨ã€‚æˆ‘ä»¬ä½¿ç”¨ GPT-3 LLM é©±åŠ¨æˆ‘ä»¬çš„ SSC åŸå‹ "David" å’Œ "Catherine"ï¼Œå¹¶åœ¨ Discord ä¸Šçš„åœ¨çº¿æ¸¸æˆç¤¾åŒº "DE (Alias)" è¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ··åˆæ–¹æ³•åˆ†æï¼ŒåŸºäºé—®å· (N=15) å’Œé‡‡è®¿ (N=8) çš„ç¤¾åŒºæˆå‘˜ï¼Œè¡¨æ˜æ•…äº‹åœ¨ç¤¾åŒºè®¾ç½®ä¸­å¯ä»¥æ˜¾è‘—æé«˜ SC çš„å‚ä¸åº¦å’Œå¸å¼•åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Fuzzy-Representation-Learning-with-Rules-based-Model"><a href="#Multi-view-Fuzzy-Representation-Learning-with-Rules-based-Model" class="headerlink" title="Multi-view Fuzzy Representation Learning with Rules based Model"></a>Multi-view Fuzzy Representation Learning with Rules based Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11473">http://arxiv.org/abs/2309.11473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Zhaohong Deng, Te Zhang, Kup-Sze Choi, Shitong Wang</li>
<li>for: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†å›¾å«ä¹‰å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè§£å†³å¤šè§†å›¾æ•°æ®æŒ–æ˜ä¸­çš„ä¸€äº›å…³é”®æŒ‘æˆ˜ã€‚</li>
<li>methods: æœ¬æ–¹æ³•åŸºäºå¯è§£é‡Šçš„ Takagi-Sugeno-Kang (TSK) æ‚åŒ–ç³»ç»Ÿï¼Œé€šè¿‡ä¸¤ä¸ªæ–¹é¢å®ç°å¤šè§†å›¾è¡¨ç¤ºå­¦ä¹ ã€‚é¦–å…ˆï¼Œå°†å¤šè§†å›¾æ•°æ®è½¬æ¢ä¸ºé«˜ç»´æ‚åŒ–ç‰¹å¾ç©ºé—´ï¼ŒåŒæ—¶åŒæ—¶æŒ–æ˜å…±åŒè§†å›¾ä¿¡æ¯å’Œæ¯ä¸ªè§†å›¾ç‰¹æœ‰ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæå‡ºäº†åŸºäº L_(2,1) è¯„ä¼°æ–¹æ³•çš„æ–°è§„èŒƒæ–¹æ³•ï¼Œä»¥æŒ–æ˜è§†å›¾ä¹‹é—´çš„ä¸€è‡´ä¿¡æ¯ï¼Œå¹¶ä¿æŒæ•°æ®çš„å‡ ä½•ç»“æ„ã€‚</li>
<li>results: å¯¹å¤šä¸ªæ ‡å‡†å¤šè§†å›¾æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒ validate the superiority of the proposed methodã€‚<details>
<summary>Abstract</summary>
Unsupervised multi-view representation learning has been extensively studied for mining multi-view data. However, some critical challenges remain. On the one hand, the existing methods cannot explore multi-view data comprehensively since they usually learn a common representation between views, given that multi-view data contains both the common information between views and the specific information within each view. On the other hand, to mine the nonlinear relationship between data, kernel or neural network methods are commonly used for multi-view representation learning. However, these methods are lacking in interpretability. To this end, this paper proposes a new multi-view fuzzy representation learning method based on the interpretable Takagi-Sugeno-Kang (TSK) fuzzy system (MVRL_FS). The method realizes multi-view representation learning from two aspects. First, multi-view data are transformed into a high-dimensional fuzzy feature space, while the common information between views and specific information of each view are explored simultaneously. Second, a new regularization method based on L_(2,1)-norm regression is proposed to mine the consistency information between views, while the geometric structure of the data is preserved through the Laplacian graph. Finally, extensive experiments on many benchmark multi-view datasets are conducted to validate the superiority of the proposed method.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šè§†è§’è¡¨ç¤ºå­¦ä¹ å·²ç»å¹¿æ³›ç ”ç©¶äº†å¤šè§†è§’æ•°æ®çš„æŒ–æ˜ã€‚ç„¶è€Œï¼Œæœ‰äº›å…³é”®æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚ä¸€æ–¹é¢ï¼Œç°æœ‰çš„æ–¹æ³•ä¸èƒ½å…¨é¢æ¢ç´¢å¤šè§†è§’æ•°æ®ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸å­¦ä¹ å¤šè§†è§’æ•°æ®ä¸­çš„å…±åŒä¿¡æ¯ï¼Œè€Œä¸æ˜¯æ¯ä¸ªè§†è§’ä¸­çš„ç‰¹å®šä¿¡æ¯ã€‚å¦ä¸€æ–¹é¢ï¼Œç”¨äºæŒ–æ˜éçº¿æ€§å…³ç³»çš„å†…æ ¸æˆ–ç¥ç»ç½‘ç»œæ–¹æ³•é€šå¸¸ç¼ºä¹å¯è§£é‡Šæ€§ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¤šè§†è§’æ‚åŒ–è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼ŒåŸºäºå¯è§£é‡Šçš„ Takagi-Sugeno-Kangï¼ˆTSKï¼‰æ‚åŒ–ç³»ç»Ÿï¼ˆMVRL_FSï¼‰ã€‚è¯¥æ–¹æ³•åœ¨ä¸¤ä¸ªæ–¹é¢å®ç°å¤šè§†è§’è¡¨ç¤ºå­¦ä¹ ã€‚é¦–å…ˆï¼Œå¤šè§†è§’æ•°æ®è¢«è½¬æ¢æˆä¸€ä¸ªé«˜ç»´æ‚åŒ–ç‰¹å¾ç©ºé—´ï¼ŒåŒæ—¶æ¢ç´¢å¤šè§†è§’æ•°æ®ä¸­çš„å…±åŒä¿¡æ¯å’Œæ¯ä¸ªè§†è§’ä¸­çš„ç‰¹å®šä¿¡æ¯ã€‚å…¶æ¬¡ï¼ŒåŸºäºL_(2,1)-normå›å½’çš„æ–°è§„åˆ™æ–¹æ³•è¢«æå‡ºï¼Œä»¥æŒ–æ˜è§†è§’ä¹‹é—´çš„ä¸€è‡´ä¿¡æ¯ï¼Œä¿ç•™æ•°æ®çš„å‡ ä½•ç»“æ„é€šè¿‡æ‹‰æ™®æ‹‰æ–¯å›¾ã€‚æœ€åï¼Œå¯¹è®¸å¤šæ ‡å‡†å¤šè§†è§’æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥éªŒè¯æè®®æ–¹æ³•çš„è¶…è¶Šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multi-Label-Takagi-Sugeno-Kang-Fuzzy-System"><a href="#Multi-Label-Takagi-Sugeno-Kang-Fuzzy-System" class="headerlink" title="Multi-Label Takagi-Sugeno-Kang Fuzzy System"></a>Multi-Label Takagi-Sugeno-Kang Fuzzy System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11469">http://arxiv.org/abs/2309.11469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiongdan Lou, Zhaohong Deng, Zhiyong Xiao, Kup-Sze Choi, Shitong Wang</li>
<li>for: æé«˜å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½</li>
<li>methods: åŸºäºå¤šæ ‡ç­¾ç›¸å…³å­¦ä¹ å’Œå¤šæ ‡ç­¾å›å½’æŸå¤±çš„å¤šæ ‡ç­¾æœæ°è¾›è¯ºå¹²å¼ç³»ç»Ÿï¼ˆML-TSK FSï¼‰</li>
<li>results: å¯¹12ä¸ªå¤šæ ‡ç­¾æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œç»“æœè¡¨æ˜ML-TSK FSä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨å„ç§è¯„ä»·æŒ‡æ ‡ä¸­è¡¨ç°ç«äº‰åŠ›å¼ºï¼Œè¡¨æ˜å®ƒå¯ä»¥æœ‰æ•ˆåœ°é€šè¿‡è¾›è¯ºå¹²å¼è§„åˆ™æ¨¡å‹ç‰¹æ€§å’Œç‰¹å¾æ ‡ç­¾å…³ç³»ï¼Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Multi-label classification can effectively identify the relevant labels of an instance from a given set of labels. However,the modeling of the relationship between the features and the labels is critical to the classification performance. To this end, we propose a new multi-label classification method, called Multi-Label Takagi-Sugeno-Kang Fuzzy System (ML-TSK FS), to improve the classification performance. The structure of ML-TSK FS is designed using fuzzy rules to model the relationship between features and labels. The fuzzy system is trained by integrating fuzzy inference based multi-label correlation learning with multi-label regression loss. The proposed ML-TSK FS is evaluated experimentally on 12 benchmark multi-label datasets. 1 The results show that the performance of ML-TSK FS is competitive with existing methods in terms of various evaluation metrics, indicating that it is able to model the feature-label relationship effectively using fuzzy inference rules and enhances the classification performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šæ ‡ç­¾åˆ†ç±»å¯ä»¥æœ‰æ•ˆåœ°ä»ç»™å®šçš„æ ‡ç­¾é›†ä¸­ç¡®å®šå®ä¾‹çš„ç›¸å…³æ ‡ç­¾ã€‚ç„¶è€Œï¼Œæ¨¡å‹ç‰¹æ€§å’Œæ ‡ç­¾ä¹‹é—´çš„å…³ç³»æ˜¯å¤šæ ‡ç­¾åˆ†ç±»æ€§èƒ½çš„å…³é”®å› ç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å¤šæ ‡ç­¾åˆ†ç±»æ–¹æ³•ï¼Œå³å¤šæ ‡ç­¾å¤šSKæ»¡è¶³ç³»ç»Ÿï¼ˆML-TSK FSï¼‰ï¼Œä»¥æé«˜åˆ†ç±»æ€§èƒ½ã€‚ML-TSK FSçš„ç»“æ„é‡‡ç”¨è§„åˆ™æ¥æ¨¡å‹ç‰¹æ€§å’Œæ ‡ç­¾ä¹‹é—´çš„å…³ç³»ã€‚è¿™ä¸ªè§„åˆ™æ˜¯é€šè¿‡å¤šæ€æ¨ç†å’Œå¤šæ ‡ç­¾ç›¸äº’å…³ç³»å­¦ä¹ æ¥è®­ç»ƒçš„ã€‚æˆ‘ä»¬å¯¹12ä¸ªå¤šæ ‡ç­¾æ•°æ®é›†è¿›è¡Œå®éªŒè¯„ä¼°äº†ML-TSK FSçš„æ€§èƒ½ã€‚ç»“æœè¡¨æ˜ï¼ŒML-TSK FSä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨ä¸åŒçš„è¯„ä»·æŒ‡æ ‡ä¸Šå…·æœ‰ç«äº‰åŠ›ï¼Œè¿™è¡¨æ˜å®ƒå¯ä»¥é€šè¿‡å¤šæ€æ¨ç†è§„åˆ™æ¥æœ‰æ•ˆåœ°æ¨¡å‹ç‰¹æ€§å’Œæ ‡ç­¾ä¹‹é—´çš„å…³ç³»ï¼Œæé«˜åˆ†ç±»æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="AudioFool-Fast-Universal-and-synchronization-free-Cross-Domain-Attack-on-Speech-Recognition"><a href="#AudioFool-Fast-Universal-and-synchronization-free-Cross-Domain-Attack-on-Speech-Recognition" class="headerlink" title="AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition"></a>AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11462">http://arxiv.org/abs/2309.11462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamad Fakih, Rouwaida Kanj, Fadi Kurdahi, Mohammed E. Fouda</li>
<li>for: é˜²æ­¢è‡ªåŠ¨è¯è¯­è¯†åˆ«ç³»ç»Ÿå—åˆ°æ•Œå¯¹æ”»å‡»ï¼Œå¯¼è‡´ç³»ç»Ÿå´©æºƒæˆ–æŸåã€‚</li>
<li>methods: ä½¿ç”¨å¯¹æ•°é¢‘åŸŸä¿®æ”¹æ”»å‡»ï¼Œä»¥ç¡®ä¿æ”»å‡»å…·æœ‰ä¸åŒç‰¹æ€§ï¼Œä¾‹å¦‚ä¸å—åŒæ­¥è°ƒåˆ¶å½±å“å’ŒèŒƒå›´æ»¤æ³¢å½±å“ã€‚</li>
<li>results: é€è¿‡å®éªŒå’Œåˆ†æï¼Œå‘ç° modified frequency domain æ”»å‡»èƒ½å¤Ÿå®ç°è¿™äº›ç‰¹æ€§ï¼Œå¹¶ä¸”åœ¨çº¿ä¸Š keyword classification ä»»åŠ¡ä¸­æä¾›äº†é«˜æ•ˆçš„æ”»å‡»æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Automatic Speech Recognition systems have been shown to be vulnerable to adversarial attacks that manipulate the command executed on the device. Recent research has focused on exploring methods to create such attacks, however, some issues relating to Over-The-Air (OTA) attacks have not been properly addressed. In our work, we examine the needed properties of robust attacks compatible with the OTA model, and we design a method of generating attacks with arbitrary such desired properties, namely the invariance to synchronization, and the robustness to filtering: this allows a Denial-of-Service (DoS) attack against ASR systems. We achieve these characteristics by constructing attacks in a modified frequency domain through an inverse Fourier transform. We evaluate our method on standard keyword classification tasks and analyze it in OTA, and we analyze the properties of the cross-domain attacks to explain the efficiency of the approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨è¯è¯­è¯†åˆ«ç³»ç»Ÿå·²ç»è¢«è¯æ˜å®¹æ˜“å—åˆ°æ•Œæ„æ”»å‡»ï¼Œè¿™äº›æ”»å‡»å¯ä»¥æ§åˆ¶è®¾å¤‡ä¸Šæ‰§è¡Œçš„å‘½ä»¤ã€‚æœ€è¿‘çš„ç ”ç©¶ä¸»è¦å…³æ³¨äºæ¢ç´¢å¦‚ä½•åˆ›å»ºè¿™äº›æ”»å‡»ï¼Œä½†æ˜¯ä¸€äº›è¿‡ç©ºä¸­æ”»å‡»ï¼ˆOTAï¼‰é—®é¢˜å°šæœªå¾—åˆ°å……åˆ†è§£å†³ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†éœ€è¦çš„æŠ—æ€§æ”»å‡»çš„å±æ€§ï¼Œå¹¶è®¾è®¡äº†ç”Ÿæˆæ”»å‡»å…·æœ‰ä»»æ„æƒ³è¦çš„å±æ€§çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸å˜æ€§å’Œè¿‡æ»¤å™¨çš„Robustnessã€‚æˆ‘ä»¬é€šè¿‡å¯¹å°„Transformeræ¥å®ç°è¿™äº›ç‰¹æ€§ï¼Œå¹¶åœ¨æ ‡å‡†å…³é”®è¯åˆ†ç±»ä»»åŠ¡ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜åˆ†æäº†è·¨é¢‘åŸŸæ”»å‡»çš„æ€§è´¨ï¼Œä»¥è§£é‡Šæˆ‘ä»¬çš„æ–¹æ³•çš„é«˜æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Generative-Agent-Based-Modeling-Unveiling-Social-System-Dynamics-through-Coupling-Mechanistic-Models-with-Generative-Artificial-Intelligence"><a href="#Generative-Agent-Based-Modeling-Unveiling-Social-System-Dynamics-through-Coupling-Mechanistic-Models-with-Generative-Artificial-Intelligence" class="headerlink" title="Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence"></a>Generative Agent-Based Modeling: Unveiling Social System Dynamics through Coupling Mechanistic Models with Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11456">http://arxiv.org/abs/2309.11456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Navid Ghaffarzadegan, Aritra Majumdar, Ross Williams, Niyousha Hosseinichimeh</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä½¿ç”¨ç”Ÿæˆäººå·¥æ™ºèƒ½å»ºæ¨¡ç¤¾ä¼šç³»ç»Ÿçš„æ–°æœºé‡ã€‚</li>
<li>methods: è¿™äº›æ¨¡å‹ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å¦‚ChatGPTæ¥è¡¨ç¤ºäººç±»å†³ç­–è¡Œä¸ºåœ¨ç¤¾ä¼šè®¾ç½®ä¸‹ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡æä¾›äº†ä¸€ä¸ªç®€å•çš„ç¤¾ä¼šè§„èŒƒä¼ æ’­æ¨¡å‹ï¼Œå¹¶å¯¹å…¶ Results è¿›è¡Œäº†å¹¿æ³›çš„è°ƒæŸ¥å’Œæ•æ„Ÿæ€§åˆ†æã€‚<details>
<summary>Abstract</summary>
We discuss the emerging new opportunity for building feedback-rich computational models of social systems using generative artificial intelligence. Referred to as Generative Agent-Based Models (GABMs), such individual-level models utilize large language models such as ChatGPT to represent human decision-making in social settings. We provide a GABM case in which human behavior can be incorporated in simulation models by coupling a mechanistic model of human interactions with a pre-trained large language model. This is achieved by introducing a simple GABM of social norm diffusion in an organization. For educational purposes, the model is intentionally kept simple. We examine a wide range of scenarios and the sensitivity of the results to several changes in the prompt. We hope the article and the model serve as a guide for building useful diffusion models that include realistic human reasoning and decision-making.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è®¨è®ºæ–°å…´çš„æœºä¼šï¼šä½¿ç”¨ç”Ÿæˆäººå·¥æ™ºèƒ½å»ºæ„å…·æœ‰åé¦ˆä¸°å¯Œçš„ç¤¾äº¤ç³»ç»Ÿæ¨¡å‹ã€‚ç§°ä¸ºç”Ÿæˆä»£ç†æ¨¡å‹ï¼ˆGABMï¼‰ï¼Œè¿™äº›ä¸ªä½“çº§æ¨¡å‹åˆ©ç”¨å¤§é‡è¯­è¨€æ¨¡å‹å¦‚ChatGPTæ¥è¡¨ç¤ºäººç±»å†³ç­–åœ¨ç¤¾äº¤è®¾ç½®ä¸­ã€‚æˆ‘ä»¬æä¾›ä¸€ä¸ªGABMä¾‹å­ï¼Œå°†äººç±»è¡Œä¸ºintegratedåˆ°æ¨¡æ‹Ÿæ¨¡å‹ä¸­ï¼Œé€šè¿‡ä¸é¢„è®­å¤§é‡è¯­è¨€æ¨¡å‹ Coupling çš„æ–¹å¼ã€‚è¿™æ˜¯é€šè¿‡å°†ç¤¾äº¤normä¼ æ’­æ¨¡å‹ç®€åŒ–ä¸º Educational  purposes çš„æ–¹å¼ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¹¿æ³›çš„æƒ…å†µï¼Œå¹¶è¯„ä¼°äº†å˜é‡çš„æ•æ„Ÿåº¦ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡æ–‡ç« å’Œæ¨¡å‹å¯ä»¥serve as a guide  Ğ´Ğ»Ñå»ºç«‹åŒ…å«ç°å®äººç±»æ€ç»´å’Œå†³ç­–çš„ä¼ æ’­æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Using-deep-learning-to-construct-stochastic-local-search-SAT-solvers-with-performance-bounds"><a href="#Using-deep-learning-to-construct-stochastic-local-search-SAT-solvers-with-performance-bounds" class="headerlink" title="Using deep learning to construct stochastic local search SAT solvers with performance bounds"></a>Using deep learning to construct stochastic local search SAT solvers with performance bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11452">http://arxiv.org/abs/2309.11452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/porscheofficial/sls_sat_solving_with_deep_learning">https://github.com/porscheofficial/sls_sat_solving_with_deep_learning</a></li>
<li>paper_authors: Maximilian Kramer, Paul Boes</li>
<li>for: è¿™ paper æ˜¯å…³äº Boolean Satisfiability problem (SAT) çš„ç ”ç©¶ï¼Œå…·ä½“æ¥è¯´æ˜¯ä½¿ç”¨ Graph Neural Networks (GNN) è®­ç»ƒ oracleï¼Œä»¥æé«˜ Stochastic Local Search (SLS) ç®—æ³•çš„æ€§èƒ½ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº† GNN è®­ç»ƒ oracleï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸¤ç§ SLS ç®—æ³•ä¸Šï¼Œä»¥è§£å†³éšæœº SAT å®ä¾‹ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ä½¿ç”¨ GNN è®­ç»ƒ oracleï¼ŒSLS ç®—æ³•çš„æ€§èƒ½å¾—åˆ°äº†æ˜æ˜¾æé«˜ï¼Œå¯ä»¥è§£å†³æ›´éš¾çš„ SAT å®ä¾‹ï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ›´å°‘çš„æ­¥éª¤æ•°ä¸‹è§£å†³ã€‚<details>
<summary>Abstract</summary>
The Boolean Satisfiability problem (SAT) is the most prototypical NP-complete problem and of great practical relevance. One important class of solvers for this problem are stochastic local search (SLS) algorithms that iteratively and randomly update a candidate assignment. Recent breakthrough results in theoretical computer science have established sufficient conditions under which SLS solvers are guaranteed to efficiently solve a SAT instance, provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. Motivated by these results and the well established ability of neural networks to learn common structure in large datasets, in this work, we train oracles using Graph Neural Networks and evaluate them on two SLS solvers on random SAT instances of varying difficulty. We find that access to GNN-based oracles significantly boosts the performance of both solvers, allowing them, on average, to solve 17% more difficult instances (as measured by the ratio between clauses and variables), and to do so in 35% fewer steps, with improvements in the median number of steps of up to a factor of 8. As such, this work bridges formal results from theoretical computer science and practically motivated research on deep learning for constraint satisfaction problems and establishes the promise of purpose-trained SAT solvers with performance guarantees.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¸ƒå°”æ»¡æ„æ€§é—®é¢˜ï¼ˆSATï¼‰æ˜¯NPå®Œå¤‡é—®é¢˜çš„æœ€å…¸å‹ä¾‹å­ï¼Œå…·æœ‰å®é™…é‡è¦æ€§ã€‚ä¸€ç§é‡è¦çš„SATè§£å†³æ–¹æ³•æ˜¯éšæœºåœ°æ›´æ–°å€™é€‰åˆ†é…çš„æ‚åŒ–æœç´¢ç®—æ³•ï¼ˆSLSï¼‰ã€‚æœ€è¿‘çš„ç†è®ºè®¡ç®—æœºç§‘å­¦æˆæœè¡¨æ˜ï¼Œå¦‚æœSLSç®—æ³•æœ‰è®¿é—®é€‚åˆçš„"oracle"ï¼Œé‚£ä¹ˆå®ƒä»¬å¯ä»¥æœ‰æ•ˆåœ°è§£å†³SATå®ä¾‹ï¼Œ provided they have access to suitable "oracles" that provide samples from an instance-specific distribution, exploiting an instance's local structure. åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œè®­ç»ƒ oracleï¼Œå¹¶å¯¹ä¸¤ç§SLSè§£å†³æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œåœ¨éšæœºSATå®ä¾‹ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡è®¿é—®GNNåŸºäº oracleï¼Œå¯ä»¥å¤§å¹…æé«˜SLSè§£å†³æ–¹æ³•çš„æ€§èƒ½ï¼Œä½¿å…¶èƒ½å¤Ÿè§£å†³æ›´éš¾çš„å®ä¾‹ï¼ˆæŒ‰ç…§æ¡ä»¶æ•°å’Œå˜é‡çš„æ¯”ç‡æ¥åº¦é‡ï¼‰ï¼Œå¹¶ä¸”åœ¨æ›´å°‘çš„æ­¥éª¤å†…å®Œæˆï¼ˆæ¯”å¦‚ï¼Œåœ¨35% fewer stepsä¸­å®Œæˆï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨ median number of steps ä¸­ï¼ŒGNNåŸºäº oracle å¯ä»¥æé«˜ SLS è§£å†³æ–¹æ³•çš„æ€§èƒ½ï¼Œæœ€é«˜å¯ä»¥æé«˜8å€ã€‚å› æ­¤ï¼Œè¿™é¡¹ç ”ç©¶å°†ç†è®ºè®¡ç®—æœºç§‘å­¦çš„æˆæœä¸æ·±åº¦å­¦ä¹ çš„å®è·µç ”ç©¶ç›¸ç»“åˆï¼Œå¹¶è¯æ˜äº†ä¸“é—¨ä¸ºSATé—®é¢˜è®­ç»ƒçš„æ·±åº¦å­¦ä¹ ç®—æ³•å¯ä»¥æä¾›æ€§èƒ½ä¿è¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="You-Only-Look-at-Screens-Multimodal-Chain-of-Action-Agents"><a href="#You-Only-Look-at-Screens-Multimodal-Chain-of-Action-Agents" class="headerlink" title="You Only Look at Screens: Multimodal Chain-of-Action Agents"></a>You Only Look at Screens: Multimodal Chain-of-Action Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11436">http://arxiv.org/abs/2309.11436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cooelf/Auto-UI">https://github.com/cooelf/Auto-UI</a></li>
<li>paper_authors: Zhuosheng Zhang, Aston Zhang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æé«˜è‡ªåŠ¨åŒ–ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ä»£ç†çš„æ•ˆç‡ï¼Œä½¿å…¶å¯ä»¥åœ¨ä¸éœ€è¦äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹è‡ªåŠ¨å®Œæˆä»»åŠ¡ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€è§£å†³æ–¹æ¡ˆï¼Œå³ç›´æ¥ä¸ç•Œé¢äº¤äº’ï¼Œä¸éœ€è¦ç¯å¢ƒè§£ææˆ–åº”ç”¨ç¨‹åºç‰¹å®šçš„APIã€‚æ­¤å¤–ï¼Œæå‡ºäº†ä¸€ç§é“¾å¼åŠ¨ä½œæŠ€æœ¯ï¼Œé€šè¿‡è€ƒè™‘å…ˆå‰å’Œåç»­åŠ¨ä½œå†å²ï¼Œå¸®åŠ©ä»£ç†å†³å®šå“ªä¸ªåŠ¨ä½œæ‰§è¡Œã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAuto-UIåœ¨æ–°çš„è®¾å¤‡æ§åˆ¶benchmark AITWä¸Šè¾¾åˆ°äº†çŠ¶æ€ç çš„æ€§èƒ½ï¼Œå…·æœ‰åŠ¨ä½œç±»å‹é¢„æµ‹ç²¾åº¦90%å’Œæ€»æˆåŠŸç‡74%ã€‚ä»£ç å…¬å¼€å¯ç”¨äº<a target="_blank" rel="noopener" href="https://github.com/cooelf/Auto-UI%E3%80%82">https://github.com/cooelf/Auto-UIã€‚</a><details>
<summary>Abstract</summary>
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30K unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. Code is publicly available at https://github.com/cooelf/Auto-UI.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨åŒ–ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ä»£ç†ï¼Œç›®çš„æ˜¯è‡ªåŠ¨åŒ–ä»»åŠ¡ï¼Œä¸éœ€è¦äººå·¥å¹²é¢„ã€‚æœ€è¿‘çš„ç ”ç©¶å·²ç»åˆ©ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥å®ç°å¤šç§ç¯å¢ƒä¸­çš„æœ‰æ•ˆäº¤äº’ã€‚ä¸ºäº†ä¸è¾“å…¥å’Œè¾“å‡ºå¯¹åº”çš„LLMçš„éœ€æ±‚ï¼Œç°æœ‰çš„æ–¹æ³•é‡‡ç”¨æ²™ç›’ç¯å¢ƒï¼Œé€šè¿‡å¤–éƒ¨å·¥å…·å’Œåº”ç”¨ç¨‹åºç‰¹å®šçš„APIæ¥è§£æç¯å¢ƒå¹¶è§£é‡Šé¢„æµ‹çš„åŠ¨ä½œã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç»å¸¸ä¼šé‡åˆ°æ¨ç†ä¸å‡†ç¡®å’Œé”™è¯¯ä¼ é€’é£é™©ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†Auto-UIï¼Œä¸€ç§å¤šæ¨¡å¼è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥ç›´æ¥ä¸ç•Œé¢äº¤äº’ï¼Œæ— éœ€è§£æç¯å¢ƒæˆ–ä¾èµ–äºåº”ç”¨ç¨‹åºç‰¹å®šçš„APIã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†é“¾æ¡åŠ¨ä½œæŠ€æœ¯ï¼Œåˆ©ç”¨å‰ä¸€ç³»åˆ—çš„å†å²åŠ¨ä½œå’Œæœªæ¥åŠ¨ä½œè®¡åˆ’ï¼Œå¸®åŠ©ä»£ç†å†³å®šæ‰§è¡Œå“ªä¸€ä¸ªåŠ¨ä½œã€‚æˆ‘ä»¬åœ¨æ–°çš„è®¾å¤‡æ§åˆ¶æ ‡å‡†AITWä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶å–å¾—äº†state-of-the-artè¡¨ç°ï¼Œå…·ä½“å¦‚ä¸‹ï¼š* åŠ¨ä½œç±»å‹é¢„æµ‹ç²¾åº¦è¾¾90%* æ€»ä½“åŠ¨ä½œæˆåŠŸç‡è¾¾74%ä»£ç å¯ä»¥åœ¨https://github.com/cooelf/Auto-UIä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Review-of-Few-Shot-Learning-in-Medical-Imaging"><a href="#A-Systematic-Review-of-Few-Shot-Learning-in-Medical-Imaging" class="headerlink" title="A Systematic Review of Few-Shot Learning in Medical Imaging"></a>A Systematic Review of Few-Shot Learning in Medical Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11433">http://arxiv.org/abs/2309.11433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eva Pachetti, Sara Colantonio</li>
<li>for: è¿™ç¯‡æ–‡ç« æ—¨åœ¨ç»™å‡ºåŒ»å­¦å½±åƒåˆ†æé¢†åŸŸä¸­å‡ ä½•å­¦å­¦ä¹ çš„ç³»ç»Ÿè¯„è®ºï¼Œå°¤å…¶æ˜¯åœ¨å‡ ä½•å­¦å­¦ä¹ æ–¹æ³•ä¸­å®ç°å°‘æ•°æ‰©å±•å­¦ä¹ ã€‚</li>
<li>methods: è¿™ç¯‡æ–‡ç« ä½¿ç”¨äº†ç³»ç»Ÿæ€§çš„æ–‡çŒ®æœå¯»æ–¹æ³•ï¼Œä»2018å¹´åˆ°2023å¹´å‘è¡¨çš„80ç¯‡ç›¸å…³æ–‡ç« ä¸­é€‰æ‹©äº†ç›¸å…³çš„æ–‡çŒ®ã€‚æ–‡ç« å°†è¿™äº›æ–‡çŒ®åˆ†ä¸ºä¸åŒçš„åŒ»ç–—ç»“æœï¼ˆå¦‚è‚¿ç˜¤åˆ†ç±»ã€ç–¾ç—…åˆ†ç±»ã€å½±åƒè°ƒæ•´ç­‰ï¼‰ã€ investigateçš„ Ğ°Ğ½Ğ°Ñ‚Ğ¾Ğ¼Ñ–å­¦ç»“æ„ï¼ˆå¦‚å¿ƒè„ã€è‚ºç­‰ï¼‰ä»¥åŠä½¿ç”¨çš„å‡ ä½•å­¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>results: æ–‡ç« æ˜¾ç¤ºäº†å‡ ä½•å­¦å­¦ä¹ å¯ä»¥åœ¨å¤§å¤šæ•°çš„ç»“æœä¸­è¶…è¿‡æ•°æ®ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶ä¸”meta-learningæ˜¯å‡ ä½•å­¦å­¦ä¹ ä¸­æœ€å—æ¬¢è¿çš„æ–¹æ³•ï¼Œå¯ä»¥é€‚åº”æ–°ä»»åŠ¡çš„å‡ ä½•å­¦å­¦ä¹ ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜å‘ç°äº†åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­å‡ ä½•å­¦å­¦ä¹ ä¸­ä½¿ç”¨çš„ä¸»è¦æŠ€æœ¯æ˜¯supervised learningå’Œsemi-supervised learningï¼Œå¹¶ä¸”è¿™äº›æŠ€æœ¯åœ¨åŒ»ç–—å½±åƒåˆ†æä¸­è¡¨ç°æœ€ä½³ã€‚æœ€åï¼Œæ–‡ç« å‘ç°äº†ä¸»è¦åº”ç”¨é¢†åŸŸä¸»è¦æ˜¯å¿ƒè„ã€è‚ºå’Œè…¹éƒ¨é¢†åŸŸã€‚<details>
<summary>Abstract</summary>
The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis, especially with meta-learning. This systematic review gives a comprehensive overview of few-shot learning in medical imaging. We searched the literature systematically and selected 80 relevant articles published from 2018 to 2023. We clustered the articles based on medical outcomes, such as tumour segmentation, disease classification, and image registration; anatomical structure investigated (i.e. heart, lung, etc.); and the meta-learning method used. For each cluster, we examined the papers' distributions and the results provided by the state-of-the-art. In addition, we identified a generic pipeline shared among all the studies. The review shows that few-shot learning can overcome data scarcity in most outcomes and that meta-learning is a popular choice to perform few-shot learning because it can adapt to new tasks with few labelled samples. In addition, following meta-learning, supervised learning and semi-supervised learning stand out as the predominant techniques employed to tackle few-shot learning challenges in medical imaging and also best performing. Lastly, we observed that the primary application areas predominantly encompass cardiac, pulmonary, and abdominal domains. This systematic review aims to inspire further research to improve medical image analysis and patient care.
</details>
<details>
<summary>æ‘˜è¦</summary>
å› ä¸ºåŒ»ç–—å½±åƒæ ‡ç­¾çš„ç¼ºä¹ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½å—åˆ°é™åˆ¶ã€‚ä¸è¿‡ï¼Œå‡ ä¸ªshotå­¦ä¹ æŠ€æœ¯å¯ä»¥è§£å†³æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œæé«˜åŒ»ç–—å½±åƒåˆ†æï¼Œç‰¹åˆ«æ˜¯åœ¨meta-learningä¸­ã€‚è¿™ä¸ªç³»ç»Ÿæ€§å®¡æŸ¥ç»™å‡ºäº†åŒ»ç–—å½±åƒä¸­å‡ ä¸ªshotå­¦ä¹ çš„å…¨é¢å›é¡¾ã€‚æˆ‘ä»¬åœ¨2018å¹´è‡³2023å¹´å‘å¸ƒçš„80ç¯‡ç›¸å…³æ–‡çŒ®ä¸­è¿›è¡Œäº†ç³»ç»Ÿæ€§æœå¯»ï¼Œå¹¶æ ¹æ®åŒ»ç–—ç»“æœï¼ˆä¾‹å¦‚è‚¿ç˜¤åˆ†ç±»ã€ç—…ç†åˆ†ç±»ã€å½±åƒè°ƒæ•´ï¼‰ã€ investigateä½“éƒ¨ï¼ˆä¾‹å¦‚å¿ƒè„ã€è‚ºéƒ¨ç­‰ï¼‰å’Œä½¿ç”¨çš„meta-learningæ–¹æ³•è¿›è¡Œåˆ†ç»„ã€‚å¯¹æ¯ä¸ªåˆ†ç»„ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ–‡çŒ®çš„åˆ†å¸ƒå’Œé¡¶å°–çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†æ‰€æœ‰ç ”ç©¶ä¸­çš„é€šç”¨æ¶æ„ã€‚å®¡æŸ¥ç»“æœè¡¨æ˜ï¼Œå‡ ä¸ªshotå­¦ä¹ å¯ä»¥åœ¨å¤§å¤šæ•°ç»“æœä¸­çªç ´æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œmeta-learningæ˜¯æœ€å—æ¬¢è¿çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¯ä»¥é€‚åº”æ–°ä»»åŠ¡ WITH FEW labelled samplesã€‚æ­¤å¤–ï¼Œåœ¨åŒ»ç–—å½±åƒä¸­ï¼Œä»¥supervised learningå’Œsemi-supervised learningä¸ºä¸»çš„æŠ€æœ¯è¢«å¤§é‡è¿ç”¨ï¼Œå¹¶ä¸”è¡¨ç°æœ€ä½³ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ä¸»è¦åº”ç”¨é¢†åŸŸä¸»è¦æ˜¯å¿ƒè„ã€è‚ºéƒ¨å’Œè…¹éƒ¨é¢†åŸŸã€‚è¿™ä¸ªç³»ç»Ÿæ€§å®¡æŸ¥çš„ç›®çš„æ˜¯é¼“åŠ±è¿›ä¸€æ­¥çš„ç ”ç©¶ï¼Œä»¥æé«˜åŒ»ç–—å½±åƒåˆ†æå’Œpatient careã€‚
</details></li>
</ul>
<hr>
<h2 id="Generative-Pre-Training-of-Time-Series-Data-for-Unsupervised-Fault-Detection-in-Semiconductor-Manufacturing"><a href="#Generative-Pre-Training-of-Time-Series-Data-for-Unsupervised-Fault-Detection-in-Semiconductor-Manufacturing" class="headerlink" title="Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing"></a>Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11427">http://arxiv.org/abs/2309.11427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sewoong Lee, JinKyou Choi, Min Su Kim</li>
<li>For: è¿™ä¸ªç ”ç©¶æ—¨åœ¨è¿ç”¨æ—¶é—´åºåˆ—æ•°æ®çš„ç‰¹å¾æ¥æ¢æµ‹åŠå¯¼ä½“åˆ¶é€ ä¸­çš„å¼‚å¸¸ç°è±¡ã€‚* Methods: ç ”ç©¶ä½¿ç”¨æ—¶é—´åºåˆ—åµŒå…¥å’Œç”Ÿæˆé¢„è®­Transformersæ¥é¢„è®­æ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶ä½¿ç”¨æ ‡ entropyæŸå¤±å‡½æ•°æ¥åˆ†ç±»å¼‚å¸¸æ—¶é—´åºåˆ—å’Œæ­£å¸¸æ—¶é—´åºåˆ—ã€‚* Results: ç ”ç©¶è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨UCSDæ—¶é—´åºåˆ—åˆ†ç±»æ•°æ®é›†å’ŒåŒ–å­¦è’¸å‘æˆé•¿ï¼ˆCVDï¼‰è®¾å¤‡çš„å¤„ç†è®°å½•ä¸Šéƒ½æ˜¾ç¤ºå‡ºæ›´å¥½çš„è¡¨ç°ï¼Œä¸è¿‡å»çš„æ— supervisionæ¨¡å‹ç›¸æ¯”ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨EERä¸Šçš„F1åˆ†æ•°æœ€é«˜ï¼Œå¹¶ä¸”ä»…ä»…0.026ä¸‹äºæ— supervisionåŸºå‡†ã€‚<details>
<summary>Abstract</summary>
This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model has the highest F1 score at Equal Error Rate (EER) across all datasets and is only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="EDMP-Ensemble-of-costs-guided-Diffusion-for-Motion-Planning"><a href="#EDMP-Ensemble-of-costs-guided-Diffusion-for-Motion-Planning" class="headerlink" title="EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning"></a>EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11414">http://arxiv.org/abs/2309.11414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kallol Saha, Vishal Mandadi, Jayaram Reddy, Ajit Srikanth, Aditya Agarwal, Bipasha Sen, Arun Singh, Madhava Krishna</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§ combining classical å’Œ deep learning çš„åŠ¨ä½œè§„åˆ’æ–¹æ³•ï¼Œä»¥æé«˜åŠ¨ä½œè§„åˆ’çš„æˆåŠŸç‡å’Œæ™®éæ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§å«åš Ensemble-of-costs-guided Diffusion for Motion Planningï¼ˆEDMPï¼‰çš„æ–¹æ³•ï¼Œå®ƒ combinines ç»å…¸çš„åŠ¨ä½œè§„åˆ’ç®—æ³•å’Œæ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œä»¥æé«˜åŠ¨ä½œè§„åˆ’çš„æˆåŠŸç‡å’Œæ™®éæ€§ã€‚EDMP ä½¿ç”¨äº†ä¸€ä¸ª diffusion-based networkï¼Œè®­ç»ƒåœ¨ä¸€ç»„å¤šå…ƒå¯è¡Œçš„åŠ¨ä½œè½¨è¿¹ä¸Šã€‚åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬Compute scene-specific costsï¼Œå¦‚ â€œç¢°æ’æˆæœ¬â€ï¼Œä»¥å¯¼å¼• diffusion ç”Ÿæˆç¬¦åˆåœºæ™¯å†…çš„ç¢°æ’æ¡ä»¶çš„æœ‰æ•ˆè½¨è¿¹ã€‚</li>
<li>results: æœ¬æ–‡çš„ç»“æœæ˜¾ç¤ºï¼ŒEDMP èƒ½å¤Ÿä¸ State-of-the-Art çš„æ·±åº¦å­¦ä¹ åŸºäºæ–¹æ³•ç›¸æ¯”ï¼ŒæˆåŠŸç‡æœ‰æ‰€æé«˜ï¼Œå¹¶ä¸”ä¿ç•™äº†ç»å…¸æ­¥éª¤çš„æ™®éæ€§ã€‚<details>
<summary>Abstract</summary>
Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific costs such as "collision cost" and guide the diffusion to generate valid trajectories that satisfy the scene-specific constraints. Further, instead of a single cost function that may be insufficient in capturing diversity across scenes, we use an ensemble of costs to guide the diffusion process, significantly improving the success rate compared to classical planners. EDMP performs comparably with SOTA deep-learning-based methods while retaining the generalization capabilities primarily associated with classical planners.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç»å…¸è¿åŠ¨è§„åˆ’ Ğ´Ğ»Ñæœºå™¨äººæ“ä½œåŒ…æ‹¬ä¸€ç»„é€šç”¨ç®—æ³•ï¼Œæ—¨åœ¨æœ€å°åŒ–Sceneç‰¹å®šçš„æ‰§è¡Œè®¡åˆ’çš„æˆæœ¬ã€‚è¿™ç§æ–¹æ³•å…·æœ‰å¾ˆå¥½çš„é€‚åº”æ€§ï¼Œå¯ä»¥ç›´æ¥åœ¨æ–°åœºæ™¯ä¸Šä½¿ç”¨ï¼Œä¸éœ€è¦ç‰¹å®šçš„è®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œä¸çŸ¥é“å¤šå…ƒæœ‰æ•ˆè½¨è¿¹çš„ç‰¹ç‚¹å’Œåœºæ™¯ç‰¹å®šçš„æˆæœ¬å‡½æ•°ï¼Œå…¨å±€çš„è§£å†³æ–¹æ¡ˆé€šå¸¸å…·æœ‰ä½æˆåŠŸç‡ã€‚æ·±åº¦å­¦ä¹ åŸºäºç®—æ³•åœ¨æˆåŠŸç‡ä¸Šæä¾›äº†å¾ˆå¤§çš„æ”¹å–„ï¼Œä½†æ˜¯å®ƒä»¬æ›´éš¾äºé‡‡ç”¨ï¼Œéœ€è¦ç‰¹å®šçš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬æå‡ºäº†EDMPï¼Œä¸€ç§ensemble-of-costs-guided Diffusion for Motion Planningï¼Œæ—¨åœ¨ç»“åˆç»å…¸å’Œæ·±åº¦å­¦ä¹ åŸºäºçš„è¿åŠ¨è§„åˆ’ã€‚æˆ‘ä»¬çš„æ‰©æ•£ç½‘ç»œè¢«è®­ç»ƒåœ¨ä¸€ç»„å¤šå…ƒå¯è¡Œçš„è½¨è¿¹ä¸Šã€‚åœ¨ä»»ä½•æ–°åœºæ™¯çš„æ¨ç†æ—¶ï¼Œæˆ‘ä»¬è®¡ç®—åœºæ™¯ç‰¹å®šçš„ç¢°æ’æˆæœ¬å’Œå¯¼å¼•æ‰©æ•£æ¥ç”Ÿæˆç¬¦åˆåœºæ™¯ç‰¹å®šçš„çº¦æŸçš„æœ‰æ•ˆè½¨è¿¹ã€‚æ­¤å¤–ï¼Œè€Œä¸æ˜¯å•ä¸€çš„æˆæœ¬å‡½æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªensemble of costsæ¥å¼•å¯¼æ‰©æ•£è¿‡ç¨‹ï¼Œæ˜æ˜¾æé«˜æˆåŠŸç‡ç›¸æ¯”ç»å…¸è§„åˆ’å™¨ã€‚EDMPå’ŒSOTAæ·±åº¦å­¦ä¹ åŸºäºæ–¹æ³•ç›¸æ¯”ï¼Œä¿ç•™äº†ç»å…¸è§„åˆ’å™¨çš„æ€»ä½“åŒ–èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Long-Form-End-to-End-Speech-Translation-via-Latent-Alignment-Segmentation"><a href="#Long-Form-End-to-End-Speech-Translation-via-Latent-Alignment-Segmentation" class="headerlink" title="Long-Form End-to-End Speech Translation via Latent Alignment Segmentation"></a>Long-Form End-to-End Speech Translation via Latent Alignment Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11384">http://arxiv.org/abs/2309.11384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter PolÃ¡k, OndÅ™ej Bojar</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§å®æ—¶åŒå£°ç¿»è¯‘æ–¹æ³•ï¼Œå¯ä»¥å¤„ç†é•¿äºå‡ ç§’é’Ÿçš„è¯­éŸ³æ•°æ®ã€‚</li>
<li>methods: è¿™ç§æ–¹æ³•ä½¿ç”¨ç°æœ‰çš„åŒå£°ç¿»è¯‘encoder-decoderæ¶æ„ï¼Œå¹¶ä½¿ç”¨ST CTCè¿›è¡Œåˆ† segmentationã€‚è¿™ä¸ªæ–¹æ³•ä¸éœ€è¦é¢å¤–çš„ç›‘ç£æˆ–å‚æ•°ï¼Œå¯ä»¥åœ¨å®æ—¶ä¸­è¿›è¡ŒåŒå£°ç¿»è¯‘å’Œåˆ† segmentationã€‚</li>
<li>results: åœ¨å¤šç§è¯­è¨€å¯¹å’Œå†…å¤–é¢†åŸŸæ•°æ®ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ Ğ´Ğ¾ÑÑ‚Ğ¸å¾—çŠ¶æ€çš„åŒå£°ç¿»è¯‘è´¨é‡ï¼Œè€Œä¸”ä¸éœ€è¦é¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚<details>
<summary>Abstract</summary>
Current simultaneous speech translation models can process audio only up to a few seconds long. Contemporary datasets provide an oracle segmentation into sentences based on human-annotated transcripts and translations. However, the segmentation into sentences is not available in the real world. Current speech segmentation approaches either offer poor segmentation quality or have to trade latency for quality. In this paper, we propose a novel segmentation approach for a low-latency end-to-end speech translation. We leverage the existing speech translation encoder-decoder architecture with ST CTC and show that it can perform the segmentation task without supervision or additional parameters. To the best of our knowledge, our method is the first that allows an actual end-to-end simultaneous speech translation, as the same model is used for translation and segmentation at the same time. On a diverse set of language pairs and in- and out-of-domain data, we show that the proposed approach achieves state-of-the-art quality at no additional computational cost.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“å‰åŒæ—¶ä¼ è¾“æ¨¡å‹å¯ä»¥å¤„ç†éŸ³é¢‘åªæœ‰å‡ ç§’é•¿ã€‚å½“å‰æ•°æ®æä¾›äº†äººæ³¨é‡Šçš„è®²è§£å’Œç¿»è¯‘ï¼Œä½†å®é™…ä¸–ç•Œä¸­æ²¡æœ‰è¿™æ ·çš„åˆ† segmentationã€‚å½“å‰çš„Speech segmentationæ–¹æ³•æˆ–è€…æä¾›ä½è´¨é‡çš„åˆ† segmentationæˆ–è€…è¦æ±‚äº¤æ¢å»¶è¿Ÿå’Œè´¨é‡ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆ† segmentationæ–¹æ³•ï¼Œç”¨äºä½å»¶è¿Ÿçš„ç«¯åˆ°ç«¯ Speech translationã€‚æˆ‘ä»¬åˆ©ç”¨ç°æœ‰çš„Speech translation encoder-decoderæ¶æ„å’Œ ST CTCï¼Œå¹¶è¯æ˜å®ƒå¯ä»¥å®Œæˆåˆ† segmentationä»»åŠ¡æ— éœ€ç›‘ç£æˆ–é¢å¤–å‚æ•°ã€‚æ ¹æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é¦–æ¬¡å®ç°äº†å®é™…çš„åŒæ—¶ Speech translationï¼Œå› ä¸ºåŒæ—¶ä½¿ç”¨äº†ç¿»è¯‘å’Œåˆ† segmentationçš„åŒä¸€æ¨¡å‹ã€‚åœ¨å¤šç§è¯­è¨€å¯¹å’Œå†…å¤–é¢†åŸŸæ•°æ®ä¸Šï¼Œæˆ‘ä»¬ç¤ºå‡ºäº†çŠ¶æ€æœºå™¨çš„è´¨é‡ï¼Œæ²¡æœ‰é¢å¤–è®¡ç®—æˆæœ¬ã€‚
</details></li>
</ul>
<hr>
<h2 id="Discuss-Before-Moving-Visual-Language-Navigation-via-Multi-expert-Discussions"><a href="#Discuss-Before-Moving-Visual-Language-Navigation-via-Multi-expert-Discussions" class="headerlink" title="Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions"></a>Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11382">http://arxiv.org/abs/2309.11382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxing Long, Xiaoqi Li, Wenzhe Cai, Hao Dong</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„é›¶åŸºç¡€Visual Language Navigationï¼ˆVLNï¼‰æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰VLNæ–¹æ³•å•ä¸€è‡ªåŠ¨æ€è€ƒçš„å±€é™æ€§ã€‚</li>
<li>methods: è¯¥æ¡†æ¶é‡‡ç”¨åŸŸä¸“å®¶çš„ååŠ©ï¼Œé€šè¿‡è®¨è®ºæ”¶é›†å…³é”®å¯¼èˆªä»»åŠ¡çš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬æŒ‡ä»¤ç†è§£ã€ç¯å¢ƒè¯†åˆ«å’Œå®Œæˆä¼°è®¡ã€‚</li>
<li>results: ç»è¿‡å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œä¸åŸŸä¸“å®¶è¿›è¡Œè®¨è®ºå¯ä»¥æœ‰æ•ˆåœ°ä¿ƒè¿›å¯¼èˆªï¼Œæé«˜æŒ‡ä»¤ç›¸å…³ä¿¡æ¯çš„ç†è§£ã€æ›´æ­£å¶æé”™è¯¯å’Œç­›é€‰ä¸ä¸€è‡´çš„è¿åŠ¨å†³ç­–ã€‚ç›¸æ¯”å•ä¸€è‡ªåŠ¨æ€è€ƒï¼Œè¯¥æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºä¼˜å¼‚ã€‚<details>
<summary>Abstract</summary>
Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting, we introduce a novel zero-shot VLN framework. Within this framework, large models possessing distinct abilities are served as domain experts. Our proposed navigation agent, namely DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks like instruction understanding, environment perception, and completion estimation. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and sifting through in-consistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> translate the following text into Simplified ChineseVisual language navigation (VLN) is an embodied task that requires a wide range of skills, including understanding, perception, and planning. Previous VLN methods have relied solely on one model's own thinking to make predictions within one round. However, even the most advanced large language model GPT4 struggles with handling multiple tasks through single-round self-thinking. In this work, inspired by expert consultation meetings, we introduce a novel zero-shot VLN framework. In this framework, large models with distinct abilities serve as domain experts. Our proposed navigation agent, called DiscussNav, can actively discuss with these experts to collect essential information before moving at every step. These discussions cover critical navigation subtasks such as understanding instructions, perceiving the environment, and estimating completion. Through comprehensive experiments, we demonstrate that discussions with domain experts can effectively facilitate navigation by perceiving instruction-relevant information, correcting inadvertent errors, and filtering out inconsistent movement decisions. The performances on the representative VLN task R2R show that our method surpasses the leading zero-shot VLN model by a large margin on all metrics. Additionally, real-robot experiments display the obvious advantages of our method over single-round self-thinking.ä¸­æ–‡ç®€ä½“ç‰ˆï¼šè§†è§‰è¯­è¨€å¯¼èˆªï¼ˆVLNï¼‰æ˜¯ä¸€ä¸ªéœ€è¦å„ç§æŠ€èƒ½çš„ä½“éªŒä»»åŠ¡ï¼ŒåŒ…æ‹¬ç†è§£ã€æ„ŸçŸ¥å’Œè§„åˆ’ã€‚å…ˆå‰çš„VLNæ–¹æ³•éƒ½æ˜¯å•ä¸€æ¨¡å‹è‡ªå·±æ€è€ƒï¼Œä½†æ˜¯å³ä½¿æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹GPT4ä¹Ÿåœ¨å¤„ç†å¤šä»»åŠ¡æ—¶ä»ç„¶é™·å…¥å›°éš¾ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œå¯å‘äºä¸“å®¶å’¨è¯¢ä¼šè®®ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„é›¶æ‰©å±•VLNæ¡†æ¶ã€‚åœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œå…·æœ‰ä¸åŒèƒ½åŠ›çš„å¤§æ¨¡å‹æœä¸ºåŸŸä¸“å®¶ã€‚æˆ‘ä»¬æå‡ºçš„å¯¼èˆªä»£ç†äººç§°ä¸ºDiscussNavï¼Œå¯ä»¥åœ¨æ¯æ­¥ç§»åŠ¨ä¹‹å‰ä¸è¿™äº›ä¸“å®¶è¿›è¡Œæ´»åŠ¨çš„è®¨è®ºï¼Œæ”¶é›†å…³é”®å¯¼èˆªå­ä»»åŠ¡çš„ä¿¡æ¯ã€‚è¿™äº›è®¨è®ºåŒ…æ‹¬ç†è§£æŒ‡ä»¤ã€è¯†åˆ«ç¯å¢ƒå’Œä¼°è®¡å®Œæˆåº¦ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº†ä¸åŸŸä¸“å®¶è¿›è¡Œè®¨è®ºå¯ä»¥æœ‰æ•ˆåœ°ä¿ƒè¿›å¯¼èˆªï¼Œæ•æ‰æŒ‡ä»¤ç›¸å…³ä¿¡æ¯ï¼Œ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²é”™è¯¯å’Œç­›é€‰å‡ºä¸ä¸€è‡´çš„ç§»åŠ¨å†³ç­–ã€‚R2Rä»»åŠ¡è¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šèƒœè¿‡é¢†å…ˆçš„é›¶æ‰©å±•VLNæ¨¡å‹ã€‚æ­¤å¤–ï¼Œå®é™…Robotå®éªŒä¹Ÿæ˜¾ç¤ºäº†æˆ‘ä»¬æ–¹æ³•åœ¨å•ä¸€è‡ªæˆ‘æ€è€ƒæ–¹é¢çš„æ˜æ˜¾ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="Incremental-Blockwise-Beam-Search-for-Simultaneous-Speech-Translation-with-Controllable-Quality-Latency-Tradeoff"><a href="#Incremental-Blockwise-Beam-Search-for-Simultaneous-Speech-Translation-with-Controllable-Quality-Latency-Tradeoff" class="headerlink" title="Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff"></a>Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11379">http://arxiv.org/abs/2309.11379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter PolÃ¡k, Brian Yan, Shinji Watanabe, Alex Waibel, OndÅ™ej Bojar</li>
<li>for:  simultanous speech translation</li>
<li>methods:  blockwise self-attentional encoder models, incremental blockwise beam search, local agreement or hold-$n$ policies</li>
<li>results: 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.Hereâ€™s the full translation in Simplified Chinese:</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹åŒæ—¶è¯­è¨€ç¿»è¯‘ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å—çŠ¶è‡ªæ³¨æ„åŠ›ç¼–ç å™¨æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨äº†å¢é‡å—wise beam searchå’Œæœ¬åœ°ä¸€è‡´æˆ–ä¿æŒ-$n$ ç­–ç•¥æ¥æ§åˆ¶è´¨é‡å’Œå»¶è¿Ÿçš„è´¨é‡ã€‚</li>
<li>results: åœ¨ MuST-C ä¸Šå®éªŒç»“æœæ˜¾ç¤ºï¼Œæ— éœ€æ”¹å˜å»¶è¿Ÿæˆ–è´¨é‡ï¼Œå¯ä»¥è·å¾—0.6-3.6 BLEU æå‡ï¼Œæˆ–è€…å¯ä»¥é™ä½0.8-1.4 s çš„å»¶è¿Ÿã€‚<details>
<summary>Abstract</summary>
Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \textit{incremental} translation to users. Further, this method lacks mechanisms for \textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode.   Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå—çº§è‡ªæ³¨æ„ç¼–ç å™¨æ¨¡å‹åœ¨åŒæ—¶è¯­éŸ³ç¿»è¯‘æ–¹é¢æœ€è¿‘å‡ å¹´æ¥å¾—åˆ°äº†ä¸€äº›æ‰¿è¯ºã€‚è¿™äº›æ¨¡å‹ä½¿ç”¨å—çº§æœç´¢å’Œå‡è®¾å¯é æ€§åˆ†æ•°æ¥å†³å®šç­‰å¾…æ›´å¤šçš„è¾“å…¥è¯­éŸ³ä¹‹å‰ç»§ç»­ç¿»è¯‘ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•ä¼šç»´æŠ¤å¤šä¸ªå‡è®¾ï¼Œç›´åˆ°æ•´ä¸ªè¯­éŸ³è¾“å…¥è¢«æ¶ˆè€—â€”â€”è¿™ç§æ–¹æ¡ˆæ— æ³•ç›´æ¥æ˜¾ç¤ºå•ä¸ªå¢é‡ç¿»è¯‘ç»™ç”¨æˆ·ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•ç¼ºä¹æ§åˆ¶è´¨é‡vså»¶è¿Ÿè´¸æ˜“çš„æœºåˆ¶ã€‚æˆ‘ä»¬æè®®ä¿®æ”¹å¢é‡å—çº§æœç´¢ï¼Œå¹¶æ·»åŠ åœ°æ–¹ä¸€è‡´æˆ–ä¿æŒ-$n$ ç­–ç•¥æ¥æ§åˆ¶è´¨é‡vså»¶è¿Ÿçš„è´¸æ˜“ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¡†æ¶åº”ç”¨äºåœ¨çº¿æˆ–ç¦»çº¿è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶è¯æ˜ä¸¤ç§ç±»å‹éƒ½å¯ä»¥åœ¨çº¿æ¨¡å¼ä¸‹ä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ Must-C ä¸Šå¾—åˆ°äº†0.6-3.6 BLEU æå‡ï¼Œæˆ–0.8-1.4 s å»¶è¿Ÿæå‡ï¼Œæ— éœ€æ”¹å˜è´¨é‡æˆ–å»¶è¿Ÿã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Preconditioned-Federated-Learning"><a href="#Preconditioned-Federated-Learning" class="headerlink" title="Preconditioned Federated Learning"></a>Preconditioned Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11378">http://arxiv.org/abs/2309.11378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyi Tao, Jindi Wu, Qun Li</li>
<li>for: è®­ç»ƒåˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä¿æŒé€šä¿¡æ•ˆç‡å’Œéšç§æ€§ã€‚</li>
<li>methods: åŸºäºæœ¬åœ°é€‚åº”å’ŒæœåŠ¡å™¨ç«¯é€‚åº”ä¸¤ä¸ªæ¡†æ¶ï¼Œé‡‡ç”¨æ–°çš„åVAR matrixé¢„conditionerï¼Œå®ç°äº†æ›´é«˜çš„é€šä¿¡æ•ˆç‡å’Œæ›´å¥½çš„é€‚åº”æ€§ã€‚</li>
<li>results: åœ¨ i.i.d. å’Œé i.i.d. æƒ…å†µä¸‹ï¼Œå®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°é¢†å…ˆçš„æ€§èƒ½æ°´å¹³ã€‚<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
federated learning (FL) æ˜¯ä¸€ç§åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥åœ¨é€šä¿¡æ•ˆç‡å’Œéšç§ä¿æŠ¤çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚æ ‡å‡†ä¼˜åŒ–æ–¹æ³•åœ¨ FL ä¸­æ˜¯è”é‚¦å¹³å‡ï¼ˆFedAvgï¼‰ï¼Œå®ƒåœ¨é€šä¿¡è½®æ¬¡ä¹‹é—´æ‰§è¡Œå¤šä¸ªæœ¬åœ° SGD æ­¥éª¤ã€‚FedAvg å·²è¢«è®¤ä¸ºåœ¨ä¸ç°ä»£é¦–ä¸ªé€‚åº”ä¼˜åŒ–ç›¸æ¯”lack algorithm adaptivityã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„é€šä¿¡æ•ˆç‡FLç®—æ³•ï¼ŒåŸºäºä¸¤ç§é€‚åº”æ¡†æ¶ï¼šæœ¬åœ°é€‚åº”ï¼ˆPreFedï¼‰å’ŒæœåŠ¡å™¨ç«¯é€‚åº”ï¼ˆPreFedOpï¼‰ã€‚æˆ‘ä»¬çš„æ–¹æ³•é‡‡ç”¨é€‚åº”æ€§çš„novelåæ–¹å·®çŸ©é˜µé¢„conditionerã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šæä¾›äº†æ”¶æ•›ä¿è¯ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ i.i.d. å’Œé i.i.d. è®¾ç½®ä¸‹è¾¾åˆ°äº†å½“å‰æœ€ä½³æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Hand-Gesture-Featured-Human-Motor-Adaptation-in-Tool-Delivery-using-Voice-Recognition"><a href="#Dynamic-Hand-Gesture-Featured-Human-Motor-Adaptation-in-Tool-Delivery-using-Voice-Recognition" class="headerlink" title="Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition"></a>Dynamic Hand Gesture-Featured Human Motor Adaptation in Tool Delivery using Voice Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11368">http://arxiv.org/abs/2309.11368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Fei, Stefano Tedeschi, Yanpei Huang, Andrew Kennedy, Ziwei Wang</li>
<li>for: è¿™ä¸ªè®ºæ–‡ç›®çš„æ˜¯æé«˜äººæœºåˆä½œçš„æ•ˆç‡ï¼Œä½¿ç”¨å¤šç§modal interactionæ–¹å¼ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥ä¸“æ³¨äºä»»åŠ¡æ‰§è¡Œï¼Œè€Œä¸éœ€è¦é¢å¤–åŸ¹è®­ç”¨æˆ·æœºå™¨äººç•Œé¢ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†æ‰‹åŠ¿è®¤è¯†ã€è¯­éŸ³è¯†åˆ«å’Œå¯ switchableæ§åˆ¶é€‚åº”ç­–ç•¥ï¼Œä»¥æä¾›ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„äººæœºåˆä½œæ¡†æ¶ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œ staticæ‰‹åŠ¿è®¤è¯†æ¨¡å—çš„å‡†ç¡®ç‡ä¸º94.3%ï¼ŒåŠ¨æ€è¿åŠ¨è®¤è¯†æ¨¡å—çš„å‡†ç¡®ç‡ä¸º97.6%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œäºº Soloæ‰§è¡Œä»»åŠ¡æ—¶ï¼Œæå‡ºçš„æ–¹æ³•å¯ä»¥æé«˜å·¥å…·äº¤eliveryçš„æ•ˆç‡ï¼Œè€Œä¸ä¼šå¹²æ‰°äººç±»æ„å›¾ã€‚<details>
<summary>Abstract</summary>
Human-robot collaboration has benefited users with higher efficiency towards interactive tasks. Nevertheless, most collaborative schemes rely on complicated human-machine interfaces, which might lack the requisite intuitiveness compared with natural limb control. We also expect to understand human intent with low training data requirements. In response to these challenges, this paper introduces an innovative human-robot collaborative framework that seamlessly integrates hand gesture and dynamic movement recognition, voice recognition, and a switchable control adaptation strategy. These modules provide a user-friendly approach that enables the robot to deliver the tools as per user need, especially when the user is working with both hands. Therefore, users can focus on their task execution without additional training in the use of human-machine interfaces, while the robot interprets their intuitive gestures. The proposed multimodal interaction framework is executed in the UR5e robot platform equipped with a RealSense D435i camera, and the effectiveness is assessed through a soldering circuit board task. The experiment results have demonstrated superior performance in hand gesture recognition, where the static hand gesture recognition module achieves an accuracy of 94.3\%, while the dynamic motion recognition module reaches 97.6\% accuracy. Compared with human solo manipulation, the proposed approach facilitates higher efficiency tool delivery, without significantly distracting from human intents.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººæœºåˆä½œå·²ç»ä¸ºç”¨æˆ·å¸¦æ¥æ›´é«˜çš„æ•ˆç‡åœ¨äº’åŠ¨ä»»åŠ¡ä¸­ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°åˆä½œæ–¹æ¡ˆä¾é å¤æ‚çš„äººæœºç•Œé¢ï¼Œå¯èƒ½ç¼ºä¹è‡ªç„¶çš„äººæœºäº¤äº’INTUITIVENESSã€‚æˆ‘ä»¬è¿˜æœŸæœ›åœ¨è®­ç»ƒæ•°æ®é‡å°‘çš„æƒ…å†µä¸‹ç†è§£äººç±»çš„æ„å›¾ã€‚ä¸ºå›ç­”è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä»‹ç»äº†ä¸€ç§åˆ›æ–°çš„äººæœºåˆä½œæ¡†æ¶ï¼Œå®ƒçµæ´»åœ°é›†æˆäº†æ‰‹åŠ¿è®¤è¯†ã€åŠ¨æ€è¿åŠ¨è®¤è¯†ã€è¯­éŸ³è¯†åˆ«å’Œå¯è°ƒåˆ¶æ§åˆ¶ç­–ç•¥ã€‚è¿™äº›æ¨¡å—æä¾›äº†ä¸€ç§ç”¨æˆ·å‹å¥½çš„æ–¹æ³•ï¼Œä½¿å¾—æœºå™¨äººå¯ä»¥æ ¹æ®ç”¨æˆ·éœ€è¦æä¾›å·¥å…·ï¼Œç‰¹åˆ«æ˜¯ç”¨æˆ·åœ¨åŒæ‰‹å·¥ä½œæ—¶ã€‚å› æ­¤ï¼Œç”¨æˆ·å¯ä»¥ä¸“æ³¨äºä»»åŠ¡æ‰§è¡Œè€Œä¸éœ€è¦é¢å¤–åŸ¹è®­äººæœºç•Œé¢çš„ä½¿ç”¨ï¼Œè€Œæœºå™¨äººå¯ä»¥ç†è§£ç”¨æˆ·çš„è‡ªç„¶å§¿åŠ¿ã€‚æœ¬æ–‡æ‰€æå‡ºçš„å¤šæ¨¡å¼äº’åŠ¨æ¡†æ¶åœ¨UR5eæœºå™¨äººå¹³å°ä¸Šæ‰§è¡Œï¼Œè£…å¤‡äº†RealSense D435iæ‘„åƒå¤´ï¼Œå¹¶é€šè¿‡ç„Šæ¥ç”µè·¯æ¿ä»»åŠ¡è¿›è¡Œè¯„ä¼°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œ staticæ‰‹åŠ¿è®¤è¯†æ¨¡å—çš„å‡†ç¡®ç‡ä¸º94.3%ï¼Œè€ŒåŠ¨æ€è¿åŠ¨è®¤è¯†æ¨¡å—çš„å‡†ç¡®ç‡è¾¾97.6%ã€‚ç›¸æ¯”äººç±»ç‹¬ç«‹æ“ä½œï¼Œæè®®çš„æ–¹æ³•å¯ä»¥æé«˜å·¥å…·äº¤ä»˜æ•ˆç‡ï¼Œæ— éœ€æ˜æ˜¾å¹²æ‰°äººç±»æ„å›¾ã€‚
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Question-Answering-for-Materials-Science-KGQA4MAT-Developing-Natural-Language-Interface-for-Metal-Organic-Frameworks-Knowledge-Graph-MOF-KG"><a href="#Knowledge-Graph-Question-Answering-for-Materials-Science-KGQA4MAT-Developing-Natural-Language-Interface-for-Metal-Organic-Frameworks-Knowledge-Graph-MOF-KG" class="headerlink" title="Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG)"></a>Knowledge Graph Question Answering for Materials Science (KGQA4MAT): Developing Natural Language Interface for Metal-Organic Frameworks Knowledge Graph (MOF-KG)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11361">http://arxiv.org/abs/2309.11361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan An, Jane Greenberg, Alex Kalinowski, Xintong Zhao, Xiaohua Hu, Fernando J. Uribe-Romo, Kyle Langlois, Jacob Furst, Diego A. GÃ³mez-GualdrÃ³n</li>
<li>for: æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªåŒ…æ‹¬161ä¸ªå¤æ‚é—®é¢˜çš„çŸ¥è¯†Graphé—®ç­”æ¿ï¼ˆKGQA4MATï¼‰ï¼Œæ—¨åœ¨æé«˜ææ–™ç§‘å­¦é¢†åŸŸçŸ¥è¯†Graphï¼ˆMOF-KGï¼‰çš„è®¿é—®æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§è‡ªç„¶è¯­è¨€ç•Œé¢æ¥æŸ¥è¯¢MOF-KGï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿæ¥ä½¿ç”¨ChatGPTå°†è‡ªç„¶è¯­è¨€é—®é¢˜ç¿»è¯‘æˆæ­£å¼çš„KGæŸ¥è¯¢è¯­è¨€ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ChatGPTå¯ä»¥æœ‰æ•ˆåœ°è§£å†³ä¸åŒå¹³å°å’ŒæŸ¥è¯¢è¯­è¨€çš„KGé—®ç­”é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥å¸®åŠ©åŠ é€Ÿææ–™ç§‘å­¦é¢†åŸŸçŸ¥è¯†Graphçš„æœç´¢å’Œæ¢ç´¢ã€‚<details>
<summary>Abstract</summary>
We present a comprehensive benchmark dataset for Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and query languages. The benchmark and the proposed approach aim to stimulate further research and development of user-friendly and efficient interfaces for querying domain-specific materials science knowledge graphs, thereby accelerating the discovery of novel materials.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„åŸºå‡†æ•°æ®é›† Ğ´Ğ»ÑçŸ¥è¯† graphsQuestion Answering in Materials Science (KGQA4MAT), WITH a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured databases and knowledge extracted from the literature. To enhance MOF-KG accessibility for domain experts, we aim to develop a natural language interface for querying the knowledge graph. We have developed a benchmark comprised of 161 complex questions involving comparison, aggregation, and complicated graph structures. Each question is rephrased in three additional variations, resulting in 644 questions and 161 KG queries. To evaluate the benchmark, we have developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We also apply the approach to the well-known QALD-9 dataset, demonstrating ChatGPT's potential in addressing KGQA issues for different platforms and query languages. The benchmark and the proposed approach aim to stimulate further research and development of user-friendly and efficient interfaces for querying domain-specific materials science knowledge graphs, thereby accelerating the discovery of novel materials.Here is the translation in Traditional Chinese:æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„åŸºå‡†æ•°æ®é›† Ğ´Ğ»ÑçŸ¥è¯† graphsQuestion Answering in Materials Science (KGQA4MAT),  WITH a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) å·²ç»å»ºç«‹äº†ç”±structured databaseså’Œæ–‡çŒ®ä¸­æå–çš„çŸ¥è¯† integrateã€‚ä¸ºäº†å¢å¼ºMOF-KGå¯¹é¢†åŸŸä¸“å®¶çš„å­˜å–ï¼Œæˆ‘ä»¬ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªè‡ªç„¶è¯­è¨€ç•Œé¢æ¥æŸ¥è¯¢çŸ¥è¯† Graphã€‚æˆ‘ä»¬å·²ç»å¼€å‘äº†ä¸€ä¸ªåŒ…å«161ä¸ªå¤æ‚é—®é¢˜ï¼Œæ¶‰åŠæ¯”è¾ƒã€æ€»å’Œã€å›¾åƒç»“æ„çš„é—®é¢˜ã€‚æ¯ä¸ªé—®é¢˜éƒ½æœ‰ä¸‰ä¸ªç‰ˆæœ¬ï¼Œå…±è®¡644ä¸ªé—®é¢˜å’Œ161ä¸ªKGæŸ¥è¯¢ã€‚ä¸ºäº†è¯„ä¼°åŸºå‡†ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªç³»ç»Ÿæ€§çš„æ–¹æ³•ï¼Œä½¿ç”¨ChatGPTæ¥å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºæ­£å¼çš„KGæŸ¥è¯¢ã€‚æˆ‘ä»¬è¿˜å°†è¿™ä¸ªæ–¹æ³•åº”ç”¨åˆ°çŸ¥åçš„QALD-9æ•°æ®é›†ä¸Šï¼Œå±•ç¤ºäº†ChatGPTå¯¹ä¸åŒå¹³å°å’ŒæŸ¥è¯¢è¯­è¨€çš„åº”ç”¨æ½œåŠ›ã€‚åŸºå‡†å’Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„ç›®çš„æ˜¯ä¿ƒè¿›é¢†åŸŸä¸“å®¶ç”¨æˆ·å‹å¥½å’Œé«˜æ•ˆçš„ç•Œé¢æ¥æŸ¥è¯¢é¢†åŸŸä¸“é—¨çš„ææ–™ç§‘å­¦çŸ¥è¯†å›¾è¡¨ï¼Œä»¥ä¾¿åŠ é€Ÿå‘ç°æ–°ææ–™çš„å‘ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="3D-Face-Reconstruction-the-Road-to-Forensics"><a href="#3D-Face-Reconstruction-the-Road-to-Forensics" class="headerlink" title="3D Face Reconstruction: the Road to Forensics"></a>3D Face Reconstruction: the Road to Forensics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11357">http://arxiv.org/abs/2309.11357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Maurizio La Cava, Giulia OrrÃ¹, Martin Drahansky, Gian Luca Marcialis, Fabio Roli</li>
<li>for: æ³•å¾‹é¢†åŸŸä¸­çš„3Dé¢éƒ¨é‡å»ºåº”ç”¨</li>
<li>methods: ä½¿ç”¨Surveillanceå½±åƒå’Œç…§ç‰‡è¿›è¡Œ3Dé¢éƒ¨é‡å»º</li>
<li>results: ç•¥è§é—®é¢˜ï¼Œå°šæœªç¡®ç«‹3Dé¢éƒ¨é‡å»ºåœ¨æ³•å¾‹é¢†åŸŸçš„ç§¯æè§’è‰²<details>
<summary>Abstract</summary>
3D face reconstruction algorithms from images and videos are applied to many fields, from plastic surgery to the entertainment sector, thanks to their advantageous features. However, when looking at forensic applications, 3D face reconstruction must observe strict requirements that still make its possible role in bringing evidence to a lawsuit unclear. An extensive investigation of the constraints, potential, and limits of its application in forensics is still missing. Shedding some light on this matter is the goal of the present survey, which starts by clarifying the relation between forensic applications and biometrics, with a focus on face recognition. Therefore, it provides an analysis of the achievements of 3D face reconstruction algorithms from surveillance videos and mugshot images and discusses the current obstacles that separate 3D face reconstruction from an active role in forensic applications. Finally, it examines the underlying data sets, with their advantages and limitations, while proposing alternatives that could substitute or complement them.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸‰ç»´é¢éƒ¨é‡å»ºç®—æ³•ä»å›¾åƒå’Œè§†é¢‘åº”ç”¨åˆ°å¤šä¸ªé¢†åŸŸï¼Œä»æ•´å½¢å¤–ç§‘åˆ°å¨±ä¹ä¸šï¼Œå› ä¸ºå®ƒä»¬çš„ä¼˜ç‚¹ã€‚ä½†å½“çœ‹åˆ°å®¡åˆ¤åº”ç”¨æ—¶ï¼Œä¸‰ç»´é¢éƒ¨é‡å»ºå¿…é¡»éµå®ˆä¸¥æ ¼çš„è¦æ±‚ï¼Œè¿™äº›è¦æ±‚ä»ç„¶ä½¿å…¶åœ¨æä¾›æ³•å¾‹è¯æ®çš„è§’è‰²æ˜¯ä¸æ¸…æ™°ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬è°ƒæŸ¥çš„ç›®çš„æ˜¯ shedding some light on this matterï¼Œå¼€å§‹ä»å®¡åˆ¤åº”ç”¨å’Œç”Ÿç‰©ometricsä¹‹é—´çš„å…³ç³»è¿›è¡Œæ¸…æ¥šçš„è§£é‡Šï¼Œå¹¶å¯¹surveillanceè§†é¢‘å’ŒæŠ“æ•å›¾åƒä¸­çš„3Dé¢éƒ¨é‡å»ºç®—æ³•çš„æˆæœè¿›è¡Œåˆ†æï¼Œå¹¶è®¨è®ºå½“å‰éšœç¢ä¸‰ç»´é¢éƒ¨é‡å»ºåœ¨å®¡åˆ¤åº”ç”¨ä¸­æ‰®æ¼”æ´»è·ƒè§’è‰²çš„åŸå› ã€‚æœ€åï¼Œå®ƒæ£€æŸ¥äº†ä¸‹é¢çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬å…¶ä¼˜ç‚¹å’Œé™åˆ¶ï¼Œå¹¶æå‡ºäº†ä»£æ›¿æˆ–è¡¥å……çš„æ–¹æ¡ˆã€‚Note: Please note that the translation is in Simplified Chinese, which is used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-on-Rare-Event-Prediction"><a href="#A-Comprehensive-Survey-on-Rare-Event-Prediction" class="headerlink" title="A Comprehensive Survey on Rare Event Prediction"></a>A Comprehensive Survey on Rare Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11356">http://arxiv.org/abs/2309.11356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chathurangi Shyalika, Ruwan Wickramarachchi, Amit Sheth</li>
<li>For: æœ¬ç ”ç©¶ä¸»è¦é’ˆå¯¹é¢‘ç‡ä½çš„ç½•è§äº‹ä»¶é¢„æµ‹ï¼Œå³ä½¿ç”¨æœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†ææ–¹æ³•æ¥é¢„æµ‹è¿™äº›äº‹ä»¶çš„å‘ç”Ÿã€‚* Methods: æœ¬æ–‡ç»¼è¿°äº†ç›®å‰é¢„æµ‹ç½•è§äº‹ä»¶çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€ç®—æ³•æ–¹æ³•å’Œè¯„ä¼°æ–¹æ³•ç­‰ï¼Œå¹¶ä»ä¸åŒçš„æ•°æ®æ¨¡å¼å’Œé¢„æµ‹æ–¹æ³•è§’åº¦è¿›è¡Œäº†æ¢³ç†å’Œåˆ†æã€‚* Results: æœ¬æ–‡ç»“æœæ˜¾ç¤ºï¼Œé¢„æµ‹ç½•è§äº‹ä»¶å­˜åœ¨è®¸å¤šæŒ‘æˆ˜ï¼Œå¦‚æ•°æ®ä¸å‡è¡¡ã€æ¨¡å‹åå‘ç­‰é—®é¢˜ï¼ŒåŒæ—¶è¿˜å­˜åœ¨è®¸å¤šç ”ç©¶ç¼ºä¹æˆ–æœªå¾—åˆ°å……åˆ†å‘æŒ¥çš„é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Rare event prediction involves identifying and forecasting events with a low probability using machine learning and data analysis. Due to the imbalanced data distributions, where the frequency of common events vastly outweighs that of rare events, it requires using specialized methods within each step of the machine learning pipeline, i.e., from data processing to algorithms to evaluation protocols. Predicting the occurrences of rare events is important for real-world applications, such as Industry 4.0, and is an active research area in statistical and machine learning. This paper comprehensively reviews the current approaches for rare event prediction along four dimensions: rare event data, data processing, algorithmic approaches, and evaluation approaches. Specifically, we consider 73 datasets from different modalities (i.e., numerical, image, text, and audio), four major categories of data processing, five major algorithmic groupings, and two broader evaluation approaches. This paper aims to identify gaps in the current literature and highlight the challenges of predicting rare events. It also suggests potential research directions, which can help guide practitioners and researchers.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç½•seenäº‹ä»¶é¢„æµ‹ involve identifyingå’Œforecastingäº‹ä»¶with a low probability usingæœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æã€‚ç”±äºæ•°æ®åˆ†å¸ƒçš„ååº¦ï¼Œå…¶ä¸­å¸¸è§äº‹ä»¶çš„é¢‘ç‡è¿œè¿œå¤§äºç½•seenäº‹ä»¶çš„é¢‘ç‡ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨ç‰¹æ®Šçš„æ–¹æ³•åœ¨æ¯ä¸ªæœºå™¨å­¦ä¹ ç®¡é“ä¸­ï¼Œä»æ•°æ®å¤„ç†åˆ°ç®—æ³•åˆ°è¯„ä¼°åè®®ã€‚é¢„æµ‹ç½•seenäº‹ä»¶çš„å‘ç”Ÿæ˜¯ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„é‡è¦é—®é¢˜ï¼Œå¦‚ç¬¬å››ä»£å·¥ä¸šï¼Œå¹¶æ˜¯æœºå™¨å­¦ä¹ çš„æ´»è·ƒç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡å…¨é¢å›é¡¾å½“å‰approaches for rare event prediction along four dimensionsï¼šç½•seenäº‹ä»¶æ•°æ®ã€æ•°æ®å¤„ç†ã€ç®—æ³•approachesã€å’Œè¯„ä¼°approachesã€‚Specifically, we consider 73 datasets from different modalitiesï¼ˆi.e., numerical, image, text, and audioï¼‰ã€å››å¤§ç±»æ•°æ®å¤„ç†ã€äº”å¤§ç®—æ³•ç»„åˆã€å’Œä¸¤å¤§è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡çš„ç›®çš„æ˜¯è¦æ ‡è¯†å½“å‰æ–‡çŒ®ä¸­çš„ç©ºç™½å’Œé¢„æµ‹ç½•seenäº‹ä»¶çš„æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº† potential research directionsï¼Œä»¥å¸®åŠ©å®è·µè€…å’Œç ”ç©¶äººå‘˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="C-cdot-ASE-Learning-Conditional-Adversarial-Skill-Embeddings-for-Physics-based-Characters"><a href="#C-cdot-ASE-Learning-Conditional-Adversarial-Skill-Embeddings-for-Physics-based-Characters" class="headerlink" title="C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters"></a>C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11351">http://arxiv.org/abs/2309.11351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, Wenping Wang</li>
<li>for: è¿™ä¸ªè®ºæ–‡ç›®æ ‡æ˜¯ä¸ºphysics-based charactersæä¾›ä¸€ä¸ªæœ‰æ•ˆçš„å­¦ä¹ æ¨èç³»ç»Ÿï¼Œä½¿å¾—è¿™äº›è§’è‰²å¯ä»¥å­¦ä¹ å¤šç§æŠ€èƒ½å¹¶æä¾›å¯æ§æ€§ã€‚</li>
<li>methods: è¿™ä¸ªç³»ç»Ÿä½¿ç”¨äº† conditional Adversarial Skill Embeddingsï¼ˆC$\cdot$ASEï¼‰ï¼Œå°†æŠ€èƒ½åŠ¨ä½œåˆ†æˆä¸åŒçš„å­é›†ï¼Œå¹¶ä½¿ç”¨ä½çº§åˆ«çš„æ¡ä»¶æ¨¡å‹æ¥å­¦ä¹ æ¡ä»¶è¡Œä¸ºåˆ†å¸ƒã€‚</li>
<li>results: è®ºæ–‡è¡¨æ˜ï¼Œä½¿ç”¨C$\cdot$ASEå¯ä»¥ç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–å’Œç°å®çš„æŠ€èƒ½åŠ¨ä½œï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­é‡ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜æä¾›äº†ä¸€ä¸ªé«˜çº§åˆ«çš„æ”¿ç­–æˆ–ç”¨æˆ·å¯ä»¥ä½¿ç”¨æŸç§æŠ€èƒ½ç‰¹å®šçš„æŒ‡å®šæ¥æ§åˆ¶è§’è‰²çš„è¡Œä¸ºã€‚<details>
<summary>Abstract</summary>
We present C$\cdot$ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for physics-based characters. Our physically simulated character can learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, outperforming state-of-the-art models, and can be repurposed in various downstream tasks. In particular, the explicit skill control handle allows a high-level policy or user to direct the character with desired skill specifications, which we demonstrate is advantageous for interactive character animation.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºC$\cdot$ASEæ¡†æ¶ï¼Œä¸€ç§é«˜æ•ˆæœ‰æ•ˆçš„æ¡†æ¶ï¼Œå­¦ä¹ å—æ¡ä»¶æ•Œæ„ç´ åµŒå…¥ï¼Œç”¨äºç‰©ç†åŸºç¡€çš„è§’è‰²ã€‚æˆ‘ä»¬çš„ç‰©ç†æ¨¡æ‹Ÿè§’è‰²å¯ä»¥å­¦ä¹ å¤šç§å¤šæ ·çš„æŠ€èƒ½ï¼ŒåŒæ—¶æä¾›å¯æ§æ€§ï¼Œé€šè¿‡ç›´æ¥æ§åˆ¶æŠ€èƒ½çš„æ‰§è¡Œã€‚C$\cdot$ASEå°†ä¸åŒçš„æŠ€èƒ½åŠ¨ä½œåˆ†æˆä¸åŒçš„å­é›†ï¼Œå¯¹å…·æœ‰ç›¸åŒæ€§çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒä½çº§åˆ«çš„æ¡ä»¶æ¨¡å‹ï¼Œå­¦ä¹ æ¡ä»¶è¡Œä¸ºåˆ†å¸ƒã€‚é€šè¿‡æŠ€èƒ½æ¡ä»¶å­¦ä¹ ï¼Œå¯ä»¥ç›´æ¥æ§åˆ¶è§’è‰²çš„æŠ€èƒ½ï¼Œå¹¶ä¸”å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡ç„¦ç‚¹æŠ€èƒ½é‡‡æ ·ã€éª¨éª¼å‰©ä½™åŠ›å’Œå…ƒç´ ç‰¹å¾æ©ç æ¥å¹³è¡¡å¤šç§æŠ€èƒ½çš„å¤æ‚æ€§ï¼Œå¼¥è¡¥åŠ¨åŠ›åŒ¹é…é—®é¢˜ï¼Œæ•æ‰æ›´åŠ æ™®éçš„è¡Œä¸ºç‰¹å¾ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæ¡ä»¶æ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–å’ŒçœŸå®çš„æŠ€èƒ½ï¼Œè¶…è¶Šå½“å‰æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­é‡ç”¨ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ¡ä»¶æ§åˆ¶æŠŠæ‰‹å…è®¸é«˜çº§æ”¿ç­–æˆ–ç”¨æˆ·æŒ‡å®šè§’è‰²çš„æ„¿æœ›æŠ€èƒ½è§„æ ¼ï¼Œæˆ‘ä»¬ç¤ºç¤ºå…¶å¯¹äº¤äº’è§’è‰²åŠ¨ç”»æœ‰åˆ©ã€‚
</details></li>
</ul>
<hr>
<h2 id="TRAVID-An-End-to-End-Video-Translation-Framework"><a href="#TRAVID-An-End-to-End-Video-Translation-Framework" class="headerlink" title="TRAVID: An End-to-End Video Translation Framework"></a>TRAVID: An End-to-End Video Translation Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11338">http://arxiv.org/abs/2309.11338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prottay Kumar Adhikary, Bandaru Sugandhi, Subhojit Ghimire, Santanu Pal, Partha Pakray<br>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æä¾›ä¸€ç§å®ç°è¯­è¨€ç¿»è¯‘çš„è§†é¢‘ç¿»è¯‘ç³»ç»Ÿï¼Œä»¥ä¾¿åœ¨ä¸åŒè¯­è¨€èƒŒæ™¯ä¸‹è¿›è¡Œæœ‰æ•ˆçš„æ²Ÿé€šã€‚methods: è¯¥ç³»ç»Ÿä½¿ç”¨äº†ä¸€ç§ç»¼åˆè¯­éŸ³å’Œè§†é¢‘çš„ç¿»è¯‘æ–¹æ³•ï¼Œé€šè¿‡å…·ä½“çš„è¯­éŸ³å’Œè§†é¢‘å¯¹åº”å…³ç³»æ¥å®ç°è§†é¢‘ä¸­çš„è¯­è¨€ç¿»è¯‘ã€‚results: è¯¥ç³»ç»Ÿå¯ä»¥å¸®åŠ©å­¦ç”Ÿå’Œç”¨æˆ·åœ¨ä½èµ„æºç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆçš„å­¦ä¹ å’Œæ²Ÿé€šï¼ŒåŒæ—¶æä¾›äº†ä¸€ç§æ›´åŠ çœŸå®å’Œå¸å¼•äººçš„å­¦ä¹ ç¯å¢ƒï¼Œä»è€Œæé«˜å­¦ä¹ æ•ˆæœå’Œå‚ä¸åº¦ã€‚<details>
<summary>Abstract</summary>
In today's globalized world, effective communication with people from diverse linguistic backgrounds has become increasingly crucial. While traditional methods of language translation, such as written text or voice-only translations, can accomplish the task, they often fail to capture the complete context and nuanced information conveyed through nonverbal cues like facial expressions and lip movements. In this paper, we present an end-to-end video translation system that not only translates spoken language but also synchronizes the translated speech with the lip movements of the speaker. Our system focuses on translating educational lectures in various Indian languages, and it is designed to be effective even in low-resource system settings. By incorporating lip movements that align with the target language and matching them with the speaker's voice using voice cloning techniques, our application offers an enhanced experience for students and users. This additional feature creates a more immersive and realistic learning environment, ultimately making the learning process more effective and engaging.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä»Šå¤©çš„å…¨çƒåŒ–ä¸–ç•Œä¸­ï¼Œä¸ä¸åŒè¯­è¨€èƒŒæ™¯çš„äººè¿›è¡Œæœ‰æ•ˆæ²Ÿé€šå·²ç»å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ä¼ ç»Ÿçš„è¯­è¨€ç¿»è¯‘æ–¹æ³•ï¼Œå¦‚æ–‡æœ¬æˆ–å£°éŸ³ç¿»è¯‘ï¼Œå¯ä»¥å®Œæˆä»»åŠ¡ï¼Œä½†å®ƒä»¬ç»å¸¸æ— æ³•æ•æ‰ spoken language ä¸­çš„å®Œæ•´ä¸Šä¸‹æ–‡å’Œç»†èŠ‚ä¿¡æ¯ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯è§†é¢‘ç¿»è¯‘ç³»ç»Ÿï¼Œä¸ä»…ç¿»è¯‘ spoken languageï¼Œè¿˜å°†ç¿»è¯‘åçš„è¯­éŸ³ä¸è¯´è¯äººçš„å˜´è¯­ynchronizeã€‚æˆ‘ä»¬çš„ç³»ç»Ÿä¸“æ³¨äºç¿»è¯‘å°åº¦å„è¯­è¨€çš„æ•™è‚²è®²è§£ï¼Œå¹¶ä¸”é’ˆå¯¹å…·æœ‰ä½èµ„æºç³»ç»Ÿçš„è®¾ç½®è¿›è¡Œè®¾è®¡ã€‚é€šè¿‡ä½¿ç”¨å£°éŸ³æ¶æ…æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„åº”ç”¨ç¨‹åºå°†å˜´è¯­ä¸ç›®æ ‡è¯­è¨€çš„å¯¹åº”è¯­éŸ³è¿›è¡ŒåŒ¹é…ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªæ›´åŠ çœŸå®å’Œæœ‰è¶£çš„å­¦ä¹ ç¯å¢ƒã€‚è¿™ç§é™„åŠ çš„ç‰¹æ€§ä½¿å¾—å­¦ä¹ è¿‡ç¨‹æ›´åŠ æœ‰æ•ˆå’Œæœ‰è¶£ã€‚
</details></li>
</ul>
<hr>
<h2 id="Gold-YOLO-Efficient-Object-Detector-via-Gather-and-Distribute-Mechanism"><a href="#Gold-YOLO-Efficient-Object-Detector-via-Gather-and-Distribute-Mechanism" class="headerlink" title="Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism"></a>Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11331">http://arxiv.org/abs/2309.11331</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Efficient-Computing">https://github.com/huawei-noah/Efficient-Computing</a></li>
<li>paper_authors: Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Kai Han, Yunhe Wang</li>
<li>for:  This paper aims to improve the object detection performance of YOLO-series models by introducing a new Gather-Distribute (GD) mechanism and implementing MAE-style pretraining.</li>
<li>methods:  The proposed Gold-YOLO model uses a GD mechanism that combines convolution and self-attention operations to improve multi-scale feature fusion. The model also uses MAE-style pretraining to enhance the performance.</li>
<li>results:  The Gold-YOLO model achieves an outstanding 39.9% AP on the COCO val2017 dataset and 1030 FPS on a T4 GPU, outperforming the previous SOTA model YOLOv6-3.0-N by +2.4% in terms of AP.<details>
<summary>Abstract</summary>
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datasets and 1030 FPS on a T4 GPU, which outperforms the previous SOTA model YOLOv6-3.0-N with similar FPS by +2.4%. The PyTorch code is available at https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO, and the MindSpore code is available at https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼ŒYOLOç³»åˆ—æ¨¡å‹åœ¨å®æ—¶å¯¹è±¡æ£€æµ‹é¢†åŸŸå–å¾—äº†é¢†å…ˆåœ°ä½ã€‚è®¸å¤šç ”ç©¶å°è¯•æé«˜åŸºçº¿ï¼Œé€šè¿‡ä¿®æ”¹æ¶æ„ã€å¢å¼ºæ•°æ®å’Œè®¾è®¡æ–°çš„æŸå¤±å‡½æ•°ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°å…ˆå‰çš„æ¨¡å‹ä»ç„¶å—åˆ°ä¿¡æ¯èåˆé—®é¢˜çš„å›°æ‰°ï¼Œå°½ç®¡Feature Pyramid Networkï¼ˆFPNï¼‰å’ŒPath Aggregation Networkï¼ˆPANetï¼‰å·²ç»å‡è½»äº†è¿™ä¸ªé—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜çº§çš„èšåˆåˆ†å‘æœºåˆ¶ï¼ˆGDï¼‰æœºåˆ¶ï¼Œé€šè¿‡ convolution å’Œè‡ªæ³¨æ„æ“ä½œå®ç°ã€‚è¿™æ–°çš„è®¾è®¡çš„æ¨¡å‹è¢«ç§°ä¸º Gold-YOLOï¼Œå®ƒæé«˜äº†å¤šå°ºåº¦ç‰¹å¾èåˆèƒ½åŠ›ï¼Œå¹¶åœ¨æ‰€æœ‰æ¨¡å‹ç¼©æ”¾æ°´å¹³ä¸Šå®ç°äº†ç†æƒ³çš„å¹³è¡¡ Ğ¼ĞµĞ¶Ğ´Ñƒå»¶è¿Ÿå’Œå‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ YOLO ç³»åˆ—æ¨¡å‹ä¸­å®æ–½äº† MAE é£æ ¼çš„é¢„è®­ç»ƒï¼Œè®© YOLO ç³»åˆ—æ¨¡å‹å¯ä»¥ä»æ— ç›‘ç£é¢„è®­ç»ƒä¸­å—ç›Šã€‚Gold-YOLO-N åœ¨ COCO val2017 æ•°æ®é›†ä¸Šè¾¾åˆ°äº†å‡ºè‰²çš„ 39.9% AP å’Œ T4 GPU ä¸Šçš„ 1030 FPSï¼Œè¶…è¿‡äº†å…ˆå‰çš„ SOTA æ¨¡å‹ YOLOv6-3.0-N çš„ç›¸ä¼¼ FPS å€¼ by +2.4%ã€‚PyTorch ä»£ç å¯ä»¥åœ¨ <https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO> æ‰¾åˆ°ï¼ŒMindSpore ä»£ç å¯ä»¥åœ¨ <https://gitee.com/mindspore/models/tree/master/research/cv/Gold_YOLO> æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Pricing-of-Applications-in-Cloud-Marketplaces-using-Game-Theory"><a href="#Dynamic-Pricing-of-Applications-in-Cloud-Marketplaces-using-Game-Theory" class="headerlink" title="Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory"></a>Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11316">http://arxiv.org/abs/2309.11316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht-Fooladi, Amir Masoud Rahmani</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨ç ”ç©¶äº‘å¸‚åœºç«äº‰å¯¹åº”çš„ä»·æ ¼ç­–ç•¥ï¼Œä»¥å¸®åŠ©ä¼ä¸šæ›´å¥½åœ°åˆ¶å®šä»·æ ¼ç­–ç•¥ã€‚</li>
<li>methods: è¯¥è®ºæ–‡é‡‡ç”¨äº†æ¸¸æˆç†è®ºæ¥è®¾è®¡åŠ¨æ€ä»·æ ¼ç­–ç•¥ï¼Œå¹¶åœ¨å§”å‘˜ä¼šä¸­è€ƒè™‘äº†å¤šå®¶æä¾›å•†çš„ç«äº‰ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡æ•°å­¦æ¨¡å‹æ¥ç ”ç©¶äº‘å¸‚åœºç«äº‰ï¼Œå¹¶è¯æ˜äº†å­˜åœ¨å’Œuniquenessçš„çº³ä»€å¹³è¡¡ï¼Œä»è€Œä¸ºä¼ä¸šæä¾›äº†æ–°çš„åŠ¨æ€ä»·æ ¼ç­–ç•¥ã€‚<details>
<summary>Abstract</summary>
The competitive nature of Cloud marketplaces as new concerns in delivery of services makes the pricing policies a crucial task for firms. so that, pricing strategies has recently attracted many researchers. Since game theory can handle such competing well this concern is addressed by designing a normal form game between providers in current research. A committee is considered in which providers register for improving their competition based pricing policies. The functionality of game theory is applied to design dynamic pricing policies. The usage of the committee makes the game a complete information one, in which each player is aware of every others payoff functions. The players enhance their pricing policies to maximize their profits. The contribution of this paper is the quantitative modeling of Cloud marketplaces in form of a game to provide novel dynamic pricing strategies; the model is validated by proving the existence and the uniqueness of Nash equilibrium of the game.
</details>
<details>
<summary>æ‘˜è¦</summary>
äº‘å¸‚åœºçš„ç«äº‰æ€§æ–°é—®é¢˜åœ¨æœåŠ¡äº¤ä»˜ä¸­å¸¦æ¥äº†ä»·æ ¼ç­–ç•¥çš„æ ¸å¿ƒä»»åŠ¡ Ğ´Ğ»Ñå…¬å¸ã€‚å› æ­¤ï¼Œä»·æ ¼ç­–ç•¥åœ¨æœ€è¿‘å¸å¼•äº†è®¸å¤šç ”ç©¶äººå‘˜ã€‚ç”±äºæ¸¸æˆç†è®ºå¯ä»¥è‰¯å¥½å¤„ç†è¿™ç§ç«äº‰ï¼Œå› æ­¤åœ¨å½“å‰ç ”ç©¶ä¸­ï¼Œè®¾è®¡äº†ä¸€ä¸ªå§”å‘˜ä¼šï¼Œè®©æä¾›è€…ä¸ºäº†æ”¹å–„å…¶ç«äº‰åŸºç¡€ä»·æ ¼ç­–ç•¥è¿›è¡Œæ³¨å†Œã€‚é€šè¿‡æ¸¸æˆç†è®ºçš„åº”ç”¨ï¼Œè®¾è®¡äº†åŠ¨æ€ä»·æ ¼ç­–ç•¥ã€‚ç”±äºå§”å‘˜ä¼šçš„å­˜åœ¨ï¼Œæ¸¸æˆå˜ä¸ºå®Œå…¨ä¿¡æ¯æ¸¸æˆï¼Œæ¯ä¸ªç©å®¶çŸ¥é“å½¼æ­¤çš„åˆ©ç›Šå‡½æ•°ã€‚ç©å®¶é€šè¿‡ä¼˜åŒ–ä»·æ ¼ç­–ç•¥æ¥ maximize åˆ©æ¶¦ã€‚æœ¬æ–‡çš„è´¡çŒ®åœ¨äºä»¥æ¸¸æˆçš„å½¢å¼å¯¹äº‘å¸‚åœºè¿›è¡Œé‡åŒ–æ¨¡å‹åŒ–ï¼Œæä¾›äº†æ–°çš„åŠ¨æ€ä»·æ ¼ç­–ç•¥ï¼›æ¨¡å‹çš„å­˜åœ¨å’Œuniqueness çš„è¯æ˜ï¼Œè¯æ˜äº†è¿™ç§æ¸¸æˆçš„ç¨³å®šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Competition-based-Pricing-Strategy-in-Cloud-Markets-using-Regret-Minimization-Techniques"><a href="#A-Competition-based-Pricing-Strategy-in-Cloud-Markets-using-Regret-Minimization-Techniques" class="headerlink" title="A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques"></a>A Competition-based Pricing Strategy in Cloud Markets using Regret Minimization Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11312">http://arxiv.org/abs/2309.11312</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Ghasemi, M. R. Meybodi, M. Dehghan, A. M. Rahmani</li>
<li>for: This paper aims to address the challenge of pricing in Cloud computing marketplaces, where providers compete without knowing each otherâ€™s pricing policies.</li>
<li>methods: The paper proposes a pricing policy based on regret minimization and applies it to an incomplete-information game modeling the competition among Cloud providers. The algorithm updates the distribution of strategies based on experienced regret, leading to faster minimization of regret and increased profits for providers.</li>
<li>results: The experimental results show that the proposed pricing policy leads to much greater increases in provider profits compared to other pricing policies, and the efficiency of various regret minimization techniques in a simulated marketplace of Cloud is discussed. Additionally, the study examines the return on investment of providers in considered organizations and finds promising results.Hereâ€™s the Chinese translation of the three key points:</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯è§£å†³äº‘è®¡ç®—å¸‚åœºåœºæ‰€ä¸­çš„ä»·æ ¼é—®é¢˜ï¼Œ provider competing without knowing each otherâ€™s pricing policiesã€‚</li>
<li>methods: è®ºæ–‡æå‡ºä¸€ç§åŸºäºåæ‚”æœ€å°åŒ–çš„ä»·æ ¼ç­–ç•¥ï¼Œå¹¶åº”ç”¨åˆ°äº†ä¸å®Œå…¨ä¿¡æ¯æ¸¸æˆä¸­æ¨¡æ‹Ÿäº‘æä¾›å•†çš„ç«äº‰ã€‚ç®—æ³•æ ¹æ®ç»éªŒçš„åæ‚”æ¥æ›´æ–°ç­–ç•¥åˆ†å¸ƒï¼Œå¯¼è‡´å¿«é€Ÿå‡å°‘åæ‚”ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ä»·æ ¼ç­–ç•¥åœ¨å…¶ä»–ä»·æ ¼ç­–ç•¥çš„æ¯”è¾ƒä¸­æ˜¾ç¤ºå‡ºäº†å¾ˆå¤§çš„å¢é•¿ï¼Œå¹¶ä¸”åœ¨æ¨¡æ‹Ÿäº‘ä¸­çš„ç«äº‰å¸‚åœºä¸­ï¼Œä¸åŒçš„åæ‚”æœ€å°åŒ–æŠ€æœ¯çš„æ•ˆç‡å¾—åˆ°äº†è¯¦ç»†çš„è®¨è®ºã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜ç ”ç©¶äº†è€ƒè™‘äº†ä¸åŒç»„ç»‡ä¸­æä¾›å•†çš„æŠ•èµ„å›æŠ¥ï¼Œå¹¶å‘ç°äº†æœ‰å‰æã€‚<details>
<summary>Abstract</summary>
Cloud computing as a fairly new commercial paradigm, widely investigated by different researchers, already has a great range of challenges. Pricing is a major problem in Cloud computing marketplace; as providers are competing to attract more customers without knowing the pricing policies of each other. To overcome this lack of knowledge, we model their competition by an incomplete-information game. Considering the issue, this work proposes a pricing policy related to the regret minimization algorithm and applies it to the considered incomplete-information game. Based on the competition based marketplace of the Cloud, providers update the distribution of their strategies using the experienced regret. The idea of iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized faster. The experimental results show much more increase in profits of the providers in comparison with other pricing policies. Besides, the efficiency of a variety of regret minimization techniques in a simulated marketplace of Cloud are discussed which have not been observed in the studied literature. Moreover, return on investment of providers in considered organizations is studied and promising results appeared.
</details>
<details>
<summary>æ‘˜è¦</summary>
äº‘è®¡ç®—ä½œä¸ºä¸€ç§æ¯”è¾ƒæ–°çš„å•†ä¸šæ¨¡å¼ï¼Œå·²ç»å¹¿æ³›ç ”ç©¶äº†ä¸åŒçš„ç ”ç©¶è€…ã€‚åœ¨äº‘è®¡ç®—å¸‚åœºä¸­ï¼Œä»·æ ¼æ˜¯ä¸€ä¸ªä¸»è¦çš„é—®é¢˜ï¼ŒProvider competing to attract more customers without knowing each other's pricing policiesã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿäº†è¿™ä¸ª incomplete-information gameã€‚åŸºäºäº‘è®¡ç®—å¸‚åœºçš„ç«äº‰æ€§ï¼Œæä¾›è€…é€šè¿‡ç»éªŒçš„ regret æ›´æ–°åˆ†å¸ƒçš„ç­–ç•¥ã€‚iteratively applying the algorithm for updating probabilities of strategies causes the regret get minimized fasterã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸å…¶ä»–ä»·æ ¼ç­–ç•¥ç›¸æ¯”ï¼Œæä¾›è€…çš„åˆ©æ¶¦å¢åŠ äº†å¾ˆå¤šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†ä¸€äº› regret minimization techniques åœ¨äº‘è®¡ç®—å¸‚åœºä¸­çš„æ•ˆç‡ï¼Œè¿™äº›resultæœªç»studied literatureã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†Providerçš„æŠ•èµ„å›æŠ¥ï¼Œå¹¶è·å¾—äº†æ‰å®çš„ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Rating-Prediction-in-Conversational-Task-Assistants-with-Behavioral-and-Conversational-Flow-Features"><a href="#Rating-Prediction-in-Conversational-Task-Assistants-with-Behavioral-and-Conversational-Flow-Features" class="headerlink" title="Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features"></a>Rating Prediction in Conversational Task Assistants with Behavioral and Conversational-Flow Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11307">http://arxiv.org/abs/2309.11307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rafaelhferreira/cta_rating_prediction">https://github.com/rafaelhferreira/cta_rating_prediction</a></li>
<li>paper_authors: Rafael Ferreira, David Semedo, JoÃ£o MagalhÃ£es</li>
<li>for: é¢„æµ‹å¯¹è¯ä»»åŠ¡åŠ©æ‰‹ï¼ˆCTAï¼‰çš„æˆåŠŸå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£ç”¨æˆ·è¡Œä¸ºå¹¶é‡‡å–ç›¸åº”çš„è¡ŒåŠ¨ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†TB-Rateræ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§å°†å¯¹è¯æµç¨‹ç‰¹å¾ä¸ç”¨æˆ·è¡Œä¸ºç‰¹å¾ç»“åˆåœ¨ä¸€èµ·çš„Transformeræ¨¡å‹ï¼Œç”¨äºåœ¨CTAåœºæ™¯ä¸‹é¢„æµ‹ç”¨æˆ·è¯„åˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†çœŸå®çš„äººç±»-æœºå™¨äººå¯¹è¯å’Œåœ¨Alexa TaskBotæŒ‘æˆ˜ä¸­æ”¶é›†çš„ç”¨æˆ·è¯„åˆ†æ•°æ®ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¯¹è¯æµç¨‹å’Œç”¨æˆ·è¡Œä¸ºæ–¹é¢çš„ç‰¹å¾å¯ä»¥åœ¨å•ä¸ªæ¨¡å‹ä¸­ç»“åˆï¼Œä»¥é¢„æµ‹Offlineè¯„åˆ†ã€‚æ­¤å¤–ï¼Œå¯¹CTAç‰¹æœ‰çš„è¡Œä¸ºç‰¹å¾è¿›è¡Œåˆ†æï¼Œå¯ä»¥ä¸ºæœªæ¥ç³»ç»Ÿæä¾›å‚è€ƒã€‚<details>
<summary>Abstract</summary>
Predicting the success of Conversational Task Assistants (CTA) can be critical to understand user behavior and act accordingly. In this paper, we propose TB-Rater, a Transformer model which combines conversational-flow features with user behavior features for predicting user ratings in a CTA scenario. In particular, we use real human-agent conversations and ratings collected in the Alexa TaskBot challenge, a novel multimodal and multi-turn conversational context. Our results show the advantages of modeling both the conversational-flow and behavioral aspects of the conversation in a single model for offline rating prediction. Additionally, an analysis of the CTA-specific behavioral features brings insights into this setting and can be used to bootstrap future systems.
</details>
<details>
<summary>æ‘˜è¦</summary>
é¢„æµ‹å¯¹è¯ä»»åŠ¡åŠ©æ‰‹ï¼ˆCTAï¼‰çš„æˆåŠŸå¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ç”¨æˆ·è¡Œä¸ºï¼Œä»è€Œæ›´å¥½åœ°è¡ŒåŠ¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†TB-Rateræ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢å™¨æ¨¡å‹ï¼Œç»“åˆå¯¹è¯æµç¨‹ç‰¹å¾å’Œç”¨æˆ·è¡Œä¸ºç‰¹å¾æ¥é¢„æµ‹ç”¨æˆ·è¯„åˆ†åœ¨CTAåœºæ™¯ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†çœŸå®çš„äººç±»-æœºå™¨äººå¯¹è¯å’Œåœ¨Alexa TaskBotæŒ‘æˆ˜ä¸­æ”¶é›†çš„ç”¨æˆ·è¯„åˆ†æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„å¤šæ¨¡å¼å’Œå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå°†å¯¹è¯æµç¨‹å’Œè¡Œä¸ºæ–¹é¢çš„ç‰¹å¾æ¨¡å‹åœ¨å•ä¸ªæ¨¡å‹ä¸­å¯ä»¥åœ¨çº¿è¯„åˆ†ä¸­è·å¾—ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼Œå¯¹CTAç‰¹æœ‰çš„è¡Œä¸ºç‰¹å¾è¿›è¡Œåˆ†æï¼Œå¯ä»¥ä¸ºæœªæ¥ç³»ç»Ÿæä¾›Bootstrapã€‚
</details></li>
</ul>
<hr>
<h2 id="FaceDiffuser-Speech-Driven-3D-Facial-Animation-Synthesis-Using-Diffusion"><a href="#FaceDiffuser-Speech-Driven-3D-Facial-Animation-Synthesis-Using-Diffusion" class="headerlink" title="FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion"></a>FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11306">http://arxiv.org/abs/2309.11306</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uuembodiedsocialai/FaceDiffuser">https://github.com/uuembodiedsocialai/FaceDiffuser</a></li>
<li>paper_authors: Stefan Stan, Kazi Injamamul Haque, Zerrin Yumak</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨è§£å†³Current methods mostly focus on deterministic deep learning methods for speech-driven 3D facial animation synthesis, which do not accurately capture non-verbal facial cues.</li>
<li>methods: è¯¥æ–¹æ³•åŸºäºDiffusion Techniqueï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å¤§è¯­éŸ³è¡¨ç¤ºæ¨¡å‹HuBERTå¯¹éŸ³é¢‘è¾“å…¥è¿›è¡Œç¼–ç ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹æ¯”äºç°æœ‰æ–¹æ³•æ—¶è¾¾åˆ°äº†æ›´å¥½æˆ–ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºäºblendshapeçš„rigged characterçš„æ•°æ®é›†ã€‚Hereâ€™s the full summary in Simplified Chinese:</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨è§£å†³Current methods mostly focus on deterministic deep learning methods for speech-driven 3D facial animation synthesis, which do not accurately capture non-verbal facial cues.</li>
<li>methods: è¯¥æ–¹æ³•åŸºäºDiffusion Techniqueï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å¤§è¯­éŸ³è¡¨ç¤ºæ¨¡å‹HuBERTå¯¹éŸ³é¢‘è¾“å…¥è¿›è¡Œç¼–ç ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹æ¯”äºç°æœ‰æ–¹æ³•æ—¶è¾¾åˆ°äº†æ›´å¥½æˆ–ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºäºblendshapeçš„rigged characterçš„æ•°æ®é›†ã€‚I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation synthesis has been a challenging task both in industry and research. Recent methods mostly focus on deterministic deep learning methods meaning that given a speech input, the output is always the same. However, in reality, the non-verbal facial cues that reside throughout the face are non-deterministic in nature. In addition, majority of the approaches focus on 3D vertex based datasets and methods that are compatible with existing facial animation pipelines with rigged characters is scarce. To eliminate these issues, we present FaceDiffuser, a non-deterministic deep learning model to generate speech-driven facial animations that is trained with both 3D vertex and blendshape based datasets. Our method is based on the diffusion technique and uses the pre-trained large speech representation model HuBERT to encode the audio input. To the best of our knowledge, we are the first to employ the diffusion method for the task of speech-driven 3D facial animation synthesis. We have run extensive objective and subjective analyses and show that our approach achieves better or comparable results in comparison to the state-of-the-art methods. We also introduce a new in-house dataset that is based on a blendshape based rigged character. We recommend watching the accompanying supplementary video. The code and the dataset will be publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>äººå·¥æ™ºèƒ½é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ç”Ÿæˆé—®é¢˜åœ¨è¡Œä¸šå’Œç ”ç©¶ä¸­éƒ½æ˜¯æŒ‘æˆ˜æ€§çš„ã€‚ç°æœ‰çš„æ–¹æ³•å¤§å¤šæ¶‰åŠå†³å®šæ€§æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå³ç»™å®šä¸€ä¸ªè¯­éŸ³è¾“å…¥ï¼Œè¾“å‡ºæ€»æ˜¯ä¸€æ ·çš„ã€‚ç„¶è€Œï¼Œç°å®ä¸­çš„éè¯­è¨€é¢éƒ¨å¾æ ‡æ˜¯ä¸å†³å®šçš„æ€§è´¨ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æ–¹æ³•éƒ½é›†ä¸­åœ¨3Dé¡¶ç‚¹åŸºæœ¬çš„æ•°æ®é›†å’Œæ–¹æ³•ä¸Šï¼Œä¸ç°æœ‰çš„äººç‰©åŠ¨ç”»ç®¡é“ç›¸å®¹çš„æ–¹æ³•scarceã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä»‹ç»FaceDiffuserï¼Œä¸€ç§éå†³å®šæ€§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆè¯­éŸ³é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºæ‰©æ•£æŠ€æœ¯ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„å¤§è¯­éŸ³è¡¨ç¤ºæ¨¡å‹HuBERTæ¥ç¼–ç éŸ³é¢‘è¾“å…¥ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ˜¯ç¬¬ä¸€ä¸ªä½¿ç”¨æ‰©æ•£æ–¹æ³•æ¥è§£å†³è¯­éŸ³é©±åŠ¨3Dé¢éƒ¨åŠ¨ç”»ç”Ÿæˆé—®é¢˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å¯¹è±¡å’Œä¸»è§‚åˆ†æï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸å½“å‰çŠ¶æ€çš„æ–¹æ³•ç›¸æ¯”æˆ–æ›´å¥½çš„æˆç»©ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ä¸ªæ–°çš„åŸºäºblendshapeçš„äººç‰©åŠ¨ç”»æ•°æ®é›†ã€‚å»ºè®®è§‚çœ‹é™„åŠ çš„è¡¥å……è§†é¢‘ã€‚ä»£ç å’Œæ•°æ®é›†å°†å…¬å¼€å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Cost-Aware-Mechanism-for-Optimized-Resource-Provisioning-in-Cloud-Computing"><a href="#A-Cost-Aware-Mechanism-for-Optimized-Resource-Provisioning-in-Cloud-Computing" class="headerlink" title="A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing"></a>A Cost-Aware Mechanism for Optimized Resource Provisioning in Cloud Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11299">http://arxiv.org/abs/2309.11299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safiye Ghasemi, Mohammad Reza Meybodi, Mehdi Dehghan Takht Fooladi, Amir Masoud Rahmani</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„èµ„æºé…ç½®æ–¹æ³•ï¼Œä»¥å‡å°‘èµ„æºé…ç½®æˆæœ¬çš„æ–¹å¼æ¥æ»¡è¶³éœ€æ±‚ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†å­¦ä¹ è‡ªåŠ¨è¿‡ç¨‹æ¥é€‰æ‹©æœ€é€‚åˆçš„èµ„æºæ¥ä¸»æœºæ¯ä¸ªæœåŠ¡ï¼Œå¹¶è€ƒè™‘æˆæœ¬å’ŒæœåŠ¡éœ€æ±‚ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è¿è¡Œè®¸å¤šä¸åŒç±»å‹çš„åº”ç”¨ç¨‹åºï¼Œå¹¶ä¸”å¯ä»¥é€‚å½“åœ°å‡å°‘èµ„æºé…ç½®æˆæœ¬ã€‚<details>
<summary>Abstract</summary>
Due to the recent wide use of computational resources in cloud computing, new resource provisioning challenges have been emerged. Resource provisioning techniques must keep total costs to a minimum while meeting the requirements of the requests. According to widely usage of cloud services, it seems more challenging to develop effective schemes for provisioning services cost-effectively; we have proposed a novel learning based resource provisioning approach that achieves cost-reduction guarantees of demands. The contributions of our optimized resource provisioning (ORP) approach are as follows. Firstly, it is designed to provide a cost-effective method to efficiently handle the provisioning of requested applications; while most of the existing models allow only workflows in general which cares about the dependencies of the tasks, ORP performs based on services of which applications comprised and cares about their efficient provisioning totally. Secondly, it is a learning automata-based approach which selects the most proper resources for hosting each service of the demanded application; our approach considers both cost and service requirements together for deploying applications. Thirdly, a comprehensive evaluation is performed for three typical workloads: data-intensive, process-intensive and normal applications. The experimental results show that our method adapts most of the requirements efficiently, and furthermore the resulting performance meets our design goals.
</details>
<details>
<summary>æ‘˜è¦</summary>
The contributions of our optimized resource provisioning (ORP) approach are as follows:1. Cost-effective method: ORP provides a cost-effective method to efficiently handle the provisioning of requested applications, while most existing models only consider workflows in general and ignore the dependencies of tasks. ORP takes into account the services that applications comprise and cares about their efficient provisioning.2. Learning automata-based approach: ORP is a learning automata-based approach that selects the most appropriate resources for hosting each service of the demanded application. Our approach considers both cost and service requirements together for deploying applications.3. Comprehensive evaluation: We conducted a comprehensive evaluation for three typical workloads: data-intensive, process-intensive, and normal applications. The experimental results show that our method adapts to most of the requirements efficiently, and the resulting performance meets our design goals.
</details></li>
</ul>
<hr>
<h2 id="CPLLM-Clinical-Prediction-with-Large-Language-Models"><a href="#CPLLM-Clinical-Prediction-with-Large-Language-Models" class="headerlink" title="CPLLM: Clinical Prediction with Large Language Models"></a>CPLLM: Clinical Prediction with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11295">http://arxiv.org/abs/2309.11295</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nadavlab/CPLLM">https://github.com/nadavlab/CPLLM</a></li>
<li>paper_authors: Ofir Ben Shoham, Nadav Rappoport</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ä¸´åºŠé¢„æµ‹æ–¹æ³•ï¼Œä»¥ä¾¿é¢„æµ‹æ‚£è€…æ˜¯å¦ä¼šåœ¨ä¸‹ä¸€æ¬¡è®¿é—®æˆ–æ¥ä¸‹æ¥çš„è¯Šæ–­ä¸­è¢«è¯Šæ–­å‡ºæŸç§ç–¾ç—…ã€‚</li>
<li>methods: è¿™ä¸ªæ–¹æ³•æ˜¯åŸºäºå·²ç»é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œé€šè¿‡quantizationå’Œæç¤ºæ¥è¿›è¡Œå¾®è°ƒï¼Œä»¥ä¾¿é¢„æµ‹æ‚£è€…çš„ç–¾ç—…é£é™©ã€‚</li>
<li>results: å¯¹äºä¸åŒçš„åŸºçº¿æ¨¡å‹ï¼ŒåŒ…æ‹¬Logistic Regressionã€RETAINå’ŒMed-BERTï¼Œæˆ‘ä»¬çš„CPLLMæ¨¡å‹åœ¨PR-AUCå’ŒROC-AUC metricä¸Šéƒ½æ˜¾ç¤ºå‡ºäº†æ˜æ˜¾çš„æå‡ï¼Œè¾ƒbaselineæ¨¡å‹æ›´é«˜ã€‚<details>
<summary>Abstract</summary>
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘å›¢é˜Ÿç°åœ¨æå‡ºäº†ä¸´åºŠé¢„æµ‹ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆCPLLMï¼‰ï¼Œè¿™ç§æ–¹æ³•æ˜¯é€šè¿‡å…ˆå‰è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œç²¾åº¦è°ƒæ•´ï¼Œä»¥é¢„æµ‹æ‚£è€…å°†åœ¨ä¸‹ä¸€æ¬¡è®¿é—®æˆ–æ¥ä¸‹æ¥çš„è¯Šæ–­ä¸­è¢«è¯Šæ–­å‡ºçš„ç–¾ç—…ã€‚æˆ‘ä»¬ä½¿ç”¨é‡åŒ–å’Œç²¾åº¦è°ƒæ•´LLMï¼Œä½¿å…¶èƒ½å¤Ÿåˆ©ç”¨æ‚£è€…å†å²è¯Šæ–­è®°å½•æ¥é¢„æµ‹ç–¾ç—…ã€‚æˆ‘ä»¬ä¸various baselinesè¿›è¡Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬Logistic Regressionã€RETAINå’ŒMed-BERTï¼Œè¿™äº›æ¨¡å‹éƒ½æ˜¯ä½¿ç”¨ç»“æ„åŒ–åŒ»ç–—è®°å½•æ•°æ®è¿›è¡Œç–¾ç—…é¢„æµ‹çš„ç°çŠ¶ä¹‹arteã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒCPLLMåœ¨PR-AUCå’ŒROC-AUCæŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡äº†æ‰€æœ‰æµ‹è¯•æ¨¡å‹ï¼Œæ˜¾ç¤ºäº†ä¸åŸºçº¿æ¨¡å‹ç›¸æ¯”è€Œè¨€çš„remarkable enhancementsã€‚
</details></li>
</ul>
<hr>
<h2 id="Overview-of-AuTexTification-at-IberLEF-2023-Detection-and-Attribution-of-Machine-Generated-Text-in-Multiple-Domains"><a href="#Overview-of-AuTexTification-at-IberLEF-2023-Detection-and-Attribution-of-Machine-Generated-Text-in-Multiple-Domains" class="headerlink" title="Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains"></a>Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11285">http://arxiv.org/abs/2309.11285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autextification/AuTexTification-Overview">https://github.com/autextification/AuTexTification-Overview</a></li>
<li>paper_authors: Areg Mikael Sarvazyan, JosÃ© Ãngel GonzÃ¡lez, Marc Franco-Salvador, Francisco Rangel, Berta Chulvi, Paolo Rosso</li>
<li>for: è¿™ç¯‡è®ºæ–‡æè¿°äº†2023å¹´çš„IberLEFå·¥ä½œåŠä¸­çš„AuTexTificationåˆ†ç±»ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ä¸ªiberianè¯­è¨€è¯„ä¼°è®ºå›ï¼ˆSEPLNï¼‰2023å¹´ä¼šè®®çš„ä¸€éƒ¨åˆ†ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æè¿°äº†AuTexTificationä»»åŠ¡çš„ä¸¤ä¸ªå­ä»»åŠ¡ï¼šç¬¬ä¸€ä¸ªå­ä»»åŠ¡æ˜¯åˆ¤æ–­æ–‡æœ¬æ˜¯äººå·¥ç”Ÿæˆçš„è¿˜æ˜¯å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ï¼›ç¬¬äºŒä¸ªå­ä»»åŠ¡æ˜¯å½’å±ä¸€ä¸ªæœºå™¨ç”Ÿæˆæ–‡æœ¬åˆ°å…­ç§ä¸åŒçš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä¸­ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡æè¿°äº†AuTexTification2023æ•°æ®é›†ï¼ŒåŒ…å«äº†è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„160,000å¤šä¸ªæ–‡æœ¬ï¼Œæ¥è‡ªäº”ä¸ªé¢†åŸŸï¼ˆå¾®åšã€è¯„è®ºã€æ–°é—»ã€æ³•å¾‹å’Œä½¿ç”¨æ•™ç¨‹ï¼‰ã€‚æ€»å…±æœ‰114ä¸ªå›¢é˜Ÿå‚åŠ äº†æ¯”èµ›ï¼Œå…¶ä¸­36ä¸ªå›¢é˜Ÿå‘é€äº†175ä¸ªè¿è¡Œï¼Œ20ä¸ªå›¢é˜Ÿå‘é€äº†å·¥ä½œç¬”è®°ã€‚åœ¨è¿™ç¯‡æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†AuTexTificationæ•°æ®é›†å’Œä»»åŠ¡ï¼Œå‚ä¸ç³»ç»Ÿï¼Œä»¥åŠç»“æœã€‚<details>
<summary>Abstract</summary>
This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Rethinking-Sensors-Modeling-Hierarchical-Information-Enhanced-Traffic-Forecasting"><a href="#Rethinking-Sensors-Modeling-Hierarchical-Information-Enhanced-Traffic-Forecasting" class="headerlink" title="Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting"></a>Rethinking Sensors Modeling: Hierarchical Information Enhanced Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11284">http://arxiv.org/abs/2309.11284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/VAN-QIAN/CIKM23-HIEST">https://github.com/VAN-QIAN/CIKM23-HIEST</a></li>
<li>paper_authors: Qian Ma, Zijian Zhang, Xiangyu Zhao, Haoliang Li, Hongwei Zhao, Yiqi Wang, Zitao Liu, Wanyu Wang</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦å…³æ³¨äºåŸå¸‚åŒ–åŠ é€Ÿæ—¶çš„äº¤é€šé¢„æµ‹ï¼Œå¹¶åœ¨ç©ºé—´æ—¶é—´é¢„æµ‹ä¸­æå‡ºäº†ä¸€ä¸ªæ–°çš„æ–¹æ³•ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ª Hierarchical Information Enhanced Spatio-Temporal prediction æ–¹æ³•ï¼ˆHIESTï¼‰ï¼Œå®ƒå°†æ„Ÿåº”å™¨ä¹‹é—´çš„ä¾èµ–æ€§åˆ†ä¸ºä¸¤å±‚ï¼šåœ°åŸŸå±‚å’Œå…¨çƒå±‚ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒHIEST æ–¹æ³•åœ¨æ¯”è¾ƒäºç°æœ‰åŸºelineä¹‹ä¸Šè·å¾—äº†leading performanceã€‚<details>
<summary>Abstract</summary>
With the acceleration of urbanization, traffic forecasting has become an essential role in smart city construction. In the context of spatio-temporal prediction, the key lies in how to model the dependencies of sensors. However, existing works basically only consider the micro relationships between sensors, where the sensors are treated equally, and their macroscopic dependencies are neglected. In this paper, we argue to rethink the sensor's dependency modeling from two hierarchies: regional and global perspectives. Particularly, we merge original sensors with high intra-region correlation as a region node to preserve the inter-region dependency. Then, we generate representative and common spatio-temporal patterns as global nodes to reflect a global dependency between sensors and provide auxiliary information for spatio-temporal dependency learning. In pursuit of the generality and reality of node representations, we incorporate a Meta GCN to calibrate the regional and global nodes in the physical data space. Furthermore, we devise the cross-hierarchy graph convolution to propagate information from different hierarchies. In a nutshell, we propose a Hierarchical Information Enhanced Spatio-Temporal prediction method, HIEST, to create and utilize the regional dependency and common spatio-temporal patterns. Extensive experiments have verified the leading performance of our HIEST against state-of-the-art baselines. We publicize the code to ease reproducibility.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€åŸå¸‚åŒ–çš„åŠ é€Ÿï¼ŒåŸå¸‚æ™ºèƒ½åŒ–å»ºè®¾ä¸­çš„äº¤é€šé¢„æµ‹å·²æˆä¸ºä¸€é¡¹é‡è¦çš„ä»»åŠ¡ã€‚åœ¨ç©ºé—´æ—¶é—´é¢„æµ‹çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œå…³é”®åœ¨äºå¦‚ä½•æ¨¡å‹æ„ŸçŸ¥å™¨ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œåŸºæœ¬ä¸Šåªè€ƒè™‘äº†æ„ŸçŸ¥å™¨ä¹‹é—´çš„å¾®å‹å…³ç³»ï¼Œå¿½ç•¥äº†æ„ŸçŸ¥å™¨çš„å®è§‚ä¾èµ–å…³ç³»ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è®¤ä¸ºåº”é‡æ–°è€ƒè™‘æ„ŸçŸ¥å™¨ä¹‹é—´çš„ä¾èµ–æ¨¡å‹åŒ–ï¼Œä»ä¸¤ä¸ªå±‚æ¬¡æ¥çœ‹ï¼šåœ°åŸŸå’Œå…¨çƒè§†è§’ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†åŸå§‹æ„ŸçŸ¥å™¨é«˜åº¦ç›¸å…³çš„å†…éƒ¨èŠ‚ç‚¹åˆå¹¶ä¸ºä¸€ä¸ªåœ°åŸŸèŠ‚ç‚¹ï¼Œä»¥ä¿ç•™å®è§‚ä¾èµ–å…³ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä»£è¡¨æ€§çš„å…¨çƒèŠ‚ç‚¹ï¼Œç”¨äºåæ˜ å…¨çƒæ„ŸçŸ¥å™¨ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå¹¶æä¾›è¾…åŠ©çš„ç©ºé—´æ—¶é—´ä¾èµ–å­¦ä¹ ä¿¡æ¯ã€‚ä¸ºäº†ä¿è¯èŠ‚ç‚¹è¡¨ç¤ºçš„é€šç”¨æ€§å’Œå®é™…æ€§ï¼Œæˆ‘ä»¬å°†MetaGCN integrate into physical data spaceã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†è·¨å±‚æ¬¡å›¾ convolutionæ¥ä¼ é€’ä¸åŒå±‚æ¬¡çš„ä¿¡æ¯ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºç©ºé—´æ—¶é—´é¢„æµ‹æ–¹æ³•ï¼Œå³ Hierarchical Information Enhanced Spatio-Temporal predictionï¼ˆHIESTï¼‰ï¼Œä»¥åˆ›é€ å’Œåˆ©ç”¨åœ°åŸŸä¾èµ–å…³ç³»å’Œå…±åŒç©ºé—´æ—¶é—´æ¨¡å¼ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†HIESTåœ¨æ¯”è¾ƒé¡¶å°–åŸºå‡†ä¸‹çš„é¢†å…ˆæ€§ã€‚æˆ‘ä»¬å…¬å¸ƒäº†ä»£ç ï¼Œä»¥ä¾¿é‡ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Open-endedness-induced-through-a-predator-prey-scenario-using-modular-robots"><a href="#Open-endedness-induced-through-a-predator-prey-scenario-using-modular-robots" class="headerlink" title="Open-endedness induced through a predator-prey scenario using modular robots"></a>Open-endedness induced through a predator-prey scenario using modular robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11275">http://arxiv.org/abs/2309.11275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitri Kachler, Karine Miras</li>
<li>for: è¿™ä¸ªç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡æ¢é™©-çŒé£Ÿæƒ…å†µå¼•å‘å¼€æ”¾æ¼”åŒ–ï¼ˆOEEï¼‰ã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨å›ºå®š morphology çš„æ¨¡å—æœºå™¨äººï¼Œå…¶æ§åˆ¶å™¨è¢«è¿›è¡Œè¿›åŒ–ã€‚æœºå™¨äººå¯ä»¥å‘é€å’Œæ¥æ”¶ä¿¡å·ï¼Œå¹¶åœ¨ç¯å¢ƒä¸­è¯†åˆ«å…¶ä»–æœºå™¨äººçš„ç›¸å¯¹ä½ç½®ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªæ ‡è®°ç³»ç»Ÿï¼Œå®ƒæ”¹å˜äº†ä¸ªä½“å¦‚ä½•è¯†åˆ«å½¼æ­¤çš„æ–¹å¼ï¼Œå¹¶é¢„è®¡ä¼šå¢åŠ è¡Œä¸ºå¤æ‚æ€§ã€‚</li>
<li>results: ç ”ç©¶å‘ç°äº†é€‚åº”ç­–ç•¥çš„å‡ºç°ï¼Œè¯æ˜äº†é€šè¿‡æ¢é™©-çŒé£Ÿ dinamics ä½¿ç”¨æ¨¡å—æœºå™¨äººæ¥å¼•å‘ OEE çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œè¿™ç§emergenceä¼¼ä¹éœ€è¦æ ¹æ®è¡Œä¸ºæ ‡å‡†æ¥æ¡ä»¶ç¹æ®–ã€‚<details>
<summary>Abstract</summary>
This work investigates how a predator-prey scenario can induce the emergence of Open-Ended Evolution (OEE). We utilize modular robots of fixed morphologies whose controllers are subject to evolution. In both species, robots can send and receive signals and perceive the relative positions of other robots in the environment. Specifically, we introduce a feature we call a tagging system: it modifies how individuals can perceive each other and is expected to increase behavioral complexity. Our results show the emergence of adaptive strategies, demonstrating the viability of inducing OEE through predator-prey dynamics using modular robots. Such emergence, nevertheless, seemed to depend on conditioning reproduction to an explicit behavioral criterion.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™é¡¹ç ”ç©¶æ¢è®¨äº†æ é£Ÿ-çŒç‰©æƒ…å†µå¦‚ä½•å¼•èµ·å¼€æ”¾æ¼”åŒ–ï¼ˆOEEï¼‰çš„å‡ºç°ã€‚æˆ‘ä»¬åˆ©ç”¨å›ºå®šå½¢æ€çš„æ¨¡å—æœºå™¨äººçš„æ§åˆ¶å™¨è¿›è¡Œè¿›åŒ–ã€‚åœ¨ä¸¤ç§æœºå™¨äººä¸­ï¼Œæœºå™¨äººå¯ä»¥å‘é€å’Œæ¥æ”¶ä¿¡å·ï¼Œå¹¶ä¸”å¯ä»¥æ„ŸçŸ¥ç¯å¢ƒä¸­å…¶ä»–æœºå™¨äººçš„ç›¸å¯¹ä½ç½®ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç‰¹å¾ï¼Œå³æ ‡è®°ç³»ç»Ÿï¼šå®ƒæ”¹å˜äº†ä¸ªä½“å¦‚ä½•æ„ŸçŸ¥å½¼æ­¤ï¼Œå¹¶ä¸”é¢„æœŸä¼šå¢åŠ è¡Œä¸ºå¤æ‚æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºäº†é€‚åº”ç­–ç•¥çš„å‡ºç°ï¼Œè¯æ˜äº†é€šè¿‡æ é£Ÿ-çŒç‰© dinamics ä½¿ç”¨æ¨¡å—æœºå™¨äººæ¥å¼•èµ· OEE çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œè¿™ç§å‡ºç°ä¼¼ä¹Ğ²Ğ¸ÑĞ¸äºå¯¹è¡Œä¸ºæ ‡å‡†çš„æ¡ä»¶ä¿®å¤ reproduceã€‚Note: Please keep in mind that the translation is not perfect and may not capture all the nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Data-Suitability-and-Performance-Testing-Using-Fault-Injection-Testing-Framework"><a href="#Machine-Learning-Data-Suitability-and-Performance-Testing-Using-Fault-Injection-Testing-Framework" class="headerlink" title="Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework"></a>Machine Learning Data Suitability and Performance Testing Using Fault Injection Testing Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11274">http://arxiv.org/abs/2309.11274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manal Rahal, Bestoun S. Ahmed, Jorgen Samuelsson<br>for:This paper aims to address the gap in testing approaches for input data in machine learning (ML) systems, specifically the resilience of ML models to intentionally-triggered data faults.methods:The proposed framework, called FIUL-Data, uses data mutators to explore vulnerabilities of ML systems against data fault injections. The framework is designed with three main ideas: mutators are not random, one mutator is applied at a time, and selected ML models are optimized beforehand.results:The FIUL-Data framework is evaluated using data from analytical chemistry, and the results show that the framework allows for the evaluation of the resilience of ML models. In most experiments, ML models show higher resilience at larger training datasets, and gradient boost performed better than support vector regression in smaller training sets. The mean squared error metric is found to be useful in evaluating the resilience of models due to its higher sensitivity to data mutation.Here is the text in Simplified Chinese:for:è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯è§£å†³æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç³»ç»Ÿä¸­è¾“å…¥æ•°æ®æµ‹è¯•æ–¹æ³•çš„å·®è·ï¼Œå…·ä½“æ˜¯æµ‹è¯•MLæ¨¡å‹å¯¹æ•°æ®faultçš„æŠ—æ€§ã€‚methods:è¯¥æè®®çš„æ¡†æ¶æ˜¯FIUL-Dataï¼Œä½¿ç”¨æ•°æ®å˜æ¢å™¨æ¥æ¢ç´¢MLç³»ç»Ÿå¯¹æ•°æ®faultçš„æ•æ„Ÿæ€§ã€‚æ¡†æ¶è®¾è®¡äº†ä¸‰ä¸ªä¸»è¦æƒ³æ³•ï¼šå˜æ¢å™¨ä¸æ˜¯éšæœºçš„ï¼Œä¸€ä¸ªå˜æ¢å™¨åœ¨ä¸€æ¬¡å®ä¾‹æ—¶åº”ç”¨ï¼Œå¹¶ä¸”é€‰æ‹©çš„MLæ¨¡å‹åœ¨å…ˆå‰ä¼˜åŒ–ã€‚results:FIUL-Dataæ¡†æ¶åœ¨åˆ†æåŒ–å­¦ä¸­ä½¿ç”¨æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶å¯ä»¥è¯„ä¼°MLæ¨¡å‹çš„æŠ—æ€§ã€‚å¤§å¤šæ•°å®éªŒç»“æœè¡¨æ˜ï¼ŒMLæ¨¡å‹åœ¨æ›´å¤§çš„è®­ç»ƒé›†ä¸Šæ˜¾ç¤ºæ›´é«˜çš„æŠ—æ€§ï¼Œå¹¶ä¸”åœ¨è¾ƒå°çš„è®­ç»ƒé›†ä¸­ï¼Œæ¢¯åº¦æ‹Ÿåˆperform Ğ»ÑƒÑ‡äºæ”¯æŒå‘é‡å›å½’ã€‚æ€»çš„æ¥è¯´ï¼Œ Mean Squared Error åº¦é‡æœ‰ç”¨äºè¯„ä¼°æ¨¡å‹çš„æŠ—æ€§ï¼Œå› ä¸ºå®ƒå¯¹æ•°æ®å˜æ¢æ›´æ•æ„Ÿã€‚<details>
<summary>Abstract</summary>
Creating resilient machine learning (ML) systems has become necessary to ensure production-ready ML systems that acquire user confidence seamlessly. The quality of the input data and the model highly influence the successful end-to-end testing in data-sensitive systems. However, the testing approaches of input data are not as systematic and are few compared to model testing. To address this gap, this paper presents the Fault Injection for Undesirable Learning in input Data (FIUL-Data) testing framework that tests the resilience of ML models to multiple intentionally-triggered data faults. Data mutators explore vulnerabilities of ML systems against the effects of different fault injections. The proposed framework is designed based on three main ideas: The mutators are not random; one data mutator is applied at an instance of time, and the selected ML models are optimized beforehand. This paper evaluates the FIUL-Data framework using data from analytical chemistry, comprising retention time measurements of anti-sense oligonucleotide. Empirical evaluation is carried out in a two-step process in which the responses of selected ML models to data mutation are analyzed individually and then compared with each other. The results show that the FIUL-Data framework allows the evaluation of the resilience of ML models. In most experiments cases, ML models show higher resilience at larger training datasets, where gradient boost performed better than support vector regression in smaller training sets. Overall, the mean squared error metric is useful in evaluating the resilience of models due to its higher sensitivity to data mutation.
</details>
<details>
<summary>æ‘˜è¦</summary>
åˆ›å»ºå¯æ¢å¤çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç³»ç»Ÿå·²ç»æˆä¸ºç¡®ä¿ç”Ÿäº§å‡†å¤‡çš„MLç³»ç»Ÿè·å¾—ç”¨æˆ·ä¿¡ä»»çš„å¿…è¦æ‰‹æ®µã€‚è¾“å…¥æ•°æ®è´¨é‡å’Œæ¨¡å‹å¯¹ç”Ÿæˆç«¯åˆ°ç«¯æµ‹è¯•çš„æˆåŠŸäº§ç”Ÿå¾ˆå¤§å½±å“ã€‚ç„¶è€Œï¼Œè¾“å…¥æ•°æ®æµ‹è¯•çš„æ–¹æ³•å¹¶ä¸å¤Ÿç³»ç»ŸåŒ–ï¼Œä¸æ¨¡å‹æµ‹è¯•ç›¸æ¯”ç›¸å¯¹è½åã€‚ä¸ºè§£å†³è¿™ä¸ªå·®è·ï¼Œæœ¬æ–‡æå‡ºäº†è¾“å…¥æ•°æ®ä¸­çš„å¼‚å¸¸æŠ•å…¥æµ‹è¯•æ¡†æ¶ï¼ˆFIUL-Dataï¼‰ï¼Œç”¨äºæµ‹è¯•MLæ¨¡å‹å¯¹å¤šç§æ„å¤–è§¦å‘çš„æ•°æ®å¼‚å¸¸çš„æŠ—æ€§ã€‚æ•°æ®å˜æ¢å™¨æ¢ç´¢äº†MLç³»ç»Ÿå¯¹å„ç§å¼‚å¸¸æŠ•å…¥çš„æ•æ„Ÿæ€§ã€‚è¯¥æ¡†æ¶åŸºäºä»¥ä¸‹ä¸‰ä¸ªä¸»è¦æƒ³æ³•ï¼šå˜æ¢å™¨ä¸æ˜¯éšæœºçš„ï¼Œåªæœ‰ä¸€ä¸ªå˜æ¢å™¨åœ¨ä¸€ä¸ªæ—¶é—´ç‚¹ä¸Šåº”ç”¨ï¼Œå¹¶ä¸”é€‰æ‹©çš„MLæ¨¡å‹åœ¨å…ˆå‰ä¼˜åŒ–ã€‚æœ¬æ–‡é€šè¿‡ä½¿ç”¨åˆ†æåŒ–å­¦æ•°æ®ï¼ŒåŒ…æ‹¬æŠ‘åˆ¶è‚½çš„é‡Šæ”¾æ—¶é—´æµ‹é‡ï¼Œå¯¹FIUL-Dataæ¡†æ¶è¿›è¡Œäº†å®è¯è¯„ä¼°ã€‚å®éªŒåœ¨ä¸¤æ­¥è¿›è¡Œï¼Œå…ˆåˆ†åˆ«åˆ†æé€‰æ‹©çš„MLæ¨¡å‹å¯¹æ•°æ®å˜æ¢çš„å“åº”ï¼Œç„¶åå¯¹å„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒFIUL-Dataæ¡†æ¶å¯ä»¥è¯„ä¼°MLæ¨¡å‹çš„æŠ—æ€§ã€‚å¤§å¤šæ•°å®éªŒæƒ…å†µä¸‹ï¼ŒMLæ¨¡å‹åœ¨æ›´å¤§çš„è®­ç»ƒé›†ä¸Šæ˜¾ç¤ºæ›´é«˜çš„æŠ—æ€§ï¼Œå…¶ä¸­æ¢¯åº¦æ‹Ÿåˆåœ¨å°è®­ç»ƒé›†ä¸­è¡¨ç°æ›´å¥½ã€‚æ€»çš„æ¥è¯´ï¼Œå¹³å‡æ–¹å·®è¯¯å·®åº¦é‡æ˜¯è¯„ä¼°MLæ¨¡å‹æŠ—æ€§çš„æœ‰ç”¨æŒ‡æ ‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Grounded-Complex-Task-Segmentation-for-Conversational-Assistants"><a href="#Grounded-Complex-Task-Segmentation-for-Conversational-Assistants" class="headerlink" title="Grounded Complex Task Segmentation for Conversational Assistants"></a>Grounded Complex Task Segmentation for Conversational Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11271">http://arxiv.org/abs/2309.11271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rafaelhferreira/grounded_task_segmentation_cta">https://github.com/rafaelhferreira/grounded_task_segmentation_cta</a></li>
<li>paper_authors: Rafael Ferreira, David Semedo, JoÃ£o MagalhÃ£es</li>
<li>for: è¿™ paper æ˜¯ä¸ºäº†æ”¹è¿› web-based  instrucional textï¼Œä½¿å…¶æ›´é€‚åˆ conversational  Settingã€‚</li>
<li>methods: è¯¥ paper ä½¿ç”¨ Transformer-based æ¶æ„è¿›è¡Œè®¡ç®—æ¨¡å‹ï¼Œä»¥åŠæŒ‰ç…§ conversational enario è¿›è¡Œ instrucional ç»“æ„æ ‡æ³¨ã€‚</li>
<li>results: ç»è¿‡æµ‹è¯•ï¼Œç”¨æˆ·å¯¹ step çš„ complexity å’Œ length æœ‰æ‰€åå¥½ï¼Œå¹¶ä¸”æå‡ºçš„æ–¹æ³•å¯ä»¥æ”¹å–„åŸå§‹çš„ web-based instrucional textï¼Œæé«˜äº† 86% çš„è¯„ä»·ã€‚<details>
<summary>Abstract</summary>
Following complex instructions in conversational assistants can be quite daunting due to the shorter attention and memory spans when compared to reading the same instructions. Hence, when conversational assistants walk users through the steps of complex tasks, there is a need to structure the task into manageable pieces of information of the right length and complexity. In this paper, we tackle the recipes domain and convert reading structured instructions into conversational structured ones. We annotated the structure of instructions according to a conversational scenario, which provided insights into what is expected in this setting. To computationally model the conversational step's characteristics, we tested various Transformer-based architectures, showing that a token-based approach delivers the best results. A further user study showed that users tend to favor steps of manageable complexity and length, and that the proposed methodology can improve the original web-based instructional text. Specifically, 86% of the evaluated tasks were improved from a conversational suitability point of view.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¯·æ±‚ä¸­çš„å¤æ‚æŒ‡ä»¤å¯èƒ½ä¼šè®©ç”¨æˆ·æ„Ÿåˆ°å›°æƒ‘ï¼Œè¿™æ˜¯å› ä¸ºä¸é˜…è¯»ç›¸åŒçš„æŒ‡ä»¤ç›¸æ¯”ï¼Œç”¨æˆ·çš„æ³¨æ„åŠ›å’Œè®°å¿† span æ›´çŸ­ã€‚å› æ­¤ï¼Œå½“ conversational assistant é€šè¿‡å¤šä¸ªæ­¥éª¤å¼•å¯¼ç”¨æˆ·å®Œæˆå¤æ‚ä»»åŠ¡æ—¶ï¼Œéœ€è¦å°†ä»»åŠ¡åˆ†è§£æˆå¯ç®¡ç†çš„å°å—ä¿¡æ¯ï¼Œä»¥ä¾¿ç”¨æˆ·æ›´å¥½åœ°ç†è§£å’Œå®Œæˆã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°† recipes é¢†åŸŸä¸­çš„æŒ‡ä»¤ç»“æ„åŒ–ä¸º conversational ç»“æ„ï¼Œå¹¶é€šè¿‡å¯¹è¯æƒ…å¢ƒè¿›è¡Œæ ‡æ³¨ï¼Œä»è€Œè·å¾—äº†æ›´æ·±åˆ»çš„ç†è§£ã€‚ä¸ºäº†è®¡ç®— conversational æ­¥éª¤çš„ç‰¹ç‚¹ï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸åŒçš„ Transformer åŸº architectureï¼Œå‘ç° token åŸºæœ¬æ³•å–å¾—äº†æœ€å¥½çš„ç»“æœã€‚è¿›ä¸€æ­¥çš„ç”¨æˆ·ç ”ç©¶è¡¨æ˜ï¼Œç”¨æˆ·åå¥½ç®¡ç† complexity å’Œ length çš„æ­¥éª¤ï¼Œè€Œæˆ‘ä»¬çš„æ–¹æ³•ologies å¯ä»¥æ”¹å–„åŸå§‹çš„ç½‘ç»œä¸Šçš„æŒ‡ä»¤æ–‡æœ¬ã€‚ç‰¹åˆ«æ˜¯ï¼Œ86% çš„è¯„ä¼°ä»»åŠ¡å¾—åˆ°äº† conversational é€‚ç”¨æ€§çš„æ”¹è¿›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Sequence-to-Sequence-Spanish-Pre-trained-Language-Models"><a href="#Sequence-to-Sequence-Spanish-Pre-trained-Language-Models" class="headerlink" title="Sequence-to-Sequence Spanish Pre-trained Language Models"></a>Sequence-to-Sequence Spanish Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11259">http://arxiv.org/abs/2309.11259</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs">https://github.com/vgaraujov/Seq2Seq-Spanish-PLMs</a></li>
<li>paper_authors: Vladimir Araujo, Maria Mihaela Trusca, Rodrigo TufiÃ±o, Marie-Francine Moens</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘é’ˆå¯¹è¥¿ç­ç‰™è¯­åºåˆ—è®­ç»ƒçš„encoder-decoderæ¨¡å‹ï¼Œç”¨äºè¿›è¡Œæ–‡æœ¬æ‘˜è¦ã€é‡å™å’Œç”Ÿæˆé—®ç­”ç­‰åºåˆ—è½¬æ¢ä»»åŠ¡ã€‚</li>
<li>methods: è¯¥è®ºæ–‡é‡‡ç”¨äº†BERTã€RoBERTaå’ŒGPTç­‰æ‰¹å¤„ç†è¯­è¨€æ¨¡å‹çš„encoder-decoderæ¶æ„ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†é€‚åº”æ€§çš„é¢„è®­ç»ƒï¼Œä»¥ä¾¿åœ¨è¥¿ç­ç‰™è¯­æ–‡æœ¬ä¸­è¿›è¡Œæ›´å¥½çš„è¡¨ç°ã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡å¯¹å„æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œå‘ç°BERTå’ŒT5æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°æœ€ä½³ï¼Œè€ŒBARTæ¨¡å‹ä¹Ÿåœ¨æŸäº›ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜å°†æ‰€æœ‰æ¨¡å‹å…¬å¼€å‘å¸ƒåˆ°ç ”ç©¶ç¤¾åŒºï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„è¥¿ç­ç‰™è¯­å¤„ç†ç ”ç©¶ã€‚<details>
<summary>Abstract</summary>
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging as top performers across all evaluated tasks. As an additional contribution, we have made all models publicly available to the research community, fostering future exploration and development in Spanish language processing.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œå¤§è§„æ¨¡çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹æŠ€æœ¯å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è‹±è¯­ä»¥å¤–è¯­è¨€çš„ç ”å‘ã€‚è™½ç„¶è¥¿ç­ç‰™è¯­æ¨¡å‹ï¼ŒåŒ…æ‹¬BERTã€RoBERTaå’ŒGPTï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰å“è¶Šè¡¨ç°ï¼Œä½†æ˜¯è¿˜ç¼ºä¹é€‚ç”¨äºåºåˆ—-åºåˆ—ä»»åŠ¡çš„encoder-decoderæ¨¡å‹ã€‚è¿™ç¯‡è®ºæ–‡åˆ›æ–°åœ°ä»‹ç»äº†è¥¿ç­ç‰™è¯­encoder-decoderæ¨¡å‹çš„å®ç°å’Œè¯„ä¼°ï¼Œå…·ä½“æ¥è¯´æ˜¯åœ¨è¥¿ç­ç‰™è¯­ corpus ä¸Šé¢„è®­ç»ƒçš„ BARTã€T5 å’Œ BERT2BERT æ ·å¼æ¨¡å‹ã€‚æˆ‘ä»¬å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ¦‚è¦ã€é‡æ–°å†™å’Œç”Ÿæˆé—®ç­”ç­‰åºåˆ—-åºåˆ—ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„å‘ç°è¡¨æ˜æ‰€æœ‰æ¨¡å‹éƒ½å…·æœ‰ç«äº‰åŠ›ï¼ŒBART å’Œ T5 åœ¨æ‰€æœ‰è¯„ä¼°ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰æ¨¡å‹å…¬å¼€å‘å¸ƒç»™ç ”ç©¶ç¤¾åŒºï¼Œä»¥ä¿ƒè¿›æœªæ¥çš„æ¢ç´¢å’Œå‘å±•åœ¨è¥¿ç­ç‰™è¯­å¤„ç†é¢†åŸŸã€‚
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Multi-Agent-Reinforcement-Learning-for-Air-Combat-Maneuvering"><a href="#Hierarchical-Multi-Agent-Reinforcement-Learning-for-Air-Combat-Maneuvering" class="headerlink" title="Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering"></a>Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11247">http://arxiv.org/abs/2309.11247</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/IDSIA/marl">https://github.com/IDSIA/marl</a></li>
<li>paper_authors: Ardian Selmonaj, Oleg Szehr, Giacomo Del Rio, Alessandro Antonucci, Adrian Schneider, Michael RÃ¼egsegger</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æä¾›ä¸€ä¸ªå¤šä»£ç†äººé—®é¢˜å†³ç­–æ¡†æ¶ï¼Œä»¥å®ç°ç²¾ç¡®çš„ç©ºä¸­ä½œæˆ˜å†³ç­–ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨å¤šä»£ç†äººé—®é¢˜å†³ç­–æ¡†æ¶ï¼Œåˆ†ä¸ºä¸¤ä¸ªé˜¶å±‚ï¼šä½å±‚ä¸ºå•ä½å¯¹æˆ˜æ–—æ§åˆ¶çš„ç»†èŠ‚æ”¿ç­–ï¼Œé«˜å±‚ä¸ºé«˜çº§æŒ‡æŒ¥å®˜ç­–ç•¥ï¼Œå¯¹äºä»»åŠ¡ç›®æ ‡è¿›è¡Œå†³ç­–ã€‚ä½å±‚ç­–ç•¥é€è¿‡å¢åŠ å¤æ‚è®­ç»ƒenarioå’Œè”èµ›è‡ªæ¸¸æˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œè€Œé«˜å±‚ç­–ç•¥åˆ™é€è¿‡å·²ç»é¢„è®­ç»ƒçš„ä½å±‚ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚</li>
<li>results: è¿™ä¸ªæ¡†æ¶çš„å®éªŒéªŒè¯è¡¨æ˜ï¼Œè¿™ç§å¤šä»£ç†äººé—®é¢˜å†³ç­–æ¡†æ¶å…·æœ‰ä¼˜åŒ–ç©ºä¸­ä½œæˆ˜å†³ç­–çš„åŠŸèƒ½ã€‚<details>
<summary>Abstract</summary>
The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play. The commander policy is trained on mission targets given pre-trained low-level policies. The empirical validation advocates the advantages of our design choices.
</details>
<details>
<summary>æ‘˜è¦</summary>
application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date, the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learning curriculum with increasingly complex training scenarios and league-based self-play. The commander policy is trained on mission targets given pre-trained low-level policies. The empirical validation advocates the advantages of our design choices.Here's the translation in Traditional Chinese:è¿ç”¨äººå·¥æ™ºèƒ½æ¨¡æ‹Ÿç©ºä¸­æ­¦å™¨æˆ˜åœºæƒ…å†µçš„åº”ç”¨æ­£åœ¨å¸å¼•è¶Šæ¥è¶Šå¤šçš„æ³¨æ„ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œé«˜ç»´åº¦çš„çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ï¼Œé«˜å¤æ‚çš„æƒ…å†µä¿¡æ¯ï¼ˆå¦‚å—æŸå’ŒèŒƒå›´ä¿¡æ¯ã€æ•°æ®æ»¡æ„åº¦ã€ä»»åŠ¡ç›®æ ‡çŸ¥è¯†ä¸å®Œæ•´ï¼‰ä»¥åŠéçº¿æ€§çš„é£è¡ŒåŠ¨åŠ›å­¦éƒ½å¯¹äºç²¾å‡†çš„ç©ºä¸­æˆ˜æ–—å†³ç­–å¸¦æ¥å·¨å¤§æŒ‘æˆ˜ã€‚å½“å¤šä¸ªä¸åŒæ€§çš„ä»£ç†äººå‚ä¸æ—¶ï¼Œè¿™äº›æŒ‘æˆ˜æ›´åŠ ä¸¥é‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå±‚æ¬¡å¤šä»£ç†äººå­¦ä¹ æ¡†æ¶ï¼Œç”¨äºç©ºä¸­æˆ˜æ–—å¤šä¸ªä¸åŒæ€§ä»£ç†äººã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œå†³ç­–è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªå±‚æ¬¡çš„æŠ½è±¡ï¼Œå…¶ä¸­ä¸“é—¨çš„ä½å±‚ç­–ç•¥æ§åˆ¶ä¸ªåˆ«å•ä½çš„è¡ŒåŠ¨ï¼Œè€Œé«˜å±‚ç­–ç•¥æ ¹æ®å…¨å±€ä»»åŠ¡ç›®æ ‡å‘å‡ºå¤§è§„æ¨¡çš„æŒ‡ä»¤ã€‚ä½å±‚ç­–ç•¥åœ¨å¢åŠ å¤æ‚çš„è®­ç»ƒenarioå’Œè”èµ›è‡ªæ¸¸ä¸­è¿›è¡Œè®­ç»ƒã€‚é«˜å±‚ç­–ç•¥åˆ™æ˜¯æ ¹æ®å·²ç»é¢„è®­çš„ä½å±‚ç­–ç•¥è¿›è¡Œè®­ç»ƒã€‚å®é™…éªŒè¯è¡¨æ˜äº†æˆ‘ä»¬çš„è®¾è®¡é€‰æ‹©çš„ä¼˜ç‚¹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Colour-Passing-Revisited-Lifted-Model-Construction-with-Commutative-Factors"><a href="#Colour-Passing-Revisited-Lifted-Model-Construction-with-Commutative-Factors" class="headerlink" title="Colour Passing Revisited: Lifted Model Construction with Commutative Factors"></a>Colour Passing Revisited: Lifted Model Construction with Commutative Factors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11236">http://arxiv.org/abs/2309.11236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malte Luttermann, Tanya Braun, Ralf MÃ¶ller, Marcel Gehrke</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºSymmetriesçš„å‡çº§æ¦‚ç‡æ¨ç†æ–¹æ³•ï¼Œä»¥å®ç°å¯é åœ°æ¦‚ç‡æ¨ç†ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†colour passingç®—æ³•ï¼Œä½†æ˜¯ç°æœ‰çš„colour passingç®—æ³•å—é™äºç‰¹å®šçš„æ¨ç†ç®—æ³•ï¼Œå¹¶ä¸”å¿½ç•¥äº†å› ç´ çš„ ĞºĞ¾Ğ¼Ğ¼ÑƒÑ‚Ğ°Ñ‚Ğ¸Ğ²æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºé€»è¾‘å˜é‡çš„ä¿®æ”¹ç‰ˆcolour passingç®—æ³•ï¼Œå¯ä»¥ç‹¬ç«‹äºç‰¹å®šçš„æ¨ç†ç®—æ³•æ¥æ„å»ºå‡çº§è¡¨ç¤ºï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨å› ç´ çš„ ĞºĞ¾Ğ¼Ğ¼ÑƒÑ‚Ğ°Ñ‚Ğ¸Ğ²æ€§ã€‚</li>
<li>results: å¯¹æ¯”äºç°æœ‰çš„colour passingç®—æ³•ï¼Œæœ¬æ–‡çš„æ–¹æ³•å¯ä»¥æ›´å¥½åœ°æ£€æµ‹Symmetriesï¼Œä»è€Œå®ç°æ›´é«˜çš„å‹ç¼©ç‡å’Œæ›´å¿«çš„åœ¨çº¿æŸ¥è¯¢é€Ÿåº¦ã€‚<details>
<summary>Abstract</summary>
Lifted probabilistic inference exploits symmetries in a probabilistic model to allow for tractable probabilistic inference with respect to domain sizes. To apply lifted inference, a lifted representation has to be obtained, and to do so, the so-called colour passing algorithm is the state of the art. The colour passing algorithm, however, is bound to a specific inference algorithm and we found that it ignores commutativity of factors while constructing a lifted representation. We contribute a modified version of the colour passing algorithm that uses logical variables to construct a lifted representation independent of a specific inference algorithm while at the same time exploiting commutativity of factors during an offline-step. Our proposed algorithm efficiently detects more symmetries than the state of the art and thereby drastically increases compression, yielding significantly faster online query times for probabilistic inference when the resulting model is applied.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¢å¼ºæ¦‚ç‡æ¨ç†åˆ©ç”¨æ¨¡å‹ä¸­çš„å¯¹ç§°æ€§æ¥å®ç°å¯è¡Œçš„æ¦‚ç‡æ¨ç†ï¼Œå…·ä½“æ¥è¯´æ˜¯é€šè¿‡å¢å¼ºçš„å¯è¡Œæ¨ç†ç®—æ³•æ¥å®ç°ã€‚ä¸ºäº†åº”ç”¨å¢å¼ºæ¨ç†ï¼Œéœ€è¦é¦–å…ˆè·å¾—å¢å¼ºè¡¨ç¤ºï¼Œè€Œç°æœ‰çš„é¢œè‰²ä¼ é€’ç®—æ³•æ˜¯state of the artçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œè¿™ä¸ªç®—æ³•å—åˆ°ç‰¹å®šæ¨ç†ç®—æ³•çš„é™åˆ¶ï¼Œè€Œä¸”å¿½ç•¥äº†å› ç´ çš„ ĞºĞ¾Ğ¼Ğ¼ÑƒÑ‚Ğ°Ñ‚Ğ¸Ğ²æ€§ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„é¢œè‰²ä¼ é€’ç®—æ³•ï¼Œä½¿ç”¨é€»è¾‘å˜é‡æ¥æ„å»ºç‹¬ç«‹äºç‰¹å®šæ¨ç†ç®—æ³•çš„å¢å¼ºè¡¨ç¤ºï¼ŒåŒæ—¶åœ¨Offlineé˜¶æ®µåˆ©ç”¨å› ç´ çš„ commutativity æ¥æé«˜å‹ç¼©ç‡ã€‚æˆ‘ä»¬çš„æè®®ç®—æ³•å¯ä»¥æ›´å¥½åœ°æ£€æµ‹æ¨¡å‹ä¸­çš„å¯¹ç§°æ€§ï¼Œä»è€Œå¯¼è‡´æ›´å¿«çš„åœ¨çº¿æŸ¥è¯¢æ—¶é—´ã€‚
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-4-as-a-Tool-for-Reviewing-Academic-Books-in-Spanish"><a href="#ChatGPT-4-as-a-Tool-for-Reviewing-Academic-Books-in-Spanish" class="headerlink" title="ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish"></a>ChatGPT-4 as a Tool for Reviewing Academic Books in Spanish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11231">http://arxiv.org/abs/2309.11231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonnathan Berrezueta-Guzman, Laura Malache-Silva, Stephan Krusche</li>
<li>For: This study evaluates the potential of ChatGPT-4 as an editing tool for Spanish literary and academic books.* Methods: The study analyzes the features and capabilities of ChatGPT-4 in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish.* Results: ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time, but faces challenges in areas such as context sensitivity and interaction with visual content. Collaboration between ChatGPT-4 and human reviewers and editors is a promising strategy for improving efficiency without compromising quality.Here are the three points in Simplified Chinese text:* For: è¿™é¡¹ç ”ç©¶è¯„ä¼°äº†OpenAIå¼€å‘çš„ChatGPT-4è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿç”¨äºè¥¿ç­ç‰™æ–‡å­¦å’Œå­¦æœ¯ä¹¦ç±çš„ç¼–è¾‘ã€‚* Methods: ç ”ç©¶åˆ†æäº†ChatGPT-4æ¨¡å‹åœ¨è¥¿ç­ç‰™æ–‡ grammarä¿®æ­£ã€é£æ ¼ä¸€è‡´æ€§å’Œè¯­è¨€ä¸°å¯Œæ€§æ–¹é¢çš„åŠŸèƒ½å’Œèƒ½åŠ›ã€‚* Results: ChatGPT-4èƒ½å¤Ÿå¿«é€Ÿå’Œå‡†ç¡®åœ°è¿›è¡Œè¯­æ³•å’Œæ‹¼å†™ä¿®æ­£ï¼Œä½†åœ¨ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§å’Œå›¾è¡¨å’Œè¡¨æ ¼äº¤äº’æ–¹é¢å­˜åœ¨æŒ‘æˆ˜ã€‚äººç±»ç¼–è¾‘å’Œè¯„å®¡è€…å’ŒChatGPT-4 collaboration å¯èƒ½æ˜¯æé«˜æ•ˆç‡è€Œæ— éœ€é™ä½è´¨é‡çš„æœ‰æ•ˆç­–ç•¥ã€‚<details>
<summary>Abstract</summary>
This study evaluates the potential of ChatGPT-4, an artificial intelligence language model developed by OpenAI, as an editing tool for Spanish literary and academic books. The need for efficient and accessible reviewing and editing processes in the publishing industry has driven the search for automated solutions. ChatGPT-4, being one of the most advanced language models, offers notable capabilities in text comprehension and generation. In this study, the features and capabilities of ChatGPT-4 are analyzed in terms of grammatical correction, stylistic coherence, and linguistic enrichment of texts in Spanish. Tests were conducted with 100 literary and academic texts, where the edits made by ChatGPT-4 were compared to those made by expert human reviewers and editors. The results show that while ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time, it still faces challenges in areas such as context sensitivity, bibliometric analysis, deep contextual understanding, and interaction with visual content like graphs and tables. However, it is observed that collaboration between ChatGPT-4 and human reviewers and editors can be a promising strategy for improving efficiency without compromising quality. Furthermore, the authors consider that ChatGPT-4 represents a valuable tool in the editing process, but its use should be complementary to the work of human editors to ensure high-caliber editing in Spanish literary and academic books.
</details>
<details>
<summary>æ‘˜è¦</summary>
Tests were conducted on 100 literary and academic texts, comparing the edits made by ChatGPT-4 to those made by expert human reviewers and editors. The results show that ChatGPT-4 is capable of making grammatical and orthographic corrections with high accuracy and in a very short time. However, it still struggles with context sensitivity, bibliometric analysis, deep contextual understanding, and interaction with visual content like graphs and tables.Despite these limitations, collaboration between ChatGPT-4 and human reviewers and editors is a promising strategy for improving efficiency without compromising quality. The authors conclude that ChatGPT-4 represents a valuable tool in the editing process, but its use should be complementary to the work of human editors to ensure high-caliber editing in Spanish literary and academic books.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Diversity-in-Online-Interactions"><a href="#Leveraging-Diversity-in-Online-Interactions" class="headerlink" title="Leveraging Diversity in Online Interactions"></a>Leveraging Diversity in Online Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11224">http://arxiv.org/abs/2309.11224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nardine Osman, Bruno Rosell i Gui, Carles Sierra</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨é€šè¿‡åœ¨çº¿è¿æ¥äººä»¬ï¼Œå¸®åŠ©ä»–ä»¬è§£å†³æ—¥å¸¸é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å£°æ˜æ€§è§„èŒƒæ¥mediateåœ¨çº¿äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åœ¨è¿æ¥äººä»¬æ—¶åˆ©ç”¨å¤šæ ·æ€§ã€‚</li>
<li>results: åœ¨ä¸åŒçš„å¤§å­¦ç«™ç‚¹ä¸Šè¿›è¡Œçš„è¯•éªŒæ˜¾ç¤ºï¼Œé€‰æ‹©çš„profileå¤šæ ·æ€§å¾—åˆ°äº†ç›¸å¯¹æˆåŠŸï¼Œå¹¶å¾—åˆ°äº†ç”¨æˆ·æ»¡æ„çš„è¯„ä»·ã€‚<details>
<summary>Abstract</summary>
This paper addresses the issue of connecting people online to help them find support with their day-to-day problems. We make use of declarative norms for mediating online interactions, and we specifically focus on the issue of leveraging diversity when connecting people. We run pilots at different university sites, and the results show relative success in the diversity of the selected profiles, backed by high user satisfaction.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡å…³æ³¨åœ¨çº¿è¿æ¥äººä»¬ï¼Œä»¥å¸®åŠ©ä»–ä»¬è§£å†³æ—¥å¸¸é—®é¢˜ã€‚æˆ‘ä»¬åˆ©ç”¨å£°æ˜æ€§è§„èŒƒæ¥è°ƒæ§åœ¨çº¿äº¤äº’ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤šæ ·æ€§è¿æ¥äººä»¬ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„å¤§å­¦ç«™ç‚¹è¿›è¡Œäº†è¯•ç‚¹ï¼Œç»“æœè¡¨æ˜åœ¨é€‰æ‹©çš„profileä¸­çš„å¤šæ ·æ€§å¾—åˆ°äº†ç›¸å¯¹æˆåŠŸï¼Œå¹¶å¾—åˆ°äº†ç”¨æˆ·æ»¡æ„çš„è¯„ä»·ã€‚Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Retrieve-Rewrite-Answer-A-KG-to-Text-Enhanced-LLMs-Framework-for-Knowledge-Graph-Question-Answering"><a href="#Retrieve-Rewrite-Answer-A-KG-to-Text-Enhanced-LLMs-Framework-for-Knowledge-Graph-Question-Answering" class="headerlink" title="Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering"></a>Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11206">http://arxiv.org/abs/2309.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yike Wu, Nan Hu, Sheng Bi, Guilin Qi, Jie Ren, Anhuan Xie, Wei Song</li>
<li>for: æé«˜çŸ¥è¯†GraphQuestionAnsweringï¼ˆKGQAï¼‰ä»»åŠ¡çš„è¡¨ç°ï¼Œè§£å†³rich world knowledgeçš„é—®é¢˜ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§Answer-sensitive KG-to-Textæ–¹æ³•ï¼Œå°†çŸ¥è¯†Graphï¼ˆKGï¼‰çŸ¥è¯†è½¬åŒ–æˆæ–‡æœ¬è¡¨ç¤ºï¼Œä»¥ä¾¿ä¸è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é›†æˆã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„KG-to-Textå¢å¼ºçš„LLMsæ¡†æ¶åœ¨KGQAä»»åŠ¡ä¸Šçš„ç­”æ¡ˆå‡†ç¡®ç‡å’ŒçŸ¥è¯†å£°æ˜çš„æœ‰ç”¨æ€§éƒ½é«˜äºä¹‹å‰çš„KG-åŠ å¼ºLLMsæ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperforms previous KG-augmented LLMs approaches regarding answer accuracy and usefulness of knowledge statements.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨çŸ¥è¯†å¯†é›†ä»»åŠ¡ä¸Šè¡¨ç°ç«äº‰æ€§å¼ºï¼Œä½†å®ƒä»¬ä»æœ‰å¸æ”¶å…¨çƒçŸ¥è¯†çš„é™åˆ¶ï¼Œç‰¹åˆ«æ˜¯é•¿å°¾çŸ¥è¯†ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†çŸ¥è¯†å›¾ï¼ˆKGï¼‰æ‰©å±•åˆ°è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰çš„æ–¹æ³•ï¼Œä»¥è§£å†³éœ€è¦ä¸°å¯Œä¸–ç•ŒçŸ¥è¯†çš„é—®é¢˜ answeringï¼ˆKGQAï¼‰ä»»åŠ¡ã€‚ç°æœ‰çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨KGçŸ¥è¯†æ¥æé«˜LLMsçš„æé—®å¯ä»¥æ˜¾è‘—æé«˜LLMsåœ¨KGQAä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¿½ç•¥äº†KGè¡¨ç¤ºå’Œæ–‡æœ¬è¡¨ç¤ºä¹‹é—´çš„å·®å¼‚ï¼Œå³KGçŸ¥è¯†çš„å½¢å¼åŒ–è¡¨è¿°ã€‚ä¸ºäº†è§£å†³è¿™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç­”æ¡ˆç›¸å…³çš„KGçŸ¥è¯†è½¬æ¢æ–¹æ³•ï¼Œå¯ä»¥å°†KGçŸ¥è¯†è½¬æ¢æˆæœ€æœ‰ç”¨çš„æ–‡æœ¬è¡¨è¿°ï¼Œä»¥ä¾¿äºKGQAä»»åŠ¡ã€‚åŸºäºè¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¢å¼ºLLMsçš„KG-to-Textæ¡†æ¶ï¼Œç”¨äºè§£å†³KGQAä»»åŠ¡ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªKGQA bencmarkä¸Šæ˜¾è‘—æé«˜äº†ç­”æ¡ˆå‡†ç¡®ç‡å’ŒçŸ¥è¯†å£°æ˜çš„ç”¨ç”¨æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Using-Artificial-Intelligence-for-the-Automation-of-Knitting-Patterns"><a href="#Using-Artificial-Intelligence-for-the-Automation-of-Knitting-Patterns" class="headerlink" title="Using Artificial Intelligence for the Automation of Knitting Patterns"></a>Using Artificial Intelligence for the Automation of Knitting Patterns</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11202">http://arxiv.org/abs/2309.11202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uduak Uboh</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†åˆ¤æ–­ä½¿ç”¨è‡ªåŠ¨åŒ–ç³»ç»Ÿæ¥åˆ†ç±»é”¦çº¹ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†æ•°æ®æ‰©å±•å’Œä¼ è¾“å­¦ä¹ æŠ€æœ¯ï¼Œé‡‡ç”¨äº†inception ResNet-V2ä½œä¸ºç‰¹å¾æå–å’Œåˆ†ç±»ç®—æ³•ã€‚</li>
<li>results: æ¨¡å‹çš„è¯„ä¼°ç»“æœæ˜¾ç¤ºäº†é«˜çš„æ¨¡å‹ç²¾åº¦ã€ç²¾åº¦ã€å›å½’ç‡å’ŒF1åˆ†æ•°ï¼Œè€Œä¸”å¤§å¤šæ•°ç±»çš„AUCåˆ†æ•°åœ¨(0.7-0.9)çš„èŒƒå›´å†…ã€‚<details>
<summary>Abstract</summary>
Knitting patterns are a crucial component in the creation and design of knitted materials. Traditionally, these patterns were taught informally, but thanks to advancements in technology, anyone interested in knitting can use the patterns as a guide to start knitting. Perhaps because knitting is mostly a hobby, with the exception of industrial manufacturing utilising specialised knitting machines, the use of Al in knitting is less widespread than its application in other fields. However, it is important to determine whether knitted pattern classification using an automated system is viable. In order to recognise and classify knitting patterns. Using data augmentation and a transfer learning technique, this study proposes a deep learning model. The Inception ResNet-V2 is the main feature extraction and classification algorithm used in the model. Metrics like accuracy, logarithmic loss, F1-score, precision, and recall score were used to evaluate the model. The model evaluation's findings demonstrate high model accuracy, precision, recall, and F1 score. In addition, the AUC score for majority of the classes was in the range (0.7-0.9). A comparative analysis was done using other pretrained models and a ResNet-50 model with transfer learning and the proposed model evaluation results surpassed all others. The major limitation for this project is time, as with more time, there might have been better accuracy over a larger number of epochs.
</details>
<details>
<summary>æ‘˜è¦</summary>
é’ˆè„Šå›¾æ¡ˆæ˜¯åˆ›ä½œå’Œè®¾è®¡é’ˆè„Šææ–™çš„å…³é”®ç»„ä»¶ã€‚åœ¨è¿‡å»ï¼Œè¿™äº›å›¾æ¡ˆé€šå¸¸æ˜¯é€šè¿‡å£è¿°ä¼ æˆçš„ï¼Œä½†ç°åœ¨éšç€æŠ€æœ¯çš„è¿›æ­¥ï¼Œä»»ä½•æ„Ÿå…´è¶£çš„äººéƒ½å¯ä»¥ä½¿ç”¨è¿™äº›å›¾æ¡ˆä½œä¸ºæŒ‡å—å¼€å§‹é’ˆè„Šã€‚ç”±äºé’ˆè„Šä¸»è¦æ˜¯ä¸€é¡¹å…´è¶£çˆ±å¥½ï¼Œé™¤äº†ç‰¹æ®Šé’ˆè„Šæœºå™¨åœ¨å·¥ä¸šç”Ÿäº§ä¸­ä½¿ç”¨å¤–ï¼Œä½¿ç”¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨é’ˆè„Šä¸­çš„åº”ç”¨èŒƒå›´ç›¸å¯¹è¾ƒå°‘ã€‚ç„¶è€Œï¼Œæ˜¯å¦å¯ä»¥ä½¿ç”¨è‡ªåŠ¨åŒ–ç³»ç»Ÿæ¥åˆ†ç±»é’ˆè„Šå›¾æ¡ˆæ˜¯ä¸€ä¸ªé‡è¦çš„é—®é¢˜ã€‚ä¸ºäº†è¯†åˆ«å’Œåˆ†ç±»é’ˆè„Šå›¾æ¡ˆï¼Œè¿™ä¸ªç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ä½¿ç”¨Inception ResNet-V2ç®—æ³•ä½œä¸ºä¸»è¦ç‰¹å¾æå–å’Œåˆ†ç±»ç®—æ³•ã€‚å¯¹æ¨¡å‹çš„è¯„ä¼°ç»“æœï¼Œå‘ç°æ¨¡å‹çš„å‡†ç¡®ç‡ã€ç²¾åº¦ã€å‡†ç¡®ç‡ã€å’ŒF1åˆ†æ•°å‡è¾¾åˆ°äº†é«˜æ°´å¹³ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°ç±»åˆ«çš„AUCåˆ†æ•°éƒ½åœ¨(0.7-0.9)ä¹‹é—´ã€‚ä¸å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹å’ŒResNet-50æ¨¡å‹è¿›è¡Œæ¯”è¾ƒåˆ†æåï¼Œè¿™ä¸ªç ”ç©¶çš„è¯„ä¼°ç»“æœè¶…è¿‡äº†å…¶ä»–æ‰€æœ‰ã€‚è¯¥é¡¹ç›®çš„ä¸»è¦é™åˆ¶æ˜¯æ—¶é—´ï¼Œå¦‚æœæœ‰æ›´å¤šçš„æ—¶é—´ï¼Œå¯èƒ½ä¼šåœ¨æ›´å¤šçš„è½®æ¬¡ä¸Šå¾—åˆ°æ›´é«˜çš„å‡†ç¡®ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="When-to-Trust-AI-Advances-and-Challenges-for-Certification-of-Neural-Networks"><a href="#When-to-Trust-AI-Advances-and-Challenges-for-Certification-of-Neural-Networks" class="headerlink" title="When to Trust AI: Advances and Challenges for Certification of Neural Networks"></a>When to Trust AI: Advances and Challenges for Certification of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11196">http://arxiv.org/abs/2309.11196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Kwiatkowska, Xiyue Zhang</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æ¢è®¨å¦‚ä½•ç¡®ä¿äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å†³ç­–çš„å®‰å…¨æ€§ï¼Œä»¥ä¾¿åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨AIæŠ€æœ¯ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†è¯æ˜å’Œè§£é‡Šæ€§æŠ€æœ¯æ¥ç¡®ä¿AIå†³ç­–çš„å®‰å…¨æ€§ã€‚</li>
<li>results: æœ¬æ–‡æå‡ºäº†æœªæ¥çš„æŒ‘æˆ˜å’Œç ”ç©¶æ–¹å‘ï¼Œä»¥ç¡®ä¿AIå†³ç­–çš„å®‰å…¨æ€§å’Œå¯é æ€§ã€‚<details>
<summary>Abstract</summary>
Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨è¿‡å»å‡ å¹´ä¸­å¾—åˆ°äº†å¿«é€Ÿå‘å±•ï¼Œç°åœ¨å®ƒå·²ç»å‡†å¤‡å¥½åœ¨å„ç§åº”ç”¨ä¸­ä½¿ç”¨ï¼Œå¦‚è‡ªä¸»ç³»ç»Ÿã€åŒ»ç–—è¯Šæ–­å’Œè‡ªç„¶è¯­è¨€å¤„ç†ã€‚è™½ç„¶åœ¨å®é™…åº”ç”¨ä¸­æ—©æœŸé‡‡ç”¨AIæŠ€æœ¯æœ‰ä¸€äº›é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ç¥ç»ç½‘ç»œå¯èƒ½å­˜åœ¨ä¸ç¨³å®šæ€§å’Œå¯é æ€§é—®é¢˜ï¼Œä»¥åŠå¯èƒ½å—åˆ°æ•Œæ„çš„ç¤ºä¾‹çš„å½±å“ã€‚åœ¨é•¿æœŸæ¥çœ‹ï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘é€‚å½“çš„å®‰å…¨ä¿éšœæŠ€æœ¯ï¼Œä»¥é™ä½å¯é¢„é˜²çš„ç³»ç»Ÿå¤±æ•ˆçš„å¯èƒ½æ€§ï¼Œå¹¶ç¡®ä¿AIå†³ç­–çš„å¯é æ€§ã€‚æœ¬æ–‡å°†å…³æ³¨è¯ä¹¦å’Œè§£é‡Šæ€§ï¼Œæä¾›äº†å®‰å…¨AIå†³ç­–çš„æŠ€æœ¯ensureçš„æ¦‚è¿°ï¼Œå¹¶è®¨è®ºæœªæ¥çš„æŒ‘æˆ˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Long-tail-Augmented-Graph-Contrastive-Learning-for-Recommendation"><a href="#Long-tail-Augmented-Graph-Contrastive-Learning-for-Recommendation" class="headerlink" title="Long-tail Augmented Graph Contrastive Learning for Recommendation"></a>Long-tail Augmented Graph Contrastive Learning for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11177">http://arxiv.org/abs/2309.11177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/im0qianqian/LAGCL">https://github.com/im0qianqian/LAGCL</a></li>
<li>paper_authors: Qian Zhao, Zhengwei Wu, Zhiqiang Zhang, Jun Zhou</li>
<li>for: æé«˜æ¨èç³»ç»Ÿä¸­Graph Convolutional Networks (GCNs)çš„æ€§èƒ½ï¼Œ Address the data sparsity issue in real-world scenarios.</li>
<li>methods: ä½¿ç”¨contrastive learningæ–¹æ³•ï¼Œå¹¶ introduce learnable long-tail augmentation approach to enhance tail nodesï¼Œ generate contrastive views based on the resulting augmented graph.</li>
<li>results: å¯¹ä¸‰ä¸ª benchmark datasetè¿›è¡Œäº†extensive experimentsï¼Œdemonstrate the significant improvement in performance of our model over the state-of-the-artsï¼Œfurther analyses demonstrate the uniformity of learned representations and the superiority of LAGCL on long-tail performance.<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) has demonstrated promising results for recommender systems, as they can effectively leverage high-order relationship. However, these methods usually encounter data sparsity issue in real-world scenarios. To address this issue, GCN-based recommendation methods employ contrastive learning to introduce self-supervised signals. Despite their effectiveness, these methods lack consideration of the significant degree disparity between head and tail nodes. This can lead to non-uniform representation distribution, which is a crucial factor for the performance of contrastive learning methods. To tackle the above issue, we propose a novel Long-tail Augmented Graph Contrastive Learning (LAGCL) method for recommendation. Specifically, we introduce a learnable long-tail augmentation approach to enhance tail nodes by supplementing predicted neighbor information, and generate contrastive views based on the resulting augmented graph. To make the data augmentation schema learnable, we design an auto drop module to generate pseudo-tail nodes from head nodes and a knowledge transfer module to reconstruct the head nodes from pseudo-tail nodes. Additionally, we employ generative adversarial networks to ensure that the distribution of the generated tail/head nodes matches that of the original tail/head nodes. Extensive experiments conducted on three benchmark datasets demonstrate the significant improvement in performance of our model over the state-of-the-arts. Further analyses demonstrate the uniformity of learned representations and the superiority of LAGCL on long-tail performance. Code is publicly available at https://github.com/im0qianqian/LAGCL
</details>
<details>
<summary>æ‘˜è¦</summary>
å›¾åƒ convolutional networks (GCNs) åœ¨æ¨èç³»ç»Ÿä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥æœ‰æ•ˆåˆ©ç”¨é«˜é˜¶å…³ç³»ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸åœ¨å®é™…åœºæ™¯ä¸­é‡åˆ°æ•°æ®ç¨€ç¼ºé—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒGCN åŸºäºçš„æ¨èæ–¹æ³•ä½¿ç”¨å¯¹ç…§å­¦ä¹ å¼•å…¥è‡ªæˆ‘è¶…visedä¿¡å·ã€‚å°½ç®¡å®ƒä»¬æœ‰æ•ˆï¼Œä½†æ˜¯å®ƒä»¬å¿½è§†äº†ä¸»è¦åº¦å·®çš„é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´éå‡è¡¡çš„è¡¨ç¤ºåˆ†å¸ƒï¼Œè¿™æ˜¯å¯¹å¯¹ç…§å­¦ä¹ æ–¹æ³•çš„è¡¨ç°éå¸¸é‡è¦çš„å› ç´ ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é•¿å°¾å¢å¼ºå›¾åƒå¯¹ç…§å­¦ä¹ ï¼ˆLAGCLï¼‰æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥å¯å­¦ä¹ çš„é•¿å°¾å¢å¼ºapproachï¼Œé€šè¿‡é¢„æµ‹é‚»å±…ä¿¡æ¯æ¥å¢å¼ºå°¾èŠ‚ç‚¹ï¼Œå¹¶åŸºäºæ‰€å¾—åˆ°çš„æ‰©å±•å›¾åƒç”Ÿæˆå¯¹ç…§è§†å›¾ã€‚ä¸ºä½¿æ•°æ®å¢å¼º schema å­¦ä¹ å¯èƒ½ï¼Œæˆ‘ä»¬è®¾è®¡äº†è‡ªåŠ¨Dropæ¨¡å—ï¼Œå°†å¤´èŠ‚ç‚¹è½¬åŒ–ä¸º pseudo-tail èŠ‚ç‚¹ï¼Œå¹¶è®¾è®¡äº†çŸ¥è¯†ä¼ é€’æ¨¡å—ï¼Œå°† pseudo-tail èŠ‚ç‚¹è¿˜åŸä¸ºå¤´èŠ‚ç‚¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œç¡®ä¿ç”Ÿæˆçš„å°¾/å¤´èŠ‚ç‚¹çš„åˆ†å¸ƒä¸åŸå§‹çš„å°¾/å¤´èŠ‚ç‚¹çš„åˆ†å¸ƒä¸€è‡´ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ ‡å‡†æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ¨¡å‹åœ¨ç°çŠ¶ä¸Šçš„æ˜¾è‘—æ”¹è¿›ã€‚è¿›ä¸€æ­¥çš„åˆ†æä¹Ÿè¡¨æ˜äº†æˆ‘ä»¬å­¦ä¹ çš„è¡¨ç¤ºçš„å‡åŒ€æ€§å’Œæˆ‘ä»¬å¯¹é•¿å°¾æ€§èƒ½çš„ä¼˜åŠ¿ã€‚ä»£ç å¯ä»¥åœ¨https://github.com/im0qianqian/LAGCL ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Are-Large-Language-Models-Really-Robust-to-Word-Level-Perturbations"><a href="#Are-Large-Language-Models-Really-Robust-to-Word-Level-Perturbations" class="headerlink" title="Are Large Language Models Really Robust to Word-Level Perturbations?"></a>Are Large Language Models Really Robust to Word-Level Perturbations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11166">http://arxiv.org/abs/2309.11166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Harry-mic/TREval">https://github.com/Harry-mic/TREval</a></li>
<li>paper_authors: Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ç§ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰ç”¨æ€§å’Œå¯é æ€§çš„æ–°æ–¹æ³•ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºé¢„è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„è¯„ä¼°æ–¹æ³•ï¼Œç§°ä¸ºTREvalï¼Œç”¨äºè¯„ä¼°LLMçš„å¯é æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹æ›´åŠ å›°éš¾çš„å¼€æ”¾é—®é¢˜æ—¶ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒTREvalå¯ä»¥å‡†ç¡®åœ°è¯„ä¼°LLMçš„å¯é æ€§ï¼Œå¹¶ä¸”å‘ç°LLMç»å¸¸å—åˆ°å•è¯æ°´å¹³çš„å¹²æ‰°ï¼Œè¿™ç§å¹²æ‰°åœ¨æ—¥å¸¸è¯­è¨€ä½¿ç”¨ä¸­å¾ˆå¸¸è§ã€‚å¦å¤–ï¼Œç ”ç©¶å‘ç°ï¼Œåœ¨è¿›è¡Œç»ƒä¹ å’Œå¼ºåŒ–è®­ç»ƒåï¼ŒLLMçš„å¯é æ€§å¾€å¾€ä¼šä¸‹é™ã€‚<details>
<summary>Abstract</summary>
The swift advancement in the scale and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the robustness of LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Our extensive empirical experiments have demonstrated that TREval provides an accurate method for evaluating the robustness of an LLM, especially when faced with more challenging open questions. Furthermore, our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations, which are commonplace in daily language usage. Notably, we were surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted. The code of TREval is available in https://github.com/Harry-mic/TREval.
</details>
<details>
<summary>æ‘˜è¦</summary>
Large Language Models (LLMs) çš„å¿«é€Ÿå‘å±•å’Œèƒ½åŠ›æé«˜ï¼Œä½¿å…¶æˆä¸ºè®¸å¤šä¸‹æ¸¸ä»»åŠ¡çš„ä¼˜ç§€å·¥å…·ã€‚é™¤äº†æé«˜æ€§èƒ½å’Œé¿å…æŸäº›æç¤ºå¯¼è‡´çš„æš´åŠ›åé¦ˆå¤–ï¼Œä¸ºäº†ç¡®ä¿ LLM çš„è´£ä»»ï¼Œä¹Ÿå¼•èµ·äº†ä¸€äº›å…³æ³¨ã€‚ç°æœ‰çš„è¯„ä¼°æ–¹æ³•ä¸»è¦åŸºäºå·²ç»å®šä¹‰çš„ä¼ ç»Ÿé—®ç­”æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¹¶ä¸ç¬¦åˆå½“ä»£ LLM çš„ä¼˜ç§€ç”Ÿæˆèƒ½åŠ›ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆç†è¯„ä¼°æ–¹æ³•ï¼Œåˆ©ç”¨é¢„è®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ä½œä¸ºè¯Šæ–­å·¥å…·æ¥è¯„ä¼° LLM çš„ robustnessï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º TREvaLã€‚æˆ‘ä»¬çš„å¹¿æ³›çš„å®éªŒè¯æ˜äº† TREval èƒ½å¤Ÿå‡†ç¡®åœ°è¯„ä¼° LLM çš„ robustnessï¼Œç‰¹åˆ«æ˜¯é¢å¯¹æ›´åŠ å›°éš¾çš„å¼€æ”¾é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒLLM  Ñ‡Ğ°ÑÑ‚Ğ¾ä¼šå—åˆ°å•è¯æ°´å¹³çš„æ‰°åŠ¨ï¼Œè¿™äº›æ‰°åŠ¨åœ¨æ—¥å¸¸è¯­è¨€ä½¿ç”¨ä¸­å¾ˆå¸¸è§ã€‚æ„å¤–åœ°ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåœ¨ fine-tuning (SFT å’Œ RLHF) è¿‡ç¨‹ä¸­ï¼ŒLLM çš„ Robustness å¾€å¾€å‡é€€ã€‚TREval çš„ä»£ç å¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°ï¼šhttps://github.com/Harry-mic/TREvalã€‚
</details></li>
</ul>
<hr>
<h2 id="ProtoExplorer-Interpretable-Forensic-Analysis-of-Deepfake-Videos-using-Prototype-Exploration-and-Refinement"><a href="#ProtoExplorer-Interpretable-Forensic-Analysis-of-Deepfake-Videos-using-Prototype-Exploration-and-Refinement" class="headerlink" title="ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement"></a>ProtoExplorer: Interpretable Forensic Analysis of Deepfake Videos using Prototype Exploration and Refinement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11155">http://arxiv.org/abs/2309.11155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Merel de Leeuw den Bouter, Javier Lloret Pardo, Zeno Geradts, Marcel Worring</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯è¯»æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é«˜åº¦ç«äº‰çš„åº”ç”¨åœºæ™¯ä¸‹ã€‚</li>
<li>methods: è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ä¸ªå¯è§†åŒ–åˆ†æè¿‡ç¨‹æ¨¡å‹ï¼Œå¹¶åŸºäºè¿™ä¸ªæ¨¡å‹æå‡ºäº†ä¸€ä¸ªåä¸ºProtoExplorerçš„å¯è§†åŒ–åˆ†æç³»ç»Ÿï¼Œç”¨äºæ¢ç´¢å’Œä¿®æ”¹åŸºäºåŸå‹çš„ä¼ªåŠ¨æ€æ£€æµ‹æ¨¡å‹ã€‚</li>
<li>results: è¿™ç¯‡æ–‡ç« é€è¿‡å¯¹å®é™…åº”ç”¨åœºæ™¯è¿›è¡Œè¯„ä¼°ï¼Œç¡®è®¤äº†è¿™ä¸ªæ–¹æ³•çš„å¯è¡Œæ€§å’Œæœ‰æ•ˆæ€§ã€‚<details>
<summary>Abstract</summary>
In high-stakes settings, Machine Learning models that can provide predictions that are interpretable for humans are crucial. This is even more true with the advent of complex deep learning based models with a huge number of tunable parameters. Recently, prototype-based methods have emerged as a promising approach to make deep learning interpretable. We particularly focus on the analysis of deepfake videos in a forensics context. Although prototype-based methods have been introduced for the detection of deepfake videos, their use in real-world scenarios still presents major challenges, in that prototypes tend to be overly similar and interpretability varies between prototypes. This paper proposes a Visual Analytics process model for prototype learning, and, based on this, presents ProtoExplorer, a Visual Analytics system for the exploration and refinement of prototype-based deepfake detection models. ProtoExplorer offers tools for visualizing and temporally filtering prototype-based predictions when working with video data. It disentangles the complexity of working with spatio-temporal prototypes, facilitating their visualization. It further enables the refinement of models by interactively deleting and replacing prototypes with the aim to achieve more interpretable and less biased predictions while preserving detection accuracy. The system was designed with forensic experts and evaluated in a number of rounds based on both open-ended think aloud evaluation and interviews. These sessions have confirmed the strength of our prototype based exploration of deepfake videos while they provided the feedback needed to continuously improve the system.
</details>
<details>
<summary>æ‘˜è¦</summary>
é«˜åº¦çš„åœºæ™¯ä¸­ï¼Œå¯ä»¥æä¾›äººç±»å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯éå¸¸é‡è¦çš„ã€‚è¿™ç§æƒ…å†µæ›´åŠ çœŸå®ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤æ‚çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œå…¶ä¸­æœ‰å¾ˆå¤šå¯è°ƒå‚æ•°ã€‚æœ€è¿‘ï¼ŒåŸå‹åŸºæ–¹æ³•åœ¨ä½¿å¾—æ·±åº¦å­¦ä¹ å¯è§£é‡Šæ–¹é¢è¡¨ç°å‡ºäº†æ‰å®çš„æŠ‘åˆ¶åŠ›ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨æ·±åº¦å‡å½±åƒåœ¨æ³•åŒ»æ–¹é¢çš„åˆ†æã€‚è™½ç„¶åŸå‹åŸºæ–¹æ³•å·²ç»åº”ç”¨äºæ·±åº¦å‡å½±åƒçš„æ£€æµ‹ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶å­˜åœ¨ä¸»è¦æŒ‘æˆ˜ï¼Œå³åŸå‹å¾€å¾€ç›¸ä¼¼ï¼Œè§£é‡Šæ€§ Ğ¼ĞµĞ¶Ğ´ÑƒåŸå‹å¼‚å¸¸ä¸ä¸€è‡´ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¯è§åˆ†æè¿‡ç¨‹æ¨¡å‹ï¼Œå¹¶åŸºäºè¿™ç§æ¨¡å‹æå‡ºäº†ProtoExplorerï¼Œä¸€ç§å¯è§åˆ†æç³»ç»Ÿï¼Œç”¨äºæ·±åº¦å‡å½±åƒæ£€æµ‹æ¨¡å‹çš„æ¢ç´¢å’Œç»†åŒ–ã€‚ProtoExploreræä¾›äº†è§†è§‰åˆ†æå’Œè§†é¢‘æ•°æ®ä¸­çš„æ—¶é—´æ»¤æ³¢åŠŸèƒ½ï¼Œå¯ä»¥è¯†åˆ«å’Œåˆ†ææ·±åº¦å‡å½±åƒã€‚å®ƒè¿˜å¯ä»¥é€šè¿‡äº¤äº’åˆ é™¤å’Œæ›¿æ¢åŸå‹æ¥å®ç°æ›´åŠ å¯è§£é‡Šå’Œä¸åæ‰§çš„é¢„æµ‹ï¼ŒåŒæ—¶ä¿æŒæ£€æµ‹ç²¾åº¦ã€‚ç³»ç»Ÿé’ˆå¯¹æ³•åŒ»ä¸“å®¶è¿›è¡Œäº†å¤šè½®è¯„ä¼°ï¼ŒåŒ…æ‹¬å¼€æ”¾å¼æ€ç»´å›ç­”è¯„ä¼°å’Œé¢è¯•ã€‚è¿™äº›è¯„ä¼°è¿‡ç¨‹ç¡®è®¤äº†æˆ‘ä»¬çš„åŸå‹åŸº exploreæ·±åº¦å‡å½±åƒçš„ä¼˜åŠ¿ï¼ŒåŒæ—¶æä¾›äº†éœ€è¦ä¸æ–­æ”¹è¿›ç³»ç»Ÿçš„åé¦ˆã€‚
</details></li>
</ul>
<hr>
<h2 id="CoT-BERT-Enhancing-Unsupervised-Sentence-Representation-through-Chain-of-Thought"><a href="#CoT-BERT-Enhancing-Unsupervised-Sentence-Representation-through-Chain-of-Thought" class="headerlink" title="CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought"></a>CoT-BERT: Enhancing Unsupervised Sentence Representation through Chain-of-Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11143">http://arxiv.org/abs/2309.11143</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZBWpro/CoT-BERT">https://github.com/ZBWpro/CoT-BERT</a></li>
<li>paper_authors: Bowen Zhang, Kehua Chang, Chunping Li</li>
<li>for: æé«˜ä¸supervised sentence representation learningçš„æ€§èƒ½ï¼Œå°è¯•ä½¿ç”¨é“¾æ¡æ€ç»´æ¥è§£é”é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„æ½œåœ¨èƒ½åŠ›ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•ï¼Œé¦–å…ˆä½¿ç”¨ç†è§£é˜¶æ®µå¯¹è¾“å…¥å¥å­è¿›è¡Œç†è§£ï¼Œç„¶åä½¿ç”¨æ‘˜è¦é˜¶æ®µå¯¹è¾“å…¥å¥å­è¿›è¡Œæ‘˜è¦ï¼Œæœ€åä½¿ç”¨æ‘˜è¦é˜¶æ®µçš„è¾“å‡ºä½œä¸ºè¾“å…¥å¥å­çš„vectoråŒ–è¡¨ç¤ºã€‚åŒæ—¶ï¼Œå¯¹å†²çªå­¦ä¹ æŸå¤±å‡½æ•°å’Œæ¨¡æ¿å¹²æ‰°æŠ€æœ¯è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»¥æé«˜æç¤ºå·¥ç¨‹çš„æ€§èƒ½ã€‚</li>
<li>results: å¯¹å¤šä¸ª robust baselineè¿›è¡Œäº†ä¸¥æ ¼çš„å®éªŒè¯æ˜ï¼Œå‘ç°CoT-BERTå¯ä»¥åœ¨ä¸éœ€è¦å…¶ä»–æ–‡æœ¬è¡¨ç¤ºæ¨¡å‹æˆ–å¤–éƒ¨æ•°æ®åº“çš„æƒ…å†µä¸‹ï¼Œä¸supervised sentence representation learningå…·æœ‰ç›¸åŒæˆ–æ›´é«˜çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Unsupervised sentence representation learning aims to transform input sentences into fixed-length vectors enriched with intricate semantic information while obviating the reliance on labeled data. Recent progress within this field, propelled by contrastive learning and prompt engineering, has significantly bridged the gap between unsupervised and supervised strategies. Nonetheless, the potential utilization of Chain-of-Thought, remains largely untapped within this trajectory. To unlock latent capabilities within pre-trained models, such as BERT, we propose a two-stage approach for sentence representation: comprehension and summarization. Subsequently, the output of the latter phase is harnessed as the vectorized representation of the input sentence. For further performance enhancement, we meticulously refine both the contrastive learning loss function and the template denoising technique for prompt engineering. Rigorous experimentation substantiates our method, CoT-BERT, transcending a suite of robust baselines without necessitating other text representation models or external databases.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸ç›‘ç£å¥å­è¡¨ç¤ºå­¦ä¹ ç›®æ ‡æ˜¯å°†è¾“å…¥å¥å­è½¬åŒ–ä¸ºå›ºå®šé•¿åº¦çš„å‘é‡ï¼Œå…·æœ‰ç»†è‡´çš„ semanticsä¿¡æ¯ï¼Œè€Œä¸éœ€è¦æ ‡æ³¨æ•°æ®ã€‚åœ¨è¿™ä¸ªé¢†åŸŸï¼Œæœ€è¿‘çš„è¿›å±•ï¼Œå—åˆ°å¯¹çŸ­æ–‡æœ¬æ£€æµ‹å’Œæå–æŠ€æœ¯çš„å½±å“ï¼Œå·²ç»å¤§å¹…åº¦å‡å°‘äº†ä¸ç›‘ç£å’Œç›‘ç£æ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚ç„¶è€Œï¼Œé“¾å¼æ€ç»´çš„æ½œåœ¨åº”ç”¨ï¼Œåœ¨è¿™ä¸ªè½¨è¿¹ä¸Šä»ç„¶å°šæœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚ä¸ºäº†è§£é”é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„å¼ºåŒ–ç‰¹æ€§ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤é˜¶æ®µæ–¹æ³•ï¼šç†è§£å’Œæ¦‚è¦ã€‚ç„¶åï¼Œåä¸€é˜¶æ®µçš„è¾“å‡ºè¢«ç”¨ä½œè¾“å…¥å¥å­çš„å‘é‡è¡¨ç¤ºã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ€§èƒ½ï¼Œæˆ‘ä»¬ä»”ç»†ä¿®æ”¹äº†å¯¹çŸ­æ–‡æœ¬æ£€æµ‹å’Œæå–æŠ€æœ¯çš„æƒé‡ï¼Œä»¥åŠæ¨¡æ¿å¹²æ‰°æŠ€æœ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼ŒCoT-BERTï¼Œåœ¨ä¸€ç³»åˆ—å¼ºå¤§çš„åŸºçº¿ä¸Šè¿›è¡Œäº†ä¸¥æ ¼çš„å®éªŒï¼Œå¹¶ä¸éœ€è¦å…¶ä»–æ–‡æœ¬è¡¨ç¤ºæ¨¡å‹æˆ–å¤–éƒ¨æ•°æ®åº“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Pseudo-Learning-for-Open-World-DeepFake-Attribution"><a href="#Contrastive-Pseudo-Learning-for-Open-World-DeepFake-Attribution" class="headerlink" title="Contrastive Pseudo Learning for Open-World DeepFake Attribution"></a>Contrastive Pseudo Learning for Open-World DeepFake Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11132">http://arxiv.org/abs/2309.11132</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution">https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution</a></li>
<li>paper_authors: Zhimin Sun, Shen Chen, Taiping Yao, Bangjie Yin, Ran Yi, Shouhong Ding, Lizhuang Ma</li>
<li>for: è¯„ä¼°æ·±ä¼ªæ£€æµ‹é¢†åŸŸä¸­åŒ¿åæ”»å‡»çš„éšè—è¿¹è±¡ï¼Œä»¥æ¨åŠ¨ç›¸å…³å‰æ²¿ç ”ç©¶ã€‚</li>
<li>methods: æå‡ºä¸€ä¸ªæ–°çš„è¯„ä¼°æŒ‡æ ‡é›†åˆcalled Open-World DeepFake Attributionï¼ˆOW-DFAï¼‰ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºå¯¹æ¯”å­¦ä¹ çš„novelæ¡†æ¶ named Contrastive Pseudo Learningï¼ˆCPLï¼‰ã€‚</li>
<li>results: ç»éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨OW-DFAä»»åŠ¡ä¸Šå…·æœ‰ä¼˜ç§€çš„è¡¨ç°ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¢å¼ºæ·±ä¼ªæ£€æµ‹é¢†åŸŸçš„å®‰å…¨æ€§ã€‚<details>
<summary>Abstract</summary>
The challenge in sourcing attribution for forgery faces has gained widespread attention due to the rapid development of generative techniques. While many recent works have taken essential steps on GAN-generated faces, more threatening attacks related to identity swapping or expression transferring are still overlooked. And the forgery traces hidden in unknown attacks from the open-world unlabeled faces still remain under-explored. To push the related frontier research, we introduce a new benchmark called Open-World DeepFake Attribution (OW-DFA), which aims to evaluate attribution performance against various types of fake faces under open-world scenarios. Meanwhile, we propose a novel framework named Contrastive Pseudo Learning (CPL) for the OW-DFA task through 1) introducing a Global-Local Voting module to guide the feature alignment of forged faces with different manipulated regions, 2) designing a Confidence-based Soft Pseudo-label strategy to mitigate the pseudo-noise caused by similar methods in unlabeled set. In addition, we extend the CPL framework with a multi-stage paradigm that leverages pre-train technique and iterative learning to further enhance traceability performance. Extensive experiments verify the superiority of our proposed method on the OW-DFA and also demonstrate the interpretability of deepfake attribution task and its impact on improving the security of deepfake detection area.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå¯¹äºä¼ªé€ çš„æŒ‘æˆ˜ï¼Œéšç€ç”ŸæˆæŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå·²ç»å—åˆ°äº†å¹¿æ³›çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œè®¸å¤šæœ€è¿‘çš„ç ”ç©¶ä»…å¯¹ç”Ÿæˆå™¨ç”Ÿæˆçš„é¢éƒ¨è¿›è¡Œäº†é‡è¦çš„æ­¥éª¤ï¼Œå°šæœªå……åˆ†å¤„ç†éšè—åœ¨æœªçŸ¥æ”»å‡»ä¸­çš„ä¼ªé€ è¿¹è±¡ã€‚ä¸ºäº†æ¨è¿›ç›¸å…³çš„å‰æ²¿ç ”ç©¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ bencmark å«åš Open-World DeepFake Attributionï¼ˆOW-DFAï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å¯¹ä¸åŒç±»å‹çš„ä¼ªé€ é¢éƒ¨è¿›è¡Œæƒé‡è¯„ä¼°ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º Contrastive Pseudo Learningï¼ˆCPLï¼‰çš„æ–°æ¡†æ¶ï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ä¸ªæ–¹æ³•æ¥è§£å†³é—®é¢˜ï¼š1ï¼‰å¼•å…¥å…¨çƒ-æœ¬åœ°æŠ•ç¥¨æ¨¡ç»„ï¼Œä»¥å¸®åŠ©ä¼ªé€ é¢éƒ¨çš„ä¸åŒæƒé‡åŒºåŸŸè¿›è¡Œæ•´åˆï¼›2ï¼‰è®¾è®¡ä¸€ç§åŸºäºä¿¡ä»»çš„è½¯å®šå¼æ ‡ç­¾ç­–ç•¥ï¼Œä»¥å‡å°‘ pseudo-noise å¯¹ä¸æ˜æ–‡ä»¶é›†çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°† CPL æ¡†æ¶æ‰©å±•ä¸ºå¤šé˜¶æ®µæ¨¡å‹ï¼Œåˆ©ç”¨é¢„è®­æŠ€æœ¯å’Œè¿­ä»£å­¦ä¹ æ¥è¿›ä¸€æ­¥å¢å¼º traceability æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºäº†æˆ‘ä»¬çš„ææ¡ˆæ–¹æ³•åœ¨ OW-DFA ä¸­çš„è¶…è¶Šæ€§å’Œæ·±åº¦ä¼ªé€ æ£€æµ‹é¢†åŸŸçš„è§£é‡Šæ€§ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Language-Oriented-Communication-with-Semantic-Coding-and-Knowledge-Distillation-for-Text-to-Image-Generation"><a href="#Language-Oriented-Communication-with-Semantic-Coding-and-Knowledge-Distillation-for-Text-to-Image-Generation" class="headerlink" title="Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation"></a>Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11127">http://arxiv.org/abs/2309.11127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyelin Nam, Jihong Park, Jinho Choi, Mehdi Bennis, Seong-Lyun Kim</li>
<li>for: æå‡ºäº†ä¸€ç§è¯­è¨€å“åº”å¼ semantic communicationï¼ˆLSCï¼‰æ¡†æ¶ï¼Œç”¨äºæœºå™¨äººä¸äººç±»ä¹‹é—´çš„è¯­è¨€äº¤äº’ã€‚</li>
<li>methods: æå‡ºäº†ä¸‰ç§æ–°ç®—æ³•ï¼š1ï¼‰Semantic Source Codingï¼ˆSSCï¼‰ï¼Œå‹ç¼©æ–‡æœ¬æç¤ºä¸­çš„ä¸»è¦è¯è¯­ï¼Œä¿æŒæç¤ºçš„è¯­æ³•ç»“æ„å’Œä¸Šä¸‹æ–‡ï¼›2ï¼‰Semantic Channel Codingï¼ˆSCCï¼‰ï¼Œä½¿ç”¨é•¿è¯­è¨€åŒä¹‰è¯ä»£æ›¿ä¸»è¦è¯è¯­ï¼Œæé«˜å…é”™æ€§ï¼›3ï¼‰Semantic Knowledge Distillationï¼ˆSKDï¼‰ï¼Œé€šè¿‡åœ¨å­¦ä¹ Listenerçš„è¯­è¨€é£æ ¼ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œå¯å‘å¼å­¦ä¹ ï¼Œç”Ÿæˆé€‚åº”Listenerçš„æç¤ºã€‚</li>
<li>results: åœ¨è¿›è¡Œæ–‡æœ¬ç”Ÿæˆåˆ°å›¾åƒä»»åŠ¡ä¸­ï¼Œæè®®çš„æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ï¼Œå¹¶é™ä½é€šä¿¡é¢‘ç‡ï¼ŒåŒæ—¶æé«˜å¹²æ‰°é€šä¿¡é¢‘ç‡ä¸‹çš„Robustnessã€‚<details>
<summary>Abstract</summary>
By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive text-to-image generation, the proposed methods achieve higher perceptual similarities with fewer transmissions while enhancing robustness in noisy communication channels.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šè¿‡å°†æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œç”Ÿæˆæ¨¡å‹ä¸å‘å±•çš„è¯­ä¹‰é€šä¿¡ï¼ˆSCï¼‰ paradigmç»“åˆèµ·æ¥ï¼Œåœ¨æœ¬æ–‡æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯­è¨€å¯å‘å‹é€šä¿¡ï¼ˆLSCï¼‰æ¡†æ¶ã€‚åœ¨LSCä¸­ï¼Œæœºå™¨é€šè¿‡ä½¿ç”¨äººç±»è¯­è¨€æ¶ˆæ¯è¿›è¡Œé€šä¿¡ï¼Œè¿™äº›æ¶ˆæ¯å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯è¿›è¡Œè§£é‡Šå’Œä¿®æ”¹ï¼Œä»¥æé«˜SCçš„æ•ˆç‡ã€‚ä¸ºäº†è¯æ˜LSCçš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§æ–°ç®—æ³•ï¼š1ï¼‰ semanticsource codingï¼ˆSSCï¼‰ï¼Œå®ƒå‹ç¼©æ–‡æœ¬æç¤ºåˆ°å…¶ä¸»è¦å¤´è¯­è¨€ï¼Œä¿ç•™æç¤ºçš„è¯­æ³•ç»“æ„å’Œä¸Šä¸‹æ–‡ï¼›2ï¼‰ semantics channel codingï¼ˆSCCï¼‰ï¼Œå®ƒé€šè¿‡å°†ä¸»è¦å¤´è¯­è¨€æ›¿æ¢ä¸ºå…¶æ›´é•¿çš„åŒä¹‰è¯ï¼Œæé«˜äº†å¯¹é”™è¯¯çš„Robustnessï¼›3ï¼‰ semantics knowledge distillationï¼ˆSKDï¼‰ï¼Œå®ƒé€šè¿‡åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æ”¶è€…çš„è¯­è¨€é£æ ¼ï¼Œç”Ÿæˆé€‚åˆæ”¶è€…çš„å¯å‘å¼æ–‡æœ¬ã€‚åœ¨ä¸€ä¸ªè¿›æ­¥æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬çš„ææ¡ˆæ–¹æ³•å¯ä»¥å®ç°æ›´é«˜çš„æ„ŸçŸ¥ç›¸ä¼¼æ€§ï¼ŒåŒæ—¶å‡å°‘ä¼ è¾“é‡ï¼Œå¹¶åœ¨å™ªå£°é€šä¿¡é¢‘é“ä¸­æé«˜äº†Robustnessã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-Complete-Topology-Aware-Correlations-Between-Relations-for-Inductive-Link-Prediction"><a href="#Learning-Complete-Topology-Aware-Correlations-Between-Relations-for-Inductive-Link-Prediction" class="headerlink" title="Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction"></a>Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11528">http://arxiv.org/abs/2309.11528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Wang, Hanzhu Chen, Qitan Lv, Zhihao Shi, Jiajun Chen, Huarui He, Hongtao Xie, Yongdong Zhang, Feng Wu</li>
<li>for: æé«˜çŸ¥è¯†å›¾çš„å®Œæ•´æ€§å’Œå¯é æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨æ— éœ€çŸ¥é“å®ä½“çš„æƒ…å†µä¸‹é¢„æµ‹é“¾æ¥å…³ç³»ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§æ–°çš„å­å›¾åŸºäºæ–¹æ³•TACOï¼Œåˆ©ç”¨é€»è¾‘ç›¸å…³æ€§betweenå…³ç³»æ¥æ¨¡å‹å›¾STRUCTUREã€‚TACOæ–¹æ³•åŒ…æ‹¬ sevenç§topological patternï¼Œå¹¶é€šè¿‡å…³ç³»ç›¸å…³ç½‘ç»œï¼ˆRCNï¼‰æ¥å­¦ä¹ æ¯ç§patternçš„é‡è¦æ€§ã€‚</li>
<li>results: å¯¹æ¯” existed state-of-the-artæ–¹æ³•ï¼ŒTACOæ–¹æ³•åœ¨é¢„æµ‹é“¾æ¥å…³ç³»ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†superiorçš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Inductive link prediction -- where entities during training and inference stages can be different -- has shown great potential for completing evolving knowledge graphs in an entity-independent manner. Many popular methods mainly focus on modeling graph-level features, while the edge-level interactions -- especially the semantic correlations between relations -- have been less explored. However, we notice a desirable property of semantic correlations between relations is that they are inherently edge-level and entity-independent. This implies the great potential of the semantic correlations for the entity-independent inductive link prediction task. Inspired by this observation, we propose a novel subgraph-based method, namely TACO, to model Topology-Aware COrrelations between relations that are highly correlated to their topological structures within subgraphs. Specifically, we prove that semantic correlations between any two relations can be categorized into seven topological patterns, and then proposes Relational Correlation Network (RCN) to learn the importance of each pattern. To further exploit the potential of RCN, we propose Complete Common Neighbor induced subgraph that can effectively preserve complete topological patterns within the subgraph. Extensive experiments demonstrate that TACO effectively unifies the graph-level information and edge-level interactions to jointly perform reasoning, leading to a superior performance over existing state-of-the-art methods for the inductive link prediction task.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¾"\induction link prediction" -- åœ¨è®­ç»ƒå’Œæ¨ç†é˜¶æ®µä¹‹é—´çš„å®ä½“å¯ä»¥ä¸åŒ -- å·²ç»å±•ç°å‡ºäº†å®Œå–„ evolving knowledge graphs çš„å·¨å¤§æ½œåŠ›ã€‚è®¸å¤šå—æ¬¢è¿çš„æ–¹æ³•ä¸»è¦å…³æ³¨å›¾çº§ç‰¹å¾ï¼Œè€Œå›¾çº§äº¤äº’ -- ç‰¹åˆ«æ˜¯å…³ç³»ä¹‹é—´çš„semantic correlation -- åˆ™å¾—åˆ°äº†æ›´å°‘çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ³¨æ„åˆ°äº†semantic correlation between relations çš„ä¸€ä¸ªæ„‰æ‚¦æ€§è´¨ï¼Œå³å®ƒä»¬æ˜¯è‡ªç„¶çš„edge-levelå’Œå®ä½“ç‹¬ç«‹çš„ã€‚è¿™æ„å‘³ç€semantic correlation between relations å…·æœ‰æ½œåœ¨çš„å¾ˆå¤§æ½œåŠ› Ğ´Ğ»Ñå®ä½“ç‹¬ç«‹çš„ inductive link prediction ä»»åŠ¡ã€‚é’ˆå¯¹è¿™ä¸€è§‚å¯Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å­å›¾åŸºäºæ–¹æ³•ï¼Œå³ TACOï¼Œç”¨äºæ¨¡å‹ topology-aware COrrelations between relations ï¼ˆTACOï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜äº†ä»»æ„ä¸¤ä¸ªå…³ç³»çš„semantic correlationå¯ä»¥è¢«åˆ†ç±»ä¸ºä¸ƒç§ topological patternï¼Œå¹¶æå‡ºäº† Relational Correlation Network (RCN) æ¥å­¦ä¹ æ¯ç§patternçš„é‡è¦æ€§ã€‚ä¸ºäº†æ›´å¥½åœ°åˆ©ç”¨ RCn çš„æ½œåŠ›ï¼Œæˆ‘ä»¬æå‡ºäº† Complete Common Neighbor induced subgraphï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä¿ç•™å®Œæ•´çš„ topological patterns within the subgraphã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒTACO èƒ½å¤Ÿå…·æœ‰å›¾çº§ä¿¡æ¯å’Œè¾¹çº§äº¤äº’çš„æ•´åˆï¼Œä»¥jointly perform reasoningï¼Œä»è€Œå¯¹ inductive link prediction ä»»åŠ¡ Ğ´Ğ¾ÑÑ‚Ğ¸å¾—æ›´é«˜çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="TrueLearn-A-Python-Library-for-Personalised-Informational-Recommendations-with-Implicit-Feedback"><a href="#TrueLearn-A-Python-Library-for-Personalised-Informational-Recommendations-with-Implicit-Feedback" class="headerlink" title="TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback"></a>TrueLearn: A Python Library for Personalised Informational Recommendations with (Implicit) Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11527">http://arxiv.org/abs/2309.11527</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Qiu, Karim Djemili, Denis Elezi, Aaneel Shalman, MarÃ­a PÃ©rez-Ortiz, Sahan Bulathwela</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†ä»‹ç»TrueLearn Pythonåº“ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºåœ¨çº¿å­¦ä¹  bayesianæ¨¡å‹çš„æ•™è‚²ï¼ˆæˆ–æ›´å¹¿æ³›çš„ä¿¡æ¯ï¼‰æ¨èç³»ç»Ÿçš„å»ºæ„ã€‚</li>
<li>methods: è¿™ä¸ªå®¶æ—æ¨¡å‹é‡‡ç”¨äº†â€å¼€æ”¾å­¦ä¹ â€æ¦‚å¿µï¼Œä½¿ç”¨äººç±»å¯ç†è§£çš„ç”¨æˆ·è¡¨ç¤ºã€‚ä¸ºäº†æé«˜å¯è¯»æ€§å’Œè®©ç”¨æˆ·æ§åˆ¶è‡ªå·±çš„æ¨¡å‹ï¼ŒTrueLearnåº“è¿˜åŒ…å«äº†ä¸åŒçš„è¡¨ç¤ºæ–¹å¼ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·è§†è§‰åŒ–è‡ªå·±çš„å­¦ä¹ è€…æ¨¡å‹ã€‚</li>
<li>results: è®ºæ–‡é™„å½•äº†ä¸€ä¸ªå·²ç»å…¬å¸ƒçš„éšå¼åé¦ˆæ•™è‚²æ•°æ®é›†ï¼Œå¹¶æä¾›äº†è¯„ä»·æŒ‡æ ‡æ¥è¡¡é‡æ¨¡å‹çš„æ€§èƒ½ã€‚TrueLearnåº“çš„å¹¿æ³›çš„æ–‡æ¡£å’Œä»£ç ç¤ºä¾‹ä½¿å¾—æœºå™¨å­¦ä¹ å¼€å‘è€…å’Œæ•™è‚²æ•°æ®æŒ–æ˜å’Œå­¦ä¹ åˆ†æä¸“å®¶å¯ä»¥å¾ˆå®¹æ˜“åœ°ä½¿ç”¨è¿™ä¸ªåº“ã€‚<details>
<summary>Abstract</summary>
This work describes the TrueLearn Python library, which contains a family of online learning Bayesian models for building educational (or more generally, informational) recommendation systems. This family of models was designed following the "open learner" concept, using humanly-intuitive user representations. For the sake of interpretability and putting the user in control, the TrueLearn library also contains different representations to help end-users visualise the learner models, which may in the future facilitate user interaction with their own models. Together with the library, we include a previously publicly released implicit feedback educational dataset with evaluation metrics to measure the performance of the models. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational data mining and learning analytic practitioners. The library and the support documentation with examples are available at https://truelearn.readthedocs.io/en/latest.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä¸ªå·¥ä½œæè¿°äº†TrueLearn Pythonåº“ï¼Œè¯¥åº“åŒ…å«ä¸€å®¶åœ¨çº¿å­¦ä¹  bayesian æ¨¡å‹ï¼Œç”¨äºå»ºç«‹æ•™è‚²ï¼ˆæˆ–æ›´å¹¿æ³›åœ°è¯´ï¼Œä¿¡æ¯ï¼‰æ¨èç³»ç»Ÿã€‚è¿™å®¶æ¨¡å‹éµå¾ªâ€œå¼€æ”¾å­¦ä¹ â€æ¦‚å¿µï¼Œä½¿ç”¨äººç±»å¯ç†è§£çš„ç”¨æˆ·è¡¨ç¤ºã€‚ä¸ºäº†æé«˜å¯è§£æ€§å’Œè®©ç”¨æˆ·æ§åˆ¶ï¼ŒTrueLearn åº“è¿˜åŒ…å«äº†ä¸åŒçš„è¡¨ç¤ºï¼Œå¸®åŠ©ç»“æŸç”¨æˆ·å¯è§†åŒ–å­¦ä¹ è€…æ¨¡å‹ï¼Œä»¥ä¾¿æœªæ¥ä¸è‡ªå·±çš„æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†åœ¨çº¿å­¦ä¹ æ•™è‚²æ•°æ®é›†ï¼Œä»¥ä¾¿è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚TrueLearn åº“çš„æ–‡æ¡£å’Œä»£ç ç¤ºä¾‹ä½¿å¾—æœºå™¨å­¦ä¹ å¼€å‘è€…å’Œæ•™è‚²æ•°æ®æŒ–æ˜å’Œå­¦ä¹ åˆ†æä¸“ä¸šäººå£«å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ã€‚åº“å’Œæ”¯æŒæ–‡æ¡£ï¼Œä»¥åŠç¤ºä¾‹å¯ä»¥åœ¨ <https://truelearn.readthedocs.io/en/latest> ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="AttentionMix-Data-augmentation-method-that-relies-on-BERT-attention-mechanism"><a href="#AttentionMix-Data-augmentation-method-that-relies-on-BERT-attention-mechanism" class="headerlink" title="AttentionMix: Data augmentation method that relies on BERT attention mechanism"></a>AttentionMix: Data augmentation method that relies on BERT attention mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11104">http://arxiv.org/abs/2309.11104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Lewy, Jacek MaÅ„dziuk</li>
<li>for: è¿™ paper æ˜¯å…³äºå¦‚ä½•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸä¸­ä½¿ç”¨æ··åˆæ–¹æ³•è¿›è¡Œæ•°æ®å¢å¼ºçš„ç ”ç©¶ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ··åˆæ–¹æ³• called AttentionMixï¼Œå®ƒåŸºäºæ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åº”ç”¨äºä»»ä½•æ³¨æ„åŠ›åŸºäºæ¨¡å‹ã€‚</li>
<li>results: åœ¨ä¸‰ä¸ªæ ‡å‡†æƒ…æ„Ÿåˆ†ç±» dataset ä¸Šæµ‹è¯•ï¼ŒAttentionMix éƒ½è¶…è¿‡äº†ä¸¤ç§ Mixup æœºåˆ¶çš„å‚è€ƒæ–¹æ³•ä»¥åŠvanilla BERT æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œæ³¨æ„åŠ›ä¿¡æ¯å¯ä»¥æœ‰æ•ˆåœ°ç”¨äº NLP é¢†åŸŸä¸­çš„æ•°æ®å¢å¼ºã€‚<details>
<summary>Abstract</summary>
The Mixup method has proven to be a powerful data augmentation technique in Computer Vision, with many successors that perform image mixing in a guided manner. One of the interesting research directions is transferring the underlying Mixup idea to other domains, e.g. Natural Language Processing (NLP). Even though there already exist several methods that apply Mixup to textual data, there is still room for new, improved approaches. In this work, we introduce AttentionMix, a novel mixing method that relies on attention-based information. While the paper focuses on the BERT attention mechanism, the proposed approach can be applied to generally any attention-based model. AttentionMix is evaluated on 3 standard sentiment classification datasets and in all three cases outperforms two benchmark approaches that utilize Mixup mechanism, as well as the vanilla BERT method. The results confirm that the attention-based information can be effectively used for data augmentation in the NLP domain.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šæ··åˆæ–¹æ³•ã€‹åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²ç»è¯æ˜æ˜¯ä¸€ç§å¼ºå¤§çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæœ‰è®¸å¤šåç»§è€…åœ¨æŒ‡å¯¼ä¸‹è¿›è¡Œå›¾åƒæ··åˆã€‚ä¸€ä¸ªæœ‰è¶£çš„ç ”ç©¶æ–¹å‘æ˜¯å°†åŸºäºæ··åˆçš„æƒ³æ³•ä¼ é€’åˆ°å…¶ä»–é¢†åŸŸï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€‚è™½ç„¶ç°æœ‰ä¸€äº›åº”ç”¨æ··åˆåˆ°æ–‡æœ¬æ•°æ®çš„æ–¹æ³•ï¼Œä½†è¿˜æ˜¯æœ‰å¾ˆå¤šç©ºé—´ Ğ´Ğ»Ñæ–°çš„ã€æ”¹è¿›çš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ··åˆæ–¹æ³•ï¼Œå³å…³æ³¨æ··åˆï¼ˆAttentionMixï¼‰ã€‚è¿™ç§æ–¹æ³•åŸºäºå…³æ³¨ä¿¡æ¯ï¼Œè€Œpaperä¸­å…³æ³¨BERTçš„æ³¨æ„æœºåˆ¶ã€‚AttentionMixå¯ä»¥åº”ç”¨äºä»»ä½•å…³æ³¨åŸºäºæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨3ä¸ªæ ‡å‡†æƒ…æ„Ÿåˆ†ç±»datasetä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå¹¶åœ¨æ‰€æœ‰3ä¸ªæ¡ˆä¾‹ä¸­è¶…è¿‡äº†ä¸¤ä¸ªå‚è€ƒæ–¹æ³•å’Œvanilla BERTæ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œå…³æ³¨ä¿¡æ¯å¯ä»¥æœ‰æ•ˆåœ°ç”¨äºNLPé¢†åŸŸä¸­çš„æ•°æ®å¢å¼ºã€‚
</details></li>
</ul>
<hr>
<h2 id="A-New-Interpretable-Neural-Network-Based-Rule-Model-for-Healthcare-Decision-Making"><a href="#A-New-Interpretable-Neural-Network-Based-Rule-Model-for-Healthcare-Decision-Making" class="headerlink" title="A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making"></a>A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11101">http://arxiv.org/abs/2309.11101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrien Benamira, Tristan Guerand, Thomas Peyrin</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œå³ $\textit{Truth Table rules}$ï¼ˆTT-rulesï¼‰ï¼Œè¯¥æ¡†æ¶ç»“åˆç¥ç»ç½‘ç»œçš„é«˜æ€§èƒ½å’Œè§„åˆ™å‹æ¨¡å‹çš„å…¨é¢å’Œå‡†ç¡®è§£é‡Šæ€§è´¨ã€‚</li>
<li>methods: TT-rules åŸºäº $\textit{Truth Table nets}$ï¼ˆTTnetï¼‰ï¼Œä¸€ç§åˆå§‹ä¸ºå½¢å¼éªŒè¯è€Œå¼€å‘çš„æ·±åº¦ç¥ç»ç½‘ç»œå®¶æ—ã€‚é€šè¿‡ä»è®­ç»ƒè¿‡ç¨‹ä¸­æå–å…¨é¢å’Œå‡†ç¡®çš„è§„åˆ™é›† $\mathcal{R}$ï¼Œä»¥ä¾¿ä½¿å¾— TT-rules æ¨¡å‹èƒ½å¤Ÿå…·å¤‡å…¨é¢å’Œå‡†ç¡®çš„è§£é‡Šæ€§ã€‚</li>
<li>results: æˆ‘ä»¬å¯¹å¥åº·åº”ç”¨åœºæ™¯ä¸­çš„æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸ç°æœ‰çš„è§£é‡Šæ€§æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼ŒTT-rules èƒ½å¤Ÿè¾¾åˆ°ä¸å…¶ä»–è§£é‡Šæ€§æ–¹æ³•ç›¸å½“æˆ–æ›´é«˜çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨å¤§å‹è¡¨æ ¼æ•°æ®é›†ä¸Šè¿›è¡Œé€‚åº”ä¹Ÿæ˜¯å¯èƒ½çš„ã€‚ç‰¹åˆ«æ˜¯ï¼ŒTT-rules æˆä¸ºäº†é¦–ä¸ªèƒ½å¤Ÿé€‚åº”å¤§å‹è¡¨æ ¼æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªçœŸå®çš„ DNA æ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†å…·æœ‰è¶…è¿‡ 20K çš„ç‰¹å¾çš„è§£é‡Šæ€§æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
In healthcare applications, understanding how machine/deep learning models make decisions is crucial. In this study, we introduce a neural network framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks. TT-rules is built upon $\textit{Truth Table nets}$ (TTnet), a family of deep neural networks initially developed for formal verification. By extracting the necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for small to large tabular datasets. After outlining the framework, we evaluate TT-rules' performance on healthcare applications and compare it to state-of-the-art rule-based methods. Our results demonstrate that TT-rules achieves equal or higher performance compared to other interpretable methods. Notably, TT-rules presents the first accurate rule-based model capable of fitting large tabular datasets, including two real-life DNA datasets with over 20K features.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨åŒ»ç–—åº”ç”¨ä¸­ï¼Œç†è§£æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å†³ç­–æ–¹æ³•æ˜¯éå¸¸é‡è¦çš„ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç¥ç»ç½‘ç»œæ¡†æ¶ï¼Œç§°ä¸ºâ€œçœŸå®è¡¨æ ¼è§„åˆ™â€ï¼ˆTT-rulesï¼‰ï¼Œè¿™ç§æ¡†æ¶ç»“åˆäº†ç¥ç»ç½‘ç»œçš„é«˜æ€§èƒ½å’Œè§„åˆ™å‹æ¨¡å‹çš„å…¨é¢å’Œå‡†ç¡®è§£é‡Šæ€§è´¨ã€‚TT-rulesåŸºäºä¸€ç§åä¸ºâ€œçœŸå®è¡¨æ ¼ç½‘ç»œâ€ï¼ˆTTnetï¼‰çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œè¯¥ç½‘ç»œæœ€åˆæ˜¯ä¸ºäº†æ­£å¼éªŒè¯è€Œå¼€å‘çš„ã€‚é€šè¿‡ä»è®­ç»ƒè¿‡ç¨‹ä¸­æå–å‡ºç¥ç»ç½‘ç»œæ¨¡å‹ä¸­çš„å¿…è¦å’Œå……åˆ†è§„åˆ™ï¼ˆglobal interpretabilityï¼‰ï¼Œå¹¶å°†è¿™äº›è§„åˆ™è½¬æ¢æˆå¯ä»¥å‡†ç¡®åœ°é¢„æµ‹ç¥ç»ç½‘ç»œè¾“å‡ºçš„è§„åˆ™å‹æ¨¡å‹ï¼ˆexact interpretabilityï¼‰ï¼ŒTT-ruleså¯ä»¥å°†ç¥ç»ç½‘ç»œè½¬æ¢æˆä¸€ç§è§„åˆ™å‹æ¨¡å‹ã€‚è¿™ç§è§„åˆ™å‹æ¨¡å‹æ”¯æŒäºŒåˆ†ç±»ã€å¤šæ ‡ç­¾åˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œé€‚ç”¨äºå°è‡³å¤§çš„è¡¨æ ¼æ•°æ®é›†ã€‚åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†TT-rulesçš„æ¡†æ¶ï¼Œå¹¶å¯¹å…¶æ€§èƒ½è¿›è¡Œäº†å¥åº·åº”ç”¨çš„è¯„ä¼°ï¼Œå¹¶ä¸å½“å‰çš„å¯è§£é‡Šæ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒTT-ruleså¯ä»¥ä¸å…¶ä»–å¯è§£é‡Šæ–¹æ³•åŒ¹é…æˆ–è¶…è¶Šå…¶æ€§èƒ½ã€‚å°¤å…¶æ˜¯TT-rulesæ˜¯é¦–ä¸ªèƒ½å¤Ÿé€‚ç”¨äºå¤§å‹è¡¨æ ¼æ•°æ®é›†çš„å‡†ç¡®è§„åˆ™å‹æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå®é™…çš„DNAæ•°æ®é›†ï¼Œæ¯ä¸ªæ•°æ®é›†æœ‰è¶…è¿‡20Kçš„ç‰¹å¾ã€‚
</details></li>
</ul>
<hr>
<h2 id="Likelihood-based-Sensor-Calibration-for-Expert-Supported-Distributed-Learning-Algorithms-in-IoT-Systems"><a href="#Likelihood-based-Sensor-Calibration-for-Expert-Supported-Distributed-Learning-Algorithms-in-IoT-Systems" class="headerlink" title="Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems"></a>Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11526">http://arxiv.org/abs/2309.11526</a></li>
<li>repo_url: None</li>
<li>paper_authors: RÃ¼diger Machhamer, Lejla Begic Fazlic, Eray Guven, David Junk, Gunes Karabulut Kurt, Stefan Naumann, Stephan Didas, Klaus-Uwe Gollmer, Ralph Bergmann, Ingo J. Timm, Guido Dartmann</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æé«˜æ„ŸçŸ¥æŠ€æœ¯ä¸­æ•°æ®æµ‹é‡çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨äº†ä¼°è®¡æ–œå°„å˜æ¢çš„æ–¹æ³•ï¼Œå¹¶åˆ©ç”¨ä¸“å®¶çŸ¥è¯†è¿›è¡Œæ”¹è¿›ã€‚å®ƒè¿˜å¯ä»¥åº”ç”¨äºè½¯ä»¶æ ¡å‡†ã€ä¸“å®¶åŸºäºé€‚åº”å’Œè”é‚¦å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>results: å®éªŒå’Œä»¿çœŸæ•°æ®éƒ½è¡¨æ˜ï¼Œè¿™ç§è§£å†³æ–¹æ¡ˆå¯ä»¥æé«˜æµ‹é‡æ•°æ®çš„ç²¾åº¦å’Œæ•ˆç‡ã€‚<details>
<summary>Abstract</summary>
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æ„Ÿæµ‹æŠ€æœ¯é¢†åŸŸï¼Œä¸€é¡¹é‡è¦ä»»åŠ¡æ˜¯æœ‰æ•ˆåœ°å®ç°æ„Ÿæµ‹å™¨ä¹‹é—´æµ‹é‡è½¬æ¢çš„æ–¹æ³•ã€‚ä¸€ç§æ€è·¯æ˜¯ä½¿ç”¨ä¼°ç®—æŠ•å½±å˜æ¢ï¼Œå¯ä»¥é€šè¿‡ä¸“å®¶çŸ¥è¯†è¿›è¡Œæ”¹è¿›ã€‚è¿™ç¯‡æ–‡ç« ä»‹ç»äº†1973å¹´ç”±å†°å·ç ”ç©¶æ‰€å‘è¡¨çš„æ”¹è¿›è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¡¨æ˜è¯¥è§£å†³æ–¹æ¡ˆå¯ä»¥é€‚ç”¨äºè½¯ä»¶å‡†ç¡®æ€§æ£€æµ‹ã€ä¸“å®¶çŸ¥è¯†åŸºäºçš„è°ƒæ•´å’Œè”é‚¦å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒå’ŒçœŸå®æµ‹é‡æ•°æ®æ¥è¯„ä¼°æˆ‘ä»¬çš„ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–åçš„æ–¹æ³•å¯ä»¥æé«˜ä»ªå™¨æµ‹é‡ç²¾åº¦ã€‚Here's the translation in Traditional Chinese:åœ¨æ„Ÿæµ‹æŠ€æœ¯é¢†åŸŸï¼Œä¸€ä¸ªé‡è¦ä»»åŠ¡æ˜¯æœ‰æ•ˆåœ°å®ç°æ„Ÿæµ‹å™¨ä¹‹é—´æµ‹é‡è½¬æ¢çš„æ–¹æ³•ã€‚ä¸€ç§æ€è·¯æ˜¯ä½¿ç”¨ä¼°ç®—æŠ•å½±å˜æ¢ï¼Œå¯ä»¥é€šè¿‡ä¸“å®¶çŸ¥è¯†è¿›è¡Œæ”¹è¿›ã€‚è¿™ç¯‡æ–‡ç« ä»‹ç»äº†1973å¹´ç”±å†°å·ç ”ç©¶æ‰€å‘è¡¨çš„æ”¹è¿›è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¡¨æ˜è¿™ä¸ªè§£å†³æ–¹æ¡ˆå¯ä»¥åº”ç”¨äºè½¯ä»¶å‡†ç¡®æ€§æ£€æµ‹ã€ä¸“å®¶çŸ¥è¯†åŸºäºçš„è°ƒæ•´å’Œè”é‚¦å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒå’ŒçœŸå®æµ‹é‡æ•°æ®æ¥è¯„ä¼°æˆ‘ä»¬çš„ç ”ç©¶ã€‚ç»“æœè¡¨æ˜ï¼Œä¼˜åŒ–åçš„æ–¹æ³•å¯ä»¥æé«˜ä»ªå™¨æµ‹é‡ç²¾åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="Practical-Probabilistic-Model-based-Deep-Reinforcement-Learning-by-Integrating-Dropout-Uncertainty-and-Trajectory-Sampling"><a href="#Practical-Probabilistic-Model-based-Deep-Reinforcement-Learning-by-Integrating-Dropout-Uncertainty-and-Trajectory-Sampling" class="headerlink" title="Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling"></a>Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11089">http://arxiv.org/abs/2309.11089</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrjun123/DPETS">https://github.com/mrjun123/DPETS</a></li>
<li>paper_authors: Wenjun Huang, Yunduan Cui, Huiyun Li, Xinyu Wu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³ç°æœ‰çš„ probabilistic model-based reinforcement learningï¼ˆMBRLï¼‰æ¨¡å‹ï¼Œå®ƒä»¬åŸºäºç¥ç»ç½‘ç»œï¼Œä½†å®ƒä»¬çš„é¢„æµ‹ç¨³å®šæ€§å’Œå‡†ç¡®æ€§æœ‰é™åˆ¶ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³dropout-based probabilistic ensembles with trajectory samplingï¼ˆDPETSï¼‰ï¼Œå®ƒå°†Monte-Carlo dropoutå’Œ trajectory samplingç»“åˆåœ¨ä¸€èµ·ï¼Œä»¥ç¨³å®šåœ°é¢„æµ‹ç³»ç»Ÿçš„ä¸ç¡®å®šæ€§ã€‚DPETSçš„æŸå¤±å‡½æ•°è®¾è®¡ç”¨äºæ›´æ­£ç¥ç»ç½‘ç»œçš„é€‚åº”é”™è¯¯ï¼Œä»¥æ›´å‡†ç¡®åœ°é¢„æµ‹ probabilistic modelsã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡åœ¨å¤šä¸ª Mujoco  benchmark controlä»»åŠ¡å’Œä¸€ä¸ªå®é™…çš„ robot arm manipulationä»»åŠ¡ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œå‘ç° DPETS å¯ä»¥åœ¨æ›´é«˜çš„ sample efficiency ä¸‹è¾¾åˆ°æ›´é«˜çš„å‡è¿”å›å€¼å’Œå¿«é€Ÿååé‡ï¼ŒåŒæ—¶è¶…è¿‡äº†ç›¸å…³çš„ MBRL æ–¹æ³•ã€‚æ­¤å¤–ï¼ŒDPETS è¿˜å¯ä»¥åœ¨é¢ä¸´é™„åŠ å¹²æ‰°å’Œå®é™…æ“ä½œä¸­è¡¨ç°å‡ºè‰²ã€‚<details>
<summary>Abstract</summary>
This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available at https://github.com/mrjun123/DPETS.
</details>
<details>
<summary>æ‘˜è¦</summary>
In the evaluation, DPETS outperforms other MBRL approaches in both average return and convergence velocity on several Mujoco benchmark control tasks with additional disturbances and one practical robot arm manipulation task. It also achieves superior performance compared to well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available on GitHub at https://github.com/mrjun123/DPETS.Translated into Simplified Chinese:è¿™ç¯‡è®ºæ–‡å…³æ³¨ç°æœ‰åŸºäºç¥ç»ç½‘ç»œçš„æ¦‚ç‡æ¨¡å‹å­¦ä¹ ï¼ˆMBRLï¼‰æ–¹æ³•çš„é¢„æµ‹ç¨³å®šæ€§ã€é¢„æµ‹å‡†ç¡®æ€§å’Œæ§åˆ¶èƒ½åŠ›ã€‚ä¸€ç§æ–°çš„æ–¹æ³•å«åš dropout-based æ¦‚ç‡é›†åˆwith trajectory samplingï¼ˆDPETSï¼‰è¢«æè®®ï¼Œå®ƒå°† Monte-Carlo dropout å’Œ trajectory sampling é›†æˆåˆ°ä¸€ä¸ªæ¡†æ¶ä¸­ï¼Œä»¥ç¨³å®šç³»ç»Ÿuncertaintyçš„é¢„æµ‹ã€‚losså‡½æ•°è®¾è®¡ç”¨äºæ›´æ­£ç¥ç»ç½‘ç»œçš„é€‚åº”é”™è¯¯ï¼Œä»¥ä¾¿æ›´å‡†ç¡®åœ°é¢„æµ‹æ¦‚ç‡æ¨¡å‹ã€‚Policyä¹Ÿè¢«æ‰©å±•ä»¥ç­›é€‰ aleatoric uncertaintyï¼Œä»¥æé«˜æ§åˆ¶èƒ½åŠ›ã€‚åœ¨è¯„ä¼°ä¸­ï¼ŒDPETS æ¯”å…¶ä»– MBRL æ–¹æ³•åœ¨å¤šä¸ª Mujoco benchmarkæ§åˆ¶ä»»åŠ¡ä¸Šï¼ˆåŒ…æ‹¬é™„åŠ å¹²æ‰°ï¼‰å’Œä¸€ä¸ªå®é™…çš„æœºæ¢°è‡‚æ§åˆ¶ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæ›´é«˜çš„å¹³å‡è¿”ç‚¹å’Œæ›´å¿«çš„è¿ç»­é€Ÿåº¦ï¼ŒåŒæ—¶ä¸è®¸å¤šå·²çŸ¥çš„æ¨¡å‹è‡ªç”±åŸºelineè¡¨ç°å‡ºæ›´å¥½çš„æ€§èƒ½ï¼Œå¹¶ä¸”å…·æœ‰æ˜¾è‘—çš„æ ·æœ¬æ•ˆç‡ã€‚DPETS çš„å¼€æºä»£ç å¯ä»¥åœ¨ GitHub ä¸Šè·å–ï¼Œåœ°å€ä¸º <https://github.com/mrjun123/DPETS>ã€‚
</details></li>
</ul>
<hr>
<h2 id="Embed-Search-Align-DNA-Sequence-Alignment-using-Transformer-Models"><a href="#Embed-Search-Align-DNA-Sequence-Alignment-using-Transformer-Models" class="headerlink" title="Embed-Search-Align: DNA Sequence Alignment using Transformer Models"></a>Embed-Search-Align: DNA Sequence Alignment using Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11087">http://arxiv.org/abs/2309.11087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Holur, K. C. Enevoldsen, Lajoyce Mboning, Thalia Georgiou, Louis-S. Bouchard, Matteo Pellegrini, Vwani Roychowdhury</li>
<li>for: è¯¥ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§åŸºäºTransformeræ¶æ„çš„DNAåºåˆ—å¯¹é½æ–¹æ³•ï¼Œä»¥æé«˜DNAåºåˆ—å¯¹é½ç²¾åº¦ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†è‡ªåŠ¨å¯¹é½ç²¾åº¦è¿›è¡Œè‡ªæˆ‘è¶…visedåŸ¹è®­ï¼Œå¹¶å¼•å…¥äº†DNAvectorå­˜å‚¨ä»¥å®ç°å…¨æ–‡æœç´¢ã€‚</li>
<li>results: è¯¥æ–¹æ³•å¯ä»¥é«˜åº¦å‡†ç¡®åœ°å¯¹é½250ä¸ªåŸºå› ç»„ä¸­çš„DNAåºåˆ—ï¼Œå¹¶ä¸”åœ¨ä¸åŒæŸ“è‰²ä½“å’Œç‰©ç§ä¸Šè¿›è¡Œäº†ä»»åŠ¡è½¬ç§»ã€‚<details>
<summary>Abstract</summary>
DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where it is necessary to conduct a genome-wide search to successfully align every read. We address this open problem by framing it as an Embed-Search-Align task. In this framework, a novel encoder model DNA-ESA generates representations of reads and fragments of the reference, which are projected into a shared vector space where the read-fragment distance is used as surrogate for alignment. In particular, DNA-ESA introduces: (1) Contrastive loss for self-supervised training of DNA sequence representations, facilitating rich sequence-level embeddings, and (2) a DNA vector store to enable search across fragments on a global scale. DNA-ESA is >97% accurate when aligning 250-length reads onto a human reference genome of 3 gigabases (single-haploid), far exceeds the performance of 6 recent DNA-Transformer model baselines and shows task transfer across chromosomes and species.
</details>
<details>
<summary>æ‘˜è¦</summary>
DNNAåºåˆ—Alignmentå«ä¹‰åœ¨å°†çŸ­DNNAè¯»ç‰© assigning åˆ°å‚è€ƒåŸºå› ç»„ä¸­æœ€æœ‰å¯èƒ½çš„ä½ç½®ä¸Šã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯ç”Ÿç‰©å­¦åˆ†æä¸­çš„å…³é”®æ­¥éª¤ï¼ŒåŒ…æ‹¬å˜å¼‚æ£€æµ‹ã€è½¬å½•ç»„å­¦å’Œepigenomicsã€‚ä¼ ç»Ÿæ–¹æ³•é€šè¿‡ä¸¤æ­¥è¿›è¡Œï¼šåŸºå› ç»„ç´¢å¼•ï¼Œç„¶åæ˜¯é«˜æ•ˆçš„æœç´¢æ¥æ‰¾åˆ°ç»™å®šè¯»ç‰©çš„å¯èƒ½ä½ç½®ã€‚åŸºäºå¤§è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ç¼–ç æ–‡æœ¬ä¸ºåµŒå…¥ä¸­çš„æˆåŠŸï¼Œæœ€è¿‘çš„åŠªåŠ›æ˜¯å¦æ˜¯ä½¿ç”¨åŒæ ·çš„Transformeræ¶æ„ç”ŸæˆDNNAåºåˆ—çš„æ•°å­—è¡¨ç¤ºã€‚è¿™äº›æ¨¡å‹åœ¨çŸ­DNNAåºåˆ—åˆ†ç±»ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ—©æœŸçš„ promiseï¼Œä¾‹å¦‚åˆ†ç±» coding vs non-coding åŒºåŸŸä»¥åŠæ¿€æ´»å™¨å’Œæ¿€å‘å™¨åºåˆ—çš„è¯†åˆ«ã€‚ä½†æ˜¯ï¼Œæ€§èƒ½åœ¨åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸Šä¸èƒ½ç›´æ¥è½¬ç§»åˆ°Alignmentä»»åŠ¡ï¼Œå› ä¸ºéœ€è¦è¿›è¡Œå…¨åŸºå› ç»„æœç´¢ä»¥æˆåŠŸåœ°å¯¹æ¯ä¸ªè¯»ç‰©è¿›è¡ŒAlignmentã€‚æˆ‘ä»¬è§£å†³è¿™ä¸ªå¼€æ”¾é—®é¢˜ by framing it as an Embed-Search-Align taskã€‚åœ¨è¿™ç§æ¡†æ¶ä¸­ï¼Œä¸€ç§æ–°çš„ç¼–ç å™¨æ¨¡å‹DNA-ESAç”Ÿæˆäº†è¯»ç‰©å’Œå‚è€ƒåŸºå› ç»„ä¸­çš„ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²çš„è¡¨ç¤ºï¼Œå¹¶å°†å®ƒä»¬æŠ•å°„åˆ°ä¸€ä¸ªå…±äº«çš„vectorç©ºé—´ä¸­ï¼Œå…¶ä¸­è¯»ç‰©-Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚çš„è·ç¦»ä½œä¸ºAlignmentçš„Surrogateã€‚ç‰¹åˆ«æ˜¯ï¼ŒDNA-ESAå¼•å…¥äº†ï¼šï¼ˆ1ï¼‰å¯¹DNNAåºåˆ—è¡¨ç¤ºè¿›è¡Œè‡ªæˆ‘è¶…vised è®­ç»ƒï¼Œä»¥è·å¾—ä¸°å¯Œçš„åºåˆ—æ°´å¹³åµŒå…¥ï¼Œä»¥åŠï¼ˆ2ï¼‰DNNA vector storeï¼Œä»¥å®ç°åœ¨å…¨çƒèŒƒå›´å†…æœç´¢å¤šä¸ª Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ²ã€‚DNNA-ESAåœ¨å¯¹3 gigabasesçš„äººç±»å‚è€ƒåŸºå› ç»„ä¸ŠAlignment 250ä¸ªé•¿åº¦çš„è¯»ç‰©æ—¶ï¼Œå‡†ç¡®ç‡é«˜äº97%ï¼Œå¤§å¹…è¶…è¿‡äº†6ä¸ªæœ€è¿‘çš„DNNA-Transformeræ¨¡å‹åŸºelineï¼Œå¹¶åœ¨ Ñ…Ñ€Ğ¾Ğ¼Ğ¾ÑĞ¾Ğ¼Ñ‹å’Œç§ç±»ä¹‹é—´æ˜¾ç¤ºä»»åŠ¡ä¼ é€’ã€‚
</details></li>
</ul>
<hr>
<h2 id="Weak-Supervision-for-Label-Efficient-Visual-Bug-Detection"><a href="#Weak-Supervision-for-Label-Efficient-Visual-Bug-Detection" class="headerlink" title="Weak Supervision for Label Efficient Visual Bug Detection"></a>Weak Supervision for Label Efficient Visual Bug Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11077">http://arxiv.org/abs/2309.11077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farrukh Rahman</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜è§†é¢‘æ¸¸æˆä¸­çš„è§†è§‰è´¨é‡ï¼Œå¹¶ Addressing the challenge of traditional testing methods being limited by resources and unable to cover the wide range of potential bugs.</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨æ— æ ‡æ³¨æ¸¸æˆè®°å½•å’ŒåŸŸç‰¹å®šçš„æ‰©å……æ¥ç”Ÿæˆæ•°æ®é›†å’Œè‡ªæˆ‘æ ‡æ³¨ç›®æ ‡ï¼Œå¹¶åœ¨é¢„è®­ç»ƒæˆ–å¤šä»»åŠ¡è®¾ç½®ä¸­ä½¿ç”¨è¿™äº›ç›®æ ‡è¿›è¡Œé¢„è®­ç»ƒã€‚æˆ‘ä»¬ä½¿ç”¨å¼±ç›‘ç£æ¥æ‰©å¤§æ•°æ®é›†ï¼Œå¹¶å®ç°äº†è‡ªä¸»å’Œäº’åŠ¨å¼å¼±ç›‘ç£ï¼Œé€šè¿‡ä¸supervised clusteringå’Œ&#x2F;æˆ–åŸºäºæ–‡æœ¬å’Œå‡ ä½•æç¤ºçš„äº¤äº’æ–¹å¼ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨Giantmapæ¸¸æˆä¸­æµ‹è¯•äº†FPPCï¼ˆé¦–ä¸ªç©å®¶æˆªå‰²&#x2F;ç¢°æ’æ¼æ´ï¼‰ï¼Œå‘ç°æˆ‘ä»¬çš„æ–¹æ³•éå¸¸æœ‰æ•ˆï¼Œè¶…è¶Šäº†å¼ºç›‘ç£åŸºçº¿ï¼Œåœ¨å®é™…ã€éå¸¸ä½é¢‘ç‡ã€ä½æ•°æ®é‡ rÃ©gimeä¸­ï¼ˆ0.336 $\rightarrow$ 0.550 F1åˆ†æ•°ï¼‰ã€‚åªéœ€5ä¸ªæ ‡æ³¨çš„â€œå¥½â€ç¤ºä¾‹ï¼ˆå³0ä¸ªæ¼æ´ï¼‰ï¼Œæˆ‘ä»¬çš„è‡ªæˆ‘æ ‡æ³¨ç›®æ ‡å°±èƒ½å¤Ÿæ•æ‰è¶³å¤Ÿçš„ä¿¡å·ï¼Œè¶…è¶Šä½æ ‡æ³¨ç›‘ç£è®¾ç½®ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„è§†è§‰æ¼æ´ä¸Šè¿›è¡Œåº”ç”¨ï¼Œå¹¶ä¸”å¯ä»¥åœ¨è§†é¢‘æ¸¸æˆä¸­æ‹“å±•åˆ°æ›´å¹¿æ³›çš„å›¾åƒå’Œè§†é¢‘ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
As video games evolve into expansive, detailed worlds, visual quality becomes essential, yet increasingly challenging. Traditional testing methods, limited by resources, face difficulties in addressing the plethora of potential bugs. Machine learning offers scalable solutions; however, heavy reliance on large labeled datasets remains a constraint. Addressing this challenge, we propose a novel method, utilizing unlabeled gameplay and domain-specific augmentations to generate datasets & self-supervised objectives used during pre-training or multi-task settings for downstream visual bug detection. Our methodology uses weak-supervision to scale datasets for the crafted objectives and facilitates both autonomous and interactive weak-supervision, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate on first-person player clipping/collision bugs (FPPC) within the expansive Giantmap game world, that our approach is very effective, improving over a strong supervised baseline in a practical, very low-prevalence, low data regime (0.336 $\rightarrow$ 0.550 F1 score). With just 5 labeled "good" exemplars (i.e., 0 bugs), our self-supervised objective alone captures enough signal to outperform the low-labeled supervised settings. Building on large-pretrained vision models, our approach is adaptable across various visual bugs. Our results suggest applicability in curating datasets for broader image and video tasks within video games beyond visual bugs.
</details>
<details>
<summary>æ‘˜è¦</summary>
Traditional video game testing methods are limited by resources and have difficulty addressing the many potential bugs that exist. Machine learning offers scalable solutions, but relying on large labeled datasets is a challenge. To address this, we propose a new method that uses unlabeled gameplay and domain-specific augmentations to generate datasets and self-supervised objectives for pre-training or multi-task settings. Our method uses weak supervision to scale the datasets and can be used in both autonomous and interactive modes, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate the effectiveness of our approach on first-person player clipping/collision bugs within the Giantmap game world, achieving an F1 score of 0.550 in a practical, low-prevalence, low-data regime with just 5 labeled "good" exemplars. Our self-supervised objective captures enough signal to outperform low-labeled supervised settings, and our approach is adaptable to various visual bugs and can be applied to curating datasets for broader image and video tasks within video games.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Tiling-A-Model-Agnostic-Adaptive-Scalable-and-Inference-Data-Centric-Approach-for-Efficient-and-Accurate-Small-Object-Detection"><a href="#Dynamic-Tiling-A-Model-Agnostic-Adaptive-Scalable-and-Inference-Data-Centric-Approach-for-Efficient-and-Accurate-Small-Object-Detection" class="headerlink" title="Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection"></a>Dynamic Tiling: A Model-Agnostic, Adaptive, Scalable, and Inference-Data-Centric Approach for Efficient and Accurate Small Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11069">http://arxiv.org/abs/2309.11069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Son The Nguyen, Theja Tulabandhula, Duy Nguyen</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æå‡ºä¸€ç§æ¨¡å‹ä¸åçš„ã€å¯é€‚åº”çš„ã€æ‰©å±•æ€§å¼ºçš„å°å¯¹è±¡æ£€æµ‹æ–¹æ³•ï¼Œä»¥æé«˜å¯¹è±¡æ£€æµ‹çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†åŠ¨æ€ç“·çº¹æ³•ï¼Œå³é¦–å…ˆä½¿ç”¨éé‡å çš„ç“·çº¹æ¥ç¡®å®šåˆå§‹æ£€æµ‹ç»“æœï¼Œç„¶åé€šè¿‡åŠ¨æ€è°ƒæ•´ç“·çº¹çš„é‡å ç‡å’Œç“·çº¹æœ€å°åŒ–å™¨æ¥è§£å†³åˆ†å¸ƒåœ¨ä¸åŒç“·çº¹ä¹‹é—´çš„ Fragmented å¯¹è±¡ï¼Œä»è€Œæé«˜æ£€æµ‹ç²¾åº¦å’Œé™ä½è®¡ç®—å¼€é”€ã€‚</li>
<li>results: ç›¸æ¯”ç°æœ‰çš„æ¨¡å‹ä¸åçš„å‡åŒ€è£å‰ªæ–¹æ³•ï¼ŒDynamic Tiling æ–¹æ³•åœ¨ä¸åŒçš„å¯¹è±¡å¤§å°å’Œç¯å¢ƒä¸‹éƒ½èƒ½å¤Ÿè¾¾åˆ°æ›´é«˜çš„æ£€æµ‹ç²¾åº¦å’Œæ•ˆç‡ï¼Œå¹¶ä¸”ä¸éœ€è¦åŠ³åŠ¨çš„é‡æ–°è°ƒæ•´ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•è¿˜å¯ä»¥åœ¨ä¸åŒçš„æ“ä½œç¯å¢ƒä¸‹è¿›è¡Œé€‚åº”ï¼Œä»¥æé«˜å¯¹è±¡æ£€æµ‹çš„å¯æ‰©å±•æ€§å’Œçµæ´»æ€§ã€‚<details>
<summary>Abstract</summary>
We introduce Dynamic Tiling, a model-agnostic, adaptive, and scalable approach for small object detection, anchored in our inference-data-centric philosophy. Dynamic Tiling starts with non-overlapping tiles for initial detections and utilizes dynamic overlapping rates along with a tile minimizer. This dual approach effectively resolves fragmented objects, improves detection accuracy, and minimizes computational overhead by reducing the number of forward passes through the object detection model. Adaptable to a variety of operational environments, our method negates the need for laborious recalibration. Additionally, our large-small filtering mechanism boosts the detection quality across a range of object sizes. Overall, Dynamic Tiling outperforms existing model-agnostic uniform cropping methods, setting new benchmarks for efficiency and accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘å›¢é˜Ÿä»‹ç»äº†ä¸€ç§åä¸ºåŠ¨æ€ç“·çº¹çš„æ¨¡å‹æ— å…³ã€å¯é€‚åº”ã€å¯æ‰©å±•çš„æ–¹æ³•ï¼Œç”¨äºå°ç‰©ä½“æ£€æµ‹ã€‚è¿™ç§æ–¹æ³•åŸºäºæˆ‘ä»¬çš„æ¨ç†æ•°æ®ä¸­å¿ƒçš„å“²å­¦ï¼Œä½¿ç”¨é overlap çš„ç“·çº¹å¼€å§‹ï¼Œç„¶åé‡‡ç”¨åŠ¨æ€é‡å ç‡å’Œç“·çº¹æœ€å°åŒ–å™¨ã€‚è¿™ç§åŒé‡æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°è§£å†³åˆ†å‰²ç‰©ä½“ï¼Œæé«˜æ£€æµ‹ç²¾åº¦ï¼Œå¹¶å‡å°‘è®¡ç®—è´Ÿæ‹…ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€‚ç”¨äºå¤šç§æ“ä½œç¯å¢ƒï¼Œæ— éœ€åŠ³è¾‘é‡æ–°è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å¤§å°ç­›é€‰æœºåˆ¶å¯ä»¥åœ¨ä¸åŒçš„ç‰©ä½“å¤§å°ä¸‹æé«˜æ£€æµ‹è´¨é‡ã€‚æ€»ä¹‹ï¼ŒåŠ¨æ€ç“·çº¹è¶…è¿‡äº†ç°æœ‰çš„æ¨¡å‹æ— å…³å‡åŒ€å‰² Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ï¼Œè®¾ç½®äº†æ–°çš„æ•ˆç‡å’Œå‡†ç¡®æ€§çš„benchmarkã€‚
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Relationship-between-LLM-Hallucinations-and-Prompt-Linguistic-Nuances-Readability-Formality-and-Concreteness"><a href="#Exploring-the-Relationship-between-LLM-Hallucinations-and-Prompt-Linguistic-Nuances-Readability-Formality-and-Concreteness" class="headerlink" title="Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness"></a>Exploring the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11064">http://arxiv.org/abs/2309.11064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vipula Rawte, Prachi Priya, S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Amit Sheth, Amitava Das</li>
<li>for:  investigate the influence of linguistic factors in prompts on the occurrence of LLM hallucinations</li>
<li>methods:  experimental study using prompts with varying levels of readability, formality, and concreteness</li>
<li>results:  prompts with greater formality and concreteness tend to result in reduced hallucinations, while the outcomes pertaining to readability are mixed.<details>
<summary>Abstract</summary>
As Large Language Models (LLMs) have advanced, they have brought forth new challenges, with one of the prominent issues being LLM hallucination. While various mitigation techniques are emerging to address hallucination, it is equally crucial to delve into its underlying causes. Consequently, in this preliminary exploratory investigation, we examine how linguistic factors in prompts, specifically readability, formality, and concreteness, influence the occurrence of hallucinations. Our experimental results suggest that prompts characterized by greater formality and concreteness tend to result in reduced hallucination. However, the outcomes pertaining to readability are somewhat inconclusive, showing a mixed pattern.
</details>
<details>
<summary>æ‘˜è¦</summary>
LLMs çš„è¿›æ­¥ä¹Ÿå¸¦æ¥äº†æ–°çš„æŒ‘æˆ˜ï¼Œå…¶ä¸­ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯ LLM å¹»è§‰ã€‚è™½ç„¶å„ç§ mitigation æŠ€æœ¯æ­£åœ¨emergingï¼Œä½†æ˜¯ä¹Ÿéå¸¸é‡è¦æ¢è®¨å¹»è§‰çš„æ·±å±‚åŸå› ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹åˆæ­¥çš„æ¢ç´¢æ€§ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æç¤ºä¸­è¯­è¨€å› ç´ å¯¹å¹»è§‰çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å¯è¯»æ€§ã€æ­£å¼åº¦å’Œå…·ä½“æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ›´æ­£å¼å’Œå…·ä½“çš„æç¤ºå¯ä»¥å‡å°‘å¹»è§‰ï¼Œä½†æ˜¯å…³äºå¯è¯»æ€§çš„ç»“æœå‘ˆæ‚åŒ–çš„æ¨¡å¼ã€‚
</details></li>
</ul>
<hr>
<h2 id="Design-of-Chain-of-Thought-in-Math-Problem-Solving"><a href="#Design-of-Chain-of-Thought-in-Math-Problem-Solving" class="headerlink" title="Design of Chain-of-Thought in Math Problem Solving"></a>Design of Chain-of-Thought in Math Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11054">http://arxiv.org/abs/2309.11054</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lqtrung1998/mwp_cot_design">https://github.com/lqtrung1998/mwp_cot_design</a></li>
<li>paper_authors: Zhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xiaoran Jin, Hang Li</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨é“¾æ¡æ€ç»´ï¼ˆCoTï¼‰åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­çš„ä½œç”¨ï¼Œå¹¶å¯¹ä¸åŒçš„ç¨‹åºCoTè¿›è¡Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€CoTã€è‡ªæˆ‘æè¿°ç¨‹åºã€æ³¨é‡Šæè¿°ç¨‹åºå’Œéæè¿°ç¨‹åºã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ investigateäº†ç¼–ç¨‹è¯­è¨€å¯¹ç¨‹åºCoTçš„å½±å“ï¼Œå¹¶å¯¹Pythonå’ŒWolframè¯­è¨€è¿›è¡Œæ¯”è¾ƒã€‚</li>
<li>methods: æœ¬ç ”ç©¶é‡‡ç”¨äº†extensive experimentsæ–¹æ³•ï¼Œåœ¨GSM8Kã€MATHQAå’ŒSVAMPä¸Šè¿›è¡Œäº†è¯„æµ‹ï¼Œå¹¶å‘ç°äº†ç¨‹åºCoTåœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­çš„ä¼˜åŠ¿ã€‚ç‰¹åˆ«æ˜¯ï¼Œæœ€ä½³ç»„åˆï¼ˆ30Bå‚æ•°ï¼‰å‡»è´¥äº†GPT-3.5-turboçš„è¡¨ç°ï¼Œå¹¶ä¸”è‡ªç„¶è¯­è¨€CoTæä¾›äº†æ›´å¤§çš„å¤šæ ·æ€§ï¼Œå› æ­¤å¯ä»¥é€šå¸¸å®ç°æ›´é«˜çš„æ€§èƒ½ã€‚</li>
<li>results: ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œç¨‹åºCoTåœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­å…·æœ‰ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯è‡ªç„¶è¯­è¨€CoTæä¾›äº†æ›´å¤§çš„å¤šæ ·æ€§ï¼Œå¯ä»¥å®ç°æ›´é«˜çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°äº†Pythonæ˜¯ç¨‹åºCoTçš„æ›´å¥½çš„ç¼–ç¨‹è¯­è¨€ã€‚ç ”ç©¶ç»“æœå¯ä»¥ä¸ºæœªæ¥çš„CoTè®¾è®¡æä¾›æœ‰ä»·å€¼çš„æŒ‡å¯¼ï¼Œå¹¶ä¸”å¯ä»¥è€ƒè™‘ç¼–ç¨‹è¯­è¨€å’Œç¼–ç¨‹é£æ ¼çš„å› ç´ è¿›è¡Œè¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into account both programming language and coding style for further advancements. Our datasets and code are publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
Chain-of-Thought (CoT) åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­æ‰®æ¼”ç€å…³é”®æ€§çš„è§’è‰²ã€‚æˆ‘ä»¬å¯¹è®¾è®¡ CoT çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œæ¯”è¾ƒäº†è‡ªç„¶è¯­è¨€ CoT ä¸ä¸åŒçš„ç¨‹åº CoTï¼ŒåŒ…æ‹¬è‡ªæˆ‘æè¿°ç¨‹åºã€æ³¨é‡Šæè¿°ç¨‹åºå’Œéæè¿°ç¨‹åºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ investigateäº†ç¼–ç¨‹è¯­è¨€å¯¹ç¨‹åº CoT çš„å½±å“ï¼Œæ¯”è¾ƒäº† Python å’Œ Wolfram è¯­è¨€ã€‚é€šè¿‡å¯¹ GSM8Kã€MATHQA å’Œ SVAMP ç­‰æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°program CoT åœ¨æ•°å­¦é—®é¢˜è§£å†³ä¸­ç»å¸¸å…·æœ‰æ›´é«˜çš„æ•ˆæœã€‚ç‰¹åˆ«æ˜¯ï¼Œä½¿ç”¨ 30B å‚æ•°çš„æœ€ä½³ç»„åˆå¯ä»¥å¾ˆå¤§å¹…åº¦åœ°è¶…è¶Š GPT-3.5-turboã€‚ç»“æœè¡¨æ˜ï¼Œè‡ªæˆ‘æè¿°ç¨‹åºå¯ä»¥æä¾›æ›´å¤šçš„å¤šæ ·æ€§ï¼Œå› æ­¤é€šå¸¸å¯ä»¥è¾¾åˆ°æ›´é«˜çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜å‘ç° Python æ¯” Wolfram æ›´é€‚åˆç”¨äº program CoTã€‚æˆ‘ä»¬çš„å®éªŒç»“æœæä¾›äº†æœªæ¥ CoT è®¾è®¡çš„ä»·å€¼æŒ‡å—ï¼Œè€ƒè™‘åˆ°ç¼–ç¨‹è¯­è¨€å’Œç¼–ç¨‹é£æ ¼ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æé«˜è¡¨è¾¾èƒ½åŠ›ã€‚æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç å…¬å¼€å¯ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Clustered-FedStack-Intermediate-Global-Models-with-Bayesian-Information-Criterion"><a href="#Clustered-FedStack-Intermediate-Global-Models-with-Bayesian-Information-Criterion" class="headerlink" title="Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion"></a>Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11044">http://arxiv.org/abs/2309.11044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Lin Li, Niall Higgins, Raj Gururajan, Xujuan Zhou, Jianming Yong</li>
<li>for: æé«˜ Federated Learningï¼ˆFLï¼‰åœ¨éIdenticalå’Œéç‹¬ç«‹åˆ†å¸ƒï¼ˆnon-IIDï¼‰å’Œæ•°æ®åç½®æ ‡ç­¾ï¼ˆimbalanced labelsï¼‰çš„æƒ…å†µä¸‹çš„æ€§èƒ½ã€‚</li>
<li>methods: ä½¿ç”¨ Stacked Federated Learningï¼ˆFedStackï¼‰æ¡†æ¶ï¼Œå¹¶é‡‡ç”¨ä¸‰ç§é›†ç¾¤æœºåˆ¶ï¼šK-Meansã€Agglomerativeå’ŒGaussian Mixture Modelsã€‚ä½¿ç”¨ Bayesian Information Criterionï¼ˆBICï¼‰ç¡®å®šé›†ç¾¤æ•°é‡ã€‚</li>
<li>results: Clustered FedStackæ¨¡å‹æ¯”åŸºelineæ¨¡å‹ WITH clusteringæœºåˆ¶è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”ä½¿ç”¨cyclical learning ratesæ¥ä¼°è®¡æ¡†æ¶çš„æ•´åˆç¨‹åº¦ã€‚<details>
<summary>Abstract</summary>
Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, and Gaussian Mixture Models, into the framework and evaluate their performance. We use Bayesian Information Criterion (BIC) with the maximum likelihood function to determine the number of clusters. The Clustered FedStack models outperform baseline models with clustering mechanisms. To estimate the convergence of our proposed framework, we use Cyclical learning rates.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœ¨çš„ Federated Learningï¼ˆFLï¼‰æŠ€æœ¯åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é¢†åŸŸä¸­éå¸¸æµè¡Œï¼Œè¿™æ˜¯å› ä¸ºå®ƒå¯ä»¥å®ç°ååŒå­¦ä¹ å¹¶ä¿æŒå®¢æˆ·ç«¯éšç§ã€‚ç„¶è€Œï¼ŒFLè¿˜é¢ä¸´ç€éæ ‡ä¸€åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰å’Œæ•°æ®åææ€§ï¼ˆimbalanced labelsï¼‰ç­‰é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›å±€é™æ€§ï¼Œç ”ç©¶äººå‘˜å·²ç»æå‡ºäº†å¤šç§æ–¹æ³•ï¼Œå¦‚ä½¿ç”¨æœ¬åœ°æ¨¡å‹å‚æ•°ã€è”é‚¦ç”Ÿæˆæ•Œæ–¹æœç´¢å­¦ä¹ å’Œè”é‚¦è¡¨ç¤ºå­¦ä¹ ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå…ˆå‰å‘è¡¨çš„ Stacked Federated Learningï¼ˆFedStackï¼‰æ¡†æ¶çš„ Novel Clustered FedStack æ¡†æ¶ã€‚æœ¬åœ°å®¢æˆ·ç«¯å°†å…¶æ¨¡å‹é¢„æµ‹ç»“æœå’Œè¾“å‡ºå±‚åŠ æƒå€¼å‘é€åˆ°æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨ç„¶åå»ºç«‹ä¸€ä¸ªå¼ºå¤§çš„å…¨å±€æ¨¡å‹ã€‚è¿™ä¸ªå…¨å±€æ¨¡å‹ä½¿ç”¨ä¸€ç§å·ç§¯æœºåˆ¶å°†æœ¬åœ°å®¢æˆ·ç«¯åˆ†ä¸ºä¸åŒçš„é›†ç¾¤ã€‚æˆ‘ä»¬åœ¨æ¡†æ¶ä¸­é‡‡ç”¨äº† K-Meansã€Agglomerative å’Œ Gaussian Mixture Models ä¸‰ç§å·ç§¯æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨ Bayesian Information Criterionï¼ˆBICï¼‰ä¸æœ€å¤§ä¼¼ç„¶å‡½æ•°æ¥ç¡®å®šé›†ç¾¤æ•°é‡ã€‚Clustered FedStack æ¨¡å‹åœ¨åŸºelineæ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä»¥ä¾¿ä¼°ç®—æˆ‘ä»¬æå‡ºçš„æ¡†æ¶çš„æ•´åˆã€‚ä¸ºäº†ä¼°ç®—æˆ‘ä»¬çš„æå‡ºçš„æ¡†æ¶çš„æ•´åˆï¼Œæˆ‘ä»¬ä½¿ç”¨ Cyclical learning ratesã€‚
</details></li>
</ul>
<hr>
<h2 id="Making-Small-Language-Models-Better-Multi-task-Learners-with-Mixture-of-Task-Adapters"><a href="#Making-Small-Language-Models-Better-Multi-task-Learners-with-Mixture-of-Task-Adapters" class="headerlink" title="Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters"></a>Making Small Language Models Better Multi-task Learners with Mixture-of-Task-Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11042">http://arxiv.org/abs/2309.11042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukang Xie, Chengyu Wang, Junbing Yan, Jiyong Zhou, Feiqi Deng, Jun Huang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§åŸºäºå°è¯­è¨€æ¨¡å‹ï¼ˆless than 1B parametersï¼‰çš„å¤šä»»åŠ¡å­¦ä¹ ç³»ç»Ÿï¼Œä»¥æ”¯æŒåŸŸpecificåº”ç”¨ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ‰©å±• transformer æ¶æ„çš„ Mixture-of-Task-Adaptersï¼ˆMTAï¼‰æ¨¡å—ï¼Œä»¥capture intra-task å’Œinter-task çŸ¥è¯†ã€‚åŒæ—¶ï¼Œæå‡ºäº†ä¸€ç§ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ–¹æ³•æ¥ä¼˜åŒ– adapter ä¹‹é—´çš„åä½œã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„ MTA æ¶æ„å’Œä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ–¹æ³•å¯ä»¥è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼ŒåŸºäº ALTER çš„ MTA-equipped è¯­è¨€æ¨¡å‹åœ¨ä¸åŒé¢†åŸŸä¸­ä¹Ÿå¾—åˆ°äº†è‰¯å¥½çš„resultã€‚<details>
<summary>Abstract</summary>
Recently, Large Language Models (LLMs) have achieved amazing zero-shot learning performance over a variety of Natural Language Processing (NLP) tasks, especially for text generative tasks. Yet, the large size of LLMs often leads to the high computational cost of model training and online deployment. In our work, we present ALTER, a system that effectively builds the multi-tAsk Learners with mixTure-of-task-adaptERs upon small language models (with <1B parameters) to address multiple NLP tasks simultaneously, capturing the commonalities and differences between tasks, in order to support domain-specific applications. Specifically, in ALTER, we propose the Mixture-of-Task-Adapters (MTA) module as an extension to the transformer architecture for the underlying model to capture the intra-task and inter-task knowledge. A two-stage training method is further proposed to optimize the collaboration between adapters at a small computational cost. Experimental results over a mixture of NLP tasks show that our proposed MTA architecture and the two-stage training method achieve good performance. Based on ALTER, we have also produced MTA-equipped language models for various domains.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸Šå®ç°äº†æƒŠäººçš„é›¶shotå­¦ä¹ æ€§èƒ½ï¼Œå°¤å…¶æ˜¯æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œå¤§å‹æ¨¡å‹çš„å¤§å°ç»å¸¸å¯¼è‡´æ¨¡å‹è®­ç»ƒå’Œåœ¨çº¿éƒ¨ç½²çš„è®¡ç®—æˆæœ¬é«˜æ¶¨ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ALTERç³»ç»Ÿï¼Œå¯ä»¥æœ‰æ•ˆåœ°å»ºç«‹å¤šä»»åŠ¡å­¦ä¹ è€…ï¼Œé€šè¿‡å°†å°å‹è¯­è¨€æ¨¡å‹ï¼ˆ Parameters <1Bï¼‰æ‰©å±•åˆ°å¤šä¸ªNLPä»»åŠ¡ï¼Œä»¥ä¾¿åŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡ï¼Œæ•æ‰ä»»åŠ¡ä¹‹é—´çš„å…±åŒç‚¹å’Œå·®å¼‚ï¼Œä»¥æ”¯æŒåŸŸpecificåº”ç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ALTERä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†mixture-of-task-adaptERsï¼ˆMTAï¼‰æ¨¡å—ï¼Œä½œä¸º transformer æ¶æ„çš„å¢å¼ºéƒ¨åˆ†ï¼Œä»¥Capture intra-taskå’Œinter-taskçŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸¤Stageè®­ç»ƒæ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å°å‹è®¡ç®—æˆæœ¬ä¸‹ä¼˜åŒ– adapter collaborationã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æè®®çš„MTAæ¶æ„å’Œä¸¤Stageè®­ç»ƒæ–¹æ³•åœ¨ä¸€ç»„å¤šç§NLPä»»åŠ¡ä¸Šå…·æœ‰è‰¯å¥½çš„è¡¨ç°ã€‚åŸºäºALTERï¼Œæˆ‘ä»¬è¿˜ç”Ÿæˆäº†å„ä¸ªé¢†åŸŸçš„MTAè¯­è¨€æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Intelligent-Transportation-Systems-Recent-Applications-and-Open-Problems"><a href="#Federated-Learning-in-Intelligent-Transportation-Systems-Recent-Applications-and-Open-Problems" class="headerlink" title="Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems"></a>Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11039">http://arxiv.org/abs/2309.11039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiying Zhang, Jun Li, Long Shi, Ming Ding, Dinh C. Nguyen, Wuzheng Tan, Jian Weng, Zhu Han</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨åŸºäºåˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æŠ€æœ¯çš„æ™ºèƒ½äº¤é€šç³»ç»Ÿï¼ˆITSï¼‰ä¸­çš„åº”ç”¨å‰æ™¯ï¼Œä»¥åŠåœ¨ä¸åŒåœºæ™¯ä¸‹å¦‚ä½•ä½¿ç”¨ Federated Learningï¼ˆFLï¼‰æ¥è§£å†³æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æŠ€æœ¯Federated Learningï¼ˆFLï¼‰æ¥è§£å†³æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­çš„é—®é¢˜ï¼ŒåŒ…æ‹¬å¯¹è±¡è¯†åˆ«ã€äº¤é€šç®¡ç†å’ŒæœåŠ¡æä¾›ç­‰åœºæ™¯ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°äº†åœ¨æ™ºèƒ½äº¤é€šç³»ç»Ÿä¸­åº”ç”¨FLåï¼Œå¯ä»¥æé«˜å¯¹è±¡è¯†åˆ«ç²¾åº¦ã€æé«˜äº¤é€šç®¡ç†æ•ˆç‡å’Œæé«˜æœåŠ¡æä¾›è´¨é‡ç­‰ã€‚ä½†æ˜¯ï¼ŒFLä¹Ÿå­˜åœ¨ä¸€äº›æŒ‘æˆ˜ï¼Œå¦‚æ•°æ®ä¸å‡åŒ€åˆ†å¸ƒã€è®¡ç®—æœºåŠ›å’Œå­˜å‚¨ç©ºé—´çš„é™åˆ¶ï¼Œä»¥åŠéšç§å’Œå®‰å…¨é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Intelligent transportation systems (ITSs) have been fueled by the rapid development of communication technologies, sensor technologies, and the Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of the vehicle networks, it is rather challenging to make timely and accurate decisions of vehicle behaviors. Moreover, in the presence of mobile wireless communications, the privacy and security of vehicle information are at constant risk. In this context, a new paradigm is urgently needed for various applications in dynamic vehicle environments. As a distributed machine learning technology, federated learning (FL) has received extensive attention due to its outstanding privacy protection properties and easy scalability. We conduct a comprehensive survey of the latest developments in FL for ITS. Specifically, we initially research the prevalent challenges in ITS and elucidate the motivations for applying FL from various perspectives. Subsequently, we review existing deployments of FL in ITS across various scenarios, and discuss specific potential issues in object recognition, traffic management, and service providing scenarios. Furthermore, we conduct a further analysis of the new challenges introduced by FL deployment and the inherent limitations that FL alone cannot fully address, including uneven data distribution, limited storage and computing power, and potential privacy and security concerns. We then examine the existing collaborative technologies that can help mitigate these challenges. Lastly, we discuss the open challenges that remain to be addressed in applying FL in ITS and propose several future research directions.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ™ºèƒ½äº¤é€šç³»ç»Ÿï¼ˆITSï¼‰å› é€šä¿¡æŠ€æœ¯ã€æ„ŸçŸ¥æŠ€æœ¯å’Œäº’è”ç½‘å¯¹è¯çš„å¿«é€Ÿå‘å±•è€Œå¾—åˆ°æ¨åŠ¨ã€‚ç„¶è€Œï¼Œç”±äºè½¦è¾†ç½‘ç»œçš„åŠ¨æ€ç‰¹æ€§ï¼Œå¾ˆéš¾åœ¨æ—¶é—´ä¸Šè¿›è¡Œå‡†ç¡®çš„è½¦è¾†è¡Œä¸ºå†³ç­–ã€‚æ­¤å¤–ï¼Œåœ¨ç§»åŠ¨æ— çº¿é€šä¿¡çš„å­˜åœ¨ä¸‹ï¼Œè½¦è¾†ä¿¡æ¯çš„éšç§å’Œå®‰å…¨æ€»æ˜¯å¤„äºé£é™©ä¹‹ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ç§æ–°çš„æ€ç»´æ–¹å¼æ˜¯ç´§è¿«çš„ï¼Œä»¥æ»¡è¶³ä¸åŒåº”ç”¨åœºæ™¯çš„éœ€æ±‚ã€‚ä½œä¸ºåˆ†å¸ƒå¼æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œè”é‚¦å­¦ä¹ ï¼ˆFLï¼‰åœ¨éšç§ä¿æŠ¤å’Œæ‰©å±•å¯æ‰©å±•æ€§ç­‰æ–¹é¢å—åˆ°äº†å¹¿æ³›çš„å…³æ³¨ã€‚æˆ‘ä»¬è¿›è¡Œäº†ITSä¸­FLæœ€æ–°çš„å‘å±•æƒ…å†µçš„å…¨é¢è¯„ä¼°ã€‚æˆ‘ä»¬é¦–å…ˆç ”ç©¶äº†ITSä¸­å­˜åœ¨çš„ä¸»è¦æŒ‘æˆ˜å’Œåº”ç”¨FLçš„åŠ¨æœºï¼Œç„¶åè¯„è®ºäº†ITSä¸­FLçš„ä¸åŒåœºæ™¯åº”ç”¨ï¼ŒåŒ…æ‹¬ç‰©ä½“è¯†åˆ«ã€äº¤é€šç®¡ç†å’ŒæœåŠ¡æä¾›ç­‰æ–¹é¢çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è¿›ä¸€æ­¥çš„åˆ†æï¼Œæ¢è®¨FLéƒ¨ç½²å¼•å…¥çš„æ–°æŒ‘æˆ˜å’ŒFLæœ¬èº«æ— æ³•è§£å†³çš„å†…åœ¨é™åˆ¶ï¼ŒåŒ…æ‹¬æ•°æ®åˆ†å¸ƒä¸å‡ã€è®¡ç®—å’Œå­˜å‚¨èƒ½åŠ›æœ‰é™å’Œéšç§å’Œå®‰å…¨é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨åº”ç”¨FLæ—¶å­˜åœ¨çš„å¼€æ”¾æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="ModelGiF-Gradient-Fields-for-Model-Functional-Distance"><a href="#ModelGiF-Gradient-Fields-for-Model-Functional-Distance" class="headerlink" title="ModelGiF: Gradient Fields for Model Functional Distance"></a>ModelGiF: Gradient Fields for Model Functional Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11013">http://arxiv.org/abs/2309.11013</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zju-vipa/modelgif">https://github.com/zju-vipa/modelgif</a></li>
<li>paper_authors: Jie Song, Zhengqi Xu, Sai Wu, Gang Chen, Mingli Song</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯é‡åŒ–ä¸åŒé¢„è®­ç»ƒæ¨¡å‹ä¹‹é—´çš„åŠŸèƒ½è·ç¦»ï¼Œä»¥ä¾¿ä¸ºå„ç§ç›®çš„è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>methods: è¯¥ paper ä½¿ç”¨äº†åŸºäº â€œåœºâ€ çš„æ€æƒ³ï¼Œæå‡ºäº† Model Gradient Field (ModelGiF)ï¼Œç”¨äºä»ä¸åŒé¢„è®­ç»ƒæ¨¡å‹ä¸­æå–åŒè°±è¡¨ç¤ºã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒModelGiF åœ¨ä»»åŠ¡ç›¸å…³æ€§åˆ¤æ–­ã€çŸ¥è¯†äº§æƒä¿æŠ¤å’Œæ¨¡å‹å¿˜å´éªŒè¯ç­‰æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä¸å½“å‰ç«äº‰è€…ç›¸æ¯”æ˜¾è‘—æ€§æ›´é«˜ã€‚<details>
<summary>Abstract</summary>
The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures or tasks. Inspired by the concept of "field" in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate the versatility of the proposed ModelGiF on these tasks, with significantly superiority performance to state-of-the-art competitors. Codes are available at https://github.com/zju-vipa/modelgif.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‡å»ä¸€ä¸ªåå¹´ï¼Œæ·±åº¦å­¦ä¹ çš„æˆåŠŸå’Œå…¬å…±é‡Šæ”¾çš„è®­ç»ƒæ¨¡å‹çš„æ¶Œç°ï¼Œä½¿å¾—æ¨¡å‹åŠŸèƒ½è·ç¦»çš„é‡åŒ–å˜å¾—éå¸¸é‡è¦ã€‚ç„¶è€Œï¼Œé‡åŒ–æ¨¡å‹åŠŸèƒ½è·ç¦»æ€»æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å†…éƒ¨å·¥ä½œæœºåˆ¶æ˜¯ä¸é€æ˜çš„ï¼Œè€Œä¸”æ¨¡å‹æˆ–ä»»åŠ¡çš„architectureå’Œtaskéƒ½æ˜¯å¤šæ ·çš„ã€‚å¼•ç”¨ç‰©ç†å­¦ä¸­çš„â€œåœºâ€æ¦‚å¿µï¼Œåœ¨è¿™ç§å·¥ä½œä¸­æˆ‘ä»¬æå‡ºäº†Model Gradient Fieldï¼ˆç®€ç§°ModelGiFï¼‰æ¥EXTRACT homogeneous representation from heterogeneous pre-trained modelsã€‚æˆ‘ä»¬å‡è®¾æ¯ä¸ªé¢„è®­ç»ƒæ·±åº¦æ¨¡å‹å…·æœ‰å”¯ä¸€çš„ModelGiF over the input spaceï¼Œå› æ­¤å¯ä»¥é€šè¿‡æ¯”è¾ƒè¿™äº›ModelGiFçš„ç›¸ä¼¼æ€§æ¥åº¦é‡æ¨¡å‹ä¹‹é—´çš„è·ç¦»ã€‚æˆ‘ä»¬éªŒè¯äº†æè®®çš„ModelGiFçš„æ•ˆæœé€šè¿‡ä¸€ç³»åˆ—æµ‹è¯•åºŠï¼ŒåŒ…æ‹¬ä»»åŠ¡ç›¸ä¼¼æ€§é¢„æµ‹ã€çŸ¥è¯†äº§æƒä¿æŠ¤å’Œæ¨¡å‹å¿˜è®°éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜æè®®çš„ModelGiFåœ¨è¿™äº›ä»»åŠ¡ä¸Šå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿æ€§èƒ½ï¼Œä¸ç°æœ‰çš„ç«äº‰å¯¹æ‰‹ç›¸æ¯”ã€‚ä»£ç å¯ä»¥åœ¨https://github.com/zju-vipa/modelgifä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Spiking-NeRF-Making-Bio-inspired-Neural-Networks-See-through-the-Real-World"><a href="#Spiking-NeRF-Making-Bio-inspired-Neural-Networks-See-through-the-Real-World" class="headerlink" title="Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World"></a>Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10987">http://arxiv.org/abs/2309.10987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingting Yao, Qinghao Hu, Tielong Liu, Zitao Mo, Zeyu Zhu, Zhengyang Zhuge, Jian Cheng</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§èƒ½æºä¼˜åŒ–çš„ç¥ç»é³—ç½‘ç»œï¼ˆSpiking Neural Networkï¼ŒSNNï¼‰ï¼Œç”¨äºå®ç°é«˜å“è´¨çš„3Dåœºæ™¯æ¸²æŸ“ï¼Œå¹¶ä¸”ä¸ç”Ÿç‰©å­¦ä¸Šçš„ç¥ç»å…ƒè¿ä½œç›¸ä¼¼ã€‚</li>
<li>methods: è¿™ä¸ªæ–¹æ³•ä½¿ç”¨äº†ç¥ç»é³—ç½‘ç»œï¼ˆSNNï¼‰å’Œå°„çº¿åœºï¼ˆNeRFï¼‰æŠ€æœ¯ï¼Œå°†å°„çº¿åœºä¸æ—¶é—´ç»´åº¦è¿›è¡Œå¯¹åº”ï¼Œä»è€Œä½¿è®¡ç®—å˜æˆäº†ä¸€ä¸ªå‘å°„-è‡ªç”±çš„æ–¹å¼ï¼Œä»¥å‡å°‘èƒ½æºæ¶ˆè€—ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥å®ç°$76.74%$çš„èƒ½æºä¼˜åŒ–ï¼Œå¹¶ä¸”ä¸ç”Ÿç‰©å­¦ä¸Šçš„ç¥ç»å…ƒè¿ä½œç›¸ä¼¼ã€‚<details>
<summary>Abstract</summary>
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irregular temporal length. We propose the temporal condensing-and-padding (TCP) strategy to tackle the masked samples to maintain regular temporal length, i.e., regular tensors, for hardware-friendly computation. Extensive experiments on a variety of datasets demonstrate that our method reduces the $76.74\%$ energy consumption on average and obtains comparable synthesis quality with the ANN baseline.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç¥ç»é£æš´ç½‘ç»œï¼ˆSNNï¼‰åœ¨è®¸å¤šä»»åŠ¡ä¸Šå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œä»¥åˆ©ç”¨å…¶èƒ½æ•ˆçš„èƒ½æºå’Œç”Ÿç‰©å¯èƒ½çš„æ™ºèƒ½æ½œåŠ›ã€‚ç„¶è€Œï¼Œç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰çš„æ¸²æŸ“é«˜è´¨é‡3Dåœºæ™¯å´éœ€è¦å·¨å¤§çš„èƒ½æºæ¶ˆè€—ï¼Œè€Œå¾ˆå°‘çš„ç ”ç©¶æ¢è®¨äº†ä»¥ç”Ÿç‰©é™è„‰ä¸ºå¯¼å‘çš„èƒ½æºæŠ‘åˆ¶æ–¹æ³•ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¥ç»è¾å°„åœºï¼ˆSpikingNeRFï¼‰ï¼Œå®ƒå°†è¾å°„åœºçš„å¼ºåº¦æ–¹å‘ä¸SNNçš„æ—¶é—´ç»´åº¦å¯¹é½ï¼Œä»¥è‡ªç„¶åœ°è®©SNNå‚ä¸è¾å°„åœºçš„é‡å»ºã€‚å› æ­¤ï¼Œè®¡ç®—å˜æˆäº†ä¸€ç§å¿«é€Ÿã€æ—  multiplication çš„æ–¹å¼ï¼Œä»è€Œé™ä½äº†èƒ½æºæ¶ˆè€—ã€‚åœ¨SpikingNeRFä¸­ï¼Œæ¯ä¸ªæ ·æœ¬ç‚¹è¢«åŒ¹é…åˆ°ç‰¹å®šçš„æ—¶é—´æ­¥ï¼Œå¹¶ä»¥æ··åˆæ–¹å¼è¡¨ç¤ºï¼Œä¿ç•™äº† voxel ç½‘æ ¼ã€‚åŸºäº voxel ç½‘æ ¼ï¼Œæ ·æœ¬ç‚¹æ˜¯å¦éœ€è¦è¢«masking ä»¥æé«˜è®­ç»ƒå’Œæ¨ç†çš„è´¨é‡ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ“ä½œä¹Ÿä¼šäº§ç”Ÿä¸è§„åˆ™çš„æ—¶é—´é•¿åº¦ã€‚æˆ‘ä»¬æå‡ºäº†æ—¶é—´condensing-and-paddingï¼ˆTCPï¼‰ç­–ç•¥ï¼Œä»¥è§£å†³masked samplesçš„é—®é¢˜ï¼Œä»¥ä¿æŒå¸¸è§„çš„æ—¶é—´é•¿åº¦ï¼Œå³å¸¸è§„çš„tensorï¼Œä¸ºç¡¬ä»¶å‹å¥½çš„è®¡ç®—ã€‚åœ¨å¤šä¸ªdatasetä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é™ä½76.74%çš„èƒ½æºæ¶ˆè€—ï¼Œå¹¶ä¸ANNåŸºçº¿ç›¸å½“çš„Synthesisè´¨é‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Is-GPT4-a-Good-Trader"><a href="#Is-GPT4-a-Good-Trader" class="headerlink" title="Is GPT4 a Good Trader?"></a>Is GPT4 a Good Trader?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10982">http://arxiv.org/abs/2309.10982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingzhe Wu<br>for: æœ¬ç ”ç©¶æ—¨åœ¨æ£€éªŒGPT-4å¯¹ç»å…¸æŠ•èµ„ç†è®ºçš„ç†è§£ç¨‹åº¦å’Œå¯¹å®é™…äº¤æ˜“æ•°æ®åˆ†æçš„ä»£ç è§£é‡Šèƒ½åŠ›ã€‚methods: æœ¬ç ”ç©¶ä½¿ç”¨GPT-4å¯¹ç‰¹å®šèµ„äº§çš„æ—¥å‡Kçº¿æ•°æ®è¿›è¡Œåˆ†æï¼ŒåŸºäºå°¼é‡‡å°”æµªå¹•ç†è®ºç­‰ç‰¹å®šç†è®ºã€‚results: æœ¬ç ”ç©¶å‘ç°GPT-4åœ¨å¯¹å®é™…äº¤æ˜“æ•°æ®åˆ†æä¸­è¡¨ç°å‡ºè¾ƒé«˜çš„è§£é‡Šæ·±åº¦å’Œå‡†ç¡®ç‡ï¼ŒåŒæ—¶æä¾›äº†æœ‰ä»·å€¼çš„æŠ•èµ„ç†è®ºåº”ç”¨æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Recently, large language models (LLMs), particularly GPT-4, have demonstrated significant capabilities in various planning and reasoning tasks \cite{cheng2023gpt4,bubeck2023sparks}. Motivated by these advancements, there has been a surge of interest among researchers to harness the capabilities of GPT-4 for the automated design of quantitative factors that do not overlap with existing factor libraries, with an aspiration to achieve alpha returns \cite{webpagequant}. In contrast to these work, this study aims to examine the fidelity of GPT-4's comprehension of classic trading theories and its proficiency in applying its code interpreter abilities to real-world trading data analysis. Such an exploration is instrumental in discerning whether the underlying logic GPT-4 employs for trading is intrinsically reliable. Furthermore, given the acknowledged interpretative latitude inherent in most trading theories, we seek to distill more precise methodologies of deploying these theories from GPT-4's analytical process, potentially offering invaluable insights to human traders.   To achieve this objective, we selected daily candlestick (K-line) data from specific periods for certain assets, such as the Shanghai Stock Index. Through meticulous prompt engineering, we guided GPT-4 to analyze the technical structures embedded within this data, based on specific theories like the Elliott Wave Theory. We then subjected its analytical output to manual evaluation, assessing its interpretative depth and accuracy vis-\`a-vis these trading theories from multiple dimensions. The results and findings from this study could pave the way for a synergistic amalgamation of human expertise and AI-driven insights in the realm of trading.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç‰¹åˆ«æ˜¯GPT-4ï¼Œåœ¨å„ç§è®¡åˆ’å’Œç†è§£ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æ˜¾è‘—çš„èƒ½åŠ›ã€‚è¿™äº›è¿›æ­¥å¼•èµ·äº†ç ”ç©¶äººå‘˜å¯¹GPT-4çš„æŠ•èµ„ alpha å›æŠ¥çš„å…´è¶£ï¼Œå¹¶å¯»æ±‚é€šè¿‡è‡ªåŠ¨è®¾è®¡ä¸åŒäºç°æœ‰å› ç´ åº“çš„é‡åŒ–å› ç´ æ¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚ä¸è¿™äº›å·¥ä½œä¸åŒï¼Œæœ¬ç ”ç©¶æ—¨åœ¨æ£€éªŒGPT-4å¯¹ç»å…¸äº¤æ˜“ç†è®ºçš„ç†è§£å’Œå¯¹å®é™…äº¤æ˜“æ•°æ®åˆ†æä¸­çš„ä»£ç è§£é‡Šèƒ½åŠ›ã€‚è¿™ç§æ¢ç´¢æœ‰åŠ©äºåˆ¤æ–­GPT-4åœ¨äº¤æ˜“ä¸­ä½¿ç”¨çš„é€»è¾‘æ˜¯å¦å…·æœ‰å†…åœ¨çš„å¯é æ€§ã€‚æ­¤å¤–ï¼Œç”±äºäº¤æ˜“ç†è®ºä¸­çš„è§£é‡Šç©ºé—´å¾€å¾€å¾ˆå¤§ï¼Œæˆ‘ä»¬å¯»æ±‚é€šè¿‡GPT-4çš„åˆ†æè¿‡ç¨‹ä¸­æå–æ›´åŠ ç²¾ç»†çš„æ–¹æ³•æ¥åº”ç”¨è¿™äº›ç†è®ºï¼Œä»è€Œä¸ºäººç±»äº¤æ˜“å‘˜æä¾›æœ‰ä»·å€¼çš„æƒ³æ³•ã€‚ä¸ºè¾¾åˆ°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ç‰¹å®šæœŸé—´çš„ä¸€äº›èµ„äº§çš„æ—¥å‡ç›˜å½¢ï¼ˆK-lineï¼‰æ•°æ®ï¼Œä¾‹å¦‚ä¸Šæµ·è‚¡ç¥¨æŒ‡æ•°ã€‚é€šè¿‡ä»”ç»†çš„æé—®å·¥ç¨‹ï¼Œæˆ‘ä»¬å¯¼å¼•GPT-4åˆ†æè¿™äº›æ•°æ®ä¸­çš„æŠ€æœ¯ç»“æ„ï¼ŒåŸºäºç‰¹å®šçš„æŠ•èµ„ç†è®ºï¼Œå¦‚æ¬§æ‹‰ç“¦vecenieç†è®ºã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹GPT-4çš„åˆ†æè¾“å‡ºè¿›è¡Œæ‰‹åŠ¨è¯„ä¼°ï¼Œè¯„ä¼°å…¶åœ¨è¿™äº›äº¤æ˜“ç†è®ºå¤šä¸ªç»´åº¦çš„è§£é‡Šæ·±åº¦å’Œå‡†ç¡®æ€§ã€‚ç ”ç©¶ç»“æœå’Œå‘ç°å¯èƒ½ä¸ºäººç±»ä¸“å®¶å’Œ AI é©±åŠ¨çš„æƒ³æ³•å¸¦æ¥ååŒåˆä½œï¼Œä¸ºäº¤æ˜“é¢†åŸŸå¸¦æ¥æ–°çš„å‘å±•ã€‚
</details></li>
</ul>
<hr>
<h2 id="AI-Driven-Patient-Monitoring-with-Multi-Agent-Deep-Reinforcement-Learning"><a href="#AI-Driven-Patient-Monitoring-with-Multi-Agent-Deep-Reinforcement-Learning" class="headerlink" title="AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning"></a>AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10980">http://arxiv.org/abs/2309.10980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Jianming Yong, Hong-Ning Dai</li>
<li>for: æé«˜åŒ»ç–—å«ç”Ÿç›‘æµ‹æ•ˆæœï¼Œå®ç°æ—¶é—´æœ‰æ•ˆçš„å¹²é¢„å’Œæ”¹å–„åŒ»ç–—ç»“æœã€‚</li>
<li>methods: ä½¿ç”¨å¤šæ™ºèƒ½æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDRLï¼‰æ–¹æ³•ï¼ŒæŠ•å…¥å¤šä¸ªå­¦ä¹ ä»£ç†ï¼Œæ¯ä¸ªä»£ç†è´Ÿè´£ç›‘æµ‹ç‰¹å®šç”Ÿç†å‚æ•°ï¼Œå¦‚å¿ƒç‡ã€å‘¼å¸å’Œä½“æ¸©ç­‰ã€‚è¿™äº›ä»£ç†ä¸é€šç”¨åŒ»ç–—ç›‘æµ‹ç¯å¢ƒäº’åŠ¨ï¼Œå­¦ä¹ æ‚£è€…çš„è¡Œä¸ºæ¨¡å¼ï¼Œæ ¹æ®ç´§æ€¥ç¨‹åº¦ä¼°ç®—ï¼Œå‘ç›¸åº”çš„åŒ»ç–—åº”æ€¥å›¢é˜Ÿï¼ˆMETsï¼‰å‘å‡ºè­¦ç¤ºã€‚</li>
<li>results: ä¸å¤šç§åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œç ”ç©¶è¡¨æ˜ï¼Œæposedçš„DRLæ–¹æ³•åœ¨å®é™…ç”Ÿç†å’Œè¿åŠ¨æ•°æ®é›†PPG-DaLiAå’ŒWESADä¸Šçš„è¡¨ç°å‡†ç¡®æ€§é«˜äºæ‰€æœ‰åŸºçº¿æ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡è°ƒæ•´Hyperparameterè¿›è¡Œä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜ä»£ç†çš„æ€»æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results with several baseline models, including Q-Learning, PPO, Actor-Critic, Double DQN, and DDPG, as well as monitoring frameworks like WISEML and CA-MAQL. Our experiments demonstrate that the proposed DRL approach outperforms all other baseline models, achieving more accurate monitoring of patient's vital signs. Furthermore, we conduct hyperparameter optimization to fine-tune the learning process of each agent. By optimizing hyperparameters, we enhance the learning rate and discount factor, thereby improving the agents' overall performance in monitoring patient health status. Our AI-driven patient monitoring system offers several advantages over traditional methods, including the ability to handle complex and uncertain environments, adapt to varying patient conditions, and make real-time decisions without external supervision.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šè¿‡äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ‚£è€…ç›‘æµ‹æ¡†æ¶ï¼Œæˆ‘ä»¬å¯ä»¥æé«˜åŒ»ç–—ç»“æœå’Œæ‚£è€…ç›‘æµ‹æ•ˆæœã€‚ä¼ ç»Ÿçš„ç›‘æµ‹ç³»ç»Ÿç»å¸¸åœ¨å¤æ‚å’ŒåŠ¨æ€çš„ç¯å¢ƒä¸­éš¾ä»¥å¤„ç†ï¼Œå¯¼è‡´æ£€æµ‹é‡è¦æƒ…å†µçš„å»¶è¿Ÿã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå¤šä»£ç†æ·±åº¦å­¦ä¹ ï¼ˆDRLï¼‰çš„æ–°å‹æ‚£è€…ç›‘æµ‹æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šä¸ªå­¦ä¹ ä»£ç†ä¹‹é—´åˆ†é…ä¸åŒçš„ç”Ÿç‰© physiological ç‰¹å¾ï¼Œä¾‹å¦‚å¿ƒç‡ã€å‘¼å¸å’Œä½“æ¸©ã€‚è¿™äº›ä»£ç†ä¸ä¸€ä¸ªé€šç”¨åŒ»ç–—ç›‘æµ‹ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ æ‚£è€…çš„è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶æ ¹æ®ç´§æ€¥ç¨‹åº¦æ¥é€šçŸ¥ç›¸åº”çš„åŒ»ç–—ç´§æ€¥é˜Ÿä¼ï¼ˆMETsï¼‰ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å®é™…çš„ç”Ÿç†å’Œè¿åŠ¨æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸å¤šç§åŸºå‡†æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬Qå­¦ä¹ ã€PPOã€actor-criticã€Double DQN å’Œ DDPG ç­‰ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„ DRL æ–¹æ³•åœ¨ç›‘æµ‹æ‚£è€…ç”Ÿå‘½ä½“å¾ä¸Šçš„å‡†ç¡®æ€§æ¯”åŸºå‡†æ¨¡å‹é«˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº† Ğ³Ğ¸Ğ¿ĞµÑ€å‚æ•°ä¼˜åŒ–ï¼Œä»¥æé«˜æ¯ä¸ªä»£ç†çš„å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡ä¼˜åŒ– Ğ³Ğ¸Ğ¿ĞµÑ€å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥æé«˜ä»£ç†çš„æ€»è¡¨ç°ï¼Œä»¥æ›´å¥½åœ°ç›‘æµ‹æ‚£è€…å¥åº·çŠ¶æ€ã€‚æˆ‘ä»¬çš„äººå·¥æ™ºèƒ½é©±åŠ¨çš„æ‚£è€…ç›‘æµ‹ç³»ç»Ÿå…·æœ‰è®¸å¤šä¼˜åŠ¿ï¼ŒåŒ…æ‹¬èƒ½å¤Ÿå¤„ç†å¤æ‚å’Œä¸ç¡®å®šçš„ç¯å¢ƒã€é€‚åº”å˜åŒ–çš„æ‚£è€…çŠ¶å†µï¼Œä»¥åŠä¸éœ€è¦å¤–éƒ¨ç›‘ç£è€Œè¡ŒåŠ¨ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.AI_2023_09_20/" data-id="clopawnm1004bag88f1dz52b3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.CL_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T11:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.CL_2023_09_20/">cs.CL - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Semi-supervised-News-Discourse-Profiling-with-Contrastive-Learning"><a href="#Semi-supervised-News-Discourse-Profiling-with-Contrastive-Learning" class="headerlink" title="Semi-supervised News Discourse Profiling with Contrastive Learning"></a>Semi-supervised News Discourse Profiling with Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11692">http://arxiv.org/abs/2309.11692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Li, Ruihong Huang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†è§£å†³æ–°é—»æŠ¥é“ä¸­æ–‡æœ¬ç»“æ„åˆ†ç±»é—®é¢˜ï¼Œä»¥ä¾¿åœ¨ä¸‹æ¸¸åº”ç”¨ä¸­ä½¿ç”¨ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³å†…æ–‡å¯¹ç…§å­¦ä¹  WITH ç²¾ç¥ï¼ˆICLDï¼‰ï¼Œåˆ©ç”¨æ–°é—»æŠ¥é“çš„ç‰¹æ®Šç»“æ„ç‰¹å¾è¿›è¡Œé‡‡æ ·ï¼Œå¹¶é€šè¿‡å¯¹æ¯”åˆ†ç±»æ¥å¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>results: è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒICLD æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³æ–°é—»æŠ¥é“ä¸­æ–‡æœ¬ç»“æ„åˆ†ç±»é—®é¢˜ï¼Œå¹¶ä¸”æ¯”ä¼ ç»Ÿçš„ç›‘ç£å­¦ä¹ æ–¹æ³•æ›´æœ‰æ•ˆã€‚<details>
<summary>Abstract</summary>
News Discourse Profiling seeks to scrutinize the event-related role of each sentence in a news article and has been proven useful across various downstream applications. Specifically, within the context of a given news discourse, each sentence is assigned to a pre-defined category contingent upon its depiction of the news event structure. However, existing approaches suffer from an inadequacy of available human-annotated data, due to the laborious and time-intensive nature of generating discourse-level annotations. In this paper, we present a novel approach, denoted as Intra-document Contrastive Learning with Distillation (ICLD), for addressing the news discourse profiling task, capitalizing on its unique structural characteristics. Notably, we are the first to apply a semi-supervised methodology within this task paradigm, and evaluation demonstrates the effectiveness of the presented approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°é—»è¯è¯­åˆ†ææ—¨åœ¨ç ”ç©¶æ¯ä¸ªæ–°é—»æ–‡ç« ä¸­çš„æ¯å¥è¯è¯­çš„äº‹ä»¶ç›¸å…³æ€§è§’è‰²ï¼Œå¹¶åœ¨å¤šç§ä¸‹æ¸¸åº”ç”¨ä¸­è¡¨ç°å‡ºæœ‰ç”¨æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨ç»™å®šçš„æ–°é—»è¯è¯­èƒŒæ™¯ä¸‹ï¼Œæ¯å¥è¯è¯­ä¼šè¢«åˆ†é…åˆ°é¢„å®šçš„ç±»åˆ«ï¼Œæ ¹æ®å®ƒä»¬æè¿°æ–°é—»äº‹ä»¶ç»“æ„ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å—åˆ°æœ‰é™çš„äººå·¥æ ‡æ³¨æ•°æ®çš„ä¸è¶³ï¼Œè¿™æ˜¯å› ä¸ºç”Ÿæˆè¯è¯­æ°´å¹³æ ‡æ³¨çš„åŠ³åŠ¨å’Œæ—¶é—´è´¹æ—¶çš„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºIntra-document Contrastive Learning with Distillationï¼ˆICLDï¼‰ï¼Œç”¨äºè§£å†³æ–°é—»è¯è¯­ profiling ä»»åŠ¡ï¼Œåˆ©ç”¨å®ƒçš„ç‹¬ç‰¹ç»“æ„ç‰¹å¾ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡åœ¨è¿™ä¸ªä»»åŠ¡å‡†åˆ™ä¸‹åº”ç”¨ semi-supervised æ–¹æ³•Ğ¾Ğ»Ğ¾Ğ³Ğ¸ï¼Œè¯„ä¼°ç»“æœè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Paradigm-Shift-in-Machine-Translation-Boosting-Translation-Performance-of-Large-Language-Models"><a href="#A-Paradigm-Shift-in-Machine-Translation-Boosting-Translation-Performance-of-Large-Language-Models" class="headerlink" title="A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models"></a>A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11674">http://arxiv.org/abs/2309.11674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fe1ixxu/alma">https://github.com/fe1ixxu/alma</a></li>
<li>paper_authors: Haoran Xu, Young Jin Kim, Amr Sharaf, Hany Hassan Awadalla</li>
<li>for: æé«˜ moderate-sized language models (LLMs) åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„è¡¨ç°</li>
<li>methods: æå‡ºäº†ä¸€ç§ç‰¹æœ‰çš„ç»ƒä¹ æ–¹æ³•ï¼Œé€šè¿‡é¦–å…ˆåœ¨å•è¯­è¨€æ•°æ®ä¸Šè¿›è¡Œåˆå§‹ç»ƒä¹ ï¼Œç„¶ååœ¨ä¸€å°é‡é«˜è´¨é‡å¹¶åˆ—æ•°æ®ä¸Šè¿›è¡Œåç»ƒä¹ ï¼Œä»¥æ¶ˆé™¤ä¼ ç»Ÿç¿»è¯‘æ¨¡å‹é€šå¸¸ä¾èµ–çš„åºå¤§å¹¶åˆ—æ•°æ®çš„éœ€æ±‚ã€‚</li>
<li>results: æ ¹æ® LLaMA-2 ä¸ºåŸºç¡€æ¨¡å‹ï¼Œå®ç°äº†åœ¨ WMTâ€™21 å’Œ WMTâ€™22 æµ‹è¯•é›†ä¸Šçš„å¹³å‡æé«˜è¶…è¿‡ 12 BLEU å’Œ 12 COMETï¼Œåœ¨ 10 ä¸ªç¿»è¯‘æ–¹å‘ä¸Šã€‚è¡¨ç°è¾ƒä¹‹å‰çš„æ‰€æœ‰å·¥ä½œæ›´å¥½ï¼Œç”šè‡³è¶…è¿‡ NLLB-54B æ¨¡å‹å’Œ GPT-3.5-text-davinci-003ï¼Œå³ä½¿åªæœ‰ 7B æˆ– 13B å‚æ•°ã€‚è¿™ç§æ–¹æ³•ä¸ºæœºå™¨ç¿»è¯‘è®­ç»ƒæ–¹æ³•æä¾›äº†åŸºç¡€ã€‚<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç”Ÿæˆå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å·²ç»å–å¾—äº†éå¸¸å‡ºè‰²çš„è¿›æ­¥ã€‚ç„¶è€Œï¼Œè¿™äº›è¿›æ­¥å¹¶æ²¡æœ‰åæ˜ åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨ä¸­å‹æ¨¡å‹ï¼ˆi.e., 7Bæˆ–13Bå‚æ•°ï¼‰ï¼Œè¿™äº›æ¨¡å‹ä»ç„¶è½åäºä¼ ç»Ÿçš„ç›‘ç£ç¼–ç å™¨-è§£ç å™¨ç¿»è¯‘æ¨¡å‹ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»å°è¯•ä½¿ç”¨ä¸åŒçš„æ–¹æ³•æ¥æé«˜è¿™äº›ä¸­å‹LLMçš„ç¿»è¯‘èƒ½åŠ›ï¼Œä½†å…¶æˆæœå¾ˆæœ‰é™ã€‚åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç‰¹æœ‰çš„ç»ƒä¹ æ–¹æ³•ï¼Œç”¨äºæé«˜LLMçš„ç¿»è¯‘èƒ½åŠ›ï¼Œä¸éœ€è¦å¤§é‡çš„å¹¶è¡Œæ•°æ®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªç»ƒä¹ é˜¶æ®µï¼šé¦–å…ˆåœ¨å•è¯­è¨€æ•°æ®ä¸Šè¿›è¡Œåˆå§‹ç»ƒä¹ ï¼Œç„¶ååœ¨ä¸€ä¸ªå°é‡é«˜è´¨é‡å¹¶è¡Œæ•°æ®ä¸Šè¿›è¡Œ subsequential ç»ƒä¹ ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºAdvanced Language Model-based trAnslatorï¼ˆALMAï¼‰ã€‚åŸºäºLLaMA-2ä½œä¸ºæˆ‘ä»¬çš„åŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹å¯ä»¥åœ¨10ä¸ªç¿»è¯‘æ–¹å‘ä¸Š average æé«˜è¶…è¿‡12ä¸ªBLEUå’Œ12ä¸ªCOMETçš„æ€§èƒ½ï¼Œç›¸æ¯”äºé›¶å¼€å§‹æ€§èƒ½ã€‚è¿™ä¸ªæ€§èƒ½é«˜äºæ‰€æœ‰ä¹‹å‰çš„å·¥ä½œï¼Œç”šè‡³è¶…è¿‡NLLB-54Bæ¨¡å‹å’ŒGPT-3.5-text-davinci-003æ¨¡å‹ï¼Œå³ä½¿åªæœ‰7Bæˆ–13Bå‚æ•°ã€‚è¿™ç§æ–¹æ³•åˆ›ç«‹äº†ä¸€ç§æ–°çš„è®­ç»ƒ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ğ°åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸã€‚
</details></li>
</ul>
<hr>
<h2 id="Construction-of-Paired-Knowledge-Graph-Text-Datasets-Informed-by-Cyclic-Evaluation"><a href="#Construction-of-Paired-Knowledge-Graph-Text-Datasets-Informed-by-Cyclic-Evaluation" class="headerlink" title="Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation"></a>Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11669">http://arxiv.org/abs/2309.11669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Mousavi, Xin Zhan, He Bai, Peng Shi, Theo Rekatsinas, Benjamin Han, Yunyao Li, Jeff Pound, Josh Susskind, Natalie Schluter, Ihab Ilyas, Navdeep Jaitly</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯è¯æ˜ä½¿ç”¨ä¸åŒå™ªéŸ³æ°´å¹³ç”Ÿæˆ Knowledge Graph (KG) å’Œæ–‡æœ¬å¯¹åº”çš„æ•°æ®é›†ï¼Œå¯ä»¥è®­ç»ƒå‰å‘å’Œåå‘ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä½†æ˜¯ä½¿ç”¨ä¸åŒçš„æ•°æ®é›†å¯èƒ½ä¼šå¯¼è‡´æ›´å¤šçš„å¹»è§‰å’Œæ›´å·®çš„æ‹Ÿåˆç‡ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº†ç”Ÿæˆæ–‡æœ¬å’Œ KG çš„cyclic evaluationæ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶é€šè¿‡æ‰‹åŠ¨åˆ›å»º WebNLG å’Œè‡ªåŠ¨åˆ›å»º TeKGen å’Œ T-REx æ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚</li>
<li>results: è¿™ paper å‘ç°ï¼Œä½¿ç”¨ä¸åŒå™ªéŸ³æ°´å¹³ç”Ÿæˆçš„æ•°æ®é›†å¯ä»¥å½±å“æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸”æ‰‹åŠ¨åˆ›å»ºçš„ WebNLG è¡¨ç°æ›´å¥½äºè‡ªåŠ¨åˆ›å»ºçš„ TeKGen å’Œ T-RExã€‚æ­¤å¤–ï¼Œä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLM) æ„å»ºçš„æ•°æ®é›†å¯ä»¥è®­ç»ƒæ¨¡å‹åœ¨æ–‡æœ¬ç”Ÿæˆä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†æ˜¯åœ¨ Knowledge Graph ç”Ÿæˆä¸­è¡¨ç°è¾ƒå·®ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰ä¸€ä¸ªå…±åŒçš„ Ontologyã€‚<details>
<summary>Abstract</summary>
Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.
</details>
<details>
<summary>æ‘˜è¦</summary>
Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However, models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation, we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Effective-Disambiguation-for-Machine-Translation-with-Large-Language-Models"><a href="#Towards-Effective-Disambiguation-for-Machine-Translation-with-Large-Language-Models" class="headerlink" title="Towards Effective Disambiguation for Machine Translation with Large Language Models"></a>Towards Effective Disambiguation for Machine Translation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11668">http://arxiv.org/abs/2309.11668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Iyer, Pinzhen Chen, Alexandra Birch</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å…·æœ‰å¤šä¹‰è¯å’Œç½•è§è¯æ„ä¹‰æ—¶ç¿»è¯‘ä¸ç¡®å®šæ€§çš„èƒ½åŠ›ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ”¹è¿›ç¿»è¯‘ä¸ç¡®å®šæ€§å¤„ç†æ–¹æ³•ï¼Œä¸€ç§æ˜¯åœ¨ Context ä¸­å­¦ä¹ ï¼Œå¦ä¸€ç§æ˜¯åœ¨ç‰¹ curaously ç¼–è¾‘çš„ä¸ç¡®å®šæ€§æ•°æ®é›†ä¸Šè¿›è¡Œç»ƒä¹ å’Œå¾®è°ƒã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸å½“å‰çŠ¶æ€çš„ç³»ç»Ÿå¦‚æ·±åº¦ç¿»è¯‘å’Œ NLLB åŒ¹é…æˆ–è¶…è¶Šï¼Œåœ¨äº”ç§è¯­è¨€æ–¹å‘ä¸­å››ç§æ–¹å‘ä¸­è¡¨ç°å‡ºè‰²ã€‚<details>
<summary>Abstract</summary>
Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effectively adapting LLMs for disambiguation during machine translation.
</details>
<details>
<summary>æ‘˜è¦</summary>
è§£å†³è¯­ä¹‰å«ä¹‰çš„æŒ‘æˆ˜ä¸€ç›´è¢«è®¤ä¸ºæ˜¯æœºå™¨ç¿»è¯‘é¢†åŸŸçš„ä¸­å¿ƒé—®é¢˜ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å«ä¹‰ambiguous sentenceè¿›è¡Œç¿»è¯‘æ€§èƒ½æµ‹è¯•çš„ä¼ ç»Ÿç¥ç»æœºå™¨ç¿»è¯‘ï¼ˆNMTï¼‰ç³»ç»Ÿæœ‰é™ï¼Œä¸èƒ½æ•æ‰è¿™äº›æƒ…å†µã€‚å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è¿™äº›æƒ…å†µä¸‹è¡¨ç°å‡ºäº†æ½œåœ¨çš„ä¼˜åŠ¿ï¼Œå¹¶æå‡ºäº†æ–°çš„æ§åˆ¶ç›®æ ‡è¾“å‡ºçš„æ–¹æ³•ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMåœ¨å«ä¹‰ambiguous sentenceä¸­ç¿»è¯‘çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†ä¸¤ç§æ”¹è¿›æ–¹æ³•ï¼Œé€šè¿‡åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ å’Œç²¾å¿ƒç¼–è¾‘çš„æ­§ä¹‰æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸ç°æœ‰çš„çŠ¶æ€æœºDeepLå’ŒNLLBç›¸å½“æˆ–è¶…è¶Šï¼Œåœ¨äº”ç§è¯­è¨€æ–¹å‘ä¸­å››ç§æ–¹å‘å–å¾—äº†æœ€ä½³æ•ˆæœã€‚æˆ‘ä»¬çš„ç ”ç©¶ä¸ºå°†LLMé€‚åº”åˆ°ç¿»è¯‘ä¸­çš„æ­§ä¹‰æä¾›äº†æœ‰ä»·å€¼çš„è§†è§’ã€‚
</details></li>
</ul>
<hr>
<h2 id="Hate-speech-detection-in-algerian-dialect-using-deep-learning"><a href="#Hate-speech-detection-in-algerian-dialect-using-deep-learning" class="headerlink" title="Hate speech detection in algerian dialect using deep learning"></a>Hate speech detection in algerian dialect using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11611">http://arxiv.org/abs/2309.11611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dihia Lanasri, Juan Olano, Sifal Klioui, Sin Liang Lee, Lamia Sekkai</li>
<li>for: å¸®åŠ©æŒæ¡åœ¨é˜¿æ‹‰ä¼¯è¯­è¨€ä¸Šçš„ä»‡æ¨è¨€è®ºæ£€æµ‹é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é˜¿å°”Ğ¶Ğ¸å°”è¯­ dialectä¸­ã€‚</li>
<li>methods: ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¶æ„å¯¹é˜¿å°”Ğ¶Ğ¸å°”ç¤¾äº¤åª’ä½“ä¸Šçš„çŸ­è®¯è¿›è¡Œåˆ†ç±»ï¼Œä»¥ç¡®å®šæ˜¯å¦åŒ…å«ä»‡æ¨è¨€è®ºã€‚</li>
<li>results: åœ¨å¯¹13500ä½™ä¸ªé˜¿å°”Ğ¶Ğ¸å°”ç¤¾äº¤åª’ä½“çŸ­è®¯çš„å®éªŒä¸­ï¼Œæå‡ºäº†ä¸€ç§å¯é çš„ä»‡æ¨è¨€è®ºæ£€æµ‹æ–¹æ³•ï¼Œå¹¶å–å¾—äº†æ‰¹åˆ¤æ€§çš„ç»“æœã€‚<details>
<summary>Abstract</summary>
With the proliferation of hate speech on social networks under different formats, such as abusive language, cyberbullying, and violence, etc., people have experienced a significant increase in violence, putting them in uncomfortable situations and threats. Plenty of efforts have been dedicated in the last few years to overcome this phenomenon to detect hate speech in different structured languages like English, French, Arabic, and others. However, a reduced number of works deal with Arabic dialects like Tunisian, Egyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in this work a complete approach for detecting hate speech on online Algerian messages. Many deep learning architectures have been evaluated on the corpus we created from some Algerian social networks (Facebook, YouTube, and Twitter). This corpus contains more than 13.5K documents in Algerian dialect written in Arabic, labeled as hateful or non-hateful. Promising results are obtained, which show the efficiency of our approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:éšç€ç¤¾äº¤åª’ä½“ä¸Šä¸åŒå½¢å¼çš„ä»‡æ¨è¨€è¯­ã€ç½‘ç»œæ¬ºå‡Œå’Œæš´åŠ›ç­‰ç­‰çš„æ™®åŠï¼Œäººä»¬å—åˆ°äº†ä¸é€‚çš„æƒ…å†µå’Œå¨èƒã€‚è¿‡å»å‡ å¹´ï¼Œä¸ºäº†è§£å†³è¿™ç§ç°è±¡ï¼Œå„ç§åŠªåŠ›å·²ç»æŠ•å…¥äº†å¾ˆå¤šæ—¶é—´å’Œç²¾åŠ›ï¼Œä»¥æ£€æµ‹ä¸åŒçš„ç»“æ„è¯­è¨€ä¸­çš„ä»‡æ¨è¨€è¯­ï¼Œå¦‚è‹±è¯­ã€æ³•è¯­ã€é˜¿æ‹‰ä¼¯è¯­ç­‰ç­‰ã€‚ç„¶è€Œï¼Œå¯¹äºé˜¿æ‹‰ä¼¯ Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚ï¼Œå¦‚çªå°¼æ–¯ã€åŸƒåŠå’Œ Golfo çš„ç ”ç©¶ç›¸å¯¹è¾ƒå°‘ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸ªç©ºç™½ï¼Œæˆ‘ä»¬åœ¨è¿™å·¥ä½œä¸­æå‡ºäº†ä¸€ä¸ªå®Œæ•´çš„æ–¹æ³•ï¼Œç”¨äºåœ¨åœ¨çº¿é˜¿å°”åŠåˆ©äºšæ¶ˆæ¯ä¸­æ£€æµ‹ä»‡æ¨è¨€è¯­ã€‚æˆ‘ä»¬åœ¨ä¸€äº›é˜¿å°”åŠåˆ©äºšç¤¾äº¤åª’ä½“ï¼ˆFacebookã€YouTubeå’ŒTwitterï¼‰ä¸Šåˆ›å»ºäº†ä¸€ä¸ªå¤§é‡çš„ corpusï¼ŒåŒ…æ‹¬13500ä½™ä¸ªæ–‡æ¡£ï¼Œç”¨é˜¿å°”åŠåˆ©äºš Ğ´Ğ¸Ğ°Ğ»ĞµĞºÑ‚çš„é˜¿æ‹‰ä¼¯è¯­ä¹¦å†™ï¼Œæ ‡æ³¨ä¸ºæœ‰ä»‡æ¨æˆ–æ— ä»‡æ¨ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¹¶è·å¾—äº†è‰¯å¥½çš„ç»“æœï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="SpeechAlign-a-Framework-for-Speech-Translation-Alignment-Evaluation"><a href="#SpeechAlign-a-Framework-for-Speech-Translation-Alignment-Evaluation" class="headerlink" title="SpeechAlign: a Framework for Speech Translation Alignment Evaluation"></a>SpeechAlign: a Framework for Speech Translation Alignment Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11585">http://arxiv.org/abs/2309.11585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Belen Alastruey, Aleix Sant, Gerard I. GÃ¡llego, David Dale, Marta R. Costa-jussÃ </li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦ä¸ºäº†è¯„ä¼°speechæ¨¡å‹ä¸­çš„source-targetå¯¹åº”é—®é¢˜æä¾›äº†ä¸€ä¸ªæ¡†æ¶ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šé¦–å…ˆï¼Œå®ƒå¼•å…¥äº†ä¸€ä¸ªè‹±æ–‡-å¾·è¯­æ–‡æœ¬ç¿»è¯‘é‡‘æ ‡å¯¹datasetï¼Œç”¨äºå»ºç«‹è¯„ä¼°æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œå®ƒå¼•å…¥äº†ä¸¤ç§æ–°çš„ç²¾åº¦æŒ‡æ ‡ï¼Œå³Speech Alignment Error Rate (SAER)å’ŒTime-weighted Speech Alignment Error Rate (TW-SAER)ï¼Œç”¨äºè¯„ä¼°speechæ¨¡å‹çš„å¯¹åº”è´¨é‡ã€‚</li>
<li>results: é€šè¿‡å‘å¸ƒSpeechAlignæ¡†æ¶ï¼Œè¿™ç¯‡è®ºæ–‡ä¸ºspeechæ¨¡å‹è¯„ä¼°æä¾›äº†å¯ accessibleçš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶é€šè¿‡ä½¿ç”¨è¿™ä¸ªæ¡†æ¶å¯¹å¼€æºSpeech Translationæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚<details>
<summary>Abstract</summary>
Speech-to-Speech and Speech-to-Text translation are currently dynamic areas of research. To contribute to these fields, we present SpeechAlign, a framework to evaluate the underexplored field of source-target alignment in speech models. Our framework has two core components. First, to tackle the absence of suitable evaluation datasets, we introduce the Speech Gold Alignment dataset, built upon a English-German text translation gold alignment dataset. Secondly, we introduce two novel metrics, Speech Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate (TW-SAER), to evaluate alignment quality in speech models. By publishing SpeechAlign we provide an accessible evaluation framework for model assessment, and we employ it to benchmark open-source Speech Translation models.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>è½¬æ¢ç»™å®šæ–‡æœ¬åˆ°ç®€åŒ–ä¸­æ–‡ã€‚</SYS>ç°åœ¨æ¼”ç¤ºçš„ Speech-to-Speech å’Œ Speech-to-Text ç¿»è¯‘æ˜¯ç ”ç©¶é¢†åŸŸçš„åŠ¨æ€é¢†åŸŸã€‚ä¸ºäº†è´¡çŒ®è¿™äº›é¢†åŸŸï¼Œæˆ‘ä»¬æå‡º SpeechAlign æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°speechæ¨¡å‹ä¸­source-targetå¯¹é½çš„é¢†åŸŸã€‚æˆ‘ä»¬çš„æ¡†æ¶æœ‰ä¸¤ä¸ªæ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œç”±äºç¼ºä¹é€‚åˆçš„è¯„ä¼°æ•°æ®é›†ï¼Œæˆ‘ä»¬å¼•å…¥ Speech Gold Alignment æ•°æ®é›†ï¼ŒåŸºäºè‹±è¯­-å¾·è¯­æ–‡æœ¬ç¿»è¯‘é‡‘æ ‡Alignmentæ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥ä¸¤ç§æ–°çš„æŒ‡æ ‡ï¼ŒSpeech Alignment Error Rate (SAER) å’Œ Time-weighted Speech Alignment Error Rate (TW-SAER)ï¼Œç”¨äºè¯„ä¼°å¯¹é½è´¨é‡åœ¨speechæ¨¡å‹ä¸­ã€‚é€šè¿‡å‘å¸ƒ SpeechAlignï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯è®¿é—®çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥å¯¹å¼€æº Speech Translation æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Singletons-and-Mention-based-Features-in-Coreference-Resolution-via-Multi-task-Learning-for-Better-Generalization"><a href="#Incorporating-Singletons-and-Mention-based-Features-in-Coreference-Resolution-via-Multi-task-Learning-for-Better-Generalization" class="headerlink" title="Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization"></a>Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11582">http://arxiv.org/abs/2309.11582</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yilunzhu/coref-mtl">https://github.com/yilunzhu/coref-mtl</a></li>
<li>paper_authors: Yilun Zhu, Siyao Peng, Sameer Pradhan, Amir Zeldes</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜è‹±è¯­æ ¸å¿ƒå…±referencingè§£å†³æ–¹æ³•ä¸­çš„æåŠæ£€æµ‹æ­¥éª¤ï¼Œä»¥æé«˜æ ¸å¿ƒå…±referencingçš„å‡†ç¡®ç‡å’ŒRobustnessã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹æ³•ï¼Œå­¦ä¹ å•ä¸ªæåŠspançš„ç‰¹å¾ä»¥åŠå®ä½“ç±»å‹å’Œä¿¡æ¯çŠ¶æ€çš„ç‰¹å¾ï¼Œä»¥æé«˜æ ¸å¿ƒå…±referencingçš„å‡†ç¡®ç‡å’ŒRobustnessã€‚</li>
<li>results: æœ¬ç ”ç©¶åœ¨OntoGUMbenchmarkä¸Š achieveæ–°çš„çŠ¶æ€æœºåˆ¶å¾—åˆ† (+2.7ç‚¹)ï¼Œå¹¶åœ¨å¤šä¸ªout-of-domainæ•°æ®é›†ä¸Šæé«˜äº†Robustness (+2.3ç‚¹çš„å¹³å‡æé«˜å€¼)ï¼Œè¿™äº›æé«˜å¯èƒ½æ˜¯ç”±äºæ›´å¥½çš„æåŠæ£€æµ‹å’Œæ›´å¤šçš„æ•°æ®æ¥è‡ªå•ä¸ªæåŠspançš„ä½¿ç”¨æ‰€è‡´ã€‚<details>
<summary>Abstract</summary>
Previous attempts to incorporate a mention detection step into end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention span data as well as other entity information. This paper presents a coreference model that learns singletons as well as features such as entity type and information status via a multi-task learning-based approach. This approach achieves new state-of-the-art scores on the OntoGUM benchmark (+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3 points on average), likely due to greater generalizability for mention detection and utilization of more data from singletons when compared to only coreferent mention pair matching.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…ˆå‰çš„å°è¯•å°†æåŠæ£€æµ‹æ­¥éª¤åŒ…å«åœ¨è‹±è¯­çš„ç«¯åˆ°ç«¯ç¥ç»æ ¸å¿ƒreferencingä¸­ï¼Œå—åˆ°ç¼ºä¹å•ä¸ªæåŠè·¨åº¦æ•°æ®ä»¥åŠå…¶ä»–å®ä½“ä¿¡æ¯çš„é™åˆ¶ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ ¸å¿ƒæ¨¡å‹ï¼Œå¯ä»¥å­¦ä¹ å•ä¸ªæåŠä»¥åŠå®ä½“ç±»å‹å’Œä¿¡æ¯çŠ¶æ€ç­‰ç‰¹å¾ï¼Œä½¿ç”¨å¤šä»»åŠ¡å­¦ä¹ çš„æ–¹å¼ã€‚è¿™ç§æ–¹æ³•åœ¨OntoGUM benchmarkä¸Šè¾¾åˆ°äº†æ–°çš„çŠ¶æ€æ€æ ‡å‡†åˆ†ï¼ˆ+2.7åˆ†ï¼‰ï¼Œå¹¶åœ¨å¤šä¸ª OUT-OF-DOMAIN æ•°æ®é›†ä¸Šæé«˜äº†é²æ£’æ€§ï¼ˆå¹³å‡+2.3åˆ†ï¼‰ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ›´å¥½çš„æåŠæ£€æµ‹å’Œæ›´å¤šçš„æ•°æ®æ¥è‡ªå•ä¸ªæåŠ span çš„åˆ©ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Examining-the-Limitations-of-Computational-Rumor-Detection-Models-Trained-on-Static-Datasets"><a href="#Examining-the-Limitations-of-Computational-Rumor-Detection-Models-Trained-on-Static-Datasets" class="headerlink" title="Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets"></a>Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11576">http://arxiv.org/abs/2309.11576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yida Mu, Xingyi Song, Kalina Bontcheva, Nikolaos Aletras</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å†…å®¹åŸºäºå’ŒContextåŸºäºçš„è°£è¨€æ¢æµ‹æ¨¡å‹åœ¨æ¢æµ‹æ–°ã€æœªçŸ¥è°£è¨€æ–¹é¢çš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å®éªŒæ–¹æ³•æ¥è¯„ä¼°å†…å®¹åŸºäºå’ŒContextåŸºäºçš„è°£è¨€æ¢æµ‹æ¨¡å‹åœ¨æ¢æµ‹æ–°ã€æœªçŸ¥è°£è¨€æ–¹é¢çš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒContextåŸºäºçš„æ¨¡å‹ä»ç„¶å—åˆ°æ¥æºå¸–å­ä¿¡æ¯çš„é™åˆ¶ï¼Œå¹¶ä¸”å¿½ç•¥äº†ä¸Šä¸‹æ–‡ä¿¡æ¯çš„é‡è¦ä½œç”¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¢è®¨äº†æ•°æ®åˆ†å‰²ç­–ç•¥å¯¹åˆ†ç±»å™¨æ€§èƒ½çš„å½±å“ï¼Œå¹¶æä¾›äº†å®è·µçš„å»ºè®®æ¥é™ä½é™æ€æ•°æ®é›†ä¸­çš„æ—¶é—´æ¦‚å¿µæ¼‚ç§»çš„å½±å“ã€‚<details>
<summary>Abstract</summary>
A crucial aspect of a rumor detection model is its ability to generalize, particularly its ability to detect emerging, previously unknown rumors. Past research has indicated that content-based (i.e., using solely source posts as input) rumor detection models tend to perform less effectively on unseen rumors. At the same time, the potential of context-based models remains largely untapped. The main contribution of this paper is in the in-depth evaluation of the performance gap between content and context-based models specifically on detecting new, unseen rumors. Our empirical findings demonstrate that context-based models are still overly dependent on the information derived from the rumors' source post and tend to overlook the significant role that contextual information can play. We also study the effect of data split strategies on classifier performance. Based on our experimental results, the paper also offers practical suggestions on how to minimize the effects of temporal concept drift in static datasets during the training of rumor detection methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸€ä¸ªé‡è¦çš„å™±å¤´æ£€æµ‹æ¨¡å‹ç‰¹ç‚¹æ˜¯å…¶èƒ½å¤Ÿæ€»ç»“ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹å‡ºç°åœ¨æœªçŸ¥å™±å¤´ã€‚è¿‡å»çš„ç ”ç©¶è¡¨æ˜ï¼Œå«æœ‰åª’ä½“æ–‡ç« ä»…ä½œè¾“å…¥çš„å†…å®¹åŸºäºå™±å¤´æ£€æµ‹æ¨¡å‹åœ¨æœªçœ‹è¿‡çš„å™±å¤´ä¸Šè¡¨ç°è¾ƒå·®ã€‚åŒæ—¶ï¼Œå™è¿°åŸºäºæ¨¡å‹çš„æ½œåŠ›ä»ç„¶æœªå¾—åˆ°å……åˆ†åˆ©ç”¨ã€‚æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®åœ¨äºå¯¹å†…å®¹å’Œå™è¿°åŸºäºæ¨¡å‹çš„æ€§èƒ½å·®å¼‚è¿›è¡Œæ·±å…¥è¯„ä¼°ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹æ–°çš„ã€æœªçŸ¥å™±å¤´ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå™è¿°åŸºäºæ¨¡å‹ä»ç„¶è¿‡åˆ†ä¾èµ–æºåª’ä½“æ–‡ç« æä¾›çš„ä¿¡æ¯ï¼Œè€Œå¿½è§†äº†Contextualä¿¡æ¯çš„é‡è¦ä½œç”¨ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†æ•°æ®åˆ†è£‚ç­–ç•¥å¯¹åˆ†ç±»å™¨æ€§èƒ½çš„å½±å“ã€‚æ ¹æ®æˆ‘ä»¬çš„å®éªŒç»“æœï¼Œæ–‡ç« è¿˜æä¾›äº†å®è·µçš„å»ºè®®ï¼Œä»¥é™ä½åœ¨è®­ç»ƒå™±å¤´æ£€æµ‹æ–¹æ³•æ—¶çš„æ—¶é—´æ¦‚å¿µé€€å˜çš„å½±å“ã€‚
</details></li>
</ul>
<hr>
<h2 id="SignBank-Multilingual-Sign-Language-Translation-Dataset"><a href="#SignBank-Multilingual-Sign-Language-Translation-Dataset" class="headerlink" title="SignBank+: Multilingual Sign Language Translation Dataset"></a>SignBank+: Multilingual Sign Language Translation Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11566">http://arxiv.org/abs/2309.11566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Moryossef, Zifan Jiang</li>
<li>for: æé«˜æ‰‹è¯­æœºå™¨ç¿»è¯‘é¢†åŸŸçš„ç ”ç©¶ï¼Œå¼ºè°ƒæ•°æ®è´¨é‡å’Œç¿»è¯‘ç³»ç»Ÿç®€åŒ–ã€‚</li>
<li>methods: ä»‹ç»SignBank+æ•°æ®é›†ï¼Œæ˜¯Optimized for machine translationçš„çº¯å‡€ç‰ˆSignBankæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ç®€å•çš„æ–‡æœ¬åˆ°æ–‡æœ¬ç¿»è¯‘æ–¹æ³•ã€‚</li>
<li>results: è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨SignBank+æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹è¶…è¿‡åŸå§‹æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹ï¼Œåˆ›é€ æ–°çš„benchmarkå’Œæä¾›å¼€æ”¾èµ„æº Ğ´Ğ»Ñæœªæ¥ç ”ç©¶ã€‚<details>
<summary>Abstract</summary>
This work advances the field of sign language machine translation by focusing on dataset quality and simplification of the translation system. We introduce SignBank+, a clean version of the SignBank dataset, optimized for machine translation. Contrary to previous works that employ complex factorization techniques for translation, we advocate for a simplified text-to-text translation approach. Our evaluation shows that models trained on SignBank+ surpass those on the original dataset, establishing a new benchmark and providing an open resource for future research.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä¸ªç ”ç©¶æé«˜äº†æ‰‹è¯­æœºå™¨ç¿»è¯‘çš„é¢†åŸŸï¼Œå…³æ³¨æ•°æ®é›†è´¨é‡å’Œç¿»è¯‘ç³»ç»Ÿç®€åŒ–ã€‚æˆ‘ä»¬ä»‹ç»äº†SignBank+ï¼Œä¸€ä¸ªä¼˜åŒ–çš„æ‰‹è¯­æ•°æ®é›†ï¼Œé€‚ç”¨äºæœºå™¨ç¿»è¯‘ã€‚ä¸å‰æœŸå·¥ä½œä¸åŒï¼Œæˆ‘ä»¬ä¸»å¼ ä½¿ç”¨ç®€å•çš„æ–‡æœ¬åˆ°æ–‡æœ¬ç¿»è¯‘æ–¹æ³•ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼ŒåŸºäºSignBank+çš„æ¨¡å‹æ¯”åŸå§‹æ•°æ®é›†æ¨¡å‹æ›´é«˜æ•ˆï¼Œåˆ›é€ äº†æ–°çš„æ ‡å‡†å’Œæä¾›äº†æœªæ¥ç ”ç©¶çš„å¼€æ”¾èµ„æºã€‚
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-reinforcement-learning-with-natural-language-subgoals"><a href="#Hierarchical-reinforcement-learning-with-natural-language-subgoals" class="headerlink" title="Hierarchical reinforcement learning with natural language subgoals"></a>Hierarchical reinforcement learning with natural language subgoals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11564">http://arxiv.org/abs/2309.11564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Ahuja, Kavya Kopparapu, Rob Fergus, Ishita Dasgupta</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯å®ç°é•¿æœŸè¡Œä¸ºçš„ç›®æ ‡å¯¼å‘è¡Œä¸ºï¼Œä½†å®ç°åœ¨ç°å®ç¯å¢ƒä¸­å…·æœ‰æŒ‘æˆ˜ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†äººç±»æ•°æ®æ¥è½¯ç€ Parametrize Goal Spaceï¼Œä½¿ç”¨æ— ç»“æ„çš„è‡ªç„¶è¯­è¨€æ¥è¡¨ç¤ºè¿™ä¸ªç©ºé—´ã€‚</li>
<li>results: è¯¥æ–¹æ³•æ¯”ä¸“å®¶å¤åˆ¶è¡Œä¸ºå’Œæ²¡æœ‰è¿™ç§ç›‘ç£ç›®æ ‡ç©ºé—´çš„HRL betterè¡¨ç°ï¼Œè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥ç»“åˆäººç±»ä¸“å®¶ç›‘ç£å’Œå¥–åŠ±å­¦ä¹ çš„ä¼˜ç‚¹ã€‚<details>
<summary>Abstract</summary>
Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefits and flexibility of reinforcement learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
hierarchical reinforcement learning æ˜¯ä¸€ç§å¸å¼•äººçš„æ–¹æ³•ï¼Œå¯ä»¥å®ç°é•¿åºåˆ—åŠ¨ä½œçš„ç›®æ ‡è¡Œä¸ºã€‚ç„¶è€Œï¼Œåœ¨çœŸå®æˆ–å¼€æ”¾çš„ç¯å¢ƒä¸­å®ç°å…·æœ‰æŒ‘æˆ˜ã€‚ä¸€ä¸ªä¸»è¦æŒ‘æˆ˜æ˜¯æ‰¾åˆ°é€‚å½“çš„ä¸‹ä¸€çº§ç›®æ ‡ç©ºé—´ï¼Œä»¥å®ç°å±‚æ¬¡ç»“æ„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œä½¿ç”¨äººç±»è§£å†³è¿™äº›ä»»åŠ¡çš„æ•°æ®æ¥è½¯ç€å†Œè¿™ä¸ªç©ºé—´ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨æ— ç»“æ„çš„è‡ªç„¶è¯­è¨€æ¥ parameterizeè¿™ä¸ªç©ºé—´ã€‚è¿™æœ‰ä¸¤ä¸ªä¼˜ç‚¹ï¼šé¦–å…ˆï¼Œå¯ä»¥è½»æ¾åœ°ä»ä¸ç†Ÿæ‚‰çš„äººå‚ä¸è€…ä¸­è·å¾—è¿™äº›æ•°æ®ï¼›å…¶æ¬¡ï¼Œå®ƒå¤Ÿçµæ´»ï¼Œå¯ä»¥è¡¨ç¤ºäººç±»ç›¸å…³ä»»åŠ¡ä¸­çš„å¹¿æ³›ä¸‹ä¸€çº§ç›®æ ‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ¯”ä¸åŒæ‰©å±•å­¦ä¹ çš„ä»£ç†äººå’Œä¸å¸¦æœ‰æ­¤ååŠ©ä¸‹ä¸€çº§ç›®æ ‡ç©ºé—´çš„ HRL è¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ç§ç»“åˆäººç±»ä¸“å®¶æŒ‡å¯¼å’Œå¼ºåŒ–å­¦ä¹ çš„æ–°æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="DreamLLM-Synergistic-Multimodal-Comprehension-and-Creation"><a href="#DreamLLM-Synergistic-Multimodal-Comprehension-and-Creation" class="headerlink" title="DreamLLM: Synergistic Multimodal Comprehension and Creation"></a>DreamLLM: Synergistic Multimodal Comprehension and Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11499">http://arxiv.org/abs/2309.11499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RunpeiDong/DreamLLM">https://github.com/RunpeiDong/DreamLLM</a></li>
<li>paper_authors: Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§èƒ½å¤Ÿç”Ÿæˆå¤šmodal language modelï¼ˆMLLMï¼‰ï¼Œå…·æœ‰è¾ƒå°‘æ³¨æ„çš„å¤šmodalç†è§£å’Œåˆ›é€ ä¹‹é—´çš„å…±è¯†ã€‚</li>
<li>methods:  DreamLLM ä½¿ç”¨ä¸¤ä¸ªåŸºæœ¬åŸåˆ™ï¼šé¦–å…ˆï¼Œé€šè¿‡ç›´æ¥æŠ½æ ·åœ¨åŸå§‹å¤šmodalç©ºé—´è¿›è¡Œè¯­è¨€å’Œå›¾åƒ posterior çš„ç”Ÿæˆæ¨¡å‹åŒ–ï¼Œä»¥é¿å… CLIP ç­‰å¤–éƒ¨ç‰¹å¾æå–å™¨çš„å±€é™æ€§å’Œä¿¡æ¯æŸå¤±ï¼Œä»è€Œè·å¾—æ›´å…¨é¢çš„å¤šmodalç†è§£ã€‚å…¶æ¬¡ï¼Œ DreamLLM å¯ä»¥ç”Ÿæˆ raw çš„ã€æ··åˆçš„æ–‡æ¡£ï¼ŒåŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒå†…å®¹ï¼Œä»¥åŠæ— ç»“æ„çš„å¸ƒå±€ï¼Œä»è€Œå­¦ä¹ æ‰€æœ‰çš„ conditionalã€marginal å’Œ joint å¤šmodalåˆ†å¸ƒã€‚</li>
<li>results: DreamLLM èƒ½å¤Ÿç”Ÿæˆå… Training çš„å¤šmodalé€šç”¨ä¸“å®¶ï¼Œåœ¨å¤šmodalæ€»ä½“ ÑĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ğ°Ñ…ä¸­è¡¨ç°å‡ºè‰²ï¼Œå—ç›Šäºæé«˜çš„å­¦ä¹ å…±è¯†ã€‚<details>
<summary>Abstract</summary>
This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
<ol>
<li>Generative modeling of both language and image posteriors through direct sampling in the raw multimodal space. This approach bypasses the limitations of external feature extractors like CLIP and enables a more comprehensive understanding of multimodal information.2. Generation of raw, interleaved documents that model both text and image contents, as well as unstructured layouts. This allows DreamLLM to effectively learn all conditional, marginal, and joint multimodal distributions.As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content, demonstrating superior performance as a zero-shot multimodal generalist. Comprehensive experiments highlight the enhanced learning synergy achieved by DreamLLM.</details></li>
</ol>
<hr>
<h2 id="Controlled-Generation-with-Prompt-Insertion-for-Natural-Language-Explanations-in-Grammatical-Error-Correction"><a href="#Controlled-Generation-with-Prompt-Insertion-for-Natural-Language-Explanations-in-Grammatical-Error-Correction" class="headerlink" title="Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction"></a>Controlled Generation with Prompt Insertion for Natural Language Explanations in Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11439">http://arxiv.org/abs/2309.11439</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masahiro Kaneko, Naoaki Okazaki</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åä¸ºæ§åˆ¶ç”Ÿæˆï¼ˆPrompt Insertionï¼ŒPIï¼‰çš„æ–¹æ³•ï¼Œç”¨äºä½¿å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼ŒLLMsï¼‰å¯ä»¥åœ¨è‡ªç„¶è¯­è¨€ä¸­æä¾›å¯¹ grammar å’Œè¯­æ³•é”™è¯¯ corrections çš„ç›´æ¥è§£é‡Šã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº† Large Language Models (LLMs) å’Œ Prompt Insertion (PI) æ–¹æ³•æ¥ç”Ÿæˆå¯¹ grammar å’Œè¯­æ³•é”™è¯¯ corrections çš„ç›´æ¥è§£é‡Šã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ PI æ–¹æ³•å¯ä»¥ä½¿ LLMs èƒ½å¤Ÿç›´æ¥åœ¨è‡ªç„¶è¯­è¨€ä¸­æä¾›å¯¹ grammar å’Œè¯­æ³•é”™è¯¯ corrections çš„è§£é‡Šï¼Œå¹¶ä¸”å¯ä»¥æé«˜å¯¹ correction reasons çš„ç”Ÿæˆæ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
In Grammatical Error Correction (GEC), it is crucial to ensure the user's comprehension of a reason for correction. Existing studies present tokens, examples, and hints as to the basis for correction but do not directly explain the reasons for corrections. Although methods that use Large Language Models (LLMs) to provide direct explanations in natural language have been proposed for various tasks, no such method exists for GEC. Generating explanations for GEC corrections involves aligning input and output tokens, identifying correction points, and presenting corresponding explanations consistently. However, it is not straightforward to specify a complex format to generate explanations, because explicit control of generation is difficult with prompts. This study introduces a method called controlled generation with Prompt Insertion (PI) so that LLMs can explain the reasons for corrections in natural language. In PI, LLMs first correct the input text, and then we automatically extract the correction points based on the rules. The extracted correction points are sequentially inserted into the LLM's explanation output as prompts, guiding the LLMs to generate explanations for the correction points. We also create an Explainable GEC (XGEC) dataset of correction reasons by annotating NUCLE, CoNLL2013, and CoNLL2014. Although generations from GPT-3 and ChatGPT using original prompts miss some correction points, the generation control using PI can explicitly guide to describe explanations for all correction points, contributing to improved performance in generating correction reasons.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¯­æ³•é”™è¯¯ corrections (GEC) ä¸­ï¼Œç¡®ä¿ç”¨æˆ·ç†è§£ correction çš„ç†ç”±æ˜¯å…³é”®ã€‚ç°æœ‰çš„ç ”ç©¶æä¾›äº† tokensã€ä¾‹å­å’Œæç¤ºï¼Œä½†æ²¡æœ‰ç›´æ¥è§£é‡Š correction çš„ç†ç”±ã€‚è™½ç„¶ä½¿ç”¨ Large Language Models (LLMs) æä¾›ç›´æ¥è§£é‡Šçš„è‡ªç„¶è¯­è¨€æ–¹æ³•å·²ç»è¢«æå‡º Ğ´Ğ»Ñå¤šä¸ªä»»åŠ¡ï¼Œä½†å¯¹ GEC çš„æ–¹æ³•ä¸å­˜åœ¨ã€‚ç”Ÿæˆ GEC  corrections çš„è§£é‡Š involves å¯¹è¾“å…¥å’Œè¾“å‡º tokens è¿›è¡Œå¯¹åº”ã€ç¡®å®š correction ç‚¹å¹¶æä¾›ç›¸åº”çš„è§£é‡Šã€‚ç„¶è€Œï¼Œä¸æ˜¯ straightforward  specify å¤æ‚çš„ç”Ÿæˆæ ¼å¼ï¼Œå› ä¸ºExplicit æ§åˆ¶ç”Ÿæˆæ˜¯ difficult çš„ã€‚è¿™ç§ç ”ç©¶å¼•å…¥ä¸€ç§åä¸º controlled generation with Prompt Insertion (PI) çš„æ–¹æ³•ï¼Œä½¿å¾— LLMs å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æ¥è§£é‡Š correction çš„ç†ç”±ã€‚åœ¨ PI ä¸­ï¼ŒLLMs é¦–å…ˆ corrections è¾“å…¥æ–‡æœ¬ï¼Œç„¶åæˆ‘ä»¬è‡ªåŠ¨æå– correction ç‚¹åŸºäºè§„åˆ™ã€‚æå–çš„ correction ç‚¹è¢«è‡ªåŠ¨æ’å…¥ LLMs çš„è§£é‡Šè¾“å‡ºä¸­ä½œä¸ºæç¤ºï¼Œå¯¼å¼• LLMs ç”Ÿæˆå¯¹ correction ç‚¹çš„è§£é‡Šã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ª Explainable GEC (XGEC) æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å« correction ç†ç”±çš„æ³¨é‡Šã€‚è™½ç„¶ GPT-3 å’Œ ChatGPT ä½¿ç”¨åŸå§‹æç¤ºç”Ÿæˆçš„ Generation ç¼ºå°‘ä¸€äº› correction ç‚¹ï¼Œä½†ä½¿ç”¨ PI çš„ç”Ÿæˆæ§åˆ¶å¯ä»¥æ˜ç¡®æŒ‡å¯¼ LLMs ç”Ÿæˆå¯¹ correction ç‚¹çš„è§£é‡Šï¼Œä»è€Œæé«˜ç”Ÿæˆ correction ç†ç”±çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Kosmos-2-5-A-Multimodal-Literate-Model"><a href="#Kosmos-2-5-A-Multimodal-Literate-Model" class="headerlink" title="Kosmos-2.5: A Multimodal Literate Model"></a>Kosmos-2.5: A Multimodal Literate Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11419">http://arxiv.org/abs/2309.11419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/Kosmos2.5">https://github.com/kyegomez/Kosmos2.5</a></li>
<li>paper_authors: Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, Furu Wei</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†å¼€å‘ä¸€ç§å¯ä»¥è¯»å–æ–‡æœ¬å……æ»¡å›¾åƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå³ Kosmos-2.5ã€‚</li>
<li>methods: è¯¥æ¨¡å‹é‡‡ç”¨äº†å¤šmodalæ–‡æœ¬æ¨¡å‹ï¼Œé€šè¿‡å…±äº«è½¬æ¢å™¨æ¶æ„ã€ä»»åŠ¡ç‰¹å®šçš„æç¤ºå’Œçµæ´»çš„æ–‡æœ¬è¡¨ç¤ºæ¥å®ç°æ–‡æœ¬è¯†åˆ«å’Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚</li>
<li>results: æ¨¡å‹åœ¨ç»ˆåˆ°çº§æ–‡æ¡£çº§æ–‡æœ¬è¯†åˆ«å’Œå›¾åƒåˆ°markdownæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥é€‚åº”å„ç§ä¸åŒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç²¾åº¦å¾®è°ƒæ¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚<details>
<summary>Abstract</summary>
We present Kosmos-2.5, a multimodal literate model for machine reading of text-intensive images. Pre-trained on large-scale text-intensive images, Kosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1) generating spatially-aware text blocks, where each block of text is assigned its spatial coordinates within the image, and (2) producing structured text output that captures styles and structures into the markdown format. This unified multimodal literate capability is achieved through a shared Transformer architecture, task-specific prompts, and flexible text representations. We evaluate Kosmos-2.5 on end-to-end document-level text recognition and image-to-markdown text generation. Furthermore, the model can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images. This work also paves the way for the future scaling of multimodal large language models.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»Kosmos-2.5ï¼Œä¸€ç§å¤šModal literateæ¨¡å‹ï¼Œç”¨äºæœºå™¨é˜…è¯»å›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ã€‚Kosmos-2.5åœ¨ä¸¤ä¸ªä¸åŒ yet ç›¸äº’åä½œçš„è¯‘å†™ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼šï¼ˆ1ï¼‰ç”Ÿæˆå…·æœ‰ç©ºé—´åæ ‡çš„æ–‡æœ¬å—ï¼Œæ¯ä¸ªæ–‡æœ¬å—åœ¨å›¾åƒä¸­è¢«åˆ†é…ç‰¹å®šçš„ç©ºé—´åæ ‡ï¼›ï¼ˆ2ï¼‰ç”Ÿæˆç¬¦åˆmarkdownæ ¼å¼çš„ç»“æ„åŒ–æ–‡æœ¬è¾“å‡ºã€‚è¿™ç§å¤šModal literateèƒ½åŠ›é€šè¿‡å…±äº«Transformeræ¶æ„ã€ä»»åŠ¡ç‰¹å®šçš„æç¤ºå’Œçµæ´»æ–‡æœ¬è¡¨ç¤ºæ–¹å¼å®ç°ã€‚æˆ‘ä»¬å¯¹Kosmos-2.5è¿›è¡Œäº†ç«¯åˆ°ç«¯æ–‡æ¡£çº§æ–‡æœ¬è¯†åˆ«å’Œå›¾åƒåˆ°markdownæ–‡æœ¬ç”Ÿæˆçš„è¯„ä¼°ã€‚æ­¤å¤–ï¼Œé€šè¿‡ç²¾å¿ƒå¾®è°ƒï¼Œå¯ä»¥å°†æ¨¡å‹é€‚åº”ä¸åŒçš„æç¤ºä»»åŠ¡ï¼Œä½¿å…¶æˆä¸ºå®é™…åº”ç”¨ä¸­æ–‡æœ¬å¼ºåº¦å›¾åƒç†è§£ä»»åŠ¡çš„é€šç”¨å·¥å…·ã€‚æ­¤é¡¹å·¥ä½œä¹Ÿä¸ºæœªæ¥æ‰©å¤§å¤šModalå¤§è¯­è¨€æ¨¡å‹çš„å‰æ™¯é“ºå¹³äº†è·¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Safurai-001-New-Qualitative-Approach-for-Code-LLM-Evaluation"><a href="#Safurai-001-New-Qualitative-Approach-for-Code-LLM-Evaluation" class="headerlink" title="Safurai 001: New Qualitative Approach for Code LLM Evaluation"></a>Safurai 001: New Qualitative Approach for Code LLM Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11385">http://arxiv.org/abs/2309.11385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openai/human-eval">https://github.com/openai/human-eval</a></li>
<li>paper_authors: Davide Cifarelli, Leonardo Boiardi, Alessandro Puppo</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§æ–°çš„å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œç”¨äºç¼–ç¨‹ååŠ©é¢†åŸŸã€‚</li>
<li>methods: è¿™ä¸ªæ¨¡å‹é©±åŠ¨äº†æœ€æ–°çš„ç¼–ç¨‹LLMçš„è¿›æ­¥ï¼Œå¹¶ä¸”é€šè¿‡æ•°æ®å·¥ç¨‹ï¼ˆåŒ…æ‹¬æœ€æ–°çš„æ•°æ®è½¬æ¢æŠ€æœ¯å’Œæç¤ºå·¥ç¨‹ï¼‰å’ŒæŒ‡ä»¤è°ƒæ•´æ¥æé«˜æ€§èƒ½ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼ŒSafurai-001å¯ä»¥è¶…è¶ŠGPT-3.5å’ŒWizardCoderåœ¨ä»£ç å¯è¯»æ€§æ–¹é¢ï¼Œæé«˜1.58%å’Œ18.78%ã€‚<details>
<summary>Abstract</summary>
This paper presents Safurai-001, a new Large Language Model (LLM) with significant potential in the domain of coding assistance. Driven by recent advancements in coding LLMs, Safurai-001 competes in performance with the latest models like WizardCoder [Xu et al., 2023], PanguCoder [Shen et al., 2023] and Phi-1 [Gunasekar et al., 2023] but aims to deliver a more conversational interaction. By capitalizing on the progress in data engineering (including latest techniques of data transformation and prompt engineering) and instruction tuning, this new model promises to stand toe-to-toe with recent closed and open source developments. Recognizing the need for an efficacious evaluation metric for coding LLMs, this paper also introduces GPT4-based MultiParameters, an evaluation benchmark that harnesses varied parameters to present a comprehensive insight into the models functioning and performance. Our assessment shows that Safurai-001 can outperform GPT-3.5 by 1.58% and WizardCoder by 18.78% in the Code Readability parameter and more.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Studying-Lobby-Influence-in-the-European-Parliament"><a href="#Studying-Lobby-Influence-in-the-European-Parliament" class="headerlink" title="Studying Lobby Influence in the European Parliament"></a>Studying Lobby Influence in the European Parliament</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11381">http://arxiv.org/abs/2309.11381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aswin Suresh, Lazar Radojevic, Francesco Salvi, Antoine Magron, Victor Kristof, Matthias Grossglauser</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†ç ”ç©¶æ¬§æ´²è®®ä¼šï¼ˆEPï¼‰çš„æ³•åˆ¶ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°ä¸­åˆ©ç›Šé›†å›¢ï¼ˆ Lobbyï¼‰çš„å½±å“ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯ï¼Œæ”¶é›†å’Œåˆ†æäº†æ¬§æ´²è®®ä¼šæˆå‘˜ï¼ˆMEPï¼‰çš„è¨€è®ºå’Œåˆ©ç›Šé›†å›¢çš„ pozition çº¸ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›æ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼æ€§å’Œæ¨è®ºï¼Œå‘ç°MEPå’Œåˆ©ç›Šé›†å›¢ä¹‹é—´çš„å¯è§£é‡Šçš„è¿æ¥ã€‚åœ¨ç¼ºä¹ground truthæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†é—´æ¥éªŒè¯ï¼Œæ¯”è¾ƒå‘ç°çš„è¿æ¥ä¸æˆ‘ä»¬è‡ªå·± curaatedçš„Retweeté“¾æ¥å’Œå…¬å¼€çš„MEPä¼šè®®è®°å½•ã€‚æˆ‘ä»¬çš„bestæ–¹æ³•å¾—åˆ°äº†0.77çš„AUCåˆ†æ•°ï¼Œä¸å¤šä¸ªåŸºçº¿ç›¸æ¯”æ˜¾è‘—æ€§æ›´é«˜ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨æ¬§æ´²è®®ä¼šçš„æ³•åˆ¶ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ°ä¸­ï¼Œåˆ©ç›Šé›†å›¢å¯¹MEPçš„å½±å“å­˜åœ¨å¯è§£é‡Šçš„è¿æ¥ã€‚æˆ‘ä»¬å¯¹ relate  Lobby å’Œæ”¿æ²»åˆ†ç»„çš„MEPè¿›è¡Œäº†æ±‡æ€»åˆ†æï¼Œå‘ç°ä¸æ”¿æ²»åˆ†ç»„çš„æ„è¯†ç›¸ç¬¦ï¼ˆä¾‹å¦‚ï¼Œä¸­é—´å·¦æ´¾ç»„ç»‡ä¸ç¤¾ä¼šé—®é¢˜ç›¸å…³ï¼‰ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™é¡¹ç ”ç©¶ã€æ–¹æ³•ã€æ•°æ®å’Œç»“æœï¼Œæ˜¯ä¸ºäº†æé«˜æ°‘ä¸»æœºæ„å†…å¤æ‚å†³ç­–è¿‡ç¨‹çš„é€æ˜åº¦åšå‡ºäº†ä¸€æ­¥å‰è¿›ã€‚<details>
<summary>Abstract</summary>
We present a method based on natural language processing (NLP), for studying the influence of interest groups (lobbies) in the law-making process in the European Parliament (EP). We collect and analyze novel datasets of lobbies' position papers and speeches made by members of the EP (MEPs). By comparing these texts on the basis of semantic similarity and entailment, we are able to discover interpretable links between MEPs and lobbies. In the absence of a ground-truth dataset of such links, we perform an indirect validation by comparing the discovered links with a dataset, which we curate, of retweet links between MEPs and lobbies, and with the publicly disclosed meetings of MEPs. Our best method achieves an AUC score of 0.77 and performs significantly better than several baselines. Moreover, an aggregate analysis of the discovered links, between groups of related lobbies and political groups of MEPs, correspond to the expectations from the ideology of the groups (e.g., center-left groups are associated with social causes). We believe that this work, which encompasses the methodology, datasets, and results, is a step towards enhancing the transparency of the intricate decision-making processes within democratic institutions.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„æ–¹æ³•ï¼Œç”¨äºç ”ç©¶æ¬§æ´²è®®ä¼šï¼ˆEPï¼‰ä¸­åˆ©ç›Šé›†å›¢ï¼ˆæ¸¸è¯´è€…ï¼‰çš„å½±å“åŠ›ã€‚æˆ‘ä»¬æ”¶é›†äº†å’Œåˆ†æäº†æ¸¸è¯´è€…çš„ä½ç½®çº¸å’ŒEPè®®å‘˜ï¼ˆMEPï¼‰çš„æ¼”è®²æ–‡æœ¬ã€‚é€šè¿‡æ¯”è¾ƒè¿™äº›æ–‡æœ¬çš„å«ä¹‰ç›¸ä¼¼æ€§å’Œæ¨å¯¼å…³ç³»ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå‘ç°MEPå’Œæ¸¸è¯´è€…ä¹‹é—´çš„å¯è¯»å–è¿æ¥ã€‚åœ¨æ²¡æœ‰ground truth datasetsçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è¿›è¡Œäº†é—´æ¥éªŒè¯ï¼Œæ¯”è¾ƒå‘ç°çš„è¿æ¥ä¸æˆ‘ä»¬è‡ªå·±curateçš„æ¨ç‰¹é“¾æ¥å’ŒMEPå…¬å¼€çš„ä¼šè®®è®°å½•ã€‚æˆ‘ä»¬çš„æœ€ä½³æ–¹æ³•åœ¨AUCåˆ†æ•°0.77è¾¾åˆ°äº†ï¼Œå¹¶ä¸å¤šä¸ªåŸºelineç›¸æ¯”è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹å‘ç°çš„è¿æ¥è¿›è¡Œäº†èšåˆåˆ†æï¼Œå‘ç°ä¸ç›¸å…³çš„æ¸¸è¯´è€…å’Œæ”¿æ²»ç»„ç»‡ç›¸å¯¹åº”ã€‚è¿™ç§ç»“æœä¸æ”¿æ²»ç»„ç»‡çš„æ„è¯†ç›¸ç¬¦ï¼Œä¾‹å¦‚ä¸­é—´å·¦æ´¾ç»„ç»‡ä¸ç¤¾ä¼šé—®é¢˜ç›¸å…³ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ç§æ–¹æ³•ã€æ•°æ®å’Œç»“æœæ˜¯æ¨è¿›æ°‘ä¸»æœºæ„å†…å¤æ‚å†³ç­–è¿‡ç¨‹çš„ä¸€æ­¥ã€‚
</details></li>
</ul>
<hr>
<h2 id="GECTurk-Grammatical-Error-Correction-and-Detection-Dataset-for-Turkish"><a href="#GECTurk-Grammatical-Error-Correction-and-Detection-Dataset-for-Turkish" class="headerlink" title="GECTurk: Grammatical Error Correction and Detection Dataset for Turkish"></a>GECTurk: Grammatical Error Correction and Detection Dataset for Turkish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11346">http://arxiv.org/abs/2309.11346</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GGLAB-KU/gecturk">https://github.com/GGLAB-KU/gecturk</a></li>
<li>paper_authors: Atakan Kara, Farrin Marouf Sofian, Andrew Bond, GÃ¶zde GÃ¼l Åahin</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŒæ­¥æ•°æ®çš„Synthetic Data Generation Pipelineï¼Œç”¨äºè§£å†³åœŸè€³å…¶è¯­è‡ªç„¶è¯­è¨€å¤„ç† tasks ä¸­çš„æ•°æ®ç¼ºä¹é—®é¢˜ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å¤šç§å¤æ‚çš„å˜æ¢å‡½æ•°æ¥å®ç°æ›´ than 20 ä¸ªä¸“å®¶ä¿®æ”¹åçš„è¯­æ³•å’Œæ‹¼å†™è§„åˆ™ï¼Œå¹¶ä»ä¸“ä¸šç¼–è¾‘çš„æ–‡ç« ä¸­ derivation äº†130,000ä¸ªé«˜è´¨é‡çš„åŒæ­¥å¥å­ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡é€šè¿‡ä¸‰ç§åŸºçº¿æ¨¡å‹ï¼ˆneural machine translation, sequence tagging, prefix tuningï¼‰å®ç°äº†å¼ºå¤§çš„ç»“æœï¼Œå¹¶é€šè¿‡å¯¹å„ç§å°˜è‚¤æ•°æ®è¿›è¡Œè¯¦ç»†çš„å®éªŒæ¥è¯æ˜äº†è¯¥è®ºæ–‡çš„å¯é‡å¤æ€§å’Œç¨³å®šæ€§ã€‚<details>
<summary>Abstract</summary>
Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the task as i) neural machine translation, ii) sequence tagging, and iii) prefix tuning with a pretrained decoder-only model, achieving strong results. Furthermore, we perform exhaustive experiments on out-of-domain datasets to gain insights on the transferability and robustness of the proposed approaches. Our results suggest that our corpus, GECTurk, is high-quality and allows knowledge transfer for the out-of-domain setting. To encourage further research on Turkish GEC, we release our datasets, baseline models, and the synthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.
</details>
<details>
<summary>æ‘˜è¦</summary>
grammatical error detectionå’Œä¿®æ­£å·¥å…·ï¼ˆGECï¼‰å¯¹æœ¬åœ°è¯­è¨€å’Œç¬¬äºŒè¯­è¨€å­¦ä¹ è€…éƒ½æœ‰ç”¨ã€‚å¼€å‘è¿™äº›å·¥å…·éœ€è¦å¤§é‡å¹¶è¡Œã€æ³¨é‡Šçš„æ•°æ®ï¼Œä½†è¿™äº›æ•°æ®å¯¹å¤§å¤šæ•°è¯­è¨€è€Œè¨€ç½•è§ã€‚Synthetic dataç”Ÿæˆæ˜¯ä¸€ç§å¸¸è§çš„åŠæ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹äº morphologically richçš„è¯­è¨€å¦‚åœŸè€³å…¶æ¥è¯´ï¼ŒSynthetic dataç”Ÿæˆå¹¶ä¸ç®€å•ï¼Œå› ä¸ºå®ƒä»¬çš„å†™ä½œè§„åˆ™éœ€è¦ fonologicalã€morphologicalå’Œ sintacticä¿¡æ¯ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§çµæ´»å¯æ‰©å±•çš„Synthetic dataç”Ÿæˆç®¡é“ï¼Œå¯ä»¥è¦†ç›–More than 20ä¸ªä¸“å®¶ç²¾å¿ƒç¼–è¾‘çš„è¯­æ³•å’Œæ‹¼å†™è§„åˆ™ï¼ˆå³å†™ä½œè§„åˆ™ï¼‰ï¼Œé€šè¿‡å¤æ‚çš„è½¬æ¢å‡½æ•°æ¥å®ç°ã€‚é€šè¿‡è¿™ç§ç®¡é“ï¼Œæˆ‘ä»¬å¾—åˆ°äº†130,000ä¸ªé«˜è´¨é‡çš„å¹¶è¡Œå¥å­ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªæ›´çœŸå®çš„æµ‹è¯•é›†ï¼Œé€šè¿‡æ‰‹åŠ¨æ³¨é‡Šä¸€äº›ç”µå½±è¯„è®ºã€‚æˆ‘ä»¬å®ç°äº†ä¸‰ç§åŸºçº¿ï¼Œå³ neural machine translationã€sequence tagging å’Œ prefix tuning with a pretrained decoder-only modelï¼Œå–å¾—äº†å‡ºè‰²çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„å¯¹out-of-domainæ•°æ®é›†çš„å®éªŒï¼Œä»¥äº†è§£ææ¡ˆæ–¹æ³•çš„ä¼ è¾“æ€§å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¥å­åº“ï¼ŒGECTurkï¼Œå…·æœ‰é«˜è´¨é‡ï¼Œå¹¶å…è®¸çŸ¥è¯†ä¼ è¾“åˆ°out-of-domain Settingã€‚ä¸ºäº†ä¿ƒè¿›åœŸè€³å…¶GECçš„ç ”ç©¶ï¼Œæˆ‘ä»¬åœ¨https://github.com/GGLAB-KU/gecturkä¸Šå‘å¸ƒäº†æˆ‘ä»¬çš„æ•°æ®é›†ã€åŸºçº¿æ¨¡å‹å’ŒSynthetic dataç”Ÿæˆç®¡é“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Improving-Article-Classification-with-Edge-Heterogeneous-Graph-Neural-Networks"><a href="#Improving-Article-Classification-with-Edge-Heterogeneous-Graph-Neural-Networks" class="headerlink" title="Improving Article Classification with Edge-Heterogeneous Graph Neural Networks"></a>Improving Article Classification with Edge-Heterogeneous Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11341">http://arxiv.org/abs/2309.11341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyvykhang/edgehetero-nodeproppred">https://github.com/lyvykhang/edgehetero-nodeproppred</a></li>
<li>paper_authors: Khang Ly, Yury Kashnitsky, Savvas Chamezopoulos, Valeria Krzhizhanovskaya</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜æ–‡ç« åˆ†ç±»çš„æ€§èƒ½ï¼Œä½¿ç”¨ç®€å•çš„å›¾ neural network (GNN) æ‹“æ‰‘ï¼Œå¹¶å°†æ–‡ç« ä¸­çš„æ–‡æœ¬metadata è½¬åŒ–ä¸ºé«˜çº§ semanticsã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨ SciBERT ç”ŸæˆèŠ‚ç‚¹ç‰¹å¾ï¼Œä»¥æ•æ‰æ–‡ç« ä¸­çš„é«˜çº§ semanticsã€‚ç„¶åï¼Œä½¿ç”¨å®Œå…¨supervised æ¨å­¦çš„node classification è¿›è¡Œå®éªŒï¼Œä½¿ç”¨ Open Graph Benchmark (OGB) ogbn-arxiv æ•°æ®é›†å’Œ PubMed è‚¥ç˜¤æ•°æ®é›†ã€‚</li>
<li>results: ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨edge-heterogeneous graphs å¯ä»¥æé«˜ GNN æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”å¯ä»¥ä½¿ç”¨ç®€å•å’Œæµ…çš„ GNN æ‹“æ‰‘æ¥è¾¾åˆ°ä¸æ›´å¤æ‚çš„ç»“æ„ç›¸åŒçš„æ€§èƒ½ã€‚åœ¨ OGB ç«èµ›ä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†ç¬¬15åçš„æˆç»©ï¼ˆå‡†ç¡®ç‡ 74.61%ï¼‰ï¼Œå¹¶åœ¨ PubMed æ•°æ®é›†ä¸Šä¸ state-of-the-art GNN ç»“æ„ç›¸å½“ï¼ˆå‡†ç¡®ç‡ 89.88%ï¼‰ã€‚<details>
<summary>Abstract</summary>
Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition with a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with sub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures using a 2-layer GraphSAGE by including additional co-authorship edges in the graph (accuracy 89.88%). The implementation is available at: $\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}$.
</details>
<details>
<summary>æ‘˜è¦</summary>
classe research output into context-specific label taxonomies æ˜¯ä¸€ä¸ªå¤æ‚ä¸”æœ‰ relevance çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œ giventhe volume of existing å’Œ newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architectures. On ogbn-arxiv, we achieve a top-15 result in the OGB competition with a 2-layer GCN (accuracy 74.61%), being the highest-scoring solution with sub-1 million parameters. On PubMed, we closely trail SOTA GNN architectures using a 2-layer GraphSAGE by including additional co-authorship edges in the graph (accuracy 89.88%). The implementation is available at: $\href{https://github.com/lyvykhang/edgehetero-nodeproppred}{\text{https://github.com/lyvykhang/edgehetero-nodeproppred}$.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Data-Collection-and-Unsupervised-Learning-for-Code-switched-Tunisian-Arabic-Automatic-Speech-Recognition"><a href="#Leveraging-Data-Collection-and-Unsupervised-Learning-for-Code-switched-Tunisian-Arabic-Automatic-Speech-Recognition" class="headerlink" title="Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition"></a>Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11327">http://arxiv.org/abs/2309.11327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Amine Ben Abdallah, Ata Kabboudi, Amir Kanoun, Salah Zaiem</li>
<li>for: This paper is written for the purpose of developing an effective Automatic Speech Recognition (ASR) solution for dialects, specifically focusing on the Tunisian dialect.</li>
<li>methods: The paper explores self-supervision, semi-supervision, and few-shot code-switching approaches to improve the state-of-the-art in ASR for Tunisian Arabic, English, and French.</li>
<li>results: The paper produces human evaluations of transcripts to avoid the noise coming from spelling inadequacies in testing references, and the models are able to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English, and French. The data used during training and testing are released for public use and further improvements.<details>
<summary>Abstract</summary>
Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvements.
</details>
<details>
<summary>æ‘˜è¦</summary>
åˆ¶å®šä¸€ä¸ªæœ‰æ•ˆçš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰è§£å†³æ–¹æ¡ˆ Ğ´Ğ»Ñæ–¹è¨€éœ€è¦åˆ›æ–°çš„æ–¹æ³•ï¼Œä¸ä»…è§£å†³æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œè¿˜èƒ½å¤Ÿæ¢ç´¢æ–¹è¨€è¯­è¨€å¤šæ ·æ€§çš„ç»†èŠ‚ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨äº†å‰è¿°çš„ASRæŒ‘æˆ˜ï¼Œå°†ç€çœ¼ç‚¹åœ¨çªå°¼æ–¯æ–¹è¨€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ”¶é›†äº†æ–‡æœ¬å’ŒéŸ³é¢‘æ•°æ®ï¼Œå¹¶åœ¨æŸäº›æƒ…å†µä¸‹è¿›è¡Œäº†æ ‡æ³¨ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ— ç›‘ç£ã€åŠç›‘ç£å’Œå°‘é‡ä»£ç äº¤æ¢çš„æ–¹æ³•ï¼Œä»¥åœ¨ä¸åŒçš„çªå°¼æ–¯æµ‹è¯•é›†ä¸Šæé«˜çŠ¶æ€ã€‚è¿™äº›æµ‹è¯•é›†æ¶µç›–äº†ä¸åŒçš„å¬éŸ³ã€è¯­è¨€å’Œè¯­è°ƒæ¡ä»¶ã€‚æœ€åï¼Œç”±äºæ²¡æœ‰ä¼ ç»Ÿçš„æ‹¼å†™æ³•ï¼Œæˆ‘ä»¬è¿›è¡Œäº†äººå·¥è¯„ä¼°æˆ‘ä»¬çš„è®²æ–‡ï¼Œä»¥é¿å…æµ‹è¯•å‚è€ƒä¸­çš„æ‚éŸ³ã€‚æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å°†çªå°¼æ–¯é˜¿æ‹‰ä¼¯è¯­ã€è‹±è¯­å’Œæ³•è¯­æ··åˆçš„è¯­éŸ³æ ·æœ¬è½¬å½•ä¸ºæ–‡æœ¬ï¼Œå¹¶åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­ä½¿ç”¨çš„æ‰€æœ‰æ•°æ®éƒ½å…¬å¼€å‘å¸ƒï¼Œä»¥ä¾¿è¿›ä¸€æ­¥çš„æ”¹è¿›ã€‚
</details></li>
</ul>
<hr>
<h2 id="DISC-LawLLM-Fine-tuning-Large-Language-Models-for-Intelligent-Legal-Services"><a href="#DISC-LawLLM-Fine-tuning-Large-Language-Models-for-Intelligent-Legal-Services" class="headerlink" title="DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services"></a>DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11325">http://arxiv.org/abs/2309.11325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, Xuanjing Huang, Zhongyu Wei</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æä¾›ä¸€ç§æ™ºèƒ½æ³•å¾‹ç³»ç»Ÿï¼Œä½¿å¾—æ³•å¾‹æœåŠ¡å¯ä»¥æ›´åŠ å…¨é¢å’Œæ™ºèƒ½åŒ–ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨å¤§é‡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¹¶é‡‡ç”¨æ³•å¾‹é€»è¾‘æç¤ºç­–ç•¥æ¥æ„å»ºç›‘ç£ç²¾åº¦è®­ç»ƒé›†ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜å¢å¼ºäº†æ¨¡å‹çš„çŸ¥è¯†è®¿é—®å’Œåˆ©ç”¨èƒ½åŠ›ã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡å¯¹DISC-Law-Evalæµ‹è¯•é›†è¿›è¡Œé‡åŒ–å’Œèµ„æ·±è¯„ä»·ï¼Œ demonstartedäº†å…¶åœ¨ä¸åŒçš„æ³•å¾‹åœºæ™¯ä¸­çš„æ•ˆæœã€‚è¯¦ç»†çš„èµ„æºå¯ä»¥åœ¨<a target="_blank" rel="noopener" href="https://github.com/FudanDISC/DISC-LawLLM%E4%B8%8A%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/FudanDISC/DISC-LawLLMä¸Šæ‰¾åˆ°ã€‚</a><details>
<summary>Abstract</summary>
We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†DISC-LawLLMï¼Œä¸€ç§æ™ºèƒ½æ³•å¾‹ç³»ç»Ÿï¼Œä½¿ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æä¾›å¹¿æ³›çš„æ³•å¾‹æœåŠ¡ã€‚æˆ‘ä»¬é‡‡ç”¨æ³•å¾‹é€»è¾‘æç¤ºç­–ç•¥æ„å»ºç›‘ç£ç²¾åº¦è®­ç»ƒé›†ï¼Œåœ¨ä¸­å›½å¸æ³•é¢†åŸŸè¿›è¡Œè¶…å‚æ•° fine-tuningï¼Œä»¥æé«˜æ¨¡å‹çš„æ³•å¾‹æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬å°†LLMåŠ è½½ä¸€ä¸ªæ£€ç´¢æ¨¡å—ï¼Œä»¥æé«˜æ¨¡å‹å¯¹å¤–éƒ¨æ³•å¾‹çŸ¥è¯†çš„è®¿é—®å’Œåˆ©ç”¨èƒ½åŠ›ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ³•å¾‹è¯„ä»·æŒ‡æ ‡ï¼ŒDISC-Law-Evalï¼Œä»¥è¯„ä¼°æ™ºèƒ½æ³•å¾‹ç³»ç»Ÿçš„æ•ˆæœä»å®¢è§‚å’Œä¸»è§‚ä¸¤ä¸ªè§’åº¦ã€‚æˆ‘ä»¬å¯¹DISC-Law-Evalè¿›è¡Œäº†é‡åŒ–å’Œè´¨é‡çš„æµ‹è¯•ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„ç³»ç»Ÿåœ¨å¤šç§æ³•å¾‹åœºæ™¯ä¸‹å¯ä»¥ä¸ºç”¨æˆ·æä¾›æœ‰æ•ˆçš„æœåŠ¡ã€‚è¯¦ç»†çš„èµ„æºå¯ä»¥åœ¨https://github.com/FudanDISC/DISC-LawLLMä¸Šæ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Wizard-of-Curiosities-Enriching-Dialogues-with-Fun-Facts"><a href="#The-Wizard-of-Curiosities-Enriching-Dialogues-with-Fun-Facts" class="headerlink" title="The Wizard of Curiosities: Enriching Dialogues with Fun Facts"></a>The Wizard of Curiosities: Enriching Dialogues with Fun Facts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11283">http://arxiv.org/abs/2309.11283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frederico Vicente, Rafael Ferreira, David Semedo, JoÃ£o MagalhÃ£es</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¢åŠ å¯¹å¯¹è¯ç³»ç»Ÿçš„ç”¨æˆ·ä½“éªŒï¼Œé€šè¿‡å¼•å…¥å¸å¼•äººçš„å¯ç¤ºã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æ¥è‡ªAmazon Alexa TaskBotæŒ‘æˆ˜çš„çœŸå®äººæœºå¯¹è¯ï¼Œå¹¶å¯¹è¿™äº›å¯¹è¯è¿›è¡Œäº†ç²¾å¿ƒå‡†å¤‡å’Œç¼–è¾‘ï¼Œä»¥åˆ›é€ ä¸€ç»„å…·æœ‰Contextualized curiositiesçš„å¯¹è¯ã€‚</li>
<li>results: æ ¹æ®å¯¹Over 1000å¯¹è¯çš„A&#x2F;Bæµ‹è¯•è¡¨æ˜ï¼Œå¯ç¤ºå¯ä»¥ä¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾å¢åŠ ç”¨æˆ·å‚ä¸åº¦ï¼Œè¿˜æé«˜ç”¨æˆ·çš„å¹³å‡ç›¸å¯¹è¯„ä»·å€¼9.7%ã€‚<details>
<summary>Abstract</summary>
Introducing curiosities in a conversation is a way to teach something new to the person in a pleasant and enjoyable way. Enriching dialogues with contextualized curiosities can improve the users' perception of a dialog system and their overall user experience. In this paper, we introduce a set of curated curiosities, targeting dialogues in the cooking and DIY domains. In particular, we use real human-agent conversations collected in the context of the Amazon Alexa TaskBot challenge, a multimodal and multi-turn conversational setting. According to an A/B test with over 1000 conversations, curiosities not only increase user engagement, but provide an average relative rating improvement of 9.7%.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨å¯¹è¯ä¸­å¼•å…¥curiositiesæ˜¯ä¸€ç§æ•™è‚²ç”¨æˆ·æ–°çŸ¥è¯†çš„æœ‰è¶£å’Œæ„‰æ‚¦çš„æ–¹å¼ã€‚åœ¨å¯¹è¯ä¸­æ·»åŠ ä¸Šä¸‹æ–‡åŒ–curiositieså¯ä»¥æé«˜å¯¹å¯¹è¯ç³»ç»Ÿçš„è¯„ä¼°å’Œç”¨æˆ·æ€»ä½“ä½“éªŒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ª curaeçš„curiositiesé›†åˆï¼Œtargeting cookingå’ŒDIYå¯¹è¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ªAmazon Alexa TaskBotæŒ‘æˆ˜çš„çœŸå®äººæœºå¯¹è¯æ”¶é›†ï¼Œä¸€ç§å¤šåª’ä½“å’Œå¤šturnå¯¹è¯Settingã€‚æ®A/Bæµ‹è¯•ï¼Œcuriositiesä¸ä»…æé«˜äº†ç”¨æˆ·å‚ä¸åº¦ï¼Œè¿˜æä¾›äº†9.7%çš„ç›¸å¯¹è¯„åˆ†æå‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Scenario-Refiner-Grounding-subjects-in-images-at-the-morphological-level"><a href="#The-Scenario-Refiner-Grounding-subjects-in-images-at-the-morphological-level" class="headerlink" title="The Scenario Refiner: Grounding subjects in images at the morphological level"></a>The Scenario Refiner: Grounding subjects in images at the morphological level</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11252">http://arxiv.org/abs/2309.11252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claudia Tagliaferri, Sofia Axioti, Albert Gatt, Denis Paperno</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ç”¨æ¥æ£€éªŒè¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿæ•æ‰è¯­è¨€ä¸­çš„å¾®å¦™ç‰¹å¾çš„ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ–¹æ³•å’Œæ•°æ®é›†æ¥æµ‹è¯•è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿåœ¨ morphological æ°´å¹³ä¸Šæ•æ‰è¯­è¨€ä¸­çš„å·®å¼‚ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè¯­è¨€æ¨¡å‹çš„é¢„æµ‹ä¸äººç±»å‚ä¸è€…çš„åˆ¤æ–­å­˜åœ¨å·®å¼‚ï¼Œå°¤å…¶æ˜¯åœ¨ grammatical æ–¹é¢å­˜åœ¨åå‘ã€‚<details>
<summary>Abstract</summary>
Derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V\&L) models capture such distinctions at the morphological level, using a a new methodology and dataset. We compare the results from V\&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.
</details>
<details>
<summary>æ‘˜è¦</summary>
derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V&L) models capture such distinctions at the morphological level, using a new methodology and dataset. We compare the results from V&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.Here's the translation in Traditional Chinese as well: derivationally related words, such as "runner" and "running", exhibit semantic differences which also elicit different visual scenarios. In this paper, we ask whether Vision and Language (V&L) models capture such distinctions at the morphological level, using a new methodology and dataset. We compare the results from V&L models to human judgements and find that models' predictions differ from those of human participants, in particular displaying a grammatical bias. We further investigate whether the human-model misalignment is related to model architecture. Our methodology, developed on one specific morphological contrast, can be further extended for testing models on capturing other nuanced language features.
</details></li>
</ul>
<hr>
<h2 id="OpenChat-Advancing-Open-source-Language-Models-with-Mixed-Quality-Data"><a href="#OpenChat-Advancing-Open-source-Language-Models-with-Mixed-Quality-Data" class="headerlink" title="OpenChat: Advancing Open-source Language Models with Mixed-Quality Data"></a>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11235">http://arxiv.org/abs/2309.11235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imoneoi/openchat">https://github.com/imoneoi/openchat</a></li>
<li>paper_authors: Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æé«˜å¼€æºè¯­è¨€æ¨¡å‹çš„æ€§èƒ½è€Œå†™çš„ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨æ··åˆè´¨é‡æ•°æ®è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º OpenChat çš„æ¡†æ¶ï¼Œç”¨äºä½¿ç”¨æ··åˆè´¨é‡æ•°æ®è¿›è¡Œè¶…å‚æ•°è°ƒæ•´ï¼Œå¹¶ä½¿ç”¨ Conditioned-Reinforcement Learning Fine-Tuning (C-RLFT) æ–¹æ³•ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ OpenChat æ¡†æ¶å’Œ C-RLFT æ–¹æ³•å¯ä»¥æé«˜å¼€æºè¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ä¸‰ä¸ªæ ‡å‡†çš„ bencmark ä¸Š achieved the highest average performance ä¸­ã€‚<details>
<summary>Abstract</summary>
Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœ¨ï¼Œå¼€æºå¤§è¯­è¨€æ¨¡å‹å¦‚LLaMAå·²ç»å‡ºç°ã€‚æœ€è¿‘çš„å‘å±•åŒ…æ‹¬ç›‘ç£ç²¾ç»†è°ƒæ•™ï¼ˆSFTï¼‰å’Œå¥–åŠ±å­¦ä¹ è°ƒæ•™ï¼ˆRLFTï¼‰ï¼Œä»¥ä½¿æ¨¡å‹ä¸äººç±»ç›®æ ‡ better alignmentã€‚ç„¶è€Œï¼ŒSFTæ–¹æ³•å°†æ‰€æœ‰è®­ç»ƒæ•°æ®è§†ä¸ºä¸€æ ·çš„è´¨é‡ï¼Œè€ŒRLFTæ–¹æ³•éœ€è¦é«˜è´¨é‡çš„å¯¹æ•°æ®è¿›è¡Œå¯¹æ¯”æˆ–æ’åã€‚åœ¨è¿™ç§ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œåä¸ºOpenChatï¼Œä»¥æé«˜å¼€æºè¯­è¨€æ¨¡å‹çš„è´¨é‡ã€‚ Specificallyï¼Œæˆ‘ä»¬è€ƒè™‘äº†é€šç”¨çš„SFTè®­ç»ƒæ•°æ®ï¼ŒåŒ…æ‹¬ä¸€å°é‡çš„ä¸“å®¶æ•°æ®å’Œå¤§é‡çš„ä¸ä¼˜åŒ–æ•°æ®ï¼Œæ— éœ€ä»»ä½•åå¥½æ ‡ç­¾ã€‚æˆ‘ä»¬æè®®äº†Cï¼ˆæ¡ä»¶ï¼‰-RLFTï¼Œå®ƒå°†ä¸åŒçš„æ•°æ®æ¥æºè§†ä¸ºç²—ç²’åŒ–å¥–åŠ±æ ‡ç­¾ï¼Œå¹¶å­¦ä¹ ä¸€ä¸ªç±»åˆ« Conditioned ç­–ç•¥ï¼Œä»¥åˆ©ç”¨ä¸åŒæ•°æ®è´¨é‡ä¿¡æ¯ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒC-RLFT çš„ä¼˜åŒ–ç­–ç•¥å¯ä»¥é€šè¿‡å•é˜¶æ®µã€RL-free ç›‘ç£å­¦ä¹ ï¼Œä»¥è½»é‡çº§å’Œé¿å…é«˜æ˜‚çš„äººç±»åå¥½æ ‡ç­¾ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬çš„ openchat-13b é€šè¿‡ C-RLFT è¿›è¡Œå¾®è°ƒï¼Œåœ¨ä¸‰ä¸ªæ ‡å‡† bench mark ä¸Š achieve æ‰€æœ‰ 13b å¼€æºè¯­è¨€æ¨¡å‹çš„æœ€é«˜å¹³å‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ AGIEval éªŒè¯æ¨¡å‹çš„é€šç”¨æ€§èƒ½ï¼Œåªæœ‰ openchat-13b åœ¨åŸºç¡€æ¨¡å‹ä¹‹ä¸Šè¶…è¶Šã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—çš„åˆ†æï¼Œä»¥è¯æ˜ OpenChat çš„æ•ˆæœå’Œå¯é æ€§ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®å’Œæ¨¡å‹éƒ½å¯ä»¥åœ¨ https://github.com/imoneoi/openchat ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Speak-While-You-Think-Streaming-Speech-Synthesis-During-Text-Generation"><a href="#Speak-While-You-Think-Streaming-Speech-Synthesis-During-Text-Generation" class="headerlink" title="Speak While You Think: Streaming Speech Synthesis During Text Generation"></a>Speak While You Think: Streaming Speech Synthesis During Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11210">http://arxiv.org/abs/2309.11210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Avihu Dekel, Slava Shechtman, Raul Fernandez, David Haws, Zvi Kons, Ron Hoory</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†è§£å†³å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„äº¤äº’é—®é¢˜ï¼Œä½¿å¾—LLMå¯ä»¥æ›´åŠ æµç•…åœ°è¿›è¡Œè¯­éŸ³äº¤äº’ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºLM2Speechçš„æ¶æ„ï¼Œå®ƒå¯ä»¥åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­åŒæ—¶ç”Ÿæˆè¯­éŸ³ï¼Œä»è€Œå‡å°‘äº†å¯¹è¯å»¶è¿Ÿã€‚LM2Speechæ¨¡ä»¿äº†ä¸€ä¸ªéæµåŠ¨æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹ï¼ŒåŒæ—¶é™åˆ¶äº†æœªæ¥ä¸Šä¸‹æ–‡çš„æš´éœ²ï¼Œä»¥ä¾¿å®ç°æµåŠ¨ã€‚å®ƒåˆ©ç”¨äº†LLMçš„éšè—åµŒå…¥ï¼Œè¿™æ˜¯æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ä¸€ä¸ªä¾§äº§å“ï¼Œå®ƒå«æœ‰æœ‰ç”¨çš„è¯­ä¹‰ä¸Šä¸‹æ–‡ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒLM2Speechå¯ä»¥ä¿æŒæ•™å¸ˆæ¨¡å‹çš„è´¨é‡ï¼ŒåŒæ—¶å‡å°‘å¯¹è¯å»¶è¿Ÿï¼Œä»¥ä¾¿å®ç°è‡ªç„¶çš„è¯­éŸ³äº¤äº’ã€‚<details>
<summary>Abstract</summary>
Large Language Models (LLMs) demonstrate impressive capabilities, yet interaction with these models is mostly facilitated through text. Using Text-To-Speech to synthesize LLM outputs typically results in notable latency, which is impractical for fluent voice conversations. We propose LLM2Speech, an architecture to synthesize speech while text is being generated by an LLM which yields significant latency reduction. LLM2Speech mimics the predictions of a non-streaming teacher model while limiting the exposure to future context in order to enable streaming. It exploits the hidden embeddings of the LLM, a by-product of the text generation that contains informative semantic context. Experimental results show that LLM2Speech maintains the teacher's quality while reducing the latency to enable natural conversations.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¾ç¤ºå‡ºå¾ˆå¼ºçš„èƒ½åŠ›ï¼Œç„¶è€Œä¸è¿™äº›æ¨¡å‹äº¤äº’é€šå¸¸æ˜¯é€šè¿‡æ–‡æœ¬è¿›è¡Œçš„ã€‚ä½¿ç”¨æ–‡æœ¬åˆ°è¯­éŸ³synthesize LLMè¾“å‡ºé€šå¸¸ä¼šå¯¼è‡´å¾ˆé•¿çš„å»¶è¿Ÿï¼Œè¿™å¯¹äºæµç•…çš„è¯­éŸ³å¯¹è¯ä¸å®ç”¨ã€‚æˆ‘ä»¬æè®®LLM2Speechï¼Œä¸€ç§æ¶æ„å¯ä»¥åœ¨æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­åŒæ—¶synthesizeè¯­éŸ³ï¼Œä»è€Œå‡å°‘å»¶è¿Ÿã€‚LLM2Speechæ¨¡ä»¿æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹ï¼Œé™åˆ¶æœªæ¥ä¸Šä¸‹æ–‡çš„æš´éœ²ï¼Œä»¥ä¾¿å®ç°æµåŠ¨ã€‚å®ƒåˆ©ç”¨LLMçš„éšè—åµŒå…¥ï¼Œè¿™æ˜¯æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹çš„äº§ç‰©ï¼Œå«æœ‰æœ‰ç”¨çš„semanticContextã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒLLM2Speechå¯ä»¥ä¿æŒæ•™å¸ˆçš„è´¨é‡ï¼ŒåŒæ—¶å‡å°‘å»¶è¿Ÿï¼Œä»¥ä¾¿å®ç°è‡ªç„¶çš„å¯¹è¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Languini-Kitchen-Enabling-Language-Modelling-Research-at-Different-Scales-of-Compute"><a href="#The-Languini-Kitchen-Enabling-Language-Modelling-Research-at-Different-Scales-of-Compute" class="headerlink" title="The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute"></a>The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11197">http://arxiv.org/abs/2309.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aleksandar StaniÄ‡, Dylan Ashley, Oleg Serikov, Louis Kirsch, Francesco Faccio, JÃ¼rgen Schmidhuber, Thomas Hofmann, Imanol Schlag</li>
<li>for:  This paper aims to provide a fair comparison of language modeling methods based on their empirical scaling trends, and to serve as a foundation for meaningful and reproducible research in the field.</li>
<li>methods: The paper introduces an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours, and uses a pre-processed dataset of books to evaluate the methods.</li>
<li>results: The paper shows that the LSTM baseline exhibits a predictable and more favourable scaling law than the GPT baseline, and that the two models intersect at roughly 50,000 accelerator hours.Here is the text in Simplified Chinese:</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯ä¸ºè¯­è¨€æ¨¡å‹æ¯”è¾ƒæä¾›å…¬å¹³çš„æ¯”è¾ƒåŸºç¡€ï¼Œå¹¶ä¸ºè¯­è¨€æ¨¡å‹ç ”ç©¶æä¾›å¯é‡å¤çš„åŸºç¡€ã€‚</li>
<li>methods: è®ºæ–‡æå‡ºäº†ä¸€ç§å®éªŒåè®®ï¼Œä½¿å¾—æ¨¡å‹æ¯”è¾ƒåŸºäºç­‰æ•ˆè®¡ç®—æ—¶é—´ï¼ˆ measured in accelerator hoursï¼‰è¿›è¡Œã€‚ä¸ºäº†è¯„ä»·æ–¹æ³•ï¼Œæ–‡ç« ä½¿ç”¨äº†ä¸€ä¸ªå·²ç»å¤„ç†è¿‡çš„å¤§å‹ã€å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„ä¹¦ç±æ•°æ®é›†ã€‚</li>
<li>results: è®ºæ–‡æ˜¾ç¤ºï¼ŒLSTMåŸºelineåœ¨è®¡ç®—æ—¶é—´ä¸Šé‡‡å–äº†ä¸€ç§å¯é¢„æµ‹çš„å’Œæ›´æœ‰åˆ©çš„æ•´ä½“å¢é•¿è§„å¾‹ï¼Œè€ŒGPTåŸºelineåœ¨æ‰€æœ‰ç­‰æ•ˆè®¡ç®—æ—¶é—´æ°´å¹³ä¸Šéƒ½ä¿æŒäº†æ›´å¥½çš„æŠ˜è¡£ç‡ã€‚ä¸¤ä¸ªåŸºelineåœ¨çº¦50,000ä¸ªåŠ é€Ÿå™¨å°æ—¶ä¸Šäº¤å‰ã€‚<details>
<summary>Abstract</summary>
The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modelling research.
</details>
<details>
<summary>æ‘˜è¦</summary>
è“å¤·é¢å¨æˆ¿ serves as both a research collective and codebase, designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modeling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and a recurrent model in the form of a novel LSTM with ten-fold throughput. While the GPT baseline achieves better perplexity throughout all our levels of compute, our LSTM baseline exhibits a predictable and more favourable scaling law. This is due to the improved throughput and the need for fewer training tokens to achieve the same decrease in test perplexity. Extrapolating the scaling laws leads of both models results in an intersection at roughly 50,000 accelerator hours. We hope this work can serve as the foundation for meaningful and reproducible language modeling research.
</details></li>
</ul>
<hr>
<h2 id="Assessment-of-Pre-Trained-Models-Across-Languages-and-Grammars"><a href="#Assessment-of-Pre-Trained-Models-Across-Languages-and-Grammars" class="headerlink" title="Assessment of Pre-Trained Models Across Languages and Grammars"></a>Assessment of Pre-Trained Models Across Languages and Grammars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11165">http://arxiv.org/abs/2309.11165</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amunozo/multilingual-assessment">https://github.com/amunozo/multilingual-assessment</a></li>
<li>paper_authors: Alberto MuÃ±oz-Ortiz, David Vilares, Carlos GÃ³mez-RodrÃ­guez</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†è¯„ä¼°å¤šè¯­è¨€å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†å™¨ï¼ˆLLMsï¼‰å¦‚ä½•å­¦ä¹ è¯­æ³•ç»“æ„ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†æŠ½è±¡åˆ°å¤šå½¢å¼è¯­æ³•ç»“æ„çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬å°†è§£æè§†ä¸ºåºåˆ—æ ‡ç­¾ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼šï¼ˆä¸€ï¼‰æ¡†æ¶åœ¨ä¸åŒç¼–ç ä¸‹å…·æœ‰ä¸€è‡´æ€§ï¼Œï¼ˆäºŒï¼‰é¢„è®­ç»ƒè¯è¯ vectors ä¸ä¼šåå¥½è¯­æ³•æ ‘è¡¨ç¤ºäºdependencyè¡¨ç¤ºï¼Œï¼ˆä¸‰ï¼‰ä½¿ç”¨å­—ç¬¦ä¸²åˆ†è¯æ˜¯éœ€è¦è¡¨ç¤ºè¯­æ³•ç»“æ„çš„ï¼Œä¸å­—ç¬¦ä¸²æ¨¡å‹ä¸åŒï¼Œï¼ˆå››ï¼‰è¯­è¨€å‡ºç°åœ¨é¢„è®­ç»ƒæ•°æ®ä¸­çš„é¢‘ç‡æ¯”ä»»åŠ¡æ•°æ®æ›´é‡è¦äºä»è¯è¯ vectors ä¸­æ¢å¤è¯­æ³•ç»“æ„ã€‚<details>
<summary>Abstract</summary>
We present an approach for assessing how multilingual large language models (LLMs) learn syntax in terms of multi-formalism syntactic structures. We aim to recover constituent and dependency structures by casting parsing as sequence labeling. To do so, we select a few LLMs and study them on 13 diverse UD treebanks for dependency parsing and 10 treebanks for constituent parsing. Our results show that: (i) the framework is consistent across encodings, (ii) pre-trained word vectors do not favor constituency representations of syntax over dependencies, (iii) sub-word tokenization is needed to represent syntax, in contrast to character-based models, and (iv) occurrence of a language in the pretraining data is more important than the amount of task data when recovering syntax from the word vectors.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œç”¨äºè¯„ä¼°å¤šè¯­è¨€å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†å™¨ï¼ˆLLMï¼‰åœ¨å¤šå½¢å¼è¯­æ³•ç»“æ„ä¸­å­¦ä¹ è¯­æ³•çš„æ–¹å¼ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å°†åˆ†æè½¬æ¢ä¸ºåºåˆ—æ ‡ç­¾æ¥æ¢å¤å¥å­å’Œä¾èµ–ç»“æ„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€äº›LLMå¹¶å¯¹13ç§UD treebanksè¿›è¡Œäº†ä¾èµ–åˆ†æå’Œ10ç§treebanksè¿›è¡Œäº†å¥å­åˆ†æã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼šï¼ˆiï¼‰æ¡†æ¶åœ¨ä¸åŒç¼–ç ä¸­å…·æœ‰ä¸€è‡´æ€§ï¼Œï¼ˆiiï¼‰é¢„è®­ç»ƒè¯è¯ vec ä¸å€¾å‘äº syntax ä¸­çš„å¥å­è¡¨ç¤ºï¼Œï¼ˆiiiï¼‰å­—ç¬¦ä¸²åˆ†è¯æ˜¯å¿…è¦çš„ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²æ¨¡å‹ï¼Œä»¥è¡¨ç¤ºè¯­æ³•ï¼Œï¼ˆivï¼‰é¢„trainingæ•°æ®ä¸­è¯­è¨€çš„å‡ºç°æ¬¡æ•°é«˜äºä»»åŠ¡æ•°æ®æ—¶ï¼Œå¯ä»¥æ›´å¥½åœ°ä»è¯ vectors ä¸­æ¢å¤è¯­æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Prototype-of-a-robotic-system-to-assist-the-learning-process-of-English-language-with-text-generation-through-DNN"><a href="#Prototype-of-a-robotic-system-to-assist-the-learning-process-of-English-language-with-text-generation-through-DNN" class="headerlink" title="Prototype of a robotic system to assist the learning process of English language with text-generation through DNN"></a>Prototype of a robotic system to assist the learning process of English language with text-generation through DNN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11142">http://arxiv.org/abs/2309.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Morales-Torres, Mario Campos-Soberanis, Diego Campos-Sobrino</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†å¸®åŠ©è‹±è¯­è‡ªå­¦è€…æé«˜è‹±è¯­æ°´å¹³çš„ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†Long Short Term Memoryï¼ˆLSTMï¼‰ç¥ç»ç½‘ç»œæ¥ç”Ÿæˆæ–‡æœ¬ï¼Œlearnersé€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ä¸ç³»ç»Ÿäº’åŠ¨ï¼Œç³»ç»Ÿæ ¹æ®å­¦ç”Ÿçš„è‹±è¯­æ°´å¹³ç”Ÿæˆæ–‡æœ¬ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œlearnersä¸ç³»ç»Ÿäº’åŠ¨åï¼Œä»–ä»¬çš„ grammatical Range æœ‰æ‰€æé«˜ã€‚<details>
<summary>Abstract</summary>
In the last ongoing years, there has been a significant ascending on the field of Natural Language Processing (NLP) for performing multiple tasks including English Language Teaching (ELT). An effective strategy to favor the learning process uses interactive devices to engage learners in their self-learning process. In this work, we present a working prototype of a humanoid robotic system to assist English language self-learners through text generation using Long Short Term Memory (LSTM) Neural Networks. The learners interact with the system using a Graphic User Interface that generates text according to the English level of the user. The experimentation was conducted using English learners and the results were measured accordingly to International English Language Testing System (IELTS) rubric. Preliminary results show an increment in the Grammatical Range of learners who interacted with the system.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘å‡ å¹´æ¥ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå†…ï¼Œæœ‰è®¸å¤šè¿›å±•ï¼Œä»¥å¸®åŠ©æ‰§è¡Œå¤šç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‹±è¯­æ•™å­¦ï¼ˆELTï¼‰ã€‚ä¸€ç§æœ‰æ•ˆçš„ç­–ç•¥æ˜¯ä½¿ç”¨äº’åŠ¨è®¾å¤‡ï¼Œä»¥å¸å¼•å­¦ç”Ÿå‚ä¸è‡ªå­¦ä¹ è¿‡ç¨‹ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªäººå·¥æ™ºèƒ½æœºå™¨äººç³»ç»Ÿï¼Œç”¨äºå¸®åŠ©è‹±è¯­è‡ªå­¦è€…é€šè¿‡æ–‡æœ¬ç”Ÿæˆæ¥æé«˜è‹±è¯­æ°´å¹³ã€‚å­¦ç”Ÿé€šè¿‡å›¾å½¢ç”¨æˆ·ç•Œé¢ä¸ç³»ç»Ÿè¿›è¡Œäº¤äº’ï¼Œç³»ç»Ÿæ ¹æ®ç”¨æˆ·çš„è‹±è¯­æ°´å¹³ç”Ÿæˆæ–‡æœ¬ã€‚å®éªŒä¸­ä½¿ç”¨äº†è‹±è¯­å­¦ä¹ è€…ï¼Œå¹¶æ ¹æ®å›½é™…è‹±è¯­è¯­è¨€è€ƒè¯•ç³»ç»Ÿï¼ˆIELTSï¼‰æ ‡å‡†è¿›è¡Œè¯„ä¼°ç»“æœã€‚åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä¸ç³»ç»Ÿäº¤äº’çš„å­¦ç”Ÿçš„ grammatical range æœ‰æ‰€å¢åŠ ã€‚
</details></li>
</ul>
<hr>
<h2 id="K-pop-Lyric-Translation-Dataset-Analysis-and-Neural-Modelling"><a href="#K-pop-Lyric-Translation-Dataset-Analysis-and-Neural-Modelling" class="headerlink" title="K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling"></a>K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11093">http://arxiv.org/abs/2309.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haven Kim, Jongmin Jung, Dasaem Jeong, Juhan Nam</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯ä¸ºäº†æ¨å¹¿æ­Œæ›²ç¿»è¯‘ç ”ç©¶çš„èŒƒå›´å’Œè¯­è¨€ï¼Œå¹¶å¯¹K-popæ­Œæ›²ç¿»è¯‘è¿›è¡Œç³»ç»Ÿæ€§çš„ç ”ç©¶ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„æ­Œè¯ç¿»è¯‘ datasetï¼Œè¯¥ datasetåŒ…å«äº†å¤§çº¦89%çš„K-popæ­Œæ›²æ­Œè¯ï¼Œå¹¶å°†éŸ©è¯­å’Œè‹±è¯­æ­Œè¯è¿›è¡Œäº†è¡Œå†…å’Œæ®µå†…çš„å¯¹åº”ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°äº†K-popæ­Œæ›²ç¿»è¯‘çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œä¸å…¶ä»–å·²ç»å¹¿æ³›ç ”ç©¶çš„ç±»å‹ä¸åŒï¼ŒåŒæ—¶è¿˜æ„å»ºäº†ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„æ­Œè¯ç¿»è¯‘æ¨¡å‹ï¼Œä»è€Œè¯æ˜äº†ä¸“é—¨ä¸ºæ­Œæ›²ç¿»è¯‘è€Œè®¾è®¡çš„ dataset çš„é‡è¦æ€§ã€‚<details>
<summary>Abstract</summary>
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for singable lyric translations.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> transtable "Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for singable lyric translations."ä¸­æ–‡ç¿»è¯‘ï¼šå­¦æœ¯ç•Œå¯¹æ­Œè¯ç¿»è¯‘ä¸€ç™¾å¹´æ¥è¿›è¡Œç ”ç©¶ï¼Œç°åœ¨å¸å¼•äº†è®¡ç®—è¯­è¨€å­¦ç ”ç©¶è€…ã€‚æˆ‘ä»¬è®¤ä¸ºå‰ä¸€ä»£ç ”ç©¶å­˜åœ¨ä¸¤ä¸ªé™åˆ¶ï¼šé¦–å…ˆï¼Œæ­Œè¯ç¿»è¯‘ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨è¥¿æ–¹ç±»å‹å’Œè¯­è¨€ä¸Šï¼Œå°šæœªå¯¹K-popè¿›è¡Œè¿‡ç ”ç©¶ï¼Œå°½ç®¡å…¶å—æ¬¢è¿ç¨‹åº¦æé«˜ã€‚å…¶æ¬¡ï¼Œæ­Œè¯ç¿»è¯‘é¢†åŸŸç¼ºä¹å…¬å…±å¯ç”¨æ•°æ®é›†ï¼Œåˆ°æˆ‘ä»¬æ‰€çŸ¥ï¼Œæ²¡æœ‰è¿™æ ·çš„æ•°æ®é›†å­˜åœ¨ã€‚ä¸ºäº†æ‰©å¤§æ­Œè¯ç¿»è¯‘ç ”ç©¶çš„ç±»å‹å’Œè¯­è¨€èŒƒå›´ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªæ–°çš„å¯å”±æ­Œè¯ç¿»è¯‘æ•°æ®é›†ï¼Œå…¶ä¸­å¤§çº¦89%æ˜¯K-popæ­Œæ›² lyricsã€‚è¿™ä¸ªæ•°æ®é›†å°†éŸ©è¯­å’Œè‹±è¯­æ­Œè¯ä¸€è¡Œä¸€è¡Œã€æ®µæ®µå¯¹é½ã€‚æˆ‘ä»¬åˆ©ç”¨äº†è¿™ä¸ªæ•°æ®é›†ï¼Œæ­ç¤ºäº†K-popæ­Œè¯ç¿»è¯‘çš„ç‹¬ç‰¹ç‰¹å¾ï¼Œä¸å…¶ä»–å¹¿æ³›ç ”ç©¶çš„ç±»å‹åŒºåˆ†å¼€æ¥ï¼Œå¹¶æ„å»ºäº†ç¥ç»ç½‘ç»œæ­Œè¯ç¿»è¯‘æ¨¡å‹ï¼Œä»è€Œå¼ºè°ƒäº†ä¸“é—¨ä¸ºå¯å”±æ­Œè¯ç¿»è¯‘è€Œè®¾ç½®çš„æ•°æ®é›†çš„é‡è¦æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Dual-Modal-Attention-Enhanced-Text-Video-Retrieval-with-Triplet-Partial-Margin-Contrastive-Learning"><a href="#Dual-Modal-Attention-Enhanced-Text-Video-Retrieval-with-Triplet-Partial-Margin-Contrastive-Learning" class="headerlink" title="Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning"></a>Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11082">http://arxiv.org/abs/2309.11082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Jiang, Hong Liu, Xuzheng Yu, Qing Wang, Yuan Cheng, Jia Xu, Zhongyi Liu, Qingpei Guo, Wei Chu, Ming Yang, Yuan Qi</li>
<li>for: This paper focuses on improving text-video retrieval, which is essential for video filtering, recommendation, and search, due to the increasing amount of web videos.</li>
<li>methods: The paper proposes two novel techniques to improve contrastive learning for text-video retrieval: 1) Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs, and 2) Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples.</li>
<li>results: The proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo, and ActivityNet.Hereâ€™s the simplified Chinese text in the format you requested:</li>
<li>for: è¿™ç¯‡è®ºæ–‡å…³æ³¨æé«˜æ–‡æœ¬è§†é¢‘ç›¸ä¼¼æ€§ retrievalï¼Œç”¨äºè§†é¢‘è¿‡æ»¤ã€æ¨èå’Œæœç´¢ï¼Œç”±äºç½‘ç»œè§†é¢‘çš„å¿«é€Ÿå¢é•¿ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸¤ç§æ–°çš„æŠ€æœ¯æ¥æé«˜å¯¹æ–‡æœ¬è§†é¢‘ç›¸ä¼¼æ€§çš„æ¢ç´¢ï¼š1ï¼‰åŒModal Attention-Enhanced Module (DMAE) æ¥æŒ–æ˜å›°éš¾çš„è´Ÿä¾‹ï¼Œ2ï¼‰Triplet Partial Margin Contrastive Learning (TPM-CL) æ¨¡å—æ¥æ„å»ºpartial order triplet samplesã€‚</li>
<li>results: è¯¥æå‡ºçš„æ–¹æ³•åœ¨å››ä¸ªå¸¸ç”¨çš„æ–‡æœ¬è§†é¢‘ç›¸ä¼¼æ€§æ•°æ®é›†ä¸Šï¼ˆMSR-VTTã€MSVDã€DiDeMoã€ActivityNetï¼‰å¾—åˆ°äº†è¾ƒé«˜çš„æ€§èƒ½ï¼Œæ¯”å¦‚ existed æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
In recent years, the explosion of web videos makes text-video retrieval increasingly essential and popular for video filtering, recommendation, and search. Text-video retrieval aims to rank relevant text/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity between texts and videos. Recently, contrastive learning methods have shown promising results for text-video retrieval, most of which focus on the construction of positive and negative pairs to learn text and video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) to mine hard negative pairs from textual and visual clues. By further introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss. Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. We thereby present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction to model subtle semantic differences. Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-used text-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼ŒWebè§†é¢‘çš„çˆ†ç‚¸å¼å¢é•¿ä½¿å¾—æ–‡æœ¬è§†é¢‘æ£€ç´¢å˜å¾—è¶Šæ¥è¶Šé‡è¦å’Œå—æ¬¢è¿ï¼Œç”¨äºè§†é¢‘ç­›é€‰ã€æ¨èå’Œæœç´¢ã€‚æ–‡æœ¬è§†é¢‘æ£€ç´¢çš„ç›®æ ‡æ˜¯å°†ç›¸å…³çš„æ–‡æœ¬å’Œè§†é¢‘æ’ååœ¨ä¸ç›¸å…³çš„æ–‡æœ¬å’Œè§†é¢‘ä¹‹å‰ã€‚æ ¸å¿ƒä»»åŠ¡æ˜¯å‡†ç¡®åº¦é‡æ–‡æœ¬å’Œè§†é¢‘ä¹‹é—´çš„è·¨Modalç›¸ä¼¼æ€§ã€‚åœ¨è¿™ä¸ªä»»åŠ¡ä¸­ï¼Œå¯¹ç…§å­¦ä¹ æ–¹æ³•å·²ç»å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå¤§å¤šæ•°æ–¹æ³•éƒ½æ˜¯é€šè¿‡å»ºç«‹æ­£ä¾‹å’Œåä¾‹æ¥å­¦ä¹ æ–‡æœ¬å’Œè§†é¢‘è¡¨ç¤ºã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å¾€å¾€å¿½ç•¥ç¡¬ä¾‹å’Œä¸åŒæ°´å¹³çš„ semantic similarityã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°çš„æŠ€æœ¯ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŒModalæ³¨æ„åŠ›å¢å¼ºæ¨¡å—ï¼ˆDMAEï¼‰ï¼Œä»¥æŒ–æ˜æ–‡æœ¬å’Œè§†é¢‘ä¸­çš„ç¡¬ä¾‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§Negative-aware InfoNCEï¼ˆNegNCEï¼‰æŸå¤±å‡½æ•°ï¼Œä»¥é€‚åº”æ€§åœ°æ ‡è¯†å’Œç‰¹åˆ«å¼ºè°ƒç¡¬ä¾‹çš„å½±å“ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ argue that triplet sampleså¯ä»¥æ›´å¥½åœ°æ¨¡å‹ç»†è‡´çš„ semantic similarityï¼Œè€Œä¸æ˜¯pairwise samplesã€‚æˆ‘ä»¬å› æ­¤æå‡ºäº†ä¸€ç§æ–°çš„Triplet Partial Margin Contrastive Learningï¼ˆTPM-CLï¼‰æ¨¡å—ï¼Œé€šè¿‡è‡ªåŠ¨ç”ŸæˆåŒ¹é…çš„æ–‡æœ¬è§†é¢‘å¯¹çš„ç¡¬ä¾‹æ¥å»ºç«‹ partial order triplet samplesã€‚TPM-CLæ¨¡å—è¿˜è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”çš„tokenæ©ç ç­–ç•¥ï¼Œä»¥æ¨¡å‹æ–‡æœ¬å’Œè§†é¢‘ä¹‹é—´çš„è·¨Modalå·®å¼‚ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°ï¼Œæå‡ºçš„æ–¹æ³•åœ¨å››ä¸ªå¸¸ç”¨çš„æ–‡æœ¬è§†é¢‘æ£€ç´¢æ•°æ®é›†ä¸Šéƒ½èƒ½å¤Ÿè¾¾åˆ°æ›´é«˜çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="UniPCM-Universal-Pre-trained-Conversation-Model-with-Task-aware-Automatic-Prompt"><a href="#UniPCM-Universal-Pre-trained-Conversation-Model-with-Task-aware-Automatic-Prompt" class="headerlink" title="UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt"></a>UniPCM: Universal Pre-trained Conversation Model with Task-aware Automatic Prompt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11065">http://arxiv.org/abs/2309.11065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cycrab/unipcm">https://github.com/cycrab/unipcm</a></li>
<li>paper_authors: Yucheng Cai, Wentao Ma, Yuchuan Wu, Shuzheng Si, Yuan Shao, Zhijian Ou, Yongbin Li</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å¯¹è¯ç³»ç»Ÿæ¨¡å‹çš„Robustnesså’Œä¼ è¾“èƒ½åŠ›ï¼Œé€šè¿‡å¤šä»»åŠ¡é¢„è®­ç»ƒã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨Task-based Automatic Prompt generationï¼ˆTAPï¼‰è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„æç¤ºã€‚</li>
<li>results: é€šè¿‡ä½¿ç”¨é«˜è´¨é‡çš„æç¤ºï¼Œæˆ‘ä»¬æ‰©å±•äº†å¯¹è¯ç³»ç»Ÿæ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†è‡³122ä¸ªä»»åŠ¡ï¼Œå¹¶å®ç°äº†å¯¹å¤šç§å¯¹è¯ä»»åŠ¡å’Œä¸åŒçš„å¯¹è¯ç³»ç»Ÿçš„ä¼˜ç§€è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
Recent research has shown that multi-task pre-training greatly improves the model's robustness and transfer ability, which is crucial for building a high-quality dialog system. However, most previous works on multi-task pre-training rely heavily on human-defined input format or prompt, which is not optimal in quality and quantity. In this work, we propose to use Task-based Automatic Prompt generation (TAP) to automatically generate high-quality prompts. Using the high-quality prompts generated, we scale the corpus of the pre-trained conversation model to 122 datasets from 15 dialog-related tasks, resulting in Universal Pre-trained Conversation Model (UniPCM), a powerful foundation model for various conversational tasks and different dialog systems. Extensive experiments have shown that UniPCM is robust to input prompts and capable of various dialog-related tasks. Moreover, UniPCM has strong transfer ability and excels at low resource scenarios, achieving SOTA results on 9 different datasets ranging from task-oriented dialog to open-domain conversation. Furthermore, we are amazed to find that TAP can generate prompts on par with those collected with crowdsourcing. The code is released with the paper.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå¤šä»»åŠ¡é¢„è®­ç»ƒå¯ä»¥å¤§å¹…æé«˜æ¨¡å‹çš„Robustnesså’Œä¼ é€’èƒ½åŠ›ï¼Œè¿™æ˜¯å»ºç«‹é«˜è´¨é‡å¯¹è¯ç³»ç»Ÿçš„å…³é”®ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å‰ä¸€äº›å·¥ä½œä¸­çš„å¤šä»»åŠ¡é¢„è®­ç»ƒéƒ½ä¾èµ–äºäººç±»å®šä¹‰çš„è¾“å…¥æ ¼å¼æˆ–æç¤ºï¼Œè¿™å¹¶ä¸æ˜¯æœ€ä½³çš„è´¨é‡å’Œé‡ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨ä»»åŠ¡åŸºæœ¬Promptç”Ÿæˆï¼ˆTAPï¼‰è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æç¤ºã€‚ä½¿ç”¨ç”Ÿæˆçš„é«˜è´¨é‡æç¤ºï¼Œæˆ‘ä»¬æ‰©å±•äº†é¢„è®­ç»ƒå¯¹è¯æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†ï¼Œè¾¾åˆ°äº†122ä¸ªå¯¹è¯ç›¸å…³ä»»åŠ¡çš„è§„æ¨¡ï¼Œå¹¶å‘½åä¸ºUniversal Pre-trained Conversation Modelï¼ˆUniPCMï¼‰ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒUniPCMå…·æœ‰è¾“å…¥æç¤ºçš„Robustnesså’Œå¤šç§å¯¹è¯ä»»åŠ¡çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼ŒUniPCMåœ¨èµ„æºä¸è¶³çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ï¼Œåœ¨9ä¸ªä¸åŒä»»åŠ¡ä¸Šè¾¾åˆ°äº†SOTAçš„ç»“æœï¼Œä»ä»»åŠ¡å‹å¯¹è¯åˆ°å¼€æ”¾é¢†åŸŸå¯¹è¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°TAPå¯ä»¥ç”Ÿæˆä¸äººç±»æ”¶é›†çš„æç¤ºç›¸å½“çš„æç¤ºã€‚ä»£ç éšç€è®ºæ–‡ä¸€èµ·å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="XATU-A-Fine-grained-Instruction-based-Benchmark-for-Explainable-Text-Updates"><a href="#XATU-A-Fine-grained-Instruction-based-Benchmark-for-Explainable-Text-Updates" class="headerlink" title="XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates"></a>XATU: A Fine-grained Instruction-based Benchmark for Explainable Text Updates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11063">http://arxiv.org/abs/2309.11063</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/megagonlabs/xatu">https://github.com/megagonlabs/xatu</a></li>
<li>paper_authors: Haopeng Zhang, Hayate Iso, Sairam Gurajada, Nikita Bhutani</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æ£€éªŒå¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç¼–è¾‘èƒ½åŠ›ï¼Œå¹¶æä¾›ä¸€ä¸ªæ–°çš„ã€ç»†åŒ–çš„æŒ‡ä»¤åŸºäºçš„æ–‡æœ¬ç¼–è¾‘benchmarkã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨äº†æ–°çš„benchmarkï¼Œå«åšXATUï¼Œå®ƒåŒ…æ‹¬äº†å„ç§è¯é¢˜å’Œæ–‡æœ¬ç±»å‹ï¼Œå¹¶ä¸”åŒ…å«äº†lexicalã€syntacticã€semanticå’Œknowledge-intensiveçš„ç¼–è¾‘æŒ‡ä»¤ã€‚ä»¥æé«˜å¯è¯»æ€§ï¼Œè¿™ä¸ªbenchmarkä½¿ç”¨äº†é«˜è´¨é‡çš„æ•°æ®æºå’Œäººå·¥æ ‡æ³¨ï¼Œä»¥è·å¾—ç»†åŒ–çš„ç¼–è¾‘æŒ‡ä»¤å’Œé‡‘æ ‡å‡†çš„ç¼–è¾‘è§£é‡Šã€‚</li>
<li>results: é€šè¿‡å¯¹ç°æœ‰çš„å¼€æ”¾å’Œå…³é—­å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæœ¬è®ºæ–‡ç¤ºå‡ºäº† instrucion tuning çš„æ•ˆæœå’Œä¸åŒæ¶æ„ä¸‹çš„ç¼–è¾‘ä»»åŠ¡çš„å½±å“ã€‚æ­¤å¤–ï¼Œå¹¿æ³›çš„å®éªŒè¿˜è¡¨æ˜äº†å¯¹æ–‡æœ¬ç¼–è¾‘ä»»åŠ¡çš„ç»†åŒ–è§£é‡Šçš„é‡è¦æ€§ã€‚<details>
<summary>Abstract</summary>
Text editing is a crucial task that involves modifying text to better align with user intents. However, existing text editing benchmark datasets have limitations in providing only coarse-grained instructions. Consequently, although the edited output may seem reasonable, it often deviates from the intended changes outlined in the gold reference, resulting in low evaluation scores. To comprehensively investigate the text editing capabilities of large language models, this paper introduces XATU, the first benchmark specifically designed for fine-grained instruction-based explainable text editing. XATU covers a wide range of topics and text types, incorporating lexical, syntactic, semantic, and knowledge-intensive edits. To enhance interpretability, we leverage high-quality data sources and human annotation, resulting in a benchmark that includes fine-grained instructions and gold-standard edit explanations. By evaluating existing open and closed large language models against our benchmark, we demonstrate the effectiveness of instruction tuning and the impact of underlying architecture across various editing tasks. Furthermore, extensive experimentation reveals the significant role of explanations in fine-tuning language models for text editing tasks. The benchmark will be open-sourced to support reproduction and facilitate future research.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation in Simplified Chinese:æ–‡æœ¬ç¼–è¾‘æ˜¯ä¸€é¡¹é‡è¦çš„ä»»åŠ¡ï¼Œå®ƒæ¶‰åŠä¿®æ”¹æ–‡æœ¬ï¼Œä½¿å…¶æ›´åŠ ç¬¦åˆç”¨æˆ·çš„æ„å›¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–‡æœ¬ç¼–è¾‘æ ‡å‡†æ•°æ®é›†æœ‰é™åˆ¶ï¼Œåªæä¾›ç²—ç•¥çš„æŒ‡ä»¤ã€‚å› æ­¤ï¼Œç¼–è¾‘åçš„è¾“å‡ºå¯èƒ½çœ‹èµ·æ¥åˆç†ï¼Œä½†å®ƒç»å¸¸ä¸é‡‘æ ‡å‡† refer ä¸­çš„ä¿®æ”¹ç»†åˆ™ä¸ç¬¦ï¼Œå¯¼è‡´è¯„ä»·åˆ†æ•°ä½ä¸‹ã€‚ä¸ºäº†å…¨é¢è°ƒæŸ¥å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç¼–è¾‘èƒ½åŠ›ï¼Œè¿™ç¯‡è®ºæ–‡å¼•å…¥ XATUï¼Œé¦–ä¸ªä¸“é—¨ä¸ºç²¾ç»†æŒ‡ä»¤åŸºäºçš„å¯è§£é‡Šæ–‡æœ¬ç¼–è¾‘æ ‡å‡†ã€‚XATU è¦†ç›–äº†å„ç§è¯é¢˜å’Œæ–‡æœ¬ç±»å‹ï¼ŒåŒ…æ‹¬è¯­æ³•ã€è¯­ä¹‰å’ŒçŸ¥è¯†ç­‰ç¼–è¾‘ã€‚ä¸ºäº†å¢å¼ºå¯è¯»æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨é«˜è´¨é‡çš„æ•°æ®æºå’Œäººå·¥æ ‡æ³¨ï¼Œä»è€Œåˆ›å»ºäº†ä¸€ä¸ªåŒ…å«ç²¾ç»†æŒ‡ä»¤å’Œé‡‘æ ‡å‡†ç¼–è¾‘è§£é‡Šçš„æ ‡å‡†ã€‚é€šè¿‡è¯„ä»·ç°æœ‰çš„å¼€æºå’Œå…³é—­å¼å¤§è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬ç¤ºå‡ºäº†æŒ‡ä»¤è°ƒæ•´å’Œæ¨¡å‹çš„åº•å±‚ç»“æ„å¯¹äºä¸åŒçš„ç¼–è¾‘ä»»åŠ¡çš„å½±å“ã€‚æ­¤å¤–ï¼Œå¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼Œè§£é‡Šåœ¨è°ƒæ•´è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç¼–è¾‘ä»»åŠ¡æ—¶å‘æŒ¥äº†é‡è¦çš„ä½œç”¨ã€‚è¿™ä¸ªæ ‡å‡†å°†è¢«å¼€æºï¼Œä»¥æ”¯æŒé‡ç°å’Œæœªæ¥ç ”ç©¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="fakenewsbr-A-Fake-News-Detection-Platform-for-Brazilian-Portuguese"><a href="#fakenewsbr-A-Fake-News-Detection-Platform-for-Brazilian-Portuguese" class="headerlink" title="fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese"></a>fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11052">http://arxiv.org/abs/2309.11052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luiz Giordani, Gilsiley DarÃº, Rhenan Queiroz, Vitor Buzinaro, Davi Keglevich Neiva, Daniel Camilo Fuentes GuzmÃ¡n, Marcos Jardel Henriques, Oilson Alberto Gonzatto Junior, Francisco Louzada</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å‡æ–°é—»çš„æ£€æµ‹ï¼Œå°¤å…¶æ˜¯åœ¨å·´è¥¿è‘¡è„ç‰™è¯­æ–°é—»æ–‡ç« ä¸­ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬TF-IDFå’ŒWord2Vecï¼Œæå–æ–‡æœ¬æ•°æ®ä¸­çš„ç‰¹å¾ã€‚å¹¶è¯„ä¼°äº†å¤šç§åˆ†ç±»ç®—æ³•ï¼Œå¦‚é€»è¾‘å›å½’ã€æ”¯æŒå‘é‡æœºã€Random Forestã€AdaBoostå’ŒLightGBMåœ¨ä¸€ä¸ªåŒ…å«çœŸå®å’Œå‡æ–°é—»æ–‡ç« çš„æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>results: æå‡ºçš„æ–¹æ³•åœ¨è¯„ä¼°ä¸­å®ç°äº†é«˜ç²¾åº¦å’ŒF1-Scoreï¼Œè¯æ˜å…¶åœ¨æ£€æµ‹å‡æ–°é—»ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªUser-friendlyçš„ç½‘é¡µå¹³å°ï¼Œfakenewsbr.comï¼Œä»¥ä¾¿ç”¨æˆ·å¯¹æ–°é—»æ–‡ç« çš„çœŸå®æ€§è¿›è¡Œå®æ—¶åˆ†æã€‚<details>
<summary>Abstract</summary>
The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. This paper presents a comprehensive study on detecting fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves high accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we developed a user-friendly web platform, fakenewsbr.com, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to assess the likelihood of fake news articles. Through empirical analysis and comparative studies, we demonstrate the potential of our approach to contribute to the fight against the spread of fake news and promote more informed media consumption.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå‡æ–°é—»çš„æ‰©æ•£å·²æˆä¸ºå½“å‰çš„ä¸€å¤§é—®é¢˜ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´è°£è¨€çš„ä¼ æ’­å’Œå…¬ä¼—æ„è¯†çš„æ‰­æ›²ã€‚è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†æ£€æµ‹å·´è¥¿è‘¡è„ç‰™è¯­å‡æ–°é—»çš„å®Œæ•´ç ”ç©¶ï¼Œä¸“æ³¨äºæ–°é—»ç±»æ–‡ç« ã€‚æˆ‘ä»¬æè®®ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ï¼Œåˆ©ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼ŒåŒ…æ‹¬TF-IDFå’ŒWord2Vecï¼Œæå–æ–‡æœ¬æ•°æ®ä¸­çš„ç‰¹å¾ã€‚æˆ‘ä»¬è¯„ä¼°äº†å¤šç§åˆ†ç±»ç®—æ³•ï¼Œå¦‚é€»è¾‘å›å½’ã€æ”¯æŒå‘é‡æœºå’ŒRandom Forestç­‰ï¼Œåœ¨ä¸€ä¸ªåŒ…å«çœŸå®å’Œå‡æ–°é—»æ–‡ç« çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†é«˜ç²¾åº¦å’ŒF1åˆ†æ•°ï¼Œè¯æ˜äº†å®ƒçš„æ•ˆivenessåœ¨è¯†åˆ«å‡æ–°é—»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç½‘ç«™ï¼Œfakenewsbr.comï¼Œä»¥ä¾¿è¯„ä¼°æ–°é—»æ–‡ç« çš„çœŸå®æ€§ã€‚æˆ‘ä»¬çš„å¹³å°æä¾›äº†å®æ—¶åˆ†æï¼Œè®©ç”¨æˆ·åœ¨å®æ—¶åŸºç¡€ä¸Šè¯„ä¼°å‡æ–°é—»æ–‡ç« çš„å¯èƒ½æ€§ã€‚é€šè¿‡å®éªŒåˆ†æå’Œæ¯”è¾ƒç ”ç©¶ï¼Œæˆ‘ä»¬è¡¨æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æŠ—å‡»å‡æ–°é—»çš„æ‰©æ•£æ–¹é¢çš„æ½œåœ¨ä½œç”¨ï¼Œå¹¶ä¿ƒè¿›æ›´æœ‰çŸ¥è¯†çš„åª’ä½“æ¶ˆè´¹ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Localize-Retrieve-and-Fuse-A-Generalized-Framework-for-Free-Form-Question-Answering-over-Tables"><a href="#Localize-Retrieve-and-Fuse-A-Generalized-Framework-for-Free-Form-Question-Answering-over-Tables" class="headerlink" title="Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables"></a>Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11049">http://arxiv.org/abs/2309.11049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wentinghome/TAGQA">https://github.com/wentinghome/TAGQA</a></li>
<li>paper_authors: Wenting Zhao, Ye Liu, Yao Wan, Yibo Wang, Zhongfen Deng, Philip S. Yu</li>
<li>for: æé«˜è¡¨æ ¼é—®ç­”ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œä»¥ç”ŸæˆåŸºäºè¡¨æ ¼æ•°æ®å’Œè‡ªç„¶è¯­è¨€ä¿¡æ¯çš„è¯¦ç»†ç­”æ¡ˆã€‚</li>
<li>methods: æå‡ºä¸€ç§ä¸‰ stage æ–¹æ³•ï¼ŒåŒ…æ‹¬è¡¨æ ¼è½¬æ¢ä¸ºå›¾å½¢å’Œç»†åŒ–ç­›é€‰ã€å¤–éƒ¨çŸ¥è¯† retrieval å’Œè¡¨æ ¼å’Œæ–‡æœ¬èåˆï¼ˆTAG-QAï¼‰ï¼Œä»¥è§£å†³åŸºäºè¡¨æ ¼æ•°æ®çš„è‡ªç”±å½¢å¼é—®ç­”çš„æŒ‘æˆ˜ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒTAG-QA èƒ½å¤Ÿç”Ÿæˆæ¯”åŸºeline æ›´åŠ å‡†ç¡®ã€å®Œæ•´çš„ç­”æ¡ˆï¼Œç‰¹åˆ«æ˜¯ä¸ pipeline-based åŸºeline TAPAS å’Œ end-to-end æ¨¡å‹ T5 ç›¸æ¯”ã€‚TAG-QA åœ¨ BLEU-4 å’Œ PARENT F-score ä¸Šæ¯” TAPAS é«˜å‡º 17% å’Œ 14%ï¼Œå¹¶é«˜äº T5 çš„ BLEU-4 å’Œ PARENT F-score ä¸Šçš„æé«˜ä¸º 16% å’Œ 12%ã€‚<details>
<summary>Abstract</summary>
Question answering on tabular data (a.k.a TableQA), which aims at generating answers to questions grounded on a provided table, has gained significant attention recently. Prior work primarily produces concise factual responses through information extraction from individual or limited table cells, lacking the ability to reason across diverse table cells. Yet, the realm of free-form TableQA, which demands intricate strategies for selecting relevant table cells and the sophisticated integration and inference of discrete data fragments, remains mostly unexplored. To this end, this paper proposes a generalized three-stage approach: Table-to- Graph conversion and cell localizing, external knowledge retrieval, and the fusion of table and text (called TAG-QA), to address the challenge of inferring long free-form answers in generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns, (2) leverages external knowledge from Wikipedia, and (3) generates answers by integrating both tabular data and natural linguistic information. Experiments showcase the superior capabilities of TAG-QA in generating sentences that are both faithful and coherent, particularly when compared to several state-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based baseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score, respectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16% and 12% on BLEU-4 and PARENT F-score, respectively.
</details>
<details>
<summary>æ‘˜è¦</summary>
é—®ç­”åŸºäºè¡¨æ ¼æ•°æ®ï¼ˆå³ TableQAï¼‰åœ¨æœ€è¿‘å‡ å¹´å†…è·å¾—äº†å¹¿æ³›å…³æ³¨ï¼Œç›®çš„æ˜¯ç”ŸæˆåŸºäºæä¾›çš„è¡¨æ ¼æ•°æ®çš„é—®é¢˜çš„å›ç­”ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œä¸»è¦é€šè¿‡æå–è¡¨æ ¼å•å…ƒä¸­çš„ä¿¡æ¯è¿›è¡Œä¿¡æ¯æŠ½å–ï¼Œç¼ºä¹èƒ½å¤Ÿè·¨å•å…ƒè¿›è¡Œæ¨ç†çš„èƒ½åŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é€šç”¨çš„ä¸‰stageapproachï¼šè¡¨æ ¼è½¬ grafå¹¶Cell Localizationï¼ˆTAG-QAï¼‰ï¼Œä»¥ç”Ÿæˆå…·æœ‰æ¨ç†èƒ½åŠ›çš„è¡¨æ ¼é—®ç­”ç³»ç»Ÿã€‚å…·ä½“æ¥è¯´ï¼ŒTAG-QAåŒ…æ‹¬ä»¥ä¸‹ä¸‰ä¸ªé˜¶æ®µï¼š1. ä½¿ç”¨å›¾ neural network æ¥æ‰¾åˆ°ç›¸å…³çš„è¡¨æ ¼å•å…ƒï¼Œå¹¶å°†å…¶ä½œä¸ºäº¤å‰å•å…ƒè¿›è¡Œæ±‡èšã€‚2. åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†æ¥æé«˜è¡¨æ ¼é—®ç­”çš„èƒ½åŠ›ã€‚3. å°†è¡¨æ ¼æ•°æ®å’Œè‡ªç„¶è¯­è¨€ä¿¡æ¯ integrate èµ·æ¥ï¼Œä»¥ç”Ÿæˆå…·æœ‰ faithful å’Œ coherent æ€§çš„å›ç­”ã€‚å®éªŒè¡¨æ˜ï¼ŒTAG-QA åœ¨ç”Ÿæˆé•¿åº¦ä¸å—é™åˆ¶çš„è‡ªç”±å½¢è¡¨æ ¼é—®ç­”æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯ä¸ä¸€äº›çŠ¶æ€ä¹‹é™…çš„åŸºå‡†å€¼è¿›è¡Œæ¯”è¾ƒã€‚åœ¨ BLEU-4 å’Œ PARENT F-score ç­‰æŒ‡æ ‡ä¸Šï¼ŒTAG-QA ä¸ TAPAS å’Œ T5 æ¨¡å‹ç›¸æ¯”ï¼Œå‡€æé«˜äº†17%å’Œ14%ã€‚æ­¤å¤–ï¼ŒTAG-QA è¿˜åœ¨ BLEU-4 å’Œ PARENT F-score ä¸Šå‡ºç°16%å’Œ12%çš„æå‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Entity-Matching-with-Complex-Attribute-Associations-using-BERT-and-Neural-Networks"><a href="#Heterogeneous-Entity-Matching-with-Complex-Attribute-Associations-using-BERT-and-Neural-Networks" class="headerlink" title="Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks"></a>Heterogeneous Entity Matching with Complex Attribute Associations using BERT and Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11046">http://arxiv.org/abs/2309.11046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shitao Wang, Jiamin Lu</li>
<li>for: Addressing the challenges of entity matching in heterogeneous data with complex attribute relationships.</li>
<li>methods: Utilizing a novel entity matching model, EMM-CCAR, built upon pre-trained models, with attention mechanisms to capture complex relationships between attributes.</li>
<li>results: Achieving improvements of approximately 4% and 1% in F1 scores compared to prevalent DER-SSM and Ditto approaches, respectively, demonstrating the effectiveness of the proposed model in handling complex attribute relationships.<details>
<summary>Abstract</summary>
Across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the matching task into a sequence matching problem to mitigate the impact of varying data formats. Moreover, by introducing attention mechanisms, it identifies complex relationships between attributes, emphasizing the degree of matching among multiple attributes rather than one-to-one correspondences. Through the integration of the EMM-CCAR model, we adeptly surmount the challenges posed by data heterogeneity and intricate attribute interdependencies. In comparison with the prevalent DER-SSM and Ditto approaches, our model achieves improvements of approximately 4% and 1% in F1 scores, respectively. This furnishes a robust solution for addressing the intricacies of attribute complexity in entity matching.
</details>
<details>
<summary>æ‘˜è¦</summary>
across various domains, data from different sources such as Baidu Baike and Wikipedia often manifest in distinct forms. Current entity matching methodologies predominantly focus on homogeneous data, characterized by attributes that share the same structure and concise attribute values. However, this orientation poses challenges in handling data with diverse formats. Moreover, prevailing approaches aggregate the similarity of attribute values between corresponding attributes to ascertain entity similarity. Yet, they often overlook the intricate interrelationships between attributes, where one attribute may have multiple associations. The simplistic approach of pairwise attribute comparison fails to harness the wealth of information encapsulated within entities.To address these challenges, we introduce a novel entity matching model, dubbed Entity Matching Model for Capturing Complex Attribute Relationships(EMM-CCAR),built upon pre-trained models. Specifically, this model transforms the matching task into a sequence matching problem to mitigate the impact of varying data formats. Moreover, by introducing attention mechanisms, it identifies complex relationships between attributes, emphasizing the degree of matching among multiple attributes rather than one-to-one correspondences. Through the integration of the EMM-CCAR model, we adeptly surmount the challenges posed by data heterogeneity and intricate attribute interdependencies. In comparison with the prevalent DER-SSM and Ditto approaches, our model achieves improvements of approximately 4% and 1% in F1 scores, respectively. This furnishes a robust solution for addressing the intricacies of attribute complexity in entity matching.
</details></li>
</ul>
<hr>
<h2 id="Named-Entity-Recognition-via-Machine-Reading-Comprehension-A-Multi-Task-Learning-Approach"><a href="#Named-Entity-Recognition-via-Machine-Reading-Comprehension-A-Multi-Task-Learning-Approach" class="headerlink" title="Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach"></a>Named Entity Recognition via Machine Reading Comprehension: A Multi-Task Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11027">http://arxiv.org/abs/2309.11027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Wang, Wenting Zhao, Yao Wan, Zhongfen Deng, Philip S. Yu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜Machine Reading Comprehensionï¼ˆMRCï¼‰åŸºäºNamed Entity Recognitionï¼ˆNERï¼‰çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å¿½ç•¥å®ä½“ç±»åˆ« Label ç›¸äº’å…³ç³»çš„æƒ…å†µä¸‹ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šä»»åŠ¡å­¦ä¹ çš„ Multi-NER æ¨¡å‹ï¼Œé€šè¿‡è‡ªæ³¨æ„æœºåˆ¶ capture å®ä½“ç±»åˆ« Label ç›¸äº’å…³ç³»ã€‚</li>
<li>results: å¯¹äºåµŒå…¥å¼ NER å’Œå¹³é¢ NER æ•°æ®é›†ï¼Œå®éªŒç»“æœè¡¨æ˜ Multi-NER å¯ä»¥åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šæé«˜æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) aims to extract and classify entity mentions in the text into pre-defined types (e.g., organization or person name). Recently, many works have been proposed to shape the NER as a machine reading comprehension problem (also termed MRC-based NER), in which entity recognition is achieved by answering the formulated questions related to pre-defined entity types through MRC, based on the contexts. However, these works ignore the label dependencies among entity types, which are critical for precisely recognizing named entities. In this paper, we propose to incorporate the label dependencies among entity types into a multi-task learning framework for better MRC-based NER. We decompose MRC-based NER into multiple tasks and use a self-attention module to capture label dependencies. Comprehensive experiments on both nested NER and flat NER datasets are conducted to validate the effectiveness of the proposed Multi-NER. Experimental results show that Multi-NER can achieve better performance on all datasets.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Joint-Modeling-of-Dialogue-Response-and-Speech-Synthesis-based-on-Large-Language-Model"><a href="#Towards-Joint-Modeling-of-Dialogue-Response-and-Speech-Synthesis-based-on-Large-Language-Model" class="headerlink" title="Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model"></a>Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11000">http://arxiv.org/abs/2309.11000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/XinyuZhou2000/Spoken_Dialogue">https://github.com/XinyuZhou2000/Spoken_Dialogue</a></li>
<li>paper_authors: Xinyu Zhou, Delong Chen, Yudong Chen</li>
<li>for: æ„å»ºä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¯¹è¯ç³»ç»Ÿï¼Œæ›´åŠ å‡†ç¡®åœ°æ¨¡æ‹Ÿäººç±»è¯­è¨€ç”Ÿæˆè¿‡ç¨‹ã€‚</li>
<li>methods: ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ¥åŒæ—¶æ¨¡å‹å¯¹è¯å“åº”å’Œè¯­è¨€ç‰¹å¾ï¼Œå¹¶åœ¨è¯­éŸ³ç»“æ„é¢„æµ‹å’Œå¯¹è¯å“åº” integrate å¤šç§è¯­è¨€ç‰¹å¾ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æ–¹æ³•æ˜¯å»ºç«‹ä¸€ä¸ªç´§å‡‘çš„å¯¹è¯ç³»ç»Ÿçš„å¯èƒ½æ€§çš„ã€‚<details>
<summary>Abstract</summary>
This paper explores the potential of constructing an AI spoken dialogue system that "thinks how to respond" and "thinks how to speak" simultaneously, which more closely aligns with the human speech production process compared to the current cascade pipeline of independent chatbot and Text-to-Speech (TTS) modules. We hypothesize that Large Language Models (LLMs) with billions of parameters possess significant speech understanding capabilities and can jointly model dialogue responses and linguistic features. We conduct two sets of experiments: 1) Prosodic structure prediction, a typical front-end task in TTS, demonstrating the speech understanding ability of LLMs, and 2) Further integrating dialogue response and a wide array of linguistic features using a unified encoding format. Our results indicate that the LLM-based approach is a promising direction for building unified spoken dialogue systems.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä¸ªè®ºæ–‡æ¢è®¨äº†æ„å»ºä¸€ä¸ªåŸºäºäººå·¥æ™ºèƒ½çš„å¯¹è¯ç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿå¯ä»¥åŒæ—¶â€œæ€è€ƒå¦‚ä½•å›ç­”â€å’Œâ€œæ€è€ƒå¦‚ä½•è¯´â€ï¼Œè¿™æ›´æ¥è¿‘äºäººç±»è¯­è¨€ç”Ÿäº§è¿‡ç¨‹ã€‚æˆ‘ä»¬å‡è®¾å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ‹¥æœ‰æ•°åäº¿ä¸ªå‚æ•°ï¼Œå…·æœ‰å¼ºå¤§çš„è¯­éŸ³ç†è§£èƒ½åŠ›ï¼Œå¯ä»¥åŒæ—¶æ¨¡å‹å¯¹è¯å›ç­”å’Œè¯­è¨€ç‰¹å¾ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸¤ç»„å®éªŒï¼š1ï¼‰è¯­è°ƒç»“æ„é¢„æµ‹ï¼Œè¿™æ˜¯å¸¸è§çš„å‰ç«¯ä»»åŠ¡åœ¨æ–‡æœ¬è¯†åˆ«ä¸­ï¼Œä»¥ç¤ºLLMçš„è¯­éŸ³ç†è§£èƒ½åŠ›ï¼›2ï¼‰å°†å¯¹è¯å›ç­”å’Œå¹¿æ³›çš„è¯­è¨€ç‰¹å¾é›†æˆä½¿ç”¨ç»Ÿä¸€ç¼–ç æ ¼å¼ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŸºäºLLMçš„æ–¹æ³•æ˜¯æ„å»ºç»Ÿä¸€çš„å¯¹è¯ç³»ç»Ÿçš„å¯èƒ½ä¹‹é“ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.CL_2023_09_20/" data-id="clopawnqd00bkag889llqduyl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/cs.LG_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T10:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/cs.LG_2023_09_20/">cs.LG - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Large-scale-Pretraining-Improves-Sample-Efficiency-of-Active-Learning-based-Molecule-Virtual-Screening"><a href="#Large-scale-Pretraining-Improves-Sample-Efficiency-of-Active-Learning-based-Molecule-Virtual-Screening" class="headerlink" title="Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening"></a>Large-scale Pretraining Improves Sample Efficiency of Active Learning based Molecule Virtual Screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11687">http://arxiv.org/abs/2309.11687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhonglin Cao, Simone Sciabola, Ye Wang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ´»æ€§å­¦ä¹ å’Œ bayesian ä¼˜åŒ–åœ¨è™šæ‹Ÿå±é€‰ä¸­çš„ç²¾åº¦å’Œæ ·æœ¬æ•ˆç‡ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„ transformer åŸºäºè¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬è™šæ‹Ÿå±é€‰ã€æ´»æ€§å­¦ä¹ å’Œ bayesian ä¼˜åŒ–ï¼Œé‡‡ç”¨é¢„è®­ç»ƒçš„ transformer åŸºäºè¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œä½œä¸ºè¯„ä¼°æ¨¡å‹ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé¢„è®­ç»ƒ transformer åŸºäºè¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œåœ¨ Bayesian ä¼˜åŒ–active learningæ¡†æ¶ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¯ä»¥åœ¨è™šæ‹Ÿå±é€‰ä¸­æé«˜ç²¾åº¦å’Œæ ·æœ¬æ•ˆç‡ï¼Œæ¯”å‰ä¸€ä¸ªåŸºelineæé«˜8%ã€‚<details>
<summary>Abstract</summary>
Virtual screening of large compound libraries to identify potential hit candidates is one of the earliest steps in drug discovery. As the size of commercially available compound collections grows exponentially to the scale of billions, brute-force virtual screening using traditional tools such as docking becomes infeasible in terms of time and computational resources. Active learning and Bayesian optimization has recently been proven as effective methods of narrowing down the search space. An essential component in those methods is a surrogate machine learning model that is trained with a small subset of the library to predict the desired properties of compounds. Accurate model can achieve high sample efficiency by finding the most promising compounds with only a fraction of the whole library being virtually screened. In this study, we examined the performance of pretrained transformer-based language model and graph neural network in Bayesian optimization active learning framework. The best pretrained models identifies 58.97% of the top-50000 by docking score after screening only 0.6% of an ultra-large library containing 99.5 million compounds, improving 8% over previous state-of-the-art baseline. Through extensive benchmarks, we show that the superior performance of pretrained models persists in both structure-based and ligand-based drug discovery. Such model can serve as a boost to the accuracy and sample efficiency of active learning based molecule virtual screening.
</details>
<details>
<summary>æ‘˜è¦</summary>
åšä¸ºè¯ç‰©å‘ç°çš„æ—©æœŸæ­¥éª¤ä¹‹ä¸€ï¼Œè™šæ‹Ÿå±é€‰å¤§è§„æ¨¡åŒ–åˆç‰©åº“ä»¥æ‰¾åˆ°æ½œåœ¨çš„é¶ç‚¹å€™é€‰è€…ã€‚éšç€å•†ä¸šå¯ç”¨çš„åŒ–åˆç‰©é›†åˆçš„è§„æ¨¡ exponentiated åˆ°äº¿é‡çº§ï¼Œä½¿ç”¨ä¼ ç»Ÿå·¥å…· such as docking è¿›è¡Œè™šæ‹Ÿå±é€‰æˆä¸ºè®¡ç®—èµ„æºå’Œæ—¶é—´ä¸Šçš„ä¸å¯è¡Œã€‚æ´»åŠ¨å­¦ä¹ å’Œ Bayesian ä¼˜åŒ–å·²ç»è¢«è¯æ˜ä¸ºè™šæ‹Ÿå±é€‰ä¸­çš„æœ‰æ•ˆæ–¹æ³•ã€‚è¿™äº›æ–¹æ³•ä¸­çš„ä¸€ä¸ªå…³é”®ç»„ä»¶æ˜¯ä¸€ä¸ªè®­ç»ƒäºå°å‹åº“ä¸­çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹åŒ–åˆç‰©çš„æ¬²è¦æ€§ã€‚ä¸€æ—¦æœ‰ä¸€ä¸ªå‡†ç¡®çš„æ¨¡å‹ï¼Œå®ƒå¯ä»¥åœ¨è™šæ‹Ÿå±é€‰ä¸­é«˜æ•ˆåœ°å¯»æ‰¾æœ€æœ‰å‰é€”çš„åŒ–åˆç‰©ï¼Œåªéœ€è™šæ‹Ÿå±é€‰å‡ºä¸€å°éƒ¨åˆ†çš„åŒ–åˆç‰©åº“ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä½¿ç”¨é¢„è®­ç»ƒçš„ transformer åŸºäºè¯­è¨€æ¨¡å‹å’Œå›¾ç¥ç»ç½‘ç»œåœ¨ Bayesian ä¼˜åŒ–æ´»åŠ¨å­¦ä¹ æ¡†æ¶ä¸­çš„è¡¨ç°ã€‚æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹å¯ä»¥åœ¨è™šæ‹Ÿå±é€‰å‡º 99.5 äº¿ä¸ªåŒ–åˆç‰©åº“ä¸­çš„ 58.97% æœ€ä½³ docking åˆ†æ•°å‰ 50000 ä¸ªåŒ–åˆç‰©ï¼Œæé«˜äº† 8% äºå‰ä¸€ä¸ªåŸºå‡†å€¼ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„ benchmark è¡¨æ˜ï¼Œé¢„è®­ç»ƒæ¨¡å‹åœ¨ç»“æ„åŸºäºå’Œè¯ç‰©åŸºäºçš„è¯ç‰©å‘ç°ä¸­çš„è¡¨ç°ä»ç„¶ä¼˜ç§€ã€‚è¿™ç§æ¨¡å‹å¯ä»¥ä¸ºæ´»åŠ¨å­¦ä¹ åŸºäºè™šæ‹Ÿå±é€‰çš„è¯ç‰©å‘ç°å¢åŠ ç²¾åº¦å’Œé‡‡æ ·æ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Popularity-Degradation-Bias-in-Local-Music-Recommendation"><a href="#Popularity-Degradation-Bias-in-Local-Music-Recommendation" class="headerlink" title="Popularity Degradation Bias in Local Music Recommendation"></a>Popularity Degradation Bias in Local Music Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11671">http://arxiv.org/abs/2309.11671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asem010/legend-pice">https://github.com/asem010/legend-pice</a></li>
<li>paper_authors: April Trainor, Douglas Turnbull</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†å½“åœ°éŸ³ä¹æ¨èä¸­çš„äººæ°”å€’é€€åè§é—®é¢˜ã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº†ä¸¤ç§ç°åœºè¡¨ç°æœ€ä½³çš„æ¨èç®—æ³•ï¼šWeight Relevance Matrix Factorization (WRMF) å’Œ Multinomial Variational Autoencoder (Mult-VAE)ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œè¿™ä¸¤ç§ç®—æ³•åœ¨æ›´å—æ¬¢è¿çš„è‰ºæœ¯å®¶ä¸Šçš„æ¨èæ€§èƒ½éƒ½æœ‰æ‰€æé«˜ï¼Œå¹¶ä¸”å±•ç°äº†äººæ°”å€’é€€åè§ã€‚ Mult-VAE åœ¨ menos popular çš„è‰ºæœ¯å®¶ä¸Šè¡¨ç°æ›´å¥½ï¼Œå› æ­¤åœ¨å½“åœ°éŸ³ä¹è‰ºæœ¯å®¶æ¨èä¸­å¯èƒ½æ›´æœ‰ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
In this paper, we study the effect of popularity degradation bias in the context of local music recommendations. Specifically, we examine how accurate two top-performing recommendation algorithms, Weight Relevance Matrix Factorization (WRMF) and Multinomial Variational Autoencoder (Mult-VAE), are at recommending artists as a function of artist popularity. We find that both algorithms improve recommendation performance for more popular artists and, as such, exhibit popularity degradation bias. While both algorithms produce a similar level of performance for more popular artists, Mult-VAE shows better relative performance for less popular artists. This suggests that this algorithm should be preferred for local (long-tail) music artist recommendation.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æœ¬åœ°éŸ³ä¹æ¨èä¸­çš„äººæ°—å€’é€€åè§å½±å“ã€‚æˆ‘ä»¬ä¸“é—¨ç ”ç©¶äº†ä¸¤ç§æœ€ä½³æ¨èç®—æ³•çš„ç²¾åº¦ï¼Œå³Weight Relevance Matrix Factorization (WRMF)å’ŒMultinomial Variational Autoencoder (Mult-VAE)ã€‚æˆ‘ä»¬å‘ç°è¿™ä¸¤ç§ç®—æ³•å¯¹æ›´å—æ¬¢è¿çš„è‰ºæœ¯å®¶çš„æ¨èæ€§èƒ½éƒ½æœ‰æ”¹å–„ï¼Œå› æ­¤å®ƒä»¬éƒ½å­˜åœ¨äººæ°—å€’é€€åè§ã€‚è™½ç„¶è¿™ä¸¤ç§ç®—æ³•åœ¨æ›´å—æ¬¢è¿çš„è‰ºæœ¯å®¶ä¸­çš„è¡¨ç°æ°´å¹³ç›¸ä¼¼ï¼Œä½†Mult-VAEåœ¨ menos popular è‰ºæœ¯å®¶ä¸­è¡¨ç°æ›´ä¼˜ã€‚è¿™è¡¨ç¤ºMult-VAEåº”è¯¥é€‰æ‹©ç”¨äºæœ¬åœ°ï¼ˆé•¿å°¾ï¼‰éŸ³ä¹è‰ºæœ¯å®¶æ¨èã€‚
</details></li>
</ul>
<hr>
<h2 id="GLM-Regression-with-Oblivious-Corruptions"><a href="#GLM-Regression-with-Oblivious-Corruptions" class="headerlink" title="GLM Regression with Oblivious Corruptions"></a>GLM Regression with Oblivious Corruptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11657">http://arxiv.org/abs/2309.11657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Sushrut Karmalkar, Jongho Park, Christos Tzamos</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†è§£å†³é€šç”¨çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰åœ¨æŸäº›æƒ…å†µä¸‹æ·»åŠ äº†éšæœºå™ªå£°çš„é—®é¢˜è€Œå†™çš„ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯¥ç®—æ³•å¯ä»¥åœ¨æœ€é€šç”¨çš„åˆ†å¸ƒæ— å…³ Settingsä¸­å®ç°ã€‚</li>
<li>results: è®ºæ–‡çš„ç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•å¯ä»¥åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æä¾›é«˜åº¦å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸”å¯ä»¥å¤„ç†æ›´å¤šçš„æ ·æœ¬è¢«éšæœºå™ªå£°æŸå®³çš„æƒ…å†µã€‚I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
We demonstrate the first algorithms for the problem of regression for generalized linear models (GLMs) in the presence of additive oblivious noise. We assume we have sample access to examples $(x, y)$ where $y$ is a noisy measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$, and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has} arbitrarily small error when compared to the true values $g(w^* \cdot x)$, rather than the noisy measurements $y$.   We present an algorithm that tackles \new{this} problem in its most general distribution-independent setting, where the solution may not \new{even} be identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the solution if it is identifiable, and otherwise returns a small list of candidates, one of which is close to the true solution. Furthermore, we \new{provide} a necessary and sufficient condition for identifiability, which holds in broad settings. \new{Specifically,} the problem is identifiable when the quantile at which $\xi + \epsilon = 0$ is known, or when the family of hypotheses does not contain candidates that are nearly equal to a translated $g(w^* \cdot x) + A$ for some real number $A$, while also having large error when compared to $g(w^* \cdot x)$.   This is the first \new{algorithmic} result for GLM regression \new{with oblivious noise} which can handle more than half the samples being arbitrarily corrupted. Prior work focused largely on the setting of linear regression, and gave algorithms under restrictive assumptions.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬å±•ç¤ºäº†ç¬¬ä¸€ä¸ªå¯¹äºé€šç”¨çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰ä¸­æ‰©å±•çš„é—®é¢˜çš„å›æº¯ç®—æ³•ã€‚æˆ‘ä»¬å‡è®¾æœ‰ä¸€ä¸ªè®¿é—®ä¾‹å­ $(x, y)$ï¼Œå…¶ä¸­ $y $ æ˜¯ $g(w^* \cdot x)$ çš„é”™è¯¯çš„æµ‹é‡ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å‡è®¾é”™è¯¯æ ‡ç­¾çš„å½¢å¼ä¸º $y = g(w^* \cdot x) + \xi + \epsilon$ï¼Œå…¶ä¸­ $\xi $ æ˜¯ç‹¬ç«‹äº $x$ çš„éšæœºé”™è¯¯ï¼Œä¸” $\Pr[\xi = 0] \geq o(1)$ï¼Œä¸” $\epsilon \sim \mathcal N(0, \sigma^2)$ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†ä¸€ä¸ªç²¾ç¡®åœ°å›ä¼  $w $ çš„å‚æ•°ï¼Œä½¿å¾— $g(w \cdot x)$ ä¸çœŸæ­£çš„å€¼ $g(w^* \cdot x)$ ä¹‹é—´çš„å·®å¼‚å¯ä»¥éšæ—¶å¯¹åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¯ä»¥åœ¨æœ€é€šç”¨çš„åˆ†å¸ƒä¸ä¾èµ–æƒ…å†µä¸‹è§£å†³è¿™ä¸ªé—®é¢˜çš„ç®—æ³•ã€‚å¦‚æœé—®é¢˜å¯è§£æï¼Œæˆ‘ä»¬çš„ç®—æ³•å°†è¿”å›ä¸€ä¸ªç²¾ç¡®çš„è§£æç»“æœï¼›å¦åˆ™ï¼Œå®ƒå°†è¿”å›ä¸€ä¸ªå°åˆ—è¡¨ï¼Œå…¶ä¸­ä¸€ä¸ªä¸çœŸå®è§£æç»“æœç›¸ä¼¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å¿…è¦å’Œå……åˆ†çš„å¯ identificability æ¡ä»¶ï¼Œè¿™æ ·åœ¨å¹¿æ³›çš„è®¾å®šä¸‹éƒ½ä¼šæˆç«‹ã€‚å…·ä½“æ¥è¯´ï¼Œé—®é¢˜å¯è§£æå½“ $\xi + \epsilon = 0$ çš„quantile çŸ¥é“ï¼Œæˆ–è€…å®¶æ—å‡è®¾ä¸åŒ…å« nearly equal to $g(w^* \cdot x) + A$ çš„å€™é€‰è€…ï¼Œè€Œä¸”åœ¨ä¸ $g(w^* \cdot x)$ æ¯”è¾ƒæ—¶æœ‰å¤§çš„è¯¯å·®ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªå¯¹ GLM å›æº¯ç®—æ³•ä¸­æ‰©å±•çš„æ•°æ®éªŒè¯é¡¹ç›®ï¼Œå¯ä»¥åº”å¯¹æ›´å¤šäºåŠæ•°çš„æ ·æœ¬è¢«ä»»æ„æŸåã€‚å…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨çº¿æ€§å›æº¯é¢†åŸŸï¼Œå¹¶æä¾›äº†å¯¹äºç‰¹å®šå‡è®¾çš„é™åˆ¶æ€§ç®—æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Drift-Control-of-High-Dimensional-RBM-A-Computational-Method-Based-on-Neural-Networks"><a href="#Drift-Control-of-High-Dimensional-RBM-A-Computational-Method-Based-on-Neural-Networks" class="headerlink" title="Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks"></a>Drift Control of High-Dimensional RBM: A Computational Method Based on Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11651">http://arxiv.org/abs/2309.11651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nian-si/rbmsolver">https://github.com/nian-si/rbmsolver</a></li>
<li>paper_authors: Baris Ata, J. Michael Harrison, Nian Si</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯å¯»æ‰¾ä¸€ç§èƒ½å¤Ÿåœ¨æ— ç©·è§„åˆ’ horizion ä¸Šæœ€å°åŒ–é¢„ç®—çš„æŠ˜æŸæ§åˆ¶æ–¹æ³•ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†æ·±åº¦ç¥ç»ç½‘ç»œæŠ€æœ¯æ¥è§£å†³è¿™ä¸ªæ§åˆ¶é—®é¢˜ï¼Œå¹¶å¯¹ä¸€äº›æµ‹è¯•é—®é¢˜è¿›è¡Œäº†å®éªŒ validateã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæŠ€æœ¯å¯ä»¥åœ¨é«˜ç»´åº¦($d&#x3D;30$) ä¸‹å®ç°é«˜ç²¾åº¦çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”è®¡ç®—æ•ˆç‡é«˜ã€‚<details>
<summary>Abstract</summary>
Motivated by applications in queueing theory, we consider a stochastic control problem whose state space is the $d$-dimensional positive orthant. The controlled process $Z$ evolves as a reflected Brownian motion whose covariance matrix is exogenously specified, as are its directions of reflection from the orthant's boundary surfaces. A system manager chooses a drift vector $\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at time $t$ depends on both $Z(t)$ and $\theta(t)$. In our initial problem formulation, the objective is to minimize expected discounted cost over an infinite planning horizon, after which we treat the corresponding ergodic control problem. Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction of one percent, and is computationally feasible in dimensions up to at least $d=30$.
</details>
<details>
<summary>æ‘˜è¦</summary>
Extending earlier work by Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510), we develop and illustrate a simulation-based computational method that relies heavily on deep neural network technology. For test problems studied thus far, our method is accurate to within a fraction of one percent, and is computationally feasible in dimensions up to at least $d=30$.Translated into Simplified Chinese:æˆ‘ä»¬å—åˆ°æ’é˜µç†è®ºåº”ç”¨çš„é©±åŠ¨ä¸‹ï¼Œè€ƒè™‘ä¸€ä¸ª Stochastic control problemï¼Œå…¶state spaceæ˜¯ $d$ ç»´æ­£æ–¹å½¢ã€‚æ§åˆ¶è¿‡ç¨‹ $Z$ æ˜¯ä¸€ä¸ªå—åˆ°ç¡®å®šçš„å‡å€¼çŸ©é˜µå½±å“çš„åå°„ Ğ‘Ñ€Ğ°ÑƒĞ½è¿åŠ¨ï¼Œå…¶æ–¹å‘å—åˆ°æ­£æ–¹å½¢è¾¹ç•Œè¡¨é¢çš„åå°„å½±å“ã€‚ç³»ç»Ÿç®¡ç†å‘˜åœ¨æ¯ä¸ªæ—¶åˆ» $t$ é€‰æ‹©ä¸€ä¸ªæ¨ç§» Ğ²ĞµĞºÑ‚Ğ¾Ñ€ $\theta(t)$ï¼ŒåŸºäº $Z$ çš„å†å²ï¼Œè€Œåœ¨æ¯ä¸ªæ—¶åˆ» $t$ çš„æˆæœ¬ç‡å–å†³äº $Z(t)$ å’Œ $\theta(t)$ã€‚åœ¨æˆ‘ä»¬çš„åˆå§‹é—®é¢˜ä¸­ï¼Œç›®æ ‡æ˜¯åœ¨æ— é™è®¡åˆ’æ—¶é—´åé¢å†…é¢„ç®—æˆæœ¬ï¼Œç„¶åå¤„ç†ç›¸åº”çš„ergodic controlé—®é¢˜ã€‚æˆ‘ä»¬å°† extending Han et al. (Proceedings of the National Academy of Sciences, 2018, 8505-8510) çš„ç ”ç©¶ï¼Œå¼€å‘äº†ä¸€ä¸ªåŸºäºæ·±åº¦ç¥ç»ç½‘ç»œæŠ€æœ¯çš„ simulational-based computational methodã€‚åœ¨æˆ‘ä»¬è¯•éªŒçš„é—®é¢˜ä¸Šï¼Œæˆ‘ä»¬çš„æ–¹æ³•ç²¾åº¦åœ¨ fraction of one percent ä»¥å†…ï¼Œå¹¶ä¸”åœ¨ç»´åº¦è‡³å°‘ $d=30$ æ—¶æ˜¯ computationally feasibleã€‚
</details></li>
</ul>
<hr>
<h2 id="Potential-and-limitations-of-random-Fourier-features-for-dequantizing-quantum-machine-learning"><a href="#Potential-and-limitations-of-random-Fourier-features-for-dequantizing-quantum-machine-learning" class="headerlink" title="Potential and limitations of random Fourier features for dequantizing quantum machine learning"></a>Potential and limitations of random Fourier features for dequantizing quantum machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11647">http://arxiv.org/abs/2309.11647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Sweke, Erik Recio, Sofiene Jerbi, Elies Gil-Fuster, Bryce Fuller, Jens Eisert, Johannes Jakob Meyer</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯å…³äºé‡å­æœºå™¨å­¦ä¹ çš„åº”ç”¨ï¼Œå…·ä½“æ¥è¯´æ˜¯å…³äºè¿‘æœŸé‡å­è®¾å¤‡ä¸Šçš„å˜é‡é‡å­æœºå™¨å­¦ä¹ ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å‚æ•°åŒ–çš„é‡å­ç”µè·¯ï¼ˆPQCï¼‰ä½œä¸ºå­¦ä¹ æ¨¡å‹ï¼Œå¹¶ç ”ç©¶äº†è¿™äº›PQCæ¨¡å‹åœ¨å‡é‡åŒ–ä¸Šçš„æ•ˆç‡ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡æå‡ºäº†å…³äºå˜é‡é‡å­æœºå™¨å­¦ä¹  regression é—®é¢˜ä¸‹å‡é‡åŒ–çš„å¿…è¦å’Œ suficient æ¡ä»¶ï¼Œå¹¶åŸºäºè¿™äº›å‡†åˆ™æå‡ºäº†å…·ä½“çš„PQCæ¶æ„è®¾è®¡å’Œä¼˜åŒ–æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Quantum machine learning is arguably one of the most explored applications of near-term quantum devices. Much focus has been put on notions of variational quantum machine learning where parameterized quantum circuits (PQCs) are used as learning models. These PQC models have a rich structure which suggests that they might be amenable to efficient dequantization via random Fourier features (RFF). In this work, we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization.
</details>
<details>
<summary>æ‘˜è¦</summary>
é‡å­æœºå™¨å­¦ä¹ æ˜¯è¿‘æœŸé‡å­è®¾å¤‡åº”ç”¨çš„ä¸€ä¸ªæœ€å…·æ¢ç´¢æ€§çš„é¢†åŸŸã€‚è®¸å¤šç ”ç©¶éƒ½é›†ä¸­åœ¨å˜é‡é‡å­æœºå™¨å­¦ä¹ ä¸­ï¼Œä½¿ç”¨å‚æ•°åŒ–é‡å­ç”µè·¯ï¼ˆPQCï¼‰ä½œä¸ºå­¦ä¹ æ¨¡å‹ã€‚è¿™äº›PQCæ¨¡å‹å…·æœ‰ä¸°å¯Œçš„ç»“æ„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯èƒ½ä¼šå—åˆ°æœ‰æ•ˆçš„å‡é‡åŒ–å¤„ç†ï¼ˆRFFï¼‰ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç¡®å®šäº†å˜é‡é‡å­æœºå™¨å­¦ä¹  regression é—®é¢˜ä¸‹çš„å¿…è¦å’Œå……åˆ†æ¡ä»¶ï¼Œä»¥ç¡®ä¿RFFå®ç°æœ‰æ•ˆçš„å‡é‡åŒ–ã€‚æˆ‘ä»¬åŸºäºè¿™äº›å‘ç°ï¼Œå¯¹PQCæ¶æ„è®¾è®¡æå‡ºäº†å…·ä½“çš„å»ºè®®ï¼Œå¹¶æ ‡è¯†äº†å¯ä»¥ä½¿ç”¨PQCåŸºäºä¼˜åŒ–å®ç°é‡å­ä¼˜åŠ¿çš„ç»“æ„ã€‚
</details></li>
</ul>
<hr>
<h2 id="Early-diagnosis-of-autism-spectrum-disorder-using-machine-learning-approaches"><a href="#Early-diagnosis-of-autism-spectrum-disorder-using-machine-learning-approaches" class="headerlink" title="Early diagnosis of autism spectrum disorder using machine learning approaches"></a>Early diagnosis of autism spectrum disorder using machine learning approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11646">http://arxiv.org/abs/2309.11646</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diponkor-bala/autism-spectrum-disorder">https://github.com/diponkor-bala/autism-spectrum-disorder</a></li>
<li>paper_authors: Rownak Ara Rasul, Promy Saha, Diponkor Bala, S M Rakib Ul Karim, Ibrahim Abdullah, Bishwajit Saha</li>
<li>for: This paper aims to utilize machine learning algorithms to identify and automate the diagnostic process for Autistic Spectrum Disorder (ASD).</li>
<li>methods: The paper employs six classification models and five popular clustering methods to analyze ASD datasets, and evaluates their performance using various metrics such as accuracy, precision, recall, specificity, F1-score, AUC, kappa, and log loss.</li>
<li>results: The paper achieves a 100% accuracy rate when hyperparameters are carefully tuned for each model, and finds that spectral clustering outperforms other benchmarking clustering models in terms of NMI and ARI metrics, demonstrating comparability to the optimal SC achieved by k-means.Hereâ€™s the Chinese version of the three key points:</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•æ¥è¯†åˆ«å’Œè‡ªåŠ¨è¯Šæ–­å¬åŠ›ç‰¹æŒ‡ç—‡ï¼ˆASDï¼‰ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨ six ç§åˆ†ç±»æ¨¡å‹å’Œ five ç§æµè¡Œçš„èšç±»æ–¹æ³•æ¥åˆ†æ ASD æ•°æ®é›†ï¼Œå¹¶è¯„ä¼°å…¶æ€§èƒ½ä½¿ç”¨å¤šç§æŒ‡æ ‡ such as å‡†ç¡®ç‡ã€ç²¾åº¦ã€ recallã€ç‰¹å¼‚æ€§ã€ F1 åˆ†æ•°ã€ AUCã€ kappa å’Œ log lossã€‚</li>
<li>results: è®ºæ–‡åœ¨hyperparameter ä»”ç»†è°ƒæ•´åï¼Œ achieved a 100% çš„å‡†ç¡®ç‡ï¼Œå¹¶å‘ç° spectral clustering åœ¨ NMI å’Œ ARI æŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä¸ k-means çš„æœ€ä½³ SC ç›¸æ¯”ã€‚<details>
<summary>Abstract</summary>
Autistic Spectrum Disorder (ASD) is a neurological disease characterized by difficulties with social interaction, communication, and repetitive activities. The severity of these difficulties varies, and those with this diagnosis face unique challenges. While its primary origin lies in genetics, identifying and addressing it early can contribute to the enhancement of the condition. In recent years, machine learning-driven intelligent diagnosis has emerged as a supplement to conventional clinical approaches, aiming to address the potential drawbacks of time-consuming and costly traditional methods. In this work, we utilize different machine learning algorithms to find the most significant traits responsible for ASD and to automate the diagnostic process. We study six classification models to see which model works best to identify ASD and also study five popular clustering methods to get a meaningful insight of these ASD datasets. To find the best classifier for these binary datasets, we evaluate the models using accuracy, precision, recall, specificity, F1-score, AUC, kappa and log loss metrics. Our evaluation demonstrates that five out of the six selected models perform exceptionally, achieving a 100% accuracy rate on the ASD datasets when hyperparameters are meticulously tuned for each model. As almost all classification models are able to get 100% accuracy, we become interested in observing the underlying insights of the datasets by implementing some popular clustering algorithms on these datasets. We calculate Normalized Mutual Information (NMI), Adjusted Rand Index (ARI) & Silhouette Coefficient (SC) metrics to select the best clustering models. Our evaluation finds that spectral clustering outperforms all other benchmarking clustering models in terms of NMI & ARI metrics and it also demonstrates comparability to the optimal SC achieved by k-means.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œè‡ªé€‚åº”è°±ç»¼åˆç—‡ï¼ˆASDï¼‰æ˜¯ä¸€ç§ä¸­æ¢ç¥ç»ç³»ç»¼åˆç—…ï¼Œè¡¨ç°ä¸ºç¤¾äº¤äº¤æµã€communicationå’Œå¤åˆ¶æ´»åŠ¨ç­‰éšœç¢ã€‚è¿™äº›éšœç¢çš„ä¸¥é‡ç¨‹åº¦ä¸åŒï¼Œæ‚£æœ‰è¿™ä¸ªè¯Šæ–­çš„äººé¢ä¸´ç€ç‹¬ç‰¹çš„æŒ‘æˆ˜ã€‚å°½ç®¡å…¶ä¸»è¦èµ·æºæ˜¯é—ä¼ çš„ï¼Œä½†å¯ä»¥é€šè¿‡æ—©æœŸè¯†åˆ«å’Œæ²»ç–—æ¥æé«˜å…¶çŠ¶å†µã€‚åœ¨è¿‡å»å‡ å¹´ä¸­ï¼ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½è¯Šæ–­æŠ€æœ¯åœ¨ä¼ ç»Ÿä¸´åºŠæ–¹æ³•çš„æ”¯æŒä¸‹ emerged as a supplement, aiming to address the potential drawbacks of time-consuming and costly traditional methods.åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„æœºå™¨å­¦ä¹ ç®—æ³•æ¥æ‰¾å‡ºASDæœ€é‡è¦çš„ç‰¹å¾å’Œè‡ªåŠ¨è¯Šæ–­è¿‡ç¨‹ã€‚æˆ‘ä»¬ç ”ç©¶äº†å…­ç§åˆ†ç±»æ¨¡å‹ï¼Œä»¥ç¡®å®šå“ªç§æ¨¡å‹æœ€é€‚åˆè¯†åˆ«ASDï¼Œå¹¶ç ”ç©¶äº†äº”ç§æµè¡Œçš„èšç±»æ–¹æ³•ï¼Œä»¥è·å¾—æœ‰æ„ä¹‰çš„ASDæ•°æ®è§è§£ã€‚ä¸ºäº†é€‰æ‹©æœ€ä½³åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬è¯„ä¼°äº†æ¨¡å‹ä½¿ç”¨ç²¾åº¦ã€å‡†ç¡®ç‡ã€å›å½’ç‡ã€ç‰¹å¾é€‰æ‹©ç‡ã€F1åˆ†æ•°ã€AUCã€Îºå’ŒæŸå¤±å‡½æ•°ç­‰æŒ‡æ ‡ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¡¨æ˜ï¼Œäº”ä¸ªé€‰æ‹©çš„æ¨¡å‹åœ¨hyperparameterä¼˜åŒ–åéƒ½èƒ½å¤Ÿè¾¾åˆ°100%çš„å‡†ç¡®ç‡ã€‚ç”±äºå¤§å¤šæ•°åˆ†ç±»æ¨¡å‹éƒ½èƒ½å¤Ÿè¾¾åˆ°100%çš„å‡†ç¡®ç‡ï¼Œæˆ‘ä»¬å¼€å§‹å…³æ³¨è¿™äº›æ•°æ®é›†çš„ä¸‹é¢éšå«çš„å«ä¹‰ã€‚æˆ‘ä»¬åœ¨è¿™äº›æ•°æ®é›†ä¸Šå®æ–½äº†ä¸€äº›æµè¡Œçš„èšç±»ç®—æ³•ï¼Œå¹¶è®¡ç®—äº†Normalized Mutual Informationï¼ˆNMIï¼‰ã€Adjusted Rand Indexï¼ˆARIï¼‰å’ŒSilhouette Coefficientï¼ˆSCï¼‰ç­‰æŒ‡æ ‡ï¼Œä»¥é€‰æ‹©æœ€ä½³èšç±»æ¨¡å‹ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°ï¼Œspectral clusteringåœ¨NMIå’ŒARIæŒ‡æ ‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”ä¸k-meansçš„æœ€ä½³SCæŒ‡æ ‡ç›¸æ¯”å¯è§‚ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Negative-Signals-with-Self-Attention-for-Sequential-Music-Recommendation"><a href="#Leveraging-Negative-Signals-with-Self-Attention-for-Sequential-Music-Recommendation" class="headerlink" title="Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation"></a>Leveraging Negative Signals with Self-Attention for Sequential Music Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11623">http://arxiv.org/abs/2309.11623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pavan Seshadri, Peter Knees</li>
<li>For: This paper focuses on improving sequential music recommendation by incorporating negative session-level feedback using transformer-based self-attentive architectures and contrastive learning.* Methods: The paper proposes using transformer-based self-attentive models to learn implicit session-level information and incorporating negative feedback through a contrastive learning task.* Results: The paper shows that incorporating negative feedback through contrastive learning results in consistent performance gains over baseline architectures ignoring negative user feedback.<details>
<summary>Abstract</summary>
Music streaming services heavily rely on their recommendation engines to continuously provide content to their consumers. Sequential recommendation consequently has seen considerable attention in current literature, where state of the art approaches focus on self-attentive models leveraging contextual information such as long and short-term user history and item features; however, most of these studies focus on long-form content domains (retail, movie, etc.) rather than short-form, such as music. Additionally, many do not explore incorporating negative session-level feedback during training. In this study, we investigate the use of transformer-based self-attentive architectures to learn implicit session-level information for sequential music recommendation. We additionally propose a contrastive learning task to incorporate negative feedback (e.g skipped tracks) to promote positive hits and penalize negative hits. This task is formulated as a simple loss term that can be incorporated into a variety of deep learning architectures for sequential recommendation. Our experiments show that this results in consistent performance gains over the baseline architectures ignoring negative user feedback.
</details>
<details>
<summary>æ‘˜è¦</summary>
éŸ³ä¹æµå¤„æœåŠ¡é‡è§†æ¨èå¼•æ“ï¼Œä»¥æä¾›ä¸æ–­çš„å†…å®¹ç»™æ¶ˆè´¹è€…ã€‚é¡ºåºæ¨èå¾—åˆ°äº†å½“å‰æ–‡çŒ®ä¸­ä¸€å®šçš„å…³æ³¨ï¼Œç°ä»£approachéƒ½æ˜¯åŸºäºè‡ªæˆ‘æ³¨æ„åŠ›æ¨¡å‹ï¼Œåˆ©ç”¨ç”¨æˆ·å†å²è®°å½•å’Œç‰©å“ç‰¹å¾è¿›è¡Œä¸Šä¸‹æ–‡ual informationã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½æ˜¯é’ˆå¯¹é•¿å½¢å†…å®¹é¢†åŸŸï¼ˆé›¶å”®ã€ç”µå½±ç­‰ï¼‰ï¼Œè€Œä¸æ˜¯çŸ­å½¢å†…å®¹é¢†åŸŸï¼ˆå¦‚éŸ³ä¹ï¼‰ã€‚å¦å¤–ï¼Œè®¸å¤šç ”ç©¶éƒ½ä¸ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«è´Ÿsession-levelåé¦ˆã€‚åœ¨è¿™ä¸ªç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ investigateä½¿ç”¨å˜æ¢å™¨åŸºäºè‡ªæˆ‘æ³¨æ„åŠ›æ¶æ„æ¥å­¦ä¹ éšè—session-levelä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¯¹æ¯”å­¦ä¹ ä»»åŠ¡ï¼Œä»¥åŒ…å«è´Ÿåé¦ˆï¼ˆä¾‹å¦‚è·³è¿‡çš„trackï¼‰ï¼Œä»¥ä¾¿æé«˜æ­£ç¡®çš„hitå’Œè´Ÿåé¦ˆhitã€‚è¿™ä¸ªä»»åŠ¡è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªç®€å•çš„æŸå¤±å‡½æ•°ï¼Œå¯ä»¥ä¸å¤šç§æ·±åº¦å­¦ä¹ æ¶æ„ç»“åˆä½¿ç”¨ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¼šå¯¼è‡´ ignore negative user feedbackçš„åŸºelineæ¶æ„çš„æ€§èƒ½æé«˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Latent-Diffusion-Models-for-Structural-Component-Design"><a href="#Latent-Diffusion-Models-for-Structural-Component-Design" class="headerlink" title="Latent Diffusion Models for Structural Component Design"></a>Latent Diffusion Models for Structural Component Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11601">http://arxiv.org/abs/2309.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Herron, Jaydeep Rade, Anushrut Jignasu, Baskar Ganapathysubramanian, Aditya Balu, Soumik Sarkar, Adarsh Krishnamurthy</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ä¸ªæ„ä»¶è®¾è®¡ç”Ÿæˆæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºç”Ÿæˆç¬¦åˆé—®é¢˜ç‰¹å®šè´Ÿè½½æ¡ä»¶çš„ç»“æ„å…ƒä»¶ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¯¹ç§°æ‰©æ•£æ¨¡å‹ï¼ˆLatent Diffusion modelï¼‰æ¥ç”Ÿæˆæ½œåœ¨çš„å…ƒä»¶è®¾è®¡ï¼Œä»¥æ»¡è¶³é—®é¢˜ç‰¹å®šçš„è´Ÿè½½æ¡ä»¶ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°å¯¹ç°æœ‰è®¾è®¡çš„ç¼–è¾‘ï¼Œå¹¶ä¸”å¯ä»¥å®ç°é«˜å“è´¨çš„ç»“æ„æ€§è¡¨ç°ã€‚æˆ‘ä»¬çš„ç ”ç©¶è·å¾—äº†é‡åŒ–çš„ç»“æœï¼Œè¯æ˜äº†ç”Ÿæˆçš„è®¾è®¡å…·æœ‰å†…åœ¨çš„è¿‘ä¹æœ€ä½³æ€§ã€‚<details>
<summary>Abstract</summary>
Recent advances in generative modeling, namely Diffusion models, have revolutionized generative modeling, enabling high-quality image generation tailored to user needs. This paper proposes a framework for the generative design of structural components. Specifically, we employ a Latent Diffusion model to generate potential designs of a component that can satisfy a set of problem-specific loading conditions. One of the distinct advantages our approach offers over other generative approaches, such as generative adversarial networks (GANs), is that it permits the editing of existing designs. We train our model using a dataset of geometries obtained from structural topology optimization utilizing the SIMP algorithm. Consequently, our framework generates inherently near-optimal designs. Our work presents quantitative results that support the structural performance of the generated designs and the variability in potential candidate designs. Furthermore, we provide evidence of the scalability of our framework by operating over voxel domains with resolutions varying from $32^3$ to $128^3$. Our framework can be used as a starting point for generating novel near-optimal designs similar to topology-optimized designs.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„æ‰©æ•£æ¨¡å‹æŠ€æœ¯è¿›æ­¥ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ï¼Œå¯¹ç”Ÿæˆæ¨¡å‹å¸¦æ¥äº†é©å‘½æ€§å˜é©ï¼Œä½¿å¾—å¯ä»¥ç”Ÿæˆé«˜è´¨é‡é€‚åº”ç”¨æˆ·éœ€æ±‚çš„å›¾åƒã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç”Ÿæˆç»“æ„ç»„ä»¶çš„æ¡†æ¶ã€‚æˆ‘ä»¬ä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹æ¥ç”Ÿæˆå¯æ»¡è¶³ç»™å®šè´Ÿè·æ¡ä»¶çš„ç»„ä»¶çš„æ½œåœ¨è®¾è®¡ã€‚ä¸å…¶ä»–ç”Ÿæˆæ–¹æ³•ï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸ç¼–è¾‘ç°æœ‰è®¾è®¡ã€‚æˆ‘ä»¬ä½¿ç”¨ç»“æ„ topology ä¼˜åŒ–ç®—æ³•æ¥è·å¾—å‡ ä½•æ•°æ®ï¼Œå¹¶åœ¨è¿™äº›æ•°æ®ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ç”Ÿæˆè‡ªç„¶near-optimalè®¾è®¡ã€‚æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†é‡åŒ–ç»“æœï¼Œè¯æ˜ç”Ÿæˆçš„è®¾è®¡å…·æœ‰ç»“æ„æ€§èƒ½çš„å¯é æ€§å’Œå¯å˜æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥åœ¨ voxel é¢†åŸŸä¸­è¿›è¡Œæ‰©å±•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ $32^3$ åˆ° $128^3$ çš„åˆ†è¾¨ç‡èŒƒå›´å†…æ“ä½œã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ä½œä¸ºç”Ÿæˆç±»ä¼¼äº topology-optimized è®¾è®¡çš„å¼€å§‹ç‚¹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multiplying-poles-to-avoid-unwanted-points-in-root-finding-and-optimization"><a href="#Multiplying-poles-to-avoid-unwanted-points-in-root-finding-and-optimization" class="headerlink" title="Multiplying poles to avoid unwanted points in root finding and optimization"></a>Multiplying poles to avoid unwanted points in root finding and optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11475">http://arxiv.org/abs/2309.11475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuyen Trung Truong</li>
<li>for: æœ¬æ–‡targets at solving the problem of avoiding the basin of attraction of a specific point in root finding and optimization.</li>
<li>methods: æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³å°†å‡½æ•°å€¼åˆ†å‰²æˆä¸€ä¸ªé€‚å½“çš„Powerä¹˜ä»¥è·ç¦»é›†Açš„è·ç¦»å‡½æ•°å€¼ï¼Œä»¥é¿å…åœ¨ä¸‹ä¸€æ¬¡ç®—æ³•ä¸­å—åˆ°é›†Açš„å¸å¼•ã€‚</li>
<li>results: æå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œå¯ä»¥å¸®åŠ©é¿å…åœ¨root findingå’Œä¼˜åŒ–ä¸­è¢«å¸å¼•åˆ°ç‰¹å®šç‚¹çš„basin of attractionä¸­ã€‚è¯¥ç®—æ³•é€‚ç”¨äºiterativeç®—æ³•ï¼Œå¹¶ä¸”å¯ä»¥åœ¨å‡½æ•°å€¼ä¸º0æ—¶å’Œå‡½æ•°å€¼éé›¶æ—¶ä¸¤ç§æƒ…å†µä¸‹è¿›è¡Œã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œå¯ä»¥å¸®åŠ©ä»ä¸€ä¸ªæ­£æ–¹å‘çš„åˆ†æ”¯ä¸­é€ƒè„±åˆ°å¦ä¸€ä¸ªåˆ†æ”¯ã€‚<details>
<summary>Abstract</summary>
In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.   In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positive and negative values). The method is very suitable for iterative algorithms which have the descent property. We also propose, based on this, an algorithm to escape the basin of attraction of a component of positive dimension to reach another component.   Along the way, we compare with main existing relevant methods in the current literature. We provide several examples to illustrate the usefulness of the new approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æ ¹å¯»æ‰¾å’Œä¼˜åŒ–ä¸­ï¼Œæœ‰è®¸å¤šæƒ…å†µä¸‹ï¼Œä½¿ç”¨ä¸€ç§å–œæ¬¢çš„æ–¹æ³•constructing sequenceå°†ä¸ä¼š converges to Aï¼ˆè¿™é‡Œï¼Œæˆ‘ä»¬ä¸ assumption Aæ˜¯ convexæˆ–è¿é€šçš„å…¶ä»–æ€§è´¨ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªäººæƒ³è¦æ‰¾åˆ°æ ¹ï¼Œå¹¶ä¸”é€‰æ‹©åˆå§‹ç‚¹åœ¨æ‹¥æœ‰1æ ¹x*çš„åŸºå› å›Šæ‹¥ï¼ˆè¿™å¯èƒ½æ˜¯ä¸€ä¸ªä¸çŸ¥é“çš„å‰æï¼‰ï¼Œé‚£ä¹ˆä¸€å®šä¼š ending up in that rootã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ç§æœºåˆ¶æ¥é¿å…è¿™ä¸ªç‚¹z*åœ¨ä¸‹ä¸€æ¬¡ç®—æ³•ä¸­ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°è¿™ä¸€ç‚¹ï¼šæˆ‘ä»¬å°†costå‡½æ•°é™¤ä»¥ä¸€ä¸ªåˆé€‚çš„powere distanceå‡½æ•°åˆ°Aã€‚è¿™ä¸ªæƒ³æ³•æ˜¯æ ¹æ®åœ¨ä¸€ä¸ªå˜é‡ä¸­æ‰¾æ‰€æœ‰æ ¹çš„æ–¹æ³•å¯å‘çš„ã€‚æˆ‘ä»¬é¦–å…ˆè§£é‡Šäº†åœ¨costå‡½æ•°çš„æœ€å°å€¼ä¸º0æ—¶çš„è¡¥åšï¼Œç„¶åè§£é‡Šå¦‚ä½•å¤„ç†éé›¶æœ€å°å€¼ï¼ˆå…è®¸æ­£è´Ÿå€¼ï¼‰ã€‚è¿™ç§æ–¹æ³•éå¸¸é€‚åˆiterativeç®—æ³•ï¼Œæˆ‘ä»¬ä¹Ÿå»ºè®®ä¸€ç§ä½¿ç”¨è¿™ç§æ–¹æ³•é€ƒè„±åŸºå› å›Šæ‹¥çš„ç»„åˆ†çš„æ–¹æ³•ã€‚åœ¨è¿›è¡Œè¿™ç§æ–¹æ³•çš„æ¯”è¾ƒä¸­ï¼Œæˆ‘ä»¬ä¸ç°æœ‰çš„ä¸»è¦ç›¸å…³æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€äº›ä¾‹å­ï¼Œä»¥ Illustrateæ–°çš„æ–¹æ³•çš„æœ‰ç”¨æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Model-free-tracking-control-of-complex-dynamical-trajectories-with-machine-learning"><a href="#Model-free-tracking-control-of-complex-dynamical-trajectories-with-machine-learning" class="headerlink" title="Model-free tracking control of complex dynamical trajectories with machine learning"></a>Model-free tracking control of complex dynamical trajectories with machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11470">http://arxiv.org/abs/2309.11470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Zheng-Meng/TrackingControl">https://github.com/Zheng-Meng/TrackingControl</a></li>
<li>paper_authors: Zheng-Meng Zhai, Mohammadamin Moradi, Ling-Wei Kong, Bryan Glaz, Mulugeta Haile, Ying-Cheng Lai</li>
<li>for: æ§åˆ¶ä¸¤è‡‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾æŠ¤æ‰‹å™¨ä½¿å…¶è·Ÿè¸ªæ¬²è¦çš„è½¨è¿¹ï¼Œåº”ç”¨äºå¤šç§ Ñ†Ğ¸Ğ²Ğ¸Ğ»ÑŒå’Œå›½é˜²é¢†åŸŸã€‚</li>
<li>methods: ä½¿ç”¨æ¨¡å‹è‡ªç”±ã€æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œåªä½¿ç”¨éƒ¨åˆ†è§‚å¯ŸçŠ¶æ€æ¥æ§åˆ¶ä¸¤è‡‚ Ñ€Ğ¾Ğ±Ğ¾Ñ‚Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹æŠ¤æ‰‹å™¨ã€‚</li>
<li>results: é€šè¿‡ä½¿ç”¨å„ç§ periodicå’Œå¼‚å¸¸ä¿¡å·ï¼Œè¯æ˜äº†æ§åˆ¶æ¡†æ¶çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨æµ‹è¯•é˜¶æ®µï¼ˆéƒ¨ç½²é˜¶æ®µï¼‰ä¸‹ç¡®è®¤äº†å…¶å¯¹æµ‹é‡å™ªå£°ã€å¹²æ‰°å’Œä¸ç¡®å®šæ€§çš„ç¨³å®šæ€§ã€‚<details>
<summary>Abstract</summary>
Nonlinear tracking control enabling a dynamical system to track a desired trajectory is fundamental to robotics, serving a wide range of civil and defense applications. In control engineering, designing tracking control requires complete knowledge of the system model and equations. We develop a model-free, machine-learning framework to control a two-arm robotic manipulator using only partially observed states, where the controller is realized by reservoir computing. Stochastic input is exploited for training, which consists of the observed partial state vector as the first and its immediate future as the second component so that the neural machine regards the latter as the future state of the former. In the testing (deployment) phase, the immediate-future component is replaced by the desired observational vector from the reference trajectory. We demonstrate the effectiveness of the control framework using a variety of periodic and chaotic signals, and establish its robustness against measurement noise, disturbances, and uncertainties.
</details>
<details>
<summary>æ‘˜è¦</summary>
éçº¿æ€§è·Ÿè¸ªæ§åˆ¶ï¼Œä½¿åŠ¨åŠ›ç³»ç»Ÿè·Ÿè¸ªæ‰€éœ€çš„è½¨è¿¹æ˜¯æœºå™¨äººæ§åˆ¶çš„åŸºç¡€ï¼Œå¹¿æ³›åº”ç”¨äºæ–‡æ˜å’Œå›½é˜²é¢†åŸŸã€‚åœ¨æ§åˆ¶å·¥ç¨‹ä¸­ï¼Œè®¾è®¡è·Ÿè¸ªæ§åˆ¶éœ€è¦å®Œæ•´çš„ç³»ç»Ÿæ¨¡å‹å’Œæ–¹ç¨‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ— æ¨¡å‹ã€æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œæ§åˆ¶ä¸¤è‡‚æœºæ¢° manipulate å™¨ä½¿ç”¨åªæœ‰éƒ¨åˆ†è§‚å¯ŸçŠ¶æ€ï¼Œæ§åˆ¶å™¨é€šè¿‡ rezzo è®¡ç®—æœºã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œåˆ©ç”¨ Stochastic è¾“å…¥ï¼Œè®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬è§‚å¯Ÿçš„éƒ¨åˆ†çŠ¶æ€å‘é‡ä½œä¸ºç¬¬ä¸€ä¸ªç»„æˆéƒ¨åˆ†ï¼Œä»¥åŠå…¶æœªæ¥çš„çŠ¶æ€å‘é‡ä½œä¸ºç¬¬äºŒä¸ªç»„æˆéƒ¨åˆ†ï¼Œå› æ­¤ neural machine å°†åè€…è§†ä¸ºå‰è€…çš„æœªæ¥çŠ¶æ€ã€‚åœ¨æµ‹è¯•ï¼ˆéƒ¨ç½²ï¼‰é˜¶æ®µï¼Œæœªæ¥çŠ¶æ€å‘é‡è¢«æ›¿æ¢ä¸ºæ¥è‡ªå‚ç…§è½¨è¿¹çš„æ‰€éœ€è§‚å¯Ÿå‘é‡ã€‚æˆ‘ä»¬ä½¿ç”¨äº†å¤šç§ periodic å’Œæ··æ²Œä¿¡å·è¿›è¡Œæµ‹è¯•ï¼Œå¹¶è¯æ˜äº†æ§åˆ¶æ¡†æ¶çš„å¯é æ€§ï¼Œå¯¹æµ‹é‡å™ªéŸ³ã€å¹²æ‰°å’Œä¸ç¡®å®šæ€§çš„æŠ—æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Digital-twins-of-nonlinear-dynamical-systems-A-perspective"><a href="#Digital-twins-of-nonlinear-dynamical-systems-A-perspective" class="headerlink" title="Digital twins of nonlinear dynamical systems: A perspective"></a>Digital twins of nonlinear dynamical systems: A perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11461">http://arxiv.org/abs/2309.11461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying-Cheng Lai</li>
<li>for: é¢„æµ‹å’Œé¿å…éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿçš„çªç„¶è§„æ¨¡äº‹ä»¶</li>
<li>methods:  sparse optimizationå’Œæœºå™¨å­¦ä¹ ä¸¤ç§æ–¹æ³•</li>
<li>results: å¯ä»¥é¢„æµ‹å’Œé¿å…éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿçš„çªç„¶è§„æ¨¡äº‹ä»¶ï¼Œæä¾›æ—©æœŸè­¦ç¤ºå’Œé¢„æµ‹æ€§è§£å†³æ–¹æ¡ˆ<details>
<summary>Abstract</summary>
Digital twins have attracted a great deal of recent attention from a wide range of fields. A basic requirement for digital twins of nonlinear dynamical systems is the ability to generate the system evolution and predict potentially catastrophic emergent behaviors so as to providing early warnings. The digital twin can then be used for system "health" monitoring in real time and for predictive problem solving. In particular, if the digital twin forecasts a possible system collapse in the future due to parameter drifting as caused by environmental changes or perturbations, an optimal control strategy can be devised and executed as early intervention to prevent the collapse. Two approaches exist for constructing digital twins of nonlinear dynamical systems: sparse optimization and machine learning. The basics of these two approaches are described and their advantages and caveats are discussed.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿçš„æ•°å­—å­ªç”Ÿæœ‰å¾ˆå¤šæœ€è¿‘çš„å…³æ³¨ï¼Œæ¥è‡ªå„ç§é¢†åŸŸã€‚æ•°å­—å­ªç”Ÿçš„åŸºæœ¬è¦æ±‚æ˜¯èƒ½å¤Ÿç”Ÿæˆç³»ç»Ÿæ¼”åŒ–å’Œé¢„æµ‹å¯èƒ½å‡ºç°çš„ç¾éš¾æ€§è¡Œä¸ºï¼Œä»¥æä¾›æ—©æœŸè­¦ç¤ºã€‚æ•°å­—å­ªç”Ÿå¯ä»¥ç”¨äºå®æ—¶ç›‘æµ‹ç³»ç»Ÿâ€œå¥åº·â€çŠ¶æ€ï¼Œå¹¶é¢„æµ‹é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¦‚æœæ•°å­—å­ªç”Ÿé¢„æµ‹ç³»ç»Ÿå°†åœ¨æœªæ¥å› ä¸ºç¯å¢ƒå˜åŒ–æˆ–å¹²æ‰°è€Œå¯¼è‡´å´©æºƒï¼Œå°±å¯ä»¥æ ¹æ®è¿™ä¸ªé¢„æµ‹æ¥è®¾è®¡å’Œæ‰§è¡Œæ—©æœŸå¹²é¢„æªæ–½ï¼Œä»¥é¿å…å´©æºƒã€‚æ„å»ºéçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿçš„æ•°å­—å­ªç”Ÿæœ‰ä¸¤ç§æ–¹æ³•ï¼šæ•£åˆ—ä¼˜åŒ–å’Œæœºå™¨å­¦ä¹ ã€‚è¿™ä¸¤ç§æ–¹æ³•çš„åŸºç¡€å’Œä¼˜ç¼ºç‚¹éƒ½æ˜¯ä»‹ç»çš„ã€‚>>>
</details></li>
</ul>
<hr>
<h2 id="Multi-Step-Model-Predictive-Safety-Filters-Reducing-Chattering-by-Increasing-the-Prediction-Horizon"><a href="#Multi-Step-Model-Predictive-Safety-Filters-Reducing-Chattering-by-Increasing-the-Prediction-Horizon" class="headerlink" title="Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon"></a>Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11453">http://arxiv.org/abs/2309.11453</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/federico-pizarrobejarano/safe-control-gym">https://github.com/federico-pizarrobejarano/safe-control-gym</a></li>
<li>paper_authors: Federico Pizarro Bejarano, Lukas Brunke, Angela P. Schoellig</li>
<li>for: This paper aims to improve the safety guarantees of learning-based controllers by reducing chattering in model predictive safety filters (MPSFs).</li>
<li>methods: The proposed approach considers input corrections over a longer horizon and uses techniques from robust MPC to prove recursive feasibility, reducing chattering by more than a factor of 4 compared to previous MPSF formulations.</li>
<li>results: The proposed approach is verified through extensive simulation and quadrotor experiments, demonstrating the preservation of desired safety guarantees and a significant reduction in chattering compared to previous MPSF formulations.<details>
<summary>Abstract</summary>
Learning-based controllers have demonstrated superior performance compared to classical controllers in various tasks. However, providing safety guarantees is not trivial. Safety, the satisfaction of state and input constraints, can be guaranteed by augmenting the learned control policy with a safety filter. Model predictive safety filters (MPSFs) are a common safety filtering approach based on model predictive control (MPC). MPSFs seek to guarantee safety while minimizing the difference between the proposed and applied inputs in the immediate next time step. This limited foresight can lead to jerky motions and undesired oscillations close to constraint boundaries, known as chattering. In this paper, we reduce chattering by considering input corrections over a longer horizon. Under the assumption of bounded model uncertainties, we prove recursive feasibility using techniques from robust MPC. We verified the proposed approach in both extensive simulation and quadrotor experiments. In experiments with a Crazyflie 2.0 drone, we show that, in addition to preserving the desired safety guarantees, the proposed MPSF reduces chattering by more than a factor of 4 compared to previous MPSF formulations.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Distribution-and-volume-based-scoring-for-Isolation-Forests"><a href="#Distribution-and-volume-based-scoring-for-Isolation-Forests" class="headerlink" title="Distribution and volume based scoring for Isolation Forests"></a>Distribution and volume based scoring for Isolation Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11450">http://arxiv.org/abs/2309.11450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest">https://github.com/porscheofficial/distribution_and_volume_based_isolation_forest</a></li>
<li>paper_authors: Hichem Dhouib, Alissa Wilms, Paul Boes</li>
<li>for: æœ¬ç ”ç©¶æå‡ºäº†ä¸¤ç§æ”¹è¿›æ–¹æ³• Ğ´Ğ»ÑIsland Forestæ–¹æ³•ï¼Œä»¥æé«˜å¼‚å¸¸æ£€æµ‹çš„ç²¾åº¦å’Œæ•ˆæœã€‚</li>
<li>methods: ç¬¬ä¸€ç§æ–¹æ³•æ˜¯åŸºäºä¿¡æ¯ç†è®ºçš„æ€»ä½“åˆ†æ•°å‡½æ•°çš„æ‰©å±•ï¼Œå¯ä»¥è€ƒè™‘æ•´ä¸ªåˆ†å¸ƒè€Œä¸ä»…ä»…æ˜¯æ ‘ensembleå¹³å‡å€¼ã€‚ç¬¬äºŒç§æ–¹æ³•æ˜¯åœ¨éš”ç¦»æ ‘ estimator  nivel replace depth-based åˆ†æ•°å‡½æ•°ã€‚</li>
<li>results: å¯¹äºç”Ÿæˆçš„æ•°æ®å’Œ34ä¸ª&#96;&#96;ADBenchâ€™â€™ benchmark datasetè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°ä½¿ç”¨è¿™ä¸¤ç§æ–¹æ³•å¯ä»¥åœ¨æŸäº›datasetä¸Šæé«˜å¼‚å¸¸æ£€æµ‹çš„ç²¾åº¦ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰datasetä¸Šå¹³å‡ä¸Šæé«˜ä¸€ç§å˜ä½“ã€‚ä»£ç å¯ä»¥åœ¨æäº¤ä¸­æ‰¾åˆ°ã€‚<details>
<summary>Abstract</summary>
We make two contributions to the Isolation Forest method for anomaly and outlier detection. The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree's leaf nodes.   We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive ``ADBench'' benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. The code to reproduce our results is made available as part of the submission.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬åšäº†ä¸¤ä¸ªè´¡çŒ®åˆ°éš”ç¦»æ£®æ—æ–¹æ³•ä¸­ï¼Œç”¨äºå¼‚å¸¸å’Œåå‡ºæ£€æµ‹ã€‚ç¬¬ä¸€ä¸ªè´¡çŒ®æ˜¯åŸºäºä¿¡æ¯ç†è®ºçš„é¢„æµ‹å‡½æ•°çš„ä¸€ç§æ‰©å±•ï¼Œç”¨äºèšåˆéšæœºæ ‘ä¼°è®¡å€¼ã€‚è¿™ä¸ªæ‰©å±•å…è®¸æˆ‘ä»¬è€ƒè™‘ä¸ä»…ensembleå‡å€¼è¿‡æ»¤ï¼Œè€Œæ˜¯æ•´ä¸ªåˆ†å¸ƒã€‚ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯å°†éš”ç¦»æ ‘ä¼°è®¡å€¼ä¸­çš„æ·±åº¦åŸºäºçš„è¯„åˆ†å‡½æ•° replaced with hyper-volume association with isolation tree leaf nodesã€‚æˆ‘ä»¬åœ¨ç”Ÿæˆæ•°æ®ä¸ŠéªŒè¯äº†è¿™ä¸¤ç§æ–¹æ³•ï¼Œå¹¶åœ¨``ADBench''benchmarkä¸­çš„34ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå‘ç°è¿™ä¸¤ç§æ–¹æ³•åœ¨ä¸€äº›æ•°æ®é›†ä¸Šæœ‰æ‰€æ”¹å–„ï¼Œè€Œä¸”åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡æ”¹å–„ã€‚æˆ‘ä»¬çš„ç»“æœå¯ä»¥åœ¨æäº¤ä¸­æ‰¾åˆ°ç›¸å…³çš„ä»£ç ã€‚
</details></li>
</ul>
<hr>
<h2 id="Deep-Networks-as-Denoising-Algorithms-Sample-Efficient-Learning-of-Diffusion-Models-in-High-Dimensional-Graphical-Models"><a href="#Deep-Networks-as-Denoising-Algorithms-Sample-Efficient-Learning-of-Diffusion-Models-in-High-Dimensional-Graphical-Models" class="headerlink" title="Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models"></a>Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11420">http://arxiv.org/abs/2309.11420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Mei, Yuchen Wu</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œåœ¨diffusion-basedç”Ÿæˆæ¨¡å‹ä¸­çš„æŠ˜è¡”æ•ˆç‡ã€‚ç°æœ‰çš„æŠ˜è¡”ç†è®ºå‡è®¾äº†æŠ˜è¡”å‡½æ•°çš„å…‰æ»‘æ€§ï¼Œä½†æ˜¯è¿™äº›ç†è®ºå—åˆ°ç»´åº¦çº¦æŸçš„å›°éš¾ï¼Œç‰¹åˆ«æ˜¯å›¾å½¢æ¨¡å‹å¦‚Markovéšæœºåœºï¼Œè¿™äº›æ¨¡å‹é€šå¸¸ç”¨äºå›¾åƒåˆ†å¸ƒã€‚</li>
<li>methods: æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨å›¾å½¢æ¨¡å‹ä¸­ï¼ŒæŠ˜è¡”å‡½æ•°å¯ä»¥é€šè¿‡å˜åˆ†æ¨ç†ç®—æ³•å¾—åˆ°æœ‰æ•ˆçš„è¿‘ä¼¼ã€‚æ­¤å¤–ï¼Œè¿™äº›ç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°è¡¨ç¤ºä¸ºç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬åœ¨Islingæ¨¡å‹ã€conditional Isingæ¨¡å‹ã€restricted Boltzmannæœºå’Œç®€å•ç¼–ç æ¨¡å‹ä¸­è¿›è¡Œäº†ç¤ºä¾‹ã€‚</li>
<li>results: æˆ‘ä»¬æä¾›äº†ä¸€ç§åŸºäºdiffusion-based samplingçš„æœ‰æ•ˆæ ·æœ¬å¤æ‚åº¦ boundï¼Œå½“æŠ˜è¡”å‡½æ•°æ˜¯é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ å¾—åˆ°çš„æ—¶å€™ã€‚<details>
<summary>Abstract</summary>
We investigate the approximation efficiency of score functions by deep neural networks in diffusion-based generative modeling. While existing approximation theories utilize the smoothness of score functions, they suffer from the curse of dimensionality for intrinsically high-dimensional data. This limitation is pronounced in graphical models such as Markov random fields, common for image distributions, where the approximation efficiency of score functions remains unestablished.   To address this, we observe score functions can often be well-approximated in graphical models through variational inference denoising algorithms. Furthermore, these algorithms are amenable to efficient neural network representation. We demonstrate this in examples of graphical models, including Ising models, conditional Ising models, restricted Boltzmann machines, and sparse encoding models. Combined with off-the-shelf discretization error bounds for diffusion-based sampling, we provide an efficient sample complexity bound for diffusion-based generative modeling when the score function is learned by deep neural networks.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘å›¢é˜Ÿç ”ç©¶ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼åˆ†å¸ƒå¼ç”Ÿæˆæ¨¡å‹ä¸­çš„åˆ†æ•°å‡½æ•°çš„æ•ˆç‡ã€‚ç°æœ‰çš„è¿‘ä¼¼ç†è®ºåˆ©ç”¨åˆ†æ•°å‡½æ•°çš„å¹³æ»‘æ€§ï¼Œä½†æ˜¯å®ƒä»¬ç”±äºæ•°ç»´åº¦çš„å°é—­è€Œå—åˆ°è¯…å’’æ€§çš„å½±å“ï¼Œç‰¹åˆ«æ˜¯å›¾å½¢æ¨¡å‹ï¼Œå¦‚å›¾åƒåˆ†å¸ƒä¸­çš„é©¬å¯å¤«éšæœºåœºï¼Œå…¶ä¸­åˆ†æ•°å‡½æ•°çš„è¿‘ä¼¼æ•ˆç‡æœªèƒ½å¾—åˆ°ç¡®å®šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å‘ç°åˆ†æ•°å‡½æ•°åœ¨å›¾å½¢æ¨¡å‹ä¸­å¯ä»¥é€šè¿‡å˜é‡æ¨ç†æ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œè‰¯å¥½çš„è¿‘ä¼¼ã€‚æ­¤å¤–ï¼Œè¿™äº›ç®—æ³•å¯ä»¥fficientåœ°è¡¨ç¤ºä¸ºç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬åœ¨å›¾åƒåˆ†å¸ƒä¸­çš„ä¼Šè¾›æ¨¡å‹ã€æ¡ä»¶ä¼Šè¾›æ¨¡å‹ã€å—é™çš„åšå°”tzæ›¼æœºå’Œç®€æ´ç¼–ç æ¨¡å‹ä¸­è¿›è¡Œäº†ç¤ºä¾‹ã€‚ä¸å¸‚åœºä¸Šçš„æ‰¹é‡è¯¯å·®è¾¹ç•Œç›¸ç»“åˆï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„æ ·æœ¬å¤æ‚åº¦ä¸‹ç•Œ Ğ´Ğ»Ñ diffusion-basedç”Ÿæˆæ¨¡å‹ï¼Œå½“åˆ†æ•°å‡½æ•°è¢«æ·±åº¦ç¥ç»ç½‘ç»œå­¦ä¹ æ—¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Transformers-versus-LSTMs-for-electronic-trading"><a href="#Transformers-versus-LSTMs-for-electronic-trading" class="headerlink" title="Transformers versus LSTMs for electronic trading"></a>Transformers versus LSTMs for electronic trading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11400">http://arxiv.org/abs/2309.11400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Bilokon, Yitao Qiu</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯ç¡®å®šTransformeræ¨¡å‹æ˜¯å¦å¯ä»¥åœ¨é‡‘èæ—¶é—´åºé¢„æµ‹ä¸­å–ä»£LSTMæ¨¡å‹ï¼Œå¹¶æ¯”è¾ƒäº†ä¸åŒçš„LSTMå’ŒTransformeræ¨¡å‹åœ¨å¤šä¸ªé‡‘èé¢„æµ‹ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†å¤šç§LSTMå’ŒTransformeræ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸€ç§æ–°çš„DLSTMæ¨¡å‹å’Œä¸€ç§é€‚åº”é‡‘èé¢„æµ‹çš„Transformeræ¨¡å‹ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒTransformeræ¨¡å‹åªæœ‰åœ¨ç»å¯¹ä»·æ ¼åºåˆ—é¢„æµ‹æ–¹é¢è¡¨ç°å‡ºæœ‰é™çš„ä¼˜åŠ¿ï¼Œè€ŒLSTMæ¨¡å‹åœ¨å·®ä»·åºåˆ—é¢„æµ‹å’Œä»·æ ¼è¿åŠ¨é¢„æµ‹æ–¹é¢è¡¨ç°æ›´å¥½å’Œæ›´ç¨³å®šã€‚<details>
<summary>Abstract</summary>
With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction.   Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM.   To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture for the Transformer-based model is designed to adapt for financial prediction. The experiment result reflects that the Transformer-based model only has the limited advantage in absolute price sequence prediction. The LSTM-based models show better and more robust performance on difference sequence prediction, such as price difference and price movement.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€äººå·¥æ™ºèƒ½çš„å¿«é€Ÿå‘å±•ï¼Œé•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ï¼Œä¸€ç§å›å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œåœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚ä¸RNNç±»ä¼¼ï¼ŒTransformeræ˜¯ç”¨äºå¤„ç†æ—¶é—´åºåˆ—æ•°æ®çš„è®¾è®¡ã€‚ç”±äºTransformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œç ”ç©¶äººå‘˜å¯¹Transformeråœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è¡¨ç°æ„Ÿåˆ°å…´è¶£ï¼Œå¹¶åœ¨æœ€è¿‘å‡ºç°äº†è®¸å¤šåŸºäºTransformerçš„è§£å†³æ–¹æ¡ˆã€‚ç„¶è€Œï¼Œåœ¨é‡‘èæ—¶é—´åºåˆ—é¢„æµ‹ä¸­ï¼ŒLSTMä»ç„¶æ˜¯ä¸»å¯¼çš„å»ºç­‘ã€‚å› æ­¤ï¼Œæœ¬ç ”ç©¶çš„é—®é¢˜æ˜¯ï¼šå¯å¦ä½¿ç”¨Transformer-basedæ¨¡å‹æ¥é¢„æµ‹é‡‘èæ—¶é—´åºåˆ—ï¼Œå¹¶è¶…è¶ŠLSTMã€‚ä¸ºäº†å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæœ¬ç ”ç©¶å¯¹å¤šç§LSTM-basedå’ŒTransformer-basedæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒï¼Œå¹¶åœ¨é«˜é¢‘é™åˆ¶ORDER BOOKæ•°æ®ä¸Šè¿›è¡Œäº†å¤šä¸ªé‡‘èé¢„æµ‹ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œä¸€ç§æ–°çš„LSTM-basedæ¨¡å‹called DLSTMè¢«å»ºç«‹ï¼Œå¹¶å¯¹Financial predictionè¿›è¡Œäº†æ–°çš„å»ºç­‘ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒTransformer-basedæ¨¡å‹åªæœ‰æœ‰é™çš„ä¼˜åŠ¿åœ¨ç»å¯¹ä»·æ ¼åºåˆ—é¢„æµ‹ä¸­ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLSTM-basedæ¨¡å‹åœ¨å·®ä»·åºåˆ—é¢„æµ‹ä¸­è¡¨ç°æ›´å¥½å’Œæ›´åŠ ç¨³å®šï¼Œä¾‹å¦‚ä»·æ ¼å·®å’Œä»·æ ¼è¿åŠ¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="SR-PredictAO-Session-based-Recommendation-with-High-Capability-Predictor-Add-On"><a href="#SR-PredictAO-Session-based-Recommendation-with-High-Capability-Predictor-Add-On" class="headerlink" title="SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On"></a>SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.12218">http://arxiv.org/abs/2309.12218</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rickyskywalker/sr-predictao-official">https://github.com/rickyskywalker/sr-predictao-official</a></li>
<li>paper_authors: Ruida Wang, Raymond Chi-Wing Wong, Weile Tan</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§èƒ½å¤Ÿåœ¨å•ä¸ªä¼šè¯ä¸­é¢„æµ‹ç”¨æˆ·ä¸‹ä¸€æ­¥è¡Œä¸ºçš„Session-based recommendationæ¨¡å‹ï¼Œä»¥è§£å†³ç°æœ‰æ¨¡å‹ä¸­éšæœºç”¨æˆ·è¡Œä¸ºçš„å½±å“ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ called SR-PredictAOï¼Œå®ƒåŒ…æ‹¬ä¸€ä¸ªé«˜èƒ½åŠ›é¢„æµ‹å™¨æ¨¡å—ï¼Œå¯ä»¥å‡è½»ç”¨æˆ·è¡Œä¸ºçš„éšæœºæ€§å¯¹é¢„æµ‹çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å¯ä»¥åº”ç”¨äºç°æœ‰æ¨¡å‹ä¸Šçš„é«˜èƒ½åŠ›é¢„æµ‹å™¨æ¨¡å—ä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨ä¸¤ä¸ªå®é™…æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†SR-PredictAOåœ¨ä¸‰ç§ç°æœ‰æ¨¡å‹ä¸Šçš„è¡¨ç°æ¯”ç°æœ‰æ¨¡å‹æ›´å¥½ï¼Œå…·ä½“æ¥è¯´ï¼ŒSR-PredictAOåœ¨HR@20å’ŒMRR@20ä¸Šæ¯”ç°æœ‰æ¨¡å‹é«˜å‡º2.9%å’Œ2.3%ã€‚æ­¤å¤–ï¼Œè¿™äº›æ”¹è¿›éƒ½æ˜¯åœ¨å¤§å¤šæ•°ç°æœ‰æ¨¡å‹ä¸Šçš„æ‰€æœ‰æ•°æ®é›†ä¸Šè¿›è¡Œçš„ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºSession-based recommendationé¢†åŸŸçš„ä¸€é¡¹é‡è¦è´¡çŒ®ã€‚<details>
<summary>Abstract</summary>
Session-based recommendation, aiming at making the prediction of the user's next item click based on the information in a single session only even in the presence of some random user's behavior, is a complex problem. This complex problem requires a high-capability model of predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm where all studies focus on how to optimize the encoder module extensively in the paradigm but they ignore how to optimize the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module which could alleviate the effect of random user's behavior for prediction. It is worth mentioning that this framework could be applied to any existing models, which could give opportunities for further optimizing the framework. Extensive experiments on two real benchmark datasets for three state-of-the-art models show that \emph{SR-PredictAO} out-performs the current state-of-the-art model by up to 2.9\% in HR@20 and 2.3\% in MRR@20. More importantly, the improvement is consistent across almost all the existing models on all datasets, which could be regarded as a significant contribution in the field.
</details>
<details>
<summary>æ‘˜è¦</summary>
Session-based æ¨èï¼Œtargeting at predicting the user's next item click based on the information in a single session, is a complex problem. This complex problem requires a high-capability model for predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm, where all studies focus on optimizing the encoder module extensively in the paradigm but ignore the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module that can alleviate the effect of random user behavior for prediction. It is worth mentioning that this framework can be applied to any existing models, which can provide opportunities for further optimizing the framework. Extensive experiments on two real benchmark datasets for three state-of-the-art models show that \emph{SR-PredictAO} outperforms the current state-of-the-art model by up to 2.9\% in HR@20 and 2.3\% in MRR@20. More importantly, the improvement is consistent across almost all existing models on all datasets, which can be regarded as a significant contribution in the field.
</details></li>
</ul>
<hr>
<h2 id="Learning-Patient-Static-Information-from-Time-series-EHR-and-an-Approach-for-Safeguarding-Privacy-and-Fairness"><a href="#Learning-Patient-Static-Information-from-Time-series-EHR-and-an-Approach-for-Safeguarding-Privacy-and-Fairness" class="headerlink" title="Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness"></a>Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11373">http://arxiv.org/abs/2309.11373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Liao, Joel Voldman</li>
<li>for: è¿™ç§ç ”ç©¶æ—¨åœ¨ investigate the ability of time-series electronic health record data to predict patient static information, and to develop a general approach to protect patient-sensitive attribute information for downstream tasks.</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº†æ—¶åºæ•°æ®å’Œæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨äº†å¤šç§æ–¹æ³•å’Œæ•°æ®åº“æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œ raw time-series data å’Œæœºå™¨å­¦ä¹ æ¨¡å‹å­¦ä¹ çš„è¡¨ç¤ºå¯ä»¥é«˜åº¦é¢„æµ‹patientçš„é™æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”Ÿç‰©æ€§åˆ«ã€å¹´é¾„å’Œè‡ªreported raceã€‚æ­¤å¤–ï¼Œè¿™äº›é¢„æµ‹æ€§èƒ½å¯ä»¥æ‰©å±•åˆ°å„ç§ç›¸å…³ç–¾ç—…å› ç´ ï¼Œå¹¶ä¸”å­˜åœ¨even when the model was trained for different tasks, using different cohorts, using different model architectures and databases.<details>
<summary>Abstract</summary>
Recent work in machine learning for healthcare has raised concerns about patient privacy and algorithmic fairness. For example, previous work has shown that patient self-reported race can be predicted from medical data that does not explicitly contain racial information. However, the extent of data identification is unknown, and we lack ways to develop models whose outcomes are minimally affected by such information. Here we systematically investigated the ability of time-series electronic health record data to predict patient static information. We found that not only the raw time-series data, but also learned representations from machine learning models, can be trained to predict a variety of static information with area under the receiver operating characteristic curve as high as 0.851 for biological sex, 0.869 for binarized age and 0.810 for self-reported race. Such high predictive performance can be extended to a wide range of comorbidity factors and exists even when the model was trained for different tasks, using different cohorts, using different model architectures and databases. Given the privacy and fairness concerns these findings pose, we develop a variational autoencoder-based approach that learns a structured latent space to disentangle patient-sensitive attributes from time-series data. Our work thoroughly investigates the ability of machine learning models to encode patient static information from time-series electronic health records and introduces a general approach to protect patient-sensitive attribute information for downstream tasks.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Using-Property-Elicitation-to-Understand-the-Impacts-of-Fairness-Constraints"><a href="#Using-Property-Elicitation-to-Understand-the-Impacts-of-Fairness-Constraints" class="headerlink" title="Using Property Elicitation to Understand the Impacts of Fairness Constraints"></a>Using Property Elicitation to Understand the Impacts of Fairness Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11343">http://arxiv.org/abs/2309.11343</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jessie Finocchiaro</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ç†è§£è®¸å¤šé¢„æµ‹ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ·»åŠ æ­£åˆ™åŒ–å‡½æ•°ä¼šå¦‚ä½•æ”¹å˜ä¼˜åŒ–ç›®æ ‡çš„æœ€å°å€¼ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨å±æ€§æè¿°æ¥æ¢è®¨è¯¸å¦‚äº§å“åˆ†å¸ƒå˜åŒ–å’Œçº¦æŸæ¾å¼›ç­‰å› ç´ å¯¹ä¼˜åŒ–ç›®æ ‡çš„å½±å“ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œæ·»åŠ æ­£åˆ™åŒ–å‡½æ•°å¯èƒ½ä¼šæ”¹å˜ä¼˜åŒ–ç›®æ ‡çš„æœ€å°å€¼ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡å±æ€§æè¿°æ¥æè¿°è¿™ç§æ”¹å˜ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘ç°åœ¨ä¸åŒçš„æ•°æ®åˆ†å¸ƒå’Œçº¦æŸæ¡ä»¶ä¸‹ï¼Œç®—æ³•å†³ç­–çš„å˜åŒ–ã€‚<details>
<summary>Abstract</summary>
Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraints.
</details>
<details>
<summary>æ‘˜è¦</summary>
é¢„æµ‹ç®—æ³•ç»å¸¸é€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°æ¥è®­ç»ƒï¼Œå¹¶å°†æ­£åˆ™å‡½æ•°æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ä»¥å®ç°ä¸€äº›çº¦æŸã€‚é¢„æœŸåœ°ï¼Œæ·»åŠ æ­£åˆ™å‡½æ•°ä¼šæ”¹å˜æŸå¤±å‡½æ•°çš„æœ€å°å€¼ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸å¾ˆäº†è§£å“ªäº›æ­£åˆ™å‡½æ•°ä¼šæ”¹å˜æŸå¤±å‡½æ•°çš„æœ€å°å€¼ï¼Œä»¥åŠè¿™äº›æ”¹å˜æ˜¯å¦‚ä½•å‘ç”Ÿçš„ã€‚æˆ‘ä»¬ä½¿ç”¨è´¢äº§æè¿°æ¥å¼€å§‹ç†è§£æŸå¤±å’Œæ­£åˆ™å‡½æ•°å¯¹äºç»™å®šé—®é¢˜å®ä¾‹çš„ä¼˜åŒ–å†³ç­–çš„å…³ç³»ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç»™å‡ºäº†æŸå¤±å’Œæ­£åˆ™å‡½æ•°å¯¹çš„å¿…è¦å’Œ suficient conditionï¼Œå¹¶è€ƒå¯Ÿäº†å¸¸è§çš„å…¬å¹³æœºå™¨å­¦ä¹  Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğµä¸­çš„ä¸€äº›æ»¡è¶³è¿™ä¸ªconditionçš„æ­£åˆ™å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¡¨æ˜ï¼Œåœ¨æ•°æ®åˆ†å¸ƒå˜åŒ–å’Œçº¦æŸç¡¬åº¦å˜åŒ–çš„æƒ…å†µä¸‹ï¼Œç®—æ³•å†³ç­–ä¼šå‘ç”Ÿå˜åŒ–ã€‚
</details></li>
</ul>
<hr>
<h2 id="WFTNet-Exploiting-Global-and-Local-Periodicity-in-Long-term-Time-Series-Forecasting"><a href="#WFTNet-Exploiting-Global-and-Local-Periodicity-in-Long-term-Time-Series-Forecasting" class="headerlink" title="WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting"></a>WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11319">http://arxiv.org/abs/2309.11319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyuan Liu, Beiliang Wu, Naiqi Li, Tao Dai, Fengmao Lei, Jigang Bao, Yong Jiang, Shu-Tao Xia</li>
<li>for: é¢„æµ‹é•¿æœŸæ—¶é—´åºåˆ—ï¼Œä½¿ç”¨æ³¢å³°å˜æ¢ç½‘ç»œï¼ˆWFTNetï¼‰æ•æ‰å…¨é¢çš„æ—¶é—´é¢‘ç‡ä¿¡æ¯ã€‚</li>
<li>methods: ä½¿ç”¨æ³¢å³°å˜æ¢å’Œ Fourier å˜æ¢ä¸¤è€…ï¼Œæ•æ‰å…¨é¢çš„æ—¶é—´é¢‘ç‡ä¿¡æ¯ï¼Œå¹¶å¼•å…¥å‘¨æœŸæ€§æƒé‡å› å­ï¼ˆPWCï¼‰è‡ªé€‚åº”åœ°å¹³è¡¡å…¨é¢å’Œæœ¬åœ°é¢‘ç‡æ¨¡å¼çš„é‡è¦æ€§ã€‚</li>
<li>results: å¯¹å¤šç§æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶ consistently è¶…è¿‡äº†å…¶ä»–åŸºå‡†å€¼ã€‚<details>
<summary>Abstract</summary>
Recent CNN and Transformer-based models tried to utilize frequency and periodicity information for long-term time series forecasting. However, most existing work is based on Fourier transform, which cannot capture fine-grained and local frequency structure. In this paper, we propose a Wavelet-Fourier Transform Network (WFTNet) for long-term time series forecasting. WFTNet utilizes both Fourier and wavelet transforms to extract comprehensive temporal-frequency information from the signal, where Fourier transform captures the global periodic patterns and wavelet transform captures the local ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to adaptively balance the importance of global and local frequency patterns. Extensive experiments on various time series datasets show that WFTNet consistently outperforms other state-of-the-art baseline.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸçš„CNNå’ŒTransformeræ¨¡å‹å°è¯•åˆ©ç”¨é¢‘ç‡å’Œå‘¨æœŸä¿¡æ¯è¿›è¡Œé•¿æœŸæ—¶é—´åºé¢„æµ‹ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰å·¥ä½œåŸºäºå‚…é‡Œå¶å˜æ¢ï¼Œè¿™æ— æ³•æ•æ‰ç»†è‡´çš„é¢‘ç‡ç»“æ„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¹‚ transformed-waveletç½‘ç»œï¼ˆWFTNetï¼‰ï¼Œç”¨äºé•¿æœŸæ—¶é—´åºé¢„æµ‹ã€‚WFTNetåˆ©ç”¨äº†å‚…é‡Œå¶å’Œwaveletå˜æ¢æ¥æå–æ—¶é—´åºåˆ—ä¸­çš„å…¨é¢æ—¶é—´é¢‘ç‡ä¿¡æ¯ï¼Œå…¶ä¸­å‚…é‡Œå¶å˜æ¢æ•æ‰åˆ°å…¨çƒæ€§å¾å‘¨æœŸæ¨¡å¼ï¼Œwaveletå˜æ¢æ•æ‰åˆ°æœ¬åœ°æ€§å¾å‘¨æœŸæ¨¡å¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ Periodicity-Weighted Coefficientï¼ˆPWCï¼‰ï¼Œä»¥é€‚åº”åœ° adaptively è¡¡é‡å…¨çƒå’Œæœ¬åœ°é¢‘ç‡æ¨¡å¼çš„é‡è¦æ€§ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„æ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†WFTNetåœ¨å…¶ä»–åŸºelineä¸Š consistently å‡çº§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Create-and-Find-Flatness-Building-Flat-Training-Spaces-in-Advance-for-Continual-Learning"><a href="#Create-and-Find-Flatness-Building-Flat-Training-Spaces-in-Advance-for-Continual-Learning" class="headerlink" title="Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning"></a>Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11305">http://arxiv.org/abs/2309.11305</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Eric8932/Create-and-Find-Flatness">https://github.com/Eric8932/Create-and-Find-Flatness</a></li>
<li>paper_authors: Wenhang Shi, Yiren Chen, Zhe Zhao, Wei Lu, Kimmo Yan, Xiaoyong Du</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ continual learning ä¸­çš„æ¶æ€§å¿˜è®°é—®é¢˜ï¼Œæé«˜ neural network åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ä¿æŒä¹‹å‰ä»»åŠ¡çŸ¥è¯†çš„èƒ½åŠ›ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§ novel çš„ Create and Find Flatnessï¼ˆC&amp;Fï¼‰æ¡†æ¶ï¼Œåœ¨æ¯ä¸ªä»»åŠ¡å­¦ä¹ é˜¶æ®µå»ºç«‹ä¸€ä¸ªé€‚åº”å½“ä»»åŠ¡çš„å¹³å¦è®­ç»ƒç©ºé—´ã€‚åœ¨å­¦ä¹ å½“å‰ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬é€‚åº”åˆ›å»ºä¸€ä¸ªæŸå¤±å‡½æ•°çš„å¹³å¦åŒºåŸŸï¼Œç„¶åæ ¹æ®å‚æ•°å¯¹å½“å‰ä»»åŠ¡çš„é‡è¦æ€§è¿›è¡Œè¯„ä¼°ã€‚åœ¨é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬ä¼šåº”ç”¨çº¦æŸä»¥æ ¹æ®å¹³å¦åº¦ï¼ŒåŒæ—¶ä¸ºæ–°ä»»åŠ¡å‡†å¤‡å¹³å¦çš„è®­ç»ƒç©ºé—´ã€‚</li>
<li>results: æˆ‘ä»¬çš„ C&amp;F æ¡†æ¶åœ¨ standalone continual learning ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–æ–¹æ³•ç»„åˆä½¿ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒC&amp;F å¯ä»¥ä¿æŒä¹‹å‰ä»»åŠ¡çŸ¥è¯†ï¼ŒåŒæ—¶å­¦ä¹ æ–°ä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„ dataset ä¸Šå…·æœ‰ç¨³å®šçš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Catastrophic forgetting remains a critical challenge in the field of continual learning, where neural networks struggle to retain prior knowledge while assimilating new information. Most existing studies emphasize mitigating this issue only when encountering new tasks, overlooking the significance of the pre-task phase. Therefore, we shift the attention to the current task learning stage, presenting a novel framework, C&F (Create and Find Flatness), which builds a flat training space for each task in advance. Specifically, during the learning of the current task, our framework adaptively creates a flat region around the minimum in the loss landscape. Subsequently, it finds the parameters' importance to the current task based on their flatness degrees. When adapting the model to a new task, constraints are applied according to the flatness and a flat space is simultaneously prepared for the impending task. We theoretically demonstrate the consistency between the created and found flatness. In this manner, our framework not only accommodates ample parameter space for learning new tasks but also preserves the preceding knowledge of earlier tasks. Experimental results exhibit C&F's state-of-the-art performance as a standalone continual learning approach and its efficacy as a framework incorporating other methods. Our work is available at https://github.com/Eric8932/Create-and-Find-Flatness.
</details>
<details>
<summary>æ‘˜è¦</summary>
catastrophic forgetting æ˜¯ä¸€ä¸ªé‡è¦æŒ‘æˆ˜åœ¨æŒç»­å­¦ä¹ é¢†åŸŸï¼Œ neural network åœ¨æ¥å—æ–°ä»»åŠ¡æ—¶å¿˜è®°ä¹‹å‰çš„çŸ¥è¯†æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚  existing studies é€šå¸¸åªå…³æ³¨åœ¨æ–°ä»»åŠ¡ä¸Š mitigating è¿™ä¸ªé—®é¢˜ï¼Œå¿½è§†äº† pre-task é˜¶æ®µçš„é‡è¦æ€§ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›é›†ä¸­åœ¨å½“å‰ä»»åŠ¡å­¦ä¹ é˜¶æ®µï¼Œæå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œ C&Fï¼ˆCreate and Find Flatnessï¼‰ï¼Œå®ƒåœ¨æ¯ä¸ªä»»åŠ¡ä¹‹å‰å»ºç«‹äº†ä¸€ä¸ªå¹³å¦çš„è®­ç»ƒç©ºé—´ã€‚ specifically, åœ¨å­¦ä¹ å½“å‰ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¼šåŠ¨æ€åˆ›å»ºä¸€ä¸ªç¼ºå¤±çš„æœ€å°å€¼é™„è¿‘çš„å¹³å¦åŒºåŸŸã€‚ ç„¶åï¼Œå®ƒä¼šæ ¹æ®å‚æ•°çš„å¹³å¦åº¦æ¥ç¡®å®šå‚æ•°çš„å½“å‰ä»»åŠ¡é‡è¦æ€§ã€‚ å½“é€‚åº”æ–°ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬ä¼šæ ¹æ®å¹³å¦åº¦åº”ç”¨çº¦æŸï¼Œå¹¶åŒæ—¶ä¸ºä¸‹ä¸€ä¸ªä»»åŠ¡å‡†å¤‡ä¸€ä¸ªå¹³å¦çš„ç©ºé—´ã€‚ æˆ‘ä»¬ç†è®ºä¸ŠéªŒè¯äº†åˆ›å»ºå’Œå‘ç°å¹³å¦çš„ä¸€è‡´æ€§ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¡†æ¶ä¸ä»…ä¸ºå­¦ä¹ æ–°ä»»åŠ¡æä¾›äº†å……è¶³çš„å‚æ•°ç©ºé—´ï¼Œè€Œä¸”ä¹Ÿä¿ç•™äº†å‰ä¸€ä¸ªä»»åŠ¡ä¸­çš„çŸ¥è¯†ã€‚ å®éªŒç»“æœè¡¨æ˜ C&F èƒ½å¤Ÿç‹¬ç«‹åœ°å®ç°çŠ¶æ€æœºå™¨å­¦ä¹ çš„è¡¨ç°ï¼ŒåŒæ—¶ä½œä¸ºå…¶ä»–æ–¹æ³•çš„æ¡†æ¶ä¹Ÿæœ‰å‡ºè‰²çš„æ•ˆæœã€‚ æˆ‘ä»¬çš„å·¥ä½œå¯ä»¥åœ¨ <https://github.com/Eric8932/Create-and-Find-Flatness> ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Beyond-Accuracy-Measuring-Representation-Capacity-of-Embeddings-to-Preserve-Structural-and-Contextual-Information"><a href="#Beyond-Accuracy-Measuring-Representation-Capacity-of-Embeddings-to-Preserve-Structural-and-Contextual-Information" class="headerlink" title="Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information"></a>Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11294">http://arxiv.org/abs/2309.11294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarwan Ali</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ–¹æ³•æ¥è¯„ä¼°åµŒå…¥çš„è´¨é‡å’Œå®¹é‡ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£åµŒå…¥åœ¨ä¸åŒåº”ç”¨ä¸­çš„æ•ˆæœã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†å¤–éƒ¨è¯„ä¼°æ–¹æ³•ï¼ˆå¦‚åˆ†ç±»å’Œèšç±»ï¼‰å’Œt-SNEåŸºäºçš„é‚»å±…åˆ†æï¼ˆå¦‚é‚»å±…ä¸€è‡´æ€§å’Œä¿¡ä»»åº¦ï¼‰æ¥å…¨é¢è¯„ä¼°åµŒå…¥çš„è´¨é‡å’Œå®¹é‡ã€‚åŒæ—¶ï¼Œä½¿ç”¨bayesianä¼˜åŒ–æŠ€æœ¯æ¥ä¼˜åŒ–è¯„ä¼° metricçš„æƒé‡ï¼Œä»¥ç¡®ä¿ä¸€ä¸ªæ•°æ®é©±åŠ¨çš„ã€ Ğ¾Ğ±ÑŠĞµĞº oriented çš„æ–¹æ³•ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡ä½¿ç”¨ä¸‰ä¸ªç”Ÿç‰©åºåˆ—æ•°æ®é›†ï¼ˆè›‹ç™½è´¨å’Œæ ¸é…¸ï¼‰å’Œå››ç§åµŒå…¥æ–¹æ³•ï¼ˆSpike2Vecã€Spaced k-mersã€PWM2Vec å’Œ AutoEncoderï¼‰è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ç»“æœè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥å¸®åŠ©ç ”ç©¶è€…å’Œå®è·µè€…æ›´å¥½åœ°ç†è§£åµŒå…¥åœ¨ä¸åŒåº”ç”¨ä¸­çš„æ•ˆæœï¼Œå¹¶æä¾›ä¸€ä¸ªé‡åŒ–çš„è¯„ä¼°æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Effective representation of data is crucial in various machine learning tasks, as it captures the underlying structure and context of the data. Embeddings have emerged as a powerful technique for data representation, but evaluating their quality and capacity to preserve structural and contextual information remains a challenge. In this paper, we address this need by proposing a method to measure the \textit{representation capacity} of embeddings. The motivation behind this work stems from the importance of understanding the strengths and limitations of embeddings, enabling researchers and practitioners to make informed decisions in selecting appropriate embedding models for their specific applications. By combining extrinsic evaluation methods, such as classification and clustering, with t-SNE-based neighborhood analysis, such as neighborhood agreement and trustworthiness, we provide a comprehensive assessment of the representation capacity. Additionally, the use of optimization techniques (bayesian optimization) for weight optimization (for classification, clustering, neighborhood agreement, and trustworthiness) ensures an objective and data-driven approach in selecting the optimal combination of metrics. The proposed method not only contributes to advancing the field of embedding evaluation but also empowers researchers and practitioners with a quantitative measure to assess the effectiveness of embeddings in capturing structural and contextual information. For the evaluation, we use $3$ real-world biological sequence (proteins and nucleotide) datasets and performed representation capacity analysis of $4$ embedding methods from the literature, namely Spike2Vec, Spaced $k$-mers, PWM2Vec, and AutoEncoder.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ•ˆæœè¡¨ç¤ºæ•°æ®çš„è¡¨ç¤ºæ˜¯æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­çš„å…³é”®ï¼Œå®ƒæ•æ‰äº†æ•°æ®çš„ä¸‹é¢ç»“æ„å’Œä¸Šä¸‹æ–‡ã€‚åµŒå…¥åœ¨æœºå™¨å­¦ä¹ ä¸­å‡ºç°ä¸ºä¸€ç§å¼ºå¤§çš„è¡¨ç¤ºæŠ€å·§ï¼Œä½†è¯„ä¼°å…¶è´¨é‡å’Œä¿æŒç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºä¸€ç§æ–¹æ³•æ¥è¡¡é‡åµŒå…¥çš„è¡¨ç¤ºèƒ½åŠ›ã€‚è¿™ç§æ–¹æ³•çš„åŠ¨æœºæ¥è‡ªäºäº†ç†è§£åµŒå…¥çš„ä¼˜åŠ£ç‚¹ï¼Œä»¥ä¾¿ç ”ç©¶è€…å’Œå®è·µè€…å¯ä»¥æ ¹æ®ç‰¹å®šåº”ç”¨é€‰æ‹©åˆé€‚çš„åµŒå…¥æ¨¡å‹ã€‚é€šè¿‡ç»“åˆå¤–éƒ¨è¯„ä¼°æ–¹æ³•ï¼ˆå¦‚åˆ†ç±»å’Œèšç±»ï¼‰å’Œt-SNEåŸºäºçš„é‚»å±…åˆ†æï¼ˆå¦‚é‚»å±…ä¸€è‡´å’Œä¿¡ä»»åº¦ï¼‰ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§å…¨é¢çš„è¯„ä¼°æ–¹æ³•ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æœç´¢ç®—æ³•ï¼ˆbayesianä¼˜åŒ–ï¼‰æ¥ä¼˜åŒ–å‚æ•°ï¼ˆå¦‚åˆ†ç±»ã€èšç±»ã€é‚»å±…ä¸€è‡´å’Œä¿¡ä»»åº¦ï¼‰ï¼Œç¡®ä¿äº†ä¸€ç§å®¢è§‚å’Œæ•°æ®é©±åŠ¨çš„æ–¹æ³•æ¥é€‰æ‹©æœ€ä½³çš„ç»¼åˆæŒ‡æ ‡ã€‚è¯¥æ–¹æ³•ä¸ä»…ä¸ºåµŒå…¥è¯„ä¼°é¢†åŸŸåšå‡ºäº†è´¡çŒ®ï¼Œè¿˜ä¸ºç ”ç©¶è€…å’Œå®è·µè€…æä¾›äº†ä¸€ä¸ªé‡åŒ–çš„è¯„ä¼°æ–¹æ³•ï¼Œä»¥è¯„ä¼°åµŒå…¥æ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•æ‰ç»“æ„å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºè¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†3ä¸ªå®é™…ç”Ÿç‰©åºåˆ—ï¼ˆè›‹ç™½è´¨å’Œæ ¸è‹·é…¸ï¼‰æ•°æ®é›†ï¼Œå¹¶å¯¹Literatureä¸­çš„4ç§åµŒå…¥æ–¹æ³•è¿›è¡Œè¡¨ç¤ºèƒ½åŠ›åˆ†æï¼Œå³Spike2Vecã€Spaced k-mersã€PWM2Vecå’ŒAutoEncoderã€‚
</details></li>
</ul>
<hr>
<h2 id="Grassroots-Operator-Search-for-Model-Edge-Adaptation"><a href="#Grassroots-Operator-Search-for-Model-Edge-Adaptation" class="headerlink" title="Grassroots Operator Search for Model Edge Adaptation"></a>Grassroots Operator Search for Model Edge Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11246">http://arxiv.org/abs/2309.11246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, Smail Niar</li>
<li>for: è¿™ç§è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ•°å­¦åŸºç¡€çš„ neural architecture searchï¼ˆNASï¼‰æ–¹æ³•ï¼Œç”¨äºé€‚åº”è¾¹ç¼˜è®¾å¤‡ä¸Šçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨ Grassroots Operator Searchï¼ˆGOSï¼‰æ–¹æ³•ï¼Œé€šè¿‡æœç´¢å’Œé€‰æ‹©é«˜æ•ˆçš„æ“ä½œç¬¦æ¥ä»£æ›¿åŸå§‹æ¨¡å‹ä¸­çš„æ“ä½œç¬¦ï¼Œä»¥æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡while maintaining high accuracyã€‚</li>
<li>results: åœ¨å¤šç§æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨Redmi Note 7Så’ŒRaspberry Pi3ç­‰è¾¹ç¼˜è®¾å¤‡ä¸Šå®ç°è‡³å°‘2.2å€çš„è®¡ç®—é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒé«˜åº¦çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œåœ¨è„‰å†²é¢‘åº¦ä¼°è®¡åº”ç”¨ä¸­ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¾¾åˆ°çŠ¶æ€ KÃ¼nstlerçš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒè®¡ç®—å¤æ‚åº¦çš„å‡å°‘ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
Hardware-aware Neural Architecture Search (HW-NAS) is increasingly being used to design efficient deep learning architectures. An efficient and flexible search space is crucial to the success of HW-NAS. Current approaches focus on designing a macro-architecture and searching for the architecture's hyperparameters based on a set of possible values. This approach is biased by the expertise of deep learning (DL) engineers and standard modeling approaches. In this paper, we present a Grassroots Operator Search (GOS) methodology. Our HW-NAS adapts a given model for edge devices by searching for efficient operator replacement. We express each operator as a set of mathematical instructions that capture its behavior. The mathematical instructions are then used as the basis for searching and selecting efficient replacement operators that maintain the accuracy of the original model while reducing computational complexity. Our approach is grassroots since it relies on the mathematical foundations to construct new and efficient operators for DL architectures. We demonstrate on various DL models, that our method consistently outperforms the original models on two edge devices, namely Redmi Note 7S and Raspberry Pi3, with a minimum of 2.2x speedup while maintaining high accuracy. Additionally, we showcase a use case of our GOS approach in pulse rate estimation on wristband devices, where we achieve state-of-the-art performance, while maintaining reduced computational complexity, demonstrating the effectiveness of our approach in practical applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ¤ç‰©æ¶æ„çš„æœå¯»æ–¹æ³•ï¼Œç§°ä¸ºGrassroots Operator Searchï¼ˆGOSï¼‰ã€‚æˆ‘ä»¬çš„HW-NASæ–¹æ³•è¿ç”¨äº†ä¸€ä¸ªåŸºäºæ¤ç‰©æ¶æ„çš„æœå¯»ç©ºé—´ï¼Œå¯»æ‰¾é«˜æ•ˆçš„æ“ä½œå™¨æ›¿ä»£ã€‚æˆ‘ä»¬å°†æ¯ä¸ªæ“ä½œå™¨è¡¨ç¤ºä¸ºä¸€äº›æ•°å­¦æŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤ captureäº†æ“ä½œå™¨çš„è¡Œä¸ºã€‚è¿™äº›æ•°å­¦æŒ‡ä»¤åæ¥ç”¨ä½œæœå¯»å’Œé€‰æ‹©é«˜æ•ˆçš„æ“ä½œå™¨æ›¿ä»£ï¼Œä»¥ç»´æŒåŸå§‹æ¨¡å‹çš„ç²¾åº¦ï¼Œå¹¶é™ä½è®¡ç®—å¤æ‚æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯ä¸€ç§åŸºäºæ¤ç‰©çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒå°†åŸºäºæ¤ç‰©æ¶æ„çš„æ•°å­¦åŸºç¡€å»ºæ„æ–°çš„é«˜æ•ˆæ“ä½œå™¨ã€‚æˆ‘ä»¬åœ¨ä¸åŒçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Redmi Note 7Så’ŒRaspberry Pi3ç­‰ä¸¤ä¸ªè¾¹ç¼˜è®¾å¤‡ä¸Šæ˜¾ç¤ºäº†è‡³å°‘2.2å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ç»´æŒé«˜ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„GOSæ–¹æ³•åœ¨è„‰æç›‘æµ‹å™¨ä¸Šçš„å®é™…åº”ç”¨ï¼Œåœ¨è¿™ä¸ªåº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å–å¾—äº†ç°æœ‰æœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶ç»´æŒäº†é™ä½çš„è®¡ç®—å¤æ‚æ€§ï¼Œå®è¯äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Prediction-of-Machine-Learning-Training-Time-to-Support-Continuous-Learning-Systems-Development"><a href="#Towards-a-Prediction-of-Machine-Learning-Training-Time-to-Support-Continuous-Learning-Systems-Development" class="headerlink" title="Towards a Prediction of Machine Learning Training Time to Support Continuous Learning Systems Development"></a>Towards a Prediction of Machine Learning Training Time to Support Continuous Learning Systems Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11226">http://arxiv.org/abs/2309.11226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesca Marzi, Giordano dâ€™Aloisio, Antinisca Di Marco, Giovanni Stilo</li>
<li>for: é¢„æµ‹æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ—¶é—´çš„é—®é¢˜åœ¨ç§‘å­¦ç¤¾åŒºä¸­å˜å¾—éå¸¸é‡è¦ã€‚å¯ä»¥é¢„æµ‹MLæ¨¡å‹è®­ç»ƒæ—¶é—´ï¼Œå¯ä»¥è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œä»¥æé«˜èƒ½æ•ˆæ€§å’Œæ€§èƒ½ã€‚æœ¬æ–‡æè¿°æˆ‘ä»¬åœ¨è¿™ä¸ªæ–¹å‘ä¸Šçš„å·¥ä½œã€‚</li>
<li>methods: æˆ‘ä»¬å¯¹ Zheng et al.æå‡ºçš„Full Parameter Time Complexity (FPTC)æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„å®è¯ç ”ç©¶ã€‚è¿™æ˜¯æˆ‘ä»¬çŸ¥é“çš„å”¯ä¸€ä¸€ç§å½¢å¼åŒ–MLæ¨¡å‹è®­ç»ƒæ—¶é—´ä¸æ•°æ®é›†å’Œæ¨¡å‹å‚æ•°ä¹‹é—´çš„å…³ç³»ã€‚æˆ‘ä»¬ç ”ç©¶äº†é€»è¾‘å›å½’å’Œéšæœºæ£®æ—åˆ†ç±»å™¨çš„å½¢ulationï¼Œå¹¶æŒ‡å‡ºäº†ä¸»è¦çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°ï¼Œä»å®éªŒç»“æœæ¥çœ‹ï¼Œè®­ç»ƒæ—¶é—´é¢„æµ‹ä¸æ•°æ®é›†ä¸Šä¸‹æ–‡æœ‰ç€ç´§å¯†çš„å…³ç³»ã€‚FPTCæ–¹æ³•ä¸èƒ½æ³›åŒ–ã€‚<details>
<summary>Abstract</summary>
The problem of predicting the training time of machine learning (ML) models has become extremely relevant in the scientific community. Being able to predict a priori the training time of an ML model would enable the automatic selection of the best model both in terms of energy efficiency and in terms of performance in the context of, for instance, MLOps architectures. In this paper, we present the work we are conducting towards this direction. In particular, we present an extensive empirical study of the Full Parameter Time Complexity (FPTC) approach by Zheng et al., which is, to the best of our knowledge, the only approach formalizing the training time of ML models as a function of both dataset's and model's parameters. We study the formulations proposed for the Logistic Regression and Random Forest classifiers, and we highlight the main strengths and weaknesses of the approach. Finally, we observe how, from the conducted study, the prediction of training time is strictly related to the context (i.e., the involved dataset) and how the FPTC approach is not generalizable.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ—¶é—´é¢„æµ‹é—®é¢˜å·²æˆä¸ºç§‘å­¦ç•Œçƒ­ç‚¹é—®é¢˜ã€‚å¦‚æœå¯ä»¥åœ¨å…ˆçŸ¥é“æ¨¡å‹è®­ç»ƒæ—¶é—´ï¼Œé‚£ä¹ˆå¯ä»¥è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼Œä»¥ä¿è¯èƒ½å¤Ÿè¾¾åˆ°æœ€ä½³æ€§èƒ½å’Œèƒ½æ•ˆç‡ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬åœ¨è¿™ä¸ªæ–¹å‘ä¸‹çš„å·¥ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹ Zheng et al. ç­‰äººæå‡ºçš„ Full Parameter Time Complexityï¼ˆFPTCï¼‰æ–¹æ³•è¿›è¡Œäº†å¹¿æ³›çš„å®è¯ç ”ç©¶ã€‚è¿™æ˜¯æˆ‘ä»¬æ‰€çŸ¥é“çš„å”¯ä¸€ä¸€ç§å½¢å¼åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ—¶é—´ä¸ºæ•°æ®é›†å’Œæ¨¡å‹å‚æ•°çš„å‡½æ•°ã€‚æˆ‘ä»¬å¯¹ Logistic Regression å’Œ Random Forest åˆ†ç±»å™¨çš„å½¢ulationè¿›è¡Œäº†ç ”ç©¶ï¼Œå¹¶å°†å…¶ä¸»è¦ä¼˜ç‚¹å’Œç¼ºç‚¹è¿›è¡Œäº†æè¿°ã€‚æœ€åï¼Œæˆ‘ä»¬å‘ç°ï¼Œä»æˆ‘ä»¬è¿›è¡Œçš„ç ”ç©¶æ¥çœ‹ï¼Œè®­ç»ƒæ—¶é—´é¢„æµ‹ä¸æ•°æ®é›†ç›¸å…³ï¼Œè€Œ FPTC æ–¹æ³•ä¸èƒ½æ³›åŒ–ã€‚ã€‹Note: Please note that the translation is in Simplified Chinese, and the grammar and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="A-Model-Based-Machine-Learning-Approach-for-Assessing-the-Performance-of-Blockchain-Applications"><a href="#A-Model-Based-Machine-Learning-Approach-for-Assessing-the-Performance-of-Blockchain-Applications" class="headerlink" title="A Model-Based Machine Learning Approach for Assessing the Performance of Blockchain Applications"></a>A Model-Based Machine Learning Approach for Assessing the Performance of Blockchain Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11205">http://arxiv.org/abs/2309.11205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlbshriAdel/BlockchainPerformanceML">https://github.com/AlbshriAdel/BlockchainPerformanceML</a></li>
<li>paper_authors: Adel Albshri, Ali Alzubaidi, Ellis Solaiman</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ç§å¯é çš„æ¨¡å‹æ–¹æ³•ï¼Œä»¥ä¾¿ä¿ƒè¿›åŒºå—é“¾åº”ç”¨ç¨‹åºçš„å¼€å‘å’Œè¯„ä¼°ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸¤ç§æœºå™¨å­¦ä¹ æ¨¡å‹åŸºæœ¬æ–¹æ³•ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ $k$  nearest neighborsï¼ˆ$k$NNï¼‰å’Œæ”¯æŒå‘é‡æœºå™¨ï¼ˆSVMï¼‰æ¨¡å‹æ¥é¢„æµ‹åŒºå—é“¾æ€§èƒ½ï¼Œä½¿ç”¨é¢„å…ˆç¡®å®šçš„é…ç½®å‚æ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç‘ç‰¹é›†ç¾¤ä¼˜åŒ–ï¼ˆSOï¼‰æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç‘ç‰¹é›†ç¾¤ä¼˜åŒ–ï¼ˆISOï¼‰æ¥å¯»æ‰¾æœ€ä½³åŒºå—é“¾é…ç½®ï¼Œä»¥è¾¾åˆ°æ‰€éœ€æ€§èƒ½æ°´å¹³ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ¨¡å‹æ¯”è¾ƒç»Ÿè®¡ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ $k$NN æ¨¡å‹å¯ä»¥æ¯” SVM æ¨¡å‹æé«˜æ€§èƒ½ï¼Œå¹¶ä¸”ä½¿ç”¨ ISO å¯ä»¥å‡å°‘ä¸ç¡®å®šæ€§ deviation çš„åå·®ã€‚<details>
<summary>Abstract</summary>
The recent advancement of Blockchain technology consolidates its status as a viable alternative for various domains. However, evaluating the performance of blockchain applications can be challenging due to the underlying infrastructure's complexity and distributed nature. Therefore, a reliable modelling approach is needed to boost Blockchain-based applications' development and evaluation. While simulation-based solutions have been researched, machine learning (ML) model-based techniques are rarely discussed in conjunction with evaluating blockchain application performance. Our novel research makes use of two ML model-based methods. Firstly, we train a $k$ nearest neighbour ($k$NN) and support vector machine (SVM) to predict blockchain performance using predetermined configuration parameters. Secondly, we employ the salp swarm optimization (SO) ML model which enables the investigation of optimal blockchain configurations for achieving the required performance level. We use rough set theory to enhance SO, hereafter called ISO, which we demonstrate to prove achieving an accurate recommendation of optimal parameter configurations; despite uncertainty. Finally, statistical comparisons indicate that our models have a competitive edge. The $k$NN model outperforms SVM by 5\% and the ISO also demonstrates a reduction of 4\% inaccuracy deviation compared to regular SO.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„åŒºå—é“¾æŠ€æœ¯è¿›æ­¥ä½¿å…¶æˆä¸ºå¤šç§é¢†åŸŸçš„å¯é  altenativeã€‚ç„¶è€Œï¼Œè¯„ä¼°åŒºå—é“¾åº”ç”¨ç¨‹åºæ€§èƒ½å¯èƒ½ä¼šå›°éš¾ç”±äºåŒºå—é“¾åŸºç¡€è®¾æ–½çš„å¤æ‚æ€§å’Œåˆ†å¸ƒå¼ç‰¹ç‚¹ã€‚å› æ­¤ï¼Œä¸€ç§å¯é çš„æ¨¡å‹æ–¹æ³•æ˜¯éœ€è¦ä¸ºåŒºå—é“¾åº”ç”¨ç¨‹åºçš„å¼€å‘å’Œè¯„ä¼°æä¾› boostã€‚è€Œä¸”ï¼Œä½¿ç”¨simulation-basedè§£å†³æ–¹æ¡ˆå·²ç»è¢«ç ”ç©¶ï¼Œä½†æ˜¯ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åŸºäºæŠ€æœ¯ rarely discussed in conjunction with evaluating blockchain application performanceã€‚æˆ‘ä»¬çš„æ–°ç ”ç©¶ä½¿ç”¨äº†ä¸¤ç§MLæ¨¡å‹åŸºäºæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ $k$ nearest neighbour ($k$NN) å’Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¥é¢„æµ‹åŒºå—é“¾æ€§èƒ½ä½¿ç”¨é¢„å…ˆç¡®å®šçš„é…ç½®å‚æ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨salp swarm optimizationï¼ˆSOï¼‰MLæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å…è®¸æˆ‘ä»¬è°ƒæŸ¥åˆ°è¾¾æ‰€éœ€æ€§èƒ½æ°´å¹³çš„ä¼˜åŒ–çš„åŒºå—é“¾é…ç½®ã€‚æˆ‘ä»¬ä½¿ç”¨ç²—è®¾ç†è®ºæ¥å¢å¼ºSOï¼Œç§°ä¸ºISOï¼Œå¹¶è¯æ˜ISOå¯ä»¥å‡†ç¡®åœ°æä¾›ä¼˜åŒ–å‚æ•°é…ç½®ï¼Œå³ä½¿å­˜åœ¨uncertaintyã€‚æœ€åï¼Œç»Ÿè®¡æ¯”è¾ƒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰ç«äº‰ä¼˜åŠ¿ã€‚$k$NNæ¨¡å‹åœ¨æ¯”è¾ƒSVMæ–¹æ³•æ—¶è¡¨ç°å‡º5%çš„æå‡ï¼Œè€ŒISOæ¨¡å‹ä¹Ÿè¡¨ç°å‡º4%çš„å‡å°‘ä¸ç¡®å®šæ€§åç§»ã€‚
</details></li>
</ul>
<hr>
<h2 id="RHALE-Robust-and-Heterogeneity-aware-Accumulated-Local-Effects"><a href="#RHALE-Robust-and-Heterogeneity-aware-Accumulated-Local-Effects" class="headerlink" title="RHALE: Robust and Heterogeneity-aware Accumulated Local Effects"></a>RHALE: Robust and Heterogeneity-aware Accumulated Local Effects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11193">http://arxiv.org/abs/2309.11193</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/givasile/RHALE">https://github.com/givasile/RHALE</a></li>
<li>paper_authors: Vasilis Gkolemis, Theodore Dalamagas, Eirini Ntoutsi, Christos Diou</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜Explainabilityæ–¹æ³•çš„ç²¾åº¦å’Œå¯é æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ç›¸å…³ç‰¹å¾æƒ…å†µä¸‹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§Robustå’ŒHeterogeneity-aware ALEï¼ˆRHALEï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥è¯„ä¼°ç‰¹å¾å¯¹è¾“å‡ºçš„å¹³å‡æ•ˆæœï¼ŒåŒæ—¶è€ƒè™‘åˆ°å®ä¾‹çº§åˆ«çš„å·®å¼‚ï¼ˆheterogeneityï¼‰ã€‚</li>
<li>results: å¯¹äº synthetic å’Œå®é™…æ•°æ®é›†ï¼ŒRHALE æ–¹æ³•æ¯”å…¶ä»–æ–¹æ³•è¡¨ç°æ›´ä¼˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›¸å…³ç‰¹å¾æƒ…å†µä¸‹ã€‚ RHALE æ–¹æ³•è¿˜å¯ä»¥è‡ªåŠ¨ç¡®å®šæœ€ä½³åˆ†å‰²æ–¹æ¡ˆï¼Œä»¥å…¼é¡¾ bias å’Œ varianceã€‚<details>
<summary>Abstract</summary>
Accumulated Local Effects (ALE) is a widely-used explainability method for isolating the average effect of a feature on the output, because it handles cases with correlated features well. However, it has two limitations. First, it does not quantify the deviation of instance-level (local) effects from the average (global) effect, known as heterogeneity. Second, for estimating the average effect, it partitions the feature domain into user-defined, fixed-sized bins, where different bin sizes may lead to inconsistent ALE estimations. To address these limitations, we propose Robust and Heterogeneity-aware ALE (RHALE). RHALE quantifies the heterogeneity by considering the standard deviation of the local effects and automatically determines an optimal variable-size bin-splitting. In this paper, we prove that to achieve an unbiased approximation of the standard deviation of local effects within each bin, bin splitting must follow a set of sufficient conditions. Based on these conditions, we propose an algorithm that automatically determines the optimal partitioning, balancing the estimation bias and variance. Through evaluations on synthetic and real datasets, we demonstrate the superiority of RHALE compared to other methods, including the advantages of automatic bin splitting, especially in cases with correlated features.
</details>
<details>
<summary>æ‘˜è¦</summary>
é›†æˆæœ¬åœ°æ•ˆåº”ï¼ˆALEï¼‰æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„è§£é‡Šæ–¹æ³•ï¼Œç”¨äºéš”ç¦»è¾“å‡ºçš„å¹³å‡æ•ˆåº”ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿå¤„ç†ç›¸å…³çš„ç‰¹å¾ ÑĞ»ÑƒÑ‡Ğ°å­ wellã€‚ç„¶è€Œï¼Œå®ƒæœ‰ä¸¤ä¸ªé™åˆ¶ã€‚é¦–å…ˆï¼Œå®ƒä¸è®¡ç®—ç‰¹å®šå®ä¾‹ï¼ˆæœ¬åœ°ï¼‰æ•ˆåº”ä¸å¹³å‡ï¼ˆå…¨å±€ï¼‰æ•ˆåº”ä¹‹é—´çš„åå·®ã€‚å…¶æ¬¡ï¼Œä¸ºè®¡ç®—å¹³å‡æ•ˆåº”ï¼Œå®ƒå°†ç‰¹å¾é¢†åŸŸåˆ†æˆç”¨æˆ·å®šä¹‰ã€å›ºå®šå¤§å°çš„åˆ†å‰²ï¼Œä¸åŒçš„åˆ†å‰²å¤§å°å¯èƒ½ä¼šå¯¼è‡´ä¸ä¸€è‡´çš„ ALE ä¼°è®¡ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† Robust and Heterogeneity-aware ALEï¼ˆRHALEï¼‰ã€‚RHALE è€ƒè™‘äº†æœ¬åœ°æ•ˆåº”çš„æ ‡å‡†å·®ï¼Œä»¥åŠè‡ªåŠ¨ç¡®å®šæœ€ä½³å˜é‡å¤§å°åˆ†å‰²ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ï¼Œä¸ºåœ¨æ¯ä¸ªåˆ†å‰²ä¸­ç²¾ç¡®ä¼°è®¡æœ¬åœ°æ•ˆåº”çš„æ ‡å‡†å·®ï¼Œåˆ†å‰²å¿…é¡»éµå¾ªä¸€ç»„å¿…è¦æ¡ä»¶ã€‚åŸºäºè¿™äº›æ¡ä»¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œå¯ä»¥è‡ªåŠ¨ç¡®å®šæœ€ä½³åˆ†å‰²ï¼Œåè°ƒä¼°è®¡åå·®å’Œæ–¹å·®ã€‚é€šè¿‡å¯¹ synthetic å’Œå®é™…æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬ç¤ºå‡ºäº† RHALE ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰è¾ƒå¥½çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›¸å…³ç‰¹å¾æƒ…å†µä¸‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Investigating-Personalization-Methods-in-Text-to-Music-Generation"><a href="#Investigating-Personalization-Methods-in-Text-to-Music-Generation" class="headerlink" title="Investigating Personalization Methods in Text to Music Generation"></a>Investigating Personalization Methods in Text to Music Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11140">http://arxiv.org/abs/2309.11140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zelaki/DreamSound">https://github.com/zelaki/DreamSound</a></li>
<li>paper_authors: Manos Plitsis, Theodoros Kouzelis, Georgios Paraskevopoulos, Vassilis Katsouros, Yannis Panagakis</li>
<li>for: è¿™ä¸ªç ”ç©¶æ¢è®¨äº†åœ¨å‡ ä¸ªshotè®¾å®šä¸‹ä¸ªæ€§åŒ–æ–‡æœ¬åˆ°éŸ³ä¹æ‰©æ•£æ¨¡å‹çš„é—®é¢˜ã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº†å·²æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•çš„ç»„åˆï¼Œä»¥åŠéŸ³é¢‘ä¸“é—¨çš„æ•°æ®å¢å¼ºæŠ€æœ¯ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œç›¸ä¼¼åº¦æŒ‡æ ‡ä¸ç”¨æˆ·å–œå¥½ç›¸å»åˆï¼Œç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•æ›´å®¹æ˜“å­¦ä¹ rhythmicéŸ³ä¹æ„é€ è€Œä¸æ˜¯melodyã€‚Please note that the above text is in Simplified Chinese.<details>
<summary>Abstract</summary>
In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music constructs more easily than melody. The code, dataset, and example material of this study are open to the research community.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†æ–‡æœ¬åˆ°éŸ³ä¹å¡«å……æ¨¡å‹åœ¨å‡ ä¸ªå°è¯•è®¾ç½®ä¸‹çš„ä¸ªæ€§åŒ–ã€‚å—æœ€è¿‘è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„è¿›æ­¥ inspiritsï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡æ¢è®¨å°†é¢„è®­ç»ƒæ–‡æœ¬åˆ°éŸ³é¢‘å¡«å……å™¨ä¸ä¸¤ç§å·²æœ‰ä¸ªæ€§åŒ–æ–¹æ³•ç»“åˆä½¿ç”¨ã€‚æˆ‘ä»¬å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“è¿›è¡Œäº†éŸ³é¢‘ç‰¹å®šæ•°æ®å¢å¼ºçš„å®éªŒï¼Œå¹¶è¯„ä¼°äº†ä¸åŒçš„è®­ç»ƒç­–ç•¥ã€‚ä¸ºè¯„ä»·ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æç¤ºå’ŒéŸ³ä¹ç‰‡æ–­é›†åˆã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ç§åµŒå…¥ç©ºé—´å’ŒéŸ³ä¹ç‰¹æœ‰çš„è¯„ä»·æŒ‡æ ‡è¿›è¡Œé‡åŒ–è¯„ä¼°ï¼Œä»¥åŠä¸€é¡¹ç”¨æˆ·ç ”ç©¶ Ğ´Ğ»Ñè´¨é‡è¯„ä¼°ã€‚æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œç›¸ä¼¼åº¦æŒ‡æ ‡ä¸ç”¨æˆ·å–œå¥½ç›¸ç¬¦ï¼Œç°æœ‰çš„ä¸ªæ€§åŒ–æ–¹æ³•æ›´å®¹æ˜“å­¦ä¹ éŸ³ä¹çš„èŠ‚å¥ç»“æ„è€Œä¸æ˜¯æ—‹å¾‹ã€‚æˆ‘ä»¬çš„ä»£ç ã€æ•°æ®é›†å’Œç ”ç©¶ææ–™å¯¹ç ”ç©¶ç¤¾åŒºå¼€æ”¾ã€‚
</details></li>
</ul>
<hr>
<h2 id="Ano-SuPs-Multi-size-anomaly-detection-for-manufactured-products-by-identifying-suspected-patches"><a href="#Ano-SuPs-Multi-size-anomaly-detection-for-manufactured-products-by-identifying-suspected-patches" class="headerlink" title="Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches"></a>Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11120">http://arxiv.org/abs/2309.11120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Xu, Juan Du, Andi Wang</li>
<li>for: This paper aims to address the challenges of existing matrix decomposition methods in image-based anomaly detection, particularly in the presence of complex backgrounds and various anomaly patterns.</li>
<li>methods: The proposed method uses a two-stage strategy that involves detecting suspected patches (Ano-SuPs) by reconstructing the input image twice: the first step is to obtain a set of normal patches by removing suspected patches, and the second step is to use those normal patches to refine the identification of patches with anomalies.</li>
<li>results: The proposed method is evaluated systematically through simulation experiments and case studies, demonstrating its effectiveness in detecting anomalies in image-based systems. The key parameters and designed steps that impact the modelâ€™s performance and efficiency are also identified.<details>
<summary>Abstract</summary>
Image-based systems have gained popularity owing to their capacity to provide rich manufacturing status information, low implementation costs and high acquisition rates. However, the complexity of the image background and various anomaly patterns pose new challenges to existing matrix decomposition methods, which are inadequate for modeling requirements. Moreover, the uncertainty of the anomaly can cause anomaly contamination problems, making the designed model and method highly susceptible to external disturbances. To address these challenges, we propose a two-stage strategy anomaly detection method that detects anomalies by identifying suspected patches (Ano-SuPs). Specifically, we propose to detect the patches with anomalies by reconstructing the input image twice: the first step is to obtain a set of normal patches by removing those suspected patches, and the second step is to use those normal patches to refine the identification of the patches with anomalies. To demonstrate its effectiveness, we evaluate the proposed method systematically through simulation experiments and case studies. We further identified the key parameters and designed steps that impact the model's performance and efficiency.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›¾åƒåŸºäºç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¾—åˆ°æ™®åŠï¼Œè¿™ä¸»è¦å½’åŠŸäºå®ƒä»¬èƒ½å¤Ÿæä¾›ä¸°å¯Œçš„ç”Ÿäº§çŠ¶å†µä¿¡æ¯ï¼Œä»¥åŠå®ç°æˆæœ¬å’Œè·å¾—ç‡çš„ä½ã€‚ç„¶è€Œï¼Œå›¾åƒèƒŒæ™¯çš„å¤æ‚æ€§å’Œå„ç§å¼‚å¸¸æ¨¡å¼å¸¦æ¥äº†å¯¹ç°æœ‰çŸ©é˜µåˆ†è§£æ–¹æ³•çš„æ–°æŒ‘æˆ˜ã€‚æ­¤å¤–ï¼Œå¼‚å¸¸ç°è±¡çš„ä¸ç¡®å®šæ€§ä¼šå¯¼è‡´å¼‚å¸¸æ±¡æŸ“é—®é¢˜ï¼Œä½¿å¾—è®¾è®¡çš„æ¨¡å‹å’Œæ–¹æ³•å®¹æ˜“å—åˆ°å¤–éƒ¨å¹²æ‰°ã€‚ä¸ºè§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤Stageç­–ç•¥å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œé€šè¿‡æ£€æµ‹å¼‚å¸¸çš„è¡¥ä¸ï¼ˆAno-SuPsï¼‰æ¥æ£€æµ‹å¼‚å¸¸ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä»è¾“å…¥å›¾åƒä¸­æå–å‡ºä¸€ç»„æ­£å¸¸è¡¥ä¸ï¼Œç„¶åä½¿ç”¨è¿™äº›æ­£å¸¸è¡¥ä¸æ¥ç²¾ç»†åœ°å®šä½å¼‚å¸¸è¡¥ä¸ã€‚ä¸ºè¯æ˜å…¶æ•ˆæœï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°é€šè¿‡å®éªŒå’Œæ¡ˆä¾‹ç ”ç©¶è¯„ä¼°äº†ææ¡ˆçš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ ‡è¯†å‡ºäº†å½±å“æ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡çš„å…³é”®å‚æ•°å’Œè®¾è®¡æ­¥éª¤ã€‚
</details></li>
</ul>
<hr>
<h2 id="Bold-but-Cautious-Unlocking-the-Potential-of-Personalized-Federated-Learning-through-Cautiously-Aggressive-Collaboration"><a href="#Bold-but-Cautious-Unlocking-the-Potential-of-Personalized-Federated-Learning-through-Cautiously-Aggressive-Collaboration" class="headerlink" title="Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration"></a>Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11103">http://arxiv.org/abs/2309.11103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kxzxvbk/Fling">https://github.com/kxzxvbk/Fling</a></li>
<li>paper_authors: Xinghao Wu, Xuefeng Liu, Jianwei Niu, Guogang Zhu, Shaojie Tang</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦å…³æ³¨åœ¨å¯¹å¤šä¸ªå®¢æˆ·è¿›è¡ŒååŒå­¦ä¹ æ—¶ï¼Œå‡å°‘éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰èµ„æ–™çš„å½±å“ï¼Œå¹¶ä¸”å°†å®¢æˆ·è®­ç»ƒçš„ä¸ªäººåŒ–æ¨¡å‹ä¸å…¶ä»–å®¢æˆ·è¿›è¡ŒååŒå­¦ä¹ ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ååŒå­¦ä¹ æŒ‡å—ï¼Œä¸ç°æœ‰çš„æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œå®ƒå…è®¸å®¢æˆ·å°†æ›´å¤šçš„å‚æ•°ä¸å…¶ä»–å®¢æˆ·å…±äº«ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™ç¯‡è®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªåä¸ºFedCACçš„æ–°ååŒå­¦ä¹ æ–¹æ³•ï¼Œå®ƒä½¿ç”¨ä¸€ä¸ªé‡å€¼æŒ‡æ•°æ¥è¯„ä¼°å„å‚æ•°çš„éç‹¬ç«‹åŒåˆ†å¸ƒæ•æ„Ÿåº¦ï¼Œå¹¶å°†å®¢æˆ·é€‰æ‹©ä¸ºååŒå­¦ä¹ è€…åŸºäºè¿™ä¸ªè¯„ä¼°ç»“æœã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFedCACæ¯”ç°æœ‰çš„æ–¹æ³•æ›´å¥½åœ°å°†å®¢æˆ·çš„å‚æ•°ä¸å…¶ä»–å®¢æˆ·å…±äº«ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å®¢æˆ·çš„èµ„æ–™åˆ†å¸ƒä¸åŒæ—¶ã€‚<details>
<summary>Abstract</summary>
Personalized federated learning (PFL) reduces the impact of non-independent and identically distributed (non-IID) data among clients by allowing each client to train a personalized model when collaborating with others. A key question in PFL is to decide which parameters of a client should be localized or shared with others. In current mainstream approaches, all layers that are sensitive to non-IID data (such as classifier layers) are generally personalized. The reasoning behind this approach is understandable, as localizing parameters that are easily influenced by non-IID data can prevent the potential negative effect of collaboration. However, we believe that this approach is too conservative for collaboration. For example, for a certain client, even if its parameters are easily influenced by non-IID data, it can still benefit by sharing these parameters with clients having similar data distribution. This observation emphasizes the importance of considering not only the sensitivity to non-IID data but also the similarity of data distribution when determining which parameters should be localized in PFL. This paper introduces a novel guideline for client collaboration in PFL. Unlike existing approaches that prohibit all collaboration of sensitive parameters, our guideline allows clients to share more parameters with others, leading to improved model performance. Additionally, we propose a new PFL method named FedCAC, which employs a quantitative metric to evaluate each parameter's sensitivity to non-IID data and carefully selects collaborators based on this evaluation. Experimental results demonstrate that FedCAC enables clients to share more parameters with others, resulting in superior performance compared to state-of-the-art methods, particularly in scenarios where clients have diverse distributions.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:personalized federated learning (PFL) å‡å°‘å®¢æˆ·ç«¯ä¹‹é—´éç‹¬ç«‹å’ŒåŒåˆ†å¸ƒæ•°æ®çš„å½±å“ï¼Œé€šè¿‡è®©æ¯ä¸ªå®¢æˆ·ç«¯è®­ç»ƒä¸ªæ€§åŒ–æ¨¡å‹å¹¶ä¸å…¶ä»–å®¢æˆ·ç«¯åˆä½œã€‚PFLä¸­çš„å…³é”®é—®é¢˜æ˜¯å†³å®šæ¯ä¸ªå®¢æˆ·ç«¯çš„å‚æ•°æ˜¯å¦è¦æœ¬åœ°åŒ–æˆ–ä¸å…¶ä»–å®¢æˆ·ç«¯å…±äº«ã€‚ç°ä»Šä¸»æµçš„æ–¹æ³•æ˜¯å°†æ‰€æœ‰æ•æ„Ÿäºéç‹¬ç«‹å’ŒåŒåˆ†å¸ƒæ•°æ®çš„å±‚ï¼ˆä¾‹å¦‚åˆ†ç±»å±‚ï¼‰éƒ½æœ¬åœ°åŒ–ã€‚è¿™ç§æ–¹æ³•çš„åŸå› æ˜¯å¯ä»¥é¿å…å› åˆä½œè€Œå¯¼è‡´çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™ç§æ–¹æ³•æ˜¯å¯¹åˆä½œçš„è¿‡åº¦ä¿å®ˆã€‚ä¾‹å¦‚ï¼Œå¯¹äºæŸä¸ªå®¢æˆ·ç«¯ï¼Œå³ä½¿å…¶å‚æ•°å®¹æ˜“å—åˆ°éç‹¬ç«‹å’ŒåŒåˆ†å¸ƒæ•°æ®çš„å½±å“ï¼Œä½†æ˜¯å®ƒä»å¯ä»¥é€šè¿‡ä¸å…¶ä»–å®¢æˆ·ç«¯çš„æ•°æ®åˆ†å¸ƒç›¸ä¼¼æ€§æ¥å…±äº«å‚æ•°ï¼Œä»è€Œè·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚è¿™ä¸€è§‚å¯Ÿå¼ºè°ƒäº†åœ¨PFLä¸­è€ƒè™‘å‚æ•°çš„æ•æ„Ÿåº¦ä»¥åŠæ•°æ®åˆ†å¸ƒçš„ç›¸ä¼¼æ€§æ˜¯éå¸¸é‡è¦çš„ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„PFLå®¢æˆ·ç«¯åä½œæŒ‡å—ï¼Œä¸ç°ä»Šä¸»æµçš„æ–¹æ³•ä¸åŒä¹‹å¤„åœ¨äºï¼Œå®ƒå…è®¸å®¢æˆ·ç«¯æ›´å¤šåœ°å…±äº«å‚æ•°ï¼Œä»è€Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åä¸ºFedCACçš„æ–°çš„PFLæ–¹æ³•ï¼Œå®ƒä½¿ç”¨ä¸€ç§é‡åŒ–çš„åº¦é‡æ¥è¯„ä¼°æ¯ä¸ªå‚æ•°çš„éç‹¬ç«‹å’ŒåŒåˆ†å¸ƒæ•°æ®çš„æ•æ„Ÿåº¦ï¼Œå¹¶ä¸”æ ¹æ®è¿™ç§è¯„ä¼°æ¥ç²¾å¿ƒé€‰æ‹©åˆä½œè€…ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒFedCACå¯ä»¥å‡å°‘å®¢æˆ·ç«¯ä¹‹é—´çš„æ•°æ®åˆ†å¸ƒå·®å¼‚ï¼Œä»è€Œå®ç°ä¸å½“å‰æœ€ä½³æ–¹æ³•ç›¸æ¯”çš„æ›´å¥½çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Delays-in-Reinforcement-Learning"><a href="#Delays-in-Reinforcement-Learning" class="headerlink" title="Delays in Reinforcement Learning"></a>Delays in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11096">http://arxiv.org/abs/2309.11096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reiniscimurs/DRL-robot-navigation">https://github.com/reiniscimurs/DRL-robot-navigation</a></li>
<li>paper_authors: Pierre Liotet</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦ç ”ç©¶äº†å»¶è¿Ÿåœ¨åŠ¨æ€ç³»ç»Ÿä¸­çš„å½±å“ï¼Œä»¥åŠå¦‚ä½•åœ¨å»¶è¿Ÿçš„æƒ…å†µä¸‹è¿›è¡Œå†³ç­–ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†é©¬å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä½œä¸ºåŸºç¡€æ¡†æ¶ï¼Œå¹¶ç ”ç©¶äº†å»¶è¿Ÿåœ¨è¿™ç§å†³ç­–è¿‡ç¨‹ä¸­çš„å½±å“ã€‚</li>
<li>results: è¯¥è®ºæ–‡å‘ç°äº†å»¶è¿Ÿå¯¹åŠ¨æ€ç³»ç»Ÿçš„å½±å“ï¼Œå¹¶æå‡ºäº†ä¸€äº›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚åŒæ—¶ï¼Œè®ºæ–‡è¿˜Draws links between celebrated frameworks of reinforcement learning literature and the one of delays.<details>
<summary>Abstract</summary>
Delays are inherent to most dynamical systems. Besides shifting the process in time, they can significantly affect their performance. For this reason, it is usually valuable to study the delay and account for it. Because they are dynamical systems, it is of no surprise that sequential decision-making problems such as Markov decision processes (MDP) can also be affected by delays. These processes are the foundational framework of reinforcement learning (RL), a paradigm whose goal is to create artificial agents capable of learning to maximise their utility by interacting with their environment.   RL has achieved strong, sometimes astonishing, empirical results, but delays are seldom explicitly accounted for. The understanding of the impact of delay on the MDP is limited. In this dissertation, we propose to study the delay in the agent's observation of the state of the environment or in the execution of the agent's actions. We will repeatedly change our point of view on the problem to reveal some of its structure and peculiarities. A wide spectrum of delays will be considered, and potential solutions will be presented. This dissertation also aims to draw links between celebrated frameworks of the RL literature and the one of delays.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>translate "Delays are inherent to most dynamical systems. Besides shifting the process in time, they can significantly affect their performance. For this reason, it is usually valuable to study the delay and account for it. Because they are dynamical systems, it is of no surprise that sequential decision-making problems such as Markov decision processes (MDP) can also be affected by delays. These processes are the foundational framework of reinforcement learning (RL), a paradigm whose goal is to create artificial agents capable of learning to maximize their utility by interacting with their environment.   RL has achieved strong, sometimes astonishing, empirical results, but delays are seldom explicitly accounted for. The understanding of the impact of delay on the MDP is limited. In this dissertation, we propose to study the delay in the agent's observation of the state of the environment or in the execution of the agent's actions. We will repeatedly change our point of view on the problem to reveal some of its structure and peculiarities. A wide spectrum of delays will be considered, and potential solutions will be presented. This dissertation also aims to draw links between celebrated frameworks of the RL literature and the one of delays." into Simplified Chinese.Here's the translation:<<SYS>>å¤šç§åŠ¨åŠ›ç³»ç»Ÿä¸­éƒ½å­˜åœ¨å»¶è¿Ÿã€‚ besides å»¶è¿Ÿæ—¶é—´çš„åç§»ï¼Œå®ƒä»¬å¯ä»¥å¯¹æ€§èƒ½äº§ç”Ÿé‡è¦å½±å“ã€‚å› æ­¤ï¼Œé€šå¸¸å€¼å¾—ç ”ç©¶å»¶è¿Ÿå¹¶è€ƒè™‘å…¶å½±å“ã€‚å› ä¸ºå®ƒä»¬æ˜¯åŠ¨åŠ›ç³»ç»Ÿï¼Œå› æ­¤ä¹Ÿä¸surprisinglyï¼Œsequential decision-making problemssuch as Markov decision processes (MDP) ä¹Ÿå¯ä»¥å—åˆ°å»¶è¿Ÿçš„å½±å“ã€‚è¿™äº›è¿‡ç¨‹æ˜¯RLçš„åŸºç¡€æ¡†æ¶ï¼ŒRLçš„ç›®æ ‡æ˜¯åˆ›å»ºå¯ä»¥åœ¨ç¯å¢ƒä¸­å­¦ä¹ æé«˜åˆ©ç”¨çš„äººå·¥æ™ºèƒ½ä»£ç†ã€‚   RLå·²ç»å–å¾—äº†å¼ºå¤§ï¼Œoccasionally astonishingçš„å®éªŒæˆæœï¼Œä½†å»¶è¿Ÿé€šå¸¸ä¸ç›´æ¥è€ƒè™‘ã€‚MDPä¸­å»¶è¿Ÿçš„ç†è§£å—é™ã€‚åœ¨è¿™ä¸ªè®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æè®®ç ”ç©¶ä»£ç† Observation of the state of the environment æˆ–æ‰§è¡Œä»£ç†åŠ¨ä½œä¸­çš„å»¶è¿Ÿã€‚æˆ‘ä»¬å°†ä¸æ–­æ›´æ”¹é—®é¢˜çš„è§†ç‚¹ï¼Œä»¥æ­ç¤ºå…¶ç»“æ„å’Œç‰¹ç‚¹ã€‚å¹¿æ³›è€ƒè™‘å»¶è¿Ÿçš„èŒƒå›´ï¼Œå¹¶æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸ªè®ºæ–‡è¿˜è®¡åˆ’æŠŠRLæ–‡çŒ®ä¸­è‘—åçš„æ¡†æ¶ä¸å»¶è¿Ÿæ¡†æ¶ç›¸è¿æ¥ã€‚
</details></li>
</ul>
<hr>
<h2 id="GPSINDy-Data-Driven-Discovery-of-Equations-of-Motion"><a href="#GPSINDy-Data-Driven-Discovery-of-Equations-of-Motion" class="headerlink" title="GPSINDy: Data-Driven Discovery of Equations of Motion"></a>GPSINDy: Data-Driven Discovery of Equations of Motion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11076">http://arxiv.org/abs/2309.11076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junette Hsin, Shubhankar Agarwal, Adam Thorpe, David Fridovich-Keil</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¯»æ‰¾å«æœ‰å™ªå£°æ•°æ®çš„éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿæ¨¡å‹ã€‚</li>
<li>methods: æˆ‘ä»¬å°† Gaussian è¿‡ç¨‹å›å½’ã€SINDy å‚æ•°å­¦ä¹ æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œä»¥ä¾¿ä»æ•°æ®ä¸­æ‰¾åˆ°éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿæ¨¡å‹ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨ä¸€ä¸ª Lotka-Volterra æ¨¡å‹å’Œä¸€ä¸ª unicycle åŠ¨åŠ›ç³»ç»Ÿä¸Šè¿›è¡Œäº†å®éªŒå’Œç¡¬ä»¶æ•°æ®å¤„ç†ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ›´å¥½åœ°æ‰¾åˆ°ç³»ç»ŸåŠ¨åŠ›å’Œé¢„æµ‹æœªæ¥è½¨è¿¹ã€‚<details>
<summary>Abstract</summary>
In this paper, we consider the problem of discovering dynamical system models from noisy data. The presence of noise is known to be a significant problem for symbolic regression algorithms. We combine Gaussian process regression, a nonparametric learning method, with SINDy, a parametric learning approach, to identify nonlinear dynamical systems from data. The key advantages of our proposed approach are its simplicity coupled with the fact that it demonstrates improved robustness properties with noisy data over SINDy. We demonstrate our proposed approach on a Lotka-Volterra model and a unicycle dynamic model in simulation and on an NVIDIA JetRacer system using hardware data. We demonstrate improved performance over SINDy for discovering the system dynamics and predicting future trajectories.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä»å«å™ªæ•°æ®ä¸­æ‰¾åˆ°åŠ¨åŠ›ç³»ç»Ÿæ¨¡å‹çš„é—®é¢˜ã€‚å™ªå£°çŸ¥é“ä¼šå¯¹ç¬¦å·å›å½’ç®—æ³•äº§ç”Ÿå¾ˆå¤§çš„å½±å“ã€‚æˆ‘ä»¬å°† Gaussian process regression å’Œ SINDy ç»“åˆèµ·æ¥ï¼Œä»¥éå‚æ•°æ–¹å¼å­¦ä¹ æ–¹æ³•æ¥è¯†åˆ«éçº¿æ€§åŠ¨åŠ›ç³»ç»Ÿæ¨¡å‹ã€‚æˆ‘ä»¬çš„æè®®çš„æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯ç®€å•æ˜“ç”¨ï¼ŒåŒæ—¶å…·æœ‰è¾ƒå¥½çš„é²æ£’æ€§ç‰¹æ€§ï¼Œåœ¨å«å™ªæ•°æ®ä¸Šè¡¨ç° mejor than SINDyã€‚æˆ‘ä»¬åœ¨ Lotka-Volterra æ¨¡å‹å’Œ unicycle åŠ¨æ€æ¨¡å‹ä¸Šè¿›è¡Œäº†åœ¨ simulate å’Œ NVIDIA JetRacer ç³»ç»Ÿä¸Šä½¿ç”¨ç¡¬ä»¶æ•°æ®è¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ›´å¥½åœ°æ‰¾åˆ°ç³»ç»ŸåŠ¨åŠ›å’Œé¢„æµ‹æœªæ¥è½¨è¿¹ã€‚
</details></li>
</ul>
<hr>
<h2 id="InkStream-Real-time-GNN-Inference-on-Streaming-Graphs-via-Incremental-Update"><a href="#InkStream-Real-time-GNN-Inference-on-Streaming-Graphs-via-Incremental-Update" class="headerlink" title="InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update"></a>InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11071">http://arxiv.org/abs/2309.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Wu, Zhaoying Li, Tulika Mitra</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æå‡ºä¸€ç§åŸºäºå›¾ neural network (GNN) çš„å®æ—¶æ¨ç†æ–¹æ³•ï¼Œä»¥é€‚åº”æµåŠ¨å›¾çš„æ›´æ–°ã€‚</li>
<li>methods: æœ¬æ–¹æ³•åŸºäºä¸¤ç‚¹å…³é”®è§è§£ï¼šï¼ˆ1ï¼‰åœ¨ $k$-hop é‚»åŸŸå†…ï¼Œå¤§å¤šæ•°èŠ‚ç‚¹ä¸å—åˆ°ä¿®æ”¹è¾¹çš„å½±å“ï¼Œå½“ä½¿ç”¨æ±‡èšå‡½æ•°æ—¶ï¼›ï¼ˆ2ï¼‰å½“æ¨¡å‹æƒé‡ä¿æŒä¸å˜ï¼Œè€Œå›¾ç»“æ„å‘ç”Ÿå˜åŒ–ï¼Œ THENode åµŒå…¥å¯ä»¥é€æ¸å‘å±•äºæ—¶é—´ã€‚åŸºäºè¿™ä¸¤ç‚¹è§è§£ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åä¸º InkStream çš„æ–°æ–¹æ³•ï¼Œç”¨äºå®æ—¶æ¨ç†ï¼Œå…·æœ‰æœ€å°çš„å†…å­˜è®¿é—®å’Œè®¡ç®—é‡ï¼ŒåŒæ—¶ä¿è¯è¾“å‡ºä¸ä¼ ç»Ÿæ–¹æ³•ç›¸åŒã€‚InkStream åŸºäºäº‹ä»¶é©±åŠ¨ç³»ç»Ÿï¼Œæ§åˆ¶äº†é—´å±‚æ•ˆåº”ä¼ æ’­å’Œå†…å±‚å¢é‡æ›´æ–°èŠ‚ç‚¹åµŒå…¥ã€‚InkStream é«˜åº¦å¯é…ç½®å’Œæ‰©å±•ï¼Œallowing users to create and process customized eventsã€‚</li>
<li>results: æˆ‘ä»¬åœ¨å››ä¸ªå¤§å›¾ä¸Šä½¿ç”¨ä¸‰ç§ GNN æ¨¡å‹è¿›è¡Œå®éªŒï¼Œæ˜¾ç¤º InkStream åœ¨ CPU é›†ç¾¤ä¸ŠåŠ é€Ÿäº† 2.5-427 å€ï¼Œåœ¨ä¸¤ä¸ªä¸åŒçš„ GPU é›†ç¾¤ä¸ŠåŠ é€Ÿäº† 2.4-343 å€ï¼Œè€Œä¸”è¾“å‡ºä¸ä¼ ç»Ÿæ–¹æ³•çš„æœ€æ–°å›¾å¿«ç…§ç›¸åŒã€‚<details>
<summary>Abstract</summary>
Classic Graph Neural Network (GNN) inference approaches, designed for static graphs, are ill-suited for streaming graphs that evolve with time. The dynamism intrinsic to streaming graphs necessitates constant updates, posing unique challenges to acceleration on GPU. We address these challenges based on two key insights: (1) Inside the $k$-hop neighborhood, a significant fraction of the nodes is not impacted by the modified edges when the model uses min or max as aggregation function; (2) When the model weights remain static while the graph structure changes, node embeddings can incrementally evolve over time by computing only the impacted part of the neighborhood. With these insights, we propose a novel method, InkStream, designed for real-time inference with minimal memory access and computation, while ensuring an identical output to conventional methods. InkStream operates on the principle of propagating and fetching data only when necessary. It uses an event-based system to control inter-layer effect propagation and intra-layer incremental updates of node embedding. InkStream is highly extensible and easily configurable by allowing users to create and process customized events. We showcase that less than 10 lines of additional user code are needed to support popular GNN models such as GCN, GraphSAGE, and GIN. Our experiments with three GNN models on four large graphs demonstrate that InkStream accelerates by 2.5-427$\times$ on a CPU cluster and 2.4-343$\times$ on two different GPU clusters while producing identical outputs as GNN model inference on the latest graph snapshot.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼ ç»Ÿçš„å›¾ neural network (GNN) æ¨ç†æ–¹æ³•ï¼Œè®¾è®¡ Ğ´Ğ»Ñé™æ­¢å›¾ï¼Œå¯¹æµåŠ¨å›¾æ¥è¯´æ˜¯ä¸é€‚ç”¨çš„ã€‚æµåŠ¨å›¾çš„åŠ¨æ€ç‰¹æ€§éœ€è¦ä¸æ–­æ›´æ–°ï¼Œè¿™ä¼šå¸¦æ¥ç‰¹æ®Šçš„åŠ é€ŸæŒ‘æˆ˜åœ¨ GPU ä¸Šã€‚æˆ‘ä»¬æ ¹æ®ä»¥ä¸‹ä¸¤ä¸ªå…³é”®å‘ç°ï¼šï¼ˆ1ï¼‰åœ¨ $k$-hop é‚»åŸŸå†…ï¼Œå¤§é‡èŠ‚ç‚¹ä¸ä¼šå—åˆ°æ”¹å˜çš„è¾¹å¯¹ GNN æ¨¡å‹è¿›è¡Œæ±‡èšæ—¶çš„å½±å“ï¼›ï¼ˆ2ï¼‰å½“æ¨¡å‹æƒé‡ä¿æŒä¸å˜è€Œå›¾ç»“æ„å‘ç”Ÿå˜åŒ–æ—¶ï¼ŒèŠ‚ç‚¹åµŒå…¥å¯ä»¥é€æ¸å‘å±•åœ¨æ—¶é—´ä¸Šï¼Œåªéœ€è®¡ç®—å½±å“çš„éƒ¨åˆ†é‚»åŸŸã€‚åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸º InkStreamï¼Œç”¨äºå®æ—¶æ¨ç†ï¼Œå…·æœ‰æœ€å°çš„å†…å­˜è®¿é—®å’Œè®¡ç®—é‡ï¼ŒåŒæ—¶ä¿è¯è¾“å‡ºå’Œæ™®é€šæ–¹æ³•ç›¸åŒã€‚InkStream è¿è¡Œåœ¨äº‹ä»¶é©±åŠ¨çš„ç³»ç»Ÿä¸Šï¼Œæ§åˆ¶é—´å±‚æ•ˆåº”ä¼ æ’­å’ŒINTRAå±‚å¢é‡æ›´æ–°èŠ‚ç‚¹åµŒå…¥ã€‚InkStream é«˜åº¦å¯ configurableï¼Œå¯ä»¥è®©ç”¨æˆ·åˆ›å»ºå’Œå¤„ç†è‡ªå®šä¹‰äº‹ä»¶ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ InkStream å¯ä»¥åœ¨ CPU é›†ç¾¤ä¸ŠåŠ é€Ÿ 2.5-427 å€ï¼Œåœ¨ä¸¤ä¸ªä¸åŒçš„ GPU é›†ç¾¤ä¸ŠåŠ é€Ÿ 2.4-343 å€ï¼Œè€Œä¸”è¾“å‡ºå’Œæ™®é€šæ–¹æ³•ç›¸åŒã€‚
</details></li>
</ul>
<hr>
<h2 id="Extreme-Scenario-Selection-in-Day-Ahead-Power-Grid-Operational-Planning"><a href="#Extreme-Scenario-Selection-in-Day-Ahead-Power-Grid-Operational-Planning" class="headerlink" title="Extreme Scenario Selection in Day-Ahead Power Grid Operational Planning"></a>Extreme Scenario Selection in Day-Ahead Power Grid Operational Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11067">http://arxiv.org/abs/2309.11067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo TerrÃ©n-Serrano, Michael Ludkovski</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ä¸ºçŸ­æœŸç”µç½‘è§„åˆ’é€‰æ‹©æç«¯æƒ…å†µï¼Œä»¥é™ä½è¿è¥é£é™©ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨ç»Ÿè®¡å‡½æ•°æ·±åº¦æŒ‡æ ‡æ¥ç­›é€‰æç«¯æƒ…å†µï¼Œä»¥ç¡®å®šæœ€æœ‰å¯èƒ½å¯¼è‡´ç½‘ç»œè¿è¥é£é™©çš„æƒ…å†µã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨ç»Ÿè®¡å‡½æ•°æ·±åº¦æŒ‡æ ‡å¯ä»¥æœ‰æ•ˆåœ°ç­›é€‰å‡ºé«˜é£é™©æƒ…å†µï¼Œå¹¶ä¸”å¯ä»¥é¢„æµ‹load sheddingã€è¿è¥æˆæœ¬ã€å‚¨å¤‡çŸ­ç¼ºå’Œå¯å˜èƒ½æºç”µåœæœºç­‰æ“ä½œé£é™©ã€‚<details>
<summary>Abstract</summary>
We propose and analyze the application of statistical functional depth metrics for the selection of extreme scenarios in day-ahead grid planning. Our primary motivation is screening of probabilistic scenarios for realized load and renewable generation, in order to identify scenarios most relevant for operational risk mitigation. To handle the high-dimensionality of the scenarios across asset classes and intra-day periods, we employ functional measures of depth to sub-select outlying scenarios that are most likely to be the riskiest for the grid operation. We investigate a range of functional depth measures, as well as a range of operational risks, including load shedding, operational costs, reserves shortfall and variable renewable energy curtailment. The effectiveness of the proposed screening approach is demonstrated through a case study on the realistic Texas-7k grid.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºå’Œåˆ†æä½¿ç”¨ç»Ÿè®¡å‡½æ•°æ·±åº¦æŒ‡æ ‡æ¥é€‰æ‹©EXTREMEåœºæ™¯åœ¨ä¸€å¤©å‰ç”µç½‘è§„åˆ’ä¸­ã€‚æˆ‘ä»¬çš„ PRIMARY motivationæ˜¯å¯¹ probabilistic scenario è¿›è¡Œå±é€‰ï¼Œä»¥ä¾¿ identific scenarios å¯¹ç”µç½‘æ“ä½œé£é™©æœ€å¤§åŒ–ã€‚ä¸ºäº†å¤„ç†ä¸åŒèµ„äº§ç±»å’Œæ—¶é—´æ®µä¹‹é—´çš„é«˜ç»´åº¦åœºæ™¯ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡½æ•°æŒ‡æ ‡æ¥å­é€‰æ‹©å¼‚å¸¸åœºæ™¯ï¼Œä»¥ä¾¿æ›´å¥½åœ°äº†è§£ç”µç½‘æ“ä½œé£é™©ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†ä¸€ç³»åˆ—å‡½æ•°æ·±åº¦æŒ‡æ ‡ï¼Œä»¥åŠä¸€ç³»åˆ—æ“ä½œé£é™©ï¼ŒåŒ…æ‹¬è· sheddingã€æ“ä½œæˆæœ¬ã€å‚¨å¤‡çŸ­ç¼ºå’Œå¯å˜å¯å†ç”Ÿèƒ½æºå‰Šå‡ã€‚æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶åŸºäºçœŸå®çš„ Texas-7k ç”µç½‘ã€‚Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Containing-Analog-Data-Deluge-at-Edge-through-Frequency-Domain-Compression-in-Collaborative-Compute-in-Memory-Networks"><a href="#Containing-Analog-Data-Deluge-at-Edge-through-Frequency-Domain-Compression-in-Collaborative-Compute-in-Memory-Networks" class="headerlink" title="Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks"></a>Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11048">http://arxiv.org/abs/2309.11048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nastaran Darabi, Amit R. Trivedi</li>
<li>for: This paper aims to improve area efficiency in deep learning inference tasks for edge computing applications, specifically addressing the challenges of limited storage and computing resources in edge devices.</li>
<li>methods: The proposed method employs two key strategies: (1) Frequency domain learning using binarized Walsh-Hadamard Transforms, which reduces the necessary parameters for DNN and enables compute-in-SRAM, and (2) a memory-immersed collaborative digitization method among CiM arrays to reduce the area overheads of conventional ADCs.</li>
<li>results: The proposed method achieves significant area and energy savings compared to a 40 nm-node 5-bit SAR ADC and 5-bit Flash ADC, as demonstrated using a 65 nm CMOS test chip. The results show that it is possible to process analog data more efficiently and selectively retain valuable data from sensors, alleviating the challenges posed by the analog data deluge.Hereâ€™s the Chinese version of the three key information points:</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æé«˜è¾¹ç¼˜è®¡ç®—åº”ç”¨ä¸­æ·±åº¦å­¦ä¹ æ¨ç†ä»»åŠ¡çš„é¢ç§¯æ•ˆç‡ï¼Œå…·ä½“æ˜¯è§£å†³è¾¹ç¼˜è®¾å¤‡çš„å­˜å‚¨å’Œè®¡ç®—èµ„æºå—é™é—®é¢˜ã€‚</li>
<li>methods: æè®®çš„æ–¹æ³•é‡‡ç”¨äº†ä¸¤ç§å…³é”®ç­–ç•¥ï¼šï¼ˆ1ï¼‰é¢‘åŸŸå­¦ä¹ ä½¿ç”¨äºŒè¿›åˆ¶æ²ƒå°”ä»€-å“ˆè¾¾å§†å˜æ¢ï¼Œå‡å°‘æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡ï¼ˆMobileNetV2ä¸­å‡å°‘87%ï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨è®¡ç®—åœ¨SRAMä¸­è¿›è¡Œè®¡ç®—ï¼Œæ›´å¥½åœ°åˆ©ç”¨å¹¶è¡Œæ€§ï¼›ï¼ˆ2ï¼‰ä½¿ç”¨Memory-immersed collaborative digitizationæ–¹æ³•ï¼Œå°† CiM æ•°ç»„ä¸å­˜å‚¨å™¨é›†æˆï¼Œä»¥é™ä½ä¼ ç»ŸADCçš„é¢ç§¯å¼€é”€ã€‚</li>
<li>results: æ ¹æ®65nmCMOSæµ‹è¯•æ¿è¡¨ç°ï¼Œæè®®çš„æ–¹æ³•å¯ä»¥å®ç°æ˜¾è‘—çš„é¢ç§¯å’Œèƒ½è€—å‡å°‘ï¼Œä¸40nmèŠ‚ç‚¹5ä½SAR ADCå’Œ5ä½Flash ADCç›¸æ¯”ã€‚é€šè¿‡æ›´æœ‰æ•ˆåœ°å¤„ç†åˆ†ææ•°æ®ï¼Œå¯ä»¥é€‰æ‹©æ€§åœ°ä¿ç•™æ„ŸçŸ¥å™¨ä¸­çš„æœ‰ä»·å€¼æ•°æ®ï¼Œä»è€Œè§£å†³åˆ†ææ•°æ®æ³›æ´ªçš„é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Edge computing is a promising solution for handling high-dimensional, multispectral analog data from sensors and IoT devices for applications such as autonomous drones. However, edge devices' limited storage and computing resources make it challenging to perform complex predictive modeling at the edge. Compute-in-memory (CiM) has emerged as a principal paradigm to minimize energy for deep learning-based inference at the edge. Nevertheless, integrating storage and processing complicates memory cells and/or memory peripherals, essentially trading off area efficiency for energy efficiency. This paper proposes a novel solution to improve area efficiency in deep learning inference tasks. The proposed method employs two key strategies. Firstly, a Frequency domain learning approach uses binarized Walsh-Hadamard Transforms, reducing the necessary parameters for DNN (by 87% in MobileNetV2) and enabling compute-in-SRAM, which better utilizes parallelism during inference. Secondly, a memory-immersed collaborative digitization method is described among CiM arrays to reduce the area overheads of conventional ADCs. This facilitates more CiM arrays in limited footprint designs, leading to better parallelism and reduced external memory accesses. Different networking configurations are explored, where Flash, SA, and their hybrid digitization steps can be implemented using the memory-immersed scheme. The results are demonstrated using a 65 nm CMOS test chip, exhibiting significant area and energy savings compared to a 40 nm-node 5-bit SAR ADC and 5-bit Flash ADC. By processing analog data more efficiently, it is possible to selectively retain valuable data from sensors and alleviate the challenges posed by the analog data deluge.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œè¾¹ç¼˜è®¡ç®—æ˜¯ä¸€ç§å…·æœ‰åº”ç”¨å‰æ™¯çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºå¤„ç†å…·æœ‰é«˜ç»´åº¦å’Œå¤š ÑĞ¿ĞµĞºÑ‚ãƒ«çš„æ•°æ®æµç•…çš„æ„Ÿåº”å™¨å’ŒIoTè®¾å¤‡ã€‚ç„¶è€Œï¼Œè¾¹ç¼˜è®¾å¤‡çš„å‚¨å­˜å’Œå¤„ç†èµ„æºæœ‰é™ï¼Œå¯¼è‡´å¤æ‚çš„é¢„æµ‹æ¨¡å‹åœ¨è¾¹ç¼˜è¿›è¡Œå®é™…é—®é¢˜ã€‚compute-in-memoryï¼ˆCiMï¼‰æŠ€æœ¯å·²ç»æˆä¸ºä¸€ç§ä¸»è¦çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥é™ä½è¿ç®—çš„èƒ½æºæ¶ˆè€—ã€‚ç„¶è€Œï¼Œå°†å‚¨å­˜å’Œå¤„ç†å¤æ‚åŒ–çš„å†…å­˜ç»†èŠ‚å’Œ/æˆ–å†…å­˜å‘¨è¾¹è®¾å¤‡ï¼Œå®é™…ä¸Šæ˜¯å°†é¢ç§¯æ•ˆç‡ä¸èƒ½æºæ•ˆç‡è¿›è¡Œäº¤æ¢ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„è§£å†³æ–¹æ¡ˆï¼Œä»¥æ”¹å–„è¾¹ç¼˜è¿ç®—ä¸­çš„é¢ç§¯æ•ˆç‡ã€‚æœ¬æ–¹æ³•ä½¿ç”¨äº†ä¸¤ä¸ªå…³é”®ç­–ç•¥ï¼šé¦–å…ˆï¼Œä½¿ç”¨é¢‘ç‡åŸŸå­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å°†æ•°æ®å‹ç¼©ä¸ºäºŒè¿›åˆ¶æ•°æ®ï¼Œå¹¶ä½¿ç”¨å¯¹ç§°çš„åæ°-å“ˆè¾¾ç›ç‰¹è½¬æ¢ï¼Œä»¥é™ä½è¿ç®—æ‰€éœ€çš„å‚æ•°æ•°é‡ï¼ˆMobileNetV2ä¸­é™ä½87%ï¼‰ï¼Œå¹¶å…è®¸åœ¨æ‰§è¡Œè¿ç®—æ—¶ä½¿ç”¨SRAMè¿›è¡Œè®¡ç®—ã€‚å…¶æ¬¡ï¼Œæè¿°äº†ä¸€ç§å†…å­˜åµŒå…¥å¼åˆä½œæ•°å­—åŒ–æ–¹æ³•ï¼Œç”¨äºå®ç° CiM é˜µåˆ—ä¸­çš„å†…å­˜ä¸ADCä¹‹é—´çš„è”ç³»ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æœ‰é™çš„é¢ç§¯è®¾è®¡ä¸­æ”¯æŒæ›´å¤šçš„ CiM é˜µåˆ—ï¼Œå®ç°æ›´å¥½çš„å¹¶è¡Œæ€§å’Œå¯¹å¤–å­˜å‚¨å™¨çš„å‡å°‘ã€‚ä¸åŒçš„ç½‘ç»œé…ç½®è¢«æ¢è®¨ï¼ŒåŒ…æ‹¬ Flashã€SA å’Œå®ƒä»¬çš„æ··åˆå¼æ•°å­—åŒ–æ­¥éª¤ã€‚ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨æœ¬æ–¹æ³•å¯ä»¥åœ¨65å¥ˆç±³CMOSè¯•éªŒæ¿ä¸Šå±•ç¤ºå‡ºå…·æœ‰æ˜æ˜¾çš„é¢ç§¯å’Œèƒ½æºä¼˜åŒ–çš„åŠŸèƒ½ã€‚é€šè¿‡æ›´æœ‰æ•ˆåœ°å¤„ç†æ•°æ®ï¼Œå¯ä»¥å¯¹æ„Ÿåº”å™¨ä¸­çš„æœ‰ç”¨æ•°æ®è¿›è¡Œé€‰æ‹©æ€§å‚¨å­˜ï¼Œä»è€Œç¼“è§£æ„Ÿåº”å™¨ä¸­çš„æ•°æ®æ½®æ±é—®é¢˜ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="A-Region-Shrinking-Based-Acceleration-for-Classification-Based-Derivative-Free-Optimization"><a href="#A-Region-Shrinking-Based-Acceleration-for-Classification-Based-Derivative-Free-Optimization" class="headerlink" title="A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization"></a>A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11036">http://arxiv.org/abs/2309.11036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦å…³æ³¨äºç§‘å­¦å’Œå·¥ç¨‹è®¾è®¡ä¼˜åŒ–é—®é¢˜ä¸­çš„æ¢¯åº¦ä¸å¯çŸ¥åˆ†å¸ƒå¼ä¼˜åŒ–ç®—æ³•çš„æ¡†æ¶ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åˆ†ç±»åŸºäº derivative-free ä¼˜åŒ–ç®—æ³•ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªå«åšå‡è®¾ç›®æ ‡éš”ç¦»ç‡çš„æ¦‚å¿µï¼Œä»¥æ›´æ–°è¿™ç±»ç®—æ³•çš„è®¡ç®—å¤æ‚æ€§Upper boundã€‚</li>
<li>results: æ ¹æ®å®éªŒç»“æœï¼Œæ–°æå‡ºçš„ â€œRACE-CARSâ€ ç®—æ³•æ¯” traditional â€œSRACOSâ€ æ›´å¿«ï¼Œå¹¶ä¸”å¯¹é»‘ç›’ä¼˜åŒ–å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è¯­è¨€æ¨¡å‹æœåŠ¡è¿›è¡Œäº†å®è¯éªŒè¯ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è¿›è¡Œäº†ä¸€ä¸ªablation experimentï¼Œæ¢è®¨äº† â€œRACE-CARSâ€ çš„æœºåˆ¶å’Œå‚æ•°ä¼˜åŒ–çš„æŒ‡å¯¼ã€‚<details>
<summary>Abstract</summary>
Derivative-free optimization algorithms play an important role in scientific and engineering design optimization problems, especially when derivative information is not accessible. In this paper, we study the framework of classification-based derivative-free optimization algorithms. By introducing a concept called hypothesis-target shattering rate, we revisit the computational complexity upper bound of this type of algorithms. Inspired by the revisited upper bound, we propose an algorithm named "RACE-CARS", which adds a random region-shrinking step compared with "SRACOS" (Hu et al., 2017).. We further establish a theorem showing the acceleration of region-shrinking. Experiments on the synthetic functions as well as black-box tuning for language-model-as-a-service demonstrate empirically the efficiency of "RACE-CARS". An ablation experiment on the introduced hyperparameters is also conducted, revealing the mechanism of "RACE-CARS" and putting forward an empirical hyperparameter-tuning guidance.
</details>
<details>
<summary>æ‘˜è¦</summary>
derivative-free ä¼˜åŒ–ç®—æ³•åœ¨ç§‘å­¦å’Œå·¥ç¨‹è®¾è®¡ä¼˜åŒ–é—®é¢˜ä¸­æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ï¼Œå°¤å…¶æ˜¯å½“ derivate ä¿¡æ¯ä¸å¯è·å–æ—¶ã€‚æœ¬æ–‡ç ”ç©¶äº†ç±»åˆ«åŸºäºçš„ derivative-free ä¼˜åŒ–ç®—æ³•æ¡†æ¶ã€‚é€šè¿‡å¼•å…¥å‡è®¾ç›®æ ‡éœ‡è¡ç‡ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†è¿™ç±»ç®—æ³•çš„è®¡ç®—å¤æ‚æ€§Upper boundã€‚ inspirited ç”± revisited Upper boundï¼Œæˆ‘ä»¬æå‡ºäº†åä¸º "RACE-CARS" çš„ç®—æ³•ï¼Œå®ƒåœ¨ "SRACOS" ï¼ˆHu et al., 2017ï¼‰ä¸­æ·»åŠ äº†éšæœºåŒºåŸŸç¼©å°æ­¥éª¤ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†åŒºåŸŸç¼©å°çš„åŠ é€Ÿã€‚å¯¹äº synthetic å‡½æ•°ä»¥åŠé»‘ç›’è°ƒå‚è¯­è¨€æ¨¡å‹æœåŠ¡ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜äº† "RACE-CARS" çš„æ•ˆç‡ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€ä¸ªablation experiment å¯¹å¼•å…¥çš„è¶…å‚æ•°ï¼Œæ¢è®¨äº† "RACE-CARS" çš„æœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªempirical è¶…å‚æ•°è°ƒæ•´æŒ‡å—ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Topology-and-Geometry-of-Neural-Representations"><a href="#The-Topology-and-Geometry-of-Neural-Representations" class="headerlink" title="The Topology and Geometry of Neural Representations"></a>The Topology and Geometry of Neural Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11028">http://arxiv.org/abs/2309.11028</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neurreps/awesome-neural-geometry">https://github.com/neurreps/awesome-neural-geometry</a></li>
<li>paper_authors: Baihan Lin, Nikolaus Kriegeskorte</li>
<li>for: è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯Characterize brain representations of perceptual and cognitive content, and distinguish different functional regions with robustness to noise and individual differences.</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº† topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics to characterize the topology of brain representations while de-emphasizing the geometry.</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨è¿™ç§æ–°çš„ç»Ÿè®¡æ–¹æ³•å¯ä»¥robust to noise and interindividual variability, and maintain excellent sensitivity to the unique representational signatures of different neural network layers and brain regions.<details>
<summary>Abstract</summary>
A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. We evaluate this new family of statistics in terms of the sensitivity and specificity for model selection using both simulations and functional MRI (fMRI) data. In the simulations, the ground truth is a data-generating layer representation in a neural network model and the models are the same and other layers in different model instances (trained from different random seeds). In fMRI, the ground truth is a visual area and the models are the same and other areas measured in different subjects. Results show that topology-sensitive characterizations of population codes are robust to noise and interindividual variability and maintain excellent sensitivity to the unique representational signatures of different neural network layers and brain regions.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸­æ–‡ç¿»è¯‘ï¼š neuroscience ä¸­çš„ä¸€ä¸ªä¸­å¿ƒé—®é¢˜æ˜¯å¦‚ä½• caracterize å¤§è„‘è¡¨å¾çš„æ„ŸçŸ¥å’Œè®¤çŸ¥å†…å®¹ã€‚ç†æƒ³çš„ caracterization åº”è¯¥èƒ½å¤Ÿåˆ†è¾¨ä¸åŒçš„åŠŸèƒ½åŒºåŸŸï¼Œå¹¶å…·æœ‰å¯¹å™ªå£°å’Œä¸ªä½“å¤§è„‘å·®å¼‚çš„æŠ—é¢¤æ€§ã€‚å‰ä¸€äº›ç ”ç©¶å·²ç»ä½¿ç”¨ representational geometry æ¥ caracterize å¤§è„‘è¡¨å¾ï¼Œå…¶å®šä¹‰ä¸ºå„ä¸ª neuron æˆ– response channel çš„è¡¨å¾å·®å¼‚çŸ©é˜µ (RDM)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ç»Ÿè®¡é‡ï¼ŒæŠ‘åˆ¶äº†ä¸ªä½“å¤§è„‘å·®å¼‚çš„è®¡ç®—ä¸åŒã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ explore ä¸€ä¸ªè¿›ä¸€æ­¥çš„æŠ½è±¡æ­¥éª¤ï¼šä» geometry åˆ°å¤§è„‘è¡¨å¾çš„ topologyã€‚æˆ‘ä»¬æå‡º topological representational similarity analysis (tRSA)ï¼Œè¿™æ˜¯ representational similarity analysis (RSA) çš„æ‰©å±•ï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºåœ°ç† topological æ‘˜è¦ç»Ÿè®¡é‡ï¼Œè¿™ä¸ªç»Ÿè®¡é‡æŠ‘åˆ¶äº† geometry çš„å½±å“ï¼Œä¸“æ³¨äºè¡¨å¾çš„ topologyã€‚æˆ‘ä»¬ä½¿ç”¨ simulate å’Œ functional MRI (fMRI) æ•°æ®æ¥è¯„ä¼°è¿™ç§æ–°çš„å®¶æ—ç»Ÿè®¡é‡çš„æ•æ„Ÿæ€§å’Œç‰¹ç‚¹ã€‚åœ¨ simulate ä¸­ï¼Œground truth æ˜¯ä¸€ä¸ªæ•°æ®ç”Ÿæˆå±‚è¡¨ç¤ºï¼Œæ¨¡å‹æ˜¯ä¸åŒçš„ random seed ç”Ÿæˆçš„ä¸åŒå±‚æ¨¡å‹å®ä¾‹ã€‚åœ¨ fMRI ä¸­ï¼Œground truth æ˜¯ä¸€ä¸ªè§†è§‰åŒºåŸŸï¼Œæ¨¡å‹æ˜¯ä¸åŒçš„è§†è§‰åŒºåŸŸå’Œä¸åŒçš„ä¸»ä½“ measured çš„ä¸åŒä¸»ä½“ã€‚ç»“æœæ˜¾ç¤ºï¼ŒåŸºäº topology çš„äººç±»ä»£è¡¨ç  caracterization æ˜¯å™ªå£°å’Œä¸ªä½“å·®å¼‚çš„æŠ—é¢¤æ€§ï¼Œå¹¶ä¿æŒäº†å¯¹ä¸åŒ neural network å±‚å’Œå¤§è„‘åŒºåŸŸçš„å”¯ä¸€è¡¨å¾ç­¾åçš„æ•æ„Ÿæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Information-Leakage-from-Data-Updates-in-Machine-Learning-Models"><a href="#Information-Leakage-from-Data-Updates-in-Machine-Learning-Models" class="headerlink" title="Information Leakage from Data Updates in Machine Learning Models"></a>Information Leakage from Data Updates in Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11022">http://arxiv.org/abs/2309.11022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Tian Hui, Farhad Farokhi, Olga Ohrimenko</li>
<li>for: æœ¬ç ”ç©¶è€ƒè™‘åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ retrained åœ¨æ›´æ–°æ•°æ®é›†ä¸Šä»¥ incorporate æœ€æ–°ä¿¡æ¯æˆ–åæ˜ åˆ†å¸ƒå˜åŒ–ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†åŸºäºæ¨¡å‹Prediction confidenceå·®å¼‚çš„æ”»å‡»æ–¹æ³•ï¼Œå¹¶å¯¹ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä»¥åŠå¤šå±‚æ„ŸçŸ¥å™¨å’ŒLogistic regressionæ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹Snapshotå¯ä»¥å¯¼è‡´æ›´é«˜çš„ä¿¡æ¯æ³„éœ²ï¼Œè€Œä¸”æ•°æ®è®°å½• WITH rare attribute value æ›´å®¹æ˜“å—åˆ°æ”»å‡»ã€‚ repeated changes å¯èƒ½ä¼šå¸¦æ¥æ›´å¤§çš„æ³„éœ²ã€‚<details>
<summary>Abstract</summary>
In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the updated model. Moreover, we observe that data records with rare values are more vulnerable to attacks, which points to the disparate vulnerability of privacy attacks in the update setting. When multiple records with the same original attribute value are updated to the same new value (i.e., repeated changes), the attacker is more likely to correctly guess the updated values since repeated changes leave a larger footprint on the trained model. These observations point to vulnerability of machine learning models to attribute inference attacks in the update setting.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ›´æ–°æ•°æ®é›†åé‡æ–°è®­ç»ƒä»¥åŒ…å«æœ€æ–°çš„ä¿¡æ¯æˆ–åæ˜ åˆ†å¸ƒå˜åŒ–ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ˜¯å¦å¯ä»¥ä»è®­ç»ƒæ•°æ®ä¸­æ¨æ–­æ›´æ–°ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œè®°å½•å±æ€§å€¼çš„æ›´æ”¹ï¼‰ã€‚åœ¨è¿™ä¸ªè®¾å®šä¸‹ï¼Œæ•Œæ–¹å¯ä»¥è®¿é—®æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¸¤ä¸ªå¿«ç…§ï¼Œå³ä¹‹å‰å’Œä¹‹åæ›´æ”¹æ•°æ®é›†å‘ç”Ÿã€‚ä¸åŒäºç°æœ‰æ–‡çŒ®ï¼Œæˆ‘ä»¬å‡è®¾å•ä¸ªæˆ–å¤šä¸ªè®­ç»ƒæ•°æ®ç‚¹çš„å±æ€§å‘ç”Ÿå˜åŒ–è€Œä¸æ˜¯æ•´ä¸ªæ•°æ®è®°å½•è¢«åˆ é™¤æˆ–æ·»åŠ ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºåŸå§‹æ¨¡å‹å’Œæ›´æ–°æ¨¡å‹é¢„æµ‹ä¿¡ä»»åº¦å·®å¼‚çš„æ”»å‡»æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå…¬å…±æ•°æ®é›†ä»¥åŠå¤šå±‚æ„ŸçŸ¥å’ŒæŠ˜è¡”å‡½æ•°æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬å‘ç°äº†ä¸¤ä¸ªå¿«ç…§çš„æ¨¡å‹å¯ä»¥å¯¼è‡´æ›´é«˜çš„ä¿¡æ¯æ³„éœ²ï¼Œè€Œä¸”æ•°æ®è®°å½•ä¸­çš„ç½•è§å€¼æ›´å®¹æ˜“å—åˆ°æ”»å‡»ï¼Œè¿™æŒ‡å‡ºäº†æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ›´æ–°è®¾å®šä¸‹çš„æ•æ„Ÿåº¦é—®é¢˜ã€‚å½“å¤šä¸ªè®°å½•ä¸­çš„åŸå§‹å±æ€§å€¼éƒ½æ›´æ–°ä¸ºåŒä¸€ä¸ªæ–°å€¼æ—¶ï¼ˆå³é‡å¤æ›´æ”¹ï¼‰ï¼Œæ”»å‡»è€…æ›´å¯èƒ½æ­£ç¡®åœ°çŒœæµ‹æ›´æ–°å€¼ï¼Œå› ä¸ºé‡å¤æ›´æ”¹ä¼šç•™ä¸‹æ›´å¤§çš„æ¨¡å‹è®­ç»ƒä¸­çš„å°è®°ã€‚è¿™äº›è§‚å¯Ÿè¡¨æ˜äº†æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨æ›´æ–°è®¾å®šä¸‹é¢ä¸´çš„å±æ€§æ¨æ–­æ”»å‡»ã€‚
</details></li>
</ul>
<hr>
<h2 id="3D-U-SAM-Network-For-Few-shot-Tooth-Segmentation-in-CBCT-Images"><a href="#3D-U-SAM-Network-For-Few-shot-Tooth-Segmentation-in-CBCT-Images" class="headerlink" title="3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images"></a>3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11015">http://arxiv.org/abs/2309.11015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifu Zhang, Zuozhu Liu, Yang Feng, Renjing Xu</li>
<li>for: è§£å†³3D dentalå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­çš„æ ·æœ¬æ•°é‡å¤ªå°‘é—®é¢˜ï¼Œæå‡ºä¸€ç§åŸºäºSAMé¢„è®­ç»ƒç½‘ç»œçš„3D-U-SAMç½‘ç»œã€‚</li>
<li>methods: é‡‡ç”¨äº†ä¸€ç§æ ¸å¿ƒæŠ½è±¡æ–¹æ³•ï¼Œå¹¶åœ¨U-Netç½‘ç»œä¸­è®¾è®¡äº†è·³è·ƒè¿æ¥ï¼Œä»¥ä¿ç•™æ›´å¤šçš„ç»†èŠ‚ä¿¡æ¯ã€‚</li>
<li>results: é€šè¿‡æ¯”è¾ƒå’Œé‡‡æ ·å¤§å°å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•å¯ä»¥æ›´å¥½åœ°è§£å†³3D dentalå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
Accurate representation of tooth position is extremely important in treatment. 3D dental image segmentation is a widely used method, however labelled 3D dental datasets are a scarce resource, leading to the problem of small samples that this task faces in many cases. To this end, we address this problem with a pretrained SAM and propose a novel 3D-U-SAM network for 3D dental image segmentation. Specifically, in order to solve the problem of using 2D pre-trained weights on 3D datasets, we adopted a convolution approximation method; in order to retain more details, we designed skip connections to fuse features at all levels with reference to U-Net. The effectiveness of the proposed method is demonstrated in ablation experiments, comparison experiments, and sample size experiments.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¾ˆé‡è¦çš„æ˜¯ç²¾ç¡®åœ°è¡¨ç¤ºç‰™é½¿çš„ä½ç½®åœ¨æ²»ç–—ä¸­ã€‚3D dentalå›¾åƒåˆ†å‰²æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•ï¼Œä½†æ ‡æ³¨çš„3D dentalæ•°æ®é›†æ˜¯ä¸€ç§ç½•è§çš„èµ„æºï¼Œå¯¼è‡´è¿™ä¸ªä»»åŠ¡åœ¨è®¸å¤šæƒ…å†µä¸‹é¢ä¸´ç€å°æ ·æœ¬é—®é¢˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒçš„SAMå¹¶æè®®ä¸€ç§3D-U-SAMç½‘ç»œ Ğ´Ğ»Ñ3D dentalå›¾åƒåˆ†å‰²ã€‚å…·ä½“æ¥è¯´ï¼Œä¸ºäº†è§£å†³ä½¿ç”¨2Dé¢„è®­ç»ƒ Ğ²ĞµÑĞ°åœ¨3Dæ•°æ®é›†ä¸Šçš„é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ ¸å¿ƒapproximationæ–¹æ³•ï¼›ä¸ºäº†ä¿ç•™æ›´å¤šçš„ç»†èŠ‚ï¼Œæˆ‘ä»¬è®¾è®¡äº†è·³è½¬è¿æ¥ï¼Œä»¥èåˆæ‰€æœ‰å±‚çš„ç‰¹å¾å‚ç…§U-Netã€‚æˆ‘ä»¬çš„æè®®æ–¹æ³•çš„æ•ˆæœåœ¨ablationå®éªŒã€æ¯”è¾ƒå®éªŒå’Œæ ·æœ¬å¤§å°å®éªŒä¸­å¾—åˆ°äº†è¯æ˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Itâ€™s-Simplex-Disaggregating-Measures-to-Improve-Certified-Robustness"><a href="#Itâ€™s-Simplex-Disaggregating-Measures-to-Improve-Certified-Robustness" class="headerlink" title="Itâ€™s Simplex! Disaggregating Measures to Improve Certified Robustness"></a>Itâ€™s Simplex! Disaggregating Measures to Improve Certified Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11005">http://arxiv.org/abs/2309.11005</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew C. Cullen, Paul Montague, Shijie Liu, Sarah M. Erfani, Benjamin I. P. Rubinstein</li>
<li>for: é˜²å¾¡æ”»å‡»çš„éš¾ç‚¹æ˜¯æ¨¡å‹é¢„æµ‹çš„ä¸å¯é æ€§ï¼Œè¿™ç§ç ”ç©¶ç”¨äº†è¯æ˜æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ï¼Œä»¥æé«˜æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>methods: è¿™ç§ç ”ç©¶ä½¿ç”¨äº†è¯æ˜æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ï¼Œé€šè¿‡è®¡ç®—æ”»å‡»å¤§å°æ¥ä¿è¯æ¨¡å‹é¢„æµ‹çš„å¯é æ€§ã€‚</li>
<li>results: è¿™ç§ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è€ƒè™‘å¯é æ€§è¯æ˜çš„è¾“å‡ºç©ºé—´ï¼Œå¯ä»¥æé«˜è¯æ˜æœºåˆ¶çš„åˆ†æï¼Œå¹¶ä¸”å¯ä»¥è¶…è¿‡ç°æœ‰çŠ¶æ€çš„è¯æ˜èŒƒå›´ã€‚å®éªŒè¯æ˜ï¼Œæ–°çš„è¯æ˜æ–¹æ³•å¯ä»¥åœ¨å™ªå£°ç‡ä¸º1æ—¶è¯æ˜9%æ›´å¤šçš„æ ·æœ¬ï¼Œå¹¶ä¸”åœ¨é¢„æµ‹ä»»åŠ¡çš„éš¾åº¦å¢åŠ æ—¶ï¼ŒRelative improvementæ›´å¤§ã€‚<details>
<summary>Abstract</summary>
Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\%$ more samples at noise scale $\sigma = 1$, with greater relative improvements observed as the difficulty of the predictive task increases.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šè®¤è¯ç±»å¼¹æ€§è¶…è¶Šé˜²å¾¡æ”»å‡»çš„è„†å¼±æ€§ï¼Œé€šè¿‡å°†æ¨¡å‹é¢„æµ‹ garantuee ä¸ºæ”»å‡»è§„æ¨¡å†…çš„ç±»å‹ä¸å˜ï¼Œä»è€Œç¡®ä¿æ¨¡å‹åœ¨æ”»å‡»ä¸‹çš„é¢„æµ‹ç¨³å®šæ€§ã€‚ although there is value in these certifications, the techniques used to assess their performance do not provide a comprehensive account of their strengths and weaknesses, as they have neglected to consider the performance of individual samples. by considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, which allow for both dataset-independent and dataset-dependent measures of certification performance. embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. empirical evaluation verifies that our new approach can certify $9\%$ more samples at noise scale $\sigma = 1$, with greater relative improvements observed as the difficulty of the predictive task increases.ã€‹
</details></li>
</ul>
<hr>
<h2 id="Towards-Data-centric-Graph-Machine-Learning-Review-and-Outlook"><a href="#Towards-Data-centric-Graph-Machine-Learning-Review-and-Outlook" class="headerlink" title="Towards Data-centric Graph Machine Learning: Review and Outlook"></a>Towards Data-centric Graph Machine Learning: Review and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10979">http://arxiv.org/abs/2309.10979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Zheng, Yixin Liu, Zhifeng Bao, Meng Fang, Xia Hu, Alan Wee-Chung Liew, Shirui Pan<br>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦å…³æ³¨æ•°æ®é©±åŠ¨AIçš„å‘å±•ï¼Œå°¤å…¶æ˜¯Graphæ•°æ®ç»“æ„çš„åº”ç”¨ã€‚methods: è®ºæ–‡æå‡ºäº†ä¸€ç§ç³»ç»ŸåŒ–æ¡†æ¶ï¼Œåä¸ºData-centric Graph Machine Learningï¼ˆDC-GMLï¼‰ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬Graphæ•°æ®ç”Ÿå‘½å‘¨æœŸä¸­çš„æ‰€æœ‰é˜¶æ®µï¼ŒåŒ…æ‹¬æ•°æ®æ”¶é›†ã€æ¢ç´¢ã€æ”¹è¿›ã€åˆ©ç”¨å’Œç»´æŠ¤ã€‚results: è®ºæ–‡æä¾›äº†ä¸€ä»½å®Œæ•´çš„taxonomyï¼Œç”¨äºå›ç­”ä¸‰ä¸ªå…³é”®çš„Graphæ•°æ®ä¸­å¿ƒé—®é¢˜ï¼š1ï¼‰å¦‚ä½•æé«˜Graphæ•°æ®çš„å¯ç”¨æ€§å’Œè´¨é‡ï¼›2ï¼‰å¦‚ä½•ä»é™é‡å¯ç”¨å’Œä½è´¨é‡çš„Graphæ•°æ®ä¸­å­¦ä¹ ï¼›3ï¼‰å¦‚ä½•å»ºç«‹åŸºäºGraphæ•°æ®çš„Machine Learningæ“ä½œç³»ç»Ÿã€‚<details>
<summary>Abstract</summary>
Data-centric AI, with its primary focus on the collection, management, and utilization of data to drive AI models and applications, has attracted increasing attention in recent years. In this article, we conduct an in-depth and comprehensive review, offering a forward-looking outlook on the current efforts in data-centric AI pertaining to graph data-the fundamental data structure for representing and capturing intricate dependencies among massive and diverse real-life entities. We introduce a systematic framework, Data-centric Graph Machine Learning (DC-GML), that encompasses all stages of the graph data lifecycle, including graph data collection, exploration, improvement, exploitation, and maintenance. A thorough taxonomy of each stage is presented to answer three critical graph-centric questions: (1) how to enhance graph data availability and quality; (2) how to learn from graph data with limited-availability and low-quality; (3) how to build graph MLOps systems from the graph data-centric view. Lastly, we pinpoint the future prospects of the DC-GML domain, providing insights to navigate its advancements and applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ•°æ®é©±åŠ¨AIâ€åœ¨æœ€è¿‘å‡ å¹´å†…å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå®ƒçš„æ ¸å¿ƒæ˜¯é€šè¿‡æ”¶é›†ã€ç®¡ç†å’Œåˆ©ç”¨æ•°æ®é©±åŠ¨AIæ¨¡å‹å’Œåº”ç”¨ç¨‹åºã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ·±å…¥å’Œå…¨é¢çš„è¯„è®ºï¼Œå¯¹ç°åœ¨çš„æ•°æ®é©±åŠ¨AIæ–¹é¢çš„åŠªåŠ›è¿›è¡Œäº†è¯¦ç»†çš„æ¢³ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨å›¾æ•°æ®strucutureä¸Šï¼Œå›¾æ•°æ®æ˜¯ç°å®ä¸–ç•Œä¸­å„ç§å„æ ·çš„å®ä½“ä¹‹é—´çš„å¤æ‚ä¾èµ–å…³ç³»çš„åŸºæœ¬è¡¨ç¤ºæ–¹å¼ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¶µç›–æ‰€æœ‰å›¾æ•°æ®ç”Ÿå‘½å‘¨æœŸé˜¶æ®µçš„ç³»ç»Ÿæ¡†æ¶ï¼Œç§°ä¸ºæ•°æ®é©±åŠ¨å›¾æœºå™¨å­¦ä¹ ï¼ˆDC-GMLï¼‰ï¼ŒåŒ…æ‹¬å›¾æ•°æ®æ”¶é›†ã€æ¢ç´¢ã€æ”¹è¿›ã€åˆ©ç”¨å’Œç»´æŠ¤ç­‰é˜¶æ®µã€‚æˆ‘ä»¬è¿˜æä¾›äº†æ¯ä¸ªé˜¶æ®µçš„ä½è¿›è¡Œä¸‰ä¸ªå…³é”®é—®é¢˜çš„ç­”æ¡ˆï¼šï¼ˆ1ï¼‰å¦‚ä½•æé«˜å›¾æ•°æ®å¯ç”¨æ€§å’Œè´¨é‡ï¼›ï¼ˆ2ï¼‰å¦‚ä½•ä»æœ‰é™å¯ç”¨æ€§å’Œä½è´¨é‡çš„å›¾æ•°æ®ä¸­å­¦ä¹ ï¼›ï¼ˆ3ï¼‰å¦‚ä½•ä»å›¾æ•°æ®ä¸­å¿ƒè§†å»ºç«‹å›¾MLOpsç³»ç»Ÿã€‚æœ€åï¼Œæˆ‘ä»¬æŒ‡å‡ºäº†DC-GMLé¢†åŸŸæœªæ¥çš„å‰æ™¯ï¼Œä¸ºå…¶å‘å±•å’Œåº”ç”¨æä¾›äº†æŒ‡å¯¼ã€‚
</details></li>
</ul>
<hr>
<h2 id="PAGER-A-Framework-for-Failure-Analysis-of-Deep-Regression-Models"><a href="#PAGER-A-Framework-for-Failure-Analysis-of-Deep-Regression-Models" class="headerlink" title="PAGER: A Framework for Failure Analysis of Deep Regression Models"></a>PAGER: A Framework for Failure Analysis of Deep Regression Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10977">http://arxiv.org/abs/2309.10977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jayaraman J. Thiagarajan, Vivek Narayanaswamy, Puja Trivedi, Rushil Anirudh</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æå‡ºä¸€ç§æ£€æµ‹æ·±åº¦å›å½’æ¨¡å‹é¢„æµ‹é”™è¯¯çš„æ¡†æ¶ï¼Œä»¥ç¡®ä¿äººå·¥æ™ºèƒ½æ¨¡å‹çš„å®‰å…¨éƒ¨ç½²ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†å»ºç«‹åœ¨æ·±åº¦æ¨¡å‹ä¸­çš„ç¨³å®šç‚¹çš„æƒ³æ³•ï¼Œå¹¶ç»“åˆäº†çŸ¥è¯† uncertainty å’Œé conformity åˆ†æ•°ï¼Œå°†æ ·æœ¬åˆ†ä¸ºä¸åŒçš„é£é™© rÃ©gimeã€‚</li>
<li>results: å¯¹äº synthetic å’Œå®é™… benchmark è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºäº† PAGER å¯ä»¥å‡†ç¡®åœ°æ£€æµ‹å‡ºæ·±åº¦å›å½’æ¨¡å‹çš„é¢„æµ‹é”™è¯¯ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„é£é™© rÃ©gime ä¸­åˆ†ç±»æ ·æœ¬ã€‚<details>
<summary>Abstract</summary>
Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analysis of model errors. Additionally, we introduce novel metrics for evaluating failure detectors in regression tasks. We demonstrate the effectiveness of PAGER on synthetic and real-world benchmarks. Our results highlight the capability of PAGER to identify regions of accurate generalization and detect failure cases in out-of-distribution and out-of-support scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
å®‰å…¨éƒ¨ç½²äººå·¥æ™ºèƒ½æ¨¡å‹éœ€è¦ç§¯ææ£€æµ‹å¯èƒ½å‡ºç°çš„é¢„æµ‹é”™è¯¯ï¼Œä»¥é¿å…é«˜æ˜‚çš„é”™è¯¯æˆæœ¬ã€‚å°½ç®¡åœ¨åˆ†ç±»é—®é¢˜ä¸Šçš„å¤±è´¥æ£€æµ‹å·²ç»æ”¶åˆ°äº†å¹¿æ³›çš„å…³æ³¨ï¼Œä½†åœ¨å›å½’ä»»åŠ¡ä¸­çš„å¤±è´¥æ¨¡å¼ç‰¹å¾åŒ–å°šæœªå¾—åˆ°äº†å……åˆ†çš„ç ”ç©¶ã€‚ç°æœ‰çš„æ–¹æ³•åŸºäºæ¨¡å‹çŸ¥è¯†ä¸ç¡®å®šæ€§æˆ–ç‰¹å¾åç§»åº¦ä¸è®­ç»ƒåˆ†å¸ƒç›¸å…³çš„æ–¹æ³•æ¥ç‰¹å¾åŒ–æ¨¡å‹é£é™©ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¡¨æ˜äº†ä¸ç¡®å®šæ€§æ˜¯ç‰¹å¾åŒ–å¤±è´¥çš„å¿…è¦æ¡ä»¶ï¼Œä½†å¹¶ä¸å¤Ÿã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†PAGERï¼ˆåŸºäºæ·±åº¦æ¨¡å‹çš„æ¦‚å¿µåˆ†æå’Œæ€»ç»“ï¼‰ï¼Œä¸€ç§æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿåœ°æ£€æµ‹å’Œç‰¹å¾åŒ–æ·±åº¦å›å½’æ¨¡å‹ä¸­çš„å¤±è´¥ã€‚åŸºäºæ·±åº¦æ¨¡å‹çš„å®‰choræ€æƒ³ï¼ŒPAGERç»“åˆäº†epistemicä¸ç¡®å®šæ€§å’Œæ–°çš„éå‡†ç¡®æ€§åˆ†æ•°ï¼Œå°†æ ·æœ¬åˆ†ä¸ºä¸åŒçš„é£é™© Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ¾Ğ¼ï¼Œä»è€Œæä¾›äº†å…¨é¢çš„æ¨¡å‹é”™è¯¯åˆ†æã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†æ–°çš„è¯„ä»·å¤±è´¥æ£€æµ‹å™¨çš„åº¦é‡æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨syntheticå’Œå®é™…ä¸–ç•Œ benchmarkä¸Šè¯æ˜äº†PAGERçš„æ•ˆæœã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼ŒPAGERèƒ½å¤Ÿæ ‡è¯†å‡ºé«˜åº¦æ™®é€‚æ³›åŒ–å’Œout-of-distributionå’Œout-of-supportåœºæ™¯ä¸­çš„å¤±è´¥æ¡ˆä¾‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Accurate-and-Scalable-Estimation-of-Epistemic-Uncertainty-for-Graph-Neural-Networks"><a href="#Accurate-and-Scalable-Estimation-of-Epistemic-Uncertainty-for-Graph-Neural-Networks" class="headerlink" title="Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks"></a>Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10976">http://arxiv.org/abs/2309.10976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, Jayaraman J. Thiagarajan</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æé«˜å›¾ neural networkï¼ˆGNNï¼‰çš„å®‰å…¨éƒ¨ç½²ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒshiftæ—¶æä¾›å‡†ç¡®çš„ä¿¡ä»»åº¦æŒ‡æ ‡ï¼ˆCIï¼‰ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§case studyæ¥ç ”ç©¶GNN CIçš„å‡†ç¡®æ€§ï¼Œå¹¶è¯æ˜äº†å¢åŠ è¡¨è¾¾èƒ½åŠ›æˆ–æ¨¡å‹å¤§å°ä¸æ€»æ˜¯èƒ½æé«˜CIæ€§èƒ½ã€‚è€Œæ˜¯ä½¿ç”¨epistemic uncertaintyé‡åŒ–ï¼ˆUQï¼‰æ–¹æ³•æ¥è°ƒæ•´CIã€‚æå‡ºäº†ä¸€ç§æ–°çš„å•æ¨¡å‹UQæ–¹æ³•â€”â€”G-$\Delta$UQï¼Œå®ƒåŸºäºæœ€è¿‘æå‡ºçš„éšæœºä¸­å¿ƒæ¡†æ¶ï¼Œæ”¯æŒç»“æ„åŒ–æ•°æ®å’Œéƒ¨åˆ†éšæœºæ€§ã€‚</li>
<li>results: å¯¹äºcovariateã€conceptå’Œå›¾å¤§å°shiftï¼ŒG-$\Delta$UQä¸ä»…åœ¨è·å¾—å‡†ç¡®çš„CIæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿˜åœ¨ä½¿ç”¨CIè¿›è¡Œæ³›åŒ–å·®åˆ†é¢„æµ‹å’ŒOODæ£€æµ‹æ–¹é¢è¡¨ç°æ›´å¥½äºå…¶ä»–popular UQæ–¹æ³•ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ç¯‡è®ºæ–‡ä¸ä»…ä»‹ç»äº†ä¸€ç§æ–°çš„GNN UQæ–¹æ³•ï¼Œè¿˜æä¾›äº†å›¾ neural networkåœ¨å®‰å…¨å…³é”®ä»»åŠ¡ä¸Šçš„æ–°çš„ç†è§£ã€‚<details>
<summary>Abstract</summary>
Safe deployment of graph neural networks (GNNs) under distribution shift requires models to provide accurate confidence indicators (CI). However, while it is well-known in computer vision that CI quality diminishes under distribution shift, this behavior remains understudied for GNNs. Hence, we begin with a case study on CI calibration under controlled structural and feature distribution shifts and demonstrate that increased expressivity or model size do not always lead to improved CI performance. Consequently, we instead advocate for the use of epistemic uncertainty quantification (UQ) methods to modulate CIs. To this end, we propose G-$\Delta$UQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity. Evaluated across covariate, concept, and graph size shifts, G-$\Delta$UQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs are used for generalization gap prediction or OOD detection. Overall, our work not only introduces a new, flexible GNN UQ method, but also provides novel insights into GNN CIs on safety-critical tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
å®‰å…¨éƒ¨ç½²å›¾ neural network (GNN) éœ€è¦æ¨¡å‹æä¾›å‡†ç¡®çš„ä¿¡ä»»æŒ‡æ ‡ (CI)ã€‚ç„¶è€Œï¼Œè™½ç„¶åœ¨è®¡ç®—æœºè§†è§‰ä¸­å·²ç»è¯æ˜äº† CI è´¨é‡ä¸‹é™äºåˆ†å¸ƒè½¬ç§»ï¼Œä½†è¿™ä¸€ç‚¹å°šæœªå¾—åˆ°å¯¹ GNN çš„ç ”ç©¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼€å§‹äº†ä¸€é¡¹æ¡ˆä¾‹ç ”ç©¶ï¼Œæ¢è®¨äº† CI å‡†ç¡®æ€§ä¸‹é™çš„æƒ…å†µï¼Œå¹¶å‘ç°å¢åŠ è¡¨è¾¾èƒ½åŠ›æˆ–æ¨¡å‹å¤§å°ä¸ä¸€å®šèƒ½æé«˜ CI æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ epistemic ä¸ç¡®å®šæ€§é‡åŒ– (UQ) æ–¹æ³•æ¥è°ƒæ•´ CIsã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† G-Î”UQï¼Œä¸€ç§æ–°çš„å•æ¨¡å‹ UQ æ–¹æ³•ï¼Œæ‰©å±•äº†æœ€è¿‘æå‡ºçš„éšæœºä¸­å¿ƒæ¡†æ¶ï¼Œä»¥æ”¯æŒç»“æ„åŒ–æ•°æ®å’Œéƒ¨åˆ†éšæœºæ€§ã€‚ç»è¿‡ covariateã€æ¦‚å¿µå’Œå›¾å¤§å°è½¬ç§»çš„è¯„ä¼°ï¼ŒG-Î”UQ ä¸ä»…åœ¨è·å¾—å‡†ç¡®çš„ CIs æ–¹é¢è¶…è¿‡äº†è®¸å¤šæµè¡Œçš„ UQ æ–¹æ³•ï¼Œè¿˜åœ¨ç”¨ CIs è¿›è¡Œæ³›åŒ–å·®åˆ†é¢„æµ‹æˆ– OOD æ¢æµ‹æ—¶è¡¨ç°æ›´å¥½ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬ä¸ä»…æå‡ºäº†ä¸€ç§æ–°çš„ã€çµæ´»çš„ GNN UQ æ–¹æ³•ï¼Œè€Œä¸”ä¸ºå®‰å…¨å…³é”®ä»»åŠ¡æä¾›äº†æ–°çš„æ€è·¯å’Œå‘ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="SPFQ-A-Stochastic-Algorithm-and-Its-Error-Analysis-for-Neural-Network-Quantization"><a href="#SPFQ-A-Stochastic-Algorithm-and-Its-Error-Analysis-for-Neural-Network-Quantization" class="headerlink" title="SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization"></a>SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10975">http://arxiv.org/abs/2309.10975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinjie Zhang, Rayan Saab</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§é«˜æ•ˆçš„ç¥ç»ç½‘ç»œå‹ç¼©æ–¹æ³•ï¼Œä»¥å‡å°‘è¿‡å‚åŒ–ç¥ç»ç½‘ç»œä¸­çš„é‡å¤æ€§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§å¿«é€Ÿçš„éšæœºç®—æ³•æ¥å‹ç¼©ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚è¯¥æ–¹æ³•åˆ©ç”¨äº†ä¸€ç§æ‰©å±•çš„æƒé‡è·¯å¾„è·Ÿè¸ªæœºåˆ¶ï¼Œä»¥åŠä¸€ç§éšæœºå‹ç¼©å™¨ã€‚å…¶è®¡ç®—å¤æ‚åº¦åªä¸ç¥ç»ç½‘ç»œä¸­neuronæ•°é‡æˆçº¿æ€§å…³ç³»ï¼Œå› æ­¤å¯ä»¥æœ‰æ•ˆåœ°å‹ç¼©å¤§å‹ç¥ç»ç½‘ç»œã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡æå‡ºäº†ä¸€ç§å…¨ç½‘ç»œè¯¯å·®è¾¹ç•Œï¼Œä»¥åŠä¸€ç§åœ¨ Gaussian æƒé‡ä¸‹å¯ä»¥å®ç°çš„é«˜æ•ˆå‹ç¼©æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯æ˜äº†ï¼Œå½“é‡‡ç”¨è¿™ç§æ–¹æ³•å‹ç¼©å¤šå±‚ç¥ç»ç½‘ç»œæ—¶ï¼Œè¯¯å·®è¡¨è¾¾çš„å¹³æ–¹å¹‚ decay Linear æ–¹å¼ä¸è¿‡å‚åŒ–ç¨‹åº¦å¢é•¿ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯æ˜äº†å¯ä»¥ä½¿ç”¨ loglog N æ¯”ç‰¹æ•°æ¥å®ç°è¯¯å·®è¾¹ç•Œç›¸å½“äºæ— é™å­—æ¯æƒ…å†µä¸‹çš„è¯¯å·®è¾¹ç•Œã€‚<details>
<summary>Abstract</summary>
Quantization is a widely used compression method that effectively reduces redundancies in over-parameterized neural networks. However, existing quantization techniques for deep neural networks often lack a comprehensive error analysis due to the presence of non-convex loss functions and nonlinear activations. In this paper, we propose a fast stochastic algorithm for quantizing the weights of fully trained neural networks. Our approach leverages a greedy path-following mechanism in combination with a stochastic quantizer. Its computational complexity scales only linearly with the number of weights in the network, thereby enabling the efficient quantization of large networks. Importantly, we establish, for the first time, full-network error bounds, under an infinite alphabet condition and minimal assumptions on the weights and input data. As an application of this result, we prove that when quantizing a multi-layer network having Gaussian weights, the relative square quantization error exhibits a linear decay as the degree of over-parametrization increases. Furthermore, we demonstrate that it is possible to achieve error bounds equivalent to those obtained in the infinite alphabet case, using on the order of a mere $\log\log N$ bits per weight, where $N$ represents the largest number of neurons in a layer.
</details>
<details>
<summary>æ‘˜è¦</summary>
é‡åŒ–æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„å‹ç¼©æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å‡å°‘æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„é‡å¤æ€§ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ·±åº¦ç¥ç»ç½‘ç»œé‡åŒ–æŠ€æœ¯ frequently lack a comprehensive error analysis due to the presence of non-convex loss functions and nonlinear activations. In this paper, we propose a fast stochastic algorithm for quantizing the weights of fully trained neural networks. Our approach leverages a greedy path-following mechanism in combination with a stochastic quantizer. Its computational complexity scales only linearly with the number of weights in the network, thereby enabling the efficient quantization of large networks. Importantly, we establish, for the first time, full-network error bounds, under an infinite alphabet condition and minimal assumptions on the weights and input data. As an application of this result, we prove that when quantizing a multi-layer network having Gaussian weights, the relative square quantization error exhibits a linear decay as the degree of over-parametrization increases. Furthermore, we demonstrate that it is possible to achieve error bounds equivalent to those obtained in the infinite alphabet case, using on the order of a mere $\log\log N$ bits per weight, where $N$ represents the largest number of neurons in a layer.Note: The translation is done using the Google Translate API, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/cs.LG_2023_09_20/" data-id="clopawnuz00piag8837ta5iyo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/eess.IV_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T09:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/eess.IV_2023_09_20/">eess.IV - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="High-content-stimulated-Raman-histology-of-human-breast-cancer"><a href="#High-content-stimulated-Raman-histology-of-human-breast-cancer" class="headerlink" title="High-content stimulated Raman histology of human breast cancer"></a>High-content stimulated Raman histology of human breast cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11642">http://arxiv.org/abs/2309.11642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongli Ni, Chinmayee Prabhu Dessai, Haonan Lin, Wei Wang, Shaoxiong Chen, Yuhao Yuan, Xiaowei Ge, Jianpeng Ao, Nolan Vild, Ji-Xin Cheng</li>
<li>for: This paper aims to provide a high-content stimulated Raman histology (HC-SRH) platform for cancer diagnosis based on un-stained breast tissues, which can provide both morphological and chemical information.</li>
<li>methods: The HC-SRH platform uses spectral unmixing in the C-H vibration window to map unsaturated lipids, cellular protein, extracellular matrix, saturated lipid, and water in breast tissue, and spectral selective sampling is implemented to boost the speed of HC-SRH.</li>
<li>results: The HC-SRH platform provides excellent contrast for various tissue components, and the advanced fiber laser-based SRS microscopy demonstrates the HC-SRH in a clinical-compatible manner, showing a clear chemical contrast of nucleic acid and solid-state ester in the fingerprint result.<details>
<summary>Abstract</summary>
Histological examination is crucial for cancer diagnosis, including hematoxylin and eosin (H&E) staining for mapping morphology and immunohistochemistry (IHC) staining for revealing chemical information. Recently developed two-color stimulated Raman histology could bypass the complex tissue processing to mimic H&E-like morphology. Yet, the underlying chemical features are not revealed, compromising the effectiveness of prognostic stratification. Here, we present a high-content stimulated Raman histology (HC-SRH) platform that provides both morphological and chemical information for cancer diagnosis based on un-stained breast tissues. Through spectral unmixing in the C-H vibration window, HC-SRH can map unsaturated lipids, cellular protein, extracellular matrix, saturated lipid, and water in breast tissue. In this way, HC-SRH provides excellent contrast for various tissue components. Considering rapidness is important in clinical trials, we implemented spectral selective sampling to boost the speed of HC-SRH by one order. We also successfully demonstrated the HC-SRH in a clinical-compatible fiber laser-based SRS microscopy. With the widely rapid tuning capability of the advanced fiber laser, a clear chemical contrast of nucleic acid and solid-state ester is shown in the fingerprint result.
</details>
<details>
<summary>æ‘˜è¦</summary>
histological æ£€æŸ¥æ˜¯ç™Œç—…è¯Šæ–­ä¸­ä¸å¯æˆ–ç¼ºçš„ï¼ŒåŒ…æ‹¬æ‚è°±å’ŒæŸ“è‰²æŠ€æœ¯ã€‚ latest developments in two-color stimulated Raman histology can mimic H&E-like morphology, but the underlying chemical features are not revealed, which compromises the effectiveness of prognostic stratification. Here, we present a high-content stimulated Raman histology (HC-SRH) platform that provides both morphological and chemical information for cancer diagnosis based on un-stained breast tissues. Through spectral unmixing in the C-H vibration window, HC-SRH can map unsaturated lipids, cellular protein, extracellular matrix, saturated lipid, and water in breast tissue. In this way, HC-SRH provides excellent contrast for various tissue components. Considering rapidness is important in clinical trials, we implemented spectral selective sampling to boost the speed of HC-SRH by one order. We also successfully demonstrated the HC-SRH in a clinical-compatible fiber laser-based SRS microscopy. With the widely rapid tuning capability of the advanced fiber laser, a clear chemical contrast of nucleic acid and solid-state ester is shown in the fingerprint result.
</details></li>
</ul>
<hr>
<h2 id="Lightning-Fast-Dual-Layer-Lossless-Coding-for-Radiance-Format-High-Dynamic-Range-Images"><a href="#Lightning-Fast-Dual-Layer-Lossless-Coding-for-Radiance-Format-High-Dynamic-Range-Images" class="headerlink" title="Lightning-Fast Dual-Layer Lossless Coding for Radiance Format High Dynamic Range Images"></a>Lightning-Fast Dual-Layer Lossless Coding for Radiance Format High Dynamic Range Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11072">http://arxiv.org/abs/2309.11072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taizo Suzuki, Sara Yukikata, Kai Yang, Taichi Yoshida</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§é«˜åŠ¨æ€èŒƒå›´å›¾åƒï¼ˆHDRIsï¼‰çš„å¿«é€ŸåŒå±‚æ— æŸç¼–ç æ–¹æ³•ã€‚</li>
<li>methods: è¯¥ç¼–ç æ–¹æ³•åŒ…æ‹¬åŸºå±‚å’Œæ— æŸå¢å¼ºå±‚ï¼Œå¯ä»¥ç›´æ¥å°†é«˜åŠ¨æ€èŒƒå›´å›¾åƒï¼ˆHDRIsï¼‰è½¬æ¢ä¸ºæ ‡å‡†åŠ¨æ€èŒƒå›´å›¾åƒï¼ˆSDRIsï¼‰ï¼Œæ— éœ€åœ¨è§£ç å™¨ç«¯æ·»åŠ é¢å¤–ç®—æ³•ï¼ŒåŒæ—¶å¯ä»¥äº§ç”Ÿé«˜è´¨é‡çš„SDRIsã€‚</li>
<li>results: å¯¹æ¯”ç°æœ‰æ–¹æ³•ï¼Œè¯¥ç¼–ç æ–¹æ³•å¯ä»¥å‡å°‘å¹³å‡æ¯”ç‰¹ç‡çº¦ä¸º1.57%-6.68%ï¼Œå¹¶ä¸”æ˜¾è‘—å‡å°‘è§£ç å™¨å®ç°æ—¶é—´çº¦ä¸º87.13%-98.96%ã€‚<details>
<summary>Abstract</summary>
This paper proposes a fast dual-layer lossless coding for high dynamic range images (HDRIs) in the Radiance format. The coding, which consists of a base layer and a lossless enhancement layer, provides a standard dynamic range image (SDRI) without requiring an additional algorithm at the decoder and can losslessly decode the HDRI by adding the residual signals (residuals) between the HDRI and SDRI to the SDRI, if desired. To suppress the dynamic range of the residuals in the enhancement layer, the coding directly uses the mantissa and exponent information from the Radiance format. To further reduce the residual energy, each mantissa is modeled (estimated) as a linear function, i.e., a simple linear regression, of the encoded-decoded SDRI in each region with the same exponent. This is called simple linear regressive mantissa estimator. Experimental results show that, compared with existing methods, our coding reduces the average bitrate by approximately $1.57$-$6.68$ % and significantly reduces the average encoder implementation time by approximately $87.13$-$98.96$ %.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¿«é€ŸåŒå±‚æ— æŸç¼–ç å™¨ï¼Œç”¨äºé«˜åŠ¨æ€èŒƒå›´å›¾åƒï¼ˆHDRIsï¼‰åœ¨è¾‰åº¦æ ¼å¼ä¸‹ã€‚è¯¥ç¼–ç å™¨ç”±åŸºå±‚å’Œæ— æŸå¢å¼ºå±‚ç»„æˆï¼Œå¯ä»¥å°†æ ‡å‡†åŠ¨æ€èŒƒå›´å›¾åƒï¼ˆSDRIï¼‰è½¬æ¢ä¸ºHDRIsï¼Œæ— éœ€é¢å¤–ç®—æ³•åœ¨è§£ç å™¨ç«¯ã€‚æ­¤å¤–ï¼Œè¯¥ç¼–ç å™¨è¿˜å¯ä»¥losslesslyè§£ç HDRIsï¼Œåªéœ€å°†å·®å¼‚ä¿¡å·ï¼ˆå·®å¼‚ï¼‰åŠ åˆ°SDRIä¸Šå³å¯ã€‚ä¸ºäº†å‡å°‘å¢å¼ºå±‚çš„åŠ¨æ€èŒƒå›´ï¼Œè¯¥ç¼–ç å™¨ç›´æ¥ä½¿ç”¨Radianceæ ¼å¼ä¸­çš„æ å¿—å’ŒæŒ‡æ•°ä¿¡æ¯ã€‚è¿›ä¸€æ­¥å‡å°‘å·®å¼‚èƒ½é‡ï¼Œæ¯ä¸ªæ å¿—éƒ½è¢«æ¨¡å‹ä¸ºåœ¨æ¯ä¸ªåŒºåŸŸä¸­çš„çº¿æ€§å‡½æ•°ï¼Œå³ç®€å•çš„çº¿æ€§å›å½’ã€‚è¿™è¢«ç§°ä¸ºç®€å•çš„çº¿æ€§å›å½’æ å¿—ä¼°è®¡å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„ç¼–ç å™¨å¯ä»¥å°†å¹³å‡æ¯”ç‰¹ç‡é™ä½çº¦1.57%-6.68%ï¼Œå¹¶æ˜¾è‘—é™ä½è§£ç å™¨å®ç°æ—¶é—´çº¦87.13%-98.96%ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/eess.IV_2023_09_20/" data-id="clopawo1h016bag88fopn3o3x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_09_20" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/20/eess.SP_2023_09_20/" class="article-date">
  <time datetime="2023-09-20T08:00:00.000Z" itemprop="datePublished">2023-09-20</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/20/eess.SP_2023_09_20/">eess.SP - 2023-09-20</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Channel-Reciprocity-Attacks-Using-Intelligent-Surfaces-with-Non-Diagonal-Phase-Shifts"><a href="#Channel-Reciprocity-Attacks-Using-Intelligent-Surfaces-with-Non-Diagonal-Phase-Shifts" class="headerlink" title="Channel Reciprocity Attacks Using Intelligent Surfaces with Non-Diagonal Phase Shifts"></a>Channel Reciprocity Attacks Using Intelligent Surfaces with Non-Diagonal Phase Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11665">http://arxiv.org/abs/2309.11665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyu Wang, Zhu Han, Lee Swindlehurst</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†ä¸€ç§åŸºäºå·ç§¯æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰æŠ€æœ¯çš„æ”»å‡»ï¼Œè¯¥æŠ€æœ¯å¯ä»¥åœ¨å¤šå¤©çº¿å‡ ä½•ç³»ç»Ÿä¸­å¼•èµ·é€šä¿¡é“¾è·¯çš„å¹²æ‰°ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†éå¯¹ç§°ï¼ˆNDï¼‰å¹…shiftçŸ©é˜µï¼ˆRISï¼‰ï¼Œé€šè¿‡éå¯¹ç§°å¹…shiftçŸ©é˜µæ¥ç ´åé€šä¿¡é“¾è·¯çš„å¯¹ç§°æ€§ï¼Œä»è€Œé™ä½ä¸‹é“¾è·¯æ€§èƒ½ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“ä¸€ä¸ªæ¶æ„çš„ND-RISè¢«éƒ¨ç½²æ—¶ï¼Œå¯ä»¥ä½¿å¾—ä¸‹é“¾è·¯æ€§èƒ½å—åˆ°æå¤§çš„é™ä½ï¼Œå¹¶ä¸”è¿™ç§æ”»å‡»æ˜¯æ— åŠ¨ä½œçš„å’Œéš¾ä»¥æ¢æµ‹çš„ã€‚<details>
<summary>Abstract</summary>
While reconfigurable intelligent surface (RIS) technology has been shown to provide numerous benefits to wireless systems, in the hands of an adversary such technology can also be used to disrupt communication links. This paper describes and analyzes an RIS-based attack on multi-antenna wireless systems that operate in time-division duplex mode under the assumption of channel reciprocity. In particular, we show how an RIS with a non-diagonal (ND) phase shift matrix (referred to here as an ND-RIS) can be deployed to maliciously break the channel reciprocity and hence degrade the downlink network performance. Such an attack is entirely passive and difficult to detect. We provide a theoretical analysis of the degradation in the sum ergodic rate that results when an arbitrary malicious ND-RIS is deployed and design an approach based on the genetic algorithm for optimizing the ND structure under partial knowledge of the available channel state information. Our simulation results validate the analysis and demonstrate that an ND-RIS channel reciprocity attack can dramatically reduce the downlink throughput.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå¼¹æ€§æ™ºèƒ½è¡¨é¢ï¼ˆRISï¼‰æŠ€æœ¯å·²ç»è¢«è¯æ˜å¯ä»¥æä¾›æ— çº¿ç³»ç»Ÿä¸­çš„è®¸å¤šä¼˜ç‚¹ï¼Œä½†åœ¨æ•Œäººæ‰‹ä¸Šå¯ä»¥ä½¿ç”¨è¿™æŠ€æœ¯æ¥ä¸­æ–­é€šä¿¡é“¾æ¥ã€‚æœ¬çº¸æè¿°äº†ä¸€ç§åŸºäºRISçš„æ”»å‡»ï¼Œå¯¹äºåœ¨æ—¶åˆ†å¤šæ™®éè°ƒå¹…æ¨¡å¼ä¸‹è¿è¡Œçš„å¤šantennaæ— çº¿ç³»ç»Ÿè¿›è¡Œç ´åã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†å¦‚ä½•åœ¨NDé¡¹ç›®ï¼ˆnon-diagonalï¼‰çš„RISä¸­éƒ¨ç½²æ¶æ„çš„é¡¹ç›®ï¼Œä»¥ç ´åé€šé“å¯¹ç§°æ€§ï¼Œä»è€Œé™ä½ä¸‹è”ç½‘æ€§èƒ½ã€‚è¿™ç§æ”»å‡»æ˜¯å®Œå…¨è¢«åŠ¨çš„ï¼Œéš¾ä»¥æ£€æµ‹ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç†è®ºåˆ†æï¼Œä»¥åŠåŸºäºç”Ÿç‰©ç®—æ³•æ¥ä¼˜åŒ–NDç»“æ„çš„æ–¹æ³•ï¼Œä»¥å¯¹ä¸å®Œå…¨çŸ¥é“å¯ç”¨é€šé“çŠ¶æ€ä¿¡æ¯è¿›è¡Œä¼˜åŒ–ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¯å®äº†åˆ†æï¼Œå¹¶æ˜¾ç¤ºäº†ND-RISé€šé“å¯¹ç§°æ€§æ”»å‡»å¯ä»¥å¯¼è‡´ä¸‹è”ç½‘é€šè¿‡ç‡çš„ä¸¥é‡ä¸‹é™ã€‚â€Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need further assistance.
</details></li>
</ul>
<hr>
<h2 id="Compression-Spectrum-Where-Shannon-meets-Fourier"><a href="#Compression-Spectrum-Where-Shannon-meets-Fourier" class="headerlink" title="Compression Spectrum: Where Shannon meets Fourier"></a>Compression Spectrum: Where Shannon meets Fourier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11640">http://arxiv.org/abs/2309.11640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditi Kathpalia, Nithin Nagaraj</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯å°†ä¿¡å·å¤„ç†å’Œä¿¡æ¯ç†è®ºä¸¤ä¸ªé¢†åŸŸè”ç³»èµ·æ¥ï¼Œä»¥ä¾¿é€šè¿‡å¯¹æ—¶é—´åºåˆ—çš„å‹ç¼©æ¥ä¼°ç®—å®ƒçš„ä¿¡æ¯é‡å’Œå‹ç¼©ç¨‹åº¦ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§æ— æŸæ•°æ®å‹ç¼©ç®—æ³•æ¥ä¼°ç®—æ—¶é—´åºåˆ—çš„ä¿¡æ¯é‡æˆ–å‹ç¼©ç¨‹åº¦ï¼Œå¹¶ä½¿ç”¨äº†Effort-to-Compressï¼ˆETCï¼‰ç®—æ³•æ¥è·å¾—ä¸€ä¸ªå‹ç¼©ÑĞ¿ĞµĞºÑ‚rumã€‚</li>
<li>results: æœ¬æ–‡é€šè¿‡åº”ç”¨å‹ç¼©ÑĞ¿ĞµĞºÑ‚rumäºäººä½“å¿ƒè·³é—´éš”ï¼ˆRRï¼‰åºåˆ—ï¼Œå‘ç°å¥åº·å¹´è½»äººRRåºåˆ—åœ¨å¾‹ log-log å°ºåº¦ä¸Šè¡¨ç°ç±»ä¼¼äº1&#x2F;f å™ªå£°ï¼Œè€Œå¥åº·è€å¹´äººRRåºåˆ—åˆ™è¡¨ç°ä¸åŒã€‚<details>
<summary>Abstract</summary>
Signal processing and Information theory are two disparate fields used for characterizing signals for various scientific and engineering applications. Spectral/Fourier analysis, a technique employed in signal processing, helps estimation of power at different frequency components present in the signal. Characterizing a time-series based on its average amount of information (Shannon entropy) is useful for estimating its complexity and compressibility (eg., for communication applications). Information theory doesn't deal with spectral content while signal processing doesn't directly consider the information content or compressibility of the signal. In this work, we attempt to bring the fields of signal processing and information theory together by using a lossless data compression algorithm to estimate the amount of information or `compressibility' of time series at different scales. To this end, we employ the Effort-to-Compress (ETC) algorithm to obtain what we call as a Compression Spectrum. This new tool for signal analysis is demonstrated on synthetically generated periodic signals, a sinusoid, chaotic signals (weak and strong chaos) and uniform random noise. The Compression Spectrum is applied on heart interbeat intervals (RR) obtained from real-world normal young and elderly subjects. The compression spectrum of healthy young RR tachograms in the log-log scale shows behaviour similar to $1/f$ noise whereas the healthy old RR tachograms show a different behaviour. We envisage exciting possibilities and future applications of the Compression Spectrum.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¿¡å·å¤„ç†å’Œä¿¡æ¯ç†è®ºæ˜¯ä¸¤ä¸ªä¸åŒçš„é¢†åŸŸï¼Œç”¨äºæè¿°ä¿¡å·çš„ä¸åŒç§‘å­¦å’Œå·¥ç¨‹åº”ç”¨ã€‚spectral/ fourieråˆ†ææ˜¯ä¿¡å·å¤„ç†ä¸­ä½¿ç”¨çš„æŠ€æœ¯ï¼Œå¯ä»¥ä¸ºä¸åŒé¢‘ç‡ç»„æˆçš„ä¿¡å·ä¼°ç®—èƒ½é‡ã€‚åŸºäºæ—¶é—´åºåˆ—çš„å¹³å‡ä¿¡æ¯é‡ï¼ˆShannon entropyï¼‰çš„Characterizingæ˜¯ç”¨äºä¼°ç®—ä¿¡å·çš„å¤æ‚æ€§å’Œå‹ç¼©æ€§ï¼ˆä¾‹å¦‚ï¼Œ Ğ´Ğ»Ñé€šä¿¡åº”ç”¨ï¼‰ã€‚ä¿¡æ¯ç†è®ºä¸è€ƒè™‘é¢‘è°±å†…å®¹ï¼Œè€Œä¿¡å·å¤„ç†ä¸ç›´æ¥è€ƒè™‘ä¿¡å·çš„ä¿¡æ¯å†…å®¹æˆ–å‹ç¼©æ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°è¯•å°†ä¿¡å·å¤„ç†å’Œä¿¡æ¯ç†è®ºä¸¤ä¸ªé¢†åŸŸè”ç³»èµ·æ¥ï¼Œä½¿ç”¨ä¸€ç§æ— æŸæ•°æ®å‹ç¼©ç®—æ³•æ¥ä¼°ç®—æ—¶é—´åºåˆ—çš„ä¿¡æ¯é‡æˆ–â€œå‹ç¼©æ€§â€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨Effort-to-Compressï¼ˆETCï¼‰ç®—æ³•è·å¾—ä¸€ä¸ªå‹ç¼©ÑĞ¿ĞµĞºÑ‚rumã€‚è¿™ç§æ–°çš„ä¿¡å·åˆ†æå·¥å…·åœ¨äººå·¥ç”Ÿæˆçš„ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµä¿¡å·ã€sinusoidä¿¡å·ã€æ··æ²Œä¿¡å·ï¼ˆå¼±å’Œå¼ºæ··æ²Œï¼‰ä»¥åŠéšæœºå™ªå£°ä¸Šè¿›è¡Œäº†åº”ç”¨ã€‚å‹ç¼©ÑĞ¿ĞµĞºÑ‚rumåœ¨å®é™…è·å¾—çš„å¿ƒè·³é—´éš”ï¼ˆRRï¼‰ä¸Šè¿›è¡Œäº†åº”ç”¨ï¼Œå¹¶åœ¨å¾„å‘å‡è¡¡å°ºåº¦ä¸Šæ˜¾ç¤ºäº†ç±»ä¼¼äº1/få™ªå£°çš„è¡Œä¸ºã€‚æˆ‘ä»¬çœ‹åˆ°äº†æœªæ¥åº”ç”¨çš„æ¿€åŠ¨äººå¿ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Brief-Architectural-Survey-of-Biopotential-Recording-Front-Ends-since-the-1970s"><a href="#Brief-Architectural-Survey-of-Biopotential-Recording-Front-Ends-since-the-1970s" class="headerlink" title="Brief Architectural Survey of Biopotential Recording Front-Ends since the 1970s"></a>Brief Architectural Survey of Biopotential Recording Front-Ends since the 1970s</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11612">http://arxiv.org/abs/2309.11612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taeju Lee, Minkyu Je</li>
<li>For: The paper provides a survey of the architecture history of biopotential recording front-ends developed since the 1970s, and discusses overall key circuit techniques for reliable and continuous signal acquisition.* Methods: The paper discusses various front-end architectures for biopotential recording, including their characteristics and challenges, depending on the bioelectric signals being measured.* Results: The paper provides an overview of the evolution of biopotential recording front-ends over the last five decades, and discusses the key circuit techniques for low power and low noise performance.Here are the three information points in Simplified Chinese text:* For: è¿™ç¯‡è®ºæ–‡æä¾›äº†1970å¹´ä»£ä»¥æ¥ç”Ÿç‰©ç”µåŠ¨åŠ›è®°å½•å‰ç«¯çš„å»ºç­‘å†å²ï¼Œå¹¶è®¨è®ºäº†é€‚ç”¨äºå¯é è¿ç»­è®°å½•çš„ä¿¡å·è·å–çš„æ€»ä½“å…³é”®ç”µè·¯æŠ€æœ¯ã€‚* Methods: è®ºæ–‡è®¨è®ºäº†ä¸åŒçš„ç”Ÿç‰©ç”µåŠ¨åŠ›è®°å½•å‰ç«¯æ¶æ„ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„ç‰¹ç‚¹å’ŒæŒ‘æˆ˜ï¼Œå…·ä½“å–å†³äºæµ‹é‡çš„ç”Ÿç‰©ç”µåŠ¨åŠ›ä¿¡å·ã€‚* Results: è®ºæ–‡æä¾›äº†è¿‡å»äº”åå¹´æ¥ç”Ÿç‰©ç”µåŠ¨åŠ›è®°å½•å‰ç«¯çš„æ¼”åŒ–å†å²ï¼Œå¹¶è®¨è®ºäº†ä½åŠŸè€—å’Œä½å™ªå£°æ€§èƒ½çš„å…³é”®ç”µè·¯æŠ€æœ¯ã€‚<details>
<summary>Abstract</summary>
Measuring the bioelectric signals is one of the key functions in wearable healthcare devices and implantable medical devices. The use of wearable healthcare devices has made continuous and immediate monitoring of personal health status possible. Implantable medical devices have played an important role throughout the fields of neuroscience, brain-machine (or brain-computer) interface, and rehabilitation technology. Over the last five decades, the bioelectric signals have been observed through a variety of biopotential recording front-ends, along with advances in semiconductor technology scaling and circuit techniques. Also, for reliable and continuous signal acquisition, the front-end architectures have evolved while maintaining low power and low noise performance. In this article, the architecture history of the biopotential recording front-ends developed since the 1970s is surveyed, and overall key circuit techniques are discussed. Depending on the bioelectric signals being measured, appropriate front-end architecture needs to be chosen, and the characteristics and challenges of each architecture are also covered in this article.
</details>
<details>
<summary>æ‘˜è¦</summary>
æµ‹é‡ç”Ÿç‰©ç”µå­ä¿¡å·æ˜¯ç°ä»£åŒ»ç–—è®¾å¤‡å’ŒåµŒå…¥å¼åŒ»ç–—è®¾å¤‡ä¸­çš„å…³é”®åŠŸèƒ½ä¹‹ä¸€ã€‚ä½¿ç”¨äº†å¯ç©¿æˆ´å¼åŒ»ç–—è®¾å¤‡ï¼Œäººä»¬å¯ä»¥å®æ—¶ã€è¿ç»­åœ°ç›‘æµ‹ä¸ªäººå¥åº·çŠ¶å†µã€‚åµŒå…¥å¼åŒ»ç–—è®¾å¤‡åœ¨ç¥ç»ç§‘å­¦ã€è„‘æœº interfaces å’ŒrehabilitationæŠ€æœ¯ç­‰é¢†åŸŸå‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚è¿‡å»äº”åå¹´ï¼Œç”Ÿç‰©ç”µå­ä¿¡å·å·²ç»é€šè¿‡å¤šç§ç”Ÿç‰©æ½œåœ¨è®°å½•å‰ç«¯ï¼Œåˆ©ç”¨åŠå¯¼ä½“æŠ€æœ¯çš„å‘å±•å’Œç”µè·¯æŠ€æœ¯çš„è¿›æ­¥ã€‚ä¸ºç¡®ä¿å¯é å’Œè¿ç»­çš„ä¿¡å·æ•è·ï¼Œå‰ç«¯æ¶æ„ä¹Ÿåœ¨ä¸æ–­å‘å±•ï¼ŒåŒæ—¶ä¿æŒä½åŠŸè€—å’Œä½å™ªæ€§èƒ½ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œè‡ª1970å¹´ä»£ä»¥æ¥å‘å±•çš„ç”Ÿç‰©æ½œåœ¨è®°å½•å‰ç«¯æ¶æ„å†å²è¢«è¯„ä¼°ï¼ŒåŒæ—¶æ€»ä½“è®²è¿°äº†å…³é”®çš„ç”µè·¯æŠ€æœ¯ã€‚æ ¹æ®æµ‹é‡çš„ç”Ÿç‰©ç”µå­ä¿¡å·ï¼Œéœ€è¦é€‰æ‹©åˆé€‚çš„å‰ç«¯æ¶æ„ï¼Œæ–‡ç« è¿˜è®²è¿°äº†æ¯ç§æ¶æ„çš„ç‰¹ç‚¹å’ŒæŒ‘æˆ˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Self-Sustaining-Oscillator-with-Frequency-Counter-for-Resonance-Frequency-Tracking-in-Micro-and-Nanomechanical-Sensing"><a href="#Self-Sustaining-Oscillator-with-Frequency-Counter-for-Resonance-Frequency-Tracking-in-Micro-and-Nanomechanical-Sensing" class="headerlink" title="Self-Sustaining Oscillator with Frequency Counter for Resonance Frequency Tracking in Micro- and Nanomechanical Sensing"></a>Self-Sustaining Oscillator with Frequency Counter for Resonance Frequency Tracking in Micro- and Nanomechanical Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11581">http://arxiv.org/abs/2309.11581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hajrudin BeÅ¡iÄ‡, Alper Demir, Veljko VukiÄ‡eviÄ‡, Johannes Steurer, Silvan Schmid</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§åŸºäºæŒ¯è¡é¢‘ç‡å˜åŒ–çš„å¥ˆç±³æœºæ¢°æ„ŸçŸ¥å™¨ï¼Œå¹¶é€šè¿‡ theoretically å’Œå®éªŒç ”ç©¶å…¶é€Ÿåº¦å’Œç²¾åº¦ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§åŸºäºæŒ¯è¡é¢‘ç‡å˜åŒ–çš„è‡ªå¸¦ç»´æŒæŒ¯è¡å™¨ï¼ˆSSOï¼‰å¥ˆç±³ç”µå­æœºæ¢°ç³»ç»Ÿï¼ˆNEMSï¼‰é…ç½®ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæŒ¯è¡é¢‘ç‡å˜åŒ–çš„é¢‘ç‡è®¡æ•°å™¨ï¼Œä»¥å®ç°é«˜é€Ÿåº¦å’Œé«˜ç²¾åº¦çš„é¢‘ç‡æµ‹é‡ã€‚</li>
<li>results: ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä¸ç°æœ‰çš„é˜¶æ®µé”å®šå¾ªç¯ï¼ˆPLLï¼‰æ–¹æ³•ç›¸æ¯”ï¼Œæå‡ºçš„æ–¹æ³•å…·æœ‰ç±»ä¼¼æˆ–æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰æ›´ä½çš„æˆæœ¬å’Œæ›´é«˜çš„ä½¿ç”¨å®¹æ˜“åº¦ã€‚å®éªŒæµ‹é‡ç»“æœä¸ç†è®ºé¢„æµ‹å‡ ä¹å®Œç¾å»åˆã€‚<details>
<summary>Abstract</summary>
Nanomechanical sensors based on detecting and tracking resonance frequency shifts are to be used in many applications. Various open- and closed-loop tracking schemes, all offering a trade-off between speed and precision, have been studied both theoretically and experimentally. In this work, we advocate the use of a frequency counter as a frequency shift monitor in conjunction with a self-sustaining oscillator (SSO) nanoelectromechanical system (NEMS) configuration. We derive a theoretical model for characterizing the speed and precision of frequency measurements with state-of-the-art frequency counters. Based on the understanding provided by this model, we introduce novel enhancements to frequency counters that result in a trade-off characteristics which is on a par with the other tracking schemes. We describe a low-cost field-programmable-gate array (FPGA) based implementation for the proposed frequency counter and use it with the SSO-NEMS device in order to study its frequency tracking performance. We compare the proposed approach with the phase-locked-loop based scheme both in theory and experimentally. Our results show that similar or better performance can be achieved at a substantially lower cost and improved ease-of-use. We obtain almost perfect correspondence between the theoretical model predictions and the experimental measurements.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‡ ç§ nanomechanical æ„Ÿæµ‹å™¨åŸºäºæ¢æµ‹å’Œè·Ÿè¸ªå¾é¢‘Shiftçš„åº”ç”¨å°†åœ¨æœªæ¥ä¸­æ™®éä½¿ç”¨ã€‚å„ç§å¼€æ”¾å’Œå…³é—­loop tracking æ–¹æ¡ˆï¼Œå‡æä¾›äº†é€Ÿåº¦å’Œç²¾åº¦ä¹‹é—´çš„äº¤æ˜“ï¼Œå·²ç»è¢«ç†è®ºå’Œå®éªŒç ”ç©¶ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨é¢‘è®¡ä½œä¸ºå¾é¢‘shift ç›‘æµ‹å™¨ï¼Œå¹¶ä¸è‡ªæŒ oscillatorï¼ˆSSOï¼‰ nanoelectromechanical systemï¼ˆNEMSï¼‰é…ç½®ä¸€èµ·ä½¿ç”¨ã€‚æˆ‘ä»¬ derivatedä¸€ä¸ªç†è®ºæ¨¡å‹ï¼Œç”¨äºCharacterizing é¢‘åº¦æµ‹é‡çš„é€Ÿåº¦å’Œç²¾åº¦ã€‚åŸºäºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€äº›æ–°çš„å¢å¼ºï¼Œä½¿å¾—é¢‘è®¡çš„äº¤æ˜“ç‰¹æ€§ä¸å…¶ä»–è·Ÿè¸ªæ–¹æ¡ˆç›¸å½“ã€‚æˆ‘ä»¬æè¿°äº†ä¸€ç§ä½æˆæœ¬çš„ field-programmable-gate arrayï¼ˆFPGAï¼‰åŸºäºå®ç°ï¼Œå¹¶ç”¨å…¶ä¸ SSO-NEMS è®¾å¤‡ä¸€èµ·ç ”ç©¶å…¶é¢‘åº¦è·Ÿè¸ªæ€§èƒ½ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†æˆ‘ä»¬çš„æ–¹æ³•ä¸é˜¶æ®µé”ç›¸æ§åˆ¶ï¼ˆPLLï¼‰ æ–¹æ¡ˆï¼Œ both theoretically and experimentallyã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œå¯ä»¥åœ¨æ›´ä½çš„æˆæœ¬å’Œæ›´å¥½çš„ä½¿ç”¨æ€§ä¸‹è¾¾åˆ°ç›¸åŒæˆ–æ›´å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬å®éªŒä¸­çš„ç»“æœä¸ç†è®ºé¢„æµ‹å‡ ä¹å®Œç¾åŒ¹é…ã€‚
</details></li>
</ul>
<hr>
<h2 id="Decision-Directed-Hybrid-RIS-Channel-Estimation-with-Minimal-Pilot-Overhead"><a href="#Decision-Directed-Hybrid-RIS-Channel-Estimation-with-Minimal-Pilot-Overhead" class="headerlink" title="Decision-Directed Hybrid RIS Channel Estimation with Minimal Pilot Overhead"></a>Decision-Directed Hybrid RIS Channel Estimation with Minimal Pilot Overhead</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11485">http://arxiv.org/abs/2309.11485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ly V. Nguyen, A. Lee Swindlehurst</li>
<li>for: æé«˜ç³»ç»Ÿspectral efficiencyï¼Œå‡å°‘é¢‘ç‡å¹²æ‰°ã€‚</li>
<li>methods: ä½¿ç”¨å…·æœ‰æ··åˆå…ƒä»¶çš„RISï¼ŒåŒæ—¶åå°„å’Œæ„ŸçŸ¥å…¥å°„ä¿¡å·ï¼Œæé«˜æ¸ é“çŠ¶æ€ä¿¡æ¯çš„å‡†ç¡®æ€§ã€‚</li>
<li>results: æ¯”ä¼ ç»Ÿé™è„‰RISæ•°ç»„ç³»ç»Ÿå…·æœ‰æ›´é«˜çš„ç³»ç»Ÿspectral efficiencyï¼Œå‡å°‘äº†é¢‘ç‡å¹²æ‰°ã€‚<details>
<summary>Abstract</summary>
To reap the benefits of reconfigurable intelligent surfaces (RIS), channel state information (CSI) is generally required. However, CSI acquisition in RIS systems is challenging and often results in very large pilot overhead, especially in unstructured channel environments. Consequently, the RIS channel estimation problem has attracted a lot of interest and also been a subject of intense study in recent years. In this paper, we propose a decision-directed RIS channel estimation framework for general unstructured channel models. The employed RIS contains some hybrid elements that can simultaneously reflect and sense the incoming signal. We show that with the help of the hybrid RIS elements, it is possible to accurately recover the CSI with a pilot overhead proportional to the number of users. Therefore, the proposed framework substantially improves the system spectral efficiency compared to systems with passive RIS arrays since the pilot overhead in passive RIS systems is proportional to the number of RIS elements times the number of users. We also perform a detailed spectral efficiency analysis for both the pilot-directed and decision-directed frameworks. Our analysis takes into account both the channel estimation and data detection errors at both the RIS and the BS. Finally, we present numerous simulation results to verify the accuracy of the analysis as well as to show the benefits of the proposed decision-directed framework.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šå¸¸éœ€è¦é€šé“çŠ¶æ€ä¿¡æ¯ï¼ˆCSIï¼‰æ¥æ”¶è·æ™ºèƒ½é‡é…ç½®è¡¨é¢ï¼ˆRISï¼‰çš„ä¼˜ç‚¹ã€‚ç„¶è€Œï¼Œåœ¨RISç³»ç»Ÿä¸­è·å–CSIæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§å’Œå¾ˆå¤§çš„å°è¯•é‡ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— ç»“æ„é€šé“ç¯å¢ƒä¸­ã€‚å› æ­¤ï¼ŒRISé€šé“ä¼°è®¡é—®é¢˜å·²ç»å¸å¼•äº†å¾ˆå¤šå…³æ³¨å¹¶æˆä¸ºäº†è¿‘å¹´æ¥çš„ç ”ç©¶ä¸»é¢˜ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æè®®äº†ä¸€ç§åŸºäºå†³ç­–çš„RISé€šé“ä¼°è®¡æ¡†æ¶ï¼Œé€‚ç”¨äºä¸€èˆ¬çš„æ— ç»“æ„é€šé“æ¨¡å‹ã€‚ employ çš„RISåŒ…å«äº†ä¸€äº›æ··åˆå…ƒç´ ï¼Œè¿™äº›å…ƒç´ å¯åŒæ—¶åå°„å’Œæ„ŸçŸ¥è¿›æ¥çš„ä¿¡å·ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨è¿™äº›æ··åˆRISå…ƒç´ ï¼Œå¯ä»¥å‡†ç¡®åœ°é‡å»ºCSIï¼Œå¹¶ä¸”å°è¯•é‡ä¸ç”¨æˆ·æ•°æˆæ­£æ¯”ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æè®®çš„æ¡†æ¶å¯ä»¥substantiallyæé«˜ç³»ç»Ÿspectral efficiencyï¼Œæ¯”pasive RISæ•°ç»„ç³»ç»Ÿæ›´é«˜ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†è¯¦ç»†çš„spectral efficiencyåˆ†æï¼ŒåŒ…æ‹¬é€šé“ä¼°è®¡å’Œæ•°æ®æ£€æµ‹é”™è¯¯åœ¨RISå’ŒBSä¹‹é—´ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†è®¸å¤šçš„ simulations ç»“æœï¼Œä»¥éªŒè¯åˆ†æçš„å‡†ç¡®æ€§ï¼Œä»¥åŠæ˜¾ç¤ºæè®®çš„å†³ç­–å¯¼å‘æ¡†æ¶çš„ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="Generalised-Hyperbolic-State-space-Models-for-Inference-in-Dynamic-Systems"><a href="#Generalised-Hyperbolic-State-space-Models-for-Inference-in-Dynamic-Systems" class="headerlink" title="Generalised Hyperbolic State-space Models for Inference in Dynamic Systems"></a>Generalised Hyperbolic State-space Models for Inference in Dynamic Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11422">http://arxiv.org/abs/2309.11422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaman KÄ±ndap, Simon Godsill</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æ¢è®¨è¿ç»­æ—¶é—´éæ ¼aussianæ»¤æ³¢é—®é¢˜ä¸­çš„éæ ¼aussianæ»¤æ³¢æ¨¡å‹ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºæ€»ä½“åŒ–å¼—æ´›ä¼¯æ©ï¼ˆGHï¼‰éšæœºè¿‡ç¨‹çš„è¿ç»­æ—¶é—´å‡å€¼åœºæ¨¡å‹ï¼Œå¹¶æä¾›äº†è¿ç»­æ—¶é—´æ¨¡æ‹Ÿæ–¹æ³•å’Œä¸€ç§åŸºäºMCMCçš„æ–°çš„æ¨æ–­æ–¹æ³•ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡é€šè¿‡åº”ç”¨åˆ°ä¸€ä¸ª sinteticallyç”Ÿæˆçš„æ•°æ®é›†å’Œä¸€ä¸ªå®é™…çš„é‡‘èæ—¶é—´åºåˆ—ä¸Šï¼Œä»¥ç¤ºå…¶èƒ½åŠ›ã€‚<details>
<summary>Abstract</summary>
In this work we study linear vector stochastic differential equation (SDE) models driven by the generalised hyperbolic (GH) L\'evy process for inference in continuous-time non-Gaussian filtering problems. The GH family of stochastic processes offers a flexible framework for modelling of non-Gaussian, heavy-tailed characteristics and includes the normal inverse-Gaussian, variance-gamma and Student-t processes as special cases. We present continuous-time simulation methods for the solution of vector SDE models driven by GH processes and novel inference methodologies using a variant of sequential Markov chain Monte Carlo (MCMC). As an example a particular formulation of Langevin dynamics is studied within this framework. The model is applied to both a synthetically generated data set and a real-world financial series to demonstrate its capabilities.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶çº¿æ€§å‘é‡æŠ½è±¡å·®åˆ†æ–¹ç¨‹ï¼ˆSDEï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”±æ€»ä½“åŒ–å¹‚ï¼ˆGHï¼‰éšæœºè¿‡ç¨‹é©±åŠ¨ã€‚GHéšæœºè¿‡ç¨‹å®¶æ—æä¾›éå¸¸çµæ´»çš„éé«˜å‡†å…¥ç‰¹æ€§æ¨¡å‹åŒ–æ¡†æ¶ï¼ŒåŒ…æ‹¬æ­£æ€åå°„å·®åˆ†ã€å·®åˆ†gammaå’Œå­¦ç”Ÿtè¿‡ç¨‹ä¸ºç‰¹æ®Šæƒ…å†µã€‚æˆ‘ä»¬æå‡ºäº†ç§¯ç´¯æ—¶é—´ simulationæ–¹æ³•æ¥è§£å†³vector SDEæ¨¡å‹ä¸­çš„GHè¿‡ç¨‹ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºMarkové“¾ Monte Carloï¼ˆMCMCï¼‰çš„æ–°çš„æ¨æ–­æ–¹æ³•ã€‚ä½œä¸ºä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§ç‰¹å®šçš„LangevinåŠ¨åŠ›å­¦å½¢å¼ã€‚è¯¥æ¨¡å‹åœ¨ä¸€ä¸ª sinteticallyç”Ÿæˆçš„æ•°æ®é›†å’Œä¸€ä¸ªå®é™…ä¸–ç•Œé‡‘èæ—¶é—´åºåˆ—ä¸Šè¿›è¡Œäº†åº”ç”¨ï¼Œä»¥ç¤ºå…¶èƒ½åŠ›ã€‚â€Note that Simplified Chinese is a romanization of Chinese, and the translation may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Active-Inference-for-Sum-Rate-Maximization-in-UAV-Assisted-Cognitive-NOMA-Networks"><a href="#Active-Inference-for-Sum-Rate-Maximization-in-UAV-Assisted-Cognitive-NOMA-Networks" class="headerlink" title="Active Inference for Sum Rate Maximization in UAV-Assisted Cognitive NOMA Networks"></a>Active Inference for Sum Rate Maximization in UAV-Assisted Cognitive NOMA Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11263">http://arxiv.org/abs/2309.11263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Obite, Ali Krayani, Atm S. Alam, Lucio Marcenaro, Arumugam Nallanathan, Carlo Regazzoni</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼ºåŒ–æœªæ¥æ— çº¿ç½‘ç»œçš„é€šä¿¡å®¹é‡ï¼Œä»¥æ»¡è¶³ç”±äº’è”ç½‘ç‰©è”ç½‘ï¼ˆIoTï¼‰ã€æ— äººæœºï¼ˆUAVï¼‰ã€è®¤çŸ¥ Ñ€Ğ°Ğ´Ğ¸Ğ¾ï¼ˆCRï¼‰å’Œå¤šæ’­è®¿é—®ï¼ˆNOMAï¼‰ç­‰æŠ€æœ¯å¼•èµ·çš„å·¨å¤§è¿æ¥é—®é¢˜ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†è®¤çŸ¥æ´»åŠ¨æ¨ç†ï¼ˆactive inferenceï¼‰ä»è®¤çŸ¥ç¥ç»ç§‘å­¦ä¸­å¯å‘ï¼Œå¹¶æå‡ºäº†ä¸€ç§åè°ƒå­é¢‘å’ŒåŠŸç‡åˆ†é…ç®—æ³•ï¼Œä»¥æœ€å¤§åŒ–æ€»æ¯”ç‰¹ç‡ã€‚</li>
<li>results:  simulationç»“æœè¡¨æ˜ï¼Œç›¸æ¯”benchmarkæ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºçš„ç®—æ³•å¯ä»¥æ›´å¥½åœ°é€‚åº”æ—¶é—´å˜åŒ–çš„ç½‘ç»œç¯å¢ƒï¼Œå¹¶æé«˜ç§¯æ€»æ¯”ç‰¹ç‡ã€‚<details>
<summary>Abstract</summary>
Given the surge in wireless data traffic driven by the emerging Internet of Things (IoT), unmanned aerial vehicles (UAVs), cognitive radio (CR), and non-orthogonal multiple access (NOMA) have been recognized as promising techniques to overcome massive connectivity issues. As a result, there is an increasing need to intelligently improve the channel capacity of future wireless networks. Motivated by active inference from cognitive neuroscience, this paper investigates joint subchannel and power allocation for an uplink UAV-assisted cognitive NOMA network. Maximizing the sum rate is often a highly challenging optimization problem due to dynamic network conditions and power constraints. To address this challenge, we propose an active inference-based algorithm. We transform the sum rate maximization problem into abnormality minimization by utilizing a generalized state-space model to characterize the time-changing network environment. The problem is then solved using an Active Generalized Dynamic Bayesian Network (Active-GDBN). The proposed framework consists of an offline perception stage, in which a UAV employs a hierarchical GDBN structure to learn an optimal generative model of discrete subchannels and continuous power allocation. In the online active inference stage, the UAV dynamically selects discrete subchannels and continuous power to maximize the sum rate of secondary users. By leveraging the errors in each episode, the UAV can adapt its resource allocation policies and belief updating to improve its performance over time. Simulation results demonstrate the effectiveness of our proposed algorithm in terms of cumulative sum rate compared to benchmark schemes.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€æ— çº¿æ•°æ®äº¤æ¢é‡çš„å¢åŠ ï¼Œå¯å‘äºäº’è”ç½‘å®‡å®™ï¼ˆIoTï¼‰ã€æ— äººæœºï¼ˆUAVï¼‰ã€è®¤çŸ¥ç”µæ³¢ï¼ˆCRï¼‰å’Œéå¯¹ç§°å¤šæ¥å…¥ï¼ˆNOMAï¼‰ç­‰æŠ€æœ¯çš„åº”ç”¨ï¼ŒFutureæ— çº¿ç½‘ç»œçš„é€šé“å®¹é‡éœ€è¦æ›´åŠ æ™ºèƒ½åœ°æé«˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸€æŒ‘æˆ˜ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ´»åŠ¨æ¨ç†çš„ JOINT å­é¢‘ç‡å’ŒåŠŸç‡åˆ†é…ç®—æ³•ã€‚é€šè¿‡å°†æ€»Bit rateæœ€å¤§åŒ–é—®é¢˜è½¬åŒ–ä¸ºå¼‚å¸¸å€¼æœ€å°åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸€ç§é€šç”¨çŠ¶æ€ç©ºé—´æ¨¡å‹æ¥æè¿°æ—¶é—´å˜åŒ–çš„ç½‘ç»œç¯å¢ƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ´»åŠ¨æ€»ä½“åŠ¨æ€ bayesian ç½‘ç»œï¼ˆActive-GDBNï¼‰è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬åœ¨çº¿ä¸Šactive inferenceé˜¶æ®µï¼Œåœ¨è¿™ä¸ªé˜¶æ®µï¼ŒUAVä½¿ç”¨ä¸€ä¸ªå±‚æ¬¡ç»“æ„çš„ GDBN ç»“æ„æ¥å­¦ä¹ ç²¾ç¡®çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ç²¾ç¡®çš„å­é¢‘ç‡å’Œè¿ç»­çš„åŠŸç‡åˆ†é…æ–¹é¢è¿›è¡Œæœ€ä½³åŒ–ã€‚åœ¨çº¿ä¸Šæ¿€æ´»æ¨ç†é˜¶æ®µï¼ŒUAVä¼šåœ¨ç²¾ç¡®çš„å­é¢‘ç‡å’Œè¿ç»­çš„åŠŸç‡åˆ†é…æ–¹é¢è¿›è¡ŒåŠ¨æ€é€‰æ‹©ï¼Œä»¥æœ€å¤§åŒ–æ¬¡çº§ç”¨æˆ·çš„æ€»Bit rateã€‚é€šè¿‡åˆ©ç”¨æ¯ä¸ªå›åˆä¸­çš„é”™è¯¯ï¼ŒUAVå¯ä»¥é€‚åº”å…¶èµ„æºåˆ†é…ç­–ç•¥å’Œä¿¡æ¯æ›´æ–°ï¼Œä»è€Œæé«˜å…¶æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ç®—æ³•åœ¨æ€»Bit rateæ–¹é¢ä¸å‚è€ƒæ–¹æ¡ˆç›¸æ¯”è¡¨ç°æ›´å¥½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Beamforming-Design-for-RIS-Aided-THz-Wideband-Communication-Systems"><a href="#Beamforming-Design-for-RIS-Aided-THz-Wideband-Communication-Systems" class="headerlink" title="Beamforming Design for RIS-Aided THz Wideband Communication Systems"></a>Beamforming Design for RIS-Aided THz Wideband Communication Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11161">http://arxiv.org/abs/2309.11161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihang Jiang, Ziqin Zhou, Xiaoyang Li, Yi Gong</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯è§£å†³åœ¨teraHzï¼ˆTHzï¼‰é€šä¿¡ç³»ç»Ÿä¸­çš„ç¬¼å½¢æ‹¥å¡«é—®é¢˜ï¼Œä»¥æé«˜æœªæ¥6Gç½‘ç»œçš„æ€§èƒ½ã€‚</li>
<li>methods: è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æŠ¤å¢™æ‰©å±•å™¨ï¼ˆRISï¼‰æ”¯æŒçš„ç¬¼å½¢æ‹¥å¡«æ¶æ„ï¼Œä»¥å‡å°‘ç¬¼å½¢æ‹¥å¡«çš„å½±å“ã€‚</li>
<li>results:  simulationsè¡¨æ˜ï¼Œæå‡ºçš„æ¶æ„èƒ½å¤Ÿæœ‰æ•ˆåœ°å‡å°‘ç¬¼å½¢æ‹¥å¡«çš„å½±å“ï¼Œæé«˜ç³»ç»Ÿçš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Benefiting from tens of GHz of bandwidth, terahertz (THz) communications has become a promising technology for future 6G networks. However, the conventional hybrid beamforming architecture based on frequency-independent phase-shifters is not able to cope with the beam split effect (BSE) in THz massive multiple-input multiple-output (MIMO) systems. Despite some work introducing the frequency-dependent phase shifts via the time delay network to mitigate the beam splitting in THz wideband communications, the corresponding issue in reconfigurable intelligent surface (RIS)-aided communications has not been well investigated. In this paper, the BSE in THz massive MIMO is quantified by analyzing the array gain loss. A new beamforming architecture has been proposed to mitigate this effect under RIS-aided communications scenarios. Simulations are performed to evaluate the effectiveness of the proposed system architecture in combating the array gain loss.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½¿ç”¨åå‡ GHzçš„å¸¦å®½ï¼ŒteraHzï¼ˆTHzï¼‰é€šä¿¡å·²æˆä¸ºæœªæ¥6Gç½‘ç»œçš„ä¿ƒè¿›æŠ€æœ¯ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ··åˆ beamformingæ¶æ„åŸºäºé¢‘ç‡ç‹¬ç«‹çš„ç›¸ä½è°ƒèŠ‚å™¨æ— æ³•åº”å¯¹THzå¤§è§„æ¨¡å¤šè¾“å…¥å¤šè¾“å‡ºï¼ˆMIMOï¼‰ç³»ç»Ÿä¸­çš„æŸåˆ†è£‚æ•ˆåº”ï¼ˆBSEï¼‰ã€‚è™½æœ‰ä¸€äº›å·¥ä½œä»‹ç»äº†é¢‘ç‡ç›¸å…³çš„ç›¸ä½åç§»é€šè¿‡æ—¶å»¶ç½‘ç»œæ¥mitigate THzå¹¿æ³›é€šä¿¡ä¸­çš„æŸåˆ†è£‚ï¼Œä½†ç›¸å…³çš„RISï¼ˆå¯ç¼–ç¨‹æ™ºèƒ½é¢ï¼‰ååŠ©é€šä¿¡åœºæ™¯çš„ç ”ç©¶å°šæœªå¾—åˆ°äº†å……åˆ†çš„æ¢è®¨ã€‚æœ¬æ–‡å¯¹THzå¤§è§„æ¨¡MIMOç³»ç»Ÿä¸­çš„BSEè¿›è¡Œäº†åˆ†æï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æŸåˆ†è£‚ Mitigation architectureã€‚é€šè¿‡å®éªŒè¯„ä¼°äº†æè®®ç³»ç»Ÿæ¶æ„çš„æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="Sum-Rate-Maximization-for-Movable-Antenna-Enabled-Multiuser-Communications"><a href="#Sum-Rate-Maximization-for-Movable-Antenna-Enabled-Multiuser-Communications" class="headerlink" title="Sum-Rate Maximization for Movable Antenna Enabled Multiuser Communications"></a>Sum-Rate Maximization for Movable Antenna Enabled Multiuser Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11135">http://arxiv.org/abs/2309.11135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenqiao Cheng, Nanxi Li, Jianchi Zhu, Xiaoming She, Chongjun Ouyang, Peng Chen</li>
<li>for: æé«˜ä¸‹é“¾ååé‡</li>
<li>methods: ä½¿ç”¨antennaä½ç½®ä¼˜åŒ–å’Œä¼ è¾“æ‰¬é€çŸ©é˜µä¼˜åŒ–</li>
<li>results: æé«˜ä¸‹é“¾ååé‡å’Œæ€§èƒ½æ¯”FPAsæ›´é«˜Hereâ€™s a more detailed explanation of each point:</li>
<li>for: The paper is written to propose a novel multiuser communication system with movable antennas (MAs) that can enhance the downlink sum-rate by exploiting the antenna position optimization.</li>
<li>methods: The paper uses a joint optimization of the transmit beamforming vector and transmit MA positions to solve the non-convex problem. The authors propose an efficient algorithm that combines fractional programming, alternating optimization, and gradient descent methods to tackle the problem. As an alternative, a zero-forcing beamforming-based design is also proposed to strike a better performance-complexity trade-off.</li>
<li>results: Numerical investigations show that the proposed algorithms achieve better performance compared with the benchmark relying on conventional fixed-position antennas (FPAs). The proposed system can improve the downlink sum-rate and provide better performance than FPAs.<details>
<summary>Abstract</summary>
A novel multiuser communication system with movable antennas (MAs) is proposed, where the antenna position optimization is exploited to enhance the downlink sum-rate. The joint optimization of the transmit beamforming vector and transmit MA positions is studied for a multiuser multiple-input single-input system. An efficient algorithm is proposed to tackle the formulated non-convex problem via capitalizing on fractional programming, alternating optimization, and gradient descent methods. To strike a better performance-complexity trade-off, a zero-forcing beamforming-based design is also proposed as an alternative. Numerical investigations are presented to verify the efficiency of the proposed algorithms and their superior performance compared with the benchmark relying on conventional fixed-position antennas (FPAs).
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°çš„å¤šç”¨æˆ·é€šä¿¡ç³»ç»Ÿï¼Œä½¿ç”¨å¯ç§»åŠ¨å¤©çº¿ï¼ˆMAï¼‰ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤©çº¿ä½ç½®ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥å¢åŠ ä¸‹é“¾æ•°æ®ç‡ã€‚åœ¨å¤šç”¨æˆ·å¤šè¾“å…¥å•è¾“å‡ºç³»ç»Ÿä¸­ï¼Œjointä¼˜åŒ–ä¼ è¾“æ‰©æ•£çŸ©é˜µå’Œå¤©çº¿ä½ç½®çš„ç®—æ³•è¢«ç ”ç©¶ã€‚é€šè¿‡ä½¿ç”¨åˆ†æ•°ç¼–ç¨‹ã€ alternateä¼˜åŒ–å’Œæ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ç®—æ³•ã€‚ä¸ºäº†æ›´å¥½åœ°å¹³è¡¡æ€§èƒ½å’Œå¤æ‚åº¦ä¹‹é—´çš„è´¸æ˜“ï¼Œä¹Ÿæå‡ºäº†ä¸€ç§åŸºäºé›¶å¹²æ‰°æ‰©æ•£çŸ©é˜µçš„è®¾è®¡ã€‚ numerically investigate the proposed algorithms and their superior performance compared with the benchmark relying on conventional fixed-position antennas (FPAs).Here is the word-for-word translation of the text into Simplified Chinese:æ–°çš„å¤šç”¨æˆ·é€šä¿¡ç³»ç»Ÿï¼Œä½¿ç”¨å¯ç§»åŠ¨å¤©çº¿ï¼ˆMAï¼‰ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å¤©çº¿ä½ç½®ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥å¢åŠ ä¸‹é“¾æ•°æ®ç‡ã€‚åœ¨å¤šç”¨æˆ·å¤šè¾“å…¥å•è¾“å‡ºç³»ç»Ÿä¸­ï¼Œjointä¼˜åŒ–ä¼ è¾“æ‰©æ•£çŸ©é˜µå’Œå¤©çº¿ä½ç½®çš„ç®—æ³•è¢«ç ”ç©¶ã€‚é€šè¿‡ä½¿ç”¨åˆ†æ•°ç¼–ç¨‹ã€ alternateä¼˜åŒ–å’Œæ¢¯åº¦ä¸‹é™æ–¹æ³•ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ç®—æ³•ã€‚ä¸ºäº†æ›´å¥½åœ°å¹³è¡¡æ€§èƒ½å’Œå¤æ‚åº¦ä¹‹é—´çš„è´¸æ˜“ï¼Œä¹Ÿæå‡ºäº†ä¸€ç§åŸºäºé›¶å¹²æ‰°æ‰©æ•£çŸ©é˜µçš„è®¾è®¡ã€‚ numerically investigate the proposed algorithms and their superior performance compared with the benchmark relying on conventional fixed-position antennas (FPAs).
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Mental-Stress-Among-College-Students-Using-Heart-Rate-and-Hand-Acceleration-Data-Collected-from-Wearable-Sensors"><a href="#Evaluating-Mental-Stress-Among-College-Students-Using-Heart-Rate-and-Hand-Acceleration-Data-Collected-from-Wearable-Sensors" class="headerlink" title="Evaluating Mental Stress Among College Students Using Heart Rate and Hand Acceleration Data Collected from Wearable Sensors"></a>Evaluating Mental Stress Among College Students Using Heart Rate and Hand Acceleration Data Collected from Wearable Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.11097">http://arxiv.org/abs/2309.11097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moein Razavi, Anthony McDonald, Ranjana Mehta, Farzan Sasangohar<br>for: This paper aims to develop a machine learning-based method for identifying stress using physiological data collected from college students.methods: The study uses wearable wrist-worn sensors and a mobile health application to collect heart rate and hand acceleration data, and self-reported stress data from college students. The XGBoost method was used to evaluate the effectiveness of the machine learning algorithms for stress detection.results: The study found that the XGBoost method was the most reliable model for identifying stress episodes, with an AUC of 0.64 and an accuracy of 84.5%. The standard deviation of hand acceleration, standard deviation of heart rate, and the minimum heart rate were the most important features for stress detection.<details>
<summary>Abstract</summary>
Stress is various mental health disorders including depression and anxiety among college students. Early stress diagnosis and intervention may lower the risk of developing mental illnesses. We examined a machine learning-based method for identification of stress using data collected in a naturalistic study utilizing self-reported stress as ground truth as well as physiological data such as heart rate and hand acceleration. The study involved 54 college students from a large campus who used wearable wrist-worn sensors and a mobile health (mHealth) application continuously for 40 days. The app gathered physiological data including heart rate and hand acceleration at one hertz frequency. The application also enabled users to self-report stress by tapping on the watch face, resulting in a time-stamped record of the self-reported stress. We created, evaluated, and analyzed machine learning algorithms for identifying stress episodes among college students using heart rate and accelerometer data. The XGBoost method was the most reliable model with an AUC of 0.64 and an accuracy of 84.5%. The standard deviation of hand acceleration, standard deviation of heart rate, and the minimum heart rate were the most important features for stress detection. This evidence may support the efficacy of identifying patterns in physiological reaction to stress using smartwatch sensors and may inform the design of future tools for real-time detection of stress.
</details>
<details>
<summary>æ‘˜è¦</summary>
stressæ˜¯å¤šç§å¤§å­¦ç”Ÿå¿ƒç†å¥åº·é—®é¢˜ï¼ŒåŒ…æ‹¬æŠ‘éƒå’Œç„¦è™‘ã€‚æ—©æœŸè¯†åˆ«å’Œ intervenciÃ³nå¯èƒ½é™ä½åˆ›å»ºå¿ƒç†ç–¾ç—…çš„é£é™©ã€‚æˆ‘ä»¬ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•æ¥è¯†åˆ«å¿ƒç†å‹åŠ›ï¼Œä½¿ç”¨è‡ªæŠ¥å‘Šå‹åŠ›ä½œä¸ºçœŸå®å‚ç…§æ•°æ®ï¼Œä»¥åŠPhysiologicalæ•°æ®ï¼Œå¦‚å¿ƒ rateå’Œæ‰‹åŠ¿åŠ é€Ÿåº¦ã€‚è¿™é¡¹ç ”ç©¶ä»å¤§å­¦æ ¡å›­ä¸­é‡‡é›†äº†54åå­¦ç”Ÿï¼Œä»–ä»¬ä½¿ç”¨è…•è¡¨ä»ªå’Œç§»åŠ¨åŒ»ç–—åº”ç”¨ç¨‹åºï¼Œè¿ç»­40å¤©æ”¶é›†æ•°æ®ã€‚åº”ç”¨ç¨‹åºè®°å½•äº†æ¯åˆ†é’Ÿä¸€æ¬¡çš„å¿ƒ rateå’Œæ‰‹åŠ¿åŠ é€Ÿåº¦æ•°æ®ï¼ŒåŒæ—¶ä¹Ÿè®©ç”¨æˆ·é€šè¿‡è§¦æ‘¸è…•è¡¨æ¥æŠ¥å‘Šå‹åŠ›ï¼Œä»è€Œè·å¾—äº†æ—¶é—´æˆ³çš„è‡ªæŠ¥å‘Šå‹åŠ›è®°å½•ã€‚æˆ‘ä»¬åˆ›å»ºã€è¯„ä¼°å’Œåˆ†æäº†æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œç”¨äºåœ¨å¤§å­¦ç”Ÿä¸­è¯†åˆ«å‹åŠ›å‘ä½œã€‚XGBoostæ–¹æ³•æ˜¯æœ€å¯é çš„æ¨¡å‹ï¼ŒAUCä¸º0.64ï¼Œå‡†ç¡®ç‡ä¸º84.5%ã€‚æ‰‹åŠ¿åŠ é€Ÿåº¦çš„æ ‡å‡†å·®ã€å¿ƒ rateçš„æ ‡å‡†å·®å’Œæœ€ä½å¿ƒ rateæ˜¯æœ€é‡è¦çš„å‹åŠ›æ£€æµ‹ç‰¹å¾ã€‚è¿™äº›è¯æ®å¯èƒ½æ”¯æŒé€šè¿‡æ™ºèƒ½æ‰‹è¡¨æ„ŸçŸ¥å™¨æ£€æµ‹å‹åŠ›çš„Patternï¼Œå¹¶ä¸”å¯èƒ½å¯¼å‘æœªæ¥çš„å®æ—¶å‹åŠ›æ£€æµ‹å·¥å…·çš„è®¾è®¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Pointing-and-Acquisition-for-Optical-Wireless-in-6G-From-Algorithms-to-Performance-Evaluation"><a href="#Pointing-and-Acquisition-for-Optical-Wireless-in-6G-From-Algorithms-to-Performance-Evaluation" class="headerlink" title="Pointing-and-Acquisition for Optical Wireless in 6G: From Algorithms to Performance Evaluation"></a>Pointing-and-Acquisition for Optical Wireless in 6G: From Algorithms to Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.10999">http://arxiv.org/abs/2309.10999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyung-Joo Moon, Chan-Byoung Chae, Kai-Kit Wong, Mohamed-Slim Alouini</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨æ¢è®¨éåœ°é¢ç½‘ç»œçš„å‘å±•å’Œè‡ªç”±ç©ºé—´å…‰å­¦é€šä¿¡æŠ€æœ¯çš„åº”ç”¨ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¼ ç»Ÿè®¾å¤‡å’Œæœºåˆ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®—æ³•ï¼Œé€šè¿‡æŠ›ç‰©çº¿é“¾å’Œåå°„å™¨æ¥å®ç°è§’åº¦ä¼°è®¡å’Œæ‰«æã€‚</li>
<li>results: é€šè¿‡å¤§é‡çš„ simulationsï¼Œè®ºæ–‡è¡¨æ˜ï¼Œæè®®çš„æ–¹æ³•å¯ä»¥æä¾›æ›´å¥½çš„é“¾æ¥å’Œç»´æŠ¤æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
The increasing demand for wireless communication services has led to the development of non-terrestrial networks, which enables various air and space applications. Free-space optical (FSO) communication is considered one of the essential technologies capable of connecting terrestrial and non-terrestrial layers. In this article, we analyze considerations and challenges for FSO communications between gateways and aircraft from a pointing-and-acquisition perspective. Based on the analysis, we first develop a baseline method that utilizes conventional devices and mechanisms. Furthermore, we propose an algorithm that combines angle of arrival (AoA) estimation through supplementary radio frequency (RF) links and beam tracking using retroreflectors. Through extensive simulations, we demonstrate that the proposed method offers superior performance in terms of link acquisition and maintenance.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€æ— çº¿é€šä¿¡æœåŠ¡çš„å¢åŠ éœ€æ±‚ï¼Œéåœ°çƒç½‘ç»œçš„å‘å±•å·²ç»æ¨åŠ¨äº†å„ç§ç©ºå¤©åº”ç”¨ã€‚è‡ªç”±ç©ºé—´å…‰å­¦ï¼ˆFSOï¼‰é€šä¿¡è¢«è®¤ä¸ºæ˜¯è¿æ¥åœ°çƒå’Œéåœ°çƒå±‚çš„é‡è¦æŠ€æœ¯ä¹‹ä¸€ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†FSOé€šä¿¡ Ğ¼ĞµĞ¶Ğ´Ñƒç½‘å…³å’Œé£æœºä»æŒ‡å‘å’Œæ•è·è§’åº¦æ¥è€ƒè™‘çš„è€ƒè™‘å› ç´ ã€‚æ ¹æ®åˆ†æç»“æœï¼Œæˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä½¿ç”¨æ™®é€šè®¾å¤‡å’Œæœºåˆ¶çš„åŸºçº¿æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æè®®ä¸€ç§æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨è¡¥å……Radioé¢‘ä¿¡å·çš„è§’åº¦ä¼°è®¡å’Œåå°„å™¨ beam Trackingã€‚é€šè¿‡å¹¿æ³›çš„ simulationsï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬æè®®çš„æ–¹æ³•å¯ä»¥æä¾›æ›´é«˜çš„é“¾æ¥å’Œç»´æŒé“¾æ¥æ€§èƒ½ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/20/eess.SP_2023_09_20/" data-id="clopawo30019rag88gucv2jec" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/34/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="page-number" href="/page/34/">34</a><span class="page-number current">35</span><a class="page-number" href="/page/36/">36</a><a class="page-number" href="/page/37/">37</a><span class="space">&hellip;</span><a class="page-number" href="/page/87/">87</a><a class="extend next" rel="next" href="/page/36/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">57</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">116</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">67</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
