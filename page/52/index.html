
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/52/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.AI_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T12:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.AI_2023_08_31/">cs.AI - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="PointLLM-Empowering-Large-Language-Models-to-Understand-Point-Clouds"><a href="#PointLLM-Empowering-Large-Language-Models-to-Understand-Point-Clouds" class="headerlink" title="PointLLM: Empowering Large Language Models to Understand Point Clouds"></a>PointLLM: Empowering Large Language Models to Understand Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16911">http://arxiv.org/abs/2308.16911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrobotlab/pointllm">https://github.com/openrobotlab/pointllm</a></li>
<li>paper_authors: Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin</li>
<li>for: 本研究旨在将大型自然语言模型（LLM）应用于三维理解，以扩展其现有的二维视觉处理能力。</li>
<li>methods: 本研究使用PointLLM，一个初步尝试将点云资料与LLM结合，以便理解点云并生成相应的回应。PointLLM使用点云编码器与强大的LLM进行有效地融合几何、外观和语言信息。</li>
<li>results:  experiments show that PointLLM outperforms existing 2D baselines and demonstrates superior performance in human-evaluated object captioning tasks, with human annotators being outperformed in over 50% of the samples.<details>
<summary>Abstract</summary>
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establish two benchmarks: Generative 3D Object Classification and 3D Object Captioning, assessed through three different methods, including human evaluation, GPT-4/ChatGPT evaluation, and traditional metrics. Experiment results show that PointLLM demonstrates superior performance over existing 2D baselines. Remarkably, in human-evaluated object captioning tasks, PointLLM outperforms human annotators in over 50% of the samples. Codes, datasets, and benchmarks are available at https://github.com/OpenRobotLab/PointLLM .
</details>
<details>
<summary>摘要</summary>
大量的自然语言处理技术（LLM）已经创造出历史上无 precedent的进步，但是它们还未全面掌握三维理解。这篇论文介绍了PointLLM，一项初步尝试，以填补这一空白，使得 LLM 可以理解点云并提供一条新的探索途径，超出了2D视觉数据的限制。PointLLM 处理了人工指令颜色点云，并生成了上下文相应的响应，这表明它对点云和常识有深刻的理解。具体来说，它利用了一个强大的 LLM 和点云编码器，以有效地融合几何、外观和语言信息。我们收集了一个新的数据集，包括660,000个简单点云和70,000个复杂点云文本指令对，以实现两个阶段的训练策略：首先是将 latent space 对齐，然后是通过指令调整已经一体化的模型。为了严格评估我们模型的感知能力和总体化能力，我们建立了两个标准准则：生成3D物体分类和3D物体描述，通过三种不同的评价方法，包括人工评估、GPT-4/ChatGPT评估和传统指标。实验结果表明，PointLLM 在已有的2D基线上表现出色，并且在人工评估3D物体描述任务中，PointLLM 在50%以上的样本中超过了人类评估员。代码、数据集和标准准则可以在 GitHub 上获取，请参考 <https://github.com/OpenRobotLab/PointLLM>。
</details></li>
</ul>
<hr>
<h2 id="StyleInV-A-Temporal-Style-Modulated-Inversion-Network-for-Unconditional-Video-Generation"><a href="#StyleInV-A-Temporal-Style-Modulated-Inversion-Network-for-Unconditional-Video-Generation" class="headerlink" title="StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation"></a>StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16909">http://arxiv.org/abs/2308.16909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/johannwyh/styleinv">https://github.com/johannwyh/styleinv</a></li>
<li>paper_authors: Yuhan Wang, Liming Jiang, Chen Change Loy</li>
<li>for: 高质量视频生成</li>
<li>methods: 学习倒整流程网络做动作生成器，具有稳定的 Temporal 协调和多样化的 Style 转换能力</li>
<li>results: 能够生成高分辨率、长寿命、具有单帧质量和时间协调的视频<details>
<summary>Abstract</summary>
Unconditional video generation is a challenging task that involves synthesizing high-quality videos that are both coherent and of extended duration. To address this challenge, researchers have used pretrained StyleGAN image generators for high-quality frame synthesis and focused on motion generator design. The motion generator is trained in an autoregressive manner using heavy 3D convolutional discriminators to ensure motion coherence during video generation. In this paper, we introduce a novel motion generator design that uses a learning-based inversion network for GAN. The encoder in our method captures rich and smooth priors from encoding images to latents, and given the latent of an initially generated frame as guidance, our method can generate smooth future latent by modulating the inversion encoder temporally. Our method enjoys the advantage of sparse training and naturally constrains the generation space of our motion generator with the inversion network guided by the initial frame, eliminating the need for heavy discriminators. Moreover, our method supports style transfer with simple fine-tuning when the encoder is paired with a pretrained StyleGAN generator. Extensive experiments conducted on various benchmarks demonstrate the superiority of our method in generating long and high-resolution videos with decent single-frame quality and temporal consistency.
</details>
<details>
<summary>摘要</summary>
不受限制的视频生成是一项具有挑战性的任务，旨在生成高质量的视频，同时保持视频的凝聚性和长度。为 Addressing this challenge, researchers have used pre-trained StyleGAN image generators for high-quality frame synthesis and focused on motion generator design. In this paper, we propose a novel motion generator design that uses a learning-based inversion network for GAN. Our method captures rich and smooth priors from encoding images to latents, and given the latent of an initially generated frame as guidance, our method can generate smooth future latent by modulating the inversion encoder temporally. Our method enjoys the advantage of sparse training and naturally constrains the generation space of our motion generator with the inversion network guided by the initial frame, eliminating the need for heavy discriminators. Moreover, our method supports style transfer with simple fine-tuning when the encoder is paired with a pretrained StyleGAN generator. Extensive experiments conducted on various benchmarks demonstrate the superiority of our method in generating long and high-resolution videos with decent single-frame quality and temporal consistency.
</details></li>
</ul>
<hr>
<h2 id="InterDiff-Generating-3D-Human-Object-Interactions-with-Physics-Informed-Diffusion"><a href="#InterDiff-Generating-3D-Human-Object-Interactions-with-Physics-Informed-Diffusion" class="headerlink" title="InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion"></a>InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16905">http://arxiv.org/abs/2308.16905</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sirui-Xu/InterDiff">https://github.com/Sirui-Xu/InterDiff</a></li>
<li>paper_authors: Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, Liang-Yan Gui</li>
<li>for: 这篇论文目标是解决3D人际物互动（HOI）的新任务，大多数现有的HOI合成研究仅仅对小物件或静止物件进行了限制，这个任务更加具有挑战性，因为它需要模型动态物件，捕捉全身动作，并确保物件之间的物理关联性。</li>
<li>methods: 我们提出了一个名为InterDiff的框架，包括两个关键步骤：（i）互动扩散，我们利用扩散模型将未来人际物互动的分布编码为Future HOI distribution；（ii）互动修正，我们引入物理学 Informed predictor，对扩散步骤中的噪声HOI进行修正。我们的关键见解是将与接触点的互动视为一个简单的模式，容易预测。</li>
<li>results: 我们在多个人际物互动数据集上进行了实验，结果显示我们的方法能够生成真实、生动、remarkably Long-term 3D HOI预测。<details>
<summary>Abstract</summary>
This paper addresses a novel task of anticipating 3D human-object interactions (HOIs). Most existing research on HOI synthesis lacks comprehensive whole-body interactions with dynamic objects, e.g., often limited to manipulating small or static objects. Our task is significantly more challenging, as it requires modeling dynamic objects with various shapes, capturing whole-body motion, and ensuring physically valid interactions. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to encode the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor to correct denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable. Experiments on multiple human-object interaction datasets demonstrate the effectiveness of our method for this task, capable of producing realistic, vivid, and remarkably long-term 3D HOI predictions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Interaction Diffusion: A diffusion model is used to encode the distribution of future human-object interactions.2. Interaction Correction: A physics-informed predictor is introduced to correct the denoised HOIs in the diffusion step.The key insight of the proposed method is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable.Experiments on multiple human-object interaction datasets demonstrate the effectiveness of the proposed method in producing realistic, vivid, and remarkably long-term 3D HOI predictions.</details></li>
</ol>
<hr>
<h2 id="Transformers-as-Support-Vector-Machines"><a href="#Transformers-as-Support-Vector-Machines" class="headerlink" title="Transformers as Support Vector Machines"></a>Transformers as Support Vector Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16898">http://arxiv.org/abs/2308.16898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mohamedehab00/A-Hybrid-Arabic-Text-Summarization-Approach-based-on-Transformers">https://github.com/mohamedehab00/A-Hybrid-Arabic-Text-Summarization-Approach-based-on-Transformers</a></li>
<li>paper_authors: Davoud Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, Samet Oymak</li>
<li>for: 这个论文主要是为了解释transformer架构中的自注意力层的优化方法和其在自然语言处理中的应用。</li>
<li>methods: 这篇论文使用了一种形式化的方法，将自注意力层的优化问题与硬margin支持向量机（SVM）问题等同起来，从而可以更好地理解transformer架构中的自注定对象的优化方法。</li>
<li>results: 研究人员通过这种形式化的方法，发现了一些关于transformer架构中自注意力层的优化方法的重要特征，包括优化方法的本质和global&#x2F;local方向的导航方式等。此外，他们还提出了一些开放的问题和研究方向，以便进一步探索transformer架构的应用和优化方法。<details>
<summary>Abstract</summary>
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. We characterize this convergence, highlighting that it can occur toward locally-optimal directions rather than global ones. (2) Complementing this, we prove the local/global directional convergence of gradient descent under suitable geometric conditions. Importantly, we show that over-parameterization catalyzes global convergence by ensuring the feasibility of the SVM problem and by guaranteeing a benign optimization landscape devoid of stationary points. (3) While our theory applies primarily to linear prediction heads, we propose a more general SVM equivalence that predicts the implicit bias with nonlinear heads. Our findings are applicable to arbitrary datasets and their validity is verified via experiments. We also introduce several open problems and research directions. We believe these findings inspire the interpretation of transformers as a hierarchy of SVMs that separates and selects optimal tokens.
</details>
<details>
<summary>摘要</summary>
自“专注是所有你需要”这个起源，transformer架构带来了启蒸的进步在自然语言处理（NLP）领域。内部的专注层在transformer架构中让输入序列$X$进行互动，通过 ComputeSoftmax$(XQK^\top X^\top)$中的对称关系，其中$(K,Q)$是可变的钥匙-请求参数。在这篇文章中，我们建立了对专注层的自动化构造和硬margin Support Vector Machine（SVM）问题之间的正式等价性。这个等价性让我们可以对1层transformer的专注层优化器（K,Q）的对称关系进行描述，并且评估这个对称关系的内部偏好。我们的研究结果如下：1. 对于具有干扰调整的专注层优化器（K,Q），当运算在干扰调整下时，专注层的优化将 converges到一个硬margin SVM解释，这个解释可以最小化综合参数$W=KQ^\top$的核心 нор。相反，直接对$W$进行优化则会导致一个弹性范围的对应目标。我们描述了这个对称关系的传播，并证明它可以发生在本地优化方向上，而不是全局优化方向上。2. 我们还证明了在适当的几何条件下，对于具有干扰调整的专注层优化器（K,Q），gradient descent将在本地和全球方向上传播。另外，我们显示了过 parameterization 可以刺激全球传播，并且保证搜索空间的稳定性和缺乏站点。3. 我们的理论主要适用于线性预测头，但我们提出了一个更一般的SVM等价性，可以预测专注层的隐含偏好。我们的发现适用于任意的数据集和预测任务，并且我们透过实验验证了我们的理论。 finally，我们提出了一些开放的问题和研究方向。我们认为这些发现可以启发人们对transformer架构的解释，将其视为一个对称层次的SVM，用于分类和选择最佳的节点。
</details></li>
</ul>
<hr>
<h2 id="PointOcc-Cylindrical-Tri-Perspective-View-for-Point-based-3D-Semantic-Occupancy-Prediction"><a href="#PointOcc-Cylindrical-Tri-Perspective-View-for-Point-based-3D-Semantic-Occupancy-Prediction" class="headerlink" title="PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction"></a>PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16896">http://arxiv.org/abs/2308.16896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wzzheng/pointocc">https://github.com/wzzheng/pointocc</a></li>
<li>paper_authors: Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, Jiwen Lu</li>
<li>for: 本文目的是提出一种高效的点云 semantic segmentation 方法，用于自动驾驶中的Scene understanding。</li>
<li>methods: 该方法使用 cylindrical tri-perspective view (TPV) 来表示点云，并使用 PointOcc 模型来处理它们。具体来说，该方法使用 distance distribution 来构建TPV，并使用空间群集 pooling 来保持点云的结构细节。最后，该方法使用 2D 脊梁来高效地处理每个 TPV 面，并使用 PointOcc 模型来汇聚每个点的特征。</li>
<li>results: 对于 3D occupancy prediction 和 LiDAR segmentation benchmark，PointOcc 方法 achieved state-of-the-art 性能，并且比其他方法快得多。具体来说，只使用 LiDAR 数据的 PointOcc 方法在 OpenOccupancy benchmark 上大幅超越了所有其他方法，包括多模式方法。<details>
<summary>Abstract</summary>
Semantic segmentation in autonomous driving has been undergoing an evolution from sparse point segmentation to dense voxel segmentation, where the objective is to predict the semantic occupancy of each voxel in the concerned 3D space. The dense nature of the prediction space has rendered existing efficient 2D-projection-based methods (e.g., bird's eye view, range view, etc.) ineffective, as they can only describe a subspace of the 3D scene. To address this, we propose a cylindrical tri-perspective view to represent point clouds effectively and comprehensively and a PointOcc model to process them efficiently. Considering the distance distribution of LiDAR point clouds, we construct the tri-perspective view in the cylindrical coordinate system for more fine-grained modeling of nearer areas. We employ spatial group pooling to maintain structural details during projection and adopt 2D backbones to efficiently process each TPV plane. Finally, we obtain the features of each point by aggregating its projected features on each of the processed TPV planes without the need for any post-processing. Extensive experiments on both 3D occupancy prediction and LiDAR segmentation benchmarks demonstrate that the proposed PointOcc achieves state-of-the-art performance with much faster speed. Specifically, despite only using LiDAR, PointOcc significantly outperforms all other methods, including multi-modal methods, with a large margin on the OpenOccupancy benchmark. Code: https://github.com/wzzheng/PointOcc.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>自动驾驶 semantic segmentation 在进化中，从稀疏点 segmentation 转向紧凑的 voxel segmentation，目标是预测每个 voxel 在关注的 3D 空间中的semantic occupancy。紧凑的预测空间使得现有的高效 2D 投影基于方法（如鸟瞰视、距离视图等）无法描述 3D 场景中的所有信息，因此我们提出了一种筒形三视角视图来有效地处理点云，并使用 PointOcc 模型来处理它们。根据 LiDAR 点云的距离分布，我们在筒形坐标系中构建了三视角视图，以更细化近距离区域的模型化。我们使用空间组合池化以保持结构细节，并采用 2D 脊梁来高效处理每个 TPV 面。最后，我们通过对每个点的 проекted 特征进行聚合来获得每个点的特征，无需任何后处理。广泛的实验表明，我们的 PointOcc 模型在 3D 占用率预测和 LiDAR 分割 benchmark 上具有州先进性，并且具有更快的速度。具体来说，只使用 LiDAR 的 PointOcc 模型可以在 OpenOccupancy benchmark 上大幅超越所有其他方法，包括多modal方法，并且具有大的差距。代码：https://github.com/wzzheng/PointOcc。
</details></li>
</ul>
<hr>
<h2 id="Language-Conditioned-Path-Planning"><a href="#Language-Conditioned-Path-Planning" class="headerlink" title="Language-Conditioned Path Planning"></a>Language-Conditioned Path Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16893">http://arxiv.org/abs/2308.16893</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Amber Xie, Youngwoon Lee, Pieter Abbeel, Stephen James</li>
<li>for:  This paper focuses on the problem of path planning for robotic manipulation tasks, specifically in contact-rich environments.</li>
<li>methods: The proposed method is called Language-Conditioned Collision Functions (LACO), which uses a single-view image, language prompt, and robot configuration to learn a collision function and enable flexible, conditional path planning.</li>
<li>results: The authors demonstrate the effectiveness of LACO in both simulation and real-world experiments, showing that it can facilitate complex, nuanced path plans that allow for safe collisions with objects in the environment.<details>
<summary>Abstract</summary>
Contact is at the core of robotic manipulation. At times, it is desired (e.g. manipulation and grasping), and at times, it is harmful (e.g. when avoiding obstacles). However, traditional path planning algorithms focus solely on collision-free paths, limiting their applicability in contact-rich tasks. To address this limitation, we propose the domain of Language-Conditioned Path Planning, where contact-awareness is incorporated into the path planning problem. As a first step in this domain, we propose Language-Conditioned Collision Functions (LACO) a novel approach that learns a collision function using only a single-view image, language prompt, and robot configuration. LACO predicts collisions between the robot and the environment, enabling flexible, conditional path planning without the need for manual object annotations, point cloud data, or ground-truth object meshes. In both simulation and the real world, we demonstrate that LACO can facilitate complex, nuanced path plans that allow for interaction with objects that are safe to collide, rather than prohibiting any collision.
</details>
<details>
<summary>摘要</summary>
“联系”是 robotic manipulation 的核心。有时候需要联系（例如操作和抓取），有时候则需要避免触碰（例如避免障碍物）。然而，传统的路径观察算法仅专注于避免冲突的路径，这限制了它们在联系丰富任务中的应用范围。为了解决这个限制，我们提出了“语言条件路径观察”领域，其中联系意识被包含到路径观察问题中。作为这个领域的第一步，我们提出了一种新的方法：Language-Conditioned Collision Functions (LACO)。LACO 是一种学习冲突函数的方法，它使用单一的图像、语言提示和机器人配置来学习冲突。LACO 预测机器人和环境之间的冲突，允许机器人进行自动、 conditional 的路径观察，不需要手动设定物体标注、点云资料或真实物体对应。在实验和实际情况下，我们证明了 LACO 可以实现复杂、细节的路径观察，允许机器人与安全冲突的物体进行互动，而不是禁止任何冲突。
</details></li>
</ul>
<hr>
<h2 id="ReZero-Region-customizable-Sound-Extraction"><a href="#ReZero-Region-customizable-Sound-Extraction" class="headerlink" title="ReZero: Region-customizable Sound Extraction"></a>ReZero: Region-customizable Sound Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16892">http://arxiv.org/abs/2308.16892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongzhi Gu, Yi Luo</li>
<li>for: 这篇论文是为了解决多通道区域特定声音提取任务（R-SE）而写的。</li>
<li>methods: 这篇论文使用了不同类型的空间区域定义（如角度窗口、球体、喷气体等），以及对这些区域的特征提取和聚合方法。它还使用了多通道扩展的带分RNN（BSRNN）模型，特制 для R-SE任务。</li>
<li>results: 实验结果表明，ReZero在不同的麦克风数据格式和系统配置下都有效，并且在 simulate 和实际记录的数据上都达到了高度的提取精度。详细的实验结果和演示可以在 <a target="_blank" rel="noopener" href="https://innerselfm.github.io/rezero/">https://innerselfm.github.io/rezero/</a> 上查看。<details>
<summary>Abstract</summary>
We introduce region-customizable sound extraction (ReZero), a general and flexible framework for the multi-channel region-wise sound extraction (R-SE) task. R-SE task aims at extracting all active target sounds (e.g., human speech) within a specific, user-defined spatial region, which is different from conventional and existing tasks where a blind separation or a fixed, predefined spatial region are typically assumed. The spatial region can be defined as an angular window, a sphere, a cone, or other geometric patterns. Being a solution to the R-SE task, the proposed ReZero framework includes (1) definitions of different types of spatial regions, (2) methods for region feature extraction and aggregation, and (3) a multi-channel extension of the band-split RNN (BSRNN) model specified for the R-SE task. We design experiments for different microphone array geometries, different types of spatial regions, and comprehensive ablation studies on different system configurations. Experimental results on both simulated and real-recorded data demonstrate the effectiveness of ReZero. Demos are available at https://innerselfm.github.io/rezero/.
</details>
<details>
<summary>摘要</summary>
我们介绍一个通用和灵活的概念抽取框架（ReZero），用于多通道区域特定声音抽取（R-SE）任务。R-SE任务的目标是在用户定义的特定空间区域内抽取所有活动目标声音（例如人类语音），而不是传统的盲目分离或预先定义的空间区域。用户可以定义空间区域为角度窗口、球体、圆锥体或其他几何图形。作为R-SE任务的解决方案，我们的ReZero框架包括以下几个方面：1. 不同类型的空间区域定义2. 空间区域特征提取和聚合方法3. 适用于R-SE任务的多通道扩展的带谱RNN（BSRNN）模型我们设计了不同的麦克风数据列表，不同类型的空间区域，以及对不同系统配置进行了全面的减少研究。实验结果表明，ReZero在模拟和真实记录的数据上具有效果。 demo 可以在 <https://innerselfm.github.io/rezero/> 上找到。
</details></li>
</ul>
<hr>
<h2 id="The-Belebele-Benchmark-a-Parallel-Reading-Comprehension-Dataset-in-122-Language-Variants"><a href="#The-Belebele-Benchmark-a-Parallel-Reading-Comprehension-Dataset-in-122-Language-Variants" class="headerlink" title="The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants"></a>The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16884">http://arxiv.org/abs/2308.16884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, Madian Khabsa</li>
<li>for: 这个论文的目的是扩展自然语言理解（NLU）标准套件中的语言覆盖率，并评估多种语言模型在不同语言环境下的表现。</li>
<li>methods: 这个论文使用了一个多选机器阅读理解（MRC）数据集，包括122种语言变种，以评估文本模型在不同语言环境下的表现。每个问题基于一篇Flores-200数据集中的短文章，并提供了四个多选答案。</li>
<li>results: 这个论文的结果表明，尽管英语中心的大语言模型（LLM）在多语言环境下具有较高的泛化能力，但是较小的多语言模型（MLM）在多语言环境下仍能够理解更多的语言。此外，研究发现大词汇量和意识construct vocabulary对低资源语言的表现有着正面的关系。<details>
<summary>Abstract</summary>
We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. We also observe that larger vocabulary size and conscious vocabulary construction correlate with better performance on low-resource languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.
</details>
<details>
<summary>摘要</summary>
我们介绍了Belebele，一个多选机器读取理解（MRC）数据集，覆盖122种语言变体。这个数据集将提高自然语言理解（NLU）标准 benchmarks 的语言覆盖率，并允许evaluate文本模型在高-,中-,低-资源语言中的表现。每个问题基于Flores-200数据集中的短段文本，有四个多选答案。问题被仔细制定，以区分不同水平的通用语言理解能力。英语数据集本身也足够挑战当前语言模型。这个数据集完全平行，可以直接比较所有语言的模型性能。我们使用这个数据集评估多语言掩码语言模型（MLM）和大语言模型（LLM）的能力。我们发表了广泛的结果，发现虽然英语中心的LLMs具有显著的cross-lingual transfer，但是 Much smaller MLMs pretrained on balance multilingual data仍然能够理解更多的语言。我们还发现，大 vocabulary size和conscious vocabulary construction相关于低资源语言中的表现。总的来说，Belebele开启了新的评估和分析多语言NLP系统的avenues。
</details></li>
</ul>
<hr>
<h2 id="Adaptation-Speed-Analysis-for-Fairness-aware-Causal-Models"><a href="#Adaptation-Speed-Analysis-for-Fairness-aware-Causal-Models" class="headerlink" title="Adaptation Speed Analysis for Fairness-aware Causal Models"></a>Adaptation Speed Analysis for Fairness-aware Causal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16879">http://arxiv.org/abs/2308.16879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujie Lin, Chen Zhao, Minglai Shao, Xujiang Zhao, Haifeng Chen</li>
<li>For: This paper explores the adaptation of two models to a domain shift in the presence of a sensitive variable (bias) in a structural causal model (SCM) with a cause-bias-effect structure.* Methods: The paper uses two models with opposite directions to align the original distribution p with the modified distribution p* due to an unknown intervention. The adaptation speeds of the two models are compared across four shift scenarios, and the connection between the adaptation speeds is proven.* Results: The paper examines the adaptation of two models to a domain shift in the presence of a sensitive variable (bias) and compares their adaptation speeds across four shift scenarios, proving the connection between the adaptation speeds of the two models across all interventions.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文研究了在Structural Causal Model（SCM）中存在敏感变量（偏见）的域转移问题中，两个模型对域转移的适应。</li>
<li>methods: 这篇论文使用了两个模型，每个模型都有相反的方向，以将原始分布p与 modify分布p*进行对应。</li>
<li>results: 这篇论文对两个模型在域转移问题中的适应速度进行了比较，并证明了两个模型在所有干扰情况下的适应速度之间的连接。<details>
<summary>Abstract</summary>
For example, in machine translation tasks, to achieve bidirectional translation between two languages, the source corpus is often used as the target corpus, which involves the training of two models with opposite directions. The question of which one can adapt most quickly to a domain shift is of significant importance in many fields. Specifically, consider an original distribution p that changes due to an unknown intervention, resulting in a modified distribution p*. In aligning p with p*, several factors can affect the adaptation rate, including the causal dependencies between variables in p. In real-life scenarios, however, we have to consider the fairness of the training process, and it is particularly crucial to involve a sensitive variable (bias) present between a cause and an effect variable. To explore this scenario, we examine a simple structural causal model (SCM) with a cause-bias-effect structure, where variable A acts as a sensitive variable between cause (X) and effect (Y). The two models, respectively, exhibit consistent and contrary cause-effect directions in the cause-bias-effect SCM. After conducting unknown interventions on variables within the SCM, we can simulate some kinds of domain shifts for analysis. We then compare the adaptation speeds of two models across four shift scenarios. Additionally, we prove the connection between the adaptation speeds of the two models across all interventions.
</details>
<details>
<summary>摘要</summary>
Original text:In machine translation tasks, to achieve bidirectional translation between two languages, the source corpus is often used as the target corpus, which involves the training of two models with opposite directions. The question of which one can adapt most quickly to a domain shift is of significant importance in many fields. Specifically, consider an original distribution p that changes due to an unknown intervention, resulting in a modified distribution p*. In aligning p with p*, several factors can affect the adaptation rate, including the causal dependencies between variables in p. In real-life scenarios, however, we have to consider the fairness of the training process, and it is particularly crucial to involve a sensitive variable (bias) present between a cause and an effect variable. To explore this scenario, we examine a simple structural causal model (SCM) with a cause-bias-effect structure, where variable A acts as a sensitive variable between cause (X) and effect (Y). The two models, respectively, exhibit consistent and contrary cause-effect directions in the cause-bias-effect SCM. After conducting unknown interventions on variables within the SCM, we can simulate some kinds of domain shifts for analysis. We then compare the adaptation speeds of two models across four shift scenarios. Additionally, we prove the connection between the adaptation speeds of the two models across all interventions.Simplified Chinese translation:在机器翻译任务中，以源文库作为目标文库，训练两个模型的对向翻译是非常重要的。具体来说，考虑一个原始分布p，由于未知的干扰而变化，导致的修改后的分布p*。在将p与p*进行对应的时候，很多因素可以影响对应速度，包括在p中的 causal 依赖关系。在实际应用中，我们需要考虑培训过程的公平性，特别是在涉及到敏感变量（偏见）的情况下。为了探讨这种情况，我们研究了一个简单的结构 causal 模型（SCM），其中变量A acts as a 敏感变量 между cause（X）和 effect（Y）。这两个模型分别在 cause-bias-effect SCM 中表现出了一致和相反的 causal 效果方向。在对 SCM 中变量进行未知干扰后，我们可以模拟一些领域变化进行分析。然后，我们将对四个干扰场景进行比较两个模型的对应速度。此外，我们还证明了两个模型在所有干扰情况下的对应速度之间的连接。
</details></li>
</ul>
<hr>
<h2 id="The-Gender-GAP-Pipeline-A-Gender-Aware-Polyglot-Pipeline-for-Gender-Characterisation-in-55-Languages"><a href="#The-Gender-GAP-Pipeline-A-Gender-Aware-Polyglot-Pipeline-for-Gender-Characterisation-in-55-Languages" class="headerlink" title="The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages"></a>The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16871">http://arxiv.org/abs/2308.16871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Muller, Belen Alastruey, Prangthip Hansanti, Elahe Kalbassi, Christophe Ropers, Eric Michael Smith, Adina Williams, Luke Zettlemoyer, Pierre Andrews, Marta R. Costa-jussà</li>
<li>for: 本研究旨在探讨语言生成系统中的性别偏见，并提出一种可能的来源——训练和评估数据中的性别表达不均衡。</li>
<li>methods: 本研究使用了一个多语言词典来自动量化大规模数据中的性别表达，并使用了WMT训练数据和开发数据来评估这种方法。</li>
<li>results: 研究发现，现有的数据集具有 masculine 表达的偏好，这可能导致语言生成系统对 masculine 性别表达优先化。研究建议在现有数据集中引入性别量化管道，并希望将其修改为均衡的性别表达。<details>
<summary>Abstract</summary>
Gender biases in language generation systems are challenging to mitigate. One possible source for these biases is gender representation disparities in the training and evaluation data. Despite recent progress in documenting this problem and many attempts at mitigating it, we still lack shared methodology and tooling to report gender representation in large datasets. Such quantitative reporting will enable further mitigation, e.g., via data augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware Polyglot Pipeline), an automatic pipeline to characterize gender representation in large-scale datasets for 55 languages. The pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text. We showcase it to report gender representation in WMT training data and development data for the News task, confirming that current data is skewed towards masculine representation. Having unbalanced datasets may indirectly optimize our systems towards outperforming one gender over the others. We suggest introducing our gender quantification pipeline in current datasets and, ideally, modifying them toward a balanced representation.
</details>
<details>
<summary>摘要</summary>
gender bias in language generation systems is difficult to eliminate. one possible source of these biases is the gender representation disparities in the training and evaluation data. despite recent progress in documenting this problem and many attempts at mitigating it, we still lack a shared methodology and tooling to report gender representation in large datasets. such quantitative reporting will enable further mitigation, e.g., via data augmentation. this paper describes the gender-aware polyglot pipeline (for gender-aware polyglot pipeline), an automatic pipeline to characterize gender representation in large-scale datasets for 55 languages. the pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text. we showcase it to report gender representation in wmt training data and development data for the news task, confirming that current data is skewed towards masculine representation. having unbalanced datasets may indirectly optimize our systems towards outperforming one gender over the others. we suggest introducing our gender quantification pipeline in current datasets and, ideally, modifying them towards a balanced representation.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Learning-Driver-Models-for-Automated-Vehicles-via-Knowledge-Sharing-and-Personalization"><a href="#Learning-Driver-Models-for-Automated-Vehicles-via-Knowledge-Sharing-and-Personalization" class="headerlink" title="Learning Driver Models for Automated Vehicles via Knowledge Sharing and Personalization"></a>Learning Driver Models for Automated Vehicles via Knowledge Sharing and Personalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16870">http://arxiv.org/abs/2308.16870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wissam Kontar, Xinzhi Zhong, Soyoung Ahn</li>
<li>for: 本研究提出了一种框架，用于通过知识共享和个性化训练自动驾驶车辆（AVs）驾驶模型。由于交通系统中的自然变化使得对AVs进行实际测试或实验很困难，因此AVs可能会缺乏一些对其安全和高效操作的关键驾驶场景。这种知识共享的方法可以帮助AVs更好地适应实际驾驶情况。</li>
<li>methods: 本研究使用了联邦学习方法，通过多辆车辆之间的知识共享和借鉴，实现个性化训练AVs的驾驶模型。这种方法不需要车辆之间共享Raw数据，从而保持了数据隐私和安全性。</li>
<li>results: 我们在实验 simulations中展示了我们的方法的性能。这种方法在交通工程中拥有广泛的应用，包括智能交通系统、交通管理和车辆间通信。研究页面上提供了代码和示例数据，访问<a target="_blank" rel="noopener" href="https://github.com/wissamkontar%E3%80%82">https://github.com/wissamkontar。</a><details>
<summary>Abstract</summary>
This paper describes a framework for learning Automated Vehicles (AVs) driver models via knowledge sharing between vehicles and personalization. The innate variability in the transportation system makes it exceptionally challenging to expose AVs to all possible driving scenarios during empirical experimentation or testing. Consequently, AVs could be blind to certain encounters that are deemed detrimental to their safe and efficient operation. It is then critical to share knowledge across AVs that increase exposure to driving scenarios occurring in the real world. This paper explores a method to collaboratively train a driver model by sharing knowledge and borrowing strength across vehicles while retaining a personalized model tailored to the vehicle's unique conditions and properties. Our model brings a federated learning approach to collaborate between multiple vehicles while circumventing the need to share raw data between them. We showcase our method's performance in experimental simulations. Such an approach to learning finds several applications across transportation engineering including intelligent transportation systems, traffic management, and vehicle-to-vehicle communication. Code and sample dataset are made available at the project page https://github.com/wissamkontar.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IoMT-Blockchain-based-Secured-Remote-Patient-Monitoring-Framework-for-Neuro-Stimulation-Device"><a href="#IoMT-Blockchain-based-Secured-Remote-Patient-Monitoring-Framework-for-Neuro-Stimulation-Device" class="headerlink" title="IoMT-Blockchain based Secured Remote Patient Monitoring Framework for Neuro-Stimulation Device"></a>IoMT-Blockchain based Secured Remote Patient Monitoring Framework for Neuro-Stimulation Device</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16857">http://arxiv.org/abs/2308.16857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sakib Ullah Sourav, Mohammad Sultan Mahmud, Md Simul Hasan Talukder, Rejwan Bin Sulaiman, Abdullah Yasin</li>
<li>for: 这篇论文的目的是提高医疗业电子设备的准确性、可靠性和生产力，通过使用互联网物联网（IoMT）技术，并利用区块链（BC）解决中心化存储和数据抢夺等问题。</li>
<li>methods: 该论文使用了一种基于IoMT的远程非侵入式脑刺激系统，使用了Android应用程序控制的硬件基于的tDCS设备，并采用了文献最佳实践来解决IoMTBC系统的问题。</li>
<li>results: 该论文的研究结果表明，使用IoMT和BC技术可以提高脑刺激系统的准确性和可靠性，并且可以实现实时远程监测病人的状况。<details>
<summary>Abstract</summary>
Biomedical Engineering's Internet of Medical Things (IoMT) is helping to improve the accuracy, dependability, and productivity of electronic equipment in the healthcare business. Real-time sensory data from patients may be delivered and subsequently analyzed through rapid development of wearable IoMT devices, such as neuro-stimulation devices with a range of functions. Data from the Internet of Things is gathered, analyzed, and stored in a single location. However, single-point failure, data manipulation, privacy difficulties, and other challenges might arise as a result of centralization. Due to its decentralized nature, blockchain (BC) can alleviate these issues. The viability of establishing a non-invasive remote neurostimulation system employing IoMT-based transcranial Direct Current Stimulation is investigated in this work (tDCS). A hardware-based prototype tDCS device has been developed that can be operated over the internet using an android application. Our suggested framework addresses the problems of IoMTBC-based systems, meets the criteria of real-time remote patient monitoring systems, and incorporates literature best practices in the relevant fields.
</details>
<details>
<summary>摘要</summary>
生物医学工程的互联网医疗物联网（IoMT）在医疗业中提高了电子设备的准确性、可靠性和生产力。通过快速开发的着装式IoMT设备，如神经刺激设备，可以实时传输患者的感知数据并进行分析。互联网物联网收集、分析和存储数据的问题，但是中央化的问题可能会出现单点故障、数据操纵、隐私问题等问题。由于其分布式的特点，区块链（BC）可以解决这些问题。本文提出了一种非侵入式远程神经刺激系统，使用IoMT基于的脑 Direct Current Stimulation（tDCS）。我们开发了一个基于硬件的tDCS设备，可以通过android应用程序在互联网上运行。我们建议的框架解决了IoMTBC系统中的问题，满足了实时远程病人监测系统的要求，并兼容了相关领域的文献最佳实践。
</details></li>
</ul>
<hr>
<h2 id="Towards-Improving-the-Expressiveness-of-Singing-Voice-Synthesis-with-BERT-Derived-Semantic-Information"><a href="#Towards-Improving-the-Expressiveness-of-Singing-Voice-Synthesis-with-BERT-Derived-Semantic-Information" class="headerlink" title="Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information"></a>Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16836">http://arxiv.org/abs/2308.16836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaohuan Zhou, Shun Lei, Weiya You, Deyi Tuo, Yuren You, Zhiyong Wu, Shiyin Kang, Helen Meng</li>
<li>for: 高品质的歌唱voice合成系统（SVS），以提高合成的嗓音表达力。</li>
<li>methods: 使用bidirectional encoder representation from Transformers（BERT）得到的含义表达 embeddings，以及歌词文本表达、能量预测器和真实音高预测器等特定设计。</li>
<li>results: 比过去的SVS模型高品质的嗓音合成，并且在专业和主观实验中表现出色。<details>
<summary>Abstract</summary>
This paper presents an end-to-end high-quality singing voice synthesis (SVS) system that uses bidirectional encoder representation from Transformers (BERT) derived semantic embeddings to improve the expressiveness of the synthesized singing voice. Based on the main architecture of recently proposed VISinger, we put forward several specific designs for expressive singing voice synthesis. First, different from the previous SVS models, we use text representation of lyrics extracted from pre-trained BERT as additional input to the model. The representation contains information about semantics of the lyrics, which could help SVS system produce more expressive and natural voice. Second, we further introduce an energy predictor to stabilize the synthesized voice and model the wider range of energy variations that also contribute to the expressiveness of singing voice. Last but not the least, to attenuate the off-key issues, the pitch predictor is re-designed to predict the real to note pitch ratio. Both objective and subjective experimental results indicate that the proposed SVS system can produce singing voice with higher-quality outperforming VISinger.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China.
</details></li>
</ul>
<hr>
<h2 id="Can-Programming-Languages-Boost-Each-Other-via-Instruction-Tuning"><a href="#Can-Programming-Languages-Boost-Each-Other-via-Instruction-Tuning" class="headerlink" title="Can Programming Languages Boost Each Other via Instruction Tuning?"></a>Can Programming Languages Boost Each Other via Instruction Tuning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16824">http://arxiv.org/abs/2308.16824</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nl2code/codem">https://github.com/nl2code/codem</a></li>
<li>paper_authors: Daoguang Zan, Ailun Yu, Bo Shen, Jiaxin Zhang, Taihong Chen, Bing Geng, Bei Chen, Jichuan Ji, Yafen Yao, Yongji Wang, Qianxiang Wang</li>
<li>for: 本研究探讨了 Whether programming languages can boost each other during the instruction fine-tuning phase of code large language models.</li>
<li>methods: 我们使用了 8 种流行的编程语言 (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) 在 StarCoder 上进行了广泛的实验。</li>
<li>results: 结果表明，编程语言可以很大程度上提高对方。例如， CodeM-Python 15B 在 Python 上训练后可以提高 Java 的 pass@1 精度达 17.95%。而即使使用 HTML  corpus 进行训练，CodeM-HTML 7B 也可以提高 Java 的 pass@1 精度达 15.24%。我们的训练数据可以在 GitHub 上下载。<details>
<summary>Abstract</summary>
When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.
</details>
<details>
<summary>摘要</summary>
当人工程师掌握了一种编程语言，学习新的编程语言就会变得更加容易。在这份报告中，我们关注于研究 whether 编程语言可以在代码大型自然语言模型的指令细化阶段互相提高。我们在 StarCoder 上进行了广泛的实验，测试了 8 种流行的编程语言（Python、JavaScript、TypeScript、C、C++、Java、Go、HTML）。结果表明，编程语言可以彼此提高。例如， CodeM-Python 15B 在 Python 上训练后，可以提高 Java 的 pass@1 精度达 17.95%。更 surprisngly，我们发现 CodeM-HTML 7B 在 HTML 语料库上训练后，可以提高 Java 的 pass@1 精度达 15.24%。我们的训练数据可以在 GitHub 上下载：https://github.com/NL2Code/CodeM。
</details></li>
</ul>
<hr>
<h2 id="Latent-Variable-Multi-output-Gaussian-Processes-for-Hierarchical-Datasets"><a href="#Latent-Variable-Multi-output-Gaussian-Processes-for-Hierarchical-Datasets" class="headerlink" title="Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets"></a>Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16822">http://arxiv.org/abs/2308.16822</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunchao Ma, Arthur Leroy, Mauricio Alvarez</li>
<li>for: 这篇论文旨在提出一种基于树结构的多输出泊尔 proces（MOGPs）的扩展，以处理具有层次结构的数据集。</li>
<li>methods: 该论文提出了一种适应层次结构数据集的MOGPs模型，其中包含一个适应层次结构数据集的 kernel function，以及一个新的 latent variables kernel，用于表达输出之间的下游关系。</li>
<li>results: 经过对both synthetic和实际数据进行了extensive的实验研究， authors 发现，该扩展模型可以显著提高对多任务数据的渐进性和泊尔表达能力。<details>
<summary>Abstract</summary>
Multi-output Gaussian processes (MOGPs) have been introduced to deal with multiple tasks by exploiting the correlations between different outputs. Generally, MOGPs models assume a flat correlation structure between the outputs. However, such a formulation does not account for more elaborate relationships, for instance, if several replicates were observed for each output (which is a typical setting in biological experiments). This paper proposes an extension of MOGPs for hierarchical datasets (i.e. datasets for which the relationships between observations can be represented within a tree structure). Our model defines a tailored kernel function accounting for hierarchical structures in the data to capture different levels of correlations while leveraging the introduction of latent variables to express the underlying dependencies between outputs through a dedicated kernel. This latter feature is expected to significantly improve scalability as the number of tasks increases. An extensive experimental study involving both synthetic and real-world data from genomics and motion capture is proposed to support our claims.
</details>
<details>
<summary>摘要</summary>
多输出泊松过程（MOGPs）已经引入以处理多个任务，通过利用不同输出之间的相关性。通常，MOGPs 模型假设输出之间的相关性平坦。然而，这种形式不会考虑更复杂的关系，例如每个输出都有多个重复观测（这是生物实验中常见的设置）。本文提出了基于层次结构的 MOGPs 扩展，我们的模型定义了适应层次结构数据的专门kernel函数，以捕捉不同层次的相关性，同时通过专门的kernel表示输出之间的依赖关系。这种特点预期会在任务数量增加时提高可扩展性。我们采用了大量的实验研究，包括synthetic和实际数据来支持我们的主张。
</details></li>
</ul>
<hr>
<h2 id="Irregular-Traffic-Time-Series-Forecasting-Based-on-Asynchronous-Spatio-Temporal-Graph-Convolutional-Network"><a href="#Irregular-Traffic-Time-Series-Forecasting-Based-on-Asynchronous-Spatio-Temporal-Graph-Convolutional-Network" class="headerlink" title="Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network"></a>Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16818">http://arxiv.org/abs/2308.16818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijia Zhang, Le Zhang, Jindong Han, Hao Liu, Jingbo Zhou, Yu Mei, Hui Xiong<br>for: 这篇论文旨在提出一个能够实现高精度交通预测的方法，以提高智能交通信号系统的效率。methods: 本文使用了 asynchronous spatio-temporal graph convolutional neural network (ASeer)，它通过连接车道的交通散射网络来模型车道间的异步空间相依性，并使用可学习的个人化时间编码来捕捉车道间的时间相依性。results: 实验结果显示，ASeer 能够实现高精度的交通预测，并且在六个度量上优于现有的方法。<details>
<summary>Abstract</summary>
Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of lanes. After that, to capture the temporal dependency within irregular traffic state sequence, a learnable personalized time encoding is devised to embed the continuous time for each lane. Then we propose a Transformable Time-aware Convolution Network that learns meta-filters to derive time-aware convolution filters with transformable filter sizes for efficient temporal convolution on the irregular sequence. Furthermore, a Semi-Autoregressive Prediction Network consisting of a state evolution unit and a semiautoregressive predictor is designed to effectively and efficiently predict variable-length traffic state sequences. Extensive experiments on two real-world datasets demonstrate the effectiveness of ASeer in six metrics.
</details>
<details>
<summary>摘要</summary>
准确预测交通流量在智能交通信号控制系统中是关键。然而，由于智能交通信号处理器生成的交通流量时序序列具有不规则性和非线性，这导致了三大新挑战：1） asynchronous spatial dependency，2） irregular temporal dependency among traffic data，和3） variable-length sequence to be predicted。为解决这些挑战，我们提出了一种异步空间-时间图 convolutional neural network（ASeer），用于预测进入智能交通 crossing 的车道 traffic states 在未来时间窗口内。 Specifically, we first model the asynchronous spatial dependency between the time-misaligned traffic state measurements of lanes by linking lanes via a traffic diffusion graph. Then, to capture the temporal dependency within the irregular traffic state sequence, we devise a learnable personalized time encoding to embed the continuous time for each lane. After that, we propose a Transformable Time-aware Convolution Network that learns meta-filters to derive time-aware convolution filters with transformable filter sizes for efficient temporal convolution on the irregular sequence. Furthermore, a Semi-Autoregressive Prediction Network consisting of a state evolution unit and a semiautoregressive predictor is designed to effectively and efficiently predict variable-length traffic state sequences. Our extensive experiments on two real-world datasets demonstrate the effectiveness of ASeer in six metrics.
</details></li>
</ul>
<hr>
<h2 id="Rank-Collapse-Causes-Over-Smoothing-and-Over-Correlation-in-Graph-Neural-Networks"><a href="#Rank-Collapse-Causes-Over-Smoothing-and-Over-Correlation-in-Graph-Neural-Networks" class="headerlink" title="Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks"></a>Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16800">http://arxiv.org/abs/2308.16800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Roth, Thomas Liebig</li>
<li>for: This paper aims to provide new theoretical insights into the issues of over-smoothing and feature over-correlation in deep graph neural networks.</li>
<li>methods: The paper uses a theoretical approach to demonstrate the prevalence of invariant subspaces in deep graph neural networks, and shows how this can lead to over-smoothing and over-correlation.</li>
<li>results: The paper’s results include a better understanding of the causes of over-smoothing and over-correlation, and the proposal of a sum of Kronecker products as a beneficial property that can prevent these issues. Additionally, the paper demonstrates the inability of existing models to capture linearly independent features in the non-linear case.<details>
<summary>Abstract</summary>
Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank collapse. We empirically extend our insights to the non-linear case, demonstrating the inability of existing models to capture linearly independent features.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Agent-Teaming-Situation-Awareness-ATSA-A-Situation-Awareness-Framework-for-Human-AI-Teaming"><a href="#Agent-Teaming-Situation-Awareness-ATSA-A-Situation-Awareness-Framework-for-Human-AI-Teaming" class="headerlink" title="Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming"></a>Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16785">http://arxiv.org/abs/2308.16785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Gao, Wei Xu, Mowei Shen, Zaifeng Gao<br>for:* The paper is written to provide a review of leading situation awareness (SA) theoretical models and to propose a new framework for SA in the human-AI teaming (HAT) context.methods:* The paper uses a literature review to identify key features and processes of HAT and to develop a new framework for SA in the HAT context.results:* The proposed Agent Teaming Situation Awareness (ATSA) framework unifies human and AI behavior and emphasizes cohesive and effective HAT through structures and components such as teaming understanding, teaming control, and the world.Here is the information in Simplified Chinese text, as requested:for:* 论文是为了提供人机合作(HAT)场景下的情况意识(SA)理论模型的回顾和新的SA模型框架。methods:* 论文通过文献综述来确定HAT场景中的关键特征和过程，并开发了新的SA模型框架。results:* 提出的Agent Teaming Situation Awareness(ATSA)框架将人机行为统一，强调团队理解、团队控制和世界等结构和组件，以实现效果的HAT。<details>
<summary>Abstract</summary>
The rapid advancements in artificial intelligence (AI) have led to a growing trend of human-AI teaming (HAT) in various fields. As machines continue to evolve from mere automation to a state of autonomy, they are increasingly exhibiting unexpected behaviors and human-like cognitive/intelligent capabilities, including situation awareness (SA). This shift has the potential to enhance the performance of mixed human-AI teams over all-human teams, underscoring the need for a better understanding of the dynamic SA interactions between humans and machines. To this end, we provide a review of leading SA theoretical models and a new framework for SA in the HAT context based on the key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA) framework unifies human and AI behavior, and involves bidirectional, and dynamic interaction. The framework is based on the individual and team SA models and elaborates on the cognitive mechanisms for modeling HAT. Similar perceptual cycles are adopted for the individual (including both human and AI) and the whole team, which is tailored to the unique requirements of the HAT context. ATSA emphasizes cohesive and effective HAT through structures and components, including teaming understanding, teaming control, and the world, as well as adhesive transactive part. We further propose several future research directions to expand on the distinctive contributions of ATSA and address the specific and pressing next steps.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的快速进步导致人机合作（HAT）在不同领域得到了普遍应用。随着机器的演进从自动化到智能化，它们开始显示出人类智能的特征和不期望的行为，包括情境意识（SA）。这种变化有可能提高混合人机队列的性能，高亮了我们更好地理解人机合作中的SA交互的需要。为此，我们提供了SA理论模型的综述和人机合作情境中SA框架（ATSA），该框架将人类和AI行为结合在一起，并包括对向和动态互动。ATSA基于个体和团队SA模型，并详细介绍了人机合作中的认知机制。在团队水平上，采用同样的观察循环，包括人类和AI的个体SA，以适应HAT特殊需求。ATSA强调合作和有效的人机合作，通过结构和组件，如团队理解、团队控制和世界，以及贯通性的交互。我们还建议了一些未来研究方向，以扩展ATSA的独特贡献和解决特定和紧迫的下一步。
</details></li>
</ul>
<hr>
<h2 id="StratMed-Relevance-Stratification-for-Low-resource-Medication-Recommendation"><a href="#StratMed-Relevance-Stratification-for-Low-resource-Medication-Recommendation" class="headerlink" title="StratMed: Relevance Stratification for Low-resource Medication Recommendation"></a>StratMed: Relevance Stratification for Low-resource Medication Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16781">http://arxiv.org/abs/2308.16781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li</li>
<li>for: 这篇论文的目的是提出一个基于人工智能的药物建议方法，以整合长期医疗历史资料和医疗知识，帮助医生诊断更加精确和安全的药物组合。</li>
<li>methods: 这篇论文使用了一个创新的相关分类机制，协调资料的长尾分布差异，并将安全和精确的药物组合表现同等化。Specifically, the authors first construct a pre-training method using deep learning networks to obtain entity representation, and then design a pyramid-like data stratification method to obtain more generalized entity relationships by reinforcing the features of unpopular entities. Based on this relationship, they designed two graph structures to express medication precision and safety at the same level to obtain visit representations.</li>
<li>results: 实验结果显示，该方法在MIMIC-III dataset上比现有的州际专业方法表现出色，在四个评估指标中（包括安全和精确）都有出色的表现。<details>
<summary>Abstract</summary>
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity relationships by reinforcing the features of unpopular entities. Based on this relationship, we designed two graph structures to express medication precision and safety at the same level to obtain visit representations. Finally, the patient's historical clinical information is fitted to generate medication combinations for the current health condition. Experiments on the MIMIC-III dataset demonstrate that our method has outperformed current state-of-the-art methods in four evaluation metrics (including safety and accuracy).
</details>
<details>
<summary>摘要</summary>
To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. This mechanism harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations.Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity relationships by reinforcing the features of unpopular entities. Based on this relationship, we designed two graph structures to express medication precision and safety at the same level to obtain visit representations. Finally, the patient's historical clinical information is fitted to generate medication combinations for the current health condition.Experiments on the MIMIC-III dataset demonstrate that our method has outperformed current state-of-the-art methods in four evaluation metrics (including safety and accuracy).
</details></li>
</ul>
<hr>
<h2 id="Efficacy-of-Neural-Prediction-Based-NAS-for-Zero-Shot-NAS-Paradigm"><a href="#Efficacy-of-Neural-Prediction-Based-NAS-for-Zero-Shot-NAS-Paradigm" class="headerlink" title="Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm"></a>Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16775">http://arxiv.org/abs/2308.16775</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minh1409/dft-npzs-nas">https://github.com/minh1409/dft-npzs-nas</a></li>
<li>paper_authors: Minh Le, Nhan Nguyen, Ngoc Hoang Luong</li>
<li>for: This paper focuses on addressing the limitation of performance indicators in prediction-based Neural Architecture Search (NAS), specifically the inability to evaluate architecture performance across varying search spaces.</li>
<li>methods: The proposed approach uses Fourier sum of sines encoding for convolutional kernels, which enables the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. An accompanying multi-layer perceptron (MLP) then ranks these architectures based on their encodings.</li>
<li>results: The approach proposed in this paper surpasses previous methods using graph convolutional networks in terms of correlation on the NAS-Bench-201 dataset and exhibits a higher convergence rate. Moreover, the extracted feature representation trained on each NAS-Benchmark is transferable to other NAS-Benchmarks, showing promising generalizability across multiple search spaces.<details>
<summary>Abstract</summary>
In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks these architectures based on their encodings. Experimental results show that our approach surpasses previous methods using graph convolutional networks in terms of correlation on the NAS-Bench-201 dataset and exhibits a higher convergence rate. Moreover, our extracted feature representation trained on each NAS-Benchmark is transferable to other NAS-Benchmarks, showing promising generalizability across multiple search spaces. The code is available at: https://github.com/minh1409/DFT-NPZS-NAS
</details>
<details>
<summary>摘要</summary>
在预测基于的神经网络搜索（NAS）中，基于图 convolutional networks 的性能指标得到了显著的成功。这些指标，通过将 feed-forward 结构表示为组成图通过一个一个式编码，面临一个限制：它们无法评估搜索空间中不同的架构性能。相比之下，手工制作的性能指标（零shot NAS），使用同一个架构并且随机初始化，可以在多个搜索空间中 generale。为了解决这一限制，我们提出了一种新的零shot NAS 方法，使用深度学习。我们的方法使用 Fourier 和平差编码来构建一个计算 feed-forward 图，其结构与被评估的架构相似。这些编码是学习的，可以提供架构的全面信息。随后的多层感知器（MLP）将这些架构按照其编码进行排序。实验结果表明，我们的方法在 NAS-Bench-201 数据集上和前一代方法使用 graph convolutional networks 相比，具有更高的相关性和更快的收敛率。此外，我们提取的特征表示在每个 NAS-Benchmark 上训练，可以在其他 NAS-Benchmark 上提取到较好的特征，表现出了良好的普适性。代码可以在：https://github.com/minh1409/DFT-NPZS-NAS 上获取。
</details></li>
</ul>
<hr>
<h2 id="Towards-Low-Barrier-Cybersecurity-Research-and-Education-for-Industrial-Control-Systems"><a href="#Towards-Low-Barrier-Cybersecurity-Research-and-Education-for-Industrial-Control-Systems" class="headerlink" title="Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems"></a>Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16769">http://arxiv.org/abs/2308.16769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Colman McGuan, Chansu Yu, Qin Lin</li>
<li>for: 这个研究旨在提供一个可靠的测试环境，以便 validate和比较各种入侵检测算法，以保护工业控制系统（ICS）。</li>
<li>methods: 我们使用了3D高精度模拟器，实现自动启动攻击，收集数据，训练机器学习模型，并评估在实际生产过程中。</li>
<li>results: 我们的Minimal Threshold and Window SVM（MinTWin SVM）模型可以实现避免伪阳性，并对物理过程异常进行感知。此外，我们在ICScybersecurity教育中使用了我们的数据集，让学生在实际ICS数据上练习机器学习理论。<details>
<summary>Abstract</summary>
The protection of Industrial Control Systems (ICS) that are employed in public critical infrastructures is of utmost importance due to catastrophic physical damages cyberattacks may cause. The research community requires testbeds for validation and comparing various intrusion detection algorithms to protect ICS. However, there exist high barriers to entry for research and education in the ICS cybersecurity domain due to expensive hardware, software, and inherent dangers of manipulating real-world systems. To close the gap, built upon recently developed 3D high-fidelity simulators, we further showcase our integrated framework to automatically launch cyberattacks, collect data, train machine learning models, and evaluate for practical chemical and manufacturing processes. On our testbed, we validate our proposed intrusion detection model called Minimal Threshold and Window SVM (MinTWin SVM) that utilizes unsupervised machine learning via a one-class SVM in combination with a sliding window and classification threshold. Results show that MinTWin SVM minimizes false positives and is responsive to physical process anomalies. Furthermore, we incorporate our framework with ICS cybersecurity education by using our dataset in an undergraduate machine learning course where students gain hands-on experience in practicing machine learning theory with a practical ICS dataset. All of our implementations have been open-sourced.
</details>
<details>
<summary>摘要</summary>
对于公共重要基础设施中使用的工业控制系统（ICS）的安全保护非常重要，因为黑客可以通过网络攻击引起严重的物理损害。研究社区需要测试平台来验证和比较不同的入侵检测算法，以保护ICS。但是，ICS安全领域的研究和教育面临着高的入门障碍，因为ICS系统的硬件、软件和实际操作是昂贵的，而且具有很高的危险性。为了解决这个问题，我们基于最近发展的3D高精度模拟器，提供了一个集成的测试平台，可以自动发起网络攻击，收集数据，训练机器学习模型，并评估实际化学和制造过程中的做法。在我们的测试平台上，我们验证了我们提出的入侵检测模型，称为最小阈值窗口支持向量机（MinTWin SVM），它利用了无监督的机器学习，并将窗口和分类阈值结合使用。结果表明，MinTWin SVM可以减少假阳性，同时快速响应物理过程异常。此外，我们将我们的框架与ICS安全教育相结合，通过使用我们的数据集在大学生Machine learning课程中进行实践，让学生通过实践机器学习理论来掌握ICS数据集的实践应用。所有我们的实现都已经开源。
</details></li>
</ul>
<hr>
<h2 id="Ladder-of-Thought-Using-Knowledge-as-Steps-to-Elevate-Stance-Detection"><a href="#Ladder-of-Thought-Using-Knowledge-as-Steps-to-Elevate-Stance-Detection" class="headerlink" title="Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection"></a>Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16763">http://arxiv.org/abs/2308.16763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kairui Hu, Ming Yan, Joey Tianyi Zhou, Ivor W. Tsang, Wen Haw Chong, Yong Keong Yap</li>
<li>for: 提高大型自然语言模型（LLM）的逻辑能力，以及提高小型LLM的性能。</li>
<li>methods: 提出了一种名为“笔脚架”（LoT）的双阶段协同优化框架，通过充分利用高质量的外部知识，提高模型生成的中间逻辑。</li>
<li>results: 对比chatGPT和chatGPT+CoT，LoT achieved a 16% improvement in stance detection task, and a 10% improvement compared to chatGPT with CoT.<details>
<summary>Abstract</summary>
Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of Large Language Models (LLMs) through the generation of intermediate rationales. However, these enhancements predominantly benefit large-scale models, leaving small LMs without significant performance improvements when directly applying CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily on their pre-trained internal knowledge. The external knowledge that is previously unknown to the model remains unexploited. This omission becomes pronounced in tasks such as stance detection, where the external background knowledge plays a pivotal role. Additionally, the large-scale architecture of LLMs inevitably present efficiency challenges during deployment. To address these challenges, we introduce the Ladder-of-Thought (LoT) for stance detection. Grounded in a dual-phase Cascaded Optimization framework, LoT directs the model to incorporate high-quality external knowledge, enhancing the intermediate rationales it generates. These bolstered rationales subsequently serve as the foundation for more precise predictions - akin to how a ladder facilitates reaching elevated goals. LoT achieves a balance between efficiency and accuracy, making it an adaptable and efficient framework for stance detection. Our empirical evaluations underscore LoT's effectiveness, marking a 16% improvement over ChatGPT and a 10% enhancement compared to ChatGPT with CoT.
</details>
<details>
<summary>摘要</summary>
链接思维提示（CoT）可以增强大语言模型（LLM）的逻辑能力，但是这些改进主要适用于大规模模型，小型LM无法直接应用CoT而获得显著性能提升。尽管LLM具有高度的逻辑能力，但CoT仍然主要基于其先前预训练的内部知识。外部知识，尚未被模型所知悉，则被忽略。这种欠缺特别manifest在tasks like stance detection中， где外部背景知识扮演着关键性的角色。此外，大规模的LLM架构在部署时依然会存在效率挑战。为了解决这些挑战，我们提出了思维阶梯（LoT） для stance detection。LoT基于双阶段分布式优化框架，使模型能够更好地利用高质量的外部知识，并在生成中提高中间逻辑。这些加强的逻辑后续成为更准确的预测基础，类似于如何使用梯子来达到更高的目标。LoT寻求效率和准确性之间的平衡，使其成为适应性强的和高效的框架。我们的实验证明了LoT的效果，与ChatGPT和ChatGPT with CoT相比，LoT提高了16%和10%。
</details></li>
</ul>
<hr>
<h2 id="Context-Aware-Query-Rewriting-for-Text-Rankers-using-LLM"><a href="#Context-Aware-Query-Rewriting-for-Text-Rankers-using-LLM" class="headerlink" title="Context Aware Query Rewriting for Text Rankers using LLM"></a>Context Aware Query Rewriting for Text Rankers using LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16753">http://arxiv.org/abs/2308.16753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijit Anand, Venktesh V, Vinay Setty, Avishek Anand</li>
<li>for: 提高文档排序任务中的查询模糊匹配问题的解决方案。</li>
<li>methods: 使用生成模型（LLMs）生成 pseudo 文档，以优化查询模糊匹配问题。</li>
<li>results: 在训练阶段使用 CAR approach rewrite 查询，可以提高文档排序任务的表现，比基eline表现提高至多33%。<details>
<summary>Abstract</summary>
Query rewriting refers to an established family of approaches that are applied to underspecified and ambiguous queries to overcome the vocabulary mismatch problem in document ranking. Queries are typically rewritten during query processing time for better query modelling for the downstream ranker. With the advent of large-language models (LLMs), there have been initial investigations into using generative approaches to generate pseudo documents to tackle this inherent vocabulary gap. In this work, we analyze the utility of LLMs for improved query rewriting for text ranking tasks. We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing. We adopt a simple, yet surprisingly effective, approach called context aware query rewriting (CAR) to leverage the benefits of LLMs for query understanding. Firstly, we rewrite ambiguous training queries by context-aware prompting of LLMs, where we use only relevant documents as context.Unlike existing approaches, we use LLM-based query rewriting only during the training phase. Eventually, a ranker is fine-tuned on the rewritten queries instead of the original queries during training. In our extensive experiments, we find that fine-tuning a ranker using re-written queries offers a significant improvement of up to 33% on the passage ranking task and up to 28% on the document ranking task when compared to the baseline performance of using original queries.
</details>
<details>
<summary>摘要</summary>
Query 重写指的是一家已经确立的方法，用于解决文档排名中 vocabulary 匹配问题。通常情况下，查询将在查询处理时进行重写，以便更好地模型查询。随着大型语言模型（LLM）的出现，有初步的调查表明，可以使用生成方法生成 pseudo 文档来解决这种遗传 vocabulary 差距。在这种工作中，我们分析了使用 LLM 进行改进查询重写的 utility。我们发现了两种使用 LLM 作为查询重写器的内在限制：一是概念漂移，只使用查询作为提示；二是大量的推理成本 durante 查询处理。我们采用一种简单 yet 有效的方法，即 context-aware 查询重写（CAR），以利用 LLM 的优势来更好地理解查询。首先，我们将ambiguous 的训练查询重写为上下文感知的 LLM 提示，并且只使用相关的文档作为上下文。不同于现有的方法，我们在训练阶段使用 LLM 进行查询重写，而不是在查询处理阶段。最后，我们在训练阶段使用重写后的查询进行rankers 的精度。在我们的广泛实验中，我们发现，使用重写后的查询可以提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表现，提高文档排名任务的表
</details></li>
</ul>
<hr>
<h2 id="Socratis-Are-large-multimodal-models-emotionally-aware"><a href="#Socratis-Are-large-multimodal-models-emotionally-aware" class="headerlink" title="Socratis: Are large multimodal models emotionally aware?"></a>Socratis: Are large multimodal models emotionally aware?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16741">http://arxiv.org/abs/2308.16741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katherine Deng, Arijit Ray, Reuben Tan, Saadia Gabriel, Bryan A. Plummer, Kate Saenko</li>
<li>for: 提高 Multimodal 语言模型对情感的认知和生成能力</li>
<li>methods: 使用多种情感标签和理由描述来评估模型的表现</li>
<li>results: 人类对人写的理由更加喜欢，而不是机器生成的理由，而且现有的captioning metric不能与人类喜好相吻合<details>
<summary>Abstract</summary>
Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a \underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s} benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-generated ones. This shows our task is harder than standard generation tasks because it starkly contrasts recent findings where humans cannot tell apart machine vs human-written news articles, for instance. We further see that current captioning metrics based on large vision-language models also fail to correlate with human preferences. We hope that these findings and our benchmark will inspire further research on training emotionally aware models.
</details>
<details>
<summary>摘要</summary>
现有的情绪预测 benchmark 包含粗糙的情绪标签，不考虑图文内容对人类的多样化情绪响应。学习多样化情绪对图文内容是重要的，因为智能机器在生成和传递内容方面发挥了中心作用。为解决这个差距，我们提议了 Socratis  benchmark，每个图文笔记 (IC) 对象被标注为多种情绪和其原因。Socratis 包含 18,000 个自由格式的反应，用于 980 种情绪的 2,075 个图文笔记对。我们使用现代大语言模型测试能否生成情绪的原因，并观察到人类更加偏好人工写的原因，相比于机器生成的原因。此外，我们还发现现有的captioning metric 基于大视语言模型并不与人类偏好相关。我们希望这些发现和我们的 benchmark 能够激发更多的情绪意识模型训练研究。
</details></li>
</ul>
<hr>
<h2 id="Robust-Networked-Federated-Learning-for-Localization"><a href="#Robust-Networked-Federated-Learning-for-Localization" class="headerlink" title="Robust Networked Federated Learning for Localization"></a>Robust Networked Federated Learning for Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16737">http://arxiv.org/abs/2308.16737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Mirzaeifard, Naveen K. D. Venkategowda, Stefan Werner</li>
<li>For: 本研究旨在解决 Federated Learning 环境中的地域化问题，该问题是非 convex 和非凸的，数据分布在多个设备上。* Methods: 我们提议一种使用 $L_1$-norm 稳定的分布式 sub-gradient 框架，以适应 Federated Learning 环境中的异常数据问题。* Results: 我们的方法可以快速地 converge to a stationary point，并在实验中证明其超越现有的地域化方法。<details>
<summary>Abstract</summary>
This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effectiveness and reliability. Through numerical simulations, we confirm the superior performance of our approach, notably in outlier-rich environments, which surpasses existing state-of-the-art localization methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Post-Deployment-Adaptation-with-Access-to-Source-Data-via-Federated-Learning-and-Source-Target-Remote-Gradient-Alignment"><a href="#Post-Deployment-Adaptation-with-Access-to-Source-Data-via-Federated-Learning-and-Source-Target-Remote-Gradient-Alignment" class="headerlink" title="Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment"></a>Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16735">http://arxiv.org/abs/2308.16735</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/felixwag/staralign">https://github.com/felixwag/staralign</a></li>
<li>paper_authors: Felix Wagner, Zeju Li, Pramit Saha, Konstantinos Kamnitsas</li>
<li>for: 本文旨在 Addressing the distribution shift problem in deep neural network deployment for medical imaging, specifically in the context of post-deployment adaptation (PDA) with limited or no labelled target data.</li>
<li>methods: 本文提出了一种新的适应框架 called FedPDA，它利用远程学习来帮助已经部署的模型适应目标数据分布。此外，文章还提出了一种新的优化方法 StarAlign，用于将源数据和目标数据之间的梯度进行对齐，以便学习一个特定的目标模型。</li>
<li>results: 文章通过使用多个医疗机构的数据库进行肿瘤检测和皮肤病分类任务，证明了 StarAlign 方法的有效性，与之前的工作相比，其表现更好。<details>
<summary>Abstract</summary>
Deployment of Deep Neural Networks in medical imaging is hindered by distribution shift between training data and data processed after deployment, causing performance degradation. Post-Deployment Adaptation (PDA) addresses this by tailoring a pre-trained, deployed model to the target data distribution using limited labelled or entirely unlabelled target data, while assuming no access to source training data as they cannot be deployed with the model due to privacy concerns and their large size. This makes reliable adaptation challenging due to limited learning signal. This paper challenges this assumption and introduces FedPDA, a novel adaptation framework that brings the utility of learning from remote data from Federated Learning into PDA. FedPDA enables a deployed model to obtain information from source data via remote gradient exchange, while aiming to optimize the model specifically for the target domain. Tailored for FedPDA, we introduce a novel optimization method StarAlign (Source-Target Remote Gradient Alignment) that aligns gradients between source-target domain pairs by maximizing their inner product, to facilitate learning a target-specific model. We demonstrate the method's effectiveness using multi-center databases for the tasks of cancer metastases detection and skin lesion classification, where our method compares favourably to previous work. Code is available at: https://github.com/FelixWag/StarAlign
</details>
<details>
<summary>摘要</summary>
部署深度神经网络在医疗影像领域面临分布shift问题，导致性能下降。协作式适应（PDA）解决这个问题，通过使用有限的标注或无标注目标数据来适应目标数据分布，而不需要访问源训练数据，因为隐私问题和它们的大小。这使得可靠的适应变得困难，因为有限的学习信号。这篇论文挑战这一假设，并引入FedPDA，一种新的适应框架，它将在联合学习中获得源数据信息，并且通过远程梯度交换来优化模型，以便适应目标领域。为了适应FedPDA，我们引入了一种新的优化方法：StarAlign（源-目标远程梯度匹配），它通过最大化源-目标对的内积来匹配梯度，以便学习一个特定的目标模型。我们使用多个医疗数据中心的数据进行肿瘤检测和皮肤涂抹分类任务，并证明了我们的方法与之前的工作相比较有利。代码可以在 GitHub 上找到：https://github.com/FelixWag/StarAlign。
</details></li>
</ul>
<hr>
<h2 id="Proof-of-Deep-Learning-Approaches-Challenges-and-Future-Directions"><a href="#Proof-of-Deep-Learning-Approaches-Challenges-and-Future-Directions" class="headerlink" title="Proof of Deep Learning: Approaches, Challenges, and Future Directions"></a>Proof of Deep Learning: Approaches, Challenges, and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16730">http://arxiv.org/abs/2308.16730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Salhab, Khaleel Mershad</li>
<li>for: 本研究主要旨在调查各种Proof of Deep Learning（PoDL）机制，了解它们的优缺点，以及它们在不同应用场景中的可能性。</li>
<li>methods: 本研究使用了多种方法，包括Literature Review、Algorithm Analysis和Future Research Direction等。</li>
<li>results: 本研究结果显示，PoDL机制可以充分利用计算能力，同时保持区块链的安全性和完整性。但是，PoDL还需要进一步的研究和开发，以便在实际应用中得到更好的效果。<details>
<summary>Abstract</summary>
The rise of computational power has led to unprecedented performance gains for deep learning models. As more data becomes available and model architectures become more complex, the need for more computational power increases. On the other hand, since the introduction of Bitcoin as the first cryptocurrency and the establishment of the concept of blockchain as a distributed ledger, many variants and approaches have been proposed. However, many of them have one thing in common, which is the Proof of Work (PoW) consensus mechanism. PoW is mainly used to support the process of new block generation. While PoW has proven its robustness, its main drawback is that it requires a significant amount of processing power to maintain the security and integrity of the blockchain. This is due to applying brute force to solve a hashing puzzle. To utilize the computational power available in useful and meaningful work while keeping the blockchain secure, many techniques have been proposed, one of which is known as Proof of Deep Learning (PoDL). PoDL is a consensus mechanism that uses the process of training a deep learning model as proof of work to add new blocks to the blockchain. In this paper, we survey the various approaches for PoDL. We discuss the different types of PoDL algorithms, their advantages and disadvantages, and their potential applications. We also discuss the challenges of implementing PoDL and future research directions.
</details>
<details>
<summary>摘要</summary>
随着计算机力的提高，深度学习模型的性能得到了历史上无 precedent 的提升。随着更多的数据变得可用并模型结构变得更加复杂，需要更多的计算机力的增加。然而，自比特币的出现以来，各种变体和方法被提出，其中大多数具有一个共同之处，即证明工作（PoW）共识机制。PoW主要用于支持新块生成过程。虽然PoW已经证明了其Robustness，但它的主要缺点是需要大量的处理能力来保持区块链的安全性和完整性。这是因为通过施加 brut force 解决哈希拟合问题。为了利用计算机能源进行有用和意义的工作而不是保持区块链的安全性，许多技术被提出，其中之一是知名的深度学习证明（PoDL）。PoDL是一种使用深度学习模型证明工作来添加新块到区块链的共识机制。在这篇文章中，我们对PoDL的不同方法进行了抽象，讨论了它们的优缺点，以及它们在不同应用场景中的潜在应用。我们还讨论了实施PoDL的挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Terrain-Diffusion-Network-Climatic-Aware-Terrain-Generation-with-Geological-Sketch-Guidance"><a href="#Terrain-Diffusion-Network-Climatic-Aware-Terrain-Generation-with-Geological-Sketch-Guidance" class="headerlink" title="Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance"></a>Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16725">http://arxiv.org/abs/2308.16725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zexin Hu, Kun Hu, Clinton Mo, Lei Pan, Zhiyong Wang</li>
<li>for: 这paper的目的是提出一种新的液态网络（TDN），用于实现更加真实的地形生成，并提供更高级别的用户控制性。</li>
<li>methods: 该方法使用了多层混清过程，并采用了用户指导的方式，以保证生成的地形更加真实和有趣。此外，该方法还使用了预训练的地形自动编码器，以提高生成的地形精度。</li>
<li>results: 经过广泛的实验，该方法在一个新的NASA Topology Images dataset上达到了状态方法的性能，并且可以生成更加真实和有趣的地形。<details>
<summary>Abstract</summary>
Sketch-based terrain generation seeks to create realistic landscapes for virtual environments in various applications such as computer games, animation and virtual reality. Recently, deep learning based terrain generation has emerged, notably the ones based on generative adversarial networks (GAN). However, these methods often struggle to fulfill the requirements of flexible user control and maintain generative diversity for realistic terrain. Therefore, we propose a novel diffusion-based method, namely terrain diffusion network (TDN), which actively incorporates user guidance for enhanced controllability, taking into account terrain features like rivers, ridges, basins, and peaks. Instead of adhering to a conventional monolithic denoising process, which often compromises the fidelity of terrain details or the alignment with user control, a multi-level denoising scheme is proposed to generate more realistic terrains by taking into account fine-grained details, particularly those related to climatic patterns influenced by erosion and tectonic activities. Specifically, three terrain synthesisers are designed for structural, intermediate, and fine-grained level denoising purposes, which allow each synthesiser concentrate on a distinct terrain aspect. Moreover, to maximise the efficiency of our TDN, we further introduce terrain and sketch latent spaces for the synthesizers with pre-trained terrain autoencoders. Comprehensive experiments on a new dataset constructed from NASA Topology Images clearly demonstrate the effectiveness of our proposed method, achieving the state-of-the-art performance. Our code and dataset will be publicly available.
</details>
<details>
<summary>摘要</summary>
《绘图基 terrain 生成》是一种目标创建虚拟环境中的真实景观，如电子游戏、动画和虚拟现实等应用。现在，基于深度学习的 terrain 生成技术在不断发展，其中以生成对抗网络（GAN）为代表。然而，这些方法经常难以满足用户控制的灵活性和生成多样性，以保证真实的地形。因此，我们提出了一种新的扩散基本方法，即 terrain 扩散网络（TDN），它可以 aktiv 地 incorporate 用户指导，考虑地形特征，如河流、山脊、盆地和峰峰。而不是遵循传统的单一杂化过程，TDN 可以生成更真实的地形，并且具有较高的用户控制性。为了实现这一目标，我们采用了多层杂化机制，包括三个不同级别的 terrain 杂化器，用于处理不同级别的地形细节。此外，为了提高 TDN 的效率，我们还引入了地形和绘图幂等空间，并采用了预训练的地形自动编码器。实验结果表明，我们的提议方法可以达到现状最佳性，并且在 NASA Topology 图像 Dataset 上进行了全面的测试。我们的代码和数据将在线公开。
</details></li>
</ul>
<hr>
<h2 id="CReHate-Cross-cultural-Re-annotation-of-English-Hate-Speech-Dataset"><a href="#CReHate-Cross-cultural-Re-annotation-of-English-Hate-Speech-Dataset" class="headerlink" title="CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset"></a>CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16705">http://arxiv.org/abs/2308.16705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Juho Kim, Alice Oh</li>
<li>for: This paper aims to address cultural biases in hate speech detection models and datasets by introducing a cross-cultural re-annotation of the SBIC dataset and analyzing differences in perceptions of hate speech among individuals from five distinct countries.</li>
<li>methods: The paper uses a cross-cultural re-annotation of the SBIC dataset, which includes annotations from Australia, Singapore, South Africa, the United Kingdom, and the United States. The authors also employ transfer learning to develop a culturally sensitive hate speech classifier.</li>
<li>results: The authors find significant differences in the perception of hate speech among individuals from different countries, with only 59.4% of the samples achieving consensus among all countries. They also develop a culturally sensitive hate speech classifier that can capture the perspectives of different nationalities.<details>
<summary>Abstract</summary>
English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate speech in the English language.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "English datasets" is translated as "英语 datasets" (Yīngyǔ datasets) in Simplified Chinese.* "predominantly" is translated as "主要" (zhǔyào) in Simplified Chinese.* "reflect" is translated as "反映" (fǎngyìng) in Simplified Chinese.* "certain nationalities" is translated as "specific nationalities" (特定国籍) in Simplified Chinese.* "cultural biases" is translated as "文化偏见" (wénhuà péndiǎn) in Simplified Chinese.* "tasks heavily influenced by subjectivity" is translated as "受主观因素影响的任务" (shòu zhǔyǎn yìnxīng de jìnzuò) in Simplified Chinese.* "CReHate" is translated as "CReHate" (CReHate) in Simplified Chinese, as it is a proper noun.* "cross-cultural re-annotation" is translated as "跨文化重标注" (kuà wénhuà zhòngbiǎozhù) in Simplified Chinese.* "sampled SBIC dataset" is translated as "采样的 SBIC 数据集" (chǎi yàng de SBIC dàta sets) in Simplified Chinese.* "significant differences" is translated as "显著差异" (xiǎng zhì kù yì) in Simplified Chinese.* "only 59.4% of the samples achieving consensus among all countries" is translated as "只有59.4% 的样本达成全球各国的一致" (zhīyǒu 59.4% de yàngbèi dàchéng quánxiàng zhìyì) in Simplified Chinese.* "culturally sensitive hate speech classifier" is translated as "文化敏感 hate speech 分类器" (wénhuà mǐngkan hate speech fènklè yì) in Simplified Chinese.* "transfer learning" is translated as "传输学习" (chuánxīng xuéxí) in Simplified Chinese.* "adept at capturing perspectives of different nationalities" is translated as "能够捕捉不同国籍的视角" (nénggòu bòshì bùdìng guójiè de zhìkǎng) in Simplified Chinese.* "nuanced nature of hate speech in the English language" is translated as "英语中的仇恨言语之细节" (Yīngyǔ zhōng de shūhèn yánwén zhī xiǎo jiě) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Fault-Injection-and-Safe-Error-Attack-for-Extraction-of-Embedded-Neural-Network-Models"><a href="#Fault-Injection-and-Safe-Error-Attack-for-Extraction-of-Embedded-Neural-Network-Models" class="headerlink" title="Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models"></a>Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16703">http://arxiv.org/abs/2308.16703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Hector, Pierre-Alain Moellic, Mathieu Dumont, Jean-Max Dutertre</li>
<li>for: 本研究主要针对于嵌入式深度神经网络模型在IoT设备上的安全性问题，特别是模型抽取攻击。</li>
<li>methods: 本研究使用了常见的缺陷插入攻击策略——安全错误攻击（SEA）来实现模型抽取攻击。攻击者具有有限的训练数据访问权限。</li>
<li>results: 研究发现，使用约1500个手动设计的输入可以成功抽取嵌入式深度神经网络模型中的至少90%最重要比特数据，以训练一个与受害模型具有相似准确率的假模型。<details>
<summary>Abstract</summary>
Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.
</details>
<details>
<summary>摘要</summary>
模型提取emerges as a critical security threat, with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of the most significant bits with about 1500 crafted inputs. These information enable to efficiently train a substitute model, with only 8% of the training dataset, that reaches high fidelity and near identical accuracy level than the victim model.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Using-Large-Language-Models-to-Automate-Category-and-Trend-Analysis-of-Scientific-Articles-An-Application-in-Ophthalmology"><a href="#Using-Large-Language-Models-to-Automate-Category-and-Trend-Analysis-of-Scientific-Articles-An-Application-in-Ophthalmology" class="headerlink" title="Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology"></a>Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16688">http://arxiv.org/abs/2308.16688</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hina Raja, Asim Munawar, Mohammad Delsoz, Mohammad Elahi, Yeganeh Madadi, Amr Hassan, Hashem Abu Serhan, Onur Inam, Luis Hermandez, Sang Tran, Wuqas Munir, Alaa Abd-Alrazaq, Hao Chen, SiamakYousefi</li>
<li>for: 这 paper 的目的是提出一种自动化文献分类方法，利用大型自然语言处理（NLP）技术和大语言模型（LLM）。</li>
<li>methods: 该方法基于 NLP 技术，包括高级 ZSL LLM 模型，对科学论文的文本内容进行处理和分析。</li>
<li>results: 实验结果表明，LLM 可以高效地自动分类大量的眼科论文，无需人工干预。在 RenD 数据集上，模型达到了平均准确率 0.86 和平均 F1 分数 0.85。Here’s the breakdown of each point:</li>
<li>for: The paper aims to propose an automated method for article classification using Large Language Models (LLMs) in the field of ophthalmology, but the model is extendable to other fields.</li>
<li>methods: The method is based on Natural Language Processing (NLP) techniques, including advanced LLMs, to process and analyze the textual content of scientific papers.</li>
<li>results: The experimental results demonstrate the effectiveness of LLMs in categorizing large number of ophthalmology papers without human intervention. The model achieved a mean accuracy of 0.86 and mean F1 of 0.85 based on the RenD dataset.<details>
<summary>Abstract</summary>
Purpose: In this paper, we present an automated method for article classification, leveraging the power of Large Language Models (LLM). The primary focus is on the field of ophthalmology, but the model is extendable to other fields. Methods: We have developed a model based on Natural Language Processing (NLP) techniques, including advanced LLMs, to process and analyze the textual content of scientific papers. Specifically, we have employed zero-shot learning (ZSL) LLM models and compared against Bidirectional and Auto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder Representations from Transformers (BERT), and its variant such as distilBERT, SciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate the effectiveness of LLMs in categorizing large number of ophthalmology papers without human intervention. Results: To evalute the LLMs, we compiled a dataset (RenD) of 1000 ocular disease-related articles, which were expertly annotated by a panel of six specialists into 15 distinct categories. The model achieved mean accuracy of 0.86 and mean F1 of 0.85 based on the RenD dataset. Conclusion: The proposed framework achieves notable improvements in both accuracy and efficiency. Its application in the domain of ophthalmology showcases its potential for knowledge organization and retrieval in other domains too. We performed trend analysis that enables the researchers and clinicians to easily categorize and retrieve relevant papers, saving time and effort in literature review and information gathering as well as identification of emerging scientific trends within different disciplines. Moreover, the extendibility of the model to other scientific fields broadens its impact in facilitating research and trend analysis across diverse disciplines.
</details>
<details>
<summary>摘要</summary>
目的：在这篇论文中，我们提出了一种自动化文章分类方法，利用大型自然语言处理（NLP）模型的力量。我们的研究领域为眼科领域，但模型可以扩展到其他领域。方法：我们开发了基于NLP技术的模型，包括高级Zero-shot学习（ZSL）模型、bi-directional和自然语言模型（BART）和其变体、Bidirectional Encoder Representations from Transformers（BERT）和其变体如distilBERT、SciBERT、PubmedBERT、BioBERT。结果：我们对1000篇眼科疾病相关文章进行了自动分类，得到了人工干预无需的高精度分类结果。结果：为评估LLMs，我们编译了1000篇眼科疾病相关文章的 dataset（RenD），由6名专家 manually标注为15种不同类别。模型在RenD dataset上取得了0.86的 mean accuracy和0.85的 mean F1。结论：我们提出的框架实现了显著的提高 both accuracy和 efficiency。在眼科领域中应用该模型，可以帮助研究者和临床医生快速地分类和检索相关文章，节省时间和劳动力，并且可以快速地发现不同领域的科学趋势。此外，模型的扩展性使其在其他科学领域中有广泛的影响，推动了研究和趋势分析的进程。
</details></li>
</ul>
<hr>
<h2 id="Everyone-Can-Attack-Repurpose-Lossy-Compression-as-a-Natural-Backdoor-Attack"><a href="#Everyone-Can-Attack-Repurpose-Lossy-Compression-as-a-Natural-Backdoor-Attack" class="headerlink" title="Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack"></a>Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16684">http://arxiv.org/abs/2308.16684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sze Jue Yang, Quang Nguyen, Chee Seng Chan, Khoa Doan</li>
<li>for: 这个论文主要关注的是机器学习模型中的潜在攻击问题，具体来说是 silent backdoor 攻击。</li>
<li>methods: 这个论文使用了一种广泛使用的lossy图像压缩算法来实现攻击，而且这种攻击不需要特殊的技能和努力，只需要点击“转换”或“保存为”按钮即可。</li>
<li>results: 这个论文的实验结果表明，这种攻击可以在多个 benchmark 数据集中 achieved 100% 攻击成功率，而且在干净标签设定下，只需要杂 poisoning 率才能达到近百分之十的攻击成功率。<details>
<summary>Abstract</summary>
The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a trigger generator as seen in prior works and only requires poisoning the data. Empirically, the proposed attack consistently achieves 100% attack success rate in several benchmark datasets such as MNIST, CIFAR-10, GTSRB and CelebA. More significantly, the proposed attack can still achieve almost 100% attack success rate with very small (approximately 10%) poisoning rates in the clean label setting. The generated trigger of the proposed attack using one lossy compression algorithm is also transferable across other related compression algorithms, exacerbating the severity of this backdoor threat. This work takes another crucial step toward understanding the extensive risks of backdoor attacks in practice, urging practitioners to investigate similar attacks and relevant backdoor mitigation methods.
</details>
<details>
<summary>摘要</summary>
Recently, backdoor attacks have posed a significant threat to the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that only a select few can launch attacks, as designing the trigger generation algorithm requires significant effort and extensive experimentation to ensure stealth and effectiveness. However, this paper reveals a more severe backdoor threat: anyone can exploit an easily accessible algorithm for silent backdoor attacks. Specifically, the attacker can use widely-used lossy image compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using these tools. Through this attack, the adversary does not need to design a trigger generator as seen in prior works and only requires poisoning the data. Our empirical results consistently achieve a 100% attack success rate in several benchmark datasets, including MNIST, CIFAR-10, GTSRB, and CelebA. Moreover, the proposed attack can still achieve almost 100% attack success rate with very small (approximately 10%) poisoning rates in the clean label setting. The generated trigger using one lossy compression algorithm is also transferable across other related compression algorithms, exacerbating the severity of this backdoor threat. This work takes another crucial step toward understanding the extensive risks of backdoor attacks in practice, urging practitioners to investigate similar attacks and relevant backdoor mitigation methods.
</details></li>
</ul>
<hr>
<h2 id="Fault-Injection-on-Embedded-Neural-Networks-Impact-of-a-Single-Instruction-Skip"><a href="#Fault-Injection-on-Embedded-Neural-Networks-Impact-of-a-Single-Instruction-Skip" class="headerlink" title="Fault Injection on Embedded Neural Networks: Impact of a Single Instruction Skip"></a>Fault Injection on Embedded Neural Networks: Impact of a Single Instruction Skip</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16665">http://arxiv.org/abs/2308.16665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clement Gaine, Pierre-Alain Moellic, Olivier Potin, Jean-Max Dutertre</li>
<li>for: 这篇论文的目的是为了研究基于32位微控制器平台的神经网络模型的安全性，并通过电磁干扰和激光干扰来模拟硬件干扰的影响。</li>
<li>methods: 该论文使用了两种干扰方式，电磁干扰和激光干扰，并在Cortex M4 32位微控制器平台上进行了实验。而不同于大多数现有的内部参数或输入值修改方法，该论文的目标是通过控制流指令跳过来模拟内存干扰的影响。</li>
<li>results: 该论文发现了一些修改攻击的潜在威胁，可以让攻击者通过修改神经网络模型的控制流来改变模型的预测结果，并且可以根据不同的恶意目标来选择合适的攻击方法。<details>
<summary>Abstract</summary>
With the large-scale integration and use of neural network models, especially in critical embedded systems, their security assessment to guarantee their reliability is becoming an urgent need. More particularly, models deployed in embedded platforms, such as 32-bit microcontrollers, are physically accessible by adversaries and therefore vulnerable to hardware disturbances. We present the first set of experiments on the use of two fault injection means, electromagnetic and laser injections, applied on neural networks models embedded on a Cortex M4 32-bit microcontroller platform. Contrary to most of state-of-the-art works dedicated to the alteration of the internal parameters or input values, our goal is to simulate and experimentally demonstrate the impact of a specific fault model that is instruction skip. For that purpose, we assessed several modification attacks on the control flow of a neural network inference. We reveal integrity threats by targeting several steps in the inference program of typical convolutional neural network models, which may be exploited by an attacker to alter the predictions of the target models with different adversarial goals.
</details>
<details>
<summary>摘要</summary>
随着神经网络模型的大规模集成和应用，特别是在关键附加系统中，其安全评估已成为紧迫需要。更specifically， deployed in embedded platforms, such as 32-bit microcontrollers, are physically accessible by adversaries and therefore vulnerable to hardware disturbances. We present the first set of experiments on the use of two fault injection means, electromagnetic and laser injections, applied on neural network models embedded on a Cortex M4 32-bit microcontroller platform. Contrary to most of state-of-the-art works dedicated to the alteration of the internal parameters or input values, our goal is to simulate and experimentally demonstrate the impact of a specific fault model that is instruction skip. For that purpose, we assessed several modification attacks on the control flow of a neural network inference. We reveal integrity threats by targeting several steps in the inference program of typical convolutional neural network models, which may be exploited by an attacker to alter the predictions of the target models with different adversarial goals.Here's the text with some additional information about the translation:I translated the text into Simplified Chinese, which is the most widely used standard for Chinese writing. I tried to preserve the original meaning and structure of the text as much as possible, while also making it more fluent and natural-sounding in Chinese.Some notes on the translation:* "附加系统" (fùjí systems) is used to refer to "embedded systems" or "critical embedded systems" in Chinese.* "神经网络模型" (shénxīn wǎngluò módelì) is used to refer to "neural network models" in Chinese.* "instruction skip" is translated as "指令跳过" (fùjì skīp) in Chinese.* "modification attacks" is translated as "修改攻击" (xiūgòu hángchè) in Chinese.* "integrity threats" is translated as "完整性威胁" (wánzhèngxìng wēidāi) in Chinese.I hope this helps! Let me know if you have any further questions or if you need any additional assistance.
</details></li>
</ul>
<hr>
<h2 id="Developing-a-Scalable-Benchmark-for-Assessing-Large-Language-Models-in-Knowledge-Graph-Engineering"><a href="#Developing-a-Scalable-Benchmark-for-Assessing-Large-Language-Models-in-Knowledge-Graph-Engineering" class="headerlink" title="Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering"></a>Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16622">http://arxiv.org/abs/2308.16622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars-Peter Meyer, Johannes Frey, Kurt Junghanns, Felix Brei, Kirill Bulert, Sabine Gründer-Fahrer, Michael Martin</li>
<li>for: 本研究旨在评估和监测大语言模型（LLMs）的性能，特别是在知识图工程（KGE）领域。</li>
<li>methods: 本研究提出了一个基准框架，包括三个挑战，用于测试LLMs的 sintaxis和错误 corrections、事实提取和数据集生成能力。</li>
<li>results: 研究发现，当使用零 shot 提示时，LLMs 对知识图生成仍然不具备能力，因此提出了一个LLM-KG-Bench框架，用于自动评估和存储 LLM 响应，以及统计数据和视觉化工具来支持提问工程和模型性能跟踪。<details>
<summary>Abstract</summary>
As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLMs）的发展速度加剧，评估和监测其性能的需求日益突出。我们提出了一个专注于知识图工程（KGE）的 benchmarcking 框架，并提出了三个挑战，其中一个是语法和错误修正，另外两个是事实提取和数据集生成。我们发现，虽然 LLMS 是一个有用的工具，但它们无法在零shot提示下帮助知识图生成。因此，我们的 LLM-KG-Bench 框架提供了自动评估和存储 LLMS 回应，以及统计数据和可视化工具，以支持提问工程和模型性能追踪。
</details></li>
</ul>
<hr>
<h2 id="High-Accuracy-Location-Information-Extraction-from-Social-Network-Texts-Using-Natural-Language-Processing"><a href="#High-Accuracy-Location-Information-Extraction-from-Social-Network-Texts-Using-Natural-Language-Processing" class="headerlink" title="High Accuracy Location Information Extraction from Social Network Texts Using Natural Language Processing"></a>High Accuracy Location Information Extraction from Social Network Texts Using Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16615">http://arxiv.org/abs/2308.16615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lossan Bonde, Severin Dembele</li>
<li>for: 这篇论文是为了预测恐怖活动的目的。</li>
<li>methods: 这篇论文使用了社交媒体文本来提取必要的信息，以建立一个适合的数据集来预测恐怖活动。</li>
<li>results: 实验表明，现有的解决方案具有低精度，而我们的解决方案可以准确地识别地点信息。<details>
<summary>Abstract</summary>
Terrorism has become a worldwide plague with severe consequences for the development of nations. Besides killing innocent people daily and preventing educational activities from taking place, terrorism is also hindering economic growth. Machine Learning (ML) and Natural Language Processing (NLP) can contribute to fighting terrorism by predicting in real-time future terrorist attacks if accurate data is available. This paper is part of a research project that uses text from social networks to extract necessary information to build an adequate dataset for terrorist attack prediction. We collected a set of 3000 social network texts about terrorism in Burkina Faso and used a subset to experiment with existing NLP solutions. The experiment reveals that existing solutions have poor accuracy for location recognition, which our solution resolves. We will extend the solution to extract dates and action information to achieve the project's goal.
</details>
<details>
<summary>摘要</summary>
恐怖主义已成为全球的恶性疾病，对国家发展造成严重的影响。除了每天杀害无辜的人和破坏教育活动外，恐怖主义还妨碍经济增长。机器学习（ML）和自然语言处理（NLP）可以帮助斗争恐怖主义，预测未来恐怖袭击的可能性，只要有准确的数据。这篇论文是一项研究项目的一部分，使用社交媒体文本提取必要的信息建立恐怖袭击预测数据集。我们收集了3000个社交媒体文本关于恐怖主义在布基纳法索的样本，使用一个子集进行了现有NLP解决方案的实验。实验表明，现有的解决方案在位置识别方面有较差的准确率，我们的解决方案可以解决这个问题。我们将延续解决方案，以提取日期和动作信息，实现项目的目标。
</details></li>
</ul>
<hr>
<h2 id="Towards-Long-Tailed-Recognition-for-Graph-Classification-via-Collaborative-Experts"><a href="#Towards-Long-Tailed-Recognition-for-Graph-Classification-via-Collaborative-Experts" class="headerlink" title="Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts"></a>Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16609">http://arxiv.org/abs/2308.16609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu Yi, Zhengyang Mao, Wei Ju, Yongdao Zhou, Luchen Liu, Xiao Luo, Ming Zhang</li>
<li>for: 本文旨在为 Graf 级别分类提供有效的分类器，以掌握 Graph 级别的表示，并且在长尾分布的 Graph 数据上进行分类。</li>
<li>methods: 本文提出了一种基于多特效学习的长尾 Graph 级别分类框架，包括对均衡表示学习和分类器训练、硬件分类минning、灵活的权重融合和知识分离等方法。</li>
<li>results: 根据七个广泛使用的 benchmark 数据集的实验结果，我们的方法 CoMe 在与基eline比较的情况下显示出了superiority，并且在长尾分布下进行 Graph 级别分类时表现出了优异的效果。<details>
<summary>Abstract</summary>
Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classification framework via Collaborative Multi-expert Learning (CoMe) to tackle the problem. To equilibrate the contributions of head and tail classes, we first develop balanced contrastive learning from the view of representation learning, and then design an individual-expert classifier training based on hard class mining. In addition, we execute gated fusion and disentangled knowledge distillation among the multiple experts to promote the collaboration in a multi-expert framework. Comprehensive experiments are performed on seven widely-used benchmark datasets to demonstrate the superiority of our method CoMe over state-of-the-art baselines.
</details>
<details>
<summary>摘要</summary>
GRAPH classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classification framework via Collaborative Multi-expert Learning (CoMe) to tackle the problem. To equilibrate the contributions of head and tail classes, we first develop balanced contrastive learning from the view of representation learning, and then design an individual-expert classifier training based on hard class mining. In addition, we execute gated fusion and disentangled knowledge distillation among the multiple experts to promote the collaboration in a multi-expert framework. Comprehensive experiments are performed on seven widely-used benchmark datasets to demonstrate the superiority of our method CoMe over state-of-the-art baselines.
</details></li>
</ul>
<hr>
<h2 id="The-Quest-of-Finding-the-Antidote-to-Sparse-Double-Descent"><a href="#The-Quest-of-Finding-the-Antidote-to-Sparse-Double-Descent" class="headerlink" title="The Quest of Finding the Antidote to Sparse Double Descent"></a>The Quest of Finding the Antidote to Sparse Double Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16596">http://arxiv.org/abs/2308.16596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Quétu, Marta Milovanović</li>
<li>for: 本文目的是找到深度学习模型的优化大小，以提高性能并避免权重递减现象。</li>
<li>methods: 本文使用了一种简单的 $\ell_2$ 正则化方法和知识整合学习方法来解决权重递减现象。</li>
<li>results: 实验结果表明，使用这种方法可以避免权重递减现象，并且可以在图像分类任务中 достичь更好的性能。<details>
<summary>Abstract</summary>
In energy-efficient schemes, finding the optimal size of deep learning models is very important and has a broad impact. Meanwhile, recent studies have reported an unexpected phenomenon, the sparse double descent: as the model's sparsity increases, the performance first worsens, then improves, and finally deteriorates. Such a non-monotonic behavior raises serious questions about the optimal model's size to maintain high performance: the model needs to be sufficiently over-parametrized, but having too many parameters wastes training resources.   In this paper, we aim to find the best trade-off efficiently. More precisely, we tackle the occurrence of the sparse double descent and present some solutions to avoid it. Firstly, we show that a simple $\ell_2$ regularization method can help to mitigate this phenomenon but sacrifices the performance/sparsity compromise. To overcome this problem, we then introduce a learning scheme in which distilling knowledge regularizes the student model. Supported by experimental results achieved using typical image classification setups, we show that this approach leads to the avoidance of such a phenomenon.
</details>
<details>
<summary>摘要</summary>
在能效学习方案中，发现优化模型的大小非常重要，它会对性能产生广泛的影响。然而，最近的研究发现了一种意外现象：随着模型的稀疏性增加，性能首先恶化，然后改善，最后恶化。这种非 monotonic 的行为引发了优化模型大小以保持高性能的严重问题：模型需要充分过 parametrization，但过多的参数会浪费训练资源。在这篇文章中，我们目标是寻找最佳的平衡，更具体地说，我们解决 sparse double descent 现象，并提供一些解决方案。首先，我们表明了一种简单的 $\ell_2$ 正则化方法可以减轻这种现象，但是这会牺牲性能/稀疏性的权衡。为了解决这个问题，我们然后引入一种知识整合学习方法，通过这种方法，学生模型可以从导师模型中学习知识。通过实验结果，我们显示了这种方法可以避免 sparse double descent 现象。
</details></li>
</ul>
<hr>
<h2 id="CL-MAE-Curriculum-Learned-Masked-Autoencoders"><a href="#CL-MAE-Curriculum-Learned-Masked-Autoencoders" class="headerlink" title="CL-MAE: Curriculum-Learned Masked Autoencoders"></a>CL-MAE: Curriculum-Learned Masked Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16572">http://arxiv.org/abs/2308.16572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neelu Madan, Nicolae-Catalin Ristea, Kamal Nasrollahi, Thomas B. Moeslund, Radu Tudor Ionescu</li>
<li>for: 提高自我超vised学习的表示学习能力</li>
<li>methods: 使用curriculum学习方法，逐渐增加masking策略的复杂度，从而训练模型学习更加复杂和可传播的表示</li>
<li>results: 训练CL-MAE模型在ImageNet上，并在五个下游任务上显示出优于MAE模型的表示学习能力<details>
<summary>Abstract</summary>
Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same reconstruction loss) to an adversary (optimizing the opposite loss), while passing through a neutral state. The transition between these behaviors is smooth, being regulated by a factor that is multiplied with the reconstruction loss of the masking module. The resulting training procedure generates an easy-to-hard curriculum. We train our Curriculum-Learned Masked Autoencoder (CL-MAE) on ImageNet and show that it exhibits superior representation learning capabilities compared to MAE. The empirical results on five downstream tasks confirm our conjecture, demonstrating that curriculum learning can be successfully used to self-supervise masked autoencoders.
</details>
<details>
<summary>摘要</summary>
马SK模型（Masked Image Modeling）已经证明是一种强大的预tex task，可以生成可以广泛应用的多个下游任务中的稳定表示。通常，这种方法 involve randomly masking patches（ tokens）在输入图像中，并且masking策略在训练过程中保持不变。在这篇论文中，我们提议了一种学习级 curriculum learningapproach，即在训练过程中不断更新masking策略，以增加自我超vised reconstruction任务的复杂性。我们 conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations。为了实现这一目标，我们提出了一种新的可学习的masking模块，具有可以生成不同复杂性的masks的能力。我们将这个模块 integrate into masked autoencoders (MAE)，并在训练过程中jointly train它。在训练过程中，我们将masking模块的行为逐渐从MAE的合作者（同样optimize reconstruction loss）转化为对手（optimize opposite loss），而在过渡过程中，masking模块的行为会随着一个因子的multiplication，以控制masking模块的权重。这种过渡过程是平滑的，使得我们可以通过训练程序来生成一个易于增加的curriculum。我们在ImageNet上训练了我们的Curriculum-Learned Masked Autoencoder (CL-MAE)，并证明了它在多个下游任务中表现出了superior representation learning capabilities。empirical results on five downstream tasks confirm our conjecture, demonstrating that curriculum learning can be successfully used to self-supervise masked autoencoders。
</details></li>
</ul>
<hr>
<h2 id="The-Power-of-MEME-Adversarial-Malware-Creation-with-Model-Based-Reinforcement-Learning"><a href="#The-Power-of-MEME-Adversarial-Malware-Creation-with-Model-Based-Reinforcement-Learning" class="headerlink" title="The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning"></a>The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16562">http://arxiv.org/abs/2308.16562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stratosphereips/meme_malware_rl">https://github.com/stratosphereips/meme_malware_rl</a></li>
<li>paper_authors: Maria Rigaki, Sebastian Garcia</li>
<li>for: This paper is written for researchers and practitioners in the field of malware detection and defense, particularly those interested in the use of machine learning and automation for malware detection.</li>
<li>methods: The paper proposes a new algorithm called MEME (Malware Evasion and Model Extraction) attacks, which combines model-based reinforcement learning and adversarial modification of Windows executable binary samples to evade malware detection.</li>
<li>results: The paper evaluates the MEME algorithm against two state-of-the-art attacks in adversarial malware creation and shows that MEME outperforms the state-of-the-art methods in terms of evasion capabilities, producing evasive malware with an evasion rate in the range of 32-73%. The paper also shows that the surrogate models produced by MEME have a high agreement with the target models, with a prediction label agreement between 97-99%.<details>
<summary>Abstract</summary>
Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of evasion capabilities in almost all cases, producing evasive malware with an evasion rate in the range of 32-73%. It also produces surrogate models with a prediction label agreement with the respective target models between 97-99%. The surrogate could be used to fine-tune and improve the evasion rate in the future.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)由于恶意软件的普及，防御者正在不断地启用自动化和机器学习作为恶意软件检测工具链的一部分。然而，机器学习模型受到了针对性攻击的威胁，需要测试模型和产品的可靠性。同时，攻击者也尝试自动生成恶意软件和绕过安全软件系统，防御者则尝试了解他们的方法。这个工作提出了一个新的算法，它将恶意软件逃脱和模型提取（MEME）攻击组合起来。MEME使用基于模型的强化学习来对Windows执行文件binary样本进行针对性修改，同时培养一个与目标模型具有高协调的副模型。为了评估这种方法，我们与三个公开发布的模型和一个安全产品作为目标进行比较。结果显示，MEME在逃脱能力方面与状态法比较，生成了97-99%的预测标签一致的副模型，并且生成了32-73%的逃脱率。这些副模型可以用来练化和改进逃脱率的未来。
</details></li>
</ul>
<hr>
<h2 id="On-a-Connection-between-Differential-Games-Optimal-Control-and-Energy-based-Models-for-Multi-Agent-Interactions"><a href="#On-a-Connection-between-Differential-Games-Optimal-Control-and-Energy-based-Models-for-Multi-Agent-Interactions" class="headerlink" title="On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions"></a>On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16539">http://arxiv.org/abs/2308.16539</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Diehl, Tobias Klosek, Martin Krüger, Nils Murzyn, Torsten Bertram</li>
<li>for: This paper is written for modeling multi-agent interactions in real-world robotics applications using game theory.</li>
<li>methods: The paper uses a combination of differential games, optimal control, and energy-based models to address challenges in applying game theory to real-world robotics.</li>
<li>results: The paper introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, and demonstrates empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones using simulated mobile robot pedestrian interactions and real-world automated driving data.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了模型多智能体交互在现实世界机器人应用中使用游戏理论。</li>
<li>methods: 这篇论文使用了分析游戏、优化控制和能量基本模型来解决在实际世界机器人应用中应用游戏理论的挑战。</li>
<li>results: 这篇论文介绍了一种新的综合学习应用程序，将神经网络用于游戏参数推理和可微游戏理论优化层，并通过模拟移动机器人人行交互和实际自动驾驶数据进行了实验，证明了游戏理论层可以提高各种神经网络背景的预测性能。<details>
<summary>Abstract</summary>
Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-AI-Revolution-Opportunities-and-Challenges-for-the-Finance-Sector"><a href="#The-AI-Revolution-Opportunities-and-Challenges-for-the-Finance-Sector" class="headerlink" title="The AI Revolution: Opportunities and Challenges for the Finance Sector"></a>The AI Revolution: Opportunities and Challenges for the Finance Sector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16538">http://arxiv.org/abs/2308.16538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carsten Maple, Lukasz Szpruch, Gregory Epiphaniou, Kalina Staykova, Simran Singh, William Penwarden, Yisi Wen, Zijian Wang, Jagdish Hariharan, Pavle Avramovic</li>
<li>for: 本研究探讨了人工智能（AI）在金融领域的应用，描述了其可能性，并讨论了其挑战。</li>
<li>methods: 本研究使用了多种方法，包括客户服务改进、诈骗检测、风险管理和信贷评估等。</li>
<li>results: 本研究发现，AI在金融领域的应用可以提高客户服务质量、提高风险管理和信贷评估等方面的效率，但同时也存在许多挑战，如透明度、解释性、公平性和信任worthiness等问题。<details>
<summary>Abstract</summary>
This report examines Artificial Intelligence (AI) in the financial sector, outlining its potential to revolutionise the industry and identify its challenges. It underscores the criticality of a well-rounded understanding of AI, its capabilities, and its implications to effectively leverage its potential while mitigating associated risks. The potential of AI potential extends from augmenting existing operations to paving the way for novel applications in the finance sector. The application of AI in the financial sector is transforming the industry. Its use spans areas from customer service enhancements, fraud detection, and risk management to credit assessments and high-frequency trading. However, along with these benefits, AI also presents several challenges. These include issues related to transparency, interpretability, fairness, accountability, and trustworthiness. The use of AI in the financial sector further raises critical questions about data privacy and security. A further issue identified in this report is the systemic risk that AI can introduce to the financial sector. Being prone to errors, AI can exacerbate existing systemic risks, potentially leading to financial crises. Regulation is crucial to harnessing the benefits of AI while mitigating its potential risks. Despite the global recognition of this need, there remains a lack of clear guidelines or legislation for AI use in finance. This report discusses key principles that could guide the formation of effective AI regulation in the financial sector, including the need for a risk-based approach, the inclusion of ethical considerations, and the importance of maintaining a balance between innovation and consumer protection. The report provides recommendations for academia, the finance industry, and regulators.
</details>
<details>
<summary>摘要</summary>
AI has the potential to transform the financial sector, with applications in customer service, fraud detection, risk management, credit assessments, and high-frequency trading. However, AI also raises several challenges, including issues related to transparency, interpretability, fairness, accountability, and trustworthiness. Additionally, the use of AI in the financial sector raises critical questions about data privacy and security.The report also highlights the systemic risk that AI can introduce to the financial sector, as it can exacerbate existing systemic risks and potentially lead to financial crises. To address these risks, the report proposes key principles for effective AI regulation in the financial sector, including a risk-based approach, ethical considerations, and a balance between innovation and consumer protection.The report provides recommendations for academia, the finance industry, and regulators, emphasizing the need for a comprehensive understanding of AI's potential and challenges to ensure the responsible use of AI in the financial sector.
</details></li>
</ul>
<hr>
<h2 id="Conditioning-Score-Based-Generative-Models-by-Neuro-Symbolic-Constraints"><a href="#Conditioning-Score-Based-Generative-Models-by-Neuro-Symbolic-Constraints" class="headerlink" title="Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints"></a>Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16534">http://arxiv.org/abs/2308.16534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Scassola, Sebastiano Saccani, Ginevra Carbone, Luca Bortolussi</li>
<li>for: 该论文旨在提出一种不需要额外训练的方法，可以从conditionale的score-based生成模型中随机抽取符合用户定义的逻辑约束的样本。</li>
<li>methods: 该方法首先解释了如何使用学习得到的分数来随机抽取不归一化分布的样本，然后定义了一种灵活且数字化的符号逻辑框架，用于编码软逻辑约束。最后，该方法结合了这两个元素，实现了一种通用但是近似的随机抽取算法。</li>
<li>results: 该论文通过对各种约束和数据进行实验，包括表格数据、图像和时间序列，证明了该方法的有效性。<details>
<summary>Abstract</summary>
Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach for various types of constraints and data: tabular data, images and time series.
</details>
<details>
<summary>摘要</summary>
Score-based和扩散模型已成为条件和无条件生成的有效方法。然而，条件生成仍基于 Either a specific training of a conditional model or classifier guidance，需要训练一个受噪声依赖的分类器，即使给定了对不受扰干的数据的分类器。我们提出一种方法，可以从无条件分数基的生成模型中采样，不需要任何额外训练。我们首先示出如何 manipulate the learned score，以采样从一个未 норmal化的分布，条件于用户定义的约束。然后，我们定义了一种灵活且数值稳定的神经符号学框架，用于编码软逻辑约束。将这两个元素组合起来，我们得到一种通用的，但是近似的条件采样算法。我们进一步开发了有效的规则，以改进近似。最后，我们证明了我们的方法对各种约束和数据类型（表格数据、图像和时间序列）具有效果。
</details></li>
</ul>
<hr>
<h2 id="Developing-Social-Robots-with-Empathetic-Non-Verbal-Cues-Using-Large-Language-Models"><a href="#Developing-Social-Robots-with-Empathetic-Non-Verbal-Cues-Using-Large-Language-Models" class="headerlink" title="Developing Social Robots with Empathetic Non-Verbal Cues Using Large Language Models"></a>Developing Social Robots with Empathetic Non-Verbal Cues Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16529">http://arxiv.org/abs/2308.16529</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoon Kyung Lee, Yoonwon Jung, Gyuyi Kang, Sowon Hahn</li>
<li>For: The paper aims to enhance the empathetic capacities of social robots by incorporating non-verbal cues.* Methods: The authors use a Large Language Model (LLM) to generate four types of empathetic non-verbal cues (Speech, Action, Facial expression, and Emotion) in a social robot.* Results: The preliminary results show that the robot is able to recognize and respond to social cues, such as nodding gestures and positive emotions, in a more authentic and context-aware manner.Here are the three key points in Simplified Chinese:* For: 增强社交机器人的共鸣能力，通过 integrate 非语言价值。* Methods: 使用 Large Language Model (LLM) 设计并标注四种共鸣非语言价值（Speech、Action、Facial expression、Emotion），并将其应用于社交机器人。* Results: 初步结果表明，机器人能够识别和响应社交价值，如护拍姿势和积极情感，以更加authentic和上下文感知的方式。<details>
<summary>Abstract</summary>
We propose augmenting the empathetic capacities of social robots by integrating non-verbal cues. Our primary contribution is the design and labeling of four types of empathetic non-verbal cues, abbreviated as SAFE: Speech, Action (gesture), Facial expression, and Emotion, in a social robot. These cues are generated using a Large Language Model (LLM). We developed an LLM-based conversational system for the robot and assessed its alignment with social cues as defined by human counselors. Preliminary results show distinct patterns in the robot's responses, such as a preference for calm and positive social emotions like 'joy' and 'lively', and frequent nodding gestures. Despite these tendencies, our approach has led to the development of a social robot capable of context-aware and more authentic interactions. Our work lays the groundwork for future studies on human-robot interactions, emphasizing the essential role of both verbal and non-verbal cues in creating social and empathetic robots.
</details>
<details>
<summary>摘要</summary>
我们提议通过 интеграción非语言cue来增强社交机器人的共鸣能力。我们的主要贡献是设计和标签四种共鸣非语言cue，简称为SAFE：语音、动作（姿势）、 facial expression 和情感，在社交机器人中。这些cue使用大自然语言模型（LLM）生成。我们开发了基于LLM的对话系统，并评估了人工辅导员定义的社交cue的对应关系。初步结果表明机器人的回应存在明显的偏好，如宁静和积极社交情感如“喜悦”和“活泼”，以及频繁的头部 nodding 动作。尽管如此，我们的方法已经导致了一个Context-aware的社交机器人，可以进行更加authentic的互动。我们的工作为未来人机交互研究提供了基础，强调语言和非语言cue在创造社交和共鸣机器人中的重要性。
</details></li>
</ul>
<hr>
<h2 id="Curvature-based-Pooling-within-Graph-Neural-Networks"><a href="#Curvature-based-Pooling-within-Graph-Neural-Networks" class="headerlink" title="Curvature-based Pooling within Graph Neural Networks"></a>Curvature-based Pooling within Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16516">http://arxiv.org/abs/2308.16516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/cedric_sanders/masterarbeit">https://gitlab.com/cedric_sanders/masterarbeit</a></li>
<li>paper_authors: Cedric Sanders, Andreas Roth, Thomas Liebig</li>
<li>for: 本文旨在提高图 neural network (GNN) 的能力，解决图学习中的过拟合和过缩减问题。</li>
<li>methods: 本文提出了一种新的池化方法叫做 CurvPool，它利用图的曲率特征来自适应地选择结构，以减少过拟合和过缩减问题。</li>
<li>results: 比较 experiment 表明，CurvPool 在图分类任务中表现出色，其精度高于其他相关方法，并且具有更好的计算复杂性和灵活性。<details>
<summary>Abstract</summary>
Over-squashing and over-smoothing are two critical issues, that limit the capabilities of graph neural networks (GNNs). While over-smoothing eliminates the differences between nodes making them indistinguishable, over-squashing refers to the inability of GNNs to propagate information over long distances, as exponentially many node states are squashed into fixed-size representations. Both phenomena share similar causes, as both are largely induced by the graph topology. To mitigate these problems in graph classification tasks, we propose CurvPool, a novel pooling method. CurvPool exploits the notion of curvature of a graph to adaptively identify structures responsible for both over-smoothing and over-squashing. By clustering nodes based on the Balanced Forman curvature, CurvPool constructs a graph with a more suitable structure, allowing deeper models and the combination of distant information. We compare it to other state-of-the-art pooling approaches and establish its competitiveness in terms of classification accuracy, computational complexity, and flexibility. CurvPool outperforms several comparable methods across all considered tasks. The most consistent results are achieved by pooling densely connected clusters using the sum aggregation, as this allows additional information about the size of each pool.
</details>
<details>
<summary>摘要</summary>
Over-squashing和over-smoothing是两个关键问题，它们限制了图神经网络（GNN）的能力。而over-smoothing使得节点变得无法分辨，而over-squashing则是GNN无法在长距离传播信息的问题，这两个问题都是由图 topology引起的。为了解决这些问题在图分类任务中，我们提出了CurvPool，一种新的池化方法。CurvPool利用图的 curvature来自适应地识别导致over-smoothing和over-squashing的结构。通过基于Balanced Forman curvature的归一化，CurvPool将节点分组成更适合的结构，以便 deeper models和融合远程信息。我们与其他当前最佳池化方法进行比较，并证明CurvPool在分类精度、计算复杂度和灵活性方面具有竞争力。CurvPool在所有考虑的任务中表现出了最佳的结果，并且在使用积加聚合 pooling densely connected clusters时，可以获得更加稳定的结果，因为这种方法可以提供更多关于pool size的信息。
</details></li>
</ul>
<hr>
<h2 id="Recommender-AI-Agent-Integrating-Large-Language-Models-for-Interactive-Recommendations"><a href="#Recommender-AI-Agent-Integrating-Large-Language-Models-for-Interactive-Recommendations" class="headerlink" title="Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"></a>Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16505">http://arxiv.org/abs/2308.16505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie</li>
<li>for: 本研究旨在结合推荐模型和大语言模型（LLM）创造一个多功能且交互的推荐系统，以提高推荐系统的功能和用户体验。</li>
<li>methods: 本研究使用了LLM作为智能核心，并结合了多种推荐模型作为工具，以实现交互式推荐。研究提出了一个有效的框架 named RecAgent，并实现了一个简单的工作流程，包括内存总线、动态示范增强任务规划和反射。</li>
<li>results: 实验结果表明，RecAgent在多个公共数据集上实现了满意的对话式推荐系统性能，比较于通用的LLM更高。<details>
<summary>Abstract</summary>
Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.   In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called RecAgent, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into RecAgent. We then propose an efficient workflow within RecAgent for task execution, incorporating key components such as a memory bus, dynamic demonstration-augmented task planning, and reflection. RecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that RecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified Chinese推荐模型在域pecific的物品推荐方面表现出色，通过对用户行为数据的抓取和分析而成为轻量级域专家。然而，它们在提供解释和进行对话方面存在限制，而大语言模型（LLM）则在指令理解、通用常识和人机交互方面表现惊人。然而，LLM缺乏域pecific的物品目录和用户行为模式的知识，特别是在与普通世界知识的领域相互独立的情况下。不得 économic nor efficient 的方式finetuning LLMs for each domain。  在这篇论文中，我们将推荐模型和LLM之间的差距bridged，将它们的优势相互结合，创造一个多才多艺的和交互的推荐系统。我们提出了一个效率的框架called RecAgent，其中LLMs acts as the brain，推荐模型 acts as tools。我们首先列出了将LLMs转化为RecAgent所需的最小必备工具。然后，我们提出了RecAgent中任务执行的高效工作流程，包括内存总线、动态示范增强任务规划和反思。RecAgent使得传统的推荐系统，如ID基于的矩阵因子化模型，成为了交互系统，通过与LLMs的集成，并通过自然语言界面提供了对用户的交互。我们在一些公共数据集上进行了实验，结果表明，RecAgent在对话推荐系统方面表现满意，比通用的LLMs更好。Note: The translation is done using a machine translation tool, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="Individually-Rational-Collaborative-Vehicle-Routing-through-Give-And-Take-Exchanges"><a href="#Individually-Rational-Collaborative-Vehicle-Routing-through-Give-And-Take-Exchanges" class="headerlink" title="Individually Rational Collaborative Vehicle Routing through Give-And-Take Exchanges"></a>Individually Rational Collaborative Vehicle Routing through Give-And-Take Exchanges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16501">http://arxiv.org/abs/2308.16501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Mingzheng Tang, Ba Phong Tran, Hoong Chuin Lau</li>
<li>For: 本研究旨在自动化物流公司间的订单交易，以便最大化总收益。* Methods: 我们提出了一种多代理方法，将内给运输问题转化为协力运输问题（CVRP），并运用单位运输问题（VRP）的原则来对两辆车的组合进行优化。我们的算法考虑了标准VRP的约束和个人合理性约束，并通过帮助竞争的物流代理人实现协力，以获得更好的总路线和系统效率。* Results: 我们透过实际测试使用重要物流公司的数据，证明了我们的算法能够快速获得许多优化的解，强调了它的实际应用性和可能性Transform the logistics industry。<details>
<summary>Abstract</summary>
In this paper, we are concerned with the automated exchange of orders between logistics companies in a marketplace platform to optimize total revenues. We introduce a novel multi-agent approach to this problem, focusing on the Collaborative Vehicle Routing Problem (CVRP) through the lens of individual rationality. Our proposed algorithm applies the principles of Vehicle Routing Problem (VRP) to pairs of vehicles from different logistics companies, optimizing the overall routes while considering standard VRP constraints plus individual rationality constraints. By facilitating cooperation among competing logistics agents through a Give-and-Take approach, we show that it is possible to reduce travel distance and increase operational efficiency system-wide. More importantly, our approach ensures individual rationality and faster convergence, which are important properties of ensuring the long-term sustainability of the marketplace platform. We demonstrate the efficacy of our approach through extensive experiments using real-world test data from major logistics companies. The results reveal our algorithm's ability to rapidly identify numerous optimal solutions, underscoring its practical applicability and potential to transform the logistics industry.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们关注了市场平台上的物流公司之间自动订单交换以优化总收益。我们提出了一种新的多代理模型，通过对协同车辆Routing问题（CVRP）进行定点剖析，以实现个体合理性。我们的提议的算法运用了汽车Routing问题（VRP）的原则，对不同物流公司的车辆对应的对应，优化总路径，同时考虑标准VRP约束以及个体合理性约束。通过在竞争物流代理之间促进合作，我们采用了“给与take”方法，从而减少旅行距离，提高系统综合效率。更重要的是，我们的方法保证了个体合理性和快速收敛，这些性质对于长期稳定性的市场平台是非常重要。我们通过使用实际的物流公司数据进行广泛的实验，证明了我们的算法的实用性和可能性。结果表明，我们的算法能够快速发现许多优化解决方案，这些解决方案在实际应用中具有实际意义和潜在的变革力。
</details></li>
</ul>
<hr>
<h2 id="Generalised-Winograd-Schema-and-its-Contextuality"><a href="#Generalised-Winograd-Schema-and-its-Contextuality" class="headerlink" title="Generalised Winograd Schema and its Contextuality"></a>Generalised Winograd Schema and its Contextuality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16498">http://arxiv.org/abs/2308.16498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kin Ian Lo, Mehrnoosh Sadrzadeh, Shane Mansfield</li>
<li>for: 这篇论文的目的是研究语言ambiguity和量子上下文性之间的关系。</li>
<li>methods: 该论文使用了sheaf-theoretic模型来研究语言ambiguity，并在Winograd schema中实验了量子上下文性。</li>
<li>results: 该研究发现，通过模拟Winograd schema的量子物理实验，可以观察到语言ambiguity中的量子上下文性。此外，该研究还发现了一种新的机制来扩展Winograd schema，使其更能模拟人类的理解。<details>
<summary>Abstract</summary>
Ambiguities in natural language give rise to probability distributions over interpretations. The distributions are often over multiple ambiguous words at a time; a multiplicity which makes them a suitable topic for sheaf-theoretic models of quantum contextuality. Previous research showed that different quantitative measures of contextuality correlate well with Psycholinguistic research on lexical ambiguities. In this work, we focus on coreference ambiguities and investigate the Winograd Schema Challenge (WSC), a test proposed by Levesque in 2011 to evaluate the intelligence of machines. The WSC consists of a collection of multiple-choice questions that require disambiguating pronouns in sentences structured according to the Winograd schema, in a way that makes it difficult for machines to determine the correct referents but remains intuitive for human comprehension. In this study, we propose an approach that analogously models the Winograd schema as an experiment in quantum physics. However, we argue that the original Winograd Schema is inherently too simplistic to facilitate contextuality. We introduce a novel mechanism for generalising the schema, rendering it analogous to a Bell-CHSH measurement scenario. We report an instance of this generalised schema, complemented by the human judgements we gathered via a crowdsourcing platform. The resulting model violates the Bell-CHSH inequality by 0.192, thus exhibiting contextuality in a coreference resolution setting.
</details>
<details>
<summary>摘要</summary>
自然语言中的歧义给出了概率分布 над  interpretations。这些分布通常包括多个歧义词的时候; 这种多样性使得它们成为量子上下文uality的适当主题。过去的研究表明了不同的量化contextuality推量well with Psycholinguistic research on lexical ambiguities。在这项工作中，我们关注核心引用ambiguities和 investigate the Winograd Schema Challenge (WSC), proposed by Levesque in 2011 to evaluate the intelligence of machines. WSC consists of a collection of multiple-choice questions that require disambiguating pronouns in sentences structured according to the Winograd schema, making it difficult for machines to determine the correct referents but remains intuitive for human comprehension.在这项工作中，我们提出了一种方法，即模拟Winograd schema为量子物理实验。然而，我们认为原始的Winograd schema是太简单，无法促进上下文uality。我们引入了一种新的机制，使得Winograd schema可以扩展，类似于Bell-CHSH测量场景。我们报道了这个扩展的schema，并通过一个人类判断平台收集了数据。得到的模型违反了Bell-CHSH不等式by 0.192，因此在核心引用解决设置下表现出了上下文uality。
</details></li>
</ul>
<hr>
<h2 id="Expanding-Frozen-Vision-Language-Models-without-Retraining-Towards-Improved-Robot-Perception"><a href="#Expanding-Frozen-Vision-Language-Models-without-Retraining-Towards-Improved-Robot-Perception" class="headerlink" title="Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception"></a>Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16493">http://arxiv.org/abs/2308.16493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Riley Tavassoli, Mani Amani, Reza Akhavian<br>for:This paper aims to improve the scene understanding of vision-language models (VLMs) by aligning the embedding spaces of different modalities, such as inertial measurement unit (IMU) data, with the vision embedding space.methods:The proposed method combines supervised and contrastive training to align the embedding spaces of different modalities with the vision embedding space, without requiring retraining of the VLM. The IMU embeddings are given directly to the model, allowing for nonlinear interactions between the query, image, and IMU signal.results:The proposed method is evaluated through experiments on human activity recognition using IMU data and visual inputs. The results show that using multiple modalities as input improves the VLM’s scene understanding and enhances its overall performance in various tasks, demonstrating the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
Vision-language models (VLMs) have shown powerful capabilities in visual question answering and reasoning tasks by combining visual representations with the abstract skill set large language models (LLMs) learn during pretraining. Vision, while the most popular modality to augment LLMs with, is only one representation of a scene. In human-robot interaction scenarios, robot perception requires accurate scene understanding by the robot. In this paper, we define and demonstrate a method of aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training, enabling the VLM to understand and reason about these additional modalities without retraining. We opt to give the model IMU embeddings directly over using a separate human activity recognition model that feeds directly into the prompt to allow for any nonlinear interactions between the query, image, and IMU signal that would be lost by mapping the IMU data to a discrete activity label. Further, we demonstrate our methodology's efficacy through experiments involving human activity recognition using IMU data and visual inputs. Our results show that using multiple modalities as input improves the VLM's scene understanding and enhances its overall performance in various tasks, thus paving the way for more versatile and capable language models in multi-modal contexts.
</details>
<details>
<summary>摘要</summary>
视力语言模型（VLM）已经展现出极强的能力在视觉问答和理解任务中，通过将视觉表示与大语言模型（LLM）在预训练时学习的抽象技能相结合。视觉，是现实中最受欢迎的感知模式，但是只是场景理解中的一种表示。在人机交互场景中，机器人需要准确地理解场景。在这篇论文中，我们定义并实现了将不同modalities（在这种情况下是测量单元（IMU）数据）的 embedding 空间与视觉 embedding 空间对齐，使得 VLM 能够理解和处理这些其他模式，无需重新训练。我们选择将 IMU 嵌入直接给模型，而不是使用一个独立的人类活动识别模型，以便保留非线性交互 между查询、图像和 IMU 信号。此外，我们通过对人类活动识别 tasks 进行实验，证明了我们的方法的有效性。我们的结果表明，将多种模式作为输入，可以提高 VLM 的场景理解和总性性能，从而开创更多功能强大的语言模型在多modal contexts。
</details></li>
</ul>
<hr>
<h2 id="In-class-Data-Analysis-Replications-Teaching-Students-while-Testing-Science"><a href="#In-class-Data-Analysis-Replications-Teaching-Students-while-Testing-Science" class="headerlink" title="In-class Data Analysis Replications: Teaching Students while Testing Science"></a>In-class Data Analysis Replications: Teaching Students while Testing Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16491">http://arxiv.org/abs/2308.16491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristina Gligoric, Tiziano Piccardi, Jake Hofman, Robert West</li>
<li>for: 这个论文目的是为了探讨在数据分析教程中包含复制任务的可行性，以及这种方法对学生、教师和科学家的影响。</li>
<li>methods: 这个研究使用了在EPFL教授的应用数据分析课程（CS-401）中包含复制任务的方法，并通过在课程进行的问卷调查来收集数据。</li>
<li>results: 研究发现学生可以复制已经发表的科学论文，大多数情况下是质量的，一些情况下是准确的。学生对复制任务的期望和实际经验之间存在差异，这些差异共同证明了对critical thinking的激励作用。此外，教师可以了解在教室中包含复制任务的成本和问题，以及这种方法对传统任务的比较。研究还发现了对科学社区的具体利益，如复制报告和科学工作中避免的复制障碍。<details>
<summary>Abstract</summary>
Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?   In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data analysis replications and what they experience by doing them along with changes in expectations about reproducibility, which together serve as evidence of attitude shifts to foster students' critical thinking. Second, we provide information for educators about how much overhead is needed to incorporate replications into the classroom and identify concerns that replications bring as compared to more traditional assignments. Third, we identify tangible benefits of the in-class data analysis replications for scientific communities, such as a collection of replication reports and insights about replication barriers in scientific work that should be avoided going forward.   Overall, we demonstrate that incorporating replication tasks into a large data science class can increase the reproducibility of scientific work as a by-product of data science instruction, thus benefiting both science and students.
</details>
<details>
<summary>摘要</summary>
科学面临着可重现危机。 previous work提议在课程中包含数据分析重复，以解决这个问题。然而，尚未确定这种方法是否实施可行，以及参与者们（学生、教师和科学家）应该期望什么。学生在课程中完成数据分析重复是否可能？教师所承担的成本和利益是什么？这种解决方案可以如何帮助评估和改进科学的状况？在 presente study中，我们在EPFL教授的应用数据分析课程（CS-401）中 integrate了数据分析重复。我们通过课程中的问naire进行了预先注册的发现，发现学生可以重复已发表的科学论文，大多数是Qualitatively相同，一些是精确相同。我们发现学生对数据分析重复的预期与实际经验存在差异，这些差异共同证明了学生的批判思维的提高。其次，我们为教师提供了包括 integrate replications into the classroom overhead和replications bring 相比传统任务的担忧。 finally，我们发现在课程中的数据分析重复提供了科学社区的 tangible benefits，如replication reports和对重复过程中的障碍的洞察，这些材料可以为未来的科学工作提供指导。总之，我们的研究表明，在课程中包含数据分析重复任务可以提高科学工作的可重现性，并为学生和科学社区带来利益。
</details></li>
</ul>
<hr>
<h2 id="Latent-Painter"><a href="#Latent-Painter" class="headerlink" title="Latent Painter"></a>Latent Painter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16490">http://arxiv.org/abs/2308.16490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shih-Chieh Su</li>
<li>for: 用于生成创意艺术动画</li>
<li>methods: 使用潜在隐藏的canvas和预测结果作为规划，通过转移一个生成的图像到另一个来实现动画变换</li>
<li>results: 能够生成具有变换性的精细艺术动画<details>
<summary>Abstract</summary>
Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
</details>
<details>
<summary>摘要</summary>
Latent diffusers革新生成AI和艺术创作。当去噪latent时，预测的原图在每步 коллектив卷积动画。然而，动画受到噪音除去器的限制，只能进行锐化处理。这个工作介绍Latent Painter，它使用latent作为画布，并使用预测的diffuser来计划生成画作动画。Latent Painter还可以在一个生成的图像与另一个图像之间进行转换，这可以发生在两个不同的检点集中的图像之间。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Adaptation-for-Point-Cloud-Upsampling-Using-Meta-Learning"><a href="#Test-Time-Adaptation-for-Point-Cloud-Upsampling-Using-Meta-Learning" class="headerlink" title="Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning"></a>Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16484">http://arxiv.org/abs/2308.16484</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Hatem, Yiming Qian, Yang Wang</li>
<li>for: 提高激活点云upsampling的模型通用性</li>
<li>methods: 使用meta-学习来适应测试数据的特点</li>
<li>results: 比对标准基准数据的表现有所提高<details>
<summary>Abstract</summary>
Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for each test instance. The updated model is then used for the final prediction. Our framework is generic and can be applied in a plug-and-play manner with existing backbone networks in point cloud upsampling. Extensive experiments demonstrate that our approach improves the performance of state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
便宜的3D扫描仪通常生成稀疏不均匀的点云，这会负面影响下游应用程序在робо特系统中。而现有的点云upsampling架构在标准测试数据上已经达到了可观的结果，但是它们在测试数据与训练数据之间的分布不同时会经受显著性能下降。为解决这个问题，这篇论文提出了一种测试时适应approach，用于提高点云upsampling模型的通用性。我们的方法不需要任何测试数据的先前信息。在meta-training中，模型参数被学习从一个集合实例级任务中，每个实例包含一对稀疏和密集的点云从训练数据中。在meta-testing中，已经训练过的模型被微调一些梯度更新，以生成每个测试实例唯一的网络参数。更新后的模型然后用于最终预测。我们的框架可以与现有的后缀网络在点云upsampling中进行插件式应用。广泛的实验证明了我们的方法可以提高现有模型的性能。
</details></li>
</ul>
<hr>
<h2 id="Point-TTA-Test-Time-Adaptation-for-Point-Cloud-Registration-Using-Multitask-Meta-Auxiliary-Learning"><a href="#Point-TTA-Test-Time-Adaptation-for-Point-Cloud-Registration-Using-Multitask-Meta-Auxiliary-Learning" class="headerlink" title="Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning"></a>Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16481">http://arxiv.org/abs/2308.16481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Hatem, Yiming Qian, Yang Wang</li>
<li>for: 提高点云注册模型的通用性和性能</li>
<li>methods: 提出了一种基于测试时适应的点云注册框架，通过三个自动适应任务来适应测试数据，并通过meta-依赖学习方法来在测试时进行适应。</li>
<li>results: 实验结果表明，该方法可以提高点云注册模型的通用性和性能，并且超过了其他现有方法的表现。<details>
<summary>Abstract</summary>
We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. During training, our model is trained using a meta-auxiliary learning approach, such that the adapted model via auxiliary tasks improves the accuracy of the primary task. Experimental results demonstrate the effectiveness of our approach in improving generalization of point cloud registration and outperforming other state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
我们介绍Point-TTA，一种新的测试时适应框架 для点云注册（PCR），可以提高注册模型的通用性和性能。学习型方法已经取得了很大的进步，但是在测试环境中普遍存在不同的3D扫描数据，这使得同一个模型在测试时难以处理所有变化。在这篇论文中，我们提议一种测试时适应方法 для PCR。我们的模型可以在测试时适应未看过的分布，无需任何测试数据的先知知识。具体来说，我们设计了三个自动编目任务，这些任务与主要PCR任务一起被优化。给定一个测试实例，我们使用这些自动编目任务适应我们的模型，并使用更新后的模型进行推理。在训练时，我们使用一种元助理学习方法来训练我们的模型，以便适应任务中的更新。实验结果表明，我们的方法可以提高点云注册的通用性和超越其他现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Transformer-Compression-via-Subspace-Projection"><a href="#Transformer-Compression-via-Subspace-Projection" class="headerlink" title="Transformer Compression via Subspace Projection"></a>Transformer Compression via Subspace Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16475">http://arxiv.org/abs/2308.16475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Hu, Jing Zhang, Chen Zhao, Cuiping Li, Hong Chen</li>
<li>for: 压缩 transformer 模型，减少隐藏尺寸</li>
<li>methods: 使用矩阵运算在压缩后的空间中进行 matrix operations</li>
<li>results: 实验结果显示，TCSP 可以实现44%的压缩率，并且精度下降不超过1.6%，超过或匹配先前的压缩方法。同时，TCSP 兼容其他针对筛子和注意头尺寸压缩的方法。<details>
<summary>Abstract</summary>
We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
</details>
<details>
<summary>摘要</summary>
我们提出TCSP方法，一种 novel方法用于压缩transformer模型，主要是降低模型的隐藏大小。我们通过将整个transform模型转映到一个子空间中，使得matrix操作可以进行在模型对于特征的压缩空间中，从而实现了重要的压缩 Parameter和计算资源。为了建立这个子空间，我们将特征矩阵，从不同层次的抽象数据实例中 derivation， decomposed为一个投影矩阵。为了评估，TCSP方法在GLUE和SQuAD评分板上压缩T5和BERT模型，实现了44%的压缩比，并且对应最多1.6%的精度下降，超过或匹配先前的压缩方法。此外，TCSP方法可以与其他对答和注意头大小压缩方法相容。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Subtask-Performance-of-Multi-modal-Large-Language-Model"><a href="#Enhancing-Subtask-Performance-of-Multi-modal-Large-Language-Model" class="headerlink" title="Enhancing Subtask Performance of Multi-modal Large Language Model"></a>Enhancing Subtask Performance of Multi-modal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16474">http://arxiv.org/abs/2308.16474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Zhao, Zhenyu Li, Feng Zhang, Xinhai Xu, Donghong Liu</li>
<li>For: This paper aims to improve the performance of multi-modal large language models (MLLMs) by selecting multiple pre-trained models to complete the same subtask and combining their results to obtain the optimal outcome.* Methods: The proposed approach involves selecting multiple pre-trained models focused on the same subtask based on distinct evaluation approaches, invoking these models in parallel to process input data, and comparing the results from multiple pre-trained models using a large language model (LLM) to choose the best outcome.* Results: The proposed approach is shown to be effective in improving the performance of MLLMs through extensive experiments using GPT-4 annotated datasets and human-annotated datasets, with results from various evaluation metrics demonstrating the approach’s effectiveness.<details>
<summary>Abstract</summary>
Multi-modal Large Language Model (MLLM) refers to a model expanded from a Large Language Model (LLM) that possesses the capability to handle and infer multi-modal data. Current MLLMs typically begin by using LLMs to decompose tasks into multiple subtasks, then employing individual pre-trained models to complete specific subtasks, and ultimately utilizing LLMs to integrate the results of each subtasks to obtain the results of the task. In real-world scenarios, when dealing with large projects, it is common practice to break down the project into smaller sub-projects, with different teams providing corresponding solutions or results. The project owner then decides which solution or result to use, ensuring the best possible outcome for each subtask and, consequently, for the entire project. Inspired by this, this study considers selecting multiple pre-trained models to complete the same subtask. By combining the results from multiple pre-trained models, the optimal subtask result is obtained, enhancing the performance of the MLLM. Specifically, this study first selects multiple pre-trained models focused on the same subtask based on distinct evaluation approaches, and then invokes these models in parallel to process input data and generate corresponding subtask results. Finally, the results from multiple pre-trained models for the same subtask are compared using the LLM, and the best result is chosen as the outcome for that subtask. Extensive experiments are conducted in this study using GPT-4 annotated datasets and human-annotated datasets. The results of various evaluation metrics adequately demonstrate the effectiveness of the proposed approach in this paper.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MaintainoMATE-A-GitHub-App-for-Intelligent-Automation-of-Maintenance-Activities"><a href="#MaintainoMATE-A-GitHub-App-for-Intelligent-Automation-of-Maintenance-Activities" class="headerlink" title="MaintainoMATE: A GitHub App for Intelligent Automation of Maintenance Activities"></a>MaintainoMATE: A GitHub App for Intelligent Automation of Maintenance Activities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16464">http://arxiv.org/abs/2308.16464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas Nadeem, Muhammad Usman Sarwar, Muhammad Zubair Malik</li>
<li>for: 本研究旨在提高软件开发项目中维护任务的效率，特别是自动化issue tracking系统上的issue报告处理。</li>
<li>methods: 本研究使用BERT模型来自动分类issue报告并将其分配给相关的开发者。</li>
<li>results:  experiments show that MaintainoMATE可以达到约80%的F1分数，并且可以将issue报告分配给相关的开发者，其F1分数达54%，与现有方法相当。<details>
<summary>Abstract</summary>
Software development projects rely on issue tracking systems at the core of tracking maintenance tasks such as bug reports, and enhancement requests. Incoming issue-reports on these issue tracking systems must be managed in an effective manner. First, they must be labelled and then assigned to a particular developer with relevant expertise. This handling of issue-reports is critical and requires thorough scanning of the text entered in an issue-report making it a labor-intensive task. In this paper, we present a unified framework called MaintainoMATE, which is capable of automatically categorizing the issue-reports in their respective category and further assigning the issue-reports to a developer with relevant expertise. We use the Bidirectional Encoder Representations from Transformers (BERT), as an underlying model for MaintainoMATE to learn the contextual information for automatic issue-report labeling and assignment tasks. We deploy the framework used in this work as a GitHub application. We empirically evaluate our approach on GitHub issue-reports to show its capability of assigning labels to the issue-reports. We were able to achieve an F1-score close to 80\%, which is comparable to existing state-of-the-art results. Similarly, our initial evaluations show that we can assign relevant developers to the issue-reports with an F1 score of 54\%, which is a significant improvement over existing approaches. Our initial findings suggest that MaintainoMATE has the potential of improving software quality and reducing maintenance costs by accurately automating activities involved in the maintenance processes. Our future work would be directed towards improving the issue-assignment module.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:软件开发项目依赖问题跟踪系统的核心，包括BUG报告和改进请求。接收到issue报告后，需要有效地管理它们。首先，它们需要被标签，然后分配给相关的开发者。这个过程是 kritical 和需要干预的，因为需要从issue报告中提取信息，这是一项劳动密集的任务。在这篇论文中，我们提出了一个统一框架，叫做MaintainoMATE，可以自动将issue报告分类到不同的类别中，并将其分配给相关的开发者。我们使用了BERT模型，作为MaintainoMATE的下一层模型，以学习issue报告中的上下文信息。我们将这个框架部署到GitHub上。我们对GitHub上的issue报告进行了实验，以示其能否将标签分配给issue报告。我们获得了一个F1分数接近80%，与现有的状态艺术结果相似。此外，我们的初步评估表明，我们可以将issue报告分配给相关的开发者，F1分数为54%，与现有方法相比，是一个显著的改进。我们的初步发现表明，MaintainoMATE有可能提高软件质量并降低维护成本，通过准确地自动化维护过程中的活动。我们未来的工作将是改进issue分配模块。
</details></li>
</ul>
<hr>
<h2 id="BioCoder-A-Benchmark-for-Bioinformatics-Code-Generation-with-Contextual-Pragmatic-Knowledge"><a href="#BioCoder-A-Benchmark-for-Bioinformatics-Code-Generation-with-Contextual-Pragmatic-Knowledge" class="headerlink" title="BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge"></a>BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16458">http://arxiv.org/abs/2308.16458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/biocoder">https://github.com/gersteinlab/biocoder</a></li>
<li>paper_authors: Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark Gerstein</li>
<li>for: 本研究开发了一个名为 BioCoder 的库，用于评估现有的预训构模型在生成生物信息学程式码方面的表现。</li>
<li>methods: BioCoder 使用了 GitHub 和 Rosalind Project 上的 Python 和 Java 程式码，以及一个对测试模型的测试框架，以评估模型的表现。</li>
<li>results: 研究发现，为了在生物信息学程式码生成中取得出色的表现，模型需要具备领域知识、实用程式码生成能力和上下文理解能力。<details>
<summary>Abstract</summary>
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, and ChatGPT. Our detailed analysis of these models emphasizes the importance of domain knowledge, pragmatic code generation, and contextual understanding. Our dataset, benchmark, Docker images, and scripts required for testing are all available at https://github.com/gersteinlab/biocoder.
</details>
<details>
<summary>摘要</summary>
Pre-trained语言模型如ChatGPT已经显著改进了代码生成。随着这些模型的扩大，代码生成的输出需要承办更加复杂的任务。在生物信息学中，生成功能程序受到域知识的限制，需要进行复杂的数据操作和函数依赖关系。为此，我们提出了BioCoder，一个用于评估现有预训练模型的代码生成能力的benchmark。在函数代码生成方面，BioCoder覆盖了可能的包依赖、类声明和全局变量。它包含1026个函数和1243个方法在Python和Java中，从GitHub和Rosalind项目中提取来的253个示例。BioCoder包含一个混淆测试框架，我们已经应用到了许多模型中，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT5+和ChatGPT。我们的详细分析表明，域知识、实用代码生成和上下文理解对代码生成的质量具有重要作用。我们的数据集、benchmark、Docker镜像和测试脚本都可以在https://github.com/gersteinlab/biocoder中获取。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Representation-Learning-Based-on-Multiple-Node-centered-Subgraphs"><a href="#Contrastive-Representation-Learning-Based-on-Multiple-Node-centered-Subgraphs" class="headerlink" title="Contrastive Representation Learning Based on Multiple Node-centered Subgraphs"></a>Contrastive Representation Learning Based on Multiple Node-centered Subgraphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16441">http://arxiv.org/abs/2308.16441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dong Li, Wenjun Wang, Minglai Shao, Chen Zhao</li>
<li>for: 学习图表示性能，即使用自我supervised方式学习图节点表示。</li>
<li>methods: 提出了一种多个节点中心子图对比学习方法，通过设计精心的多个节点中心子图来增强节点表示的自适应能力。</li>
<li>results: 在多个实际世界数据集和不同下游任务中，模型已经实现了状态级 результаts。<details>
<summary>Abstract</summary>
As the basic element of graph-structured data, node has been recognized as the main object of study in graph representation learning. A single node intuitively has multiple node-centered subgraphs from the whole graph (e.g., one person in a social network has multiple social circles based on his different relationships). We study this intuition under the framework of graph contrastive learning, and propose a multiple node-centered subgraphs contrastive representation learning method to learn node representation on graphs in a self-supervised way. Specifically, we carefully design a series of node-centered regional subgraphs of the central node. Then, the mutual information between different subgraphs of the same node is maximized by contrastive loss. Experiments on various real-world datasets and different downstream tasks demonstrate that our model has achieved state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
为图structured数据的基本元素，节点已被认为是学习图表示的主要对象。一个单个节点拥有多个基于整个图的节点中心子图（例如，一个社交网络中的一个人有多个基于他不同关系的社交圈）。我们在图矩阵学习框架下研究这一感知，并提出了多个节点中心子图对比学习方法来自顺supervised的学习节点表示。具体来说，我们特别设计了一系列基于中心节点的节点中心子图。然后，通过对不同节点的子图进行对比损失，最大化不同节点之间的共通信息。实验结果表明，我们的模型在实际世界数据集和不同下游任务中均达到了状态率的Result。
</details></li>
</ul>
<hr>
<h2 id="BenchTemp-A-General-Benchmark-for-Evaluating-Temporal-Graph-Neural-Networks"><a href="#BenchTemp-A-General-Benchmark-for-Evaluating-Temporal-Graph-Neural-Networks" class="headerlink" title="BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks"></a>BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16385">http://arxiv.org/abs/2308.16385</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qianghuangwhu/benchtemp">https://github.com/qianghuangwhu/benchtemp</a></li>
<li>paper_authors: Qiang Huang, Jiawei Jiang, Xi Susie Rao, Ce Zhang, Zhichao Han, Zitao Zhang, Xin Wang, Yongjun He, Quanqing Xu, Yang Zhao, Chuang Hu, Shuo Shang, Bo Du</li>
<li>for: 评估Temporal Graph Neural Networks (TGNNs)的性能，提供一个通用的评估平台。</li>
<li>methods: 使用BenchTemp benchmark suite，包括各种任务和设置，对TGNN模型进行比较。</li>
<li>results: 对多种代表性TGNN模型进行了广泛的比较，包括效果率和效率两个指标。<details>
<summary>Abstract</summary>
To handle graphs in which features or connectivities are evolving over time, a series of temporal graph neural networks (TGNNs) have been proposed. Despite the success of these TGNNs, the previous TGNN evaluations reveal several limitations regarding four critical issues: 1) inconsistent datasets, 2) inconsistent evaluation pipelines, 3) lacking workload diversity, and 4) lacking efficient comparison. Overall, there lacks an empirical study that puts TGNN models onto the same ground and compares them comprehensively. To this end, we propose BenchTemp, a general benchmark for evaluating TGNN models on various workloads. BenchTemp provides a set of benchmark datasets so that different TGNN models can be fairly compared. Further, BenchTemp engineers a standard pipeline that unifies the TGNN evaluation. With BenchTemp, we extensively compare the representative TGNN models on different tasks (e.g., link prediction and node classification) and settings (transductive and inductive), w.r.t. both effectiveness and efficiency metrics. We have made BenchTemp publicly available at https://github.com/qianghuangwhu/benchtemp.
</details>
<details>
<summary>摘要</summary>
为了处理时间演化的图像，一系列的时间图神经网络（TGNN）已经被提议。尽管这些TGNN模型具有成功的表现，但之前的TGNN评价显示了四个关键问题的局限性：1）不一致的数据集，2）不一致的评价流水线，3）缺乏工作负荷多样性，4）缺乏高效的比较。总的来说，没有一个实证研究可以将TGNN模型放在一起，并对其进行全面的比较。为此，我们提出了BenchTemp，一个通用的benchmark用于评价TGNN模型的多种工作负荷。BenchTemp提供了一组benchmark数据集，以便不同的TGNN模型可以公平地比较。此外，BenchTemp还设计了一个标准的评价流水线，以确保TGNN模型在不同任务（如链接预测和节点分类）和设置（推uctive和induction）下进行公平的评价。通过BenchTemp，我们对不同的TGNN模型进行了广泛的比较，并对其效果和效率指标进行了评价。BenchTemp已经在https://github.com/qianghuangwhu/benchtemp上公开发布。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Privacy-in-Graph-Neural-Networks-Attacks-Preservation-and-Applications"><a href="#A-Survey-on-Privacy-in-Graph-Neural-Networks-Attacks-Preservation-and-Applications" class="headerlink" title="A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications"></a>A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16375">http://arxiv.org/abs/2308.16375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Zhang, Yuying Zhao, Zhaoqing Li, Xueqi Cheng, Yu Wang, Olivera Kotevska, Philip S. Yu, Tyler Derr</li>
<li>For: The paper aims to provide a comprehensive overview of attacks on graph data and privacy preservation techniques in graph neural networks (GNNs).* Methods: The paper categorizes privacy preservation techniques in GNNs and reviews datasets and applications for analyzing&#x2F;solving privacy issues in GNNs.* Results: The paper outlines potential directions for future research to build better privacy-preserving GNNs.Here’s the Chinese version of the three key points:* For: 论文旨在提供图数据的攻击和图神经网络（GNNs）中的隐私保护技术的全面回顾。* Methods: 论文对GNNs中的隐私保护技术进行分类，并评估图数据分析&#x2F;解决隐私问题的数据和应用程序。* Results: 论文提出未来研究的可能方向，以建立更好的隐私保护GNNs。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 在处理图structured数据方面已经吸引了广泛的注意力，但是许多这些模型强调高性能，如准确率，而忽略了隐私考虑，这在现代社会中是一个重要的问题，因为隐私攻击是普遍的。为解决这个问题，研究人员开始了隐私保护GNNs的开发。 despite this progress， there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.Here's the translation in Traditional Chinese:graph neural networks (GNNs) 在处理图structured数据方面已经吸引了广泛的注意力，但是许多这些模型强调高性能，如准确率，而忽略了隐私考虑，这在现代社会中是一个重要的问题，因为隐私攻击是普遍的。为解决这个问题，研究人员开始了隐私保护GNNs的开发。 despite this progress， there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.AI_2023_08_31/" data-id="clot2mh8b003bx7889f741lsj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.CL_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T11:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.CL_2023_08_31/">cs.CL - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TouchStone-Evaluating-Vision-Language-Models-by-Language-Models"><a href="#TouchStone-Evaluating-Vision-Language-Models-by-Language-Models" class="headerlink" title="TouchStone: Evaluating Vision-Language Models by Language Models"></a>TouchStone: Evaluating Vision-Language Models by Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16890">http://arxiv.org/abs/2308.16890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, Jingren Zhou</li>
<li>for: 评估大视觉语言模型（LVLMs）的多种能力，包括认知、理解和处理视觉信息，以及对话技巧和文学创作能力。</li>
<li>methods: 使用强大的语言模型（LLMs）作为评判者，对LVLMs的多种能力进行全面评估，包括开放世界图像和问题，涵盖五大类能力和27个子任务。</li>
<li>results: 通过验证，表明强大的LVLMs，如GPT-4，可以通过文本能力alone评估多modal对话质量，与人类偏好相align。<details>
<summary>Abstract</summary>
Large vision-language models (LVLMs) have recently witnessed rapid advancements, exhibiting a remarkable capacity for perceiving, understanding, and processing visual information by connecting visual receptor with large language models (LLMs). However, current assessments mainly focus on recognizing and reasoning abilities, lacking direct evaluation of conversational skills and neglecting visual storytelling abilities. In this paper, we propose an evaluation method that uses strong LLMs as judges to comprehensively evaluate the various abilities of LVLMs. Firstly, we construct a comprehensive visual dialogue dataset TouchStone, consisting of open-world images and questions, covering five major categories of abilities and 27 subtasks. This dataset not only covers fundamental recognition and comprehension but also extends to literary creation. Secondly, by integrating detailed image annotations we effectively transform the multimodal input content into a form understandable by LLMs. This enables us to employ advanced LLMs for directly evaluating the quality of the multimodal dialogue without requiring human intervention. Through validation, we demonstrate that powerful LVLMs, such as GPT-4, can effectively score dialogue quality by leveraging their textual capabilities alone, aligning with human preferences. We hope our work can serve as a touchstone for LVLMs' evaluation and pave the way for building stronger LVLMs. The evaluation code is available at https://github.com/OFA-Sys/TouchStone.
</details>
<details>
<summary>摘要</summary>
首先，我们建立了一个完整的视觉对话 dataset TouchStone，包括开放世界的图片和问题，涵盖五大类能力和 27 个子任务。这个 dataset 不仅覆盖基本的识别和理解，也扩展到文学创作。其次，通过将复杂的视觉内容转换为可以由 LLMs 理解的形式，我们可以直接使用高级 LLMs 评估多模式对话质量，不需要人工干预。经过验证，我们展示了强大的 LVLMs，如 GPT-4，可以通过它们的文本能力 alone 评估对话质量，与人类偏好相Alignment。我们希望这个工作可以成为 LVLMs 评估的 touchstone，导向建立更强大的 LVLMs。评估代码可以在 https://github.com/OFA-Sys/TouchStone 上获取。
</details></li>
</ul>
<hr>
<h2 id="Simple-LLM-Prompting-is-State-of-the-Art-for-Robust-and-Multilingual-Dialogue-Evaluation"><a href="#Simple-LLM-Prompting-is-State-of-the-Art-for-Robust-and-Multilingual-Dialogue-Evaluation" class="headerlink" title="Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation"></a>Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16797">http://arxiv.org/abs/2308.16797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/johndmendonca/dialevalml">https://github.com/johndmendonca/dialevalml</a></li>
<li>paper_authors: John Mendonça, Patrícia Pereira, João Paulo Carvalho, Alon Lavie, Isabel Trancoso</li>
<li>for: 这 paper 是为了开发一个可以评估多语言对话系统的自动对话评估指标的框架。</li>
<li>methods: 这 paper 使用了现有评估模型的优势，同时采用了新的大语言模型（LLM）提问 paradigm。</li>
<li>results: 这 paper 的实验结果表明，其框架在多个 benchmark 上的 Mean Spearman correlation 分数均达到了state of the art Water mark，并在 DSTC11 Track 4 “Automatic Evaluation Metrics for Open-Domain Dialogue Systems” 的 Robust 和 Multilingual 任务中排名第一。<details>
<summary>Abstract</summary>
Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain Dialogue Systems", proving the evaluation capabilities of prompted LLMs.
</details>
<details>
<summary>摘要</summary>
尽管有很多研究努力在自动对话评价指标的发展中，但对于其他语言的对话评价却得到了少量的关注。同时，确保评价结果对Semantically相同的答案具有不变性也是一个被忽略的话题。为了实现自动对话评价指标的稳定性和多语言性，我们提出了一种新的框架，利用现有评价模型的优势以及新的大语言模型（LLM）的推荐 paradigm。实验结果显示，我们的框架在多个 benchmark 上取得了 state of the art 的 Mean Spearman correlation 分数，并在 DSTC11 Track 4 "Automatic Evaluation Metrics for Open-Domain Dialogue Systems" 的 Robust 和 Multilingual 任务上取得了第一名，证明了提高 LLM 的评价能力。
</details></li>
</ul>
<hr>
<h2 id="Towards-Multilingual-Automatic-Dialogue-Evaluation"><a href="#Towards-Multilingual-Automatic-Dialogue-Evaluation" class="headerlink" title="Towards Multilingual Automatic Dialogue Evaluation"></a>Towards Multilingual Automatic Dialogue Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16795">http://arxiv.org/abs/2308.16795</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Mendonça, Alon Lavie, Isabel Trancoso</li>
<li>for: 这篇论文主要针对的问题是开发robust的多语言对话评估指标的主要限制因素是多语言数据的缺乏和开源多语言对话系统的有限可用性。</li>
<li>methods: 作者提议一种绕过这些限制的方法是利用强大的多语言预训练自然语言处理模型，并使用机器翻译将英语对话数据扩展到多语言数据。</li>
<li>results: 作者经验表明，直接使用翻译后的数据进行训练是不足以超越基线的多语言模型，而需要仔细筛选翻译后的数据使用MT质量评估 metric，以避免低质量翻译对性能的影响。<details>
<summary>Abstract</summary>
The main limiting factor in the development of robust multilingual dialogue evaluation metrics is the lack of multilingual data and the limited availability of open sourced multilingual dialogue systems. In this work, we propose a workaround for this lack of data by leveraging a strong multilingual pretrained LLM and augmenting existing English dialogue data using Machine Translation. We empirically show that the naive approach of finetuning a pretrained multilingual encoder model with translated data is insufficient to outperform the strong baseline of finetuning a multilingual model with only source data. Instead, the best approach consists in the careful curation of translated data using MT Quality Estimation metrics, excluding low quality translations that hinder its performance.
</details>
<details>
<summary>摘要</summary>
主要的限制因素是多语言对话评估指标的发展缺乏多语言数据和开源的多语言对话系统的有限可用性。在这种工作中，我们提出了一种绕过这种缺乏数据的 workaround，利用强大的多语言预训练深度学习模型，并通过机器翻译来扩展现有的英语对话数据。我们经验显示，直接使用翻译后的数据进行训练是不够的，而是需要仔细筛选翻译后的数据使用MT质量评估指标，排除低质量翻译，以保证其表现。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-PLM-Performance-on-Labour-Market-Tasks-via-Instruction-based-Finetuning-and-Prompt-tuning-with-Rules"><a href="#Enhancing-PLM-Performance-on-Labour-Market-Tasks-via-Instruction-based-Finetuning-and-Prompt-tuning-with-Rules" class="headerlink" title="Enhancing PLM Performance on Labour Market Tasks via Instruction-based Finetuning and Prompt-tuning with Rules"></a>Enhancing PLM Performance on Labour Market Tasks via Instruction-based Finetuning and Prompt-tuning with Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16770">http://arxiv.org/abs/2308.16770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jarno Vrolijk, David Graus</li>
<li>for: 本研究旨在探讨如何使用预训练语言模型（PLM）在劳动市场特定应用中提高表示性。</li>
<li>methods: 本研究使用了提示基于调整和 instrucion tuning 方法，无需 exemplars 和数据增强，可以在劳动市场特定应用中提高 PLM 的表现。</li>
<li>results: 研究结果表明，使用提示基于调整和 instrucion tuning 方法可以在劳动市场特定应用中提高 PLM 的表现，而无需添加新的模型层、手动标注和数据增强。<details>
<summary>Abstract</summary>
The increased digitization of the labour market has given researchers, educators, and companies the means to analyze and better understand the labour market. However, labour market resources, although available in high volumes, tend to be unstructured, and as such, research towards methodologies for the identification, linking, and extraction of entities becomes more and more important. Against the backdrop of this quest for better labour market representations, resource constraints and the unavailability of large-scale annotated data cause a reliance on human domain experts. We demonstrate the effectiveness of prompt-based tuning of pre-trained language models (PLM) in labour market specific applications. Our results indicate that cost-efficient methods such as PTR and instruction tuning without exemplars can significantly increase the performance of PLMs on downstream labour market applications without introducing additional model layers, manual annotations, and data augmentation.
</details>
<details>
<summary>摘要</summary>
随着劳动市场的数字化，研究者、教育者和公司得到了分析和更好地理解劳动市场的工具。然而，劳动市场资源，即使在大量存在，通常是不结构化的，因此对方法ologies for the identification, linking, and extraction of entities的研究变得越来越重要。在这种寻求更好的劳动市场表示方面，因为资源受限和大规模annotated data的不可得性，人际域专家的依赖度增加。我们示示了适用Prompt-based tuning的pre-trained语言模型（PLM）在劳动市场特定应用中的效果。我们的结果表明，不需要添加更多的模型层、手动标注和数据扩展的cost-efficient方法，如PTR和instruction tuning without exemplars，可以大幅提高PLMs在下游劳动市场应用中的性能。
</details></li>
</ul>
<hr>
<h2 id="SpeechTokenizer-Unified-Speech-Tokenizer-for-Speech-Large-Language-Models"><a href="#SpeechTokenizer-Unified-Speech-Tokenizer-for-Speech-Large-Language-Models" class="headerlink" title="SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models"></a>SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16692">http://arxiv.org/abs/2308.16692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu</li>
<li>for: 这paper aimed to evaluate the suitability of existing speech tokens for speech language modeling and to propose a unified speech tokenizer for speech large language models.</li>
<li>methods: The paper proposed a unified speech tokenizer called SpeechTokenizer, which adopts the Encoder-Decoder architecture with residual vector quantization (RVQ).</li>
<li>results: The SpeechTokenizer performed comparably to EnCodec in speech reconstruction and demonstrated strong performance on the SLMTokBench benchmark. Additionally, the Unified Speech Language Model (USLM) outperformed VALL-E in zero-shot Text-to-Speech tasks.<details>
<summary>Abstract</summary>
Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.
</details>
<details>
<summary>摘要</summary>
当前的大语言模型建立于不连续的语音表示方式上，可以分为 semantics 和 acoustic 两类。然而，现有的语音表示token并不是专门为语音语言模型设计的。为了评估语音表示token的适用程度，我们建立了首个benchmarkSLMTokBench。我们的结果表明， neither semantic  noch acoustic tokens 是理想的。因此，我们提出了 SpeechTokenizer，一种通用的语音tokenizer для语音大语言模型。SpeechTokenizer采用了 Encoder-Decoder 架构和剩余 вектор量化（RVQ）。在不同的RVQ层中，SpeechTokenizer层次分解不同的语音信息。此外，我们构建了 Unified Speech Language Model (USLM)，利用 SpeechTokenizer。实验表明，SpeechTokenizer与EnCodec相当在语音重建任务中，并在SLMTokBench标准差中表现出色。此外，USLM在零基本Text-to-Speech任务中表现出优于 VALL-E。代码和模型可以在https://github.com/ZhangXInFD/SpeechTokenizer/上获取。
</details></li>
</ul>
<hr>
<h2 id="DictaBERT-A-State-of-the-Art-BERT-Suite-for-Modern-Hebrew"><a href="#DictaBERT-A-State-of-the-Art-BERT-Suite-for-Modern-Hebrew" class="headerlink" title="DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew"></a>DictaBERT: A State-of-the-Art BERT Suite for Modern Hebrew</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16687">http://arxiv.org/abs/2308.16687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaltiel Shmidman, Avi Shmidman, Moshe Koppel</li>
<li>for: 这个论文是为了提出一个新的现代希伯来BERT模型，以及两个特定任务的两个精度版本：prefix segmentation和 morphological tagging。</li>
<li>methods: 这个论文使用了BERT模型，并在其基础之上进行了特定任务的精度版本。</li>
<li>results: 论文表明了这些模型在不同的标准测试数据上的表现，并释放了这些模型以便进一步的研究和开发。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
We present DictaBERT, a new state-of-the-art pre-trained BERT model for modern Hebrew, outperforming existing models on most benchmarks. Additionally, we release two fine-tuned versions of the model, designed to perform two specific foundational tasks in the analysis of Hebrew texts: prefix segmentation and morphological tagging. These fine-tuned models allow any developer to perform prefix segmentation and morphological tagging of a Hebrew sentence with a single call to a HuggingFace model, without the need to integrate any additional libraries or code. In this paper we describe the details of the training as well and the results on the different benchmarks. We release the models to the community, along with sample code demonstrating their use. We release these models as part of our goal to help further research and development in Hebrew NLP.
</details>
<details>
<summary>摘要</summary>
我们介绍DictaBERT，一个新的现代希伯来预训练BERT模型，在大多数标准准则上超越现有模型。此外，我们释放了两个精度调整版本的模型，用于执行希伯来文本分析中两个基本任务：前缀分 segmentation和 morphological tagging。这两个精度调整版本使得任何开发者可以通过一个HuggingFace模型的单调用来完成希伯来句子的前缀分 segmentation和 morphological tagging，无需额外的库或代码集成。在这篇文章中，我们详细描述了训练细节以及不同的标准准则的结果。我们将这些模型公开发布，并附送示例代码以示其使用。我们发布这些模型，以帮助进一步推动希伯来自然语言处理的研究和开发。
</details></li>
</ul>
<hr>
<h2 id="Towards-Spontaneous-Style-Modeling-with-Semi-supervised-Pre-training-for-Conversational-Text-to-Speech-Synthesis"><a href="#Towards-Spontaneous-Style-Modeling-with-Semi-supervised-Pre-training-for-Conversational-Text-to-Speech-Synthesis" class="headerlink" title="Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis"></a>Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16593">http://arxiv.org/abs/2308.16593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiqin Li, Shun Lei, Qiaochu Huang, Yixuan Zhou, Zhiyong Wu, Shiyin Kang, Helen Meng</li>
<li>for: 提高自然语言对话中的启发行为标注数据和自然语言对话中的表达质量</li>
<li>methods: 使用半监督预训练方法，同时考虑文本和语音信息，以检测对话中的启发行为标注</li>
<li>results: 实验结果表明，提posed方法可以实现高质量的自然语言对话synthesis，同时能够模型对话中的启发行为和预测对话中的自然语言表达<details>
<summary>Abstract</summary>
The spontaneous behavior that often occurs in conversations makes speech more human-like compared to reading-style. However, synthesizing spontaneous-style speech is challenging due to the lack of high-quality spontaneous datasets and the high cost of labeling spontaneous behavior. In this paper, we propose a semi-supervised pre-training method to increase the amount of spontaneous-style speech and spontaneous behavioral labels. In the process of semi-supervised learning, both text and speech information are considered for detecting spontaneous behaviors labels in speech. Moreover, a linguistic-aware encoder is used to model the relationship between each sentence in the conversation. Experimental results indicate that our proposed method achieves superior expressive speech synthesis performance with the ability to model spontaneous behavior in spontaneous-style speech and predict reasonable spontaneous behavior from text.
</details>
<details>
<summary>摘要</summary>
人们在对话中的自发行为通常使得语音更加人类化，然而 sintesizing自发样式语音具有高质量数据和标注自发行为的高成本。在这篇论文中，我们提出了一种半supervised预训练方法，以增加自发样式语音和自发行为标签。在半supervised学习中，我们考虑了文本和语音信息，以检测speech中的自发行为标签。此外，我们使用语言意识encoder来模型对话中每句话之间的关系。实验结果表明，我们的提议方法可以实现高水平的表达语音合成性能，同时能够模型自发样式语音中的自发行为和从文本中预测合理的自发行为。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-Sentiment-Composition-with-Latent-Semantic-Tree"><a href="#Interpreting-Sentiment-Composition-with-Latent-Semantic-Tree" class="headerlink" title="Interpreting Sentiment Composition with Latent Semantic Tree"></a>Interpreting Sentiment Composition with Latent Semantic Tree</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16588">http://arxiv.org/abs/2308.16588</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/changmenseng/semantic_tree">https://github.com/changmenseng/semantic_tree</a></li>
<li>paper_authors: Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jiansong Chen, Jun Zhao, Kang Liu</li>
<li>for: 这篇论文是为了提出一种新的 sentiment composition 方法，以解决传统的 hierarchical trees 存在偏度和难以理解的问题。</li>
<li>methods: 该方法使用 semantic tree，一种基于 context-free grammar (CFG) 的新树形式，来理解 sentiment composition 的原则性。 semantic tree 是一个 latent variable，通过 inside algorithm 进行抽象，以提高分类性能。</li>
<li>results: 该方法在常规和领域适应分类任务中 achieves 更好或竞争性的结果，同时也可以生成合理的树解释。<details>
<summary>Abstract</summary>
As the key to sentiment analysis, sentiment composition considers the classification of a constituent via classifications of its contained sub-constituents and rules operated on them. Such compositionality has been widely studied previously in the form of hierarchical trees including untagged and sentiment ones, which are intrinsically suboptimal in our view. To address this, we propose semantic tree, a new tree form capable of interpreting the sentiment composition in a principled way. Semantic tree is a derivation of a context-free grammar (CFG) describing the specific composition rules on difference semantic roles, which is designed carefully following previous linguistic conclusions. However, semantic tree is a latent variable since there is no its annotation in regular datasets. Thus, in our method, it is marginalized out via inside algorithm and learned to optimize the classification performance. Quantitative and qualitative results demonstrate that our method not only achieves better or competitive results compared to baselines in the setting of regular and domain adaptation classification, and also generates plausible tree explanations.
</details>
<details>
<summary>摘要</summary>
As the key to sentiment analysis, sentiment composition considers the classification of a constituent via classifications of its contained sub-constituents and rules operated on them. Such compositionality has been widely studied previously in the form of hierarchical trees including untagged and sentiment ones, which are intrinsically suboptimal in our view. To address this, we propose semantic tree, a new tree form capable of interpreting the sentiment composition in a principled way. Semantic tree is a derivation of a context-free grammar (CFG) describing the specific composition rules on difference semantic roles, which is designed carefully following previous linguistic conclusions. However, semantic tree is a latent variable since there is no its annotation in regular datasets. Thus, in our method, it is marginalized out via inside algorithm and learned to optimize the classification performance. Quantitative and qualitative results demonstrate that our method not only achieves better or competitive results compared to baselines in the setting of regular and domain adaptation classification, and also generates plausible tree explanations.Here's the translation in Traditional Chinese:作为情感分析的关键，情感组合考虑 класифіcation的构成单元 через其包含的子单元的类别和运算之规则。这种结构已经在过去广泛研究过，通常用树结构，包括未标的树和情感树，这些树结构是我们看来不理想的。为了解决这个问题，我们提出了含义树，一种新的树形式，可以在原理上解释情感组合。含义树是基于特定的语言结构（CFG），描述了不同Semantic Role的特定composing规则，这是以前的语言结论为基础设计的。然而，含义树是一个隐藏变量，因为没有它的标注在常规dataset中。因此，在我们的方法中，它是通过内部算法和学习来抑制标注的。结果显示，我们的方法不仅在常规和预设类别的设定下实现了更好或竞争性的结果，还可以生成合理的树解释。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Text-Style-Transfer-with-Deep-Generative-Models"><a href="#Unsupervised-Text-Style-Transfer-with-Deep-Generative-Models" class="headerlink" title="Unsupervised Text Style Transfer with Deep Generative Models"></a>Unsupervised Text Style Transfer with Deep Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16584">http://arxiv.org/abs/2308.16584</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Zhongtao Jiang, Yuanzhe Zhang, Yiming Ju, Kang Liu</li>
<li>for: 该论文提出了一种总结式文本风格转移框架，用于无监督地将文本风格转换为另一种风格。</li>
<li>methods: 该框架基于深度生成模型，对每个句子-标签对进行模型化，并利用数据中的依赖关系学习句子的内容和风格代码。</li>
<li>results: 该方法在三个标准评测 benchmark 上进行了实验，自动和人工评估结果都显示了与多个强基eline相比的更好或竞争的效果。<details>
<summary>Abstract</summary>
We present a general framework for unsupervised text style transfer with deep generative models. The framework models each sentence-label pair in the non-parallel corpus as partially observed from a complete quadruplet which additionally contains two latent codes representing the content and style, respectively. These codes are learned by exploiting dependencies inside the observed data. Then a sentence is transferred by manipulating them. Our framework is able to unify previous embedding and prototype methods as two special forms. It also provides a principled perspective to explain previously proposed techniques in the field such as aligned encoder and adversarial training. We further conduct experiments on three benchmarks. Both automatic and human evaluation results show that our methods achieve better or competitive results compared to several strong baselines.
</details>
<details>
<summary>摘要</summary>
我们提出了一种总体框架，用于无监督文本风格传输with deep生成模型。这个框架每个句子-标签对在非平行 corpus 中被视为部分观察到的完整四元组，其中包括两个隐藏代码，表示内容和风格。这些代码通过利用观察数据中的依赖关系学习。然后，一个句子可以通过操作这些代码进行传输。我们的框架可以将之前的嵌入和原型方法视为两种特殊形式，并提供了一个理性的视角来解释过去的相关技术，如对齐编码器和对抗训练。我们进一步进行了三个标准测试。自动和人工评估结果都显示，我们的方法可以与一些强大基eline相比，或者达到相同的结果。
</details></li>
</ul>
<hr>
<h2 id="Improving-Mandarin-Prosodic-Structure-Prediction-with-Multi-level-Contextual-Information"><a href="#Improving-Mandarin-Prosodic-Structure-Prediction-with-Multi-level-Contextual-Information" class="headerlink" title="Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information"></a>Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16577">http://arxiv.org/abs/2308.16577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Chen, Changhe Song, Deyi Tuo, Xixin Wu, Shiyin Kang, Zhiyong Wu, Helen Meng</li>
<li>for: 提高文本到语音合成中的自然性和 inteligibilty</li>
<li>methods: 使用多级上下文信息（包括 между话语语言信息和当前话语语言信息），使用多任务学习（MTL）解决方案预测语音结构</li>
<li>results: 对两个数据集进行了对jective评估，获得了更高的F1分数，并且在主观 preference测试中也表明了合成语音的自然性得到了改进。<details>
<summary>Abstract</summary>
For text-to-speech (TTS) synthesis, prosodic structure prediction (PSP) plays an important role in producing natural and intelligible speech. Although inter-utterance linguistic information can influence the speech interpretation of the target utterance, previous works on PSP mainly focus on utilizing intrautterance linguistic information of the current utterance only. This work proposes to use inter-utterance linguistic information to improve the performance of PSP. Multi-level contextual information, which includes both inter-utterance and intrautterance linguistic information, is extracted by a hierarchical encoder from character level, utterance level and discourse level of the input text. Then a multi-task learning (MTL) decoder predicts prosodic boundaries from multi-level contextual information. Objective evaluation results on two datasets show that our method achieves better F1 scores in predicting prosodic word (PW), prosodic phrase (PPH) and intonational phrase (IPH). It demonstrates the effectiveness of using multi-level contextual information for PSP. Subjective preference tests also indicate the naturalness of synthesized speeches are improved.
</details>
<details>
<summary>摘要</summary>
To achieve this, a hierarchical encoder extracts multi-level contextual information from the input text, including character level, utterance level, and discourse level. Then, a multi-task learning (MTL) decoder predicts prosodic boundaries based on the multi-level contextual information.Experimental results on two datasets show that our method outperforms previous methods in predicting prosodic word (PW), prosodic phrase (PPH), and intonational phrase (IPH) with higher F1 scores. Subjective preference tests also indicate that the synthesized speeches produced by our method are more natural-sounding.This work demonstrates the effectiveness of using multi-level contextual information for PSP, and has important implications for improving the naturalness and intelligibility of TTS synthesis.
</details></li>
</ul>
<hr>
<h2 id="Thesis-Distillation-Investigating-The-Impact-of-Bias-in-NLP-Models-on-Hate-Speech-Detection"><a href="#Thesis-Distillation-Investigating-The-Impact-of-Bias-in-NLP-Models-on-Hate-Speech-Detection" class="headerlink" title="Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate Speech Detection"></a>Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16549">http://arxiv.org/abs/2308.16549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatma Elsafoury</li>
<li>for: 本研究探讨了NLP模型中偏见的影响，具体来说是从三个角度：解释性、偏见刻板印象和公平性。</li>
<li>methods: 本研究使用了NLP模型的解释性、偏见刻板印象和公平性来探讨偏见的影响。</li>
<li>results: 研究发现，NLP模型中的偏见从三个角度都会影响 hate speech 检测任务，而且不integrating social sciences在研究偏见的NLP模型中，我们无法有效地解决偏见的问题。<details>
<summary>Abstract</summary>
This paper is a summary of the work in my PhD thesis. In which, I investigate the impact of bias in NLP models on the task of hate speech detection from three perspectives: explainability, offensive stereotyping bias, and fairness. I discuss the main takeaways from my thesis and how they can benefit the broader NLP community. Finally, I discuss important future research directions. The findings of my thesis suggest that bias in NLP models impacts the task of hate speech detection from all three perspectives. And that unless we start incorporating social sciences in studying bias in NLP models, we will not effectively overcome the current limitations of measuring and mitigating bias in NLP models.
</details>
<details>
<summary>摘要</summary>
这份论文是我博士论文的摘要，其中我 investigate了NLP模型中偏见的影响在仇视言语检测任务中，从三个角度：可解性、偏见刻板印象和公平。我讲述了我的博士论文的主要答案和如何对整个NLP社区有益。最后，我讲述了未来研究的重要方向。我的论文发现，NLP模型中的偏见会影响仇视言语检测任务从三个角度，而且如果我们不开始在研究NLP模型中的偏见时，我们无法有效地解决NLP模型中的偏见问题。Here's a word-for-word translation:这份论文是我博士论文的摘要，其中我 investigate了NLP模型中偏见的影响在仇视言语检测任务中，从三个角度：可解性、偏见刻板印象和公平。我讲述了我的博士论文的主要答案和如何对整个NLP社区有益。最后，我讲述了未来研究的重要方向。我的论文发现，NLP模型中的偏见会影响仇视言语检测任务从三个角度，而且如果我们不开始在研究NLP模型中的偏见时，我们无法有效地解决NLP模型中的偏见问题。
</details></li>
</ul>
<hr>
<h2 id="Time-Varying-Quasi-Closed-Phase-Analysis-for-Accurate-Formant-Tracking-in-Speech-Signals"><a href="#Time-Varying-Quasi-Closed-Phase-Analysis-for-Accurate-Formant-Tracking-in-Speech-Signals" class="headerlink" title="Time-Varying Quasi-Closed-Phase Analysis for Accurate Formant Tracking in Speech Signals"></a>Time-Varying Quasi-Closed-Phase Analysis for Accurate Formant Tracking in Speech Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16540">http://arxiv.org/abs/2308.16540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhananjaya Gowda, Sudarsana Reddy Kadiri, Brad Story, Paavo Alku</li>
<li>for: 这篇论文提出了一种新的准确地估计和跟踪speech信号中的声门形态使用时间变化 quasi-closed-phase (TVQCP)分析方法。</li>
<li>methods: 该方法 combinesthree approaches to improve formant estimation and tracking: (1) it uses temporally weighted quasi-closed-phase analysis to derive closed-phase estimates of the vocal tract with reduced interference from the excitation source, (2) it increases the residual sparsity by using the $L_1$ optimization, and (3) it uses time-varying linear prediction analysis over long time windows to impose a continuity constraint on the vocal tract model and hence on the formant trajectories.</li>
<li>results: 对于各种合成和自然语音信号的实验表明，提出的TVQCP方法比传统和流行的formant tracking工具，如Wavesurfer和Praat（基于动态规划）、KARMA算法（基于加尔曼滤波）和DeepFormants（基于深度神经网络）perform better。<details>
<summary>Abstract</summary>
In this paper, we propose a new method for the accurate estimation and tracking of formants in speech signals using time-varying quasi-closed-phase (TVQCP) analysis. Conventional formant tracking methods typically adopt a two-stage estimate-and-track strategy wherein an initial set of formant candidates are estimated using short-time analysis (e.g., 10--50 ms), followed by a tracking stage based on dynamic programming or a linear state-space model. One of the main disadvantages of these approaches is that the tracking stage, however good it may be, cannot improve upon the formant estimation accuracy of the first stage. The proposed TVQCP method provides a single-stage formant tracking that combines the estimation and tracking stages into one. TVQCP analysis combines three approaches to improve formant estimation and tracking: (1) it uses temporally weighted quasi-closed-phase analysis to derive closed-phase estimates of the vocal tract with reduced interference from the excitation source, (2) it increases the residual sparsity by using the $L_1$ optimization and (3) it uses time-varying linear prediction analysis over long time windows (e.g., 100--200 ms) to impose a continuity constraint on the vocal tract model and hence on the formant trajectories. Formant tracking experiments with a wide variety of synthetic and natural speech signals show that the proposed TVQCP method performs better than conventional and popular formant tracking tools, such as Wavesurfer and Praat (based on dynamic programming), the KARMA algorithm (based on Kalman filtering), and DeepFormants (based on deep neural networks trained in a supervised manner). Matlab scripts for the proposed method can be found at: https://github.com/njaygowda/ftrack
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法用于准确地计算和跟踪语音信号中的声门特征（formant）。传统的声门跟踪方法通常采用两个阶段的估计和跟踪策略，其中首先使用短时间分析（例如10-50ms）来估计初始声门候选者，然后使用动态计划或线性状态空间模型来跟踪。这种方法的主要缺点是跟踪阶段，即使非常好，也无法改善初始估计阶段的声门估计精度。我们的提议的TVQCP方法则提供了一种单阶段的声门跟踪，其中估计和跟踪阶段被结合到一起。TVQCP分析结合了三种方法来提高声门估计和跟踪精度：1. 使用时间权重 quasi-closed-phase 分析来 deriv closed-phase 估计值，减少干扰来自激发源的干扰。2. 使用 $L_1$ 优化增加剩余稀热性。3. 使用时间变化的线性预测分析在长时间窗口（例如100-200ms）来强制施加 vocals tract 模型中的连续性约束。我们在各种 sintetic 和自然语音信号上进行了声门跟踪实验，结果显示，我们的TVQCP方法在与传统和流行的声门跟踪工具（如Wavesurfer和Praat）进行比较时，表现出了更高的精度。Matlab 脚本 для我们的方法可以在以下 GitHub 地址找到：https://github.com/njaygowda/ftrack。
</details></li>
</ul>
<hr>
<h2 id="The-Smart-Data-Extractor-a-Clinician-Friendly-Solution-to-Accelerate-and-Improve-the-Data-Collection-During-Clinical-Trials"><a href="#The-Smart-Data-Extractor-a-Clinician-Friendly-Solution-to-Accelerate-and-Improve-the-Data-Collection-During-Clinical-Trials" class="headerlink" title="The Smart Data Extractor, a Clinician Friendly Solution to Accelerate and Improve the Data Collection During Clinical Trials"></a>The Smart Data Extractor, a Clinician Friendly Solution to Accelerate and Improve the Data Collection During Clinical Trials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16537">http://arxiv.org/abs/2308.16537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie Quennelle, Maxime Douillet, Lisa Friedlander, Olivia Boyer, Anita Burgun, Antoine Neuraz, Nicolas Garcelon</li>
<li>for: 提高医疗数据收集效率和质量，避免人工劳动和错误</li>
<li>methods: 提出一种半自动的数据收集系统，能够自动提取各种数据，包括病历记录</li>
<li>results: 对比手动和半自动数据收集方法，发现半自动方法的平均时间为3’22’’，比手动方法要快，并且错误数量较少（46个整个减少到163个），提供一种容易使用、易于理解和快速的便携式临床研究表单填写解决方案，提高数据收集效率和质量，避免人工劳动和错误<details>
<summary>Abstract</summary>
In medical research, the traditional way to collect data, i.e. browsing patient files, has been proven to induce bias, errors, human labor and costs. We propose a semi-automated system able to extract every type of data, including notes. The Smart Data Extractor pre-populates clinic research forms by following rules. We performed a cross-testing experiment to compare semi-automated to manual data collection. 20 target items had to be collected for 79 patients. The average time to complete one form was 6'81'' for manual data collection and 3'22'' with the Smart Data Extractor. There were also more mistakes during manual data collection (163 for the whole cohort) than with the Smart Data Extractor (46 for the whole cohort). We present an easy to use, understandable and agile solution to fill out clinical research forms. It reduces human effort and provides higher quality data, avoiding data re-entry and fatigue induced errors.
</details>
<details>
<summary>摘要</summary>
医学研究中，传统的数据收集方式，即阅读病人文件，已经被证明会导致偏见、错误、人工劳动和成本增加。我们提议一种半自动的数据收集系统，能够自动提取所有类型的数据，包括笔记。智能数据抽取器按照规则自动填充临床研究表单。我们进行了跨测试实验，比较半自动和手动数据收集方式。对79名病人的20个目标项进行了收集。手动数据收集的平均时间为6'81''，而智能数据抽取器的平均时间为3'22''。此外，手动数据收集中还有更多的错误（总共163个），与智能数据抽取器相比（46个）。我们提供了一种易于使用、易于理解、快速的解决方案，快速填充临床研究表单，减少人工劳动，提供更高质量的数据，避免数据重复和劳动 induced错误。
</details></li>
</ul>
<hr>
<h2 id="Link-Prediction-for-Wikipedia-Articles-as-a-Natural-Language-Inference-Task"><a href="#Link-Prediction-for-Wikipedia-Articles-as-a-Natural-Language-Inference-Task" class="headerlink" title="Link Prediction for Wikipedia Articles as a Natural Language Inference Task"></a>Link Prediction for Wikipedia Articles as a Natural Language Inference Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16469">http://arxiv.org/abs/2308.16469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chau-Thang Phan, Quoc-Nam Nguyen, Kiet Van Nguyen</li>
<li>for: 本文提出了一种解决自动理解大规模知识库结构的链接预测问题的系统，并在Data Science and Advanced Analytics 2023 Competition “Efficient and Effective Link Prediction” (DSAA-2023 Competition)中提交了该系统。</li>
<li>methods: 本文提出了一种将链接预测视为自然语言理解（NLI）任务的方法，基于最近的自然语言处理和理解技术，将链接预测视为两篇文章之间的文本关系预测任务。</li>
<li>results: 本文的实现基于Sentence Pair Classification for Link Prediction for the Wikipedia Articles task，在公共测试集上 achiev 0.99996 Macro F1-score和1.00000 Macro F1-score，与第一名和第二名的分数相同。<details>
<summary>Abstract</summary>
Link prediction task is vital to automatically understanding the structure of large knowledge bases. In this paper, we present our system to solve this task at the Data Science and Advanced Analytics 2023 Competition "Efficient and Effective Link Prediction" (DSAA-2023 Competition) with a corpus containing 948,233 training and 238,265 for public testing. This paper introduces an approach to link prediction in Wikipedia articles by formulating it as a natural language inference (NLI) task. Drawing inspiration from recent advancements in natural language processing and understanding, we cast link prediction as an NLI task, wherein the presence of a link between two articles is treated as a premise, and the task is to determine whether this premise holds based on the information presented in the articles. We implemented our system based on the Sentence Pair Classification for Link Prediction for the Wikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000 Macro F1-score for the public and private test sets, respectively. Our team UIT-NLP ranked 3rd in performance on the private test set, equal to the scores of the first and second places. Our code is publicly for research purposes.
</details>
<details>
<summary>摘要</summary>
很重要的任务是预测wiki文章之间的链接，以自动理解大量知识库的结构。在这篇论文中，我们介绍了我们在“高效高效链接预测”（DSAA-2023）比赛中解决这个任务的系统，采用了948233个训练文章和238265个测试文章。本论文将链接预测问题转化为自然语言推理（NLI）任务， Drawing inspiration from recent advances in natural language processing and understanding, we cast link prediction as an NLI task, where the presence of a link between two articles is treated as a premise, and the task is to determine whether this premise holds based on the information presented in the articles. We implemented our system based on the Sentence Pair Classification for Link Prediction for the Wikipedia Articles task. Our system achieved 0.99996 Macro F1-score and 1.00000 Macro F1-score for the public and private test sets, respectively. Our team UIT-NLP ranked 3rd in performance on the private test set, equal to the scores of the first and second places. Our code is publicly available for research purposes.
</details></li>
</ul>
<hr>
<h2 id="Sparkles-Unlocking-Chats-Across-Multiple-Images-for-Multimodal-Instruction-Following-Models"><a href="#Sparkles-Unlocking-Chats-Across-Multiple-Images-for-Multimodal-Instruction-Following-Models" class="headerlink" title="Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models"></a>Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16463">http://arxiv.org/abs/2308.16463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HYPJUDY/Sparkles">https://github.com/HYPJUDY/Sparkles</a></li>
<li>paper_authors: Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, Yutong Lu<br>for: SparklesChat is designed to handle open-ended dialogues across multiple images, addressing the challenge of maintaining dialogue coherence in multimodal instruction-following tasks.methods: SparklesChat uses a multimodal instruction-following model that integrates text and images, and is trained on the newly introduced SparklesDialogue dataset. The model is evaluated using the SparklesEval benchmark, which assesses conversational competence across multiple images and dialogue turns.results: SparklesChat outperformed MiniGPT-4 on established vision-and-language benchmarks and scored 8.56 out of 10 on SparklesEval, demonstrating its effectiveness in understanding and reasoning across multiple images and dialogue turns. Qualitative evaluations also showed the model’s generality in handling real-world applications.<details>
<summary>Abstract</summary>
Large language models exhibit enhanced zero-shot performance on various tasks when fine-tuned with instruction-following data. Multimodal instruction-following models extend these capabilities by integrating both text and images. However, existing models such as MiniGPT-4 face challenges in maintaining dialogue coherence in scenarios involving multiple images. A primary reason is the lack of a specialized dataset for this critical application. To bridge these gaps, we present SparklesChat, a multimodal instruction-following model for open-ended dialogues across multiple images. To support the training, we introduce SparklesDialogue, the first machine-generated dialogue dataset tailored for word-level interleaved multi-image and text interactions. Furthermore, we construct SparklesEval, a GPT-assisted benchmark for quantitatively assessing a model's conversational competence across multiple images and dialogue turns. Our experiments validate the effectiveness of SparklesChat in understanding and reasoning across multiple images and dialogue turns. Specifically, SparklesChat outperformed MiniGPT-4 on established vision-and-language benchmarks, including the BISON binary image selection task and the NLVR2 visual reasoning task. Moreover, SparklesChat scored 8.56 out of 10 on SparklesEval, substantially exceeding MiniGPT-4's score of 3.91 and nearing GPT-4's score of 9.26. Qualitative evaluations further demonstrate SparklesChat's generality in handling real-world applications. All resources will be available at https://github.com/HYPJUDY/Sparkles.
</details>
<details>
<summary>摘要</summary>
大型语言模型在不同任务上显示出强化零学习性能，当精通化 instruction-following 数据时。多模式 instruction-following 模型将这些能力扩展到包括文字和图像在内的多个模式。然而，现有的模型，如 MiniGPT-4，在多个图像场景中维持对话一致性存在问题。主要原因是缺乏特殊化的数据集。为了bridging这些差距，我们提出 SparklesChat，一个多模式 instruction-following 模型，用于开放式对话过程中的多个图像。为支持训练，我们引入 SparklesDialogue，首个特别设计 для word-level 跨多个图像和文字互动的机器生成对话数据集。此外，我们建立 SparklesEval，一个基于 GPT 的测试工具，用于量化评估模型在多个图像和对话转换中的对话能力。我们的实验显示 SparklesChat 在多个图像和对话转换中理解和推理能力有所提高。具体来说，SparklesChat 在已知的视觉和语言标准 benchmark 上表现出色，包括BISON  binary 图像选择任务和 NLVR2 视觉理解任务。此外，SparklesChat 在 SparklesEval 上获得 8.56 分，大幅超过 MiniGPT-4 的 3.91 分，并且接近 GPT-4 的 9.26 分。实验结果还表明 SparklesChat 在实际应用中具有一般性。所有资源将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distillation-from-Non-streaming-to-Streaming-ASR-Encoder-using-Auxiliary-Non-streaming-Layer"><a href="#Knowledge-Distillation-from-Non-streaming-to-Streaming-ASR-Encoder-using-Auxiliary-Non-streaming-Layer" class="headerlink" title="Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer"></a>Knowledge Distillation from Non-streaming to Streaming ASR Encoder using Auxiliary Non-streaming Layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16415">http://arxiv.org/abs/2308.16415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyuhong Shim, Jinkyu Lee, Simyung Chang, Kyuwoong Hwang</li>
<li>for: 提高流式自动语音识别（ASR）模型的性能</li>
<li>methods: 使用层到层知识储 transmit 教师Encoder 到学生Encoder</li>
<li>results: 比前 Token 概率储 transmit 方法减少词错率<details>
<summary>Abstract</summary>
Streaming automatic speech recognition (ASR) models are restricted from accessing future context, which results in worse performance compared to the non-streaming models. To improve the performance of streaming ASR, knowledge distillation (KD) from the non-streaming to streaming model has been studied, mainly focusing on aligning the output token probabilities. In this paper, we propose a layer-to-layer KD from the teacher encoder to the student encoder. To ensure that features are extracted using the same context, we insert auxiliary non-streaming branches to the student and perform KD from the non-streaming teacher layer to the non-streaming auxiliary layer. We design a special KD loss that leverages the autoregressive predictive coding (APC) mechanism to encourage the streaming model to predict unseen future contexts. Experimental results show that the proposed method can significantly reduce the word error rate compared to previous token probability distillation methods.
</details>
<details>
<summary>摘要</summary>
流式自动语音识别（ASR）模型因无法访问未来上下文，因此其性能较差于非流式模型。为提高流式ASR的性能，我们已经研究了知识塑化（KD）从非流式到流式模型，主要关注输出token概率的匹配。在这篇论文中，我们提议一种层到层KD从教师encoder到学生encoder。为确保使用相同上下文提取特征，我们在学生encoder中插入了auxiliary非流式分支，并在教师层和auxiliary层之间进行KD。我们设计了一种特殊的KD损失，使用自适应预测编码（APC）机制，以促进流式模型预测未见的未来上下文。实验结果表明，我们的方法可以在前期token概率塑化方法中减少词错率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.CL_2023_08_31/" data-id="clot2mhah00ajx78812b2gl3j" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/cs.LG_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T10:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/cs.LG_2023_08_31/">cs.LG - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Note-on-Randomized-Kaczmarz-Algorithm-for-Solving-Doubly-Noisy-Linear-Systems"><a href="#A-Note-on-Randomized-Kaczmarz-Algorithm-for-Solving-Doubly-Noisy-Linear-Systems" class="headerlink" title="A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems"></a>A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16904">http://arxiv.org/abs/2308.16904</a></li>
<li>repo_url: None</li>
<li>paper_authors: El Houcine Bergou, Soumia Boucherouite, Aritra Dutta, Xin Li, Anna Ma</li>
<li>for:  Linear systems with noisy coefficient matrices and right-hand side vectors, and the need for efficient iterative solvers.</li>
<li>methods:  Randomized Kaczmarz (RK) algorithm and its convergence analysis in the presence of both additive and multiplicative noise.</li>
<li>results:  The paper provides a robust analysis of RK’s convergence for noisy linear systems, without requiring knowledge of the noiseless coefficient matrix, and demonstrates the effectiveness of the method through comprehensive numerical experiments.Here’s the full text in Simplified Chinese:</li>
<li>for:  Linear systems $Ax&#x3D;b$ 频繁出现在实践中，需要有效的迭代解算法。它们经常受到操作错误或数据收集过程中的干扰，导致系统受到干扰。过去的十年，Randomized Kaczmarz（RK）算法已经被广泛研究，作为有效的迭代解算法。但是，RK在干扰 regime 的整合研究尚不充分，只考虑了右侧向量 $b$ 的干扰。在实际中， Matrix $A$ 也可能受到干扰。</li>
<li>methods:  RK 算法在干扰 Linear Systems 中的整合分析，包括 $A$ 和 $b$ 的干扰。</li>
<li>results:  paper 提供了 Robust 的 RK 整合分析方法，不需要知道干扰后的 Matrix $A$，并通过对各种干扰条件的分析，可以控制 RK 的整合。实验证明了这些理论成果的实际可行性。<details>
<summary>Abstract</summary>
Large-scale linear systems, $Ax=b$, frequently arise in practice and demand effective iterative solvers. Often, these systems are noisy due to operational errors or faulty data-collection processes. In the past decade, the randomized Kaczmarz (RK) algorithm has been studied extensively as an efficient iterative solver for such systems. However, the convergence study of RK in the noisy regime is limited and considers measurement noise in the right-hand side vector, $b$. Unfortunately, in practice, that is not always the case; the coefficient matrix $A$ can also be noisy. In this paper, we analyze the convergence of RK for noisy linear systems when the coefficient matrix, $A$, is corrupted with both additive and multiplicative noise, along with the noisy vector, $b$. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger} \|_2^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$ represents a noisy version of $A$. We claim that our analysis is robust and realistically applicable, as we do not require information about the noiseless coefficient matrix, $A$, and considering different conditions on noise, we can control the convergence of RK. We substantiate our theoretical findings by performing comprehensive numerical experiments.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learning-to-Taste-A-Multimodal-Wine-Dataset"><a href="#Learning-to-Taste-A-Multimodal-Wine-Dataset" class="headerlink" title="Learning to Taste: A Multimodal Wine Dataset"></a>Learning to Taste: A Multimodal Wine Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16900">http://arxiv.org/abs/2308.16900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thoranna Bender, Simon Møe Sørensen, Alireza Kashani, K. Eldjarn Hjorleifsson, Grethe Hyldig, Søren Hauberg, Serge Belongie, Frederik Warburg</li>
<li>for: 研究视觉、语言和味道之间的关系</li>
<li>methods: 使用大量多模态葡萄酒数据集（WineSensed），包括897k图像和824k葡萄酒评分，以及5k对精细味道距离的人工味道试验结果</li>
<li>results: 提出一种将人类经验与自动机器相似性kernels合并的低维度概念嵌入算法，可以提高粗略味道分类（酒精度、国家、葡萄、价格、评分），并与人类味道感受相似。<details>
<summary>Abstract</summary>
We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
</details>
<details>
<summary>摘要</summary>
我们介绍WineSensed，一个大型多模态葡萄酒数据集，用于研究视觉感知、语言和味道之间的关系。该数据集包括897万张葡萄酒标签图像和824万瓶葡萄酒的评论，来自Vivino平台。它包含了350万个唯一的年份、地区、评分、酒精含量、价格和葡萄种植物的注释。我们对一个子集进行了葡萄酒味道 экспериimento，让256名参与者按照葡萄酒的味道相似性进行排序，共计超过5000个对的Pairwise味道距离。我们提议一种低维度概念嵌入算法，结合人类经验和自动机器相似性kernels。我们示示了这个共享概念嵌入空间可以超越分类（酒精含量、国家、葡萄、价格、评分），并与人类对味道的细致感知相匹配。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-UAV-Enhanced-Networks-Joint-Coverage-and-Convergence-Time-Optimization"><a href="#Federated-Learning-in-UAV-Enhanced-Networks-Joint-Coverage-and-Convergence-Time-Optimization" class="headerlink" title="Federated Learning in UAV-Enhanced Networks: Joint Coverage and Convergence Time Optimization"></a>Federated Learning in UAV-Enhanced Networks: Joint Coverage and Convergence Time Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16889">http://arxiv.org/abs/2308.16889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariam Yahya, Setareh Maghsudi, Slawomir Stanczak</li>
<li>for: 本研究旨在实现无人机增强无线网络中的联邦学习（Federated Learning，FL），并且将其应用于具有能源限制的无线传输网络中。</li>
<li>methods: 本研究使用了多元目标多臂枪械理论来优化网络覆盖，同时降低了联邦学习延迟。另外，我们还提出了一个特别适用于大量动作集和严格能源限制的解决方案，使用了单一最佳臂识别算法来寻找最佳臂，以实现最大化网络覆盖和最小化联邦学习延迟。</li>
<li>results:  NUMERICAL  результаTS show the effectiveness of our approach, and demonstrate that our proposed method can significantly improve the coverage of the wireless sensor network while minimizing the FL delay.<details>
<summary>Abstract</summary>
Federated learning (FL) involves several devices that collaboratively train a shared model without transferring their local data. FL reduces the communication overhead, making it a promising learning method in UAV-enhanced wireless networks with scarce energy resources. Despite the potential, implementing FL in UAV-enhanced networks is challenging, as conventional UAV placement methods that maximize coverage increase the FL delay significantly. Moreover, the uncertainty and lack of a priori information about crucial variables, such as channel quality, exacerbate the problem. In this paper, we first analyze the statistical characteristics of a UAV-enhanced wireless sensor network (WSN) with energy harvesting. We then develop a model and solution based on the multi-objective multi-armed bandit theory to maximize the network coverage while minimizing the FL delay. Besides, we propose another solution that is particularly useful with large action sets and strict energy constraints at the UAVs. Our proposal uses a scalarized best-arm identification algorithm to find the optimal arms that maximize the ratio of the expected reward to the expected energy cost by sequentially eliminating one or more arms in each round. Then, we derive the upper bound on the error probability of our multi-objective and cost-aware algorithm. Numerical results show the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
Federated 学习（FL）包括多个设备共同训练一个共享模型，而不需要传输本地数据。FL 减少了通信开销，使其成为无人机增强无线网络中的优秀学习方法，具有紧张的能源资源。不过，在实施 FL 中，使用 convent ional 无人机布局方法可能会增加延迟，而且无法预知 crucial 变量，如通道质量，使得问题更加复杂。在这篇论文中，我们首先分析了一个由无人机增强的无线传感器网络（WSN）中的能量收集。然后，我们开发了一个基于多目标多臂投机理论的模型和解决方案，以最大化网络覆盖率，同时最小化 FL 延迟。此外，我们还提出了一个特点是在大动作集和严格能源限制下特别有用的解决方案。我们的建议使用一个权重积分算法来找出最佳的臂，以最大化预期回报与预期能源成本的比率。然后，我们 deriv 了Upper bound 上的错误概率。数字结果表明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Prediction-of-Diblock-Copolymer-Morphology-via-Machine-Learning"><a href="#Prediction-of-Diblock-Copolymer-Morphology-via-Machine-Learning" class="headerlink" title="Prediction of Diblock Copolymer Morphology via Machine Learning"></a>Prediction of Diblock Copolymer Morphology via Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16886">http://arxiv.org/abs/2308.16886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyun Park, Boyuan Yu, Juhae Park, Ge Sun, Emad Tajkhorshid, Juan J. de Pablo, Ludwig Schneider</li>
<li>for: 该研究旨在加速大域颗粒材料形态演化计算，以便更好地理解粒子在材料中的Diffusion行为。</li>
<li>methods: 该方法利用了粒子级别的empirical模型和宏观尺度的数学模型之间的分离，并通过使用人工智能来学习Stochastic驱动的异常消失过程。</li>
<li>results: 该研究 validate了一种基于UNet架构的Explainable AI方法，可以快速计算大域颗粒材料的形态演化，并且可以生成大型系统和长时间的轨迹来研究异常密度和其演化过程。<details>
<summary>Abstract</summary>
A machine learning approach is presented to accelerate the computation of block polymer morphology evolution for large domains over long timescales. The strategy exploits the separation of characteristic times between coarse-grained particle evolution on the monomer scale and slow morphological evolution over mesoscopic scales. In contrast to empirical continuum models, the proposed approach learns stochastically driven defect annihilation processes directly from particle-based simulations. A UNet architecture that respects different boundary conditions is adopted, thereby allowing periodic and fixed substrate boundary conditions of arbitrary shape. Physical concepts are also introduced via the loss function and symmetries are incorporated via data augmentation. The model is validated using three different use cases. Explainable artificial intelligence methods are applied to visualize the morphology evolution over time. This approach enables the generation of large system sizes and long trajectories to investigate defect densities and their evolution under different types of confinement. As an application, we demonstrate the importance of accessing late-stage morphologies for understanding particle diffusion inside a single block. This work has implications for directed self-assembly and materials design in micro-electronics, battery materials, and membranes.
</details>
<details>
<summary>摘要</summary>
machine learning方法提出以加速大域领域内部链 polymer 结构演化计算。该策略利用粗粒体EVOLUTION的特征时间分解，从粗粒体 simulations 直接学习随机驱动的 Defect 消失过程。与经验法模型不同，该方法从粗粒体基本上学习随机驱动的 Defect 消失过程。采用UNet 架构，并遵循不同的边界条件，以 periodic 和固定substrate 边界条件。通过引入物理概念到损失函数中，并通过数据扩展来 incorporate Symmetries。该模型通过三个不同的应用 validate。使用 Explainable AI 方法可以Visualize 链 polymer 结构演化过程。这种方法可以生成大型系统和长时间轨迹，以调查各种封闭环境中 Defect 的浓度和演化。作为应用，我们示出了在一个块内部粒子 diffusion 的重要性。这种方法有关 directed self-assembly 和材料设计在微电子、电池材料和膜料等领域。
</details></li>
</ul>
<hr>
<h2 id="Information-Theoretically-Optimal-Sample-Complexity-of-Learning-Dynamical-Directed-Acyclic-Graphs"><a href="#Information-Theoretically-Optimal-Sample-Complexity-of-Learning-Dynamical-Directed-Acyclic-Graphs" class="headerlink" title="Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs"></a>Information Theoretically Optimal Sample Complexity of Learning Dynamical Directed Acyclic Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16859">http://arxiv.org/abs/2308.16859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mishfad Shaikh Veedu, Deepjyoti Deka, Murti V. Salapaka</li>
<li>for: 本文研究了一种 Linear Dynamical System (LDS) 上的 Directed Acyclic Graph (DAG) 的下面复杂度。 Specifically, the paper studies the sample complexity of learning the underlying DAG of an LDS over a DAG, where the nodal states are temporally correlated and driven by unobserved exogenous noise sources.</li>
<li>methods: 本文提出了一种基于 PSD 矩阵的度量和算法来重建 DAG。 The proposed metric and algorithm are inspired by the static settings, but are modified to accommodate the temporal correlations in the nodal states. The paper also considers the case where the equal noise PSD assumption can be relaxed.</li>
<li>results: 本文证明了 DAG 的下面复杂度为 $n &#x3D; \Theta(q\log(p&#x2F;q))$, where $p$ is the number of nodes and $q$ is the maximum number of parents per node. This upper bound is proven using a concentration bound for the PSD estimation, as well as a matching min-max lower bound based on generalized Fano’s inequality.<details>
<summary>Abstract</summary>
In this article, the optimal sample complexity of learning the underlying interaction/dependencies of a Linear Dynamical System (LDS) over a Directed Acyclic Graph (DAG) is studied. The sample complexity of learning a DAG's structure is well-studied for static systems, where the samples of nodal states are independent and identically distributed (i.i.d.). However, such a study is less explored for DAGs with dynamical systems, where the nodal states are temporally correlated. We call such a DAG underlying an LDS as \emph{dynamical} DAG (DDAG). In particular, we consider a DDAG where the nodal dynamics are driven by unobserved exogenous noise sources that are wide-sense stationary (WSS) in time but are mutually uncorrelated, and have the same {power spectral density (PSD)}. Inspired by the static settings, a metric and an algorithm based on the PSD matrix of the observed time series are proposed to reconstruct the DDAG. The equal noise PSD assumption can be relaxed such that identifiability conditions for DDAG reconstruction are not violated. For the LDS with WSS (sub) Gaussian exogenous noise sources, it is shown that the optimal sample complexity (or length of state trajectory) needed to learn the DDAG is $n=\Theta(q\log(p/q))$, where $p$ is the number of nodes and $q$ is the maximum number of parents per node. To prove the sample complexity upper bound, a concentration bound for the PSD estimation is derived, under two different sampling strategies. A matching min-max lower bound using generalized Fano's inequality also is provided, thus showing the order optimality of the proposed algorithm.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了一个线性动力系统（LDS）下的指向图（DAG）的下面复杂性。已经对静止系统的结构学习了许多，但是对动态系统的研究较少。我们称这种DAG为动态DAG（DDAG）。特别是，我们考虑了一个DDAG，其中节点动力是由未观察的外部噪声源驱动，这些噪声源是时间方向上广泛站立的（WSS），且没有相互相关。根据静止设置，我们提出了一个度量和一个算法，使用观察时间序列的PSD矩阵来重建DDAG。PSD假设可以被放宽，以便不论identifiability condition不被违反。对LDS WITH WSS（子） Gaussian噪声源，我们证明了需要学习DDAG的样本复杂度为 $n = \Theta(q\log(p/q))$,其中$p$ 是节点数量，$q$ 是每个节点最多的父节点数。为证明样本复杂度上限，我们 derivated一个PSD估计的吸引 bound，以及一个通用Fano的不等式来提供下界。因此，我们得出了DDAG的学习算法的度量优化性。
</details></li>
</ul>
<hr>
<h2 id="Majorization-Minimization-for-sparse-SVMs"><a href="#Majorization-Minimization-for-sparse-SVMs" class="headerlink" title="Majorization-Minimization for sparse SVMs"></a>Majorization-Minimization for sparse SVMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16858">http://arxiv.org/abs/2308.16858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Benfenati, Emilie Chouzenoux, Giorgia Franchini, Salla Latva-Aijo, Dominik Narnhofer, Jean-Christophe Pesquet, Sebastian J. Scott, Mahsa Yousefi</li>
<li>for: 本研究旨在提出一种基于稀疏Promoting-Regularized squared hinge loss minimization的支持向量机器学习（SVM）训练方法，以便应用快速训练方法和提高性能。</li>
<li>methods: 本研究使用了稀疏Promoting-Regularized squared hinge loss minimization方法，该方法利用了对函数梯度的Lippenchitz可微性，并可以处理稀疏正则化项，以提高选择最重要特征的能力。</li>
<li>results: 根据对三个不同数据集的测试和比较，提出的方法在指标（准确率、精度、回归率和F 1 分数）和计算成本两个方面具有良好的表现。<details>
<summary>Abstract</summary>
Several decades ago, Support Vector Machines (SVMs) were introduced for performing binary classification tasks, under a supervised framework. Nowadays, they often outperform other supervised methods and remain one of the most popular approaches in the machine learning arena. In this work, we investigate the training of SVMs through a smooth sparse-promoting-regularized squared hinge loss minimization. This choice paves the way to the application of quick training methods built on majorization-minimization approaches, benefiting from the Lipschitz differentiabililty of the loss function. Moreover, the proposed approach allows us to handle sparsity-preserving regularizers promoting the selection of the most significant features, so enhancing the performance. Numerical tests and comparisons conducted on three different datasets demonstrate the good performance of the proposed methodology in terms of qualitative metrics (accuracy, precision, recall, and F 1 score) as well as computational cost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Natural-Quantum-Monte-Carlo-Computation-of-Excited-States"><a href="#Natural-Quantum-Monte-Carlo-Computation-of-Excited-States" class="headerlink" title="Natural Quantum Monte Carlo Computation of Excited States"></a>Natural Quantum Monte Carlo Computation of Excited States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16848">http://arxiv.org/abs/2308.16848</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Pfau, Simon Axelrod, Halvard Sutterud, Ingrid von Glehn, James S. Spencer</li>
<li>for: 这个论文是为了估计量子系统的最低升阶态而写的。</li>
<li>methods: 这个方法使用变分 Monte Carlo 算法，无需任何自由参数和显式正交化不同态态。</li>
<li>results: 这个方法可以准确地估计量子系统的升阶态，并可以计算不同态之间的偏振矩。这种方法在分子物理中将是非常有用，例如可以准确地估计分子的升阶能量和振荡矩。<details>
<summary>Abstract</summary>
We present a variational Monte Carlo algorithm for estimating the lowest excited states of a quantum system which is a natural generalization of the estimation of ground states. The method has no free parameters and requires no explicit orthogonalization of the different states, instead transforming the problem of finding excited states of a given system into that of finding the ground state of an expanded system. Expected values of arbitrary observables can be calculated, including off-diagonal expectations between different states such as the transition dipole moment. Although the method is entirely general, it works particularly well in conjunction with recent work on using neural networks as variational Ansatze for many-electron systems, and we show that by combining this method with the FermiNet and Psiformer Ansatze we can accurately recover vertical excitation energies and oscillator strengths on molecules as large as benzene. Beyond the examples on molecules presented here, we expect this technique will be of great interest for applications of variational quantum Monte Carlo to atomic, nuclear and condensed matter physics.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种变分 Monte Carlo 算法，用于估算量子系统的最低强度态。这种方法是自然推广估算系统的基准态的方法，没有自由参数，不需要显式对不同态进行正交化。将问题变为找到系统的扩展系统的基准态。可以计算出任意观测量的期望值，包括不同态之间的偏移量，如轨道电动势矩。这种方法是普适的，但特别适用于使用神经网络作为多电子系统的变量 Ansatz，我们示出了将这种方法与 FermiNet 和 Psiformer Ansatz 结合使用可以准确地回归分子上的垂直升降能和振荡强度。此外，我们预期这种技术在原子、核和 condensed matter 物理中将具有广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="FedDD-Toward-Communication-efficient-Federated-Learning-with-Differential-Parameter-Dropout"><a href="#FedDD-Toward-Communication-efficient-Federated-Learning-with-Differential-Parameter-Dropout" class="headerlink" title="FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout"></a>FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16835">http://arxiv.org/abs/2308.16835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiying Feng, Xu Chen, Qiong Wu, Wen Wu, Xiaoxi Zhang, Qianyi Huang</li>
<li>for: 提高 Federated Learning（FL）的通信效率和模型融合能力，解决客户端间网络环境不同而导致的长时间通信延迟和模型融合问题。</li>
<li>methods: 提出了一种基于模型参数抽象的 Federated Learning 方案，即 Dropout 率分配和上传参数选择两个关键模块，通过优化客户端对应的模型参数上传比例和选择重要参数上传，以适应不同客户端的各种各样的网络环境和数据特点。</li>
<li>results: 通过 teorically 分析和实验评估，显示了提出的 FedDD 方案可以在通信效率和模型融合能力两个方面具有出色的表现，同时具有强大的泛化能力，能够适应数据的罕见类。<details>
<summary>Abstract</summary>
Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model parameter uploading ratios tailored to different clients' heterogeneous conditions and also select the proper set of important model parameters for uploading subject to clients' dropout rate constraints. Specifically, the dropout rate allocation is formulated as a convex optimization problem, taking system heterogeneity, data heterogeneity, and model heterogeneity among clients into consideration. The uploaded parameter selection strategy prioritizes on eliciting important parameters for uploading to speedup convergence. Furthermore, we theoretically analyze the convergence of the proposed FedDD scheme. Extensive performance evaluations demonstrate that the proposed FedDD scheme can achieve outstanding performances in both communication efficiency and model convergence, and also possesses a strong generalization capability to data of rare classes.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 需要频繁交换模型参数，这会导致长途通信延迟，特别是当客户端环境差异较大时。此外，服务器还需要等待最慢的客户端（即废物，可能有最大模型大小、最低计算能力或最差网络条件）上传参数，这可能会对通信效率产生重大影响。常见的客户端选择方法，如部分客户端选择，会导致计算资源浪费和全局模型的泛化弱化。为解决这个问题，我们在这篇论文中提出了参数掉弃法，并对其进行了修改，从而提出了一种新的 Federated Learning 方案——Differential Parameter Dropout (FedDD)。FedDD 包括两个关键模块：Dropout 率分配和上传参数选择，它们会优化模型参数上传比例适应不同客户端的各种差异，同时选择合适的重要模型参数上传。具体来说，Dropout 率分配是一个凸型优化问题，考虑到系统差异、数据差异和模型差异。上传参数选择策略强调选择重要的参数上传，以加速协同整合。此外，我们还 theoretically 分析了 FedDD 方案的收敛性。EXT 的性能评估表明，提出的 FedDD 方案可以在通信效率和模型收敛之间取得极佳的平衡，同时具有强大的泛化能力，对数据的罕见类型进行泛化。
</details></li>
</ul>
<hr>
<h2 id="Joint-Semantic-Native-Communication-and-Inference-via-Minimal-Simplicial-Structures"><a href="#Joint-Semantic-Native-Communication-and-Inference-via-Minimal-Simplicial-Structures" class="headerlink" title="Joint Semantic-Native Communication and Inference via Minimal Simplicial Structures"></a>Joint Semantic-Native Communication and Inference via Minimal Simplicial Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16789">http://arxiv.org/abs/2308.16789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiyang Zhao, Hang Zou, Mehdi Bennis, Merouane Debbah, Ebtesam Almazrouei, Faouzi Bader</li>
<li>for: 在本文中，我们研究了 semantic communication and inference 问题，在其中一个学生代理（Mobile device）向一个教师代理（Cloud server）提出查询以生成更高级别的数据semantics。</li>
<li>methods: 教师首先将其数据映射到 k-order simplicial complex 中，并学习其高阶相关性。为了有效地传输信息和进行推理，教师寻找最小 suffice 和不变的 semantic structures，并通过judiciously removing simplices 选择由Hodge Laplacians 选择的 simplicial structures。学生本地运行自己的查询，基于masked simplicial convolutional autoencoder (SCAE)，并利用本地和远程教师的知识。</li>
<li>results: 数字结果表明我们提出的方法有效地提高了查询准确率，不同通道条件和 simplicial 结构下。在一个合作作者数据集上，去掉 simplicial 结构 ranked Laplacian values 可以减少payload大小85%，不会影响准确率。 joint semantic communication and inference by masked SCAE 可以提高查询准确率25%，比本地学生基于查询和远程教师基于查询的准确率高15%。最后，我们发现 incorporating channel semantics 可以有效地提高推理准确率，特别是在低 SNR 值下。<details>
<summary>Abstract</summary>
In this work, we study the problem of semantic communication and inference, in which a student agent (i.e. mobile device) queries a teacher agent (i.e. cloud sever) to generate higher-order data semantics living in a simplicial complex. Specifically, the teacher first maps its data into a k-order simplicial complex and learns its high-order correlations. For effective communication and inference, the teacher seeks minimally sufficient and invariant semantic structures prior to conveying information. These minimal simplicial structures are found via judiciously removing simplices selected by the Hodge Laplacians without compromising the inference query accuracy. Subsequently, the student locally runs its own set of queries based on a masked simplicial convolutional autoencoder (SCAE) leveraging both local and remote teacher's knowledge. Numerical results corroborate the effectiveness of the proposed approach in terms of improving inference query accuracy under different channel conditions and simplicial structures. Experiments on a coauthorship dataset show that removing simplices by ranking the Laplacian values yields a 85% reduction in payload size without sacrificing accuracy. Joint semantic communication and inference by masked SCAE improves query accuracy by 25% compared to local student based query and 15% compared to remote teacher based query. Finally, incorporating channel semantics is shown to effectively improve inference accuracy, notably at low SNR values.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了 semantic communication和推理问题，在其中一个学生代理（即移动设备）向一个教师代理（即云服务器）发送查询来生成更高级数据 semantics。特别是，教师首先将其数据映射到 k-order simplicial complex 中，并学习其高级相关性。为了有效地传输信息并进行推理，教师寻找最小 suffice 和不变的 semantic structures ，以便在传输信息之前进行准备。这些最小 simplicial structures 通过 judiciously  removing simplices 选择 Hodge Laplacians 而获得。然后，学生本地运行自己的查询，基于 masked simplicial convolutional autoencoder (SCAE) ，利用本地和远程教师的知识。numerical results 表明提议的方法可以在不同的通道条件和 simplicial structures 下提高推理查询精度。在 coauthorship 数据集上，通过将 simplices 按照 Laplacian 值排序来减少payload大小，可以得到85%的减少，而不会影响准确性。 joint semantic communication and inference by masked SCAE 可以提高查询精度，比 мест学生基于查询的精度提高25%，比远程教师基于查询的精度提高15%。最后， incorporating channel semantics 可以有效地提高推理精度，特别是在低 SNR 值下。
</details></li>
</ul>
<hr>
<h2 id="Constructing-Indoor-Region-based-Radio-Map-without-Location-Labels"><a href="#Constructing-Indoor-Region-based-Radio-Map-without-Location-Labels" class="headerlink" title="Constructing Indoor Region-based Radio Map without Location Labels"></a>Constructing Indoor Region-based Radio Map without Location Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16759">http://arxiv.org/abs/2308.16759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Xing, Junting Chen</li>
<li>for: construct a radio map without location labels</li>
<li>methods: signal subspace model with sequential prior, integrated segmentation and clustering algorithm</li>
<li>results: reduces region localization error by roughly 50% compared to baseline, outperforms some supervised localization schemes<details>
<summary>Abstract</summary>
Radio map construction requires a large amount of radio measurement data with location labels, which imposes a high deployment cost. This paper develops a region-based radio map from received signal strength (RSS) measurements without location labels. The construction is based on a set of blindly collected RSS measurement data from a device that visits each region in an indoor area exactly once, where the footprints and timestamps are not recorded. The main challenge is to cluster the RSS data and match clusters with the physical regions. Classical clustering algorithms fail to work as the RSS data naturally appears as non-clustered due to multipaths and noise. In this paper, a signal subspace model with a sequential prior is constructed for the RSS data, and an integrated segmentation and clustering algorithm is developed, which is shown to find the globally optimal solution in a special case. Furthermore, the clustered data is matched with the physical regions using a graph-based approach. Based on real measurements from an office space, the proposed scheme reduces the region localization error by roughly 50% compared to a weighted centroid localization (WCL) baseline, and it even outperforms some supervised localization schemes, including k-nearest neighbor (KNN), support vector machine (SVM), and deep neural network (DNN), which require labeled data for training.
</details>
<details>
<summary>摘要</summary>
Radio 地图构建需要大量的无线测量数据与位置标签，这会导致高达 deployment 成本。本文基于接收信号强度（RSS）测量数据无法获取位置标签，开发了一种基于区域的无线地图。该构建基于一组隐藏收集的 RSS 测量数据，来自设备在室内区域中访问每个区域仅一次，无法记录足迹和时间戳。主要挑战是将 RSS 数据分组并与物理区域匹配。经典的分组算法无法工作，因为 RSS 数据自然出现非分布的特征，即 multipath 和噪声。本文构建了一个信号特征空间模型，并开发了一种整合分组和 clustering 算法，该算法在特定情况下能够找到全球最佳解决方案。此外，分组后的数据与物理区域进行图形相匹配。基于实际测量的办公室空间数据，提出的方案与权重中心位置标注（WCL）基准相比，减少了地区本地化错误约50%，甚至超过了一些指导式本地化方案，包括 k-最近邻（KNN）、支持向量机（SVM）和深度神经网络（DNN），这些方案需要训练数据。
</details></li>
</ul>
<hr>
<h2 id="Training-Neural-Networks-Using-Reproducing-Kernel-Space-Interpolation-and-Model-Reduction"><a href="#Training-Neural-Networks-Using-Reproducing-Kernel-Space-Interpolation-and-Model-Reduction" class="headerlink" title="Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction"></a>Training Neural Networks Using Reproducing Kernel Space Interpolation and Model Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16754">http://arxiv.org/abs/2308.16754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Arthur Werneburg</li>
<li>for: 这篇论文是关于使用插值技术来训练神经网络的理论研究。</li>
<li>methods: 这篇论文使用了 reproduce kernel Hilbert space 理论来推广神经网络的训练方法，并研究了 associate Hilbert space 的概念以提高 activation function 的表达能力。</li>
<li>results: 该论文提出了一种基于多重复杂函数理论的神经网络 architecture，称为 Prolongation Neural Networks (PNN)，并证明了 PNN 在噪音环境中表现更好于当前state-of-the-art 方法。<details>
<summary>Abstract</summary>
We introduce and study the theory of training neural networks using interpolation techniques from reproducing kernel Hilbert space theory. We generalize the method to Krein spaces, and show that widely-used neural network architectures are subsets of reproducing kernel Krein spaces (RKKS). We study the concept of "associated Hilbert spaces" of RKKS and develop techniques to improve upon the expressivity of various activation functions. Next, using concepts from the theory of functions of several complex variables, we prove a computationally applicable, multidimensional generalization of the celebrated Adamjan- Arov-Krein (AAK) theorem. The theorem yields a novel class of neural networks, called Prolongation Neural Networks (PNN). We demonstrate that, by applying the multidimensional AAK theorem to gain a PNN, one can gain performance superior to both our interpolatory methods and current state-of-the-art methods in noisy environments. We provide useful illustrations of our methods in practice.
</details>
<details>
<summary>摘要</summary>
我们介绍和研究使用 interpolating 技术训练神经网络的理论，并将其推广到 Krein 空间中。我们显示了现有的神经网络架构是 reproduce 的 kernel Krein 空间（RKKS）的子集。我们研究了 associate 的希尔伯特空间 的概念，并开发了提高各种激活函数表达能力的技巧。接着，使用函数多个复数变量理论，我们证明了一种计算可行的、多维度泛化的 Adamjan-Arov-Krein（AAK）定理。这个定理提供了一种新的神经网络，称为 Prolongation Neural Networks（PNN）。我们示出了，通过将多维度 AAK 定理应用于 PNN，可以在噪音环境中获得性能更高的结果，比我们的 interpolatory 方法和当前领域的状态OF-the-art 方法更好。我们在实践中提供了有用的示例。
</details></li>
</ul>
<hr>
<h2 id="Moreau-Envelope-ADMM-for-Decentralized-Weakly-Convex-Optimization"><a href="#Moreau-Envelope-ADMM-for-Decentralized-Weakly-Convex-Optimization" class="headerlink" title="Moreau Envelope ADMM for Decentralized Weakly Convex Optimization"></a>Moreau Envelope ADMM for Decentralized Weakly Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16752">http://arxiv.org/abs/2308.16752</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Mirzaeifard, Naveen K. D. Venkategowda, Alexander Jung, Stefan Werner</li>
<li>for: 提出一种基于分布式优化的 proximal ADMM 算法，用于解决各种 convex 和非 convex 优化问题。</li>
<li>methods: 使用 Moreau envelope function 分析 ADMM 算法的收敛性，并计算 dual 变量更新步骤中的 bound。</li>
<li>results: 对一系列 numerical experiments 进行了比较，发现提出的方法比 widely-used approaches 更快和稳定。<details>
<summary>Abstract</summary>
This paper proposes a proximal variant of the alternating direction method of multipliers (ADMM) for distributed optimization. Although the current versions of ADMM algorithm provide promising numerical results in producing solutions that are close to optimal for many convex and non-convex optimization problems, it remains unclear if they can converge to a stationary point for weakly convex and locally non-smooth functions. Through our analysis using the Moreau envelope function, we demonstrate that MADM can indeed converge to a stationary point under mild conditions. Our analysis also includes computing the bounds on the amount of change in the dual variable update step by relating the gradient of the Moreau envelope function to the proximal function. Furthermore, the results of our numerical experiments indicate that our method is faster and more robust than widely-used approaches.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种靠近的多向量方法（MADM），用于分布式优化。 current ADMM 算法可以在许多凹陷和非凹陷优化问题上提供优秀的数值结果，但是无法确定是否可以到达一个稳定点 для弱 convex 和地方非滑降函数。 我们通过使用Moreau函数的包装函数进行分析，并证明了 MADM 可以到达稳定点，只要满足一定的轻量级条件。 我们的分析还包括计算 dual 变量更新步骤中的变化 bounds，通过将 proximal 函数的梯度与 Moreau 函数的梯度相关。 此外，我们的数值实验结果表明，我们的方法比广泛使用的方法更快和更稳定。
</details></li>
</ul>
<hr>
<h2 id="Robust-Representation-Learning-for-Unreliable-Partial-Label-Learning"><a href="#Robust-Representation-Learning-for-Unreliable-Partial-Label-Learning" class="headerlink" title="Robust Representation Learning for Unreliable Partial Label Learning"></a>Robust Representation Learning for Unreliable Partial Label Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16718">http://arxiv.org/abs/2308.16718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Shi, Dong-Dong Wu, Xin Geng, Min-Ling Zhang</li>
<li>for: 提高弱监督学习中对假标签的耐质量性能</li>
<li>methods: 提出了一种基于不可靠部分标签学习的不可靠度耐质量学习框架（URRL），并提出了一种双战略，包括KNN基于候选标签集修正和一致规则基于标签冲突的修正方法</li>
<li>results: 对多个 datasets 进行了广泛的实验，并证明了该方法可以比现有方法在不同的不可靠性和模糊性下表现出较好的性能<details>
<summary>Abstract</summary>
Partial Label Learning (PLL) is a type of weakly supervised learning where each training instance is assigned a set of candidate labels, but only one label is the ground-truth. However, this idealistic assumption may not always hold due to potential annotation inaccuracies, meaning the ground-truth may not be present in the candidate label set. This is known as Unreliable Partial Label Learning (UPLL) that introduces an additional complexity due to the inherent unreliability and ambiguity of partial labels, often resulting in a sub-optimal performance with existing methods. To address this challenge, we propose the Unreliability-Robust Representation Learning framework (URRL) that leverages unreliability-robust contrastive learning to help the model fortify against unreliable partial labels effectively. Concurrently, we propose a dual strategy that combines KNN-based candidate label set correction and consistency-regularization-based label disambiguation to refine label quality and enhance the ability of representation learning within the URRL framework. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art PLL methods on various datasets with diverse degrees of unreliability and ambiguity. Furthermore, we provide a theoretical analysis of our approach from the perspective of the expectation maximization (EM) algorithm. Upon acceptance, we pledge to make the code publicly accessible.
</details>
<details>
<summary>摘要</summary>
受限 Label Learning（PLL）是一种弱有监督学习方法，每个训练实例都会被分配一组候选标签，但只有一个标签是真实的。然而，这种理想化的假设并不总是成立，因为可能存在注释错误，导致真实的标签不在候选标签集中。这被称为不可靠受限 Label Learning（UPLL），它带来了额外的复杂性，由于受限标签的不可靠性和抽象性，通常会导致现有方法的下降性能。为解决这个挑战，我们提出了不可靠性Robust Representation Learning框架（URRL），它利用不可靠性Robust contrastive learning来帮助模型在受限标签下坚持effectively。同时，我们提出了两个策略：一是KNN基于候选标签集 corrections，二是Consistency regularization基于标签卷积来纠正标签质量并增强表示学习的能力。广泛的实验显示，我们提出的方法在不同的数据集上与已有PLL方法进行比较，具有更高的性能。此外，我们还提供了基于EM算法的理论分析。接受后，我们将代码公开访问。
</details></li>
</ul>
<hr>
<h2 id="Everything-Everywhere-All-in-One-Evaluation-Using-Multiverse-Analysis-to-Evaluate-the-Influence-of-Model-Design-Decisions-on-Algorithmic-Fairness"><a href="#Everything-Everywhere-All-in-One-Evaluation-Using-Multiverse-Analysis-to-Evaluate-the-Influence-of-Model-Design-Decisions-on-Algorithmic-Fairness" class="headerlink" title="Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness"></a>Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16681">http://arxiv.org/abs/2308.16681</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reliable-ai/fairml-multiverse">https://github.com/reliable-ai/fairml-multiverse</a></li>
<li>paper_authors: Jan Simson, Florian Pfisterer, Christoph Kern<br>for: This paper aims to study the fairness of algorithmic decision-making (ADM) systems and provide a method for analyzing their fairness.methods: The authors introduce the method of multiverse analysis for algorithmic fairness, which turns implicit design decisions into explicit ones and demonstrates their fairness implications.results: The authors use an exemplary case study of predicting public health coverage for vulnerable populations to illustrate how decisions during the design of a machine learning system can have surprising effects on its fairness, and how to detect these effects using multiverse analysis. The results show that the method can be used to better understand variability and robustness of algorithmic fairness.<details>
<summary>Abstract</summary>
A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. When designed well, these systems promise more objective decisions while saving large amounts of resources and freeing up human time. However, when ADM systems are not designed well, they can lead to unfair decisions which discriminate against societal groups. The downstream effects of ADMs critically depend on the decisions made during the systems' design and implementation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these design decisions are made implicitly, without knowing exactly how they will influence the final system. It is therefore important to make explicit the decisions made during the design of ADM systems and understand how these decisions affect the fairness of the resulting system.   To study this issue, we draw on insights from the field of psychology and introduce the method of multiverse analysis for algorithmic fairness. In our proposed method, we turn implicit design decisions into explicit ones and demonstrate their fairness implications. By combining decisions, we create a grid of all possible "universes" of decision combinations. For each of these universes, we compute metrics of fairness and performance. Using the resulting dataset, one can see how and which decisions impact fairness. We demonstrate how multiverse analyses can be used to better understand variability and robustness of algorithmic fairness using an exemplary case study of predicting public health coverage of vulnerable populations for potential interventions. Our results illustrate how decisions during the design of a machine learning system can have surprising effects on its fairness and how to detect these effects using multiverse analysis.
</details>
<details>
<summary>摘要</summary>
很多系统在全球使用算法决策（ADM）来自动化以前由人类做出的决策。当这些系统设计得好时，它们承诺会提供更Objective的决策，同时节省大量资源并释放人类时间。然而，当ADM系统不好设计时，它们可能会导致不公正的决策，排挤社会群体。downstream效果 OF ADMs取决于系统设计和实施阶段中的决策，因为数据中的偏见可以在模型化管道中被减少或加强。许多这些设计决策是通过不确定的方式进行，不知道它们会在最终系统中产生什么影响。因此，需要将ADM系统的设计决策变为显式的，并理解这些决策如何影响系统的公平性。为了解决这个问题，我们从心理学中借鉴了一些思想，并提出了用于算法公平的多宇托分析方法。在我们的提议方法中，我们将设计决策转换为显式的决策，并证明这些决策对公平性的影响。我们创建了一个包含所有可能的决策组合的网格。对每个这些宇托，我们计算公平性和性能的度量。使用这些数据，我们可以看到哪些决策对公平性产生了影响。我们示例案例研究了预测护理覆盖的护理人口，以及可能的交叉 intervención。我们的结果表明，设计决策可能对算法公平产生不期望的影响，并如何使用多宇托分析来探测这些影响。
</details></li>
</ul>
<hr>
<h2 id="Branches-of-a-Tree-Taking-Derivatives-of-Programs-with-Discrete-and-Branching-Randomness-in-High-Energy-Physics"><a href="#Branches-of-a-Tree-Taking-Derivatives-of-Programs-with-Discrete-and-Branching-Randomness-in-High-Energy-Physics" class="headerlink" title="Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics"></a>Branches of a Tree: Taking Derivatives of Programs with Discrete and Branching Randomness in High Energy Physics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16680">http://arxiv.org/abs/2308.16680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Kagan, Lukas Heinrich</li>
<li>for: 高能物理领域中的程序 diffeentiation</li>
<li>methods:  gradient estimation techniques</li>
<li>results: 开 up gradient based optimization in detector design optimization, simulator tuning, or data analysis and reconstruction optimizationHere’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 本文为高能物理领域中的程序 diffeentiation提供了方法。</li>
<li>methods: 本文使用了多种gradient estimation techniques。</li>
<li>results: 本文通过gradient estimation techniques，开 up了高能物理中的探测设计优化、模拟器调整、数据分析和重建优化。I hope this helps!<details>
<summary>Abstract</summary>
We propose to apply several gradient estimation techniques to enable the differentiation of programs with discrete randomness in High Energy Physics. Such programs are common in High Energy Physics due to the presence of branching processes and clustering-based analysis. Thus differentiating such programs can open the way for gradient based optimization in the context of detector design optimization, simulator tuning, or data analysis and reconstruction optimization. We discuss several possible gradient estimation strategies, including the recent Stochastic AD method, and compare them in simplified detector design experiments. In doing so we develop, to the best of our knowledge, the first fully differentiable branching program.
</details>
<details>
<summary>摘要</summary>
我们提议使用多种梯度估计技术来启用高能物理中程序中的随机性的 differentiability。这些程序在高能物理中非常普遍，因为它们包含分支过程和归一化分析。因此，可以通过梯度基本优化来优化探测设计优化、模拟调试、数据分析和重建优化。我们讨论了多种可能的梯度估计策略，包括最近的随机AD方法，并在简化的探测设计实验中进行比较。在这之前，我们开发了，以我们所知道的，第一个完全可导分支程序。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-nsNet2-Efficient-Deep-Noise-Suppression-with-Early-Exiting"><a href="#Dynamic-nsNet2-Efficient-Deep-Noise-Suppression-with-Early-Exiting" class="headerlink" title="Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting"></a>Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16678">http://arxiv.org/abs/2308.16678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Riccardo Miccini, Alaa Zniber, Clément Laroche, Tobias Piechowiak, Martin Schoeberl, Luca Pezzarossa, Ouassim Karrakchou, Jens Sparsø, Mounir Ghogho</li>
<li>for: 提高资源受限设备上深度降噪模型的性能和资源利用率</li>
<li>methods: 基于nsNet2的早退模型，实现多级准确率和计算复杂度的负担减少，并通过分流信息流来考虑引入的动态性</li>
<li>results: 通过Established metrics显示了模型性能和计算复杂度之间的负担减少和平衡Here’s a breakdown of each point:</li>
<li>for: The paper is written to improve the performance and resource utilization of deep noise suppression models on resource-constrained devices.</li>
<li>methods: The paper proposes an early-exiting model based on nsNet2, which provides multiple levels of accuracy and resource savings by halting computations at different stages. The original architecture is adapted by splitting the information flow to account for injected dynamism.</li>
<li>results: The paper shows the trade-offs between performance and computational complexity based on established metrics.<details>
<summary>Abstract</summary>
Although deep learning has made strides in the field of deep noise suppression, leveraging deep architectures on resource-constrained devices still proved challenging. Therefore, we present an early-exiting model based on nsNet2 that provides several levels of accuracy and resource savings by halting computations at different stages. Moreover, we adapt the original architecture by splitting the information flow to take into account the injected dynamism. We show the trade-offs between performance and computational complexity based on established metrics.
</details>
<details>
<summary>摘要</summary>
although deep learning has made great strides in the field of deep noise suppression, leveraging deep architectures on resource-constrained devices still proved challenging. Therefore, we present an early-exiting model based on nsNet2 that provides several levels of accuracy and resource savings by halting computations at different stages. Moreover, we adapt the original architecture by splitting the information flow to take into account the injected dynamism. We show the trade-offs between performance and computational complexity based on established metrics.Here's the word-for-word translation:深度学习在深度噪声抑制领域已经做出了很大的进步，但是在有限资源的设备上运行深度建筑仍然是一个挑战。因此，我们提出了基于nsNet2的早退模型，可以在不同的阶段停止计算，并提供了几个级别的准确性和资源节省。此外，我们修改了原始建筑，将信息流分成多个流程，以考虑在注入的动态中的变化。我们根据已知的度量表示出了性能和计算复杂度之间的交易。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Decentralized-Federated-Learning-via-One-Bit-Compressive-Sensing"><a href="#Communication-Efficient-Decentralized-Federated-Learning-via-One-Bit-Compressive-Sensing" class="headerlink" title="Communication-Efficient Decentralized Federated Learning via One-Bit Compressive Sensing"></a>Communication-Efficient Decentralized Federated Learning via One-Bit Compressive Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16671">http://arxiv.org/abs/2308.16671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shenglong Zhou, Kaidi Xu, Geoffrey Ye Li</li>
<li>for: 这个论文目的是为了实现分散式联合学习（DFL）中的模型训练，并且在分散式环境中实现模型训练的效率和稳定性。</li>
<li>methods: 本论文使用了一个基于对调方向法（iADM）的新算法，并且将模型受限于简洁性的限制，使得可以使用一位数探测（1BCS）来实现模型训练。在训练过程中，仅有部分邻居参与训练，使得算法具有耐慢端机器的特性。此外，算法使用关注点解决方法来解决复杂的子问题，并且使用关注点的关注点解决方法来解决复杂的子问题。</li>
<li>results: 数据实验表明，这个算法在通信和计算效率方面具有优秀的性能，并且可以实现模型训练的稳定性和可靠性。<details>
<summary>Abstract</summary>
Decentralized federated learning (DFL) has gained popularity due to its practicality across various applications. Compared to the centralized version, training a shared model among a large number of nodes in DFL is more challenging, as there is no central server to coordinate the training process. Especially when distributed nodes suffer from limitations in communication or computational resources, DFL will experience extremely inefficient and unstable training. Motivated by these challenges, in this paper, we develop a novel algorithm based on the framework of the inexact alternating direction method (iADM). On one hand, our goal is to train a shared model with a sparsity constraint. This constraint enables us to leverage one-bit compressive sensing (1BCS), allowing transmission of one-bit information among neighbour nodes. On the other hand, communication between neighbour nodes occurs only at certain steps, reducing the number of communication rounds. Therefore, the algorithm exhibits notable communication efficiency. Additionally, as each node selects only a subset of neighbours to participate in the training, the algorithm is robust against stragglers. Additionally, complex items are computed only once for several consecutive steps and subproblems are solved inexactly using closed-form solutions, resulting in high computational efficiency. Finally, numerical experiments showcase the algorithm's effectiveness in both communication and computation.
</details>
<details>
<summary>摘要</summary>
随着各种应用场景的实际需求，分布式联合学习（DFL）在最近几年得到了广泛的关注。与中央服务器协调训练过程相比，在DFL中训练共享模型对一大量节点进行训练是更加困难，因为没有中央服务器可以协调训练过程。尤其是当分布式节点受到通信或计算资源的限制时，DFL会遭遇极其不稳定和不fficient的训练。为了解决这些挑战，在这篇论文中，我们开发了基于不准确的 alternate direction 方法（iADM）的一种新算法。一方面，我们的目标是在共享模型中实现稀疏性约束。这种约束使我们可以利用一位数据压缩感知（1BCS），允许邻居节点之间传输一位信息。另一方面，邻居节点之间的通信只在某些步骤发生，因此算法的通信效率非常高。此外，每个节点只选择一 subset of 邻居节点参与训练，使算法具有抗异常节点（straggler）的性能。此外，复杂的项目只在几个连续步骤中计算一次，并使用关闭式解决方案解决减法问题，导致高效的计算性能。最后，数字实验证明算法在通信和计算方面的效果非常出色。
</details></li>
</ul>
<hr>
<h2 id="What-can-we-learn-from-quantum-convolutional-neural-networks"><a href="#What-can-we-learn-from-quantum-convolutional-neural-networks" class="headerlink" title="What can we learn from quantum convolutional neural networks?"></a>What can we learn from quantum convolutional neural networks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16664">http://arxiv.org/abs/2308.16664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chukwudubem Umeano, Annie E. Paine, Vincent E. Elfving, Oleksandr Kyriienko</li>
<li>for: 这篇论文主要探讨了量子卷积神经网络（QCNN）的应用和性能。</li>
<li>methods: 这篇论文使用了量子卷积神经网络（QCNN）模型，并对其进行了严格的分析和测试。</li>
<li>results: 研究发现，使用QCNN模型可以高效地识别量子阶段，并且可以通过选择合适的基准函数来构建高性能的决策边界。此外，研究还发现，QCNN模型的泛化能力强度取决于嵌入类型，而使用旋转基准函数和快速变化的特征MAP可以提高模型的性能。<details>
<summary>Abstract</summary>
We can learn from analyzing quantum convolutional neural networks (QCNNs) that: 1) working with quantum data can be perceived as embedding physical system parameters through a hidden feature map; 2) their high performance for quantum phase recognition can be attributed to generation of a very suitable basis set during the ground state embedding, where quantum criticality of spin models leads to basis functions with rapidly changing features; 3) pooling layers of QCNNs are responsible for picking those basis functions that can contribute to forming a high-performing decision boundary, and the learning process corresponds to adapting the measurement such that few-qubit operators are mapped to full-register observables; 4) generalization of QCNN models strongly depends on the embedding type, and that rotation-based feature maps with the Fourier basis require careful feature engineering; 5) accuracy and generalization of QCNNs with readout based on a limited number of shots favor the ground state embeddings and associated physics-informed models. We demonstrate these points in simulation, where our results shed light on classification for physical processes, relevant for applications in sensing. Finally, we show that QCNNs with properly chosen ground state embeddings can be used for fluid dynamics problems, expressing shock wave solutions with good generalization and proven trainability.
</details>
<details>
<summary>摘要</summary>
我们可以从分析量子卷积神经网络（QCNN）中学习到以下几点：1. 在处理量子数据时，可以将物理系统的参数嵌入到隐藏特征地图中，并且这个嵌入可以帮助我们获得更好的表现。2. QCNNs 的高性能在量子阶梯识别 зада问题上可以推广到基于物理系统的权威性，这是因为在这些系统中，基于量子扭转的特征函数在嵌入过程中会快速变化。3. QCNNs 中的填充层负责选择适合形成高性能决策边界的基底函数，并且学习过程将量子扭转映射到全域观测器上。4. QCNNs 的对应类型强烈取决于嵌入类型，而且使用Rotation-based特征函数和Fourier基底需要特别的特征工程。5. QCNNs 的精度和通用性强烈取决于嵌入类型和读取方式，而且使用有限次调试的读取方式优先预测类型和物理知识。我们在模拟中证明了这些点，并且显示了这些模型在感测领域中的应用。最后，我们还证明了 QCNNs 以适当的嵌入类型和物理知识可以用于流体动力学问题，表示出对于冲击波解的好通用性和训练可靠性。
</details></li>
</ul>
<hr>
<h2 id="Autoencoder-based-Online-Data-Quality-Monitoring-for-the-CMS-Electromagnetic-Calorimeter"><a href="#Autoencoder-based-Online-Data-Quality-Monitoring-for-the-CMS-Electromagnetic-Calorimeter" class="headerlink" title="Autoencoder-based Online Data Quality Monitoring for the CMS Electromagnetic Calorimeter"></a>Autoencoder-based Online Data Quality Monitoring for the CMS Electromagnetic Calorimeter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16659">http://arxiv.org/abs/2308.16659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhirami Harilal, Kyungmin Park, Michael Andrews, Manfred Paulini</li>
<li>for: 这项研究旨在开发一个基于深度学习的ECAL数据质量监测系统，以快速检测和诊断ECAL仪器中的各种问题，以确保物理数据的质量。</li>
<li>methods: 该研究使用了无监督的深度学习方法，开发了一个在实时中运行的自适应压缩器，以检测ECAL中未seen的异常。</li>
<li>results: 该系统能够有效地检测ECAL中的异常，并保持false discovery rate在$10^{-2}$到$10^{-4}$之间，超过了现有的标准准则。实际中的性能也得到了验证，并在2018和2022年的LHC冲撞数据中探测到了一些隐藏的问题。<details>
<summary>Abstract</summary>
The online Data Quality Monitoring system (DQM) of the CMS electromagnetic calorimeter (ECAL) is a crucial operational tool that allows ECAL experts to quickly identify, localize, and diagnose a broad range of detector issues that would otherwise hinder physics-quality data taking. Although the existing ECAL DQM system has been continuously updated to respond to new problems, it remains one step behind newer and unforeseen issues. Using unsupervised deep learning, a real-time autoencoder-based anomaly detection system is developed that is able to detect ECAL anomalies unseen in past data. After accounting for spatial variations in the response of the ECAL and the temporal evolution of anomalies, the new system is able to efficiently detect anomalies while maintaining an estimated false discovery rate between $10^{-2}$ to $10^{-4}$, beating existing benchmarks by about two orders of magnitude. The real-world performance of the system is validated using anomalies found in 2018 and 2022 LHC collision data. Additionally, first results from deploying the autoencoder-based system in the CMS online DQM workflow for the ECAL barrel during Run 3 of the LHC are presented, showing its promising performance in detecting obscure issues that could have been missed in the existing DQM system.
</details>
<details>
<summary>摘要</summary>
在CMS电磁calorimeter（ECAL）的在线数据质量监测系统（DQM）中，一个重要的运作工具可以帮助ECAL专家快速发现、定位和诊断各种探测器问题，这些问题会否妨碍物理质量数据收集。虽然现有的ECAL DQM系统已经不断更新以应对新的问题，但它仍然一步落后于新的问题。使用无监督深度学习，一个实时自适应器基于异常检测系统被开发出来，可以在ECAL中检测到未在过去数据中出现的异常。在考虑ECAL的响应特性和时间演化的情况下，新系统可以高效地检测异常，并且保持估计的假发现率在10^-2到10^-4之间，超过现有的标准 benchmark by about two orders of magnitude。实际性的性能 Validated using anomalies found in 2018 and 2022 LHC collision data. In addition, the first results of deploying the autoencoder-based system in the CMS online DQM workflow for the ECAL barrel during Run 3 of the LHC are presented, showing its promising performance in detecting obscure issues that could have been missed in the existing DQM system.
</details></li>
</ul>
<hr>
<h2 id="A-Causal-Discovery-Approach-To-Learn-How-Urban-Form-Shapes-Sustainable-Mobility-Across-Continents"><a href="#A-Causal-Discovery-Approach-To-Learn-How-Urban-Form-Shapes-Sustainable-Mobility-Across-Continents" class="headerlink" title="A Causal Discovery Approach To Learn How Urban Form Shapes Sustainable Mobility Across Continents"></a>A Causal Discovery Approach To Learn How Urban Form Shapes Sustainable Mobility Across Continents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16599">http://arxiv.org/abs/2308.16599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Wagner, Florian Nachtigall, Lukas Franken, Nikola Milojevic-Dupont, Rafael H. M. Pereira, Nicolas Koch, Jakob Runge, Marta Gonzalez, Felix Creutzig</li>
<li>for: 本研究旨在提供对城市规划和交通系统低碳化发展的准确指导，通过掌握城市建设环境对旅行的影响关系。</li>
<li>methods: 本研究采用了 causal discovery 和可解释机器学习框架，通过高分辨率移动数据来探索城市形态对城市内旅行的影响关系。</li>
<li>results: 研究发现，城市的距离市中心、人口结构和密度对其他城市形态特征产生间接影响，而城市规划师和城市管理者可以通过了解这些影响关系来做出更加有效的城市规划决策。<details>
<summary>Abstract</summary>
Global sustainability requires low-carbon urban transport systems, shaped by adequate infrastructure, deployment of low-carbon transport modes and shifts in travel behavior. To adequately implement alterations in infrastructure, it's essential to grasp the location-specific cause-and-effect mechanisms that the constructed environment has on travel. Yet, current research falls short in representing causal relationships between the 6D urban form variables and travel, generalizing across different regions, and modeling urban form effects at high spatial resolution. Here, we address all three gaps by utilizing a causal discovery and an explainable machine learning framework to detect urban form effects on intra-city travel based on high-resolution mobility data of six cities across three continents. We show that both distance to city center, demographics and density indirectly affect other urban form features. By considering the causal relationships, we find that location-specific influences align across cities, yet vary in magnitude. In addition, the spread of the city and the coverage of jobs across the city are the strongest determinants of travel-related emissions, highlighting the benefits of compact development and associated benefits. Differences in urban form effects across the cities call for a more holistic definition of 6D measures. Our work is a starting point for location-specific analysis of urban form effects on mobility behavior using causal discovery approaches, which is highly relevant for city planners and municipalities across continents.
</details>
<details>
<summary>摘要</summary>
Here, we address these gaps by using a causal discovery and explainable machine learning framework to detect urban form effects on intra-city travel based on high-resolution mobility data of six cities across three continents. We find that both distance to city center, demographics, and density indirectly affect other urban form features. By considering the causal relationships, we show that location-specific influences align across cities, yet vary in magnitude. Additionally, the spread of the city and the coverage of jobs across the city are the strongest determinants of travel-related emissions, highlighting the benefits of compact development and associated benefits.Differences in urban form effects across cities call for a more holistic definition of 6D measures. Our work is a starting point for location-specific analysis of urban form effects on mobility behavior using causal discovery approaches, which is highly relevant for city planners and municipalities across continents.
</details></li>
</ul>
<hr>
<h2 id="Development-and-validation-of-an-interpretable-machine-learning-based-calculator-for-predicting-5-year-weight-trajectories-after-bariatric-surgery-a-multinational-retrospective-cohort-SOPHIA-study"><a href="#Development-and-validation-of-an-interpretable-machine-learning-based-calculator-for-predicting-5-year-weight-trajectories-after-bariatric-surgery-a-multinational-retrospective-cohort-SOPHIA-study" class="headerlink" title="Development and validation of an interpretable machine learning-based calculator for predicting 5-year weight trajectories after bariatric surgery: a multinational retrospective cohort SOPHIA study"></a>Development and validation of an interpretable machine learning-based calculator for predicting 5-year weight trajectories after bariatric surgery: a multinational retrospective cohort SOPHIA study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16585">http://arxiv.org/abs/2308.16585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Saux, Pierre Bauvin, Violeta Raverdy, Julien Teigny, Hélène Verkindt, Tomy Soumphonphakdy, Maxence Debert, Anne Jacobs, Daan Jacobs, Valerie Monpellier, Phong Ching Lee, Chin Hong Lim, Johanna C Andersson-Assarsson, Lena Carlsson, Per-Arne Svensson, Florence Galtier, Guelareh Dezfoulian, Mihaela Moldovanu, Severine Andrieux, Julien Couster, Marie Lepage, Erminia Lembo, Ornella Verrastro, Maud Robert, Paulina Salminen, Geltrude Mingrone, Ralph Peterli, Ricardo V Cohen, Carlos Zerrweck, David Nocca, Carel W Le Roux, Robert Caiazzo, Philippe Preux, François Pattou</li>
<li>for: 预测个人5年减肥轨迹后operation。</li>
<li>methods: 使用机器学习模型，通过选择变量并构建可读性树来预测个人5年减肥轨迹。</li>
<li>results: 在多国多中心的测试 cohort 中，模型的 median absolute deviation（MAD）和 root mean squared error（RMSE）的Body Mass Index（BMI）值均在2.8kg&#x2F;m${}^2$ 和4.7kg&#x2F;m${}^2$ 之间，mean difference between predicted and observed BMI均为-0.3kg&#x2F;m${}^2$。这个模型已经被 integrate into an easy-to-use and interpretable web-based prediction tool，以帮助在进行前置决策。<details>
<summary>Abstract</summary>
Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (SleevePass [NCT00793143] and SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year followup after Roux-en-Y gastric bypass, sleeve gastrectomy, or gastric band. Patients with a previous history of bariatric surgery or large delays between scheduled and actual visits were excluded. The training cohort comprised patients from two centres in France (ABOS and BAREVAL). The primary outcome was BMI at 5 years. A model was developed using least absolute shrinkage and selection operator to select variables and the classification and regression trees algorithm to build interpretable regression trees. The performances of the model were assessed through the median absolute deviation (MAD) and root mean squared error (RMSE) of BMI. Findings10 231 patients from 12 centres in ten countries were included in the analysis, corresponding to 30 602 patient-years. Among participants in all 12 cohorts, 7701 (75$\bullet$3%) were female, 2530 (24$\bullet$7%) were male. Among 434 baseline attributes available in the training cohort, seven variables were selected: height, weight, intervention type, age, diabetes status, diabetes duration, and smoking status. At 5 years, across external testing cohorts the overall mean MAD BMI was 2$\bullet$8 kg/m${}^2$ (95% CI 2$\bullet$6-3$\bullet$0) and mean RMSE BMI was 4$\bullet$7 kg/m${}^2$ (4$\bullet$4-5$\bullet$0), and the mean difference between predicted and observed BMI was-0$\bullet$3 kg/m${}^2$ (SD 4$\bullet$7). This model is incorporated in an easy to use and interpretable web-based prediction tool to help inform clinical decision before surgery. InterpretationWe developed a machine learning-based model, which is internationally validated, for predicting individual 5-year weight loss trajectories after three common bariatric interventions.
</details>
<details>
<summary>摘要</summary>
背景：肥胖手术后Weight loss trajectories vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery.方法：我们在多国多中心的retrospective observational study中包括了18岁以上成年参与者（包括ABOS [NCT01129297]、BAREVAL [NCT02310178]、瑞典肥胖Subjects研究和荷兰肥胖临床[Nederlandse Obesitas Kliniek]）和两个随机化试验（SleevePass [NCT00793143] 和 SM-BOSS [NCT00356213]），涵盖欧洲、美洲和亚洲，并进行了5年跟踪。排除了前一次肥胖手术的患者或延迟了实际访问的时间。教学组包括了法国两个中心（ABOS和BAREVAL）。结果：10231名患者从12个中心的10个国家被包括在分析中，共计30602名患者-年。参与者中的7701名（75%）是女性，2530名（24%）是男性。在训练组中可以提供的434个基eline attribute中，选择了7个变量：身高、体重、 intervención类型、年龄、 диабе尼状况、 диабе尼持续时间和吸烟状况。在5年后的外部测试组中，总平均MAD BMI为2.8 kg/m${}^2$（95% CI 2.6-3.0），RMSE BMI为4.7 kg/m${}^2$（4.4-5.0），预测与实际BMI之差为-0.3 kg/m${}^2$（SD 4.7）。这个模型已经被 integrating into an easy-to-use and interpretable web-based prediction tool to help inform clinical decision before surgery。 interpret：我们使用机器学习来开发了一个在多国 Validated model for predicting individual 5-year weight loss trajectories after three common bariatric interventions。
</details></li>
</ul>
<hr>
<h2 id="MONDEO-Multistage-Botnet-Detection"><a href="#MONDEO-Multistage-Botnet-Detection" class="headerlink" title="MONDEO: Multistage Botnet Detection"></a>MONDEO: Multistage Botnet Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16570">http://arxiv.org/abs/2308.16570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TLDart/mondeo">https://github.com/TLDart/mondeo</a></li>
<li>paper_authors: Duarte Dias, Bruno Sousa, Nuno Antunes</li>
<li>for: The paper is written to detect DNS-based botnet malware in mobile devices using a lightweight and flexible mechanism called MONDEO.</li>
<li>methods: MONDEO uses four detection stages: Blacklisting&#x2F;Whitelisting, Query rate analysis, DGA analysis, and Machine learning evaluation to identify botnet malware.</li>
<li>results: MONDEO was tested against several datasets and achieved high performance with RandomForest classifiers, making it a useful tool for detecting botnet malware in mobile devices.Here’s the same information in Simplified Chinese text:</li>
<li>for: 该文章是为探测移动设备中的 DNS 基于恶意软件使用轻量级、灵活的机制 MONDEO。</li>
<li>methods: MONDEO 使用四个检测阶段：黑名单&#x2F;白名单、查询速率分析、 DGA 分析和机器学习评估来识别恶意软件。</li>
<li>results: MONDEO 在多个数据集上进行测试，并 achieved 高性能使用 RandomForest 分类器，使其成为移动设备中探测恶意软件的有用工具。<details>
<summary>Abstract</summary>
Mobile devices have widespread to become the most used piece of technology. Due to their characteristics, they have become major targets for botnet-related malware. FluBot is one example of botnet malware that infects mobile devices. In particular, FluBot is a DNS-based botnet that uses Domain Generation Algorithms (DGA) to establish communication with the Command and Control Server (C2). MONDEO is a multistage mechanism with a flexible design to detect DNS-based botnet malware. MONDEO is lightweight and can be deployed without requiring the deployment of software, agents, or configuration in mobile devices, allowing easy integration in core networks. MONDEO comprises four detection stages: Blacklisting/Whitelisting, Query rate analysis, DGA analysis, and Machine learning evaluation. It was created with the goal of processing streams of packets to identify attacks with high efficiency, in the distinct phases. MONDEO was tested against several datasets to measure its efficiency and performance, being able to achieve high performance with RandomForest classifiers. The implementation is available at github.
</details>
<details>
<summary>摘要</summary>
mobile devices已经广泛普及，成为现代技术中最受欢迎的一种。由于它们的特点，它们成为了主要的针对 botnet 恶意软件的目标。fluBot是一种 DNS 基于的 botnet 恶意软件，通过Domain Generation Algorithms（DGA）与命令和控制服务器（C2）进行通信。MONDEO是一种多stage机制，具有灵活的设计，用于探测 DNS 基于的 botnet 恶意软件。MONDEO 轻量级，不需要在移动设备中部署软件、代理或配置，可以方便地集成到核心网络中。MONDEO 包括四个检测阶段：黑名单/白名单、查询率分析、DGA 分析和机器学习评估。它的目标是处理流量包来确定攻击，高效率地完成任务。MONDEO 在多个数据集上进行测试，并能够达到高效率的RandomForest 分类器。实现可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Forecasting-Emergency-Department-Crowding-with-Advanced-Machine-Learning-Models-and-Multivariable-Input"><a href="#Forecasting-Emergency-Department-Crowding-with-Advanced-Machine-Learning-Models-and-Multivariable-Input" class="headerlink" title="Forecasting Emergency Department Crowding with Advanced Machine Learning Models and Multivariable Input"></a>Forecasting Emergency Department Crowding with Advanced Machine Learning Models and Multivariable Input</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16544">http://arxiv.org/abs/2308.16544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jalmari Tuominen, Eetu Pulkkinen, Jaakko Peltonen, Juho Kanniainen, Niku Oksala, Ari Palomäki, Antti Roine</li>
<li>for: 预测急诊室（ED）填满情况，以提高患者安全性和健康结果。</li>
<li>methods: 使用先进的机器学习模型（ML）预测ED填满情况24小时前，并使用电子健康记录数据、床位数据、交通数据和天气变量等多变量输入。</li>
<li>results: N-BEATS和LightGBM模型在比较准确性方面表现出色，与统计学 benchmark 相比提高11%和9%；DeepAR模型预测下一天拥挤情况的ROC曲线为0.76（95% CI 0.69-0.84）。<details>
<summary>Abstract</summary>
Emergency department (ED) crowding is a significant threat to patient safety and it has been repeatedly associated with increased mortality. Forecasting future service demand has the potential patient outcomes. Despite active research on the subject, several gaps remain: 1) proposed forecasting models have become outdated due to quick influx of advanced machine learning models (ML), 2) amount of multivariable input data has been limited and 3) discrete performance metrics have been rarely reported. In this study, we document the performance of a set of advanced ML models in forecasting ED occupancy 24 hours ahead. We use electronic health record data from a large, combined ED with an extensive set of explanatory variables, including the availability of beds in catchment area hospitals, traffic data from local observation stations, weather variables, etc. We show that N-BEATS and LightGBM outpeform benchmarks with 11 % and 9 % respective improvements and that DeepAR predicts next day crowding with an AUC of 0.76 (95 % CI 0.69-0.84). To the best of our knowledge, this is the first study to document the superiority of LightGBM and N-BEATS over statistical benchmarks in the context of ED forecasting.
</details>
<details>
<summary>摘要</summary>
急诊室拥堵是一种严重的 patient safety 问题，已经被重复地与增加 mortality 相关。预测未来服务需求有可能改善 patient outcomes。Despite 多年的研究，还有几个空白：1）提议的预测模型已经因为快速的机器学习模型（ML）的涌入而过时，2）数据的多变量输入受限，3）绝对性表现指标很少被报道。在这种研究中，我们记录了一些高级 ML 模型在预测急诊室占用 24 小时前的表现。我们使用了一个大型、集成的急诊室数据，包括抢救区域医院床位可用性、当地观测站交通数据、天气变量等多个说服变量。我们发现，N-BEATS 和 LightGBM 在比较均匀的情况下表现出了11%和9%的提升，而 DeepAR 预测下一天拥堵的 AUC 为 0.76（95% CI 0.69-0.84）。据我们所知，这是第一个证明 LightGBM 和 N-BEATS 在急诊室预测中超过统计标准的研究。
</details></li>
</ul>
<hr>
<h2 id="Scalable-Incomplete-Multi-View-Clustering-with-Structure-Alignment"><a href="#Scalable-Incomplete-Multi-View-Clustering-with-Structure-Alignment" class="headerlink" title="Scalable Incomplete Multi-View Clustering with Structure Alignment"></a>Scalable Incomplete Multi-View Clustering with Structure Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16541">http://arxiv.org/abs/2308.16541</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wy1019/simvc-sa">https://github.com/wy1019/simvc-sa</a></li>
<li>paper_authors: Yi Wen, Siwei Wang, Ke Liang, Weixuan Liang, Xinhang Wan, Xinwang Liu, Suyuan Liu, Jiyuan Liu, En Zhu</li>
<li>for: This paper focuses on the problem of incomplete multi-view clustering (IMVC) and proposes a novel incomplete anchor graph learning framework called Scalable Incomplete Multi-View Clustering with Structure Alignment (SIMVC-SA) to tackle the issues of inter-view discrepancy and anchor misalignment.</li>
<li>methods: The proposed method constructs view-specific anchor graphs to capture complementary information from different views, and aligns the cross-view anchor correspondence using a novel structure alignment module. The anchor graph construction and alignment are jointly optimized in the unified framework to enhance clustering quality.</li>
<li>results: Extensive experiments on seven incomplete benchmark datasets demonstrate the effectiveness and efficiency of the proposed method, with linear time and space complexity correlated with the number of samples. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/wy1019/SIMVC-SA">https://github.com/wy1019/SIMVC-SA</a>.<details>
<summary>Abstract</summary>
The success of existing multi-view clustering (MVC) relies on the assumption that all views are complete. However, samples are usually partially available due to data corruption or sensor malfunction, which raises the research of incomplete multi-view clustering (IMVC). Although several anchor-based IMVC methods have been proposed to process the large-scale incomplete data, they still suffer from the following drawbacks: i) Most existing approaches neglect the inter-view discrepancy and enforce cross-view representation to be consistent, which would corrupt the representation capability of the model; ii) Due to the samples disparity between different views, the learned anchor might be misaligned, which we referred as the Anchor-Unaligned Problem for Incomplete data (AUP-ID). Such the AUP-ID would cause inaccurate graph fusion and degrades clustering performance. To tackle these issues, we propose a novel incomplete anchor graph learning framework termed Scalable Incomplete Multi-View Clustering with Structure Alignment (SIMVC-SA). Specially, we construct the view-specific anchor graph to capture the complementary information from different views. In order to solve the AUP-ID, we propose a novel structure alignment module to refine the cross-view anchor correspondence. Meanwhile, the anchor graph construction and alignment are jointly optimized in our unified framework to enhance clustering quality. Through anchor graph construction instead of full graphs, the time and space complexity of the proposed SIMVC-SA is proven to be linearly correlated with the number of samples. Extensive experiments on seven incomplete benchmark datasets demonstrate the effectiveness and efficiency of our proposed method. Our code is publicly available at https://github.com/wy1019/SIMVC-SA.
</details>
<details>
<summary>摘要</summary>
成功的多视图划分（MVC）取决于所有视图都是完整的，但是样本通常只有部分可用，这引起了划分不完整的多视图划分（IMVC）的研究。虽然一些基于锚点的IMVC方法已经提出，但它们仍然受到以下缺点的影响：一、大多数现有方法忽视了视图之间的差异，强制跨视图表示保持一致，这会让模型的表示能力受损；二、由于不同视图中的样本差异，学习的锚点可能会偏移，我们称之为缺失锚点问题（AUP-ID）。这种AUP-ID会导致不正确的图融合和下降划分性能。为解决这些问题，我们提出了一种基于缺失锚点的新型多视图划分框架，名为扩展可靠多视图划分with结构对齐（SIMVC-SA）。我们特别是建立视图特定的锚点图来捕捉不同视图中的补做信息。为解决AUP-ID，我们提出了一种新的结构对齐模块，以修正跨视图锚点对应关系。同时，锚点图建构和对齐在我们的统一框架中同步优化，以提高划分质量。通过锚点图建构而不是全图建构，我们提出的SIMVC-SA的时间和空间复杂度被证明为与样本数量直接相关。我们的代码可以在https://github.com/wy1019/SIMVC-SA上获取。Extensive experiments on seven incomplete benchmark datasets demonstrate the effectiveness and efficiency of our proposed method. Our code is publicly available at https://github.com/wy1019/SIMVC-SA.
</details></li>
</ul>
<hr>
<h2 id="Echocardiographic-View-Classification-with-Integrated-Out-of-Distribution-Detection-for-Enhanced-Automatic-Echocardiographic-Analysis"><a href="#Echocardiographic-View-Classification-with-Integrated-Out-of-Distribution-Detection-for-Enhanced-Automatic-Echocardiographic-Analysis" class="headerlink" title="Echocardiographic View Classification with Integrated Out-of-Distribution Detection for Enhanced Automatic Echocardiographic Analysis"></a>Echocardiographic View Classification with Integrated Out-of-Distribution Detection for Enhanced Automatic Echocardiographic Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16483">http://arxiv.org/abs/2308.16483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaeik Jeon, Seongmin Ha, Yeonyee E. Yoon, Jiyeon Kim, Hyunseok Jeong, Dawun Jeong, Yeonggul Jang, Youngtaek Hong, Hyuk-Jae Chang</li>
<li>for: 这份研究旨在提高自动echocardiography分类的精度和可靠性，以便在诊断和评估心脏病的过程中帮助医生。</li>
<li>methods: 这篇研究使用了深度学习的方法，包括31种标本分类和对于没有分布（OOD）的检测。</li>
<li>results: 研究结果显示，ECHO-VICODE可以实现高精度和可靠性的心脏病分类，并且能够有效地检测没有分布的检测结果。<details>
<summary>Abstract</summary>
In the rapidly evolving field of automatic echocardiographic analysis and interpretation, automatic view classification is a critical yet challenging task, owing to the inherent complexity and variability of echocardiographic data. This study presents ECHOcardiography VIew Classification with Out-of-Distribution dEtection (ECHO-VICODE), a novel deep learning-based framework that effectively addresses this challenge by training to classify 31 classes, surpassing previous studies and demonstrating its capacity to handle a wide range of echocardiographic views. Furthermore, ECHO-VICODE incorporates an integrated out-of-distribution (OOD) detection function, leveraging the relative Mahalanobis distance to effectively identify 'near-OOD' instances commonly encountered in echocardiographic data. Through extensive experimentation, we demonstrated the outstanding performance of ECHO-VICODE in terms of view classification and OOD detection, significantly reducing the potential for errors in echocardiographic analyses. This pioneering study significantly advances the domain of automated echocardiography analysis and exhibits promising prospects for substantial applications in extensive clinical research and practice.
</details>
<details>
<summary>摘要</summary>
在自动echocardiographic分析和解释领域中，自动视类别是一项挑战性的任务，主要因为echocardiographic数据的内在复杂性和变化性。本研究提出了ECHOcardiography View Classification with Out-of-Distribution Detection（ECHO-VICODE），一种深度学习基础的框架，能够有效地解决这个挑战。ECHO-VICODE通过训练31个类别，超过了先前的研究，并证明了其能够处理广泛的echocardiographic视图。此外，ECHO-VICODE还包括内置的out-of-distribution（OOD）检测功能，利用相对的Mahalanobis距离有效地标识echocardiographic数据中的“近OOD”实例。经过广泛的实验，我们证明了ECHO-VICODE在视图类别和OOD检测方面的出色性能，明显减少了echocardiographic分析中的可能的错误。这项创新的研究在自动echocardiography分析领域中具有先驱性，展现出了广泛的临床研究和实践应用的潜在前景。
</details></li>
</ul>
<hr>
<h2 id="A-Policy-Adaptation-Method-for-Implicit-Multitask-Reinforcement-Learning-Problems"><a href="#A-Policy-Adaptation-Method-for-Implicit-Multitask-Reinforcement-Learning-Problems" class="headerlink" title="A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems"></a>A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16471">http://arxiv.org/abs/2308.16471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satoshi Yamamori, Jun Morimoto</li>
<li>for: 这个研究旨在应对动态运动生成任务中的聚合和碰撞，小的政策参数变化可能导致应力的完全不同结果。例如，在足球游戏中，几分之一的对策变化可以导致球会飞行在完全不同的方向上，但是不需要完全不同的技能。</li>
<li>methods: 这个研究使用了多任务强化学习算法来适应单一运动类别中的不同目标或环境，以及不同的奖励函数或物理环境的 Parameters。</li>
<li>results: 研究结果显示，提案的方法可以适应不同的目标位置或球的弹性系数的隐藏变化，而标准的预像随机化方法则无法处理不同的任务设定。<details>
<summary>Abstract</summary>
In dynamic motion generation tasks, including contact and collisions, small changes in policy parameters can lead to extremely different returns. For example, in soccer, the ball can fly in completely different directions with a similar heading motion by slightly changing the hitting position or the force applied to the ball or when the friction of the ball varies. However, it is difficult to imagine that completely different skills are needed for heading a ball in different directions. In this study, we proposed a multitask reinforcement learning algorithm for adapting a policy to implicit changes in goals or environments in a single motion category with different reward functions or physical parameters of the environment. We evaluated the proposed method on the ball heading task using a monopod robot model. The results showed that the proposed method can adapt to implicit changes in the goal positions or the coefficients of restitution of the ball, whereas the standard domain randomization approach cannot cope with different task settings.
</details>
<details>
<summary>摘要</summary>
在动态动作生成任务中，包括 contacts 和碰撞，小型政策参数变化可以导致极其不同的返回。例如，在足球中，通过些微改变球头位置或发球力量，球可以飞向完全不同的方向，但是不需要完全不同的技能。在本研究中，我们提出了一种多任务强化学习算法，用于适应单个动作类别中的不同目标或环境中的隐式变化。我们使用一个单脚机器人模型进行评估。结果表明，我们的方法可以适应不同的目标位置或球的归退率，而标准领域随机化方法无法处理不同的任务设定。
</details></li>
</ul>
<hr>
<h2 id="Domain-adaptive-Message-Passing-Graph-Neural-Network"><a href="#Domain-adaptive-Message-Passing-Graph-Neural-Network" class="headerlink" title="Domain-adaptive Message Passing Graph Neural Network"></a>Domain-adaptive Message Passing Graph Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16470">http://arxiv.org/abs/2308.16470</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shenxiaocam/dm_gnn">https://github.com/shenxiaocam/dm_gnn</a></li>
<li>paper_authors: Xiao Shen, Shirui Pan, Kup-Sze Choi, Xi Zhou</li>
<li>for: 本研究旨在Addressing the challenge of cross-network node classification (CNNC), which involves classifying nodes in a label-deficient target network using the knowledge from a source network with abundant labels.</li>
<li>methods: 本研究提出了一种域 adaptive message passing graph neural network (DM-GNN), which integrates graph neural network (GNN) with conditional adversarial domain adaptation. DM-GNN 可以学习具有识别力的表示，同时也可以在不同网络之间进行转移学习。</li>
<li>results: 对于 eleven state-of-the-art methods, 本研究的 DM-GNN 显示出更高的效果，能够更好地匹配类别分布 across networks.<details>
<summary>Abstract</summary>
Cross-network node classification (CNNC), which aims to classify nodes in a label-deficient target network by transferring the knowledge from a source network with abundant labels, draws increasing attention recently. To address CNNC, we propose a domain-adaptive message passing graph neural network (DM-GNN), which integrates graph neural network (GNN) with conditional adversarial domain adaptation. DM-GNN is capable of learning informative representations for node classification that are also transferrable across networks. Firstly, a GNN encoder is constructed by dual feature extractors to separate ego-embedding learning from neighbor-embedding learning so as to jointly capture commonality and discrimination between connected nodes. Secondly, a label propagation node classifier is proposed to refine each node's label prediction by combining its own prediction and its neighbors' prediction. In addition, a label-aware propagation scheme is devised for the labeled source network to promote intra-class propagation while avoiding inter-class propagation, thus yielding label-discriminative source embeddings. Thirdly, conditional adversarial domain adaptation is performed to take the neighborhood-refined class-label information into account during adversarial domain adaptation, so that the class-conditional distributions across networks can be better matched. Comparisons with eleven state-of-the-art methods demonstrate the effectiveness of the proposed DM-GNN.
</details>
<details>
<summary>摘要</summary>
The proposed DM-GNN consists of three main components:1. GNN encoder: A GNN encoder is constructed using dual feature extractors to separate ego-embedding learning from neighbor-embedding learning. This allows the model to jointly capture commonality and discrimination between connected nodes.2. Label propagation node classifier: A label propagation node classifier is proposed to refine each node's label prediction by combining its own prediction and its neighbors' prediction.3. Conditional adversarial domain adaptation: Conditional adversarial domain adaptation is performed to take the neighborhood-refined class-label information into account during adversarial domain adaptation, allowing the class-conditional distributions across networks to be better matched.The proposed DM-GNN is evaluated using eleven state-of-the-art methods, and the results demonstrate its effectiveness in node classification tasks.In simplified Chinese, the text can be translated as: crossed-network node classification (CNNC) 是一种将知识从有 labels 的源网络转移到无 labels 的目标网络中进行分类的技术，在最近引起了越来越多的注意。为了解决 CNNC，我们提出了一个域 adapted 的讯息传递图 neural network (DM-GNN)，它结合了图 neural network (GNN) 和 conditional adversarial domain adaptation。DM-GNN 可以学习对网络分类的 informative 表现，并且可以跨网络传递。DM-GNN 的主要Component包括：1. GNN  Encoder：使用 dual feature extractors 将 ego-embedding 学习和 neighbor-embedding 学习分开，以便同时捕捉网络中连接的node之间的共同性和分别性。2. Label Propagation Node Classifier：提出了一个 label propagation 节点分类器，可以透过节点的自己预测和邻居预测来优化节点的类别预测。3. Conditional Adversarial Domain Adaptation：在 conditional adversarial domain adaptation 中，使用节点预测的类别信息来对网络进行域对应，以便更好地匹配网络间的类别分布。DM-GNN 在 eleven state-of-the-art 方法中进行评估，结果显示了它的效果。
</details></li>
</ul>
<hr>
<h2 id="Computing-excited-states-of-molecules-using-normalizing-flows"><a href="#Computing-excited-states-of-molecules-using-normalizing-flows" class="headerlink" title="Computing excited states of molecules using normalizing flows"></a>Computing excited states of molecules using normalizing flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16468">http://arxiv.org/abs/2308.16468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahya Saleh, Álvaro Fernández Corral, Armin Iske, Jochen Küpper, Andrey Yachmenev</li>
<li>for: 用于计算量子系统的ground和 excited状态</li>
<li>methods: 使用线性 span基函数的拟合方法，通过组合normalizing flows进行优化</li>
<li>results: 在计算三原料H$_2$S分子的很多振荡态和一些电子状态的诸如氢原子、分子氢离子和碳原子等一元电子系统中，达到了更高的精度和加速基aset快速整合。<details>
<summary>Abstract</summary>
We present a new nonlinear variational framework for simultaneously computing ground and excited states of quantum systems. Our approach is based on approximating wavefunctions in the linear span of basis functions that are augmented and optimized \emph{via} composition with normalizing flows. The accuracy and efficiency of our approach are demonstrated in the calculations of a large number of vibrational states of the triatomic H$_2$S molecule as well as ground and several excited electronic states of prototypical one-electron systems including the hydrogen atom, the molecular hydrogen ion, and a carbon atom in a single-active-electron approximation. The results demonstrate significant improvements in the accuracy of energy predictions and accelerated basis-set convergence even when using normalizing flows with a small number of parameters. The present approach can be also seen as the optimization of a set of intrinsic coordinates that best capture the underlying physics within the given basis set.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的非线性变分方法，用于同时计算量子系统的基态和激发态。我们的方法基于使用线性span中的基函数，并通过组合normalizing flow进行优化和增强。我们在计算了大量的振荡态状态的H2S分子以及电子系统的基态和一些激发态的许多例子中，显示了我们的方法的精度和效率。结果表明，使用normalizing flow的少量参数可以大幅提高基准集合的准确性和基准集合的快速收敛。此外，我们的方法也可以看作是在给定基准集合中优化一组内在坐标，以最好捕捉下面物理的内在物理。
</details></li>
</ul>
<hr>
<h2 id="Least-Squares-Maximum-and-Weighted-Generalization-Memorization-Machines"><a href="#Least-Squares-Maximum-and-Weighted-Generalization-Memorization-Machines" class="headerlink" title="Least Squares Maximum and Weighted Generalization-Memorization Machines"></a>Least Squares Maximum and Weighted Generalization-Memorization Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16456">http://arxiv.org/abs/2308.16456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Wang, Zhen Wang, Yuan-Hai Shao</li>
<li>for: 这个论文提出了一种新的记忆机制，用于改进支持向量机器学习（LSSVM）模型。这种机制可以准确分区训练集而不导致过拟合。</li>
<li>methods: 该论文提出了两种记忆影响模型（MIMM和WIMM），以及一些不同的记忆影响函数。这些模型可以降解到LSSVM模型中。</li>
<li>results: 实验结果表明，我们的MIMM和WIMM模型在总体性能和时间成本方面都有优势，比LSSVM和其他记忆模型更好。<details>
<summary>Abstract</summary>
In this paper, we propose a new way of remembering by introducing a memory influence mechanism for the least squares support vector machine (LSSVM). Without changing the equation constraints of the original LSSVM, this mechanism, allows an accurate partitioning of the training set without overfitting. The maximum memory impact model (MIMM) and the weighted impact memory model (WIMM) are then proposed. It is demonstrated that these models can be degraded to the LSSVM. Furthermore, we propose some different memory impact functions for the MIMM and WIMM. The experimental results show that that our MIMM and WIMM have better generalization performance compared to the LSSVM and significant advantage in time cost compared to other memory models.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的记忆机制，用于改进最小二乘支持向量机（LSSVM）的性能。无需更改原始LSSVM的方程约束，这种机制可以准确地分区训练集而不导致过拟合。然后，我们提出了最大记忆影响模型（MIMM）和权重记忆影响模型（WIMM）。我们还提出了一些不同的记忆影响函数 для MIMM 和 WIMM。实验结果表明，我们的 MIMM 和 WIMM 在泛化性能方面表现更好，而且在时间成本方面具有显著的优势 compared to 其他记忆模型。
</details></li>
</ul>
<hr>
<h2 id="Listen-to-Minority-Encrypted-Traffic-Classification-for-Class-Imbalance-with-Contrastive-Pre-Training"><a href="#Listen-to-Minority-Encrypted-Traffic-Classification-for-Class-Imbalance-with-Contrastive-Pre-Training" class="headerlink" title="Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training"></a>Listen to Minority: Encrypted Traffic Classification for Class Imbalance with Contrastive Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16453">http://arxiv.org/abs/2308.16453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Juncheng Guo, Qige Song, Jiang Xie, Yafei Sang, Shuyuan Zhao, Yongzheng Zhang</li>
<li>for: 本研究旨在提出一种新的隐私传输分类（ETC）框架，以便在移动互联网环境中有效地管理 encrypted traffic。</li>
<li>methods: 本文提出了一种新的 Pre-trAining Semi-Supervised ETC 框架（PASS），具有以下三个关键组成部分：1）对原始训练集进行采样和对比性预训练，以避免类别偏袋问题；2）使用半导引优化策略，以利用大量无标示流量数据并减轻人工标注工作负担；3）采用强化的循环优化策略，以提高 ETC 方法的泛化能力。</li>
<li>results: 对四个公共数据集进行比较，PASS 方法的性能比 state-of-the-art ETC 方法和普通采样方法更高，特别是在面临类别偏袋和流量同质化问题时。PASS 方法可以适应不同的特征提取器，并且可以在不同的网络环境下进行高效地隐私传输分类。<details>
<summary>Abstract</summary>
Mobile Internet has profoundly reshaped modern lifestyles in various aspects. Encrypted Traffic Classification (ETC) naturally plays a crucial role in managing mobile Internet, especially with the explosive growth of mobile apps using encrypted communication. Despite some existing learning-based ETC methods showing promising results, three-fold limitations still remain in real-world network environments, 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping homogeneous traffic by pulling positive traffic pairs closer and pushing negative pairs away. Meanwhile, PASS designs a semi-supervised optimization strategy based on pseudo-label iteration and dynamic loss weighting algorithms in order to effectively utilize massive unlabeled traffic data and alleviate manual train dataset annotation workload. PASS outperforms state-of-the-art ETC methods and generic sampling approaches on four public datasets with significant class imbalance and traffic homogeneity, remarkably pushing the F1 of Cross-Platform215 with 1.31%, ISCX-17 with 9.12%. Furthermore, we validate the generality of the contrastive pre-training and pseudo-label iteration components of PASS, which can adaptively benefit ETC methods with diverse feature extractors.
</details>
<details>
<summary>摘要</summary>
Mobile 互联网已经深深影响现代生活方式，Encrypted Traffic Classification（ETC）在移动互联网管理中扮演着关键角色，尤其是在移动应用程序使用加密通信的情况下。despite some existing learning-based ETC methods showing promising results, there are still three-fold limitations in real-world network environments, including 1) label bias caused by traffic class imbalance, 2) traffic homogeneity caused by component sharing, and 3) training with reliance on sufficient labeled traffic. None of the existing ETC methods can address all these limitations. In this paper, we propose a novel Pre-trAining Semi-Supervised ETC framework, dubbed PASS. Our key insight is to resample the original train dataset and perform contrastive pre-training without using individual app labels directly to avoid label bias issues caused by class imbalance, while obtaining a robust feature representation to differentiate overlapping homogeneous traffic by pulling positive traffic pairs closer and pushing negative pairs away. Meanwhile, PASS designs a semi-supervised optimization strategy based on pseudo-label iteration and dynamic loss weighting algorithms in order to effectively utilize massive unlabeled traffic data and alleviate manual train dataset annotation workload. PASS outperforms state-of-the-art ETC methods and generic sampling approaches on four public datasets with significant class imbalance and traffic homogeneity, remarkably pushing the F1 of Cross-Platform215 with 1.31%, ISCX-17 with 9.12%. Furthermore, we validate the generality of the contrastive pre-training and pseudo-label iteration components of PASS, which can adaptively benefit ETC methods with diverse feature extractors.
</details></li>
</ul>
<hr>
<h2 id="AntM-2-C-A-Large-Scale-Dataset-For-Multi-Scenario-Multi-Modal-CTR-Prediction"><a href="#AntM-2-C-A-Large-Scale-Dataset-For-Multi-Scenario-Multi-Modal-CTR-Prediction" class="headerlink" title="AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction"></a>AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16437">http://arxiv.org/abs/2308.16437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoxin Huan, Ke Ding, Ang Li, Xiaolu Zhang, Xu Min, Yong He, Liang Zhang, Jun Zhou, Linjian Mo, Jinjie Gu, Zhongyi Liu, Wenliang Zhong, Guannan Zhang<br>for:The paper is written for proposing a new CTR dataset, AntM$^{2}$C, to address the limitations of existing CTR datasets.methods:The paper uses a multi-scenario multi-modal approach, including 200 million users and 6 million items, to provide a more comprehensive understanding of user preferences. The dataset includes 200 features, such as ID-based features, raw text features, and image features.results:The paper provides comparisons with baseline methods on several typical CTR tasks based on the AntM$^{2}$C dataset, which is currently the largest-scale CTR dataset available.<details>
<summary>Abstract</summary>
Click-through rate (CTR) prediction is a crucial issue in recommendation systems. There has been an emergence of various public CTR datasets. However, existing datasets primarily suffer from the following limitations. Firstly, users generally click different types of items from multiple scenarios, and modeling from multiple scenarios can provide a more comprehensive understanding of users. Existing datasets only include data for the same type of items from a single scenario. Secondly, multi-modal features are essential in multi-scenario prediction as they address the issue of inconsistent ID encoding between different scenarios. The existing datasets are based on ID features and lack multi-modal features. Third, a large-scale dataset can provide a more reliable evaluation of models, fully reflecting the performance differences between models. The scale of existing datasets is around 100 million, which is relatively small compared to the real-world CTR prediction. To address these limitations, we propose AntM$^{2}$C, a Multi-Scenario Multi-Modal CTR dataset based on industrial data from Alipay. Specifically, AntM$^{2}$C provides the following advantages: 1) It covers CTR data of 5 different types of items, providing insights into the preferences of users for different items, including advertisements, vouchers, mini-programs, contents, and videos. 2) Apart from ID-based features, AntM$^{2}$C also provides 2 multi-modal features, raw text and image features, which can effectively establish connections between items with different IDs. 3) AntM$^{2}$C provides 1 billion CTR data with 200 features, including 200 million users and 6 million items. It is currently the largest-scale CTR dataset available. Based on AntM$^{2}$C, we construct several typical CTR tasks and provide comparisons with baseline methods. The dataset homepage is available at https://www.atecup.cn/home.
</details>
<details>
<summary>摘要</summary>
Click-through rate (CTR) 预测是推荐系统中的关键问题。随着不同场景的数据的出现，当前的 dataset 受到以下一些限制：首先，用户通常从多个场景中点击不同类型的项目，模型需要从多个场景中学习，以提供更全面的用户偏好。现有 dataset 仅包含同一类型的项目的数据，来自单一场景。其次，多 modal 特征是多场景预测中非常重要的，它们可以解决不同 ID 编码之间的不一致问题。现有 dataset 基于 ID 特征，缺乏多 modal 特征。第三，大规模 dataset 可以提供更可靠的评估，全面反映模型之间的性能差异。现有 dataset 的规模约为 100 万，相比实际 CTR 预测中更小。为解决这些限制，我们提出 AntM$^{2}$C，一个基于互联网数据的多场景多modal CTR dataset。具体来说，AntM$^{2}$C 提供了以下优势：1. 覆盖 5 种不同类型的项目的 CTR 数据，为用户偏好的研究提供了新的视角。2. 除 ID 特征之外，AntM$^{2}$C 还提供了 2 个多 modal 特征，包括原始文本和图像特征，可以有效建立不同 ID 之间的连接。3. AntM$^{2}$C 提供了 1 亿 CTR 数据，包括 200 万用户和 6 万个项目。它是目前已知最大规模的 CTR dataset。基于 AntM$^{2}$C，我们构建了一些典型的 CTR 任务，并与基准方法进行比较。 dataset 的主页可以在 <https://www.atecup.cn/home> 中找到。
</details></li>
</ul>
<hr>
<h2 id="On-the-Equivalence-between-Implicit-and-Explicit-Neural-Networks-A-High-dimensional-Viewpoint"><a href="#On-the-Equivalence-between-Implicit-and-Explicit-Neural-Networks-A-High-dimensional-Viewpoint" class="headerlink" title="On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint"></a>On the Equivalence between Implicit and Explicit Neural Networks: A High-dimensional Viewpoint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16425">http://arxiv.org/abs/2308.16425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zenan Ling, Zhenyu Liao, Robert C. Qiu</li>
<li>for: 研究高维隐式神经网络的性质和特点。</li>
<li>methods: 提供高维隐式神经网络的 conjugate kernels 和 neural tangent kernels 的等价性。</li>
<li>results: 在高维情况下，隐式和显式神经网络之间存在等价关系。In English, this translates to:</li>
<li>for: Investigating the properties and characteristics of high-dimensional implicit neural networks.</li>
<li>methods: Providing the high-dimensional equivalents for the corresponding conjugate kernels and neural tangent kernels.</li>
<li>results: Demonstrating the equivalence between implicit and explicit networks in high dimensions.<details>
<summary>Abstract</summary>
Implicit neural networks have demonstrated remarkable success in various tasks. However, there is a lack of theoretical analysis of the connections and differences between implicit and explicit networks. In this paper, we study high-dimensional implicit neural networks and provide the high dimensional equivalents for the corresponding conjugate kernels and neural tangent kernels. Built upon this, we establish the equivalence between implicit and explicit networks in high dimensions.
</details>
<details>
<summary>摘要</summary>
高维隐藏神经网络（Implicit Neural Networks）在各种任务中表现出色，但是对于隐藏和显式网络之间的连接和差异的理论分析尚缺乏。本文研究高维隐藏神经网络，并提供高维等价的拟合kernel和神经折射kernel。基于这些结果，我们证明高维隐藏神经网络和显式神经网络在高维空间中是等价的。
</details></li>
</ul>
<hr>
<h2 id="DECODE-DilatEd-COnvolutional-neural-network-for-Detecting-Extreme-mass-ratio-inspirals"><a href="#DECODE-DilatEd-COnvolutional-neural-network-for-Detecting-Extreme-mass-ratio-inspirals" class="headerlink" title="DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals"></a>DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16422">http://arxiv.org/abs/2308.16422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Zhao, Yue Zhou, Ruijun Shi, Zhoujian Cao, Zhixiang Ren</li>
<li>for: 探测EXTREME MASS RATIO INSPIRALS (EMRIs) 的检测具有复杂的波形、长时间和低信号响应率 (SNR)，因此比单簇binary coalescences更加困难。</li>
<li>methods: 我们介绍了一种名为DECODE的端到端模型，该模型通过频域序列模型来检测EMRI信号。DECODE的核心是一个扩展的 causal convolutional neural network，通过训练使用 TDI-1.5 探测器响应来训练。</li>
<li>results: 我们对1年的多通道 TDI 数据进行了评估，并取得了一个真正正确率为 96.3%，假阳性率为 1%，并且每个检测只需0.01秒钟的计算时间。我们还通过三个示例 EMRI 信号的可见化来证明DECODE 的强大潜力。<details>
<summary>Abstract</summary>
The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to their complex waveforms, extended duration, and low signal-to-noise ratio (SNR), making them more challenging to be identified compared to compact binary coalescences. While matched filtering-based techniques are known for their computational demands, existing deep learning-based methods primarily handle time-domain data and are often constrained by data duration and SNR. In addition, most existing work ignores time-delay interferometry (TDI) and applies the long-wavelength approximation in detector response calculations, thus limiting their ability to handle laser frequency noise. In this study, we introduce DECODE, an end-to-end model focusing on EMRI signal detection by sequence modeling in the frequency domain. Centered around a dilated causal convolutional neural network, trained on synthetic data considering TDI-1.5 detector response, DECODE can efficiently process a year's worth of multichannel TDI data with an SNR of around 50. We evaluate our model on 1-year data with accumulated SNR ranging from 50 to 120 and achieve a true positive rate of 96.3% at a false positive rate of 1%, keeping an inference time of less than 0.01 seconds. With the visualization of three showcased EMRI signals for interpretability and generalization, DECODE exhibits strong potential for future space-based gravitational wave data analyses.
</details>
<details>
<summary>摘要</summary>
<<SYS>Translate into Simplified Chinese.</SYS>探测Extreme Mass Ratio Inspirals（EMRI）的过程非常复杂，因为它们的复杂波形、长时间 duration和低信号响应率（SNR），使其与固体二体合并更加棘手。尽管匹配滤波器基本技术需要大量计算资源，现有的深度学习基于方法主要处理时间频谱数据，并且通常受到数据持续时间和SNR的限制。此外，大多数现有工作忽略了时间延迟折射（TDI）和探测器响应计算中的长波长approximation，因此限制了它们对激光频率噪声的处理能力。在本研究中，我们介绍DECODE，一种集中在顺序模型频率域的端到端模型，用于EMRI信号检测。 DECODE中心在填充 causal 卷积神经网络，通过对合成数据进行TDI-1.5探测器响应训练，可以高效处理一年的多通道TDI数据，SNR约为50。我们对1年的数据进行评估，SNR值分别为50-120，DECODE实现了true positive率96.3%，false positive率1%，计算时间下than 0.01秒。通过对三个示例EMRI信号进行可见化和总结，DECODE表现出了对未来空间 gravitational wave数据分析的强大潜力。
</details></li>
</ul>
<hr>
<h2 id="CktGNN-Circuit-Graph-Neural-Network-for-Electronic-Design-Automation"><a href="#CktGNN-Circuit-Graph-Neural-Network-for-Electronic-Design-Automation" class="headerlink" title="CktGNN: Circuit Graph Neural Network for Electronic Design Automation"></a>CktGNN: Circuit Graph Neural Network for Electronic Design Automation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16406">http://arxiv.org/abs/2308.16406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zehao-dong/CktGNN">https://github.com/zehao-dong/CktGNN</a></li>
<li>paper_authors: Zehao Dong, Weidong Cao, Muhan Zhang, Dacheng Tao, Yixin Chen, Xuan Zhang</li>
<li>for: 这paper的目的是提出一种基于神经网络的电子设计自动化方法，用于快速和高效地设计分析电路。</li>
<li>methods: 这paper使用了一种名为Circuit Graph Neural Network（CktGNN）的方法，该方法同时自动生成电路拓扑和设备大小。CktGNN使用了两层GNN框架，将电路表示为一系列嵌入式GNN的输入。这种方法可以大幅提高设计效率，降低了消息传递的数量。</li>
<li>results:  experiments表明，CktGNN在Open Circuit Benchmark（OCB）上表现出色，在 represenation-based optimization frameworks 下，其性能较其他最近的GNN基elines和人工设计更高。这些结果预示了一种基于学习的电子设计自动化方法的可能性。<details>
<summary>Abstract</summary>
The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have mostly been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves design efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains $10$K distinct operational amplifiers with carefully-extracted circuit specifications. OCB is also equipped with communicative circuit generation and evaluation capabilities such that it can help to generalize CktGNN to design various analog circuits by producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and human experts' manual designs. Our work paves the way toward a learning-based open-sourced design automation for analog circuits. Our source code is available at \url{https://github.com/zehao-dong/CktGNN}.
</details>
<details>
<summary>摘要</summary>
electronic design automation of analog circuits 是integrated circuit field中长期的挑战，因为设计空间很大，设计几何选择很复杂。在过去的几十年中，大量的研究努力都是专注于将晶体大小调整为 givent circuit topology。本文发表了一个名为Circuit Graph Neural Network（CktGNN）的网络模型，可以同时自动生成图形和设备调整，基于encoder-dependent的优化子routines。尤其是，CktGNN将图形编码为一个二级GNN框架（of nested GNN），将图形视为一系列对known subgraph basis的 комбинации。这样可以对设计效率产生重要的改善，将讯息传递需要的子图减少。然而，几乎所有的学习帮助图形设计自动化都受到一个问题的限制：缺乏公共的底线来进行 canonical assessment和可重复的研究。为了解决这个挑战，我们创建了Open Circuit Benchmark（OCB），一个开源的数据集，包含10,000个不同的操作增强器，对于这些图形进行了精确的特性extraction。OCB还具有通信式图形生成和评估功能，可以帮助CktGNN对各种图形进行设计。实验结果表明，CktGNN通过表现基础优化框架比其他最近的强大GNN基eline和人工设计来得到了很好的成绩。我们的工作开启了一个学习基于的开源设计自动化的未来。我们的代码可以在https://github.com/zehao-dong/CktGNN中找到。
</details></li>
</ul>
<hr>
<h2 id="Balancing-between-the-Local-and-Global-Structures-LGS-in-Graph-Embedding"><a href="#Balancing-between-the-Local-and-Global-Structures-LGS-in-Graph-Embedding" class="headerlink" title="Balancing between the Local and Global Structures (LGS) in Graph Embedding"></a>Balancing between the Local and Global Structures (LGS) in Graph Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16403">http://arxiv.org/abs/2308.16403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Miller, Vahan Huroyan, Stephen Kobourov</li>
<li>for: 这个论文的目的是提出一种方法来平衡本地和全局结构（LGS）在图像模型中，通过可调参数。</li>
<li>methods: 该方法使用了一些图像模型，以及一些已知的质量指标，如压力和邻域保持。</li>
<li>results: 该研究通过synthetic和实际数据进行了评估，并结果表明LGS与现有方法竞争，使用了新的质量指标cluster distance preservation来评估中间结构捕捉。所有代码、数据、实验和分析都可以在线获取。<details>
<summary>Abstract</summary>
We present a method for balancing between the Local and Global Structures (LGS) in graph embedding, via a tunable parameter. Some embedding methods aim to capture global structures, while others attempt to preserve local neighborhoods. Few methods attempt to do both, and it is not always possible to capture well both local and global information in two dimensions, which is where most graph drawing live. The choice of using a local or a global embedding for visualization depends not only on the task but also on the structure of the underlying data, which may not be known in advance. For a given graph, LGS aims to find a good balance between the local and global structure to preserve. We evaluate the performance of LGS with synthetic and real-world datasets and our results indicate that it is competitive with the state-of-the-art methods, using established quality metrics such as stress and neighborhood preservation. We introduce a novel quality metric, cluster distance preservation, to assess intermediate structure capture. All source-code, datasets, experiments and analysis are available online.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法来平衡本地和全局结构（LGS）在图像中，通过可调参数。一些图像方法是捕捉全局结构，而另一些则尝试保留本地邻居。许多方法都不是同时能够 capture well both local和全局信息在两个维度，这是大多数图形描述生成器生活的地方。选择使用本地或全局嵌入 для可视化取决于任务以及对下面数据的结构，可能不知道在先。对于给定的图，LGS寻找一个好的本地和全局结构平衡，以保留。我们通过 synthetic和实际数据进行评估，结果表明LGS与当前状态的方法竞争，使用已确立的质量指标 such as 压力和邻居保持。我们引入了一个新的质量指标，即群集距离保持，来评估中间结构捕捉。所有源代码、数据集、实验和分析都可以在线获取。
</details></li>
</ul>
<hr>
<h2 id="Improving-Robustness-and-Accuracy-of-Ponzi-Scheme-Detection-on-Ethereum-Using-Time-Dependent-Features"><a href="#Improving-Robustness-and-Accuracy-of-Ponzi-Scheme-Detection-on-Ethereum-Using-Time-Dependent-Features" class="headerlink" title="Improving Robustness and Accuracy of Ponzi Scheme Detection on Ethereum Using Time-Dependent Features"></a>Improving Robustness and Accuracy of Ponzi Scheme Detection on Ethereum Using Time-Dependent Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16391">http://arxiv.org/abs/2308.16391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phuong Duy Huynh, Son Hoang Dau, Xiaodong Li, Phuc Luong, Emanuele Viterbo</li>
<li>for: The paper aims to detect Ponzi schemes in the cryptocurrency market, specifically using transaction data to improve the robustness and accuracy of detection.</li>
<li>methods: The authors propose new detection models that rely only on transactions and introduce novel time-dependent features to capture Ponzi behavior characteristics.</li>
<li>results: The proposed models achieve considerably higher accuracy, precision, recall, and F1-score than existing transaction-based models, making them more effective in detecting Ponzi schemes in the cryptocurrency market.Here’s the simplified Chinese text for the three information points:</li>
<li>for: 本研究旨在探讨区块链市场中 Ponzi 骗财 schemes 的探测方法，使用交易数据来提高检测的可靠性和准确性。</li>
<li>methods: 作者提出了一种基于交易数据的新的检测模型，该模型具有更高的可靠性、准确性、报告率和 F1 分数，可以更好地检测区块链市场中的 Ponzi 骗财。</li>
<li>results: 提出的模型在存在 Ponzi 骗财的情况下的检测效果比现有的交易基于模型更高，可以更好地保证检测的可靠性和准确性。<details>
<summary>Abstract</summary>
The rapid development of blockchain has led to more and more funding pouring into the cryptocurrency market, which also attracted cybercriminals' interest in recent years. The Ponzi scheme, an old-fashioned fraud, is now popular on the blockchain, causing considerable financial losses to many crypto-investors. A few Ponzi detection methods have been proposed in the literature, most of which detect a Ponzi scheme based on its smart contract source code or opcode. The contract-code-based approach, while achieving very high accuracy, is not robust: first, the source codes of a majority of contracts on Ethereum are not available, and second, a Ponzi developer can fool a contract-code-based detection model by obfuscating the opcode or inventing a new profit distribution logic that cannot be detected (since these models were trained on existing Ponzi logics only). A transaction-based approach could improve the robustness of detection because transactions, unlike smart contracts, are harder to be manipulated. However, the current transaction-based detection models achieve fairly low accuracy. We address this gap in the literature by developing new detection models that rely only on the transactions, hence guaranteeing the robustness, and moreover, achieve considerably higher Accuracy, Precision, Recall, and F1-score than existing transaction-based models. This is made possible thanks to the introduction of novel time-dependent features that capture Ponzi behaviours characteristics derived from our comprehensive data analyses on Ponzi and non-Ponzi data from the XBlock-ETH repository
</details>
<details>
<summary>摘要</summary>
随着区块链技术的快速发展，更多的投资者转移到了 криптовалюencies Market，其中也吸引了黑客的关注。在最近几年中， Ponzi 型骗财活动在区块链上变得非常流行，导致了许多 крип投资者遭受了重大的金融损失。文献中已经提出了一些 Ponzi 探测方法，大多数是基于智能合约源代码或 opcode 进行探测，但是这种方法并不够 robust。首先，大多数 Ethereum 上的合约源代码不可用，而且 Ponzi 开发者可以使用混淆或新的利润分配逻辑来欺骗合约代码检测模型。在交易基础上进行探测可以提高检测的Robustness，但现有的交易基础上的检测模型的准确率相对较低。我们在文献中填补这个空白，通过开发基于交易的新检测模型，以 guarantees 的Robustness和更高的准确率、精度、回归率和 F1 分数来检测 Ponzi。这种新的检测模型基于我们对 Ponzi 和非 Ponzi 数据进行了全面的分析，从而提取了 Ponzi 行为特征的时间依赖特征。
</details></li>
</ul>
<hr>
<h2 id="Multi-Objective-Decision-Transformers-for-Offline-Reinforcement-Learning"><a href="#Multi-Objective-Decision-Transformers-for-Offline-Reinforcement-Learning" class="headerlink" title="Multi-Objective Decision Transformers for Offline Reinforcement Learning"></a>Multi-Objective Decision Transformers for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16379">http://arxiv.org/abs/2308.16379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelghani Ghanem, Philippe Ciblat, Mounir Ghogho</li>
<li>for: 提高停滞RL的效果，使其更能够利用 transformer 模型的注意力机制。</li>
<li>methods: 将 offline RL  Reformulated 为多目标优化问题，并引入 action space regions 来改善 trajectory 表示。</li>
<li>results:  experiments 表明，我们的提议可以更好地使用 transformer 模型的注意力机制，并且可以与现有 state-of-the-art 方法匹配或超越其性能。<details>
<summary>Abstract</summary>
Offline Reinforcement Learning (RL) is structured to derive policies from static trajectory data without requiring real-time environment interactions. Recent studies have shown the feasibility of framing offline RL as a sequence modeling task, where the sole aim is to predict actions based on prior context using the transformer architecture. However, the limitation of this single task learning approach is its potential to undermine the transformer model's attention mechanism, which should ideally allocate varying attention weights across different tokens in the input context for optimal prediction. To address this, we reformulate offline RL as a multi-objective optimization problem, where the prediction is extended to states and returns. We also highlight a potential flaw in the trajectory representation used for sequence modeling, which could generate inaccuracies when modeling the state and return distributions. This is due to the non-smoothness of the action distribution within the trajectory dictated by the behavioral policy. To mitigate this issue, we introduce action space regions to the trajectory representation. Our experiments on D4RL benchmark locomotion tasks reveal that our propositions allow for more effective utilization of the attention mechanism in the transformer model, resulting in performance that either matches or outperforms current state-of-the art methods.
</details>
<details>
<summary>摘要</summary>
偏向式学习（RL）是结构化的，以 derive 策略从静止轨迹数据中获得，不需要实时环境交互。 recent studies have shown that framing offline RL as a sequence modeling task is feasible, where the sole aim is to predict actions based on prior context using the transformer architecture. However, this single task learning approach may undermine the transformer model's attention mechanism, which should ideally allocate varying attention weights across different tokens in the input context for optimal prediction. To address this, we reformulate offline RL as a multi-objective optimization problem, where the prediction is extended to states and returns. We also highlight a potential flaw in the trajectory representation used for sequence modeling, which could generate inaccuracies when modeling the state and return distributions. This is due to the non-smoothness of the action distribution within the trajectory dictated by the behavioral policy. To mitigate this issue, we introduce action space regions to the trajectory representation. Our experiments on D4RL benchmark locomotion tasks reveal that our propositions allow for more effective utilization of the attention mechanism in the transformer model, resulting in performance that either matches or outperforms current state-of-the-art methods.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is written in a formal and literal style, which may not always be the most idiomatic or natural way to express the ideas in Chinese.
</details></li>
</ul>
<hr>
<h2 id="SARATHI-Efficient-LLM-Inference-by-Piggybacking-Decodes-with-Chunked-Prefills"><a href="#SARATHI-Efficient-LLM-Inference-by-Piggybacking-Decodes-with-Chunked-Prefills" class="headerlink" title="SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"></a>SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16369">http://arxiv.org/abs/2308.16369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Ramachandran Ramjee</li>
<li>for: 提高 LLVM 推理性能</li>
<li>methods: 使用 chunked-prefills 和 decode-maximal batching 技术</li>
<li>results: 实现了 significanly 提高 LLVM 推理性能，包括 decode throughput 提高 10 倍，总体 throughput 提高 1.33 倍，并且减少了 pipeline bubbles。<details>
<summary>Abstract</summary>
Large Language Model (LLM) inference consists of two distinct phases - prefill phase which processes the input prompt and decode phase which generates output tokens autoregressively. While the prefill phase effectively saturates GPU compute at small batch sizes, the decode phase results in low compute utilization as it generates one token at a time per request. The varying prefill and decode times also lead to imbalance across micro-batches when using pipeline parallelism, resulting in further inefficiency due to bubbles.   We present SARATHI to address these challenges. SARATHI employs chunked-prefills, which splits a prefill request into equal sized chunks, and decode-maximal batching, which constructs a batch using a single prefill chunk and populates the remaining slots with decodes. During inference, the prefill chunk saturates GPU compute, while the decode requests 'piggyback' and cost up to an order of magnitude less compared to a decode-only batch. Chunked-prefills allows constructing multiple decode-maximal batches from a single prefill request, maximizing coverage of decodes that can piggyback. Furthermore, the uniform compute design of these batches ameliorates the imbalance between micro-batches, significantly reducing pipeline bubbles.   Our techniques yield significant improvements in inference performance across models and hardware. For the LLaMA-13B model on A6000 GPU, SARATHI improves decode throughput by up to 10x, and accelerates end-to-end throughput by up to 1.33x. For LLaMa-33B on A100 GPU, we achieve 1.25x higher end-to-end-throughput and up to 4.25x higher decode throughput. When used with pipeline parallelism on GPT-3, SARATHI reduces bubbles by 6.29x, resulting in an end-to-end throughput improvement of 1.91x.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）推理包括两个不同阶段：预填阶段和解码阶段。预填阶段处理输入提示，而解码阶段通过将输出 tokens 生成 autoregressively。在预填阶段， GPU 计算资源会受到小批量大小的限制，而在解码阶段，每个请求都会生成一个 token，导致计算资源利用率低。此外，预填和解码时间的变化也会导致微批处理中的不均衡，从而导致更多的气泡。为解决这些挑战，我们提出了 SARATHI。SARATHI 使用 chunked-prefills，将预填请求分割成等大小的块，并使用 decode-maximal batching，将每个块中的 decode 和预填块相结合，以实现 GPU 计算资源的最大利用。在推理过程中，预填块会使 GPU 计算资源充沛，而 decode 请求会“乘车”并产生相对论少的计算成本。chunked-prefills 允许构建多个 decode-maximal batches，从而最大化 decode 的覆盖率。此外，uniform 的 compute 设计也减轻了微批处理中的不均衡，从而减少气泡。我们的技术可以在不同的模型和硬件上实现显著的推理性能提升。对于 LLaMA-13B 模型和 A6000 GPU，SARATHI 可以提高解码吞吐量 by up to 10x，并提高总体吞吐量 by up to 1.33x。对于 LLaMa-33B 模型和 A100 GPU，我们可以达到 1.25x 更高的总体吞吐量和 up to 4.25x 更高的解码吞吐量。当用于 GPT-3 pipeline parallelism 时，SARATHI 可以减少气泡 by 6.29x，从而实现总体吞吐量的提升 by 1.91x。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/cs.LG_2023_08_31/" data-id="clot2mhez00oyx7884xnx0j21" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/eess.IV_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T09:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/eess.IV_2023_08_31/">eess.IV - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Twofold-Structured-Features-Based-Siamese-Network-for-Infrared-Target-Tracking"><a href="#Twofold-Structured-Features-Based-Siamese-Network-for-Infrared-Target-Tracking" class="headerlink" title="Twofold Structured Features-Based Siamese Network for Infrared Target Tracking"></a>Twofold Structured Features-Based Siamese Network for Infrared Target Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16676">http://arxiv.org/abs/2308.16676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Jie Yan, Yun-Kai Xu, Qian Chen, Xiao-Fang Kong, Guo-Hua Gu, A-Jun Shao, Min-Jie Wan</li>
<li>for: 提高干扰目标跟踪的精度和速度，尤其是在目标形状和大小变化时。</li>
<li>methods: 使用两重结构特征的SIAMESE网络，其中一个是通过将深度semantic信息和浅层空间信息融合在一起，以提高探测目标的能力。另一个是通过模板更新机制来有效地处理目标外观变化引起的迟迟跟踪失败。</li>
<li>results: 在VOT-TIR 2016 dataset上进行了质量和量测试，并证明了我们的方法可以具有与其他状态革新跟踪器相当的跟踪性能和实时跟踪速度。<details>
<summary>Abstract</summary>
Nowadays, infrared target tracking has been a critical technology in the field of computer vision and has many applications, such as motion analysis, pedestrian surveillance, intelligent detection, and so forth. Unfortunately, due to the lack of color, texture and other detailed information, tracking drift often occurs when the tracker encounters infrared targets that vary in size or shape. To address this issue, we present a twofold structured features-based Siamese network for infrared target tracking. First of all, in order to improve the discriminative capacity for infrared targets, a novel feature fusion network is proposed to fuse both shallow spatial information and deep semantic information into the extracted features in a comprehensive manner. Then, a multi-template update module based on template update mechanism is designed to effectively deal with interferences from target appearance changes which are prone to cause early tracking failures. Finally, both qualitative and quantitative experiments are carried out on VOT-TIR 2016 dataset, which demonstrates that our method achieves the balance of promising tracking performance and real-time tracking speed against other out-of-the-art trackers.
</details>
<details>
<summary>摘要</summary>
现在，红外目标跟踪技术在计算机视觉领域具有重要意义，有很多应用，如运动分析、人员监测、智能检测等等。然而，由于红外目标缺乏颜色、文本和其他细节信息，跟踪偏移经常发生，当跟踪器遇到变形或大小不同的红外目标时。为解决这个问题，我们提出了一种两重结构特征基于Siamese网络的红外目标跟踪方法。首先，为了提高红外目标的抑制能力，我们提出了一种新的特征融合网络，将表层空间信息和深度 semantics信息融合到提取的特征中，以实现全面的特征融合。然后，我们设计了基于模板更新机制的多模板更新模块，以有效地处理目标外观变化所导致的跟踪失败。最后，我们对VOT-TIR 2016 dataset进行了质量和kvantitativerexperiment，结果显示，我们的方法可以协调出色的跟踪性和实时跟踪速度，与其他状态OF-THE-ART tracker相比。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/eess.IV_2023_08_31/" data-id="clot2mhl7016fx788ca47hjo5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_08_31" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/31/eess.SP_2023_08_31/" class="article-date">
  <time datetime="2023-08-31T08:00:00.000Z" itemprop="datePublished">2023-08-31</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/31/eess.SP_2023_08_31/">eess.SP - 2023-08-31</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Amplitude-Prediction-from-Uplink-to-Downlink-CSI-against-Receiver-Distortion-in-FDD-Systems"><a href="#Amplitude-Prediction-from-Uplink-to-Downlink-CSI-against-Receiver-Distortion-in-FDD-Systems" class="headerlink" title="Amplitude Prediction from Uplink to Downlink CSI against Receiver Distortion in FDD Systems"></a>Amplitude Prediction from Uplink to Downlink CSI against Receiver Distortion in FDD Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16882">http://arxiv.org/abs/2308.16882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaojin Qing, Zilong Wang, Qing Ye, Wenhui Liu, Linsi He</li>
<li>for: 提高FDDFeMassive MIMO系统中的下行频道信息准确预测精度，解决由接收器扭曲引起的频率匹配问题。</li>
<li>methods: 使用减少接收器扭曲的传统方法提取上行CSI的幅特征，然后使用专门设计的轻量级幅学习网络（Dist-LeaNet）来抑制接收器扭曲和调整下行CSI的幅相似性。</li>
<li>results: 对FDD系统进行了严格的实验，结果表明，考虑接收器扭曲，提出的方案可以提高下行频道信息预测精度，同时降低传输和处理延迟。<details>
<summary>Abstract</summary>
In frequency division duplex (FDD) massive multiple-input multiple-output (mMIMO) systems, the reciprocity mismatch caused by receiver distortion seriously degrades the amplitude prediction performance of channel state information (CSI). To tackle this issue, from the perspective of distortion suppression and reciprocity calibration, a lightweight neural network-based amplitude prediction method is proposed in this paper. Specifically, with the receiver distortion at the base station (BS), conventional methods are employed to extract the amplitude feature of uplink CSI. Then, learning along the direction of the uplink wireless propagation channel, a dedicated and lightweight distortion-learning network (Dist-LeaNet) is designed to restrain the receiver distortion and calibrate the amplitude reciprocity between the uplink and downlink CSI. Subsequently, by cascading, a single hidden layer-based amplitude-prediction network (Amp-PreNet) is developed to accomplish amplitude prediction of downlink CSI based on the strong amplitude reciprocity. Simulation results show that, considering the receiver distortion in FDD systems, the proposed scheme effectively improves the amplitude prediction accuracy of downlink CSI while reducing the transmission and processing delay.
</details>
<details>
<summary>摘要</summary>
在分频分配多输入多输出（FDD）大规模多输入多输出（mMIMO）系统中，接收器损害导致的回归不匹配严重下降了频率预测性能。为解决这个问题，本文从损害抑制和回归准确的角度提出了一种轻量级神经网络基于频率预测方法。具体来说，通过在基站（BS）上提取接收器损害的干扰特征，然后通过学习在下降频率通信频道方向上，设计了专门的、轻量级的干扰学习网络（Dist-LeaNet），以抑制接收器损害并准确地做回归准确性between uplink和downlink CSI。接着，通过堆叠，一个单hidden layer基于频率预测网络（Amp-PreNet）被开发出来实现频率预测的下降频率CSI。 simulation结果表明，在考虑到FDD系统中接收器损害的情况下，提出的方案可以有效提高下降频率CSI的频率预测精度，同时降低传输和处理延迟。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Optimization-of-Reconfigurable-Intelligent-Surfaces-Based-on-S-Parameters-Multiport-Network-Theory"><a href="#Analysis-and-Optimization-of-Reconfigurable-Intelligent-Surfaces-Based-on-S-Parameters-Multiport-Network-Theory" class="headerlink" title="Analysis and Optimization of Reconfigurable Intelligent Surfaces Based on $S$-Parameters Multiport Network Theory"></a>Analysis and Optimization of Reconfigurable Intelligent Surfaces Based on $S$-Parameters Multiport Network Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16856">http://arxiv.org/abs/2308.16856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Abrardo, Alberto Toccafondi, Marco Di Renzo</li>
<li>for: 本研究考虑了可重新配置智能表面（RIS），并使用多口网络理论来模型它。</li>
<li>methods: 我们首先比较了使用Z参数和S参数来表示RIS，并证明它们之间的等价性，并讨论它们的不同特点。然后，我们开发了一种优化RIS配置的算法，以优化电磁共振 Coupling的影响。</li>
<li>results: 我们显示，基于S参数优化算法比基于Z参数优化算法更高效，这是因为小修改步长的提案算法会导致S参数中更大的变化，从而增加算法的速度。<details>
<summary>Abstract</summary>
In this paper, we consider a reconfigurable intelligent surface (RIS) and model it by using multiport network theory. We first compare the representation of RIS by using $Z$-parameters and $S$-parameters, by proving their equivalence and discussing their distinct features. Then, we develop an algorithm for optimizing the RIS configuration in the presence of electromagnetic mutual coupling. We show that the proposed algorithm based on optimizing the $S$-parameters results in better performance than existing algorithms based on optimizing the $Z$-parameters. This is attributed to the fact that small perturbations of the step size of the proposed algorithm result in larger variations of the $S$-parameters, hence increasing the convergence speed of the algorithm.
</details>
<details>
<summary>摘要</summary>
在本文中，我们考虑了可重新配置智能表面（RIS），并使用多ports网络理论来建模。我们首先比较了使用$Z$-参数和$S$-参数来表示RIS，并证明它们之间的等价性，并讨论它们的不同特点。然后，我们开发了一种优化RIS配置的算法，基于优化$S$-参数，并证明这种算法在电磁共振 coupling的存在下比既有算法更好。这是因为小幅修改算法中的步长，会导致$S$-参数的大小更大变化，从而提高算法的速度增长。
</details></li>
</ul>
<hr>
<h2 id="On-the-Performance-of-RIS-Aided-Spatial-Scattering-Modulation-for-mmWave-Transmission"><a href="#On-the-Performance-of-RIS-Aided-Spatial-Scattering-Modulation-for-mmWave-Transmission" class="headerlink" title="On the Performance of RIS-Aided Spatial Scattering Modulation for mmWave Transmission"></a>On the Performance of RIS-Aided Spatial Scattering Modulation for mmWave Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16804">http://arxiv.org/abs/2308.16804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xusheng Zhu, Wen Chen, Zhendong Li, Qingqing Wu, Ziheng Zhang, Kunlun Wang, Jun Li</li>
<li>for:  investigate a state-of-the-art reconfigurable intelligent surface (RIS)-assisted spatial scattering modulation (SSM) scheme for millimeter-wave (mmWave) systems</li>
<li>methods:  utilize line-of-sight (LoS) and non-LoS links in the transmitter-RIS and RIS-receiver channels, respectively, and employ the maximum likelihood detector at the receiver</li>
<li>results:  derive the conditional pairwise error probability (CPEP) expression for the RIS-SSM scheme under two scenarios, and obtain the union upper bound of average bit error probability (ABEP) based on the CPEP expression, all of which are validated by Monte Carlo simulations.Here is the Chinese translation of the three key information points:</li>
<li>for: 研究一种基于智能表面（RIS）的干扰干扰（SSM）方案，用于毫米波（mmWave）系统</li>
<li>methods: 利用传输器-RIS通道中的直线视野（LoS）和非直线视野（non-LoS）链接，并在接收器上使用最大可能性探测器</li>
<li>results:  derive CPEP表达式，并根据其而获得ABEP上限，所有结果经过了Monte Carlo仿真验证。<details>
<summary>Abstract</summary>
In this paper, we investigate a state-of-the-art reconfigurable intelligent surface (RIS)-assisted spatial scattering modulation (SSM) scheme for millimeter-wave (mmWave) systems, where a more practical scenario that the RIS is near the transmitter while the receiver is far from RIS is considered. To this end, the line-of-sight (LoS) and non-LoS links are utilized in the transmitter-RIS and RIS-receiver channels, respectively. By employing the maximum likelihood detector at the receiver, the conditional pairwise error probability (CPEP) expression for the RIS-SSM scheme is derived under the two scenarios that the received beam demodulation is correct or not. Furthermore, the union upper bound of average bit error probability (ABEP) is obtained based on the CPEP expression. Finally, the derivation results are exhaustively validated by the Monte Carlo simulations.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一种基于快速可配置智能表面（RIS）的扩展频率（mmWave）系统中的空间扩散调制（SSM）方案，其中假设RIS位于发送器的近距离上，而接收器则位于RIS的远距离上。为此，在发送器-RIS和RIS-接收器通道中分别使用了直线视线（LoS）和非直线视线（non-LoS）链路。通过使用最大可能性检测器（Maximum Likelihood Detector，MLD）在接收器端，我们 derivated了RIS-SSM方案的假设捷径误差概率（CPEP）表达。然后，基于CPEP表达，我们获得了union最大上限 bound of average bit error probability（ABEP）。最后，我们使用Monte Carlo仿真 validate了 derive结果。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-Using-RIDNet-Assisted-OMP-for-Hybrid-field-THz-Massive-MIMO-Systems"><a href="#Channel-Estimation-Using-RIDNet-Assisted-OMP-for-Hybrid-field-THz-Massive-MIMO-Systems" class="headerlink" title="Channel Estimation Using RIDNet Assisted OMP for Hybrid-field THz Massive MIMO Systems"></a>Channel Estimation Using RIDNet Assisted OMP for Hybrid-field THz Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16638">http://arxiv.org/abs/2308.16638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hasan Nayir, Erhan Karakoca, Ali Görçin, Khalid Qaraqe<br>for:* 这篇论文旨在提出一种基于Recursive Information Distillation Network（RIDNet）和Orthogonal Matching Pursuit（OMP）的混合场 THz mMIMO通道估计方法，以解决hybrid-field THz mMIMO通道估计的挑战。methods:* 该方法使用RIDNet和OMP结合，对hybrid-field THz mMIMO通道进行估计，包括远场和近场组件。results:* 实验结果表明，提议的RIDNet基于方法在所有Signal-to-Noise Ratio（SNR）域内都具有较低的通道估计误差，特别是在低SNR域。此外，结果还表明，使用RIDNet基于方法可以使用较少的RF链和尝试符号来达到与OMP算法相同的性能。<details>
<summary>Abstract</summary>
The terahertz (THz) band radio access with larger available bandwidth is anticipated to provide higher capacities for next-generation wireless communication systems. However, higher path loss at THz frequencies significantly limits the wireless communication range. Massive multiple-input multiple-output (mMIMO) is an attractive technology to increase the Rayleigh distance by generating higher gain beams using low wavelength and highly directive antenna array aperture. In addition, both far-field and near-field components of the antenna system should be considered for modelling THz electromagnetic propagation, where the channel estimation for this environment becomes a challenging task. This paper proposes a novel channel estimation method using a recursive information distillation network (RIDNet) together with orthogonal matching pursuit (OMP) for hybrid-field THz mMIMO channels, including both far-field and near-field components. The simulation experiments are performed using the ray-tracing tool. The results indicate that the proposed RIDNet-based method consistently provides lower channel estimation errors compared to the conventional OMP algorithm for all signal-to-noise ratio (SNR) regimes, and the performance gap becomes higher at low SNR regimes. Furthermore, the results imply that the same error performance of the OMP can be achieved by the RIDNet-based method using a lower number of RF chains and pilot symbols.
</details>
<details>
<summary>摘要</summary>
频率为teraHz（THz）的无线访问带宽更大，预计会提供下一代无线通信系统更高的容量。然而，THz频率上的跟踪损耗非常大，限制无线通信范围。大规模多输入多输出（mMIMO）技术可以提高Rayleigh距离，通过生成更高的投射高度和高度指向性的天线阵列。此外，需考虑天线系统的远场和近场组分，模拟THz电磁传播。由于这种环境的通道估计成为了一项挑战。这篇文章提出了一种使用重征信息蒸馈网络（RIDNet）和对匹配追求（OMP）算法的新通道估计方法，用于hybrid-field THz mMIMO通道，包括远场和近场组分。实验使用了射线跟踪工具。结果表明，提议的RIDNet基于方法在所有信号响应率（SNR）域内都提供了更低的通道估计错误，并且在低SNR域的性能差距变得更大。此外，结果表明，使用RIDNet基于方法可以通过使用较低的RF链和射频标志符来实现相同的错误性。
</details></li>
</ul>
<hr>
<h2 id="Design-Challenges-for-the-Implementation-of-Smart-Homes"><a href="#Design-Challenges-for-the-Implementation-of-Smart-Homes" class="headerlink" title="Design Challenges for the Implementation of Smart Homes"></a>Design Challenges for the Implementation of Smart Homes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16602">http://arxiv.org/abs/2308.16602</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Nesreen Mufid</li>
<li>for: 这个研究的目标是设计和实现一个智能家庭模型，以提高家庭安全性和可用性。</li>
<li>methods: 该模型使用可靠的移动网络，让用户可以在外出时监控家庭内部的情况，并通过不同的探测器检测火灾、气体泄漏、水泄漏和偷窃等问题。此外，家庭内还设置了一个摄像头，为用户提供全景视图。</li>
<li>results: 该模型可以帮助用户在外出时监控家庭内部的情况，并在火灾、气体泄漏、水泄漏和偷窃等问题发生时通知用户，让用户有时间采取行动。此外，用户还可以通过移动应用程序远程控制家庭的照明系统，灯光的开关和灭火。<details>
<summary>Abstract</summary>
Home automation for many years had faced challenges that limit its spreading around the world. These challenges caused by the high cost of Own such a home, inflexibility system (cannot be monitored outside the home) and issues to achieve optimal security. Our main objective is to design and implement a smart home model that is simple, affordable to the users. The proposed system provide flexibility to monitor the home, using the reliable cellular network. The user will be able what is inside the home when he /she is away from home. In addition to that, our model overcome the issue of the security by providing different sensors that detects smoke, gas, leakage of water and incases of burglary. Moreover, a camera will be available in the home to give a full view for the user when he/she is outside the home. The user will be informed by an application on his/she phone incase if there is a fire, water leakage and if someone break into the house. This will give the user a chance to take an action if such cases happened. Furthermore, the user can monitor the lighting system of the home, by giving the user a chance to turn the lights on and off remotely.
</details>
<details>
<summary>摘要</summary>
家庭自动化系统在多年来一直面临着限制其在全球蔓延的挑战。这些挑战是由于高昂的家庭所有成本、不灵活的系统（不能在外部监控）以及实现优质安全性的问题所致。我们的主要目标是设计并实施一个简单、可Affordable的家庭自动化模型。提议的系统提供了外部监控家庭的灵活性，使用可靠的手机网络。用户将能够在离家时了解家内情况，并且可以通过应用程序在手机上获得相关信息。此外，我们的模型还解决了安全性问题，通过设置烟报、气体探测器、水泄漏检测器和窃贼检测器等多种感知器来实现。此外，家中还将安装一个摄像头，以为用户在外部提供全景视图。用户通过应用程序接收有关家内情况的通知，如果发生火灾、水泄漏或窃贼等情况，他们可以及时采取相应的行动。此外，用户还可以通过移动设备控制家庭照明系统，启用和灭火按钮。
</details></li>
</ul>
<hr>
<h2 id="Data-Aided-Channel-Estimation-Utilizing-Gaussian-Mixture-Models"><a href="#Data-Aided-Channel-Estimation-Utilizing-Gaussian-Mixture-Models" class="headerlink" title="Data-Aided Channel Estimation Utilizing Gaussian Mixture Models"></a>Data-Aided Channel Estimation Utilizing Gaussian Mixture Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16601">http://arxiv.org/abs/2308.16601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franz Weißer, Nurettin Turan, Dominik Semmler, Wolfgang Utschick</li>
<li>for: 提高多用户系统中通道估计质量</li>
<li>methods: 使用数据符号和导航符号，提出两种方法，包括基于所有接收符号的子空间估计和基于 Gaussian mixture model 的通道估计器</li>
<li>results: 对实际通道测量数据进行数值仪表示，提议方法比 studied state-of-the-art 通道估计器 superior 性能<details>
<summary>Abstract</summary>
In this work, we propose two methods that utilize data symbols in addition to pilot symbols for improved channel estimation quality in a multi-user system, so-called semi-blind channel estimation. To this end, a subspace is estimated based on all received symbols and utilized to improve the estimation quality of a Gaussian mixture model-based channel estimator, which solely uses pilot symbols for channel estimation. Both of the proposed approaches allow for parallelization. Even the precomputation of estimation filters, which is beneficial in terms of computational complexity, is enabled by one of the proposed methods. Numerical simulations for real channel measurement data available to us show that the proposed methods outperform the studied state-of-the-art channel estimators.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了两种方法，利用数据符号以外的导航符号进行改进的通道估计质量，称为半不可见通道估计。为此，我们根据所有接收的符号 estimate一个子空间，并使用这个子空间来改进基于 Gaussian mixture model 的通道估计器，该仅使用导航符号进行通道估计。两种提出的方法均允许并行计算。而且，一种方法甚至可以在预计算估计filter的过程中启用并行计算。我们对我们手中的实际通道测量数据进行数值仿真，结果表明，我们提出的方法可以比 studied state-of-the-art 通道估计器表现更好。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-for-XL-MIMO-Systems-with-Polar-Domain-Multi-Scale-Residual-Dense-Network"><a href="#Channel-Estimation-for-XL-MIMO-Systems-with-Polar-Domain-Multi-Scale-Residual-Dense-Network" class="headerlink" title="Channel Estimation for XL-MIMO Systems with Polar-Domain Multi-Scale Residual Dense Network"></a>Channel Estimation for XL-MIMO Systems with Polar-Domain Multi-Scale Residual Dense Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.16400">http://arxiv.org/abs/2308.16400</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HaoLei-tnunder/Channel_Estimation_for_XL-MIMO_Systems_with_Polar-Domain_Multi-Scale_Residual_Dense_Network">https://github.com/HaoLei-tnunder/Channel_Estimation_for_XL-MIMO_Systems_with_Polar-Domain_Multi-Scale_Residual_Dense_Network</a></li>
<li>paper_authors: Hao Lei, Jiayi Zhang, Huahua Xiao, Xiaodan Zhang, Bo Ai, Derrick Wing Kwan Ng</li>
<li>for: 实现未来无线通信的广泛应用需要精准的通道状态信息，xl-mimo技术可以提供巨大的性能提升potential。</li>
<li>methods: 作者提出了基于 polar-domain sparse 的多重遗传神经网络（P-MRDN）和多scale residual dense network（P-MSRDN），以提高xl-mimo系统中的通道估计精度。</li>
<li>results: 实验结果显示，提议的方案比现有参考方案有更高的性能，并且 channel sparsity 对方案的影响相对较小。<details>
<summary>Abstract</summary>
Extremely large-scale multiple-input multiple-output (XL-MIMO) is a promising technique to enable versatile applications for future wireless communications.To realize the huge potential performance gain, accurate channel state information is a fundamental technical prerequisite. In conventional massive MIMO, the channel is often modeled by the far-field planar-wavefront with rich sparsity in the angular domain that facilitates the design of low-complexity channel estimation. However, this sparsity is not conspicuous in XL-MIMO systems due to the non-negligible near-field spherical-wavefront. To address the inherent performance loss of the angular-domain channel estimation schemes, we first propose the polar-domain multiple residual dense network (P-MRDN) for XL-MIMO systems based on the polar-domain sparsity of the near-field channel by improving the existing MRDN scheme. Furthermore, a polar-domain multi-scale residual dense network (P-MSRDN) is designed to improve the channel estimation accuracy. Finally, simulation results reveal the superior performance of the proposed schemes compared with existing benchmark schemes and the minimal influence of the channel sparsity on the proposed schemes.
</details>
<details>
<summary>摘要</summary>
“EXTREMELY LARGE-SCALE MULTIPLE-INPUT MULTIPLE-OUTPUT（XL-MIMO）是未来无线通讯技术的应用中的一个有前途的技术。为了实现这个巨大的性能提升，精确的通道状态信息是一个基本的技术前提。在传统的大规模MIMO系统中，通道通常被Modeled为距离较远的平面波front，这给了设计低复杂度的通道估测设计提供了帮助。但是，这种稀疏性不是XL-MIMO系统中的主要特点，因为近场球面波front的影响不可忽略。为了解决传统angular-domain channel estimation scheme的自然的性能损失，我们首先提出了 polar-domain multiple residual dense network（P-MRDN），这是基于近场通道的polar-domain稀疏性改进了现有的MRDN scheme。其次，我们设计了 polar-domain multi-scale residual dense network（P-MSRDN），以提高通道估测精度。最后，我们通过实验结果表明了我们提出的方案比现有的参考方案有更高的性能，并且通道稀疏性对我们提出的方案的影响较小。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/31/eess.SP_2023_08_31/" data-id="clot2mhmh019tx788cnly2z7g" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.SD_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T15:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.SD_2023_08_30/">cs.SD - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Review-of-Differentiable-Digital-Signal-Processing-for-Music-Speech-Synthesis"><a href="#A-Review-of-Differentiable-Digital-Signal-Processing-for-Music-Speech-Synthesis" class="headerlink" title="A Review of Differentiable Digital Signal Processing for Music &amp; Speech Synthesis"></a>A Review of Differentiable Digital Signal Processing for Music &amp; Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15422">http://arxiv.org/abs/2308.15422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Hayes, Jordie Shier, György Fazekas, Andrew McPherson, Charalampos Saitis</li>
<li>for: 这篇论文主要针对的是音乐和语音合成领域中的差分可控数字信号处理技术的应用。</li>
<li>methods: 该论文使用了差分可控数字信号处理技术，其中包括后退传播和权重调整等方法。</li>
<li>results: 该论文对音乐和语音合成任务进行了评估，并结果表明这些技术可以提高音乐和语音的生成质量。同时，论文还提出了一些未来研究的挑战，如优化症状、真实世界情况下的稳定性和设计决策。<details>
<summary>Abstract</summary>
The term "differentiable digital signal processing" describes a family of techniques in which loss function gradients are backpropagated through digital signal processors, facilitating their integration into neural networks. This article surveys the literature on differentiable audio signal processing, focusing on its use in music & speech synthesis. We catalogue applications to tasks including music performance rendering, sound matching, and voice transformation, discussing the motivations for and implications of the use of this methodology. This is accompanied by an overview of digital signal processing operations that have been implemented differentiably. Finally, we highlight open challenges, including optimisation pathologies, robustness to real-world conditions, and design trade-offs, and discuss directions for future research.
</details>
<details>
<summary>摘要</summary>
“差分可读取数字信号处理”是一家技术集合，其中损失函数导数通过数字信号处理器进行反propagation，以便将其 интегрирова到神经网络中。本文对差分音频信号处理的文献进行了报告，专注于它在音乐与语音合成中的应用。我们列出了各种应用场景，包括音乐演奏渲染、声音匹配和语音转换，并讨论了使用这种方法的动机和影响。此外，我们还提供了对数字信号处理操作的差分实现的概述。最后，我们指出了当前的开放挑战，包括优化症状、对实际 Condition 的Robustness以及设计贸易OFF，并讨论了未来研究的方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.SD_2023_08_30/" data-id="clot2mhhe00vzx7887z75cxn9" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.CV_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T13:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.CV_2023_08_30/">cs.CV - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="3D-Adversarial-Augmentations-for-Robust-Out-of-Domain-Predictions"><a href="#3D-Adversarial-Augmentations-for-Robust-Out-of-Domain-Predictions" class="headerlink" title="3D Adversarial Augmentations for Robust Out-of-Domain Predictions"></a>3D Adversarial Augmentations for Robust Out-of-Domain Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15479">http://arxiv.org/abs/2308.15479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Lehner, Stefano Gasperini, Alvaro Marcos-Ramiro, Michael Schmidt, Nassir Navab, Benjamin Busam, Federico Tombari<br>for:  This paper aims to improve the generalization of 3D object detection and semantic segmentation models to out-of-domain data.methods: The authors use adversarial examples to augment the training set and improve the models’ robustness to out-of-domain data. They learn a set of vectors that deform the objects in an adversarial fashion while preserving their plausibility.results: The authors show that their approach substantially improves the robustness and generalization of both 3D object detection and 3D semantic segmentation methods to out-of-domain data, achieving better performance on a variety of scenarios using data from KITTI, Waymo, and CrashD for object detection, and data from SemanticKITTI, Waymo, and nuScenes for semantic segmentation.Here’s the simplified Chinese text for the three key points:for: 这篇论文目标是提高3D物体检测和 semantic segmentation 模型对非标型数据的泛化性。methods: 作者使用对抗示例来增强训练集，以提高模型对非标型数据的Robustness。他们学习了一组扭曲物体的vector，以 preserve their plausibility。results: 作者表明，他们的方法可以大幅提高3D物体检测和 semantic segmentation 模型对非标型数据的泛化性，在不同场景下，使用KITTI、Waymo和CrashD数据集进行3D物体检测，以及使用SemanticKITTI、Waymo和nuScenes数据集进行semantic segmentation，并且在训练使用标准单个数据集，而不是使用多个数据集。<details>
<summary>Abstract</summary>
Since real-world training datasets cannot properly sample the long tail of the underlying data distribution, corner cases and rare out-of-domain samples can severely hinder the performance of state-of-the-art models. This problem becomes even more severe for dense tasks, such as 3D semantic segmentation, where points of non-standard objects can be confidently associated to the wrong class. In this work, we focus on improving the generalization to out-of-domain data. We achieve this by augmenting the training set with adversarial examples. First, we learn a set of vectors that deform the objects in an adversarial fashion. To prevent the adversarial examples from being too far from the existing data distribution, we preserve their plausibility through a series of constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform adversarial augmentation by applying the learned sample-independent vectors to the available objects when training a model. We conduct extensive experiments across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D semantic segmentation. Despite training on a standard single dataset, our approach substantially improves the robustness and generalization of both 3D object detection and 3D semantic segmentation methods to out-of-domain data.
</details>
<details>
<summary>摘要</summary>
自实际训练数据集不能正确采样下游数据分布的长尾，因此角落情况和罕见的非预训练数据样本会严重影响当前最佳模型的性能。这个问题在某些笔直的任务上，如3D语义分割，变得更加严重，因为非标准对象的点可以坚定地归类到错误的类型上。在这种情况下，我们关注提高对非预训练数据的泛化。我们实现这一目标通过在训练集中添加对抗示例来实现。首先，我们学习一组可以妄图对象的变形向量。为了保证对抗示例不过于远离现有数据分布，我们保留其可能性通过一系列约束，包括感知器和形状的平滑性。然后，我们通过应用学习的样本独立向量来对可用的对象进行对抗增强。我们在各种场景下进行了广泛的实验，包括KITTI、Waymo和CrashD上的3D物体检测，以及SemanticKITTI、Waymo和nuScenes上的3D语义分割。尽管我们只使用了标准单个数据集进行训练，但我们的方法可以很大程度上提高3D物体检测和3D语义分割方法对于非预训练数据的泛化性和Robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Adaptive-Tangent-Feature-Perspective-of-Neural-Networks"><a href="#An-Adaptive-Tangent-Feature-Perspective-of-Neural-Networks" class="headerlink" title="An Adaptive Tangent Feature Perspective of Neural Networks"></a>An Adaptive Tangent Feature Perspective of Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15478">http://arxiv.org/abs/2308.15478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel LeJeune, Sina Alemohammad</li>
<li>for: 了解神经网络中特征学习的机制</li>
<li>methods: 使用线性模型在抽象特征空间进行学习，并在训练过程中允许特征进行变换</li>
<li>results: 提出了一种基于线性变换的特征学习框架，并证明该框架在神经网络中可以提供更多的特征学习细节，以及一种适应特征实现的抽象特征分类方法可以在MNIST和CIFAR-10上具有许多 órders of magnitude 的采样复杂性优势。<details>
<summary>Abstract</summary>
In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classification has an order of magnitude lower sample complexity than the fixed tangent feature model on MNIST and CIFAR-10.
</details>
<details>
<summary>摘要</summary>
为了更好地理解神经网络中的特征学习，我们提出了一个框架来理解在斜缩Feature空间中的线性模型。我们考虑了特征的线性变换，从而导致参数和变换的共同优化问题，其中包括bilinear插值约束。我们表明这个优化问题有相应的线性约束优化问题，并且具有结构化正则化，以鼓励约束低维解决方案。在神经网络结构下，我们获得了特征和几何函数的变化，从而提供了特征对kernel函数的影响的更多的准确信息。此外，我们还证明了在实际神经网络中，适用于拟合特征的tanent特征分类模型具有训练样本的一个数量级减少。
</details></li>
</ul>
<hr>
<h2 id="Learning-Modulated-Transformation-in-GANs"><a href="#Learning-Modulated-Transformation-in-GANs" class="headerlink" title="Learning Modulated Transformation in GANs"></a>Learning Modulated Transformation in GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15472">http://arxiv.org/abs/2308.15472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ceyuan Yang, Qihang Zhang, Yinghao Xu, Jiapeng Zhu, Yujun Shen, Bo Dai</li>
<li>for: 提高 generative adversarial networks (GANs) 的模型灵活性和可重用性，以便更好地处理各种生成任务，包括图像生成、3D-aware图像生成和视频生成。</li>
<li>methods: 提出一种名为 modulated transformation module (MTM) 的插件模块，该模块可以预测空间偏移，并根据隐藏码来控制变量位置，以便模型更好地处理几何变换。</li>
<li>results: 在多种生成任务上进行了广泛的实验，并证明了该方法可以与当前的状态略进行无需任何参数调整。特别是，在人体生成 tasks 上，我们提高了 StyleGAN3 的 FID 值从 21.36 下降至 13.60， demonstrate 了学习模ulated geometry transformation 的能力。<details>
<summary>Abstract</summary>
The success of style-based generators largely benefits from style modulation, which helps take care of the cross-instance variation within data. However, the instance-wise stochasticity is typically introduced via regular convolution, where kernels interact with features at some fixed locations, limiting its capacity for modeling geometric variation. To alleviate this problem, we equip the generator in generative adversarial networks (GANs) with a plug-and-play module, termed as modulated transformation module (MTM). This module predicts spatial offsets under the control of latent codes, based on which the convolution operation can be applied at variable locations for different instances, and hence offers the model an additional degree of freedom to handle geometry deformation. Extensive experiments suggest that our approach can be faithfully generalized to various generative tasks, including image generation, 3D-aware image synthesis, and video generation, and get compatible with state-of-the-art frameworks without any hyper-parameter tuning. It is noteworthy that, towards human generation on the challenging TaiChi dataset, we improve the FID of StyleGAN3 from 21.36 to 13.60, demonstrating the efficacy of learning modulated geometry transformation.
</details>
<details>
<summary>摘要</summary>
成功的风格基本生成器主要受益于风格调整，它可以处理数据中的跨实例变化。然而，实例具有的随机性通常通过常规 convolution 引入，其中核函数与特征在固定位置相互作用，限制模型的形态变换能力。为解决这个问题，我们在生成对抗网络（GANs）中增加了可替换模块，称为模ulated transformation module（MTM）。这个模块根据隐藏代码预测空间偏移，并基于这些偏移进行变量位置的 convolution 操作，从而为模型增加了一个额外的自由度来处理形态变换。我们的方法可以广泛应用于不同的生成任务，包括图像生成、三维感知图像合成和视频生成，并与当前最佳框架相容无需任何超参数调整。特别是，我们在挑战性的 TaiChi 数据集上进行人体生成 task 时，提高了 StyleGAN3 的 FID 从 21.36 下降至 13.60，这表明我们学习了模ulated geometry transformation 的能力。
</details></li>
</ul>
<hr>
<h2 id="Input-margins-can-predict-generalization-too"><a href="#Input-margins-can-predict-generalization-too" class="headerlink" title="Input margins can predict generalization too"></a>Input margins can predict generalization too</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15466">http://arxiv.org/abs/2308.15466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sayantann11/all-classification-templetes-for-ML">https://github.com/sayantann11/all-classification-templetes-for-ML</a></li>
<li>paper_authors: Coenraad Mouton, Marthinus W. Theunissen, Marelie H. Davel</li>
<li>for:  investigate the relationship between generalization and classification margins in deep neural networks</li>
<li>methods:  use margin measurements, specifically constrained margins, to predict generalization ability</li>
<li>results:  constrained margins achieve highly competitive scores and outperform other margin measurements in general, providing a novel insight into the relationship between generalization and classification margins.<details>
<summary>Abstract</summary>
Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained. We develop such a measure based on input margins, which we refer to as `constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and outperform other margin measurements in general. This provides a novel insight on the relationship between generalization and classification margins, and highlights the importance of considering the data manifold for investigations of generalization in DNNs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Online-Overexposed-Pixels-Hallucination-in-Videos-with-Adaptive-Reference-Frame-Selection"><a href="#Online-Overexposed-Pixels-Hallucination-in-Videos-with-Adaptive-Reference-Frame-Selection" class="headerlink" title="Online Overexposed Pixels Hallucination in Videos with Adaptive Reference Frame Selection"></a>Online Overexposed Pixels Hallucination in Videos with Adaptive Reference Frame Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15462">http://arxiv.org/abs/2308.15462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yazhou Xing, Amrita Mazumdar, Anjul Patney, Chao Liu, Hongxu Yin, Qifeng Chen, Jan Kautz, Iuri Frosio</li>
<li>for: 解决LDR相机无法处理宽动态范围输入的问题，提高图像质量。</li>
<li>methods: 使用变换器基于深度神经网络（DNN）推断缺失HDR细节。在减少参数学习中，使用多尺度DNN和适当的成本函数来实现状态艺术质量。 Additionally, using a reference frame from the past as an additional input to aid the reconstruction of overexposed areas.</li>
<li>results: 在减少参数学习中，使用这种方法可以获得状态艺术质量，而不需要使用复杂的获取机制或高Dynamic范围成像处理。我们的示例视频可以在<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view中找到。</a><details>
<summary>Abstract</summary>
Low dynamic range (LDR) cameras cannot deal with wide dynamic range inputs, frequently leading to local overexposure issues. We present a learning-based system to reduce these artifacts without resorting to complex acquisition mechanisms like alternating exposures or costly processing that are typical of high dynamic range (HDR) imaging. We propose a transformer-based deep neural network (DNN) to infer the missing HDR details. In an ablation study, we show the importance of using a multiscale DNN and train it with the proper cost function to achieve state-of-the-art quality. To aid the reconstruction of the overexposed areas, our DNN takes a reference frame from the past as an additional input. This leverages the commonly occurring temporal instabilities of autoexposure to our advantage: since well-exposed details in the current frame may be overexposed in the future, we use reinforcement learning to train a reference frame selection DNN that decides whether to adopt the current frame as a future reference. Without resorting to alternating exposures, we obtain therefore a causal, HDR hallucination algorithm with potential application in common video acquisition settings. Our demo video can be found at https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view
</details>
<details>
<summary>摘要</summary>
低动态范围（LDR）摄像机不能处理宽动态范围输入，导致本地过度曝光问题。我们提出了一种学习基于的系统，以减少这些缺陷而不需要复杂的获取机制如alternating exposures或高动态范围（HDR）拍摄。我们提议使用 transformer 基于的深度神经网络（DNN）来推理缺失 HDR 细节。在一个ablation study中，我们表明了使用多尺度 DNN 和适当的成本函数以 дости得状态的质量。为了重建过度曝光的区域，我们的 DNN 接受了过去的参考帧作为额外输入。这样利用了自动曝光的 temporal 不稳定性，我们使用 reinforcement learning 来训练参考帧选择 DNN，以确定是否采用当前帧作为未来的参考。无需alternating exposures，我们得到了一个 causal、HDR 幻化算法，可能在常见的视频拍摄设置中应用。我们的 demo 视频可以在 <https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view> 找到。
</details></li>
</ul>
<hr>
<h2 id="Canonical-Factors-for-Hybrid-Neural-Fields"><a href="#Canonical-Factors-for-Hybrid-Neural-Fields" class="headerlink" title="Canonical Factors for Hybrid Neural Fields"></a>Canonical Factors for Hybrid Neural Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15461">http://arxiv.org/abs/2308.15461</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brentyi/tilted">https://github.com/brentyi/tilted</a></li>
<li>paper_authors: Brent Yi, Weijia Zeng, Sam Buchanan, Yi Ma</li>
<li>for: 本文主要针对的问题是射线对齐信号的抽象特征量化方法引入的不良偏见问题，并提出一种解决方法。</li>
<li>methods: 本文使用了学习一组均衡变换的方法，以消除这些偏见。</li>
<li>results: 实验结果表明，使用这种方法可以提高图像、签名距离和辐射场重建质量、稳定性、压缩率和运行时间。<details>
<summary>Abstract</summary>
Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x larger, while highlighting weaknesses of neural field evaluation procedures.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Characterize the undesirable biases that these architectures have for axis-aligned signals, which can lead to radiance field reconstruction differences of up to 2 PSNR.2. Explore how learning a set of canonicalizing transformations can improve representations by removing these biases.We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance can be done with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, and observe improvements in quality, robustness, compactness, and runtime. Our results show that TILTED can enable capabilities comparable to baselines that are 2x larger, while highlighting weaknesses of neural field evaluation procedures.</details></li>
</ol>
<hr>
<h2 id="Pseudo-Boolean-Polynomials-Approach-To-Edge-Detection-And-Image-Segmentation"><a href="#Pseudo-Boolean-Polynomials-Approach-To-Edge-Detection-And-Image-Segmentation" class="headerlink" title="Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation"></a>Pseudo-Boolean Polynomials Approach To Edge Detection And Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15453">http://arxiv.org/abs/2308.15453</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tendai Mapungwana Chikake, Boris Goldengorin, Alexey Samosyuk</li>
<li>for: 用于图像Edge检测和分割</li>
<li>methods: 使用pseudo-Boolean波尔次数计算image patches，进行二分类 blob和边区域的分类</li>
<li>results: 在简单图像中成功实现Edge检测和分割，并在复杂图像中进行应用In English, this translates to:</li>
<li>for: Used for image edge detection and segmentation</li>
<li>methods: Using pseudo-Boolean polynomials calculated on image patches for binary classification of blob and edge regions</li>
<li>results: Successfully implemented edge detection and segmentation on simple images and applied to complex images like aerial landscapes<details>
<summary>Abstract</summary>
We introduce a deterministic approach to edge detection and image segmentation by formulating pseudo-Boolean polynomials on image patches. The approach works by applying a binary classification of blob and edge regions in an image based on the degrees of pseudo-Boolean polynomials calculated on patches extracted from the provided image. We test our method on simple images containing primitive shapes of constant and contrasting colour and establish the feasibility before applying it to complex instances like aerial landscape images. The proposed method is based on the exploitation of the reduction, polynomial degree, and equivalence properties of penalty-based pseudo-Boolean polynomials.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种权值Deterministic逻辑来实现图像边检测和分割，通过在图像块上计算 pseudo-Boolean 多项式。该方法基于对图像块上的二分类，将图像分为 blob 和边区域，并基于计算的 pseudo-Boolean 多项式度量来进行分类。我们在简单的图像中使用了固定颜色和对比度的基本形状进行测试，并证明了该方法的可行性。然后，我们将该方法应用于复杂的飞行图像。该方法基于 pseudo-Boolean 多项式的减少、度量和等价性属性的利用。
</details></li>
</ul>
<hr>
<h2 id="WrappingNet-Mesh-Autoencoder-via-Deep-Sphere-Deformation"><a href="#WrappingNet-Mesh-Autoencoder-via-Deep-Sphere-Deformation" class="headerlink" title="WrappingNet: Mesh Autoencoder via Deep Sphere Deformation"></a>WrappingNet: Mesh Autoencoder via Deep Sphere Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15413">http://arxiv.org/abs/2308.15413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Lei, Muhammad Asad Lodhi, Jiahao Pang, Junghyun Ahn, Dong Tian</li>
<li>for: 本研究旨在实现基于 mesh 数据的无监督学习，以便学习更有意义的表示。</li>
<li>methods: 本文提出了一种基于 bottleneck 的 mesh autoencoder，通过专门 Representing mesh 连接情况的基本图来促进学习共享的 latent 空间表示对象形状。</li>
<li>results: 对比点云学习，WRAPPINGNET 可以提供更高质量的重建和竞争性的分类结果，同时可以进行不同类型对象之间的 latent  interpolate。<details>
<summary>Abstract</summary>
There have been recent efforts to learn more meaningful representations via fixed length codewords from mesh data, since a mesh serves as a complete model of underlying 3D shape compared to a point cloud. However, the mesh connectivity presents new difficulties when constructing a deep learning pipeline for meshes. Previous mesh unsupervised learning approaches typically assume category-specific templates, e.g., human face/body templates. It restricts the learned latent codes to only be meaningful for objects in a specific category, so the learned latent spaces are unable to be used across different types of objects. In this work, we present WrappingNet, the first mesh autoencoder enabling general mesh unsupervised learning over heterogeneous objects. It introduces a novel base graph in the bottleneck dedicated to representing mesh connectivity, which is shown to facilitate learning a shared latent space representing object shape. The superiority of WrappingNet mesh learning is further demonstrated via improved reconstruction quality and competitive classification compared to point cloud learning, as well as latent interpolation between meshes of different categories.
</details>
<details>
<summary>摘要</summary>
有些最近的努力是通过固定长度代码Word来学习更有意义的表示，从网格数据中得到更多的信息，因为网格作为三维形态的完整模型，比点云更有优势。然而，网格连接会对深度学习管道的构建带来新的挑战。以前的无监督学习方法通常假设特定类别的模板，例如人脸/身体模板。这限制学习的幂等空间只能对特定类别的对象进行有意义的学习，因此学习的幂等空间无法在不同类别的对象之间进行使用。在这种工作中，我们介绍了WrappingNet，首个能够进行总体网格无监督学习的自动编码器。它引入了瓶颈部分的新基graph，用于表示网格连接，这被证明可以促进学习对象形状的共享 latent space。我们通过对网格学习和点云学习进行比较，以及在不同类别的网格之间进行 latent  interpolate 等方法来证明 WrappingNet 的优越性。
</details></li>
</ul>
<hr>
<h2 id="Robust-Long-Tailed-Learning-via-Label-Aware-Bounded-CVaR"><a href="#Robust-Long-Tailed-Learning-via-Label-Aware-Bounded-CVaR" class="headerlink" title="Robust Long-Tailed Learning via Label-Aware Bounded CVaR"></a>Robust Long-Tailed Learning via Label-Aware Bounded CVaR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15405">http://arxiv.org/abs/2308.15405</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Zhu, Runpeng Yu, Xing Tang, Yifei Wang, Yuan Fang, Yisen Wang</li>
<li>for: 实际世界中的核心类别问题 often 会出现不对称或长尾分布，导致模型训练时对少数类别表现差。这种情况下，单简的模型通常对少数类别表现不佳。</li>
<li>methods: 本文提出了两种基于CVaR（Conditional Value at Risk）的新方法来改善长尾学习的表现，并提供了严谨的理论保证。特别是，我们首先引入了Label-Aware Bounded CVaR（LAB-CVaR）损失函数，以解决原始CVaR的偏预结果问题，然后设计了LAB-CVaR的最佳质量上限。基于LAB-CVaR，我们还提出了LAB-CVaR with logit adjustment（LAB-CVaR-logit）损失函数，并提供了理论支持。</li>
<li>results: 实际实验结果显示，我们的提案方法在实际世界中的长尾标签分布下表现出色，较以单简的模型表现更好。<details>
<summary>Abstract</summary>
Data in the real-world classification problems are always imbalanced or long-tailed, wherein the majority classes have the most of the samples that dominate the model training. In such setting, the naive model tends to have poor performance on the minority classes. Previously, a variety of loss modifications have been proposed to address the long-tailed leaning problem, while these methods either treat the samples in the same class indiscriminatingly or lack a theoretical guarantee. In this paper, we propose two novel approaches based on CVaR (Conditional Value at Risk) to improve the performance of long-tailed learning with a solid theoretical ground. Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization process, where we also offer the theoretical support. Extensive experiments on real-world datasets with long-tailed label distributions verify the superiority of our proposed methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>>世界上的实际分类问题中的数据总是偏斜或长尾分布，其中多数类占据了模型训练中的大多数样本。在这种情况下，简单的模型通常会对少数类表现不佳。先前，一些损失修改方法已经被提出，但这些方法可能会对同一类的样本待遇不公平，或者缺乏理论保证。在本文中，我们提出了两种基于CVaR（Conditional Value at Risk）的新方法，以改进长尾学习的性能，并提供了坚实的理论基础。 Specifically, we first introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss to overcome the pessimistic result of the original CVaR, and further design the optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to stabilize the optimization process, where we also offer the theoretical support. 实际上，我们对实际上的长尾标签分布进行了广泛的实验，并证明了我们提出的方法的优越性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.CV_2023_08_30/" data-id="clot2mhcl00hpx788bllx2e33" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.AI_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T12:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.AI_2023_08_30/">cs.AI - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-General-Purpose-Self-Supervised-Model-for-Computational-Pathology"><a href="#A-General-Purpose-Self-Supervised-Model-for-Computational-Pathology" class="headerlink" title="A General-Purpose Self-Supervised Model for Computational Pathology"></a>A General-Purpose Self-Supervised Model for Computational Pathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15474">http://arxiv.org/abs/2308.15474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard J. Chen, Tong Ding, Ming Y. Lu, Drew F. K. Williamson, Guillaume Jaume, Bowen Chen, Andrew Zhang, Daniel Shao, Andrew H. Song, Muhammad Shaban, Mane Williams, Anurag Vaidya, Sharifa Sahai, Lukas Oldenburg, Luca L. Weishaupt, Judy J. Wang, Walt Williams, Long Phi Le, Georg Gerber, Faisal Mahmood</li>
<li>for: 本研究旨在提出一种通用的自助学习模型，用于解决生物病理学图像分类和诊断问题。</li>
<li>methods: 该模型使用了100万个各种组织类型的病理图像补充，并通过自助学习方法进行预训练。</li>
<li>results: 该模型在33种不同的临床任务中表现出色，包括分类、诊断和疾病类型划分等。此外，模型还能够在各种组织类型和诊断难度不同的情况下进行泛化和转移。<details>
<summary>Abstract</summary>
Tissue phenotyping is a fundamental computational pathology (CPath) task in learning objective characterizations of histopathologic biomarkers in anatomic pathology. However, whole-slide imaging (WSI) poses a complex computer vision problem in which the large-scale image resolutions of WSIs and the enormous diversity of morphological phenotypes preclude large-scale data annotation. Current efforts have proposed using pretrained image encoders with either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets, but have not been extensively developed and evaluated across diverse tissue types at scale. We introduce UNI, a general-purpose self-supervised model for pathology, pretrained using over 100 million tissue patches from over 100,000 diagnostic haematoxylin and eosin-stained WSIs across 20 major tissue types, and evaluated on 33 representative CPath clinical tasks in CPath of varying diagnostic difficulties. In addition to outperforming previous state-of-the-art models, we demonstrate new modeling capabilities in CPath such as resolution-agnostic tissue classification, slide classification using few-shot class prototypes, and disease subtyping generalization in classifying up to 108 cancer types in the OncoTree code classification system. UNI advances unsupervised representation learning at scale in CPath in terms of both pretraining data and downstream evaluation, enabling data-efficient AI models that can generalize and transfer to a gamut of diagnostically-challenging tasks and clinical workflows in anatomic pathology.
</details>
<details>
<summary>摘要</summary>
组织现象评估是computational pathology（CPath）的基本任务，它的目标是通过学习标注组织生物marker的Objective characterizations，以帮助诊断医学。然而，整个标本影像（WSI）对于计算机视觉问题而言是一个复杂的问题，因为标本影像的大规模分辨率和丰富多样性的形态现象对于大规模标注数据的生成提供了一定的挑战。目前的努力已经提议使用预训归数图像Encoder， either transfer learning from natural image datasets or self-supervised pretraining on publicly-available histopathology datasets，但这些努力尚未得到了广泛的发展和评估。我们提出了UNI，一个通用的自主学习模型，预训使用了1000万个组织小图像，来自100000多个诊断HE染色标本影像，并在20个主要组织类型上进行了33个CPath临床任务的评估。此外，我们还展示了一些新的模型化能力，例如resolution-agnostic tissue classification、slides classification using few-shot class prototypes、和疾病分类对108种癌症的分类。UNI在CPath中进行了无监督学习的扩展，并在这些任务上实现了资料效率的AI模型，可以对诊断挑战性任务和临床工作流程进行普遍和转移。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Contrastive-Learning-and-Tabular-Attention-for-Automated-Alzheimer’s-Disease-Prediction"><a href="#Multimodal-Contrastive-Learning-and-Tabular-Attention-for-Automated-Alzheimer’s-Disease-Prediction" class="headerlink" title="Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer’s Disease Prediction"></a>Multimodal Contrastive Learning and Tabular Attention for Automated Alzheimer’s Disease Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15469">http://arxiv.org/abs/2308.15469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weichen Huang</li>
<li>for: 这个研究旨在开发一个多 modal 对照学习框架，以利用 MRI 扫描和 PET 等神经成像数据，并处理 AD 疾病数据中的值得注意的表格资料。</li>
<li>methods: 这个框架使用了一个新的表格注意模组，可以强调和排名表格中的重要特征。它还使用了多 modal 对照学习技术，以将图像和表格资料结合在一起。</li>
<li>results: 实验结果显示，这个框架可以从 ADNI 数据库中的逾 882 个 MRI 扫描标本中检测出 AD 疾病，并且可以实现高于 83.8% 的准确率，与前一代的州度优化技术相比，提高了约 10%。<details>
<summary>Abstract</summary>
Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, almost a 10% increase from previous state of the art.
</details>
<details>
<summary>摘要</summary>
alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD) datasets contain valuable tabular data including AD biomarkers and clinical assessments. Existing computer vision approaches struggle to utilize this additional information. To address these needs, we propose a generalizable framework for multimodal contrastive learning of image data and tabular data, a novel tabular attention module for amplifying and ranking salient features in tables, and the application of these techniques onto Alzheimer's disease prediction. Experimental evaulations demonstrate the strength of our framework by detecting Alzheimer's disease (AD) from over 882 MR image slices from the ADNI database. We take advantage of the high interpretability of tabular data and our novel tabular attention approach and through attribution of the attention scores for each row of the table, we note and rank the most predominant features. Results show that the model is capable of an accuracy of over 83.8%, almost a 10% increase from previous state of the art.Here's the translation in Traditional Chinese:附加了脑成像技术，如MRI扫描和PET，Alzheimer病（AD）数据集包含重要的表格资料，包括AD标识和临床评估。现有的计算机视觉方法对这些额外资讯难以使用。为解决这些需求，我们提出了一个通用的多modal对比学习框架，一个新的表格注意模组，以强调和排名表格中的重要特征。我们还应用了这些技术 onto Alzheimer病预测。实验评估显示了我们的框架在ADNI数据库中的882个MRI图像探针中检测到Alzheimer病的能力，比前一代的state of the art约10%高。我们利用表格资料的高解释性和我们的新的表格注意方法，通过每行表格中的注意分数汇总，发现和排名表格中的最主要特征。结果显示模型可以达到83.8%的精度，比前一代的state of the art约10%高。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Loss-Functions-Traffic-Predictions-in-Regular-and-Congestion-Scenarios"><a href="#A-Comparative-Study-of-Loss-Functions-Traffic-Predictions-in-Regular-and-Congestion-Scenarios" class="headerlink" title="A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios"></a>A Comparative Study of Loss Functions: Traffic Predictions in Regular and Congestion Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15464">http://arxiv.org/abs/2308.15464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xieyangxinyu/a-comparative-study-of-loss-functions-traffic-predictions-in-regular-and-congestion-scenarios">https://github.com/xieyangxinyu/a-comparative-study-of-loss-functions-traffic-predictions-in-regular-and-congestion-scenarios</a></li>
<li>paper_authors: Yangxinyu Xie, Tanwi Mallick</li>
<li>for: 这 paper 的目的是提高深度学习模型在交通预测中的精度，特别是预测堵塞情况。</li>
<li>methods: 这 paper 使用了多种积分函数，包括 MAE-Focal Loss 和 Gumbel Loss，来解决传统损失函数的局限性。</li>
<li>results: 经过大规模实验，这 paper 发现 MAE-Focal Loss 和 Gumbel Loss 在预测交通速度方面具有最高效果，能够准确预测堵塞情况而不会妨碍正常交通预测。<details>
<summary>Abstract</summary>
Spatiotemporal graph neural networks have achieved state-of-the-art performance in traffic forecasting. However, they often struggle to forecast congestion accurately due to the limitations of traditional loss functions. While accurate forecasting of regular traffic conditions is crucial, a reliable AI system must also accurately forecast congestion scenarios to maintain safe and efficient transportation. In this paper, we explore various loss functions inspired by heavy tail analysis and imbalanced classification problems to address this issue. We evaluate the efficacy of these loss functions in forecasting traffic speed, with an emphasis on congestion scenarios. Through extensive experiments on real-world traffic datasets, we discovered that when optimizing for Mean Absolute Error (MAE), the MAE-Focal Loss function stands out as the most effective. When optimizing Mean Squared Error (MSE), Gumbel Loss proves to be the superior choice. These choices effectively forecast traffic congestion events without compromising the accuracy of regular traffic speed forecasts. This research enhances deep learning models' capabilities in forecasting sudden speed changes due to congestion and underscores the need for more research in this direction. By elevating the accuracy of congestion forecasting, we advocate for AI systems that are reliable, secure, and resilient in practical traffic management scenarios.
</details>
<details>
<summary>摘要</summary>
现代交通预测中使用的空间时间图 neural network 已经达到了领先的性能水平。然而，它们经常因传统的损失函数的局限性而难以准确预测堵塞情况。尽管正确预测常规交通情况是非常重要，但一个可靠的 AI 系统也必须准确预测堵塞场景，以保证安全和高效的交通运输。在这篇论文中，我们探讨了各种启发自重态分析和不均衡分类问题的损失函数，以解决这一问题。我们对这些损失函数在预测交通速度方面的效果进行了广泛的实验，发现了使用 MAE-Focal Loss 函数时，MAE 函数在预测堵塞场景中表现最佳。使用 MSE 函数时，Gumbel Loss 函数表现最佳。这些选择可以准确预测交通堵塞事件，不会 compromise 正常交通速度预测的准确性。这项研究提高了深度学习模型在预测快速变化的能力，并强调了对堵塞预测的需求。我们认为，通过提高堵塞预测的准确性，可以建立可靠、安全、可靠的 AI 系统，以满足实际交通管理场景中的需求。
</details></li>
</ul>
<hr>
<h2 id="ParaGuide-Guided-Diffusion-Paraphrasers-for-Plug-and-Play-Textual-Style-Transfer"><a href="#ParaGuide-Guided-Diffusion-Paraphrasers-for-Plug-and-Play-Textual-Style-Transfer" class="headerlink" title="ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer"></a>ParaGuide: Guided Diffusion Paraphrasers for Plug-and-Play Textual Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15459">http://arxiv.org/abs/2308.15459</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zacharyhorvitz/ParaGuide">https://github.com/zacharyhorvitz/ParaGuide</a></li>
<li>paper_authors: Zachary Horvitz, Ajay Patel, Chris Callison-Burch, Zhou Yu, Kathleen McKeown</li>
<li>for: 文章的目的是将文本的风格特征转换为新的风格，保留 semantic information。</li>
<li>methods: 本研究使用了一个新的扩散基础架构，叫做 ParaGuide，可以灵活地适应任意目标风格。这个方法使用了句子重写条件下的扩散模型，以及Gradient-based guidance from both off-the-shelf classifiers和强大的现有风格嵌入。</li>
<li>results: 研究在Enron Email Corpus上进行了人工和自动评估，和强大的基eline均以上。它可以成功地将文本的风格转换为新的风格，保留 semantic information。<details>
<summary>Abstract</summary>
Textual style transfer is the task of transforming stylistic properties of text while preserving meaning. Target "styles" can be defined in numerous ways, ranging from single attributes (e.g, formality) to authorship (e.g, Shakespeare). Previous unsupervised style-transfer approaches generally rely on significant amounts of labeled data for only a fixed set of styles or require large language models. In contrast, we introduce a novel diffusion-based framework for general-purpose style transfer that can be flexibly adapted to arbitrary target styles at inference time. Our parameter-efficient approach, ParaGuide, leverages paraphrase-conditioned diffusion models alongside gradient-based guidance from both off-the-shelf classifiers and strong existing style embedders to transform the style of text while preserving semantic information. We validate the method on the Enron Email Corpus, with both human and automatic evaluations, and find that it outperforms strong baselines on formality, sentiment, and even authorship style transfer.
</details>
<details>
<summary>摘要</summary>
文本风格转换是将文本的风格属性转换为另一种风格的任务，保持意义不变。目标风格可以定义为多种方式，从单一特征（例如正式度）到作者（例如莎士比亚）。前一代无监督风格转换方法通常需要大量标注数据，仅适用于固定的风格集或需要大型语言模型。相比之下，我们介绍了一种新的扩散基于框架，可以通过扩散模型来实现通用风格转换，并在推理时适应任意目标风格。我们的参数有效的方法， ParaGuide，利用了句子重构conditional扩散模型，并与梯度导航从存储类фика器和强有力的现有风格编码器来转换文本的风格，保持 semantic information。我们在恩рон电子邮件集上验证了该方法，并与人类和自动评估表明，它在正式度、情感和作者风格转换方面超过了强大基eline。
</details></li>
</ul>
<hr>
<h2 id="From-SMOTE-to-Mixup-for-Deep-Imbalanced-Classification"><a href="#From-SMOTE-to-Mixup-for-Deep-Imbalanced-Classification" class="headerlink" title="From SMOTE to Mixup for Deep Imbalanced Classification"></a>From SMOTE to Mixup for Deep Imbalanced Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15457">http://arxiv.org/abs/2308.15457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ntucllab/imbalanced-dl">https://github.com/ntucllab/imbalanced-dl</a></li>
<li>paper_authors: Wei-Chao Cheng, Tan-Ha Mai, Hsuan-Tien Lin</li>
<li>for: 本研究旨在探讨深度学习中对异质数据的处理方法，尤其是SMOTE数据增强技术是否有利于深度学习。</li>
<li>methods: 本研究使用了SMOTE技术，以及其扩展版本——软标签SMOTE和混合技术。</li>
<li>results: 研究发现，通过将SMOTE和混合技术结合使用，可以提高深度学习模型的泛化性能，并且在极端异质数据上达到最佳性能。<details>
<summary>Abstract</summary>
Given imbalanced data, it is hard to train a good classifier using deep learning because of the poor generalization of minority classes. Traditionally, the well-known synthetic minority oversampling technique (SMOTE) for data augmentation, a data mining approach for imbalanced learning, has been used to improve this generalization. However, it is unclear whether SMOTE also benefits deep learning. In this work, we study why the original SMOTE is insufficient for deep learning, and enhance SMOTE using soft labels. Connecting the resulting soft SMOTE with Mixup, a modern data augmentation technique, leads to a unified framework that puts traditional and modern data augmentation techniques under the same umbrella. A careful study within this framework shows that Mixup improves generalization by implicitly achieving uneven margins between majority and minority classes. We then propose a novel margin-aware Mixup technique that more explicitly achieves uneven margins. Extensive experimental results demonstrate that our proposed technique yields state-of-the-art performance on deep imbalanced classification while achieving superior performance on extremely imbalanced data. The code is open-sourced in our developed package https://github.com/ntucllab/imbalanced-DL to foster future research in this direction.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-Do-Program-of-Thoughts-Work-for-Reasoning"><a href="#When-Do-Program-of-Thoughts-Work-for-Reasoning" class="headerlink" title="When Do Program-of-Thoughts Work for Reasoning?"></a>When Do Program-of-Thoughts Work for Reasoning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15452">http://arxiv.org/abs/2308.15452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/easyinstruct">https://github.com/zjunlp/easyinstruct</a></li>
<li>paper_authors: Zhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, Huajun Chen</li>
<li>for: 本研究旨在探讨 Large Language Models (LLMs) 在肢体人工智能领域的逻辑能力如何提高，以及程序语言的影响。</li>
<li>methods: 本研究提出了一种 complexity-impacted reasoning score (CIRS)，用于衡量程序语言的结构和逻辑特性对逻辑能力的影响。CIRS 使用抽象树来编码结构信息，并通过考虑困难性和环状复杂性来计算逻辑复杂性。</li>
<li>results: 研究发现，不同的程序数据复杂性会影响 LLMS 的逻辑能力提升。优化的复杂度是关键的，程序帮助提示可以提高逻辑能力。研究还提出了一种自动生成和分级算法，并应用于数学逻辑和代码生成任务。广泛的结果表明了我们的提出的方法的有效性。<details>
<summary>Abstract</summary>
The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach. Code will be integrated into the EasyInstruct framework at https://github.com/zjunlp/EasyInstruct.
</details>
<details>
<summary>摘要</summary>
LLMs的逻辑能力在人工智能中扮演着关键性角色。虽有有效的程序激活提示法 для LLMs，但代码数据对复杂逻辑任务的改进仍然未得到足够的探讨。为解决这个空白，我们提出了复杂性影响逻辑能力指数（CIRS），它将结构性和逻辑性特征相结合，用于衡量代码数据对逻辑能力的相关性。我们使用抽象树来编码结构信息，并根据难度和 cyclomatic complexity来计算逻辑复杂性。经 empirical 分析发现，不 всех复杂度的代码数据可以被 LLMS 学习或理解。优质的复杂度是关键的，以便通过程序帮助提示来提高逻辑能力。然后，我们设计了自动生成和分配算法，并应用它到数学逻辑和代码生成任务中。广泛的结果表明了我们的提出的方法的有效性。代码将会被集成到 EasyInstruct 框架中，可以在 <https://github.com/zjunlp/EasyInstruct> 中找到。
</details></li>
</ul>
<hr>
<h2 id="Complementing-Onboard-Sensors-with-Satellite-Map-A-New-Perspective-for-HD-Map-Construction"><a href="#Complementing-Onboard-Sensors-with-Satellite-Map-A-New-Perspective-for-HD-Map-Construction" class="headerlink" title="Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction"></a>Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15427">http://arxiv.org/abs/2308.15427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Gao, Jiawei Fu, Haodong Jing, Nanning Zheng</li>
<li>for: 提高自动驾驶系统中的高清晰地图建构方法的性能，使其更敏感于废弃环境。</li>
<li>methods: 补充车载感知器上的信息使用卫星地图，提高HD地图建构方法的性能。提出一种层次融合模块，通过Feature级别融合和BEV级别融合来实现卫星地图信息的更好融合。</li>
<li>results: 在扩展nuScenes数据集上，证明了我们的模块可以准确地融合到现有的HD地图建构方法中，提高其在HD地图Semantic segmentation和实例检测任务中的性能。<details>
<summary>Abstract</summary>
High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboard Bird's Eye View (BEV) features and satellite features in feature-level fusion. An alignment module is introduced before concatenation in BEV-level fusion to mitigate the impact of misalignment between the two features. The experimental results on the augmented nuScenes dataset showcase the seamless integration of our module into three existing HD map construction methods. It notably enhances their performance in both HD map semantic segmentation and instance detection tasks.
</details>
<details>
<summary>摘要</summary>
高清定义（HD）地图在自动驾驶系统中扮演着关键角色。现有方法尝试在实时基础上构建HD地图，但这些方法的性能受周围环境的影响很大，因为搭载在车辆上的感知器件具有较弱的远程探测能力。在本研究中，我们发现可以通过补充搭载在车辆上的感知器件与卫星地图的信息来提高HD地图构建方法的性能。为进一步研究，我们释放了卫星地图块作为nuScenes数据集的补充数据集。此外，我们提议了一种层次融合模块，使得更好地融合卫星地图信息与现有方法。具体来说，我们设计了一个基于分割和距离的注意mask，通过交叉注意机制来融合搭载在 Bird's Eye View（BEV）上的特征和卫星特征在特征层融合。在BEV层融合之前，我们引入了一个对齐模块，以mitigate卫星特征和BEV特征之间的偏移的影响。实验结果表明，我们的模块可以覆盖现有的三种HD地图构建方法，并在HD地图semantic segmentation和实例检测任务中显著提高其性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.AI_2023_08_30/" data-id="clot2mh890033x7881v0uhe9k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.CL_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T11:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.CL_2023_08_30/">cs.CL - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Vulgar-Remarks-Detection-in-Chittagonian-Dialect-of-Bangla"><a href="#Vulgar-Remarks-Detection-in-Chittagonian-Dialect-of-Bangla" class="headerlink" title="Vulgar Remarks Detection in Chittagonian Dialect of Bangla"></a>Vulgar Remarks Detection in Chittagonian Dialect of Bangla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15448">http://arxiv.org/abs/2308.15448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanjim Mahmud, Michal Ptaszynski, Fumito Masui</li>
<li>for: 本研究旨在探讨社交媒体上的负面言语 automatic detection方法，尤其是在低资源语言如锡兰语方言上。</li>
<li>methods: 本研究使用了指导学习和深度学习算法来检测社交媒体上的侮辱言语。Logistic Regression实现了可观的准确率（0.91），而简单的RNN具有Word2vec和fastTex的组合实现了较低的准确率（0.84-0.90），这说明了NN算法需要更多的数据。</li>
<li>results: 本研究显示，使用指导学习和深度学习算法可以准确地检测社交媒体上的侮辱言语，但是NN算法需要更多的数据以实现更高的准确率。<details>
<summary>Abstract</summary>
The negative effects of online bullying and harassment are increasing with Internet popularity, especially in social media. One solution is using natural language processing (NLP) and machine learning (ML) methods for the automatic detection of harmful remarks, but these methods are limited in low-resource languages like the Chittagonian dialect of Bangla.This study focuses on detecting vulgar remarks in social media using supervised ML and deep learning algorithms.Logistic Regression achieved promising accuracy (0.91) while simple RNN with Word2vec and fastTex had lower accuracy (0.84-0.90), highlighting the issue that NN algorithms require more data.
</details>
<details>
<summary>摘要</summary>
互联网欺凌和 Harrasment 的负面影响随着互联网的普及而增加，尤其在社交媒体上。一种解决方案是使用自然语言处理（NLP）和机器学习（ML）方法进行自动发现危险评论，但这些方法在低资源语言如锡兰语的拼写方言上有限。本研究关注社交媒体中的粗鄙评论使用监督式 ML 和深度学习算法探测。Logistic Regression 达到了可靠的准确率（0.91），而简单的 RNN 与 Word2vec 和 fastTex 的准确率（0.84-0.90）较低，表明 NN 算法需要更多的数据。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Learning-Curves-During-Language-Model-Pre-Training-Learning-Forgetting-and-Stability"><a href="#Characterizing-Learning-Curves-During-Language-Model-Pre-Training-Learning-Forgetting-and-Stability" class="headerlink" title="Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability"></a>Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15419">http://arxiv.org/abs/2308.15419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tylerachang/lm-learning-curves">https://github.com/tylerachang/lm-learning-curves</a></li>
<li>paper_authors: Tyler A. Chang, Zhuowen Tu, Benjamin K. Bergen</li>
<li>for: 这些语言模型在预训练中学习的问题是什么？</li>
<li>methods: 这些语言模型在预训练中使用了什么方法？</li>
<li>results: 这些语言模型在预训练中得到了什么结果？Here are the answers in Simplified Chinese text:</li>
<li>for: 这些语言模型在预训练中学习的问题是如何快速预测语言模型的性能？</li>
<li>methods: 这些语言模型在预训练中使用了自适应语言模型的预训练方法？</li>
<li>results: 这些语言模型在预训练中得到了快速预测语言模型的稳定性和性能？<details>
<summary>Abstract</summary>
How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be "forgotten" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives. Our work contributes to a better understanding of language model pre-training dynamics and informs the deployment of stable language models in practice.
</details>
<details>
<summary>摘要</summary>
<<SYS>>我们使用五个权重autoregressive英语语言模型的预训练运行来研究语言模型如何预测。我们从100万个字Context中提取学习曲线，并观察到语言模型在预训练过程中首先生成短重复的短语，然后学习 longer和更 coherent的文本。我们量化每个Token在Context中的最终难度、内Run变化、年龄 acquisition、忘记性和 Cross-Run变化。我们发现更常见的Token在Context中更容易预测， exhibit less variability within和across预训练运行，learn Earlier，并更可能被"忘记" during预训练。高 n-gram概率更加强调这些效果。独立于目标Token，更短和更频繁的Contexts呈现marginally more stable and quickly acquired预测。 parts of speech的影响也很小，although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives。我们的工作对语言模型预训练动力学的更好理解，并可以帮助实践中稳定地部署语言模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.CL_2023_08_30/" data-id="clot2mhae00a7x78850zf0dxr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_30" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/30/cs.LG_2023_08_30/" class="article-date">
  <time datetime="2023-08-30T10:00:00.000Z" itemprop="datePublished">2023-08-30</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/30/cs.LG_2023_08_30/">cs.LG - 2023-08-30</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Policy-composition-in-reinforcement-learning-via-multi-objective-policy-optimization"><a href="#Policy-composition-in-reinforcement-learning-via-multi-objective-policy-optimization" class="headerlink" title="Policy composition in reinforcement learning via multi-objective policy optimization"></a>Policy composition in reinforcement learning via multi-objective policy optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15470">http://arxiv.org/abs/2308.15470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shruti Mishra, Ankit Anand, Jordan Hoffmann, Nicolas Heess, Martin Riedmiller, Abbas Abdolmaleki, Doina Precup</li>
<li>for: 本研究旨在使用相关的先进教学策略来帮助强化学习代理人学习成功的行为策略。</li>
<li>methods: 本研究使用了多目标策略优化算法（Multi-Objective Maximum a Posteriori Policy Optimization，简称MOPPO），其中包括任务目标以及教学策略作为多个目标。</li>
<li>results: 研究表明，在继续使用教学策略的情况下，强化学习代理人可以更快速地学习任务，特别是在缺乏形成奖励的情况下。在连续观察和动作空间的两个领域中，我们的代理人成功地组合了教学策略序列和并行，并能够进一步扩展教学策略以解决任务。<details>
<summary>Abstract</summary>
We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies. The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting. Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm \citep{abdolmaleki2020distributional}, we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards. In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task.   Depending on the specified combination of task and teacher(s), teacher(s) may naturally act to limit the final performance of an agent. The extent to which agents are required to adhere to teacher policies are determined by hyperparameters which determine both the effect of teachers on learning speed and the eventual performance of the agent on the task. In the {\tt humanoid} domain \citep{deepmindcontrolsuite2018}, we also equip agents with the ability to control the selection of teachers. With this ability, agents are able to meaningfully compose from the teacher policies to achieve a superior task reward on the {\tt walk} task than in cases without access to the teacher policies. We show the resemblance of composed task policies with the corresponding teacher policies through videos.
</details>
<details>
<summary>摘要</summary>
我们使用已有的教师策略来帮助权威学习代理人学习成功行为策略。教师策略被引入为目标之一，同时与任务目标一起使用多目标策略优化算法 \citep{abdolmaleki2020distributional}。我们在连续观察和动作空间的两个领域中表示，我们的代理人可以成功组合教师策略并且可以进一步扩展教师策略以解决任务。在指定的任务和教师的组合下，教师可能会自然地限制代理人的最终表现。代理人需要遵循教师策略的程度由参数决定，这些参数不仅影响代理人学习速度，还影响代理人在任务上的最终表现。在{\tt humanoid}领域 \citep{deepmindcontrolsuite2018}中，我们还让代理人控制选择教师的能力。通过这种能力，代理人能够有意义地从教师策略中组合任务策略，在{\tt walk}任务上比不使用教师策略的情况下更好的完成任务。我们通过视频显示，组合的任务策略与相应的教师策略之间的相似性。
</details></li>
</ul>
<hr>
<h2 id="Random-feature-approximation-for-general-spectral-methods"><a href="#Random-feature-approximation-for-general-spectral-methods" class="headerlink" title="Random feature approximation for general spectral methods"></a>Random feature approximation for general spectral methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15434">http://arxiv.org/abs/2308.15434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mike Nguyen, Nicole Mücke</li>
<li>for: 本文研究了大规模算法中使用权重方法的减速技术，以及深度神经网络的分析方法。</li>
<li>methods: 本文使用了随机特征approximation技术，并对权重方法进行了分析。</li>
<li>results: 本文对权重方法的泛化性质进行了研究，并在不同的常数空间中获得了优化的学习率。<details>
<summary>Abstract</summary>
Random feature approximation is arguably one of the most popular techniques to speed up kernel methods in large scale algorithms and provides a theoretical approach to the analysis of deep neural networks. We analyze generalization properties for a large class of spectral regularization methods combined with random features, containing kernel methods with implicit regularization such as gradient descent or explicit methods like Tikhonov regularization. For our estimators we obtain optimal learning rates over regularity classes (even for classes that are not included in the reproducing kernel Hilbert space), which are defined through appropriate source conditions. This improves or completes previous results obtained in related settings for specific kernel algorithms.
</details>
<details>
<summary>摘要</summary>
随机特征近似是大规模算法中最受欢迎的技术之一，它提供了对深度神经网络的分析理论方法。我们分析了一大类spectral regularization方法，包括梯度下降或特ikhonov regularization等kernel方法，其中的泛化性质得到了改进或完善。我们的估计器可以在不同的常数类型下获得最佳学习速率，这些常数类型包括 reproduce kernel Hilbert space以外的类型。这些结果与之前在相关的设置中获得的结果相匹配或完善。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-solar-flare-forecasting-using-historical-magnetogram-data"><a href="#Probabilistic-solar-flare-forecasting-using-historical-magnetogram-data" class="headerlink" title="Probabilistic solar flare forecasting using historical magnetogram data"></a>Probabilistic solar flare forecasting using historical magnetogram data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.15410">http://arxiv.org/abs/2308.15410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swri-idea-lab/idea-lab-flare-forecast">https://github.com/swri-idea-lab/idea-lab-flare-forecast</a></li>
<li>paper_authors: Kiera van der Sande, Andrés Muñoz-Jaramillo, Subhamoy Chatterjee</li>
<li>for: 这个论文旨在利用机器学习技术预测太阳风暴。</li>
<li>methods: 这篇论文使用了卷积神经网络提取全天磁图像的特征，并将这些特征与磁图像和风暴历史信息相结合，使用了逻辑回归模型生成可カリibrated的风暴预测。</li>
<li>results: 包括历史数据在内的多种仪器的日常磁图像数据可以提高预测的准确性和可靠性，磁图像单一帧不含 significatively更多有用信息，而风暴历史信息比我们提取的磁图像特征更具预测力。<details>
<summary>Abstract</summary>
Solar flare forecasting research using machine learning (ML) has focused on high resolution magnetogram data from the SDO/HMI era covering Solar Cycle 24 and the start of Solar Cycle 25, with some efforts looking back to SOHO/MDI for data from Solar Cycle 23. In this paper, we consider over 4 solar cycles of daily historical magnetogram data from multiple instruments. This is the first attempt to take advantage of this historical data for ML-based flare forecasting. We apply a convolutional neural network (CNN) to extract features from full-disk magnetograms together with a logistic regression model to incorporate scalar features based on magnetograms and flaring history. We use an ensemble approach to generate calibrated probabilistic forecasts of M-class or larger flares in the next 24 hours. Overall, we find that including historical data improves forecasting skill and reliability. We show that single frame magnetograms do not contain significantly more relevant information than can be summarized in a small number of scalar features, and that flaring history has greater predictive power than our CNN-extracted features. This indicates the importance of including temporal information in flare forecasting models.
</details>
<details>
<summary>摘要</summary>
太阳风暴预测研究使用机器学习（ML）专注于高分辨率磁场图像从SDO/HMI时期的太阳周期24和太阳周期25之前的一些努力，也有一些努力回到SOHO/MDI上的数据。在这篇论文中，我们考虑了4个太阳周期的日常历史磁场数据从多个仪器。这是第一次利用历史数据为ML-基于的太阳风暴预测。我们使用卷积神经网络（CNN）提取磁场图像的特征，并将磁场图像和风暴历史中的一些缺失特征加以逻辑回归模型。我们使用一个集成方法生成标准化的可信度预测M级或大于M级太阳风暴在下一个24小时内发生。总的来说，我们发现包含历史数据可以提高预测技巧和可靠性。我们显示单个帧磁场图像不含有足够重要的信息，而风暴历史更有预测力量。这表明包含时间信息在太阳风暴预测模型中是非常重要的。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/30/cs.LG_2023_08_30/" data-id="clot2mhf000p0x78874ftd0d0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/51/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/50/">50</a><a class="page-number" href="/page/51/">51</a><span class="page-number current">52</span><a class="page-number" href="/page/53/">53</a><a class="page-number" href="/page/54/">54</a><span class="space">&hellip;</span><a class="page-number" href="/page/90/">90</a><a class="extend next" rel="next" href="/page/53/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">130</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">122</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">61</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">70</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">65</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
