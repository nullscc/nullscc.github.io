
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/25/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-eess.IV_2023_10_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/07/eess.IV_2023_10_07/" class="article-date">
  <time datetime="2023-10-07T09:00:00.000Z" itemprop="datePublished">2023-10-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/07/eess.IV_2023_10_07/">eess.IV - 2023-10-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Hardware-Algorithm-Co-design-Enabling-Processing-in-Pixel-in-Memory-P2M-for-Neuromorphic-Vision-Sensors"><a href="#Hardware-Algorithm-Co-design-Enabling-Processing-in-Pixel-in-Memory-P2M-for-Neuromorphic-Vision-Sensors" class="headerlink" title="Hardware-Algorithm Co-design Enabling Processing-in-Pixel-in-Memory (P2M) for Neuromorphic Vision Sensors"></a>Hardware-Algorithm Co-design Enabling Processing-in-Pixel-in-Memory (P2M) for Neuromorphic Vision Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.16844">http://arxiv.org/abs/2310.16844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Abdullah-Al Kaiser, Akhilesh R. Jaiswal</li>
<li>for: 这篇论文的目的是为了解决边缘设备具有计算能力限制的问题，尤其是对于计算机见的应用，以节省能源和传输带宽。</li>
<li>methods: 这篇论文使用了不同的方法，包括靠近感应器处理、内部感应器处理和内部像素处理，以将计算进行更加靠近感应器，从而节省传输带宽。特别是在像素中进行的内部像素处理，通过将不同的操作结合在一起，以提高能效性。</li>
<li>results: 这篇论文的结果显示，这些方法可以提高边缘设备的能效性和传输带宽，并且可以降低训练时间和能源消耗。此外，这篇论文还提出了一些硬件设计和数据分析技术，以提高内部像素处理的泄漏性能。<details>
<summary>Abstract</summary>
The high volume of data transmission between the edge sensor and the cloud processor leads to energy and throughput bottlenecks for resource-constrained edge devices focused on computer vision. Hence, researchers are investigating different approaches (e.g., near-sensor processing, in-sensor processing, in-pixel processing) by executing computations closer to the sensor to reduce the transmission bandwidth. Specifically, in-pixel processing for neuromorphic vision sensors (e.g., dynamic vision sensors (DVS)) involves incorporating asynchronous multiply-accumulate (MAC) operations within the pixel array, resulting in improved energy efficiency. In a CMOS implementation, low overhead energy-efficient analog MAC accumulates charges on a passive capacitor; however, the capacitor's limited charge retention time affects the algorithmic integration time choices, impacting the algorithmic accuracy, bandwidth, energy, and training efficiency. Consequently, this results in a design trade-off on the hardware aspect-creating a need for a low-leakage compute unit while maintaining the area and energy benefits. In this work, we present a holistic analysis of the hardware-algorithm co-design trade-off based on the limited integration time posed by the hardware and techniques to improve the leakage performance of the in-pixel analog MAC operations.
</details>
<details>
<summary>摘要</summary>
因为边缘设备的数据传输量过高，导致边缘设备具有限制的资源表现出能量和吞吐瓶颈问题。因此，研究人员正在调查不同的方法（如靠近传感器处理、在传感器处理、在像素处理），以便在传感器处理计算更近，减少传输带宽。特别是在像素处理方面，在神经网络感知器（如动态视sensors（DVS））中包含异步多乘法（MAC）操作，可以提高能效性。在CMOS实现中，低负荷能效的分析器可以在passive capacitor上储存电荷，但限制电容器的储存时间影响算法集成时间选择，从而影响算法的准确率、带宽、能效和训练效率。因此，这会导致硬件方面的设计决策——创造低泄漏计算单元，同时维持面积和能效的优点。在这种工作中，我们提供了硬件-算法共设计的硬件限制和提高泄漏性的技术分析。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/07/eess.IV_2023_10_07/" data-id="closbroyr01810g88brnb0u0l" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/07/eess.SP_2023_10_07/" class="article-date">
  <time datetime="2023-10-07T08:00:00.000Z" itemprop="datePublished">2023-10-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/07/eess.SP_2023_10_07/">eess.SP - 2023-10-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="OTFS-based-Joint-Radar-and-Communication-Signal-Analysis-using-the-Ambiguity-Function"><a href="#OTFS-based-Joint-Radar-and-Communication-Signal-Analysis-using-the-Ambiguity-Function" class="headerlink" title="OTFS based Joint Radar and Communication: Signal Analysis using the Ambiguity Function"></a>OTFS based Joint Radar and Communication: Signal Analysis using the Ambiguity Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04947">http://arxiv.org/abs/2310.04947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shalanika Dayarathna, Peter Smith, Rajitha Senanayake, Jamie Evans</li>
<li>for: 这篇论文旨在研究 ортогональ时频空间（OTFS）模调在共同雷达和通信系统中的适用性。</li>
<li>methods: 作者通过分析数据模调对雷达探测性能的影响， derivation of the ambiguity function（AF）of the OTFS waveform， and characterization of the radar global accuracy。</li>
<li>results: 作者通过分析数据分布对AF的行为进行了准确的approximation，并证明OTFS波形在雷达性能方面的全球性能与OFDM波形相当。<details>
<summary>Abstract</summary>
Orthogonal time frequency space (OTFS) modulation has recently been identified as a suitable waveform for joint radar and communication systems. Focusing on the effect of data modulation on the radar sensing performance, we derive the ambiguity function (AF) of the OTFS waveform and characterize the radar global accuracy. We evaluate the behavior of the AF with respect to the distribution of the modulated data and derive an accurate approximation for the mean and variance of the AF, thus, approximating its distribution by a Rice distribution. Finally, we evaluate the global radar performance of the OTFS waveform with the OFDM waveform.
</details>
<details>
<summary>摘要</summary>
Orthogonal time frequency space (OTFS) 模ulation 已经被认为是合适的探测和通信系统之waveform。我们专注于数据模ulation对探测性能的影响， derivation ambiguity function (AF) 的OTFS 波形，并Characterize 激光全球精度。我们分析了对于数据分布的影响，并 derive 精度和方差的准确估计，因此可以简化 AF 的分布为Rice distribution。最后，我们评估了 OTFS 波形和 OFDM 波形之间的全球激光性能。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Linear-Least-Squares-Estimation-of-Fiber-Longitudinal-Optical-Power-Profile"><a href="#Linear-Least-Squares-Estimation-of-Fiber-Longitudinal-Optical-Power-Profile" class="headerlink" title="Linear Least Squares Estimation of Fiber-Longitudinal Optical Power Profile"></a>Linear Least Squares Estimation of Fiber-Longitudinal Optical Power Profile</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04936">http://arxiv.org/abs/2310.04936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takeo Sasai, Minami Takahashi, Masanori Nakamura, Etsushi Yamazaki, Yoshiaki Kisaka</li>
<li>for: 这篇论文提出了一种线性最小二乘方法用于光纤 longitudinal 功率Profile 估算 (PPE)，该方法可以在光纤通信系统中高精度地估算光信号力度分布。</li>
<li>methods: 该方法使用线性最小二乘方法来估算光纤中的力度分布，并通过全球最优化来找到最优的估算结果。</li>
<li>results: 实验结果表明，该方法可以准确地估算光纤中的力度分布，RMS 误差为 0.18 dB。此外，该方法还能够成功地检测到小于 0.77 dB 的损失缺陷。<details>
<summary>Abstract</summary>
This paper presents a linear least squares method for fiber-longitudinal power profile estimation (PPE), which estimates an optical signal power distribution throughout a fiber-optic link at a coherent receiver. The method finds the global optimum in least square estimation of longitudinal power profiles, thus closely matching true optical power profiles and locating loss anomalies in a link with high spatial resolution. Experimental results show that the method achieves accurate PPE with an RMS error from OTDR of 0.18 dB. Consequently, it successfully identifies a loss anomaly as small as 0.77 dB, demonstrating the potential of a coherent receiver in locating even splice and connector losses. The method is also evaluated under a WDM condition with optimal system fiber launch power, highlighting its feasibility for use in operations. Furthermore, a fundamental limit for stable estimation and spatial resolution of least-squares-based PPE is quantitatively discussed in relation to the ill-posedness of PPE by evaluating the condition number of a nonlinear perturbation matrix.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "fiber-longitudinal" is translated as "长itudinal" (cháng yǐng) to emphasize the direction of the power profile estimation.* "optical signal power distribution" is translated as "光学信号电压分布" (guāng yǐng xìn yuè diàn yuè fēn bù) to emphasize the distribution of the optical signal power.* "coherent receiver" is translated as "同步接收器" (tóng xù jì huò) to emphasize the type of receiver used in the method.* "loss anomalies" is translated as "损失异常" (shèng shí yì cháng) to emphasize the type of anomalies detected by the method.* "high spatial resolution" is translated as "高空间分辨率" (gāo kōng jìan fēn biéng rù) to emphasize the accuracy of the method in locating the loss anomalies.* "WDM condition" is translated as "WDM条件" (WDM tiáo jiàn) to emphasize the specific condition under which the method is evaluated.* "optimal system fiber launch power" is translated as "最佳系统纤维发射功率" (zuì jì system fàng xiàng yì yì) to emphasize the importance of the launch power in the method.
</details></li>
</ul>
<hr>
<h2 id="A-Grouping-based-Scheduler-for-Efficient-Channel-Utilization-under-Age-of-Information-Constraints"><a href="#A-Grouping-based-Scheduler-for-Efficient-Channel-Utilization-under-Age-of-Information-Constraints" class="headerlink" title="A Grouping-based Scheduler for Efficient Channel Utilization under Age of Information Constraints"></a>A Grouping-based Scheduler for Efficient Channel Utilization under Age of Information Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04817">http://arxiv.org/abs/2310.04817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lehan Wang, Jingzhou Sun, Yuxuan Sun, Sheng Zhou, Zhisheng Niu</li>
<li>for: 这个论文是为了解决一个复杂的大规模状态更新系统中的一个问题，即一个融合中心从多个来源收集状态信息，每个来源都有其自己的年龄信息（AoI）约束。</li>
<li>methods: 该论文提出了一种分组方法，即将来源分为不同的分组，以解决这个复杂的问题。具体来说，首先将来源按照AoI约束进行分组，然后为每个分组设计优化的协调器。</li>
<li>results:  simulation 结果显示，提出的二步分组算法（TGA）可以减少通道使用率，与一个先前的工作相比，在许多情况下显著减少了channel使用率，并且与一个 derive 的下界相比，当有多个来源时，channel使用率为0.42%。<details>
<summary>Abstract</summary>
We consider a status information updating system where a fusion center collects the status information from a large number of sources and each of them has its own age of information (AoI) constraints. A novel grouping-based scheduler is proposed to solve this complex large-scale problem by dividing the sources into different scheduling groups. The problem is then transformed into deriving the optimal grouping scheme. A two-step grouping algorithm (TGA) is proposed: 1) Given AoI constraints, we first identify the sources with harmonic AoI constraints, then design a fast grouping method and an optimal scheduler for these sources. Under harmonic AoI constraints, each constraint is divisible by the smallest one and the sum of reciprocals of the constraints with the same value is divisible by the reciprocal of the smallest one. 2) For the other sources without such a special property, we pack the sources which can be scheduled together with minimum update rates into the same group. Simulations show the channel usage of the proposed TGA is significantly reduced as compared to a recent work and is 0.42% larger than a derived lower bound when the number of sources is large.
</details>
<details>
<summary>摘要</summary>
我团队考虑了一个状态信息更新系统，该系统中的拥有者中心从多个来源收集状态信息，每个来源都有自己的年龄信息（AoI）约束。我们提出了一种分组方式的计划器，用于解决这个复杂的大规模问题。问题转化为找到最佳分组方案。我们提出了两步分组算法（TGA）：1. 给定AoI约束，我们首先将来源分为具有幂等AoI约束的源组和其他无特殊性质的源组。在幂等AoI约束下，每个约束都是最小一个的分母，并且所有具有相同值的约束的 reciprocal 之和是最小一个的分母的reciprocal的分母。2. 对于其他无特殊性质的来源，我们将具有最小更新频率的来源打包在同一组中。实验表明，提案的TGA Channel使用率significantly reduced compared to recent work, and is 0.42% larger than a derived lower bound when the number of sources is large.
</details></li>
</ul>
<hr>
<h2 id="Age-of-Information-Guaranteed-Scheduling-for-Asynchronous-Status-Updates-in-Collaborative-Perception"><a href="#Age-of-Information-Guaranteed-Scheduling-for-Asynchronous-Status-Updates-in-Collaborative-Perception" class="headerlink" title="Age of Information Guaranteed Scheduling for Asynchronous Status Updates in Collaborative Perception"></a>Age of Information Guaranteed Scheduling for Asynchronous Status Updates in Collaborative Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04813">http://arxiv.org/abs/2310.04813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lehan Wang, Jingzhou Sun, Yuxuan Sun, Sheng Zhou, Zhisheng Niu</li>
<li>for: The paper is written for collaborative perception (CP) systems where a fusion center monitors various regions using multiple sources, and the center has different age of information (AoI) constraints for different regions.</li>
<li>methods: The paper proposes an algorithm called scheduling for CP with asynchronous status updates (SCPA) to minimize the number of required channels and subject to AoI constraints with asynchronous status updates.</li>
<li>results: According to numerical results, the number of channels required by SCPA can reach only 12% more than a derived lower bound.<details>
<summary>Abstract</summary>
We consider collaborative perception (CP) systems where a fusion center monitors various regions by multiple sources. The center has different age of information (AoI) constraints for different regions. Multi-view sensing data for a region generated by sources can be fused by the center for a reliable representation of the region. To ensure accurate perception, differences between generation time of asynchronous status updates for CP fusion should not exceed a certain threshold. An algorithm named scheduling for CP with asynchronous status updates (SCPA) is proposed to minimize the number of required channels and subject to AoI constraints with asynchronous status updates. SCPA first identifies a set of sources that can satisfy the constraints with minimum updating rates. It then chooses scheduling intervals and offsets for the sources such that the number of required channels is optimized. According to numerical results, the number of channels required by SCPA can reach only 12% more than a derived lower bound.
</details>
<details>
<summary>摘要</summary>
我们考虑了协同感知（CP）系统，其中统计中心监控多个区域，并且从多个来源获取多元观察数据。中心具有不同的资讯年龄（AoI）限制 для不同的区域。多元感知数据 для一个区域由来源生成，可以在中心进行融合，以获得区域的可靠表现。确保正确感知，协同感知融合中的不同时间生成的 asynchronous status updates 差异应小于一定阈值。一个名为协同感知 avec asynchronous status updates 的算法（SCPA）被提出，以最小化需要的通道数量，并且遵循 AoI 限制。SCPA 首先 identific 一群可以满足限制的来源，然后选择这些来源的调度间隔和偏移量，以便优化通道数量。根据数据显示，SCPA 可以对需要的通道数量进行最小化，与一个 derive 的下限相差只有12%。
</details></li>
</ul>
<hr>
<h2 id="Score-based-Diffusion-Models-With-Self-supervised-Learning-For-Accelerated-3D-Multi-contrast-Cardiac-Magnetic-Resonance-Imaging"><a href="#Score-based-Diffusion-Models-With-Self-supervised-Learning-For-Accelerated-3D-Multi-contrast-Cardiac-Magnetic-Resonance-Imaging" class="headerlink" title="Score-based Diffusion Models With Self-supervised Learning For Accelerated 3D Multi-contrast Cardiac Magnetic Resonance Imaging"></a>Score-based Diffusion Models With Self-supervised Learning For Accelerated 3D Multi-contrast Cardiac Magnetic Resonance Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04669">http://arxiv.org/abs/2310.04669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Liu, Zhuo-Xu Cui, Congcong Liu, Hairong Zheng, Haifeng Wang, Yihang Zhou, Yanjie Zhu<br>for:这种研究旨在加速3D-MC-CMR成像过程，以提高其广泛应用的可行性。methods:研究人员提出了一种基于自我监督学习的新方法，使用得分基于扩散模型来加速3D-MC-CMR成像。这种方法首先建立了快照测量和MR图像之间的映射，然后使用自我监督 Bayesian 重建网络来恢复MR图像。最后，研究人员开发了一种三维分布的分数基于扩散模型，以捕捉3D-MC-CMR图像的自然分布。results:实验结果表明，该方法比传统的压缩感知和现有的自我监督深度学习MRI重建方法更高效。它还可以在高速度压缩率14的情况下获得高质量的T1和T1rho参数地图，与参照地图相似。<details>
<summary>Abstract</summary>
Long scan time significantly hinders the widespread applications of three-dimensional multi-contrast cardiac magnetic resonance (3D-MC-CMR) imaging. This study aims to accelerate 3D-MC-CMR acquisition by a novel method based on score-based diffusion models with self-supervised learning. Specifically, we first establish a mapping between the undersampled k-space measurements and the MR images, utilizing a self-supervised Bayesian reconstruction network. Secondly, we develop a joint score-based diffusion model on 3D-MC-CMR images to capture their inherent distribution. The 3D-MC-CMR images are finally reconstructed using the conditioned Langenvin Markov chain Monte Carlo sampling. This approach enables accurate reconstruction without fully sampled training data. Its performance was tested on the dataset acquired by a 3D joint myocardial T1 and T1rho mapping sequence. The T1 and T1rho maps were estimated via a dictionary matching method from the reconstructed images. Experimental results show that the proposed method outperforms traditional compressed sensing and existing self-supervised deep learning MRI reconstruction methods. It also achieves high quality T1 and T1rho parametric maps close to the reference maps obtained by traditional mapping sequences, even at a high acceleration rate of 14.
</details>
<details>
<summary>摘要</summary>
长时间扫描减少了三维多contrast室内Magnetic Resonance成像（3D-MC-CMR）的广泛应用。本研究目的是加速3D-MC-CMR获取的速度。我们采用了一种基于得分分布模型的自我超vised学习方法。首先，我们使用一种自我超vised Bayesian重建网络将不完全样本空间测量映射到MR图像中。其次，我们开发了一种三维分布Score-based扩散模型，以捕捉3D-MC-CMR图像的内在分布。最后，我们使用 conditioned Langenvin Markov chain Monte Carlo采样来重建图像。这种方法可以准确重建图像，不需要完全的样本数据。我们对一个3D联合肌肉T1和T1rho映射序列上获取的数据进行了实验测试。结果表明，我们的方法比传统的压缩感知和现有的自我超vised深度学习MRI重建方法更好。它还可以在高速度减少14%的情况下获得高质量的T1和T1rho参数图像，与传统映射序列中的参考图像几乎相同。
</details></li>
</ul>
<hr>
<h2 id="Space-Observation-by-the-Australia-Telescope-Compact-Array-Performance-Characterization-using-GPS-Satellite-Observation"><a href="#Space-Observation-by-the-Australia-Telescope-Compact-Array-Performance-Characterization-using-GPS-Satellite-Observation" class="headerlink" title="Space Observation by the Australia Telescope Compact Array: Performance Characterization using GPS Satellite Observation"></a>Space Observation by the Australia Telescope Compact Array: Performance Characterization using GPS Satellite Observation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04653">http://arxiv.org/abs/2310.04653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamed Nosrati, Stephanie Smith, Douglas B. Hayman</li>
<li>for: 用于升级澳大利亚天体望远镜数据启用空间卫星定位应用</li>
<li>methods: 基于干扰数据的系统模型，实现距离和方向估算</li>
<li>results: 与最新的二线元素（TLE）进行比较，显示距离和方向信息都得到了明显改善<details>
<summary>Abstract</summary>
In order to operationalize the Australia Telescope Compact Array (ATCA) for space situational awareness (SSA) applications, we develop a system model for range and direction of arrival (DOA) estimation based on the interferometric data. We employ the observational data collected from global positioning system (GPS) satellites to evaluate the developed model and demonstrate that, compared to a priori location propagated from the most recent two-line element (TLE), both range and direction information are improved significantly.
</details>
<details>
<summary>摘要</summary>
为了使澳大利亚望远镜数组（ATCA）用于空间定位意识（SSA）应用，我们开发了基于互相干涉数据的系统模型，以便估算距离和方向信息。我们使用全球定位系统（GPS）卫星的观测数据来评估我们开发的模型，并证明在比之前两行元素（TLE）的位置进行质量提高后，距离和方向信息都得到了显著改善。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/07/eess.SP_2023_10_07/" data-id="closbrp0701bm0g8883rscrqy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/cs.SD_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T15:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/cs.SD_2023_10_06/">cs.SD - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DPM-TSE-A-Diffusion-Probabilistic-Model-for-Target-Sound-Extraction"><a href="#DPM-TSE-A-Diffusion-Probabilistic-Model-for-Target-Sound-Extraction" class="headerlink" title="DPM-TSE: A Diffusion Probabilistic Model for Target Sound Extraction"></a>DPM-TSE: A Diffusion Probabilistic Model for Target Sound Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04567">http://arxiv.org/abs/2310.04567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiarui Hai, Helin Wang, Dongchao Yang, Karan Thakkar, Najim Dehak, Mounya Elhilali</li>
<li>for: target sound extraction (TSE)</li>
<li>methods:  diffusion probabilistic modeling (DPM)</li>
<li>results:  cleaner target renderings and improved separability from unwanted sounds, with significant improvement in perceived quality<details>
<summary>Abstract</summary>
Common target sound extraction (TSE) approaches primarily relied on discriminative approaches in order to separate the target sound while minimizing interference from the unwanted sources, with varying success in separating the target from the background. This study introduces DPM-TSE, a first generative method based on diffusion probabilistic modeling (DPM) for target sound extraction, to achieve both cleaner target renderings as well as improved separability from unwanted sounds. The technique also tackles common background noise issues with DPM by introducing a correction method for noise schedules and sample steps. This approach is evaluated using both objective and subjective quality metrics on the FSD Kaggle 2018 dataset. The results show that DPM-TSE has a significant improvement in perceived quality in terms of target extraction and purity.
</details>
<details>
<summary>摘要</summary>
通用目标声音提取（TSE）方法主要依靠推论方法，以分离目标声音而减少背景干扰，Resultsof varying success in separating the target from the background. This study introduces DPM-TSE, a first generative method based on diffusion probabilistic modeling (DPM) for target sound extraction, to achieve both cleaner target renderings as well as improved separability from unwanted sounds. The technique also tackles common background noise issues with DPM by introducing a correction method for noise schedules and sample steps. This approach is evaluated using both objective and subjective quality metrics on the FSD Kaggle 2018 dataset. The results show that DPM-TSE has a significant improvement in perceived quality in terms of target extraction and purity.Note: Simplified Chinese is also known as Mandarin Chinese, and is the official language of China. It is written using the Simplified Chinese characters, which are used in mainland China and Singapore. Traditional Chinese is also widely used, and is the official language of Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Analysis-on-the-Influence-of-Synchronization-Error-on-Fixed-filter-Active-Noise-Control"><a href="#Analysis-on-the-Influence-of-Synchronization-Error-on-Fixed-filter-Active-Noise-Control" class="headerlink" title="Analysis on the Influence of Synchronization Error on Fixed-filter Active Noise Control"></a>Analysis on the Influence of Synchronization Error on Fixed-filter Active Noise Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04249">http://arxiv.org/abs/2310.04249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guo Yu</li>
<li>for: 这个研究旨在investigating the synchronization error of digital Active Noise Control (ANC) system.</li>
<li>methods: 该研究采用了fixed-filter strategy, which is a viable alternative to traditional adaptive algorithms in addressing the challenges of computing complexity and instability, but with a potential trade-off in terms of noise reduction efficacy.</li>
<li>results: 该研究expects to provide a theoretical investigation into the synchronization error of the digital ANC system.<details>
<summary>Abstract</summary>
The efficacy of active noise control technology in mitigating urban noise, particularly in relation to low-frequency components, has been well-established. In the realm of traditional academic research, adaptive algorithms, such as the filtered reference least mean square method, are extensively employed to achieve real-time noise reduction in many applications. Nevertheless, the utilization of this technology in commercial goods is often hindered by its significant computing complexity and inherent instability. In this particular scenario, the adoption of the fixed-filter strategy emerges as a viable alternative for addressing these challenges, albeit with a potential trade-off in terms of noise reduction efficacy. This work aims to conduct a theoretical investigation into the synchronization error of the digital Active Noise Control (ANC) system. Keywords: Fixed-filter, Active noise control, Multichannel active noise control.
</details>
<details>
<summary>摘要</summary>
“active noise control技术在城市噪声缓解方面的效果已得到了广泛证明。在传统学术研究中，适应算法如 filtered reference least mean square method 广泛应用于实时噪声减少多种应用。然而，商业产品中使用这技术时常受到计算复杂性和内置不稳定性的限制。在这种情况下， fixed-filter 策略 emerges 作为一种可行的替代方案，尽管可能存在噪声减少效果的潜在交换。本工作的目的是 investigate 数字 active noise control（ANC）系统的同步误差。关键字： fixed-filter, active noise control, multichannel active noise control。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="U-Style-Cascading-U-nets-with-Multi-level-Speaker-and-Style-Modeling-for-Zero-Shot-Voice-Cloning"><a href="#U-Style-Cascading-U-nets-with-Multi-level-Speaker-and-Style-Modeling-for-Zero-Shot-Voice-Cloning" class="headerlink" title="U-Style: Cascading U-nets with Multi-level Speaker and Style Modeling for Zero-Shot Voice Cloning"></a>U-Style: Cascading U-nets with Multi-level Speaker and Style Modeling for Zero-Shot Voice Cloning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04004">http://arxiv.org/abs/2310.04004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Li, Zhichao Wang, Xinfa Zhu, Jian Cong, Qiao Tian, Yuping Wang, Lei Xie</li>
<li>for: zero-shot speaker cloning, synthesize speech for any target speaker unseen during TTS system building</li>
<li>methods: employ Grad-TTS as the backbone, cascade speaker- and style-specific encoders between text encoder and diffusion decoder, use signal perturbation to explicitly decompose into speaker- and style-specific modeling parts</li>
<li>results: significantly surpass state-of-the-art methods in unseen speaker cloning regarding naturalness and speaker similarity, achieve flexible combinations of desired speaker timbre and style in zero-shot voice cloning<details>
<summary>Abstract</summary>
Zero-shot speaker cloning aims to synthesize speech for any target speaker unseen during TTS system building, given only a single speech reference of the speaker at hand. Although more practical in real applications, the current zero-shot methods still produce speech with undesirable naturalness and speaker similarity. Moreover, endowing the target speaker with arbitrary speaking styles in the zero-shot setup has not been considered. This is because the unique challenge of zero-shot speaker and style cloning is to learn the disentangled speaker and style representations from only short references representing an arbitrary speaker and an arbitrary style. To address this challenge, we propose U-Style, which employs Grad-TTS as the backbone, particularly cascading a speaker-specific encoder and a style-specific encoder between the text encoder and the diffusion decoder. Thus, leveraging signal perturbation, U-Style is explicitly decomposed into speaker- and style-specific modeling parts, achieving better speaker and style disentanglement. To improve unseen speaker and style modeling ability, these two encoders conduct multi-level speaker and style modeling by skip-connected U-nets, incorporating the representation extraction and information reconstruction process. Besides, to improve the naturalness of synthetic speech, we adopt mean-based instance normalization and style adaptive layer normalization in these encoders to perform representation extraction and condition adaptation, respectively. Experiments show that U-Style significantly surpasses the state-of-the-art methods in unseen speaker cloning regarding naturalness and speaker similarity. Notably, U-Style can transfer the style from an unseen source speaker to another unseen target speaker, achieving flexible combinations of desired speaker timbre and style in zero-shot voice cloning.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNZero-shot speaker cloning aims to synthesize speech for any target speaker unseen during TTS system building, given only a single speech reference of the speaker at hand. Although more practical in real applications, the current zero-shot methods still produce speech with undesirable naturalness and speaker similarity. Moreover, endowing the target speaker with arbitrary speaking styles in the zero-shot setup has not been considered. This is because the unique challenge of zero-shot speaker and style cloning is to learn the disentangled speaker and style representations from only short references representing an arbitrary speaker and an arbitrary style. To address this challenge, we propose U-Style, which employs Grad-TTS as the backbone, particularly cascading a speaker-specific encoder and a style-specific encoder between the text encoder and the diffusion decoder. Thus, leveraging signal perturbation, U-Style is explicitly decomposed into speaker- and style-specific modeling parts, achieving better speaker and style disentanglement. To improve unseen speaker and style modeling ability, these two encoders conduct multi-level speaker and style modeling by skip-connected U-nets, incorporating the representation extraction and information reconstruction process. Besides, to improve the naturalness of synthetic speech, we adopt mean-based instance normalization and style adaptive layer normalization in these encoders to perform representation extraction and condition adaptation, respectively. Experiments show that U-Style significantly surpasses the state-of-the-art methods in unseen speaker cloning regarding naturalness and speaker similarity. Notably, U-Style can transfer the style from an unseen source speaker to another unseen target speaker, achieving flexible combinations of desired speaker timbre and style in zero-shot voice cloning.
</details></li>
</ul>
<hr>
<h2 id="Music-Recommendation-Based-on-Audio-Fingerprint"><a href="#Music-Recommendation-Based-on-Audio-Fingerprint" class="headerlink" title="Music Recommendation Based on Audio Fingerprint"></a>Music Recommendation Based on Audio Fingerprint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17655">http://arxiv.org/abs/2310.17655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Saldaña Ulloa</li>
<li>for: 用于建立一种音乐推荐过程中的更加稳健的指纹。</li>
<li>methods: 将不同的音频特征结合使用，以获得一个高维向量。然后，使用PCA方法选择95%的主成分，以减少值的数量。最后，计算每个指纹与整个数据集的相似性矩阵。</li>
<li>results: 使用这些PCA-指纹，实现了89%的成功推荐率（推荐的歌曲的类别与目标歌曲的类别匹配），基于200首个人音乐库中的歌曲，每首歌曲被标注为相应的歌手们的类别。<details>
<summary>Abstract</summary>
This work combined different audio features to obtain a more robust fingerprint to be used in a music recommendation process. The combination of these methods resulted in a high-dimensional vector. To reduce the number of values, PCA was applied to the set of resulting fingerprints, selecting the number of principal components that corresponded to an explained variance of $95\%$. Finally, with these PCA-fingerprints, the similarity matrix of each fingerprint with the entire data set was calculated. The process was applied to 200 songs from a personal music library; the songs were tagged with the artists' corresponding genres. The recommendations (fingerprints of songs with the closest similarity) were rated successful if the recommended songs' genre matched the target songs' genre. With this procedure, it was possible to obtain an accuracy of $89\%$ (successful recommendations out of total recommendation requests).
</details>
<details>
<summary>摘要</summary>
这个工作将不同的音频特征结合起来，以获得更加鲜明的音乐推荐指标。这些方法的组合导致了一个高维度的向量。为了减少值的数量，对这些指标进行了PCA处理，选择了Explained variance的95%。最后，使用这些PCA指标，计算了每个指标与整个数据集的相似性矩阵。这个过程采用了200首个人音乐库中的歌曲，这些歌曲被标注为艺术家的相应类别。推荐（与整个数据集最相似的歌曲指标）被评估为成功，如果推荐的歌曲的类别与目标歌曲的类别匹配。通过这种方式，可以获得89%的准确率（成功推荐请求数量 / 总推荐请求数量）。
</details></li>
</ul>
<hr>
<h2 id="Layer-Adapted-Implicit-Distribution-Alignment-Networks-for-Cross-Corpus-Speech-Emotion-Recognition"><a href="#Layer-Adapted-Implicit-Distribution-Alignment-Networks-for-Cross-Corpus-Speech-Emotion-Recognition" class="headerlink" title="Layer-Adapted Implicit Distribution Alignment Networks for Cross-Corpus Speech Emotion Recognition"></a>Layer-Adapted Implicit Distribution Alignment Networks for Cross-Corpus Speech Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03992">http://arxiv.org/abs/2310.03992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Zhao, Yuan Zong, Jincen Wang, Hailun Lian, Cheng Lu, Li Zhao, Wenming Zheng<br>for:The paper proposes a new unsupervised domain adaptation method called LIDAN to address the challenge of cross-corpus speech emotion recognition.methods:LIDAN extends the previous ICASSP work, DIDAN, by introducing a novel regularization term called layer-adapted implicit distribution alignment (LIDA) that considers emotion labels at different levels of granularity.results:LIDAN surpasses recent state-of-the-art explicit unsupervised DA methods in tackling cross-corpus SER tasks, as demonstrated by extensive experiments on EmoDB, eNTERFACE, and CASIA corpora.<details>
<summary>Abstract</summary>
In this paper, we propose a new unsupervised domain adaptation (DA) method called layer-adapted implicit distribution alignment networks (LIDAN) to address the challenge of cross-corpus speech emotion recognition (SER). LIDAN extends our previous ICASSP work, deep implicit distribution alignment networks (DIDAN), whose key contribution lies in the introduction of a novel regularization term called implicit distribution alignment (IDA). This term allows DIDAN trained on source (training) speech samples to remain applicable to predicting emotion labels for target (testing) speech samples, regardless of corpus variance in cross-corpus SER. To further enhance this method, we extend IDA to layer-adapted IDA (LIDA), resulting in LIDAN. This layer-adpated extention consists of three modified IDA terms that consider emotion labels at different levels of granularity. These terms are strategically arranged within different fully connected layers in LIDAN, aligning with the increasing emotion-discriminative abilities with respect to the layer depth. This arrangement enables LIDAN to more effectively learn emotion-discriminative and corpus-invariant features for SER across various corpora compared to DIDAN. It is also worthy to mention that unlike most existing methods that rely on estimating statistical moments to describe pre-assumed explicit distributions, both IDA and LIDA take a different approach. They utilize an idea of target sample reconstruction to directly bridge the feature distribution gap without making assumptions about their distribution type. As a result, DIDAN and LIDAN can be viewed as implicit cross-corpus SER methods. To evaluate LIDAN, we conducted extensive cross-corpus SER experiments on EmoDB, eNTERFACE, and CASIA corpora. The experimental results demonstrate that LIDAN surpasses recent state-of-the-art explicit unsupervised DA methods in tackling cross-corpus SER tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的无监督领域适应（DA）方法，即层 adapted implicit distribution alignment networks（LIDAN），以解决跨 Corpora 的语音情感识别（SER）挑战。LIDAN 是我们之前的 ICASSP 工作的扩展，深度隐式分布对接网络（DIDAN），其关键贡献在于引入了一个新的正则化项 called implicit distribution alignment（IDA）。这个项使得 DIDAN 在 source 语音样本训练后可以有效地预测 testing 语音样本的情感标签，不管跨 Corpora 的语音样本变化。为了进一步改进这种方法，我们延伸 IDA 到层 adapted IDA（LIDA），得到 LIDAN。这个层 adapted 扩展包括三个修改后的 IDA 项，这些项在不同的全连接层中适应不同的情感细分水平。这种适应安排使得 LIDAN 可以更好地学习语音样本中的情感特征，并且可以更好地适应不同 Corpora 的语音样本。值得一提的是，不同于大多数现有方法，DIDAN 和 LIDAN 不需要 estimating 统计 moments 来描述预设的显式分布，而是直接使用 target 样本重建的思想，从而bridge 特征分布差距。因此，DIDAN 和 LIDAN 可以视为隐式跨 Corpora SER 方法。为了评估 LIDAN，我们在 EmoDB、eNTERFACE 和 CASIA  corpora 上进行了广泛的 cross-Corpus SER 实验。实验结果表明，LIDAN 超越了最近的显式无监督 DA 方法，在跨 Corpora SER 任务中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Emotion-Transfer-For-Cross-Lingual-Speech-Synthesis"><a href="#Zero-Shot-Emotion-Transfer-For-Cross-Lingual-Speech-Synthesis" class="headerlink" title="Zero-Shot Emotion Transfer For Cross-Lingual Speech Synthesis"></a>Zero-Shot Emotion Transfer For Cross-Lingual Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03963">http://arxiv.org/abs/2310.03963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuke Li, Xinfa Zhu, Yi Lei, Hai Li, Junhui Liu, Danming Xie, Lei Xie</li>
<li>for: 这个研究目的是为了实现零次调变的语言转换 speech synthesis，即将语言转换后的 speech 转换为具有不同语言的表情。</li>
<li>methods: 这个研究使用了 DelightfulTTS  нейрон网络架构，并导入了特定设计的模组来模型不同语言的语言特有的调变特征和语言共享的情感表现。 Specifically, 使用了 non-autoregressive predictive coding (NPC) 模组来学习语言特有的 speech 调变，并从 HuBERT 预训练模型中提取了具有强一致能力的共享情感表现。 此外，还使用了层次情感模型来捕捉不同语言之间的更全面的情感表现。</li>
<li>results: 实验结果显示，提案的框架可以实现零次调变的语言转换 speech synthesis，即将语言转换后的 speech 转换为具有不同语言的表情，而不需要调变训练数据。<details>
<summary>Abstract</summary>
Zero-shot emotion transfer in cross-lingual speech synthesis aims to transfer emotion from an arbitrary speech reference in the source language to the synthetic speech in the target language. Building such a system faces challenges of unnatural foreign accents and difficulty in modeling the shared emotional expressions of different languages. Building on the DelightfulTTS neural architecture, this paper addresses these challenges by introducing specifically-designed modules to model the language-specific prosody features and language-shared emotional expressions separately. Specifically, the language-specific speech prosody is learned by a non-autoregressive predictive coding (NPC) module to improve the naturalness of the synthetic cross-lingual speech. The shared emotional expression between different languages is extracted from a pre-trained self-supervised model HuBERT with strong generalization capabilities. We further use hierarchical emotion modeling to capture more comprehensive emotions across different languages. Experimental results demonstrate the proposed framework's effectiveness in synthesizing bi-lingual emotional speech for the monolingual target speaker without emotional training data.
</details>
<details>
<summary>摘要</summary>
zero-shot 情感传递在跨语言speech sintesis中目标是将来源语言中的任意speech作为参考，将情感传递到目标语言的synthetic speech中。建立这种系统面临着不自然的外语口音和不同语言之间共享的情感表达模型化的挑战。基于DelightfulTTS神经网络架构，本文通过特制的模块来分立语言特有的态度特征和共享的情感表达，以提高跨语言speech的自然性。具体来说，使用非autoregressive predictive coding（NPC）模块来学习语言特有的speech态度，以提高跨语言speech的自然性。同时，使用层次情感模型来捕捉不同语言之间的共享情感。实验结果表明我们提出的框架能够在没有情感培训数据的情况下，为单语言target speakerSynthesize bi-lingual emotional speech。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/cs.SD_2023_10_06/" data-id="closbrous00xw0g88h84ndb0g" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/eess.AS_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T14:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/eess.AS_2023_10_06/">eess.AS - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Optimal-model-based-beamforming-and-independent-steering-for-spherical-loudspeaker-arrays"><a href="#Optimal-model-based-beamforming-and-independent-steering-for-spherical-loudspeaker-arrays" class="headerlink" title="Optimal model-based beamforming and independent steering for spherical loudspeaker arrays"></a>Optimal model-based beamforming and independent steering for spherical loudspeaker arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04202">http://arxiv.org/abs/2310.04202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boaz Rafaely, Dima Khaykin</li>
<li>for: 研究方向性喇声广播的方法，使用圆形喇声器阵列控制声波的三维空间方向性。</li>
<li>methods: 使用物理模型基于的优化框架，在圆形喇声器阵列中实现独立执行。</li>
<li>results: 实验证明了理论框架的可靠性，并且在圆形喇声器阵列中实现了独立执行。<details>
<summary>Abstract</summary>
Spherical loudspeaker arrays have been recently studied for directional sound radiation, where the compact arrangement of the loudspeaker units around a sphere facilitated the control of sound radiation in three-dimensional space. Directivity of sound radiation, or beamforming, was achieved by driving each loudspeaker unit independently, where the design of beamforming weights was typically achieved by numerical optimization with reference to a given desired beam pattern. This is in contrast to the methods already developed for microphone arrays in general and spherical microphone arrays in particular, where beamformer weights are designed to satisfy a wider range of objectives, related to directivity, robustness, and side-lobe level, for example. This paper presents the development of a physical-model-based, optimal beamforming framework for spherical loudspeaker arrays, similar to the framework already developed for spherical microphone arrays, facilitating efficient beamforming in the spherical harmonics domain, with independent steering. In particular, it is shown that from a beamforming perspective, the spherical loudspeaker array is similar to the spherical microphone array with microphones arranged around a rigid sphere. Experimental investigation validates the theoretical framework of beamformer design.
</details>
<details>
<summary>摘要</summary>
圆形 loudspeaker 阵列在近期研究中被用于指向性声波发射，其中圆形 loudspeaker 单元的紧凑排布使得三维空间中声波发射的控制变得更加容易。通过独立驱动每个 loudspeaker 单元，实现了声波发射的指向性，也就是 beamforming。与现有的 Microphone 阵列和圆形 Microphone 阵列的方法不同，这里的 beamforming 权重设计通常通过数字优化来实现，以满足更加宽泛的目标，包括指向性、Robustness 和侧射强度等。本文介绍了一种基于物理模型的、优化 beamforming 框架 для圆形 loudspeaker 阵列，与圆形 Microphone 阵列的框架类似，可以有效地在圆函数频谱中进行 beamforming，并且可以独立控制声波发射的方向。特别是，从 beamforming 的视角来看，圆形 loudspeaker 阵列与圆形 Microphone 阵列的声波发射方式类似。实验室调查 validate 了这种理论框架。
</details></li>
</ul>
<hr>
<h2 id="Zones-of-quiet-in-a-broadband-diffuse-sound-field"><a href="#Zones-of-quiet-in-a-broadband-diffuse-sound-field" class="headerlink" title="Zones of quiet in a broadband diffuse sound field"></a>Zones of quiet in a broadband diffuse sound field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04191">http://arxiv.org/abs/2310.04191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boaz Rafaely</li>
<li>for: 本研究探讨了广频杂音场中的安静区域，并使用了最新的广频杂音场的空间时间相关性研究来开发一个理论框架，以研究控制广频杂音的当地活动声控系统，如活动头rest。</li>
<li>methods: 本研究使用了广频杂音场的空间时间相关性研究，并对各种杂音场进行了分析和计算，以 derivation of the diffuse field zones of quiet in the near-field and the far-field of the secondary source。</li>
<li>results: 研究结果表明，在低通滤波后的杂音场中，安静区域的大小与中心频率相关，并且在一定程度上可以通过对各种杂音场进行分析和计算来预测安静区域的大小。<details>
<summary>Abstract</summary>
The zones of quiet in pure-tone diffuse sound fields have been studied extensively in the past, both theoretically and experimentally, with the well known result of the 10\,dB attenuation extending to about a tenth of a wavelength. Recent results on the spatial-temporal correlation of broadband diffuse sound fields are used in this study to develop a theoretical framework for predicting the extension of the zones of quiet in broadband diffuse sound fields. This can be used to study the acoustic limitations imposed on local active sound control systems such as an active headrest when controlling broadband noise. Spatial-temporal correlation is first revised, after which derivations of the diffuse field zones of quiet in the near-field and the far-field of the secondary source are presented. The theoretical analysis is supported by simulation examples comparing the zones of quiet for diffuse fields excited by tonal and broadband signals. It is shown that as a first approximation the zone of quiet of a low-pass filtered noise is comparable to that of a pure-tone with a frequency equal to the center frequency of the broadband noise bandwidth.
</details>
<details>
<summary>摘要</summary>
在过去，混响频率场中的幽静区域已经得到了广泛的研究，both theoretically和experimentally，以得到知名的10dB抑制范围延伸约为一个波长的一半。在这种研究中，我们使用了最近的广band混响场的空间时间相关性研究，开发了一种用于预测混响场中幽静区域的理论框架。这可以用来研究控制广band噪声的地方活动声控系统，如活动头rest。首先，我们修改了空间时间相关性，然后提出了混响场中幽静区域的近场和远场 derivations。 theoretical分析得到了通过对比幽静区域的混响场 excited by tonal和广band信号的simulation例子。结果显示，作为一个初步的approximation，混响场中幽静区域的zone of quiet与一个中心频率为混响场宽频率范围的低通滤波器噪声的zone of quiet几乎相同。
</details></li>
</ul>
<hr>
<h2 id="Spatial-sampling-and-beamforming-for-spherical-microphone-arrays"><a href="#Spatial-sampling-and-beamforming-for-spherical-microphone-arrays" class="headerlink" title="Spatial sampling and beamforming for spherical microphone arrays"></a>Spatial sampling and beamforming for spherical microphone arrays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04169">http://arxiv.org/abs/2310.04169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boaz Rafaely</li>
<li>for: 这篇论文主要写于圆形麦克风阵的声学记录、语音通信和室内噪声分析等领域。</li>
<li>methods: 论文总结了最近的空间采样方法，包括圆形麦克风阵的各种配置，从单一的固定圆形到自由位置的麦克风。同时还介绍了各种射频方法，包括延迟和总和法和道尔芬-切比雪夫法，以及更高级的优化方法，通常在圆形傅里叶域中进行。</li>
<li>results: 论文回顾了最近的圆形麦克风阵 beamforming 方法的进展，包括延迟和总和法、道尔芬-切比雪夫法以及更高级的优化方法。<details>
<summary>Abstract</summary>
Spherical microphone arrays have been recently studied for spatial sound recording, speech communication, and sound field analysis for room acoustics and noise control. Complementary theoretical studies presented progress in spatial sampling and beamforming methods. This paper reviews recent results in spatial sampling that facilitate a wide range of spherical array configurations, from a single rigid sphere to free positioning of microphones. The paper then presents an overview of beamforming methods recently presented for spherical arrays, from the widely used delay-and-sum and Dolph-Chebyshev, to the more advanced optimal methods, typically performed in the spherical harmonics domain.
</details>
<details>
<summary>摘要</summary>
圆形微型麦克风数组在声学记录、语音通信和室内声学雷达控制中得到了最近的研究。相关理论研究提出了在圆形麦克风数组中的空间抽样和扩散方法的进步。本文将介绍最近在圆形麦克风数组中的空间抽样技术，从单一固定圆形麦克风到自由位置的麦克风。然后将介绍圆形麦克风数组中的扩散方法，从通用的延迟和总和到更高级的优化方法，通常在圆形傅里叶域内进行。
</details></li>
</ul>
<hr>
<h2 id="A-privacy-preserving-method-using-secret-key-for-convolutional-neural-network-based-speech-classification"><a href="#A-privacy-preserving-method-using-secret-key-for-convolutional-neural-network-based-speech-classification" class="headerlink" title="A privacy-preserving method using secret key for convolutional neural network-based speech classification"></a>A privacy-preserving method using secret key for convolutional neural network-based speech classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04035">http://arxiv.org/abs/2310.04035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shoko Niwa, Sayaka Shiota, Hitoshi Kiya</li>
<li>for: 本研究旨在提出一种隐私保护方法，用于 convolutional neural network（CNN）基于语音分类任务中。相比于图像分类领域中的隐私保护研究，语音分类领域尚未得到足够的关注。本研究提供一种基于随机矩阵的加密方法，以保护语音数据的隐私。</li>
<li>methods: 本研究使用了一种基于随机矩阵的加密方法，其中使用了一个可逆的随机矩阵来生成加密后的语音数据。加密后的语音数据可以通过使用一个可逆的随机矩阵来解密，并且可以完全复用原始数据。在实验中，本研究使用了自主学习前端系统，并在语音识别（ASR）和语音认证（ASV）任务中进行了实验。</li>
<li>results: 实验结果表明，使用了本研究提出的加密方法后，语音数据仍然可以完全复用原始数据，并且对于恢复攻击有很好的鲁棒性。此外，本研究还评估了加密后语音数据的难度恢复原始信息。<details>
<summary>Abstract</summary>
In this paper, we propose a privacy-preserving method with a secret key for convolutional neural network (CNN)-based speech classification tasks. Recently, many methods related to privacy preservation have been developed in image classification research fields. In contrast, in speech classification research fields, little research has considered these risks. To promote research on privacy preservation for speech classification, we provide an encryption method with a secret key in CNN-based speech classification systems. The encryption method is based on a random matrix with an invertible inverse. The encrypted speech data with a correct key can be accepted by a model with an encrypted kernel generated using an inverse matrix of a random matrix. Whereas the encrypted speech data is strongly distorted, the classification tasks can be correctly performed when a correct key is provided. Additionally, in this paper, we evaluate the difficulty of reconstructing the original information from the encrypted spectrograms and waveforms. In our experiments, the proposed encryption methods are performed in automatic speech recognition~(ASR) and automatic speaker verification~(ASV) tasks. The results show that the encrypted data can be used completely the same as the original data when a correct secret key is provided in the transformer-based ASR and x-vector-based ASV with self-supervised front-end systems. The robustness of the encrypted data against reconstruction attacks is also illustrated.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种保持隐私的方法，用于在卷积神经网络（CNN）基于的语音分类任务中。在图像分类研究领域中，最近已经有许多隐私保护方法的研究。然而，在语音分类研究领域，很少有研究者考虑到这些风险。为了促进语音分类领域中的隐私保护研究，我们提供了一种使用随机矩阵的加密方法。这种加密方法基于一个可逆的随机矩阵。具有正确密钥的加密语音数据可以通过一个使用逆矩阵生成的加密神经网络进行接受。然而，加密语音数据具有强烈的扭曲，但是在正确密钥提供下，分类任务仍然可以正确完成。此外，在这篇论文中，我们评估了加密后的原始信息重建的困难度。在我们的实验中，我们使用自动语音识别（ASR）和自动说话人验证（ASV）任务中的转换器基于ASR和x-vector基于ASV自适应前端系统进行实现。结果显示，当正确密钥提供时，加密数据可以完全 Replace original data，并且在转换器基于ASR和x-vector基于ASV自适应前端系统中，加密数据的稳定性也得到了证明。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/eess.AS_2023_10_06/" data-id="closbrow9011u0g88g07ze5b1" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/cs.CV_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T13:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/cs.CV_2023_10_06/">cs.CV - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="An-Algorithm-to-Train-Unrestricted-Sequential-Discrete-Morphological-Neural-Networks"><a href="#An-Algorithm-to-Train-Unrestricted-Sequential-Discrete-Morphological-Neural-Networks" class="headerlink" title="An Algorithm to Train Unrestricted Sequential Discrete Morphological Neural Networks"></a>An Algorithm to Train Unrestricted Sequential Discrete Morphological Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04584">http://arxiv.org/abs/2310.04584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Diego Marcondes, Mariana Feldman, Junior Barrera</li>
<li>for: 这个论文是为了描述一种基于深度学习的数学 morphology（MM）操作 insertion into convolutional neural networks（CNN）的方法，以及这种方法的应用在二值图像转换中。</li>
<li>methods: 这种方法使用的是一种新的离散 morphological neural networks（DMNN），它可以表示特定的 W-操作类型，并使用机器学习算法来学习这些操作的参数。</li>
<li>results: 这种方法可以在二值图像转换中提供更高的性能，并且可以在不具备域知识的情况下进行应用。<details>
<summary>Abstract</summary>
With the advent of deep learning, there have been attempts to insert mathematical morphology (MM) operators into convolutional neural networks (CNN), and the most successful endeavor to date has been the morphological neural networks (MNN). Although MNN have performed better than CNN in solving some problems, they inherit their black-box nature. Furthermore, in the case of binary images, they are approximations, which loose the Boolean lattice structure of MM operators and, thus, it is not possible to represent a specific class of W-operators with desired properties. In a recent work, we proposed the Discrete Morphological Neural Networks (DMNN) for binary image transformation to represent specific classes of W-operators and estimate them via machine learning. We also proposed a stochastic lattice gradient descent algorithm (SLGDA) to learn the parameters of Canonical Discrete Morphological Neural Networks (CDMNN), whose architecture is composed only of operators that can be decomposed as the supremum, infimum, and complement of erosions and dilations. In this paper, we propose an algorithm to learn unrestricted sequential DMNN (USDMNN), whose architecture is given by the composition of general W-operators. We consider the representation of a W-operator by its characteristic Boolean function, and then learn it via a SLGDA in the Boolean lattice of functions. Although both the CDMNN and USDMNN have the Boolean lattice structure, USDMNN are not as dependent on prior information about the problem at hand, and may be more suitable in instances in which the practitioner does not have strong domain knowledge. We illustrate the algorithm in a practical example.
</details>
<details>
<summary>摘要</summary>
Deep learning 技术的出现，有人尝试插入数学形态（MM）运算到卷积神经网络（CNN）中，最成功的尝试是形态神经网络（MNN）。 although MNN 在解决一些问题上表现比 CNN 更好，但它们继承了黑盒模式，无法表示特定的 W-运算器。 在二进制图像的情况下，MNN 是一种近似方法，不能保持 Boolean 网格结构，因此无法表示特定的 W-运算器。在我们的最近工作中，我们提出了逻辑分割神经网络（DMNN）来解决这个问题。 DMNN 可以表示特定的 W-运算器，并通过机器学习来参数化。 我们还提出了一种Stochastic Lattice Gradient Descent Algorithm（SLGDA）来学习 Canonical Discrete Morphological Neural Networks（CDMNN）的参数，其架构由 supremum、infimum 和扩散、减小的操作组成。在这篇论文中，我们提出了一种算法来学习不受限制的顺序 DMNN（USDMNN）。 USDMNN 的架构由 general W-运算器组成。 我们认为 W-运算器的特征 Boolean 函数可以表示它，然后通过 SLGDA 在 Boolean 网格中学习它。 虽然 CDMNN 和 USDMNN 都具有 Boolean 网格结构，但 USDMNN 不受具体问题的先验知识的限制，可能更适合在具体问题上使用。 我们在实践中 illustrate 了这种算法。
</details></li>
</ul>
<hr>
<h2 id="Universal-Humanoid-Motion-Representations-for-Physics-Based-Control"><a href="#Universal-Humanoid-Motion-Representations-for-Physics-Based-Control" class="headerlink" title="Universal Humanoid Motion Representations for Physics-Based Control"></a>Universal Humanoid Motion Representations for Physics-Based Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04582">http://arxiv.org/abs/2310.04582</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris Kitani, Weipeng Xu</li>
<li>for: 这种新的运动表示法可以用于physics-based humanoid控制，它可以涵盖各种人工智能控制任务中的各种运动样式。</li>
<li>methods: 这种运动表示法使用了一种含有变量信息瓶颈的encoder-decoder结构，并通过学习自然人类运动数据来塑造运动表示。同时，它还使用了一种优化的采样策略来提高模型表达能力和采样效率。</li>
<li>results: 通过使用这种运动表示法，研究人员可以解决各种生成任务（如攻击和地形越过）和运动跟踪任务使用VR控制器。这种运动表示法可以生成长时间、稳定、多样化的人类运动，并且可以在各种复杂任务中表现出自然和现实的人类行为。<details>
<summary>Abstract</summary>
We present a universal motion representation that encompasses a comprehensive range of motor skills for physics-based humanoid control. Due to the high-dimensionality of humanoid control as well as the inherent difficulties in reinforcement learning, prior methods have focused on learning skill embeddings for a narrow range of movement styles (e.g. locomotion, game characters) from specialized motion datasets. This limited scope hampers its applicability in complex tasks. Our work closes this gap, significantly increasing the coverage of motion representation space. To achieve this, we first learn a motion imitator that can imitate all of human motion from a large, unstructured motion dataset. We then create our motion representation by distilling skills directly from the imitator. This is achieved using an encoder-decoder structure with a variational information bottleneck. Additionally, we jointly learn a prior conditioned on proprioception (humanoid's own pose and velocities) to improve model expressiveness and sampling efficiency for downstream tasks. Sampling from the prior, we can generate long, stable, and diverse human motions. Using this latent space for hierarchical RL, we show that our policies solve tasks using natural and realistic human behavior. We demonstrate the effectiveness of our motion representation by solving generative tasks (e.g. strike, terrain traversal) and motion tracking using VR controllers.
</details>
<details>
<summary>摘要</summary>
我们提出了一种涵盖广泛人形机器人控制的通用运动表示方法。由于人形机器人控制的维度较高以及学习奖励学习的自然难度，先前的方法通常是从专门的运动数据集中学习一些特定的运动风格（如行走、游戏角色）的技能嵌入。这限制了其应用在复杂任务中。我们的工作将这个差距减少，显著扩大运动表示空间的覆盖率。为了实现这一点，我们首先学习了一个可以模仿所有人类运动的运动模仿器。然后，我们通过变量信息瓶颈的encoder-decoder结构来创建我们的运动表示。此外，我们同时学习了一个受过优化的先天条件，以提高模型表达力和下游任务的采样效率。从这个幽Defaults中，我们可以生成长、稳定、多样化的人类运动。使用这个潜在空间进行层次RL，我们展示了我们的策略可以通过自然和现实的人类行为解决任务。我们通过生成任务（如击打、地形穿越）和使用VR控制器进行运动跟踪来证明了我们的运动表示的效果。
</details></li>
</ul>
<hr>
<h2 id="VTON-IT-Virtual-Try-On-using-Image-Translation"><a href="#VTON-IT-Virtual-Try-On-using-Image-Translation" class="headerlink" title="VTON-IT: Virtual Try-On using Image Translation"></a>VTON-IT: Virtual Try-On using Image Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04558">http://arxiv.org/abs/2310.04558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuntos/viton-it">https://github.com/shuntos/viton-it</a></li>
<li>paper_authors: Santosh Adhikari, Bishnu Bhusal, Prashant Ghimire, Anil Shrestha</li>
<li>for: 这项研究旨在提供一种基于生成对抗网络的虚拟试穿服务（Virtual Try-On），以帮助用户在线上快速适应不同的服装。</li>
<li>methods: 该研究使用 semantic segmentation 和基于生成对抗网络的图像翻译网络，以生成高品质的虚拟试穿图像。</li>
<li>results: 研究发现，该方法可以生成高分辨率的自然图像，并保留人体图像中的细节 texture。<details>
<summary>Abstract</summary>
Virtual Try-On (trying clothes virtually) is a promising application of the Generative Adversarial Network (GAN). However, it is an arduous task to transfer the desired clothing item onto the corresponding regions of a human body because of varying body size, pose, and occlusions like hair and overlapped clothes. In this paper, we try to produce photo-realistic translated images through semantic segmentation and a generative adversarial architecture-based image translation network. We present a novel image-based Virtual Try-On application VTON-IT that takes an RGB image, segments desired body part, and overlays target cloth over the segmented body region. Most state-of-the-art GAN-based Virtual Try-On applications produce unaligned pixelated synthesis images on real-life test images. However, our approach generates high-resolution natural images with detailed textures on such variant images.
</details>
<details>
<summary>摘要</summary>
虚拟试穿（虚拟尝试服装）是生成对抗网络（GAN）的应用之一，但是将想要的服装 Item onto 人体部位的任务是困难的，因为人体大小、姿势和遮盾物如头发和覆盖的衣服会导致困难。在这篇论文中，我们尝试通过 semantic segmentation 和基于生成对抗网络的图像翻译网络来生成高品质的译文图像。我们提出了一个名为 VTON-IT 的新型虚拟试穿应用，它从 RGB 图像中提取欲要的体部，并将目标衣服覆盖到提取的体部上。大多数现状的 GAN-based Virtual Try-On 应用程序在实际测试图像上生成不一致的Pixelated Synthesis图像，但我们的方法可以在 variant 图像上生成高分辨率的自然图像，具有细腻的文件。
</details></li>
</ul>
<hr>
<h2 id="MeSa-Masked-Geometric-and-Supervised-Pre-training-for-Monocular-Depth-Estimation"><a href="#MeSa-Masked-Geometric-and-Supervised-Pre-training-for-Monocular-Depth-Estimation" class="headerlink" title="MeSa: Masked, Geometric, and Supervised Pre-training for Monocular Depth Estimation"></a>MeSa: Masked, Geometric, and Supervised Pre-training for Monocular Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04551">http://arxiv.org/abs/2310.04551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Osama Khan, Junbang Liang, Chun-Kai Wang, Shan Yang, Yu Lou<br>for:* The paper aims to improve the performance of monocular depth estimation models by proposing a comprehensive framework called MeSa, which leverages the complementary strengths of masked, geometric, and supervised pre-training.methods:* The paper uses a combination of pre-training techniques, including masked pre-training, geometric pre-training, and supervised pre-training, to improve the representations of the later layers of the model.* The authors use a layer-wise analysis technique called CKA to evaluate the effectiveness of their pre-training strategy.results:* The paper demonstrates performance improvements in both the in-distribution and out-of-distribution settings on the NYUv2 and IBims-1 datasets, compared to the SOTA SSL method.* The authors show that their approach surpasses the masked pre-training SSL method by a substantial margin of 17.1% on the RMSE, and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset.<details>
<summary>Abstract</summary>
Pre-training has been an important ingredient in developing strong monocular depth estimation models in recent years. For instance, self-supervised learning (SSL) is particularly effective by alleviating the need for large datasets with dense ground-truth depth maps. However, despite these improvements, our study reveals that the later layers of the SOTA SSL method are actually suboptimal. By examining the layer-wise representations, we demonstrate significant changes in these later layers during fine-tuning, indicating the ineffectiveness of their pre-trained features for depth estimation. To address these limitations, we propose MeSa, a comprehensive framework that leverages the complementary strengths of masked, geometric, and supervised pre-training. Hence, MeSa benefits from not only general-purpose representations learnt via masked pre training but also specialized depth-specific features acquired via geometric and supervised pre-training. Our CKA layer-wise analysis confirms that our pre-training strategy indeed produces improved representations for the later layers, overcoming the drawbacks of the SOTA SSL method. Furthermore, via experiments on the NYUv2 and IBims-1 datasets, we demonstrate that these enhanced representations translate to performance improvements in both the in-distribution and out-of-distribution settings. We also investigate the influence of the pre-training dataset and demonstrate the efficacy of pre-training on LSUN, which yields significantly better pre-trained representations. Overall, our approach surpasses the masked pre-training SSL method by a substantial margin of 17.1% on the RMSE. Moreover, even without utilizing any recently proposed techniques, MeSa also outperforms the most recent methods and establishes a new state-of-the-art for monocular depth estimation on the challenging NYUv2 dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Iris-Liveness-Detection-Competition-LivDet-Iris-–-The-2023-Edition"><a href="#Iris-Liveness-Detection-Competition-LivDet-Iris-–-The-2023-Edition" class="headerlink" title="Iris Liveness Detection Competition (LivDet-Iris) – The 2023 Edition"></a>Iris Liveness Detection Competition (LivDet-Iris) – The 2023 Edition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04541">http://arxiv.org/abs/2310.04541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Tinsley, Sandip Purnapatra, Mahsa Mitcheff, Aidan Boyd, Colton Crum, Kevin Bowyer, Patrick Flynn, Stephanie Schuckers, Adam Czajka, Meiling Fang, Naser Damer, Xingyu Liu, Caiyong Wang, Xianyun Sun, Zhaohua Chang, Xinyue Li, Guangzhe Zhao, Juan Tapia, Christoph Busch, Carlos Aravena, Daniel Schulz</li>
<li>for: 这个研究报告描述了2023年度的’’LivDet’’系列眼睛展示攻击检测（PAD）竞赛的结果。</li>
<li>methods: 这次竞赛新增了基于生成 adversarial network（GAN）生成的眼睛图像作为攻击工具（PAI）的一类，以及人工智能的检测PAI的参考准则。</li>
<li>results:  Clarkson University和 Notre Dame大学提供了竞赛中使用的图像集，包括7种不同的PAI类型的样本，以及基eline PAD算法。 Fraunhofer IGD、北京市市政工程学院和 Höchschule Darmstadt提交了共8个PAD算法的结果。 根据不同的PAI类型，分析了准确率结果，并与人工准确率进行比较。 总的来说，Fraunhofer IGD的算法（使用注意力基于像素精度网络）获得了最佳权重准确率（37.31%的平均分类错误率），而北京市市政工程学院的算法（使用等权重）获得了平均分类率（22.15%）。这些结果表明，眼睛PAD仍然是一个具有挑战性的问题。<details>
<summary>Abstract</summary>
This paper describes the results of the 2023 edition of the ''LivDet'' series of iris presentation attack detection (PAD) competitions. New elements in this fifth competition include (1) GAN-generated iris images as a category of presentation attack instruments (PAI), and (2) an evaluation of human accuracy at detecting PAI as a reference benchmark. Clarkson University and the University of Notre Dame contributed image datasets for the competition, composed of samples representing seven different PAI categories, as well as baseline PAD algorithms. Fraunhofer IGD, Beijing University of Civil Engineering and Architecture, and Hochschule Darmstadt contributed results for a total of eight PAD algorithms to the competition. Accuracy results are analyzed by different PAI types, and compared to human accuracy. Overall, the Fraunhofer IGD algorithm, using an attention-based pixel-wise binary supervision network, showed the best-weighted accuracy results (average classification error rate of 37.31%), while the Beijing University of Civil Engineering and Architecture's algorithm won when equal weights for each PAI were given (average classification rate of 22.15%). These results suggest that iris PAD is still a challenging problem.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Unreasonable-Effectiveness-of-Linear-Prediction-as-a-Perceptual-Metric"><a href="#The-Unreasonable-Effectiveness-of-Linear-Prediction-as-a-Perceptual-Metric" class="headerlink" title="The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric"></a>The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05986">http://arxiv.org/abs/2310.05986</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsevero/linear-autoregressive-similarity-index">https://github.com/dsevero/linear-autoregressive-similarity-index</a></li>
<li>paper_authors: Daniel Severo, Lucas Theis, Johannes Ballé</li>
<li>for: 这个论文旨在构建一种基于推理时的可视系统嵌入，不需要训练数据或深度神经网络特征。</li>
<li>methods: 该方法使用了一个权重最小二乘（WLS）问题，定义在像素级别，并在推理时解决，以捕捉全像和局部图像特征。</li>
<li>results: 实验表明，使用这种方法可以与基于学习的深度特征方法（如LPIPS和PIM）竞争，而且与手动设计的方法（如MS-SSIM）具有相似的计算成本。<details>
<summary>Abstract</summary>
We show how perceptual embeddings of the visual system can be constructed at inference-time with no training data or deep neural network features. Our perceptual embeddings are solutions to a weighted least squares (WLS) problem, defined at the pixel-level, and solved at inference-time, that can capture global and local image characteristics. The distance in embedding space is used to define a perceptual similarity metric which we call LASI: Linear Autoregressive Similarity Index. Experiments on full-reference image quality assessment datasets show LASI performs competitively with learned deep feature based methods like LPIPS (Zhang et al., 2018) and PIM (Bhardwaj et al., 2020), at a similar computational cost to hand-crafted methods such as MS-SSIM (Wang et al., 2003). We found that increasing the dimensionality of the embedding space consistently reduces the WLS loss while increasing performance on perceptual tasks, at the cost of increasing the computational complexity. LASI is fully differentiable, scales cubically with the number of embedding dimensions, and can be parallelized at the pixel-level. A Maximum Differentiation (MAD) competition (Wang & Simoncelli, 2008) between LASI and LPIPS shows that both methods are capable of finding failure points for the other, suggesting these metrics can be combined.
</details>
<details>
<summary>摘要</summary>
我们展示了如何在推理时构建视觉系统的感知嵌入，无需训练数据或深度神经网络特征。我们的感知嵌入是解定 weights 最小二乘（WLS）问题的解，定义在像素级别，并在推理时解决，可以捕捉全局和本地图像特征。在嵌入空间中的距离被用来定义一个感知相似度指标，我们称之为线性感知相似度指标（LASI）。我们的实验表明，LASI在全参照图像质量评估数据集上与基于深度学习特征的方法如LPIPS（Zhang et al., 2018）和PIM（Bhardwaj et al., 2020）竞争，同时与手工设计的方法如MS-SSIM（Wang et al., 2003）相似的计算成本。我们发现，增加嵌入空间维度可以逐渐降低WLS损失，同时提高感知任务的性能，但是会增加计算复杂度。LASI是完全导数的，可以在像素级别并行化，并且呈 кубические增长与嵌入维度相关。在MAD竞赛（Wang & Simoncelli, 2008）中，LASI和LPIPS之间进行了竞争，表明这两个指标可以相互找到失败点，这些指标可以结合使用。
</details></li>
</ul>
<hr>
<h2 id="URLOST-Unsupervised-Representation-Learning-without-Stationarity-or-Topology"><a href="#URLOST-Unsupervised-Representation-Learning-without-Stationarity-or-Topology" class="headerlink" title="URLOST: Unsupervised Representation Learning without Stationarity or Topology"></a>URLOST: Unsupervised Representation Learning without Stationarity or Topology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04496">http://arxiv.org/abs/2310.04496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Yun, Juexiao Zhang, Bruno Olshausen, Yann LeCun, Yubei Chen</li>
<li>for: 该论文旨在开发一种可以从高维数据中学习有意义表示的无监督学习方法，不受数据模式性和结构的限制。</li>
<li>methods: 该模型结合学习自组织层、密度调整spectral clustering和伪讯采样层。</li>
<li>results: 与现有无监督学习方法相比，该模型在多种数据模式下可以学习有意义表示，并且在许多数据集上表现出色。<details>
<summary>Abstract</summary>
Unsupervised representation learning has seen tremendous progress but is constrained by its reliance on data modality-specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, human vision processes visual signals derived from irregular and non-stationary sampling lattices yet accurately perceives the geometry of the world. We introduce a novel framework that learns from high-dimensional data lacking stationarity and topology. Our model combines a learnable self-organizing layer, density adjusted spectral clustering, and masked autoencoders. We evaluate its effectiveness on simulated biological vision data, neural recordings from the primary visual cortex, and gene expression datasets. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without depending on stationarity or topology. It also outperforms other methods not dependent on these factors, setting a new benchmark in the field. This work represents a step toward unsupervised learning methods that can generalize across diverse high-dimensional data modalities.
</details>
<details>
<summary>摘要</summary>
无监督表征学学习在很大程度上得到了进步，但它受到数据类型特有的静止和结构的限制，这与生物智能系统不同。例如，人类视觉处理视觉信号来自不规则和不静止的抽样网络，然而准确地感知世界的几何结构。我们提出了一种新的框架，可以从缺乏静止和结构的高维数据学习有意义的表示。我们的模型结合可学习的自组织层、适应率调整的спектраль clustering和掩码自适应器。我们对用于生物视觉数据、视觉核心区域神经记录和基因表达数据进行评估，与现状的无监督学习方法SimCLR和MAE相比，我们的模型在不同的数据模式下学习有意义的表示，不依赖于静止和结构。它还超过了其他不依赖于这些因素的方法，创造了新的benchmark在这个领域。这种工作代表了无监督学习方法在多种高维数据模式下的总结。
</details></li>
</ul>
<hr>
<h2 id="Alice-Benchmarks-Connecting-Real-World-Object-Re-Identification-with-the-Synthetic"><a href="#Alice-Benchmarks-Connecting-Real-World-Object-Re-Identification-with-the-Synthetic" class="headerlink" title="Alice Benchmarks: Connecting Real World Object Re-Identification with the Synthetic"></a>Alice Benchmarks: Connecting Real World Object Re-Identification with the Synthetic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04416">http://arxiv.org/abs/2310.04416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoxiao Sun, Yue Yao, Shengjin Wang, Hongdong Li, Liang Zheng</li>
<li>for: 本研究的目的是提供一个大规模的实验数据集和评估协议，以便研究从合成数据学习摄像头识别（re-ID）领域的新方法。</li>
<li>methods: 本研究使用了现有的PersonX和VehicleX作为合成源领域，并收集了两个具有挑战性的实际世界目标数据集：AlicePerson和AliceVehicle。</li>
<li>results: 本研究提供了一个大规模的实验数据集和评估协议，以便研究从合成数据学习摄像头识别领域的新方法。在本研究中，我们还提供了一个线上服务器，让社区可以便捷地评估和比较不同的方法。<details>
<summary>Abstract</summary>
For object re-identification (re-ID), learning from synthetic data has become a promising strategy to cheaply acquire large-scale annotated datasets and effective models, with few privacy concerns. Many interesting research problems arise from this strategy, e.g., how to reduce the domain gap between synthetic source and real-world target. To facilitate developing more new approaches in learning from synthetic data, we introduce the Alice benchmarks, large-scale datasets providing benchmarks as well as evaluation protocols to the research community. Within the Alice benchmarks, two object re-ID tasks are offered: person and vehicle re-ID. We collected and annotated two challenging real-world target datasets: AlicePerson and AliceVehicle, captured under various illuminations, image resolutions, etc. As an important feature of our real target, the clusterability of its training set is not manually guaranteed to make it closer to a real domain adaptation test scenario. Correspondingly, we reuse existing PersonX and VehicleX as synthetic source domains. The primary goal is to train models from synthetic data that can work effectively in the real world. In this paper, we detail the settings of Alice benchmarks, provide an analysis of existing commonly-used domain adaptation methods, and discuss some interesting future directions. An online server will be set up for the community to evaluate methods conveniently and fairly.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本中文转化为简化字符串。<</SYS>>对象重新标识（re-ID）学习从合成数据中得到了一种可靠的扩大大规模标注数据和效果模型，而且具有少量隐私问题。这种策略引发了许多有趣的研究问题，例如如何减少合成源和实际世界目标之间的领域差异。为了推动学习合成数据中的新方法，我们介绍了Alice数据集，这是一个大规模的数据集，同时提供了评估协议和评价标准。在Alice数据集中，我们提供了两个对象重新标识任务：人体和车辆重新标识。我们收集和标注了一些复杂的实际世界目标数据：AlicePerson和AliceVehicle，这些数据包括不同的照明、图像分辨率等。作为我们实际目标的一个重要特点，我们不手动确保了它的训练集的含义，以便更加接近实际领域适应测试enario。因此，我们 reuse existing PersonX和VehicleX作为合成源领域。我们的主要目标是通过合成数据来训练可以在实际世界中工作的模型。在这篇论文中，我们详细介绍了Alice数据集的设置，分析了现有的通用领域适应方法，并讨论了一些有趣的未来方向。我们将设立一个在线服务器，以便社区可以便捷地评估方法并公平地评估。
</details></li>
</ul>
<hr>
<h2 id="CIFAR-10-Warehouse-Broad-and-More-Realistic-Testbeds-in-Model-Generalization-Analysis"><a href="#CIFAR-10-Warehouse-Broad-and-More-Realistic-Testbeds-in-Model-Generalization-Analysis" class="headerlink" title="CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis"></a>CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04414">http://arxiv.org/abs/2310.04414</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sxzrt/CIFAR-10-W">https://github.com/sxzrt/CIFAR-10-W</a></li>
<li>paper_authors: Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi Huang, Liang Zheng</li>
<li>for: 本研究目的是探讨机器学习模型在不同的未知环境中表现的研究问题。</li>
<li>methods: 本文引入了CIFAR-10-Warehouse测试环境，包含180个由搜索图像引擎和扩散模型不同方式生成的数据集。</li>
<li>results: 对CIFAR-10-W进行了广泛的 benchmarking 和比较实验，并显示了这些任务中的新和 interessante 结论。<details>
<summary>Abstract</summary>
Analyzing model performance in various unseen environments is a critical research problem in the machine learning community. To study this problem, it is important to construct a testbed with out-of-distribution test sets that have broad coverage of environmental discrepancies. However, existing testbeds typically either have a small number of domains or are synthesized by image corruptions, hindering algorithm design that demonstrates real-world effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of 180 datasets collected by prompting image search engines and diffusion models in various ways. Generally sized between 300 and 8,000 images, the datasets contain natural images, cartoons, certain colors, or objects that do not naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen the understanding of two generalization tasks: domain generalization and model accuracy prediction in various out-of-distribution environments. We conduct extensive benchmarking and comparison experiments and show that CIFAR-10-W offers new and interesting insights inherent to these tasks. We also discuss other fields that would benefit from CIFAR-10-W.
</details>
<details>
<summary>摘要</summary>
研究机器学习模型在不同环境下的性能分析是机器学习社区中的一个关键问题。为了研究这个问题，构建一个包含多个环境差异的测试环境是非常重要的。然而，现有的测试环境通常只有一小部分的领域，或者通过图像损害Synthesized，这限制了算法设计的实际效果。在这篇论文中，我们介绍了CIFAR-10-Warehouse，包含180个数据集，通过图像搜索引擎和扩散模型的不同方法收集。这些数据集的大小通常在300和8,000个图像之间，包含自然图像、动漫、特定颜色或者不自然出现的对象。通过CIFAR-10-W，我们想要提高评估和深入理解两个总结任务：领域总结和模型在多个不同环境下的准确率预测。我们进行了广泛的比较和实验，并显示了CIFAR-10-W提供了新和有趣的总结预测和模型性能评估的视角。此外，我们还讨论了其他领域可以从CIFAR-10-W中受益。
</details></li>
</ul>
<hr>
<h2 id="FedConv-Enhancing-Convolutional-Neural-Networks-for-Handling-Data-Heterogeneity-in-Federated-Learning"><a href="#FedConv-Enhancing-Convolutional-Neural-Networks-for-Handling-Data-Heterogeneity-in-Federated-Learning" class="headerlink" title="FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning"></a>FedConv: Enhancing Convolutional Neural Networks for Handling Data Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04412">http://arxiv.org/abs/2310.04412</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucsc-vlaa/fedconv">https://github.com/ucsc-vlaa/fedconv</a></li>
<li>paper_authors: Peiran Xu, Zeyu Wang, Jieru Mei, Liangqiong Qu, Alan Yuille, Cihang Xie, Yuyin Zhou</li>
<li>for: 这篇论文旨在探讨 Federated Learning (FL) 中不同设备上的数据不一致性问题，以及如何使用不同的架构元素来改善 FL 的性能。</li>
<li>methods: 该论文采用了一系列的实验研究，以探讨不同架构元素（如激活函数和归一化层）对 FL 性能的影响。</li>
<li>results: 研究发现，通过灵活地修改架构元素，纯 CNN 可以在处理不同数据客户端的 FL 中达到与 ViT 相当或甚至超过其 robustness 水平。此外，该方法可以与现有 FL 技术结合使用，并在多种 FL 标准准样上达到状态 arts 解决方案。<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging paradigm in machine learning, where a shared model is collaboratively learned using data from multiple devices to mitigate the risk of data leakage. While recent studies posit that Vision Transformer (ViT) outperforms Convolutional Neural Networks (CNNs) in addressing data heterogeneity in FL, the specific architectural components that underpin this advantage have yet to be elucidated. In this paper, we systematically investigate the impact of different architectural elements, such as activation functions and normalization layers, on the performance within heterogeneous FL. Through rigorous empirical analyses, we are able to offer the first-of-its-kind general guidance on micro-architecture design principles for heterogeneous FL.   Intriguingly, our findings indicate that with strategic architectural modifications, pure CNNs can achieve a level of robustness that either matches or even exceeds that of ViTs when handling heterogeneous data clients in FL. Additionally, our approach is compatible with existing FL techniques and delivers state-of-the-art solutions across a broad spectrum of FL benchmarks. The code is publicly available at https://github.com/UCSC-VLAA/FedConv
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种emerging paradigm在机器学习中，where a shared model is collaboratively learned using data from multiple devices to mitigate the risk of data leakage. While recent studies posit that Vision Transformer (ViT) outperforms Convolutional Neural Networks (CNNs) in addressing data heterogeneity in FL, the specific architectural components that underpin this advantage have yet to be elucidated. In this paper, we systematically investigate the impact of different architectural elements, such as activation functions and normalization layers, on the performance within heterogeneous FL. Through rigorous empirical analyses, we are able to offer the first-of-its-kind general guidance on micro-architecture design principles for heterogeneous FL.   Intriguingly, our findings indicate that with strategic architectural modifications, pure CNNs can achieve a level of robustness that either matches or even exceeds that of ViTs when handling heterogeneous data clients in FL. Additionally, our approach is compatible with existing FL techniques and delivers state-of-the-art solutions across a broad spectrum of FL benchmarks. The code is publicly available at https://github.com/UCSC-VLAA/FedConv.
</details></li>
</ul>
<hr>
<h2 id="Latent-Consistency-Models-Synthesizing-High-Resolution-Images-with-Few-Step-Inference"><a href="#Latent-Consistency-Models-Synthesizing-High-Resolution-Images-with-Few-Step-Inference" class="headerlink" title="Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference"></a>Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04378">http://arxiv.org/abs/2310.04378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luosiallen/latent-consistency-model">https://github.com/luosiallen/latent-consistency-model</a></li>
<li>paper_authors: Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, Hang Zhao</li>
<li>for: 本研究旨在提高潜在扩散模型（LDM）的渐进生成速度，通过借鉴一致模型（song et al.）提出了潜在一致模型（LCM），可以快速地进行推理，并且可以在任何已经训练过LDM的基础上进行快速的高质量插值。</li>
<li>methods: 本研究使用了一种新的方法——潜在一致模型（LCM），它是通过解决一个扩展的概率流ODE（PF-ODE）来直接预测潜在空间中的解决方案，从而消除了许多迭代过程，使得渐进生成速度加快。</li>
<li>results: 根据LAION-5B-Aesthetics dataset的评估结果，LCMs可以在几步中实现高质量的文本到图像生成，并且与已有的潜在扩散模型（LDM）相比，LCMs可以快速地进行推理，从而提高渐进生成的速度。<details>
<summary>Abstract</summary>
Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling process is computationally intensive and leads to slow generation. Inspired by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion (rombach et al). Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Efficiently distilled from pre-trained classifier-free guided diffusion models, a high-quality 768 x 768 2~4-step LCM takes only 32 A100 GPU hours for training. Furthermore, we introduce Latent Consistency Fine-tuning (LCF), a novel method that is tailored for fine-tuning LCMs on customized image datasets. Evaluation on the LAION-5B-Aesthetics dataset demonstrates that LCMs achieve state-of-the-art text-to-image generation performance with few-step inference. Project Page: https://latent-consistency-models.github.io/
</details>
<details>
<summary>摘要</summary>
Latent Diffusion models (LDMs) 已经取得了高分辨率图像合成的惊人成绩。然而，iterative sampling过程 computationally intensive，导致生成过程慢。 inspirited by Consistency Models (song et al.), we propose Latent Consistency Models (LCMs), 可以快速地进行推理，只需要 minimal steps，在任何预训练LDMs上，包括Stable Diffusion (rombach et al)。通过视为 solves an augmented probability flow ODE (PF-ODE)，LCMs是设计用来直接预测 latent space中的解决方案，从而减少了许多迭代和允许快速、高精度的抽象。从预训练的 classifier-free guided diffusion models中高质量地备取32 A100 GPU hours for training。此外，我们介绍了Latent Consistency Fine-tuning (LCF)，一种适用于精度地 fine-tune LCMs on customized image dataset。对于 LAION-5B-Aesthetics dataset的评估表明，LCMs可以在几步推理中实现state-of-the-art的文本到图像生成性能。项目页面：https://latent-consistency-models.github.io/
</details></li>
</ul>
<hr>
<h2 id="SwimXYZ-A-large-scale-dataset-of-synthetic-swimming-motions-and-videos"><a href="#SwimXYZ-A-large-scale-dataset-of-synthetic-swimming-motions-and-videos" class="headerlink" title="SwimXYZ: A large-scale dataset of synthetic swimming motions and videos"></a>SwimXYZ: A large-scale dataset of synthetic swimming motions and videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04360">http://arxiv.org/abs/2310.04360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fiche Guénolé, Sevestre Vincent, Gonzalez-Barral Camila, Leglaive Simon, Séguier Renaud</li>
<li>for: 本研究旨在提供一个可靠的Synthetic dataset of swimming motions和视频，以便进行人体动作分析和评估。</li>
<li>methods: 研究人员通过使用计算机视觉技术，从多个视频源中提取了3.4万帧的动作数据，并对其进行了二维和三维关节标注。</li>
<li>results: 研究人员通过对SwimXYZ dataset进行分析和应用，实现了人体动作的分类和2D姿态估计。<details>
<summary>Abstract</summary>
Technologies play an increasingly important role in sports and become a real competitive advantage for the athletes who benefit from it. Among them, the use of motion capture is developing in various sports to optimize sporting gestures. Unfortunately, traditional motion capture systems are expensive and constraining. Recently developed computer vision-based approaches also struggle in certain sports, like swimming, due to the aquatic environment. One of the reasons for the gap in performance is the lack of labeled datasets with swimming videos. In an attempt to address this issue, we introduce SwimXYZ, a synthetic dataset of swimming motions and videos. SwimXYZ contains 3.4 million frames annotated with ground truth 2D and 3D joints, as well as 240 sequences of swimming motions in the SMPL parameters format. In addition to making this dataset publicly available, we present use cases for SwimXYZ in swimming stroke clustering and 2D pose estimation.
</details>
<details>
<summary>摘要</summary>
科技在体育中扮演着越来越重要的角色，成为运动员获得优势的重要工具。其中，基于动作捕捉的技术在不同体育运动中增加了优势。然而，传统的动作捕捉系统费用高且限制性强。近些年来，基于计算机视觉的方法也在某些运动中表现不佳，如游泳，因为水下环境会导致计算机视觉的准确率下降。一个导致这种差距的原因是游泳动作的标注数据缺乏。为解决这个问题，我们介绍了SwimXYZ，一个人工生成的游泳动作数据集。SwimXYZ包含340万帧的标注2D和3D关节点，以及240个游泳动作序列在SMPL参数格式下。此外，我们还公布了SwimXYZ数据集，并提供了游泳动作分组和2D姿态估计的应用场景。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Deep-Joint-Source-Channel-Coding-with-Decoder-Only-Side-Information"><a href="#Distributed-Deep-Joint-Source-Channel-Coding-with-Decoder-Only-Side-Information" class="headerlink" title="Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information"></a>Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04311">http://arxiv.org/abs/2310.04311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Selim F. Yilmaz, Ezgi Ozyilkan, Deniz Gunduz, Elza Erkip<br>for: 这个论文主要是关于优化低延迟图像传输过无线频道时，当接收端只有相关的侧信息时（温erner-Ziv场景）。methods: 作者使用了数据驱动的联合源-频道编码（JSCC）方法，该方法在实际 finite blocklength  régime中已经显示出performanse superiority，并且可以提供恰当的衰减。在接收端，作者提出了一种新的神经网络架构，该架构将多个阶段的解码器侧信息integrated into the network.results: 作者的研究结果表明，提案的方法可以有效地integrate侧信息，在所有频道噪听水平下，对多种损失函数进行了改进，特别是在低频率信号噪听水平和小带宽比下表现更好。同时，作者还提供了源代码，以便进一步的研究和复现结果。<details>
<summary>Abstract</summary>
We consider low-latency image transmission over a noisy wireless channel when correlated side information is present only at the receiver side (the Wyner-Ziv scenario). In particular, we are interested in developing practical schemes using a data-driven joint source-channel coding (JSCC) approach, which has been previously shown to outperform conventional separation-based approaches in the practical finite blocklength regimes, and to provide graceful degradation with channel quality. We propose a novel neural network architecture that incorporates the decoder-only side information at multiple stages at the receiver side. Our results demonstrate that the proposed method succeeds in integrating the side information, yielding improved performance at all channel noise levels in terms of the various distortion criteria considered here, especially at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs). We also provide the source code of the proposed method to enable further research and reproducibility of the results.
</details>
<details>
<summary>摘要</summary>
我们考虑在噪声无线通道上传输低延迟图像，其中接收方具有相关的侧信息（悉尼-зи夫场景）。我们的研究关注使用数据驱动的联合源-通道编码（JSCC）方法，这种方法在实际的有限块长度尺度下已经被证明可以超越传统的分离基于方法，并且可以在通道质量下提供温饱的适应。我们提出了一种新的神经网络架构，该架构在接收方多个阶段都包含了解oder侧的副本信息。我们的结果表明，提案的方法可以有效地集成侧信息，在所有通道噪声水平下都提高了性能，特别是在低通道信噪比（SNR）和小带宽比（BR）下表现更佳。我们还提供了提案的源代码，以便进一步的研究和复现结果。
</details></li>
</ul>
<hr>
<h2 id="Convergent-ADMM-Plug-and-Play-PET-Image-Reconstruction"><a href="#Convergent-ADMM-Plug-and-Play-PET-Image-Reconstruction" class="headerlink" title="Convergent ADMM Plug and Play PET Image Reconstruction"></a>Convergent ADMM Plug and Play PET Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04299">http://arxiv.org/abs/2310.04299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Sureau, Mahdi Latreche, Marion Savanier, Claude Comtat</li>
<li>for: 这个论文旨在研究基于模型基本的变量重建法和独立学习深度神经网络算法的混合PET重建方法。</li>
<li>methods: 该方法使用ADMM插件和玩家框架，并在学习过程中添加了一个约束来保证网络参数的稳定性。</li>
<li>results: 实验表明，当不在学习过程中 enforcing该约束时，ADMM算法不会 converges。而在 enforcing该约束时，方法实际上可以达到意义的稳定点。<details>
<summary>Abstract</summary>
In this work, we investigate hybrid PET reconstruction algorithms based on coupling a model-based variational reconstruction and the application of a separately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play framework. Following recent results in optimization, fixed point convergence of the scheme can be achieved by enforcing an additional constraint on network parameters during learning. We propose such an ADMM algorithm and show in a realistic [18F]-FDG synthetic brain exam that the proposed scheme indeed lead experimentally to convergence to a meaningful fixed point. When the proposed constraint is not enforced during learning of the DNN, the proposed ADMM algorithm was observed experimentally not to converge.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们研究了基于模型基于变量重建和独立学习深度神经网络操作符（DNN）的混合PET重建算法，并在ADMM插件和撤退框架中应用。根据最近的优化结果，我们提出了一种强制在学习过程中加入网络参数的约束，以实现Fixed point convergence的方案。我们提出的ADMM算法，并在一个实际的 [18F]-FDG Synthetic brain exam中展示了，该方案实际上可以导致实际上达到一个意义的Fixed point。当不加入学习DNN的约束时，我们发现了ADMM算法在学习过程中不会 converge。
</details></li>
</ul>
<hr>
<h2 id="Graph-learning-in-robotics-a-survey"><a href="#Graph-learning-in-robotics-a-survey" class="headerlink" title="Graph learning in robotics: a survey"></a>Graph learning in robotics: a survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04294">http://arxiv.org/abs/2310.04294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesca Pistilli, Giuseppe Averta</li>
<li>for: 本文旨在探讨深度神经网络在 роботех学中的应用，以便充分发挥其潜力。</li>
<li>methods: 本文总结了图像基于模型的基本知识，包括其建立、训练过程和应用。同时，它还讨论了在实际应用中遇到的最新进展和挑战，例如感知、决策和控制的集成。</li>
<li>results: 本文提供了各种机器人应用，如身体和接触模型、机器人操作、动作识别、舰队动力规划等，以及这些应用中图像学习的可能性和局限性。这篇文章旨在为读者提供图像学习在机器人领域的全面了解，并提出未来研究的可能性。<details>
<summary>Abstract</summary>
Deep neural networks for graphs have emerged as a powerful tool for learning on complex non-euclidean data, which is becoming increasingly common for a variety of different applications. Yet, although their potential has been widely recognised in the machine learning community, graph learning is largely unexplored for downstream tasks such as robotics applications. To fully unlock their potential, hence, we propose a review of graph neural architectures from a robotics perspective. The paper covers the fundamentals of graph-based models, including their architecture, training procedures, and applications. It also discusses recent advancements and challenges that arise in applied settings, related for example to the integration of perception, decision-making, and control. Finally, the paper provides an extensive review of various robotic applications that benefit from learning on graph structures, such as bodies and contacts modelling, robotic manipulation, action recognition, fleet motion planning, and many more. This survey aims to provide readers with a thorough understanding of the capabilities and limitations of graph neural architectures in robotics, and to highlight potential avenues for future research.
</details>
<details>
<summary>摘要</summary>
深度神经网络 для图有效地处理复杂非欧几何数据，这种数据在各种应用中日益普遍。然而，虽然机器学习社区对其潜力广泛认可，但图学习在机器人应用中尚未得到广泛探索。为充分发挥其潜力，我们提出了对图神经建筑从机器人视角进行评审的评论。文章覆盖了图基于模型的基础知识，包括建筑、训练过程和应用。它还讨论了在实践中的进展和挑战，如感知、决策和控制的集成。最后，文章提供了许多机器人应用，例如身体和接触模型、机器人操作、动作识别、队伍运策划等，它们均可以从图学习中受益。这篇评论的目标是为读者提供图神经建筑在机器人领域的能力和局限性的全面了解，并高亮未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Servoing-by-Recombining-Demonstrations"><a href="#Compositional-Servoing-by-Recombining-Demonstrations" class="headerlink" title="Compositional Servoing by Recombining Demonstrations"></a>Compositional Servoing by Recombining Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04271">http://arxiv.org/abs/2310.04271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Argus, Abhijeet Nayak, Martin Büchner, Silvio Galesso, Abhinav Valada, Thomas Brox</li>
<li>for: 本文旨在提高视觉服务器的任务转移能力和多任务能力，并使其更加稳定和高精度。</li>
<li>methods: 本文使用图 traversal 方法来解决视觉服务器任务，并通过分解和重新组合示例来实现多任务能力。</li>
<li>results: 实验和实践结果表明，我们的方法可以提高任务相关的成功率，并且在高精度场景下达到更高的稳定性和效率。<details>
<summary>Abstract</summary>
Learning-based manipulation policies from image inputs often show weak task transfer capabilities. In contrast, visual servoing methods allow efficient task transfer in high-precision scenarios while requiring only a few demonstrations. In this work, we present a framework that formulates the visual servoing task as graph traversal. Our method not only extends the robustness of visual servoing, but also enables multitask capability based on a few task-specific demonstrations. We construct demonstration graphs by splitting existing demonstrations and recombining them. In order to traverse the demonstration graph in the inference case, we utilize a similarity function that helps select the best demonstration for a specific task. This enables us to compute the shortest path through the graph. Ultimately, we show that recombining demonstrations leads to higher task-respective success. We present extensive simulation and real-world experimental results that demonstrate the efficacy of our approach.
</details>
<details>
<summary>摘要</summary>
学习基于图像输入的掌控策略经常表现出任务传递能力不足。相比之下，视觉服务方法可以在高精度场景中实现高效的任务传递，只需要几个示范。在这种工作中，我们提出了一种将视觉服务任务转换为图论探索的框架。我们的方法不仅扩展了视觉服务的稳定性，还允许基于几个任务特定示范的多任务能力。我们将示范图分解成多个示范，并将它们重新组合在一起。为在推理情况下探索示范图，我们利用一个相似性函数来选择最佳示范。这使得我们可以计算最短路径。最终，我们发现将示范重新组合可以获得更高的任务特定成功率。我们在 simulate 和实际实验中展示了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Camouflaged-Object-Detection-A-Large-Scale-Dataset-and-Benchmark"><a href="#Collaborative-Camouflaged-Object-Detection-A-Large-Scale-Dataset-and-Benchmark" class="headerlink" title="Collaborative Camouflaged Object Detection: A Large-Scale Dataset and Benchmark"></a>Collaborative Camouflaged Object Detection: A Large-Scale Dataset and Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04253">http://arxiv.org/abs/2310.04253</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zc199823/bbnet--cocod">https://github.com/zc199823/bbnet--cocod</a></li>
<li>paper_authors: Cong Zhang, Hongbo Bi, Tian-Zhu Xiang, Ranwan Wu, Jinghui Tong, Xiufang Wang</li>
<li>for: 这个论文是为了研究一种新的隐身物体检测任务（CoCOD），该任务的目标是从一组相关图像中同时检测具有同样属性的隐身物体。</li>
<li>methods: 作者提出了一种基eline模型，名为 bilateral-branch network (BBNet)，该模型在单个图像和图像组之间进行协同探索和综合隐身征 clue，以实现准确的隐身物体检测。</li>
<li>results: 作者在提出的 CoCOD8K dataset上进行了广泛的实验，并与 18 种现有模型进行比较。结果表明，提出的方法和模型在 CoCOD 任务中表现出了显著的优异性。<details>
<summary>Abstract</summary>
In this paper, we provide a comprehensive study on a new task called collaborative camouflaged object detection (CoCOD), which aims to simultaneously detect camouflaged objects with the same properties from a group of relevant images. To this end, we meticulously construct the first large-scale dataset, termed CoCOD8K, which consists of 8,528 high-quality and elaborately selected images with object mask annotations, covering 5 superclasses and 70 subclasses. The dataset spans a wide range of natural and artificial camouflage scenes with diverse object appearances and backgrounds, making it a very challenging dataset for CoCOD. Besides, we propose the first baseline model for CoCOD, named bilateral-branch network (BBNet), which explores and aggregates co-camouflaged cues within a single image and between images within a group, respectively, for accurate camouflaged object detection in given images. This is implemented by an inter-image collaborative feature exploration (CFE) module, an intra-image object feature search (OFS) module, and a local-global refinement (LGR) module. We benchmark 18 state-of-the-art models, including 12 COD algorithms and 6 CoSOD algorithms, on the proposed CoCOD8K dataset under 5 widely used evaluation metrics. Extensive experiments demonstrate the effectiveness of the proposed method and the significantly superior performance compared to other competitors. We hope that our proposed dataset and model will boost growth in the COD community. The dataset, model, and results will be available at: https://github.com/zc199823/BBNet--CoCOD.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提供了一项全面的研究，探讨一种新的任务 called 协同掩饰物体检测（CoCOD），该任务的目标是从一组相关图像中同时检测掩饰物体，并且这些物体具有同样的属性。为了实现这一目标，我们在这篇论文中 méticulously 构建了首个大规模数据集，名为 CoCOD8K，该数据集包含 8,528 高质量和精心选择的图像，以及对象 маску 注解，涵盖 5 个超类和 70 个 subclass。该数据集覆盖了自然和人工掩饰场景，图像中的物体外观和背景具有多样性，使得这个数据集对 CoCOD 非常吃力。此外，我们提出了首个 CoCOD 基线模型，名为 bilateral-branch 网络（BBNet），该模型在单个图像和图像集内进行协同掩饰缓解（CFE）、图像内对象搜索（OFS）和本地-全球精度调整（LGR），以确定准确的掩饰物体。我们对 18 个状态机器学习模型进行了 benchmarking，其中包括 12 COD 算法和 6 CoSOD 算法，并在 5 个常用的评价指标下进行了评估。广泛的实验表明我们提出的方法和模型在 CoCOD 中表现出色，与其他竞争对手相比显著性更高。我们希望我们的提出的数据集、模型和结果将能够推动 COD 社区的发展。数据集、模型和结果将在 GitHub 上公开：https://github.com/zc199823/BBNet--CoCOD。
</details></li>
</ul>
<hr>
<h2 id="Semantic-segmentation-of-longitudinal-thermal-images-for-identification-of-hot-and-cool-spots-in-urban-areas"><a href="#Semantic-segmentation-of-longitudinal-thermal-images-for-identification-of-hot-and-cool-spots-in-urban-areas" class="headerlink" title="Semantic segmentation of longitudinal thermal images for identification of hot and cool spots in urban areas"></a>Semantic segmentation of longitudinal thermal images for identification of hot and cool spots in urban areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04247">http://arxiv.org/abs/2310.04247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasantha Ramani, Pandarasamy Arjunan, Kameshwar Poolla, Clayton Miller</li>
<li>For: This paper aims to analyze thermal images collected at the neighborhood scale to identify hot and cool spots in urban areas, with the goal of helping urban planners develop strategies to mitigate the urban heat island (UHI) effect, improve building energy efficiency, and maximize outdoor thermal comfort.* Methods: The authors use state-of-the-art deep learning models to segment various urban features such as buildings, vegetation, sky, and roads from thermal images. They train the models using a subset of the thermal image dataset and compare the performance of different models, including U-Net, DeepLabV3, DeeplabV3+, FPN, and PSPnet.* Results: The authors find that the U-Net segmentation model with a &#96;resnet34’ CNN backbone achieves the highest mIoU score of 0.99 on the test dataset, and that the masks generated using the segmentation models accurately extract the temperature from thermal images and closely match the temperature extracted using ground truth masks. The authors use the masks to identify hot and cool spots in the urban feature at various instances of time.<details>
<summary>Abstract</summary>
This work presents the analysis of semantically segmented, longitudinally, and spatially rich thermal images collected at the neighborhood scale to identify hot and cool spots in urban areas. An infrared observatory was operated over a few months to collect thermal images of different types of buildings on the educational campus of the National University of Singapore. A subset of the thermal image dataset was used to train state-of-the-art deep learning models to segment various urban features such as buildings, vegetation, sky, and roads. It was observed that the U-Net segmentation model with `resnet34' CNN backbone has the highest mIoU score of 0.99 on the test dataset, compared to other models such as DeepLabV3, DeeplabV3+, FPN, and PSPnet. The masks generated using the segmentation models were then used to extract the temperature from thermal images and correct for differences in the emissivity of various urban features. Further, various statistical measure of the temperature extracted using the predicted segmentation masks is shown to closely match the temperature extracted using the ground truth masks. Finally, the masks were used to identify hot and cool spots in the urban feature at various instances of time. This forms one of the very few studies demonstrating the automated analysis of thermal images, which can be of potential use to urban planners for devising mitigation strategies for reducing the urban heat island (UHI) effect, improving building energy efficiency, and maximizing outdoor thermal comfort.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-the-Authenticity-of-Rendered-Portraits-with-Identity-Consistent-Transfer-Learning"><a href="#Enhancing-the-Authenticity-of-Rendered-Portraits-with-Identity-Consistent-Transfer-Learning" class="headerlink" title="Enhancing the Authenticity of Rendered Portraits with Identity-Consistent Transfer Learning"></a>Enhancing the Authenticity of Rendered Portraits with Identity-Consistent Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04194">http://arxiv.org/abs/2310.04194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luyuan Wang, Yiqian Wu, Yongliang Yang, Chen Liu, Xiaogang Jin</li>
<li>for: 该论文旨在提高计算机图形学中的虚拟人像生成质量，并减少’’uncanny valley’’效应。</li>
<li>methods: 该论文使用了传输学习来学习一个映射，从虚拟人像的特征空间传递到真实人像的特征空间。</li>
<li>results: 该论文通过对 DRFHQ 数据集进行精心适应，使用 StyleGAN2 生成器，实现了提高虚拟人像的真实感和减少’’uncanny valley’’效应。<details>
<summary>Abstract</summary>
Despite rapid advances in computer graphics, creating high-quality photo-realistic virtual portraits is prohibitively expensive. Furthermore, the well-know ''uncanny valley'' effect in rendered portraits has a significant impact on the user experience, especially when the depiction closely resembles a human likeness, where any minor artifacts can evoke feelings of eeriness and repulsiveness. In this paper, we present a novel photo-realistic portrait generation framework that can effectively mitigate the ''uncanny valley'' effect and improve the overall authenticity of rendered portraits. Our key idea is to employ transfer learning to learn an identity-consistent mapping from the latent space of rendered portraits to that of real portraits. During the inference stage, the input portrait of an avatar can be directly transferred to a realistic portrait by changing its appearance style while maintaining the facial identity. To this end, we collect a new dataset, Daz-Rendered-Faces-HQ (DRFHQ), that is specifically designed for rendering-style portraits. We leverage this dataset to fine-tune the StyleGAN2 generator, using our carefully crafted framework, which helps to preserve the geometric and color features relevant to facial identity. We evaluate our framework using portraits with diverse gender, age, and race variations. Qualitative and quantitative evaluations and ablation studies show the advantages of our method compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
尽管计算机图形技术得到了快速的进步，但创建高质量的图像化人脸仍然是非常昂贵的。此外，在rendered portrait中的“uncanny valley”效应也对用户体验产生了显著的影响，特别是当描述的人脸非常真实时，任何小误差都可能引起 eeriness和repulsiveness的感受。在这篇论文中，我们提出了一种新的图像化人脸框架，可以有效减少“uncanny valley”效应，提高渲染人脸的 authenticity。我们的关键思想是通过转移学习学习一个人脸的概率空间中的mapping，以便在渲染人脸的过程中保持人脸的facial identity。在推理阶段，输入的人脸可以直接被转换为真实的人脸，只需要改变其外观风格，而不会失去人脸的特征。为了实现这一目标，我们收集了一个新的数据集，DRFHQ（Daz-Rendered-Faces-HQ），这个数据集专门用于渲染风格的人脸。我们利用这个数据集来精心调整StyleGAN2生成器，使其保持人脸的几何和颜色特征。我们对这种方法进行了质量和量化的评估，以及减少方法的ablation study，以证明我们的方法与当前的方法相比有优势。
</details></li>
</ul>
<hr>
<h2 id="Bridging-the-Gap-between-Human-Motion-and-Action-Semantics-via-Kinematic-Phrases"><a href="#Bridging-the-Gap-between-Human-Motion-and-Action-Semantics-via-Kinematic-Phrases" class="headerlink" title="Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases"></a>Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04189">http://arxiv.org/abs/2310.04189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinpeng Liu, Yong-Lu Li, Ailing Zeng, Zizheng Zhou, Yang You, Cewu Lu</li>
<li>for: 这篇论文的目的是建立一个可靠的动作 Semantic 和动作之间的映射关系，但是这是一个复杂的多对多问题。</li>
<li>methods: 我们提出了 Kinematic Phrases (KP) 作为一种 mediator，使得可以统一动作知识库并建立动作理解系统。KP 可以自动将动作转换为文本描述，无需主观偏见，这也附生出了一种新的自动动作生成比赛指标——Kinematic Prompt Generation (KPG)。</li>
<li>results: 在广泛的实验中，我们的方法表现出了超过其他方法的优势。<details>
<summary>Abstract</summary>
The goal of motion understanding is to establish a reliable mapping between motion and action semantics, while it is a challenging many-to-many problem. An abstract action semantic (i.e., walk forwards) could be conveyed by perceptually diverse motions (walk with arms up or swinging), while a motion could carry different semantics w.r.t. its context and intention. This makes an elegant mapping between them difficult. Previous attempts adopted direct-mapping paradigms with limited reliability. Also, current automatic metrics fail to provide reliable assessments of the consistency between motions and action semantics. We identify the source of these problems as the significant gap between the two modalities. To alleviate this gap, we propose Kinematic Phrases (KP) that take the objective kinematic facts of human motion with proper abstraction, interpretability, and generality characteristics. Based on KP as a mediator, we can unify a motion knowledge base and build a motion understanding system. Meanwhile, KP can be automatically converted from motions and to text descriptions with no subjective bias, inspiring Kinematic Prompt Generation (KPG) as a novel automatic motion generation benchmark. In extensive experiments, our approach shows superiority over other methods. Our code and data would be made publicly available at https://foruck.github.io/KP.
</details>
<details>
<summary>摘要</summary>
目的是建立有可靠映射的动作 Semantics 和动作 Meaning 之间的映射，这是一个复杂的多对多问题。抽象的动作 semantics（例如走向前）可以通过多种感知多样的动作（走动手指或抓握）进行表达，而一个动作可以在不同的上下文和意图下具有不同的 semantics。这使得找到一个简洁的映射变得困难。前一些尝试采用了直接映射 paradigms，但其可靠性有限。此外，当前自动度量不能提供有效的动作 Semantics 和动作之间的一致性评估。我们认为这一问题的来源是动作和 Semantics 之间的巨大差距。为了缓解这个差距，我们提出了机械学术短语（KP），它可以对人体动作的 объекively 知识进行抽象、可读性和一般特征。基于 KP 作为中介，我们可以统一动作知识库和建立动作理解系统。此外， KP 可以自动从动作中转换成文本描述，无需主观偏见，这 inspirits 动机Prompt Generation（KPG）作为一种新的自动动作生成标准。在广泛的实验中，我们的方法表现出了其他方法的优越性。我们的代码和数据将在 <https://foruck.github.io/KP> 上公开。
</details></li>
</ul>
<hr>
<h2 id="Whole-Slide-Multiple-Instance-Learning-for-Predicting-Axillary-Lymph-Node-Metastasis"><a href="#Whole-Slide-Multiple-Instance-Learning-for-Predicting-Axillary-Lymph-Node-Metastasis" class="headerlink" title="Whole Slide Multiple Instance Learning for Predicting Axillary Lymph Node Metastasis"></a>Whole Slide Multiple Instance Learning for Predicting Axillary Lymph Node Metastasis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04187">http://arxiv.org/abs/2310.04187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/glejdis/whole-slide-mil-for-predicting-axillary-lymph-node-metastasis">https://github.com/glejdis/whole-slide-mil-for-predicting-axillary-lymph-node-metastasis</a></li>
<li>paper_authors: Glejdis Shkëmbi, Johanna P. Müller, Zhe Li, Katharina Breininger, Peter Schüffler, Bernhard Kainz</li>
<li>for: 本研究旨在开发一种基于深度学习（深度学习）的分类管道，用于从数字核心针刺样本（CNB）图像中提取临床信息，比现有方法减少一步。</li>
<li>methods: 本研究使用了一个公共可用的数据集，包含1058名患者的数据，以评估不同基线状态的深度学习模型在分类肿瘤 метастаisis的状况基于CNB图像。此外，还进行了一项广泛的数据扩充研究。</li>
<li>results: 研究发现，使用不同的数据扩充技术可以提高模型的性能，并且手动肿瘤 segmentation 和注释步骤进行了评估。<details>
<summary>Abstract</summary>
Breast cancer is a major concern for women's health globally, with axillary lymph node (ALN) metastasis identification being critical for prognosis evaluation and treatment guidance. This paper presents a deep learning (DL) classification pipeline for quantifying clinical information from digital core-needle biopsy (CNB) images, with one step less than existing methods. A publicly available dataset of 1058 patients was used to evaluate the performance of different baseline state-of-the-art (SOTA) DL models in classifying ALN metastatic status based on CNB images. An extensive ablation study of various data augmentation techniques was also conducted. Finally, the manual tumor segmentation and annotation step performed by the pathologists was assessed.
</details>
<details>
<summary>摘要</summary>
乳癌是女性健康的主要问题， axillary lymph node（ALN） метастази的识别对诊断评估和治疗指导是关键。这篇论文介绍了一种深度学习（DL）分类管道，用于从数字核心针刺影像中提取临床信息，比现有方法少一步。使用了1058名患者的公共可用数据集来评估不同基eline状态的DL模型在分类ALN肿瘤状态基于CNB影像的性能。此外，还进行了广泛的数据增强技术的抽象研究。最后，评估了病理医生手动肿瘤分割和标注步骤。
</details></li>
</ul>
<hr>
<h2 id="DiffPrompter-Differentiable-Implicit-Visual-Prompts-for-Semantic-Segmentation-in-Adverse-Conditions"><a href="#DiffPrompter-Differentiable-Implicit-Visual-Prompts-for-Semantic-Segmentation-in-Adverse-Conditions" class="headerlink" title="DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions"></a>DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04181">http://arxiv.org/abs/2310.04181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sanket Kalwar, Mihir Ungarala, Shruti Jain, Aaron Monis, Krishna Reddy Konda, Sourav Garg, K Madhava Krishna</li>
<li>for: 这篇论文目的是提高自动驾驶系统在不良天气情况下的Semantic segmentation能力。</li>
<li>methods: 这篇论文提出了一种新的可微分的视觉和latent prompting机制，可以扩展现有的adaptor在基础模型中的学习能力。我们的提案的 $\nabla$HFC图像处理封页在不良天气情况下表现出色，而传统方法往往无法应对。</li>
<li>results: 我们的方法可以将visual prompts和latent prompts联合训练，实现了在Out-of-distribution情况下的表现优化。我们的可微分的视觉提醒可以充分利用平行和串行架构，实现更好地提高object segmentation任务的性能。经过了一系列的实验和评估，我们提供了实践证据支持我们的方法的有效性。<details>
<summary>Abstract</summary>
Semantic segmentation in adverse weather scenarios is a critical task for autonomous driving systems. While foundation models have shown promise, the need for specialized adaptors becomes evident for handling more challenging scenarios. We introduce DiffPrompter, a novel differentiable visual and latent prompting mechanism aimed at expanding the learning capabilities of existing adaptors in foundation models. Our proposed $\nabla$HFC image processing block excels particularly in adverse weather conditions, where conventional methods often fall short. Furthermore, we investigate the advantages of jointly training visual and latent prompts, demonstrating that this combined approach significantly enhances performance in out-of-distribution scenarios. Our differentiable visual prompts leverage parallel and series architectures to generate prompts, effectively improving object segmentation tasks in adverse conditions. Through a comprehensive series of experiments and evaluations, we provide empirical evidence to support the efficacy of our approach. Project page at https://diffprompter.github.io.
</details>
<details>
<summary>摘要</summary>
“严阵天气下的 semantic segmentation 是自动驾驶系统中的一个重要任务。 Foundation model 已经显示了承认的能力，但对于更加具体的enario 需要特殊的 adaptor 来扩展学习能力。我们介绍 DiffPrompter，一种新的可 differentiable 的visual 和 latent 启发机制，用于扩展现有 adaptor 的学习能力。我们的 proposed $\nabla$HFC 图像处理封页在恶劣天气下表现特别出色， conventional 方法通常在这些情况下失败。此外，我们调查了同时训练 visual 和 latent 启发的共同优点，并证明这种结合方法可以在非常规情况下明显提高性能。我们的可 differentiable 的 visual 启发使用并行和串行架构来生成启发，实际地改善了对于恶劣天气的object segmentation任务。通过了一系列的实验和评估，我们提供了实践证据支持我们的方法的有效性。Project page 为 https://diffprompter.github.io。”
</details></li>
</ul>
<hr>
<h2 id="Degradation-Aware-Self-Attention-Based-Transformer-for-Blind-Image-Super-Resolution"><a href="#Degradation-Aware-Self-Attention-Based-Transformer-for-Blind-Image-Super-Resolution" class="headerlink" title="Degradation-Aware Self-Attention Based Transformer for Blind Image Super-Resolution"></a>Degradation-Aware Self-Attention Based Transformer for Blind Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04180">http://arxiv.org/abs/2310.04180</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/i2-multimedia-lab/dsat">https://github.com/i2-multimedia-lab/dsat</a></li>
<li>paper_authors: Qingguo Liu, Pan Gao, Kang Han, Ningzhong Liu, Wei Xiang</li>
<li>for: 提出了一种基于Transformer的盲超分辨率网络模型，以适应各种不确定噪声的环境。</li>
<li>methods: 该模型 integrates CNN和Transformer两种组件，首先使用CNN模ulated by degradation information来EXTRACT LOCAL FEATURES，然后employs degradation-aware Transformer来EXTRACT GLOBAL SEMANTIC FEATURES。</li>
<li>results: 对多个流行的大规模 benchmark dataset进行测试，实现了与现有方法相比的最佳性能，包括Urban100 dataset的PSNR提高0.94 dB和26.62 dB。Source code可以在<a target="_blank" rel="noopener" href="https://github.com/I2-Multimedia-Lab/DSAT/tree/main%E4%B8%AD%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/I2-Multimedia-Lab/DSAT/tree/main中获取。</a><details>
<summary>Abstract</summary>
Compared to CNN-based methods, Transformer-based methods achieve impressive image restoration outcomes due to their abilities to model remote dependencies. However, how to apply Transformer-based methods to the field of blind super-resolution (SR) and further make an SR network adaptive to degradation information is still an open problem. In this paper, we propose a new degradation-aware self-attention-based Transformer model, where we incorporate contrastive learning into the Transformer network for learning the degradation representations of input images with unknown noise. In particular, we integrate both CNN and Transformer components into the SR network, where we first use the CNN modulated by the degradation information to extract local features, and then employ the degradation-aware Transformer to extract global semantic features. We apply our proposed model to several popular large-scale benchmark datasets for testing, and achieve the state-of-the-art performance compared to existing methods. In particular, our method yields a PSNR of 32.43 dB on the Urban100 dataset at $\times$2 scale, 0.94 dB higher than DASR, and 26.62 dB on the Urban100 dataset at $\times$4 scale, 0.26 dB improvement over KDSR, setting a new benchmark in this area. Source code is available at: https://github.com/I2-Multimedia-Lab/DSAT/tree/main.
</details>
<details>
<summary>摘要</summary>
Comparing to CNN-based methods, Transformer-based methods achieve impressive image restoration outcomes due to their ability to model remote dependencies. However, how to apply Transformer-based methods to the field of blind super-resolution (SR) and further make an SR network adaptive to degradation information is still an open problem. In this paper, we propose a new degradation-aware self-attention-based Transformer model, where we incorporate contrastive learning into the Transformer network for learning the degradation representations of input images with unknown noise. In particular, we integrate both CNN and Transformer components into the SR network, where we first use the CNN modulated by the degradation information to extract local features, and then employ the degradation-aware Transformer to extract global semantic features. We apply our proposed model to several popular large-scale benchmark datasets for testing, and achieve the state-of-the-art performance compared to existing methods. In particular, our method yields a PSNR of 32.43 dB on the Urban100 dataset at $\times$2 scale, 0.94 dB higher than DASR, and 26.62 dB on the Urban100 dataset at $\times$4 scale, 0.26 dB improvement over KDSR, setting a new benchmark in this area. 源代码可以在 GitHub 上获取：https://github.com/I2-Multimedia-Lab/DSAT/tree/main.
</details></li>
</ul>
<hr>
<h2 id="Entropic-Score-metric-Decoupling-Topology-and-Size-in-Training-free-NAS"><a href="#Entropic-Score-metric-Decoupling-Topology-and-Size-in-Training-free-NAS" class="headerlink" title="Entropic Score metric: Decoupling Topology and Size in Training-free NAS"></a>Entropic Score metric: Decoupling Topology and Size in Training-free NAS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04179">http://arxiv.org/abs/2310.04179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niccolò Cavagnero, Luca Robbiano, Francesca Pistilli, Barbara Caputo, Giuseppe Averta</li>
<li>for: 这个研究旨在提高适用于边缘应用的高性能卷积神经网络设计，特别是面临资源受限的实际应用情况下。</li>
<li>methods: 本研究提出了一个新的训练自由度量表（Entropic Score），用于估算神经网络的表达能力，以及一种循环搜索算法来独立地搜索神经网络的结构和大小。</li>
<li>results: 本研究获得了在 less than 1 GPU 小时内完全设计高性能的 Hybrid Transformers 模型，并在 ImageNet 类别任务上获得了最高精度和最快速的 NAS 方法。<details>
<summary>Abstract</summary>
Neural Networks design is a complex and often daunting task, particularly for resource-constrained scenarios typical of mobile-sized models. Neural Architecture Search is a promising approach to automate this process, but existing competitive methods require large training time and computational resources to generate accurate models. To overcome these limits, this paper contributes with: i) a novel training-free metric, named Entropic Score, to estimate model expressivity through the aggregated element-wise entropy of its activations; ii) a cyclic search algorithm to separately yet synergistically search model size and topology. Entropic Score shows remarkable ability in searching for the topology of the network, and a proper combination with LogSynflow, to search for model size, yields superior capability to completely design high-performance Hybrid Transformers for edge applications in less than 1 GPU hour, resulting in the fastest and most accurate NAS method for ImageNet classification.
</details>
<details>
<summary>摘要</summary>
neural networks 设计是一个复杂和具有挑战性的任务，特别是在移动设备上进行训练的小型模型 scenario 下。 neuronal architecture search 是一种有前途的方法，可以自动化这个过程，但现有的竞争性方法具有大量训练时间和计算资源，以生成高精度模型。为了突破这些限制，这篇论文做出了以下贡献：1. 一种新的训练时间无关的指标， named Entropic Score，可以通过汇集 activations 的元素级 entropy 来估算模型表达能力。2. 一种循环搜索算法，可以分别 yet synergistically 搜索模型的结构和大小。 Entropic Score 表现出了remarkable 的能力来搜索模型的结构，而与 LogSynflow 的组合可以在 less than 1 GPU 小时内完全设计高性能的 Hybrid Transformers 模型，并在 ImageNet 预测中达到最快和最准确的 NAS 方法。
</details></li>
</ul>
<hr>
<h2 id="Improving-Neural-Radiance-Field-using-Near-Surface-Sampling-with-Point-Cloud-Generation"><a href="#Improving-Neural-Radiance-Field-using-Near-Surface-Sampling-with-Point-Cloud-Generation" class="headerlink" title="Improving Neural Radiance Field using Near-Surface Sampling with Point Cloud Generation"></a>Improving Neural Radiance Field using Near-Surface Sampling with Point Cloud Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04152">http://arxiv.org/abs/2310.04152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hye Bin Yoo, Hyun Min Han, Sung Soo Hwang, Il Yong Chun</li>
<li>for: 提高NeRF的渲染质量和减少训练时间</li>
<li>methods: 采用近表面抽象法，使用训练集中的深度图像来估算3D对象的表面，并且在这个表面附近进行采样。同时，该方法还提出了一种3D点云生成方法和一种简单的修正方法来获取novel view中的深度信息。</li>
<li>results: 实验结果显示，提出的近表面采样NeRF框架可以显著提高NeRF的渲染质量，并且可以减少NeRF模型的训练时间。<details>
<summary>Abstract</summary>
Neural radiance field (NeRF) is an emerging view synthesis method that samples points in a three-dimensional (3D) space and estimates their existence and color probabilities. The disadvantage of NeRF is that it requires a long training time since it samples many 3D points. In addition, if one samples points from occluded regions or in the space where an object is unlikely to exist, the rendering quality of NeRF can be degraded. These issues can be solved by estimating the geometry of 3D scene. This paper proposes a near-surface sampling framework to improve the rendering quality of NeRF. To this end, the proposed method estimates the surface of a 3D object using depth images of the training set and sampling is performed around there only. To obtain depth information on a novel view, the paper proposes a 3D point cloud generation method and a simple refining method for projected depth from a point cloud. Experimental results show that the proposed near-surface sampling NeRF framework can significantly improve the rendering quality, compared to the original NeRF and a state-of-the-art depth-based NeRF method. In addition, one can significantly accelerate the training time of a NeRF model with the proposed near-surface sampling framework.
</details>
<details>
<summary>摘要</summary>
神经辐射场（NeRF）是一种崛起的视图合成方法，它在三维空间中随机 sampling 点并估算它们的存在和颜色概率。NeRF 的缺点是它需要训练时间很长，因为它需要随机 sampling 大量的三维点。此外，如果从遮盖区域或不可能存在的空间中随机 sampling 点，NeRF 的渲染质量将受到降低。这些问题可以通过估算三维场景的geometry来解决。这篇论文提议一种靠近表面 sampling 框架，以改善 NeRF 的渲染质量。为此，提议方法使用训练集的深度图像来估算三维 объек 的表面，然后在那里进行随机 sampling。要在新视图中获取深度信息，论文提议一种三维点云生成方法和一种简单的修正方法。实验结果表明，提议的靠近表面 sampling NeRF 框架可以 significatively 改善 NeRF 的渲染质量，相比于原始 NeRF 和一种状态流行的深度基于 NeRF 方法。此外，可以通过提议的靠近表面 sampling 框架快速加速 NeRF 模型的训练时间。
</details></li>
</ul>
<hr>
<h2 id="TiC-Exploring-Vision-Transformer-in-Convolution"><a href="#TiC-Exploring-Vision-Transformer-in-Convolution" class="headerlink" title="TiC: Exploring Vision Transformer in Convolution"></a>TiC: Exploring Vision Transformer in Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04134">http://arxiv.org/abs/2310.04134</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zs670980918/msa-conv">https://github.com/zs670980918/msa-conv</a></li>
<li>paper_authors: Song Zhang, Qingzhong Wang, Jiang Bian, Haoyi Xiong</li>
<li>for: 提高transformer模型在不同尺度图像处理中的灵活性和计算效率，即使不需要重新训练或resize图像。</li>
<li>methods: 提出了Multi-Head Self-Attention Convolution（MSA-Conv），它将自我注意力 incorporated into generalized convolutions，包括标准、扩展和深度的卷积。</li>
<li>results: 提出了Vision Transformer in Convolution（TiC），并实现了两种可能性提高策略：Multi-Directional Cyclic Shifted Mechanism和Inter-Pooling Mechanism。通过实验证明了TiC的总效果，并通过精准权重分析证明了MSA-Conv和两种可能性提高策略的性能提升。<details>
<summary>Abstract</summary>
While models derived from Vision Transformers (ViTs) have been phonemically surging, pre-trained models cannot seamlessly adapt to arbitrary resolution images without altering the architecture and configuration, such as sampling the positional encoding, limiting their flexibility for various vision tasks. For instance, the Segment Anything Model (SAM) based on ViT-Huge requires all input images to be resized to 1024$\times$1024. To overcome this limitation, we propose the Multi-Head Self-Attention Convolution (MSA-Conv) that incorporates Self-Attention within generalized convolutions, including standard, dilated, and depthwise ones. Enabling transformers to handle images of varying sizes without retraining or rescaling, the use of MSA-Conv further reduces computational costs compared to global attention in ViT, which grows costly as image size increases. Later, we present the Vision Transformer in Convolution (TiC) as a proof of concept for image classification with MSA-Conv, where two capacity enhancing strategies, namely Multi-Directional Cyclic Shifted Mechanism and Inter-Pooling Mechanism, have been proposed, through establishing long-distance connections between tokens and enlarging the effective receptive field. Extensive experiments have been carried out to validate the overall effectiveness of TiC. Additionally, ablation studies confirm the performance improvement made by MSA-Conv and the two capacity enhancing strategies separately. Note that our proposal aims at studying an alternative to the global attention used in ViT, while MSA-Conv meets our goal by making TiC comparable to state-of-the-art on ImageNet-1K. Code will be released at https://github.com/zs670980918/MSA-Conv.
</details>
<details>
<summary>摘要</summary>
“ mentre i modelli derivati dai Vision Transformers (ViTs) hanno avuto un'espansione fenomenale, i modelli pre-tramati non possono adattarsi facilmente alle immagini di risoluzione arbitraria senza modificare l'architettura e la configurazione, come la sampling del coding posizionale, limitando la loro flessibilità per diverse task di visione. Ad esempio, il modello Segment Anything Model (SAM) basato su ViT-Huge richiede che tutte le immagini di input siano resezzate a 1024x1024. Per superare questa limitazione, propongo la Multi-Head Self-Attention Convolution (MSA-Conv) che incorpora l'Auto-Attention all'interno delle convolutioni generalizzate, compresse standard, diluate e depthwise. In questo modo, i transformers possono gestire immagini di dimensioni diverse senza dover rinunciare o ridimensionare, riducendo i costi computazionali rispetto all'attenzione globale in ViT, che cresce costoso con l'aumentare delle dimensioni dell'immagine. Successivamente, presento il Vision Transformer in Convolution (TiC) come una prova di concetto per la classificazione di immagini con MSA-Conv, dove due strategie di miglioria della capacità, ossia la Multi-Directional Cyclic Shifted Mechanism e l'Inter-Pooling Mechanism, sono state proposte, attraverso la creazione di connessioni a distanza lunga tra i token e l'aumento del campo rettangolare efficace. Sono state eseguite estese esperienze per validare l'efficacia generale di TiC. Inoltre, gli studi di ablazione hanno confermato l'improvemento delle prestazioni ottenuto da MSA-Conv e dalle due strategie di miglioria della capacità separate. Nota che la nostra proposta si rivolge allo studio di un'alternativa all'attenzione globale utilizzata in ViT, mentre MSA-Conv incontra il nostro obiettivo facendo di TiC comparabile ai migliori risultati su ImageNet-1K. Il codice verrà rilasciato sul sito GitHub https://github.com/zs670980918/MSA-Conv.”
</details></li>
</ul>
<hr>
<h2 id="VI-Diff-Unpaired-Visible-Infrared-Translation-Diffusion-Model-for-Single-Modality-Labeled-Visible-Infrared-Person-Re-identification"><a href="#VI-Diff-Unpaired-Visible-Infrared-Translation-Diffusion-Model-for-Single-Modality-Labeled-Visible-Infrared-Person-Re-identification" class="headerlink" title="VI-Diff: Unpaired Visible-Infrared Translation Diffusion Model for Single Modality Labeled Visible-Infrared Person Re-identification"></a>VI-Diff: Unpaired Visible-Infrared Translation Diffusion Model for Single Modality Labeled Visible-Infrared Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04122">http://arxiv.org/abs/2310.04122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Han Huang, Yan Huang, Liang Wang<br>for:VI-ReID task with single-modality labeled datamethods:unpaired image-to-image translation techniques, diffusion model (VI-Diff)results:outperforms existing diffusion and GAN models, promising solution for VI-ReID task with single-modality labeled data.<details>
<summary>Abstract</summary>
Visible-Infrared person re-identification (VI-ReID) in real-world scenarios poses a significant challenge due to the high cost of cross-modality data annotation. Different sensing cameras, such as RGB/IR cameras for good/poor lighting conditions, make it costly and error-prone to identify the same person across modalities. To overcome this, we explore the use of single-modality labeled data for the VI-ReID task, which is more cost-effective and practical. By labeling pedestrians in only one modality (e.g., visible images) and retrieving in another modality (e.g., infrared images), we aim to create a training set containing both originally labeled and modality-translated data using unpaired image-to-image translation techniques. In this paper, we propose VI-Diff, a diffusion model that effectively addresses the task of Visible-Infrared person image translation. Through comprehensive experiments, we demonstrate that VI-Diff outperforms existing diffusion and GAN models, making it a promising solution for VI-ReID with single-modality labeled data. Our approach can be a promising solution to the VI-ReID task with single-modality labeled data and serves as a good starting point for future study. Code will be available.
</details>
<details>
<summary>摘要</summary>
visible-infrared人识别（VI-ReID）在实际场景中存在 significanthigh cost of cross-modality数据标注问题。不同的感知镜头，如RGB/IR镜头 для不同的照明条件，使得在不同感知模式之间进行人识别变得昂贵和容易出错。为了解决这个问题，我们研究了使用单模态标注数据来进行VI-ReID任务，这更加经济实用。我们将人员标注在一个模式（例如可见图像）中，然后在另一个模式（例如红外图像）中进行检索。我们希望通过不同的图像对应关系技术来创建一个包含原始标注和模式翻译数据的训练集。在这篇论文中，我们提出了VI-Diff，一种难涨模型，可以有效地解决可见红外人像翻译任务。通过广泛的实验，我们证明了VI-Diff在 diffusion和GAN模型之上表现出色，使其成为VI-ReID任务中单模态标注数据的可靠解决方案。我们的方法可以在VI-ReID任务中提供可靠的解决方案，并作为未来研究的开端。代码将可以提供。
</details></li>
</ul>
<hr>
<h2 id="Aorta-Segmentation-from-3D-CT-in-MICCAI-SEG-A-2023-Challenge"><a href="#Aorta-Segmentation-from-3D-CT-in-MICCAI-SEG-A-2023-Challenge" class="headerlink" title="Aorta Segmentation from 3D CT in MICCAI SEG.A. 2023 Challenge"></a>Aorta Segmentation from 3D CT in MICCAI SEG.A. 2023 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04114">http://arxiv.org/abs/2310.04114</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Project-MONAI/MONAI">https://github.com/Project-MONAI/MONAI</a></li>
<li>paper_authors: Andriy Myronenko, Dong Yang, Yufan He, Daguang Xu</li>
<li>for: 这个研究是为了提出一种自动化的血管分割方法，以帮助早期发现和监测血管疾病。</li>
<li>methods: 这个研究使用了一种名为Auto3DSeg的自动化分割方法，可以在MONAI中使用。</li>
<li>results: 这个方法在3D CT图像中对血管进行分割，得到了平均的 dice分数0.920和95%的 Hausdorff 距离6.013，这与其他参赛者相比，得到了第一名和赢得了SEG.A. 2023挑战。<details>
<summary>Abstract</summary>
Aorta provides the main blood supply of the body. Screening of aorta with imaging helps for early aortic disease detection and monitoring. In this work, we describe our solution to the Segmentation of the Aorta (SEG.A.231) from 3D CT challenge. We use automated segmentation method Auto3DSeg available in MONAI. Our solution achieves an average Dice score of 0.920 and 95th percentile of the Hausdorff Distance (HD95) of 6.013, which ranks first and wins the SEG.A. 2023 challenge.
</details>
<details>
<summary>摘要</summary>
“冠状动脉提供身体主要血液供应。实时侦测冠状动脉可以早期检测和监控冠状疾病。这里我们介绍我们对三维CT图像中的冠状动脉分类挑战的解决方案。我们使用自动分类方法Auto3DSeg，该方法在MONAI中可用。我们的解决方案得到了0.920的 dice分数和6.013的 Hausdorff距离（HD95）的95%分布，它在SEG.A. 2023挑战中排名第一，获得了首奖。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Dense-Random-Texture-Detection-using-Beta-Distribution-Statistics"><a href="#Dense-Random-Texture-Detection-using-Beta-Distribution-Statistics" class="headerlink" title="Dense Random Texture Detection using Beta Distribution Statistics"></a>Dense Random Texture Detection using Beta Distribution Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04111">http://arxiv.org/abs/2310.04111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soeren Molander</li>
<li>for: 检测粗糙随机文本使用完全连接点 sampling on image edges</li>
<li>methods: 使用完全连接点 sampling on image edges，计算点对点的L2距离，并对每个点进行边检查，如果 intersects with image edge，则添加 unity value，否则添加 zero。从而计算出完全连接边图的edge excess index，该指标在[1.0..2.0]范围内，表示不存在边的情况。</li>
<li>results: 该方法应用于实时SLAM-based moving object detection中，点受限于跟踪框（ROIs）。<details>
<summary>Abstract</summary>
This note describes a method for detecting dense random texture using fully connected points sampled on image edges. An edge image is randomly sampled with points, the standard L2 distance is calculated between all connected points in a neighbourhood. For each point, a check is made if the point intersects with an image edge. If this is the case, a unity value is added to the distance, otherwise zero. From this an edge excess index is calculated for the fully connected edge graph in the range [1.0..2.0], where 1.0 indicate no edges. The ratio can be interpreted as a sampled Bernoulli process with unknown probability. The Bayesian posterior estimate of the probability can be associated with its conjugate prior which is a Beta($\alpha$, $\beta$) distribution, with hyper parameters $\alpha$ and $\beta$ related to the number of edge crossings. Low values of $\beta$ indicate a texture rich area, higher values less rich. The method has been applied to real-time SLAM-based moving object detection, where points are confined to tracked boxes (rois).
</details>
<details>
<summary>摘要</summary>
这份备忘录详细介绍了一种用于检测紧密随机文本的方法，该方法基于完全连接点在图像边缘上进行采样。首先，一张边像被随机采样点，然后计算所有连接点的标准L2距离。对于每个点，检查该点是否与图像边缘交叠。如果交叠，则将unity值添加到距离中，否则为零。根据这些距离，计算一个完全连接边图的Edge Excess Index，该指标在[1.0..2.0]范围内，其中1.0表示没有边。这个指标可以被解释为一个随机 Bernoulli 过程的采样，其中unknown probability。使用 conjugate prior 的 Bayesian posterior estimator，其中 conjugate prior 是一个 Beta（α，β）分布，其中 α 和 β 参数与边 crossing 相关。低值 beta 指标表示繁殖的文本区域，高值则表示缺乏文本。该方法已经应用于基于 SLAM 实时移动 объек特点检测，其中点被限制在跟踪的盒子（ROI）中。
</details></li>
</ul>
<hr>
<h2 id="Automated-3D-Segmentation-of-Kidneys-and-Tumors-in-MICCAI-KiTS-2023-Challenge"><a href="#Automated-3D-Segmentation-of-Kidneys-and-Tumors-in-MICCAI-KiTS-2023-Challenge" class="headerlink" title="Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023 Challenge"></a>Automated 3D Segmentation of Kidneys and Tumors in MICCAI KiTS 2023 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04110">http://arxiv.org/abs/2310.04110</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Project-MONAI/MONAI">https://github.com/Project-MONAI/MONAI</a></li>
<li>paper_authors: Andriy Myronenko, Dong Yang, Yufan He, Daguang Xu</li>
<li>for: 本文参加2023年度的肾茵减减挑战（KiTS），用于比较多种解决方案的肾茵分割问题。</li>
<li>methods: 本文使用MONAI中的Auto3DSeg自动分割工具进行肾茵分割。</li>
<li>results: 本文的解决方案在KiTS 2023挑战中得到了平均 dice 值为0.835和表面 dice 值为0.723，并获得了肾茵减减挑战的冠军。<details>
<summary>Abstract</summary>
Kidney and Kidney Tumor Segmentation Challenge (KiTS) 2023 offers a platform for researchers to compare their solutions to segmentation from 3D CT. In this work, we describe our submission to the challenge using automated segmentation of Auto3DSeg available in MONAI. Our solution achieves the average dice of 0.835 and surface dice of 0.723, which ranks first and wins the KiTS 2023 challenge.
</details>
<details>
<summary>摘要</summary>
“干织肿瘤分类挑战（KiTS）2023 提供了一个研究者可以比较他们的解决方案的平台。在这个工作中，我们描述了我们对于自动分类的 Auto3DSeg 可用于 MONAI 的解决方案。我们的解决方案实现了平均 dice 0.835 和表面 dice 0.723，排名第一，获得 KiTS 2023 挑战的冠军。”Note that "KiTS" is short for "Kidney and Kidney Tumor Segmentation Challenge", and "MONAI" is a medical imaging analysis platform.
</details></li>
</ul>
<hr>
<h2 id="ClusVPR-Efficient-Visual-Place-Recognition-with-Clustering-based-Weighted-Transformer"><a href="#ClusVPR-Efficient-Visual-Place-Recognition-with-Clustering-based-Weighted-Transformer" class="headerlink" title="ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer"></a>ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04099">http://arxiv.org/abs/2310.04099</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Xu, Pourya Shamsolmoali, Jie Yang</li>
<li>for: 这篇论文的目的是提出一个新的方法来解决视觉地点识别（VPR）中的缺失和重复信息问题。</li>
<li>methods: 这篇论文使用了一个新的架构，即汇集基于权重的transformer网络（CWTNet），并引入了一个新的优化后VLAD层（OptLAD）以降低模型的维度和提高效率。</li>
<li>results: 实验结果显示，这篇论文的模型在四个VPR数据集上表现较好，并且比较简单。<details>
<summary>Abstract</summary>
Visual place recognition (VPR) is a highly challenging task that has a wide range of applications, including robot navigation and self-driving vehicles. VPR is particularly difficult due to the presence of duplicate regions and the lack of attention to small objects in complex scenes, resulting in recognition deviations. In this paper, we present ClusVPR, a novel approach that tackles the specific issues of redundant information in duplicate regions and representations of small objects. Different from existing methods that rely on Convolutional Neural Networks (CNNs) for feature map generation, ClusVPR introduces a unique paradigm called Clustering-based Weighted Transformer Network (CWTNet). CWTNet leverages the power of clustering-based weighted feature maps and integrates global dependencies to effectively address visual deviations encountered in large-scale VPR problems. We also introduce the optimized-VLAD (OptLAD) layer that significantly reduces the number of parameters and enhances model efficiency. This layer is specifically designed to aggregate the information obtained from scale-wise image patches. Additionally, our pyramid self-supervised strategy focuses on extracting representative and diverse information from scale-wise image patches instead of entire images, which is crucial for capturing representative and diverse information in VPR. Extensive experiments on four VPR datasets show our model's superior performance compared to existing models while being less complex.
</details>
<details>
<summary>摘要</summary>
“视觉地点识别（VPR）是一项非常具有挑战性的任务，它在 робо特 naviation 和自动驾驶汽车等领域有广泛的应用。VPR 尤其是由于区域重复和小 объек 的忽略，导致识别偏差。在这篇论文中，我们提出了 ClusVPR，一种新的方法，用于解决 VPR 中的特定问题。与现有方法不同，ClusVPR 不仅仅采用 Convolutional Neural Networks（CNNs）来生成特征地图，而是引入了归一化-based Weighted Transformer Network（CWTNet）。CWTNet 利用了归一化-based 权重特征地图，并考虑到全局依赖关系，以有效地解决 VPR 中的视觉偏差。我们还提出了优化后的 VLAD（OptLAD）层，它可以减少参数的数量，提高模型的效率。这层特地设计用于聚合 scale-wise 图像块中的信息。此外，我们还提出了一种适应性自我超vised 策略，它会从 scale-wise 图像块中提取代表性和多样化的信息，而不是整个图像，这是关键的 для捕捉 VPR 中的代表性和多样化信息。我们在四个 VPR 数据集上进行了广泛的实验，结果显示我们的模型在与现有模型的比较中表现出优于性，同时具有较低的复杂度。”
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Chess-Recognition"><a href="#End-to-End-Chess-Recognition" class="headerlink" title="End-to-End Chess Recognition"></a>End-to-End Chess Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04086">http://arxiv.org/abs/2310.04086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Athanasios Masouris, Jan van Gemert</li>
<li>for: 识别棋盘配置，即从棋盘图像中识别棋子的配置。</li>
<li>methods: 我们采用深度学习模型，并提出了两种新的方法来直接从整个图像中预测棋盘配置，从而避免了传统的顺序处理方法中的错误积累和中间注解的需求。</li>
<li>results: 我们使用新建的 Chess Recognition Dataset (ChessReD) 进行训练和测试，并证明了我们的方法在这个新的标准数据集上的表现，其中 board recognition accuracy 为 15.26%（相比现有的状态的艺术而言，这是大约7倍的提升）。<details>
<summary>Abstract</summary>
Chess recognition refers to the task of identifying the chess pieces configuration from a chessboard image. Contrary to the predominant approach that aims to solve this task through the pipeline of chessboard detection, square localization, and piece classification, we rely on the power of deep learning models and introduce two novel methodologies to circumvent this pipeline and directly predict the chessboard configuration from the entire image. In doing so, we avoid the inherent error accumulation of the sequential approaches and the need for intermediate annotations. Furthermore, we introduce a new dataset, Chess Recognition Dataset (ChessReD), specifically designed for chess recognition that consists of 10,800 images and their corresponding annotations. In contrast to existing synthetic datasets with limited angles, this dataset comprises a diverse collection of real images of chess formations captured from various angles using smartphone cameras; a sensor choice made to ensure real-world applicability. We use this dataset to both train our model and evaluate and compare its performance to that of the current state-of-the-art. Our approach in chess recognition on this new benchmark dataset outperforms related approaches, achieving a board recognition accuracy of 15.26% ($\approx$7x better than the current state-of-the-art).
</details>
<details>
<summary>摘要</summary>
<<SYS>>cheshire recognition指的是从棋盘图像中识别棋子的配置。与传统方法不同，我们不是通过棋盘检测、方块定位和棋子类别化的管道来解决这个任务，而是直接将棋盘配置从整个图像中预测。这样可以避免累积错误的问题，并不需要中间注解。此外，我们还提出了两种新的方法ологи，以避免管道式approach的缺点。为了训练和评估我们的模型，我们创建了一个新的数据集，棋盘识别数据集（ChessReD）。这个数据集包含10800个图像和其相应的注解，与现有的 sintetic数据集不同，这个数据集包含了多种角度的真实棋盘形态，通过智能手机摄像头拍摄。我们使用这个数据集来训练和评估我们的模型，并与当前状态的最佳方法进行比较。我们的approach在这个新的benchmark数据集上表现出色，实现了棋盘识别精度15.26%（大约7倍于当前状态的最佳方法）。>>>
</details></li>
</ul>
<hr>
<h2 id="In-the-Blink-of-an-Eye-Event-based-Emotion-Recognition"><a href="#In-the-Blink-of-an-Eye-Event-based-Emotion-Recognition" class="headerlink" title="In the Blink of an Eye: Event-based Emotion Recognition"></a>In the Blink of an Eye: Event-based Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04043">http://arxiv.org/abs/2310.04043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanghaiwei1234/single-eye-emotion-recognition">https://github.com/zhanghaiwei1234/single-eye-emotion-recognition</a></li>
<li>paper_authors: Haiwei Zhang, Jiqing Zhang, Bo Dong, Pieter Peers, Wenwei Wu, Xiaopeng Wei, Felix Heide, Xin Yang</li>
<li>for: 这种眼镜可以识别人们的情绪，特别是在照明条件变化时。</li>
<li>methods: 这种方法使用了生物体现的事件驱动摄像机和一种新型的轻量级神经网络SEEN。</li>
<li>results: 对于单眼事件驱动摄像机和神经网络SEEN，我们在压缩数据集上进行了广泛验证和证明，并证明了该方法的有效性。<details>
<summary>Abstract</summary>
We introduce a wearable single-eye emotion recognition device and a real-time approach to recognizing emotions from partial observations of an emotion that is robust to changes in lighting conditions. At the heart of our method is a bio-inspired event-based camera setup and a newly designed lightweight Spiking Eye Emotion Network (SEEN). Compared to conventional cameras, event-based cameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher temporal resolution. Thus, the captured events can encode rich temporal cues under challenging lighting conditions. However, these events lack texture information, posing problems in decoding temporal information effectively. SEEN tackles this issue from two different perspectives. First, we adopt convolutional spiking layers to take advantage of the spiking neural network's ability to decode pertinent temporal information. Second, SEEN learns to extract essential spatial cues from corresponding intensity frames and leverages a novel weight-copy scheme to convey spatial attention to the convolutional spiking layers during training and inference. We extensively validate and demonstrate the effectiveness of our approach on a specially collected Single-eye Event-based Emotion (SEE) dataset. To the best of our knowledge, our method is the first eye-based emotion recognition method that leverages event-based cameras and spiking neural network.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种穿戴式单眼情感识别设备和一种实时方法来从部分情感识别中robust避免变化的照明条件。我们的方法的核心是基于生物体的bio-inspired事件驱动摄像头设置和一种新设计的轻量级Spiking Eye Emotion Network (SEEN)。相比传统摄像头，事件驱动摄像头可以提供更高的动态范围（达到140 dBvs. 80 dB）和更高的时间分辨率。因此，捕获的事件可以嵌入丰富的时间信息。然而，这些事件缺乏文本信息，从而导致解码时间信息的问题。SEEN解决了这个问题从两个不同的角度。首先，我们采用了 convolutional spiking层来利用快速神经网络的能力来解码相关的时间信息。其次，SEEN学习了提取相应的空间信息，并使用一种新的重复计数套件来传递空间注意力到convolutional spiking层 durante training和inference。我们对特制的Single-eye Event-based Emotion (SEE)数据集进行了广泛验证和证明了我们的方法的有效性。据我们所知，我们的方法是首个通过事件驱动摄像头和快速神经网络进行眼球情感识别的方法。
</details></li>
</ul>
<hr>
<h2 id="Robust-Multimodal-Learning-with-Missing-Modalities-via-Parameter-Efficient-Adaptation"><a href="#Robust-Multimodal-Learning-with-Missing-Modalities-via-Parameter-Efficient-Adaptation" class="headerlink" title="Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation"></a>Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03986">http://arxiv.org/abs/2310.03986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif</li>
<li>for: 提高多modalitate下批处程序的总性表现</li>
<li>methods: 利用低级别适应和调制中间特征来补偿缺失modalities</li>
<li>results: 提高多modalitate下的鲁棒性，在一些情况下超越独立的、专门为可用modalitate组合培育的网络In English, this means:</li>
<li>for: To improve the overall performance of downstream tasks in multimodal learning</li>
<li>methods: Using low-rank adaptation and modulation of intermediate features to compensate for missing modalities</li>
<li>results: Improving robustness in multimodal learning, outperforming independent networks trained for available modality combinations in some cases.<details>
<summary>Abstract</summary>
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 0.7% of the total parameters in most experiments). We conduct a series of experiments to highlight the robustness of our proposed method using diverse datasets for RGB-thermal and RGB-Depth semantic segmentation, multimodal material segmentation, and multimodal sentiment analysis tasks. Our proposed method demonstrates versatility across various tasks and datasets, and outperforms existing methods for robust multimodal learning with missing modalities.
</details>
<details>
<summary>摘要</summary>
多模态学习旨在利用多种数据来提高下游任务的总性能。可以利用多模态数据的重复性来使多模态系统具有缺失或损坏观测的某些相关模态时的Robustness。然而，我们发现许多现有的多模态网络在测试时缺失一或多个模态时表现出现较差的性能。为实现缺失模态的Robustness，我们提议使用简单和参数有效的适应过程来修改预训练的多模态网络。具体来说，我们利用低级别适应和修改中间特征来补做缺失模态。我们示出，这种适应可以部分弥补由缺失模态导致的性能下降，并在一些情况下超过独立、专门为可用模态组合培 trained的独立网络。我们的提议适应需要非常少的参数（例如， fewer than 0.7% of the total parameters in most experiments）。我们通过多种实验表明了我们提议的方法的Robustness，使用RGB-热成像、RGB-深度semantic segmentation、多模态物体 segmentation和多模态情感分析任务。我们的提议方法具有多任务多数据集的多样性，并在不同任务和数据集上超越现有的robust多模态学习方法。
</details></li>
</ul>
<hr>
<h2 id="Towards-Increasing-the-Robustness-of-Predictive-Steering-Control-Autonomous-Navigation-Systems-Against-Dash-Cam-Image-Angle-Perturbations-Due-to-Pothole-Encounters"><a href="#Towards-Increasing-the-Robustness-of-Predictive-Steering-Control-Autonomous-Navigation-Systems-Against-Dash-Cam-Image-Angle-Perturbations-Due-to-Pothole-Encounters" class="headerlink" title="Towards Increasing the Robustness of Predictive Steering-Control Autonomous Navigation Systems Against Dash Cam Image Angle Perturbations Due to Pothole Encounters"></a>Towards Increasing the Robustness of Predictive Steering-Control Autonomous Navigation Systems Against Dash Cam Image Angle Perturbations Due to Pothole Encounters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03959">http://arxiv.org/abs/2310.03959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivam Aarya</li>
<li>for: 本研究旨在提高自动驾驶车辆的稳定性和安全性，通过对摄像头数据进行修正来减少摄像头角度变化引起的迹象预测错误。</li>
<li>methods: 本研究使用了一种新的修正模型，该模型可以根据摄像头数据进行修正，以减少由摄像头角度变化引起的迹象预测错误。</li>
<li>results: 在使用公共可用数据集进行评估时，本研究发现该修正模型可以将迹象预测错误率降低至2.3%，从而提高自动驾驶车辆的稳定性和安全性。<details>
<summary>Abstract</summary>
Vehicle manufacturers are racing to create autonomous navigation and steering control algorithms for their vehicles. These software are made to handle various real-life scenarios such as obstacle avoidance and lane maneuvering. There is some ongoing research to incorporate pothole avoidance into these autonomous systems. However, there is very little research on the effect of hitting a pothole on the autonomous navigation software that uses cameras to make driving decisions. Perturbations in the camera angle when hitting a pothole can cause errors in the predicted steering angle. In this paper, we present a new model to compensate for such angle perturbations and reduce any errors in steering control prediction algorithms. We evaluate our model on perturbations of publicly available datasets and show our model can reduce the errors in the estimated steering angle from perturbed images to 2.3%, making autonomous steering control robust against the dash cam image angle perturbations induced when one wheel of a car goes over a pothole.
</details>
<details>
<summary>摘要</summary>
自动驾驶车制造商正在奔腾地开发自动导航和推力控制算法，以适应不同的实际景景，如避免障碍物和车道弯道。然而，关于弹射坑的影响在自动驾驶系统中的研究很少。当车辆过坑时，摄像头角度的偏移会导致驾驶控制预测错误。在这篇论文中，我们提出了一种新的模型，以减少由摄像头角度偏移引起的驾驶控制预测错误。我们使用公共可用的数据集进行评估，并证明我们的模型可以将摄像头角度偏移引起的错误降低至2.3%，使自动驾驶控制更加Robust againstdash cam image angle perturbations induced by potholes.
</details></li>
</ul>
<hr>
<h2 id="Understanding-prompt-engineering-may-not-require-rethinking-generalization"><a href="#Understanding-prompt-engineering-may-not-require-rethinking-generalization" class="headerlink" title="Understanding prompt engineering may not require rethinking generalization"></a>Understanding prompt engineering may not require rethinking generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03957">http://arxiv.org/abs/2310.03957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Akinwande, Yiding Jiang, Dylan Sam, J. Zico Kolter</li>
<li>for: 这篇论文旨在解释逻辑推理模型在不需要训练的情况下，如何具有良好的泛化性。</li>
<li>methods: 该论文使用的方法是通过设计提示来建立分类器，而不需要显式的训练过程。</li>
<li>results: 该论文显示，使用提示的方法可以具有remarkably tight的泛化 bound，并且可以用来 justify the widespread practice of prompt engineering，即通过设计提示来实现良好的测试性能。<details>
<summary>Abstract</summary>
Zero-shot learning in prompted vision-language models, the practice of crafting prompts to build classifiers without an explicit training process, has achieved impressive performance in many settings. This success presents a seemingly surprising observation: these methods suffer relatively little from overfitting, i.e., when a prompt is manually engineered to achieve low error on a given training set (thus rendering the method no longer actually zero-shot), the approach still performs well on held-out test data. In this paper, we show that we can explain such performance well via recourse to classical PAC-Bayes bounds. Specifically, we show that the discrete nature of prompts, combined with a PAC-Bayes prior given by a language model, results in generalization bounds that are remarkably tight by the standards of the literature: for instance, the generalization bound of an ImageNet classifier is often within a few percentage points of the true test error. We demonstrate empirically that this holds for existing handcrafted prompts and prompts generated through simple greedy search. Furthermore, the resulting bound is well-suited for model selection: the models with the best bound typically also have the best test performance. This work thus provides a possible justification for the widespread practice of prompt engineering, even if it seems that such methods could potentially overfit the training data.
</details>
<details>
<summary>摘要</summary>
zero-shot learning 在提示语言模型中，通过手动设计提示来建立分类器而不需要显式训练过程，已经实现了许多场景中的出色表现。这一成功呈现出一个意外的观察：这些方法具有相对较少的过拟合现象，即在手动设计提示以实现训练集的低错误率（这样的方法再不是真正的零shot learning）时，这种方法仍然能够在封闭测试数据上表现良好。在这篇论文中，我们展示了我们可以通过经典的 PAC-Bayes  bound 来解释这种表现。具体来说，我们表明了提示的整数性，加上基于语言模型的 PAC-Bayes  prior，导致的泛化 bound 是文献中非常紧张的：例如，ImageNet 分类器的泛化 bound 经常在真实测试错误率的几个百分点之间。我们通过实验证明了这一点，并且表明了这种 bound 适用于模型选择：具有最好的 bound 的模型通常也有最好的测试性能。这项工作因此提供了逻辑 justify  prompt engineering 的做法，即使这些方法可能会过拟合训练数据。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Descent-Provably-Solves-Nonlinear-Tomographic-Reconstruction"><a href="#Gradient-Descent-Provably-Solves-Nonlinear-Tomographic-Reconstruction" class="headerlink" title="Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction"></a>Gradient Descent Provably Solves Nonlinear Tomographic Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03956">http://arxiv.org/abs/2310.03956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Fridovich-Keil, Fabrizio Valdivia, Gordon Wetzstein, Benjamin Recht, Mahdi Soltanolkotabi</li>
<li>for: This paper aims to improve the accuracy and reduce artifacts in computed tomography (CT) reconstruction, particularly in the presence of high-density materials such as metal.</li>
<li>methods: The authors propose a direct nonlinear CT reconstruction technique that bypasses the conventional preprocessing step of inverting the nonlinear measurement preprocessing. Instead, they use gradient descent to optimize the nonlinear forward model and reconstruct the underlying signal directly from the raw measurements.</li>
<li>results: The authors demonstrate the effectiveness of their proposed technique through experiments on synthetic and real 3D volumes using cone-beam CT. They show that their approach reduces metal artifacts compared to a commercial reconstruction of a human skull with metal dental crowns, and achieves a near minimal number of random measurements with a geometric rate of convergence. Additionally, they prove similar results in the under-determined setting where the number of measurements is significantly smaller than the dimension of the signal, by enforcing prior structural information about the signal through constraints on the optimization variables.<details>
<summary>Abstract</summary>
In computed tomography (CT), the forward model consists of a linear Radon transform followed by an exponential nonlinearity based on the attenuation of light according to the Beer-Lambert Law. Conventional reconstruction often involves inverting this nonlinearity as a preprocessing step and then solving a convex inverse problem. However, this nonlinear measurement preprocessing required to use the Radon transform is poorly conditioned in the vicinity of high-density materials, such as metal. This preprocessing makes CT reconstruction methods numerically sensitive and susceptible to artifacts near high-density regions. In this paper, we study a technique where the signal is directly reconstructed from raw measurements through the nonlinear forward model. Though this optimization is nonconvex, we show that gradient descent provably converges to the global optimum at a geometric rate, perfectly reconstructing the underlying signal with a near minimal number of random measurements. We also prove similar results in the under-determined setting where the number of measurements is significantly smaller than the dimension of the signal. This is achieved by enforcing prior structural information about the signal through constraints on the optimization variables. We illustrate the benefits of direct nonlinear CT reconstruction with cone-beam CT experiments on synthetic and real 3D volumes. We show that this approach reduces metal artifacts compared to a commercial reconstruction of a human skull with metal dental crowns.
</details>
<details>
<summary>摘要</summary>
在计算Tomography（CT）中，前向模型包括线性的朗逊变换，然后是基于减弱光的泽米特律的不对称非线性。常见的重建通常需要在这种非线性预处理中进行逆转，然后解决一个凸 inverse problem。然而，这种非线性测量预处理在高密度材料，如金属附近，是糟糕的conditioned。这种预处理使CT重建方法数值敏感和受到artifacts的影响。在这篇论文中，我们研究了一种技术，其中信号直接从Raw Measurements中重建 через非线性前向模型。虽这个优化是非凸的，但我们显示了gradient descent可提able地 converge到全局最优点，完美地重建下面的信号，使用最小量的随机测量。我们也证明了相似的结果在下determined setting中，其中测量量 Significantly smaller than the dimension of the signal。这是通过在优化变量上添加信号的先验结构信息来实现的。我们在Synthetic和实际3Dvolumes上进行了 cone-beam CT实验，并示出了这种方法可以减少金属残余相比于一个商业重建的人骨头with metal dental crowns。
</details></li>
</ul>
<hr>
<h2 id="ILSH-The-Imperial-Light-Stage-Head-Dataset-for-Human-Head-View-Synthesis"><a href="#ILSH-The-Imperial-Light-Stage-Head-Dataset-for-Human-Head-View-Synthesis" class="headerlink" title="ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis"></a>ILSH: The Imperial Light-Stage Head Dataset for Human Head View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03952">http://arxiv.org/abs/2310.03952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiali Zheng, Youngkyoon Jang, Athanasios Papaioannou, Christos Kampouris, Rolandos Alexandros Potamias, Foivos Paraperas Papantoniou, Efstathios Galanakis, Ales Leonardis, Stefanos Zafeiriou</li>
<li>for: 本研究 introduce Imperial Light-Stage Head (ILSH) dataset, a novel light-stage-captured human head dataset to support view synthesis academic challenges for human heads.</li>
<li>methods: 本研究使用 specifically designed light-stage to capture high-resolution (4K) human head images, and addresses challenges (preprocessing, ethical issues) in collecting high-quality data.</li>
<li>results: 研究 obtained 1,248 close-up head images, border masks, and camera pose pairs from 52 subjects captured using 24 cameras with all 82 lighting sources turned on.<details>
<summary>Abstract</summary>
This paper introduces the Imperial Light-Stage Head (ILSH) dataset, a novel light-stage-captured human head dataset designed to support view synthesis academic challenges for human heads. The ILSH dataset is intended to facilitate diverse approaches, such as scene-specific or generic neural rendering, multiple-view geometry, 3D vision, and computer graphics, to further advance the development of photo-realistic human avatars. This paper details the setup of a light-stage specifically designed to capture high-resolution (4K) human head images and describes the process of addressing challenges (preprocessing, ethical issues) in collecting high-quality data. In addition to the data collection, we address the split of the dataset into train, validation, and test sets. Our goal is to design and support a fair view synthesis challenge task for this novel dataset, such that a similar level of performance can be maintained and expected when using the test set, as when using the validation set. The ILSH dataset consists of 52 subjects captured using 24 cameras with all 82 lighting sources turned on, resulting in a total of 1,248 close-up head images, border masks, and camera pose pairs.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了帝国光台头（ILSH）数据集，这是一个新的光台捕捉的人头数据集，旨在支持人头视 synthesis学术挑战。ILSH数据集旨在促进多种方法的发展，如场景特定或通用神经渲染、多视图几何、3D视觉和计算机图形等，以提高人头化的图像质量。本文介绍了使用特定设计的光台捕捉高分辨率（4K）人头图像的过程，以及收集数据时遇到的挑战和伦理问题的处理方法。此外，文章还详细介绍了数据集的分区方法，包括训练集、验证集和测试集的分割。我们的目标是设计和支持一个公平的视 synthesis挑战任务，以便在使用测试集时和使用验证集时的表现水平具有相同的稳定性。ILSH数据集包含52名参与者，通过24台摄像头拍摄，共有82个灯光源 turned on，得到了1,248个close-up头像、边框mask和相机pose对。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/cs.CV_2023_10_06/" data-id="closbrop700ji0g889ia269i3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/cs.AI_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T12:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/cs.AI_2023_10_06/">cs.AI - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Copy-Suppression-Comprehensively-Understanding-an-Attention-Head"><a href="#Copy-Suppression-Comprehensively-Understanding-an-Attention-Head" class="headerlink" title="Copy Suppression: Comprehensively Understanding an Attention Head"></a>Copy Suppression: Comprehensively Understanding an Attention Head</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04625">http://arxiv.org/abs/2310.04625</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/callummcdougall/seri-mats-2023-streamlit-pages">https://github.com/callummcdougall/seri-mats-2023-streamlit-pages</a></li>
<li>paper_authors: Callum McDougall, Arthur Conmy, Cody Rushing, Thomas McGrath, Neel Nanda</li>
<li>for: 本研究主要针对语言模型中的一个重要组件——注意头10.7（L10H7），以及它在模型训练过程中的作用。</li>
<li>methods: 本研究使用了GPT-2 Small语言模型，并通过分析模型的 weights 来描述 L10H7 的减震机制。</li>
<li>results: 研究发现，L10H7 可以减少模型的复制行为，并且这种减震机制对于自修复（self-repair）具有重要作用。自修复指的是，当模型中的某些组件被 удали时，下游神经网络部分会进行补做，以维持模型的正确性。研究还发现，自修复的一个重要机制就是复制减震。<details>
<summary>Abstract</summary>
We present a single attention head in GPT-2 Small that has one main role across the entire training distribution. If components in earlier layers predict a certain token, and this token appears earlier in the context, the head suppresses it: we call this copy suppression. Attention Head 10.7 (L10H7) suppresses naive copying behavior which improves overall model calibration. This explains why multiple prior works studying certain narrow tasks found negative heads that systematically favored the wrong answer. We uncover the mechanism that the Negative Heads use for copy suppression with weights-based evidence and are able to explain 76.9% of the impact of L10H7 in GPT-2 Small. To the best of our knowledge, this is the most comprehensive description of the complete role of a component in a language model to date. One major effect of copy suppression is its role in self-repair. Self-repair refers to how ablating crucial model components results in downstream neural network parts compensating for this ablation. Copy suppression leads to self-repair: if an initial overconfident copier is ablated, then there is nothing to suppress. We show that self-repair is implemented by several mechanisms, one of which is copy suppression, which explains 39% of the behavior in a narrow task. Interactive visualisations of the copy suppression phenomena may be seen at our web app https://copy-suppression.streamlit.app/
</details>
<details>
<summary>摘要</summary>
我们提出了一个单一的注意头在GPT-2 Small中，这个注意头在整个训练分布中有一个主要角色。如果在earlier层中的 компонент预测了某个 tokens，并且这个 tokens 在上下文中出现得更早，那么这个注意头会对它进行抑制：我们称这为copy suppression。注意头10.7（L10H7）对于naive copying行为进行抑制，这解释了为什么多个先前的研究在特定的狭频任务中发现了负面的头。我们探索了这个机制的负面头使用 weights-based evidence 的实际方式，并能够解释76.9%的L10H7在GPT-2 Small中的影响。根据我们所知，这是 language model 中 Component 的最完整的角色描述至今。一个主要的效果 OF copy suppression 是 self-repair。self-repair 指的是当模型中的重要部分被删除时，下游神经网络部分会对此进行补偿。copy suppression 导致 self-repair：如果初始的骄傲 copier 被删除，那么没有什么可以对其进行抑制。我们显示了 self-repair 是通过多种机制实现的，其中一种是 copy suppression，这解释了39%的行为在狭频任务中。可以在我们的网页应用中看到互动的visualisations of the copy suppression 现象：https://copy-suppression.streamlit.app/
</details></li>
</ul>
<hr>
<h2 id="Deconstructing-Cooperation-and-Ostracism-via-Multi-Agent-Reinforcement-Learning"><a href="#Deconstructing-Cooperation-and-Ostracism-via-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Deconstructing Cooperation and Ostracism via Multi-Agent Reinforcement Learning"></a>Deconstructing Cooperation and Ostracism via Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04623">http://arxiv.org/abs/2310.04623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Atsushi Ueshima, Shayegan Omidshafiei, Hirokazu Shirado</li>
<li>for: 这个论文探讨了在生物系统、人类社会和多代理系统中的合作挑战，以及如何通过网络重构来解决这些挑战。</li>
<li>methods: 作者使用了多代理人学习 simulate the Prisoner’s Dilemma game，并研究了在不同的连接策略下，合作和网络重构之间的复杂 causal 动力。</li>
<li>results: 研究发现，网络重构可以促进双方合作，即使一方总是合作。此外，研究还发现， ostracism 是 network rewiring 的关键因素，但 ostracism  alone 不能使合作出现。相反， ostracism 是在学习合作后才出现的，并且已有的合作则由 ostracism 加强。<details>
<summary>Abstract</summary>
Cooperation is challenging in biological systems, human societies, and multi-agent systems in general. While a group can benefit when everyone cooperates, it is tempting for each agent to act selfishly instead. Prior human studies show that people can overcome such social dilemmas while choosing interaction partners, i.e., strategic network rewiring. However, little is known about how agents, including humans, can learn about cooperation from strategic rewiring and vice versa. Here, we perform multi-agent reinforcement learning simulations in which two agents play the Prisoner's Dilemma game iteratively. Each agent has two policies: one controls whether to cooperate or defect; the other controls whether to rewire connections with another agent. This setting enables us to disentangle complex causal dynamics between cooperation and network rewiring. We find that network rewiring facilitates mutual cooperation even when one agent always offers cooperation, which is vulnerable to free-riding. We then confirm that the network-rewiring effect is exerted through agents' learning of ostracism, that is, connecting to cooperators and disconnecting from defectors. However, we also find that ostracism alone is not sufficient to make cooperation emerge. Instead, ostracism emerges from the learning of cooperation, and existing cooperation is subsequently reinforced due to the presence of ostracism. Our findings provide insights into the conditions and mechanisms necessary for the emergence of cooperation with network rewiring.
</details>
<details>
<summary>摘要</summary>
合作在生物系统、人类社会和多代理系统中都是挑战。而每个代理都可能会选择自利而不是合作。人类研究表明，人们可以在选择互动伙伴时超越社会困境，即策略网络重启。然而，关于代理如何从策略重启中学习合作以及vice versa， ainda不够了解。在这里，我们通过多代理学习回归 simulations进行了 investigate。两个代理在谎言游戏中互动，每个代理有两个策略：一个控制合作或背叛；另一个控制与另一个代理的连接。这种设置允许我们分离复杂的 causal 动力。我们发现，网络重启可以促进互合作，即使一个代理总是合作，容易受到恶意骗取。然后，我们确认了网络重启的效果是通过代理学习排斥来实现的，即与合作者连接并与背叛者断开。然而，我们也发现，排斥本身不足以使合作出现。相反，排斥是由学习合作而起的，并且现有的合作后来受到了排斥的加强。我们的发现可以为合作的出现和维护提供条件和机制。
</details></li>
</ul>
<hr>
<h2 id="Model-Compression-in-Practice-Lessons-Learned-from-Practitioners-Creating-On-device-Machine-Learning-Experiences"><a href="#Model-Compression-in-Practice-Lessons-Learned-from-Practitioners-Creating-On-device-Machine-Learning-Experiences" class="headerlink" title="Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences"></a>Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04621">http://arxiv.org/abs/2310.04621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fred Hohman, Mary Beth Kery, Donghao Ren, Dominik Moritz</li>
<li>for: 本研究旨在推动机器学习（ML） computations 在日常用户设备上进行，以提高隐私、响应速度和新智能用户体验的普及。</li>
<li>methods: 本研究使用了 Apple 专家的采访研究，汲取了具有实践经验的专家在模型压缩方面的tacit knowledge。</li>
<li>results: 研究发现了一些实践中的技术策略和设计考虑，以及在不同硬件平台上实现高效模型的具体步骤。此外，研究还提出了一些工具设计建议，以便使得在设备上进行 ML computations 的工作更加容易。<details>
<summary>Abstract</summary>
On-device machine learning (ML) promises to improve the privacy, responsiveness, and proliferation of new, intelligent user experiences by moving ML computation onto everyday personal devices. However, today's large ML models must be drastically compressed to run efficiently on-device, a hurtle that requires deep, yet currently niche expertise. To engage the broader human-centered ML community in on-device ML experiences, we present the results from an interview study with 30 experts at Apple that specialize in producing efficient models. We compile tacit knowledge that experts have developed through practical experience with model compression across different hardware platforms. Our findings offer pragmatic considerations missing from prior work, covering the design process, trade-offs, and technical strategies that go into creating efficient models. Finally, we distill design recommendations for tooling to help ease the difficulty of this work and bring on-device ML into to more widespread practice.
</details>
<details>
<summary>摘要</summary>
“设备机器学习（ML）将提高用户隐私、响应速度和新智能体验的普及，但现在的大型ML模型需要压缩运行在日常个人设备上，这是一项需要深厚专业知识的挑战。为了让更广泛的人类中心ML社区参与在设备上ML经验，我们公布了30名Apple专家在模型压缩 across不同硬件平台的实践经验。我们汇集了专家在实践中发展的潜在知识，包括设计过程、让步和技术策略。我们的发现缺失在先前的工作中，提供了实用的建议，以帮助抵消这项工作的困难。最后，我们提炼了工具设计的建议，以便更好地普及设备上ML技术。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Hong Kong, Macau, and Taiwan. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SlotGNN-Unsupervised-Discovery-of-Multi-Object-Representations-and-Visual-Dynamics"><a href="#SlotGNN-Unsupervised-Discovery-of-Multi-Object-Representations-and-Visual-Dynamics" class="headerlink" title="SlotGNN: Unsupervised Discovery of Multi-Object Representations and Visual Dynamics"></a>SlotGNN: Unsupervised Discovery of Multi-Object Representations and Visual Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04617">http://arxiv.org/abs/2310.04617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Rezazadeh, Athreyi Badithela, Karthik Desingh, Changhyun Choi</li>
<li>for: 这 paper 是用于学习多对象动力学从视觉数据中的不监督技术。</li>
<li>methods: 这 paper 使用了两个新的建筑：SlotTransport 和 SlotGNN。SlotTransport 是一种基于槽注意力的无监督物体发现算法，使用了特征传输机制来保持时间Alignment在物体-中心表示中。SlotGNN 是一种无监督图像基于 Scene 的动力学模型，使用了发现的槽来预测未来Scene 的状态。</li>
<li>results: 这 paper  demonstarted SlotTransport 可以学习准确地编码 both visual 和位置信息，而 SlotGNN 可以在多对象重新排序和长期预测等 robotic 任务中准确预测 slots 和它们的动力学行为。此外，这 paper 的无监督方法在实际世界中也得到了证明。<details>
<summary>Abstract</summary>
Learning multi-object dynamics from visual data using unsupervised techniques is challenging due to the need for robust, object representations that can be learned through robot interactions. This paper presents a novel framework with two new architectures: SlotTransport for discovering object representations from RGB images and SlotGNN for predicting their collective dynamics from RGB images and robot interactions. Our SlotTransport architecture is based on slot attention for unsupervised object discovery and uses a feature transport mechanism to maintain temporal alignment in object-centric representations. This enables the discovery of slots that consistently reflect the composition of multi-object scenes. These slots robustly bind to distinct objects, even under heavy occlusion or absence. Our SlotGNN, a novel unsupervised graph-based dynamics model, predicts the future state of multi-object scenes. SlotGNN learns a graph representation of the scene using the discovered slots from SlotTransport and performs relational and spatial reasoning to predict the future appearance of each slot conditioned on robot actions. We demonstrate the effectiveness of SlotTransport in learning object-centric features that accurately encode both visual and positional information. Further, we highlight the accuracy of SlotGNN in downstream robotic tasks, including challenging multi-object rearrangement and long-horizon prediction. Finally, our unsupervised approach proves effective in the real world. With only minimal additional data, our framework robustly predicts slots and their corresponding dynamics in real-world control tasks.
</details>
<details>
<summary>摘要</summary>
学习多对象动力学从视觉数据中使用无监督技术是具有挑战性的，因为需要Robust，可以通过机器人互动学习的对象表示。这篇论文提出了一个新的框架，包括两种新的架构：SlotTransport用于从RGB图像中发现对象表示，以及SlotGNN用于基于RGB图像和机器人互动预测多对象场景的共同动力学。我们的SlotTransport架构基于插槽注意力来无监督地发现对象，并使用特征传输机制来保持时间Alignment在对象中心表示。这使得可以发现具有固定组合的对象插槽，即使在压抑或缺失情况下也能够稳定地绑定到对象。我们的SlotGNN是一种新的无监督图形学模型，它使用发现的插槽来学习场景的图形表示，并在插槽之间进行关系和空间的推理来预测未来场景的 appears。我们 demonstarte了SlotTransport的有效性在学习对象中心特征，以及SlotGNN在下游机器人任务中的准确性。最后，我们的无监督方法在实际世界中得到了证明，只需要最少的额外数据，我们的框架就可以在实际控制任务中Robust地预测插槽和它们的相应动力学。
</details></li>
</ul>
<hr>
<h2 id="DeepSpeed4Science-Initiative-Enabling-Large-Scale-Scientific-Discovery-through-Sophisticated-AI-System-Technologies"><a href="#DeepSpeed4Science-Initiative-Enabling-Large-Scale-Scientific-Discovery-through-Sophisticated-AI-System-Technologies" class="headerlink" title="DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies"></a>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04610">http://arxiv.org/abs/2310.04610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, Pete Luferenko, Divya Kumar, Jonathan Weyn, Ruixiong Zhang, Sylwester Klocek, Volodymyr Vragov, Mohammed AlQuraishi, Gustaf Ahdritz, Christina Floristean, Cristina Negri, Rao Kotamarthi, Venkatram Vishwanath, Arvind Ramanathan, Sam Foreman, Kyle Hippe, Troy Arcomano, Romit Maulik, Maxim Zvyagin, Alexander Brace, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, Carla M. Mann, Michael Irvin, J. Gregory Pauloski, Logan Ward, Valerie Hayot, Murali Emani, Zhen Xie, Diangen Lin, Maulik Shukla, Ian Foster, James J. Davis, Michael E. Papka, Thomas Brettin, Prasanna Balaprakash, Gina Tourassi, John Gounley, Heidi Hanson, Thomas E Potok, Massimiliano Lupo Pasini, Kate Evans, Dan Lu, Dalton Lunga, Junqi Yin, Sajal Dash, Feiyi Wang, Mallikarjun Shankar, Isaac Lyngaas, Xiao Wang, Guojing Cong, Pei Zhang, Ming Fan, Siyan Liu, Adolfy Hoisie, Shinjae Yoo, Yihui Ren, William Tang, Kyle Felker, Alexey Svyatkovskiy, Hang Liu, Ashwin Aji, Angela Dalton, Michael Schulte, Karl Schulz, Yuntian Deng, Weili Nie, Josh Romero, Christian Dallago, Arash Vahdat, Chaowei Xiao, Thomas Gibbs, Anima Anandkumar, Rick Stevens</li>
<li>for: 这个论文目的是要探讨深度学习如何应用于自然科学领域，以推动科学探索和发现。</li>
<li>methods: 这个研究使用了DeepSpeed4Science倡议，利用深度学习系统技术创新来帮助领域专家解释今天最大的科学谜团。</li>
<li>results: 这个研究获得了初步的进展，通过对结构生物学研究中的两个系统挑战进行解决。<details>
<summary>Abstract</summary>
In the upcoming decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. To answer this call, we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. By leveraging DeepSpeed's current technology pillars (training, inference and compression) as base technology enablers, DeepSpeed4Science will create a new set of AI system technologies tailored for accelerating scientific discoveries by addressing their unique complexity beyond the common technical approaches used for accelerating generic large language models (LLMs). In this paper, we showcase the early progress we made with DeepSpeed4Science in addressing two of the critical system challenges in structural biology research.
</details>
<details>
<summary>摘要</summary>
在未来的一个 década，深度学习可能会革命化自然科学，提高我们对自然现象的模型和预测能力。这可能会开启一个新的科学探索时代，带来重要的进步 across 多个领域，从药物开发到可再生能源。为回答这个呼吁，我们提出了 DeepSpeed4Science  iniciativa（deepspeed4science.ai），旨在通过人工智能系统技术创新，帮助领域专家解开今天最大的科学谜团。通过利用 DeepSpeed 的当前技术柱（训练、推理和压缩）作为基础技术驱动者，DeepSpeed4Science 将创造一个新的人工智能系统技术，用于加速科学发现，超越常见的技术方法used for 加速通用大语言模型（LLMs）。在这篇论文中，我们展示了 DeepSpeed4Science 在结构生物研究中的早期进展。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Performance-Study-of-Large-Language-Models-on-Novel-AI-Accelerators"><a href="#A-Comprehensive-Performance-Study-of-Large-Language-Models-on-Novel-AI-Accelerators" class="headerlink" title="A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators"></a>A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04607">http://arxiv.org/abs/2310.04607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Murali Emani, Sam Foreman, Varuni Sastry, Zhen Xie, Siddhisanket Raskar, William Arnold, Rajeev Thakur, Venkatram Vishwanath, Michael E. Papka</li>
<li>for: 本研究使用大型自然语言模型（LLM）来加速科学应用，检验不同AI加速器硬件系统的性能特点。</li>
<li>methods: 本研究使用多种AI加速器和GPU进行比较性能测试，包括一个核心转换块微型Benchmark、GPT-2模型和GenSLM科学应用案例。</li>
<li>results: 研究发现不同AI加速器在处理LLM模型时的性能特点，包括序列长度、缩放行为、缺失率和梯度积累步骤的影响。<details>
<summary>Abstract</summary>
Artificial intelligence (AI) methods have become critical in scientific applications to help accelerate scientific discovery. Large language models (LLMs) are being considered as a promising approach to address some of the challenging problems because of their superior generalization capabilities across domains. The effectiveness of the models and the accuracy of the applications is contingent upon their efficient execution on the underlying hardware infrastructure. Specialized AI accelerator hardware systems have recently become available for accelerating AI applications. However, the comparative performance of these AI accelerators on large language models has not been previously studied. In this paper, we systematically study LLMs on multiple AI accelerators and GPUs and evaluate their performance characteristics for these models. We evaluate these systems with (i) a micro-benchmark using a core transformer block, (ii) a GPT- 2 model, and (iii) an LLM-driven science use case, GenSLM. We present our findings and analyses of the models' performance to better understand the intrinsic capabilities of AI accelerators. Furthermore, our analysis takes into account key factors such as sequence lengths, scaling behavior, sparsity, and sensitivity to gradient accumulation steps.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）方法已成为科学应用中的关键因素，以加速科学发现。大型语言模型（LLM）被视为解决一些挑战性问题的有望方法，因为它们在领域之间具有优秀的总体化能力。模型的效果和应用的准确性取决于其下面硬件基础设施的高效运行。特殊的AI加速器硬件系统在最近成为了加速AI应用的选择。然而，这些AI加速器对大型语言模型的性能尚未被系统性研究。在这篇论文中，我们系统地研究了多种AI加速器和GPU在LLM上的性能特点。我们使用（i）一个核心变换块的微型 benchmark，（ii）GPT-2模型，以及（iii）一个基于LLM的科学应用use case，GenSLM。我们提供我们的发现和分析结果，以更好地理解AI加速器的内在能力。此外，我们的分析考虑了序列长度、缩放行为、稀疏性和梯度积累步骤的影响。
</details></li>
</ul>
<hr>
<h2 id="A-neuro-symbolic-framework-for-answering-conjunctive-queries"><a href="#A-neuro-symbolic-framework-for-answering-conjunctive-queries" class="headerlink" title="A neuro-symbolic framework for answering conjunctive queries"></a>A neuro-symbolic framework for answering conjunctive queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04598">http://arxiv.org/abs/2310.04598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Barceló, Tamara Cucumides, Floris Geerts, Juan Reutter, Miguel Romero</li>
<li>for:  answering arbitrary conjunctive queries over incomplete knowledge graphs</li>
<li>methods:  approximating cyclic queries with an infinite family of tree-like queries, leveraging existing neuro-symbolic models</li>
<li>results:  strong guarantees of completeness and optimality, competitive results and improved performance with existentially quantified variables.Here’s the Chinese version:</li>
<li>for:  answering incomplete knowledge graphs中的任意谱 queries</li>
<li>methods:  approximating循环 queries by infinite family of树型 queries, 利用现有的 neuralsymbolic models</li>
<li>results:  strong guarantees of completeness和optimality, competitive results and improved performance with existentially quantified variables.<details>
<summary>Abstract</summary>
The problem of answering logical queries over incomplete knowledge graphs is receiving significant attention in the machine learning community. Neuro-symbolic models are a promising recent approach, showing good performance and allowing for good interpretability properties. These models rely on trained architectures to execute atomic queries, combining them with modules that simulate the symbolic operators in queries. Unfortunately, most neuro-symbolic query processors are limited to the so-called tree-like logical queries that admit a bottom-up execution, where the leaves are constant values or anchors, and the root is the target variable. Tree-like queries, while expressive, fail short to express properties in knowledge graphs that are important in practice, such as the existence of multiple edges between entities or the presence of triangles.   We propose a framework for answering arbitrary conjunctive queries over incomplete knowledge graphs. The main idea of our method is to approximate a cyclic query by an infinite family of tree-like queries, and then leverage existing models for the latter. Our approximations achieve strong guarantees: they are complete, i.e. there are no false negatives, and optimal, i.e. they provide the best possible approximation using tree-like queries. Our method requires the approximations to be tree-like queries where the leaves are anchors or existentially quantified variables. Hence, we also show how some of the existing neuro-symbolic models can handle these queries, which is of independent interest. Experiments show that our approximation strategy achieves competitive results, and that including queries with existentially quantified variables tends to improve the general performance of these models, both on tree-like queries and on our approximation strategy.
</details>
<details>
<summary>摘要</summary>
machine learning 社区中受到“回答逻辑查询 над 不完整知识图”的问题 receiving significant attention。 neuro-symbolic 模型是一种可靠的新方法，表现良好并具有良好解释性质。这些模型通过训练架构来执行原子查询，并将其与模块组合以模拟查询中的符号运算。然而，大多数 neuro-symbolic 查询处理器都受到限制，只能处理叶子结构式查询，即叶子是常量值或吊钩，根是目标变量。叶子结构式查询，虽然表达力强，但缺少在实际中重要的知识图特性，如多个边 между实体或存在三角形。 我们提出了一个 answering arbitrary conjunctive queries over incomplete knowledge graphs 的框架。我们的方法的主要想法是将 cyclic 查询近似为无穷多个 tree-like 查询，然后利用现有模型来处理后者。我们的近似 garanties 是完整的，即无 false negatives，以及优化的，即它们在 tree-like 查询中提供了最好的近似。我们的方法需要近似是 tree-like 查询的叶子是吊钩或 universally quantified 变量。因此，我们还证明了一些现有的 neuro-symbolic 模型可以处理这些查询，这是独立有趣的。实验显示，我们的近似策略实现了竞争力的结果，并且包括含 universally quantified 变量的查询通常会提高这些模型的总性能，不 только在 tree-like 查询上，还在我们的近似策略上。
</details></li>
</ul>
<hr>
<h2 id="Segmented-Harmonic-Loss-Handling-Class-Imbalanced-Multi-Label-Clinical-Data-for-Medical-Coding-with-Large-Language-Models"><a href="#Segmented-Harmonic-Loss-Handling-Class-Imbalanced-Multi-Label-Clinical-Data-for-Medical-Coding-with-Large-Language-Models" class="headerlink" title="Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical Data for Medical Coding with Large Language Models"></a>Segmented Harmonic Loss: Handling Class-Imbalanced Multi-Label Clinical Data for Medical Coding with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04595">http://arxiv.org/abs/2310.04595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surjya Ray, Pratik Mehta, Hongen Zhang, Ada Chaman, Jian Wang, Chung-Jen Ho, Michael Chiou, Tashfeen Suleman</li>
<li>for: 这篇论文旨在评估大自然语言模型（LLM）在医疗领域的应用，特别是在实际噪音数据上进行医疗编码任务。</li>
<li>methods: 作者使用了encoder-based LLMs，如BERT，并开发了一种新的损失函数，即分割和解耦多个类别的数据集的Segmented Harmonic Loss。此外，作者还提出了一种基于embedding相似性的技术来处理噪音数据。</li>
<li>results: 作者的实验结果表明，当使用提议的损失函数进行训练时，LLMs在噪音长尾数据上达到了显著性能提升，与状态 искусственный智能的F1分数相比，提高了十几个百分点。<details>
<summary>Abstract</summary>
The precipitous rise and adoption of Large Language Models (LLMs) have shattered expectations with the fastest adoption rate of any consumer-facing technology in history. Healthcare, a field that traditionally uses NLP techniques, was bound to be affected by this meteoric rise. In this paper, we gauge the extent of the impact by evaluating the performance of LLMs for the task of medical coding on real-life noisy data. We conducted several experiments on MIMIC III and IV datasets with encoder-based LLMs, such as BERT. Furthermore, we developed Segmented Harmonic Loss, a new loss function to address the extreme class imbalance that we found to prevail in most medical data in a multi-label scenario by segmenting and decoupling co-occurring classes of the dataset with a new segmentation algorithm. We also devised a technique based on embedding similarity to tackle noisy data. Our experimental results show that when trained with the proposed loss, the LLMs achieve significant performance gains even on noisy long-tailed datasets, outperforming the F1 score of the state-of-the-art by over ten percentage points.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的急剧升级和推广，历史上任何消费者面向技术中最快的采用率都不能比拟。医疗领域，曾经使用自然语言处理技术（NLP），不可避免地受到这种飞速的影响。在这篇论文中，我们评估了LLM在医疗数据中的表现，使用实际生成的噪音数据进行评估。我们在MIMIC III和IV dataset上进行了多个实验，使用了BERT等encoder-based LLM。此外，我们还提出了一种新的损失函数——分割和解除相互关联的类别损失函数（Segmented Harmonic Loss），用于Addressing the extreme class imbalance problem in most medical data。此外，我们还提出了一种基于 embedding similarity的技术来处理噪音数据。我们的实验结果表明，当使用我们提出的损失函数进行训练时，LLM在噪音长尾数据上表现出了明显的性能提升，与状态之前的F1分数高出十个百分点以上。
</details></li>
</ul>
<hr>
<h2 id="Can-pruning-make-Large-Language-Models-more-efficient"><a href="#Can-pruning-make-Large-Language-Models-more-efficient" class="headerlink" title="Can pruning make Large Language Models more efficient?"></a>Can pruning make Large Language Models more efficient?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04573">http://arxiv.org/abs/2310.04573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sia Gholami, Marwan Omar</li>
<li>for: 这篇论文旨在探讨用于Transformer架构的Weight遗传减少，以提高模型的 Computational Efficiency、环境影响和资源有限的平台上的部署。</li>
<li>methods: 这篇论文使用了多种剪辑方法，包括阶层剪辑、梯度剪辑和混合剪辑，并评估它们对模型性能、模型大小和computational demand的影响。</li>
<li>results: 研究发现，对于Transformer架构，适当选择剪辑参数可以实现轻量级化模型，而不会对模型性能造成严重干扰。此外，给定的剪辑方法可以提高模型的普遍化能力。<details>
<summary>Abstract</summary>
Transformer models have revolutionized natural language processing with their unparalleled ability to grasp complex contextual relationships. However, the vast number of parameters in these models has raised concerns regarding computational efficiency, environmental impact, and deployability on resource-limited platforms. To address these challenges, this paper investigates the application of weight pruning-a strategic reduction of model parameters based on their significance-as an optimization strategy for Transformer architectures. Through extensive experimentation, we explore various pruning methodologies, highlighting their impact on model performance, size, and computational demands. Our findings suggest that with judicious selection of pruning hyperparameters, significant reductions in model size are attainable without considerable compromise on performance. Moreover, when coupled with post-pruning fine-tuning strategies, some pruned models even exhibit enhanced generalization capabilities. This work seeks to bridge the gap between model efficiency and performance, paving the way for more scalable and environmentally responsible deep learning applications.
</details>
<details>
<summary>摘要</summary>
<translate text="Transformer models have revolutionized natural language processing with their unparalleled ability to grasp complex contextual relationships. However, the vast number of parameters in these models has raised concerns regarding computational efficiency, environmental impact, and deployability on resource-limited platforms. To address these challenges, this paper investigates the application of weight pruning-a strategic reduction of model parameters based on their significance-as an optimization strategy for Transformer architectures. Through extensive experimentation, we explore various pruning methodologies, highlighting their impact on model performance, size, and computational demands. Our findings suggest that with judicious selection of pruning hyperparameters, significant reductions in model size are attainable without considerable compromise on performance. Moreover, when coupled with post-pruning fine-tuning strategies, some pruned models even exhibit enhanced generalization capabilities. This work seeks to bridge the gap between model efficiency and performance, paving the way for more scalable and environmentally responsible deep learning applications." language="zh-CN" /></SYS>Transformer 模型已经对自然语言处理带来革命性的变革，但是它们的庞大参数数量也引发了计算效率、环境影响和资源有限平台上的部署的担忧。为了解决这些挑战，这篇论文探讨了在Transformer架构中应用权重剔除（一种基于参数重要性的参数剔除策略）的应用。通过广泛的实验，我们探讨了不同的剔除方法，并对它们的影响对模型性能、体积和计算需求进行了详细的探讨。我们的发现表明，通过合理地选择剔除超参数的参数，可以实现大幅减少模型体积，而不会对性能造成重要的损害。此外，当与后期剔除精度练习结合使用时，一些剔除后的模型甚至会表现出更高的泛化能力。这种工作旨在bridging模型效率和性能之间的差距，为更可持续和环保的深度学习应用开辟新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Knolling-bot-A-Transformer-based-Approach-to-Organizing-a-Messy-Table"><a href="#Knolling-bot-A-Transformer-based-Approach-to-Organizing-a-Messy-Table" class="headerlink" title="Knolling bot: A Transformer-based Approach to Organizing a Messy Table"></a>Knolling bot: A Transformer-based Approach to Organizing a Messy Table</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04566">http://arxiv.org/abs/2310.04566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Hu, Zhizhuo Zhang, Ruibo Liu, Philippe Wyder, Hod Lipson</li>
<li>for:  equip domestic robots with the ability to perform simple household tidying tasks</li>
<li>methods:  transformer-based approach that predicts the next position of an item in a sequence of neatly positioned items, integrated with a visual perception model and a physical robot arm</li>
<li>results:  a machine that declutters and organizes a dozen freeform items of various shapes and sizes<details>
<summary>Abstract</summary>
In this study, we propose an approach to equip domestic robots with the ability to perform simple household tidying tasks. We focus specifically on 'knolling,' an activity related to organizing scattered items into neat and space-efficient arrangements. Unlike the uniformity of industrial environments, household settings present unique challenges due to their diverse array of items and the subjectivity of tidiness. Here, we draw inspiration from natural language processing (NLP) and utilize a transformer-based approach that predicts the next position of an item in a sequence of neatly positioned items. We integrate the knolling model with a visual perception model and a physical robot arm to demonstrate a machine that declutters and organizes a dozen freeform items of various shapes and sizes.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们提出了一种方法，以使家庭机器人具备简单的家务整理功能。我们专注于“整理”活动，即将杂乱的物品整理成整洁和高效的排序。与工业环境的统一性不同，家庭环境具有各种不同的物品和整理主观性。我们 Draw inspiration from自然语言处理（NLP），并使用变换器基本方法预测下一个item的位置序列中的整理位置。我们将整理模型与视觉识别模型和物理机器臂集成，以示一种机器人可以整理和组织多达十二种不同形状和大小的自由形态物品的机器人。
</details></li>
</ul>
<hr>
<h2 id="Binary-Quantification-and-Dataset-Shift-An-Experimental-Investigation"><a href="#Binary-Quantification-and-Dataset-Shift-An-Experimental-Investigation" class="headerlink" title="Binary Quantification and Dataset Shift: An Experimental Investigation"></a>Binary Quantification and Dataset Shift: An Experimental Investigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04565">http://arxiv.org/abs/2310.04565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pglez82/quant_datasetshift">https://github.com/pglez82/quant_datasetshift</a></li>
<li>paper_authors: Pablo González, Alejandro Moreo, Fabrizio Sebastiani</li>
<li>for: 本研究的目的是调查现有的量化方法在不同类型的数据集shift下的性能，以便开发更加普遍应用的方法。</li>
<li>methods: 本研究使用了一系列的数据生成协议，以模拟不同类型的数据集shift，然后测试现有的量化方法在这些数据集上的性能。</li>
<li>results: 研究发现，许多现有的量化方法只是对优先概率shift进行了robust性测试，而对其他类型的数据集shift并不是robust enough。此外，无论是现有的量化方法还是一些新的方法，都没有能够在所有类型的数据集shift下达到良好的性能。<details>
<summary>Abstract</summary>
Quantification is the supervised learning task that consists of training predictors of the class prevalence values of sets of unlabelled data, and is of special interest when the labelled data on which the predictor has been trained and the unlabelled data are not IID, i.e., suffer from dataset shift. To date, quantification methods have mostly been tested only on a special case of dataset shift, i.e., prior probability shift; the relationship between quantification and other types of dataset shift remains, by and large, unexplored. In this work we carry out an experimental analysis of how current quantification algorithms behave under different types of dataset shift, in order to identify limitations of current approaches and hopefully pave the way for the development of more broadly applicable methods. We do this by proposing a fine-grained taxonomy of types of dataset shift, by establishing protocols for the generation of datasets affected by these types of shift, and by testing existing quantification methods on the datasets thus generated. One finding that results from this investigation is that many existing quantification methods that had been found robust to prior probability shift are not necessarily robust to other types of dataset shift. A second finding is that no existing quantification method seems to be robust enough to dealing with all the types of dataset shift we simulate in our experiments. The code needed to reproduce all our experiments is publicly available at https://github.com/pglez82/quant_datasetshift.
</details>
<details>
<summary>摘要</summary>
“量化任务”是指在已经有标签数据上训练预测器，然后用这些预测器预测未标注数据中类别的浸泡率值的超vised learning任务。这种任务特别有用，当标签数据和未标注数据不是独立同分布（IID）时。目前，量化方法已经主要测试在优先概率偏移的特殊情况下，关于其他类型的数据偏移情况，尚未得到广泛的研究。在这项工作中，我们通过设计细化的数据偏移类型分类、生成受到这些类型偏移的数据集，并测试现有的量化方法在这些数据集上的性能。我们的发现之一是，许多已知的量化方法，曾被发现对优先概率偏移有效，但并不一定对其他类型的数据偏移有效。另一个发现是，现有的任何量化方法都无法对我们在实验中 simulate 的所有类型数据偏移应用。相关的代码可以在 GitHub 上获取，网址是 <https://github.com/pglez82/quant_datasetshift>。
</details></li>
</ul>
<hr>
<h2 id="ReLU-Strikes-Back-Exploiting-Activation-Sparsity-in-Large-Language-Models"><a href="#ReLU-Strikes-Back-Exploiting-Activation-Sparsity-in-Large-Language-Models" class="headerlink" title="ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models"></a>ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04564">http://arxiv.org/abs/2310.04564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad Farajtabar</li>
<li>for: 这个研究旨在探讨如何在大型语言模型（LLMs）中使用ReLU活化函数以提高效率，并且降低在资源受限的设备上的计算成本。</li>
<li>methods: 这个研究使用了ReLU活化函数，并且探讨了将ReLU活化函数应用于LLMs中的练习方法，以及如何将ReLU活化函数与其他活化函数比较。</li>
<li>results: 研究发现，使用ReLU活化函数不会影响LLMs的性能和测试速度，但是可以降低计算成本和运算量。此外，这个研究还提出了一些实践的策略，可以将LLMs的推导运算量降低到3倍以上，并且对性能进行最小的贡献损失。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Foundation-Models-for-Knowledge-Graph-Reasoning"><a href="#Towards-Foundation-Models-for-Knowledge-Graph-Reasoning" class="headerlink" title="Towards Foundation Models for Knowledge Graph Reasoning"></a>Towards Foundation Models for Knowledge Graph Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04562">http://arxiv.org/abs/2310.04562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/DeepGraphLearning/ULTRA">https://github.com/DeepGraphLearning/ULTRA</a></li>
<li>paper_authors: Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, Zhaocheng Zhu</li>
<li>for: 这个研究旨在建立基础模型，以便在语言和视觉领域中进行推论。</li>
<li>methods: 本研究使用了一种名为ULTRA的方法，它可以将基础表示学习到任何语言和视觉资料上，并且可以在不同的知识库中进行推论。</li>
<li>results: 研究发现，使用ULTRA方法可以在57个不同的知识库中进行零式推论，并且可以与特定Graph上训练的强基础模型进行比较。 Fine-tuning可以进一步提高表现。<details>
<summary>Abstract</summary>
Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap. The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies. In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations as a function conditioned on their interactions. Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph. Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. Fine-tuning further boosts the performance.
</details>
<details>
<summary>摘要</summary>
基础模型在语言和视觉领域具有对任何文本和视觉输入进行推理的能力，归功于可转移的表示，如语言中的词汇表。知识图（KG）的不同实体和关系词汇通常不重叠。设计基础模型在KG上的主要挑战是学习可转移的表示，以便在任何图中进行推理。在这种情况下，我们在ULTRA方法中提出了一种学习通用和可转移的图表示的方法。ULTRA建立了基于交互的关系表示，这使得预训练的ULTRA模型可以在未看过的图上进行零基本推理，并且可以通过细化来进一步提高性能。在57个不同的知识图上进行了链接预测实验，我们发现了一个预训练的ULTRA模型在不同的图上的零基本推理性能经常与强基eline模型相当或更高，并且细化可以进一步提高性能。
</details></li>
</ul>
<hr>
<h2 id="Lie-Neurons-Adjoint-Equivariant-Neural-Networks-for-Semisimple-Lie-Algebras"><a href="#Lie-Neurons-Adjoint-Equivariant-Neural-Networks-for-Semisimple-Lie-Algebras" class="headerlink" title="Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras"></a>Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie Algebras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04521">http://arxiv.org/abs/2310.04521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tzu-Yuan Lin, Minghan Zhu, Maani Ghaffari</li>
<li>for: 这篇论文提出了一种随 Lie 代数数据输入的适应 invariants 神经网络，用于处理输入数据的变换。</li>
<li>methods: 该模型使用 conjugate 关系来捕捉变换之间的协变关系，并且利用了killings form 的不变性来扩展到任意半凝arametric Lie 代数。</li>
<li>results: 模型在 homography 模型中的应用显示了其在sl(3) Lie 代数上的扩展性和灵活性。<details>
<summary>Abstract</summary>
This paper proposes an adjoint-equivariant neural network that takes Lie algebra data as input. Various types of equivariant neural networks have been proposed in the literature, which treat the input data as elements in a vector space carrying certain types of transformations. In comparison, we aim to process inputs that are transformations between vector spaces. The change of basis on transformation is described by conjugations, inducing the adjoint-equivariance relationship that our model is designed to capture. Leveraging the invariance property of the Killing form, the proposed network is a general framework that works for arbitrary semisimple Lie algebras. Our network possesses a simple structure that can be viewed as a Lie algebraic generalization of a multi-layer perceptron (MLP). This work extends the application of equivariant feature learning. As an example, we showcase its value in homography modeling using sl(3) Lie algebra.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种随李代数数据输入的随变神经网络。文献中已经提出了各种类型的等变神经网络，它们将输入数据视为元素在一个向量空间上的变换。与之相比，我们的模型处理的输入是变换于向量空间之间的转换。这种转换的变换基的更改被描述为 conjugation，导致我们的模型拥有随变性关系。通过利用李代数内积的不变性，我们的网络是一种通用的李代数扩展，可以应用于任意半简单李代数。我们的网络结构简单，可以视为李代数扩展的多层感知器（MLP）的普遍化。这项工作扩展了等变特征学习的应用范围。例如，我们使用 sl(3) 李代数示例表明其价值在投影学中。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Free-Clients-in-Federated-Learning-for-Focused-Model-Enhancement"><a href="#Utilizing-Free-Clients-in-Federated-Learning-for-Focused-Model-Enhancement" class="headerlink" title="Utilizing Free Clients in Federated Learning for Focused Model Enhancement"></a>Utilizing Free Clients in Federated Learning for Focused Model Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04515">http://arxiv.org/abs/2310.04515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Narayan Ravi, Ilan Shomorony</li>
<li>for: 本研究旨在解决 Federated Learning (FL) 中非优先级客户端参与的挑战，提出了一种名为 Prioritized FL 的分布式机器学习方法。</li>
<li>methods: 该方法使用匹配策略选择非优先级客户端，根据其数据上模型损失与全局数据上模型损失之间的相似程度来决定使用非优先级客户端的更新。</li>
<li>results: 该方法在多种synthetic和benchmark数据集上显示了更快的收敛速度和更高的测试准确率，比基eline更好。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a distributed machine learning approach to learn models on decentralized heterogeneous data, without the need for clients to share their data. Many existing FL approaches assume that all clients have equal importance and construct a global objective based on all clients. We consider a version of FL we call Prioritized FL, where the goal is to learn a weighted mean objective of a subset of clients, designated as priority clients. An important question arises: How do we choose and incentivize well aligned non priority clients to participate in the federation, while discarding misaligned clients? We present FedALIGN (Federated Adaptive Learning with Inclusion of Global Needs) to address this challenge. The algorithm employs a matching strategy that chooses non priority clients based on how similar the models loss is on their data compared to the global data, thereby ensuring the use of non priority client gradients only when it is beneficial for priority clients. This approach ensures mutual benefits as non priority clients are motivated to join when the model performs satisfactorily on their data, and priority clients can utilize their updates and computational resources when their goals align. We present a convergence analysis that quantifies the trade off between client selection and speed of convergence. Our algorithm shows faster convergence and higher test accuracy than baselines for various synthetic and benchmark datasets.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）是一种分布式机器学习方法，用于在分散的非同构数据上学习模型，而不需要客户端共享其数据。许多现有的FL方法假设所有客户端都有相同的重要性，并将所有客户端的数据结构组合成一个全球目标。我们称之为优先FL，其目的是学习一个优先客户端的Weighted Mean目标。一个重要的问题是如何选择和激励与优先客户端不同步的客户端参加联邦，而且抛弃不同步的客户端？我们提出了FedALIGN（联邦适应学习具有全球需求的匹配）来解决这个挑战。这个算法使用一个匹配策略，选择非优先客户端基于它们的模型损失与全球数据之间的相似度，以确保非优先客户端的 gradients 只在优先客户端的目标有益时使用。这种方法确保了非优先客户端的动机 join 联邦，并且优先客户端可以利用其更新和计算资源，当它们的目标相似。我们提供了一个对 client 选择和速度快速融合的可读性分析。我们的算法在多个 sintetic 和 benchmark 数据上显示 faster convergence 和更高的测试精度。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Uniform-Sampling-Offline-Reinforcement-Learning-with-Imbalanced-Datasets"><a href="#Beyond-Uniform-Sampling-Offline-Reinforcement-Learning-with-Imbalanced-Datasets" class="headerlink" title="Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets"></a>Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04413">http://arxiv.org/abs/2310.04413</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Improbable-AI/dw-offline-rl">https://github.com/Improbable-AI/dw-offline-rl</a></li>
<li>paper_authors: Zhang-Wei Hong, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, Pulkit Agrawal</li>
<li>for: 本研究旨在提出一种新的Offline Policy学习算法，以解决现有数据集中具有偏袋性的问题。</li>
<li>methods: 该算法使用了一种新的采样策略，允许策略仅受到“好的数据”的限制，而不是所有数据集中的所有动作。</li>
<li>results: 研究表明，该算法可以在72个偏袋数据集中实现显著的性能提升，并且在D4RL数据集和三种不同的Offline RL算法中也显示出良好的效果。<details>
<summary>Abstract</summary>
Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in the dataset (i.e., uniform sampling). We present a realization of the sampling strategy and an algorithm that can be used as a plug-and-play module in standard offline RL algorithms. Our evaluation demonstrates significant performance gains in 72 imbalanced datasets, D4RL dataset, and across three different offline RL algorithms. Code is available at https://github.com/Improbable-AI/dw-offline-rl.
</details>
<details>
<summary>摘要</summary>
偏好离线策略学习是目标是通过现有的轨迹数据集来学习决策策略，而不是收集更多数据。使用奖励学习（RL）而不是监督学习技术，例如行为复制，的 PRIMARY 动机是找到一个可以高于数据集中的轨迹平均返回的策略。然而，我们实际上发现，当数据集受到低质量轨迹的影响时，当前的偏好离线RL算法并没有显著提高数据集中的平均返回。我们认为这是因为当前的算法假设在数据集中很近的轨迹。如果数据集主要由低质量轨迹组成，这个假设迫使策略模仿低质量的动作。我们解决这个问题，我们提出了一种采样策略，即只允许策略遵循“好的数据”（即高质量轨迹），而不是所有数据集中的动作（即均匀采样）。我们采用了这种采样策略，并开发了一种可以作为标准偏好离线RL算法中的插件模块。我们的评估表明，在72个不均衡数据集、D4RL数据集和三种不同的偏好离线RL算法中，我们的方法具有显著的性能提升。代码可以在https://github.com/Improbable-AI/dw-offline-rl 上获取。
</details></li>
</ul>
<hr>
<h2 id="Policy-Gradient-Training-of-Language-Models-for-Ranking"><a href="#Policy-Gradient-Training-of-Language-Models-for-Ranking" class="headerlink" title="Policy-Gradient Training of Language Models for Ranking"></a>Policy-Gradient Training of Language Models for Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04407">http://arxiv.org/abs/2310.04407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ge Gao, Jonathan D. Chang, Claire Cardie, Kianté Brantley, Thorsten Joachim</li>
<li>for: This paper aims to improve the training of text retrieval models for decision-making systems by introducing a novel training algorithm called Neural PG-RANK.</li>
<li>methods: Neural PG-RANK uses a Plackett-Luce ranking policy to learn to rank, which is a principled method that relies little on complex heuristics. The algorithm unifies the training objective with downstream decision-making quality.</li>
<li>results: The paper presents extensive experiments on various text retrieval benchmarks, showing that Neural PG-RANK achieves remarkable in-domain performance improvement and substantial out-of-domain generalization to some critical datasets used in downstream question answering tasks.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目标是提高决策系统中文本检索模型的训练方法，通过引入一种新的训练算法called Neural PG-RANK。</li>
<li>methods: Neural PG-RANK使用Plackett-Luce排序策略来学习排序，这是一种原理性的方法，它减少了复杂的规则的依赖。这种算法将训练目标与下游决策质量集成起来。</li>
<li>results: 论文提供了多个文本检索 benchmark 上的广泛实验结果，显示 Neural PG-RANK 在具体领域性能上有remarkable提升，并在一些关键的问答任务上具有显著的泛化性。<details>
<summary>Abstract</summary>
Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems via policy gradient, with little reliance on complex heuristics, and it effectively unifies the training objective with downstream decision-making quality. We conduct extensive experiments on various text retrieval benchmarks. The results demonstrate that when the training objective aligns with the evaluation setup, Neural PG-RANK yields remarkable in-domain performance improvement, with substantial out-of-domain generalization to some critical datasets employed in downstream question answering tasks.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)文本检索对于将知识 integrate into 语言处理管道中扮演着关键的角色，从 chat 式网页搜索到问答系统。当前的 state-of-the-art 文本检索模型利用预训练的大语言模型（LLM）来实现竞争性表现，但是通过 Typical 对比损失来训练 LLM-based 检索器时需要复杂的规则，包括选择困难的负例和使用额外的监督作为学习信号。这种依赖于规则的问题来自于对比损失本身是规则的，不直接优化下游决策质量的度量。为解决这个问题，我们引入 Neural PG-RANK，一种新的训练算法，通过将 LLM 实例化为 Plackett-Luce 排序策略来学习排序。Neural PG-RANK 提供了一种原则性的方法，通过策略梯度来训练检索器，减少复杂的规则，并具有良好的决策质量相关性。我们在多个文本检索标准benchmark上进行了广泛的实验。结果表明，当训练目标与评估setup相一致时，Neural PG-RANK 在域中显著提高表现，同时具有一定的外部泛化能力，在一些关键的问答任务上进行了重要的应用。
</details></li>
</ul>
<hr>
<h2 id="Language-Agent-Tree-Search-Unifies-Reasoning-Acting-and-Planning-in-Language-Models"><a href="#Language-Agent-Tree-Search-Unifies-Reasoning-Acting-and-Planning-in-Language-Models" class="headerlink" title="Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models"></a>Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04406">http://arxiv.org/abs/2310.04406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andyz245/LanguageAgentTreeSearch">https://github.com/andyz245/LanguageAgentTreeSearch</a></li>
<li>paper_authors: Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang</li>
<li>for: 提高大型自然语言模型（LLM）在决策任务中的表现，并推广其作为自主代理机制的应用。</li>
<li>methods: 基于Monte Carlo搜索的语言代理搜索（LATS）框架，利用LLM作为代理、价值函数和优化器，充分发挥其内在优势。环境反馈机制为外部问题解决机制，超越现有方法的局限性。</li>
<li>results: 在多个领域（编程、HotPotQA、WebShop）进行了实验，demonstrated LATS在理解和行为方面的效果和通用性。例如，在HumanEval上使用GPT-4时达到94.4%的分数，在WebShop上使用GPT-3.5时平均分数为75.9。<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在各种决策任务上表现出色，但它们依赖于简单的行为过程，无法普遍应用为自主代理。我们介绍了 Language Agent Tree Search（LATS）框架，它将 LLM 作为计划、行为和理解的基础，挖掘它们的潜在能力。 drew inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS 使用 LLM 作为代理、价值函数和优化器，将其用于更好的决策。在这种方法中，环境提供了外部反馈，这意味着更加积极和适应的问题解决机制，超越现有技术的局限性。我们在多个领域进行了实验，包括编程、HotPotQA 和 WebShop，展示了 LATS 在理解和行为方面的可应用性。特别是，LATS 在 HumanEval 上使用 GPT-4  achieve 94.4%，并在 WebShop 上使用 GPT-3.5 获得了平均分数为 75.9，这说明了我们的方法的有效性和通用性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Self-Consistency-for-Data-Efficient-Amortized-Bayesian-Inference"><a href="#Leveraging-Self-Consistency-for-Data-Efficient-Amortized-Bayesian-Inference" class="headerlink" title="Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference"></a>Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04395">http://arxiv.org/abs/2310.04395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marvin Schmitt, Daniel Habermann, Paul-Christian Bürkner, Ullrich Köthe, Stefan T. Radev</li>
<li>for: 提高折衔折 Bayesian inference 的效率和准确性，通过抽象 JOINT 模型 $p(\theta, y)$ 中的 universality symmetries 利用。</li>
<li>methods: 借鉴 Bayes 定理，估算 marginal likelihood 基于approximate representation 的 JOINT 模型。在完美准确情况下，marginal likelihood 固定不变，但approximation error 导致不 desirable variance 在不同参数值上。我们将这种偏差作为损失函数，加速 conditional neural density estimator 的学习动态。</li>
<li>results: 在一个简单的 Toy 问题和一个实际模型中，我们应用了我们的方法，并观察到提高了效率和准确性。<details>
<summary>Abstract</summary>
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
</details>
<details>
<summary>摘要</summary>
我们提出一种方法，用于提高权重抽象概率推理（ABI）的效率和准确性。我们利用概率联合模型 $p(\theta, y)$ 中的universal symmetry来优化。总之，我们在推理 Bayes 定理时进行反向推理，并基于approximate representation来估算 marginal likelihood。在完美approximation情况下， marginal likelihood 对所有参数值都是常数。然而，approximation error 会导致不良的偏差在不同参数值上的 marginal likelihood 估算中。我们将这种不Symmetry 表示为损失函数，以加速 conditional neural density estimator 的学习动态。我们对一个简单的 Toy problem 和一个实际模型进行应用。Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="FMM-Head-Enhancing-Autoencoder-based-ECG-anomaly-detection-with-prior-knowledge"><a href="#FMM-Head-Enhancing-Autoencoder-based-ECG-anomaly-detection-with-prior-knowledge" class="headerlink" title="FMM-Head: Enhancing Autoencoder-based ECG anomaly detection with prior knowledge"></a>FMM-Head: Enhancing Autoencoder-based ECG anomaly detection with prior knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05848">http://arxiv.org/abs/2310.05848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giacomo Verardo, Magnus Boman, Samuel Bruchfeld, Marco Chiesa, Sabine Koch, Gerald Q. Maguire Jr., Dejan Kostic</li>
<li>for: 检测电子心电图数据中异常点的检测是重要的，以提供适时 intervención  для高风险患者。</li>
<li>methods: 使用多种AutoEncoder模型（AE）来解决异常检测任务，但这些模型不考虑ECG领导的特定模式。我们则将AE的解码部分替换为基于ECG形态的重建头（namely, FMM-Head），以提高异常检测能力。</li>
<li>results: 我们的模型在AUROC指标上比现有模型高，最高提升0.31，而且具有相对较少的模型大小和可解释的提取特征。模型的处理时间也比解决优化问题来获得相同参数的时间更快，适用于实时ECG参数提取和异常检测。<details>
<summary>Abstract</summary>
Detecting anomalies in electrocardiogram data is crucial to identifying deviations from normal heartbeat patterns and providing timely intervention to at-risk patients. Various AutoEncoder models (AE) have been proposed to tackle the anomaly detection task with ML. However, these models do not consider the specific patterns of ECG leads and are unexplainable black boxes. In contrast, we replace the decoding part of the AE with a reconstruction head (namely, FMM-Head) based on prior knowledge of the ECG shape. Our model consistently achieves higher anomaly detection capabilities than state-of-the-art models, up to 0.31 increase in area under the ROC curve (AUROC), with as little as half the original model size and explainable extracted features. The processing time of our model is four orders of magnitude lower than solving an optimization problem to obtain the same parameters, thus making it suitable for real-time ECG parameters extraction and anomaly detection.
</details>
<details>
<summary>摘要</summary>
“检测电子心脏ogram（ECG）数据中的偏差是关键的，以提供对有问题的患者进行时间对称的评估和治疗。多种机器学习（ML）模型已经被提出供侦测偏差任务，但这些模型未考虑ECG领域的特定模式，而且是黑盒子，不可解释。相比之下，我们将AE模型的解码部分替换为基于ECG形状的重建头（FMM-Head），我们的模型在与国际顶尖模型进行比较时，具有更高的偏差检测能力，最高可以提高0.31倍的ROC曲线面积（AUROC），并且仅需要原始模型的一半大小，同时具有可解释的提取特征。我们的模型处理时间只需四个数据类型的几个排序，相较于解决似然最小化问题以取得相同的参数，处理时间是四个数据类型的四个排序，这使得我们的模型适合实时ECG参数提取和偏差检测。”
</details></li>
</ul>
<hr>
<h2 id="Hermes-Unlocking-Security-Analysis-of-Cellular-Network-Protocols-by-Synthesizing-Finite-State-Machines-from-Natural-Language-Specifications"><a href="#Hermes-Unlocking-Security-Analysis-of-Cellular-Network-Protocols-by-Synthesizing-Finite-State-Machines-from-Natural-Language-Specifications" class="headerlink" title="Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications"></a>Hermes: Unlocking Security Analysis of Cellular Network Protocols by Synthesizing Finite State Machines from Natural Language Specifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04381">http://arxiv.org/abs/2310.04381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/synsec-den/hermes-spec-to-fsm">https://github.com/synsec-den/hermes-spec-to-fsm</a></li>
<li>paper_authors: Abdullah Al Ishtiaq, Sarkar Snigdha Sarathi Das, Syed Md Mukit Rashid, Ali Ranjbar, Kai Tu, Tianwei Wu, Zhezheng Song, Weixuan Wang, Mujtahid Akon, Rui Zhang, Syed Rafiul Hussain</li>
<li>for: 本 paper 提供了一个终端框架，即 Hermes，用于自动从自然语言 cellular 规范中生成正式表示。</li>
<li>methods: 本 paper 使用了一个神经网络构成分析器，即 NEUTREX，处理 transition-relevant 文本，提取 transition  ком成分 (即状态、条件和动作)。它还设计了一个专门的语言，用于将这些 transition  ком成分转换为逻辑式ula by 利用依赖关系 parse tree。最后，它将这些逻辑式ula 编译为生成转换和建立正式模型为 finite state machines。</li>
<li>results: 本 paper 使用 Hermes 评估 4G NAS、5G NAS 和 5G RRC 规范，获得了81-87% 的总准确率，与现有的方法相比有所提高。对于提取的模型进行安全分析，发现了3个新的攻击和 Identified 19个先前的攻击在 4G 和 5G 规范中，以及7个偏差在商业 4G 底层。<details>
<summary>Abstract</summary>
In this paper, we present Hermes, an end-to-end framework to automatically generate formal representations from natural language cellular specifications. We first develop a neural constituency parser, NEUTREX, to process transition-relevant texts and extract transition components (i.e., states, conditions, and actions). We also design a domain-specific language to translate these transition components to logical formulas by leveraging dependency parse trees. Finally, we compile these logical formulas to generate transitions and create the formal model as finite state machines. To demonstrate the effectiveness of Hermes, we evaluate it on 4G NAS, 5G NAS, and 5G RRC specifications and obtain an overall accuracy of 81-87%, which is a substantial improvement over the state-of-the-art. Our security analysis of the extracted models uncovers 3 new vulnerabilities and identifies 19 previous attacks in 4G and 5G specifications, and 7 deviations in commercial 4G basebands.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了 Hermes，一个端到端框架，用于自动生成 формаль Representation 从自然语言 celular 规范中。我们首先开发了一个神经网络成分分析器，名为 NEUTREX，用于处理过渡相关的文本，并提取过渡组件（即状态、条件和动作）。我们还设计了域特定语言，用于将过渡组件翻译成逻辑公式，通过利用依赖树来做这个翻译。最后，我们编译这些逻辑公式，生成过渡和创建正式模型为有限状态机。为了证明 Hermes 的效果，我们对 4G NAS、5G NAS 和 5G RRC 规范进行了评估，并获得了总准确率在 81-87% 之间，这与当前状态的技术具有显著的提升。我们的安全分析中发现了 3 个新的攻击点和 19 个之前的攻击在 4G 和 5G 规范中，以及 7 个偏差在商业 4G 基带中。
</details></li>
</ul>
<hr>
<h2 id="Confronting-Reward-Model-Overoptimization-with-Constrained-RLHF"><a href="#Confronting-Reward-Model-Overoptimization-with-Constrained-RLHF" class="headerlink" title="Confronting Reward Model Overoptimization with Constrained RLHF"></a>Confronting Reward Model Overoptimization with Constrained RLHF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04373">http://arxiv.org/abs/2310.04373</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tedmoskovitz/constrainedrl4lms">https://github.com/tedmoskovitz/constrainedrl4lms</a></li>
<li>paper_authors: Ted Moskovitz, Aaditya K. Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca D. Dragan, Stephen McAleer</li>
<li>for: 本研究旨在解决复杂的语言评估问题，通过调整 composite reward models (CRMs) 的权重来避免过优化现象。</li>
<li>methods: 本研究使用 constrained reinforcement learning 方法，通过 Lagrange multipliers 学习动态权重，以避免每个 CRM 的用处提升点。</li>
<li>results: 研究发现，对于不同的 correlation  междуcomponent RMs，可以通过调整权重来避免过优化现象，并且可以通过 gradient-free optimization 方法来在单个运行中进行优化。<details>
<summary>Abstract</summary>
Large language models are typically aligned with human preferences by optimizing $\textit{reward models}$ (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to $\textit{overoptimization}$, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform, to our knowledge, the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding each RM's threshold of usefulness. Our method addresses the problem of weighting component RMs by learning dynamic weights, naturally expressed by Lagrange multipliers. As a result, each RM stays within the range at which it is an effective proxy, improving evaluation performance. Finally, we introduce an adaptive method using gradient-free optimization to identify and optimize towards these points during a single run.
</details>
<details>
<summary>摘要</summary>
大型语言模型通常与人类偏好相互align，通过优化 $\textit{奖励模型}$ (RM) 适应人类反馈。然而，人类偏好是多方面的，因此常会从多个简单的奖励模型中获得奖励，每个模型都 capture 不同的语言质量方面。这本身就是一个挑战，因为将这些组件 RM 组合时难以设置合适的权重。另外，因为任何 RM 都只是人类评价的对称 proxy，这个过程容易出现 $\textit{过乎优化}$ 现象，即当 RM 获得更高的奖励后，人类评价会变得更差。在这篇论文中，我们执行了我们知道的第一个关于过乎优化在composite RM 中的研究，显示了成分 RM 之间的联乘效应有着重要的影响。我们随后引入了一种方法来解决这个问题，使用受限的循环学习来防止代理人超过每个 RM 的有用性阈值。我们的方法可以自然地学习动态的权重，由拉格朗日积分自然地表达。因此，每个 RM 都保持在其有用性范围内，提高评估性能。最后，我们引入了一种适应方法，使用梯度自由优化来识别和优化这些点 During a single run。
</details></li>
</ul>
<hr>
<h2 id="A-Language-Agent-Approach-to-Formal-Theorem-Proving"><a href="#A-Language-Agent-Approach-to-Formal-Theorem-Proving" class="headerlink" title="A Language-Agent Approach to Formal Theorem-Proving"></a>A Language-Agent Approach to Formal Theorem-Proving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04353">http://arxiv.org/abs/2310.04353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trishullab/copra">https://github.com/trishullab/copra</a></li>
<li>paper_authors: Amitayush Thakur, Yeming Wen, Swarat Chaudhuri</li>
<li>for: 这篇论文旨在开发一种基于大语言模型（LLM）的语言代理方法，用于控制任务。</li>
<li>methods: 该方法使用一个高容量黑盒LMM（GPT-4）作为策略，并在状态备份搜索中使用外部数据库中的证明和定义。在搜索过程中，策略可以选择证明策略和从外部数据库中检索证明和定义，并将每次选择的策略执行在基础证明框架中。搜索还跟踪选择历史记录，并使用其来减少幻觉和不必要的LMM查询。</li>
<li>results: 在Lean的miniF2F标准套件和Compcert项目中的Coq任务上，COPRA比一次性调用GPT-4和已经精心适应证明数据的当前状态-of-the-art模型更快地找到正确的证明。<details>
<summary>Abstract</summary>
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.   We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-art models fine-tuned on proof data, at finding correct proofs quickly.
</details>
<details>
<summary>摘要</summary>
language agents，具有大型语言模型（LLM）可进行在上下文中学习的能力，最近被认为是控制任务的有望的方法。我们现在提出了第一个语言代理方法，用于形式证明。我们的方法，COPRA，使用一个高容量的黑盒子LLM（GPT-4）作为一个状态备用搜索的策略。在搜索过程中，策略可以选择证明策略和从外部数据库中检索证明和定义。每次选择的策略都会在下面的证明框架中执行，并将执行反馈用于下一次策略调用的建议。搜索还跟踪了历史记录中的选定信息，并使用其来减少幻想和不必要的LLM查询。 我们对miniF2F测试准则和Coq项目中的一组任务进行评估。在这些准则上，COPRA表现出了较一次GPT-4的 invoke 和状态当前模型，快速找到正确的证明。
</details></li>
</ul>
<hr>
<h2 id="Neur2RO-Neural-Two-Stage-Robust-Optimization"><a href="#Neur2RO-Neural-Two-Stage-Robust-Optimization" class="headerlink" title="Neur2RO: Neural Two-Stage Robust Optimization"></a>Neur2RO: Neural Two-Stage Robust Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04345">http://arxiv.org/abs/2310.04345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khalil-research/neur2ro">https://github.com/khalil-research/neur2ro</a></li>
<li>paper_authors: Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias B. Khalil</li>
<li>for: 这篇论文旨在提出一种高效的机器学习驱动的二stage robust优化算法（Neur2RO），用于解决具有最差情况不确定性的决策问题。</li>
<li>methods: 论文提出了一种基于神经网络的 column-and-constraint generation（CCG）算法，通过嵌入神经网络到 CCG 中来实现高质量的解决方案。</li>
<li>results: 实验表明，Neur2RO 可以快速地获得高质量的解决方案，比如在 knapsack 问题上，Neur2RO 可以在几秒钟内获得比best-known值的2%的解决方案，而且在更大和更复杂的实例上，Neur2RO 还可以获得更好的解决方案。在 capital budgeting 问题上，Neur2RO 比三种 $k$-adaptability 算法更高效，尤其是在最大实例上，解决时间减少了5到10倍。<details>
<summary>Abstract</summary>
Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Neur2RO finds solutions that are within roughly $2\%$ of the best-known values in a few seconds compared to the three hours of the state-of-the-art exact branch-and-price algorithm; for larger and more complex instances, Neur2RO finds even better solutions. For capital budgeting, Neur2RO outperforms three variants of the $k$-adaptability algorithm, particularly on the largest instances, with a 5 to 10-fold reduction in solution time. Our code and data are available at https://github.com/khalil-research/Neur2RO.
</details>
<details>
<summary>摘要</summary>
Robust优化提供了一个数学框架，用于模型和解决面临最坏情况不确定性的决策问题。这项工作关注了两阶段稳健优化（2RO）问题，其中第一阶段和第二阶段决策在不确定性实现前后分别进行。这将导致一个嵌套的最小值最大值最小值优化问题，计算上非常复杂，特别是当决策是离散的时候。我们提出了Neur2RO，一种高效的机器学习驱动的列和约束生成（CCG）实现。具体来说，我们通过一种新的神经网络架构来估算第二阶段问题的价值函数，这种神经网络架构易于优化。将我们的神经网络 embedding到 CCG 中，可以快速获得高质量的解决方案，经实验表明，在两个 2RO benchmark 中，Neur2RO 可以在几秒钟内获得比best-known值几乎2%的解决方案，而且在更大和更复杂的实例中，Neur2RO 可以获得更好的解决方案。对于资本投入问题，Neur2RO 可以在三个 $k$-adaptability 算法的基础上更好地解决问题，尤其是在最大实例中，解决时间减少了5-10倍。我们的代码和数据可以在 <https://github.com/khalil-research/Neur2RO> 中找到。
</details></li>
</ul>
<hr>
<h2 id="T-Rep-Representation-Learning-for-Time-Series-using-Time-Embeddings"><a href="#T-Rep-Representation-Learning-for-Time-Series-using-Time-Embeddings" class="headerlink" title="T-Rep: Representation Learning for Time Series using Time-Embeddings"></a>T-Rep: Representation Learning for Time Series using Time-Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04486">http://arxiv.org/abs/2310.04486</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archibald Fraikin, Adrien Bennetot, Stéphanie Allassonnière</li>
<li>for: 本研究旨在Addressing the challenges of multivariate time series data in machine learning, specifically unlabeled, high-dimensional, noisy, and missing data.</li>
<li>methods: 提出了T-Rep方法，它是一种自动编写的方法，通过学习时间序列表示来捕捉时间特征，包括趋势、周期性和分布变化。</li>
<li>results: 与现有的自动编写方法进行比较，T-Rep在下游分类、预测和异常检测任务中表现出色，在缺失数据情况下也表现更加稳定。此外，通过准确可见性实验，表明学习的表示具有可读性。<details>
<summary>Abstract</summary>
Multivariate time series present challenges to standard machine learning techniques, as they are often unlabeled, high dimensional, noisy, and contain missing data. To address this, we propose T-Rep, a self-supervised method to learn time series representations at a timestep granularity. T-Rep learns vector embeddings of time alongside its feature extractor, to extract temporal features such as trend, periodicity, or distribution shifts from the signal. These time-embeddings are leveraged in pretext tasks, to incorporate smooth and fine-grained temporal dependencies in the representations, as well as reinforce robustness to missing data. We evaluate T-Rep on downstream classification, forecasting, and anomaly detection tasks. It is compared to existing self-supervised algorithms for time series, which it outperforms in all three tasks. We test T-Rep in missing data regimes, where it proves more resilient than its counterparts. Finally, we provide latent space visualisation experiments, highlighting the interpretability of the learned representations.
</details>
<details>
<summary>摘要</summary>
多变量时间序列呈现出标准机器学习技术的挑战，因为它们通常无标签、高维、噪音和存在散失数据。为解决这一问题，我们提出T-Rep方法，这是一种自我超级vised的方法，用于在时间步长级别上学习时间序列表示。T-Rep将时间序列与其特征提取器一起学习 vector embedding，以EXTRACT时间特征，如趋势、周期性和分布变化。这些时间Embedding被用于预text任务中，以捕捉细致的时间相关性，以及在散失数据的情况下强化Robustness。我们对T-Rep进行下游分类、预测和异常检测任务的评估，并与现有的自我超级vised算法进行比较。 results show that T-Rep在所有三个任务中表现出色，并在散失数据的情况下更加稳定。最后，我们进行了隐藏空间视觉实验，以显示学习的表示的可读性。
</details></li>
</ul>
<hr>
<h2 id="Adjustable-Robust-Reinforcement-Learning-for-Online-3D-Bin-Packing"><a href="#Adjustable-Robust-Reinforcement-Learning-for-Online-3D-Bin-Packing" class="headerlink" title="Adjustable Robust Reinforcement Learning for Online 3D Bin Packing"></a>Adjustable Robust Reinforcement Learning for Online 3D Bin Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04323">http://arxiv.org/abs/2310.04323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Pan, Yize Chen, Fangzhen Lin</li>
<li>for: 解决在线三维堆包含问题（3D-BPP）的有效政策设计，因为问题的不可预测性和物理约束，带来了长期的挑战。</li>
<li>methods: 我们首先引入了一种排序基于的攻击者，以Investigate现有的DRL和冒险方法在解决3D-BPP问题中的实际Robustness。然后，我们提出了一种可调策略Robust reinforcement learning（AR2L）框架，可以efficient地调整Robustness的权重，以实现策略在平均和worst-case环境中的desired平衡。</li>
<li>results: 我们的实验表明，AR2L是一种多功能的策略，可以提高策略的Robustness，同时保持 Nominal 情况下的性能在接受 Water level。<details>
<summary>Abstract</summary>
Designing effective policies for the online 3D bin packing problem (3D-BPP) has been a long-standing challenge, primarily due to the unpredictable nature of incoming box sequences and stringent physical constraints. While current deep reinforcement learning (DRL) methods for online 3D-BPP have shown promising results in optimizing average performance over an underlying box sequence distribution, they often fail in real-world settings where some worst-case scenarios can materialize. Standard robust DRL algorithms tend to overly prioritize optimizing the worst-case performance at the expense of performance under normal problem instance distribution. To address these issues, we first introduce a permutation-based attacker to investigate the practical robustness of both DRL-based and heuristic methods proposed for solving online 3D-BPP. Then, we propose an adjustable robust reinforcement learning (AR2L) framework that allows efficient adjustment of robustness weights to achieve the desired balance of the policy's performance in average and worst-case environments. Specifically, we formulate the objective function as a weighted sum of expected and worst-case returns, and derive the lower performance bound by relating to the return under a mixture dynamics. To realize this lower bound, we adopt an iterative procedure that searches for the associated mixture dynamics and improves the corresponding policy. We integrate this procedure into two popular robust adversarial algorithms to develop the exact and approximate AR2L algorithms. Experiments demonstrate that AR2L is versatile in the sense that it improves policy robustness while maintaining an acceptable level of performance for the nominal case.
</details>
<details>
<summary>摘要</summary>
“设计有效的策略 для在线三维弹性问题（3D-BPP）已经是一个长期的挑战，主要是因为来宾盒子序列的不可预测性和严格的物理限制。现有的深度强化学习（DRL）方法可以对在线3D-BPP中的均值性表现进行优化，但是它们在实际世界中可能会失败，因为一些最差情况可能会出现。标准的Robust DRL算法往往将最差情况的表现优先考虑，导致在正常问题域中的表现不佳。为解决这些问题，我们首先引入了一个 permutation-based 攻击者，以investigate the practical robustness of both DRL-based和heuristic methods proposed for solving online 3D-BPP。然后，我们提出了一个可调robust reinforcement learning（AR2L）框架，可以实现政策的性能平衡。具体来说，我们将目标函数设计为一个权重加权的总和，并 derivethe lower performance bound by relating to the return under a mixture dynamics。为实现这个下界，我们运用了一个迭代程序，寻找相应的混合动力学和改善相应的政策。我们将这个程序整合到了两种流行的Robust adversarial algorithms中，以开发出精确和近似的AR2L算法。实验结果显示，AR2L是一个多元的策略，可以提高政策的Robustness，同时维持nominal case中的表现水准。”
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Robust-Group-level-Emotion-Recognition-via-Uncertainty-Aware-Learning"><a href="#Towards-A-Robust-Group-level-Emotion-Recognition-via-Uncertainty-Aware-Learning" class="headerlink" title="Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning"></a>Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04306">http://arxiv.org/abs/2310.04306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Zhu, Qirong Mao, Jialin Zhang, Xiaohua Huang, Wenming Zheng</li>
<li>for: 这篇论文是为了提出一种能够在不约束环境下更好地进行人群情感识别（GER）的方法。</li>
<li>methods: 该方法使用了不确定性感知（UAL）技术，通过显式地模型每个个体的不确定性，使用 Gaussian 分布中的杂态 embedding 来捕捉每个个体的可能性。在推理阶段，通过这种杂态性，生成多种情感预测。此外，还开发了一个图像增强模块，以提高模型对严重噪声的抗颤响性。</li>
<li>results: 实验结果表明，该方法在三个通用的数据库上达到了高效性和普适性。<details>
<summary>Abstract</summary>
Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty of each individual, we utilize stochastic embedding drawn from a Gaussian distribution instead of deterministic point embedding. This representation captures the probabilities of different emotions and generates diverse predictions through this stochasticity during the inference stage. Furthermore, uncertainty-sensitive scores are adaptively assigned as the fusion weights of individuals' face within each group. Moreover, we develop an image enhancement module to enhance the model's robustness against severe noise. The overall three-branch model, encompassing face, object, and scene component, is guided by a proportional-weighted fusion strategy and integrates the proposed uncertainty-aware method to produce the final group-level output. Experimental results demonstrate the effectiveness and generalization ability of our method across three widely used databases.
</details>
<details>
<summary>摘要</summary>
group-level emotion recognition (GER) 是人类行为分析中不可或缺的一部分，旨在在多人场景中识别总体的情感。然而，现有的方法均是通过结合多种情感迹象来实现，而忽略了无结构环境中的自然不确定性，如群体中的堵塞和遮挡。此外，只有群体级别的标签可用，因此在同一个群体中不一致的情感预测可能会混淆网络。在这篇论文中，我们提出了一种不确定性意识学习（UAL）方法，以提取更加稳定的表示 для GER。我们通过显式地模型每个个体的不确定性，使用 Gaussian 分布中的随机点 embedding，而不是固定点 embedding。这种表示捕捉了不同情感的概率，并在推理阶段通过随机性产生多种预测。此外，我们还开发了一个图像增强模块，以提高模型对严重噪声的抗锋性。总体来说，我们的三支分支模型，包括人脸、物体和场景组件，采用比例权重混合策略，并将我们提出的不确定性意识学习方法 integrate 到生成最终群体级别输出。实验结果表明我们的方法在三个通用的数据库上表现出色，并且具有普适性和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Coding-by-Design-GPT-4-empowers-Agile-Model-Driven-Development"><a href="#Coding-by-Design-GPT-4-empowers-Agile-Model-Driven-Development" class="headerlink" title="Coding by Design: GPT-4 empowers Agile Model Driven Development"></a>Coding by Design: GPT-4 empowers Agile Model Driven Development</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04304">http://arxiv.org/abs/2310.04304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed R. Sadik, Sebastian Brulin, Markus Olhofer</li>
<li>for: 这个研究的目的是提出一种基于 OpenAI GPT-4 的 Agile Model-Driven Development (MDD) 方法，以便在使用自然语言生成代码时解决模型中的歧义性问题。</li>
<li>methods: 这种方法包括在首层和第二层使用 Unified Model Language (UML) 图文 Representation，然后在第三层使用 GPT-4 自动生成代码。在第二层，我们引入了两组约束来减少模型的歧义性，包括 Object Constraints Language (OCL) 和 FIPA  ontology。</li>
<li>results: 我们的研究表明，使用这种方法可以生成符合预期 UML 序列图的行为，并且对代码结构进行了比较。结果表明，使用ontology-constrained模型可以生成更复杂的代码，但这种代码仍然可以被轻松地测试和维护。<details>
<summary>Abstract</summary>
Generating code from a natural language using Large Language Models (LLMs) such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it's evident that this approach has its own limitations. The inherent ambiguity of natural language presents challenges for complex software designs. Accordingly, our research offers an Agile Model-Driven Development (MDD) approach that enhances code auto-generation using OpenAI's GPT-4. Our work emphasizes "Agility" as a significant contribution to the current MDD method, particularly when the model undergoes changes or needs deployment in a different programming language. Thus, we present a case-study showcasing a multi-agent simulation system of an Unmanned Vehicle Fleet. In the first and second layer of our approach, we constructed a textual representation of the case-study using Unified Model Language (UML) diagrams. In the next layer, we introduced two sets of constraints that minimize model ambiguity. Object Constraints Language (OCL) is applied to fine-tune the code constructions details, while FIPA ontology is used to shape communication semantics and protocols. Ultimately, leveraging GPT-4, our last layer auto-generates code in both Java and Python. The Java code is deployed within the JADE framework, while the Python code is deployed in PADE framework. Concluding our research, we engaged in a comprehensive evaluation of the generated code. From a behavioural standpoint, the auto-generated code aligned perfectly with the expected UML sequence diagram. Structurally, we compared the complexity of code derived from UML diagrams constrained solely by OCL to that influenced by both OCL and FIPA-ontology. Results indicate that ontology-constrained model produce inherently more intricate code, but it remains manageable and low-risk for further testing and maintenance.
</details>
<details>
<summary>摘要</summary>
使用大型自然语言模型（LLM）如ChatGPT生成代码看起来是一项创新的技术，但是随着更广泛的使用，这种方法的限制也变得更加明显。自然语言的内在抽象性会导致复杂的软件设计困难。因此，我们的研究提出了一种基于Model-Driven Development（MDD）的Agile模型驱动方法，通过OpenAI的GPT-4提高代码自动生成。我们的工作强调“适应”作为我们的贡献，特别是当模型进行变更或需要在不同编程语言中部署时。因此，我们提供了一个多代理 simulations系统的无人车队例子。在我们的方法中，在第一层和第二层，我们使用Unified Model Language（UML）图文描述了这个例子。在下一层，我们引入了两组约束，以降低模型的抽象性。Object Constraints Language（OCL）用于细化代码构造细节，而FIPAontology用于形成通信协议和 semantics。最后，通过GPT-4，我们的最后一层自动生成代码在Java和Python两种编程语言中。Java代码在JADE框架中部署，而Python代码在PADE框架中部署。在结束我们的研究后，我们进行了全面的代码生成评估。从行为上来看，自动生成的代码与预期的UML序列图完全匹配。从结构上来看，我们比较了由UML图文描述的代码和只由OCL约束的代码的复杂性。结果表明，基于ontology的模型生成的代码具有更高的复杂性，但是它仍然可以被成功地测试和维护。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Representations-for-Intervention-Extrapolation"><a href="#Identifying-Representations-for-Intervention-Extrapolation" class="headerlink" title="Identifying Representations for Intervention Extrapolation"></a>Identifying Representations for Intervention Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04295">http://arxiv.org/abs/2310.04295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Ravikumar, Niklas Pfister, Jonas Peters</li>
<li>for: 本研究旨在提高当前表征学学习方法的泛化性和稳定性。</li>
<li>methods: 本研究使用了可识别表征学学习方法，并结合了权重变换约束来保证表征的线性不变性。</li>
<li>results: 研究表明，通过使用可识别表征学学习方法，可以在不见到干预的情况下预测干预的效果，并且可以减少干预的影响。<details>
<summary>Abstract</summary>
The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. Despite recent progress in questions of identifiability, more theoretical results demonstrating concrete advantages of these methods for downstream tasks are needed. In this paper, we consider the task of intervention extrapolation: predicting how interventions affect an outcome, even when those interventions are not observed at training time, and show that identifiable representations can provide an effective solution to this task even if the interventions affect the outcome non-linearly. Our setup includes an outcome Y, observed features X, which are generated as a non-linear transformation of latent features Z, and exogenous action variables A, which influence Z. The objective of intervention extrapolation is to predict how interventions on A that lie outside the training support of A affect Y. Here, extrapolation becomes possible if the effect of A on Z is linear and the residual when regressing Z on A has full support. As Z is latent, we combine the task of intervention extrapolation with identifiable representation learning, which we call Rep4Ex: we aim to map the observed features X into a subspace that allows for non-linear extrapolation in A. We show using Wiener's Tauberian theorem that the hidden representation is identifiable up to an affine transformation in Z-space, which is sufficient for intervention extrapolation. The identifiability is characterized by a novel constraint describing the linearity assumption of A on Z. Based on this insight, we propose a method that enforces the linear invariance constraint and can be combined with any type of autoencoder. We validate our theoretical findings through synthetic experiments and show that our approach succeeds in predicting the effects of unseen interventions.
</details>
<details>
<summary>摘要</summary>
《表观性和 causal 表示学习的前提是提高当前表示学习 paradigm 的普遍性或Robustness。尽管最近的进展在表示可识别性方面，但需要更多的理论成果，证明这些方法在下游任务中的优点。在这篇论文中，我们考虑了 intervención extrapolation 任务：预测干扰的影响，即在训练时没有观察到的干扰，并显示了可识别表示可以提供非线性干扰预测的有效解决方案。我们的设置包括结果 Y，观察特征 X，它们是非线性变换的隐藏特征 Z 的生成，以及外部动作变量 A，它们影响 Z。干扰 extrapolation 的目标是预测 A 在训练支持外的干扰对 Y 的影响。在 Z 是隐藏的情况下，推断成为可能，如果 A 对 Z 的效果是线性的，并且 Z 中的剩余差异拥有完整的支持。我们将 Representation 4 Ex（Rep4Ex）任务与可识别表示学习结合在一起，即将观察特征 X 映射到一个允许非线性推断 A 的子空间。根据维ener 的 Tauberian 定理，隐藏表示是可识别的，即在 Z 空间中可以确定一个 afine 变换。这种可识别性是基于一个新的约束，描述了 A 对 Z 的线性假设。根据这一点，我们提出了一种方法，可以与任何类型的 autoencoder 结合使用，并且可以满足这种约束。我们通过synthetic实验验证了我们的理论发现，并证明了我们的方法可以预测未见干扰的效果。
</details></li>
</ul>
<hr>
<h2 id="Searching-for-Optimal-Runtime-Assurance-via-Reachability-and-Reinforcement-Learning"><a href="#Searching-for-Optimal-Runtime-Assurance-via-Reachability-and-Reinforcement-Learning" class="headerlink" title="Searching for Optimal Runtime Assurance via Reachability and Reinforcement Learning"></a>Searching for Optimal Runtime Assurance via Reachability and Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04288">http://arxiv.org/abs/2310.04288</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristina Miller, Christopher K. Zeitler, William Shen, Kerianne Hobbs, Sayan Mitra, John Schierman, Mahesh Viswanathan</li>
<li>for: 本研究旨在开发一种可靠的runtime assurance system (RTA)，以确保安全性while exercising an untrusted或实验性控制器。</li>
<li>methods: 本研究使用 reward shaping和 reinforcement learning来解决RTA的optimal设计问题，可以保证安全性并利用机器学习技术来提高可扩展性。</li>
<li>results: 对于一些复杂的安全需求的3D空间飞机模型，我们的方法可以保证安全性并提高实验控制器的使用率，比已有方法更高。<details>
<summary>Abstract</summary>
A runtime assurance system (RTA) for a given plant enables the exercise of an untrusted or experimental controller while assuring safety with a backup (or safety) controller. The relevant computational design problem is to create a logic that assures safety by switching to the safety controller as needed, while maximizing some performance criteria, such as the utilization of the untrusted controller. Existing RTA design strategies are well-known to be overly conservative and, in principle, can lead to safety violations. In this paper, we formulate the optimal RTA design problem and present a new approach for solving it. Our approach relies on reward shaping and reinforcement learning. It can guarantee safety and leverage machine learning technologies for scalability. We have implemented this algorithm and present experimental results comparing our approach with state-of-the-art reachability and simulation-based RTA approaches in a number of scenarios using aircraft models in 3D space with complex safety requirements. Our approach can guarantee safety while increasing utilization of the experimental controller over existing approaches.
</details>
<details>
<summary>摘要</summary>
traducción al chino simplificado:runtime assurance system (RTA) for a given plant enables the exercise of an untrusted or experimental controller while assuring safety with a backup (or safety) controller. The relevant computational design problem is to create a logic that assures safety by switching to the safety controller as needed, while maximizing some performance criteria, such as the utilization of the untrusted controller. Existing RTA design strategies are well-known to be overly conservative and, in principle, can lead to safety violations. In this paper, we formulate the optimal RTA design problem and present a new approach for solving it. Our approach relies on reward shaping and reinforcement learning. It can guarantee safety and leverage machine learning technologies for scalability. We have implemented this algorithm and present experimental results comparing our approach with state-of-the-art reachability and simulation-based RTA approaches in a number of scenarios using aircraft models in 3D space with complex safety requirements. Our approach can guarantee safety while increasing utilization of the experimental controller over existing approaches.Notes:* "runtime assurance system" (RTA) is translated as "runtime assurance system" (RTA)* "untrusted or experimental controller" is translated as "untrusted or experimental controller"* "backup (or safety) controller" is translated as "backup (or safety) controller"* "computational design problem" is translated as "computational design problem"* "reward shaping and reinforcement learning" is translated as "奖励形态和返回学习"* "state-of-the-art reachability and simulation-based RTA approaches" is translated as "现有的可达性和模拟基于RTA方法"* "experimental results" is translated as "实验结果"* "complex safety requirements" is translated as "复杂的安全要求"
</details></li>
</ul>
<hr>
<h2 id="Assessing-Robustness-via-Score-Based-Adversarial-Image-Generation"><a href="#Assessing-Robustness-via-Score-Based-Adversarial-Image-Generation" class="headerlink" title="Assessing Robustness via Score-Based Adversarial Image Generation"></a>Assessing Robustness via Score-Based Adversarial Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04285">http://arxiv.org/abs/2310.04285</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Kollovieh, Lukas Gosch, Yan Scholten, Marten Lienen, Stephan Günnemann</li>
<li>for: 本研究旨在探讨针对抗骚扰攻击的限制， traditional的方法只能在 $\ell_p$ 概率范围内进行攻击，但是这些限制无法捕捉所有Semantic-preserving的骚扰。</li>
<li>methods: 本文提出了 Score-Based Adversarial Generation（ScoreAG）框架，利用了分布式生成模型的进步，可以生成不受 $\ell_p$ 概率范围限制的骚扰例，称为不限制骚扰例。</li>
<li>results: 对多个benchmark进行了广泛的实验，得到的结果显示，ScoreAG具有与现状最佳的攻击和防御性能，而且可以纠正攻击者所搅入的杂谏。<details>
<summary>Abstract</summary>
Most adversarial attacks and defenses focus on perturbations within small $\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond $\ell_p$-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art attacks and defenses across multiple benchmarks. This work highlights the importance of investigating adversarial examples bounded by semantics rather than $\ell_p$-norm constraints. ScoreAG represents an important step towards more encompassing robustness assessments.
</details>
<details>
<summary>摘要</summary>
大多数敌对攻击和防御都集中在小$\ell_p$-norm的偏差内。然而，$\ell_p$ 威胁模型无法捕捉所有具有 semantic-preserving 偏差的攻击，因此robustness评估的范围有限。在这项工作中，我们介绍了Score-Based Adversarial Generation（ScoreAG），一种新的框架，利用了得到的分数基本生成模型来生成 beyond $\ell_p$-norm 的攻击例子，即所谓的无限攻击例子，超越它们的局限性。与传统方法不同，ScoreAG保留了图像的核心含义，并生成了真实的攻击例子，可以是对现有图像进行变换还是从头开始生成全新的图像。我们进一步利用了ScoreAG的生成能力来纯化图像，实际上提高了分类器的 robustness。我们的广泛的实验证明了 ScoreAG 与当前状态的攻击和防御技术在多个benchmark上具有相同的性能。这项工作强调了对于 adversarial examples 的Semantic bounded 而不是 $\ell_p$-norm 的限制是更加重要的。ScoreAG 代表了更全面的robustness评估的一个重要步阶。
</details></li>
</ul>
<hr>
<h2 id="From-task-structures-to-world-models-What-do-LLMs-know"><a href="#From-task-structures-to-world-models-What-do-LLMs-know" class="headerlink" title="From task structures to world models: What do LLMs know?"></a>From task structures to world models: What do LLMs know?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04276">http://arxiv.org/abs/2310.04276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilker Yildirim, L. A. Paul</li>
<li>for: 本研究探讨了大语言模型具备知识的方面，挑战我们对智能和知识的假设。</li>
<li>methods: 本研究使用了大语言模型，探讨了这些模型具备的能力是否可以视为知识。</li>
<li>results: 研究发现，大语言模型具备一种称为”工具知识”的知识，这种知识定义为一组能力。然而，这种知识与人类agens所表现的”世界知识”之间存在关系，需要进一步探讨。<details>
<summary>Abstract</summary>
In what sense does a large language model have knowledge? The answer to this question extends beyond the capabilities of a particular AI system, and challenges our assumptions about the nature of knowledge and intelligence. We answer by granting LLMs "instrumental knowledge"; knowledge defined by a certain set of abilities. We then ask how such knowledge is related to the more ordinary, "worldly" knowledge exhibited by human agents, and explore this in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. We discuss ways LLMs could recover degrees of worldly knowledge, and suggest such recovery will be governed by an implicit, resource-rational tradeoff between world models and task demands.
</details>
<details>
<summary>摘要</summary>
哪种意义上的知识具有大语言模型知识？答案超出了某个AI系统的能力，挑战我们对知识和智能的假设。我们回答是通过授予LLMs“工具知识”，即知识定义为某些能力。然后我们问这种知识与人类代理的“日常”知识之间的关系，并通过考虑这种知识是否可以通过认知科学中的结构化世界模型来捕捉。我们讨论LLMs如何恢复世界知识的方式，并建议这种恢复受到了隐式的资源可用性评估和任务需求的交互。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Evaluation-of-Large-Language-Models-on-Benchmark-Biomedical-Text-Processing-Tasks"><a href="#A-Comprehensive-Evaluation-of-Large-Language-Models-on-Benchmark-Biomedical-Text-Processing-Tasks" class="headerlink" title="A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks"></a>A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04270">http://arxiv.org/abs/2310.04270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang</li>
<li>for: 本研究旨在评估大型自然语言模型（LLM）在生物医学领域中的表现。</li>
<li>methods: 我们使用4种Popular LLM在6种多样化的生物医学任务中进行了广泛的评估，并在26个数据集上进行了对比。</li>
<li>results: 我们发现，在生物医学数据集中，具有较小训练集的零 shot LLM可以超越当前状态的拟合biomedical模型。此外，我们发现不同的LLM在不同任务中的表现可能会异常，而且 их表现仍然远低于经过大训练集的精心调整的生物医学模型。然而，我们的发现表明LLM在缺乏大量标注数据的任务中可能是一种有价值的工具。<details>
<summary>Abstract</summary>
Recently, Large Language Models (LLM) have demonstrated impressive capability to solve a wide range of tasks. However, despite their success across various tasks, no prior work has investigated their capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of LLMs on benchmark biomedical tasks. For this purpose, we conduct a comprehensive evaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets. To the best of our knowledge, this is the first work that conducts an extensive evaluation and comparison of various LLMs in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot LLMs even outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs quite specialized even in the biomedical domain. We also find that not a single LLM can outperform other LLMs in all tasks, with the performance of different LLMs may vary depending on the task. While their performance is still quite poor in comparison to the biomedical models that were fine-tuned on large training sets, our findings demonstrate that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.
</details>
<details>
<summary>摘要</summary>
Interestingly, we find that in biomedical datasets with smaller training sets, zero-shot LLMs outperform the current state-of-the-art fine-tuned biomedical models. This suggests that pretraining on large text corpora makes LLMs specialized in the biomedical domain. We also find that no single LLM can outperform other LLMs in all tasks, and the performance of different LLMs varies depending on the task. While their performance is still poor compared to fine-tuned biomedical models, our findings show that LLMs have the potential to be a valuable tool for various biomedical tasks that lack large annotated data.
</details></li>
</ul>
<hr>
<h2 id="DRIFT-Deep-Reinforcement-Learning-for-Intelligent-Floating-Platforms-Trajectories"><a href="#DRIFT-Deep-Reinforcement-Learning-for-Intelligent-Floating-Platforms-Trajectories" class="headerlink" title="DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories"></a>DRIFT: Deep Reinforcement Learning for Intelligent Floating Platforms Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04266">http://arxiv.org/abs/2310.04266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elharirymatteo/rans">https://github.com/elharirymatteo/rans</a></li>
<li>paper_authors: Matteo El-Hariry, Antoine Richard, Vivek Muralidharan, Baris Can Yalcin, Matthieu Geist, Miguel Olivares-Mendez</li>
<li>For: This paper introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments.* Methods: The paper uses state-of-the-art deep reinforcement learning techniques to train policies capable of precise maneuvers amid dynamic and unpredictable conditions.* Results: The paper achieves robustness, adaptability, and good transferability from simulation to reality, and provides a comprehensive platform for researchers with open-access on GitHub.<details>
<summary>Abstract</summary>
This investigation introduces a novel deep reinforcement learning-based suite to control floating platforms in both simulated and real-world environments. Floating platforms serve as versatile test-beds to emulate microgravity environments on Earth. Our approach addresses the system and environmental uncertainties in controlling such platforms by training policies capable of precise maneuvers amid dynamic and unpredictable conditions. Leveraging state-of-the-art deep reinforcement learning techniques, our suite achieves robustness, adaptability, and good transferability from simulation to reality. Our Deep Reinforcement Learning (DRL) framework provides advantages such as fast training times, large-scale testing capabilities, rich visualization options, and ROS bindings for integration with real-world robotic systems. Beyond policy development, our suite provides a comprehensive platform for researchers, offering open-access at https://github.com/elharirymatteo/RANS/tree/ICRA24.
</details>
<details>
<summary>摘要</summary>
这个研究引入了一套基于深度优化学习的控制浮 плаform  suite，可以在模拟和实际环境中控制浮 плаform。浮 плаform 作为地球上模拟微重力环境的软件平台，我们的方法可以在不稳定和随机的环境中做精准的操作。通过使用现代深度优化学习技术，我们的框架可以实现对系统和环境不确定性的适应，以及从模拟到实际的好转移性。我们的深度优化学习（DRL）框架具有快速训练时间、大规模测试能力、丰富的视觉化选项以及ROS绑定，以便与实际机器人系统集成。此外，我们的框架还提供了许多研究平台，包括开源的Github上的https://github.com/elharirymatteo/RANS/tree/ICRA24。
</details></li>
</ul>
<hr>
<h2 id="Ada-Instruct-Adapting-Instruction-Generators-for-Complex-Reasoning"><a href="#Ada-Instruct-Adapting-Instruction-Generators-for-Complex-Reasoning" class="headerlink" title="Ada-Instruct: Adapting Instruction Generators for Complex Reasoning"></a>Ada-Instruct: Adapting Instruction Generators for Complex Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04484">http://arxiv.org/abs/2310.04484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangitu/ada-instruct">https://github.com/wangitu/ada-instruct</a></li>
<li>paper_authors: Wanyun Cui, Qianle Wang</li>
<li>for: 提高大语言模型（LLM）下推荐练习任务的效果，以提高模型的推荐能力。</li>
<li>methods: 利用开源大语言模型进行精度调整，以生成较长的复杂指令，以解决现有方法无法生成长度超过100的指令问题。</li>
<li>results: 对多种应用（代码完成、数学逻辑、常识理解）进行了实验 Validation，并证明了 Ada-Instruct 的优越性，比其基础模型、自我指导方法和当前状态艺术模型更好。<details>
<summary>Abstract</summary>
Generating diverse and sophisticated instructions for downstream tasks by Large Language Models (LLMs) is pivotal for advancing the effect. Current approaches leverage closed-source LLMs, employing in-context prompting for instruction generation. However, in this paper, we found that in-context prompting cannot generate complex instructions with length $\ge 100$ for tasks like code completion.   To solve this problem, we introduce Ada-Instruct, an adaptive instruction generator developed by fine-tuning open-source LLMs. Our pivotal finding illustrates that fine-tuning open-source LLMs with a mere ten samples generates long instructions that maintain distributional consistency for complex reasoning tasks. We empirically validated Ada-Instruct's efficacy across different applications, including code completion, mathematical reasoning, and commonsense reasoning. The results underscore Ada-Instruct's superiority, evidencing its improvements over its base models, current self-instruct methods, and other state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现有 Approaches 使用关闭源 LLMs 进行下游任务的指令生成。然而，这篇论文发现，在 Context 提示下，不能生成长度超过 100 的复杂指令。为解决这个问题，我们介绍了 Ada-Instruct，一种基于开源 LLMs 的适应指令生成器。我们的重要发现是，只需要 Fine-Tuning 开源 LLMs 的 Ten 个样本，就可以生成长指令，保持分布性一致性。我们对 Ada-Instruct 的可行性进行了不同应用的实验 validate，包括代码完成、数学逻辑和常识逻辑。结果表明 Ada-Instruct 的优势，比 Base 模型、现有自我指令方法和其他状态艺术模型都更好。
</details></li>
</ul>
<hr>
<h2 id="Improving-Reinforcement-Learning-Efficiency-with-Auxiliary-Tasks-in-Non-Visual-Environments-A-Comparison"><a href="#Improving-Reinforcement-Learning-Efficiency-with-Auxiliary-Tasks-in-Non-Visual-Environments-A-Comparison" class="headerlink" title="Improving Reinforcement Learning Efficiency with Auxiliary Tasks in Non-Visual Environments: A Comparison"></a>Improving Reinforcement Learning Efficiency with Auxiliary Tasks in Non-Visual Environments: A Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04241">http://arxiv.org/abs/2310.04241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Lange, Noah Krystiniak, Raphael C. Engelhardt, Wolfgang Konen, Laurenz Wiskott</li>
<li>for: 提高实际RL环境中效率和可靠性的假设observation representation学习方法。</li>
<li>methods: 比较常见的 auxillary task 基于，与之前没有 decoupled representation learning 方法进行比较。</li>
<li>results: 在简单的摆铃到复杂的模拟 robotics 任务中，表示学习环境动力学是更有利于预测奖励的。这些发现可以指导未来的假设observation representation学习方法的开发，并推动RL解决方案在实际场景中的应用。<details>
<summary>Abstract</summary>
Real-world reinforcement learning (RL) environments, whether in robotics or industrial settings, often involve non-visual observations and require not only efficient but also reliable and thus interpretable and flexible RL approaches. To improve efficiency, agents that perform state representation learning with auxiliary tasks have been widely studied in visual observation contexts. However, for real-world problems, dedicated representation learning modules that are decoupled from RL agents are more suited to meet requirements. This study compares common auxiliary tasks based on, to the best of our knowledge, the only decoupled representation learning method for low-dimensional non-visual observations. We evaluate potential improvements in sample efficiency and returns for environments ranging from a simple pendulum to a complex simulated robotics task. Our findings show that representation learning with auxiliary tasks only provides performance gains in sufficiently complex environments and that learning environment dynamics is preferable to predicting rewards. These insights can inform future development of interpretable representation learning approaches for non-visual observations and advance the use of RL solutions in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
现实世界中的强化学习（RL）环境，无论是机器人或工业场景，通常会包含非视觉观察和需要高效、可靠、可解释和灵活的RL方法。为了提高效率，在视觉观察上进行状态表示学习的代理人已经广泛研究。然而，在实际问题中，专门为状态表示学习设计的模块更适合满足需求。本研究比较了常见的辅助任务，基于我们所知道的唯一分离状态学习方法，对低维非视觉观察进行评估。我们发现，只有在 suficiently complex 环境下，代理人可以从状态学习中获得性能提升。此外，我们发现，学习环境动态是更好地预测奖励，而不是预测奖励本身。这些发现可以指导未来的解释状态学习方法的发展，以及RL解决方案在实际场景中的应用。
</details></li>
</ul>
<hr>
<h2 id="The-WayHome-Long-term-Motion-Prediction-on-Dynamically-Scaled"><a href="#The-WayHome-Long-term-Motion-Prediction-on-Dynamically-Scaled" class="headerlink" title="The WayHome: Long-term Motion Prediction on Dynamically Scaled"></a>The WayHome: Long-term Motion Prediction on Dynamically Scaled</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04232">http://arxiv.org/abs/2310.04232</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kay Scheerer, Thomas Michalke, Juergen Mathes</li>
<li>for: This paper is written for the purpose of developing a novel motion forecasting approach for autonomous vehicles, specifically to accurately predict the motion of other objects in the surrounding environment.</li>
<li>methods: The paper uses a neural network-based model to predict multiple heatmaps for every traffic participant in the vicinity of the autonomous vehicle, with one heatmap per timestep. The heatmaps are then used as input to a novel sampling algorithm that extracts coordinates corresponding to the most likely future positions.</li>
<li>results: The approach improves state-of-the-art miss rate performance for the function-relevant prediction interval of 3 seconds, while being competitive in longer prediction intervals (up to eight seconds). The evaluation is done on the public 2022 Waymo motion challenge.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了开发一种新的自动驾驶车动力预测方法，特别是准确预测周围环境中其他对象的运动。</li>
<li>methods: 该论文使用神经网络模型预测每个交通参与者的多个热图，每个热图一个时间步长。热图然后用作输入，并使用一种新的采样算法提取最有可能性的未来位置坐标。</li>
<li>results: 该方法在3秒预测函数重要预测间隔中提高了状态艺术预测率，同时在更长的预测间隔（Up to 8秒）中保持竞争力。评估基于2022年 Waymo 动力挑战。<details>
<summary>Abstract</summary>
One of the key challenges for autonomous vehicles is the ability to accurately predict the motion of other objects in the surrounding environment, such as pedestrians or other vehicles. In this contribution, a novel motion forecasting approach for autonomous vehicles is developed, inspired by the work of Gilles et al. [1]. We predict multiple heatmaps with a neuralnetwork-based model for every traffic participant in the vicinity of the autonomous vehicle; with one heatmap per timestep. The heatmaps are used as input to a novel sampling algorithm that extracts coordinates corresponding to the most likely future positions. We experiment with different encoders and decoders, as well as a comparison of two loss functions. Additionally, a new grid-scaling technique is introduced, showing further improved performance. Overall, our approach improves stateof-the-art miss rate performance for the function-relevant prediction interval of 3 seconds while being competitive in longer prediction intervals (up to eight seconds). The evaluation is done on the public 2022 Waymo motion challenge.
</details>
<details>
<summary>摘要</summary>
一个关键挑战 для自动驾驶车是正确预测周围环境中其他对象的运动，如行人或其他车辆。在这篇论文中，我们开发了一种新的运动预测方法， Drawing inspiration from the work of Gilles et al. [1]。我们预测每个交通参与者的vicinity中每帧的多个热图，使用神经网络模型，并使用一种新的采样算法来提取最有可能性的未来位置坐标。我们对不同的编码器和解码器进行了比较，以及两种损失函数的比较。此外，我们还引入了一种新的网格缩放技术，以提高性能。总的来说，我们的方法在3秒预测时间内的状态-相关性表现得更好，并且在更长的预测时间（Up to 8秒）中保持竞争力。我们的评估基于2022年 Waymo 动作挑战的公共数据集。
</details></li>
</ul>
<hr>
<h2 id="A-Fixed-Parameter-Tractable-Algorithm-for-Counting-Markov-Equivalence-Classes-with-the-same-Skeleton"><a href="#A-Fixed-Parameter-Tractable-Algorithm-for-Counting-Markov-Equivalence-Classes-with-the-same-Skeleton" class="headerlink" title="A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton"></a>A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04218">http://arxiv.org/abs/2310.04218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vidya Sagar Sharma</li>
<li>for: 本文是针对 conditional dependencies 的 Bayesian networks （也称为 causal DAGs）的一种工具。</li>
<li>methods: 本文使用的方法包括 combinaatorial characterizations 和 fixed parameter tractable algorithm。</li>
<li>results: 本文取得了一种可以在 polynomial time 内解决 Markov equivalence classes 的问题的方法，该方法基于 treewidth 和 maximum degree 的参数。<details>
<summary>Abstract</summary>
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).   These combinatorial characterizations also suggest several natural algorithmic questions. One of these is: given an undirected graph $G$ as input, how many distinct Markov equivalence classes have the skeleton $G$? Much work has been devoted in the last few years to this and other closely related problems. However, to the best of our knowledge, a polynomial time algorithm for the problem remains unknown.   In this paper, we make progress towards this goal by giving a fixed parameter tractable algorithm for the above problem, with the parameters being the treewidth and the maximum degree of the input graph $G$. The main technical ingredient in our work is a construction we refer to as shadow, which lets us create a "local description'' of long-range constraints imposed by the combinatorial characterizations of MECs.
</details>
<details>
<summary>摘要</summary>
causal DAGs (也称为 bayesian networks) 是一种流行的工具，用于编码 conditional dependencies between random variables。在 causal DAG 中，random variables 被视为Vertices在 DAGC 中，并且假设每个 random variable 独立于其父节点 conditioned on its parents。然而，可能存在两个不同的 causal DAG 在同一组 random variables 上，仅仅编码出同样的 conditional dependencies。这些 causal DAG 被称为 Markov 等价（MEC）。在过去几十年中，美丽的 combinatorial  caracterizations 被发展出来，并知道，在特定的情况下，所有 DAG 在同一个 MEC 中必须有同样的 skeleton （基本的无向图）和 v-structures （由 $a\rightarrow b \leftarrow c$ 组成的嵌入式子图）。这些 combinatorial caracterizations 还提出了许多自然的 algorithmic 问题。其中一个是：给一个无向图 $G$ 作为输入，在 $G$ 中有多少个不同的 Markov 等价类？在过去几年中，许多工作已经被投入到这个和相关的问题上。然而，到目前为止，一个 polynomial time 算法 для这个问题仍然未知。在本文中，我们在这个问题上做出了进展，提供了一个 fixed parameter tractable 算法，其中的参数是 treewidth 和最大度数。我们的主要技术成分是一种我们称为 "shadow" 的建构，它允许我们创建 "local description" 来描述 long-range 约束，它们由 MEC 的 combinatorial caracterizations 强制实施。
</details></li>
</ul>
<hr>
<h2 id="Keyword-Augmented-Retrieval-Novel-framework-for-Information-Retrieval-integrated-with-speech-interface"><a href="#Keyword-Augmented-Retrieval-Novel-framework-for-Information-Retrieval-integrated-with-speech-interface" class="headerlink" title="Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface"></a>Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04205">http://arxiv.org/abs/2310.04205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amaze18/speeKAR">https://github.com/amaze18/speeKAR</a></li>
<li>paper_authors: Anupam Purwar, Rahul Sundar</li>
<li>For: The paper aims to improve the efficiency and cost-effectiveness of language model-based knowledge retrieval systems, particularly for speech-based interfaces.* Methods: The authors propose a keyword-based search framework that uses a smaller language model to generate keywords and compare them with the query, reducing the time and cost of context identification. They also use a larger language model to provide answers based on a prompt tailored for Q&amp;A.* Results: The authors demonstrate that the use of keywords in context identification reduces the overall inference time and cost of information retrieval, making it more feasible to integrate speech-based interfaces with language model-based systems.<details>
<summary>Abstract</summary>
Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation. This becomes accentuated when one wants to integrate a speech interface on top of a text based knowledge retrieval system. Besides, for commercial search and chat-bot applications, complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly. In the present study, the authors have addressed the aforementioned problem by first developing a keyword based search framework which augments discovery of the context from the document to be provided to the LLM. The keywords in turn are generated by a relatively smaller LLM and cached for comparison with keywords generated by the same smaller LLM against the query raised. This significantly reduces time and cost to find the context within documents. Once the context is set, a larger LLM uses that to provide answers based on a prompt tailored for Q\&A. This research work demonstrates that use of keywords in context identification reduces the overall inference time and cost of information retrieval. Given this reduction in inference time and cost with the keyword augmented retrieval framework, a speech based interface for user input and response readout was integrated. This allowed a seamless interaction with the language model.
</details>
<details>
<summary>摘要</summary>
Retrieving answers quickly and at a low cost without relying on hallucinations from a combination of structured and unstructured data using language models is a major challenge. This is what prevents the use of language models in knowledge retrieval automation. This becomes especially pronounced when one wants to integrate a speech interface on top of a text-based knowledge retrieval system. Moreover, relying solely on commercial large language models (LLMs) like GPT 3.5 for commercial search and chatbot applications can be very costly. In this study, the authors addressed this problem by first developing a keyword-based search framework that enhances the discovery of context from the document to be provided to the LLM. The keywords are generated by a smaller LLM and cached for comparison with keywords generated by the same smaller LLM against the query raised. This significantly reduces the time and cost of finding the context within documents. Once the context is established, a larger LLM uses that to provide answers based on a prompt tailored for Q&A. This research demonstrates that using keywords in context identification reduces the overall inference time and cost of information retrieval. With this reduction in inference time and cost, a speech-based interface for user input and response readout was integrated, allowing for seamless interaction with the language model.
</details></li>
</ul>
<hr>
<h2 id="A-Bi-objective-Perspective-on-Controllable-Language-Models-Reward-Dropout-Improves-Off-policy-Control-Performance"><a href="#A-Bi-objective-Perspective-on-Controllable-Language-Models-Reward-Dropout-Improves-Off-policy-Control-Performance" class="headerlink" title="A Bi-objective Perspective on Controllable Language Models: Reward Dropout Improves Off-policy Control Performance"></a>A Bi-objective Perspective on Controllable Language Models: Reward Dropout Improves Off-policy Control Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04483">http://arxiv.org/abs/2310.04483</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonymous-user01/controllability-of-lm-anonymous">https://github.com/anonymous-user01/controllability-of-lm-anonymous</a></li>
<li>paper_authors: Changhun Lee, Chiehyeon Lim</li>
<li>for: 这 paper 是研究 CLM (可控语言模型) 的理论方面，特别是通过 би对象函数优化来同时提高奖励和概率目标。</li>
<li>methods: 这 paper 使用了 bi-objective 优化方法，包括 reward upper bound 和 Pareto improvement&#x2F;optimality conditions。</li>
<li>results: 研究结果表明，Reward Dropout 方法可以 guarantees policy improvement based on Pareto improvement condition，并且在五个 CLM  benchmark 数据集上进行了实验，发现 Reward Dropout 可以显著提高 CLM 的性能。<details>
<summary>Abstract</summary>
We study the theoretical aspects of CLMs (Controllable Language Models) from a bi-objective optimization perspective. Specifically, we consider the CLMs as an off-policy RL problem that requires simultaneously maximizing the reward and likelihood objectives. Our main contribution consists of three parts. First, we establish the theoretical foundations of CLM by presenting reward upper bound and Pareto improvement/optimality conditions. Second, we analyze conditions that improve and violate Pareto optimality itself, respectively. Finally, we propose Reward Dropout, a simple yet powerful method to guarantee policy improvement based on a Pareto improvement condition. Our theoretical outcomes are supported by not only deductive proofs but also empirical results. The performance of Reward Dropout was evaluated on five CLM benchmark datasets, and it turns out that the Reward Dropout significantly improves the performance of CLMs.
</details>
<details>
<summary>摘要</summary>
我们研究控制语言模型（CLM）的理论方面，从双目标优化的角度来看。特别是，我们将CLM视为不同策略RL问题，需要同时 maximize reward和likelihood目标。我们的主要贡献有三个部分：第一部分是建立CLM的理论基础，提出了奖励upper bound和Pareto改进/优化条件。第二部分是分析Pareto优化的条件，并探讨Pareto优化的改进和违反情况。第三部分是提出了一种简单 yet powerful的方法——奖励抽样（Reward Dropout），可以基于Pareto改进条件来保证策略改进。我们的理论成果不仅得到了deductive证明，还得到了实验的支持。我们在五个CLM标准测试集上评估了奖励抽样的性能，结果显示，奖励抽样可以显著提高CLM的性能。
</details></li>
</ul>
<hr>
<h2 id="EMOFM-Ensemble-MLP-mOdel-with-Feature-based-Mixers-for-Click-Through-Rate-Prediction"><a href="#EMOFM-Ensemble-MLP-mOdel-with-Feature-based-Mixers-for-Click-Through-Rate-Prediction" class="headerlink" title="EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate Prediction"></a>EMOFM: Ensemble MLP mOdel with Feature-based Mixers for Click-Through Rate Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04482">http://arxiv.org/abs/2310.04482</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yujian Betterest Li, Kai Wu</li>
<li>for: 预测点击率 (CTR) 预测</li>
<li>methods: 使用网络基于方法进行类型化特征提取和跨Field信息融合，并使用简单插入混合器进行场&amp;类型 wise ensemble 模型构建</li>
<li>results: 在实验中，提议的模型比基eline模型高效，并且可视化优化过程和简介减少实验结果。未来工作可能包括对不同类型的交互进行考虑。<details>
<summary>Abstract</summary>
Track one of CTI competition is on click-through rate (CTR) prediction. The dataset contains millions of records and each field-wise feature in a record consists of hashed integers for privacy. For this task, the keys of network-based methods might be type-wise feature extraction and information fusion across different fields. Multi-layer perceptrons (MLPs) are able to extract field feature, but could not efficiently fuse features. Motivated by the natural fusion characteristic of cross attention and the efficiency of transformer-based structures, we propose simple plug-in mixers for field/type-wise feature fusion, and thus construct an field&type-wise ensemble model, namely EMOFM (Ensemble MLP mOdel with Feature-based Mixers). In the experiments, the proposed model is evaluated on the dataset, the optimization process is visualized and ablation studies are explored. It is shown that EMOFM outperforms compared baselines. In the end, we discuss on future work. WARNING: The comparison might not be fair enough since the proposed method is designed for this data in particular while compared methods are not. For example, EMOFM especially takes different types of interactions into consideration while others do not. Anyway, we do hope that the ideas inside our method could help other developers/learners/researchers/thinkers and so on.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese: Track one of CTI competition is on click-through rate (CTR) prediction. The dataset contains millions of records and each field-wise feature in a record consists of hashed integers for privacy. For this task, the keys of network-based methods might be type-wise feature extraction and information fusion across different fields. Multi-layer perceptrons (MLPs) are able to extract field feature, but could not efficiently fuse features. Motivated by the natural fusion characteristic of cross attention and the efficiency of transformer-based structures, we propose simple plug-in mixers for field/type-wise feature fusion, and thus construct an field&type-wise ensemble model, namely EMOFM (Ensemble MLP mOdel with Feature-based Mixers). In the experiments, the proposed model is evaluated on the dataset, the optimization process is visualized and ablation studies are explored. It is shown that EMOFM outperforms compared baselines. In the end, we discuss on future work. WARNING: The comparison might not be fair enough since the proposed method is designed for this data in particular while compared methods are not. For example, EMOFM especially takes different types of interactions into consideration while others do not. Anyway, we do hope that the ideas inside our method could help other developers/learners/researchers/thinkers and so on.Please note that the translation is in Simplified Chinese, and the formatting of the text may be different from the original English version.Here's the translation:跟踪一项CTI竞赛是Click-through rate（CTR）预测。数据集包含数百万条记录，每个记录中的每个字段特征都是使用哈希值进行隐私保护。为了实现这个任务，网络基于方法的关键可能是类型化特征提取和不同字段之间的信息融合。多层感知器（MLP）可以提取字段特征，但不能高效融合特征。我们受到自然融合特性和转换结构的支持，提出了简单的插入混合器来实现字段/类型特征融合，并构建了一个场&类型特征混合模型，即EMOFM（场&类型特征混合MLP模型）。在实验中，我们评估了提案模型在数据集上，Visualize优化过程和缺省研究。结果表明，EMOFM在比较基线上表现出色。在结尾，我们讨论了未来工作。警告：比较可能不公平，因为我们的方法是为这些数据而设计的，而比较方法则未经设计。例如，EMOFM特别是考虑不同类型的互动，而其他方法并没有。然而，我们希望将我们的方法中的想法传递给其他开发者/学习者/研究者/思想家等。
</details></li>
</ul>
<hr>
<h2 id="Conversational-Financial-Information-Retrieval-Model-ConFIRM"><a href="#Conversational-Financial-Information-Retrieval-Model-ConFIRM" class="headerlink" title="Conversational Financial Information Retrieval Model (ConFIRM)"></a>Conversational Financial Information Retrieval Model (ConFIRM)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13001">http://arxiv.org/abs/2310.13001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Choi, William Gazeley, Siu Ho Wong, Tingting Li</li>
<li>for: 这 paper 是为了探讨利用大语言模型（LLM）在金融领域中的应用。</li>
<li>methods: 这 paper 使用了一种名为 ConFIRM 的 conversational financial information retrieval模型，包括两个模块：首先，生成金融领域特有的问答对话集；其次，评估多个参数精细调整方法的查询分类任务的准确率。</li>
<li>results: 据测试集数据，ConFIRM 可以达到高于 90% 的准确率，这对于 regulatory compliance 是必要的。ConFIRM 提供了一种数据效率的解决方案，用于提取金融对话系统中的精确查询意图。<details>
<summary>Abstract</summary>
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.   ConFIRM comprises two modules:   1) a method to synthesize finance domain-specific question-answer pairs, and   2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.   ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLM）的快速增长，利用它们的特性在特定领域如金融方面的应用值得探索。然而，受控领域如金融的特殊要求，需要针对领域进行优化的框架。我们介绍ConFIRM，一个基于LLM的对话金融信息检索模型，适用于查询意图分类和知识库标签。ConFIRM包括两个模块：1. 生成金融领域特定的问答对数生成方法。2. 评估参数高效微调方法的查询分类任务评估。我们生成了超过4000个样本，并评估了测试集上的准确率。ConFIRM达到了90%的准确率，这是必要的证明合规遵守。ConFIRM提供了数据效率的解决方案，用于提取金融对话系统中精准的查询意图。
</details></li>
</ul>
<hr>
<h2 id="Introducing-the-Attribution-Stability-Indicator-a-Measure-for-Time-Series-XAI-Attributions"><a href="#Introducing-the-Attribution-Stability-Indicator-a-Measure-for-Time-Series-XAI-Attributions" class="headerlink" title="Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions"></a>Introducing the Attribution Stability Indicator: a Measure for Time Series XAI Attributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04178">http://arxiv.org/abs/2310.04178</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/visual-xai-for-time-series/attribution-stability-indicator">https://github.com/visual-xai-for-time-series/attribution-stability-indicator</a></li>
<li>paper_authors: Udo Schlegel, Daniel A. Keim</li>
<li>for: 这篇论文旨在提供一种可解释性模型，用于满足递归时序数据领域中增长的需求。</li>
<li>methods: 该论文使用了 perturbation 分析和相关性分析来评估对时序数据的解释性模型。</li>
<li>results: 该论文提出了一种robustness和可信度的评价指标，即 Attribution Stability Indicator (ASI)，并通过三个整体时序分类 datasets 的分析，证明了 ASI 的可靠性和有用性。<details>
<summary>Abstract</summary>
Given the increasing amount and general complexity of time series data in domains such as finance, weather forecasting, and healthcare, there is a growing need for state-of-the-art performance models that can provide interpretable insights into underlying patterns and relationships. Attribution techniques enable the extraction of explanations from time series models to gain insights but are hard to evaluate for their robustness and trustworthiness. We propose the Attribution Stability Indicator (ASI), a measure to incorporate robustness and trustworthiness as properties of attribution techniques for time series into account. We extend a perturbation analysis with correlations of the original time series to the perturbed instance and the attributions to include wanted properties in the measure. We demonstrate the wanted properties based on an analysis of the attributions in a dimension-reduced space and the ASI scores distribution over three whole time series classification datasets.
</details>
<details>
<summary>摘要</summary>
随着金融、天气预测和医疗等领域时序数据的增加和总体复杂度，需要更高级的性能模型，以提供可读性的下溯。但是评估Attribution技术的可靠性和信任性具有挑战。我们提出Attribution Stability Indicator（ASI），一种将可靠性和信任性作为时序Attribution技术的质量因素。我们通过对原始时序数据的推动分析，包括原始时序和推动分析的相关性，来扩展ASI的评估。我们通过对三个整体时序分类 dataset的Attribution分析，示出了ASI分数的分布，并证明了所需的属性。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Relation-Attentive-Graph-Neural-Networks-for-Fraud-Detection"><a href="#Dynamic-Relation-Attentive-Graph-Neural-Networks-for-Fraud-Detection" class="headerlink" title="Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection"></a>Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04171">http://arxiv.org/abs/2310.04171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bdi-lab/drag">https://github.com/bdi-lab/drag</a></li>
<li>paper_authors: Heehyeon Kim, Jinhyeok Choi, Joyce Jiyoung Whang</li>
<li>for: 检测fraudster在社交媒体上的活动，例如留言或交易。</li>
<li>methods: 使用图 neural network（GNN）和动态关系注意力机制来解决这个分类问题。</li>
<li>results: 对实际数据集进行实验，我们的方法（DRAG）的性能高于现有的fraud detection方法。Here’s the full translation of the abstract in Simplified Chinese:检测fraudster在社交媒体上的活动，例如留言或交易，是一个重要的问题。在这篇论文中，我们使用图 neural network（GNN）和动态关系注意力机制来解决这个分类问题。我们首先提出了一种动态关系注意力机制，使得我们可以在不同的层次上学习不同类型的关系，并且可以在不同层次上进行归一化。然后，我们使用这种机制来对图进行归一化，并且使用一个可学习的注意力函数来权衡不同类型的关系。最后，我们对实际数据集进行实验，并证明了我们的方法（DRAG）的性能高于现有的fraud detection方法。<details>
<summary>Abstract</summary>
Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively computes the attention coefficients for each node. Experimental results show that our method, DRAG, outperforms state-of-the-art fraud detection methods on real-world benchmark datasets.
</details>
<details>
<summary>摘要</summary>
针对欺诈者通过假评论或异常交易伤害其他用户的行为，欺诈检测目标是找到这些欺诈者。基于图的欺诈检测方法将这个问题视为一个分类问题，其中有两个类别：欺诈或正常。我们使用图神经网络（GNNs）来解决这个问题，并提出了动态关系注意机制。根据许多真实世界图中包含不同类型的关系的观察，我们提议学习每个关系的节点表示，并使用学习的注意函数来对每个关系分配不同的注意系数。此外，我们将不同层的节点表示进行组合，以考虑目标节点的本地和全局结构，从而提高欺诈检测在异质图上的性能。通过在所有聚合过程中使用动态图注意，我们的方法可以适应性计算每个节点的注意系数。实验结果表明，我们的方法DRAG在真实世界 benchmark 数据集上超过了当前最佳的欺诈检测方法。
</details></li>
</ul>
<hr>
<h2 id="Document-Level-Relation-Extraction-with-Relation-Correlation-Enhancement"><a href="#Document-Level-Relation-Extraction-with-Relation-Correlation-Enhancement" class="headerlink" title="Document-Level Relation Extraction with Relation Correlation Enhancement"></a>Document-Level Relation Extraction with Relation Correlation Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13000">http://arxiv.org/abs/2310.13000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lumia-group/lace">https://github.com/lumia-group/lace</a></li>
<li>paper_authors: Yusheng Huang, Zhouhan Lin</li>
<li>for: 本研究旨在提高文档关系提取（DocRE）模型的性能，通过显著地利用关系相互关系。</li>
<li>methods: 我们提出了一种关系图方法，使用先前知道的关系统统征信息来构建关系图，并使用重要性权重Matrix来引导关系信息的传播。</li>
<li>results: 我们的方法可以很好地与现有模型结合使用，并且在多关系提取 task 中提高性能，证明了关系相互关系的考虑对 DocRE  task 是重要的。<details>
<summary>Abstract</summary>
Document-level relation extraction (DocRE) is a task that focuses on identifying relations between entities within a document. However, existing DocRE models often overlook the correlation between relations and lack a quantitative analysis of relation correlations. To address this limitation and effectively capture relation correlations in DocRE, we propose a relation graph method, which aims to explicitly exploit the interdependency among relations. Firstly, we construct a relation graph that models relation correlations using statistical co-occurrence information derived from prior relation knowledge. Secondly, we employ a re-weighting scheme to create an effective relation correlation matrix to guide the propagation of relation information. Furthermore, we leverage graph attention networks to aggregate relation embeddings. Importantly, our method can be seamlessly integrated as a plug-and-play module into existing models. Experimental results demonstrate that our approach can enhance the performance of multi-relation extraction, highlighting the effectiveness of considering relation correlations in DocRE.
</details>
<details>
<summary>摘要</summary>
文档关系提取（DocRE）是一个着眼于文档中实体之间关系的任务。然而，现有的 DocRE 模型通常忽视关系之间的相关性并缺乏关系相关性的量化分析。为了解决这些限制并有效地捕捉关系相关性，我们提议一种关系图方法，它利用优先知识中的关系统统统计协会信息来模型关系相关性。首先，我们构建一个关系图，其中模型了关系之间的相关性。然后，我们采用一种重新权重分配方法，以创建一个有效的关系相关性矩阵，以导引关系信息的协会。此外，我们利用图注意网络来聚合关系嵌入。重要的是，我们的方法可以轻松地与现有模型集成，成为插件模块。实验结果表明，我们的方法可以提高多关系提取的性能，从而证明了考虑关系相关性在 DocRE 中的重要性。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Neuron-Segmentation-with-Multi-Agent-Reinforcement-Learning"><a href="#Self-Supervised-Neuron-Segmentation-with-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning"></a>Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04148">http://arxiv.org/abs/2310.04148</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ydchen0806/dbmim">https://github.com/ydchen0806/dbmim</a></li>
<li>paper_authors: Yinda Chen, Wei Huang, Shenglong Zhou, Qi Chen, Zhiwei Xiong<br>for: This paper aims to improve the performance of supervised neuron segmentation methods by using self-supervised learning and reinforcement learning to pretrain a decision-based mask image model (MIM).methods: The proposed method utilizes reinforcement learning to automatically search for the optimal image masking ratio and masking strategy, and treats each input patch as an agent with a shared behavior policy to enable multi-agent collaboration.results: The proposed approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation, as demonstrated by experiments conducted on representative EM datasets.<details>
<summary>Abstract</summary>
The performance of existing supervised neuron segmentation methods is highly dependent on the number of accurate annotations, especially when applied to large scale electron microscopy (EM) data. By extracting semantic information from unlabeled data, self-supervised methods can improve the performance of downstream tasks, among which the mask image model (MIM) has been widely used due to its simplicity and effectiveness in recovering original information from masked images. However, due to the high degree of structural locality in EM images, as well as the existence of considerable noise, many voxels contain little discriminative information, making MIM pretraining inefficient on the neuron segmentation task. To overcome this challenge, we propose a decision-based MIM that utilizes reinforcement learning (RL) to automatically search for optimal image masking ratio and masking strategy. Due to the vast exploration space, using single-agent RL for voxel prediction is impractical. Therefore, we treat each input patch as an agent with a shared behavior policy, allowing for multi-agent collaboration. Furthermore, this multi-agent model can capture dependencies between voxels, which is beneficial for the downstream segmentation task. Experiments conducted on representative EM datasets demonstrate that our approach has a significant advantage over alternative self-supervised methods on the task of neuron segmentation. Code is available at \url{https://github.com/ydchen0806/dbMiM}.
</details>
<details>
<summary>摘要</summary>
现有的监督学习神经分 segmentation 方法的性能受到标注数量的影响，特别是在大规模电子顺icroscopy (EM) 数据上。通过提取 semantic 信息于无标注数据中，自动学习方法可以提高下游任务的性能。在这些下游任务中，mask image model (MIM) 得到了广泛的应用，因为它的简单性和能效性在恢复原始信息的masked 图像中。然而，由于EM图像中的结构本地性和噪声的存在，许多 voxel 含有少量特征信息，使得 MIM 预training 不具有效果。为了解决这个挑战，我们提出了一种决策基于的 MIM，通过 reinforcement learning (RL) 自动搜索最佳的图像掩蔽比率和掩蔽策略。由于搜索空间的庞大，单个agent RL 为 voxel 预测是不实用的。因此，我们将每个输入 patch 作为一个agent，并将其共享行为策略。此外，这种多agent模型可以捕捉voxels之间的依赖关系，这对下游分 segmentation 任务是有利的。在代表性的 EM 数据集上进行的实验表明，我们的方法在神经分 segmentation 任务中具有显著的优势。代码可以在 \url{https://github.com/ydchen0806/dbMiM} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Acoustic-and-linguistic-representations-for-speech-continuous-emotion-recognition-in-call-center-conversations"><a href="#Acoustic-and-linguistic-representations-for-speech-continuous-emotion-recognition-in-call-center-conversations" class="headerlink" title="Acoustic and linguistic representations for speech continuous emotion recognition in call center conversations"></a>Acoustic and linguistic representations for speech continuous emotion recognition in call center conversations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04481">http://arxiv.org/abs/2310.04481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manon Macary, Marie Tahon, Yannick Estève, Daniel Luzzati</li>
<li>for: 这项研究旨在自动检测真实的客户服务电话对话中的满意度和不满度。</li>
<li>methods: 这项研究使用预训练的语音表示进行转移学习，以提高客户服务质量。</li>
<li>results: 实验结果表明，使用预训练的语音表示可以获得大量的性能提升，而语言内容是满意度预测的主要 contribuutor。<details>
<summary>Abstract</summary>
The goal of our research is to automatically retrieve the satisfaction and the frustration in real-life call-center conversations. This study focuses an industrial application in which the customer satisfaction is continuously tracked down to improve customer services. To compensate the lack of large annotated emotional databases, we explore the use of pre-trained speech representations as a form of transfer learning towards AlloSat corpus. Moreover, several studies have pointed out that emotion can be detected not only in speech but also in facial trait, in biological response or in textual information. In the context of telephone conversations, we can break down the audio information into acoustic and linguistic by using the speech signal and its transcription. Our experiments confirms the large gain in performance obtained with the use of pre-trained features. Surprisingly, we found that the linguistic content is clearly the major contributor for the prediction of satisfaction and best generalizes to unseen data. Our experiments conclude to the definitive advantage of using CamemBERT representations, however the benefit of the fusion of acoustic and linguistic modalities is not as obvious. With models learnt on individual annotations, we found that fusion approaches are more robust to the subjectivity of the annotation task. This study also tackles the problem of performances variability and intends to estimate this variability from different views: weights initialization, confidence intervals and annotation subjectivity. A deep analysis on the linguistic content investigates interpretable factors able to explain the high contribution of the linguistic modality for this task.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reinforcement-Learning-with-Fast-and-Forgetful-Memory"><a href="#Reinforcement-Learning-with-Fast-and-Forgetful-Memory" class="headerlink" title="Reinforcement Learning with Fast and Forgetful Memory"></a>Reinforcement Learning with Fast and Forgetful Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04128">http://arxiv.org/abs/2310.04128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/proroklab/ffm">https://github.com/proroklab/ffm</a></li>
<li>paper_authors: Steven Morad, Ryan Kortvelesy, Stephan Liwicki, Amanda Prorok</li>
<li>for: 这篇论文主要是为了解决模型自由RL中存储问题，提出了一种专门为RL设计的快速和忘记型内存模型（Fast and Forgetful Memory，FFM），以提高奖励和训练效率。</li>
<li>methods: FFM使用了 Computational Psychology 中的强结构预先知识，将RL的模型搜索空间约束到一个固定的空间，并且可以作为 RNN 的替换部件，无需改变任何超参数。</li>
<li>results: FFM在多种回合RL benchmark 和算法上达到了更高的奖励，并且在训练速度方面也有了两个数量级的提升，具体来说是logarithmic time和linear space复杂度。<details>
<summary>Abstract</summary>
Nearly all real world tasks are inherently partially observable, necessitating the use of memory in Reinforcement Learning (RL). Most model-free approaches summarize the trajectory into a latent Markov state using memory models borrowed from Supervised Learning (SL), even though RL tends to exhibit different training and efficiency characteristics. Addressing this discrepancy, we introduce Fast and Forgetful Memory, an algorithm-agnostic memory model designed specifically for RL. Our approach constrains the model search space via strong structural priors inspired by computational psychology. It is a drop-in replacement for recurrent neural networks (RNNs) in recurrent RL algorithms, achieving greater reward than RNNs across various recurrent benchmarks and algorithms without changing any hyperparameters. Moreover, Fast and Forgetful Memory exhibits training speeds two orders of magnitude faster than RNNs, attributed to its logarithmic time and linear space complexity. Our implementation is available at https://github.com/proroklab/ffm.
</details>
<details>
<summary>摘要</summary>
大多数实际任务都是半观察的，因此需要在强化学习（RL）中使用记忆。大多数无模型方法会把轨迹摘要为一个 latent Markov state 使用来自supervised Learning（SL）的记忆模型，although RL 的训练和效率特点与 SL 有所不同。为解决这个差异，我们介绍 Fast and Forgetful Memory，一种专门为 RL 设计的记忆模型。我们通过强制模型搜索空间的计算机科学逻辑约束，使其成为 RNN 的替换部件，在不同的 recurrent 算法和hyperparameters 下实现更高的奖励。此外，Fast and Forgetful Memory 的训练速度比 RNN 快两个数量级，这是因为它的循环时间复杂度为对数几何，而不是 RNN 的线性循环时间复杂度。我们的实现可以在 GitHub 上找到：https://github.com/proroklab/ffm。
</details></li>
</ul>
<hr>
<h2 id="Making-Users-Indistinguishable-Attribute-wise-Unlearning-in-Recommender-Systems"><a href="#Making-Users-Indistinguishable-Attribute-wise-Unlearning-in-Recommender-Systems" class="headerlink" title="Making Users Indistinguishable: Attribute-wise Unlearning in Recommender Systems"></a>Making Users Indistinguishable: Attribute-wise Unlearning in Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.05847">http://arxiv.org/abs/2310.05847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Yizhao Zhang, Zhongxuan Han, Dan Meng, Jun Wang</li>
<li>for: 隐私担忧在推荐系统中增加了关注，因此推荐系统中的推学算法忘记（Unlearning）已经得到了越来越多的关注。现有的研究主要使用训练数据作为忘记目标。然而，我们发现攻击者可以从一个已经训练过的模型中提取私人信息，例如性别、种族和年龄，即使这些信息没有直接出现在训练数据中。我们称这些未看到的信息为特征，并将其作为忘记目标。</li>
<li>methods: 为了保护用户的敏感特征，我们提出了特征忘记（Attribute Unlearning，AU），它的目的是降低攻击性能和使目标特征变得无法识别。在这篇论文中，我们关注一个具有严格且实际意义的AU问题，即在训练完成后进行忘记（Post-Training Attribute Unlearning，PoT-AU）。为解决PoT-AU问题，我们设计了一个两部分损失函数，包括i）特征标签无法识别的混淆损失，和ii）正则化损失，以防止模型受到训练变化而导致推荐性能下降。</li>
<li>results: 我们使用批量Descendant Gradient Algorithm进行优化。对于三个真实的数据集进行了广泛的实验，结果表明我们的提出的方法有效地解决了PoT-AU问题。<details>
<summary>Abstract</summary>
With the growing privacy concerns in recommender systems, recommendation unlearning, i.e., forgetting the impact of specific learned targets, is getting increasing attention. Existing studies predominantly use training data, i.e., model inputs, as the unlearning target. However, we find that attackers can extract private information, i.e., gender, race, and age, from a trained model even if it has not been explicitly encountered during training. We name this unseen information as attribute and treat it as the unlearning target. To protect the sensitive attribute of users, Attribute Unlearning (AU) aims to degrade attacking performance and make target attributes indistinguishable. In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we design a two-component loss function that consists of i) distinguishability loss: making attribute labels indistinguishable from attackers, and ii) regularization loss: preventing drastic changes in the model that result in a negative impact on recommendation performance. Specifically, we investigate two types of distinguishability measurements, i.e., user-to-user and distribution-to-distribution. We use the stochastic gradient descent algorithm to optimize our proposed loss. Extensive experiments on three real-world datasets demonstrate the effectiveness of our proposed methods.
</details>
<details>
<summary>摘要</summary>
In this paper, we focus on a strict but practical setting of AU, namely Post-Training Attribute Unlearning (PoT-AU), where unlearning can only be performed after the training of the recommendation model is completed. To address the PoT-AU problem in recommender systems, we design a two-component loss function that consists of:1. Distinguishability loss: making attribute labels indistinguishable from attackers.2. Regularization loss: preventing drastic changes in the model that result in a negative impact on recommendation performance.We investigate two types of distinguishability measurements, i.e., user-to-user and distribution-to-distribution. We use the stochastic gradient descent algorithm to optimize our proposed loss. Extensive experiments on three real-world datasets demonstrate the effectiveness of our proposed methods.
</details></li>
</ul>
<hr>
<h2 id="Auto-survey-Challenge"><a href="#Auto-survey-Challenge" class="headerlink" title="Auto-survey Challenge"></a>Auto-survey Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04480">http://arxiv.org/abs/2310.04480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanh Gia Hieu Khuong, Benedictus Kent Rachmat</li>
<li>for: This paper is written for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers in various disciplines.</li>
<li>methods: The paper uses a simulated peer-review mechanism and human organizers in an editorial oversight capacity to evaluate the LLMs’ performance.</li>
<li>results: The paper presents the design of a competition for the AutoML conference 2023, where entrants are tasked with presenting stand-alone models that can author and appraise articles based on designated prompts, and the assessment criteria include clarity, reference appropriateness, accountability, and substantive value of the content.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文是为了评估大语言模型（LLM）在不同领域的自动撰写和评论报告的能力而写的。</li>
<li>methods: 这篇论文使用了模拟的同行评审机制和人类组织者在编辑监督性的情况下评估LLM的表现。</li>
<li>results: 这篇论文介绍了AutoML会议2023年的一项竞赛，参赛者需要提交独立的模型，能够根据指定的提示自动撰写和评论文章，并且评估标准包括明确度、参考适用性、责任和内容的价值。<details>
<summary>Abstract</summary>
We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的平台，用于评估大自然语言模型（LLM）能够自主撰写和评论论文，涵盖了各种学科，包括科学、人文社会科学、教育和法律。在这个框架下，人工智能系统在模拟学术审查机制下进行自动审查，人类组织者在编辑监督 capacity 中发挥作用。为2023年AutoML会议，我们组织了一场竞赛，参赛者需要提交独立的模型，可以从指定的提示中撰写文章，并且进行评估。评价标准包括明确度、引用适用性、负责任性和内容的实际价值。本文介绍了竞赛的设计，包括提交基eline和评估方法。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Data-Geometry-to-Mitigate-CSM-in-Steganalysis"><a href="#Leveraging-Data-Geometry-to-Mitigate-CSM-in-Steganalysis" class="headerlink" title="Leveraging Data Geometry to Mitigate CSM in Steganalysis"></a>Leveraging Data Geometry to Mitigate CSM in Steganalysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04479">http://arxiv.org/abs/2310.04479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rony Abecidan, Vincent Itier, Jérémie Boulanger, Patrick Bas, Tomáš Pevný</li>
<li>for: 本研究旨在 mitigating Cover Source Mismatch (CSM) issue in steganalysis, by exploring a grid of processing pipelines and developing a strategy for selecting or deriving customized training datasets.</li>
<li>methods: 本研究使用了一种基于几何学的优化策略，通过Computer Tomography (DCTr) 特征下的子空间距离计算出高相关性的操作 regret。</li>
<li>results: 实验 validate 了我们的几何学基于优化策略，相比传统原子方法，在理想的假设下表现出色。详细的实验结果可以在github.com&#x2F;RonyAbecidan&#x2F;LeveragingGeometrytoMitigateCSM 上找到。<details>
<summary>Abstract</summary>
In operational scenarios, steganographers use sets of covers from various sensors and processing pipelines that differ significantly from those used by researchers to train steganalysis models. This leads to an inevitable performance gap when dealing with out-of-distribution covers, commonly referred to as Cover Source Mismatch (CSM). In this study, we consider the scenario where test images are processed using the same pipeline. However, knowledge regarding both the labels and the balance between cover and stego is missing. Our objective is to identify a training dataset that allows for maximum generalization to our target. By exploring a grid of processing pipelines fostering CSM, we discovered a geometrical metric based on the chordal distance between subspaces spanned by DCTr features, that exhibits high correlation with operational regret while being not affected by the cover-stego balance. Our contribution lies in the development of a strategy that enables the selection or derivation of customized training datasets, enhancing the overall generalization performance for a given target. Experimental validation highlights that our geometry-based optimization strategy outperforms traditional atomistic methods given reasonable assumptions. Additional resources are available at github.com/RonyAbecidan/LeveragingGeometrytoMitigateCSM.
</details>
<details>
<summary>摘要</summary>
在操作场景中，隐写者使用来自不同感知和处理管道的集合，这与研究人员用于训练隐写检测模型的集合有很大差异。这导致了对于非典型覆盖（CSM）的性能差距。在本研究中，我们考虑的enario是测试图像通过同一个管道进行处理。然而，关于标签和覆盖的权重知道的信息缺失。我们的目标是找到一个允许最大化适应性的训练集。通过探索一个离散的处理管道集合，我们发现了一种基于分割距离的 геометри metric，它与操作 regret 具有高相关性，而不受覆盖-隐写权重的影响。我们的贡献在于开发了一种基于几何学的选择或 derivation 训练集策略，以提高给定目标的总体适应性。实验验证表明，我们的几何学基于优化策略在理想情况下比传统原子方法表现更好。更多资源可以在github.com/RonyAbecidan/LeveragingGeometrytoMitigateCSM中找到。
</details></li>
</ul>
<hr>
<h2 id="Nash-Welfare-and-Facility-Location"><a href="#Nash-Welfare-and-Facility-Location" class="headerlink" title="Nash Welfare and Facility Location"></a>Nash Welfare and Facility Location</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04102">http://arxiv.org/abs/2310.04102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Lam, Haris Aziz, Toby Walsh</li>
<li>for: 解决资源分配问题中兼顾公平和效率的问题</li>
<li>methods: 使用纳什公平函数，将个体成本转换为Utility，并提供一个多阶段搜索算法来寻找最大化纳什公平的设施位置</li>
<li>results: 提供一个纳什公平函数来衡量设施位置的公平性和效率，并证明这种方法可以在多阶段搜索中实现可靠的近似解Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to address the problem of locating a facility to serve a set of agents located along a line, and to provide a compromise between fairness and efficiency in resource allocation problems.</li>
<li>methods: The paper uses the Nash welfare objective function, which is defined as the product of the agents’ utilities, to convert individual costs to utilities and analyze the facility placement that maximizes the Nash welfare. The paper also provides a polynomial-time approximation algorithm to compute this facility location.</li>
<li>results: The paper proves results suggesting that the proposed facility location algorithm achieves a good balance of fairness and efficiency, and also proposes a strategy-proof mechanism with a bounded approximation ratio for Nash welfare from a mechanism design perspective.<details>
<summary>Abstract</summary>
We consider the problem of locating a facility to serve a set of agents located along a line. The Nash welfare objective function, defined as the product of the agents' utilities, is known to provide a compromise between fairness and efficiency in resource allocation problems. We apply this welfare notion to the facility location problem, converting individual costs to utilities and analyzing the facility placement that maximizes the Nash welfare. We give a polynomial-time approximation algorithm to compute this facility location, and prove results suggesting that it achieves a good balance of fairness and efficiency. Finally, we take a mechanism design perspective and propose a strategy-proof mechanism with a bounded approximation ratio for Nash welfare.
</details>
<details>
<summary>摘要</summary>
我团队考虑了一个服务多个代理的设施问题。我们使用纳什利益函数，它是资源分配问题中的一种妥协之处，来解决这个问题。我们将个人成本转换为利益，并分析最大化纳什利益的设施位置。我们提供一个可靠的多阶段批处理算法，并证明它可以实现一个良好的平衡 между公平和效率。最后，我们从机制设计的角度出发，并提出一个具有 bounded approximation ratio的纳什利益响应的机制。Note: "纳什利益函数" (Nash welfare function) is a direct translation of the English term, and "纳什利益响应" (Nash welfare response) is a translation of the phrase "strategy-proof mechanism".
</details></li>
</ul>
<hr>
<h2 id="A-Deeply-Supervised-Semantic-Segmentation-Method-Based-on-GAN"><a href="#A-Deeply-Supervised-Semantic-Segmentation-Method-Based-on-GAN" class="headerlink" title="A Deeply Supervised Semantic Segmentation Method Based on GAN"></a>A Deeply Supervised Semantic Segmentation Method Based on GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04081">http://arxiv.org/abs/2310.04081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhao, Qiyu Wei, Zeng Zeng</li>
<li>for: 这篇论文旨在提高智能交通系统中的交通安全性，通过精确地识别和位置化不同类型的路面元素，如路面裂隙、车道和交通标志。</li>
<li>methods: 本研究提出一个改进的semantic segmentation模型，结合了抗对抗学习和现有的semantic segmentation技术，以提高模型在交通图像中捕捉复杂和微妙的特征的能力。</li>
<li>results: 比较 existing methods，如SEGAN，本研究获得了明显的性能提升，特别是在路面裂隙数据集上。这些改进可以归因于对抗学习和semantic segmentation之间的相互补充作用，导致更精确和丰富的路面结构和状态表现。<details>
<summary>Abstract</summary>
In recent years, the field of intelligent transportation has witnessed rapid advancements, driven by the increasing demand for automation and efficiency in transportation systems. Traffic safety, one of the tasks integral to intelligent transport systems, requires accurately identifying and locating various road elements, such as road cracks, lanes, and traffic signs. Semantic segmentation plays a pivotal role in achieving this task, as it enables the partition of images into meaningful regions with accurate boundaries. In this study, we propose an improved semantic segmentation model that combines the strengths of adversarial learning with state-of-the-art semantic segmentation techniques. The proposed model integrates a generative adversarial network (GAN) framework into the traditional semantic segmentation model, enhancing the model's performance in capturing complex and subtle features in transportation images. The effectiveness of our approach is demonstrated by a significant boost in performance on the road crack dataset compared to the existing methods, \textit{i.e.,} SEGAN. This improvement can be attributed to the synergistic effect of adversarial learning and semantic segmentation, which leads to a more refined and accurate representation of road structures and conditions. The enhanced model not only contributes to better detection of road cracks but also to a wide range of applications in intelligent transportation, such as traffic sign recognition, vehicle detection, and lane segmentation.
</details>
<details>
<summary>摘要</summary>
在最近的几年，智能交通领域已经受到了自动化和效率的需求的推动，导致了智能交通系统的快速发展。交通安全是智能交通系统中的一项重要任务，需要准确地识别和定位不同的路面元素，如路面裂隙、车道和交通标志。Semantic segmentation在完成这项任务中扮演着关键的角色，它使得图像被分割成有意义的区域，边界准确。在本研究中，我们提出了一种改进的Semantic segmentation模型，该模型结合了对抗学习和当今最佳Semantic segmentation技术的优势。我们的提案把生成对抗网络（GAN）框架 incorporated into the traditional Semantic segmentation模型，从而提高模型对复杂和细微的特征的捕捉能力。我们的方法在路面裂隙数据集上表现出了显著的提高，与现有的方法相比，例如SEGAN，即使在较为复杂的场景下也能够更好地识别路面裂隙。这种提高可以归因于对抗学习和Semantic segmentation的相互作用，导致了更加精细和准确的路面结构和状况的表示。提高的模型不仅有助于更好地检测路面裂隙，还有广泛的应用前景在智能交通领域，如交通标志识别、车辆检测和车道分 segmentation。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Aspect-Extraction-from-Scientific-Texts"><a href="#Automatic-Aspect-Extraction-from-Scientific-Texts" class="headerlink" title="Automatic Aspect Extraction from Scientific Texts"></a>Automatic Aspect Extraction from Scientific Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04074">http://arxiv.org/abs/2310.04074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anna-marshalova/automatic-aspect-extraction-from-scientific-texts">https://github.com/anna-marshalova/automatic-aspect-extraction-from-scientific-texts</a></li>
<li>paper_authors: Anna Marshalova, Elena Bruches, Tatiana Batura</li>
<li>for: 本研究的目的是开发一种自动从俄语科学文献中提取主要元素的工具，以便进行科学文献综述。</li>
<li>methods: 本研究使用的方法包括创建了跨领域俄语科学文献数据集，并使用多语言BERT模型在这些数据上进行了微调。</li>
<li>results: 研究表明，使用微调后的多语言BERT模型可以在不同领域的俄语科学文献中提取主要元素，并且可以在不同领域之间进行泛化。<details>
<summary>Abstract</summary>
Being able to extract from scientific papers their main points, key insights, and other important information, referred to here as aspects, might facilitate the process of conducting a scientific literature review. Therefore, the aim of our research is to create a tool for automatic aspect extraction from Russian-language scientific texts of any domain. In this paper, we present a cross-domain dataset of scientific texts in Russian, annotated with such aspects as Task, Contribution, Method, and Conclusion, as well as a baseline algorithm for aspect extraction, based on the multilingual BERT model fine-tuned on our data. We show that there are some differences in aspect representation in different domains, but even though our model was trained on a limited number of scientific domains, it is still able to generalize to new domains, as was proved by cross-domain experiments. The code and the dataset are available at \url{https://github.com/anna-marshalova/automatic-aspect-extraction-from-scientific-texts}.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将科学文献中的主要点、关键发现和其他重要信息（以下简称“方面”）抽取出来，可能会使科学文献复习更加容易。因此，我们的研究目标是开发一种自动从俄语科学文献中提取方面的工具。在这篇论文中，我们提供了跨领域俄语科学文献数据集，每篇文献都有标注的方面，包括任务、贡献、方法和结论。此外，我们还提出了一个基线算法，基于多语言BERT模型，并在我们的数据集上进行了微调。我们发现在不同领域中，方面的表示存在一些差异，但是我们的模型即使只在有限的科学领域进行了训练，仍然能够在新领域中广泛应用。代码和数据集可以在 GitHub 上找到：<https://github.com/anna-marshalova/automatic-aspect-extraction-from-scientific-texts>。
</details></li>
</ul>
<hr>
<h2 id="AI-Regulation-in-Europe-From-the-AI-Act-to-Future-Regulatory-Challenges"><a href="#AI-Regulation-in-Europe-From-the-AI-Act-to-Future-Regulatory-Challenges" class="headerlink" title="AI Regulation in Europe: From the AI Act to Future Regulatory Challenges"></a>AI Regulation in Europe: From the AI Act to Future Regulatory Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04072">http://arxiv.org/abs/2310.04072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Hacker</li>
<li>for: 本文探讨了欧盟对人工智能（AI）的规制，与英国更为分化和自律的方法进行比较，并提出了一种混合规制策略，强调了需要灵活和安全坚实的执行。</li>
<li>methods: 本文研究了AI法，作为人工智能技术面临多方面挑战的先驱立法努力，尽管该法有缺陷，但是是一个重要的立法成就。</li>
<li>results: 本文预测了未来的规制挑战，如管理恶势力内容、环境问题和杂合威胁，并强调了需要立即创建规则 для规制高性能、可能是开源的AI系统访问。虽然AI法是一个重要的立法成就，但是需要进一步的细化和全球合作，以有效管理在不断发展的AI技术。<details>
<summary>Abstract</summary>
This chapter provides a comprehensive discussion on AI regulation in the European Union, contrasting it with the more sectoral and self-regulatory approach in the UK. It argues for a hybrid regulatory strategy that combines elements from both philosophies, emphasizing the need for agility and safe harbors to ease compliance. The paper examines the AI Act as a pioneering legislative effort to address the multifaceted challenges posed by AI, asserting that, while the Act is a step in the right direction, it has shortcomings that could hinder the advancement of AI technologies. The paper also anticipates upcoming regulatory challenges, such as the management of toxic content, environmental concerns, and hybrid threats. It advocates for immediate action to create protocols for regulated access to high-performance, potentially open-source AI systems. Although the AI Act is a significant legislative milestone, it needs additional refinement and global collaboration for the effective governance of rapidly evolving AI technologies.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这章节讲述欧盟的人工智能规则，与英国更为领域化和自 réglementaire的方法进行比较，强调混合 réglementaire 策略，并强调需要灵活和安全庇护以便遵守。文章分析了人工智能法为人工智能多重挑战所采取的先驱法律努力，但认为法律有缺陷可能会阻碍人工智能技术的发展。文章还预测将来的规制挑战，如抑制危险内容、环境问题和杂合威胁。它主张立即创建规则了高性能、可能是开源的人工智能系统的访问协议。虽然人工智能法是一个重要的立法里程碑，但它需要进一步的细化和全球合作，以有效管理在不断发展的人工智能技术。
</details></li>
</ul>
<hr>
<h2 id="ByteStack-ID-Integrated-Stacked-Model-Leveraging-Payload-Byte-Frequency-for-Grayscale-Image-based-Network-Intrusion-Detection"><a href="#ByteStack-ID-Integrated-Stacked-Model-Leveraging-Payload-Byte-Frequency-for-Grayscale-Image-based-Network-Intrusion-Detection" class="headerlink" title="ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection"></a>ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.09298">http://arxiv.org/abs/2310.09298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Irfan Khan, Yasir Ali Farrukh, Syed Wali</li>
<li>for: 本研究旨在提高网络安全性，通过 packet-level 数据分析，快速、准确地识别多样化的攻击类型。</li>
<li>methods: 本文提出了 “ByteStack-ID” 方法，基于 payload 数据频率分布生成的灰度图像，以及 packet-level 信息为基础，采用堆式方法拟合，并具有高度优化、一体化的特点。</li>
<li>results: 实验结果表明，ByteStack-ID 框架在多类分类任务中表现杰出，与基eline模型和现有方法相比，具有较高的精度、回归率和 F1 分数。特别是，ByteStack-ID 在多类分类任务中达到了 81% 的macro F1 分数。<details>
<summary>Abstract</summary>
In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstanding effectiveness of the ByteStack-ID framework, consistently outperforming baseline models and state-of-the-art approaches across pivotal performance metrics, including precision, recall, and F1-score. Impressively, our proposed approach achieves an exceptional 81\% macro F1-score in multiclass classification tasks. In a landscape marked by the continuous evolution of network threats, ByteStack-ID emerges as a robust and versatile security solution, relying solely on packet-level information extracted from network traffic data.
</details>
<details>
<summary>摘要</summary>
在网络安全领域中，能快速和准确地识别多样化的攻击类型在网络流量中是极其重要的。本文介绍了一种名为“ByteStack-ID”的创新方法，这种方法是专门为packet-level攻击检测设计的。ByteStack-ID的核心思想是利用 payload 数据的频率分布生成灰度图像，这是一种新的技术，它能够大幅提高模型对复杂数据模式的识别能力。与传统的网络入侵检测系统（NIDS）不同，ByteStack-ID 仅仅基于流量数据，而不是基于流量的总是。在核心思想上，ByteStack-ID 是一种堆叠方法，但它不同于传统的堆叠方法，它可以将多个基础学习层集成到 concatenated 的基础学习层中，创造出一个高度优化的、统一的模型。实验结果证明，ByteStack-ID 框架在多类分类任务中表现杰出，与基准模型和当前领域的state-of-the-art方法相比，在重要的性能指标上均取得了出色的成绩。特别是，我们的提议方法在多类分类任务中达到了81%的macro F1-score，这表明ByteStack-ID 在网络安全领域中是一种Robust和多样化的安全解决方案。
</details></li>
</ul>
<hr>
<h2 id="Kick-Bad-Guys-Out-Zero-Knowledge-Proof-Based-Anomaly-Detection-in-Federated-Learning"><a href="#Kick-Bad-Guys-Out-Zero-Knowledge-Proof-Based-Anomaly-Detection-in-Federated-Learning" class="headerlink" title="Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning"></a>Kick Bad Guys Out! Zero-Knowledge-Proof-Based Anomaly Detection in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04055">http://arxiv.org/abs/2310.04055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Han, Wenxuan Wu, Baturalp Buyukates, Weizhao Jin, Yuhang Yao, Qifan Zhang, Salman Avestimehr, Chaoyang He</li>
<li>for: 防止 federated learning 系统中的恶意客户端提交损害模型以实现对抗目标，如阻碍全局模型的协调或让全局模型对某些数据进行误分类。</li>
<li>methods: 使用 cutting-edge anomaly detection 技术，包括：i) 检测攻击发生并在攻击发生时执行防御操作；ii) 在攻击发生时，进一步检测恶意客户端模型并从中除掉无害的模型；iii) 使用零知识证明机制来保证服务器上的防御机制的诚实执行。</li>
<li>results: 通过广泛的实验证明了提posed approach的超越性表现。<details>
<summary>Abstract</summary>
Federated learning (FL) systems are vulnerable to malicious clients that submit poisoned local models to achieve their adversarial goals, such as preventing the convergence of the global model or inducing the global model to misclassify some data. Many existing defense mechanisms are impractical in real-world FL systems, as they require prior knowledge of the number of malicious clients or rely on re-weighting or modifying submissions. This is because adversaries typically do not announce their intentions before attacking, and re-weighting might change aggregation results even in the absence of attacks. To address these challenges in real FL systems, this paper introduces a cutting-edge anomaly detection approach with the following features: i) Detecting the occurrence of attacks and performing defense operations only when attacks happen; ii) Upon the occurrence of an attack, further detecting the malicious client models and eliminating them without harming the benign ones; iii) Ensuring honest execution of defense mechanisms at the server by leveraging a zero-knowledge proof mechanism. We validate the superior performance of the proposed approach with extensive experiments.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）系统容易受到黑客的攻击，这些黑客可能会提交毒害的本地模型来达到他们的恶意目的，例如阻碍全球模型的聚合或让全球模型错分析一些数据。许多现有的防御机制不实用于实际的FL系统中，因为它们需要先知道黑客的数量或基于重新挂绫或修改提交。这是因为敌人通常不会在进攻前宣布他们的意图，而重新挂绫可能会在没有进攻的情况下改变聚合结果。为了解决实际FL系统中的这些挑战，这篇研究论文引入了一种前沿的异常探测方法，具有以下特点：1. 当进攻发生时，探测进攻并执行防御操作；2. 进攻发生后，进一步探测侵略客模型，并将其淘汰无害的模型；3. 在服务器端进行诚实的防御机制执行，运用零知识证明机制。我们透过广泛的实验证明了提案的方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="Higher-Order-DeepTrails-Unified-Approach-to-Trails"><a href="#Higher-Order-DeepTrails-Unified-Approach-to-Trails" class="headerlink" title="Higher-Order DeepTrails: Unified Approach to *Trails"></a>Higher-Order DeepTrails: Unified Approach to *Trails</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04477">http://arxiv.org/abs/2310.04477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lsx-uniwue/deeptrails">https://github.com/lsx-uniwue/deeptrails</a></li>
<li>paper_authors: Tobias Koopmann, Jan Pfister, André Markus, Astrid Carolus, Carolin Wienrich, Andreas Hotho</li>
<li>for: 本研究旨在分析和理解人类行为，以便在不同的设定下提高和优化基础设施或用户界面。</li>
<li>methods: 本研究使用自动逆进语言模型来分析整个序列，以模型高阶相关性在序列中。</li>
<li>results: 本研究可以轻松地适应先前的工作中的不同设定，如 HypTrails、MixedTrails 和 SubTrails，同时具有uniqueadvantages：1. 模型高阶相关性 между状态转移，2. 能够识别提出的假设缺陷，3. 自然地实现了所有设定的统一模型。<details>
<summary>Abstract</summary>
Analyzing, understanding, and describing human behavior is advantageous in different settings, such as web browsing or traffic navigation. Understanding human behavior naturally helps to improve and optimize the underlying infrastructure or user interfaces. Typically, human navigation is represented by sequences of transitions between states. Previous work suggests to use hypotheses, representing different intuitions about the navigation to analyze these transitions. To mathematically grasp this setting, first-order Markov chains are used to capture the behavior, consequently allowing to apply different kinds of graph comparisons, but comes with the inherent drawback of losing information about higher-order dependencies within the sequences. To this end, we propose to analyze entire sequences using autoregressive language models, as they are traditionally used to model higher-order dependencies in sequences. We show that our approach can be easily adapted to model different settings introduced in previous work, namely HypTrails, MixedTrails and even SubTrails, while at the same time bringing unique advantages: 1. Modeling higher-order dependencies between state transitions, while 2. being able to identify short comings in proposed hypotheses, and 3. naturally introducing a unified approach to model all settings. To show the expressiveness of our approach, we evaluate our approach on different synthetic datasets and conclude with an exemplary analysis of a real-world dataset, examining the behavior of users who interact with voice assistants.
</details>
<details>
<summary>摘要</summary>
分析、理解和描述人类行为是在不同的设定中有利，如网络浏览或交通导航。理解人类行为自然地 помоляет改进和优化下面的基础设施或用户界面。通常，人类导航被表示为状态转移序列。先前的工作建议使用假设，表示不同的导航 intuitions 来分析这些转移。使用首领链来数学地抓住这种设定，可以应用不同的图比较方法，但是会隐藏高阶的序列相互关系信息。为了解决这个问题，我们提议使用拟合语言模型来分析整个序列，这种模型通常用于模型序列中的高阶相互关系。我们表明，我们的方法可以轻松地适应先前的工作中引入的不同设定，即 HypTrails、MixedTrails 和 SubTrails，同时带来独特的优势：1. 模型状态转移中的高阶相互关系，2. 能够识别提出的假设缺陷，3. 自然地引入一个统一的方法来模型所有设定。为了证明我们的方法的表达力，我们在不同的 sintetic 数据集上进行了评估，并在一个实际的数据集上进行了 exemplary 分析，探讨用户与语音助手交互的行为。
</details></li>
</ul>
<hr>
<h2 id="Observation-Guided-Diffusion-Probabilistic-Models"><a href="#Observation-Guided-Diffusion-Probabilistic-Models" class="headerlink" title="Observation-Guided Diffusion Probabilistic Models"></a>Observation-Guided Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04041">http://arxiv.org/abs/2310.04041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junoh Kang, Jinyoung Choi, Sungik Choi, Bohyung Han</li>
<li>for: 提高速度抽样和质量控制之间的平衡，建立一种新的扩散模型，即观察指导扩散概率模型（OGDM）。</li>
<li>methods: 通过在Markov链中integrating观察过程的指导，以原则性的方式重新定义培训目标。这里引入基于观察的一个额外损失项，使用bernoulli分布判断输入是否处于噪声真实抽象上，从而优化具有更高准确性的负逻恒 log-likelihood。</li>
<li>results: 通过对强 diffusion model 基础版本进行训练，实现更好的降噪网络，而无需增加计算成本。此外，我们的训练方法可以与不同的快速推理策略结合使用，并且在精度控制和速度抽样之间具有优势。我们通过对多种推理方法进行评估，证明了我们的训练算法的效iveness。<details>
<summary>Abstract</summary>
We propose a novel diffusion model called observation-guided diffusion probabilistic model (OGDM), which effectively addresses the trade-off between quality control and fast sampling. Our approach reestablishes the training objective by integrating the guidance of the observation process with the Markov chain in a principled way. This is achieved by introducing an additional loss term derived from the observation based on the conditional discriminator on noise level, which employs Bernoulli distribution indicating whether its input lies on the (noisy) real manifold or not. This strategy allows us to optimize the more accurate negative log-likelihood induced in the inference stage especially when the number of function evaluations is limited. The proposed training method is also advantageous even when incorporated only into the fine-tuning process, and it is compatible with various fast inference strategies since our method yields better denoising networks using the exactly same inference procedure without incurring extra computational cost. We demonstrate the effectiveness of the proposed training algorithm using diverse inference methods on strong diffusion model baselines.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的扩散模型，即观察指导扩散概率模型（OGDM），可以有效地解决速度控制和质量控制之间的牵扯。我们的方法通过在Markov链中 integrate观察过程的指导来重新定义培训目标。这是通过在噪声水平上采用bernoulli分布来判断输入是否处于噪声真实抽象上来实现的。这种策略使得我们可以在推理阶段更好地优化更准确的负逻辑极限，特别是当功能评估数量有限时。我们的训练方法还具有优化准确性的优势，并且可以轻松地与各种快速推理策略结合使用，不需要额外的计算成本。我们通过使用不同的推理方法对强大的扩散模型基准进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Demystifying-Embedding-Spaces-using-Large-Language-Models"><a href="#Demystifying-Embedding-Spaces-using-Large-Language-Models" class="headerlink" title="Demystifying Embedding Spaces using Large Language Models"></a>Demystifying Embedding Spaces using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04475">http://arxiv.org/abs/2310.04475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Jihwan Jeong, Lior Shani, Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier</li>
<li>for: 使得嵌入模型中的信息更易理解和更广泛使用，使用大自然语言模型（LLM）直接与嵌入 vectors 交互，将抽象矢量转化为可理解的故事。</li>
<li>methods: 使用 LLM 将嵌入 vectors 注入到 LLM 中，以便查询和探索复杂嵌入数据。</li>
<li>results: 在多种多样的任务上，包括增强概念活动矢量（CAV）、传达新嵌入实体、和解码用户喜好 recommender 系统中的用户喜好。<details>
<summary>Abstract</summary>
Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work couples the immense information potential of embeddings with the interpretative power of LLMs.
</details>
<details>
<summary>摘要</summary>
Currently, embeddings have become a crucial tool for representing complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. However, these embeddings often preclude direct interpretation, making it difficult to understand their meaning. Although downstream tasks can utilize these compressed representations, interpreting them usually requires visualization methods such as dimensionality reduction or specialized machine learning interpretability techniques.This paper aims to address the challenge of making embeddings more interpretable and broadly useful by using Large Language Models (LLMs) to directly interact with embeddings. By injecting embeddings into LLMs, we can transform abstract vectors into understandable narratives, enabling users to query and explore complex embedding data. We demonstrate our approach on a variety of diverse tasks, including enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems. Our work combines the vast information potential of embeddings with the interpretive power of LLMs, unlocking new possibilities for understanding and working with complex data.
</details></li>
</ul>
<hr>
<h2 id="Reverse-Chain-A-Generic-Rule-for-LLMs-to-Master-Multi-API-Planning"><a href="#Reverse-Chain-A-Generic-Rule-for-LLMs-to-Master-Multi-API-Planning" class="headerlink" title="Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning"></a>Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04474">http://arxiv.org/abs/2310.04474</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, Jing Zheng</li>
<li>for: 这篇论文目的是为了增强大型自然语言处理模型（LLMs）的表现，使其能够通过外部API进行函数调用，而不需要精度调整。</li>
<li>methods: 这篇论文提出了一种简单又可控的目标驱动方法，称为“反链”，以使LLMs能够通过提示来使用外部API。</li>
<li>results: 实验结果表明，reverse Chain可以具有多 функ数调用的可能性，而且可以提高现有LLMs的工具使用能力。<details>
<summary>Abstract</summary>
While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of LLMs, function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper proposes a simple yet controllable target-driven approach called Reverse Chain to empower LLMs with capabilities to use external APIs with only prompts. Given that most open-source LLMs have limited tool-use or tool-plan capabilities, LLMs in Reverse Chain are only employed to implement simple tasks, e.g., API selection and argument completion, and a generic rule is employed to implement a controllable multiple functions calling. In this generic rule, after selecting a final API to handle a given task via LLMs, we first ask LLMs to fill the required arguments from user query and context. Some missing arguments could be further completed by letting LLMs select another API based on API description before asking user. This process continues until a given task is completed. Extensive numerical experiments indicate an impressive capability of Reverse Chain on implementing multiple function calling. Interestingly enough, the experiments also reveal that tool-use capabilities of the existing LLMs, e.g., ChatGPT, can be greatly improved via Reverse Chain.
</details>
<details>
<summary>摘要</summary>
While enabling large language models to implement function calling (known as APIs) can greatly enhance the performance of LLMs, function calling is still a challenging task due to the complicated relations between different APIs, especially in a context-learning setting without fine-tuning. This paper proposes a simple yet controllable target-driven approach called Reverse Chain to empower LLMs with capabilities to use external APIs with only prompts. Given that most open-source LLMs have limited tool-use or tool-plan capabilities, LLMs in Reverse Chain are only employed to implement simple tasks, e.g., API selection and argument completion, and a generic rule is employed to implement a controllable multiple functions calling. In this generic rule, after selecting a final API to handle a given task via LLMs, we first ask LLMs to fill the required arguments from user query and context. Some missing arguments could be further completed by letting LLMs select another API based on API description before asking user. This process continues until a given task is completed. Extensive numerical experiments indicate an impressive capability of Reverse Chain on implementing multiple function calling. Interestingly enough, the experiments also reveal that tool-use capabilities of the existing LLMs, e.g., ChatGPT, can be greatly improved via Reverse Chain.Translation notes:* "large language models" (大型语言模型) is translated as "LLMs" (LLM) for brevity.* "function calling" (函数调用) is translated as "API" (API) to refer to the specific task or action being performed.* "context-learning setting" (上下文学习设置) is translated as "context-learning" (上下文学习) to emphasize the learning aspect.* "tool-use or tool-plan capabilities" (工具使用或计划能力) is translated as "tool-use capabilities" (工具使用能力) to simplify the phrase.* "Reverse Chain" (返回链) is translated as "Reverse Chain" (返回链) to maintain the original English name for clarity.* "extensive numerical experiments" (详细数学实验) is translated as "extensive numerical experiments" (详细数学实验) to maintain the original phrase for clarity.* "tool-use capabilities" (工具使用能力) is translated as "tool-use capabilities" (工具使用能力) to maintain consistency.
</details></li>
</ul>
<hr>
<h2 id="Effective-Slogan-Generation-with-Noise-Perturbation"><a href="#Effective-Slogan-Generation-with-Noise-Perturbation" class="headerlink" title="Effective Slogan Generation with Noise Perturbation"></a>Effective Slogan Generation with Noise Perturbation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04472">http://arxiv.org/abs/2310.04472</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joannekim0420/slogangeneration">https://github.com/joannekim0420/slogangeneration</a></li>
<li>paper_authors: Jongeun Kim, MinChung Kim, Taehwan Kim</li>
<li>for: 本研究旨在自动生成优秀的企业品牌标语，以帮助企业建立独特的品牌identify。</li>
<li>methods: 本研究使用预训练的 transformer T5 模型，并在新提出的 1:N 匹配对数据集上加入噪声扰动。这种方法可以生成更加独特和凝重的品牌标语。</li>
<li>results: 根据 ROUGE1、ROUGEL 和夹角相似度指标，以及人类评测者的评价，本研究的方法在生成品牌标语方面表现更好，比基eline模型和其他 transformer-based 模型更高。<details>
<summary>Abstract</summary>
Slogans play a crucial role in building the brand's identity of the firm. A slogan is expected to reflect firm's vision and brand's value propositions in memorable and likeable ways. Automating the generation of slogans with such characteristics is challenging. Previous studies developted and tested slogan generation with syntactic control and summarization models which are not capable of generating distinctive slogans. We introduce a a novel apporach that leverages pre-trained transformer T5 model with noise perturbation on newly proposed 1:N matching pair dataset. This approach serves as a contributing fator in generting distinctive and coherent slogans. Turthermore, the proposed approach incorporates descriptions about the firm and brand into the generation of slogans. We evaluate generated slogans based on ROUGE1, ROUGEL and Cosine Similarity metrics and also assess them with human subjects in terms of slogan's distinctiveness, coherence, and fluency. The results demonstrate that our approach yields better performance than baseline models and other transformer-based models.
</details>
<details>
<summary>摘要</summary>
标语对企业形象的建立扮演着关键角色。一句标语应该反映公司的未来方向和品牌价值提供程序，而且需要具备记忆性和喜欢性。但自动生成标语这种特点具备的挑战。先前的研究已经开发和测试了 syntax控制和摘要模型，但这些模型不能生成独特的标语。我们提出了一种新的方法，利用预训练的 transformer T5 模型，并在新的 1:N 匹配对数据集上加入噪音诱导。这种方法可以为生成独特和流畅的标语做出贡献。此外，我们的方法还会 incorporate 公司和品牌的描述信息到标语的生成中。我们根据 ROUGE1、ROUGEL 和 Kosine 相似度度量来评估生成的标语，并通过人类评审者对标语的独特性、 coherence 和流畅性进行评估。结果表明，我们的方法在基eline模型和 transformer-based 模型之上表现更好。
</details></li>
</ul>
<hr>
<h2 id="Excision-and-Recovery-Enhancing-Surface-Anomaly-Detection-with-Attention-based-Single-Deterministic-Masking"><a href="#Excision-and-Recovery-Enhancing-Surface-Anomaly-Detection-with-Attention-based-Single-Deterministic-Masking" class="headerlink" title="Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking"></a>Excision and Recovery: Enhancing Surface Anomaly Detection with Attention-based Single Deterministic Masking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04010">http://arxiv.org/abs/2310.04010</a></li>
<li>repo_url: None</li>
<li>paper_authors: YeongHyeon Park, Sungho Kang, Myung Jin Kim, Yeonho Lee, Juneho Yi</li>
<li>for: 这篇论文的目的是提出一种基于剪辑和恢复的异常检测方法，以解决Surface Inspection中的量度不均匀问题。</li>
<li>methods: 这篇论文使用了一个单检定的剪辑方法，即隐藏可疑异常区域并将其剪辑出来，以导致恢复模型的复原错误较大。</li>
<li>results: 实验结果显示，提出的EAR模型在一个常用的Surface Inspection数据集KolektorSDD2上，具有较好的异常检测性和更高的处理速率，相比于现有的方法。<details>
<summary>Abstract</summary>
Anomaly detection (AD) in surface inspection is an essential yet challenging task in manufacturing due to the quantity imbalance problem of scarce abnormal data. To overcome the above, a reconstruction encoder-decoder (ED) such as autoencoder or U-Net which is trained with only anomaly-free samples is widely adopted, in the hope that unseen abnormals should yield a larger reconstruction error than normal. Over the past years, researches on self-supervised reconstruction-by-inpainting have been reported. They mask out suspected defective regions for inpainting in order to make them invisible to the reconstruction ED to deliberately cause inaccurate reconstruction for abnormals. However, their limitation is multiple random masking to cover the whole input image due to defective regions not being known in advance. We propose a novel reconstruction-by-inpainting method dubbed Excision and Recovery (EAR) that features single deterministic masking. For this, we exploit a pre-trained spatial attention model to predict potential suspected defective regions that should be masked out. We also employ a variant of U-Net as our ED to further limit the reconstruction ability of the U-Net model for abnormals, in which skip connections of different layers can be selectively disabled. In the training phase, all the skip connections are switched on to fully take the benefits from the U-Net architecture. In contrast, for inferencing, we only keep deeper skip connections with shallower connections off. We validate the effectiveness of EAR using an MNIST pre-trained attention for a commonly used surface AD dataset, KolektorSDD2. The experimental results show that EAR achieves both better AD performance and higher throughput than state-of-the-art methods. We expect that the proposed EAR model can be widely adopted as training and inference strategies for AD purposes.
</details>
<details>
<summary>摘要</summary>
anomaly detection (AD) 在表面检查中是一项重要但困难的任务，因为数据异常的量不均。为了解决这个问题，广泛采用一种重建编码器-解码器（ED），例如自动编码器或 U-Net，这些模型在只有正常样本上训练时具有良好的性能。在过去几年中，关于自我超级重建-填充的研究得到了报道。它们会将受到怀疑的缺陷区域填充，以便使其在重建过程中变得不可见，从而让异常样本的重建结果更加准确。然而，这些方法的限制是需要多个随机的填充，因为缺陷区域没有被预先知道。我们提出了一种新的重建-填充方法，称为摘除和恢复（EAR）。这种方法具有单个决定性的填充。我们利用一个预训练的空间注意力模型来预测可能的缺陷区域，并使用一种变体的 U-Net 作为我们的 ED。在训练阶段，所有的跳跃连接都被打开，以便完全利用 U-Net 体系的优势。在推理阶段，我们只保留更深层的跳跃连接，使得更浅层的跳跃连接被关闭。我们验证了 EAR 方法的有效性，使用 MNIST 预训练注意力模型，并在常用的表面异常检测数据集 KolektorSDD2 上进行了实验。实验结果表明，EAR 方法在异常检测性能和通过率方面都超过了当前的方法。我们预计，提出的 EAR 模型将广泛采用作为训练和推理策略。
</details></li>
</ul>
<hr>
<h2 id="Fast-Neighborhood-Search-Heuristics-for-the-Colorful-Bin-Packing-Problem"><a href="#Fast-Neighborhood-Search-Heuristics-for-the-Colorful-Bin-Packing-Problem" class="headerlink" title="Fast Neighborhood Search Heuristics for the Colorful Bin Packing Problem"></a>Fast Neighborhood Search Heuristics for the Colorful Bin Packing Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04471">http://arxiv.org/abs/2310.04471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/renanfernandofranco/fast-neighborhood-search-heuristics-for-the-colorful-bin-packing-problem">https://gitlab.com/renanfernandofranco/fast-neighborhood-search-heuristics-for-the-colorful-bin-packing-problem</a></li>
<li>paper_authors: Renan F. F. da Silva, Yulle G. F. Borges, Rafael C. S. Schouery</li>
<li>for: 解决颜色分别压缩问题 (Colorful Bin Packing Problem, CBPP)，这是加载问题 (Bin Packing Problem, BPP) 的扩展。</li>
<li>methods: 提出了加载问题的有效规则和新规则，以及一种基于变量邻域搜索 (Variable Neighborhood Search, VNS) 和精算法 (Greedy Randomized Adaptive Search, GRASP) 的协同搜索策略。</li>
<li>results: 结果表明我们的协同策略比 VNS 更高效，并且两种方法都可以解决许多元素的实例，即使实际上有很多元素。<details>
<summary>Abstract</summary>
The Colorful Bin Packing Problem (CBPP) is a generalization of the Bin Packing Problem (BPP). The CBPP consists of packing a set of items, each with a weight and a color, in bins of limited capacity, minimizing the number of used bins and satisfying the constraint that two items of the same color cannot be packed side by side in the same bin. In this article, we proposed an adaptation of BPP heuristics and new heuristics for the CBPP. Moreover, we propose a set of fast neighborhood search algorithms for CBPP. These neighborhoods are applied in a meta-heuristic approach based on the Variable Neighborhood Search (VNS) and a matheuristic approach that mixes linear programming with the meta-heuristics VNS and Greedy Randomized Adaptive Search (GRASP). The results indicate that our matheuristic is superior to VNS and that both approaches can find near-optimal solutions for a large number instances, even for instances with many items.
</details>
<details>
<summary>摘要</summary>
《彩色箱包问题（CBPP）》是对《箱包问题（BPP）》的推广。CBPP包括将一组物品，每个物品具有重量和颜色，打包在容量有限的箱中，最小化使用箱的数量，并满足两个同色物品不能在同一箱中并列的约束。在这篇文章中，我们提出了BPP的适应和新的CBPP启发式。此外，我们还提出了一组快速邻居搜索算法 дляCBPP。这些邻居是在基于变量邻居搜索（VNS）和精确优化的GRASP meta-heuristic方法中应用的。结果表明，我们的矩阵启发式在VNS的基础上实现了更好的性能，而两种方法都可以对大量实例进行优化。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Multi-Marginal-Optimal-Transport-for-Network-Alignment"><a href="#Hierarchical-Multi-Marginal-Optimal-Transport-for-Network-Alignment" class="headerlink" title="Hierarchical Multi-Marginal Optimal Transport for Network Alignment"></a>Hierarchical Multi-Marginal Optimal Transport for Network Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04470">http://arxiv.org/abs/2310.04470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichen Zeng, Boxin Du, Si Zhang, Yinglong Xia, Zhining Liu, Hanghang Tong</li>
<li>for: Multi-network alignment, an essential prerequisite for joint learning on multiple networks.</li>
<li>methods: Hierarchical multi-marginal optimal transport framework (HOT) with fused Gromov-Wasserstein (FGW) barycenter and generalized multi-marginal FGW distance.</li>
<li>results: Significant improvements over the state-of-the-art in both effectiveness and scalability.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文主要用于多个网络之间的对应关系找索，也就是多网络学习的先修。</li>
<li>methods: 我们提出了一种层次多参数最优运输框架（HOT），通过粘合Gromov-Wasserstein（FGW）中心来划分多个网络，并基于多参数FGW距离来对多个网络进行对应关系找索。</li>
<li>results: 我们的HOT方法在效果和可扩展性两个方面具有显著的改善，比如当前的状态OFTHEART。<details>
<summary>Abstract</summary>
Finding node correspondence across networks, namely multi-network alignment, is an essential prerequisite for joint learning on multiple networks. Despite great success in aligning networks in pairs, the literature on multi-network alignment is sparse due to the exponentially growing solution space and lack of high-order discrepancy measures. To fill this gap, we propose a hierarchical multi-marginal optimal transport framework named HOT for multi-network alignment. To handle the large solution space, multiple networks are decomposed into smaller aligned clusters via the fused Gromov-Wasserstein (FGW) barycenter. To depict high-order relationships across multiple networks, the FGW distance is generalized to the multi-marginal setting, based on which networks can be aligned jointly. A fast proximal point method is further developed with guaranteed convergence to a local optimum. Extensive experiments and analysis show that our proposed HOT achieves significant improvements over the state-of-the-art in both effectiveness and scalability.
</details>
<details>
<summary>摘要</summary>
找到多个网络之间的节点对应关系，即多网络对Alignment，是 JOINT learning 多网络的重要前提。despite 在多网络对Alignment 方面取得了很大的成功，文献中关于多网络对Alignment 的研究相对落后，这是因为解决多网络对Alignment 问题的解空间是 exponential 增长的，而且缺乏高阶差异度量。为了填补这一漏洞，我们提出了一种基于层次多margin optimal transport 框架的 HOT 方法，用于多网络对Alignment。为了处理庞大的解空间，我们将多个网络 decomposed 成更小的对应集合，使用 Fused Gromov-Wasserstein (FGW) 中心进行aligned clustering。此外，我们将 FGW 距离推广到多margin  Setting，以便在多个网络之间 depict 高阶关系。基于这些推广的 FGW 距离，我们可以将多个网络进行 JOINT 对Alignment。进一步，我们开发了一种快速的 proximal point method，并证明其 convergence 是一个本地最优点。广泛的实验和分析表明，我们的提出的 HOT 方法可以在效果和可扩展性两个方面取得显著的改进。
</details></li>
</ul>
<hr>
<h2 id="CUPre-Cross-domain-Unsupervised-Pre-training-for-Few-Shot-Cell-Segmentation"><a href="#CUPre-Cross-domain-Unsupervised-Pre-training-for-Few-Shot-Cell-Segmentation" class="headerlink" title="CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation"></a>CUPre: Cross-domain Unsupervised Pre-training for Few-Shot Cell Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03981">http://arxiv.org/abs/2310.03981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weibin Liao, Xuhong Li, Qingzhong Wang, Yanwu Xu, Zhaozheng Yin, Haoyi Xiong</li>
<li>for: 这篇论文的目的是提出一种几何对称预训练方法（CUPre），以便在几何预训练中将通用物品识别和实例分类的能力转移到细胞的视觉领域，并且仅使用少量标注的细胞图像进行预训练。</li>
<li>methods: 这篇论文使用的方法包括：1) alternate multi-task pre-training（AMT2），2) 热度对称对抗学习（MoCo），3) 实例分类。</li>
<li>results: 实验结果显示，使用CUPre进行预训练后，在几何细胞分类和检测任务中，可以实现高精度的结果，比较 existing pre-training 方法更高。<details>
<summary>Abstract</summary>
While pre-training on object detection tasks, such as Common Objects in Contexts (COCO) [1], could significantly boost the performance of cell segmentation, it still consumes on massive fine-annotated cell images [2] with bounding boxes, masks, and cell types for every cell in every image, to fine-tune the pre-trained model. To lower the cost of annotation, this work considers the problem of pre-training DNN models for few-shot cell segmentation, where massive unlabeled cell images are available but only a small proportion is annotated. Hereby, we propose Cross-domain Unsupervised Pre-training, namely CUPre, transferring the capability of object detection and instance segmentation for common visual objects (learned from COCO) to the visual domain of cells using unlabeled images. Given a standard COCO pre-trained network with backbone, neck, and head modules, CUPre adopts an alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in every iteration of pre-training, AMT2 first trains the backbone with cell images from multiple cell datasets via unsupervised momentum contrastive learning (MoCo) [3], and then trains the whole model with vanilla COCO datasets via instance segmentation. After pre-training, CUPre fine-tunes the whole model on the cell segmentation task using a few annotated images. We carry out extensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4] datasets in few-shot instance segmentation settings. The experiment shows that CUPre can outperform existing pre-training methods, achieving the highest average precision (AP) for few-shot cell segmentation and detection.
</details>
<details>
<summary>摘要</summary>
而�PREtraining on object detection tasks, such as Common Objects in Contexts (COCO) [1], can significantly boost the performance of cell segmentation, but it still requires a large amount of fine-annotated cell images [2] with bounding boxes, masks, and cell types for every cell in every image to fine-tune the pre-trained model. To reduce the cost of annotation, this work considers the problem of pre-training deep neural network (DNN) models for few-shot cell segmentation, where massive unlabeled cell images are available but only a small proportion is annotated. Hereby, we propose Cross-domain Unsupervised Pre-training, namely CUPre, which transfers the capability of object detection and instance segmentation for common visual objects (learned from COCO) to the visual domain of cells using unlabeled images. Given a standard COCO pre-trained network with backbone, neck, and head modules, CUPre adopts an alternate multi-task pre-training (AMT2) procedure with two sub-tasks -- in every iteration of pre-training, AMT2 first trains the backbone with cell images from multiple cell datasets via unsupervised momentum contrastive learning (MoCo) [3], and then trains the whole model with vanilla COCO datasets via instance segmentation. After pre-training, CUPre fine-tunes the whole model on the cell segmentation task using a few annotated images. We conduct extensive experiments to evaluate CUPre using LIVECell [2] and BBBC038 [4] datasets in few-shot instance segmentation settings. The experiment shows that CUPre can outperform existing pre-training methods, achieving the highest average precision (AP) for few-shot cell segmentation and detection.
</details></li>
</ul>
<hr>
<h2 id="Perfect-Alignment-May-be-Poisonous-to-Graph-Contrastive-Learning"><a href="#Perfect-Alignment-May-be-Poisonous-to-Graph-Contrastive-Learning" class="headerlink" title="Perfect Alignment May be Poisonous to Graph Contrastive Learning"></a>Perfect Alignment May be Poisonous to Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03977">http://arxiv.org/abs/2310.03977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyu Liu, Huayi Tang, Yong Liu</li>
<li>for: 本文旨在研究图像学习中的增强技术，以及这些技术如何影响下游任务的表现。</li>
<li>methods: 本文使用了图像增强法，包括增强图像的尺寸、颜色、纹理等方面，以及使用了对照学习来验证图像的表现。</li>
<li>results: 研究发现，图像增强法可以提高下游任务的表现，但是需要在增强程度和类型上进行调整。同时，对照学习可以验证图像的表现，但是需要避免过于准确的对照，以免导致模型的泛化能力受到影响。<details>
<summary>Abstract</summary>
Graph Contrastive Learning (GCL) aims to learn node representations by aligning positive pairs and separating negative ones. However, limited research has been conducted on the inner law behind specific augmentations used in graph-based learning. What kind of augmentation will help downstream performance, how does contrastive learning actually influence downstream tasks, and why the magnitude of augmentation matters? This paper seeks to address these questions by establishing a connection between augmentation and downstream performance, as well as by investigating the generalization of contrastive learning. Our findings reveal that GCL contributes to downstream tasks mainly by separating different classes rather than gathering nodes of the same class. So perfect alignment and augmentation overlap which draw all intra-class samples the same can not explain the success of contrastive learning. Then in order to comprehend how augmentation aids the contrastive learning process, we conduct further investigations into its generalization, finding that perfect alignment that draw positive pair the same could help contrastive loss but is poisonous to generalization, on the contrary, imperfect alignment enhances the model's generalization ability. We analyse the result by information theory and graph spectrum theory respectively, and propose two simple but effective methods to verify the theories. The two methods could be easily applied to various GCL algorithms and extensive experiments are conducted to prove its effectiveness.
</details>
<details>
<summary>摘要</summary>
graph对比学习（GCL）目标是通过对正方向对对应的锚点进行对应，以便学习锚点表示。然而，有限的研究已经进行了关于在图基于学习中使用特定的扩充方法的内律。这种扩充方法会帮助下游性能，如何对下游任务产生影响，以及为什么扩充方法的大小重要吗？这篇论文旨在回答这些问题，同时也进行了对扩充和对比学习的总体化研究。我们的发现表明，GCL在下游任务中的主要贡献是将不同类型的锚点分开，而不是将同类型的锚点集中。因此，完美对齐和扩充 overlap，即将所有同类型的锚点都设为同一个样本，无法解释对比学习的成功。为了更好地理解扩充对对比学习的作用，我们进行了进一步的总体研究，发现了以下结论：完美对齐可以帮助对比损失，但是对总体化有毒。相反，不完美对齐可以增强模型的总体化能力。我们通过信息学和图谱论断来分析结果，并提出了两种简单 yet 有效的方法来验证这些理论。这两种方法可以轻松应用于各种GCL算法，并进行了广泛的实验来证明其效果。
</details></li>
</ul>
<hr>
<h2 id="Sub-token-ViT-Embedding-via-Stochastic-Resonance-Transformers"><a href="#Sub-token-ViT-Embedding-via-Stochastic-Resonance-Transformers" class="headerlink" title="Sub-token ViT Embedding via Stochastic Resonance Transformers"></a>Sub-token ViT Embedding via Stochastic Resonance Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03967">http://arxiv.org/abs/2310.03967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dong Lao, Yangchao Wu, Tian Yu Liu, Alex Wong, Stefano Soatto</li>
<li>For: The paper is written to address the issue of quantization artifacts in Vision Transformers (ViTs) and to propose a zero-shot method to improve the handling of spatial quantization in pre-trained ViTs.* Methods: The proposed method, called Stochastic Resonance Transformer (SRT), ensembles the features obtained from perturbing input images via sub-token spatial translations, inspired by Stochastic Resonance. SRT can be applied at any layer, on any task, and does not require any fine-tuning.* Results: The paper shows that SRT can effectively super-resolve features of pre-trained ViTs, capturing more of the local fine-grained structures that might otherwise be neglected as a result of tokenization. SRT outperforms the baseline models by an average of 4.7% and 14.9% on the RMSE and RMSE-log metrics across three different architectures for monocular depth prediction, and by an average of 2.4% in F&amp;J score for semi-supervised video object segmentation. Additionally, the paper shows that SRT improves upon the base model by an average of 2.1% on the maxF metric for unsupervised salient region segmentation, and yields consistent improvements of up to 2.6% and 1.0% respectively for image retrieval and object discovery.<details>
<summary>Abstract</summary>
We discover the presence of quantization artifacts in Vision Transformers (ViTs), which arise due to the image tokenization step inherent in these architectures. These artifacts result in coarsely quantized features, which negatively impact performance, especially on downstream dense prediction tasks. We present a zero-shot method to improve how pre-trained ViTs handle spatial quantization. In particular, we propose to ensemble the features obtained from perturbing input images via sub-token spatial translations, inspired by Stochastic Resonance, a method traditionally applied to climate dynamics and signal processing. We term our method ``Stochastic Resonance Transformer" (SRT), which we show can effectively super-resolve features of pre-trained ViTs, capturing more of the local fine-grained structures that might otherwise be neglected as a result of tokenization. SRT can be applied at any layer, on any task, and does not require any fine-tuning. The advantage of the former is evident when applied to monocular depth prediction, where we show that ensembling model outputs are detrimental while applying SRT on intermediate ViT features outperforms the baseline models by an average of 4.7% and 14.9% on the RMSE and RMSE-log metrics across three different architectures. When applied to semi-supervised video object segmentation, SRT also improves over the baseline models uniformly across all metrics, and by an average of 2.4% in F&J score. We further show that these quantization artifacts can be attenuated to some extent via self-distillation. On the unsupervised salient region segmentation, SRT improves upon the base model by an average of 2.1% on the maxF metric. Finally, despite operating purely on pixel-level features, SRT generalizes to non-dense prediction tasks such as image retrieval and object discovery, yielding consistent improvements of up to 2.6% and 1.0% respectively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Thought-Propagation-An-Analogical-Approach-to-Complex-Reasoning-with-Large-Language-Models"><a href="#Thought-Propagation-An-Analogical-Approach-to-Complex-Reasoning-with-Large-Language-Models" class="headerlink" title="Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models"></a>Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03965">http://arxiv.org/abs/2310.03965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junchi Yu, Ran He, Rex Ying</li>
<li>for: 提高大型自然语言模型（LLM）的复杂理解能力</li>
<li>methods: 利用相似问题的解决方案和推理策略增强LLM的推理能力</li>
<li>results: 相比基eline，TP提高了短路推理任务的优化解决率12%，创作写作任务的人类偏好提高13%，LLM-Agent规划任务的完成率提高15%<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textit{from scratch}. To address these issues, we propose \textbf{\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\% improvement of human preference in Creative Writing, and 15\% enhancement in the task completion rate of LLM-Agent Planning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在逻辑任务中取得了杰出的成功，但现有的提示方法不能重复这些成果和受到累累逻辑的影响，因为它们将LLM训练自scratch。为了解决这些问题，我们提出了“思维传播”（TP），它探索相关的问题和将其解决方案传播到LLM中，以增强它们的复杂逻辑能力。这些相关问题的解决方案可以重复使用，因此可以将这些问题的解决方案传播到LLM中，以激发它们对新的问题的解决。TP可以与现有的提示方法相容，允许插件和改进在广泛的任务中，而不需要大量的问题特定的提示工程。实验结果显示，TP与基准相比，平均提高了12%的最佳解决方案找到率，13%的人类偏好在创意写作中，和15%的任务完成率。
</details></li>
</ul>
<hr>
<h2 id="A-Learnable-Counter-condition-Analysis-Framework-for-Functional-Connectivity-based-Neurological-Disorder-Diagnosis"><a href="#A-Learnable-Counter-condition-Analysis-Framework-for-Functional-Connectivity-based-Neurological-Disorder-Diagnosis" class="headerlink" title="A Learnable Counter-condition Analysis Framework for Functional Connectivity-based Neurological Disorder Diagnosis"></a>A Learnable Counter-condition Analysis Framework for Functional Connectivity-based Neurological Disorder Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03964">http://arxiv.org/abs/2310.03964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/es-kang/learnable-counter-condition-FC">https://github.com/es-kang/learnable-counter-condition-FC</a></li>
<li>paper_authors: Eunsong Kang, Da-woon Heo, Jiwon Lee, Heung-Il Suk</li>
<li>for: 这研究旨在了解 neuroscience 疾病的生物特征，通过深度学习模型进行识别疾病并进行后续分析以找出疾病相关的生物标志物。</li>
<li>methods: 我们提出了一种新的一体化框架，它将诊断和特征提取集成在一起，并提出了一种适应性注意力网络来实现特征选择。我们还提出了一种功能网络关系编码器，它可以捕捉全局的功能连接 topological 性，而不需要先定义函数网络之间的边。</li>
<li>results: 我们的框架可以提供更高的诊断精度和解释力，并且通过对抗条件分析来描述疾病相关的神经生物学特征。我们使用了大量的 REST-meta-MDD 和 ABIDE 数据集，并证明了我们的框架在疾病识别方面的优异性。<details>
<summary>Abstract</summary>
To understand the biological characteristics of neurological disorders with functional connectivity (FC), recent studies have widely utilized deep learning-based models to identify the disease and conducted post-hoc analyses via explainable models to discover disease-related biomarkers. Most existing frameworks consist of three stages, namely, feature selection, feature extraction for classification, and analysis, where each stage is implemented separately. However, if the results at each stage lack reliability, it can cause misdiagnosis and incorrect analysis in afterward stages. In this study, we propose a novel unified framework that systemically integrates diagnoses (i.e., feature selection and feature extraction) and explanations. Notably, we devised an adaptive attention network as a feature selection approach to identify individual-specific disease-related connections. We also propose a functional network relational encoder that summarizes the global topological properties of FC by learning the inter-network relations without pre-defined edges between functional networks. Last but not least, our framework provides a novel explanatory power for neuroscientific interpretation, also termed counter-condition analysis. We simulated the FC that reverses the diagnostic information (i.e., counter-condition FC): converting a normal brain to be abnormal and vice versa. We validated the effectiveness of our framework by using two large resting-state functional magnetic resonance imaging (fMRI) datasets, Autism Brain Imaging Data Exchange (ABIDE) and REST-meta-MDD, and demonstrated that our framework outperforms other competing methods for disease identification. Furthermore, we analyzed the disease-related neurological patterns based on counter-condition analysis.
</details>
<details>
<summary>摘要</summary>
为了理解神经疾病的生物特征，现有研究广泛采用深度学习基本模型来识别疾病，并通过可解释模型来找出疾病相关的生物标志物。现有的框架通常包括三个阶段，即特征选择、特征提取 для分类和分析，每个阶段都是独立实现的。然而，如果每个阶段的结果无法靠拢，可能会导致诊断和错误分析。在这种情况下，我们提出了一种新的一体化框架，系统地结合诊断和解释。特别是，我们设计了适应性注意力网络作为特征选择方法，以特定疾病相关的连接进行个体特定的诊断。此外，我们提出了功能网络关系编码器，以学习无定义的函数网络关系，从而捕捉全局的 topological 特征。最后，我们的框架还提供了一种新的解释力，即对神经科学的解释，也称为对 condition 分析。我们在FC中反转诊断信息（i.e., counter-condition FC），将正常脑变异常，并将异常脑变正常。我们使用了两个大的休息态功能磁共振成像（fMRI）数据集，ABIDE 和 REST-meta-MDD，并证明了我们的框架在疾病识别方面高效。此外，我们还分析了疾病相关的神经学特征。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Natural-Language-Inference-for-Reducing-Large-Language-Model-Ungrounded-Hallucinations"><a href="#Chain-of-Natural-Language-Inference-for-Reducing-Large-Language-Model-Ungrounded-Hallucinations" class="headerlink" title="Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations"></a>Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03951">http://arxiv.org/abs/2310.03951</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/conli_hallucination">https://github.com/microsoft/conli_hallucination</a></li>
<li>paper_authors: Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal</li>
<li>for: 这篇论文是为了探讨大型自然语言模型（LLMs）如何检测和解决它们生成的幻视文本。</li>
<li>methods: 这篇论文提出了一个几何构造，用于检测和解决 LLMs 生成的幻视文本。这个构造使用了自然语言推理链（CoNLI）来检测幻视文本，并通过后期重写来实现幻视文本的减少。</li>
<li>results: 这篇论文的结果显示，这个几何构造可以实现高度的幻视文本检测和减少，并且可以在不需要精确调整或域专标语言调整的情况下，实现文本质量的提高。<details>
<summary>Abstract</summary>
Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）可以生成流畅的自然语言文本，当给定相关的文档作为背景上下文时。这种能力吸引了产业应用的广泛关注。然而，LLM容易产生没有文档支持的幻见。在这篇论文中，我们提出了一种层次结构的检测和 Mitigate 幻见的框架。我们的框架使用自然语言推理链（CoNLI） для幻见检测和幻见减少via重写。我们的方法实现了状态艺术的幻见检测性能和提高文本质量通过重写，无需任何细化或域特定的提问工程。我们展示了这个简单的插件和玩家可以作为有效的幻见检测和减少选择，在不同的上下文中实现竞争性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/cs.AI_2023_10_06/" data-id="closbroku00590g887zyz00m0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/cs.CL_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T11:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/cs.CL_2023_10_06/">cs.CL - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="From-Nuisance-to-News-Sense-Augmenting-the-News-with-Cross-Document-Evidence-and-Context"><a href="#From-Nuisance-to-News-Sense-Augmenting-the-News-with-Cross-Document-Evidence-and-Context" class="headerlink" title="From Nuisance to News Sense: Augmenting the News with Cross-Document Evidence and Context"></a>From Nuisance to News Sense: Augmenting the News with Cross-Document Evidence and Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04592">http://arxiv.org/abs/2310.04592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeremiah Milbauer, Ziqi Ding, Zhijin Wu, Tongshuang Wu</li>
<li>for: 帮助用户更好地理解新闻故事，避免受到各种信息来源的混乱和谣言的影响。</li>
<li>methods: 使用referenceless fact verification技术，将多种新闻articles integrate into一个中心主题，并在文章中提供相关的补充信息和鉴别信息。</li>
<li>results: 在试验中，NEWSSENSE能够帮助用户更好地找到关键信息，验证新闻文章的准确性，并探索不同的视角。<details>
<summary>Abstract</summary>
Reading and understanding the stories in the news is increasingly difficult. Reporting on stories evolves rapidly, politicized news venues offer different perspectives (and sometimes different facts), and misinformation is rampant. However, existing solutions merely aggregate an overwhelming amount of information from heterogenous sources, such as different news outlets, social media, and news bias rating agencies. We present NEWSSENSE, a novel sensemaking tool and reading interface designed to collect and integrate information from multiple news articles on a central topic, using a form of reference-free fact verification. NEWSSENSE augments a central, grounding article of the user's choice by linking it to related articles from different sources, providing inline highlights on how specific claims in the chosen article are either supported or contradicted by information from other articles. Using NEWSSENSE, users can seamlessly digest and cross-check multiple information sources without disturbing their natural reading flow. Our pilot study shows that NEWSSENSE has the potential to help users identify key information, verify the credibility of news articles, and explore different perspectives.
</details>
<details>
<summary>摘要</summary>
阅读和理解新闻故事越来越Difficult。新闻报道不断发展，政治化的新闻场景提供不同的视角（有时还有不同的事实），而且谣言游走在社交媒体上。然而，现有的解决方案只是将过载量的信息从多个来源集中到一起，如不同的新闻报道、社交媒体和新闻偏见评级机构。我们介绍NEWSSENSE，一种新的意义感知工具和阅读界面，可以收集和结合多篇关于同一主题的新闻文章，使用参照文章的无需准备。NEWSSENSE将用户选择的中心文章与不同来源的相关文章集成，并在文章中提供 inline 高亮，以显示特定文章中的声明是否由其他文章中的信息支持或否认。使用NEWSSENSE，用户可以无需干扰自然阅读流程，轻松摄取和检查多个信息源。我们的试点研究表明，NEWSSENSE有助于用户发现关键信息、验证新闻文章的可靠性和探索不同的视角。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Information-in-Text-Explanations"><a href="#Measuring-Information-in-Text-Explanations" class="headerlink" title="Measuring Information in Text Explanations"></a>Measuring Information in Text Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04557">http://arxiv.org/abs/2310.04557</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klonnet23/helloy-word">https://github.com/klonnet23/helloy-word</a></li>
<li>paper_authors: Zining Zhu, Frank Rudzicz</li>
<li>for: 这篇论文的目的是为了探讨文本解释的评估方法，以及两种流行的文本解释方法的评估标准。</li>
<li>methods: 这篇论文使用信息理论的框架来评估文本解释的质量，并量化了文本解释的信息流动。</li>
<li>results: 研究发现，使用信息理论的评估方法可以帮助评估文本解释的质量，并且可以揭示不同文本解释方法的下面机制。例如，NLEs在传输输入相关信息和目标相关信息之间存在一定的权衡，而 rationales 则不具有这种机制。<details>
<summary>Abstract</summary>
Text-based explanation is a particularly promising approach in explainable AI, but the evaluation of text explanations is method-dependent. We argue that placing the explanations on an information-theoretic framework could unify the evaluations of two popular text explanation methods: rationale and natural language explanations (NLE). This framework considers the post-hoc text pipeline as a series of communication channels, which we refer to as ``explanation channels''. We quantify the information flow through these channels, thereby facilitating the assessment of explanation characteristics. We set up tools for quantifying two information scores: relevance and informativeness. We illustrate what our proposed information scores measure by comparing them against some traditional evaluation metrics. Our information-theoretic scores reveal some unique observations about the underlying mechanisms of two representative text explanations. For example, the NLEs trade-off slightly between transmitting the input-related information and the target-related information, whereas the rationales do not exhibit such a trade-off mechanism. Our work contributes to the ongoing efforts in establishing rigorous and standardized evaluation criteria in the rapidly evolving field of explainable AI.
</details>
<details>
<summary>摘要</summary>
文本基本解释方法在可解释AI中具有极高的潜力，但评估文本解释的方法受到方法的限制。我们认为将文本解释放在信息理论框架上可以统一两种受欢迎的文本解释方法：理由和自然语言解释（NLE）的评估。这个框架视文本解释管道为一系列通信频道，我们称之为“解释频道”。我们量化这些频道中的信息流动，从而促进解释特征的评估。我们设立了量化两种信息分数的工具：相关性和启示性。我们explain了我们所提出的信息分数是什么，并与传统评估指标进行比较。我们的信息理论分数表明了两种文本解释的下面机制。例如，NLEs在传递输入相关信息和目标相关信息之间进行了微妙的 equilibrio，而 rationales没有这种平衡机制。我们的工作贡献到了在可解释AI领域积极发展的评估标准化的努力。
</details></li>
</ul>
<hr>
<h2 id="Module-wise-Adaptive-Distillation-for-Multimodality-Foundation-Models"><a href="#Module-wise-Adaptive-Distillation-for-Multimodality-Foundation-Models" class="headerlink" title="Module-wise Adaptive Distillation for Multimodality Foundation Models"></a>Module-wise Adaptive Distillation for Multimodality Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04550">http://arxiv.org/abs/2310.04550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liang, Jiahui Yu, Ming-Hsuan Yang, Matthew Brown, Yin Cui, Tuo Zhao, Boqing Gong, Tianyi Zhou</li>
<li>for: 这个研究旨在提高预训练多Modal基础模型的可靠性和可扩展性，通过将大型教师模型转易为小型学习模型。</li>
<li>methods: 这个研究使用了层刻散布法，将大型教师模型中的各层数据转易为小型学习模型中的各层数据，以提高学习模型的可靠性和可扩展性。</li>
<li>results: 这个研究透过实验发现，使用OPTIMA算法可以将模型更好地调整，从而提高预训练多Modal基础模型的可靠性和可扩展性。<details>
<summary>Abstract</summary>
Pre-trained multimodal foundation models have demonstrated remarkable generalizability but pose challenges for deployment due to their large sizes. One effective approach to reducing their sizes is layerwise distillation, wherein small student models are trained to match the hidden representations of large teacher models at each layer. Motivated by our observation that certain architecture components, referred to as modules, contribute more significantly to the student's performance than others, we propose to track the contributions of individual modules by recording the loss decrement after distillation each module and choose the module with a greater contribution to distill more frequently. Such an approach can be naturally formulated as a multi-armed bandit (MAB) problem, where modules and loss decrements are considered as arms and rewards, respectively. We then develop a modified-Thompson sampling algorithm named OPTIMA to address the nonstationarity of module contributions resulting from model updating. Specifically, we leverage the observed contributions in recent history to estimate the changing contribution of each module and select modules based on these estimations to maximize the cumulative contribution. We evaluate the effectiveness of OPTIMA through distillation experiments on various multimodal understanding and image captioning tasks, using the CoCa-Large model (Yu et al., 2022) as the teacher model.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的多Modal基础模型已经表现出了惊人的通用性，但是它们的大小却带来了部署的挑战。一种有效的减少大小的方法是层WISE的distillation，其中小的学生模型在每层都被训练以匹配大的教师模型的隐藏表示。我们发现了一些architecture组件，被称为模块，在学生的性能中发挥了更大的作用，我们因此提议在distillation过程中跟踪这些模块的贡献。我们可以将这种方法形式化为多重投掷（MAB）问题，其中模块和loss减掉被视为手中的武器和奖励，分别。我们然后开发了一种修改后Thompson投掷算法，名为OPTIMA，以解决模块贡献的不平等。我们利用了最近历史中每个模块的贡献 Observation，来估算每个模块的变化贡献，并根据这些估算选择模块，以最大化总贡献。我们通过在多种多Modal理解和图像描述任务上进行distillation实验，使用Yu et al.（2022）的CoCa-Large模型作为教师模型，证明OPTIMA的有效性。
</details></li>
</ul>
<hr>
<h2 id="Envisioning-Narrative-Intelligence-A-Creative-Visual-Storytelling-Anthology"><a href="#Envisioning-Narrative-Intelligence-A-Creative-Visual-Storytelling-Anthology" class="headerlink" title="Envisioning Narrative Intelligence: A Creative Visual Storytelling Anthology"></a>Envisioning Narrative Intelligence: A Creative Visual Storytelling Anthology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04529">http://arxiv.org/abs/2310.04529</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/USArmyResearchLab/ARL-Creative-Visual-Storytelling">https://github.com/USArmyResearchLab/ARL-Creative-Visual-Storytelling</a></li>
<li>paper_authors: Brett A. Halperin, Stephanie M. Lukin</li>
<li>for: 这篇论文收集了100个视觉故事作者参与的系统创作过程中的图片序列，并进行了密切的阅读和主题分析，描述了5种在视觉故事创作过程中出现的变化：（1）描述vs预测；（2）动态特征实体&#x2F;物体；（3）感受场景信息；（4）调整情感；（5）编码故事偏见。</li>
<li>methods: 这篇论文采用了密切的阅读和主题分析来研究视觉故事创作过程中的变化。</li>
<li>results: 这篇论文描述了5种在视觉故事创作过程中出现的变化，并提出了计算视觉故事创作的智能 критерионов：创作、可靠、表达、固有和负责任。<details>
<summary>Abstract</summary>
In this paper, we collect an anthology of 100 visual stories from authors who participated in our systematic creative process of improvised story-building based on image sequences. Following close reading and thematic analysis of our anthology, we present five themes that characterize the variations found in this creative visual storytelling process: (1) Narrating What is in Vision vs. Envisioning; (2) Dynamically Characterizing Entities/Objects; (3) Sensing Experiential Information About the Scenery; (4) Modulating the Mood; (5) Encoding Narrative Biases. In understanding the varied ways that people derive stories from images, we offer considerations for collecting story-driven training data to inform automatic story generation. In correspondence with each theme, we envision narrative intelligence criteria for computational visual storytelling as: creative, reliable, expressive, grounded, and responsible. From these criteria, we discuss how to foreground creative expression, account for biases, and operate in the bounds of visual storyworlds.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们收集了100个视觉故事作者参与我们系统化的创作过程中的故事建构，并进行了仔细的阅读和主题分析。根据我们的分析，我们发现了5种主题，它们描述了在这种创作过程中变化的方式：1. 描述视野中的事物 vs. 预测未来的情境2. 动态 caracterize entities/objects3. 感受景色中的情感信息4. 调节情感5. 编码故事偏见在理解人们如何从图像中获得故事的过程中，我们提供了收集故事驱动的训练数据的考虑事项，以 Inform automatic story generation。与每个主题相对应，我们提出了计算视觉故事创作的智能准则：创造力、可靠性、表达力、基于现实的、负责任。从这些准则中，我们讨论了如何强调创作表达，考虑偏见，并在视觉故事世界中运行。
</details></li>
</ul>
<hr>
<h2 id="RECOMP-Improving-Retrieval-Augmented-LMs-with-Compression-and-Selective-Augmentation"><a href="#RECOMP-Improving-Retrieval-Augmented-LMs-with-Compression-and-Selective-Augmentation" class="headerlink" title="RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation"></a>RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04408">http://arxiv.org/abs/2310.04408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/carriex/recomp">https://github.com/carriex/recomp</a></li>
<li>paper_authors: Fangyuan Xu, Weijia Shi, Eunsol Choi</li>
<li>for: 提高语言模型（LM）在各种任务上的表现，如语言模型化和开放问答任务。</li>
<li>methods: 使用各种文档检索和简要生成技术，如提取式压缩器和抽象压缩器，将检索到的文档简化为短文本摘要，以提高LM的表现。</li>
<li>results: 在语言模型化任务和开放问答任务中，实现了6%的压缩率，而无需 sacrifiSing表现质量，并且可以在不同的LM上进行模型转换。<details>
<summary>Abstract</summary>
Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation.We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.
</details>
<details>
<summary>摘要</summary>
LMs的表现可以通过在推理时预先附加文档来提高表现，但这些文档经常 span hundreds of words，使推理成本增加substantially。我们提议将检索到的文档压缩成短文档摘要，以降低计算成本并使LM不必检索长文档中重要信息。我们提出了两种压缩器：一种是提取用于检索到的文档中有用句子的抽取压缩器，另一种是通过将多个文档的信息合并来生成摘要的抽取压缩器。两种压缩器都是根据LM在输入中预先附加摘要来提高LM的表现，而且保持摘要简洁。如果检索到的文档与输入无关或无法提供LM任何新信息，我们的压缩器可以返回空串，实现选择性的扩展。我们在语言模型任务和开放问题 answering任务中评估了我们的方法，实现了 compression rate as low as 6% ，与传统摘要模型相比，表现出明显的提升。我们还证明了我们的压缩器可以在不同LM上进行转移，并为推理任务提供大致 faithful 的摘要。
</details></li>
</ul>
<hr>
<h2 id="Improving-Stability-in-Simultaneous-Speech-Translation-A-Revision-Controllable-Decoding-Approach"><a href="#Improving-Stability-in-Simultaneous-Speech-Translation-A-Revision-Controllable-Decoding-Approach" class="headerlink" title="Improving Stability in Simultaneous Speech Translation: A Revision-Controllable Decoding Approach"></a>Improving Stability in Simultaneous Speech Translation: A Revision-Controllable Decoding Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04399">http://arxiv.org/abs/2310.04399</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junkun Chen, Jian Xue, Peidong Wang, Jing Pan, Jinyu Li</li>
<li>for: 本研究旨在解决同时扩展语言翻译中的精度稳定问题，尤其是在实时通信中。</li>
<li>methods: 本研究提出了一种新的修订控制方法，通过在搜索剔除过程中设置允许的修订窗口，以避免选择需要大量修订的候选翻译，从而减少了精度稳定问题。</li>
<li>results: 实验结果表明，提议的方法可以重要地改善翻译稳定性，而无需妥协substantially translation质量。<details>
<summary>Abstract</summary>
Simultaneous Speech-to-Text translation serves a critical role in real-time crosslingual communication. Despite the advancements in recent years, challenges remain in achieving stability in the translation process, a concern primarily manifested in the flickering of partial results. In this paper, we propose a novel revision-controllable method designed to address this issue. Our method introduces an allowed revision window within the beam search pruning process to screen out candidate translations likely to cause extensive revisions, leading to a substantial reduction in flickering and, crucially, providing the capability to completely eliminate flickering. The experiments demonstrate the proposed method can significantly improve the decoding stability without compromising substantially on the translation quality.
</details>
<details>
<summary>摘要</summary>
simultaneous 语音到文本翻译在实时跨语言通信中发挥关键作用。 despite recent advances, there are still challenges in achieving stability in the translation process, primarily manifesting as flickering of partial results. in this paper, we propose a novel revision-controllable method to address this issue. our method introduces an allowed revision window within the beam search pruning process to screen out candidate translations likely to cause extensive revisions, leading to a substantial reduction in flickering and, crucially, providing the capability to completely eliminate flickering. the experiments demonstrate that the proposed method can significantly improve decoding stability without compromising translation quality.
</details></li>
</ul>
<hr>
<h2 id="Amortizing-intractable-inference-in-large-language-models"><a href="#Amortizing-intractable-inference-in-large-language-models" class="headerlink" title="Amortizing intractable inference in large language models"></a>Amortizing intractable inference in large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04363">http://arxiv.org/abs/2310.04363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, Nikolay Malkin</li>
<li>for: 这个论文的目的是怎样使用自然语言模型（LLM）来实现知识压缩和数据效果的融合。</li>
<li>methods: 这个论文使用了权重级分布的权重学习算法（GFlowNets）来让LLM进行权重学习，以实现更好的数据效果。</li>
<li>results: 这个论文的实验结果表明，通过使用这种分布匹配方法，LLM可以更好地适应具有多步骤的理智和工具使用的任务。<details>
<summary>Abstract</summary>
Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.
</details>
<details>
<summary>摘要</summary>
自适应大语言模型（LLM）通过下一个元素的 conditional 分布压缩知识从训练数据。这限制了可追踪的知识查询到开始到终止的排序样本。然而，许多有用的任务，包括序列续写、填充和其他受限制的生成任务，涉及到无法解决的 posterior 分布。我们解决这个限制，使用权重学习算法来精心 fine-tune LLM，以实现各种可追踪的 posterior 分布。我们称之为泛化 Bayesian 推理（GFlowNets）。我们经验表明，这种分布匹配方法可以作为最大likelihood 训练和奖励最大化策略的有效替代方案。作为重要应用，我们解释了链条思维为 latent 变量模型问题，并示出我们的方法可以实现数据效率地适应 LLM 到需要多步合理化和工具使用的任务。
</details></li>
</ul>
<hr>
<h2 id="Transferring-speech-generic-and-depression-specific-knowledge-for-Alzheimer’s-disease-detection"><a href="#Transferring-speech-generic-and-depression-specific-knowledge-for-Alzheimer’s-disease-detection" class="headerlink" title="Transferring speech-generic and depression-specific knowledge for Alzheimer’s disease detection"></a>Transferring speech-generic and depression-specific knowledge for Alzheimer’s disease detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04358">http://arxiv.org/abs/2310.04358</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyun Cui, Wen Wu, Wei-Qiang Zhang, Ji Wu, Chao Zhang</li>
<li>for: 本研究旨在提高阿尔茨染色肿瘤病（AD）诊断精度，通过知识传递来解决干预数据稀缺问题。</li>
<li>methods: 本研究使用了知识传递，特别是从普通语音和文本数据预训练的基础模型中提取的语音特征知识，以及高危病症抑郁症诊断任务的知识同时传递。</li>
<li>results: 实验结果表明，提出的方法可以提高AD和抑郁症诊断精度，并在ADReSSo数据集上实现了状态最佳的F1分数0.928。<details>
<summary>Abstract</summary>
The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and depression detection, and produces a state-of-the-art F1 score of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.
</details>
<details>
<summary>摘要</summary>
抑郁症和阿尔茨海默症诊断从自然语言中进行探测已经吸引了越来越多的关注，但训练数据的稀缺性仍然是一个重要的问题。本文通过知识传递来解决这个问题，具体来说是从speech-通用和抑郁症特定的知识中进行传递。本文首先研究了基于大量的语音和文本数据预训练的基础模型，然后对不同的中间块进行块级分析，以实现AD诊断。此外，本文还提出了同时传递speech抑郁症诊断任务的知识，根据抑郁症和AD的高共同发病率。本文提出了并行知识传递框架，并同时学习这两个任务之间的共享信息。实验结果表明，提出的方法可以提高AD和抑郁症的诊断精度，并在常用的ADReSSo数据集上获得了0.928的F1分数，创下了状态精度记录。
</details></li>
</ul>
<hr>
<h2 id="Large-Scale-Korean-Text-Dataset-for-Classifying-Biased-Speech-in-Real-World-Online-Services"><a href="#Large-Scale-Korean-Text-Dataset-for-Classifying-Biased-Speech-in-Real-World-Online-Services" class="headerlink" title="Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services"></a>Large-Scale Korean Text Dataset for Classifying Biased Speech in Real-World Online Services</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04313">http://arxiv.org/abs/2310.04313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dasol-choi/komultitext">https://github.com/dasol-choi/komultitext</a></li>
<li>paper_authors: Dasol Choi, Jooyoung Song, Eunsun Lee, Jinwoo Seo, Heejune Park, Dongbin Na</li>
<li>for: 这篇论文的目的是提出一种新的大规模 hate speech 检测算法，以解决韩国社交媒体平台上的偏见语言问题。</li>
<li>methods: 该论文使用了BERT基于语言模型，并对文本样本进行多任务学习，包括偏见检测、荒情检测和不雅语检测。</li>
<li>results: 该论文的方法可以超越人类水平的准确率，并且可以同时检测多种类型的偏见语言。这些结果可以为实际的 hate speech 和偏见 mitigation 提供实用的解决方案，从而改善在线社区的健康状况。<details>
<summary>Abstract</summary>
With the growth of online services, the need for advanced text classification algorithms, such as sentiment analysis and biased text detection, has become increasingly evident. The anonymous nature of online services often leads to the presence of biased and harmful language, posing challenges to maintaining the health of online communities. This phenomenon is especially relevant in South Korea, where large-scale hate speech detection algorithms have not yet been broadly explored. In this paper, we introduce a new comprehensive, large-scale dataset collected from a well-known South Korean SNS platform. Our proposed dataset provides annotations including (1) Preferences, (2) Profanities, and (3) Nine types of Bias for the text samples, enabling multi-task learning for simultaneous classification of user-generated texts. Leveraging state-of-the-art BERT-based language models, our approach surpasses human-level accuracy across diverse classification tasks, as measured by various metrics. Beyond academic contributions, our work can provide practical solutions for real-world hate speech and bias mitigation, contributing directly to the improvement of online community health. Our work provides a robust foundation for future research aiming to improve the quality of online discourse and foster societal well-being. All source codes and datasets are publicly accessible at https://github.com/Dasol-Choi/KoMultiText.
</details>
<details>
<summary>摘要</summary>
随着在线服务的发展，卷积式文本分类算法，如情感分析和偏见文本检测，的需求日益明显。无名氏的在线服务通常会导致偏见和有害语言的存在，对于维护在线社区的健康带来挑战。这种现象特别在韩国是普遍存在的，在这里，大规模的偏见排除算法还没有广泛探索。在这篇论文中，我们介绍了一个新的全面的大规模数据集，从韩国知名的社交媒体平台收集得到的。我们的提议的数据集包括了（1）偏好、（2）荒唐词汇和（3）九种偏见的注释，使得文本样本可以同时进行多任务学习。利用现代BERT基于语言模型，我们的方法超越人类水平的准确率，在多种 метриках上测试。我们的工作不仅有学术价值，还可以实际地减少在线偏见和偏见，直接提高在线社区的健康。我们的工作提供了对于改善在线讨论质量和促进社会 благополучи性的坚实基础。所有代码和数据集都公开可访问于https://github.com/Dasol-Choi/KoMultiText。
</details></li>
</ul>
<hr>
<h2 id="Written-and-spoken-corpus-of-real-and-fake-social-media-postings-about-COVID-19"><a href="#Written-and-spoken-corpus-of-real-and-fake-social-media-postings-about-COVID-19" class="headerlink" title="Written and spoken corpus of real and fake social media postings about COVID-19"></a>Written and spoken corpus of real and fake social media postings about COVID-19</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04237">http://arxiv.org/abs/2310.04237</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ng Bee Chin, Ng Zhi Ee Nicole, Kyla Kwan, Lee Yong Han Dylann, Liu Fang, Xu Hong</li>
<li>for: This study aims to investigate the linguistic traits of fake news and real news in both written and speech data.</li>
<li>methods: The study uses a dataset of COVID-19 related tweets and TikTok videos, which are fact-checked and labeled as ‘Real’, ‘Fake’, or ‘Questionable’. The Linguistic Inquiry and Word Count (LIWC) software is used to detect patterns in linguistic data.</li>
<li>results: The study finds a set of linguistic features that distinguish fake news from real news in both written and speech data, offering valuable insights into the role of language in shaping trust, social media interactions, and the propagation of fake news.Here is the same information in Simplified Chinese text:</li>
<li>for: 这个研究是 investigate fake news 和 real news 的语言特征。</li>
<li>methods: 研究使用 COVID-19 相关的 tweets 和 TikTok 视频数据集，并使用 credible sources 进行验证和标注为 ‘Real’、’Fake’ 或 ‘Questionable’。使用 Linguistic Inquiry and Word Count (LIWC) 软件检测语言数据中的特征。</li>
<li>results: 研究发现 fake news 和 real news 的语言特征，提供有价值的信息，用于理解信任、社交媒体互动以及假新闻的传播。<details>
<summary>Abstract</summary>
This study investigates the linguistic traits of fake news and real news. There are two parts to this study: text data and speech data. The text data for this study consisted of 6420 COVID-19 related tweets re-filtered from Patwa et al. (2021). After cleaning, the dataset contained 3049 tweets, with 2161 labeled as 'real' and 888 as 'fake'. The speech data for this study was collected from TikTok, focusing on COVID-19 related videos. Research assistants fact-checked each video's content using credible sources and labeled them as 'Real', 'Fake', or 'Questionable', resulting in a dataset of 91 real entries and 109 fake entries from 200 TikTok videos with a total word count of 53,710 words. The data was analysed using the Linguistic Inquiry and Word Count (LIWC) software to detect patterns in linguistic data. The results indicate a set of linguistic features that distinguish fake news from real news in both written and speech data. This offers valuable insights into the role of language in shaping trust, social media interactions, and the propagation of fake news.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这个研究 investigate fake news 和 real news 的语言特征。研究包括两部分：文本数据和说话数据。文本数据包括从 Patwa et al. (2021) 筛选出的 6420 个 COVID-19 相关的推文，经过清洁，剩下 3049 个推文，其中 2161 个被标记为 "real"，888 个被标记为 "fake"。说话数据来自 TikTok，关注 COVID-19 相关的视频，研究助手使用可靠的来源进行 факт-核查，并将每个视频的内容分为 "Real"、"Fake" 或 "问题" 三类，共有 91 个实际的入口和 109 个假的入口，总共 53,710 个字。数据被利用 Linguistic Inquiry and Word Count (LIWC) 软件分析，探测文本数据中的语言特征。结果显示， fake news 和 real news 之间存在一组语言特征，这些特征可以在文本数据和说话数据中被探测出来。这些发现对于语言在建立信任、社交媒体互动和假新闻传播中的作用提供了有价值的信息。
</details></li>
</ul>
<hr>
<h2 id="mlirSynth-Automatic-Retargetable-Program-Raising-in-Multi-Level-IR-using-Program-Synthesis"><a href="#mlirSynth-Automatic-Retargetable-Program-Raising-in-Multi-Level-IR-using-Program-Synthesis" class="headerlink" title="mlirSynth: Automatic, Retargetable Program Raising in Multi-Level IR using Program Synthesis"></a>mlirSynth: Automatic, Retargetable Program Raising in Multi-Level IR using Program Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04196">http://arxiv.org/abs/2310.04196</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Brauckmann, Elizabeth Polgreen, Tobias Grosser, Michael F. P. O’Boyle</li>
<li>for: 本研究旨在提高现代硬件上的编译效率，但现有的程序无法直接利用MLIR的高性能编译。为了解决这问题，本文提出了一种新的方法mlirSynth，可以自动将低级MLIR语言转换为高级MLIR语言，而无需手动定义转换规则。</li>
<li>methods: mlirSynth使用现有的MLIR语言定义来构建一个程序空间，并使用类型约束和等价关系进行有效的搜索。</li>
<li>results: 对Polybench测试集的分析显示，mlirSynth可以实现更高的覆盖率，并且在Intel和AMD两种硬件平台上实现了2.5倍和3.4倍的平均增速，相比之下现有的编译流程。此外，mlirSynth还可以对域特定加速器进行重定向，实现了TPU上的21.6倍的平均增速。<details>
<summary>Abstract</summary>
MLIR is an emerging compiler infrastructure for modern hardware, but existing programs cannot take advantage of MLIR's high-performance compilation if they are described in lower-level general purpose languages. Consequently, to avoid programs needing to be rewritten manually, this has led to efforts to automatically raise lower-level to higher-level dialects in MLIR. However, current methods rely on manually-defined raising rules, which limit their applicability and make them challenging to maintain as MLIR dialects evolve.   We present mlirSynth -- a novel approach which translates programs from lower-level MLIR dialects to high-level ones without manually defined rules. Instead, it uses available dialect definitions to construct a program space and searches it effectively using type constraints and equivalences. We demonstrate its effectiveness \revi{by raising C programs} to two distinct high-level MLIR dialects, which enables us to use existing high-level dialect specific compilation flows. On Polybench, we show a greater coverage than previous approaches, resulting in geomean speedups of 2.5x (Intel) and 3.4x (AMD) over state-of-the-art compilation flows for the C programming language. mlirSynth also enables retargetability to domain-specific accelerators, resulting in a geomean speedup of 21.6x on a TPU.
</details>
<details>
<summary>摘要</summary>
MLIR 是一个emerging compiler 基础设施 для modern 硬件，但现有的程式不能够利用 MLIR 的高性能编译。因此，以避免程式需要手动 rewrite，导致了对 MLIR dialects 的自动提升的努力。然而，现有的方法仍然 rely  на手动定义的提升规则，这限制了它们的应用范围和维护可能性。我们提出了 mlirSynth，一个新的方法，可以将程式从低层 MLIR dialects 提升到高层 dialects without manually defined rules。它使用可用的 dialect definitions 建立一个程式空间，并使用类型条件和等价关系进行有效的搜寻。我们透过将 C 程式提升到两种不同的高层 MLIR dialects，以使用现有的高层 dialect specific compilation flows。在 Polybench 上，我们显示了更高的覆盖率，导致 geomean 加速率为 2.5x (Intel) 和 3.4x (AMD)  compared to state-of-the-art compilation flows for the C programming language。mlirSynth 还允许透过域对应加速器的应用，导致一个 geomean 加速率为 21.6x 在 TPU 上。
</details></li>
</ul>
<hr>
<h2 id="How-to-Capture-Higher-order-Correlations-Generalizing-Matrix-Softmax-Attention-to-Kronecker-Computation"><a href="#How-to-Capture-Higher-order-Correlations-Generalizing-Matrix-Softmax-Attention-to-Kronecker-Computation" class="headerlink" title="How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation"></a>How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04064">http://arxiv.org/abs/2310.04064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Alman, Zhao Song</li>
<li>for: 这个论文是为了研究一种基于 triple-wise 相关的推理扩展，用于解决 transformer 无法解决的问题。</li>
<li>methods: 这个论文使用的方法是基于 tensor 的一种扩展，可以快速计算 triple-wise 相关的推理结果。</li>
<li>results: 这个论文的结果表明，如果输入矩阵中的元素均小于 $o(\sqrt[3]{\log n})$，那么可以在 $n^{1+o(1)}$ 时间内将“tensor-type”的注意力矩阵近似计算出来。但如果输入矩阵中的元素可能达到 $\Omega(\sqrt[3]{\log n})$，那么不存在 faster than $n^{3-o(1)}$ 的算法。<details>
<summary>Abstract</summary>
In the classical transformer attention scheme, we are given three $n \times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \times d$ size matrix $D^{-1} \exp(QK^\top) V$ where $D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations:   $\bullet$ On the positive side, if all entries of the input matrices are bounded above by $o(\sqrt[3]{\log n})$ then we show how to approximate the ``tensor-type'' attention matrix in $n^{1+o(1)}$ time.   $\bullet$ On the negative side, we show that if the entries of the input matrices may be as large as $\Omega(\sqrt[3]{\log n})$, then there is no algorithm that runs faster than $n^{3-o(1)}$ (assuming the Strong Exponential Time Hypothesis from fine-grained complexity theory).   We also show that our construction, algorithms, and lower bounds naturally generalize to higher-order tensors and correlations. Interestingly, the higher the order of the tensors, the lower the bound on the entries needs to be for an efficient algorithm. Our results thus yield a natural tradeoff between the boundedness of the entries, and order of the tensor one may use for more expressive, efficient attention computation.
</details>
<details>
<summary>摘要</summary>
在 классическом transformer 注意机制中，我们给定三个 $n \times d$ 大小矩阵 $Q$, $K$, $V$（问题、键和值符号），目标是计算一个新的 $n \times d$ 大小矩阵 $D^{-1} \exp(QK^\top) V$，其中 $D = \text{diag}( \exp(QK^\top) I_n )$。在这个工作中，我们研究一种扩展 attention，它可以捕捉 triple-wise 相关性。这种扩展可以解决 transformer 无法解决的问题，但是可能增加计算的难度。我们证明，在受限的输入矩阵中（这种情况在实践中经常出现，并且有良好的理论和实践研究），实际上存在一个近线时间算法。具体来说，我们证明如果输入矩阵中所有元素都是 $o(\sqrt[3]{\log n})$ 的Upper bound，那么可以在 $n^{1+o(1)}$ 时间内 aproximate“tensor-type” 注意矩阵。然而，如果输入矩阵中元素可能达到 $\Omega(\sqrt[3]{\log n})$，那么我们证明无法在 $n^{3-o(1)}$ 时间内完成扩展计算。此外，我们还证明我们的构造、算法和下界自然地推广到更高阶的tensor和相关性。有趣的是，与tensor的阶数成正比，输入矩阵中元素的下界需要越低，以便有效地进行注意计算。我们的结果因此表示在bounded-entry setting中，注意计算的效率和tensor阶数之间存在自然的负反关系。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-the-Reasoning-with-Redundant-Information-Provided-Ability-of-Large-Language-Models"><a href="#Analysis-of-the-Reasoning-with-Redundant-Information-Provided-Ability-of-Large-Language-Models" class="headerlink" title="Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models"></a>Analysis of the Reasoning with Redundant Information Provided Ability of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04039">http://arxiv.org/abs/2310.04039</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenbei Xie</li>
<li>for: 这个研究的目的是评估大语言模型（LLMs）在含重复信息的情景下的推理能力。</li>
<li>methods: 研究使用了一种新的问答任务（RRIP），其中提供了多种含重复信息的变体。两个流行的大语言模型（LlaMA2-13B-chat和GPT-3.5）在传统的问答任务上达到了moderate成功，但在RRIP任务上表现不佳。</li>
<li>results: 研究发现，当 LLMS面临含重复信息的情景时，其表现不佳。这种情况透视了当前 LLMS 在推理方面的局限性，并建议将未来的训练数据包含更多的重复信息，以提高 RRIP 任务的表现。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks, especially in reasoning, a cornerstone for achieving Artificial General Intelligence (AGI). However, commonly used benchmarks may not fully encapsulate the inferential abilities of these models in real-world scenarios. To address this gap, a new form of Question-Answering (QA) task, termed Reasoning with Redundant Information Provided (RRIP), is introduced. The study designed a modified version of the grade school math 8K (GSM-8K) dataset which has several variants focusing on different attributes of redundant information. This investigation evaluates two popular LLMs, LlaMA2-13B-chat and generative pre-trained transformer 3.5 (GPT-3.5), contrasting their performance on traditional QA tasks against the RRIP tasks. Findings indicate that while these models achieved moderate success on standard QA benchmarks, their performance notably declines when assessed on RRIP tasks. The study not only highlights the limitations of current LLMs in handling redundant information but also suggests that future training of these models should focus on incorporating redundant information into the training data to increase the performance on RRIP tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Financial-Sentiment-Analysis-via-Retrieval-Augmented-Large-Language-Models"><a href="#Enhancing-Financial-Sentiment-Analysis-via-Retrieval-Augmented-Large-Language-Models" class="headerlink" title="Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models"></a>Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04027">http://arxiv.org/abs/2310.04027</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT-RAG">https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT-RAG</a></li>
<li>paper_authors: Boyu Zhang, Hongyang Yang, Tianyu Zhou, Ali Babar, Xiao-Yang Liu</li>
<li>for: 这个论文是为了提高金融 sentiment 分析的精度和效果而写的。</li>
<li>methods: 该论文使用了 Large Language Models (LLMs) 和检索增强模块，以解决传统 NLP 模型在金融 sentiment 分析中的局限性和不足。</li>
<li>results: 该论文对比传统模型和其他 LLMs (如 ChatGPT 和 LLaMA)，实现了15% 到 48% 的性能提升。<details>
<summary>Abstract</summary>
Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15\% to 48\% performance gain in accuracy and F1 score.
</details>
<details>
<summary>摘要</summary>
financial sentiment分析是决定性的 для评估和投资决策。传统的NLP模型 however，受其参数大小和训练数据范围的限制，导致其泛化能力和效果在这个领域受到限制。最近，大语言模型（LLMs）在庞大的文本资源上进行预训练后表现出色，因为它们在不同的NLP任务上显示出了出色的零扩展能力。然而，直接将LLMs应用于金融 sentiment分析存在挑战：预训练目标和predicting sentiment标签之间的差异可能会降低 LLMS 的预测性能。此外，金融新闻通常简短，缺乏充分的上下文，可能会使 LLMS 的 sentiment分析成本不可靠。为了解决这些挑战，我们提出了一个结合检索增强的 LLMS 框架。该框架包括一个受 instrucion 训练的 LLMS 模块，以及一个检索增强模块，该模块可以从可靠的外部源中检索更多的上下文。与传统模型和 ChatGPT 以及 LLaMA 类 LLMS 进行比较，我们的方法在精度和 F1 分数方面实现了15% 到 48% 的性能提升。
</details></li>
</ul>
<hr>
<h2 id="SemStamp-A-Semantic-Watermark-with-Paraphrastic-Robustness-for-Text-Generation"><a href="#SemStamp-A-Semantic-Watermark-with-Paraphrastic-Robustness-for-Text-Generation" class="headerlink" title="SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation"></a>SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03991">http://arxiv.org/abs/2310.03991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abe Bohan Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, Yulia Tsvetkov</li>
<li>for: 本研究旨在提高 watermarking 算法的抗伪性，以防止 paraphrase 攻击。</li>
<li>methods: 本研究使用了 locality-sensitive hashing (LSH) 和 sentence-level rejection sampling 等技术，实现了 sentence-level semantic watermarking。</li>
<li>results: 对比传统的token-level watermarking方法，本研究的方法更加抗伪，并且能够更好地保持生成质量。 experiments 表明，本研究的novel semantic watermark algorithm 在 common 和 bigram paraphrase 攻击下具有更高的抗伪性和生成质量。<details>
<summary>Abstract</summary>
Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by an LLM, and conducts sentence-level rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. A margin-based constraint is used to enhance its robustness. To show the advantages of our algorithm, we propose a "bigram" paraphrase attack using the paraphrase that has the fewest bigram overlaps with the original sentence. This attack is shown to be effective against the existing token-level watermarking method. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on both common and bigram paraphrase attacks, but also is better at preserving the quality of generation.
</details>
<details>
<summary>摘要</summary>
现有的水印算法受到重写攻击的威胁，因为它们是基于语句级别的设计。为解决这个问题，我们提出SemStamp，一种可靠的句子级别Semantic水印算法，基于locality-sensitive hashing（LSH）。这个算法会将生成器LM生成的候选句子编码和LSH-对hash，然后通过句子级别弃权探针探查，直到找到水印分区中的句子。另外，我们还使用了一个margin-based的约束，以提高其可靠性。为证明我们的算法的优势，我们提出了一个"bigram"重写攻击，使用最少bigram重写的句子进行攻击。实验结果显示，我们的新的句子水印算法不只是在常规重写和bigram重写攻击下更加可靠，而且也能保持生成质量的好。
</details></li>
</ul>
<hr>
<h2 id="Dementia-Assessment-Using-Mandarin-Speech-with-an-Attention-based-Speech-Recognition-Encoder"><a href="#Dementia-Assessment-Using-Mandarin-Speech-with-an-Attention-based-Speech-Recognition-Encoder" class="headerlink" title="Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder"></a>Dementia Assessment Using Mandarin Speech with an Attention-based Speech Recognition Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03985">http://arxiv.org/abs/2310.03985</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jason7580/End-to-End-ASR-and-Dementia-detection-system">https://github.com/jason7580/End-to-End-ASR-and-Dementia-detection-system</a></li>
<li>paper_authors: Zih-Jyun Lin, Yi-Ju Chen, Po-Chih Kuo, Likai Huang, Chaur-Jong Hu, Cheng-Yu Chen</li>
<li>for: 这篇论文是为了提出一个基于语音识别模型的慢性认知评估系统，以帮助早期识别 деменція。</li>
<li>methods: 这篇论文使用了一个注意力型语音识别模型，并将其扩展为一个类型识别模型，以进行慢性认知评估。</li>
<li>results: 这篇论文获得了92.04%的准确率在认知症识别方面，并在临床认知评估分数预测方面得到了9%的平均绝对误差。<details>
<summary>Abstract</summary>
Dementia diagnosis requires a series of different testing methods, which is complex and time-consuming. Early detection of dementia is crucial as it can prevent further deterioration of the condition. This paper utilizes a speech recognition model to construct a dementia assessment system tailored for Mandarin speakers during the picture description task. By training an attention-based speech recognition model on voice data closely resembling real-world scenarios, we have significantly enhanced the model's recognition capabilities. Subsequently, we extracted the encoder from the speech recognition model and added a linear layer for dementia assessment. We collected Mandarin speech data from 99 subjects and acquired their clinical assessments from a local hospital. We achieved an accuracy of 92.04% in Alzheimer's disease detection and a mean absolute error of 9% in clinical dementia rating score prediction.
</details>
<details>
<summary>摘要</summary>
德мен诊断需要一系列不同的测试方法，这是复杂和时间consuming的。早期发现德门可以防止病情加重。这篇论文使用语音识别模型构建了一个专门为普通话说者设计的德门评估系统。通过在真实场景中训练关注型语音识别模型，我们已经显著提高了模型的识别能力。然后，我们从语音识别模型中提取了编码器，并添加了一个线性层进行德门评估。我们从当地医院收集了99名患者的普通话语音数据，并获得了他们的临床评估。我们达到了阿尔茨heimer病 detection的准确率92.04%和临床德门评估分数预测的平均绝对误差9%。
</details></li>
</ul>
<hr>
<h2 id="HuBERTopic-Enhancing-Semantic-Representation-of-HuBERT-through-Self-supervision-Utilizing-Topic-Model"><a href="#HuBERTopic-Enhancing-Semantic-Representation-of-HuBERT-through-Self-supervision-Utilizing-Topic-Model" class="headerlink" title="HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model"></a>HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03975">http://arxiv.org/abs/2310.03975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takashi Maekaku, Jiatong Shi, Xuankai Chang, Yuya Fujita, Shinji Watanabe</li>
<li>for: 提高 HuBERT 模型的 semantic representation，使其更好地捕捉语音内容中的多方面含义。</li>
<li>methods: 我们使用 topic model 对 pseudo-labels 进行分类，生成每个语音句子的话题标签。然后，我们将话题标签作为教师，将其添加到 HuBERT 模型中，以便在无监督的情况下提高模型的泛化能力。</li>
<li>results: 我们的方法在大多数任务中达到了比基eline更好的性能，包括自动语音识别和 SUPERB 任务中的五个任务。此外，我们发现话题标签包含了各种语音句子中的信息，如 gender、speaker 和其主题，这表明我们的方法可以有效地捕捉语音内容中的多方面含义。<details>
<summary>Abstract</summary>
Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Quantized-Transformer-Language-Model-Implementations-on-Edge-Devices"><a href="#Quantized-Transformer-Language-Model-Implementations-on-Edge-Devices" class="headerlink" title="Quantized Transformer Language Model Implementations on Edge Devices"></a>Quantized Transformer Language Model Implementations on Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03971">http://arxiv.org/abs/2310.03971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao, Pratik Satam, Soheil Salehi</li>
<li>for: 这个研究的目的是将大规模的 transformer-based 模型，如 BERT，转换为适合资源受限的边缘设备的 FlatBuffer 格式，以提高它们在这些设备上的部署和运算效率。</li>
<li>methods: 这个研究使用了 FlatBuffer 的转换技术来将大规模的 BERT 模型转换为更小的 MobileBERT 模型，并进一步使用量化技术来将 MobileBERT 模型转换为适合边缘设备的硬件。</li>
<li>results: 这个研究的结果显示，相比 Original BERT 大模型， converted 和量化的 MobileBERT 模型具有 160$\times$ 小的库存储空间，并且在边缘设备上进行评估时能够保持至少一则 tweet 每秒的速度，却是 Original BERT 模型的 4.1% 损失。此外，这个研究也诉说了在无服务器环境中进行隐私保证的特点。<details>
<summary>Abstract</summary>
Large-scale transformer-based models like the Bidirectional Encoder Representations from Transformers (BERT) are widely used for Natural Language Processing (NLP) applications, wherein these models are initially pre-trained with a large corpus with millions of parameters and then fine-tuned for a downstream NLP task. One of the major limitations of these large-scale models is that they cannot be deployed on resource-constrained devices due to their large model size and increased inference latency. In order to overcome these limitations, such large-scale models can be converted to an optimized FlatBuffer format, tailored for deployment on resource-constrained edge devices. Herein, we evaluate the performance of such FlatBuffer transformed MobileBERT models on three different edge devices, fine-tuned for Reputation analysis of English language tweets in the RepLab 2013 dataset. In addition, this study encompassed an evaluation of the deployed models, wherein their latency, performance, and resource efficiency were meticulously assessed. Our experiment results show that, compared to the original BERT large model, the converted and quantized MobileBERT models have 160$\times$ smaller footprints for a 4.1% drop in accuracy while analyzing at least one tweet per second on edge devices. Furthermore, our study highlights the privacy-preserving aspect of TinyML systems as all data is processed locally within a serverless environment.
</details>
<details>
<summary>摘要</summary>
大规模转换器基模型如 bidirectional Encoder Representations from Transformers (BERT) 在自然语言处理 (NLP) 应用中广泛使用，其中这些模型首先通过大量 Parameters 的预训练来初始化，然后为下游 NLP 任务进行细化。一个主要的 limitation 是这些大规模模型无法在有限的设备上部署，因为它们的模型大小和执行时间增加。为了解决这些限制，这些大规模模型可以转换为适合部署在有限资源的边缘设备的 FlatBuffer 格式。在这种情况下，我们评估了这些转换后的 MobileBERT 模型在三个不同的边缘设备上的性能，并对这些部署的模型进行了精心的评估。我们的实验结果表明，相比于原始 BERT 大模型，转换并量化后的 MobileBERT 模型具有 160 倍小于的占用空间，对于一个 4.1% 的精度下降，可以在边缘设备上分析至少一条微博每秒。此外，我们的研究强调了无人化 ML 系统的隐私保护特点，所有数据都是在无人化环境中进行本地处理。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/cs.CL_2023_10_06/" data-id="closbron000c70g884ydfcuzh" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/cs.LG_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T10:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/cs.LG_2023_10_06/">cs.LG - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Profit-Benchmarking-Personalization-and-Robustness-Trade-off-in-Federated-Prompt-Tuning"><a href="#Profit-Benchmarking-Personalization-and-Robustness-Trade-off-in-Federated-Prompt-Tuning" class="headerlink" title="Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning"></a>Profit: Benchmarking Personalization and Robustness Trade-off in Federated Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04627">http://arxiv.org/abs/2310.04627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Collins, Shanshan Wu, Sewoong Oh, Khe Chai Sim</li>
<li>for: 这个论文主要目标是解决 federated learning 中的个性化和Robustness 之间的贸易关系，以及如何在 computation-limited 的设置下实现 parameter-efficient fine-tuning (PEFT) approaches。</li>
<li>methods: 这个论文使用了 FedAvg 和 FedSGD plus personalization (通过客户端本地微调) 算法，并在多种 гипер参数设置下对这些算法进行了 benchmarking。它们还使用了一种常见的 PEFT 方法 — 提示调整 — 来训练大型语言模型 (LLMs)。</li>
<li>results: 研究发现，在使用小学习率并且使用多个本地循环进行个性化时， federated-trained 提示可以 surprisingly Robust。此外，研究还表明，通过添加 Regularization 和 interpolating 两个提示来改善个性化 vs Robustness 的贸易关系。<details>
<summary>Abstract</summary>
In many applications of federated learning (FL), clients desire models that are personalized using their local data, yet are also robust in the sense that they retain general global knowledge. However, the presence of data heterogeneity across clients induces a fundamental trade-off between personalization (i.e., adaptation to a local distribution) and robustness (i.e., not forgetting previously learned general knowledge). It is critical to understand how to navigate this personalization vs robustness trade-off when designing federated systems, which are increasingly moving towards a paradigm of fine-tuning large foundation models. Due to limited computational and communication capabilities in most federated settings, this foundation model fine-tuning must be done using parameter-efficient fine-tuning (PEFT) approaches. While some recent work has studied federated approaches to PEFT, the personalization vs robustness trade-off of federated PEFT has been largely unexplored. In this work, we take a step towards bridging this gap by benchmarking fundamental FL algorithms -- FedAvg and FedSGD plus personalization (via client local fine-tuning) -- applied to one of the most ubiquitous PEFT approaches to large language models (LLMs) -- prompt tuning -- in a multitude of hyperparameter settings under varying levels of data heterogeneity. Our results show that federated-trained prompts can be surprisingly robust when using a small learning rate with many local epochs for personalization, especially when using an adaptive optimizer as the client optimizer during federated training. We also demonstrate that simple approaches such as adding regularization and interpolating two prompts are effective in improving the personalization vs robustness trade-off in computation-limited settings with few local updates allowed for personalization.
</details>
<details>
<summary>摘要</summary>
在许多 Federated Learning (FL) 应用中，客户端希望使用本地数据个性化模型，但同时也希望保持一致性，即不忘记之前学习的总体知识。然而，客户端数据的不同性会导致个性化与一致性之间的基本质量权衡。在设计 Federated 系统时，了解这种个性化与一致性质量权衡是非常重要的。由于大多数 Federated 设置下的计算和通信能力有限，因此需要使用基于大基础模型的 fine-tuning approaches。虽然一些最近的工作已经研究了 Federated 的 fine-tuning 方法，但 Federated 的个性化与一致性质量权衡仍然得不到充分的研究。在这项工作中，我们通过对 FedAvg 和 FedSGD 等基本 FL 算法进行个性化（通过客户端本地练习）和大基础模型 fine-tuning 的比较，在不同的 гиперпарамет Setting 下对 Federated 训练的个性化与一致性质量进行了评估。我们的结果表明，使用小学习率和多个本地 epoch 进行个性化时， federated-trained 提问可以 surprisingly 强大。我们还证明了简单的方法，如添加正则化和 interpolating 两个提问，可以在计算有限的情况下提高个性化与一致性之间的质量权衡。
</details></li>
</ul>
<hr>
<h2 id="FluxGAN-A-Physics-Aware-Generative-Adversarial-Network-Model-for-Generating-Microstructures-That-Maintain-Target-Heat-Flux"><a href="#FluxGAN-A-Physics-Aware-Generative-Adversarial-Network-Model-for-Generating-Microstructures-That-Maintain-Target-Heat-Flux" class="headerlink" title="FluxGAN: A Physics-Aware Generative Adversarial Network Model for Generating Microstructures That Maintain Target Heat Flux"></a>FluxGAN: A Physics-Aware Generative Adversarial Network Model for Generating Microstructures That Maintain Target Heat Flux</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04622">http://arxiv.org/abs/2310.04622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artem K. Pimachev, Manoj Settipalli, Sanghamitra Neogi</li>
<li>For: The paper is written for the purpose of proposing a physics-aware generative adversarial network (GAN) model, called FluxGAN, which can generate high-quality images of large microstructures and describe their thermal properties.* Methods: The FluxGAN model uses a synthesis-by-parts approach and is trained on a dataset of 2D images of microstructures, which allows it to generate arbitrary large size images at low computational cost. The model learns about the relationship between local structural features and physical processes, such as heat flux, due to external temperature gradients.* Results: The paper demonstrates that the FluxGAN model can be used to generate designs of thermal sprayed coatings that satisfy target thermal properties. The model is also capable of generating coating microstructures and physical processes in 3D domain after being trained on 2D examples. The approach has the potential to transform the design and optimization of thermal sprayed coatings for various applications, including high-temperature and long-duration operation of gas turbines for aircraft or ground-based power generators.Here is the same information in Simplified Chinese text:*  для: 这篇论文是为了提出一种物理意识的生成反抗网络模型（FluxGAN），可以同时生成高质量的大微结构图像和其热物理特性的描述。* 方法: FluxGAN模型使用分割-synthesis Approach，并在一个包含微结构图像的数据集上训练。这使得模型可以生成任意大小的图像，而且计算成本很低。在训练过程中，模型学习了微结构特征和外部温度变化对物理过程的关系。* 结果: 论文展示了FluxGAN模型可以用来生成满足目标热性质的热涂层设计。模型还可以从2D例子上训练而生成3D域中的层结构和物理过程。这种方法有可能对涂层设计和优化的各种应用，包括高温和长时间运行的飞机发动机或地面发电机产生高效的影响。<details>
<summary>Abstract</summary>
We propose a physics-aware generative adversarial network model, FluxGAN, capable of simultaneously generating high-quality images of large microstructures and description of their thermal properties. During the training phase, the model learns about the relationship between the local structural features and the physical processes, such as the heat flux in the microstructures, due to external temperature gradients. Once trained, the model generates new structural and associated heat flux environments, bypassing the computationally expensive modeling. Our model provides a cost effective and efficient approach over conventional modeling techniques, such as the finite element method (FEM), for describing the thermal properties of microstructures. The conventional approach requires computational modeling that scales with the size of the microstructure model, therefore limiting the simulation to a given size, resolution, and complexity of the model. In contrast, the FluxGAN model uses synthesis-by-part approach and generates arbitrary large size images at low computational cost. We demonstrate that the model can be utilized to generate designs of thermal sprayed coatings that satisfies target thermal properties. Furthermore, the model is capable of generating coating microstructures and physical processes in three-dimensional (3D) domain after being trained on two-dimensional (2D) examples. Our approach has the potential to transform the design and optimization of thermal sprayed coatings for various applications, including high-temperature and long-duration operation of gas turbines for aircraft or ground-based power generators.
</details>
<details>
<summary>摘要</summary>
我们提出了一种具有物理意识的生成对抗网络模型，FluxGAN，可以同时生成高质量的大型微结构图像和其热性质描述。在训练阶段，模型学习了本地结构特征与外部温度 gradients 对热流的关系。一旦训练完成，模型可以通过 circumventing 计算代价高昂的模型计算，生成新的结构和相关热流环境。我们的模型提供了一种可靠且高效的方法，比如金属元件法（FEM），用于描述微结构的热性质。传统方法需要计算模型，其计算复杂度与微结构模型的大小成直接关系，因此只能在给定大小、分辨率和复杂度下进行模拟。相比之下，FluxGAN 模型使用生成合并法，可以生成任意大小的图像，并且计算成本较低。我们示例中，我们使用了 FluxGAN 模型来生成符合目标热性质的热涂层设计。此外，模型还可以在三维空间中生成层次结构和物理过程，并且可以在训练于二维例子后在三维空间中生成层次结构。我们的方法具有可能改变热涂层设计和优化的应用，包括高温和长时间运行的液体发动机，如飞机或地面发电机。
</details></li>
</ul>
<hr>
<h2 id="A-Topological-Perspective-on-Demystifying-GNN-Based-Link-Prediction-Performance"><a href="#A-Topological-Perspective-on-Demystifying-GNN-Based-Link-Prediction-Performance" class="headerlink" title="A Topological Perspective on Demystifying GNN-Based Link Prediction Performance"></a>A Topological Perspective on Demystifying GNN-Based Link Prediction Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04612">http://arxiv.org/abs/2310.04612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuwvandy/topo_lp_gnn">https://github.com/yuwvandy/topo_lp_gnn</a></li>
<li>paper_authors: Yu Wang, Tong Zhao, Yuying Zhao, Yunchao Liu, Xueqi Cheng, Neil Shah, Tyler Derr<br>for:* 这种研究旨在探讨Graph Neural Networks (GNNs)在链接预测 (LP) 中的表现不均衡性，以及这种不均衡性的原因。methods:* 该研究使用了GNNs来学习节点嵌入，并对不同节点的LP表现进行分析。* 提出了一个新的度量指标Topological Concentration (TC)，基于每个节点的本地子图与其邻居节点的子图的交集。* 对TC指标与其他度量指标之间的相关性进行了实验证明。results:* 发现TC指标与LP表现之间存在高度相关性，而不是使用度量指标如度和子图密度。* 通过TC指标可以更好地确定低表现节点，并且可以预测节点之间的交互变化。* 提出了一种可Scalable的 Approximated Topological Concentration (ATC)，以便在计算TC指标时降低计算复杂性。* 研究了通过增强TC指标来提高LP表现的可能性，并对这种方法的局限性进行了讨论。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have shown great promise in learning node embeddings for link prediction (LP). While numerous studies aim to improve the overall LP performance of GNNs, none have explored its varying performance across different nodes and its underlying reasons. To this end, we aim to demystify which nodes will perform better from the perspective of their local topology. Despite the widespread belief that low-degree nodes exhibit poorer LP performance, our empirical findings provide nuances to this viewpoint and prompt us to propose a better metric, Topological Concentration (TC), based on the intersection of the local subgraph of each node with the ones of its neighbors. We empirically demonstrate that TC has a higher correlation with LP performance than other node-level topological metrics like degree and subgraph density, offering a better way to identify low-performing nodes than using cold-start. With TC, we discover a novel topological distribution shift issue in which newly joined neighbors of a node tend to become less interactive with that node's existing neighbors, compromising the generalizability of node embeddings for LP at testing time. To make the computation of TC scalable, We further propose Approximated Topological Concentration (ATC) and theoretically/empirically justify its efficacy in approximating TC and reducing the computation complexity. Given the positive correlation between node TC and its LP performance, we explore the potential of boosting LP performance via enhancing TC by re-weighting edges in the message-passing and discuss its effectiveness with limitations. Our code is publicly available at https://github.com/YuWVandy/Topo_LP_GNN.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经表现出很好的可能性在学习节点嵌入中进行链接预测（LP）。虽然许多研究努力提高 GNNs 的总体 LP 性能，但是没有探讨其各节点的不同性和下面的原因。为了解这些问题，我们想要解读哪些节点会在本地拓扑结构上表现更好。一些普遍的信念认为，低度节点会表现更差的 LP 性能，但我们的实证发现了这个观点的复杂性，并提出了一个更好的指标：拓扑强度（TC），基于每个节点的本地子图与其邻居节点的交集。我们通过实验表明，TC 与 LP 性能之间存在更高的相关性，而不是其他节点级别拓扑指标，如度和子图密度。这意味着可以通过TC来更好地识别 LP 性能下降的节点，而不是使用冷启动。与TC相关的发现是，在新加入邻居节点的情况下，节点之间的互动性会降低，这会影响节点的总体 LP 性能。为了使TC计算可扩展，我们还提出了一种近似TC的方法： Approximated Topological Concentration（ATC），并论证了其可以准确地近似TC并降低计算复杂性。我们发现，节点的TC与其 LP 性能之间存在正相关性，因此我们可以通过增强TC来提高 LP 性能，例如通过重新权重边在消息传递中。我们的代码可以在https://github.com/YuWVandy/Topo_LP_GNN上获取。
</details></li>
</ul>
<hr>
<h2 id="Robust-Transfer-Learning-with-Unreliable-Source-Data"><a href="#Robust-Transfer-Learning-with-Unreliable-Source-Data" class="headerlink" title="Robust Transfer Learning with Unreliable Source Data"></a>Robust Transfer Learning with Unreliable Source Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04606">http://arxiv.org/abs/2310.04606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqing Fan, Cheng Gao, Jason M. Klusowski</li>
<li>for: 这paper解决了转移学习中的抗抵抗性和weak transferable signal问题。</li>
<li>methods: 我们引入了一个新的量 called ‘’ambiguity level’’，用于度量目标和来源分布之间的差异，并提出了一种简单的转移学习过程。我们还证明了一个普遍的定理，表明这个新量与转移学习中的风险改进之间的关系。</li>
<li>results: 我们的’’Transfer Around Boundary’’（TAB）模型，通过考虑目标和来源数据的性能平衡，能够提高分类，同时避免负转移。此外，我们在非 Parametric 分类和логисти回归任务上表现了TAB模型的效果，达到了最佳的上界，即logarithmic factor。验证研究也证明了TAB模型的效果。此外，我们还提供了简单的方法来 bound Excess Misclassification Error  без需要特殊的转移学习知识。<details>
<summary>Abstract</summary>
This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distribution. We introduce a novel quantity called the ''ambiguity level'' that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements. Our proposed ''Transfer Around Boundary'' (TAB) model, with a threshold balancing the performance of target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer. Moreover, we demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors. Simulation studies lend further support to the effectiveness of TAB. We also provide simple approaches to bound the excess misclassification error without the need for specialized knowledge in transfer learning.
</details>
<details>
<summary>摘要</summary>
Our proposed "Transfer Around Boundary" (TAB) model, which uses a threshold to balance the performance of the target and source data, is efficient and robust, and can improve classification while avoiding negative transfer. We demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds that are optimal up to logarithmic factors. Simulation studies also support the effectiveness of TAB.Furthermore, we provide simple approaches to bound the excess misclassification error without the need for specialized knowledge in transfer learning. Our results show that the TAB model can be a useful tool for addressing the challenges of robust transfer learning in a variety of applications.
</details></li>
</ul>
<hr>
<h2 id="Learning-Optimal-Power-Flow-Value-Functions-with-Input-Convex-Neural-Networks"><a href="#Learning-Optimal-Power-Flow-Value-Functions-with-Input-Convex-Neural-Networks" class="headerlink" title="Learning Optimal Power Flow Value Functions with Input-Convex Neural Networks"></a>Learning Optimal Power Flow Value Functions with Input-Convex Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04605">http://arxiv.org/abs/2310.04605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Rosemberg, Mathieu Tanneau, Bruno Fanzeres, Joaquim Garcia, Pascal Van Hentenryck</li>
<li>for:  solves the Optimal Power Flow (OPF) problem with machine learning (ML) to improve the speed of analysis and enable real-time decision-making in power systems.</li>
<li>methods:  uses ML to learn convex approximate solutions that can be solved more quickly than traditional methods, while still maintaining a high level of accuracy.</li>
<li>results:  enables faster exploration of vast solution spaces in complex power system problems, allowing for more efficient and practical decision-making.<details>
<summary>Abstract</summary>
The Optimal Power Flow (OPF) problem is integral to the functioning of power systems, aiming to optimize generation dispatch while adhering to technical and operational constraints. These constraints are far from straightforward; they involve intricate, non-convex considerations related to Alternating Current (AC) power flow, which are essential for the safety and practicality of electrical grids. However, solving the OPF problem for varying conditions within stringent time frames poses practical challenges. To address this, operators resort to model simplifications of varying accuracy. Unfortunately, better approximations (tight convex relaxations) are often computationally intractable. This research explores machine learning (ML) to learn convex approximate solutions for faster analysis in the online setting while still allowing for coupling into other convex dependent decision problems. By trading off a small amount of accuracy for substantial gains in speed, they enable the efficient exploration of vast solution spaces in these complex problems.
</details>
<details>
<summary>摘要</summary>
OPTimal Power Flow (OPF) 问题是电力系统的关键问题，旨在优化发电规划，同时遵循技术和运营限制。这些限制并不简单，涉及到复杂的交流电流流体系，这些限制对电力网络的安全性和实用性具有重要性。然而，为了解决在不同条件下的变化，在紧张时间framworks中解决OPF问题具有实际挑战。为此，操作人员通常采用模型简化，以提高解决速度。然而，更好的近似（紧密的 convex relaxation）经常是计算易于过载。这个研究探讨了机器学习（ML），以学习减少精度的convex近似解决方案，以更快地进行在线分析，同时仍能与其他convex相互依赖的决策问题相集成。通过折衔精度和速度之间的平衡，他们可以快速探索复杂的问题空间。
</details></li>
</ul>
<hr>
<h2 id="PriViT-Vision-Transformers-for-Fast-Private-Inference"><a href="#PriViT-Vision-Transformers-for-Fast-Private-Inference" class="headerlink" title="PriViT: Vision Transformers for Fast Private Inference"></a>PriViT: Vision Transformers for Fast Private Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04604">http://arxiv.org/abs/2310.04604</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nyu-dice-lab/privit">https://github.com/nyu-dice-lab/privit</a></li>
<li>paper_authors: Naren Dhyani, Jianqiao Mo, Minsu Cho, Ameya Joshi, Siddharth Garg, Brandon Reagen, Chinmay Hegde</li>
<li>for: 这篇论文的目的是提出一种可以在安全多方计算（MPC）协议下进行私人执行的深度模型，以提高计算机视觉应用的性能。</li>
<li>methods: 该论文提出了一种基于导数的算法，可以选择性地”Taylorize” ViT中的非线性运算（自注意、Feed-Forward rectifiers、层normalization），以维护其预测精度。</li>
<li>results:  experiments表明，该算法可以在多个标准图像分类任务上提高MPC协议下的私人执行性能，并且与现有的设计MPCCompatible transformer架构的方法相比，在达到精度-延迟的Pareto前沿上表现更好。<details>
<summary>Abstract</summary>
The Vision Transformer (ViT) architecture has emerged as the backbone of choice for state-of-the-art deep models for computer vision applications. However, ViTs are ill-suited for private inference using secure multi-party computation (MPC) protocols, due to the large number of non-polynomial operations (self-attention, feed-forward rectifiers, layer normalization). We propose PriViT, a gradient based algorithm to selectively "Taylorize" nonlinearities in ViTs while maintaining their prediction accuracy. Our algorithm is conceptually simple, easy to implement, and achieves improved performance over existing approaches for designing MPC-friendly transformer architectures in terms of achieving the Pareto frontier in latency-accuracy. We confirm these improvements via experiments on several standard image classification tasks. Public code is available at https://github.com/NYU-DICE-Lab/privit.
</details>
<details>
<summary>摘要</summary>
“视野变数器（ViT）架构已经成为现代 компьютер视觉应用中的后座架构，但是ViTs在安全多方计算（MPC）协议下进行私人推干时存在问题，因为它们具有大量非多项式操作（自我注意力、传递反射、层常化）。我们提出了PriViT，一个基于梯度的算法，可以选择性地“Taylorize” ViTs 中的非线性性，以维持其预测精度。我们的算法是概念简单、易于实现，并在实现Pareto点（延迟精度）上提供了改进。我们透过对多个标准图像分类任务进行实验，证实了这些改进。相关的公共代码可以在https://github.com/NYU-DICE-Lab/privit中找到。”
</details></li>
</ul>
<hr>
<h2 id="Deep-Model-Predictive-Optimization"><a href="#Deep-Model-Predictive-Optimization" class="headerlink" title="Deep Model Predictive Optimization"></a>Deep Model Predictive Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04590">http://arxiv.org/abs/2310.04590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jisacks/dmpo">https://github.com/jisacks/dmpo</a></li>
<li>paper_authors: Jacob Sacks, Rwik Rana, Kevin Huang, Alex Spitzer, Guanya Shi, Byron Boots<br>for: 本研究旨在设计Robotics中的坚固政策，以实现复杂和灵活的行为在实际世界中。methods: 本研究使用Deep Model Predictive Optimization（DMPO），通过经验学习内部循环的MPC优化算法，直接对控制问题进行适应。results: DMPO在一个真实的四旋翼机追踪任务中表现出色，比基eline MPC算法提高性能，并且可以在 fewer samples 和更少的内存下进行适应。在附加的风暴预测情况下，DMPO可以灵活适应零shot，并且仍然超越所有基eline。更多结果可以在 <a target="_blank" rel="noopener" href="https://tinyurl.com/mr2ywmnw">https://tinyurl.com/mr2ywmnw</a> 获取。<details>
<summary>Abstract</summary>
A major challenge in robotics is to design robust policies which enable complex and agile behaviors in the real world. On one end of the spectrum, we have model-free reinforcement learning (MFRL), which is incredibly flexible and general but often results in brittle policies. In contrast, model predictive control (MPC) continually re-plans at each time step to remain robust to perturbations and model inaccuracies. However, despite its real-world successes, MPC often under-performs the optimal strategy. This is due to model quality, myopic behavior from short planning horizons, and approximations due to computational constraints. And even with a perfect model and enough compute, MPC can get stuck in bad local optima, depending heavily on the quality of the optimization algorithm. To this end, we propose Deep Model Predictive Optimization (DMPO), which learns the inner-loop of an MPC optimization algorithm directly via experience, specifically tailored to the needs of the control problem. We evaluate DMPO on a real quadrotor agile trajectory tracking task, on which it improves performance over a baseline MPC algorithm for a given computational budget. It can outperform the best MPC algorithm by up to 27% with fewer samples and an end-to-end policy trained with MFRL by 19%. Moreover, because DMPO requires fewer samples, it can also achieve these benefits with 4.3X less memory. When we subject the quadrotor to turbulent wind fields with an attached drag plate, DMPO can adapt zero-shot while still outperforming all baselines. Additional results can be found at https://tinyurl.com/mr2ywmnw.
</details>
<details>
<summary>摘要</summary>
robotics 中的一个主要挑战是设计Robust的策略，以实现复杂且灵活的行为在真实世界中。一个端的spectrum中，我们有model-free reinforcement learning（MFRL），它非常灵活和通用，但经常导致脆弱的策略。相比之下，model predictive control（MPC）在每个时间步骤 continually re-plans，以保持对偏移和模型不准确的Robust性。然而，尽管在实际世界中获得了成功，MPC经常下 Perform 优化策略。这是因为模型质量、短时间内的自我优化行为和计算约束导致的approximation。而且， même avec un modèle parfait et suffisamment de calcul, MPC peut se retrouver dans des optima local mauvaises, en fonction de la qualité de l'algorithme d'optimisation.为了解决这个问题，我们提出了Deep Model Predictive Optimization（DMPO），它通过经验直接学习MPC优化算法的内部循环，特别是适应控制问题的需求。我们在一个真实的quadrotor agile trajectory tracking任务上评估了DMPO，并与基准MPC算法进行比较。DMPO可以在给定的计算预算下提高性能，相比baseline MPCA algorithm by up to 27% with fewer samples，并且可以在练习量和MFRL中训练的策略之间进行比较。此外，由于DMPO需要 fewer samples，它还可以实现这些优势，并且可以在4.3倍的内存中进行学习。当我们将quadrotor expose to turbulent wind fields with an attached drag plate时，DMPO可以适应zero-shot，并且仍然超过所有基准。详细的结果可以在https://tinyurl.com/mr2ywmnw 找到。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Equal-Opportunity-on-Statistical-Discrimination"><a href="#The-Impact-of-Equal-Opportunity-on-Statistical-Discrimination" class="headerlink" title="The Impact of Equal Opportunity on Statistical Discrimination"></a>The Impact of Equal Opportunity on Statistical Discrimination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04585">http://arxiv.org/abs/2310.04585</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Y. Zhu</li>
<li>for: 这个论文是为了扩展公司的工具箱，以便通过合理的方式执行法规。</li>
<li>methods: 论文使用机器学习生成公司对个人的不可见类别信仰，从而使得法规可以更加有效。</li>
<li>results: 研究表明，通过要求公司选择决策策略来平等化真正正确率，可以消除统计性隔离。<details>
<summary>Abstract</summary>
I modify the canonical statistical discrimination model of Coate and Loury (1993) by assuming the firm's belief about an individual's unobserved class is machine learning-generated and, therefore, contractible. This expands the toolkit of a regulator beyond belief-free regulations like affirmative action. Contractible beliefs make it feasible to require the firm to select a decision policy that equalizes true positive rates across groups -- what the algorithmic fairness literature calls equal opportunity. While affirmative action does not necessarily end statistical discrimination, I show that imposing equal opportunity does.
</details>
<details>
<summary>摘要</summary>
我修改了科恩和劳雷（1993）的标准统计歧视模型，假设企业对个人未知类别的信念是通过机器学习生成的，因此可控。这扩展了管理者的工具包，包括不基于信念的法规，如有利预测。可控信念使得可以要求企业选择决策策略，使true positive rate across groups相同，这与算法公平 литературе称为equal opportunity。虽然有利预测不一定会消除统计歧视，但我表明，要求equal opportunity会消除它。
</details></li>
</ul>
<hr>
<h2 id="Self-Confirming-Transformer-for-Locally-Consistent-Online-Adaptation-in-Multi-Agent-Reinforcement-Learning"><a href="#Self-Confirming-Transformer-for-Locally-Consistent-Online-Adaptation-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Self-Confirming Transformer for Locally Consistent Online Adaptation in Multi-Agent Reinforcement Learning"></a>Self-Confirming Transformer for Locally Consistent Online Adaptation in Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04579">http://arxiv.org/abs/2310.04579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Li, Juan Guevara, Xinghong Xie, Quanyan Zhu</li>
<li>for: 本研究旨在提高offline reinforcement learning（RL）的在线适应性，即使在online testing中opponents（外部控制不能的agent） exhibit nonstationary behaviors。</li>
<li>methods: 本研究使用transformer architecture和self-confirming loss（SCL）来Address the online nonstationarity。</li>
<li>results: 实验结果表明，使用SCT可以在online testing中适应nonstationary opponents，并 achieved higher returns than vanilla transformers和offline MARL baselines。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) leverages previously collected data to extract policies that return satisfying performance in online environments. However, offline RL suffers from the distribution shift between the offline dataset and the online environment. In the multi-agent RL (MARL) setting, this distribution shift may arise from the nonstationary opponents (exogenous agents beyond control) in the online testing who display distinct behaviors from those recorded in the offline dataset. Hence, the key to the broader deployment of offline MARL is the online adaptation to nonstationary opponents. Recent advances in large language models have demonstrated the surprising generalization ability of the transformer architecture in sequence modeling, which prompts one to wonder \textit{whether the offline-trained transformer policy adapts to nonstationary opponents during online testing}. This work proposes the self-confirming loss (SCL) in offline transformer training to address the online nonstationarity, which is motivated by the self-confirming equilibrium (SCE) in game theory. The gist is that the transformer learns to predict the opponents' future moves based on which it acts accordingly. As a weaker variant of Nash equilibrium (NE), SCE (equivalently, SCL) only requires local consistency: the agent's local observations do not deviate from its conjectures, leading to a more adaptable policy than the one dictated by NE focusing on global optimality. We evaluate the online adaptability of the self-confirming transformer (SCT) by playing against nonstationary opponents employing a variety of policies, from the random one to the benchmark MARL policies. Experimental results demonstrate that SCT can adapt to nonstationary opponents online, achieving higher returns than vanilla transformers and offline MARL baselines.
</details>
<details>
<summary>摘要</summary>
偏向学习（Offline Reinforcement Learning）可以利用先前收集的数据提取策略，以实现在线环境中达到满意性的表现。然而，偏向学习在线环境中会面临数据分布的变化问题。在多代理学习（Multi-agent Reinforcement Learning） Setting中，这种分布变化可能来自于在线测试中的非站台式对手（exogenous agents beyond control），这些对手在online测试中展现出与偏向学习数据中记录的行为不同的行为。因此，延伸到更广泛的部署需要在线适应非站台式对手。最近的大语言模型技术的进步表明了变换体系的抽象能力在序列模型中，这使得我们可以思考，“是否在线测试中，偏向学习过的变换策略能够适应非站台式对手？”这项工作提出了在线适应的自Confirming损失（SCL），以解决在线非站台式对手的问题。SCL的核心思想是，变换学习如何预测对手的未来行动，然后根据这些预测行动进行反应。相比 Nash平衡（Nash Equilibrium），SCE（自Confirming Equilibrium）只需要本地一致性：代理的本地观察不会与其推测相偏离，从而导致更适应的策略。我们通过在不同策略上对非站台式对手进行在线适应测试，评估SCT（自Confirming transformer）在线适应能力。实验结果表明，SCT可以在线适应非站台式对手，并在返回上超过了普通变换和偏向学习基准值。
</details></li>
</ul>
<hr>
<h2 id="Transformer-Based-Neural-Surrogate-for-Link-Level-Path-Loss-Prediction-from-Variable-Sized-Maps"><a href="#Transformer-Based-Neural-Surrogate-for-Link-Level-Path-Loss-Prediction-from-Variable-Sized-Maps" class="headerlink" title="Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction from Variable-Sized Maps"></a>Transformer-Based Neural Surrogate for Link-Level Path Loss Prediction from Variable-Sized Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04570">http://arxiv.org/abs/2310.04570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas M. Hehn, Tribhuvanesh Orekondy, Ori Shental, Arash Behboodi, Juan Bucheli, Akash Doshi, June Namgoong, Taesang Yoo, Ashwin Sampath, Joseph B. Soriaga</li>
<li>for: 预测传输器-接收器位置的信道损失是许多场景的关键，如网络规划和手动遮挡。</li>
<li>methods: 本文提出了一种基于转换器的神经网络架构，可以从不同维度的地图数据中预测无线通信频率特性。</li>
<li>results: 我们的方法可以高效地学习主要的信道损失从稀疏的训练数据中，并在新地图上进行良好的泛化。<details>
<summary>Abstract</summary>
Estimating path loss for a transmitter-receiver location is key to many use-cases including network planning and handover. Machine learning has become a popular tool to predict wireless channel properties based on map data. In this work, we present a transformer-based neural network architecture that enables predicting link-level properties from maps of various dimensions and from sparse measurements. The map contains information about buildings and foliage. The transformer model attends to the regions that are relevant for path loss prediction and, therefore, scales efficiently to maps of different size. Further, our approach works with continuous transmitter and receiver coordinates without relying on discretization. In experiments, we show that the proposed model is able to efficiently learn dominant path losses from sparse training data and generalizes well when tested on novel maps.
</details>
<details>
<summary>摘要</summary>
估算发射器-接收器位置之间的信道损失是许多应用场景的关键，如网络规划和手动 переключение。机器学习已成为预测无线通道属性的受欢迎工具。在这种工作中，我们提出了基于变换器的神经网络架构，可以从不同维度的地图数据中预测链路级属性。地图中包含建筑和植被信息。变换器模型会关注 relevante 区域，因此可以有效缩放到不同的地图大小。此外，我们的方法不需要发射器和接收器坐标的精确分解，可以使用连续坐标。在实验中，我们发现提议的模型可以高效地从笔记数据中学习主要的信道损失，并在新的地图上具有良好的泛化性。
</details></li>
</ul>
<hr>
<h2 id="DragD3D-Vertex-based-Editing-for-Realistic-Mesh-Deformations-using-2D-Diffusion-Priors"><a href="#DragD3D-Vertex-based-Editing-for-Realistic-Mesh-Deformations-using-2D-Diffusion-Priors" class="headerlink" title="DragD3D: Vertex-based Editing for Realistic Mesh Deformations using 2D Diffusion Priors"></a>DragD3D: Vertex-based Editing for Realistic Mesh Deformations using 2D Diffusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04561">http://arxiv.org/abs/2310.04561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tianhaoxie/DragD3D">https://github.com/tianhaoxie/DragD3D</a></li>
<li>paper_authors: Tianhao Xie, Eugene Belilovsky, Sudhir Mudur, Tiberiu Popa</li>
<li>for: 本研究旨在提供一种全面考虑物体全局上下文的本地矩阵编辑方法，以实现全面的、真实的和自然的形状变换。</li>
<li>methods: 本方法基于经典的 геометрическиеARAP（最大可能牢固）正则化和大规模扩散模型生成的2D优先顺序，并使用最近引入的DDS损失函数评估图像的准确性。 DragD3Dcombines approximate gradients of DDS with gradients from ARAP loss to modify mesh vertices via neural Jacobian field, while satisfying vertex constraints.</li>
<li>results: 研究表明， DragD3D可以实现高质量、真实和自然的形状变换，并且可以考虑物体的全局上下文。 DragD3D的实现超过了只使用 геометрические正则化的结果。<details>
<summary>Abstract</summary>
Direct mesh editing and deformation are key components in the geometric modeling and animation pipeline. Direct mesh editing methods are typically framed as optimization problems combining user-specified vertex constraints with a regularizer that determines the position of the rest of the vertices. The choice of the regularizer is key to the realism and authenticity of the final result. Physics and geometry-based regularizers are not aware of the global context and semantics of the object, and the more recent deep learning priors are limited to a specific class of 3D object deformations. In this work, our main contribution is a local mesh editing method called DragD3D for global context-aware realistic deformation through direct manipulation of a few vertices. DragD3D is not restricted to any class of objects. It achieves this by combining the classic geometric ARAP (as rigid as possible) regularizer with 2D priors obtained from a large-scale diffusion model. Specifically, we render the objects from multiple viewpoints through a differentiable renderer and use the recently introduced DDS loss which scores the faithfulness of the rendered image to one from a diffusion model. DragD3D combines the approximate gradients of the DDS with gradients from the ARAP loss to modify the mesh vertices via neural Jacobian field, while also satisfying vertex constraints. We show that our deformations are realistic and aware of the global context of the objects, and provide better results than just using geometric regularizers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<<SYS>>直接矩阵编辑和变形是创建三维模型和动画管道中的关键组件。直接矩阵编辑方法通常是形式为优化问题，将用户指定的顶点约束与一种regularizer相结合，该regularizer determines the position of the rest of the vertices。选择这种regularizer是创建最终结果的真实性和准确性的关键。物理和几何基于的regularizers不具备物体全局上下文和 semantics的认知，而最近的深度学习假设只是限制到特定类型的3D对象变形。在这项工作中，我们的主要贡献是一种全球上下文相关的实实地摆动方法，即DragD3D，通过直接控制一些顶点来实现高真实性的变形。DragD3D不受任何类型的物体限制。它通过将经典的几何ARAP（as rigid as possible）regulator与多视点 render 后的2D priors结合，使用最近引入的DDS损失（scores the faithfulness of the rendered image to one from a diffusion model）来修改矩阵顶点，同时满足顶点约束。我们表明了我们的变形是真实的，aware of the global context of the objects，并提供了更好的结果，比只使用几何regularizers。
</details></li>
</ul>
<hr>
<h2 id="Talk-like-a-Graph-Encoding-Graphs-for-Large-Language-Models"><a href="#Talk-like-a-Graph-Encoding-Graphs-for-Large-Language-Models" class="headerlink" title="Talk like a Graph: Encoding Graphs for Large Language Models"></a>Talk like a Graph: Encoding Graphs for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04560">http://arxiv.org/abs/2310.04560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bahare Fatemi, Jonathan Halcrow, Bryan Perozzi</li>
<li>for: 这研究旨在探讨如何将图structured data编码为文本，以便由大型自然语言模型（LLM）进行理解。</li>
<li>methods: 研究者采用了多种图编码方法，包括Graph2Vec、DeepWalk和LPA。</li>
<li>results: 研究发现， LLM 在图理解任务中表现强度各不相同，具体来说是：1）编码方法的选择，2）图任务的性质，3）图结构本身。这些结果提供了对编码图为文本的策略的有价值的指导，并示出了对图理解任务中 LLM 性能的改进。<details>
<summary>Abstract</summary>
Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8% to 61.8%, depending on the task.
</details>
<details>
<summary>摘要</summary>
GRAPHs 是一种强大的工具，用于表示和分析复杂关系的实际应用，如社交网络、推荐系统和计算金融。 理解 GRAPH 是为了从复杂系统中提取关系之间的信息，发现隐藏的模式和趋势。 虽然自动化的文本推理得到了很大的进步，但 GRAPH 推理 WITH 大型自然语言模型（LLM）仍然是一个未研究的问题。 在这个工作中，我们进行了 GRAPH 结构数据作为文本的第一次全面研究。我们发现， LLM 在 GRAPH 理解任务中的表现因三个基本因素而异常：（1） GRAPH 编码方法，（2） GRAPH 任务的本质，以及（3）考虑 GRAPH 的结构。这些新的结果提供了值得关注的投入，并证明了如何选择正确的编码器可以提高 LLM 中 GRAPH 理解任务的性能，从4.8% 到 61.8%，具体取决于任务。
</details></li>
</ul>
<hr>
<h2 id="Multi-decadal-Sea-Level-Prediction-using-Neural-Networks-and-Spectral-Clustering-on-Climate-Model-Large-Ensembles-and-Satellite-Altimeter-Data"><a href="#Multi-decadal-Sea-Level-Prediction-using-Neural-Networks-and-Spectral-Clustering-on-Climate-Model-Large-Ensembles-and-Satellite-Altimeter-Data" class="headerlink" title="Multi-decadal Sea Level Prediction using Neural Networks and Spectral Clustering on Climate Model Large Ensembles and Satellite Altimeter Data"></a>Multi-decadal Sea Level Prediction using Neural Networks and Spectral Clustering on Climate Model Large Ensembles and Satellite Altimeter Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04540">http://arxiv.org/abs/2310.04540</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saumya Sinha, John Fasullo, R. Steven Nerem, Claire Monteleoni</li>
<li>for: 这项研究的目的是预测未来30年全球海平面水位变化趋势。</li>
<li>methods: 该研究使用机器学习（ML）技术预测海平面水位变化趋势，并提供了不确定性估计。</li>
<li>results: 研究发现，通过使用特征连接神经网络（FCNN），可以根据气候模型预测来预测海平面水位变化趋势。此外，研究还发现将空间数据分割并学习专门的ML模型对每个分割区域的预测有所提高。<details>
<summary>Abstract</summary>
Sea surface height observations provided by satellite altimetry since 1993 show a rising rate (3.4 mm/year) for global mean sea level. While on average, sea level has risen 10 cm over the last 30 years, there is considerable regional variation in the sea level change. Through this work, we predict sea level trends 30 years into the future at a 2-degree spatial resolution and investigate the future patterns of the sea level change. We show the potential of machine learning (ML) in this challenging application of long-term sea level forecasting over the global ocean. Our approach incorporates sea level data from both altimeter observations and climate model simulations. We develop a supervised learning framework using fully connected neural networks (FCNNs) that can predict the sea level trend based on climate model projections. Alongside this, our method provides uncertainty estimates associated with the ML prediction. We also show the effectiveness of partitioning our spatial dataset and learning a dedicated ML model for each segmented region. We compare two partitioning strategies: one achieved using domain knowledge, and the other employing spectral clustering. Our results demonstrate that segmenting the spatial dataset with spectral clustering improves the ML predictions.
</details>
<details>
<summary>摘要</summary>
卫星探雷数据自1993年起显示全球海平面水位的升高率为3.4毫米/年。虽然在过去30年平均海平面上升10厘米，但地域性差异在海平面变化中很大。通过这项工作，我们预测未来30年海平面趋势，并研究未来海平面变化的 Patterns。我们利用机器学习（ML）技术来实现这项挑战性的海平面预测任务。我们的方法包括将海平面数据从探雷观测和气候模型仿真数据中提取出来，并使用全连接神经网络（FCNN）来预测海平面趋势。同时，我们的方法还提供了与ML预测相关的不确定性估计。我们还表明，将空间数据分割并学习每个分割区域专门的ML模型可以提高ML预测的准确性。我们比较了两种分割策略：一种基于领域知识，另一种使用spectral clustering。我们的结果表明，使用spectral clustering分割空间数据可以提高ML预测的准确性。
</details></li>
</ul>
<hr>
<h2 id="Generating-Less-Certain-Adversarial-Examples-Improves-Robust-Generalization"><a href="#Generating-Less-Certain-Adversarial-Examples-Improves-Robust-Generalization" class="headerlink" title="Generating Less Certain Adversarial Examples Improves Robust Generalization"></a>Generating Less Certain Adversarial Examples Improves Robust Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04539">http://arxiv.org/abs/2310.04539</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trustmlrg/edac">https://github.com/trustmlrg/edac</a></li>
<li>paper_authors: Minxing Zhang, Michael Backes, Xiao Zhang</li>
<li>for: 该研究旨在解释深度神经网络受到敌意示例的攻击的原因，并提出一种基于外部梯度的方法来提高模型的Robustness。</li>
<li>methods: 该研究使用了对抗训练方法，并提出了一种基于对抗 cer certainty 的方法来减少模型的对抗过拟合。</li>
<li>results: 实验结果表明，该方法能够有效地减少对抗过拟合，并可以生成具有更好的Robustness的模型。<details>
<summary>Abstract</summary>
Recent studies have shown that deep neural networks are vulnerable to adversarial examples. Numerous defenses have been proposed to improve model robustness, among which adversarial training is most successful. In this work, we revisit the robust overfitting phenomenon. In particular, we argue that overconfident models produced during adversarial training could be a potential cause, supported by the empirical observation that the predicted labels of adversarial examples generated by models with better robust generalization ability tend to have significantly more even distributions. Based on the proposed definition of adversarial certainty, we incorporate an extragradient step in the adversarial training framework to search for models that can generate adversarially perturbed inputs with lower certainty, further improving robust generalization. Our approach is general and can be easily combined with other variants of adversarial training methods. Extensive experiments on image benchmarks demonstrate that our method effectively alleviates robust overfitting and is able to produce models with consistently improved robustness.
</details>
<details>
<summary>摘要</summary>
研究最近发现深度神经网络容易受到攻击性示例的影响。许多防御方法已经被提出，其中最成功的是对抗训练。在这项工作中，我们重新检视了Robust Overfitting现象。我们认为，在对抗训练中生成的模型会产生过于自信的问题，这是由于我们观察到了由模型产生的攻击示例预测结果的分布变得更加均匀。基于我们定义的对抗确定性，我们在对抗训练框架中添加了一个extragradient步骤，以搜索具有更低自信度的模型，从而进一步提高对抗泛化。我们的方法是通用的，可以轻松地与其他对抗训练方法结合使用。我们在图像准则上进行了广泛的实验，并证明了我们的方法可以有效地减轻Robust Overfitting现象，并生成具有改善的对抗性。
</details></li>
</ul>
<hr>
<h2 id="LLM4DV-Using-Large-Language-Models-for-Hardware-Test-Stimuli-Generation"><a href="#LLM4DV-Using-Large-Language-Models-for-Hardware-Test-Stimuli-Generation" class="headerlink" title="LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation"></a>LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04535">http://arxiv.org/abs/2310.04535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixi Zhang, Greg Chadwick, Hugo McNally, Yiren Zhao, Robert Mullins</li>
<li>for: 这 paper 是为了提高硬件设计验证的测试过程中的自动生成测试输入的效率而写的。</li>
<li>methods: 这 paper 使用了大型自然语言模型 (LLM) 的力量，并提出了一个新的测试框架，即 LLM4DV。这个框架包括一个互动式提取测试输入的模板，以及四种创新的提示改进，以支持管道执行并进一步提高其性能。</li>
<li>results: 对于三个自定义的设计下测试 (DUT) 模块，我们的实验表明，LLM4DV 在简单的 DUT 场景下能够高效地使用基本的数学逻辑和预训练知识来处理测试输入。虽然在复杂任务下其效率有所下降，但它仍然在相对 терms 中超过了传统的受限制随机测试 (CRT)。<details>
<summary>Abstract</summary>
Test stimuli generation has been a crucial but labor-intensive task in hardware design verification. In this paper, we revolutionize this process by harnessing the power of large language models (LLMs) and present a novel benchmarking framework, LLM4DV. This framework introduces a prompt template for interactively eliciting test stimuli from the LLM, along with four innovative prompting improvements to support the pipeline execution and further enhance its performance. We compare LLM4DV to traditional constrained-random testing (CRT), using three self-designed design-under-test (DUT) modules. Experiments demonstrate that LLM4DV excels in efficiently handling straightforward DUT scenarios, leveraging its ability to employ basic mathematical reasoning and pre-trained knowledge. While it exhibits reduced efficiency in complex task settings, it still outperforms CRT in relative terms. The proposed framework and the DUT modules used in our experiments will be open-sourced upon publication.
</details>
<details>
<summary>摘要</summary>
实验刺激生成是对硬件设计验证的重要但是劳动密集的任务。在这篇论文中，我们使用大型自然语言模型（LLM）来推翻这个过程，并提出了一个新的测试框架，即LLM4DV。这个框架包括一个互动式提示模板，以及四种创新的提示改进，以支持管线执行和进一步提高其性能。我们与传统的受限制随机测试（CRT）进行比较，使用三个自己设计的设计下的模组（DUT）。实验结果显示，LLM4DV在简单的DUT场景中能够高效地运行，利用其基本的数学逻辑和预先训练知识。然而，在复杂的任务设定中，其效率较低，但仍然在相对的 терminus上高于CRT。我们将提出的框架和DUT模组使用在实验中的会公开开源。
</details></li>
</ul>
<hr>
<h2 id="DPGOMI-Differentially-Private-Data-Publishing-with-Gaussian-Optimized-Model-Inversion"><a href="#DPGOMI-Differentially-Private-Data-Publishing-with-Gaussian-Optimized-Model-Inversion" class="headerlink" title="DPGOMI: Differentially Private Data Publishing with Gaussian Optimized Model Inversion"></a>DPGOMI: Differentially Private Data Publishing with Gaussian Optimized Model Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04528">http://arxiv.org/abs/2310.04528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjie Chen, Sen-ching S. Cheung, Chen-Nee Chuah</li>
<li>for: 保护敏感数据在GAN训练中的隐私</li>
<li>methods: 提出了一种新的差分隐私数据发布方法 called Differentially Private Data Publishing with Gaussian Optimized Model Inversion (DPGOMI)</li>
<li>results: DPGOMI在CIFAR10和SVHN标准数据集上表现出优于标准DP-GAN方法，同时保持同等的隐私水平<details>
<summary>Abstract</summary>
High-dimensional data are widely used in the era of deep learning with numerous applications. However, certain data which has sensitive information are not allowed to be shared without privacy protection. In this paper, we propose a novel differentially private data releasing method called Differentially Private Data Publishing with Gaussian Optimized Model Inversion (DPGOMI) to address this issue. Our approach involves mapping private data to the latent space using a public generator, followed by a lower-dimensional DP-GAN with better convergence properties. We evaluate the performance of DPGOMI on standard datasets CIFAR10 and SVHN. Our results show that DPGOMI outperforms the standard DP-GAN method in terms of Inception Score, Fr\'echet Inception Distance, and classification performance, while providing the same level of privacy. Our proposed approach offers a promising solution for protecting sensitive data in GAN training while maintaining high-quality results.
</details>
<details>
<summary>摘要</summary>
高维数据在深度学习时代广泛应用，但某些敏感信息不得分享无隐私保护。本文提出了一种新的差分隐私数据发布方法called differentially private data publishing with Gaussian optimized model inversion (DPGOMI)，以解决这个问题。我们的方法包括将私人数据映射到隐藏空间使用公共生成器，然后使用更好的协调性DP-GAN进行Lower-dimensional化。我们对标准Dataset CIFAR10和SVHN进行评估，结果显示DPGOMI在Inception Score、Fréchet Inception Distance和分类性能方面与标准DP-GAN方法比较，同时保持同等的隐私水平。我们的提议的方法可以保护深度学习中敏感数据的隐私，同时保持高质量结果。
</details></li>
</ul>
<hr>
<h2 id="SPADE-Sparsity-Guided-Debugging-for-Deep-Neural-Networks"><a href="#SPADE-Sparsity-Guided-Debugging-for-Deep-Neural-Networks" class="headerlink" title="SPADE: Sparsity-Guided Debugging for Deep Neural Networks"></a>SPADE: Sparsity-Guided Debugging for Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04519">http://arxiv.org/abs/2310.04519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arshia Soltani Moakhar, Eugenia Iofinova, Dan Alistarh</li>
<li>for: 提高深度学习模型的可解释性，即理解模型如何做出具体的决策。</li>
<li>methods: 使用 sample-targeted pruning 技术，从已经训练好的模型和目标样本开始，提供一个 “trace” 的网络执行跟踪，以减少网络中不必要的连接，提高模型的可解释性。</li>
<li>results: 对多种可解释性方法进行测试，发现使用 SPADE 预处理后，图像锐度地图的准确率得到了显著提高，同时neuron visualization 也得到了改善，帮助人们更好地理解网络的行为。<details>
<summary>Abstract</summary>
Interpretability, broadly defined as mechanisms for understanding why and how machine learning models reach their decisions, is one of the key open goals at the intersection of deep learning theory and practice. Towards this goal, multiple tools have been proposed to aid a human examiner in reasoning about a network's behavior in general or on a set of instances. However, the outputs of these tools-such as input saliency maps or neuron visualizations-are frequently difficult for a human to interpret, or even misleading, due, in particular, to the fact that neurons can be multifaceted, i.e., a single neuron can be associated with multiple distinct feature combinations. In this paper, we present a new general approach to address this problem, called SPADE, which, given a trained model and a target sample, uses sample-targeted pruning to provide a "trace" of the network's execution on the sample, reducing the network to the connections that are most relevant to the specific prediction. We demonstrate that preprocessing with SPADE significantly increases both the accuracy of image saliency maps across several interpretability methods and the usefulness of neuron visualizations, aiding humans in reasoning about network behavior. Our findings show that sample-specific pruning of connections can disentangle multifaceted neurons, leading to consistently improved interpretability.
</details>
<details>
<summary>摘要</summary>
优化机器学习模型的理解性是一个当前的开放问题，即使用深度学习理论和实践的交叉点。为了解决这个问题，多种工具已经被提出来帮助人类分析网络的行为。然而，这些工具的输出，如输入突出地图或神经视觉，经常难以 для人类理解，甚至是误leading的，因为神经元可以有多个不同的特征组合。在这篇论文中，我们提出了一种新的通用方法，called SPADE，它可以给一个已经训练的模型和一个目标样本提供一个"轨迹"，用于描述网络在该样本上的执行，从而减少网络到最 relevante的连接。我们示示了使用 SPADE 预处理可以显著提高图像突出地图的准确性和神经视觉的有用性，帮助人类更好地理解网络的行为。我们的发现表明，对特定样本进行预处理的连接可以分解多面神经元，导致Consistent improvement in interpretability。
</details></li>
</ul>
<hr>
<h2 id="Domain-Randomization-for-Sim2real-Transfer-of-Automatically-Generated-Grasping-Datasets"><a href="#Domain-Randomization-for-Sim2real-Transfer-of-Automatically-Generated-Grasping-Datasets" class="headerlink" title="Domain Randomization for Sim2real Transfer of Automatically Generated Grasping Datasets"></a>Domain Randomization for Sim2real Transfer of Automatically Generated Grasping Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04517">http://arxiv.org/abs/2310.04517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johann-Huber/qd_grasp">https://github.com/Johann-Huber/qd_grasp</a></li>
<li>paper_authors: Johann Huber, François Hélénon, Hippolyte Watrelot, Faiz Ben Amar, Stéphane Doncieux<br>for: 这paper主要针对的是如何使用数据驱动方法解决机器人抓取问题，以及如何 Addressing the challenge of sparse rewards in grasping.methods: 本paper使用了Quality-Diversity（QD）方法生成了超过7000个抓取轨迹，并在实际世界中进行了测试。results: 研究发现了几个领域随机化的质量标准和实际世界之间的相关性，并且确定了未来研究抓取问题的关键挑战。此外，QD方法已经被提议用于使抓取更加强健对域随机化。在Franka Research 3臂上，QD方法实现了84%的传输率。<details>
<summary>Abstract</summary>
Robotic grasping refers to making a robotic system pick an object by applying forces and torques on its surface. Many recent studies use data-driven approaches to address grasping, but the sparse reward nature of this task made the learning process challenging to bootstrap. To avoid constraining the operational space, an increasing number of works propose grasping datasets to learn from. But most of them are limited to simulations. The present paper investigates how automatically generated grasps can be exploited in the real world. More than 7000 reach-and-grasp trajectories have been generated with Quality-Diversity (QD) methods on 3 different arms and grippers, including parallel fingers and a dexterous hand, and tested in the real world. Conducted analysis on the collected measure shows correlations between several Domain Randomization-based quality criteria and sim-to-real transferability. Key challenges regarding the reality gap for grasping have been identified, stressing matters on which researchers on grasping should focus in the future. A QD approach has finally been proposed for making grasps more robust to domain randomization, resulting in a transfer ratio of 84% on the Franka Research 3 arm.
</details>
<details>
<summary>摘要</summary>
（简体中文） robotic grasping 指的是使 robotic 系统用力和扭矩对物体进行抓取。Recent studies 多使用数据驱动方法解决抓取问题，但这个任务的奖励稀缺性使得学习过程具有挑战。To avoid constraining the operational space, increasing number of works propose grasping datasets to learn from. However, most of them are limited to simulations. 本文 investigate 如何在实际世界中利用自动生成的抓取。More than 7000 reach-and-grasp trajectories have been generated with Quality-Diversity (QD) methods on 3 different arms and grippers, including parallel fingers and a dexterous hand, and tested in the real world. Collected measure analysis shows correlations between several Domain Randomization-based quality criteria and sim-to-real transferability. Key challenges regarding the reality gap for grasping have been identified, stressing matters on which researchers on grasping should focus in the future. A QD approach has finally been proposed for making grasps more robust to domain randomization, resulting in a transfer ratio of 84% on the Franka Research 3 arm.
</details></li>
</ul>
<hr>
<h2 id="Generative-Diffusion-From-An-Action-Principle"><a href="#Generative-Diffusion-From-An-Action-Principle" class="headerlink" title="Generative Diffusion From An Action Principle"></a>Generative Diffusion From An Action Principle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04490">http://arxiv.org/abs/2310.04490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akhil Premkumar</li>
<li>for: 这个论文主要用于描述一种生成扩散模型，它可以将给定的数据集转化为通用的噪声。</li>
<li>methods: 这种模型使用反扩散过程来生成新的样本，并通过训练神经网络来匹配数据集的梯度。</li>
<li>results: 通过将反扩散转化为优化控制问题，这种方法可以从动作原理中得出Score匹配，并将不同类型的扩散模型相连接。<details>
<summary>Abstract</summary>
Generative diffusion models synthesize new samples by reversing a diffusive process that converts a given data set to generic noise. This is accomplished by training a neural network to match the gradient of the log of the probability distribution of a given data set, also called the score. By casting reverse diffusion as an optimal control problem, we show that score matching can be derived from an action principle, like the ones commonly used in physics. We use this insight to demonstrate the connection between different classes of diffusion models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将给定文本翻译成简化中文。</SYS>>生成扩散模型可以Synthesize新样本，通过逆扩散过程将给定数据集转化为通用噪声。这是通过训练神经网络匹配给定数据集的梯度，也就是score的对数分布的梯度。我们将逆扩散视为优化控制问题，从而显示出score匹配可以由动作原理 derivation。我们利用这一点来描述不同类型的扩散模型之间的连接。
</details></li>
</ul>
<hr>
<h2 id="BrainSCUBA-Fine-Grained-Natural-Language-Captions-of-Visual-Cortex-Selectivity"><a href="#BrainSCUBA-Fine-Grained-Natural-Language-Captions-of-Visual-Cortex-Selectivity" class="headerlink" title="BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity"></a>BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04420">http://arxiv.org/abs/2310.04420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew F. Luo, Margaret M. Henderson, Michael J. Tarr, Leila Wehbe</li>
<li>for: 了解高等观觉 cortical 的功能组织</li>
<li>methods: 使用 data-driven 方法生成自然语言描述，并使用 contrastive vision-language 模型和大型自然语言模型生成可读的描述</li>
<li>results: 实现 voxel-level 描述，并通过 text-conditioned 图像生成技术发现 fine-grained semantic selectivity in body-selective areas<details>
<summary>Abstract</summary>
Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -- Semantic Captioning Using Brain Alignments ("BrainSCUBA") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high predicted activations. Finally, to demonstrate how our method enables scientific discovery, we perform exploratory investigations on the distribution of "person" representations in the brain, and discover fine-grained semantic selectivity in body-selective areas. Unlike earlier studies that decode text, our method derives voxel-wise captions of semantic selectivity. Our results show that BrainSCUBA is a promising means for understanding functional preferences in the brain, and provides motivation for further hypothesis-driven investigation of visual cortex.
</details>
<details>
<summary>摘要</summary>
Our method, called Semantic Captioning Using Brain Alignments (BrainSCUBA), builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high predicted activations.To demonstrate the potential of our method for scientific discovery, we perform exploratory investigations on the distribution of "person" representations in the brain. Our results reveal fine-grained semantic selectivity in body-selective areas, which is unlike earlier studies that have only decoded text. Our method derives voxel-wise captions of semantic selectivity, providing a new means for understanding functional preferences in the brain. Our results show that BrainSCUBA is a promising approach for understanding the functional organization of the higher visual cortex, and provides motivation for further hypothesis-driven investigation of visual cortex.Translated into Simplified Chinese:理解高级视觉 cortical 的功能组织是生物学的中心关注点。过去的研究主要使用手动选择的刺激来映射视觉和semantic 的选择性，这可能会导致结果受到先前的假设的影响。我们的方法可以让我们跳过这些传统的方法，我们引入了一种数据驱动的方法，该方法可以生成预测最大启动个 voxel 的自然语言描述。我们的方法，叫做 BrainSCUBA，基于视觉语言模型学习的丰富嵌入空间，并使用预训练的大语言模型来生成可读的描述。我们验证了我们的方法，通过高级视觉区域的细胞级描述。我们进一步进行了基于描述的图像生成，并证明我们的图像具有高预测活动。为了证明我们的方法的可能性，我们进行了探索性的寻究 "人" 表示在大脑中的分布。我们的结果表明，在体部选择区域中存在细致的semantic 选择性，这与之前的研究只是解码文本不同。我们的方法可以 derive  voxel 级的描述，提供了一种新的方法来理解大视觉 cortical 的功能组织。我们的结果表明，BrainSCUBA 是一种有前途的方法，可以帮助我们更好地理解大视觉 cortical 的功能组织，并提供了新的假设来研究视觉 cortical。
</details></li>
</ul>
<hr>
<h2 id="Functional-Interpolation-for-Relative-Positions-Improves-Long-Context-Transformers"><a href="#Functional-Interpolation-for-Relative-Positions-Improves-Long-Context-Transformers" class="headerlink" title="Functional Interpolation for Relative Positions Improves Long Context Transformers"></a>Functional Interpolation for Relative Positions Improves Long Context Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04418">http://arxiv.org/abs/2310.04418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli</li>
<li>for: 提高 transformer 模型在训练时间 longer than training 输入的性能</li>
<li>methods: 提出一种新的函数相对位编码FIRE，通过进行进度 interpolating 来提高 transformer 模型对 longer 上下文的泛化</li>
<li>results: FIRE 模型在零 shot 语言模型和长文本benchmark上表现更好，可以更好地泛化到 longer 上下文<details>
<summary>Abstract</summary>
Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.
</details>
<details>
<summary>摘要</summary>
防止 transformer 模型在训练时未使用的输入长度上 decay 性能是扩展context length的重要挑战。虽然 transformer 架构没有处理输入序列长度的限制，但选择的位置编码方法可能会限制这些模型在 longer inputs 上的性能。我们提出一种新的函数相对位置编码，称为 FIRE，以提高 transformer 对更长上下文的总体化。我们理论上证明 FIRE 可以表示一些流行的相对位置编码，如 T5 的 RPE、Alibi 和 Kerple。我们随后通过实验表明 FIRE 模型在 zero-shot 语言模型和长文本benchmark上具有更好的总体化能力。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Random-Feature-Model"><a href="#Diffusion-Random-Feature-Model" class="headerlink" title="Diffusion Random Feature Model"></a>Diffusion Random Feature Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04417">http://arxiv.org/abs/2310.04417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Esha Saha, Giang Tran</li>
<li>for: 这 paper 的目的是提出一种可解释的扩散模型，用于解决复杂的机器学习任务。</li>
<li>methods: 这 paper 使用了扩散模型的思想，并结合了随机特征模型的优点，以实现可解释性和数值相当的结果。 specifically, 作者们使用了现有的概率分布的扩展结果和分配匹配性的特性， derive 一种基于扩散模型的深度随机特征模型。</li>
<li>results: 作者们通过在时尚 MNIST 数据集和工具音频数据集上生成样本，验证了他们的模型的可解释性和数值相当性。<details>
<summary>Abstract</summary>
Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
</details>
<details>
<summary>摘要</summary>
Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.Translation in Simplified Chinese:Diffusion probabilistic models 已经成功地使用来生成噪音中的数据。然而，大多数 diffusion models  computationally expensive 和难以理解，lacking theoretical justification。Random feature models 在 interpretable 方面受到了欢迎，但它们在复杂的机器学习任务上的应用还受限。在这项工作中，我们提出了一种基于 diffusion model 的深度随机特征模型，这种模型具有可读性和与完全连接神经网络相同数量的可训练参数。我们将 existing results for random features 推广，并使用 score matching 属性 derive generalization bounds between the distribution of sampled data and the true distribution。我们验证了我们的发现，通过在 fashion MNIST dataset 和 instrumental audio data 上生成样本。
</details></li>
</ul>
<hr>
<h2 id="Why-Do-We-Need-Weight-Decay-in-Modern-Deep-Learning"><a href="#Why-Do-We-Need-Weight-Decay-in-Modern-Deep-Learning" class="headerlink" title="Why Do We Need Weight Decay in Modern Deep Learning?"></a>Why Do We Need Weight Decay in Modern Deep Learning?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04415">http://arxiv.org/abs/2310.04415</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tml-epfl/why-weight-decay">https://github.com/tml-epfl/why-weight-decay</a></li>
<li>paper_authors: Maksym Andriushchenko, Francesco D’Angelo, Aditya Varre, Nicolas Flammarion</li>
<li>for: 这篇论文探讨了现代深度学习中广泛使用的权重衰减技术，包括大语言模型的训练。尽管它在现代深度学习中广泛使用，但它的作用仍然不够了解。</li>
<li>methods: 这篇论文使用了现代深度学习中常用的SGD优化器，并研究了权重衰减对于过参数化深度网络的影响。</li>
<li>results: 研究发现，权重衰减不仅不是一种直接的正则化效果，而且可以改变深度学习训练的动态，从而提高SGD优化器的性能。具体来说，权重衰减可以增强SGD优化器中的损失稳定机制，使得过参数化深度网络在训练过程中更加稳定。同时，权重衰减还可以在bfloat16混合精度训练中防止损失快速增长，从而提高大语言模型的训练效果。<details>
<summary>Abstract</summary>
Weight decay is a broadly used technique for training state-of-the-art deep networks, including large language models. Despite its widespread usage, its role remains poorly understood. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For overparameterized deep networks, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for underparameterized large language models trained with nearly online SGD, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss. Moreover, we show that weight decay also prevents sudden loss divergences for bfloat16 mixed-precision training which is a crucial tool for LLM training. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight decay is never useful as an explicit regularizer but instead changes the training dynamics in a desirable way. Our code is available at https://github.com/tml-epfl/why-weight-decay.
</details>
<details>
<summary>摘要</summary>
“weight decay”是现代深度学习中广泛使用的技术，包括大型语言模型的训练。尽管其广泛使用，但其作用仍然不够了解。在这个工作中，我们强调“weight decay”在现代深度学习中的角色与经典学习理论中的调整效应不同。对于过parameterized的深度网络来说，我们显示了weight decay对优化律动的影响，增强了SGD的隐式调整机制，使得这些网络在训练中更加稳定。相反，对于大型语言模型使用nearly online SGD进行训练的情况下，我们描述了weight decay如何在数据来调整偏差和变分之间的平衡，导致训练损失下降。此外，我们还证明了weight decay可以防止bfloat16混合精度训练中的损失峰值分化。总之，我们提出了一个统一的见解，即“weight decay”不是一个直接的调整器，而是改变训练律动的方式，以提高深度学习模型的性能。我们的代码可以在https://github.com/tml-epfl/why-weight-decay上获取。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Predicting-and-Better-Resolving-Q-Value-Divergence-in-Offline-RL"><a href="#Understanding-Predicting-and-Better-Resolving-Q-Value-Divergence-in-Offline-RL" class="headerlink" title="Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL"></a>Understanding, Predicting and Better Resolving Q-Value Divergence in Offline-RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04411">http://arxiv.org/abs/2310.04411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueyang130/seem">https://github.com/yueyang130/seem</a></li>
<li>paper_authors: Yang Yue, Rui Lu, Bingyi Kang, Shiji Song, Gao Huang<br>for: 这个研究的目的是解释在无实际动力学训练的情况下Q值估计的异常情况，并提出一种改进的解决方案。methods: 该研究使用了NTK来计算Q网络在训练过程中的演变性，并提出了一种自适应 eigenvalues度量来评估训练过程中的异常情况。results: 研究发现，自适应 eigenvalues度量可以准确地预测训练过程中Q值估计的异常情况，并且可以预测模型的 нор 的增长和崩溃步骤。实验结果与理论分析一致。此外，研究还提出了一种改进方案，通过修改模型的架构来避免异常情况，并通过广泛的实验研究证明其效果。<details>
<summary>Abstract</summary>
The divergence of the Q-value estimation has been a prominent issue in offline RL, where the agent has no access to real dynamics. Traditional beliefs attribute this instability to querying out-of-distribution actions when bootstrapping value targets. Though this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent. In this work, we aim to thoroughly comprehend this mechanism and attain an improved solution. We first identify a fundamental pattern, self-excitation, as the primary cause of Q-value estimation divergence in offline RL. Then, we propose a novel Self-Excite Eigenvalue Measure (SEEM) metric based on Neural Tangent Kernel (NTK) to measure the evolving property of Q-network at training, which provides an intriguing explanation of the emergence of divergence. For the first time, our theory can reliably decide whether the training will diverge at an early stage, and even predict the order of the growth for the estimated Q-value, the model's norm, and the crashing step when an SGD optimizer is used. The experiments demonstrate perfect alignment with this theoretic analysis. Building on our insights, we propose to resolve divergence from a novel perspective, namely improving the model's architecture for better extrapolating behavior. Through extensive empirical studies, we identify LayerNorm as a good solution to effectively avoid divergence without introducing detrimental bias, leading to superior performance. Experimental results prove that it can still work in some most challenging settings, i.e. using only 1 transitions of the dataset, where all previous methods fail. Moreover, it can be easily plugged into modern offline RL methods and achieve SOTA results on many challenging tasks. We also give unique insights into its effectiveness.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate into Simplified ChineseOffline RL 中的 Q-值估计差异问题很 prominent,  agent 无法访问实际 dynamics.  traditional beliefs 认为这种不稳定性来自于查询 out-of-distribution 动作时的估计 Q-value.  although this issue can be alleviated with policy constraints or conservative Q estimation, a theoretical understanding of the underlying mechanism causing the divergence has been absent.在这项工作中，我们希望彻底了解这种机制，并提出一个改进的解决方案。我们首先认为自我触发（self-excitation）是 offline RL 中 Q-值估计差异的基本原因。然后，我们提出了一种基于 Neural Tangent Kernel (NTK) 的 Self-Excite Eigenvalue Measure (SEEM) 度量，用于测量 Q-网络在训练过程中的演变性质。这提供了让人感到奇异的解释：Q-网络的训练过程中的差异是如何 emerge 的？我们的理论可以在训练的早期准确地判断 Whether the training will diverge, 并且可以预测 Q-值估计的增长顺序、模型的 norm 和折冲步骤的增长速率。实验结果与我们的分析完全一致。基于我们的发现，我们提出了一种新的方法，以改进模型的架构，以便更好地推断。通过广泛的实验研究，我们发现层 normalization (LayerNorm) 是一种有效的方法，可以避免差异而不导致偏见。这种方法可以在许多最有挑战性的任务上实现 SOTA 结果。此外，我们还给出了这种方法的独特效果。在这项工作中，我们还进行了一系列的实验研究，以证明我们的发现和方法的有效性。我们发现，只需使用 dataset 中的一个转移，我们可以使用 LayerNorm 来避免差异，并且可以在许多最有挑战性的任务上实现 SOTA 结果。此外，我们还发现，层 normalization 可以轻松地整合到现有的 offline RL 方法中，以提高其性能。
</details></li>
</ul>
<hr>
<h2 id="On-the-Embedding-Collapse-when-Scaling-up-Recommendation-Models"><a href="#On-the-Embedding-Collapse-when-Scaling-up-Recommendation-Models" class="headerlink" title="On the Embedding Collapse when Scaling up Recommendation Models"></a>On the Embedding Collapse when Scaling up Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04400">http://arxiv.org/abs/2310.04400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingzhuo Guo, Junwei Pan, Ximei Wang, Baixu Chen, Jie Jiang, Mingsheng Long</li>
<li>for: 本研究旨在探讨大深度基本模型在推荐系统中的应用，并评估大模型是否能够得到更好的性能。</li>
<li>methods: 本研究使用实验和理论分析来探讨大模型中的嵌入层 collapse 问题，并提出一种简单 yet effective 的多嵌入设计来解决这个问题。</li>
<li>results: 经过广泛的实验证明，提出的多嵌入设计能够提供不同的推荐模型中的可扩展性。<details>
<summary>Abstract</summary>
Recent advances in deep foundation models have led to a promising trend of developing large recommendation models to leverage vast amounts of available data. However, we experiment to scale up existing recommendation models and observe that the enlarged models do not improve satisfactorily. In this context, we investigate the embedding layers of enlarged models and identify a phenomenon of embedding collapse, which ultimately hinders scalability, wherein the embedding matrix tends to reside in a low-dimensional subspace. Through empirical and theoretical analysis, we demonstrate that the feature interaction module specific to recommendation models has a two-sided effect. On the one hand, the interaction restricts embedding learning when interacting with collapsed embeddings, exacerbating the collapse issue. On the other hand, feature interaction is crucial in mitigating the fitting of spurious features, thereby improving scalability. Based on this analysis, we propose a simple yet effective multi-embedding design incorporating embedding-set-specific interaction modules to capture diverse patterns and reduce collapse. Extensive experiments demonstrate that this proposed design provides consistent scalability for various recommendation models.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:近期深基模型的进步导致了推荐模型的大型化，以利用庞大数据。然而，我们对现有模型进行扩大，并观察到扩大后的模型不会改善。在这种情况下，我们对扩大后的模型的嵌入层进行调查，并发现一种嵌入潮涨现象，这 ultimately hinders scalability, wherein the embedding matrix tends to reside in a low-dimensional subspace.通过实验和理论分析，我们证明了推荐模型特有的Feature interaction模块有两个面向的效果。一方面，交互 restricts嵌入学习 When interacting with collapsed embeddings, exacerbating the collapse issue。另一方面，Feature interaction is crucial in mitigating the fitting of spurious features, thereby improving scalability。基于这种分析，我们提出了一种简单 yet effective的多嵌入设计，其中包括嵌入集specific interaction模块，以 Capture diverse patterns and reduce collapse。经验示出，该设计可以提供不同推荐模型的一致性的可扩展性。
</details></li>
</ul>
<hr>
<h2 id="MBTFNet-Multi-Band-Temporal-Frequency-Neural-Network-For-Singing-Voice-Enhancement"><a href="#MBTFNet-Multi-Band-Temporal-Frequency-Neural-Network-For-Singing-Voice-Enhancement" class="headerlink" title="MBTFNet: Multi-Band Temporal-Frequency Neural Network For Singing Voice Enhancement"></a>MBTFNet: Multi-Band Temporal-Frequency Neural Network For Singing Voice Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04369">http://arxiv.org/abs/2310.04369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Xu, Zhouxuan Chen, Zhili Tan, Shubo Lv, Runduo Han, Wenjiang Zhou, Weifeng Zhao, Lei Xie</li>
<li>for: 这个论文旨在提出一种新的多频带时间频率神经网络（MBTFNet），用于歌唱音频提升。</li>
<li>methods: 这个模型利用了inter和intra频道模型，以及双路模型，以更好地处理全频率信号。</li>
<li>results:  experiments表明，提出的模型在比较多种现状背景音频提升模型时，表现明显更好。<details>
<summary>Abstract</summary>
A typical neural speech enhancement (SE) approach mainly handles speech and noise mixtures, which is not optimal for singing voice enhancement scenarios. Music source separation (MSS) models treat vocals and various accompaniment components equally, which may reduce performance compared to the model that only considers vocal enhancement. In this paper, we propose a novel multi-band temporal-frequency neural network (MBTFNet) for singing voice enhancement, which particularly removes background music, noise and even backing vocals from singing recordings. MBTFNet combines inter and intra-band modeling for better processing of full-band signals. Dual-path modeling are introduced to expand the receptive field of the model. We propose an implicit personalized enhancement (IPE) stage based on signal-to-noise ratio (SNR) estimation, which further improves the performance of MBTFNet. Experiments show that our proposed model significantly outperforms several state-of-the-art SE and MSS models.
</details>
<details>
<summary>摘要</summary>
一般来说，神经音频增强（SE）方法主要处理speech和噪声混合，而这并不是最佳的歌唱voice增强场景。音乐源分离（MSS）模型对 vocals和不同伴奏元素进行同等处理，这可能会降低模型性能，比如只考虑 vocals增强。在这篇论文中，我们提出了一种新的多频带时间频率神经网络（MBTFNet），它特别是从歌曲录制中除去背景音乐、噪声和 backing vocals。MBTFNet组合了内部和外部模型，以更好地处理全带信号。我们还引入了双路模型，以扩大模型的感知范围。我们提出了隐式个性化增强（IPE）阶段，基于信号噪声比（SNR）估计，以进一步提高MBTFNet的性能。实验表明，我们提出的模型在多个状态的SE和MSS模型之上显著超越。
</details></li>
</ul>
<hr>
<h2 id="A-Marketplace-Price-Anomaly-Detection-System-at-Scale"><a href="#A-Marketplace-Price-Anomaly-Detection-System-at-Scale" class="headerlink" title="A Marketplace Price Anomaly Detection System at Scale"></a>A Marketplace Price Anomaly Detection System at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04367">http://arxiv.org/abs/2310.04367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshit Sarpal, Qiwen Kang, Fangping Huang, Yang Song, Lijie Wan</li>
<li>for: 这篇论文是为了解决在线市场平台上每天有大量价格更新，但这些更新可能会导致数据质量问题的问题。</li>
<li>methods: 该论文提出了一种可扩展的价格异常检测框架，使用邻域和历史价格趋势来生成一个可靠的价格上限。</li>
<li>results: 论文的方法可以提高精准anchor覆盖率，特别是在高易受影响的 Item 子集中。在高易受影响的 Item 子集中，论文的方法可以提高精准anchor覆盖率高达46.6%。<details>
<summary>Abstract</summary>
Online marketplaces execute large volume of price updates that are initiated by individual marketplace sellers each day on the platform. This price democratization comes with increasing challenges with data quality. Lack of centralized guardrails that are available for a traditional online retailer causes a higher likelihood for inaccurate prices to get published on the website, leading to poor customer experience and potential for revenue loss. We present MoatPlus (Masked Optimal Anchors using Trees, Proximity-based Labeling and Unsupervised Statistical-features), a scalable price anomaly detection framework for a growing marketplace platform. The goal is to leverage proximity and historical price trends from unsupervised statistical features to generate an upper price bound. We build an ensemble of models to detect irregularities in price-based features, exclude irregular features and use optimized weighting scheme to build a reliable price bound in real-time pricing pipeline. We observed that our approach improves precise anchor coverage by up to 46.6% in high-vulnerability item subsets
</details>
<details>
<summary>摘要</summary>
在线市场上执行大量的价格更新，每天由个人市场商家发起的请求非常大。这种价格民主化带来了数据质量的增加挑战。由于在传统的在线零售商家中没有中央的指南箱，导致更高的假象价格在网站上发布，从而导致客户体验不佳，并且可能导致收入损失。我们介绍了MoatPlus（嵌入最优锚点使用树、距离基于标签和无监督统计特征），一个可扩展的价格异常检测框架，用于一个快速发展的市场平台。我们的目标是利用距离和历史价格趋势从无监督统计特征中生成一个可靠的价格上限。我们建立了一个ensemble模型，检测价格基本特征中的异常，排除异常特征，并使用优化的权重方案生成一个可靠的价格 bound。我们观察到，我们的方法可以提高准确的锚点覆盖率达到46.6%在高抵触性ITEM subsets中。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Transformer-Activation-Sparsity-with-Dynamic-Inference"><a href="#Exploiting-Transformer-Activation-Sparsity-with-Dynamic-Inference" class="headerlink" title="Exploiting Transformer Activation Sparsity with Dynamic Inference"></a>Exploiting Transformer Activation Sparsity with Dynamic Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04361">http://arxiv.org/abs/2310.04361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikołaj Piórczyński, Filip Szatkowski, Klaudia Bałazy, Bartosz Wójcik</li>
<li>for: 降低 transformer 模型的执行成本，使其更加实用。</li>
<li>methods: 使用 activation sparsity 和 mixture of experts (MoE) 技术，将 dense 模型转换成 sparse MoE 版本。</li>
<li>results: 可以培训小型闭合网络，成功地预测每个专家的贡献。 另外，还提出了一种动态确定每个token执行的专家数量的机制。 DSTI 可以应用于任何 transformer 基于的架构，并无影响准确性。 对 BERT-base 分类模型，可以减少执行成本约 60%。<details>
<summary>Abstract</summary>
Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference cost by almost 60%.
</details>
<details>
<summary>摘要</summary>
“对于变数增强模型（Transformer）来说，即使它们在表现上表现出色，但是它们仍面临实际的限制，主要是 Computational cost 高昂。同时，先前的研究表明，这些模型中存在较多的活动缺失，这表明这些模型中存在累累的计算。在这篇论文中，我们提出了一种方法，即动态简化transformer推干（DSTI），它可以将transformer模型转换为其简化的 Mixture of Experts（MoE）版本，并且在推干过程中强制实施活动缺失。我们显示了可以训练小型的闸道网络，并且可以成功预测每个专家的相对贡献。此外，我们引入了一个机制，将专家的数量 dynamically 决定为每个 Token 的需要。DSTI 可以应用于任何基于 transformer 的架构，并且对精度造成无法可预测的影响。对于 BERT-base 分类模型，我们可以降低推干成本约 60%。”
</details></li>
</ul>
<hr>
<h2 id="Integrating-Transformations-in-Probabilistic-Circuits"><a href="#Integrating-Transformations-in-Probabilistic-Circuits" class="headerlink" title="Integrating Transformations in Probabilistic Circuits"></a>Integrating Transformations in Probabilistic Circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04354">http://arxiv.org/abs/2310.04354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Schierenbeck, Vladimir Vutov, Thorsten Dickhaus, Michael Beetz</li>
<li>for: 这个研究旨在解决 probabilistic circuits 的预测限制，并提出了一种方法来缓解这种限制。</li>
<li>methods: 这种方法基于独立成分分析（ICA），并是基于 joint probability trees 的扩展。</li>
<li>results: 对七个 benchmark 数据集和实际 робот数据进行测试，该方法能够 achieve higher likelihoods 使用 fewer parameters，并可以进行有效的采样和approximate inference。<details>
<summary>Abstract</summary>
This study addresses the predictive limitation of probabilistic circuits and introduces transformations as a remedy to overcome it. We demonstrate this limitation in robotic scenarios. We motivate that independent component analysis is a sound tool to preserve the independence properties of probabilistic circuits. Our approach is an extension of joint probability trees, which are model-free deterministic circuits. By doing so, it is demonstrated that the proposed approach is able to achieve higher likelihoods while using fewer parameters compared to the joint probability trees on seven benchmark data sets as well as on real robot data. Furthermore, we discuss how to integrate transformations into tree-based learning routines. Finally, we argue that exact inference with transformed quantile parameterized distributions is not tractable. However, our approach allows for efficient sampling and approximate inference.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "probabilistic circuits" ⇒ 概率Circuit (Simplified Chinese)* "independent component analysis" ⇒ 独立元分析 (Simplified Chinese)* "joint probability trees" ⇒ 共同概率树 (Simplified Chinese)* "transformations" ⇒ 变换 (Simplified Chinese)* "quantile parameterized distributions" ⇒ 量词分布 (Simplified Chinese)* "exact inference" ⇒ 精确推理 (Simplified Chinese)* "efficient sampling" ⇒ 高效采样 (Simplified Chinese)* "approximate inference" ⇒ 近似推理 (Simplified Chinese)
</details></li>
</ul>
<hr>
<h2 id="Fair-Feature-Importance-Scores-for-Interpreting-Tree-Based-Methods-and-Surrogates"><a href="#Fair-Feature-Importance-Scores-for-Interpreting-Tree-Based-Methods-and-Surrogates" class="headerlink" title="Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates"></a>Fair Feature Importance Scores for Interpreting Tree-Based Methods and Surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04352">http://arxiv.org/abs/2310.04352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Camille Olivia Little, Debolina Halder Lina, Genevera I. Allen</li>
<li>for: 这种论文旨在提高机器学习系统的可解释性和公平性。</li>
<li>methods: 本论文提出了一种新的公平特征重要度分数，用于解释树状模型中各特征对公平性的贡献。</li>
<li>results: 通过实验和实际例子，论文表明了该分数的有效性，可以用于解释树状模型、树状集合和其他复杂机器学习系统中的公平性。<details>
<summary>Abstract</summary>
Across various sectors such as healthcare, criminal justice, national security, finance, and technology, large-scale machine learning (ML) and artificial intelligence (AI) systems are being deployed to make critical data-driven decisions. Many have asked if we can and should trust these ML systems to be making these decisions. Two critical components are prerequisites for trust in ML systems: interpretability, or the ability to understand why the ML system makes the decisions it does, and fairness, which ensures that ML systems do not exhibit bias against certain individuals or groups. Both interpretability and fairness are important and have separately received abundant attention in the ML literature, but so far, there have been very few methods developed to directly interpret models with regard to their fairness. In this paper, we focus on arguably the most popular type of ML interpretation: feature importance scores. Inspired by the use of decision trees in knowledge distillation, we propose to leverage trees as interpretable surrogates for complex black-box ML models. Specifically, we develop a novel fair feature importance score for trees that can be used to interpret how each feature contributes to fairness or bias in trees, tree-based ensembles, or tree-based surrogates of any complex ML system. Like the popular mean decrease in impurity for trees, our Fair Feature Importance Score is defined based on the mean decrease (or increase) in group bias. Through simulations as well as real examples on benchmark fairness datasets, we demonstrate that our Fair Feature Importance Score offers valid interpretations for both tree-based ensembles and tree-based surrogates of other ML systems.
</details>
<details>
<summary>摘要</summary>
各个领域，如医疗、刑事司法、国家安全、金融和技术，都在大规模机器学习（ML）和人工智能（AI）系统中进行关键的数据驱动决策。许多人问到我们是否可以和应该信任这些ML系统来做出决策。两个关键组件是必需的 для信任ML系统：可解释性，即理解ML系统为什么会做出这些决策，以及公平，即确保ML系统不会对某些个人或群体产生偏见。两者都很重要，而且在ML文献中已经得到了充分的关注，但是直到现在，尚未有多少方法可以直接解释模型的公平性。在这篇论文中，我们将关注最受欢迎的ML解释方法之一：特征重要性分数。通过使用决策树在知识储存中的使用，我们提议利用树来解释复杂黑盒ML模型。我们开发了一种新的公平特征重要性分数，可以用来解释树、树 ensemble 或任何复杂ML系统的公平性或偏见中各个特征的贡献。与popular的mean decrease in impurity一样，我们的公平特征重要性分数基于mean decrease (或增加) in group bias。通过实验和真实的例子，我们示例了我们的公平特征重要性分数在树 ensemble 和树基于其他ML系统的surrogate 中提供了有效的解释。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Grasp-from-Somewhere-to-Anywhere"><a href="#Learning-to-Grasp-from-Somewhere-to-Anywhere" class="headerlink" title="Learning to Grasp: from Somewhere to Anywhere"></a>Learning to Grasp: from Somewhere to Anywhere</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04349">http://arxiv.org/abs/2310.04349</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Johann-Huber/qd_grasp">https://github.com/Johann-Huber/qd_grasp</a></li>
<li>paper_authors: François Hélénon, Johann Huber, Faïz Ben Amar, Stéphane Doncieux</li>
<li>for: 研究人员想要对不同物体和机械臂进行自动化抓取，但抓取问题仍然是一个尚未完全解决的多学科问题，特别是在对不 convention 的形状或高度自适应的情况下。</li>
<li>methods: 本研究使用了Quality-Diversity（QD）方法，通过学习物体抓取的具体位置，并将其应用到新的物体位置。使用 RGB-D 数据流，视觉管线首先检测目标物体，预测其6DOF 位置，然后追踪它。</li>
<li>results: 本研究成功地将 QD 生成的抓取轨迹适应到新的物体位置，并在实际世界中进行了多种物体和机械臂的测试。实际应用中获得的转移率与 simulation 中获得的转移率相似，显示了提案的方法的效率。<details>
<summary>Abstract</summary>
Robotic grasping is still a partially solved, multidisciplinary problem where data-driven techniques play an increasing role. The sparse nature of rewards make the automatic generation of grasping datasets challenging, especially for unconventional morphologies or highly actuated end-effectors. Most approaches for obtaining large-scale datasets rely on numerous human-provided demonstrations or heavily engineered solutions that do not scale well. Recent advances in Quality-Diversity (QD) methods have investigated how to learn object grasping at a specific pose with different robot morphologies. The present work introduces a pipeline for adapting QD-generated trajectories to new object poses. Using an RGB-D data stream, the vision pipeline first detects the targeted object, predicts its 6-DOF pose, and finally tracks it. An automatically generated reach-and-grasp trajectory can then be adapted by projecting it relatively to the object frame. Hundreds of trajectories have been deployed into the real world on several objects and with different robotic setups: a Franka Research 3 with a parallel gripper and a UR5 with a dexterous SIH Schunk hand. The transfer ratio obtained when applying transformation to the object pose matches the one obtained when the object pose matches the simulation, demonstrating the efficiency of the proposed approach.
</details>
<details>
<summary>摘要</summary>
机器人抓取仍然是一个部分解决的多学科问题，数据驱动技术在解决这个问题中扮演着越来越重要的角色。由于抓取任务的奖励 sparse，自动生成抓取数据集的问题特别是对于不 convent ional 的形态或高度 actuated 的底部件而更加挑战。大多数方法都需要大量的人类示例或者高度工程化的解决方案，这些方法并不具有扩展性。现有的进展在 Quality-Diversity（QD）方法中，研究如何通过Specific pose 学习对象抓取。本文介绍一个管道，用于适应 QD 生成的轨迹。使用 RGB-D 数据流，视觉管道首先检测目标对象，预测其 Six-DOF 位姿，然后跟踪它。接着，通过将 QD 生成的轨迹 проек到对象帧，自动生成的抓取轨迹可以进行适应。在实际世界中，已经部署了数百个轨迹，在多种对象和不同的机器人设置下进行了测试：Franka Research 3  WITH 平行握手和 UR5  WITH 灵活 SIH Schunk 手。转换对象姿势时的转换率与 simulation 中的转换率相同，表明提posed 方法的效率。
</details></li>
</ul>
<hr>
<h2 id="Functional-Geometry-Guided-Protein-Sequence-and-Backbone-Structure-Co-Design"><a href="#Functional-Geometry-Guided-Protein-Sequence-and-Backbone-Structure-Co-Design" class="headerlink" title="Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design"></a>Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04343">http://arxiv.org/abs/2310.04343</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jocelynsong/naepro">https://github.com/jocelynsong/naepro</a></li>
<li>paper_authors: Zhenqiao Song, Yunlong Zhao, Wenxian Shi, Yang Yang, Lei Li</li>
<li>for: 本文提出了一种基于自动探测功能位点的蛋白质序列和结构设计模型，即 NAEPro。</li>
<li>methods:  NAEPro 使用了一个拥有自适应和对称层的交互网络，可以捕捉全序列的全局相关性和三维空间中最近氨酸的本地影响。</li>
<li>results:  experimental results 表明，NAEPro 在两个蛋白质数据集，$\beta$-lactamase 和 myoglobin 上表现出最高的氨酸恢复率、TM-score 和最低的 RMSD。这些发现证明了 NAEPro 的能力设计高效的蛋白质序列和结构。<details>
<summary>Abstract</summary>
Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capability of our model to design protein sequences and structures that closely resemble their natural counterparts. Furthermore, in-depth analysis further confirms our model's ability to generate highly effective proteins capable of binding to their target metallocofactors. We provide code, data and models in Github.
</details>
<details>
<summary>摘要</summary>
蛋白质是生物组织中重要的macromolecule，负责许多生物过程的核心功能。设计功能蛋白质的设计是非常重要的。蛋白质的序列和结构是强相関的，它们共同决定蛋白质的功能。在这篇文章中，我们提出了NAEPro，一个可以同时设计蛋白质序列和结构的模型，基于自动检测到的功能位点。NAEPro运用了跨维度对称层和注意力层，可以捕捉整个序列的全球相关和三维空间中最近的氨酸的本地影响。这种架构可以实现有效且经济的讯息传递。我们评估了我们的模型和几个强大的基eline在β-lactamase和myoglobin两个蛋白质数据集上。实验结果显示，我们的模型在所有竞争者中连续获得最高的氨酸重建率、TM-score和最低的RMSD。这些发现证明了我们的模型能够设计功能蛋白质序列和结构，与自然蛋白质序列和结构相似。此外，我们进行了深入分析，确认了我们的模型能够生成高效的蛋白质，可以与其标的金属复合物结合。我们在GitHub上提供了代码、数据和模型。
</details></li>
</ul>
<hr>
<h2 id="Applying-Reinforcement-Learning-to-Option-Pricing-and-Hedging"><a href="#Applying-Reinforcement-Learning-to-Option-Pricing-and-Hedging" class="headerlink" title="Applying Reinforcement Learning to Option Pricing and Hedging"></a>Applying Reinforcement Learning to Option Pricing and Hedging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04336">http://arxiv.org/abs/2310.04336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zoran Stoiljkovic</li>
<li>for: This paper provides an overview of recent advances in reinforcement learning for pricing and hedging financial instruments, with a focus on the Q-Learning Black Scholes approach.</li>
<li>methods: The paper uses a model-free and data-driven approach that bridges the traditional Black and Scholes model with novel artificial intelligence algorithms.</li>
<li>results: The algorithm is found to be an accurate estimator under different levels of volatility and hedging frequency, and exhibits robust performance across various levels of option’s moneyness. Additionally, the algorithm incorporates proportional transaction costs, which have diverse impacts on profit and loss.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文提供了现代金融工具价格和保险的最新进展，主要关注黑施勒斯（2017）提出的Q学习黑施勒斯方法。这种学习方法将传统的黑施勒斯模型与人工智能算法相结合，实现了完全无模型和数据驱动的选项价格和保险。</li>
<li>methods: 本论文使用无模型和数据驱动的Q学习方法，bridges传统的黑施勒斯模型与新的人工智能算法。</li>
<li>results: 结果表明，该模型在不同的状态变量和场景下具有准确估计的性能，并且在不同的投资额和保险频率下展现出了稳定性。此外，该方法在不同的货币性水平下也具有稳定性。最后，该算法包含了不同状态变量的负担费用，这些负担费用会影响盈亏。<details>
<summary>Abstract</summary>
This thesis provides an overview of the recent advances in reinforcement learning in pricing and hedging financial instruments, with a primary focus on a detailed explanation of the Q-Learning Black Scholes approach, introduced by Halperin (2017). This reinforcement learning approach bridges the traditional Black and Scholes (1973) model with novel artificial intelligence algorithms, enabling option pricing and hedging in a completely model-free and data-driven way. This paper also explores the algorithm's performance under different state variables and scenarios for a European put option. The results reveal that the model is an accurate estimator under different levels of volatility and hedging frequency. Moreover, this method exhibits robust performance across various levels of option's moneyness. Lastly, the algorithm incorporates proportional transaction costs, indicating diverse impacts on profit and loss, affected by different statistical properties of the state variables.
</details>
<details>
<summary>摘要</summary>
这个论文提供了现代补偿学习在财务工具估价和投资风险管理方面的最新进展，主要强调黑尔伯恩（2017）提出的Q学习黑沃尔（1973）方法的详细解释。这种补偿学习方法结合了传统的黑沃尔模型和新型人工智能算法，实现了完全无模型和数据驱动的选项估价和投资风险管理。本文还探讨了算法在不同状态变量和场景下的性能，发现该模型在不同的震动率和投资频率下具有准确估计性。此外，这种方法在不同的财务质量下也表现了强劲的稳定性。最后，该算法包含了不同状态变量的负担成本，表明不同状态变量对利润和损失产生的多种影响。
</details></li>
</ul>
<hr>
<h2 id="Saliency-Guided-Hidden-Associative-Replay-for-Continual-Learning"><a href="#Saliency-Guided-Hidden-Associative-Replay-for-Continual-Learning" class="headerlink" title="Saliency-Guided Hidden Associative Replay for Continual Learning"></a>Saliency-Guided Hidden Associative Replay for Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04334">http://arxiv.org/abs/2310.04334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baithebest/sharc">https://github.com/baithebest/sharc</a></li>
<li>paper_authors: Guangji Bai, Qilong Zhao, Xiaoyang Jiang, Yifei Zhang, Liang Zhao</li>
<li>for: 这个研究旨在提出一个基于协调 associative memory 和 replay 的新方法，以解决 continual learning 中的严重遗传问题。</li>
<li>methods: 本研究使用 sparse memory encoding 技术，将重要的数据段落存储在 associative memory 中，并使用 content-focused memory retrieval 机制，以快速和几乎完美地回传数据。</li>
<li>results: 实验结果显示，该方法可以有效解决 continual learning 中的严重遗传问题，并且在不同的 continual learning 任务中表现出色。<details>
<summary>Abstract</summary>
Continual Learning is a burgeoning domain in next-generation AI, focusing on training neural networks over a sequence of tasks akin to human learning. While CL provides an edge over traditional supervised learning, its central challenge remains to counteract catastrophic forgetting and ensure the retention of prior tasks during subsequent learning. Amongst various strategies to tackle this, replay based methods have emerged as preeminent, echoing biological memory mechanisms. However, these methods are memory intensive, often preserving entire data samples, an approach inconsistent with humans selective memory retention of salient experiences. While some recent works have explored the storage of only significant portions of data in episodic memory, the inherent nature of partial data necessitates innovative retrieval mechanisms. Current solutions, like inpainting, approximate full data reconstruction from partial cues, a method that diverges from genuine human memory processes. Addressing these nuances, this paper presents the Saliency Guided Hidden Associative Replay for Continual Learning. This novel framework synergizes associative memory with replay-based strategies. SHARC primarily archives salient data segments via sparse memory encoding. Importantly, by harnessing associative memory paradigms, it introduces a content focused memory retrieval mechanism, promising swift and near-perfect recall, bringing CL a step closer to authentic human memory processes. Extensive experimental results demonstrate the effectiveness of our proposed method for various continual learning tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-Losses-for-Decision-Focused-Learning"><a href="#Robust-Losses-for-Decision-Focused-Learning" class="headerlink" title="Robust Losses for Decision-Focused Learning"></a>Robust Losses for Decision-Focused Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04328">http://arxiv.org/abs/2310.04328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Noah Schutte, Krzysztof Postek, Neil Yorke-Smith</li>
<li>for: 本研究旨在探讨用于做出精细决策的优化模型中的不确定参数，以及如何通过预测来训练这些参数。</li>
<li>methods: 本研究使用了预测后优化的方法，即使用预测模型来预测参数的不确定性，然后使用这些预测来训练优化模型。</li>
<li>results: 研究发现，使用 robust regret loss 可以更好地预测实际的 regret，并且可以降低测试样本上的 regret。<details>
<summary>Abstract</summary>
Optimization models used to make discrete decisions often contain uncertain parameters that are context-dependent and are estimated through prediction. To account for the quality of the decision made based on the prediction, decision-focused learning (end-to-end predict-then-optimize) aims at training the predictive model to minimize regret, i.e., the loss incurred by making a suboptimal decision. Despite the challenge of this loss function being possibly non-convex and in general non-differentiable, effective gradient-based learning approaches have been proposed to minimize the expected loss, using the empirical loss as a surrogate. However, empirical regret can be an ineffective surrogate because the uncertainty in the optimization model makes the empirical regret unequal to the expected regret in expectation. To illustrate the impact of this inequality, we evaluate the effect of aleatoric and epistemic uncertainty on the accuracy of empirical regret as a surrogate. Next, we propose three robust loss functions that more closely approximate expected regret. Experimental results show that training two state-of-the-art decision-focused learning approaches using robust regret losses improves test-sample empirical regret in general while keeping computational time equivalent relative to the number of training epochs.
</details>
<details>
<summary>摘要</summary>
优化模型常用于作出精细决策，它们的参数通常受到上下文依赖和预测的不确定性影响。为了考虑决策质量基于预测的问题，决策专注学习（终端预测然后优化） targets 在减少 regret 方面进行训练，即决策时的损失。然而，这个损失函数可能是非凸的，甚至不可导的，这使得有效的梯度学习方法成为了一个挑战。不过，使用 empirical 损失作为代理来减少预期损失的方法已经被提出。然而，empirical  regret 可能不能准确地反映预期损失，因为优化模型中的不确定性会导致 empirical  regret 与预期损失之间的差异。为了描述这种不同，我们评估了 aleatoric 和 epistemic uncertainty 对 empirical regret 的影响。接着，我们提出了三种robust regret loss，这些损失函数更好地预测预期损失。实验结果表明，使用这些 robust regret loss 训练两种现有的决策专注学习方法可以提高测试样本 empirical regret 的准确性，而不会增加计算时间相对于训练纪录数量。
</details></li>
</ul>
<hr>
<h2 id="Program-Synthesis-with-Best-First-Bottom-Up-Search"><a href="#Program-Synthesis-with-Best-First-Bottom-Up-Search" class="headerlink" title="Program Synthesis with Best-First Bottom-Up Search"></a>Program Synthesis with Best-First Bottom-Up Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04327">http://arxiv.org/abs/2310.04327</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saqib Ameen, Levi H. S. Lelis</li>
<li>for: 解决程序合成任务中的搜索问题，使用成本函数引导搜索，以优化程序生成。</li>
<li>methods: 引入一种新的最佳先进搜索算法，称为“蜜蜂搜索”（Bee Search），可以在成本函数引导下，在最佳先进顺序下进行程序生成。该算法不会产生比解决方案更贵的程序，并且可以在内存中生成程序。同时，我们还引入了一种新的成本函数，可以更好地利用模型提供的信息。</li>
<li>results: 实验结果表明，使用蜜蜂搜索和新的成本函数可以在使用更复杂的域特定语言（DSL）时，比前方法更高效；在使用更简单的 DSL 时，蜜蜂搜索和前方法的性能相同。此外，新的成本函数在字符串处理任务上表现更高效。<details>
<summary>Abstract</summary>
Cost-guided bottom-up search (BUS) algorithms use a cost function to guide the search to solve program synthesis tasks. In this paper, we show that current state-of-the-art cost-guided BUS algorithms suffer from a common problem: they can lose useful information given by the model and fail to perform the search in a best-first order according to a cost function. We introduce a novel best-first bottom-up search algorithm, which we call Bee Search, that does not suffer information loss and is able to perform cost-guided bottom-up synthesis in a best-first manner. Importantly, Bee Search performs best-first search with respect to the generation of programs, i.e., it does not even create in memory programs that are more expensive than the solution program. It attains best-first ordering with respect to generation by performing a search in an abstract space of program costs. We also introduce a new cost function that better uses the information provided by an existing cost model. Empirical results on string manipulation and bit-vector tasks show that Bee Search can outperform existing cost-guided BUS approaches when employing more complex domain-specific languages (DSLs); Bee Search and previous approaches perform equally well with simpler DSLs. Furthermore, our new cost function with Bee Search outperforms previous cost functions on string manipulation tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Cost-guided bottom-up search (BUS) algorithms use a cost function to guide the search to solve program synthesis tasks. In this paper, we show that current state-of-the-art cost-guided BUS algorithms suffer from a common problem: they can lose useful information given by the model and fail to perform the search in a best-first order according to a cost function. We introduce a novel best-first bottom-up search algorithm, which we call Bee Search, that does not suffer information loss and is able to perform cost-guided bottom-up synthesis in a best-first manner. Importantly, Bee Search performs best-first search with respect to the generation of programs, i.e., it does not even create in memory programs that are more expensive than the solution program. It attains best-first ordering with respect to generation by performing a search in an abstract space of program costs. We also introduce a new cost function that better uses the information provided by an existing cost model. Empirical results on string manipulation and bit-vector tasks show that Bee Search can outperform existing cost-guided BUS approaches when employing more complex domain-specific languages (DSLs); Bee Search and previous approaches perform equally well with simpler DSLs. Furthermore, our new cost function with Bee Search outperforms previous cost functions on string manipulation tasks.")]Here's the translation:<<SYS>>成本导向底层搜索（BUS）算法使用成本函数来引导搜索解决程序生成任务。在这篇论文中，我们显示现有状态对抗BUS算法受到一个共同问题：它们可能会在模型提供的信息上失去有用信息并不能按照成本函数进行最优先搜索。我们介绍了一种新的最优先底层搜索算法，我们称之为“蜜蜂搜索”（Bee Search）。蜜蜂搜索不会产生更昂贵的程序，并且可以在成本空间中进行最优先搜索。我们还引入了一个新的成本函数，该函数更好地利用现有成本模型提供的信息。实验结果表明，蜜蜂搜索在使用更复杂的域特定语言（DSL）时可以超过现有的成本导向BUS方法，并且与 simpler DSL 相同，蜜蜂搜索和前一代方法的性能相同。此外，我们的新成本函数与蜜蜂搜索在串 manipulate 任务上表现更高效。
</details></li>
</ul>
<hr>
<h2 id="Latent-Graph-Inference-with-Limited-Supervision"><a href="#Latent-Graph-Inference-with-Limited-Supervision" class="headerlink" title="Latent Graph Inference with Limited Supervision"></a>Latent Graph Inference with Limited Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04314">http://arxiv.org/abs/2310.04314</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jianglin954/LGI-LS">https://github.com/Jianglin954/LGI-LS</a></li>
<li>paper_authors: Jianglin Lu, Yi Xu, Huan Wang, Yue Bai, Yun Fu</li>
<li>for: 提高 latent graph inference（LGI）的性能，特别是在有限的监督下。</li>
<li>methods: 提出了一种方法来Restore the corrupted affinities和Recover the missed supervision，包括定义 pivot nodes 和使用CUR matrix decomposition。</li>
<li>results: 在多个 benchmark 上实现了提高 LGI 的性能，特别是在有限的监督下（6.12% 提高 Pubmed 上，只需要0.3% 的标注率）。<details>
<summary>Abstract</summary>
Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss. Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization. In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones. To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI. The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities. We begin by defining the pivotal nodes as $k$-hop starved nodes, which can be identified based on a given adjacency matrix. Considering the high computational burden, we further present a more efficient alternative inspired by CUR matrix decomposition. Subsequently, we eliminate the starved nodes by reconstructing the destroyed connections. Extensive experiments on representative benchmarks demonstrate that reducing the starved nodes consistently improves the performance of state-of-the-art LGI methods, especially under extremely limited supervision (6.12% improvement on Pubmed with a labeling rate of only 0.3%).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Foundational-Models-for-Molecular-Learning-on-Large-Scale-Multi-Task-Datasets"><a href="#Towards-Foundational-Models-for-Molecular-Learning-on-Large-Scale-Multi-Task-Datasets" class="headerlink" title="Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets"></a>Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04292">http://arxiv.org/abs/2310.04292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/datamol-io/graphium">https://github.com/datamol-io/graphium</a></li>
<li>paper_authors: Dominique Beaini, Shenyang Huang, Joao Alex Cunha, Zhiyi Li, Gabriela Moisescu-Pareja, Oleksandr Dymov, Samuel Maddrell-Mander, Callum McLean, Frederik Wenkel, Luis Müller, Jama Hussein Mohamud, Ali Parviz, Michael Craig, Michał Koziarski, Jiarui Lu, Zhaocheng Zhu, Cristian Gabellini, Kerstin Klaser, Josef Dean, Cas Wognum, Maciej Sypetkowski, Guillaume Rabusseau, Reihaneh Rabbany, Jian Tang, Christopher Morris, Ioannis Koutis, Mirco Ravanelli, Guy Wolf, Prudencio Tossou, Hadrien Mary, Therence Bois, Andrew Fitzgibbon, Błażej Banaszewski, Chad Martin, Dominic Masters<br>for:这篇论文的目的是为了提供大量的分类标签数据集，以促进分子学机器学习领域中的基础模型的发展。methods:这篇论文使用了7个新的数据集，分别是ToyMix、LargeMix和UltraLarge三个类别，这些数据集的标签数据量非常大，涵盖了10亿分子和3000多个稀聚定义的任务，总计超过1300亿个个别标签，其中包括量子和生物性的标签。results:这篇论文提供了一些基线结果，以便在这些数据集上进行多任务和多级分子学机器学习模型的训练。Empirical studies show that training on large amounts of quantum data can improve the performance of low-resource biological datasets, indicating that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks。Here’s the format you requested:for: 这篇论文的目的是为了提供大量的分类标签数据集，以促进分子学机器学习领域中的基础模型的发展。methods: 这篇论文使用了7个新的数据集，分别是ToyMix、LargeMix和UltraLarge三个类别，这些数据集的标签数据量非常大，涵盖了10亿分子和3000多个稀聚定义的任务，总计超过1300亿个个别标签，其中包括量子和生物性的标签。results: 这篇论文提供了一些基线结果，以便在这些数据集上进行多任务和多级分子学机器学习模型的训练。Empirical studies show that training on large amounts of quantum data can improve the performance of low-resource biological datasets, indicating that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks。<details>
<summary>Abstract</summary>
Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. In this work, we present seven novel datasets categorized by size into three distinct categories: ToyMix, LargeMix and UltraLarge. These datasets push the boundaries in both the scale and the diversity of supervised labels for molecular learning. They cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, our datasets contain 300 times more data points than the widely used OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset. In addition, to support the development of foundational models based on our proposed datasets, we present the Graphium graph machine learning library which simplifies the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets. Finally, we present a range of baseline results as a starting point of multi-task and multi-level training on these datasets. Empirically, we observe that performance on low-resource biological datasets show improvement by also training on large amounts of quantum data. This indicates that there may be potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.
</details>
<details>
<summary>摘要</summary>
近期，预训练基础模型已经导致多个领域的进步。在分子学学习中， however，由于数据集通常是手工筛选，因此通常较小，缺乏带有标签的特征数据集和管理这些数据集的代码库，这限制了基础模型的发展。在这项工作中，我们提出了七个新的数据集，分为三个不同类别：ToyMix、LargeMix和UltraLarge。这些数据集在标签数量和多样性方面都为分子学学习带来了新的纪录。它们涵盖了 nearly 100 million 分子和超过 3000 稀缺定的任务，总计超过 1300 亿个标签，其中包括量子和生物性质的标签。与此相比，我们的数据集包含 300 个更多的数据点，than the widely used OGB-LSC PCQM4Mv2 数据集，并且 13 个更多的量子只数据集。此外，为基于我们提出的数据集开发基础模型，我们提供了 Graphium 图机器学习库，该库 simplifies the process of building and training 分子机器学习模型，特别是在多任务和多级分子数据集上。最后，我们提供了一系列的基线结果，作为多任务和多级训练的开始点。我们观察到，在资源受限的生物数据集上，通过同时训练大量量子数据也能够提高表现。这表明，可能在基础模型的多任务和多级训练和精度训练下进行 fine-tuning 可以获得更好的性能。
</details></li>
</ul>
<hr>
<h2 id="On-the-Error-Propagation-of-Inexact-Deflation-for-Principal-Component-Analysis"><a href="#On-the-Error-Propagation-of-Inexact-Deflation-for-Principal-Component-Analysis" class="headerlink" title="On the Error-Propagation of Inexact Deflation for Principal Component Analysis"></a>On the Error-Propagation of Inexact Deflation for Principal Component Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04283">http://arxiv.org/abs/2310.04283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangshuo Liao, Junhyung Lyle Kim, Cruz Barnum, Anastasios Kyrillidis</li>
<li>for: 本研究旨在解决数据分析中常用的主成分分析（PCA）问题，尤其是高维数据的情况下。</li>
<li>methods: 本研究使用了逐次扫描法（deflation method）来找出主成分。</li>
<li>results: 本研究提供了两个主要结果：一是当副程序用于找到主成分向量时是通用的时，二是当使用力迭代法（power iteration）作为副程序时，可以获得更紧的误差界限。这两个结果都是关于PCA的误差卷积的数学分析。<details>
<summary>Abstract</summary>
Principal Component Analysis (PCA) is a popular tool in data analysis, especially when the data is high-dimensional. PCA aims to find subspaces, spanned by the so-called \textit{principal components}, that best explain the variance in the dataset. The deflation method is a popular meta-algorithm -- used to discover such subspaces -- that sequentially finds individual principal components, starting from the most important one and working its way towards the less important ones. However, due to its sequential nature, the numerical error introduced by not estimating principal components exactly -- e.g., due to numerical approximations through this process -- propagates, as deflation proceeds. To the best of our knowledge, this is the first work that mathematically characterizes the error propagation of the inexact deflation method, and this is the key contribution of this paper. We provide two main results: $i)$ when the sub-routine for finding the leading eigenvector is generic, and $ii)$ when power iteration is used as the sub-routine. In the latter case, the additional directional information from power iteration allows us to obtain a tighter error bound than the analysis of the sub-routine agnostic case. As an outcome, we provide explicit characterization on how the error progresses and affects subsequent principal component estimations for this fundamental problem.
</details>
<details>
<summary>摘要</summary>
主成分分析（PCA）是数据分析中广泛使用的工具，特别是当数据维度很高时。PCA的目标是找到数据集中最好解释协方差的子空间，这些子空间被称为“主成分”。抽象法是一种广泛使用的meta-算法，它逐次找到数据集中的主成分，从最重要的开始，向更不重要的进行。然而，由于其逐次性，在不精确地计算主成分时的数字错误会在抽象进程中卷毁。根据我们所知，这是首次对不精确抽象法的错误卷毁进行数学 caracterization的研究，这是本文的关键贡献。我们提供两个主要结果：i) 当找到主成分的子routine是通用的时，ii) 当使用力耗迭代法作为子routine时。在后者情况下，通过力耗迭代法提供的方向信息，我们可以获得更紧张的错误 bound，比sub-routine agnostic case的分析更精确。因此，我们提供了主成分估计过程中错误的明确 caracterization，并且描述了错误如何在后续主成分估计中传播。
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-modelling-of-tip-clearance-variations-on-multi-stage-axial-compressors-aerodynamics"><a href="#Deep-learning-modelling-of-tip-clearance-variations-on-multi-stage-axial-compressors-aerodynamics" class="headerlink" title="Deep learning modelling of tip clearance variations on multi-stage axial compressors aerodynamics"></a>Deep learning modelling of tip clearance variations on multi-stage axial compressors aerodynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04264">http://arxiv.org/abs/2310.04264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Bruni, Sepehr Maleki, Senthil K. Krishnababu</li>
<li>for: 这个论文是为了应用深度学习方法于物理模拟（CFD），以提高液压机的性能和生产效率。</li>
<li>methods: 这个论文使用了深度学习框架，以实时预测多Stage液压机中缘 clearance 变化对流场和 aerodynamic performance 的影响。</li>
<li>results: 该框架被证明可扩展到工业应用，并在实时达到 CFD 参照值的准确性。已经部署的模型可以直接integrated到液压机的生产和建造过程中，以便分析性能的影响和减少 física tests 的需求。<details>
<summary>Abstract</summary>
Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics) for turbomachinery applications, have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the flow field and aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
</details>
<details>
<summary>摘要</summary>
使用深度学习方法进行物理模拟，如计算流体动力学（CFD），在液压机应用中尚未得到了广泛的工业应用。这篇文章描述了一种深度学习框架的开发和应用，用于实时预测多Stage液压机中缺口变化对流体场和 aerodynamic性能的影响。提出的架构被证明可扩展到工业应用，并在实时达到了CFD标准的准确率。已经部署的模型可以直接integrated into the manufacturing and build process of gas turbines，从而提供了对性能的分析和可能性reducing expensive physical tests的机会。</SYS>Note: The above text is translated into Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Bringing-Quantum-Algorithms-to-Automated-Machine-Learning-A-Systematic-Review-of-AutoML-Frameworks-Regarding-Extensibility-for-QML-Algorithms"><a href="#Bringing-Quantum-Algorithms-to-Automated-Machine-Learning-A-Systematic-Review-of-AutoML-Frameworks-Regarding-Extensibility-for-QML-Algorithms" class="headerlink" title="Bringing Quantum Algorithms to Automated Machine Learning: A Systematic Review of AutoML Frameworks Regarding Extensibility for QML Algorithms"></a>Bringing Quantum Algorithms to Automated Machine Learning: A Systematic Review of AutoML Frameworks Regarding Extensibility for QML Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04238">http://arxiv.org/abs/2310.04238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dennis Klau, Marc Zöller, Christian Tutschku</li>
<li>for: 这个研究旨在选择和分析现有的AutoML框架，以便 incorporating Quantum Machine Learning（QML）算法到自动解决方案中，并解决不同类型的ML问题的一组工业使用情况。</li>
<li>methods: 该研究使用了多phase、多 criterion 方法来筛选可用的开源工具，并从技术和AutoML角度进行评估框架。</li>
<li>results: 研究选择了Ray和AutoGluon作为适用的低级和高级框架，并基于这些发现建立了一个扩展的自动化量子机器学习（AutoQML）框架，并在特定硬件和软件约束下实现了QC特有的管道步骤和决策特征。<details>
<summary>Abstract</summary>
This work describes the selection approach and analysis of existing AutoML frameworks regarding their capability of a) incorporating Quantum Machine Learning (QML) algorithms into this automated solving approach of the AutoML framing and b) solving a set of industrial use-cases with different ML problem types by benchmarking their most important characteristics. For that, available open-source tools are condensed into a market overview and suitable frameworks are systematically selected on a multi-phase, multi-criteria approach. This is done by considering software selection approaches, as well as in terms of the technical perspective of AutoML. The requirements for the framework selection are divided into hard and soft criteria regarding their software and ML attributes. Additionally, a classification of AutoML frameworks is made into high- and low-level types, inspired by the findings of. Finally, we select Ray and AutoGluon as the suitable low- and high-level frameworks respectively, as they fulfil all requirements sufficiently and received the best evaluation feedback during the use-case study. Based on those findings, we build an extended Automated Quantum Machine Learning (AutoQML) framework with QC-specific pipeline steps and decision characteristics for hardware and software constraints.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这个研究描述了自动机器学习（AutoML）框架的选择和分析，包括它们是否可以 incorporating Quantum Machine Learning（QML）算法到自动解决方案中，以及解决不同类型的工业用 caso 问题。为此，我们对可用的开源工具进行了市场概述，并 sistematicamente 选择了适合的框架，基于多个多个 criterion 的多 phase approach。这些 criterion 包括软件选择方法和技术上的 AutoML 特点。框架的选择要求分为硬件和软件两个类别，即 Software 和 ML 属性。此外，我们还对 AutoML 框架进行了分类，将其分为高级和低级两类，这是根据了发现的结果。最后，我们选择了 Ray 和 AutoGluon 作为最适合的低级和高级框架。基于这些发现，我们建立了一个扩展的自动量子机器学习（AutoQML）框架，它包括量子计算机Specific 管道步骤和决策特征，以满足硬件和软件约束。
</details></li>
</ul>
<hr>
<h2 id="Cost-Effective-Retraining-of-Machine-Learning-Models"><a href="#Cost-Effective-Retraining-of-Machine-Learning-Models" class="headerlink" title="Cost-Effective Retraining of Machine Learning Models"></a>Cost-Effective Retraining of Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04216">http://arxiv.org/abs/2310.04216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananth Mahadevan, Michael Mathioudakis</li>
<li>for: 这 paper 的目的是提出一种自动化和经济的机器学习模型重新训练决策算法，以优化数据变化和模型衰退的交互关系。</li>
<li>methods: 该 paper 使用了一种基于成本考虑的 Cost-Aware Retraining Algorithm (Cara)，通过考虑不同的数据和模型因素，自动决定是否需要重新训练机器学习模型。</li>
<li>results: 该 paper 通过对 sintetic 数据集和实际数据集进行分析和实验，证明了 Cara 可以适应不同的数据漂移和重新训练成本，同时保持与最佳回档算法相似的性能。<details>
<summary>Abstract</summary>
It is important to retrain a machine learning (ML) model in order to maintain its performance as the data changes over time. However, this can be costly as it usually requires processing the entire dataset again. This creates a trade-off between retraining too frequently, which leads to unnecessary computing costs, and not retraining often enough, which results in stale and inaccurate ML models. To address this challenge, we propose ML systems that make automated and cost-effective decisions about when to retrain an ML model. We aim to optimize the trade-off by considering the costs associated with each decision. Our research focuses on determining whether to retrain or keep an existing ML model based on various factors, including the data, the model, and the predictive queries answered by the model. Our main contribution is a Cost-Aware Retraining Algorithm called Cara, which optimizes the trade-off over streams of data and queries. To evaluate the performance of Cara, we analyzed synthetic datasets and demonstrated that Cara can adapt to different data drifts and retraining costs while performing similarly to an optimal retrospective algorithm. We also conducted experiments with real-world datasets and showed that Cara achieves better accuracy than drift detection baselines while making fewer retraining decisions, ultimately resulting in lower total costs.
</details>
<details>
<summary>摘要</summary>
“重新训练机器学习（ML）模型是重要的，以保持模型的性能随着数据的变化而改善。然而，这可能会带来高昂的计算成本，因为通常需要重新处理整个数据集。这创造了一个 retraining 的权衡问题， retraining 的频率需要考虑计算成本。我们提出了一种自动化并经济的 ML 系统，可以自动决定是否需要重新训练 ML 模型。我们的研究集中在决定是否需要重新训练或保留现有的 ML 模型，根据数据、模型以及模型回答的预测查询。我们的主要贡献是一种名为 Cara 的 Cost-Aware Retraining Algorithm，可以优化权衡。为评估 Cara 的性能，我们分析了 sintetic 数据集并证明了 Cara 可以适应不同的数据漂移和重新训练成本，同时与潜在的回顾算法相似。我们还对实际数据集进行了实验，并证明了 Cara 可以在较低的总成本下达到更高的准确率。”
</details></li>
</ul>
<hr>
<h2 id="Non-Redundant-Graph-Neural-Networks-with-Improved-Expressiveness"><a href="#Non-Redundant-Graph-Neural-Networks-with-Improved-Expressiveness" class="headerlink" title="Non-Redundant Graph Neural Networks with Improved Expressiveness"></a>Non-Redundant Graph Neural Networks with Improved Expressiveness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04190">http://arxiv.org/abs/2310.04190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franka Bause, Samir Moustafa, Johannes Langguth, Wilfried N. Gansterer, Nils M. Kriege</li>
<li>for: 这篇论文旨在提出一种基于 Message Passing Graph Neural Networks (MPGNNs) 的新汇聚方法，以提高表示力和避免过损压缩。</li>
<li>methods: 该方法基于 neighborhood trees 的新汇聚 scheme，可以控制冗余，从而提高表示力和避免过损压缩。</li>
<li>results: 实验表明，该方法可以提高表示力和避免过损压缩，并且在 widely-used benchmark datasets 上实现高分类率。<details>
<summary>Abstract</summary>
Message passing graph neural networks iteratively compute node embeddings by aggregating messages from all neighbors. This procedure can be viewed as a neural variant of the Weisfeiler-Leman method, which limits their expressive power. Moreover, oversmoothing and oversquashing restrict the number of layers these networks can effectively utilize. The repeated exchange and encoding of identical information in message passing amplifies oversquashing. We propose a novel aggregation scheme based on neighborhood trees, which allows for controlling the redundancy by pruning branches of the unfolding trees underlying standard message passing. We prove that reducing redundancy improves expressivity and experimentally show that it alleviates oversquashing. We investigate the interaction between redundancy in message passing and redundancy in computation and propose a compact representation of neighborhood trees, from which we compute node and graph embeddings via a neural tree canonization technique. Our method is provably more expressive than the Weisfeiler-Leman method, less susceptible to oversquashing than message passing neural networks, and provides high classification accuracy on widely-used benchmark datasets.
</details>
<details>
<summary>摘要</summary>
message passing 图 нейрон网络逐步计算节点嵌入，通过所有邻居的消息汇总来计算节点嵌入。这种过程可以视为一种神经网络中的Weisfeiler-Leman方法的变体，它限制了它们的表达力。另外，过滤和压缩限制了图层数，这些图层数可以有效利用。在消息传递中重复交换和编码相同的信息会增加压缩。我们提出了一种基于邻域树的新的聚合方法，可以控制干扰的强度，通过隐藏树的层次结构来减少干扰。我们证明了减少干扰可以提高表达力，并在实验中证明它可以缓解过滤。我们研究消息传递中干扰和计算中干扰之间的交互，并提出一种紧凑的表示方法，从而计算节点和图嵌入。我们的方法比Weisfeiler-Leman方法更表达力强，对消息传递中干扰更敏感，并在广泛使用的 benchmark 数据集上达到高精度分类。
</details></li>
</ul>
<hr>
<h2 id="Amortized-Network-Intervention-to-Steer-the-Excitatory-Point-Processes"><a href="#Amortized-Network-Intervention-to-Steer-the-Excitatory-Point-Processes" class="headerlink" title="Amortized Network Intervention to Steer the Excitatory Point Processes"></a>Amortized Network Intervention to Steer the Excitatory Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04159">http://arxiv.org/abs/2310.04159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zitao Song, Wendi Ren, Shuang Li</li>
<li>for: 这个研究旨在解决大规模网络干预问题，专门是指引刺激点过程，如传染病毒或交通壅塞控制。</li>
<li>methods: 我们的模型基于学习掌控方法，使用神经数据流函数（Neural ODEs）来捕捉网络上刺激点过程的时间变化。我们的方法结合了Gradient-Descent基于的Model Predictive Control（GD-MPC），以便满足对策略的灵活性，并考虑对策略的约束。</li>
<li>results: 我们的方法可以实现网络上刺激点过程的有效控制，并且可以在实际应用中运用，例如减少传染病毒的传播和减少碳排放。<details>
<summary>Abstract</summary>
We tackle the challenge of large-scale network intervention for guiding excitatory point processes, such as infectious disease spread or traffic congestion control. Our model-based reinforcement learning utilizes neural ODEs to capture how the networked excitatory point processes will evolve subject to the time-varying changes in network topology. Our approach incorporates Gradient-Descent based Model Predictive Control (GD-MPC), offering policy flexibility to accommodate prior knowledge and constraints. To address the intricacies of planning and overcome the high dimensionality inherent to such decision-making problems, we design an Amortize Network Interventions (ANI) framework, allowing for the pooling of optimal policies from history and other contexts, while ensuring a permutation equivalent property. This property enables efficient knowledge transfer and sharing across diverse contexts. Our approach has broad applications, from curbing infectious disease spread to reducing carbon emissions through traffic light optimization, and thus has the potential to address critical societal and environmental challenges.
</details>
<details>
<summary>摘要</summary>
我们面临大规模网络干预挑战，以导引刺激点过程，如感染病毒传播或交通堵塞控制。我们的模型基于学习环境动量（Neural ODEs），用于捕捉网络上刺激点过程在时间变化的网络结构下发展的变化。我们的方法结合梯度下降基于模型预测控制（GD-MPC），以便满足先前知识和约束。为了 Addressing the intricacies of planning and high-dimensional decision-making problems, we design an Amortize Network Interventions (ANI) framework, which pools optimal policies from history and other contexts, while ensuring a permutation equivalent property. This property enables efficient knowledge transfer and sharing across diverse contexts. Our approach has broad applications, from curbing infectious disease spread to reducing carbon emissions through traffic light optimization, and thus has the potential to address critical societal and environmental challenges.Here's the word-for-word translation of the text into Simplified Chinese:我们面临大规模网络干预挑战，以导引刺激点过程，如感染病毒传播或交通堵塞控制。我们的模型基于学习环境动量（Neural ODEs），用于捕捉网络上刺激点过程在时间变化的网络结构下发展的变化。我们的方法结合梯度下降基于模型预测控制（GD-MPC），以便满足先前知识和约束。为了 Addressing the intricacies of planning and high-dimensional decision-making problems, we design an Amortize Network Interventions (ANI) framework, which pools optimal policies from history and other contexts, while ensuring a permutation equivalent property。 This property enables efficient knowledge transfer and sharing across diverse contexts。 Our approach has broad applications, from curbing infectious disease spread to reducing carbon emissions through traffic light optimization, and thus has the potential to address critical societal and environmental challenges。
</details></li>
</ul>
<hr>
<h2 id="From-Zero-to-Hero-Detecting-Leaked-Data-through-Synthetic-Data-Injection-and-Model-Querying"><a href="#From-Zero-to-Hero-Detecting-Leaked-Data-through-Synthetic-Data-Injection-and-Model-Querying" class="headerlink" title="From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying"></a>From Zero to Hero: Detecting Leaked Data through Synthetic Data Injection and Model Querying</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04145">http://arxiv.org/abs/2310.04145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biao Wu, Qiang Huang, Anthony K. H. Tung</li>
<li>for: 本研究旨在保护数据的知识产权，尤其是在机器学习应用萌芽的时候，数据训练过程中的数据泄露问题日益突出。</li>
<li>methods: 本文提出了一种新的方法——本地分布变化合成(\textsc{LDSS})，用于检测模型训练过程中的数据泄露。\textsc{LDSS}通过在所有者的数据集中插入一小量的合成数据，使得模型训练过程中的数据泄露得到了有效的检测。</li>
<li>results: EXTENSIVE experiments表明，\textsc{LDSS} 具有可靠性、可靠性、准确性、安全性和效率。在七种不同的分类模型和五个实际 dataset 上，\textsc{LDSS} 都取得了优秀的结果。<details>
<summary>Abstract</summary>
Safeguarding the Intellectual Property (IP) of data has become critically important as machine learning applications continue to proliferate, and their success heavily relies on the quality of training data. While various mechanisms exist to secure data during storage, transmission, and consumption, fewer studies have been developed to detect whether they are already leaked for model training without authorization. This issue is particularly challenging due to the absence of information and control over the training process conducted by potential attackers.   In this paper, we concentrate on the domain of tabular data and introduce a novel methodology, Local Distribution Shifting Synthesis (\textsc{LDSS}), to detect leaked data that are used to train classification models. The core concept behind \textsc{LDSS} involves injecting a small volume of synthetic data--characterized by local shifts in class distribution--into the owner's dataset. This enables the effective identification of models trained on leaked data through model querying alone, as the synthetic data injection results in a pronounced disparity in the predictions of models trained on leaked and modified datasets. \textsc{LDSS} is \emph{model-oblivious} and hence compatible with a diverse range of classification models, such as Naive Bayes, Decision Tree, and Random Forest. We have conducted extensive experiments on seven types of classification models across five real-world datasets. The comprehensive results affirm the reliability, robustness, fidelity, security, and efficiency of \textsc{LDSS}.
</details>
<details>
<summary>摘要</summary>
保护数据的知识产权（IP）在机器学习应用得更加重要，因为机器学习模型的成功几乎完全取决于训练数据的质量。虽然有各种机制来保护数据在存储、传输和消耗过程中，但有 fewer studies 是用于检测模型是否在未经授权的情况下使用训练数据。这个问题特别Difficult 是因为攻击者可能不具备训练过程的信息和控制。在这篇论文中，我们专注于表格数据领域，并提出了一种新的方法——本地分布式同步生成(\textsc{LDSS）），用于检测训练模型的数据泄露。\textsc{LDSS} 的核心思想是将小量的合成数据（具有本地分布的类别分布差异）注入到所有者的数据集中。这使得可以通过模型查询 alone 确定是否使用了未经授权的训练数据，因为合成数据注入会导致模型在使用修改后的数据集上预测结果出现显著差异。\textsc{LDSS} 是“模型无关”的，可以与多种分类模型（如普通概率、决策树、随机森林）结合使用。我们在五个实际数据集上进行了七种分类模型的广泛实验，结果证明了 \textsc{LDSS} 的可靠性、稳定性、准确性、安全性和效率。
</details></li>
</ul>
<hr>
<h2 id="Routing-Arena-A-Benchmark-Suite-for-Neural-Routing-Solvers"><a href="#Routing-Arena-A-Benchmark-Suite-for-Neural-Routing-Solvers" class="headerlink" title="Routing Arena: A Benchmark Suite for Neural Routing Solvers"></a>Routing Arena: A Benchmark Suite for Neural Routing Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04140">http://arxiv.org/abs/2310.04140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniela Thyssens, Tim Dernedde, Jonas K. Falkner, Lars Schmidt-Thieme</li>
<li>for: 该论文主要研究目标是提出一个基于Machine Learning的路径选择策略评估 benchmark suite，以便更好地评估不同方法的性能和比较不同领域中的基eline。</li>
<li>methods: 该论文提出了一种新的评估协议，该协议包括两个主要的评估情况：一是预先固定的时间预算，二是任意时间的性能评估。此外，该论文还提出了一种新的评估指标——Weighted Relative Average Performance（WRAP），用于衡量不同方法的运行时效率。</li>
<li>results: 该论文的初步实验结果表明，最新的操作研究方法在解决交通问题上获得了最佳解的解决方案和运行时效率的最佳性能。然而，一些发现还提出了使用神经网络方法的优势，并促使我们对神经网络方法的概念如何进行重新定义。<details>
<summary>Abstract</summary>
Neural Combinatorial Optimization has been researched actively in the last eight years. Even though many of the proposed Machine Learning based approaches are compared on the same datasets, the evaluation protocol exhibits essential flaws and the selection of baselines often neglects State-of-the-Art Operations Research approaches. To improve on both of these shortcomings, we propose the Routing Arena, a benchmark suite for Routing Problems that provides a seamless integration of consistent evaluation and the provision of baselines and benchmarks prevalent in the Machine Learning- and Operations Research field. The proposed evaluation protocol considers the two most important evaluation cases for different applications: First, the solution quality for an a priori fixed time budget and secondly the anytime performance of the respective methods. By setting the solution trajectory in perspective to a Best Known Solution and a Base Solver's solutions trajectory, we furthermore propose the Weighted Relative Average Performance (WRAP), a novel evaluation metric that quantifies the often claimed runtime efficiency of Neural Routing Solvers. A comprehensive first experimental evaluation demonstrates that the most recent Operations Research solvers generate state-of-the-art results in terms of solution quality and runtime efficiency when it comes to the vehicle routing problem. Nevertheless, some findings highlight the advantages of neural approaches and motivate a shift in how neural solvers should be conceptualized.
</details>
<details>
<summary>摘要</summary>
neurolinkOptimization 在过去八年内得到了active研究。虽然许多提出的机器学习基于方法在同一个数据集上进行比较，但评价协议具有重要的缺陷，而且基准选择frequently neglectsState-of-the-Art操作研究方法。为了改进这两点，我们提出了Routing Arena，一个用于路由问题的benchmark集合，它提供了一个协调的评价方法和机器学习和操作研究领域中常见的基准和benchmark。我们的评价方法考虑了不同应用场景中的两个最重要的评价情况：一是预先固定的时间预算，二是任意时间内的方法性能。我们还提出了一个新的评价指标——Weighted Relative Average Performance（WRAP），它可以量化许多 neural routing solvers中的运行时效率。经过首次实验评估，我们发现最新的操作研究 solvers在解决交通流量问题时达到了状态体验率和运行时效率的国际前景。然而，一些发现还提出了神经方法的优势，并促使我们对神经方法的概念如何进行重新思考。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Myopia-Learning-from-Positive-and-Unlabeled-Data-through-Holistic-Predictive-Trends"><a href="#Beyond-Myopia-Learning-from-Positive-and-Unlabeled-Data-through-Holistic-Predictive-Trends" class="headerlink" title="Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends"></a>Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04078">http://arxiv.org/abs/2310.04078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wxr99/holisticpu">https://github.com/wxr99/holisticpu</a></li>
<li>paper_authors: Xinrui Wang, Wenhai Wan, Chuanxin Geng, Shaoyuan LI, Songcan Chen</li>
<li>for: 本研究旨在提出一种基于带有正例和无标例的PUL方法，以优化二分类模型的训练。</li>
<li>methods: 该方法利用了一种启发式的做法，即在每次训练中采样正例数据，以确保正例和无标例之间的分布尽可平衡。此外，该方法还使用了一种新的时间点 процесс（TPP）模型，来识别正例和无标例之间的变化趋势。</li>
<li>results: 实验表明，该方法在具有高差异度的实际应用场景中表现出色，相比传统PUL方法，该方法可以提高$11.3%$的关键指标。<details>
<summary>Abstract</summary>
Learning binary classifiers from positive and unlabeled data (PUL) is vital in many real-world applications, especially when verifying negative examples is difficult. Despite the impressive empirical performance of recent PUL methods, challenges like accumulated errors and increased estimation bias persist due to the absence of negative labels. In this paper, we unveil an intriguing yet long-overlooked observation in PUL: \textit{resampling the positive data in each training iteration to ensure a balanced distribution between positive and unlabeled examples results in strong early-stage performance. Furthermore, predictive trends for positive and negative classes display distinctly different patterns.} Specifically, the scores (output probability) of unlabeled negative examples consistently decrease, while those of unlabeled positive examples show largely chaotic trends. Instead of focusing on classification within individual time frames, we innovatively adopt a holistic approach, interpreting the scores of each example as a temporal point process (TPP). This reformulates the core problem of PUL as recognizing trends in these scores. We then propose a novel TPP-inspired measure for trend detection and prove its asymptotic unbiasedness in predicting changes. Notably, our method accomplishes PUL without requiring additional parameter tuning or prior assumptions, offering an alternative perspective for tackling this problem. Extensive experiments verify the superiority of our method, particularly in a highly imbalanced real-world setting, where it achieves improvements of up to $11.3\%$ in key metrics. The code is available at \href{https://github.com/wxr99/HolisticPU}{https://github.com/wxr99/HolisticPU}.
</details>
<details>
<summary>摘要</summary>
学习二分类器从正例和无标例数据（PUL）是许多实际应用中非常重要的，特别是当验证负例很困难时。虽然 latest PUL 方法在实际性方面表现出色，但是缺乏负例的存在会导致积累的错误和提高估计偏差。在这篇论文中，我们发现了PUL 中很长时间未被注意的一点：在每个训练轮中对正例数据进行重新采样，以确保正例和无标例数据之间的分布均衡，会在早期得到强大的表现。具体来说，无标例负例的分布下降，而无标例正例的分布则显示出很大的混乱趋势。而不是围绕各个时间帧的分类，我们创新地采用一种整体方法，视每个例的分数（输出概率）为时间点 процесс（TPP）。这将PUL 的核心问题重新定义为识别这些分数的趋势。我们然后提出一种基于 TPP 的新度量方法，并证明其在预测变化时的极限无偏性。与传统方法不同的是，我们的方法不需要额外的参数调整或假设，可以作为PUL 问题的另一种视角。广泛的实验证明了我们的方法的优越性，特别是在实际中具有很大的不均衡性的场景中，其在关键指标上提高了11.3%。代码可以在 \href{https://github.com/wxr99/HolisticPU}{https://github.com/wxr99/HolisticPU} 中找到。
</details></li>
</ul>
<hr>
<h2 id="Overview-of-AdaBoost-Reconciling-its-views-to-better-understand-its-dynamics"><a href="#Overview-of-AdaBoost-Reconciling-its-views-to-better-understand-its-dynamics" class="headerlink" title="Overview of AdaBoost : Reconciling its views to better understand its dynamics"></a>Overview of AdaBoost : Reconciling its views to better understand its dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18323">http://arxiv.org/abs/2310.18323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Perceval Beja-Battais</li>
<li>for: 本文旨在探讨AdaBoost算法的不同视图和其相关的动力学。</li>
<li>methods: 本文将从Friend和Schapire的原始视图开始，然后探讨不同视图的AdaBoost算法，并将它们统一到同一个形式上。</li>
<li>results: 本文希望能帮助非专家读者更好地理解AdaBoost算法的动力学和不同视图之间的关系。<details>
<summary>Abstract</summary>
Boosting methods have been introduced in the late 1980's. They were born following the theoritical aspect of PAC learning. The main idea of boosting methods is to combine weak learners to obtain a strong learner. The weak learners are obtained iteratively by an heuristic which tries to correct the mistakes of the previous weak learner. In 1995, Freund and Schapire [18] introduced AdaBoost, a boosting algorithm that is still widely used today. Since then, many views of the algorithm have been proposed to properly tame its dynamics. In this paper, we will try to cover all the views that one can have on AdaBoost. We will start with the original view of Freund and Schapire before covering the different views and unify them with the same formalism. We hope this paper will help the non-expert reader to better understand the dynamics of AdaBoost and how the different views are equivalent and related to each other.
</details>
<details>
<summary>摘要</summary>
boosting方法在1980年代晚期出现。它们的出现是基于PAC学习理论的。boosting方法的主要想法是将弱学习器合并而成一个强学习器。弱学习器是通过一种规则来获取，这种规则会尝试修复前一个弱学习器的错误。在1995年，Freund和Schapire（18）引入了AdaBoost算法，这是目前仍然广泛使用的。 desde entonces, 多种视角对算法进行了提出，以适应其动态。在这篇论文中，我们将尝试涵盖所有可能的视角，并将它们统一到同一种形式中。我们希望这篇论文能够帮助非专家读者更好地理解AdaBoost的动态和不同视角之间的关系。
</details></li>
</ul>
<hr>
<h2 id="DEFT-A-new-distance-based-feature-set-for-keystroke-dynamics"><a href="#DEFT-A-new-distance-based-feature-set-for-keystroke-dynamics" class="headerlink" title="DEFT: A new distance-based feature set for keystroke dynamics"></a>DEFT: A new distance-based feature set for keystroke dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04059">http://arxiv.org/abs/2310.04059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nuwan Kaluarachchi, Sevvandi Kandanaarachchi, Kristen Moore, Arathi Arakala</li>
<li>for: 用于用户身份验证和识别</li>
<li>methods: 使用新的键盘距离特征，与之前未被考虑的键盘动态特征结合，提供全面的打印行为特征</li>
<li>results: 在三种常见设备上（桌面、手机、平板电脑）测试DEFT模型，与现有状态的方法相比，实现了准确率超过99%，错误率低于10%<details>
<summary>Abstract</summary>
Keystroke dynamics is a behavioural biometric utilised for user identification and authentication. We propose a new set of features based on the distance between keys on the keyboard, a concept that has not been considered before in keystroke dynamics. We combine flight times, a popular metric, with the distance between keys on the keyboard and call them as Distance Enhanced Flight Time features (DEFT). This novel approach provides comprehensive insights into a person's typing behaviour, surpassing typing velocity alone. We build a DEFT model by combining DEFT features with other previously used keystroke dynamic features. The DEFT model is designed to be device-agnostic, allowing us to evaluate its effectiveness across three commonly used devices: desktop, mobile, and tablet. The DEFT model outperforms the existing state-of-the-art methods when we evaluate its effectiveness across two datasets. We obtain accuracy rates exceeding 99% and equal error rates below 10% on all three devices.
</details>
<details>
<summary>摘要</summary>
键盘动态是一种行为生物特征，用于用户认证和身份验证。我们提出了一组基于键盘键位距离的新特征，这是之前在键盘动态中没有考虑过的概念。我们将这些特征与已经广泛使用的飞行时间相结合，并称之为距离增强飞行时间特征（DEFT）。这种新的方法可以带来用户键盘输入行为的全面的了解，超过了单纯的输入速度。我们构建了DEFT模型，并将其与其他已经使用的键盘动态特征相结合。这个DEFT模型是设备无关的，因此我们可以在桌面、手机和平板电脑上评估其效果。我们发现DEFT模型在两个数据集上的效果比现有状态的方法更高，我们在三个设备上获得了准确率超过99%，并且错误率低于10%。
</details></li>
</ul>
<hr>
<h2 id="AUTOPARLLM-GNN-Guided-Automatic-Code-Parallelization-using-Large-Language-Models"><a href="#AUTOPARLLM-GNN-Guided-Automatic-Code-Parallelization-using-Large-Language-Models" class="headerlink" title="AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models"></a>AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04047">http://arxiv.org/abs/2310.04047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quazi Ishtiaque Mahmud, Ali TehraniJamsaz, Hung D Phan, Nesreen K. Ahmed, Ali Jannesari</li>
<li>for: 自动发现并生成并行代码的框架，以提高Sequential Programs中的并行化效率。</li>
<li>methods: 使用heterogeneous Graph Neural Network (GNN)来发现并行特征和并行模式，并使用LLM-based code generator生成并行版本的Sequential Programs。</li>
<li>results: 对11个应用程序进行了evaluation，并显示AUTOPARLLM可以提高当前LLM-based模型的并行代码生成效果，并且可以提高平均运行时间（在NAS Parallel Benchmark和Rodinia Benchmark上提高了3.4%和2.9%）。此外，提出了OMPScore来评估生成的并行代码质量，并表明OMPScore与人工评估之间存在更高的相关性（最多提高75%的Spearman相关性）。<details>
<summary>Abstract</summary>
Parallelizing sequentially written programs is a challenging task. Even experienced developers need to spend considerable time finding parallelism opportunities and then actually writing parallel versions of sequentially written programs. To address this issue, we present AUTOPARLLM, a framework for automatically discovering parallelism and generating the parallel version of the sequentially written program. Our framework consists of two major components: i) a heterogeneous Graph Neural Network (GNN) based parallelism discovery and parallel pattern detection module, and ii) an LLM-based code generator to generate the parallel counterpart of the sequential programs. We use the GNN to learn the flow-aware characteristics of the programs to identify parallel regions in sequential programs and then construct an enhanced prompt using the GNN's results for the LLM-based generator to finally produce the parallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11 applications of 2 well-known benchmark suites: NAS Parallel Benchmark and Rodinia Benchmark. Our results show that AUTOPARLLM is indeed effective in improving the state-of-the-art LLM-based models for the task of parallel code generation in terms of multiple code generation metrics. AUTOPARLLM also improves the average runtime of the parallel code generated by the state-of-the-art LLMs by as high as 3.4% and 2.9% for the NAS Parallel Benchmark and Rodinia Benchmark respectively. Additionally, to overcome the issue that well-known metrics for translation evaluation have not been optimized to evaluate the quality of the generated parallel code, we propose OMPScore for evaluating the quality of the generated code. We show that OMPScore exhibits a better correlation with human judgment than existing metrics, measured by up to 75% improvement of Spearman correlation.
</details>
<details>
<summary>摘要</summary>
自动找到并生成并行代码的框架，我们提出了AUTOPARLLM。它包括两个主要组件：一个基于多型神经网络（GNN）的并行性发现和并行模式检测模块，以及一个基于LLM的代码生成器。我们使用GNN来学习程序的流程特征，以确定并行区域在连续编程中，然后使用GNN的结果构建了加强的提示，并使用LLM-based代码生成器生成并行程序的相应版本。我们对11个应用程序进行了 NAS Parallel Benchmark 和 Rodinia Benchmark 的测试，结果表明，AUTOPARLLM可以在 LLM-based 模型中提高并行代码生成的状态态度，并且在 NAS Parallel Benchmark 和 Rodinia Benchmark 中平均提高了3.4%和2.9%的运行时间。此外，为了解决现有评价纪录不适应评估生成的并行代码质量的问题，我们提出了OMPScore，它可以评估生成的代码质量，并且与人类判断之间 exhibits 更高的相关性，提高了75%的斯佩曼相关性。
</details></li>
</ul>
<hr>
<h2 id="Joint-Projection-Learning-and-Tensor-Decomposition-Based-Incomplete-Multi-view-Clustering"><a href="#Joint-Projection-Learning-and-Tensor-Decomposition-Based-Incomplete-Multi-view-Clustering" class="headerlink" title="Joint Projection Learning and Tensor Decomposition Based Incomplete Multi-view Clustering"></a>Joint Projection Learning and Tensor Decomposition Based Incomplete Multi-view Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04038">http://arxiv.org/abs/2310.04038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weilvnju/jpltd">https://github.com/weilvnju/jpltd</a></li>
<li>paper_authors: Wei Lv, Chao Zhang, Huaxiong Li, Xiuyi Jia, Chunlin Chen</li>
<li>for:  address the problems of incomplete multi-view data and suboptimal graph construction in existing methods</li>
<li>methods:  introduces an orthogonal projection matrix to project high-dimensional features into a lower-dimensional space, learns similarity graphs for instances of different views, and stacks these graphs into a third-order low-rank tensor to explore high-order correlations</li>
<li>results:  outperforms state-of-the-art methods on several benchmark datasets, with an effective optimization algorithm to solve the JPLTD model<details>
<summary>Abstract</summary>
Incomplete multi-view clustering (IMVC) has received increasing attention since it is often that some views of samples are incomplete in reality. Most existing methods learn similarity subgraphs from original incomplete multi-view data and seek complete graphs by exploring the incomplete subgraphs of each view for spectral clustering. However, the graphs constructed on the original high-dimensional data may be suboptimal due to feature redundancy and noise. Besides, previous methods generally ignored the graph noise caused by the inter-class and intra-class structure variation during the transformation of incomplete graphs and complete graphs. To address these problems, we propose a novel Joint Projection Learning and Tensor Decomposition Based method (JPLTD) for IMVC. Specifically, to alleviate the influence of redundant features and noise in high-dimensional data, JPLTD introduces an orthogonal projection matrix to project the high-dimensional features into a lower-dimensional space for compact feature learning.Meanwhile, based on the lower-dimensional space, the similarity graphs corresponding to instances of different views are learned, and JPLTD stacks these graphs into a third-order low-rank tensor to explore the high-order correlations across different views. We further consider the graph noise of projected data caused by missing samples and use a tensor-decomposition based graph filter for robust clustering.JPLTD decomposes the original tensor into an intrinsic tensor and a sparse tensor. The intrinsic tensor models the true data similarities. An effective optimization algorithm is adopted to solve the JPLTD model. Comprehensive experiments on several benchmark datasets demonstrate that JPLTD outperforms the state-of-the-art methods. The code of JPLTD is available at https://github.com/weilvNJU/JPLTD.
</details>
<details>
<summary>摘要</summary>
隐藏多视图协同分 clustering（IMVC）在过去几年内已经收到了越来越多的关注，因为实际情况下样本的一些视图通常是 incomplete。现有的方法通常是从原始的 incomplete multi-view 数据中学习 Similarity subgraphs，然后通过探索每个视图中的 incomplete subgraphs 进行 spectral clustering。然而，在原始高维数据上构建的图可能是不优化的，这是因为特征的重复和噪声。此外，前一些方法通常忽略了在转换 incomplete graph 和 complete graph 过程中因为 inter-class 和 intra-class 结构变化而引起的图像噪声。为了解决这些问题，我们提出了一种新的 Joint Projection Learning and Tensor Decomposition Based 方法（JPLTD）。具体来说，为了减少高维特征的重复和噪声，JPLTD 引入了一个正交投影矩阵，将高维特征投影到一个lower-dimensional space中进行紧凑特征学习。同时，基于这个lower-dimensional space，JPLTD 学习了不同视图中的相似图，并将这些图栅stacked 成一个 third-order low-rank tensor，以探索不同视图之间的高阶相关性。我们还考虑了投影数据中的图像噪声，使用了基于 tensor 的分解方法进行 Robust clustering。JPLTD 将原始 tensor 分解为内在 tensor 和稀疏 tensor。内在 tensor 表示真实数据的相似性。我们采用了一种有效的优化算法来解决 JPLTD 模型。在多个 benchmark 数据集上进行了广泛的实验，结果表明，JPLTD 可以比 state-of-the-art 方法更高效。JPLTD 的代码可以在 GitHub 上找到：https://github.com/weilvNJU/JPLTD。
</details></li>
</ul>
<hr>
<h2 id="Genetic-prediction-of-quantitative-traits-a-machine-learner’s-guide-focused-on-height"><a href="#Genetic-prediction-of-quantitative-traits-a-machine-learner’s-guide-focused-on-height" class="headerlink" title="Genetic prediction of quantitative traits: a machine learner’s guide focused on height"></a>Genetic prediction of quantitative traits: a machine learner’s guide focused on height</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04028">http://arxiv.org/abs/2310.04028</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucie Bourguignon, Caroline Weis, Catherine R. Jutzeler, Michael Adamer</li>
<li>for: 本文旨在为机器学习社区提供Current state of the art模型和相关细节，以便在开发新模型时更好地预测人类特征。</li>
<li>methods: 本文使用了height作为连续型特征的示例，并介绍了 referential datasets、干扰因素、特征选择和常用度量。</li>
<li>results: 本文提供了一个对现有模型和相关细节的概述，以便更好地理解和应用这些模型。<details>
<summary>Abstract</summary>
Machine learning and deep learning have been celebrating many successes in the application to biological problems, especially in the domain of protein folding. Another equally complex and important question has received relatively little attention by the machine learning community, namely the one of prediction of complex traits from genetics. Tackling this problem requires in-depth knowledge of the related genetics literature and awareness of various subtleties associated with genetic data. In this guide, we provide an overview for the machine learning community on current state of the art models and associated subtleties which need to be taken into consideration when developing new models for phenotype prediction. We use height as an example of a continuous-valued phenotype and provide an introduction to benchmark datasets, confounders, feature selection, and common metrics.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "Machine learning and deep learning have been celebrating many successes in the application to biological problems, especially in the domain of protein folding. Another equally complex and important question has received relatively little attention by the machine learning community, namely the one of prediction of complex traits from genetics. Tackling this problem requires in-depth knowledge of the related genetics literature and awareness of various subtleties associated with genetic data. In this guide, we provide an overview for the machine learning community on current state of the art models and associated subtleties which need to be taken into consideration when developing new models for phenotype prediction. We use height as an example of a continuous-valued phenotype and provide an introduction to benchmark datasets, confounders, feature selection, and common metrics." into Simplified Chinese.中文简体版：机器学习和深度学习在生物问题中获得了许多成功，特别是在蛋白质折叠领域。然而，机器学习社区对复杂 trait 预测从遗传学方面Received relatively little attention，这是一个equally complex and important question。解决这个问题需要对相关的遗传学 литературе进行深入的了解，并对遗传数据中的各种细节进行了解。在这个指南中，我们为机器学习社区提供了现状概述，包括当前的状态艺术模型和相关的细节，以及开发新模型时需要考虑的因素。我们使用身高为continue 值的 fenotype 的例子，并介绍了标准 datasets，干扰因素、特征选择和常用度量。
</details></li>
</ul>
<hr>
<h2 id="PGraphDTA-Improving-Drug-Target-Interaction-Prediction-using-Protein-Language-Models-and-Contact-Maps"><a href="#PGraphDTA-Improving-Drug-Target-Interaction-Prediction-using-Protein-Language-Models-and-Contact-Maps" class="headerlink" title="PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps"></a>PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04017">http://arxiv.org/abs/2310.04017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rakesh Bal, Yijia Xiao, Wei Wang<br>for:这个研究旨在提高药物标的互动预测精度，以促进药物探索。methods:本研究使用了蛋白语言模型（PLM）和聚合Current Models，以提高DTI预测的精度。results:研究结果显示，提案的方法在与基准模型比较之下表现出色，具有许多优点，如精度和速度等。<details>
<summary>Abstract</summary>
Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outperform the baseline models considered in this study, presenting a compelling case for further development in this direction. We anticipate that the insights gained from this work will significantly narrow the search space for potential drugs targeting specific proteins, thereby accelerating drug discovery. Code and data for PGraphDTA are available at https://anonymous.4open.science/r/PGraphDTA.
</details>
<details>
<summary>摘要</summary>
开发和发现新药物是一项复杂且需要资源的努力，通常需要大量的成本、时间投入和安全问题。新药物发现的关键之一是确定新药物-标的（DT）交互。现有的计算方法 для预测DT交互主要集中在 binary 分类任务上，试图确定DT对的交互是否存在。然而，蛋白质-药物交互存在着绑定强度的连续分布，这种挑战正在减少对预测的准确性。在这种研究中，我们调查了不同的DT预测技术和我们的提议，并通过广泛的实验来证明我们的提议方法可以超越基eline模型。我们的方法包括PLM（蛋白质语言模型）的集成和当前模型中的Contact Map信息作为逻辑偏好。我们通过广泛的实验证明，我们的提议方法可以超越基eline模型，这为药物发现提供了一个吸引人的可能性。我们期望通过这种研究，能够减少潜在药物 targeting 特定蛋白质的搜索空间，从而加速药物发现。PGraphDTA 代码和数据可以在 <https://anonymous.4open.science/r/PGraphDTA> 上下载。
</details></li>
</ul>
<hr>
<h2 id="Anonymous-Learning-via-Look-Alike-Clustering-A-Precise-Analysis-of-Model-Generalization"><a href="#Anonymous-Learning-via-Look-Alike-Clustering-A-Precise-Analysis-of-Model-Generalization" class="headerlink" title="Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization"></a>Anonymous Learning via Look-Alike Clustering: A Precise Analysis of Model Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04015">http://arxiv.org/abs/2310.04015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adel Javanmard, Vahab Mirrokni</li>
<li>for: 这篇论文目的是探讨一种自然的技术—看类 clustering，以取代个人敏感特征，并评估这种方法对模型的泛化能力的影响。</li>
<li>methods: 本文使用了一种称为Convex Gaussian Minimax Theorem（CGMT）的精确分析方法，以了解模型的泛化误差。</li>
<li>results: 研究发现，在某些高维度情况下，使用看类 clustering 训练模型可以对泛化误差进行调整，并且在一些 finite-sample numerical experiments 中证实了这一点。<details>
<summary>Abstract</summary>
While personalized recommendations systems have become increasingly popular, ensuring user data protection remains a top concern in the development of these learning systems. A common approach to enhancing privacy involves training models using anonymous data rather than individual data. In this paper, we explore a natural technique called \emph{look-alike clustering}, which involves replacing sensitive features of individuals with the cluster's average values. We provide a precise analysis of how training models using anonymous cluster centers affects their generalization capabilities. We focus on an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis is based on the Convex Gaussian Minimax Theorem (CGMT) and allows us to theoretically understand the role of different model components on the generalization error. In addition, we demonstrate that in certain high-dimensional regimes, training over anonymous cluster centers acts as a regularization and improves generalization error of the trained models. Finally, we corroborate our asymptotic theory with finite-sample numerical experiments where we observe a perfect match when the sample size is only of order of a few hundreds.
</details>
<details>
<summary>摘要</summary>
personalized recommendation systems 已经变得越来越受欢迎，保护用户数据的安全仍然是开发这些学习系统的主要挑战。一种常见的方法来增强隐私是使用匿名数据进行模型训练而不是个人数据。在这篇论文中，我们探讨一种自然的技术called“look-alike clustering”，这种技术把个人敏感特征替换为群集的平均值。我们提供了精确的分析，证明训练使用匿名群集中心的模型会带来一定的泛化误差。我们将关注一个 asymptotic 的情况，在这个情况下，特征维度与训练集大小成正比。我们的分析基于Convex Gaussian Minimax Theorem (CGMT)，允许我们从理论角度理解不同模型组件对泛化误差的影响。此外，我们还证明在某些高维度情况下，使用匿名群集中心进行训练会作为一种正则化，提高训练模型的泛化误差。最后，我们通过finite-sample 的numerical experiments来证明我们的极限理论，并发现采用这种方法可以在样本大小只有几百的情况下达到完美匹配。
</details></li>
</ul>
<hr>
<h2 id="Accelerating-optimization-over-the-space-of-probability-measures"><a href="#Accelerating-optimization-over-the-space-of-probability-measures" class="headerlink" title="Accelerating optimization over the space of probability measures"></a>Accelerating optimization over the space of probability measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04006">http://arxiv.org/abs/2310.04006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shi Chen, Qin Li, Oliver Tse, Stephen J. Wright</li>
<li>for: 优化机器学习问题中的梯度下降问题</li>
<li>methods: 使用哈密顿流方法，类似于矩阵方法在欧几丁度空间中</li>
<li>results: 实现了无限阶 converges 率，数字示例证明了这一点<details>
<summary>Abstract</summary>
Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
</details>
<details>
<summary>摘要</summary>
“加速梯度基本优化方法是一个有亮点的实用和理论问题，尤其在机器学习应用中。大多数研究都集中在欧几何空间上进行优化，但在许多机器学习问题中需要优化概率分布空间，因此研究加速梯度方法在这种情况下也是非常有价值的。为此，我们提出了一种哈密顿流方法，与欧几何空间中的点基方法类似。我们证明了这种方法可以实现任意高阶准确率。numerical examples validate our claim。”Here's the word-for-word translation:“加速梯度基本优化方法是一个有亮点的实用和理论问题，尤其在机器学习应用中。大多数研究都集中在欧几何空间上进行优化，但在许多机器学习问题中需要优化概率分布空间，因此研究加速梯度方法在这种情况下也是非常有价值的。为此，我们提出了一种哈密顿流方法，与欧几何空间中的点基方法类似。我们证明了这种方法可以实现任意高阶准确率。numerical examples validate our claim。”
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Federated-Learning-in-a-Wireless-World-with-Foundation-Models"><a href="#The-Role-of-Federated-Learning-in-a-Wireless-World-with-Foundation-Models" class="headerlink" title="The Role of Federated Learning in a Wireless World with Foundation Models"></a>The Role of Federated Learning in a Wireless World with Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04003">http://arxiv.org/abs/2310.04003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihan Chen, Howard H. Yang, Y. C. Tay, Kai Fong Ernest Chong, Tony Q. S. Quek</li>
<li>for: 本文探讨了基于无线网络的联邦学习（FL）和基本模型（FM）之间的交互，以及将FM应用于FL中的可能性和挑战。</li>
<li>methods: 本文提出了多种新的思路和方法，用于实现将FM与FL相结合的未来智能网络。这些方法包括使用分布式计算和数据处理来帮助FM的训练，以及使用FM来提高FL的性能。</li>
<li>results: 本文提出了许多未来智能网络的研究挑战和机遇，包括如何使用FM和FL来提高网络性能和安全性，以及如何处理数据隐私和安全问题。<details>
<summary>Abstract</summary>
Foundation models (FMs) are general-purpose artificial intelligence (AI) models that have recently enabled multiple brand-new generative AI applications. The rapid advances in FMs serve as an important contextual backdrop for the vision of next-generation wireless networks, where federated learning (FL) is a key enabler of distributed network intelligence. Currently, the exploration of the interplay between FMs and FL is still in its nascent stage. Naturally, FMs are capable of boosting the performance of FL, and FL could also leverage decentralized data and computing resources to assist in the training of FMs. However, the exceptionally high requirements that FMs have for computing resources, storage, and communication overhead would pose critical challenges to FL-enabled wireless networks. In this article, we explore the extent to which FMs are suitable for FL over wireless networks, including a broad overview of research challenges and opportunities. In particular, we discuss multiple new paradigms for realizing future intelligent networks that integrate FMs and FL. We also consolidate several broad research directions associated with these paradigms.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们探讨 FMs 是否适用于 FL over wireless networks，包括研究挑战和机遇的广泛概述。具体来说，我们讨论了多种新的实现未来智能网络的方法，以及这些方法相关的多个广泛的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Runtime-Monitoring-DNN-Based-Perception"><a href="#Runtime-Monitoring-DNN-Based-Perception" class="headerlink" title="Runtime Monitoring DNN-Based Perception"></a>Runtime Monitoring DNN-Based Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03999">http://arxiv.org/abs/2310.03999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chih-Hong Cheng, Michael Luttenberger, Rongjie Yan</li>
<li>for: 本文旨在介绍一些用于实时验证深度神经网络（DNN）应用的方法，以确保这些应用不会导致安全问题。</li>
<li>methods: 文章提到了一些在机器学习社区中提出的监控方法，以及一些由正式方法社区提出的监控方法。两者之间的决策边界创建方式有所不同。</li>
<li>results: 文章强调了需要仔细设计监控器，特别是在数据可用性外 опера作域的情况下。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are instrumental in realizing complex perception systems. As many of these applications are safety-critical by design, engineering rigor is required to ensure that the functional insufficiency of the DNN-based perception is not the source of harm. In addition to conventional static verification and testing techniques employed during the design phase, there is a need for runtime verification techniques that can detect critical events, diagnose issues, and even enforce requirements. This tutorial aims to provide readers with a glimpse of techniques proposed in the literature. We start with classical methods proposed in the machine learning community, then highlight a few techniques proposed by the formal methods community. While we surely can observe similarities in the design of monitors, how the decision boundaries are created vary between the two communities. We conclude by highlighting the need to rigorously design monitors, where data availability outside the operational domain plays an important role.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="AdaRec-Adaptive-Sequential-Recommendation-for-Reinforcing-Long-term-User-Engagement"><a href="#AdaRec-Adaptive-Sequential-Recommendation-for-Reinforcing-Long-term-User-Engagement" class="headerlink" title="AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement"></a>AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03984">http://arxiv.org/abs/2310.03984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghai Xue, Qingpeng Cai, Tianyou Zuo, Bin Yang, Lantao Hu, Peng Jiang, Kun Gai, Bo An</li>
<li>for: 优化长期用户参与度的推荐任务中的RL算法。</li>
<li>methods: 提出了一种新的 Adaptive Sequential Recommendation（AdaRec）模式，使用距离基于表达函数损失来抽取用户互动轨迹中的隐藏信息，以反映RL策略与当前用户行为模式之间的适应度。</li>
<li>results: 在 simulator-based 和实际推荐任务中，AdaRec 展现出了与所有基准算法相比的长期性表现优异。<details>
<summary>Abstract</summary>
Growing attention has been paid to Reinforcement Learning (RL) algorithms when optimizing long-term user engagement in sequential recommendation tasks. One challenge in large-scale online recommendation systems is the constant and complicated changes in users' behavior patterns, such as interaction rates and retention tendencies. When formulated as a Markov Decision Process (MDP), the dynamics and reward functions of the recommendation system are continuously affected by these changes. Existing RL algorithms for recommendation systems will suffer from distribution shift and struggle to adapt in such an MDP. In this paper, we introduce a novel paradigm called Adaptive Sequential Recommendation (AdaRec) to address this issue. AdaRec proposes a new distance-based representation loss to extract latent information from users' interaction trajectories. Such information reflects how RL policy fits to current user behavior patterns, and helps the policy to identify subtle changes in the recommendation system. To make rapid adaptation to these changes, AdaRec encourages exploration with the idea of optimism under uncertainty. The exploration is further guarded by zero-order action optimization to ensure stable recommendation quality in complicated environments. We conduct extensive empirical analyses in both simulator-based and live sequential recommendation tasks, where AdaRec exhibits superior long-term performance compared to all baseline algorithms.
</details>
<details>
<summary>摘要</summary>
《增强用户持续参与的推荐算法》随着用户行为模式的不断变化，在大规模在线推荐系统中，RL算法的优化问题已经吸引了越来越多的关注。然而，这些变化会导致RL算法的分布shift，使得现有的RL算法难以适应。在这篇论文中，我们介绍了一种新的推荐算法called Adaptive Sequential Recommendation（AdaRec），用于解决这个问题。AdaRec提出了一种基于距离的表示损失来提取用户交互轨迹中的隐藏信息。这种信息反映RL策略是否适应当前用户行为模式，并帮助RL策略识别推荐系统中的微scopic变化。为了快速适应这些变化，AdaRec鼓励探索，并通过 Zero-order action optimization 来保证在复杂环境中的稳定推荐质量。我们在模拟器和实际推荐任务中进行了广泛的实验研究，并证明了AdaRec在长期性方面比所有基准算法表现出色。
</details></li>
</ul>
<hr>
<h2 id="Ultimate-limit-on-learning-non-Markovian-behavior-Fisher-information-rate-and-excess-information"><a href="#Ultimate-limit-on-learning-non-Markovian-behavior-Fisher-information-rate-and-excess-information" class="headerlink" title="Ultimate limit on learning non-Markovian behavior: Fisher information rate and excess information"></a>Ultimate limit on learning non-Markovian behavior: Fisher information rate and excess information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.03968">http://arxiv.org/abs/2310.03968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul M. Riechers</li>
<li>for: 本文探讨了从时间序列数据中学习未知参数的基本限制，并发现了optimal inference的最佳尺度是几何函数关于观测长度的平方根。</li>
<li>methods: 本文使用了参数化的模型类型，利用观测序列概率的fisher信息来下界模型参数的变分。</li>
<li>results: 作者发现了一个简单的关闭式表达式，用于描述对于不同Markov顺序的情况下的信息率。此外，作者还获得了对于 Observation-induced metadynamic的lower bound，以及不同模型的变分。<details>
<summary>Abstract</summary>
We address the fundamental limits of learning unknown parameters of any stochastic process from time-series data, and discover exact closed-form expressions for how optimal inference scales with observation length. Given a parametrized class of candidate models, the Fisher information of observed sequence probabilities lower-bounds the variance in model estimation from finite data. As sequence-length increases, the minimal variance scales as the square inverse of the length -- with constant coefficient given by the information rate. We discover a simple closed-form expression for this information rate, even in the case of infinite Markov order. We furthermore obtain the exact analytic lower bound on model variance from the observation-induced metadynamic among belief states. We discover ephemeral, exponential, and more general modes of convergence to the asymptotic information rate. Surprisingly, this myopic information rate converges to the asymptotic Fisher information rate with exactly the same relaxation timescales that appear in the myopic entropy rate as it converges to the Shannon entropy rate for the process. We illustrate these results with a sequence of examples that highlight qualitatively distinct features of stochastic processes that shape optimal learning.
</details>
<details>
<summary>摘要</summary>
我们研究了任何测量过程的不知数参数的基本限制，并发现了对观测长度的优化推断的准确闭形表达。给定一个参数化的模型类，观测序列概率的鱼 informationsLower bounds模型参数的噪声Variance from finite data。随着序列长度增加，最小噪声 scaling为时间平方 reciprocal —— with constant coefficient given by the information rate。我们还发现了观测引起的层次隐藏函数的下界，以及这个函数的closed-form表达，包括无穷Markov顺序的情况。此外，我们还发现了不同类型的收敛模式，包括短暂、指数和更一般的收敛模式，并且这些收敛模式与 asymptotic Fisher information rate的relaxation timescales完全相同。我们通过一系列示例来highlight这些结果的Qualitatively distinct features of stochastic processes that shape optimal learning.Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/cs.LG_2023_10_06/" data-id="closbrors00qz0g88eid159zi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/eess.IV_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T09:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/eess.IV_2023_10_06/">eess.IV - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Plug-and-Play-Image-Registration-Network"><a href="#A-Plug-and-Play-Image-Registration-Network" class="headerlink" title="A Plug-and-Play Image Registration Network"></a>A Plug-and-Play Image Registration Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04297">http://arxiv.org/abs/2310.04297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhao Hu, Weijie Gan, Zhixin Sun, Hongyu An, Ulugbek S. Kamilov<br>for: 这个研究旨在开发一个基于深度学习的形变影像注册（DIR）方法，以提高生物医学影像注册的精度和效率。methods: 这个方法基于一个条件enced Convolutional Neural Network（CNN）来估计两个输入影像之间的注册场，并透过将CNN检测器”嵌入”到一个迭代法中，以增强注册的稳定性和准确性。results: 我们的方法在OASIS和CANDI dataset上的数据显示，能够 дости得生物医学影像注册的州度顶峰性能。<details>
<summary>Abstract</summary>
Deformable image registration (DIR) is an active research topic in biomedical imaging. There is a growing interest in developing DIR methods based on deep learning (DL). A traditional DL approach to DIR is based on training a convolutional neural network (CNN) to estimate the registration field between two input images. While conceptually simple, this approach comes with a limitation that it exclusively relies on a pre-trained CNN without explicitly enforcing fidelity between the registered image and the reference. We present plug-and-play image registration network (PIRATE) as a new DIR method that addresses this issue by integrating an explicit data-fidelity penalty and a CNN prior. PIRATE pre-trains a CNN denoiser on the registration field and "plugs" it into an iterative method as a regularizer. We additionally present PIRATE+ that fine-tunes the CNN prior in PIRATE using deep equilibrium models (DEQ). PIRATE+ interprets the fixed-point iteration of PIRATE as a network with effectively infinite layers and then trains the resulting network end-to-end, enabling it to learn more task-specific information and boosting its performance. Our numerical results on OASIS and CANDI datasets show that our methods achieve state-of-the-art performance on DIR.
</details>
<details>
<summary>摘要</summary>
扭形图像registratio (DIR) 是生物医学成像领域的活跃研究领域。随着深度学习 (DL) 的发展，DIR 方法也在不断地演化。传统的 DL 方法是通过训练一个卷积神经网络 (CNN) 来估计两个输入图像之间的 registrtion 场。然而，这种方法存在一个限制，即完全依赖于预训练的 CNN，而不是直接强制图像注册和参考图像之间的准确性。我们提出了一种新的 DIR 方法，即插件和游戏图像注册网络 (PIRATE)，它通过结合显式数据准确性罚和 CNN 先验来解决这个问题。PIRATE 先训练了一个 CNN 减噪器在注册场景中，然后将其作为 PIRATE 的规则进行插入。此外，我们还提出了 PIRATE+，它在 PIRATE 中使用深度平衡模型 (DEQ) 进行 fine-tuning，从而使 PIRATE 能够更好地学习任务特有的信息，提高其性能。我们的数字结果表明，我们的方法在 OASIS 和 CANDI 数据集上实现了 DIR 领域的状态能力。
</details></li>
</ul>
<hr>
<h2 id="Towards-Non-contact-3D-Ultrasound-for-Wrist-Imaging"><a href="#Towards-Non-contact-3D-Ultrasound-for-Wrist-Imaging" class="headerlink" title="Towards Non-contact 3D Ultrasound for Wrist Imaging"></a>Towards Non-contact 3D Ultrasound for Wrist Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04296">http://arxiv.org/abs/2310.04296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antony Jerald, A. N. Madhavanunni, Gayathri Malamal, Mahesh Raveendranatha Panicker<br>for:The paper aims to develop a novel approach for non-contact freehand 3D ultrasound imaging with minimal complexity added to existing point of care ultrasound (POCUS) systems.methods:The proposed approach uses a mechanical track for non-contact ultrasound scanning, which restricts the probe motion to a linear plane and simplifies the acquisition and 3D reconstruction process. A pipeline for US 3D volume reconstruction using an US research platform and a GPU-based edge device is developed.results:The proposed approach is demonstrated through ex-vivo and in-vivo experiments, showing its efficacy in providing accurate 3D US imaging with adjustable field of view capability, non-contact design, and low cost of deployment without significantly altering the existing setup.Please note that the above information is in Simplified Chinese:for: 这篇论文的目的是开发一种新的非接触自由手3D超声成像方法，以便在现有的点检超声系统（POCUS）上增加了最小复杂度。methods: 该提议使用机械轨迹进行非接触超声扫描，这限制了探针的运动范围为直线平面，以简化获取和3D重建过程。一个基于超声研究平台和GPU的边缘设备上的3D超声卷积管道也被开发。results: 该提议的效果通过外科和生物实验显示，能够提供精度的3D超声成像，并且具有可调Field of view功能、非接触设计和低成本部署，不会对现有设置产生显著影响。<details>
<summary>Abstract</summary>
Objective: The objective of this work is an attempt towards non-contact freehand 3D ultrasound imaging with minimal complexity added to the existing point of care ultrasound (POCUS) systems. Methods: This study proposes a novel approach of using a mechanical track for non-contact ultrasound (US) scanning. The approach thus restricts the probe motion to a linear plane, to simplify the acquisition and 3D reconstruction process. A pipeline for US 3D volume reconstruction employing an US research platform and a GPU-based edge device is developed. Results: The efficacy of the proposed approach is demonstrated through ex-vivo and in-vivo experiments. Conclusion: The proposed approach with the adjustable field of view capability, non-contact design, and low cost of deployment without significantly altering the existing setup would open doors for up gradation of traditional systems to a wide range of 3D US imaging applications. Significance: Ultrasound (US) imaging is a popular clinical imaging modality for the point-of-care bedside imaging, particularly of the wrist/knee in the pediatric population due to its non-invasive and radiation free nature. However, the limited views of tissue structures obtained with 2D US in such scenarios make the diagnosis challenging. To overcome this, 3D US imaging which uses 2D US images and their orientation/position to reconstruct 3D volumes was developed. The accurate position estimation of the US probe at low cost has always stood as a challenging task in 3D reconstruction. Additionally, US imaging involves contact, which causes difficulty to pediatric subjects while monitoring live fractures or open wounds. Towards overcoming these challenges, a novel framework is attempted in this work.
</details>
<details>
<summary>摘要</summary>
Methods: This study proposes a new approach using a mechanical track for non-contact US scanning, which simplifies the acquisition and 3D reconstruction process by restricting the probe motion to a linear plane. A pipeline for US 3D volume reconstruction using an US research platform and a GPU-based edge device is developed.Results: The effectiveness of the proposed approach is demonstrated through ex-vivo and in-vivo experiments.Conclusion: The proposed approach with adjustable field of view capability, non-contact design, and low cost of deployment without significantly altering the existing setup would expand the applications of 3D US imaging in a wide range of clinical scenarios, particularly in pediatric populations.Significance: Traditional 2D US imaging has limited views of tissue structures, making diagnosis challenging. 3D US imaging overcomes this limitation by using 2D US images and their orientation/position to reconstruct 3D volumes. However, accurate probe position estimation at low cost has been a long-standing challenge in 3D reconstruction. Additionally, contact-based US imaging can be difficult for pediatric subjects, particularly when monitoring live fractures or open wounds. This novel framework addresses these challenges and has the potential to upgrade traditional systems for a wide range of 3D US imaging applications.In Simplified Chinese:目标：本研究的目标是开发一种新的、非接触、低成本的ultrasound（US）图像三维重建方法，以便在现有的点检查ultrasound（POCUS）系统上进行最小化的修改。方法：这种研究提议使用机械轨迹来实现非接触US扫描，这将简化获取和三维重建过程，并且只有在 linear 平面上进行探针运动。在US研究平台和GPU基于的边缘设备上开发了US三维图像重建的管道。结果：经过对外部和内部实验，效果表明了该方法的可行性。结论：该方法可以提供可调适的视场、非接触设计和低成本实施，不会对现有设置进行重大改变。这将扩展3D US图像 reconstruction在各种临床应用中的可能性，特别是在儿童人口中。重要性：传统的2D US图像有限制的视场，从而使诊断变得困难。3D US图像使用2D US图像和其orientation/position来重建3DVolume，从而突破这些限制。然而，在3D重建中准确地计算探针位置的低成本问题一直是一个挑战。此外，基于接触的US图像扫描可能会对儿童人口产生困难，特别是在监测活动骨折或开放性伤口时。这种新的框架可以解决这些挑战，并且有可能升级传统系统，以扩展3D US图像重建的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Hessian-based-Similarity-Metric-for-Multimodal-Medical-Image-Registration"><a href="#Hessian-based-Similarity-Metric-for-Multimodal-Medical-Image-Registration" class="headerlink" title="Hessian-based Similarity Metric for Multimodal Medical Image Registration"></a>Hessian-based Similarity Metric for Multimodal Medical Image Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04009">http://arxiv.org/abs/2310.04009</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Eskandari, Houssem-Eddine Gueziri, D. Louis Collins</li>
<li>for: 这个论文主要是为了提出一种新的医学影像匹配算法，用于衡量不同医学影像模式之间的相似性。</li>
<li>methods: 该论文使用了一种基于幂函数的方法，通过研究两个完美匹配的图像板块之间的偏微分关系，来量化它们之间的相似性。</li>
<li>results: 该论文通过实验表明，该新的相似性度量可以快速和精度地衡量不同医学影像模式之间的相似性，并且可以快速和精度地进行医学影像匹配。<details>
<summary>Abstract</summary>
One of the fundamental elements of both traditional and certain deep learning medical image registration algorithms is measuring the similarity/dissimilarity between two images. In this work, we propose an analytical solution for measuring similarity between two different medical image modalities based on the Hessian of their intensities. First, assuming a functional dependence between the intensities of two perfectly corresponding patches, we investigate how their Hessians relate to each other. Secondly, we suggest a closed-form expression to quantify the deviation from this relationship, given arbitrary pairs of image patches. We propose a geometrical interpretation of the new similarity metric and an efficient implementation for registration. We demonstrate the robustness of the metric to intensity nonuniformities using synthetic bias fields. By integrating the new metric in an affine registration framework, we evaluate its performance for MRI and ultrasound registration in the context of image-guided neurosurgery using target registration error and computation time.
</details>
<details>
<summary>摘要</summary>
一种基本元素 OF both traditional and certain deep learning医疗图像注册算法是测量两个图像之间的相似性/不同性。在这项工作中，我们提出了一个分析解决方案，用于测量两种不同医疗图像模式之间的相似性，基于图像强度的赫西安关系。首先，我们假设两个完美匹配的图像块之间存在函数依赖关系，然后我们研究了这两个赫西安之间的关系。其次，我们提出了一个具有closed-form表达式，用于衡量这种关系的偏差，给出任意两个图像块的对应。我们提出了一种几何解释这个新的相似度标准和一种高效的实现方式，并且在affine注册框架中集成了这个标准。我们通过使用模拟的扭曲场来证明metric的稳定性，并且在MRI和ultrasound注册中进行了image-guided neurosurgery的应用，通过target registration error和计算时间来评估metric的性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/eess.IV_2023_10_06/" data-id="closbroyr017z0g885w5y5q54" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/06/eess.SP_2023_10_06/" class="article-date">
  <time datetime="2023-10-06T08:00:00.000Z" itemprop="datePublished">2023-10-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/06/eess.SP_2023_10_06/">eess.SP - 2023-10-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Deep-Learning-Based-Active-Spatial-Channel-Gain-Prediction-Using-a-Swarm-of-Unmanned-Aerial-Vehicles"><a href="#Deep-Learning-Based-Active-Spatial-Channel-Gain-Prediction-Using-a-Swarm-of-Unmanned-Aerial-Vehicles" class="headerlink" title="Deep Learning Based Active Spatial Channel Gain Prediction Using a Swarm of Unmanned Aerial Vehicles"></a>Deep Learning Based Active Spatial Channel Gain Prediction Using a Swarm of Unmanned Aerial Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04547">http://arxiv.org/abs/2310.04547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enes Krijestorac, Danijela Cabric</li>
<li>for: 预测无线通道增强（CG）在空间中的预测是许多重要无线网络设计问题的必需工具。本文开发了采用环境特定特征，即建筑地图和CG测量，以实现高精度预测的预测方法。</li>
<li>methods: 我们提出了两种活动预测方法，即基于深度学习（DL）和 Kriging  interpolación。第一种方法不依赖发送器位置，并利用3D地图补做不精确的预测。我们使用DL来 incorporate 3D maps into prediction和 reinforcement learning for optimal path planning for UAVs based on DL prediction。第二种方法基于 Kriging interpolación，需要知道发送器位置，而且不能使用3D地图。我们在一个基于射线追踪的通道模拟器中训练和评估两种提议的方法。</li>
<li>results: 我们通过 simulated experiments demonstrate the importance of active prediction compared to prediction based on randomly collected measurements of channel gain。另外，我们还表明使用 DL 和 3D maps，可以在不知道发送器位置的情况下实现高精度预测。 finally，我们还证明了在使用多个 UAVs 采集测量时，协调的路径规划对于活动预测具有重要的重要性。<details>
<summary>Abstract</summary>
Prediction of wireless channel gain (CG) across space is a necessary tool for many important wireless network design problems. In this paper, we develop prediction methods that use environment-specific features, namely building maps and CG measurements, to achieve high prediction accuracy. We assume that measurements are collected using a swarm of coordinated unmanned aerial vehicles (UAVs). We develop novel active prediction approaches which consist of both methods for UAV path planning for optimal measurement collection and methods for prediction of CG across space based on the collected measurements. We propose two active prediction approaches based on deep learning (DL) and Kriging interpolation. The first approach does not rely on the location of the transmitter and utilizes 3D maps to compensate for the lack of it. We utilize DL to incorporate 3D maps into prediction and reinforcement learning for optimal path planning for the UAVs based on DL prediction. The second active prediction approach is based on Kriging interpolation, which requires known transmitter location and cannot utilize 3D maps. We train and evaluate the two proposed approaches in a ray-tracing-based channel simulator. Using simulations, we demonstrate the importance of active prediction compared to prediction based on randomly collected measurements of channel gain. Furthermore, we show that using DL and 3D maps, we can achieve high prediction accuracy even without knowing the transmitter location. We also demonstrate the importance of coordinated path planning for active prediction when using multiple UAVs compared to UAVs collecting measurements independently in a greedy manner.
</details>
<details>
<summary>摘要</summary>
<<SYS>>按照以下准则进行简化中文翻译：1. 使用标准中文翻译词汇和 grammar2. 尽可能简化语句结构和表达3. 保留原文的意思和主题Prediction of wireless channel gain (CG) across space is a crucial tool for many important wireless network design problems. In this paper, we develop prediction methods that use environment-specific features, namely building maps and CG measurements, to achieve high prediction accuracy. We assume that measurements are collected using a swarm of coordinated unmanned aerial vehicles (UAVs). We develop novel active prediction approaches that consist of both methods for UAV path planning for optimal measurement collection and methods for prediction of CG across space based on the collected measurements. We propose two active prediction approaches based on deep learning (DL) and Kriging interpolation. The first approach does not rely on the location of the transmitter and utilizes 3D maps to compensate for the lack of it. We utilize DL to incorporate 3D maps into prediction and reinforcement learning for optimal path planning for the UAVs based on DL prediction. The second active prediction approach is based on Kriging interpolation, which requires known transmitter location and cannot utilize 3D maps. We train and evaluate the two proposed approaches in a ray-tracing-based channel simulator. Using simulations, we demonstrate the importance of active prediction compared to prediction based on randomly collected measurements of channel gain. Furthermore, we show that using DL and 3D maps, we can achieve high prediction accuracy even without knowing the transmitter location. We also demonstrate the importance of coordinated path planning for active prediction when using multiple UAVs compared to UAVs collecting measurements independently in a greedy manner.Translated text:预测无线通道增强（CG）在空间是许多重要无线网络设计问题中的必需工具。在这篇论文中，我们开发了预测方法，使用环境特定特征，namely building maps和CG测量，以实现高预测精度。我们假设测量是通过一群协调的无人飞行器（UAVs）进行收集。我们开发了两种活动预测方法，它们分别基于深度学习（DL）和 Kriging  interpolate。第一种方法不依赖发送器位置，并利用3D地图补偿发送器位置的缺失。我们利用 DL 将3D地图 incorporated into prediction，并通过强化学习对 UAVs 的路径规划进行优化，基于 DL 预测。第二种方法基于 Kriging interpolate，它需要知道发送器位置，而且无法使用3D地图。我们在一个基于投影法的通道模拟器中训练和评估了两种提议的方法。使用仿真，我们表明了活动预测比随机收集通道增强的预测更重要。此外，我们还表明了使用 DL 和3D地图，我们可以在不知道发送器位置的情况下实现高预测精度。我们还 demonstarted 多个 UAVs 协调的路径规划对活动预测的重要性。
</details></li>
</ul>
<hr>
<h2 id="Evolution-of-High-Throughput-Satellite-Systems-Vision-Requirements-and-Key-Technologies"><a href="#Evolution-of-High-Throughput-Satellite-Systems-Vision-Requirements-and-Key-Technologies" class="headerlink" title="Evolution of High Throughput Satellite Systems: Vision, Requirements, and Key Technologies"></a>Evolution of High Throughput Satellite Systems: Vision, Requirements, and Key Technologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04389">http://arxiv.org/abs/2310.04389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olfa Ben Yahia, Zineb Garroussi, Olivier Bélanger, Brunilde Sansò, Jean-François Frigon, Stéphane Martel, Antoine Lesage-Landry, Gunes Karabulut Kurt</li>
<li>For: The paper provides a comprehensive state-of-the-art of high throughput satellite (HTS) systems and envisions the next generation of extremely high-throughput satellite (EHTS) systems.* Methods: The paper discusses various techniques such as beamforming, advanced modulation techniques, reconfigurable phased array technologies, and electronically steerable antennas that are being used to improve the performance of HTS systems.* Results: The paper provides a vision for future EHTS systems that will maximize spectrum reuse and data rates, and flexibly steer capacity to satisfy user demand. Additionally, the paper introduces a novel architecture for future regenerative payloads and summarizes the challenges imposed by this architecture.<details>
<summary>Abstract</summary>
High throughput satellites (HTS), with their digital payload technology, are expected to play a key role as enablers of the upcoming 6G networks. HTS are mainly designed to provide higher data rates and capacities. Fueled by technological advancements including beamforming, advanced modulation techniques, reconfigurable phased array technologies, and electronically steerable antennas, HTS have emerged as a fundamental component for future network generation. This paper offers a comprehensive state-of-the-art of HTS systems, with a focus on standardization, patents, channel multiple access techniques, routing, load balancing, and the role of software-defined networking (SDN). In addition, we provide a vision for next-satellite systems that we named as extremely-HTS (EHTS) toward autonomous satellites supported by the main requirements and key technologies expected for these systems. The EHTS system will be designed such that it maximizes spectrum reuse and data rates, and flexibly steers the capacity to satisfy user demand. We introduce a novel architecture for future regenerative payloads while summarizing the challenges imposed by this architecture.
</details>
<details>
<summary>摘要</summary>
高通信率卫星（HTS）预计将扮演6G网络的关键激活器。HTS主要用于提供更高的数据速率和容量。驱动技术的进步，包括射频扫描、高级调制技术、可编程相位阵列技术和电子扫描天线，使HTS成为未来网络代表性的组件。本文提供了HTS系统的全面状态艺术，强调标准化、套件、通道多访问技术、路由、负荷均衡和软件定义网络（SDN）的角色。此外，我们还提出了下一代卫星系统，我们称之为“极高通信率卫星”（EHTS），该系统将具备自主卫星的主要需求和关键技术。EHTS系统将实现spectrum reuse和数据速率的最大化，并可以自动调整容量来满足用户需求。我们还介绍了未来复合 payload 的新架构，并总结了这种架构带来的挑战。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Backpressure-Routing-Using-Wireless-Link-Features"><a href="#Enhanced-Backpressure-Routing-Using-Wireless-Link-Features" class="headerlink" title="Enhanced Backpressure Routing Using Wireless Link Features"></a>Enhanced Backpressure Routing Using Wireless Link Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04364">http://arxiv.org/abs/2310.04364</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongyuan Zhao, Gunjan Verma, Ananthram Swami, Santiago Segarra</li>
<li>for: 提高 wireless multi-hop 网络中分布式Routing和Scheduling的效率和延迟</li>
<li>methods: 使用 Biased BP 和短路寻址机制，不增加每次时间步骤的信号量 overhead</li>
<li>results: 提出了优化积分偏好、保持偏好在移动环境下、以及 incorporating sojourn time awareness into biased BP 等三个长期挑战，并通过分析和实验证明其效果。<details>
<summary>Abstract</summary>
Backpressure (BP) routing is a well-established framework for distributed routing and scheduling in wireless multi-hop networks. However, the basic BP scheme suffers from poor end-to-end delay due to the drawbacks of slow startup, random walk, and the last packet problem. Biased BP with shortest path awareness can address the first two drawbacks, and sojourn time-based backlog metrics were proposed for the last packet problem. Furthermore, these BP variations require no additional signaling overhead in each time step compared to the basic BP. In this work, we further address three long-standing challenges associated with the aforementioned low-cost BP variations, including optimal scaling of the biases, bias maintenance under mobility, and incorporating sojourn time awareness into biased BP. Our analysis and experimental results show that proper scaling of biases can be achieved with the help of common link features, which can effectively reduce end-to-end delay of BP by mitigating the random walk of packets under low-to-medium traffic, including the last packet scenario. In addition, our low-overhead bias maintenance scheme is shown to be effective under mobility, and our bio-inspired sojourn time-aware backlog metric is demonstrated to be more efficient and effective for the last packet problem than existing approaches when incorporated into biased BP.
</details>
<details>
<summary>摘要</summary>
背压路由（BP）是无线多项网络中分布路由和排程的一个成熟框架。然而，基本BP方案受到终端到终端延迟的问题，包括启动时间较慢、随机漫步和最后一个包问题。偏好BP可以解决首两个问题，而且可以使用游历时间-基础的伙伴度量来解决最后一个包问题。此外，这些BP变化不需要每个时间步骤中额外的讯号过程。在这个工作中，我们进一步解决了这些低成本BP变化的三个长期挑战，包括对偏好的优化维护、在移动环境中维护偏好以及将游历时间意识到偏好BP中。我们的分析和实验结果显示，正确地对偏好进行缩小可以使用通用链接特征来减少BP对终端的延迟，包括最后一个包enario。此外，我们的低负载维护方案在移动环境中是有效的，并且将游历时间意识到偏好BP中的方法比较高效和有效。
</details></li>
</ul>
<hr>
<h2 id="A-physics-informed-generative-model-for-passive-radio-frequency-sensing"><a href="#A-physics-informed-generative-model-for-passive-radio-frequency-sensing" class="headerlink" title="A physics-informed generative model for passive radio-frequency sensing"></a>A physics-informed generative model for passive radio-frequency sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04173">http://arxiv.org/abs/2310.04173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefano Savazzi, Federica Fieramosca, Sanaz Kianoush, Vittorio Rampa, Michele D’amico</li>
<li>for: 研究人员使用电romagnetic (EM) 体模型来预测无线设备附近的电磁波强度，并且应用于通信和位置测定等问题。</li>
<li>methods: 使用physics-informed生成神经网络 (GNN) 模型，将电磁波体diffraction方法 incorporated into variational autoencoder (VAE) 技术，以便模拟&#x2F;重建缺失的样本或学习受物理法则约束的数据分布。</li>
<li>results: 与传统diffraction-based EM body工具相比，提出的 EM-informed生成模型能够更好地预测真实的电磁波强度，并且在实际测量数据上验证了其有效性。<details>
<summary>Abstract</summary>
Electromagnetic (EM) body models predict the impact of human presence and motions on the Radio-Frequency (RF) stray radiation received by wireless devices nearby. These wireless devices may be co-located members of a Wireless Local Area Network (WLAN) or even cellular devices connected with a Wide Area Network (WAN). Despite their accuracy, EM models are time-consuming methods which prevent their adoption in strict real-time computational imaging problems and Bayesian estimation, such as passive localization, RF tomography, and holography. Physics-informed Generative Neural Network (GNN) models have recently attracted a lot of attention thanks to their potential to reproduce a process by incorporating relevant physical laws and constraints. Thus, GNNs can be used to simulate/reconstruct missing samples, or learn physics-informed data distributions. The paper discusses a Variational Auto-Encoder (VAE) technique and its adaptations to incorporate a relevant EM body diffraction method with applications to passive RF sensing and localization/tracking. The proposed EM-informed generative model is verified against classical diffraction-based EM body tools and validated on real RF measurements. Applications are also introduced and discussed.
</details>
<details>
<summary>摘要</summary>
电磁体（EM）模型预测人员存在和运动对附近无线设备接收的 радио频偏振（RF）杂谱的影响。这些无线设备可能是分布在同一个地方的无线本地网络（WLAN）成员或者连接到宽带网络（WAN）的无线设备。尽管它们的准确性很高，但EM模型是时间consuming的方法，这阻碍了它们在严格的实时计算图像问题和 bayesian估计中的采用。物理学 Informed Generative Neural Network（GNN）模型在最近吸引了很多关注，因为它们可以通过包含相关的物理法律和约束来重现一个过程。因此，GNN可以用来 simulate/重construct缺失的样本，或者学习物理学 Informed 数据分布。文章介绍了一种 Variational Auto-Encoder（VAE）技术和其修改，以包含相关的EM体 diffraction 方法，并应用于无线RF感知和定位/跟踪。提出的EM-informed生成模型被证明了 классиical diffraction-based EM体工具和实际RF测量。应用也是介绍和讨论的。
</details></li>
</ul>
<hr>
<h2 id="Physics-assisted-machine-learning-for-THz-spectroscopy-sensing-moisture-on-plant-leaves"><a href="#Physics-assisted-machine-learning-for-THz-spectroscopy-sensing-moisture-on-plant-leaves" class="headerlink" title="Physics-assisted machine learning for THz spectroscopy: sensing moisture on plant leaves"></a>Physics-assisted machine learning for THz spectroscopy: sensing moisture on plant leaves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.04056">http://arxiv.org/abs/2310.04056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Milan Koumans, Daan Meulendijks, Haiko Middeljans, Djero Peeters, Jacob C. Douma, Dook van Mechelen</li>
<li>for: 这个论文旨在用机器学习技术提高 THz 时间域спектроскопи亮度，以实现实用应用。</li>
<li>methods: 该论文使用了决策树和卷积神经网络等机器学习技术，基于光物理学知识进行辅助。</li>
<li>results: 研究人员通过对 12,000 个水pattern 的 THz 时间域数据进行分析，提出了关于决定水Pattern 的重要发现，并证明了这些模型在不同的测试集上的普适性。<details>
<summary>Abstract</summary>
Signal processing techniques are of vital importance to bring THz spectroscopy to a maturity level to reach practical applications. In this work, we illustrate the use of machine learning techniques for THz time-domain spectroscopy assisted by domain knowledge based on light-matter interactions. We aim at the potential agriculture application to determine the amount of free water on plant leaves, so-called leaf wetness. This quantity is important for understanding and predicting plant diseases that need leaf wetness for disease development. The overall transmission of a moist plant leaf for 12,000 distinct water patterns was experimentally acquired using THz time-domain spectroscopy. We report on key insights of applying decision trees and convolutional neural networks to the data using physics-motivated choices. Eventually, we discuss the generalizability of these models to determine leaf wetness after testing them on cases with increasing deviations from the training set.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Signal processing techniques are of vital importance to bring THz spectroscopy to a maturity level to reach practical applications. In this work, we illustrate the use of machine learning techniques for THz time-domain spectroscopy assisted by domain knowledge based on light-matter interactions. We aim at the potential agriculture application to determine the amount of free water on plant leaves, so-called leaf wetness. This quantity is important for understanding and predicting plant diseases that need leaf wetness for disease development. The overall transmission of a moist plant leaf for 12,000 distinct water patterns was experimentally acquired using THz time-domain spectroscopy. We report on key insights of applying decision trees and convolutional neural networks to the data using physics-motivated choices. Eventually, we discuss the generalizability of these models to determine leaf wetness after testing them on cases with increasing deviations from the training set." into Simplified Chinese.中文简体版：信号处理技术对于 THz спектроскопия的成熔度具有核心重要性，以实现实用应用。本工作介绍了基于光物理相互作用的机器学习技术在 THz 时域спектроскопии中的应用。我们target了农业应用，通过测量植物叶子上的自由水量，也称为叶质湿度。这个量对于理解和预测植物疾病非常重要，疾病发展需要叶质湿度。我们通过 THz 时域спектроскопии实验获得了12,000个不同水平的叶质湿度数据。我们使用决策树和卷积神经网络对数据进行分析，并根据物理原理进行选择。最后，我们讨论了这些模型在不同于训练集的情况下的泛化性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/06/eess.SP_2023_10_06/" data-id="closbrp0801bo0g888l1nf1xe" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/24/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><a class="page-number" href="/page/27/">27</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/26/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
