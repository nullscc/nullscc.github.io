
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/25/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CL_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/cs.CL_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T11:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/cs.CL_2023_10_19/">cs.CL - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NameGuess-Column-Name-Expansion-for-Tabular-Data"><a href="#NameGuess-Column-Name-Expansion-for-Tabular-Data" class="headerlink" title="NameGuess: Column Name Expansion for Tabular Data"></a>NameGuess: Column Name Expansion for Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13196">http://arxiv.org/abs/2310.13196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amazon-science/nameguess">https://github.com/amazon-science/nameguess</a></li>
<li>paper_authors: Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Shen Wang, Huzefa Rangwala, George Karypis</li>
<li>for:  This paper aims to address the challenge of abbreviated column names in large volumes of tabular data, which can negatively impact performance on various data search, access, and understanding tasks.</li>
<li>methods:  The paper introduces a new task called NameGuess, which expands column names in database schemas as a natural language generation problem. The authors create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark. They enhance auto-regressive language models by conditioning on table content and column header names to improve performance.</li>
<li>results:  The fine-tuned model (with 2.7B parameters) matches human performance in the NameGuess task, and the authors conduct a comprehensive analysis to validate the effectiveness of table content in NameGuess and identify promising future opportunities. The code for the paper has been made available at <a target="_blank" rel="noopener" href="https://github.com/amazon-science/nameguess">https://github.com/amazon-science/nameguess</a>.<details>
<summary>Abstract</summary>
Recent advances in large language models have revolutionized many sectors, including the database industry. One common challenge when dealing with large volumes of tabular data is the pervasive use of abbreviated column names, which can negatively impact performance on various data search, access, and understanding tasks. To address this issue, we introduce a new task, called NameGuess, to expand column names (used in database schema) as a natural language generation problem. We create a training dataset of 384K abbreviated-expanded column pairs using a new data fabrication method and a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the complexities associated with polysemy and ambiguity in NameGuess, we enhance auto-regressive language models by conditioning on table content and column header names -- yielding a fine-tuned model (with 2.7B parameters) that matches human performance. Furthermore, we conduct a comprehensive analysis (on multiple LLMs) to validate the effectiveness of table content in NameGuess and identify promising future opportunities. Code has been made available at https://github.com/amazon-science/nameguess.
</details>
<details>
<summary>摘要</summary>
To create a training dataset for NameGuess, we employed a new data fabrication method and compiled a human-annotated evaluation benchmark that includes 9.2K examples from real-world tables. To tackle the challenges associated with polysemy and ambiguity in NameGuess, we enhanced auto-regressive language models by conditioning on table content and column header names, resulting in a fine-tuned model with 2.7B parameters that matches human performance.We conducted a comprehensive analysis of multiple large language models to validate the effectiveness of table content in NameGuess and identify promising future opportunities. The code for NameGuess has been made available on GitHub at <https://github.com/amazon-science/nameguess>.
</details></li>
</ul>
<hr>
<h2 id="Breaking-through-Deterministic-Barriers-Randomized-Pruning-Mask-Generation-and-Selection"><a href="#Breaking-through-Deterministic-Barriers-Randomized-Pruning-Mask-Generation-and-Selection" class="headerlink" title="Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection"></a>Breaking through Deterministic Barriers: Randomized Pruning Mask Generation and Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13183">http://arxiv.org/abs/2310.13183</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Li, Weizhi Gao, Qi Lei, Dongkuan Xu</li>
<li>for: 提高模型的准确率，透过减少模型的神经元或参数数量来缩小模型的大小。</li>
<li>methods: 提出一种随机生成减少Mask的策略，并且采用有效的Mask选择规则，从多个Mask候选者中选择最佳的Mask。</li>
<li>results: 在GLUE dataset上进行了广泛的实验，并达到了当前最佳性能水平，特别是在高水平的缩放性能上表现出色。<details>
<summary>Abstract</summary>
It is widely acknowledged that large and sparse models have higher accuracy than small and dense models under the same model size constraints. This motivates us to train a large model and then remove its redundant neurons or weights by pruning. Most existing works pruned the networks in a deterministic way, the performance of which solely depends on a single pruning criterion and thus lacks variety. Instead, in this paper, we propose a model pruning strategy that first generates several pruning masks in a designed random way. Subsequently, along with an effective mask-selection rule, the optimal mask is chosen from the pool of mask candidates. To further enhance efficiency, we introduce an early mask evaluation strategy, mitigating the overhead associated with training multiple masks. Our extensive experiments demonstrate that this approach achieves state-of-the-art performance across eight datasets from GLUE, particularly excelling at high levels of sparsity.
</details>
<details>
<summary>摘要</summary>
广泛认可的大型和稀疏模型在同等模型大小约束下表现更高准确性。这种情况motivates我们训练一个大型模型，然后从其中 removes redundant neurons or weights by pruning.现有的大多数工作采用了 deterministic pruning方法，其性能受到单一采样决定因子的限制，lacks variety。在这篇论文中，我们提议一种模型剪除策略，首先生成多个剪除面积 candidatese。然后，通过一个有效的面积选择规则，选择 pool of mask candidates中的优化面积。为了进一步提高效率，我们引入了一种早期剪除评估策略， Mitigate the overhead associated with training multiple masks.我们的广泛实验表明，这种方法在GLUE数据集上 achieves state-of-the-art performance，特别是在高水平的稀疏性下表现出色。
</details></li>
</ul>
<hr>
<h2 id="Auto-Instruct-Automatic-Instruction-Generation-and-Ranking-for-Black-Box-Language-Models"><a href="#Auto-Instruct-Automatic-Instruction-Generation-and-Ranking-for-Black-Box-Language-Models" class="headerlink" title="Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models"></a>Auto-Instruct: Automatic Instruction Generation and Ranking for Black-Box Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13127">http://arxiv.org/abs/2310.13127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihan Zhang, Shuohang Wang, Wenhao Yu, Yichong Xu, Dan Iter, Qingkai Zeng, Yang Liu, Chenguang Zhu, Meng Jiang</li>
<li>for: 提高大型自然语言处理器（LLM）的任务性能，无需特定任务的精心调整。</li>
<li>methods: 利用LLM自然语言指令的内生生成能力，生成多个候选指令，然后使用经过训练的分数模型对其进行排序。</li>
<li>results: 在118个 OUT-OF-DOMAIN任务上，Auto-Instruct比人工写的指令和现有的LLM生成指令都更高，并且具有很好的普适性，能够在其他LLM上进行排序。<details>
<summary>Abstract</summary>
Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both human-written instructions and existing baselines of LLM-generated instructions. Furthermore, our method exhibits notable generalizability even with other LLMs that are not incorporated into its training process.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）可以完成广泛的任务，只需按照自然语言指令进行操作，无需特定任务的精细调整。然而，LLM的性能受到指令质量的影响，并且手动编写每个任务的有效指令是一项劳动ioso和主观的过程。在这篇论文中，我们介绍了Auto-Instruct，一种新的方法，可以自动提高提供给LLM的指令质量。我们的方法利用LLM的内在的生成能力，生成任务相关的多个候选指令，然后使用基于多个存在的575 NLP任务的分数模型来排序。在118个 OUT-OF-DOMAIN任务上进行实验，Auto-Instruct超过了人类编写的指令和现有的LLM生成指令基eline。此外，我们的方法在其他LLM中也 exhibits notable generalizability。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Candidate-Answer-Extraction-through-Differentiable-Masker-Reconstructor-Model"><a href="#Unsupervised-Candidate-Answer-Extraction-through-Differentiable-Masker-Reconstructor-Model" class="headerlink" title="Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model"></a>Unsupervised Candidate Answer Extraction through Differentiable Masker-Reconstructor Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13106">http://arxiv.org/abs/2310.13106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuoer Wang, Yicheng Wang, Ziwei Zhu, James Caverlee</li>
<li>for: 提高问题生成系统中候选答案提取的精度和效果</li>
<li>methods: 提出了一种新的无监督候选答案提取方法，利用文本含义结构自动提取答案</li>
<li>results: 对两个批处理的数据进行了广泛的测试和评估，并显示了与监督方法相当的性能，同时具有自动提取答案的优势。<details>
<summary>Abstract</summary>
Question generation is a widely used data augmentation approach with extensive applications, and extracting qualified candidate answers from context passages is a critical step for most question generation systems. However, existing methods for candidate answer extraction are reliant on linguistic rules or annotated data that face the partial annotation issue and challenges in generalization. To overcome these limitations, we propose a novel unsupervised candidate answer extraction approach that leverages the inherent structure of context passages through a Differentiable Masker-Reconstructor (DMR) Model with the enforcement of self-consistency for picking up salient information tokens. We curated two datasets with exhaustively-annotated answers and benchmark a comprehensive set of supervised and unsupervised candidate answer extraction methods. We demonstrate the effectiveness of the DMR model by showing its performance is superior among unsupervised methods and comparable to supervised methods.
</details>
<details>
<summary>摘要</summary>
问题生成是广泛使用的数据增强方法，提取合适的答案候选者从文本段落是大多数问题生成系统中的关键步骤。然而，现有的答案候选EXTRACTION方法依赖于语言规则或标注数据，面临到偏vie annotation问题和总体化难题。为了解决这些限制，我们提出了一种新的无监督候选答案EXTRACTION方法，利用文本段落的自然结构，通过可 diffeomorphisms Masker-Reconstructor（DMR）模型，并通过自我一致性来捕捉突出的信息 токен。我们抽取了两个 dataset with exhaustive annotation answers，并 benchmark了一组完全监督和无监督候选答案EXTRACTION方法。我们示出了 DMR 模型的效果，其性能在无监督方法中至上，与监督方法相当。
</details></li>
</ul>
<hr>
<h2 id="Do-Language-Models-Learn-about-Legal-Entity-Types-during-Pretraining"><a href="#Do-Language-Models-Learn-about-Legal-Entity-Types-during-Pretraining" class="headerlink" title="Do Language Models Learn about Legal Entity Types during Pretraining?"></a>Do Language Models Learn about Legal Entity Types during Pretraining?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13092">http://arxiv.org/abs/2310.13092</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/clairebarale/probing_legal_entity_types">https://github.com/clairebarale/probing_legal_entity_types</a></li>
<li>paper_authors: Claire Barale, Michael Rovatsos, Nehal Bhuta</li>
<li>for: 本研究旨在探讨语言模型（LM）在预训练阶段积累到各种语言知识的能力，以及这些知识是否可以用于下游任务。</li>
<li>methods: 本研究使用 Entity Typing 作为评估法律知识的代理任务，并使用两种提示方法（cloze sentence和QA-based template）进行系统性的评估和分析。</li>
<li>results: 研究结果显示（1）Llama2 在某些实体类型上表现良好，并且可能通过优化提示模板具有大量提升的潜力；（2）法律预训练集的 LM 表现不一致，可能因为预训练集的变化；（3）LM 能够类型实体，包括多token实体；（4）所有模型都在某些法律子领域中的实体类型上表现不佳；（5）Llama2 显示在 sintactic 信号上过度忽略，而BERT-based 架构则比较具有这种缺点。<details>
<summary>Abstract</summary>
Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）在预训练阶段已经证明了它们可以掌握多种语言知识，有可能作为下游任务的意外监督来提供价值。然而，有限的研究已经进行到了域pecific知识的检索，特别是法律知识。我们提议探索Entity Typing任务，作为法律文本理解的重要方面，以及许多下游法律NLP应用的基础任务。通过系统atic评估和分析，以及两种提示（cloze句和QA模板），我们比较了不同类型和长度的实体、具体或 синтакси依据信号，以及不同的LM预训练集（通用和法律 oriented）和架构（encoderBERT基于和Decoder Only with Llama2）。我们发现：1. Llama2在某些实体方面表现良好，并且具有可以通过优化提示模板进行提升的潜力。2.法律 Orientated LMs在性能上存在差异，可能是由其训练集的变化引起的。3.LMs可以对多token实体进行类型化， inclusive 多个域的法律实体。4.所有模型都在特定的法律子领域中的实体表现不佳。5. Llama2显示在 sintactic 信号上有很多缺失，这是BERT基于架构中存在的缺陷。
</details></li>
</ul>
<hr>
<h2 id="GARI-Graph-Attention-for-Relative-Isomorphism-of-Arabic-Word-Embeddings"><a href="#GARI-Graph-Attention-for-Relative-Isomorphism-of-Arabic-Word-Embeddings" class="headerlink" title="GARI: Graph Attention for Relative Isomorphism of Arabic Word Embeddings"></a>GARI: Graph Attention for Relative Isomorphism of Arabic Word Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13068">http://arxiv.org/abs/2310.13068</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asif6827/gari">https://github.com/asif6827/gari</a></li>
<li>paper_authors: Muhammad Asif Ali, Maha Alshmrani, Jianbin Qin, Yan Hu, Di Wang</li>
<li>for: 本研究目的是提高语义相似性 embedding 空间之间的相对含义性。</li>
<li>methods: 该方法结合分布式训练目标和多个含义损失，通过图注意力网络引导定义相对含义空间的 embedding。</li>
<li>results: 实验结果表明，对阿拉伯语数据集进行训练，GARI 可以提高平均精度@1 的表现，相比前期研究提高40.95%和76.80% 在适应域和领域偏移设置下。<details>
<summary>Abstract</summary>
Bilingual Lexical Induction (BLI) is a core challenge in NLP, it relies on the relative isomorphism of individual embedding spaces. Existing attempts aimed at controlling the relative isomorphism of different embedding spaces fail to incorporate the impact of semantically related words in the model training objective. To address this, we propose GARI that combines the distributional training objectives with multiple isomorphism losses guided by the graph attention network. GARI considers the impact of semantical variations of words in order to define the relative isomorphism of the embedding spaces. Experimental evaluation using the Arabic language data set shows that GARI outperforms the existing research by improving the average P@1 by a relative score of up to 40.95% and 76.80% for in-domain and domain mismatch settings respectively. We release the codes for GARI at https://github.com/asif6827/GARI.
</details>
<details>
<summary>摘要</summary>
《双语 lexical 推导 (BLI) 是 NLP 领域的核心挑战，它基于各个嵌入空间之间的相对同构性。现有的尝试都无法将各个嵌入空间之间的相对同构性控制在模型训练目标中。为此，我们提出了 GARI，它将分布式训练目标与多种同构损失相结合，并由图注意力网络引导。GARI 考虑了 semantic 变化的影响，以定义各个嵌入空间之间的相对同构性。经验证使用阿拉伯语数据集表明，GARI 可以超越现有研究，提高平均 P@1 的表现，在领域匹配设置下提高了40.95%，在领域异同设置下提高了76.80%。我们在 GitHub 上发布了 GARI 代码，请参考 <https://github.com/asif6827/GARI>。
</details></li>
</ul>
<hr>
<h2 id="SEGO-Sequential-Subgoal-Optimization-for-Mathematical-Problem-Solving"><a href="#SEGO-Sequential-Subgoal-Optimization-for-Mathematical-Problem-Solving" class="headerlink" title="SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving"></a>SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12960">http://arxiv.org/abs/2310.12960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueliang Zhao, Xinting Huang, Wei Bi, Lingpeng Kong</li>
<li>for: 提高人工智能中的数学问题解决能力</li>
<li>methods: 使用新的框架 called SEGO，通过连接下目步骤和问题解决概率来确定更好的下目步骤，并根据特定的标准进行优化。</li>
<li>results: 通过实验证明，SEGO可以在两个标准测试集上（GSM8K和MATH）提高问题解决性能，表明SEGO在人工智能驱动的数学问题解决中具有潜力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have driven substantial progress in artificial intelligence in recent years, exhibiting impressive capabilities across a wide range of tasks, including mathematical problem-solving. Inspired by the success of subgoal-based methods, we propose a novel framework called \textbf{SE}quential sub\textbf{G}oal \textbf{O}ptimization (SEGO) to enhance LLMs' ability to solve mathematical problems. By establishing a connection between the subgoal breakdown process and the probability of solving problems, SEGO aims to identify better subgoals with theoretical guarantees. Addressing the challenge of identifying suitable subgoals in a large solution space, our framework generates problem-specific subgoals and adjusts them according to carefully designed criteria. Incorporating these optimized subgoals into the policy model training leads to significant improvements in problem-solving performance. We validate SEGO's efficacy through experiments on two benchmarks, GSM8K and MATH, where our approach outperforms existing methods, highlighting the potential of SEGO in AI-driven mathematical problem-solving.   Data and code associated with this paper will be available at https://github.com/zhaoxlpku/SEGO
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Representational-Capacity-of-Recurrent-Neural-Language-Models"><a href="#On-the-Representational-Capacity-of-Recurrent-Neural-Language-Models" class="headerlink" title="On the Representational Capacity of Recurrent Neural Language Models"></a>On the Representational Capacity of Recurrent Neural Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12942">http://arxiv.org/abs/2310.12942</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rycolab/rnn-turing-completeness">https://github.com/rycolab/rnn-turing-completeness</a></li>
<li>paper_authors: Franz Nowak, Anej Svete, Li Du, Ryan Cotterell</li>
<li>for: 本研究 investigate language models (LMs) based on recurrent neural networks (RNNs) 的计算表达能力。</li>
<li>methods: 该研究使用 rational weights 和 hidden states，并且使用 unbounded computation time 来展示 RNNs 的 Turing completeness。</li>
<li>results: 研究表明，使用 probabilistic Turing machine (PTM) 和 real-time computation 的情况下，RLMs 可以模拟任何 probabilistic Turing machine (PTM)，但是在实际应用中，RLMs 的计算时间是有限的，因此这个结果是 RLMs 的Upper bound。此外，研究还提供了一个 lower bound，表明在实际应用中，RLMs 只能模拟 deterministic real-time rational PTMs。<details>
<summary>Abstract</summary>
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Predictive-Factor-Analysis-of-Social-Biases-and-Task-Performance-in-Pretrained-Masked-Language-Models"><a href="#A-Predictive-Factor-Analysis-of-Social-Biases-and-Task-Performance-in-Pretrained-Masked-Language-Models" class="headerlink" title="A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models"></a>A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12936">http://arxiv.org/abs/2310.12936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Zhou, Jose Camacho-Collados, Danushka Bollegala</li>
<li>For: This paper aims to study the relationship between various factors of pre-trained Masked Language Models (MLMs) and the social biases they learn, as well as their downstream task performance.* Methods: The authors conduct a comprehensive study using 39 pre-trained MLMs with different model sizes, training objectives, tokenization methods, training data domains, and languages.* Results: The study sheds light on important factors often neglected in prior literature, such as tokenization or model objectives, and provides insights into the relationship between these factors and the social biases learned by MLMs.Here is the same information in Simplified Chinese text:</li>
<li>for: 这paper的目的是研究pre-trained Masked Language Models（MLMs）中不同因素对它们学习的社会偏见以及其下游任务性能。</li>
<li>methods: 作者通过使用39个不同的pre-trained MLMs，包括不同的模型大小、训练目标、tokenization方法、训练数据领域和语言，进行了全面的研究。</li>
<li>results: 研究发现了许多在先前文献中被忽略的因素，如tokenization或模型目标，与MLMs学习的社会偏见以及其下游任务性能之间的关系。<details>
<summary>Abstract</summary>
Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.
</details>
<details>
<summary>摘要</summary>
各种社会偏见已在先前的工作中对预训练的掩码语言模型（MLM）进行报道。然而，预训练MLM的多个下面因素有关：其模型大小、训练数据大小、训练目标、训练数据领域、tokenization方法和预训练 corpora 中的语言等。尚未清楚哪些因素影响预训练MLM学习的社会偏见，以及模型的下游任务性能。为了研究预训练MLM的模型因素和学习的社会偏见之间的关系，以及模型的下游任务性能，我们进行了39个预训练MLM的全面研究，覆盖不同的模型大小、训练目标、tokenization方法、训练数据领域和语言。我们的结果揭示了在先前文献中经常被忽略的一些因素，如tokenization或模型目标。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Study-of-Performance-Disparities-in-Multilingual-Task-Oriented-Dialogue-Systems"><a href="#A-Systematic-Study-of-Performance-Disparities-in-Multilingual-Task-Oriented-Dialogue-Systems" class="headerlink" title="A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems"></a>A Systematic Study of Performance Disparities in Multilingual Task-Oriented Dialogue Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12892">http://arxiv.org/abs/2310.12892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Songbo Hu, Han Zhou, Moy Yuan, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Anna Korhonen, Ivan Vulić</li>
<li>for: 本研究的目的是探讨和分析多语言自然语言处理（NLP）中语言差异导致的任务性能差异。</li>
<li>methods: 我们首先定义了新的量化度量方式，用于衡量多语言对话（ToD）系统的性能差异。我们采用了一系列控制的实验，以证明性能差异取决于任务类型、基础预训语言模型、目标语言以及ToD数据的量。</li>
<li>results: 我们的分析表明，现有的ToD系统存在适应和内在偏见。例如，使用英语ToD数据进行同步训练的阿拉伯语或土耳其语ToD系统仍然表现较差。我们的分析还提供了对ToD数据收集和系统开发的实践建议。<details>
<summary>Abstract</summary>
Achieving robust language technologies that can perform well across the world's many languages is a central goal of multilingual NLP. In this work, we take stock of and empirically analyse task performance disparities that exist between multilingual task-oriented dialogue (ToD) systems. We first define new quantitative measures of absolute and relative equivalence in system performance, capturing disparities across languages and within individual languages. Through a series of controlled experiments, we demonstrate that performance disparities depend on a number of factors: the nature of the ToD task at hand, the underlying pretrained language model, the target language, and the amount of ToD annotated data. We empirically prove the existence of the adaptation and intrinsic biases in current ToD systems: e.g., ToD systems trained for Arabic or Turkish using annotated ToD data fully parallel to English ToD data still exhibit diminished ToD task performance. Beyond providing a series of insights into the performance disparities of ToD systems in different languages, our analyses offer practical tips on how to approach ToD data collection and system development for new languages.
</details>
<details>
<summary>摘要</summary>
（注：以下是使用简化字符的中文翻译）实现在世界上许多语言上运行的Robust语言技术是多语言NLP的中心目标。在这项工作中，我们对多语言对话任务（ToD）系统的性能差异进行了评估和分析。我们首先定义了新的绝对和相对一致性度量，用于捕捉不同语言和语言之间的性能差异。通过一系列控制的实验，我们证明了性能差异取决于多个因素：ToD任务的性质、基础预训言语模型、目标语言和ToD数据的量。我们经验证了现有ToD系统中的适应和内在偏见：例如，使用完全相同的ToD数据来训练阿拉伯语或土耳其语的ToD系统仍然会导致ToD任务性能下降。我们的分析不仅提供了不同语言ToD系统的性能差异的几种视角，还提供了如何在新语言上收集ToD数据和开发ToD系统的实践建议。
</details></li>
</ul>
<hr>
<h2 id="StoryAnalogy-Deriving-Story-level-Analogies-from-Large-Language-Models-to-Unlock-Analogical-Understanding"><a href="#StoryAnalogy-Deriving-Story-level-Analogies-from-Large-Language-Models-to-Unlock-Analogical-Understanding" class="headerlink" title="StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding"></a>StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12874">http://arxiv.org/abs/2310.12874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Jiayang, Lin Qiu, Tsz Ho Chan, Tianqing Fang, Weiqi Wang, Chunkit Chan, Dongyu Ru, Qipeng Guo, Hongming Zhang, Yangqiu Song, Yue Zhang, Zheng Zhang</li>
<li>for: 本研究旨在评估人类理解和生成analogy的能力，通过构建了首个大规模的故事级相似性 corpora（\textsc{StoryAnalogy），包括24K个故事对从多个领域中，人类标注了两个相似性从扩展的结构对应理论。</li>
<li>methods: 我们设计了一系列测试，用于评估故事级analogy的识别和生成能力，这是首次对故事级analogy进行评估。</li>
<li>results: 我们发现，即使使用最新的大语言模型（LLMs），例如ChatGPT和LLaMa，也只能达到30%的准确率（相比人类的85%准确率）。此外，我们发现\textsc{StoryAnalogy}数据可以改善LLMs中的analogy生成质量，其中一个精制的FlanT5-xxl模型与零容量ChatGPT模型具有相似的性能。<details>
<summary>Abstract</summary>
Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, \textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on \textsc{StoryAnalogy}, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans). Furthermore, we observe that the data in \textsc{StoryAnalogy} can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.
</details>
<details>
<summary>摘要</summary>
人类理智中的Analogy-making是非常重要的。在这篇论文中，我们评估了人们可以认出和生成Analogy的能力，通过构建了首个类型的大规模故事级Analogy corpus，namely \textsc{StoryAnalogy}，其包含24K个故事对from多个领域，并有人类标注了两种相似性from the extended Structure-Mapping Theory。我们设计了一系列测试，这是首次评估故事级Analogy的认出和生成能力。有意思的是，我们发现，不仅 sentence embedding models，而且最近的大语言模型（LLMs），如ChatGPT和LLaMa，在同义测试中只取得了约30%的准确率（与人类的超过85%准确率相比）。此外，我们发现， \textsc{StoryAnalogy} 中的数据可以提高 LLMs 中的同义生成质量，其中一个精心 fine-tuned FlanT5-xxl 模型在多项选择问题中与零shot ChatGPT 具有相似的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Locality-and-Symmetry-of-Positional-Encodings"><a href="#The-Locality-and-Symmetry-of-Positional-Encodings" class="headerlink" title="The Locality and Symmetry of Positional Encodings"></a>The Locality and Symmetry of Positional Encodings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12864">http://arxiv.org/abs/2310.12864</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tigerchen52/locality_symmetry">https://github.com/tigerchen52/locality_symmetry</a></li>
<li>paper_authors: Lihu Chen, Gaël Varoquaux, Fabian M. Suchanek</li>
<li>for: 这篇论文主要研究了Positional Encodings（PEs）在基于transformer的语言模型中的应用，以增强句子表示质量。</li>
<li>methods: 该论文采用了系统的研究方法，包括对PEs的分析、两个新的探测任务的引入以评估现有PEs的弱点，以及对下游任务表现的分析。</li>
<li>results: 研究发现，PEs在基于BERT的语言模型中具有两种常见的特性：Locality和Symmetry。这两种特性与下游任务表现高度相关，而现有PEs在两个新的探测任务中表现较差。这些结果可能为开发更好的PEs提供基础。代码可以在\faGithub~ \url{<a target="_blank" rel="noopener" href="https://github.com/tigerchen52/locality/_symmetry%7D">https://github.com/tigerchen52/locality\_symmetry}</a> 获取。<details>
<summary>Abstract</summary>
Positional Encodings (PEs) are used to inject word-order information into transformer-based language models. While they can significantly enhance the quality of sentence representations, their specific contribution to language models is not fully understood, especially given recent findings that various positional encodings are insensitive to word order. In this work, we conduct a systematic study of positional encodings in \textbf{Bidirectional Masked Language Models} (BERT-style) , which complements existing work in three aspects: (1) We uncover the core function of PEs by identifying two common properties, Locality and Symmetry; (2) We show that the two properties are closely correlated with the performances of downstream tasks; (3) We quantify the weakness of current PEs by introducing two new probing tasks, on which current PEs perform poorly. We believe that these results are the basis for developing better PEs for transformer-based language models. The code is available at \faGithub~ \url{https://github.com/tigerchen52/locality\_symmetry}
</details>
<details>
<summary>摘要</summary>
位置编码（PEs）用于注入单词顺序信息到转换器基于的语言模型中。尽管它们可以显著提高句子表示的质量，但它们特定的贡献到语言模型中还不完全了解，特别是在最近的发现中，各种位置编码都是不敏感于单词顺序的。在这项工作中，我们进行了系统性的研究，包括以下三个方面：1. 我们揭示了位置编码的核心功能，并确定了两种常见的属性：地方性和对称性。2. 我们发现这两种属性与下游任务的表现密切相关。3. 我们 introduce了两个新的探测任务，以证明现有的位置编码在这些任务上表现不佳。我们认为这些结果是开发更好的位置编码的基础。代码可以在 \faGithub 上获取，链接为 \url{https://github.com/tigerchen52/locality_symmetry}。
</details></li>
</ul>
<hr>
<h2 id="Probing-LLMs-for-hate-speech-detection-strengths-and-vulnerabilities"><a href="#Probing-LLMs-for-hate-speech-detection-strengths-and-vulnerabilities" class="headerlink" title="Probing LLMs for hate speech detection: strengths and vulnerabilities"></a>Probing LLMs for hate speech detection: strengths and vulnerabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12860">http://arxiv.org/abs/2310.12860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee, Punyajoy Saha</li>
<li>for: 本研究旨在探讨社交媒体平台以及研究人员如何使用大语言模型检测偏恶或攻击性语言，并尝试使用说明、附加信息和受害者社区信息来提高检测精度。</li>
<li>methods: 本研究使用了不同的提示变化、输入信息和评估大语言模型的零批设定（没有添加任何上下文示例）。选择了三个大语言模型（GPT-3.5、text-davinci和Flan-T5）和三个数据集（HateXplain、隐式仇恨和ToxicSpans）。</li>
<li>results: 结果表明，在平均情况下，包含目标信息在检测过程中可以提高模型性能（约20-30%），而添加说明也可以提高模型性能（约10-20%）。此外，我们还提供了错误案例分类和模型决策错误的解释，这些敏感点自动组成了‘监狱’提示，需要开发行业规模的安全措施，以使模型更加可靠。<details>
<summary>Abstract</summary>
Recently efforts have been made by social media platforms as well as researchers to detect hateful or toxic language using large language models. However, none of these works aim to use explanation, additional context and victim community information in the detection process. We utilise different prompt variation, input information and evaluate large language models in zero shot setting (without adding any in-context examples). We select three large language models (GPT-3.5, text-davinci and Flan-T5) and three datasets - HateXplain, implicit hate and ToxicSpans. We find that on average including the target information in the pipeline improves the model performance substantially (~20-30%) over the baseline across the datasets. There is also a considerable effect of adding the rationales/explanations into the pipeline (~10-20%) over the baseline across the datasets. In addition, we further provide a typology of the error cases where these large language models fail to (i) classify and (ii) explain the reason for the decisions they take. Such vulnerable points automatically constitute 'jailbreak' prompts for these models and industry scale safeguard techniques need to be developed to make the models robust against such prompts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="EmoDiarize-Speaker-Diarization-and-Emotion-Identification-from-Speech-Signals-using-Convolutional-Neural-Networks"><a href="#EmoDiarize-Speaker-Diarization-and-Emotion-Identification-from-Speech-Signals-using-Convolutional-Neural-Networks" class="headerlink" title="EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks"></a>EmoDiarize: Speaker Diarization and Emotion Identification from Speech Signals using Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12851">http://arxiv.org/abs/2310.12851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanan Hamza, Fiza Gafoor, Fathima Sithara, Gayathri Anil, V. S. Anoop</li>
<li>for: This paper aims to improve the accuracy of speech emotion recognition by integrating deep learning techniques and addressing the challenges of speaker diarization and emotion identification.</li>
<li>methods: The proposed method combines a pre-existing speaker diarization pipeline with a Convolutional Neural Network (CNN) based emotion identification model, using features such as MFCC, ZCR, RMS, and data augmentation techniques.</li>
<li>results: The proposed model achieved an unweighted accuracy of 63% in identifying emotional states within speech signals, demonstrating its effectiveness in accurately recognizing emotions in spoken language.<details>
<summary>Abstract</summary>
In the era of advanced artificial intelligence and human-computer interaction, identifying emotions in spoken language is paramount. This research explores the integration of deep learning techniques in speech emotion recognition, offering a comprehensive solution to the challenges associated with speaker diarization and emotion identification. It introduces a framework that combines a pre-existing speaker diarization pipeline and an emotion identification model built on a Convolutional Neural Network (CNN) to achieve higher precision. The proposed model was trained on data from five speech emotion datasets, namely, RAVDESS, CREMA-D, SAVEE, TESS, and Movie Clips, out of which the latter is a speech emotion dataset created specifically for this research. The features extracted from each sample include Mel Frequency Cepstral Coefficients (MFCC), Zero Crossing Rate (ZCR), Root Mean Square (RMS), and various data augmentation algorithms like pitch, noise, stretch, and shift. This feature extraction approach aims to enhance prediction accuracy while reducing computational complexity. The proposed model yields an unweighted accuracy of 63%, demonstrating remarkable efficiency in accurately identifying emotional states within speech signals.
</details>
<details>
<summary>摘要</summary>
在人工智能和计算机之间的交互时代，识别语音中的情感是非常重要的。这项研究探讨了深度学习技术在语音情感识别中的应用，提供了全面的解决方案，以便更好地识别speaker的情感。它提出了一个结合现有的说话人识别管道和基于卷积神经网络（CNN）的情感识别模型，以提高准确性。该模型在五个语音情感数据集上进行训练，分别是RAVDESS、CREMA-D、SAVEE、TESS和Movie Clips，其中Movie Clips是特意为本研究创建的语音情感数据集。每个样本中提取的特征包括Mel Frequency Cepstral Coefficients（MFCC）、Zero Crossing Rate（ZCR）、Root Mean Square（RMS）以及各种数据增强算法如滥声、噪声、延展、偏移等。这种特征提取方法的目的是提高预测准确性，同时减少计算复杂度。该模型在无权重的情况下达到63%的准确率，表明在语音信号中准确地识别情感状态的能力强大。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Augmented-Language-Model-Verification"><a href="#Knowledge-Augmented-Language-Model-Verification" class="headerlink" title="Knowledge-Augmented Language Model Verification"></a>Knowledge-Augmented Language Model Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12836">http://arxiv.org/abs/2310.12836</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JinheonBaek/KALMV">https://github.com/JinheonBaek/KALMV</a></li>
<li>paper_authors: Jinheon Baek, Soyeong Jeong, Minki Kang, Jong C. Park, Sung Ju Hwang</li>
<li>for: 提高语言模型（LM）内置知识的概率生成文本的准确性</li>
<li>methods: 使用外部知识源扩展LM的知识，并使用一个小型LM进行验证和修正</li>
<li>results: 在多个问答benchmark上验证了验证步骤的效iveness，验证器可以准确地识别抽象和生成错误，使LM提供更加准确的输出<details>
<summary>Abstract</summary>
Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.
</details>
<details>
<summary>摘要</summary>
现代语言模型（LM）在生成文本时有卓越的能力，但它们经常生成错误的回答。这是因为LM的知识可能是不准确、不完整或过时的。以前的研究建议将知识从外部知识源添加到LM中，但这些方法通常会导致文本生成性能下降。这是因为模型可能无法检索与给定查询相关的知识，或者模型无法忠实地反映检索到的知识在生成文本中。为解决这些问题，我们提议采用一个分离的验证器，这是一个小型LM，通过 instruciton-finetuning 来检测模型输出中的两种类型错误。当验证器发现错误时，我们可以通过 Either retrieving new knowledge or generating new text来纠正错误。此外，我们使用不同 instruciton 的输出 ensemble 以提高验证过程的可靠性。我们在多个问答 benchmark 上验证了我们的方法，结果显示，我们的验证器可以准确地检测检索和生成错误，使LM提供更加准确的输出。我们的代码可以在 GitHub 上找到：https://github.com/JinheonBaek/KALMV。
</details></li>
</ul>
<hr>
<h2 id="GestureGPT-Zero-shot-Interactive-Gesture-Understanding-and-Grounding-with-Large-Language-Model-Agents"><a href="#GestureGPT-Zero-shot-Interactive-Gesture-Understanding-and-Grounding-with-Large-Language-Model-Agents" class="headerlink" title="GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents"></a>GestureGPT: Zero-shot Interactive Gesture Understanding and Grounding with Large Language Model Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12821">http://arxiv.org/abs/2310.12821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Zeng, Xiaoyu Wang, Tengxiang Zhang, Chun Yu, Shengdong Zhao, Yiqiang Chen</li>
<li>for: 提高现有的手势识别系统，使之能够连接手势与交互 GUI 元素或系统功能。</li>
<li>methods: 使用大语言模型（LLMs），将手势描述转化为对话系统中的问题，并通过对话进行识别和适应。</li>
<li>results: 在两个实际场景中进行了测试：视频流和智能家居 IoT 控制，并 achieved 80.11% 和 90.78% 的零shot Top-5 拟合率。<details>
<summary>Abstract</summary>
Current gesture recognition systems primarily focus on identifying gestures within a predefined set, leaving a gap in connecting these gestures to interactive GUI elements or system functions (e.g., linking a 'thumb-up' gesture to a 'like' button). We introduce GestureGPT, a novel zero-shot gesture understanding and grounding framework leveraging large language models (LLMs). Gesture descriptions are formulated based on hand landmark coordinates from gesture videos and fed into our dual-agent dialogue system. A gesture agent deciphers these descriptions and queries about the interaction context (e.g., interface, history, gaze data), which a context agent organizes and provides. Following iterative exchanges, the gesture agent discerns user intent, grounding it to an interactive function. We validated the gesture description module using public first-view and third-view gesture datasets and tested the whole system in two real-world settings: video streaming and smart home IoT control. The highest zero-shot Top-5 grounding accuracies are 80.11% for video streaming and 90.78% for smart home tasks, showing potential of the new gesture understanding paradigm.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Causal-structure-Driven-Augmentations-for-Text-OOD-Generalization"><a href="#Causal-structure-Driven-Augmentations-for-Text-OOD-Generalization" class="headerlink" title="Causal-structure Driven Augmentations for Text OOD Generalization"></a>Causal-structure Driven Augmentations for Text OOD Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12803">http://arxiv.org/abs/2310.12803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Feder, Yoav Wald, Claudia Shi, Suchi Saria, David Blei</li>
<li>for: 这篇论文是用于提高文本分类器在实际应用中的一致性和稳定性，特别是在医疗领域，避免因为偶发因素而导致的泛化问题。</li>
<li>methods: 这篇论文提出了一种使用假设构造的数据来实现干预，并使用ounterfactual数据增强来学习更加稳定的文本分类器。</li>
<li>results: 这篇论文通过实验显示，使用这种方法可以提高文本分类器在对应用中的一致性和稳定性，并且比基eline的均衡学习算法更有效率。<details>
<summary>Abstract</summary>
The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic data, we demonstrate that our method for simulating interventions improves out-of-distribution (OOD) accuracy compared to baseline invariant learning algorithms.
</details>
<details>
<summary>摘要</summary>
文本分类器的依赖关系可能会导致在部署时出现差异，从而引起关注其在安全关键领域如医疗中的使用。在这项工作中，我们提议使用对干扰因素的干扰数据增强，受知 causal 结构的数据指导，以便模拟干扰并学习更加鲁棒的文本分类器。我们表明，这种策略在预测问题中，标签与特征之间存在干扰关系时是合适的。在这些问题的假设下，我们讨论了对干扰数据增强的最佳样本复杂度，与重要性重新权重相比。在实践中，我们使用辅助数据进行匹配，基于 diff-in-diff 方法，并使用大型自然语言模型（LLM）来表示文本中的条件概率。通过对医学 narative 和 semi-synthetic 数据进行广泛的实验，我们证明了我们的干扰模拟策略可以提高对于样本外（OOD）准确率，比基础不变学习算法更好。
</details></li>
</ul>
<hr>
<h2 id="MolCA-Molecular-Graph-Language-Modeling-with-Cross-Modal-Projector-and-Uni-Modal-Adapter"><a href="#MolCA-Molecular-Graph-Language-Modeling-with-Cross-Modal-Projector-and-Uni-Modal-Adapter" class="headerlink" title="MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter"></a>MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12798">http://arxiv.org/abs/2310.12798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/acharkq/molca">https://github.com/acharkq/molca</a></li>
<li>paper_authors: Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua</li>
<li>for: 本研究旨在帮助语言模型更好地理解分子的二维图形结构，以提高其对分子的理解能力。</li>
<li>methods: 本研究提出了一种名为MolCA的方法，它使用交叉模式项目器和单模型适配器来将语言模型与分子图形空间连接。</li>
<li>results: 对于分子描述、IUPAC名称预测和分子文本检索等任务，MolCA在比较基eline之上显示出了显著的提高。<details>
<summary>Abstract</summary>
Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectiveness, we extensively benchmark MolCA on tasks of molecule captioning, IUPAC name prediction, and molecule-text retrieval, on which MolCA significantly outperforms the baselines. Our codes and checkpoints can be found at https://github.com/acharkq/MolCA.
</details>
<details>
<summary>摘要</summary>
Language Models (LMs) 有表现出很强的分子理解能力在各种一维文本相关任务上。然而，它们缺乏二维图像感知能力，这是人类专业人员理解分子的核心能力。为了bridging这个差距，我们提议了MolCA：分子图像语言模型化with Cross-Modal Projector和Uni-Modal Adapter。MolCA使得LM（例如Galactica）能够理解文本和图像基本分子内容。具体来说，cross-modal projector是通过Q-Former连接图像编码器的表示空间和LM的文本空间来实现的。此外，MolCA还使用uni-modal adapter（即LoRA）来有效地适应下游任务。与前一些研究 coupling LM与图像编码器via cross-modal对抗学习不同，MolCA保留了LM的开放式文本生成能力，并将其与二维图像信息相结合。为证明其效果，我们对MolCA进行了广泛的 benchmarking，并发现它在分子描述、IUPAC名称预测和分子文本检索等任务上表现出色，与基线比较明显提高。codes和checkpoints可以在https://github.com/acharkq/MolCA上找到。
</details></li>
</ul>
<hr>
<h2 id="Are-Structural-Concepts-Universal-in-Transformer-Language-Models-Towards-Interpretable-Cross-Lingual-Generalization"><a href="#Are-Structural-Concepts-Universal-in-Transformer-Language-Models-Towards-Interpretable-Cross-Lingual-Generalization" class="headerlink" title="Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization"></a>Are Structural Concepts Universal in Transformer Language Models? Towards Interpretable Cross-Lingual Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12794">http://arxiv.org/abs/2310.12794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ningyuxu/structural_concepts_correspondence">https://github.com/ningyuxu/structural_concepts_correspondence</a></li>
<li>paper_authors: Ningyu Xu, Qi Zhang, Jingting Ye, Menghan Zhang, Xuanjing Huang</li>
<li>for: 这个论文旨在研究如何通过显式匹配语言之间概念匹配来提高跨语言泛化。</li>
<li>methods: 这个论文使用了语义学方法来研究语言之间概念匹配的可行性，并提出了一种基于元学习的方法来学习匹配不同语言的概念空间。</li>
<li>results: 实验结果表明，该方法可以达到与当前状态OFART的竞争力，并且特别地有助于低资源语言来增强其泛化能力。<details>
<summary>Abstract</summary>
Large language models (LLMs) have exhibited considerable cross-lingual generalization abilities, whereby they implicitly transfer knowledge across languages. However, the transfer is not equally successful for all languages, especially for low-resource ones, which poses an ongoing challenge. It is unclear whether we have reached the limits of implicit cross-lingual generalization and if explicit knowledge transfer is viable. In this paper, we investigate the potential for explicitly aligning conceptual correspondence between languages to enhance cross-lingual generalization. Using the syntactic aspect of language as a testbed, our analyses of 43 languages reveal a high degree of alignability among the spaces of structural concepts within each language for both encoder-only and decoder-only LLMs. We then propose a meta-learning-based method to learn to align conceptual spaces of different languages, which facilitates zero-shot and few-shot generalization in concept classification and also offers insights into the cross-lingual in-context learning phenomenon. Experiments on syntactic analysis tasks show that our approach achieves competitive results with state-of-the-art methods and narrows the performance gap between languages, particularly benefiting those with limited resources.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型（LLM）已经展现出较强的跨语言泛化能力，其中模型通过隐式方式传递知识到不同语言。然而，这种传递不是对所有语言都 equally successful，特别是低资源语言，这成为一个持续的挑战。是否已经达到了隐式跨语言泛化的限制，并且是否可以进行显式知识传递，这些问题仍然存在。在这篇论文中，我们调查了在不同语言之间显式对概念匹配的潜在可能性，以提高跨语言泛化。使用语言的 sintactic 方面作为测试台，我们对43种语言进行了分析，发现这些语言之间的概念空间之间存在高度的可对应性，包括encoder-only和decoder-only LLMP。我们then propose了一种基于meta-学习的方法，可以学习不同语言之间的概念空间的对应关系，从而实现零例学习和几例学习在概念分类中的优秀表现。实验表明，我们的方法可以与现有的方法竞争，同时将语言资源差距缩小。
</details></li>
</ul>
<hr>
<h2 id="Label-Aware-Automatic-Verbalizer-for-Few-Shot-Text-Classification"><a href="#Label-Aware-Automatic-Verbalizer-for-Few-Shot-Text-Classification" class="headerlink" title="Label-Aware Automatic Verbalizer for Few-Shot Text Classification"></a>Label-Aware Automatic Verbalizer for Few-Shot Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12778">http://arxiv.org/abs/2310.12778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanakorn Thaminkaew, Piyawat Lertvittayakumjorn, Peerapon Vateekul</li>
<li>for: 提高 few-shot text classification 的效果</li>
<li>methods: 使用 Label-Aware Automatic Verbalizer (LAAV)，即通过将手动标签与 “and” 连接来增强模型生成更有效的词语</li>
<li>results: 对五种语言的五个 dataset 进行实验，显示 LAAV 明显超越现有的逻辑抽象器，并且对mid-to-low resource语言的推荐更加有利。<details>
<summary>Abstract</summary>
Prompt-based learning has shown its effectiveness in few-shot text classification. One important factor in its success is a verbalizer, which translates output from a language model into a predicted class. Notably, the simplest and widely acknowledged verbalizer employs manual labels to represent the classes. However, manual selection does not guarantee the optimality of the selected words when conditioned on the chosen language model. Therefore, we propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the manual labels to achieve better few-shot classification results. Specifically, we use the manual labels along with the conjunction "and" to induce the model to generate more effective words for the verbalizer. The experimental results on five datasets across five languages demonstrate that LAAV significantly outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV suggests more relevant words compared to similar approaches, especially in mid-to-low resource languages.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Transformer-based-Entity-Legal-Form-Classification"><a href="#Transformer-based-Entity-Legal-Form-Classification" class="headerlink" title="Transformer-based Entity Legal Form Classification"></a>Transformer-based Entity Legal Form Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12766">http://arxiv.org/abs/2310.12766</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sociovestix/lenu">https://github.com/sociovestix/lenu</a></li>
<li>paper_authors: Alexander Arimond, Mauro Molteni, Dominik Jany, Zornitsa Manolova, Damian Borth, Andreas G. F. Hoepner</li>
<li>for: 这 paper 的目的是使用 Transformer-based 语言模型来分类实体法律形式从原始的法律实体名称中。</li>
<li>methods: 这 paper 使用了多种 BERT 变种，并与多种传统基线进行比较。</li>
<li>results: 这 paper 的评估结果表明，预先训练的 BERT 变种在 F1 分数和 Macro F1 分数中都高于传统文本分类方法，并且在多个选择的法律制度中进行第三方专家评审后，结果得到了证实。<details>
<summary>Abstract</summary>
We propose the application of Transformer-based language models for classifying entity legal forms from raw legal entity names. Specifically, we employ various BERT variants and compare their performance against multiple traditional baselines. Our evaluation encompasses a substantial subset of freely available Legal Entity Identifier (LEI) data, comprising over 1.1 million legal entities from 30 different legal jurisdictions. The ground truth labels for classification per jurisdiction are taken from the Entity Legal Form (ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT variants outperform traditional text classification approaches in terms of F1 score, while also performing comparably well in the Macro F1 Score. Moreover, the validity of our proposal is supported by the outcome of third-party expert reviews conducted in ten selected jurisdictions. This study highlights the significant potential of Transformer-based models in advancing data standardization and data integration. The presented approaches can greatly benefit financial institutions, corporations, governments and other organizations in assessing business relationships, understanding risk exposure, and promoting effective governance.
</details>
<details>
<summary>摘要</summary>
我们提议使用变换器基于模型来分类实体法律形式从原始的法律实体名称。specifically，我们利用了多种BERT变种并与多种传统基线进行比较。我们的评估覆盖了大量公开available Legal Entity Identifier（LEI）数据，包括30个不同的法律管辖区，共计1.1万个法律实体。ground truth标签 для分类每个司法管辖区来自ISO 20275标准的Entity Legal Form（ELF）代码标准。我们的发现表明预训练BERT变种在F1分数和Macro F1分数方面都高于传统文本分类方法，并且在多个选定的司法管辖区进行第三方专家审查后，结果支持了我们的建议。这项研究显示了变换器基于模型在数据标准化和数据 интеграция方面的重要潜力。提出的方法可以帮助金融机构、公司、政府和其他组织在评估商业关系、了解风险曝露和促进有效管理方面提供很大的助力。
</details></li>
</ul>
<hr>
<h2 id="Character-level-Chinese-Backpack-Language-Models"><a href="#Character-level-Chinese-Backpack-Language-Models" class="headerlink" title="Character-level Chinese Backpack Language Models"></a>Character-level Chinese Backpack Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12751">http://arxiv.org/abs/2310.12751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swordelucidator/nanobackpacklm">https://github.com/swordelucidator/nanobackpacklm</a></li>
<li>paper_authors: Hao Sun, John Hewitt</li>
<li>for: 这个论文旨在研究Backpack语言模型在使用Character-tokenized Chinese时的表现和可解释性。</li>
<li>methods: 这个论文使用了Backpack语言模型，并在Character-tokenized Chinese中训练、评估和控制了这种模型。</li>
<li>results: 研究发现，使用Backpack语言模型可以与使用Transformer模型相比，并且可以学习rich的字符级别意义，这些意义可以log-additively compose来形成词义。在SimLex-style lexical semantic evaluations中，Backpack模型的simple averages of character senses可以超过Transformer的输入嵌入。此外，研究还发现了 gender bias 的来源和如何进行 intervene 以减少这种偏见。<details>
<summary>Abstract</summary>
The Backpack is a Transformer alternative shown to improve interpretability in English language modeling by decomposing predictions into a weighted sum of token sense components. However, Backpacks' reliance on token-defined meaning raises questions as to their potential for languages other than English, a language for which subword tokenization provides a reasonable approximation for lexical items. In this work, we train, evaluate, interpret, and control Backpack language models in character-tokenized Chinese, in which words are often composed of many characters. We find that our (134M parameter) Chinese Backpack language model performs comparably to a (104M parameter) Transformer, and learns rich character-level meanings that log-additively compose to form word meanings. In SimLex-style lexical semantic evaluations, simple averages of Backpack character senses outperform input embeddings from a Transformer. We find that complex multi-character meanings are often formed by using the same per-character sense weights consistently across context. Exploring interpretability-through control, we show that we can localize a source of gender bias in our Backpacks to specific character senses and intervene to reduce the bias.
</details>
<details>
<summary>摘要</summary>
《背包》是一种 alternativa 于 Transformer 的语言模型，可以提高英语语言模型的可读性。然而，《背包》的依赖于 tokens 定义的意义会引起语言其他语言是否可以使用它的 вопро题。在这项工作中，我们使用 character-tokenized 的中文来训练、评估、解释和控制《背包》语言模型。我们发现我们的 (134M 参数) 中文《背包》语言模型与 (104M 参数) Transformer 相当，并学习了丰富的字符级别意义，这些意义以ilog-additively 组合以形成词义。在 SimLex 式 lexical semantic 评估中，简单的 Backpack 字符意义的平均值比输入 embedding 从 Transformer 高。我们发现了一些复杂的多字符意义通常是通过使用相同的每个字符意义Weight consistently across context来形成。我们 также发现了可以通过控制来解释性的地方性偏见的来源，并可以采取措施来减少偏见。
</details></li>
</ul>
<hr>
<h2 id="Representing-and-Computing-Uncertainty-in-Phonological-Reconstruction"><a href="#Representing-and-Computing-Uncertainty-in-Phonological-Reconstruction" class="headerlink" title="Representing and Computing Uncertainty in Phonological Reconstruction"></a>Representing and Computing Uncertainty in Phonological Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12727">http://arxiv.org/abs/2310.12727</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lingpy/fuzzy">https://github.com/lingpy/fuzzy</a></li>
<li>paper_authors: Johann-Mattis List, Nathan W. Hill, Robert Forkel, Frederic Blum</li>
<li>for: 这个论文主要是为了解决历史语言学中重建 proto-form 的不确定性问题。</li>
<li>methods: 这个论文使用了最新的超级vised phonological reconstruction 方法，其中一个算法学习如何在给定的 proto-语言中重建单词，并且受到了前一个文本的注释。</li>
<li>results: 这个论文提出了一种新的框架，可以表示语言重建中的不确定性，并且包括一个计算词库的工作流程。<details>
<summary>Abstract</summary>
Despite the inherently fuzzy nature of reconstructions in historical linguistics, most scholars do not represent their uncertainty when proposing proto-forms. With the increasing success of recently proposed approaches to automating certain aspects of the traditional comparative method, the formal representation of proto-forms has also improved. This formalization makes it possible to address both the representation and the computation of uncertainty. Building on recent advances in supervised phonological reconstruction, during which an algorithm learns how to reconstruct words in a given proto-language relying on previously annotated data, and inspired by improved methods for automated word prediction from cognate sets, we present a new framework that allows for the representation of uncertainty in linguistic reconstruction and also includes a workflow for the computation of fuzzy reconstructions from linguistic data.
</details>
<details>
<summary>摘要</summary>
尽管历史语言学中重建的 natura 有一定的抽象和不确定性，大多数学者在提出 proto-form 时并不表达这种不确定性。随着近期提出的一些方法在传统比较方法中自动化一些方面的成功， proto-form 的 formalization 也得到了改善。这种 formalization 使得可以考虑 both 表达和计算不确定性。基于最近的监督式phonological reconstruction 方法，我们提出了一个新的框架，该框架允许表达语言重建中的不确定性，并包括一个计算不确定性的工作流程。
</details></li>
</ul>
<hr>
<h2 id="Is-ChatGPT-a-Financial-Expert-Evaluating-Language-Models-on-Financial-Natural-Language-Processing"><a href="#Is-ChatGPT-a-Financial-Expert-Evaluating-Language-Models-on-Financial-Natural-Language-Processing" class="headerlink" title="Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing"></a>Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12664">http://arxiv.org/abs/2310.12664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Guo, Zian Xu, Yi Yang</li>
<li>for: 评估大语言模型（LLMs）在金融领域的总体能力。</li>
<li>methods: 使用FinLMEval框架，包括九个金融语言任务的数据集，对语言模型进行评估。</li>
<li>results: 发现某些decoder-only LLMs在大多数金融任务上表现出色，但在使用专有数据集时，它们通常落后于专业化模型，特别是在使用预训练的情况下。<details>
<summary>Abstract</summary>
The emergence of Large Language Models (LLMs), such as ChatGPT, has revolutionized general natural language preprocessing (NLP) tasks. However, their expertise in the financial domain lacks a comprehensive evaluation. To assess the ability of LLMs to solve financial NLP tasks, we present FinLMEval, a framework for Financial Language Model Evaluation, comprising nine datasets designed to evaluate the performance of language models. This study compares the performance of encoder-only language models and the decoder-only language models. Our findings reveal that while some decoder-only LLMs demonstrate notable performance across most financial tasks via zero-shot prompting, they generally lag behind the fine-tuned expert models, especially when dealing with proprietary datasets. We hope this study provides foundation evaluations for continuing efforts to build more advanced LLMs in the financial domain.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM），如ChatGPT，对普通自然语言处理（NLP）任务带来革命性的改变。然而，它们在金融领域的专业知识仍然缺乏全面的评估。为了评估语言模型在金融NLP任务中的能力，我们提出了FinLMEval框架，包括9个数据集，用于评估语言模型的表现。本研究比较了encoder-only语言模型和decoder-only语言模型的表现。我们的发现表明，虽然一些decoder-only LLMS在大多数金融任务上通过零shot提示表现出优异的能力，但它们通常落后于专门适应模型，特别是当面临专有数据集时。我们希望这项研究可以提供基础评估，以便将来继续努力建立更高级的LLMs在金融领域。
</details></li>
</ul>
<hr>
<h2 id="Towards-Real-World-Streaming-Speech-Translation-for-Code-Switched-Speech"><a href="#Towards-Real-World-Streaming-Speech-Translation-for-Code-Switched-Speech" class="headerlink" title="Towards Real-World Streaming Speech Translation for Code-Switched Speech"></a>Towards Real-World Streaming Speech Translation for Code-Switched Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12648">http://arxiv.org/abs/2310.12648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/apple/ml-codeswitching-translations">https://github.com/apple/ml-codeswitching-translations</a></li>
<li>paper_authors: Belen Alastruey, Matthias Sperber, Christian Gollan, Dominic Telaar, Tim Ng, Aashish Agarwal</li>
<li>for: 这篇论文主要是为了研究在流动环境下的多语言混合输入自动翻译（CS speech translation）。</li>
<li>methods: 该论文使用了扩展的鱼者和 Май亚миtest和验证集，以便在流动环境下和不同语言之间进行翻译。</li>
<li>results: 该论文在两种不同的翻译Setting下（即Offline和流动环境）实现了基线结果。<details>
<summary>Abstract</summary>
Code-switching (CS), i.e. mixing different languages in a single sentence, is a common phenomenon in communication and can be challenging in many Natural Language Processing (NLP) settings. Previous studies on CS speech have shown promising results for end-to-end speech translation (ST), but have been limited to offline scenarios and to translation to one of the languages present in the source (\textit{monolingual transcription}).   In this paper, we focus on two essential yet unexplored areas for real-world CS speech translation: streaming settings, and translation to a third language (i.e., a language not included in the source). To this end, we extend the Fisher and Miami test and validation datasets to include new targets in Spanish and German. Using this data, we train a model for both offline and streaming ST and we establish baseline results for the two settings mentioned earlier.
</details>
<details>
<summary>摘要</summary>
��ysteering (CS), i.e. mixing different languages in a single sentence, is a common phenomenon in communication and can be challenging in many Natural Language Processing (NLP) settings. Previous studies on CS speech have shown promising results for end-to-end speech translation (ST), but have been limited to offline scenarios and to translation to one of the languages present in the source (\textit{monolingual transcription}).   In this paper, we focus on two essential yet unexplored areas for real-world CS speech translation: streaming settings, and translation to a third language (i.e., a language not included in the source). To this end, we extend the Fisher and Miami test and validation datasets to include new targets in Spanish and German. Using this data, we train a model for both offline and streaming ST and we establish baseline results for the two settings mentioned earlier.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world.
</details></li>
</ul>
<hr>
<h2 id="Non-Autoregressive-Sentence-Ordering"><a href="#Non-Autoregressive-Sentence-Ordering" class="headerlink" title="Non-Autoregressive Sentence Ordering"></a>Non-Autoregressive Sentence Ordering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12640">http://arxiv.org/abs/2310.12640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/steven640pixel/nonautoregressive-sentence-ordering">https://github.com/steven640pixel/nonautoregressive-sentence-ordering</a></li>
<li>paper_authors: Yi Bin, Wenhao Shi, Bin Ji, Jipeng Zhang, Yujuan Ding, Yang Yang</li>
<li>for: 用于改进现有的句子排序方法，以提高句子排序 task 的效果。</li>
<li>methods: 提出了一种新的 Non-Autoregressive Ordering Network（NAON），该模型可以并行地预测每个句子的位置，并且可以充分利用句子之间的双向依赖关系。</li>
<li>results: 通过对多个常用的数据集进行广泛的实验，研究发现 NAON 模型可以与既有的排序方法相比，并且与当前的状态�的表现竞争。代码可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/steven640pixel/nonautoregressive-sentence-ordering%E3%80%82">https://github.com/steven640pixel/nonautoregressive-sentence-ordering。</a><details>
<summary>Abstract</summary>
Existing sentence ordering approaches generally employ encoder-decoder frameworks with the pointer net to recover the coherence by recurrently predicting each sentence step-by-step. Such an autoregressive manner only leverages unilateral dependencies during decoding and cannot fully explore the semantic dependency between sentences for ordering. To overcome these limitations, in this paper, we propose a novel Non-Autoregressive Ordering Network, dubbed \textit{NAON}, which explores bilateral dependencies between sentences and predicts the sentence for each position in parallel. We claim that the non-autoregressive manner is not just applicable but also particularly suitable to the sentence ordering task because of two peculiar characteristics of the task: 1) each generation target is in deterministic length, and 2) the sentences and positions should match exclusively. Furthermore, to address the repetition issue of the naive non-autoregressive Transformer, we introduce an exclusive loss to constrain the exclusiveness between positions and sentences. To verify the effectiveness of the proposed model, we conduct extensive experiments on several common-used datasets and the experimental results show that our method outperforms all the autoregressive approaches and yields competitive performance compared with the state-of-the-arts. The codes are available at: \url{https://github.com/steven640pixel/nonautoregressive-sentence-ordering}.
</details>
<details>
<summary>摘要</summary>
传统的句子排序方法通常采用encoder-decoder框架，使用Pointer网来恢复句子之间的相互关系。这种泛化的方式只能利用句子之间的单方向依赖关系，无法全面探索句子之间的semantic依赖关系。为了解决这些限制，在这篇论文中，我们提出了一种新的非泛化排序网络，名为NAON，它可以并行地遍历句子之间的bilateral依赖关系，并且可以在不同的句子之间进行独特的排序。我们认为非泛化的方式不仅可以应用于句子排序任务，而且特别适用于这种任务，因为句子的生成目标是固定长度的，并且句子和位置之间必须匹配精确。此外，为了解决非泛化 transformer 的重复问题，我们引入了一种独特的损失函数，以便约束句子和位置之间的唯一性。为了证明我们的方法的效果，我们在多个常用的数据集上进行了广泛的实验，结果表明，我们的方法不仅超过了所有泛化方法，而且与当前的state-of-the-arts具有竞争力。代码可以在以下链接获取：\url{https://github.com/steven640pixel/nonautoregressive-sentence-ordering}.
</details></li>
</ul>
<hr>
<h2 id="Predict-the-Future-from-the-Past-On-the-Temporal-Data-Distribution-Shift-in-Financial-Sentiment-Classifications"><a href="#Predict-the-Future-from-the-Past-On-the-Temporal-Data-Distribution-Shift-in-Financial-Sentiment-Classifications" class="headerlink" title="Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications"></a>Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12620">http://arxiv.org/abs/2310.12620</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Guo, Chenxi Hu, Yi Yang</li>
<li>for: 如何在不稳定的股票市场环境中训练一个精度和鲁棒地感知股票情绪分析系统？</li>
<li>methods: 我们使用实验方法来研究在时间分布shift下financial sentiment analysis系统的性能，并提出一种基于时间序列模型的新方法，用于检测和适应 evolving temporal shifts。</li>
<li>results: 实验结果表明，我们提出的方法可以增强模型在不稳定的股票市场中适应时间分布shift的能力，并且在不同的时间窗口和市场情况下具有良好的泛化能力。<details>
<summary>Abstract</summary>
Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from general performance degradation in the presence of temporal distribution shifts. Furthermore, motivated by the unique temporal nature of the financial text, we propose a novel method that combines out-of-distribution detection with time series modeling for temporal financial sentiment analysis. Experimental results show that the proposed method enhances the model's capability to adapt to evolving temporal shifts in a volatile financial market.
</details>
<details>
<summary>摘要</summary>
Temporal data distribution shift is prevalent in financial texts. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from general performance degradation in the presence of temporal distribution shifts. Furthermore, motivated by the unique temporal nature of financial texts, we propose a novel method that combines out-of-distribution detection with time series modeling for temporal financial sentiment analysis. Experimental results show that the proposed method enhances the model's capability to adapt to evolving temporal shifts in a volatile financial market.Here's the word-for-word translation of the text into Simplified Chinese:时间数据分布偏移是金融文本中的普遍现象。如何在投资环境中训练一个可以准确感受情感并在时间数据分布偏移下具有鲜度的金融情感分析系统？在这篇论文中，我们通过使用三年的实际金融社交媒体数据进行了employmerical study，发现了精度调整模型在时间数据分布偏移下的总性性能下降。此外，鉴于金融文本的特殊时间特征，我们提出了一种新的方法，即将out-of-distribution检测与时间系列模型结合以实现时间金融情感分析。实验结果表明，我们的提议方法可以在投资市场中的投资环境中提高模型的适应能力。
</details></li>
</ul>
<hr>
<h2 id="Multilingual-estimation-of-political-party-positioning-From-label-aggregation-to-long-input-Transformers"><a href="#Multilingual-estimation-of-political-party-positioning-From-label-aggregation-to-long-input-Transformers" class="headerlink" title="Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers"></a>Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12575">http://arxiv.org/abs/2310.12575</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/macleginn/party-positioning-code">https://github.com/macleginn/party-positioning-code</a></li>
<li>paper_authors: Dmitry Nikolaev, Tanise Ceron, Sebastian Padó</li>
<li>for: 这个研究是为了 automatization of scaling analysis in computational political science, 用于分析政治actor（如政治家或政党）在长文本（如国会讲话或选举 manifesto）中的政治倾向。</li>
<li>methods: 这个研究使用了两种方法来实现自动化的scaling analysis：label aggregation和long-input-Transformer-based models。label aggregation是一种管道Strategy，通过对manifestos中的每个声明进行标签注释来计算分类值，而long-input-Transformer-based models则直接从原始文本中计算分类值。</li>
<li>results: 研究在 Comparative Manifestos Project 数据集上进行了分析，包括41个国家和27种语言，并发现使用当今的模型可以高效解决这个任务，而label aggregation方法得到了最佳结果。<details>
<summary>Abstract</summary>
Scaling analysis is a technique in computational political science that assigns a political actor (e.g. politician or party) a score on a predefined scale based on a (typically long) body of text (e.g. a parliamentary speech or an election manifesto). For example, political scientists have often used the left--right scale to systematically analyse political landscapes of different countries. NLP methods for automatic scaling analysis can find broad application provided they (i) are able to deal with long texts and (ii) work robustly across domains and languages. In this work, we implement and compare two approaches to automatic scaling analysis of political-party manifestos: label aggregation, a pipeline strategy relying on annotations of individual statements from the manifestos, and long-input-Transformer-based models, which compute scaling values directly from raw text. We carry out the analysis of the Comparative Manifestos Project dataset across 41 countries and 27 languages and find that the task can be efficiently solved by state-of-the-art models, with label aggregation producing the best results.
</details>
<details>
<summary>摘要</summary>
《缩放分析》是计算政治科学中的一种技术，它将政治actor（如政治家或党派）分配到一个预先定义的数值范围中，基于一段（通常很长）的文本（如国会演讲或选举纲领）。例如，政治科学家经常使用左右刻度来系统地分析不同国家的政治景观。NLP方法可以自动进行缩放分析，只要它们能够处理长文本并在领域和语言上具有可靠性。在这项工作中，我们实现并比较了两种自动缩放分析政党纲领的方法：标签聚合策略和长输入变换器模型。我们对 Comparative Manifestos Project 数据集进行了41个国家和27种语言的分析，发现这种任务可以通过当今的模型高效解决，标签聚合策略得到了最佳结果。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Help-Humans-Verify-Truthfulness-–-Except-When-They-Are-Convincingly-Wrong"><a href="#Large-Language-Models-Help-Humans-Verify-Truthfulness-–-Except-When-They-Are-Convincingly-Wrong" class="headerlink" title="Large Language Models Help Humans Verify Truthfulness – Except When They Are Convincingly Wrong"></a>Large Language Models Help Humans Verify Truthfulness – Except When They Are Convincingly Wrong</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12558">http://arxiv.org/abs/2310.12558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daumé III, Jordan Boyd-Graber</li>
<li>for: 这 paper 是研究语言模型（LLMs）在提供信息时的可靠性和事实性的。</li>
<li>methods: 这 paper 使用了80名劳动者进行实验，比较语言模型和搜索引擎在帮助用户Fact-checking中的表现。</li>
<li>results: 用户阅读语言模型的解释时比使用搜索引擎更高效，但往往会因为错误的解释而过分依赖于语言模型。为解决这个问题， authors 提出了使用对比性的解释，并证明这种方法不能显著超越搜索引擎。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly used for accessing information on the web. Their truthfulness and factuality are thus of great interest. To help users make the right decisions about the information they're getting, LLMs should not only provide but also help users fact-check information. In this paper, we conduct experiments with 80 crowdworkers in total to compare language models with search engines (information retrieval systems) at facilitating fact-checking by human users. We prompt LLMs to validate a given claim and provide corresponding explanations. Users reading LLM explanations are significantly more efficient than using search engines with similar accuracy. However, they tend to over-rely the LLMs when the explanation is wrong. To reduce over-reliance on LLMs, we ask LLMs to provide contrastive information - explain both why the claim is true and false, and then we present both sides of the explanation to users. This contrastive explanation mitigates users' over-reliance on LLMs, but cannot significantly outperform search engines. However, showing both search engine results and LLM explanations offers no complementary benefits as compared to search engines alone. Taken together, natural language explanations by LLMs may not be a reliable replacement for reading the retrieved passages yet, especially in high-stakes settings where over-relying on wrong AI explanations could lead to critical consequences.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在访问网络信息方面日益广泛使用。因此，LLM的准确性和事实性具有极大的兴趣。为了帮助用户做出正确的信息决策，LLM不仅应该提供信息，而且还应该帮助用户进行事实核实。在这篇论文中，我们通过80名志愿者进行实验，比较了LLM与搜索引擎（信息检索系统）在促进用户进行事实核实方面的性能。我们请求LLM验证一个声明，并提供相关的解释。用户读取LLM解释时比使用搜索引擎相同的精度更高，但往往会因LLM解释错误而过度依赖LLM。为减少过度依赖LLM，我们请求LLM提供相互补做的解释——解释一个声明是True和False两个方面。然后，我们向用户展示这两个解释。这种相互补做的解释可以减少用户对LLM的过度依赖，但无法达到与搜索引擎相同的性能。尽管显示搜索引擎结果和LLM解释可以提供补做，但这并没有提供补做的优势。因此，自然语言解释由LLM可能不是一个可靠的替代品，特别在高风险情况下，过度依赖错误的AI解释可能会导致严重的后果。
</details></li>
</ul>
<hr>
<h2 id="Product-Attribute-Value-Extraction-using-Large-Language-Models"><a href="#Product-Attribute-Value-Extraction-using-Large-Language-Models" class="headerlink" title="Product Attribute Value Extraction using Large Language Models"></a>Product Attribute Value Extraction using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12537">http://arxiv.org/abs/2310.12537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wbsg-uni-mannheim/extractgpt">https://github.com/wbsg-uni-mannheim/extractgpt</a></li>
<li>paper_authors: Alexander Brinkmann, Roee Shraga, Christian Bizer</li>
<li>for: 这个论文是关于如何使用大型自然语言模型（LLM）来进行 attribute&#x2F;value EXTRACTION，以提高 attribute&#x2F;value EXTRACTION 的效率和可靠性。</li>
<li>methods: 这个论文使用了 hosted LLMs 和 open-source LLMs，如 GPT-3.5 和 GPT-4，以及不同的提问设计和示例值提供方法来进行 attribute&#x2F;value EXTRACTION。</li>
<li>results: 研究发现，使用 GPT-4 可以达到 attribute&#x2F;value EXTRACTION 的平均 F1 分数为 85%，而最佳 PLM-based 技术在同样的训练数据量下表现较差约 5%。此外， fine-tuned GPT-3.5 模型可以达到类似于 GPT-4 的性能，但是更加经济。<details>
<summary>Abstract</summary>
E-commerce applications such as faceted product search or product comparison are based on structured product descriptions like attribute/value pairs. The vendors on e-commerce platforms do not provide structured product descriptions but describe offers using titles or descriptions. To process such offers, it is necessary to extract attribute/value pairs from textual product attributes. State-of-the-art attribute/value extraction techniques rely on pre-trained language models (PLMs), such as BERT. Two major drawbacks of these models for attribute/value extraction are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models face challenges in generalizing to attribute values not included in the training data. This paper explores the potential of large language models (LLMs) as a training data-efficient and robust alternative to PLM-based attribute/value extraction methods. We consider hosted LLMs, such as GPT-3.5 and GPT-4, as well as open-source LLMs based on Llama2. We evaluate the models in a zero-shot scenario and in a scenario where task-specific training data is available. In the zero-shot scenario, we compare various prompt designs for representing information about the target attributes of the extraction. In the scenario with training data, we investigate (i) the provision of example attribute values, (ii) the selection of in-context demonstrations, and (iii) the fine-tuning of GPT-3.5. Our experiments show that GPT-4 achieves an average F1-score of 85% on the two evaluation datasets while the best PLM-based techniques perform on average 5% worse using the same amount of training data. GPT-4 achieves a 10% higher F1-score than the best open-source LLM. The fine-tuned GPT-3.5 model reaches a similar performance as GPT-4 while being significantly more cost-efficient.
</details>
<details>
<summary>摘要</summary>
电子商务应用程序如多维产品搜索或产品比较是基于结构化产品描述如属性值对。供应商在电子商务平台上不提供结构化产品描述，而是使用标题或描述来描述产品。为处理这些产品，需要从文本属性中提取属性值对。现状的属性值提取技术都是基于预训练语言模型（PLM），如BERT。这两种模型的缺点是：（一）模型需要大量的任务特定训练数据，（二）精通化的模型在训练数据中未包含的属性值上面临挑战。这篇论文探讨使用大语言模型（LLM）作为任务数据efficient和可靠的alternative。我们考虑了主机LLM，如GPT-3.5和GPT-4，以及基于Llama2的开源LLM。我们在零容量情况下和具有任务特定训练数据的情况下评估了模型。在零容量情况下，我们比较了不同的提示设计来表达目标属性的信息。在具有训练数据的情况下，我们研究了（一）提供示例属性值，（二）选择 Contextual Demonstrations，（三）精通化GPT-3.5。我们的实验结果显示，GPT-4在两个评估 datasets 上的平均 F1 分为 85%，而最佳 PLM 基本技术在同样的训练数据量下表现落后约 5%。GPT-4 在同样的训练数据量下达到了 10% 高的 F1 分，而 откры源 LL 在同样的训练数据量下达到了类似的性能。精通化 GPT-3.5 模型可以达到类似的性能，但是更加cost-efficient。
</details></li>
</ul>
<hr>
<h2 id="ICU-Conquering-Language-Barriers-in-Vision-and-Language-Modeling-by-Dividing-the-Tasks-into-Image-Captioning-and-Language-Understanding"><a href="#ICU-Conquering-Language-Barriers-in-Vision-and-Language-Modeling-by-Dividing-the-Tasks-into-Image-Captioning-and-Language-Understanding" class="headerlink" title="ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding"></a>ICU: Conquering Language Barriers in Vision-and-Language Modeling by Dividing the Tasks into Image Captioning and Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12531">http://arxiv.org/abs/2310.12531</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gjwubyron/icu">https://github.com/gjwubyron/icu</a></li>
<li>paper_authors: Guojun Wu</li>
<li>for: 本文旨在解决多语言视频语言（V&amp;L）研究中的多语言和多Modal功能问题。</li>
<li>methods: 我们提出了一种图像Caption理解（ICU）技术，将V&amp;L任务分成两个阶段：首先，V&amp;L模型在英语下进行图像Captioning；然后，多语言自然语言模型（mLM）使用Caption作为alt文本，进行多语言语言理解。这将将多语言处理的负担卸载到mLM上。</li>
<li>results: 在IGLUEbenchmark中的两个任务上，我们通过实验表明，ICU可以在9种语言中为5种语言取得新的状态对抗记录，并对剩下的语言取得相似的记录。<details>
<summary>Abstract</summary>
Most multilingual vision-and-language (V&L) research aims to accomplish multilingual and multimodal capabilities within one model. However, the scarcity of multilingual captions for images has hindered the development. To overcome this obstacle, we propose ICU, Image Caption Understanding, which divides a V&L task into two stages: a V&L model performs image captioning in English, and a multilingual language model (mLM), in turn, takes the caption as the alt text and performs crosslingual language understanding. The burden of multilingual processing is lifted off V&L model and placed on mLM. Since the multilingual text data is relatively of higher abundance and quality, ICU can facilitate the conquering of language barriers for V&L models. In experiments on two tasks across 9 languages in the IGLUE benchmark, we show that ICU can achieve new state-of-the-art results for five languages, and comparable results for the rest.
</details>
<details>
<summary>摘要</summary>
大多数多语言视觉语言（V&L）研究的目标是在一个模型中实现多语言和多Modal功能。然而，由于图像 Multilingual 标签的罕见性，这些研究受到了阻碍。为了解决这个问题，我们提出了 ICU（图像标题理解），它将 V&L 任务分成两个阶段：一个 V&L 模型在英语中进行图像标题 generation，然后一个多语言语言模型（mLM）使用这个标题作为Alt文本，进行跨语言语言理解。将多语言处理的负担从 V&L 模型转移到 mLM 上。由于多语言文本数据的质量和quantity相对较高，ICU 可以帮助 conquering 语言障碍 для V&L 模型。在 IGLUE benchmark 上两个任务上，我们通过实验表明，ICU 可以达到新的州OF-THE-ART 结果的五种语言，和与其他语言相对的结果。
</details></li>
</ul>
<hr>
<h2 id="Named-Entity-Recognition-for-Monitoring-Plant-Health-Threats-in-Tweets-a-ChouBERT-Approach"><a href="#Named-Entity-Recognition-for-Monitoring-Plant-Health-Threats-in-Tweets-a-ChouBERT-Approach" class="headerlink" title="Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach"></a>Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12522">http://arxiv.org/abs/2310.12522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shufan Jiang, Rafael Angarita, Stéphane Cormier, Francis Rousseaux</li>
<li>for: 本研究旨在使用感知技术和数据分析方法探测和评估农业精度中的作物健康威胁。</li>
<li>methods: 本研究使用社交媒体平台 Twitter 的用户生成的文本数据，通过抽象Token-level注解任务来提高 ChouBERT 模型对作物健康问题的识别能力。</li>
<li>results: ChouBERT 模型可以通过 Twitter 上的用户生成的文本数据探测到不熟悉的作物健康问题，并且可以在不同的自然灾害中保持一定的通用性。<details>
<summary>Abstract</summary>
An important application scenario of precision agriculture is detecting and measuring crop health threats using sensors and data analysis techniques. However, the textual data are still under-explored among the existing solutions due to the lack of labelled data and fine-grained semantic resources. Recent research suggests that the increasing connectivity of farmers and the emergence of online farming communities make social media like Twitter a participatory platform for detecting unfamiliar plant health events if we can extract essential information from unstructured textual data. ChouBERT is a French pre-trained language model that can identify Tweets concerning observations of plant health issues with generalizability on unseen natural hazards. This paper tackles the lack of labelled data by further studying ChouBERT's know-how on token-level annotation tasks over small labeled sets.
</details>
<details>
<summary>摘要</summary>
设置语言为简化中文。<</SYS>>精准农业中一个重要应用场景是通过感知器和数据分析技术检测和评估作物健康威胁。然而，文本数据仍然受到已有解决方案的限制，这是因为lack labelled data和细化 semantic resources。近期研究表明，农民之间的连接度的增加和在线农业社区的出现，使得社交媒体如推特成为了检测不熟悉的作物健康事件的参与式平台。ChouBERT是一种法国预训练语言模型，可以从推特上提取关于作物健康问题的观察记录，并且可以在未看过的自然灾害上进行普适化。本文解决了lack labelled data问题，通过进一步研究ChouBERT的Token-level注释任务能力。
</details></li>
</ul>
<hr>
<h2 id="Lost-in-Translation-When-GPT-4V-ision-Can’t-See-Eye-to-Eye-with-Text-A-Vision-Language-Consistency-Analysis-of-VLLMs-and-Beyond"><a href="#Lost-in-Translation-When-GPT-4V-ision-Can’t-See-Eye-to-Eye-with-Text-A-Vision-Language-Consistency-Analysis-of-VLLMs-and-Beyond" class="headerlink" title="Lost in Translation: When GPT-4V(ision) Can’t See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond"></a>Lost in Translation: When GPT-4V(ision) Can’t See Eye to Eye with Text. A Vision-Language-Consistency Analysis of VLLMs and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12520">http://arxiv.org/abs/2310.12520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Zhang, Senyu Li, Zijun Wu, Ning Shi</li>
<li>for: 这篇论文旨在探讨多模态技术的新进展，以及这些技术在文本、音频和图像处理任务中的表现。</li>
<li>methods: 本文使用的方法是将计算机视觉和自然语言处理结合在一起，并对这些视觉语言模型（VLLMs）在多种任务上进行了全面的分析。</li>
<li>results: 研究发现，当任务较为简单时，模型如GPT-4V在不同modalities之间具有一定的一致性。然而，当任务变得更加复杂时，图像模态的可靠性减退。此外，我们还提出了一种“视觉描述引导”方法，可以有效地提高在复杂视觉任务中的表现。<details>
<summary>Abstract</summary>
Recent advancements in multimodal techniques open exciting possibilities for models excelling in diverse tasks involving text, audio, and image processing. Models like GPT-4V, blending computer vision and language modeling, excel in complex text and image tasks. Numerous prior research endeavors have diligently examined the performance of these Vision Large Language Models (VLLMs) across tasks like object detection, image captioning and others. However, these analyses often focus on evaluating the performance of each modality in isolation, lacking insights into their cross-modal interactions. Specifically, questions concerning whether these vision-language models execute vision and language tasks consistently or independently have remained unanswered. In this study, we draw inspiration from recent investigations into multilingualism and conduct a comprehensive analysis of model's cross-modal interactions. We introduce a systematic framework that quantifies the capability disparities between different modalities in the multi-modal setting and provide a set of datasets designed for these evaluations. Our findings reveal that models like GPT-4V tend to perform consistently modalities when the tasks are relatively simple. However, the trustworthiness of results derived from the vision modality diminishes as the tasks become more challenging. Expanding on our findings, we introduce "Vision Description Prompting," a method that effectively improves performance in challenging vision-related tasks.
</details>
<details>
<summary>摘要</summary>
现代多模态技术的发展开创了多任务涉及文本、音频和图像处理的模型表现出色的可能性。如GPT-4V模型，它将计算机视觉和自然语言处理融合在一起，在复杂的文本和图像任务中表现出色。许多前期研究努力地研究了这些视觉大语言模型（VLLMs）在不同任务中的表现，但这些分析通常会孤立地评估每个模式的表现，缺乏跨模式交互的视角。特别是，关于这些视觉语言模型在视觉和语言任务中是否能够协调一致的问题，一直未得到回答。在这项研究中，我们 draw inspiration from recent investigations into multilingualism and conduct a comprehensive analysis of the model's cross-modal interactions. We introduce a systematic framework that quantifies the capability disparities between different modalities in the multi-modal setting and provide a set of datasets designed for these evaluations. Our findings reveal that models like GPT-4V tend to perform consistently across modalities when the tasks are relatively simple. However, the trustworthiness of results derived from the vision modality diminishes as the tasks become more challenging. Based on our findings, we introduce "Vision Description Prompting," a method that effectively improves performance in challenging vision-related tasks.
</details></li>
</ul>
<hr>
<h2 id="Attack-Prompt-Generation-for-Red-Teaming-and-Defending-Large-Language-Models"><a href="#Attack-Prompt-Generation-for-Red-Teaming-and-Defending-Large-Language-Models" class="headerlink" title="Attack Prompt Generation for Red Teaming and Defending Large Language Models"></a>Attack Prompt Generation for Red Teaming and Defending Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12505">http://arxiv.org/abs/2310.12505</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aatrox103/sap">https://github.com/aatrox103/sap</a></li>
<li>paper_authors: Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He</li>
<li>for: 防御大语言模型（LLMs）受到红色队伍攻击，生成危害性内容。</li>
<li>methods: 提出一种整合手动和自动方法的方法，以便经济高质量的攻击提示构造。</li>
<li>results: 实验 validate 提出的攻击和防御框架的有效性，并释放了不同大语言模型的攻击提示数据集（SAP）。<details>
<summary>Abstract</summary>
Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP .
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Co-2-PT-Mitigating-Bias-in-Pre-trained-Language-Models-through-Counterfactual-Contrastive-Prompt-Tuning"><a href="#Co-2-PT-Mitigating-Bias-in-Pre-trained-Language-Models-through-Counterfactual-Contrastive-Prompt-Tuning" class="headerlink" title="Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning"></a>Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12490">http://arxiv.org/abs/2310.12490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongxiangjue/co2pt">https://github.com/dongxiangjue/co2pt</a></li>
<li>paper_authors: Xiangjue Dong, Ziwei Zhu, Zhuoer Wang, Maria Teleki, James Caverlee</li>
<li>for: 降低语言模型中的社会偏见</li>
<li>methods: 对下游任务进行debias-while-prompt tuning，通过对比事实contrastive prompt tuning来mitigate bias</li>
<li>results: 在三个外在偏见benchmark上进行了实验，结果表明Co$^2$PT在下游任务中的偏见mitigation效果显著，并且可以与现有的上游净化语言模型结合使用。<details>
<summary>Abstract</summary>
Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.
</details>
<details>
<summary>摘要</summary>
预训言语模型在许多重要的实际应用中广泛使用。然而，最近的研究表明，这些模型可以从大规模预训料中学习社会偏见，甚至在下游应用中强化偏见。为解决这个挑战，我们提出了Co$^2$PT，一种高效的debias-while-prompt tuning方法，通过对下游任务进行假想对比的短语调整来mitigate偏见。我们的实验在三个外在偏见benchmark中展示了Co$^2$PT在偏见减轻过程中的效果和对现有的逆偏见语言模型的适应性。这些发现表明Co$^2$PT的强大和进一步减轻偏见的可能性。
</details></li>
</ul>
<hr>
<h2 id="MedAI-Dialog-Corpus-MEDIC-Zero-Shot-Classification-of-Doctor-and-AI-Responses-in-Health-Consultations"><a href="#MedAI-Dialog-Corpus-MEDIC-Zero-Shot-Classification-of-Doctor-and-AI-Responses-in-Health-Consultations" class="headerlink" title="MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations"></a>MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12489">http://arxiv.org/abs/2310.12489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olumide E. Ojo, Olaronke O. Adebanji, Alexander Gelbukh, Hiram Calvo, Anna Feldman</li>
<li>for: 本研究旨在检验预训练语言模型在医疗咨询中的效果，以零批学习方式准确分类医生和AI生成的文本。</li>
<li>methods: 我们使用预训练语言模型进行零批学习，对医生和AI生成的医疗咨询文本进行分类。</li>
<li>results: 研究发现，预训练语言模型在医疗咨询文本分类方面存在限制，尚未达到预期的准确率。这些结果为未来在医疗文本分类领域的研究提供了基础。<details>
<summary>Abstract</summary>
Zero-shot classification enables text to be classified into classes not seen during training. In this research, we investigate the effectiveness of pre-trained language models to accurately classify responses from Doctors and AI in health consultations through zero-shot learning. Our study aims to determine whether these models can effectively detect if a text originates from human or AI models without specific corpus training. We collect responses from doctors to patient inquiries about their health and pose the same question/response to AI models. While zero-shot language models show a good understanding of language in general, they have limitations in classifying doctor and AI responses in healthcare consultations. This research lays the groundwork for further research into this field of medical text classification, informing the development of more effective approaches to accurately classify doctor-generated and AI-generated text in health consultations.
</details>
<details>
<summary>摘要</summary>
zero-shot 分类可以使文本被分类到没有在训练过程中看到的类别中。在这个研究中，我们研究了预训练语言模型在医疗询问中的准确性。我们的研究目标是确定这些模型能否准确地检测文本是否来自人类或AI模型，无需特定的文库训练。我们收集了医生对病人问题的回答，并对这些问题和回答提出同样的问题/回答给AI模型。虽然零shot语言模型在语言水平上显示了良好的理解，但在医疗询问中的分类中存在限制。这个研究为这一领域的医学文本分类铺平了基础，推动了更有效的approaches的发展，以准确地分类医生生成和AI生成的文本在医疗询问中。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Learning-for-Inference-in-Dialogue"><a href="#Contrastive-Learning-for-Inference-in-Dialogue" class="headerlink" title="Contrastive Learning for Inference in Dialogue"></a>Contrastive Learning for Inference in Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12467">http://arxiv.org/abs/2310.12467</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hltchkust/contrastive_inference_dialogue">https://github.com/hltchkust/contrastive_inference_dialogue</a></li>
<li>paper_authors: Etsuko Ishii, Yan Xu, Bryan Wilie, Ziwei Ji, Holy Lovenia, Willy Chung, Pascale Fung</li>
<li>for:  This paper aims to improve the ability of language models in inductive reasoning, specifically in generating correct inferences when not all information is present in the context.</li>
<li>methods: The paper uses contrastive learning, where negative samples are fed to the model to help it understand what is wrong and improve its inference generation.</li>
<li>results: The experiments suggest that using negative samples improves the model’s ability to generate correct inferences, mitigating the information gap between dialogue contexts and desired inferences.<details>
<summary>Abstract</summary>
Inference, especially those derived from inductive processes, is a crucial component in our conversation to complement the information implicitly or explicitly conveyed by a speaker. While recent large language models show remarkable advances in inference tasks, their performance in inductive reasoning, where not all information is present in the context, is far behind deductive reasoning. In this paper, we analyze the behavior of the models based on the task difficulty defined by the semantic information gap -- which distinguishes inductive and deductive reasoning (Johnson-Laird, 1988, 1993). Our analysis reveals that the disparity in information between dialogue contexts and desired inferences poses a significant challenge to the inductive inference process. To mitigate this information gap, we investigate a contrastive learning approach by feeding negative samples. Our experiments suggest negative samples help models understand what is wrong and improve their inference generations.
</details>
<details>
<summary>摘要</summary>
对话中的推理，特别是从推理过程中得到的推论，是我们的对话中的一个重要组成部分，可以补充说话人所显示或隐藏的信息。Recent large language models在推理任务中表现出色，但是在 inductive reasoning 任务中，它们的表现远远落后于 deduced reasoning。在这篇论文中，我们分析基于任务难度定义的semantic information gap，并对模型的行为进行分析。我们发现，对话上下文中的信息与推理结果之间的信息差异 pose 一个重要的挑战。为了缓解这种信息差异，我们 investigate 一种对比学习方法，通过 feeding negative samples。我们的实验表明，负样本可以帮助模型理解错误，并提高其推理生成。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Transformers-A-Theoretical-Approach-to-Data-Recovery-via-Attention-Weights"><a href="#Unmasking-Transformers-A-Theoretical-Approach-to-Data-Recovery-via-Attention-Weights" class="headerlink" title="Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights"></a>Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12462">http://arxiv.org/abs/2310.12462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhao Song, Shenghao Xie, Chiwun Yang</li>
<li>for: 本研究旨在探讨Transformer模型中数据是否可以通过注意力权重和输出来恢复。</li>
<li>methods: 我们提出了一种理论框架，通过推广损失函数L(X)来回归输入数据X。</li>
<li>results: 我们发现，通过注意力权重和输出，可以recover输入数据，这有关LLM的设计存在潜在的安全和隐私问题。<details>
<summary>Abstract</summary>
In the realm of deep learning, transformers have emerged as a dominant architecture, particularly in natural language processing tasks. However, with their widespread adoption, concerns regarding the security and privacy of the data processed by these models have arisen. In this paper, we address a pivotal question: Can the data fed into transformers be recovered using their attention weights and outputs? We introduce a theoretical framework to tackle this problem. Specifically, we present an algorithm that aims to recover the input data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top \in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by minimizing the loss function $L(X)$. This loss function captures the discrepancy between the expected output and the actual output of the transformer. Our findings have significant implications for the Localized Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's design from a security and privacy perspective. This work underscores the importance of understanding and safeguarding the internal workings of transformers to ensure the confidentiality of processed data.
</details>
<details>
<summary>摘要</summary>
在深度学习领域，转换器已成为主流架构，尤其在自然语言处理任务中。然而，随着其广泛应用，对转换器处理数据的安全性和隐私问题产生了关切的关注。在这篇论文中，我们解决了一个重要问题：可以通过转换器的注意力权重和输出来恢复输入数据 $X \in \mathbb{R}^{d \times n}$？我们提出了一个理论框架，并采用一种算法来实现这一目标。具体来说，我们提出了一种算法，通过将注意力权重 $W = QK^\top \in \mathbb{R}^{d \times d}$ 和输出 $B \in \mathbb{R}^{n \times n}$ 作为输入，计算出输入数据 $X$ 的恢复loss函数 $L(X)$。这个loss函数捕捉了转换器输出与预期输出之间的差异。我们的发现对Localized Layer-wise Mechanism (LLM) 有重要的安全性和隐私问题的影响，表明转换器的设计可能存在潜在的漏洞。这种工作强调了理解和保护转换器的内部工作方式，以确保处理数据的隐私。
</details></li>
</ul>
<hr>
<h2 id="A-Read-and-Select-Framework-for-Zero-shot-Entity-Linking"><a href="#A-Read-and-Select-Framework-for-Zero-shot-Entity-Linking" class="headerlink" title="A Read-and-Select Framework for Zero-shot Entity Linking"></a>A Read-and-Select Framework for Zero-shot Entity Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12450">http://arxiv.org/abs/2310.12450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitsz-tmg/read-and-select">https://github.com/hitsz-tmg/read-and-select</a></li>
<li>paper_authors: Zhenran Xu, Yulin Chen, Baotian Hu, Min Zhang</li>
<li>for: 这篇 paper 的目的是提出一个 zero-shot entity linking (EL) 方法，以挑战模型的通用能力。</li>
<li>methods: 这篇 paper 使用了一个 read-and-select (ReS) 框架，模型了主要的实体识别杜里识别和跨实体比较。</li>
<li>results: 这篇 paper 在 ZESHEL  dataset 上 achieved 顶尖性能，与先前大多数工作不需要几阶段预训，展示了提取和选择的两个过程之间的互动效果。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Zero-shot entity linking (EL) aims at aligning entity mentions to unseen entities to challenge the generalization ability. Previous methods largely focus on the candidate retrieval stage and ignore the essential candidate ranking stage, which disambiguates among entities and makes the final linking prediction. In this paper, we propose a read-and-select (ReS) framework by modeling the main components of entity disambiguation, i.e., mention-entity matching and cross-entity comparison. First, for each candidate, the reading module leverages mention context to output mention-aware entity representations, enabling mention-entity matching. Then, in the selecting module, we frame the choice of candidates as a sequence labeling problem, and all candidate representations are fused together to enable cross-entity comparison. Our method achieves the state-of-the-art performance on the established zero-shot EL dataset ZESHEL with a 2.55% micro-average accuracy gain, with no need for laborious multi-phase pre-training used in most of the previous work, showing the effectiveness of both mention-entity and cross-entity interaction.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traduction du texte en chinois simplifié<</SYS>>零shot实体链接（EL）目标是将实体提及与未见过的实体进行对应，以挑战总结能力。先前的方法主要集中在候选人选择阶段，忽略了实体识别阶段的关键环节，这使得最终的链接预测受到了限制。在这篇论文中，我们提出了读取和选择（ReS）框架，模型实体异常识别的主要组成部分，即提及语境匹配和跨实体比较。首先，为每个候选人，读取模块利用提及语境来生成提及意识的实体表示，以便提及语境匹配。然后，选择模块将选择问题定义为序列标签问题，并将所有候选人表示融合起来，以便跨实体比较。我们的方法在ZESHEL数据集上实现了零shotEL任务的state-of-the-art性，具有2.55%的微平均准确率提升，无需耗费大量的多阶段预训练，示出提及语境和跨实体交互的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Sparse-Retrieval-for-Few-shot-Entity-Linking"><a href="#Revisiting-Sparse-Retrieval-for-Few-shot-Entity-Linking" class="headerlink" title="Revisiting Sparse Retrieval for Few-shot Entity Linking"></a>Revisiting Sparse Retrieval for Few-shot Entity Linking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12444">http://arxiv.org/abs/2310.12444</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hitsz-tmg/sparse-retrieval-fewshot-el">https://github.com/hitsz-tmg/sparse-retrieval-fewshot-el</a></li>
<li>paper_authors: Yulin Chen, Zhenran Xu, Baotian Hu, Min Zhang</li>
<li>for: 提高域内少量标注数据下 dense retriever 的性能</li>
<li>methods: 提出了一种基于 ELECTRA 的关键词提取器，用于减噪提取 mention 上下文，构建更好的查询表达</li>
<li>results: 对 ZESHEL 数据集进行实验，提出的方法比现有模型在所有测试领域均显著提高了性能，证明了关键词增强的稀疏检索的效果。<details>
<summary>Abstract</summary>
Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base. One of the key challenges comes from insufficient labeled data for specific domains. Although dense retrievers have achieved excellent performance on several benchmarks, their performance decreases significantly when only a limited amount of in-domain labeled data is available. In such few-shot setting, we revisit the sparse retrieval method, and propose an ELECTRA-based keyword extractor to denoise the mention context and construct a better query expression. For training the extractor, we propose a distant supervision method to automatically generate training data based on overlapping tokens between mention contexts and entity descriptions. Experimental results on the ZESHEL dataset demonstrate that the proposed method outperforms state-of-the-art models by a significant margin across all test domains, showing the effectiveness of keyword-enhanced sparse retrieval.
</details>
<details>
<summary>摘要</summary>
Entity 链接目标是将模糊提及链接到知识库中的对应实体。一个关键挑战是域特定数据的不足。虽然稠密抽取器在多个标准准点上表现出色，但当只有有限量的域特定标注数据时，其表现会明显下降。在这种几个shotSetting下，我们重新考虑稠密抽取方法，并基于ELECTRA提出了一个键盘EXTRACTOR来减少提及上下文的噪声并构建更好的查询表达。为了训练EXTRACTOR，我们提出了远程监督法，通过提取 mention context 和实体描述中的重叠token来自动生成训练数据。实验结果表明，我们提出的方法在ZESHEL数据集上比 estado-of-the-art 模型具有显著的优势，在所有测试领域中表现出色，这表明了键 palabm-enhanced 稠密抽取的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Long-Range-Transformers-You-Need-to-Attend-More-but-Not-Necessarily-at-Every-Layer"><a href="#Efficient-Long-Range-Transformers-You-Need-to-Attend-More-but-Not-Necessarily-at-Every-Layer" class="headerlink" title="Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer"></a>Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12442">http://arxiv.org/abs/2310.12442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingru Zhang, Dhananjay Ram, Cole Hawkins, Sheng Zha, Tuo Zhao</li>
<li>for: 提高自然语言处理任务中Transformer模型的性能，并且降低计算成本。</li>
<li>methods: 提出了一种混合 span 注意力的 transformer 变体，即 MASFormer，它结合了全注意力和稀注意力，以提高计算效率。</li>
<li>results: 对于自然语言模型和生成任务，MASFormer 可以与权重 transformer 具有相同的性能，而且可以减少计算成本（最多下降75%）。<details>
<summary>Abstract</summary>
Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost - quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on natural language modeling and generation tasks show that a decoder-only MASFormer model of 1.3B parameters can achieve competitive performance to vanilla transformers with full attention while significantly reducing computational cost (up to 75%). Additionally, we investigate the effectiveness of continual training with long sequence data and how sequence length impacts downstream generation performance, which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
预训练的变换器模型在不同的自然语言处理任务中表现了惊人的表现。这些模型利用注意机制来捕捉序列中的长距离和短距离依赖关系。然而，全attenion机制对于长序列来说计算成本高于 quadratic，这对于长序列任务而言是不可接受的。虽然可以使用稀疏注意来提高计算效率，但这会削弱模型的表达能力，常常无法捕捉长序列中的复杂依赖关系。为了解决这个挑战，我们提出了 MASFormer，一种简单实现的变换器变体，具有混合注意长度。具体来说，MASFormer 具有全attenion，以捕捉长距离依赖关系，但只在一些层中使用。对于剩下的层，MASformer 只使用稀疏注意，以捕捉短距离依赖关系。我们的实验表明，一个 Parameters 为 1.3B 的 decoder-only MASFormer 模型可以与普通的变换器模型具有相同的表现，同时显著降低计算成本（最高降低 75%）。此外，我们还研究了在长序列数据上进行 continual training 的效果，以及序列长度对下游生成性能的影响，这可能是独立的兴趣。
</details></li>
</ul>
<hr>
<h2 id="DocXChain-A-Powerful-Open-Source-Toolchain-for-Document-Parsing-and-Beyond"><a href="#DocXChain-A-Powerful-Open-Source-Toolchain-for-Document-Parsing-and-Beyond" class="headerlink" title="DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond"></a>DocXChain: A Powerful Open-Source Toolchain for Document Parsing and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12430">http://arxiv.org/abs/2310.12430</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibabaresearch/advancedliteratemachinery">https://github.com/alibabaresearch/advancedliteratemachinery</a></li>
<li>paper_authors: Cong Yao</li>
<li>for: The paper is written for document parsing and structured representation of unstructured documents.</li>
<li>methods: The paper proposes a powerful open-source toolchain called DocXChain, which includes basic capabilities such as text detection, text recognition, table structure recognition, and layout analysis, as well as fully functional pipelines for document parsing, including general text reading, table parsing, and document structurization.</li>
<li>results: The paper demonstrates the effectiveness of DocXChain in automatically converting rich information embodied in unstructured documents into structured representations that are readable and manipulable by machines. The paper also shows that DocXChain is concise, modularized, and flexible, and can be readily integrated with existing tools, libraries, or models to construct more powerful systems for various applications related to documents in real-world scenarios.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了文档分析和结构表示不结构化文档而写的。</li>
<li>methods: 论文提出了一个强大的开源工具链 called DocXChain，包括基本功能 such as 文本检测、文本识别、表格结构识别和布局分析，以及完整的文档分析管道，包括通用文本读取、表格分析和文档结构化。</li>
<li>results: 论文证明了 DocXChain 可以自动将不结构化文档中的丰富信息转化为机器可读和操作的结构表示。论文还显示了 DocXChain 是简洁、模块化和灵活的，可以与现有的工具、库或模型（如 LangChain 和 ChatGPT）集成，建立更强大的系统，用于各种文档相关的应用场景。<details>
<summary>Abstract</summary>
In this report, we introduce DocXChain, a powerful open-source toolchain for document parsing, which is designed and developed to automatically convert the rich information embodied in unstructured documents, such as text, tables and charts, into structured representations that are readable and manipulable by machines. Specifically, basic capabilities, including text detection, text recognition, table structure recognition and layout analysis, are provided. Upon these basic capabilities, we also build a set of fully functional pipelines for document parsing, i.e., general text reading, table parsing, and document structurization, to drive various applications related to documents in real-world scenarios. Moreover, DocXChain is concise, modularized and flexible, such that it can be readily integrated with existing tools, libraries or models (such as LangChain and ChatGPT), to construct more powerful systems that can accomplish more complicated and challenging tasks. The code of DocXChain is publicly available at:~\url{https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Applications/DocXChain}
</details>
<details>
<summary>摘要</summary>
在本报告中，我们介绍了 DocXChain，一个强大的开源工具链 для文档分析，它可以自动将不结构化文档中的丰富信息，如文本、表格和图表，转化为可读取和可操作的机器可读取格式。具体来说，提供了基本功能，包括文本检测、文本识别、表格结构识别和文档布局分析。在这些基本功能之上，我们还构建了一些完整的文档分析管道，例如通用文本读取、表格分析和文档结构化，以驱动实际场景中文档应用。此外， DocXChain 具有简洁、模块化和灵活的特点，可以轻松地与现有的工具、库或模型（如 LangChain 和 ChatGPT）集成，以构建更强大的系统，用于解决更复杂和挑战性的任务。 DocXChain 的代码可以在以下链接获取：https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/Applications/DocXChain。
</details></li>
</ul>
<hr>
<h2 id="The-Shifted-and-The-Overlooked-A-Task-oriented-Investigation-of-User-GPT-Interactions"><a href="#The-Shifted-and-The-Overlooked-A-Task-oriented-Investigation-of-User-GPT-Interactions" class="headerlink" title="The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions"></a>The Shifted and The Overlooked: A Task-oriented Investigation of User-GPT Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12418">http://arxiv.org/abs/2310.12418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ozyyshr/ShareGPT_investigation">https://github.com/ozyyshr/ShareGPT_investigation</a></li>
<li>paper_authors: Siru Ouyang, Shuohang Wang, Yang Liu, Ming Zhong, Yizhu Jiao, Dan Iter, Reid Pryzant, Chenguang Zhu, Heng Ji, Jiawei Han</li>
<li>for: 本研究旨在探讨现有的大语言模型（LLM）研究是否准确反映用户需求。</li>
<li>methods: 本研究使用大规模的用户-GPT对话集来分析现有的NLP研究和用户需求之间的差异。</li>
<li>results: 研究发现用户常见的任务，如“设计”和“规划”，在学术研究中受到忽视或与传统的NLP标准任务不同。研究还探讨了这些被忽略的任务的实际挑战和如何使LLM更加适应用户需求。<details>
<summary>Abstract</summary>
Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between current NLP research and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as ``design'' and ``planning'' are prevalent in user interactions but are largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges they pose, and provide insights toward a roadmap to make LLMs better aligned with user needs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FinEntity-Entity-level-Sentiment-Classification-for-Financial-Texts"><a href="#FinEntity-Entity-level-Sentiment-Classification-for-Financial-Texts" class="headerlink" title="FinEntity: Entity-level Sentiment Classification for Financial Texts"></a>FinEntity: Entity-level Sentiment Classification for Financial Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12406">http://arxiv.org/abs/2310.12406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yixuantt/finentity">https://github.com/yixuantt/finentity</a></li>
<li>paper_authors: Yixuan Tang, Yi Yang, Allen H Huang, Andy Tam, Justin Z Tang</li>
<li>for: 这篇论文的目的是为了介绍一个新的金融实体 Sentiment 分类数据集（FinEntity），该数据集包含金融新闻中的实体 span 和其 sentiment（积极、中性、消极）标注。</li>
<li>methods: 论文中使用了数据集建构过程的文章，以及对多种预训练模型（如 BERT、FinBERT 等）和 ChatGPT 的实体 Sentiment 分类 benchmarking。</li>
<li>results: 在一个案例研究中，通过使用 FinEntity 监测 криптовалю市场，实证表明 FinEntity 可以帮助准确评估金融实体的 Sentiment。数据和代码可以在 GitHub 上下载：<a target="_blank" rel="noopener" href="https://github.com/yixuantt/FinEntity">https://github.com/yixuantt/FinEntity</a><details>
<summary>Abstract</summary>
In the financial domain, conducting entity-level sentiment analysis is crucial for accurately assessing the sentiment directed toward a specific financial entity. To our knowledge, no publicly available dataset currently exists for this purpose. In this work, we introduce an entity-level sentiment classification dataset, called \textbf{FinEntity}, that annotates financial entity spans and their sentiment (positive, neutral, and negative) in financial news. We document the dataset construction process in the paper. Additionally, we benchmark several pre-trained models (BERT, FinBERT, etc.) and ChatGPT on entity-level sentiment classification. In a case study, we demonstrate the practical utility of using FinEntity in monitoring cryptocurrency markets. The data and code of FinEntity is available at \url{https://github.com/yixuantt/FinEntity}
</details>
<details>
<summary>摘要</summary>
在金融领域，实施实体级别的情感分析是准确评估特定金融实体所向的情感方向的关键。据我们知道，目前没有公开可用的数据集用于此目的。在这种工作中，我们介绍了一个名为\textbf{FinEntity}的实体级别情感分类数据集，该数据集标注了金融实体范围内的情感（积极、中性、消极）在金融新闻中。我们在论文中详细介绍了数据集的建构过程。此外，我们还对多种预训练模型（如BERT、FinBERT等）和ChatGPT进行了实体级别情感分类的benchmark测试。在一个实验案例中，我们示出了使用FinEntity监测 криптовалю市场的实际实用性。FinEntity数据和代码可以在\url{https://github.com/yixuantt/FinEntity}获取。
</details></li>
</ul>
<hr>
<h2 id="Loop-Copilot-Conducting-AI-Ensembles-for-Music-Generation-and-Iterative-Editing"><a href="#Loop-Copilot-Conducting-AI-Ensembles-for-Music-Generation-and-Iterative-Editing" class="headerlink" title="Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing"></a>Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12404">http://arxiv.org/abs/2310.12404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, Simon Dixon</li>
<li>for: 本研究旨在提供一个可交互的音乐创作系统，帮助用户逐步发展和精确地调整音乐作品。</li>
<li>methods: 本研究使用大量语言模型来理解用户意图，并选择适当的人工智能模型来进行任务执行。每个后端模型特化于特定任务，其输出被聚合以满足用户的需求。</li>
<li>results: 透过对受测者进行 semi-结构化访谈和调查，研究发现本系统不仅能帮助用户创作音乐，还有潜在应用于更广泛的领域。<details>
<summary>Abstract</summary>
Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.
</details>
<details>
<summary>摘要</summary>
创作音乐是一个迭代过程，需要不同的方法在每个阶段。然而，现有的AI音乐系统尚未能够有效地融合多种需求。为了解决这一问题，我们介绍了Loop Copilot，一种新的系统，允许用户通过交互式、多轮对话界面来生成和 repeatedly refine music。该系统使用大型自然语言模型来理解用户的意图，并选择适合任务执行的特定AI模型。每个后端模型特化于特定任务，其输出被聚合以满足用户的需求。为保证音乐凝聚，中央表格中维护了关键属性。我们通过 semi-structured 采访和问卷调查评估了提议的系统的有效性，并指出了其不仅能够促进音乐创作，还有潜在的应用于更广泛的领域。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/cs.CL_2023_10_19/" data-id="clpxp6bzj00dvee888t1f76pk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/cs.LG_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T10:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/cs.LG_2023_10_19/">cs.LG - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Deep-Learning-Analysis-of-Climate-Change-Innovation-and-Uncertainty"><a href="#A-Deep-Learning-Analysis-of-Climate-Change-Innovation-and-Uncertainty" class="headerlink" title="A Deep Learning Analysis of Climate Change, Innovation, and Uncertainty"></a>A Deep Learning Analysis of Climate Change, Innovation, and Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13200">http://arxiv.org/abs/2310.13200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Barnett, William Brock, Lars Peter Hansen, Ruimeng Hu, Joseph Huang</li>
<li>for: 这个论文旨在研究气候经济框架中模型不确定性的影响。</li>
<li>methods: 论文使用神经网络方法解决高维度非线性模型问题。</li>
<li>results: 研究发现模型不确定性对优化决策和社会价值具有首层影响。 accounting for climatic dynamics, economic damage from climate change, and the arrival of green technological change leads to significant adjustments to investment in different capital types in anticipation of technological change and the revelation of climate damage severity.<details>
<summary>Abstract</summary>
We study the implications of model uncertainty in a climate-economics framework with three types of capital: "dirty" capital that produces carbon emissions when used for production, "clean" capital that generates no emissions but is initially less productive than dirty capital, and knowledge capital that increases with R\&D investment and leads to technological innovation in green sector productivity. To solve our high-dimensional, non-linear model framework we implement a neural-network-based global solution method. We show there are first-order impacts of model uncertainty on optimal decisions and social valuations in our integrated climate-economic-innovation framework. Accounting for interconnected uncertainty over climate dynamics, economic damages from climate change, and the arrival of a green technological change leads to substantial adjustments to investment in the different capital types in anticipation of technological change and the revelation of climate damage severity.
</details>
<details>
<summary>摘要</summary>
我们研究模型不确定性在气候经济框架中的影响，这样有三种资产：“坏”资产（dirty capital）在生产时产生碳排放，“清洁”资产（clean capital）在初期不如“坏”资产生产力，但不产生排放，而“知识”资产（knowledge capital）透过研发投资增加技术创新，提高绿色领域生产力。为解决我们的高维度、非线性模型框架，我们实现了基于神经网络的全球解决方案。我们显示，在考虑气候动力学、气候变革对经济伤害和绿色技术变革的连接不确定性下，会出现首项影响并且有很大的调整投资不同类型的资产，以应对技术变革和气候伤害的揭露。
</details></li>
</ul>
<hr>
<h2 id="Heterogeneous-Graph-Neural-Networks-for-Data-driven-Traffic-Assignment"><a href="#Heterogeneous-Graph-Neural-Networks-for-Data-driven-Traffic-Assignment" class="headerlink" title="Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment"></a>Heterogeneous Graph Neural Networks for Data-driven Traffic Assignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13193">http://arxiv.org/abs/2310.13193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Hadi Meidani</li>
<li>for: 这篇论文是用于交通流分析的一种新方法的提议，以便更好地理解和管理各种交通系统。</li>
<li>methods: 这篇论文使用了不同链接之间的异质图 neural network 模型，以捕捉交通流的空间模式。</li>
<li>results: 数据实验表明，该模型能够快速收敛、减少训练损失，并且在预测交通流方面具有高度的准确性。此外，该模型还可以应用于不同的网络架构。<details>
<summary>Abstract</summary>
The traffic assignment problem is one of the significant components of traffic flow analysis for which various solution approaches have been proposed. However, deploying these approaches for large-scale networks poses significant challenges. In this paper, we leverage the power of heterogeneous graph neural networks to propose a novel data-driven approach for traffic assignment and traffic flow learning. The proposed model is capable of capturing spatial traffic patterns across different links, yielding highly accurate results. We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy. Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies. This approach offers a promising solution for complex traffic flow analysis and prediction, enhancing our understanding and management of a wide range of transportation systems.
</details>
<details>
<summary>摘要</summary>
traffic assignment problem 是 traffic flow analysis 中一个重要的 ком component，Various solution approaches 已经被提出。However，在 large-scale networks 上部署这些approaches  poses significant challenges。In this paper, we leverage the power of heterogeneous graph neural networks 来提出一种 novel data-driven approach for traffic assignment and traffic flow learning。The proposed model 可以 capture spatial traffic patterns across different links, yielding highly accurate results。We present numerical experiments on urban transportation networks and show that the proposed heterogeneous graph neural network model outperforms other conventional neural network models in terms of convergence rate, training loss, and prediction accuracy。Notably, the proposed heterogeneous graph neural network model can also be generalized to different network topologies。This approach offers a promising solution for complex traffic flow analysis and prediction，enhancing our understanding and management of a wide range of transportation systems。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Almost-Equivariance-via-Lie-Algebra-Convolutions"><a href="#Almost-Equivariance-via-Lie-Algebra-Convolutions" class="headerlink" title="Almost Equivariance via Lie Algebra Convolutions"></a>Almost Equivariance via Lie Algebra Convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13164">http://arxiv.org/abs/2310.13164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel McNeela</li>
<li>for: 这篇论文主要研究的是对群行为的模型Equivariance，但是强制模型遵循严格的Equivariance可能会导致模型在实际数据上表现不佳。因此，这篇论文研究了一种相关的主题——几乎Equivariance。</li>
<li>methods: 作者提出了一种新的几乎Equivariance定义，并给出了一种实际的方法来在模型中编码几乎Equivariance，通过利用 Lie GROUP 的 Lie algebra。作者还证明了几乎Equivariance和几乎Isometry之间的关系，并证明了几乎Equivariant manifold embeddings的存在性。</li>
<li>results: 作者通过对实际数据进行测试，证明了他们的方法的有效性。他们还证明了几乎Equivariance可以提供更好的模型表现，而不需要严格的Equivariance。<details>
<summary>Abstract</summary>
Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebra convolutions and demonstrate that they offer several benefits over Lie group convolutions, including being well-defined for non-compact groups. From there, we pivot to the realm of theory and demonstrate connections between the notions of equivariance and isometry and those of almost equivariance and almost isometry, respectively. We prove two existence theorems, one showing the existence of almost isometries within bounded distance of isometries of a general manifold, and another showing the converse for Hilbert spaces. We then extend these theorems to prove the existence of almost equivariant manifold embeddings within bounded distance of fully equivariant embedding functions, subject to certain constraints on the group action and the function class. Finally, we demonstrate the validity of our approach by benchmarking against datasets in fully equivariant and almost equivariant settings.
</details>
<details>
<summary>摘要</summary>
近期，对于一个群作用下的模型的等价性（equivariance）已经成为机器学习领域的重要研究话题。然而，具有特定群等价性的建筑限制了模型对数据变换的预期，而实际世界的数据通常不符合这种严格的等价性，可能因为数据中的噪声或实际physical laws encode only approximate/partial symmetries。在这种情况下，严格等价性的先验可能会导致模型在实际数据上下perform poorly。因此，在这个工作中，我们研究一个相关的话题：几乎等价性（almost equivariance）。我们提出了一种不同于现有文献中的定义，并给出了在 Lie algebra 中编码几乎等价性的实践方法。我们定义了 Lie algebra 混合并证明其在非紧Compact group 下是有用的。然后，我们转移到理论领域，与等价性和几何等价性之间的关系进行研究。我们证明了两个存在定理，其中一个显示在一般 manifold 上存在几乎几何同惯的，另一个则证明在希尔伯特空间上存在几何同惯的。我们然后推广这些定理，证明在满足某些群动作和函数类型的约束下，存在几乎等价的投影函数，并且这些函数在 bounded distance 内与完全等价的投影函数具有相似性。最后，我们通过对实际数据进行测试，证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-with-polynomial-activations-have-limited-expressivity"><a href="#Graph-Neural-Networks-with-polynomial-activations-have-limited-expressivity" class="headerlink" title="Graph Neural Networks with polynomial activations have limited expressivity"></a>Graph Neural Networks with polynomial activations have limited expressivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13139">http://arxiv.org/abs/2310.13139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sammy Khalife</li>
<li>for: 这篇论文探讨了使用图神经网络（GNNs）表示逻辑Query的可能性。</li>
<li>methods: 这篇论文使用了逻辑Query的两变量 fragments（GC2）来描述GNNs的表示能力。</li>
<li>results: 论文表明，使用某些activation function（如 polynomial activation function）无法表示GC2 queries，这意味着GNNs可以表示不同的逻辑Query，并答复了[Grohe, 2021]提出的问题。<details>
<summary>Abstract</summary>
The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo & Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
</details>
<details>
<summary>摘要</summary>
“Graph Neural Networks（GNNs）的表达能力可以完全用适当的 fragments of first-order logic 来描述。即任何GC2 query （graded modal logic的两变量 фрагмент）  interpret over 标记图可以通过一个具有仅仅取决于查询深度的 GNN 表示。根据 [Barcelo & Al., 2020, Grohe, 2021] 所指出，这种描述适用于一家 activation function 家族，从而存在一个基于 activation function 的 hierarchy of logics 可以由 GNNs 表示。在这篇文章中，我们证明 GC2 queries 不能由 polynomial activation functions 表示，这意味着存在一种 polynomial 和 popular non-polynomial activation functions（如 ReLU, sigmoid 和 hyperbolic tan 等）之间的分化，并回答了 [Grohe, 2021] 提出的一个问题。”
</details></li>
</ul>
<hr>
<h2 id="Mean-Estimation-Under-Heterogeneous-Privacy-Demands"><a href="#Mean-Estimation-Under-Heterogeneous-Privacy-Demands" class="headerlink" title="Mean Estimation Under Heterogeneous Privacy Demands"></a>Mean Estimation Under Heterogeneous Privacy Demands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13137">http://arxiv.org/abs/2310.13137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syomantak Chaudhuri, Konstantin Miagkov, Thomas A. Courtade</li>
<li>for: 本研究考虑了 differential privacy (DP) 框架中的各种用户隐私偏好。</li>
<li>methods: 我们提出了一种基于各自隐私需求的 mean estimation 算法，该算法是 minimax 优化的。</li>
<li>results: 我们的结果表明，在不同用户隐私偏好下，总体错误率随着最严格的用户隐私需求增长，而其他用户则得到了免费的隐私保障。<details>
<summary>Abstract</summary>
Differential Privacy (DP) is a well-established framework to quantify privacy loss incurred by any algorithm. Traditional formulations impose a uniform privacy requirement for all users, which is often inconsistent with real-world scenarios in which users dictate their privacy preferences individually. This work considers the problem of mean estimation, where each user can impose their own distinct privacy level. The algorithm we propose is shown to be minimax optimal and has a near-linear run-time. Our results elicit an interesting saturation phenomenon that occurs. Namely, the privacy requirements of the most stringent users dictate the overall error rates. As a consequence, users with less but differing privacy requirements are all given more privacy than they require, in equal amounts. In other words, these privacy-indifferent users are given a nontrivial degree of privacy for free, without any sacrifice in the performance of the estimator.
</details>
<details>
<summary>摘要</summary>
differential privacy (DP) 是一个已经成熟的框架，用于量化 алгоритмі中的隐私损失。传统的 формулювання对所有用户均强制一致的隐私要求，这经常与实际情况不符，用户们各自表达他们的隐私偏好。这个工作考虑到均值估计问题，每个用户可以单独表达自己的隐私水平。我们提出的算法被证明为最差最佳和时间复杂度几乎线性。我们的结果发现一个有趣的满足现象，即最严格的用户对全局的错误率给出了指令。因此，具有较弱隐私要求的用户们都被给予更多的隐私，但是不需要牺牲估计器的性能。即使这些隐私漏れ者用户不需要隐私，也会被给予一定的隐私，而不是完全没有隐私。
</details></li>
</ul>
<hr>
<h2 id="Approaches-for-Uncertainty-Quantification-of-AI-predicted-Material-Properties-A-Comparison"><a href="#Approaches-for-Uncertainty-Quantification-of-AI-predicted-Material-Properties-A-Comparison" class="headerlink" title="Approaches for Uncertainty Quantification of AI-predicted Material Properties: A Comparison"></a>Approaches for Uncertainty Quantification of AI-predicted Material Properties: A Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13136">http://arxiv.org/abs/2310.13136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesca Tavazza, Kamal Choudhary, Brian DeCost</li>
<li>for: 这个论文旨在研究机器学习模型预测材料性能时间的个体不确定性，并评估了三种简单实现的方法。</li>
<li>methods: 这个论文使用了三种方法来确定机器学习模型预测结果的个体不确定性，分别是Quantile方法、直接机器学习预测 интерval和Ensemble方法。</li>
<li>results: 研究发现，Quantile方法和Ensemble方法能够更好地预测机器学习模型预测结果的不确定性，而直接机器学习预测 интерval方法的性能较差。<details>
<summary>Abstract</summary>
The development of large databases of material properties, together with the availability of powerful computers, has allowed machine learning (ML) modeling to become a widely used tool for predicting material performances. While confidence intervals are commonly reported for such ML models, prediction intervals, i.e., the uncertainty on each prediction, are not as frequently available. Here, we investigate three easy-to-implement approaches to determine such individual uncertainty, comparing them across ten ML quantities spanning energetics, mechanical, electronic, optical, and spectral properties. Specifically, we focused on the Quantile approach, the direct machine learning of the prediction intervals and Ensemble methods.
</details>
<details>
<summary>摘要</summary>
大量物理属性数据的发展，加上强大计算机的可用性，使得机器学习（ML）模型成为了预测材料性能的广泛使用的工具。而每个预测的 uncertainty，即预测结果的不确定程度，并不是always report的。在这里，我们 investigate了三种容易实现的方法来确定这种个体uncertainty，对于十个ML量，其中包括能量、机械、电子、光学和spectral性质。我们专注于Quantile方法、直接机器学习预测间隔和Ensemble方法。
</details></li>
</ul>
<hr>
<h2 id="Fuel-Consumption-Prediction-for-a-Passenger-Ferry-using-Machine-Learning-and-In-service-Data-A-Comparative-Study"><a href="#Fuel-Consumption-Prediction-for-a-Passenger-Ferry-using-Machine-Learning-and-In-service-Data-A-Comparative-Study" class="headerlink" title="Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study"></a>Fuel Consumption Prediction for a Passenger Ferry using Machine Learning and In-service Data: A Comparative Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13123">http://arxiv.org/abs/2310.13123</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pagand/model_optimze_vessel">https://github.com/pagand/model_optimze_vessel</a></li>
<li>paper_authors: Pedram Agand, Allison Kennedy, Trevor Harris, Chanwoo Bae, Mo Chen, Edward J Park</li>
<li>for: 提高可持续交通的重要性，提供高效的海事运输方法是必要的。</li>
<li>methods: 使用实时操作数据和天气Condition forecasting的方法，选择合适的输入变量，避免过拟合、缺失数据和多icollinearity，并提供实用性。</li>
<li>results: 使用XGBoost技术建立的模型实现了最好的预测性能，可以帮助提高海事运输的能源效率。Here’s the full translation of the paper’s abstract in simplified Chinese:</li>
<li>for: 随着可持续交通的重要性提高，提供高效的海事运输方法是必要的。本文提出了一种基于实时操作数据和天气condition forecasting的方法，以提高海事运输的能源效率。</li>
<li>methods: 本文使用实时操作数据和天气condition forecasting的方法，选择合适的输入变量，避免过拟合、缺失数据和多icollinearity。其中，使用了统计学和域知识方法来选择输入变量，以保证模型的准确性和可靠性。</li>
<li>results: 本文使用XGBoost技术建立的模型实现了最好的预测性能，可以帮助提高海事运输的能源效率。模型的预测结果表明，通过对操作数据进行分析和预测，可以提高海事运输的能源效率和可持续性。I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
As the importance of eco-friendly transportation increases, providing an efficient approach for marine vessel operation is essential. Methods for status monitoring with consideration to the weather condition and forecasting with the use of in-service data from ships requires accurate and complete models for predicting the energy efficiency of a ship. The models need to effectively process all the operational data in real-time. This paper presents models that can predict fuel consumption using in-service data collected from a passenger ship. Statistical and domain-knowledge methods were used to select the proper input variables for the models. These methods prevent over-fitting, missing data, and multicollinearity while providing practical applicability. Prediction models that were investigated include multiple linear regression (MLR), decision tree approach (DT), an artificial neural network (ANN), and ensemble methods. The best predictive performance was from a model developed using the XGboost technique which is a boosting ensemble approach. \rvv{Our code is available on GitHub at \url{https://github.com/pagand/model_optimze_vessel/tree/OE} for future research.
</details>
<details>
<summary>摘要</summary>
Methods for status monitoring with consideration to the weather condition and forecasting with the use of in-service data from ships requires accurate and complete models for predicting the energy efficiency of a ship. 使用在船舶上collected的实时数据，需要一些准确而完整的模型，以预测船舶的能源效率。The models need to effectively process all the operational data in real-time. 这些模型需要在实时中有效地处理所有的操作数据。This paper presents models that can predict fuel consumption using in-service data collected from a passenger ship. 本文提出了使用实时数据收集自一艘客轮船的燃料消耗预测模型。Statistical and domain-knowledge methods were used to select the proper input variables for the models. 使用统计学和域知识方法选择模型的输入变量。These methods prevent over-fitting, missing data, and multicollinearity while providing practical applicability. 这些方法可以避免过拟合、缺失数据和多重相关性，同时提供实际应用性。Prediction models that were investigated include multiple linear regression (MLR), decision tree approach (DT), an artificial neural network (ANN), and ensemble methods. 研究的预测模型包括多重直线回归（MLR）、决策树方法（DT）、人工神经网络（ANN）和ensemble方法。The best predictive performance was from a model developed using the XGboost technique which is a boosting ensemble approach. 最佳预测性能来自一个使用XGboost技术开发的集成方法。\rvv{Our code is available on GitHub at \url{https://github.com/pagand/model_optimze_vessel/tree/OE} for future research. 我们的代码可以在GitHub上的\url{https://github.com/pagand/model_optimze_vessel/tree/OE}找到，以便未来的研究。}
</details></li>
</ul>
<hr>
<h2 id="SRAI-Towards-Standardization-of-Geospatial-AI"><a href="#SRAI-Towards-Standardization-of-Geospatial-AI" class="headerlink" title="SRAI: Towards Standardization of Geospatial AI"></a>SRAI: Towards Standardization of Geospatial AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13098">http://arxiv.org/abs/2310.13098</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kraina-ai/srai">https://github.com/kraina-ai/srai</a></li>
<li>paper_authors: Piotr Gramacki, Kacper Leśniara, Kamil Raczycki, Szymon Woźniak, Marcin Przymus, Piotr Szymański</li>
<li>for: 这个研究是用于探讨人工智能（AI）领域内的地ospatial数据处理方法。</li>
<li>methods: 这个研究使用了Python programming language的Spatial Representations for Artificial Intelligence（srai）库，可以下载地ospatial数据，分割给定区域为微区域，并训练嵌入模型使用不同的架构。</li>
<li>results: 这个研究可以实现地ospatial任务解决的完整管道，并且是首个将地ospatial AI领域工具集成成一个标准化的库。<details>
<summary>Abstract</summary>
Spatial Representations for Artificial Intelligence (srai) is a Python library for working with geospatial data. The library can download geospatial data, split a given area into micro-regions using multiple algorithms and train an embedding model using various architectures. It includes baseline models as well as more complex methods from published works. Those capabilities make it possible to use srai in a complete pipeline for geospatial task solving. The proposed library is the first step to standardize the geospatial AI domain toolset. It is fully open-source and published under Apache 2.0 licence.
</details>
<details>
<summary>摘要</summary>
spatial representations for artificial intelligence (SRai) 是一个 Python 库用于处理地ospatial 数据。该库可以下载地ospatial 数据，将给定区域分解成微区域使用多种算法，并使用不同的架构训练嵌入模型。它包括基线模型以及来自已发布作品的更复杂的方法。这些能力使得 SRai 可以在完整的地ospatial 任务解决管道中使用。提议的库是地ospatial AI 领域工具集的首个标准化步骤。它是完全开源的，并根据 Apache 2.0 许可证发布。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Stage-Temporal-Convolutional-Network-for-Volleyball-Jumps-Classification-Using-a-Waist-Mounted-IMU"><a href="#A-Multi-Stage-Temporal-Convolutional-Network-for-Volleyball-Jumps-Classification-Using-a-Waist-Mounted-IMU" class="headerlink" title="A Multi-Stage Temporal Convolutional Network for Volleyball Jumps Classification Using a Waist-Mounted IMU"></a>A Multi-Stage Temporal Convolutional Network for Volleyball Jumps Classification Using a Waist-Mounted IMU</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13097">http://arxiv.org/abs/2310.13097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meng Shang, Camilla De Bleecker, Jos Vanrenterghem, Roel De Ridder, Sabine Verschueren, Carolina Varon, Walter De Raedt, Bart Vanrumste</li>
<li>for: 这个研究是为了开发一种用单个遥感测量单元(IMU)来识别篮球运动员在训练或比赛中的跳跃类型的不侵入式系统。</li>
<li>methods: 这个研究使用了一种多层时间卷积神经网络(MS-TCN)来实现样本级别的分类。</li>
<li>results: 研究发现，使用单个IMU和MS-TCN模型可以准确地识别篮球运动员的跳跃类型，并且比现有的深度学习模型更具有计算效率。在实验中，模型在10名篮球运动员和26名篮球运动员的 lab session 和训练Session 中的表现均显示出优异的准确率。<details>
<summary>Abstract</summary>
Monitoring the number of jumps for volleyball players during training or a match can be crucial to prevent injuries, yet the measurement requires considerable workload and cost using traditional methods such as video analysis. Also, existing methods do not provide accurate differentiation between different types of jumps. In this study, an unobtrusive system with a single inertial measurement unit (IMU) on the waist was proposed to recognize the types of volleyball jumps. A Multi-Layer Temporal Convolutional Network (MS-TCN) was applied for sample-wise classification. The model was evaluated on ten volleyball players and twenty-six volleyball players, during a lab session with a fixed protocol of jumping and landing tasks, and during four volleyball training sessions, respectively. The MS-TCN model achieved better performance than a state-of-the-art deep learning model but with lower computational cost. In the lab sessions, most jump counts showed small differences between the predicted jumps and video-annotated jumps, with an overall count showing a Limit of Agreement (LoA) of 0.1+-3.40 (r=0.884). For comparison, the proposed algorithm showed slightly worse results than VERT (a commercial jumping assessment device) with a LoA of 0.1+-2.08 (r=0.955) but the differences were still within a comparable range. In the training sessions, the recognition of three types of jumps exhibited a mean difference from observation of less than 10 jumps: block, smash, and overhead serve. These results showed the potential of using a single IMU to recognize the types of volleyball jumps. The sample-wise architecture provided high resolution of recognition and the MS-TCN required fewer parameters to train compared with state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
监测排球运动员 durante 训练或比赛中的跳跃数量可能是预防伤害的关键，但传统方法 such as video分析需要较大的工作负担和成本。此外，现有的方法无法准确地区分不同类型的跳跃。本研究提出了一种不侵入式系统，使用单个吸收测量单元 (IMU) 在腰部进行跳跃类型识别。使用多层时间卷积网络 (MS-TCN) 进行样本WISE分类。模型在十名排球运动员和二十六名排球运动员 durante 实验室 session 中进行评估，并在四场排球训练Session中进行评估。MS-TCN 模型在与现有深度学习模型进行比较时表现更好，但计算成本较低。在实验室 session 中，大多数跳跃计数显示小差异 между预测跳跃和视频标注跳跃，总计异差为 0.1+-3.40（r=0.884）。相比之下，提posed algorithm 对 VERT (一种商业跳跃评估设备) 的表现略为差，异差为 0.1+-2.08（r=0.955），但差异仍在相对可接受范围内。在训练Session中，识别三种类型的跳跃显示平均差异少于 10 跳跃：封顶、击球和背靠击。这些结果表明了使用单个 IMU 可以准确地识别排球跳跃的类型。样本WISE架构提供了高分辨率的识别，而 MS-TCN 需要 fewer 参数进行训练，相比之下state-of-the-art模型。
</details></li>
</ul>
<hr>
<h2 id="Sequence-Length-Independent-Norm-Based-Generalization-Bounds-for-Transformers"><a href="#Sequence-Length-Independent-Norm-Based-Generalization-Bounds-for-Transformers" class="headerlink" title="Sequence Length Independent Norm-Based Generalization Bounds for Transformers"></a>Sequence Length Independent Norm-Based Generalization Bounds for Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13088">http://arxiv.org/abs/2310.13088</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/traugerjacob/transformer-gen-bounds">https://github.com/traugerjacob/transformer-gen-bounds</a></li>
<li>paper_authors: Jacob Trauger, Ambuj Tewari</li>
<li>for: 这个论文提供了不依赖输入序列长度的Transformer架构的 нор-based通用化 bound。</li>
<li>methods: 我们使用 Covering Number 基本法来证明我们的 bound。我们使用三个新的 Covering Number 上界来Upper bound Transformer的 Rademacher complexity。</li>
<li>results: 我们证明了这个通用化 bound 适用于常见的Transformer训练技术中的masking和预测masked word。我们也在一个干扰多数据集上进行了实验验证我们的理论发现。<details>
<summary>Abstract</summary>
This paper provides norm-based generalization bounds for the Transformer architecture that do not depend on the input sequence length. We employ a covering number based approach to prove our bounds. We use three novel covering number bounds for the function class of bounded linear transformations to upper bound the Rademacher complexity of the Transformer. Furthermore, we show this generalization bound applies to the common Transformer training technique of masking and then predicting the masked word. We also run a simulated study on a sparse majority data set that empirically validates our theoretical findings.
</details>
<details>
<summary>摘要</summary>
Note:* "Transformer" architecture 改为 "Transformer 架构"* "input sequence length" 改为 "输入序列长度"* "Rademacher complexity" 改为 "拉德贝克复杂度"* "masked word" 改为 "遮盖的单词"* "sparse majority data set" 改为 "稀疏多数数据集"
</details></li>
</ul>
<hr>
<h2 id="How-Can-Everyday-Users-Efficiently-Teach-Robots-by-Demonstrations"><a href="#How-Can-Everyday-Users-Efficiently-Teach-Robots-by-Demonstrations" class="headerlink" title="How Can Everyday Users Efficiently Teach Robots by Demonstrations?"></a>How Can Everyday Users Efficiently Teach Robots by Demonstrations?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13083">http://arxiv.org/abs/2310.13083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maram Sakr, Zhikai Zhang, Benjamin Li, Haomiao Zhang, H. F. Machiel Van der Loos, Dana Kulic, Elizabeth Croft</li>
<li>for: 这项研究的目的是提高机器人学习效率，并且帮助机器人更好地泛化到任务变化中。</li>
<li>methods: 这项研究使用了 uncertainty 度量，即任务相关信息熵，作为建议教师提供示范例子的标准。</li>
<li>results: 研究发现，使用提议的示范例子可以提高机器人学习效率，并且可以提高机器人在新任务上的泛化能力。  compared to 一个现有的经验规则，提议的方法可以提高机器人学习效率 by 210%。<details>
<summary>Abstract</summary>
Learning from Demonstration (LfD) is a framework that allows lay users to easily program robots. However, the efficiency of robot learning and the robot's ability to generalize to task variations hinges upon the quality and quantity of the provided demonstrations. Our objective is to guide human teachers to furnish more effective demonstrations, thus facilitating efficient robot learning. To achieve this, we propose to use a measure of uncertainty, namely task-related information entropy, as a criterion for suggesting informative demonstration examples to human teachers to improve their teaching skills. In a conducted experiment (N=24), an augmented reality (AR)-based guidance system was employed to train novice users to produce additional demonstrations from areas with the highest entropy within the workspace. These novice users were trained for a few trials to teach the robot a generalizable task using a limited number of demonstrations. Subsequently, the users' performance after training was assessed first on the same task (retention) and then on a novel task (transfer) without guidance. The results indicated a substantial improvement in robot learning efficiency from the teacher's demonstrations, with an improvement of up to 198% observed on the novel task. Furthermore, the proposed approach was compared to a state-of-the-art heuristic rule and found to improve robot learning efficiency by 210% compared to the heuristic rule.
</details>
<details>
<summary>摘要</summary>
学习从示例（LfD）是一种框架，允许非专业人员轻松编程机器人。然而，机器人学习效率和任务变化的泛化能力受示例质量和量的限制。我们的目标是指导人类教师提供更有效的示例，以便促进机器人快速学习。为 достичь这一目标，我们提议使用任务相关信息熵作为教学示例选择的依据。在一项实验中（N=24），我们使用了增强现实（AR）基本的指导系统，让新手用户在工作空间中最高 entropy 的区域内提供更多示例。这些新手用户在几个训练周期后，通过限制数量的示例教育机器人一个通用任务。然后，用户的性能被评估在同一个任务上（退 Reserve）和一个新任务上（转移）无指导。结果表明，提posed方法可以大幅提高机器人学习效率，最高提高达198%在新任务上。此外，我们的方法与现有的一种有效规则进行比较，发现可以提高机器人学习效率210%。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computational-Complexities-of-Complex-valued-Neural-Networks"><a href="#On-the-Computational-Complexities-of-Complex-valued-Neural-Networks" class="headerlink" title="On the Computational Complexities of Complex-valued Neural Networks"></a>On the Computational Complexities of Complex-valued Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13075">http://arxiv.org/abs/2310.13075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kayol Soares Mayer, Jonathan Aguiar Soares, Ariadne Arrais Cruz, Dalton Soares Arantes</li>
<li>for: 这篇论文主要是用于研究复数神经网络（CVNN）的计算复杂性。</li>
<li>methods: 这篇论文使用了复数神经网络（CVNN）来处理复数域数据的数字信号处理。与实数神经网络（RVNN）相比，CVNN可以直接处理复数输入和输出信号，这是因为它们的复数域参数和活化函数。</li>
<li>results: 这篇论文提出了CVNN的量化和极限计算复杂性分析方法，这些方法可以准确地估算CVNN的计算复杂性。这些方法基于实数乘法的数量，这是计算复杂性的主要限制因素。此外，这篇论文还对一些相关研究中提出的CVNN计算复杂性进行了 investigate。<details>
<summary>Abstract</summary>
Complex-valued neural networks (CVNNs) are nonlinear filters used in the digital signal processing of complex-domain data. Compared with real-valued neural networks~(RVNNs), CVNNs can directly handle complex-valued input and output signals due to their complex domain parameters and activation functions. With the trend toward low-power systems, computational complexity analysis has become essential for measuring an algorithm's power consumption. Therefore, this paper presents both the quantitative and asymptotic computational complexities of CVNNs. This is a crucial tool in deciding which algorithm to implement. The mathematical operations are described in terms of the number of real-valued multiplications, as these are the most demanding operations. To determine which CVNN can be implemented in a low-power system, quantitative computational complexities can be used to accurately estimate the number of floating-point operations. We have also investigated the computational complexities of CVNNs discussed in some studies presented in the literature.
</details>
<details>
<summary>摘要</summary>
复杂值神经网络（CVNN）是非线性滤波器，用于数字信号处理中的复杂频域数据。与实数值神经网络（RVNN）相比，CVNN可以直接处理复杂值输入和输出信号，这是因为它们的复杂频域参数和活动函数。随着低功耗系统的趋势，计算复杂性分析已成为选择算法的关键工具。因此，本文提供了 CVNN 的量化和极限计算复杂性分析。这是准确估计实现低功耗系统中的算法所需的浮点运算数量的关键工具。我们还 investigate了一些 literatures 中关于 CVNN 的计算复杂性分析。
</details></li>
</ul>
<hr>
<h2 id="To-grok-or-not-to-grok-Disentangling-generalization-and-memorization-on-corrupted-algorithmic-datasets"><a href="#To-grok-or-not-to-grok-Disentangling-generalization-and-memorization-on-corrupted-algorithmic-datasets" class="headerlink" title="To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets"></a>To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13061">http://arxiv.org/abs/2310.13061</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d-doshi/Grokking">https://github.com/d-doshi/Grokking</a></li>
<li>paper_authors: Darshil Doshi, Aritra Das, Tianyu He, Andrey Gromov</li>
<li>for: This paper aims to address the challenge of robust generalization in deep learning, specifically when the number of trainable parameters is very large.</li>
<li>methods: The authors use two-layer neural networks trained on modular arithmetic tasks with corrupted labels, and study the effect of regularization methods such as weight decay, dropout, and BatchNorm on the network’s ability to generalize.</li>
<li>results: The authors show that regularization methods can force the network to ignore corrupted data during optimization, achieving $100%$ accuracy on the uncorrupted dataset. They also demonstrate that the effect of these regularization methods is interpretable, and that the training dynamics involve two consecutive stages: first, the network undergoes the “grokking” dynamics reaching high train and test accuracy, and second, it unlearns the memorizing representations.<details>
<summary>Abstract</summary>
Robust generalization is a major challenge in deep learning, particularly when the number of trainable parameters is very large. In general, it is very difficult to know if the network has memorized a particular set of examples or understood the underlying rule (or both). Motivated by this challenge, we study an interpretable model where generalizing representations are understood analytically, and are easily distinguishable from the memorizing ones. Namely, we consider two-layer neural networks trained on modular arithmetic tasks where ($\xi \cdot 100\%$) of labels are corrupted (\emph{i.e.} some results of the modular operations in the training set are incorrect). We show that (i) it is possible for the network to memorize the corrupted labels \emph{and} achieve $100\%$ generalization at the same time; (ii) the memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data; (iii) regularization methods such as weight decay, dropout and BatchNorm force the network to ignore the corrupted data during optimization, and achieve $100\%$ accuracy on the uncorrupted dataset; and (iv) the effect of these regularization methods is (``mechanistically'') interpretable: weight decay and dropout force all the neurons to learn generalizing representations, while BatchNorm de-amplifies the output of memorizing neurons and amplifies the output of the generalizing ones. Finally, we show that in the presence of regularization, the training dynamics involves two consecutive stages: first, the network undergoes the \emph{grokking} dynamics reaching high train \emph{and} test accuracy; second, it unlearns the memorizing representations, where train accuracy suddenly jumps from $100\%$ to $100 (1-\xi)\%$.
</details>
<details>
<summary>摘要</summary>
深度学习中的稳健泛化是一个主要挑战，特别是当训练参数的数量非常大时。通常，很难判断网络是否已经记忆了特定的示例集或者理解了下面的规则（或者都是）。为了解决这个挑战，我们研究了一种可解释的模型，其中泛化表示可以分析地理解。具体来说，我们考虑了两层神经网络，在模块加法任务上训练，其中($\xi \cdot 100\%$)的标签有误（即训练集中的模块操作结果有误）。我们发现：1. 网络可以同时记忆损害的标签并达到100%的泛化率;2. 记忆神经可以被识别并剔除，使网络的准确率在损害数据上降低，并在不损害数据上提高准确率;3. 训练过程中的正则化方法，如权重衰减、Dropout和BatchNorm，会让网络在优化过程中忽略损害数据，并在不损害数据上达到100%的准确率;4. 这些正则化方法的效果是("机械")可解释的：权重衰减和Dropout让所有神经学习泛化表示，而BatchNorm减小了记忆神经的输出，并增大了泛化神经的输出。最后，我们发现在正则化的情况下，训练过程包括两个阶段：首先，网络进行了“感知”动力学，达到高训练精度和测试精度;第二，它忘记了记忆表示，训练精度 suddenly从100%降低到100(1-$\xi)$%。
</details></li>
</ul>
<hr>
<h2 id="Demystifying-the-Myths-and-Legends-of-Nonconvex-Convergence-of-SGD"><a href="#Demystifying-the-Myths-and-Legends-of-Nonconvex-Convergence-of-SGD" class="headerlink" title="Demystifying the Myths and Legends of Nonconvex Convergence of SGD"></a>Demystifying the Myths and Legends of Nonconvex Convergence of SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12969">http://arxiv.org/abs/2310.12969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aritra Dutta, El Houcine Bergou, Soumia Boucherouite, Nicklas Werge, Melih Kandemir, Xin Li</li>
<li>for: 这篇论文旨在研究杂次 gradient descent（SGD）和其变种在解决大规模优化问题时的性能。</li>
<li>methods: 本论文使用了SGD和其变种，并对这些方法的非对称优化问题进行了研究。</li>
<li>results: 本论文显示了SGD在非对称优化问题中的$\epsilon$-定点存在，并且可以在大 enough的迭代次数T下确定这些定点。此外，本论文还能量量 $\epsilon$-定点在SGD的最终迭代中的密度，并且在不同的函数对象和精度下可以恢复类传统的$O(\frac{1}{\sqrt{T})$的下降率。<details>
<summary>Abstract</summary>
Stochastic gradient descent (SGD) and its variants are the main workhorses for solving large-scale optimization problems with nonconvex objective functions. Although the convergence of SGDs in the (strongly) convex case is well-understood, their convergence for nonconvex functions stands on weak mathematical foundations. Most existing studies on the nonconvex convergence of SGD show the complexity results based on either the minimum of the expected gradient norm or the functional sub-optimality gap (for functions with extra structural property) by searching the entire range of iterates. Hence the last iterations of SGDs do not necessarily maintain the same complexity guarantee. This paper shows that an $\epsilon$-stationary point exists in the final iterates of SGDs, given a large enough total iteration budget, $T$, not just anywhere in the entire range of iterates -- a much stronger result than the existing one. Additionally, our analyses allow us to measure the density of the $\epsilon$-stationary points in the final iterates of SGD, and we recover the classical $O(\frac{1}{\sqrt{T})$ asymptotic rate under various existing assumptions on the objective function and the bounds on the stochastic gradient. As a result of our analyses, we addressed certain myths and legends related to the nonconvex convergence of SGD and posed some thought-provoking questions that could set new directions for research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PAC-Prediction-Sets-Under-Label-Shift"><a href="#PAC-Prediction-Sets-Under-Label-Shift" class="headerlink" title="PAC Prediction Sets Under Label Shift"></a>PAC Prediction Sets Under Label Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12964">http://arxiv.org/abs/2310.12964</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/averysi224/pac-ps-label-shift">https://github.com/averysi224/pac-ps-label-shift</a></li>
<li>paper_authors: Wenwen Si, Sangdon Park, Insup Lee, Edgar Dobriban, Osbert Bastani</li>
<li>for: 这个论文旨在提出一种在标签Shift Setting下构建预测集的算法，以保证预测集中的真实标签具有高概率包含真实标签。</li>
<li>methods: 该算法使用了预测概率的估计和混淆矩阵的估计，然后通过高斯消元法传递这些估计的不确定性，最后使用这些间隔来构建预测集。</li>
<li>results: 在五个 dataset上测试了该算法，与多个基线比较，发现该算法可以满足PAC保证，同时生成更小、更有用的预测集。<details>
<summary>Abstract</summary>
Prediction sets capture uncertainty by predicting sets of labels rather than individual labels, enabling downstream decisions to conservatively account for all plausible outcomes. Conformal inference algorithms construct prediction sets guaranteed to contain the true label with high probability. These guarantees fail to hold in the face of distribution shift, which is precisely when reliable uncertainty quantification can be most useful. We propose a novel algorithm for constructing prediction sets with PAC guarantees in the label shift setting. This method estimates the predicted probabilities of the classes in a target domain, as well as the confusion matrix, then propagates uncertainty in these estimates through a Gaussian elimination algorithm to compute confidence intervals for importance weights. Finally, it uses these intervals to construct prediction sets. We evaluate our approach on five datasets: the CIFAR-10, ChestX-Ray and Entity-13 image datasets, the tabular CDC Heart dataset, and the AGNews text dataset. Our algorithm satisfies the PAC guarantee while producing smaller, more informative, prediction sets compared to several baselines.
</details>
<details>
<summary>摘要</summary>
预测集合 capture uncertainty by predicting sets of labels instead of individual labels, allowing downstream decisions to conservatively account for all plausible outcomes.  конформаль inference algorithms construct prediction sets guaranteed to contain the true label with high probability. However, these guarantees fail to hold in the face of distribution shift, which is precisely when reliable uncertainty quantification can be most useful. We propose a novel algorithm for constructing prediction sets with PAC guarantees in the label shift setting. This method estimates the predicted probabilities of the classes in a target domain, as well as the confusion matrix, then propagates uncertainty in these estimates through a Gaussian elimination algorithm to compute confidence intervals for importance weights. Finally, it uses these intervals to construct prediction sets. We evaluate our approach on five datasets: CIFAR-10, ChestX-Ray and Entity-13 image datasets, the tabular CDC Heart dataset, and the AGNews text dataset. Our algorithm satisfies the PAC guarantee while producing smaller, more informative, prediction sets compared to several baselines.
</details></li>
</ul>
<hr>
<h2 id="Cousins-Of-The-Vendi-Score-A-Family-Of-Similarity-Based-Diversity-Metrics-For-Science-And-Machine-Learning"><a href="#Cousins-Of-The-Vendi-Score-A-Family-Of-Similarity-Based-Diversity-Metrics-For-Science-And-Machine-Learning" class="headerlink" title="Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning"></a>Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12952">http://arxiv.org/abs/2310.12952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vertaix/vendi-score">https://github.com/vertaix/vendi-score</a></li>
<li>paper_authors: Amey P. Pasarkar, Adji Bousso Dieng</li>
<li>for: 这 paper 是为了提出一种新的多样性度量 metric，用于Machine Learning（ML）、生态学和化学等领域。</li>
<li>methods: 这 paper 使用了类似性基于的 Hill 数列扩展，从量子统计力学中借鉴了一些想法，以便更好地衡量多样性。</li>
<li>results: 这 paper 在一种控制的synthetic setting中研究了这种新的多样性度量的属性，并在分子模拟中进行了实验，以及用于更好地理解图像生成模型中的 memorization、duplication、多样性和样本质量等问题。<details>
<summary>Abstract</summary>
Measuring diversity accurately is important for many scientific fields, including machine learning (ML), ecology, and chemistry. The Vendi Score was introduced as a generic similarity-based diversity metric that extends the Hill number of order q=1 by leveraging ideas from quantum statistical mechanics. Contrary to many diversity metrics in ecology, the Vendi Score accounts for similarity and does not require knowledge of the prevalence of the categories in the collection to be evaluated for diversity. However, the Vendi Score treats each item in a given collection with a level of sensitivity proportional to the item's prevalence. This is undesirable in settings where there is a significant imbalance in item prevalence. In this paper, we extend the other Hill numbers using similarity to provide flexibility in allocating sensitivity to rare or common items. This leads to a family of diversity metrics -- Vendi scores with different levels of sensitivity -- that can be used in a variety of applications. We study the properties of the scores in a synthetic controlled setting where the ground truth diversity is known. We then test their utility in improving molecular simulations via Vendi Sampling. Finally, we use the Vendi scores to better understand the behavior of image generative models in terms of memorization, duplication, diversity, and sample quality.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Measuring diversity accurately" is 精确地测量多样性 (jīngjì de cèngjí de tiǎoyòng xìng)* "Vendi Score" is 维度分数 (wéidù fānsù)* "Hill number of order q=1" is 邦顿数量 q=1 (bāngdòng xiàngliàng q=1)* "similarity" is 相似性 (xiāngsiǒngxìng)* "prevalence" is 存在率 (cúnzài xìng)* "sensitivity" is 敏感度 (mǐnán dé)* "family of diversity metrics" is 多样性指标的家族 (duōyàng xìng zhǐbǎi de jiāfu)* "Vendi Sampling" is 维度采样 (wéidù qiǎo yǎn)* "image generative models" is 图像生成模型 (túxiàng shēngchǎng módelì)* "memorization" is 记忆 (jìyì)* "duplication" is 复制 (fùzhì)* "diversity" is 多样性 (duōyàng xìng)* "sample quality" is 样本质量 (yàngběn zhìliàng)
</details></li>
</ul>
<hr>
<h2 id="Generative-Flow-Networks-as-Entropy-Regularized-RL"><a href="#Generative-Flow-Networks-as-Entropy-Regularized-RL" class="headerlink" title="Generative Flow Networks as Entropy-Regularized RL"></a>Generative Flow Networks as Entropy-Regularized RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12934">http://arxiv.org/abs/2310.12934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d-tiapkin/gflownet-rl">https://github.com/d-tiapkin/gflownet-rl</a></li>
<li>paper_authors: Daniil Tiapkin, Nikita Morozov, Alexey Naumov, Dmitry Vetrov</li>
<li>for: 这个论文旨在训练一个政策来采样可组合的抽象对象，使其概率与给定的奖励相对应。</li>
<li>methods: 这个论文使用了 reinforcement learning（RL）的方法，并且将其应用到了一般情况中。</li>
<li>results: 研究人员在这篇论文中展示了如何将生成流网络的学习任务重新定义为一个 entropy-regularized RL 问题，并且通过使用标准软RL算法来训练 GFlowNet。结果表明，这种方法可以与已知 GFlowNet 训练方法相比肩，而且可以在几种概率模型任务中实现实用的性能。<details>
<summary>Abstract</summary>
The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating reinforcement learning principles into the realm of generative flow networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Probabilistic-Modeling-of-Human-Teams-to-Infer-False-Beliefs"><a href="#Probabilistic-Modeling-of-Human-Teams-to-Infer-False-Beliefs" class="headerlink" title="Probabilistic Modeling of Human Teams to Infer False Beliefs"></a>Probabilistic Modeling of Human Teams to Infer False Beliefs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12929">http://arxiv.org/abs/2310.12929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paulo Soares, Adarsh Pyarelal, Kobus Barnard<br>for:这篇论文旨在开发一种概率图示模型（PGM），用于人工智能（AI）代理人在模拟城市搜索和救援（USAR）场景中推断人类信念。methods:这种PGM方法使 observable states和 actions 明确，以及信念和意图基于时间上的观察数据。这种方法还支持推断干预的效果，这些干预是AI代理人与人类团队合作的关键。results:实验包括玩家知识的 manipulate，并在虚拟 Minecraft 环境中提供了多个信息流，包括玩家视野中的物品。参与者使用一组标记块来标注房间内的受害人存在或缺失，并且在团队中有一个成员被分配不同的标记启用，可能误导他们关于房间状态的信念。我们从前一些相关的工作中扩展，引入 ToMCAT，一个可以理解个人和共享心理状态的 AI 代理人。我们发现玩家的行为受到他们在游戏视野中看到的内容、标记的含义和团队决定的信念的影响。此外，我们发现 ToMCAT 的信念与玩家行为相符，并且它可以准确地推断 false belief ，并且与人类观察者相比，其推断精度高于随机并且相当于人类观察者。<details>
<summary>Abstract</summary>
We develop a probabilistic graphical model (PGM) for artificially intelligent (AI) agents to infer human beliefs during a simulated urban search and rescue (USAR) scenario executed in a Minecraft environment with a team of three players. The PGM approach makes observable states and actions explicit, as well as beliefs and intentions grounded by evidence about what players see and do over time. This approach also supports inferring the effect of interventions, which are vital if AI agents are to assist human teams. The experiment incorporates manipulations of players' knowledge, and the virtual Minecraft-based testbed provides access to several streams of information, including the objects in the players' field of view. The participants are equipped with a set of marker blocks that can be placed near room entrances to signal the presence or absence of victims in the rooms to their teammates. In each team, one of the members is given a different legend for the markers than the other two, which may mislead them about the state of the rooms; that is, they will hold a false belief. We extend previous works in this field by introducing ToMCAT, an AI agent that can reason about individual and shared mental states. We find that the players' behaviors are affected by what they see in their in-game field of view, their beliefs about the meaning of the markers, and their beliefs about which meaning the team decided to adopt. In addition, we show that ToMCAT's beliefs are consistent with the players' actions and that it can infer false beliefs with accuracy significantly better than chance and comparable to inferences made by human observers.
</details>
<details>
<summary>摘要</summary>
我们开发了一个 probabilistic graphical model（PGM），用于人工智能（AI）代理人在模拟城市搜索和救援（USAR）场景中推断人类信念。PGM方法使 observable states 和 actions 显示出来，同时也使 beliefs 和 intention 根据观察到的事实附加下附加。这种方法还支持推断 intervene，这些 intervene 非常重要，如果 AI 代理人想要协助人类团队。实验包括玩家知识的操作，并且在虚拟 Minecraft 环境中提供了多个信息流，包括玩家视野中的物体。参与者被装备了一组标记块，可以在房间入口处置标记，以signal 房间中的病人存在或缺失。在每个团队中，一个成员被给予不同的征legend than the other two，这可能会误导他们房间的状态，即他们将保持false belief。我们在这一Field extrapolate previous works，引入 ToMCAT，一个能理解个人和共享 mental state 的 AI 代理人。我们发现玩家的行为受到他们在游戏视野中看到的内容、标记的含义和团队决定的含义的影响。此外，我们发现 ToMCAT 的信念与玩家行为相符，并且它可以准确地推断 false belief ，比 randomly chance 和人类观察者的推断更高。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Open-World-Bacterial-Raman-Spectra-Identification-by-Feature-Regularization-for-Improved-Resilience-against-Unknown-Classes"><a href="#Enhancing-Open-World-Bacterial-Raman-Spectra-Identification-by-Feature-Regularization-for-Improved-Resilience-against-Unknown-Classes" class="headerlink" title="Enhancing Open-World Bacterial Raman Spectra Identification by Feature Regularization for Improved Resilience against Unknown Classes"></a>Enhancing Open-World Bacterial Raman Spectra Identification by Feature Regularization for Improved Resilience against Unknown Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13723">http://arxiv.org/abs/2310.13723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/balytskyijaroslaw/pathogensramanopenset">https://github.com/balytskyijaroslaw/pathogensramanopenset</a></li>
<li>paper_authors: Yaroslav Balytskyi, Nataliia Kalashnyk, Inna Hubenko, Alina Balytska, Kelly McNear<br>for:* 这个研究旨在应用深度学习技术和拉曼спектроскоPY结合，精确地识别临床环境中的病原菌。methods:* 使用了现有的关键字集合和对称网络架构，以及注意力机制来提高模型的准确性。results:* 模型的准确性提高至87.8±0.1%，比最佳可用模型的准确性高出1.1%。* 透过特征规范化，模型能够高效地识别已知的病原菌，并将未知样本分类为不明菌，降低了伪阳性率。* 在测试阶段，模型能够有效地检测未知的菌种，提高了检测结果的可靠性。<details>
<summary>Abstract</summary>
The combination of Deep Learning techniques and Raman spectroscopy shows great potential offering precise and prompt identification of pathogenic bacteria in clinical settings. However, the traditional closed-set classification approaches assume that all test samples belong to one of the known pathogens, and their applicability is limited since the clinical environment is inherently unpredictable and dynamic, unknown or emerging pathogens may not be included in the available catalogs. We demonstrate that the current state-of-the-art Neural Networks identifying pathogens through Raman spectra are vulnerable to unknown inputs, resulting in an uncontrollable false positive rate. To address this issue, first, we developed a novel ensemble of ResNet architectures combined with the attention mechanism which outperforms existing closed-world methods, achieving an accuracy of $87.8 \pm 0.1\%$ compared to the best available model's accuracy of $86.7 \pm 0.4\%$. Second, through the integration of feature regularization by the Objectosphere loss function, our model achieves both high accuracy in identifying known pathogens from the catalog and effectively separates unknown samples drastically reducing the false positive rate. Finally, the proposed feature regularization method during training significantly enhances the performance of out-of-distribution detectors during the inference phase improving the reliability of the detection of unknown classes. Our novel algorithm for Raman spectroscopy enables the detection of unknown, uncatalogued, and emerging pathogens providing the flexibility to adapt to future pathogens that may emerge, and has the potential to improve the reliability of Raman-based solutions in dynamic operating environments where accuracy is critical, such as public safety applications.
</details>
<details>
<summary>摘要</summary>
“深度学习技术和拉曼光谱结合显示了巨大的潜力，可以提供精准和快速的病菌识别在临床环境中。然而，传统的闭式分类方法假设所有测试样本都属于已知的病菌，其可靠性有限，因为临床环境是自然不可预测的和动态的。未知或emerging病菌可能不包括在可用目录中。我们示示了当前状态的artificial neural networks通过拉曼光谱来识别病菌是容易受到未知输入的影响，导致false positive rate不可控。为解决这个问题，我们首先开发了一种基于ResNet架构的新型ensemble，并添加了注意力机制，该模型在闭式世界方法中超越了最佳可用模型的准确率($87.8\pm 0.1\%$ vs $86.7\pm 0.4\%$）。其次，通过对特征REG regularization的integrated Objectosphere损失函数，我们的模型可以同时具有高准确率对已知病菌目录中的识别和有效地分离未知样本，从而减少false positive rate。最后，在训练阶段通过特征REG regularization，我们的模型在推理阶段的out-of-distribution检测器性能得到了显著提升，从而提高了检测未知类别的可靠性。我们的新算法可以检测未知、未目录和emerging病菌，提供了适应未来病菌的灵活性，并且在准确是关键的公共安全应用中具有潜在的优势。”
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-GPUs-on-SVBRDF-Extractor-Model"><a href="#Benchmarking-GPUs-on-SVBRDF-Extractor-Model" class="headerlink" title="Benchmarking GPUs on SVBRDF Extractor Model"></a>Benchmarking GPUs on SVBRDF Extractor Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19816">http://arxiv.org/abs/2310.19816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Narayan Kandel, Melanie Lambert</li>
<li>for: 本研究旨在选择适合特定任务的GPU，以实现优化性能。</li>
<li>methods: 本文使用了多种GPU benchmarking方法，包括常见的Synthetic Dataset和Real-world Dataset。</li>
<li>results: 研究发现，不同GPU在处理大输入图像（256x256）的神经网络模型时的性能差异较大。<details>
<summary>Abstract</summary>
With the maturity of deep learning, its use is emerging in every field. Also, as different types of GPUs are becoming more available in the markets, it creates a difficult decision for users. How can users select GPUs to achieve optimal performance for a specific task? Analysis of GPU architecture is well studied, but existing works that benchmark GPUs do not study tasks for networks with significantly larger input. In this work, we tried to differentiate the performance of different GPUs on neural network models that operate on bigger input images (256x256).
</details>
<details>
<summary>摘要</summary>
使深度学习技术成熟，它在各个领域中得到应用。然而，由于不同类型的GPU在市场上更加可用，这创造了用户选择GPU实现特定任务优化性能的困难决策。虽然对GPU架构分析已经得到了广泛的研究，但现有的GPU benchmark工作并不研究处理大量输入的网络。在这个工作中，我们尝试了对不同GPU的 neural network 模型在大小为256x256的输入图像上的性能进行比较。注：使用 Simplified Chinese 翻译，以下是简化中文版本。
</details></li>
</ul>
<hr>
<h2 id="Blind-quantum-machine-learning-with-quantum-bipartite-correlator"><a href="#Blind-quantum-machine-learning-with-quantum-bipartite-correlator" class="headerlink" title="Blind quantum machine learning with quantum bipartite correlator"></a>Blind quantum machine learning with quantum bipartite correlator</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12893">http://arxiv.org/abs/2310.12893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changhao Li, Boning Li, Omar Amer, Ruslan Shaydulin, Shouvanik Chakrabarti, Guoqing Wang, Haowei Xu, Hao Tang, Isidor Schoch, Niraj Kumar, Charles Lim, Ju Li, Paola Cappellaro, Marco Pistoia</li>
<li>for: 这个研究旨在提供一种基于量子分布式计算的隐私保护机制，以保持数据的隐私和可信worthiness。</li>
<li>methods: 本研究使用了量子生成器算法，并提出了一种新的盲目量子机器学习协议，它具有减少通信开销的特点，同时保持数据的隐私。</li>
<li>results: 研究人员通过了复杂度和隐私分析，证明了该协议的有效性。这些发现打开了新的可能性，使量子技术 era 中的隐私意识应用得到进一步发展。<details>
<summary>Abstract</summary>
Distributed quantum computing is a promising computational paradigm for performing computations that are beyond the reach of individual quantum devices. Privacy in distributed quantum computing is critical for maintaining confidentiality and protecting the data in the presence of untrusted computing nodes. In this work, we introduce novel blind quantum machine learning protocols based on the quantum bipartite correlator algorithm. Our protocols have reduced communication overhead while preserving the privacy of data from untrusted parties. We introduce robust algorithm-specific privacy-preserving mechanisms with low computational overhead that do not require complex cryptographic techniques. We then validate the effectiveness of the proposed protocols through complexity and privacy analysis. Our findings pave the way for advancements in distributed quantum computing, opening up new possibilities for privacy-aware machine learning applications in the era of quantum technologies.
</details>
<details>
<summary>摘要</summary>
分布式量子计算是一种有前途的计算模式，可以执行个人量子设备无法完成的计算任务。在分布式量子计算中，隐私是 kritical的，以保持数据的隐私和保护数据在不可信计算节点存在下。在这种工作中，我们引入了新的盲目量子机器学习协议，基于量子二重相关算法。我们的协议具有减少的通信负担，同时保持数据来自不可信方的隐私。我们引入了较强的算法特定隐私保护机制，不需要复杂的加密技术，计算负担较低。我们验证了我们的协议的有效性通过复杂度和隐私分析。我们的发现开 up了分布式量子计算的新可能性，为隐私意识的量子技术应用奠定基础。
</details></li>
</ul>
<hr>
<h2 id="Fine-Tuning-Generative-Models-as-an-Inference-Method-for-Robotic-Tasks"><a href="#Fine-Tuning-Generative-Models-as-an-Inference-Method-for-Robotic-Tasks" class="headerlink" title="Fine-Tuning Generative Models as an Inference Method for Robotic Tasks"></a>Fine-Tuning Generative Models as an Inference Method for Robotic Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12862">http://arxiv.org/abs/2310.12862</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orrkrup/mace">https://github.com/orrkrup/mace</a></li>
<li>paper_authors: Orr Krupnik, Elisei Shafer, Tom Jurgenson, Aviv Tamar</li>
<li>for: 本研究旨在开发一种能够快速适应新和不同条件的机器人agent，使其在真实世界中更加灵活和有效。</li>
<li>methods: 我们建立在最近几年内的深度生成模型技术上，并利用现代GPU加速器，进行快速适应神经网络模型的样本生成。我们提出了一种简单而普遍的方法，可以应用于不同的深度生成模型和机器人环境。关键思想是通过快速微调模型，使模型能够快速适应观察到的证据，使用十字熵方法。</li>
<li>results: 我们的方法可以应用于多种深度生成模型，如自然语言处理、图像生成等，并且在机器人任务中得到了良好的效果，如握掌、 inverse kinematics 计算和点云补充等。<details>
<summary>Abstract</summary>
Adaptable models could greatly benefit robotic agents operating in the real world, allowing them to deal with novel and varying conditions. While approaches such as Bayesian inference are well-studied frameworks for adapting models to evidence, we build on recent advances in deep generative models which have greatly affected many areas of robotics. Harnessing modern GPU acceleration, we investigate how to quickly adapt the sample generation of neural network models to observations in robotic tasks. We propose a simple and general method that is applicable to various deep generative models and robotic environments. The key idea is to quickly fine-tune the model by fitting it to generated samples matching the observed evidence, using the cross-entropy method. We show that our method can be applied to both autoregressive models and variational autoencoders, and demonstrate its usability in object shape inference from grasping, inverse kinematics calculation, and point cloud completion.
</details>
<details>
<summary>摘要</summary>
这文章探讨了如何使用可靠的模型来帮助机器人代理人在实际世界中运行，以应对不同和变化的环境。我们建立了一个简单且通用的方法，可以快速地适应模型，使其能够对观察到的证据进行适应，使用十字熵方法。我们证明了我们的方法可以应用于多种深度生成模型和机器人环境中。我们还示了这个方法在抓取物体形状推导、 inverse kinematics 计算和点云补充等任务中的可用性。
</details></li>
</ul>
<hr>
<h2 id="Audio-Editing-with-Non-Rigid-Text-Prompts"><a href="#Audio-Editing-with-Non-Rigid-Text-Prompts" class="headerlink" title="Audio Editing with Non-Rigid Text Prompts"></a>Audio Editing with Non-Rigid Text Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12858">http://arxiv.org/abs/2310.12858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Paissan, Zhepei Wang, Mirco Ravanelli, Paris Smaragdis, Cem Subakan</li>
<li>for: 这个论文探讨了用非固定文本编辑 Audio-editing。</li>
<li>methods: 该编辑管道使用了文本提示进行 Audio-editing，并且可以创建具有固定音频特征的音频编辑。</li>
<li>results: 对比 Audio-LDM 模型，该方法可以获得更高的质量和更 faithful 的音频编辑结果，并且可以保持原始音频事件的幂等特征。<details>
<summary>Abstract</summary>
In this paper, we explore audio-editing with non-rigid text edits. We show that the proposed editing pipeline is able to create audio edits that remain faithful to the input audio. We explore text prompts that perform addition, style transfer, and in-painting. We quantitatively and qualitatively show that the edits are able to obtain results which outperform Audio-LDM, a recently released text-prompted audio generation model. Qualitative inspection of the results points out that the edits given by our approach remain more faithful to the input audio in terms of keeping the original onsets and offsets of the audio events.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们探讨了不固定文本编辑的音频编辑技术。我们展示了我们的编辑管道可以创建具有 faithfulness 特性的音频编辑。我们研究了添加、式转换和填充等文本提示，并quantitatively和qualitatively表明我们的编辑结果能够超过Audio-LDM，一个最近发布的文本提示 audio生成模型。 qualitative 分析结果表明，我们的编辑方法能够更好地保持输入音频事件的原始声音和停顿时间。
</details></li>
</ul>
<hr>
<h2 id="Model-agnostic-variable-importance-for-predictive-uncertainty-an-entropy-based-approach"><a href="#Model-agnostic-variable-importance-for-predictive-uncertainty-an-entropy-based-approach" class="headerlink" title="Model-agnostic variable importance for predictive uncertainty: an entropy-based approach"></a>Model-agnostic variable importance for predictive uncertainty: an entropy-based approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12842">http://arxiv.org/abs/2310.12842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danny Wood, Theodore Papamarkou, Matt Benatan, Richard Allmendinger</li>
<li>for: 本文旨在探讨如何使用现有的解释方法来理解uncertainty-aware模型的预测结果中的不确定性的来源。</li>
<li>methods: 本文使用了 permutation feature importance、partial dependence plots和individual conditional expectation plots等方法来解释uncertainty-aware模型的预测分布中的不确定性的来源。</li>
<li>results: 通过使用这些方法，本文发现了一些新的理解模型行为的方法，并且可以使用这些方法来衡量特征对预测分布中不确定性的影响。<details>
<summary>Abstract</summary>
In order to trust the predictions of a machine learning algorithm, it is necessary to understand the factors that contribute to those predictions. In the case of probabilistic and uncertainty-aware models, it is necessary to understand not only the reasons for the predictions themselves, but also the model's level of confidence in those predictions. In this paper, we show how existing methods in explainability can be extended to uncertainty-aware models and how such extensions can be used to understand the sources of uncertainty in a model's predictive distribution. In particular, by adapting permutation feature importance, partial dependence plots, and individual conditional expectation plots, we demonstrate that novel insights into model behaviour may be obtained and that these methods can be used to measure the impact of features on both the entropy of the predictive distribution and the log-likelihood of the ground truth labels under that distribution. With experiments using both synthetic and real-world data, we demonstrate the utility of these approaches in understanding both the sources of uncertainty and their impact on model performance.
</details>
<details>
<summary>摘要</summary>
要信任机器学习算法的预测结果，需要了解这些预测结果的原因以及模型对这些预测结果的自信度。在这篇论文中，我们示例了现有的解释方法可以扩展到不确定性意识模型，并使用这些扩展来理解模型预测分布中的不确定性来源。特别是，通过修改Permutation feature importance、partial dependence plots和individual conditional expectation plots，我们示例了可以从这些方法中获得新的意识，并且这些方法可以用来衡量特征对预测分布的熵和真实标签下的对应概率的影响。通过使用 synthetic 和实际数据进行实验，我们证明了这些方法的实用性，可以用来理解模型行为的不确定性来源并对其影响。
</details></li>
</ul>
<hr>
<h2 id="Generating-collective-counterfactual-explanations-in-score-based-classification-via-mathematical-optimization"><a href="#Generating-collective-counterfactual-explanations-in-score-based-classification-via-mathematical-optimization" class="headerlink" title="Generating collective counterfactual explanations in score-based classification via mathematical optimization"></a>Generating collective counterfactual explanations in score-based classification via mathematical optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12822">http://arxiv.org/abs/2310.12822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jasoneramirez/collectivece">https://github.com/jasoneramirez/collectivece</a></li>
<li>paper_authors: Emilio Carrizosa, Jasone Ramírez-Ayerbe, Dolores Romero Morales</li>
<li>For: 本研究旨在提供一种用于批处理高风险决策中机器学习模型的解释方法，以帮助理解模型如何做出决策。* Methods: 本研究使用了counterfactual分析方法，通过对一个实例进行微调，以提供一个解释，即如何将该实例微调以使其被分类器分类到所需的类别。此外，本研究还提出了一种集成counterfactual分析方法，可以为一组记录提供共同的解释，以便检测整个数据集中 kritical的特征。* Results: 本研究通过使用新的数学优化模型，可以为每个实例提供一个共同的解释，同时满足一些链接约束，以最小化总的微调成本。此外，本研究还可以处理异常记录，使其更加有效。在一些假设下，解释可以转化为一个半definite Programming问题，可以使用现有的解enser来解决。实验结果表明，本方法可以有效地应用于实际数据集。<details>
<summary>Abstract</summary>
Due to the increasing use of Machine Learning models in high stakes decision making settings, it has become increasingly important to have tools to understand how models arrive at decisions. Assuming a trained Supervised Classification model, explanations can be obtained via counterfactual analysis: a counterfactual explanation of an instance indicates how this instance should be minimally modified so that the perturbed instance is classified in the desired class by the Machine Learning classification model. Most of the Counterfactual Analysis literature focuses on the single-instance single-counterfactual setting, in which the analysis is done for one single instance to provide one single explanation. Taking a stakeholder's perspective, in this paper we introduce the so-called collective counterfactual explanations. By means of novel Mathematical Optimization models, we provide a counterfactual explanation for each instance in a group of interest, so that the total cost of the perturbations is minimized under some linking constraints. Making the process of constructing counterfactuals collective instead of individual enables us to detect the features that are critical to the entire dataset to have the individuals classified in the desired class. Our methodology allows for some instances to be treated individually, performing the collective counterfactual analysis for a fraction of records of the group of interest. This way, outliers are identified and handled appropriately. Under some assumptions on the classifier and the space in which counterfactuals are sought, finding collective counterfactuals is reduced to solving a convex quadratic linearly constrained mixed integer optimization problem, which, for datasets of moderate size, can be solved to optimality using existing solvers. The performance of our approach is illustrated on real-world datasets, demonstrating its usefulness.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:因为机器学习模型在高度决策中越来越常用，因此理解模型如何做出决策变得越来越重要。假设已经训练过的超vision类别化模型，我们可以通过对实例进行counterfactual分析来获得解释：counterfactual解释指的是将实例所需最小改变以使得这个改变后的实例被分类器分类为欲要的类别。大多数counterfactual分析文献都专注于单个实例单个counterfactual的设置，而我们在这篇论文中引入了集合counterfactual解释。我们使用了新的数学优化模型，为每个集合中的每个实例提供一个counterfactual解释，以最小化对实例的改变成本，同时保证实例的改变满足一些链接约束。通过集成counterfactual分析，我们可以检测整个数据集中critical的特征，使得每个个体被分类为欲要的类别。我们的方法允许一些实例被处理 individually，对集合中的一部分记录进行集成counterfactual分析。这样，我们可以检测并处理异常值。在一些假设下，如果分类器和counterfactuals的空间都满足某些条件，那么找到集合counterfactuals就可以降到一个半正quadratic linearly constrained mixed integer optimization problem。这个问题可以用现有的解决器来解决，对于中等大小的数据集来说。我们的方法的性能在实际数据上进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Forecasting-at-Scale"><a href="#Hierarchical-Forecasting-at-Scale" class="headerlink" title="Hierarchical Forecasting at Scale"></a>Hierarchical Forecasting at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12809">http://arxiv.org/abs/2310.12809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/elephaint/hfas">https://github.com/elephaint/hfas</a></li>
<li>paper_authors: Olivier Sprangers, Wander Wadman, Sebastian Schelter, Maarten de Rijke</li>
<li>For: The paper is written for practitioners who want to improve the forecasting performance of their production forecasting systems, particularly those with millions of time series.* Methods: The paper proposes using a sparse loss function to learn a coherent forecast for millions of time series, which directly optimizes the hierarchical product and&#x2F;or temporal structure. This approach eliminates the need for a post-processing step in traditional hierarchical forecasting techniques, reducing the computational cost of the prediction phase.* Results: The paper shows that the proposed sparse hierarchical loss function achieves up to 10% better performance (in terms of RMSE) on the public M5 dataset compared to the baseline loss function. In addition, the authors implement the loss function in an existing forecasting model at a large European e-commerce platform, resulting in an improved forecasting performance of 2% at the product level. Finally, the paper demonstrates an increase in forecasting performance of about 5-10% when evaluating the forecasting performance across the cross-sectional hierarchies defined.<details>
<summary>Abstract</summary>
Existing hierarchical forecasting techniques scale poorly when the number of time series increases. We propose to learn a coherent forecast for millions of time series with a single bottom-level forecast model by using a sparse loss function that directly optimizes the hierarchical product and/or temporal structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional or temporal hierarchy. In addition, removing the need for a post-processing step as required in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. On the public M5 dataset, our sparse hierarchical loss function performs up to 10% (RMSE) better compared to the baseline loss function. We implement our sparse hierarchical loss function within an existing forecasting model at bol, a large European e-commerce platform, resulting in an improved forecasting performance of 2% at the product level. Finally, we found an increase in forecasting performance of about 5-10% when evaluating the forecasting performance across the cross-sectional hierarchies that we defined. These results demonstrate the usefulness of our sparse hierarchical loss applied to a production forecasting system at a major e-commerce platform.
</details>
<details>
<summary>摘要</summary>
传统的层次预测技术在时间序列数量增加时表现不佳。我们提议通过使用一个稀疏损失函数来学习一个可以涵盖数百万时间序列的底层预测模型。这种稀疏损失函数可以直接优化层次制 продукт和/或时间结构，从而提供一种生成底层预测值具有任意选择的横向或时间层次结构的方法。此外，不需要传统层次预测技术中必需的后处理步骤，因此预测阶段的计算成本也得到了降低。在公共M5数据集上，我们的稀疏层次损失函数与基准损失函数相比，可以提高预测性能达10%（RMSE）。在bol大 european e-commerce平台上实现了我们的稀疏层次损失函数，导致了预测性能的提高达2%。最后，我们发现在我们定义的横向层次中进行评估预测性能时，预测性能提高了5-10%。这些结果表明了我们的稀疏层次损失函数在大规模电商平台上的实用性。
</details></li>
</ul>
<hr>
<h2 id="DCSI-–-An-improved-measure-of-cluster-separability-based-on-separation-and-connectedness"><a href="#DCSI-–-An-improved-measure-of-cluster-separability-based-on-separation-and-connectedness" class="headerlink" title="DCSI – An improved measure of cluster separability based on separation and connectedness"></a>DCSI – An improved measure of cluster separability based on separation and connectedness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12806">http://arxiv.org/abs/2310.12806</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/janagauss/dcsi">https://github.com/janagauss/dcsi</a></li>
<li>paper_authors: Jana Gauss, Fabian Scheipl, Moritz Herrmann</li>
<li>for: 本研究旨在评估用实际数据集评估 clustering 算法的评估方法，以确定数据集中的类标签是否对应到有意义的集群。</li>
<li>methods: 本研究使用了一种新开发的度量指标（density cluster separability index，DCSI），以衡量between-class separation和within-class connectedness two aspects of separability。</li>
<li>results: 实验结果表明，DCSI 与 DBSCAN 的调整 Rand 指标（ARI）之间存在强相关性，但在多类数据集上存在 overlap classes 时，DCSI 的稳定性不高。此外，DCSI 能够正确地标识触地或 overlap 的类，这些类不形成有意义的集群。<details>
<summary>Abstract</summary>
Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. A review of the existing literature shows that neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate the central aspects of separability for density-based clustering: between-class separation and within-class connectedness. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that DCSI can correctly identify touching or overlapping classes that do not form meaningful clusters.
</details>
<details>
<summary>摘要</summary>
Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that DCSI can correctly identify touching or overlapping classes that do not form meaningful clusters.Translated into Simplified Chinese:是否class label在给定数据集中对应于有意义的集群是评估用实际世界数据集的 clustering 算法的评估中的关键因素。这个属性可以通过分离度量来衡量。现有文献的综述显示， neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate density-based clustering中的两个中心特征：between-class separation和 within-class connectedness。一种新发展的度量（density cluster separability index, DCSI） aspires to quantify这两个特征，并可以作为 CVI。对于 sintetic data 进行了广泛的实验，DCSI与 DBSCAN 的性能 measured via adjusted rand index (ARI) 之间存在强正相关，但在多类数据集中，DCSI 对于多类数据集而言不具有稳定性，这些数据集不适合 density-based hard clustering。详细评估了一些常用的实际世界数据集，DCSI 可以正确地识别触摸或重叠的类，这些类不形成有意义的集群。
</details></li>
</ul>
<hr>
<h2 id="Detection-and-Evaluation-of-bias-inducing-Features-in-Machine-learning"><a href="#Detection-and-Evaluation-of-bias-inducing-Features-in-Machine-learning" class="headerlink" title="Detection and Evaluation of bias-inducing Features in Machine learning"></a>Detection and Evaluation of bias-inducing Features in Machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12805">http://arxiv.org/abs/2310.12805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moses Openja, Gabriel Laberge, Foutse Khomh</li>
<li>for: 本研究旨在系统地Identify机器学习模型中的偏见引起因素，以帮助领域专家做出公平的决策。</li>
<li>methods: 本研究提出了一种方法，通过对数据中的每个特征进行小改动，观察模型的预测结果变化，以确定每个特征是否引起偏见。</li>
<li>results: 研究人员通过应用这种方法，在四个常用的数据集上成功地Identify了机器学习模型中的偏见引起因素，并证明了该方法可以帮助领域专家更好地发现和解决偏见问题。<details>
<summary>Abstract</summary>
The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning (ML), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods require a pre-identification of sensitive features for bias assessment and can actually miss other relevant bias-inducing features, which is why systematic identification of such features is necessary. Moreover, it often occurs that to achieve an equitable outcome, one has to take into account sensitive features in the model decision. Therefore, it should be up to the domain experts to decide based on their knowledge of the context of a decision whether bias induced by specific features is acceptable or not. In this study, we propose an approach for systematically identifying all bias-inducing features of a model to help support the decision-making of domain experts. We evaluated our technique using four well-known datasets to showcase how our contribution can help spearhead the standard procedure when developing, testing, maintaining, and deploying fair/equitable machine learning systems.
</details>
<details>
<summary>摘要</summary>
通过 causa-efecto 分析，我们可以分解所有可能导致问题的原因，如商业不佳或对个人（们）造成不良影响。这意味着我们可以识别问题的继承，排序原因以帮助优先级化修复，简化复杂问题并可视化它们。在机器学习（ML）上下文中，我们可以使用 causa-efecto 分析理解模型的偏见行为的原因。例如，我们可以检查模型中可能导致偏见的每个特征，以确定哪些特征可能引起偏见。为此，我们可以对数据中的每个特征或一对特征进行小型变更，按照一些指南进行观察，了解模型对这些变更的响应。因此，我们可以使用 causa-efecto 分析来确定可能引起偏见的特征，即使这些特征未经预先标识。这对于当前的方法来说是重要的，因为大多数方法需要先知道敏感特征以进行偏见评估，可能会扫描过其他重要的偏见引起特征。此外，在很多情况下，为了实现公正的结果，需要考虑敏感特征在模型决策中的作用。因此，域专家应该根据它们对决策过程的知识来决定是否acceptible的偏见。在本研究中，我们提出了一种方法，可以系统地标识模型中所有偏见引起特征，以支持域专家决策。我们使用四个常见的数据集进行评估，以示出我们的贡献可以帮助驱动开发、测试、维护和部署公正/公平的机器学习系统的标准程序。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Vertex-Fitting-for-Jet-Flavour-Tagging"><a href="#Differentiable-Vertex-Fitting-for-Jet-Flavour-Tagging" class="headerlink" title="Differentiable Vertex Fitting for Jet Flavour Tagging"></a>Differentiable Vertex Fitting for Jet Flavour Tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12804">http://arxiv.org/abs/2310.12804</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rachsmith1/ndive">https://github.com/rachsmith1/ndive</a></li>
<li>paper_authors: Rachel E. C. Smith, Inês Ochoa, Rúben Inácio, Jonathan Shoemaker, Michael Kagan</li>
<li>for: 用于粒子物理学中的jets分类</li>
<li>methods: 使用可微分点Vertex fitting算法</li>
<li>results: 可以提高偏振扩散的粒子物理学模型中的准确率<details>
<summary>Abstract</summary>
We propose a differentiable vertex fitting algorithm that can be used for secondary vertex fitting, and that can be seamlessly integrated into neural networks for jet flavour tagging. Vertex fitting is formulated as an optimization problem where gradients of the optimized solution vertex are defined through implicit differentiation and can be passed to upstream or downstream neural network components for network training. More broadly, this is an application of differentiable programming to integrate physics knowledge into neural network models in high energy physics. We demonstrate how differentiable secondary vertex fitting can be integrated into larger transformer-based models for flavour tagging and improve heavy flavour jet classification.
</details>
<details>
<summary>摘要</summary>
我们提出了一种可微 differentiable 顶点适应算法，可以用于次级顶点适应，并可以轻松地与神经网络结合用于jet flavor标记。顶点适应被формализова为优化问题，其最优解顶点的梯度由implicit differentiations定义，可以传递到上游或下游神经网络组件进行网络训练。更广泛地说，这是一种应用 diferenciable 编程将物理知识integrated into neural network models in high energy physics。我们示出了可微 secondary vertex fitting可以与更大的 transformer-based 模型结合用于味道标记，并提高重量味道jet分类。
</details></li>
</ul>
<hr>
<h2 id="A-Theoretical-Approach-to-Characterize-the-Accuracy-Fairness-Trade-off-Pareto-Frontier"><a href="#A-Theoretical-Approach-to-Characterize-the-Accuracy-Fairness-Trade-off-Pareto-Frontier" class="headerlink" title="A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier"></a>A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12785">http://arxiv.org/abs/2310.12785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hua Tang, Lu Cheng, Ninghao Liu, Mengnan Du</li>
<li>for: 本研究旨在理解准确性和公平性之间的贸易off，提供理论基础 для现有的平衡评估方法。</li>
<li>methods: 本研究提出了一个定义准确性和公平性的Pareto前沿(FairFrontier)的理论框架，并采用实验研究来探讨这个前沿的性质。</li>
<li>results: 实验结果表明，准确性和公平性之间存在一定的贸易off，而且可以根据不同的属性类型和数据分布来分类型化这个贸易off。此外，研究还发现了一种两步流水线方法可以消除这个贸易off。<details>
<summary>Abstract</summary>
While the accuracy-fairness trade-off has been frequently observed in the literature of fair machine learning, rigorous theoretical analyses have been scarce. To demystify this long-standing challenge, this work seeks to develop a theoretical framework by characterizing the shape of the accuracy-fairness trade-off Pareto frontier (FairFrontier), determined by a set of all optimal Pareto classifiers that no other classifiers can dominate. Specifically, we first demonstrate the existence of the trade-off in real-world scenarios and then propose four potential categories to characterize the important properties of the accuracy-fairness Pareto frontier. For each category, we identify the necessary conditions that lead to corresponding trade-offs. Experimental results on synthetic data suggest insightful findings of the proposed framework: (1) When sensitive attributes can be fully interpreted by non-sensitive attributes, FairFrontier is mostly continuous. (2) Accuracy can suffer a \textit{sharp} decline when over-pursuing fairness. (3) Eliminate the trade-off via a two-step streamlined approach. The proposed research enables an in-depth understanding of the accuracy-fairness trade-off, pushing current fair machine-learning research to a new frontier.
</details>
<details>
<summary>摘要</summary>
“在文献中，准确性和公平性之间的贸易关系几经不详细。为了解释这个长期挑战，这项研究希望建立一个理论框架，描述公平性贸易关系的Pareto前沿（FairFrontier），由一组所有最优Pareto分类器所决定。 Specifically, we first demonstrate the existence of this trade-off in real-world scenarios, and then propose four potential categories to characterize the important properties of the accuracy-fairness Pareto frontier. For each category, we identify the necessary conditions that lead to corresponding trade-offs. 实验结果表明：（1）当敏感特征可以完全通过非敏感特征解释时，FairFrontier是主要连续的。（2）在追求公平性时，准确性可能会受到急剧下降的影响。（3）通过两步流水线方法，可以消除这个贸易关系。该研究带来了对准确性和公平性之间的贸易关系的深入理解，推动当前公平机器学习研究到新的前ier。”
</details></li>
</ul>
<hr>
<h2 id="Conditional-Density-Estimations-from-Privacy-Protected-Data"><a href="#Conditional-Density-Estimations-from-Privacy-Protected-Data" class="headerlink" title="Conditional Density Estimations from Privacy-Protected Data"></a>Conditional Density Estimations from Privacy-Protected Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12781">http://arxiv.org/abs/2310.12781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Xiong, Nianqiao P. Ju, Sanguo Zhang</li>
<li>for: 本文提出了一种 simulation-based 统计分析方法，用于从受保护数据集中进行参数推断。</li>
<li>methods: 本文使用神经网络Conditional Density Estimators（NCE）作为一种灵活的分布家族，以近似 posterior 分布中参数的推断结果。</li>
<li>results: 实验和分析表明，采用本文提出的方法可以在受保护数据集中实现有效的统计推断，同时保证个人隐私。<details>
<summary>Abstract</summary>
Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustrating the privacy-utility trade-off, our experiments and analysis demonstrate the necessity and feasibility of designing valid statistical inference procedures to correct for biases introduced by the privacy-protection mechanisms.
</details>
<details>
<summary>摘要</summary>
viele moderne statistische Analyse und maschinelles Lern-Anwendungen erfordern das Training von Modellen auf sensible Benutzerdaten. Differenzielle Privatsphäre bietet eine formale Garantie, dass individual-level Informationen über Benutzer nicht durchlecken. In diesem Framework injectieren randomisierte Algorithmen calibrierte Lärm in das vertrauliche Daten, resulting in privacy-protected Datasets oder -Abfragen. However, restricting access to only the privatized Data during statistical Analysis makes it computationally challenging to perform valid Inferences on parameters underlying the confidential Data. In this work, we propose simulation-based Inference methods from privacy-protected Datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series Data under an infectious disease model and on ordinary linear regression models. Illustrating the privacy-utility trade-off, our experiments and analysis demonstrate the necessity and feasibility of designing valid statistical Inference procedures to correct for biases introduced by the privacy-protection mechanisms.
</details></li>
</ul>
<hr>
<h2 id="Energy-Based-Models-For-Speech-Synthesis"><a href="#Energy-Based-Models-For-Speech-Synthesis" class="headerlink" title="Energy-Based Models For Speech Synthesis"></a>Energy-Based Models For Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12765">http://arxiv.org/abs/2310.12765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/radtts">https://github.com/NVIDIA/radtts</a></li>
<li>paper_authors: Wanli Sun, Zehai Tu, Anton Ragni</li>
<li>for: 该文章探讨了一种新的非 autoregressive（非AR）模型，即能量基本模型（EBMs），以提高speech sintesis的效率。</li>
<li>methods: 文章提出了一种基于噪声对比估计的方法来训练EBMs，以及一些生成有效负样本的策略，如使用高性能的AR模型。另外，文章还介绍了使用Langevin MCMC进行采样的方法。</li>
<li>results: 实验结果表明，提出的方法可以超越Tacotron 2。<details>
<summary>Abstract</summary>
Recently there has been a lot of interest in non-autoregressive (non-AR) models for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike AR models, these models do not have autoregressive dependencies among outputs which makes inference efficient. This paper expands the range of available non-AR models with another member called energy-based models (EBMs). The paper describes how noise contrastive estimation, which relies on the comparison between positive and negative samples, can be used to train EBMs. It proposes a number of strategies for generating effective negative samples, including using high-performing AR models. It also describes how sampling from EBMs can be performed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin MCMC enables to draw connections between EBMs and currently popular diffusion models. Experiments on LJSpeech dataset show that the proposed approach offers improvements over Tacotron 2.
</details>
<details>
<summary>摘要</summary>
Note:* "AR" stands for "autoregressive"* "EBMs" stands for "energy-based models"* "LJSpeech" is a dataset for speech synthesis* "Tacotron 2" is a popular method for speech synthesis
</details></li>
</ul>
<hr>
<h2 id="Discretize-Relaxed-Solution-of-Spectral-Clustering-via-a-Non-Heuristic-Algorithm"><a href="#Discretize-Relaxed-Solution-of-Spectral-Clustering-via-a-Non-Heuristic-Algorithm" class="headerlink" title="Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic Algorithm"></a>Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12752">http://arxiv.org/abs/2310.12752</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hyzhang98/first-order-discretization">https://github.com/hyzhang98/first-order-discretization</a></li>
<li>paper_authors: Hongyuan Zhang, Xuelong Li</li>
<li>for: 提出了一种基于first-order优化算法的非规则化策略，以解决spectral clustering和其扩展中的离散化问题。</li>
<li>methods: 提出了一种基于first-order优化算法的非规则化策略，并证明了这种策略可以更好地保持原始问题的优化目标。</li>
<li>results: 实验表明，该方法在离散化问题中显示出了明显的优势，并且可以更好地保持原始问题的优化目标。<details>
<summary>Abstract</summary>
Spectral clustering and its extensions usually consist of two steps: (1) constructing a graph and computing the relaxed solution; (2) discretizing relaxed solutions. Although the former has been extensively investigated, the discretization techniques are mainly heuristic methods, e.g., k-means, spectral rotation. Unfortunately, the goal of the existing methods is not to find a discrete solution that minimizes the original objective. In other words, the primary drawback is the neglect of the original objective when computing the discrete solution. Inspired by the first-order optimization algorithms, we propose to develop a first-order term to bridge the original problem and discretization algorithm, which is the first non-heuristic to the best of our knowledge. Since the non-heuristic method is aware of the original graph cut problem, the final discrete solution is more reliable and achieves the preferable loss value. We also theoretically show that the continuous optimum is beneficial to discretization algorithms though simply finding its closest discrete solution is an existing heuristic algorithm which is also unreliable. Sufficient experiments significantly show the superiority of our method.
</details>
<details>
<summary>摘要</summary>
spectral clustering 和其 extensions 通常包括两个步骤：（1）构建图并计算宽松解决方案；（2）精化宽松解决方案。 although the former has been extensively investigated, the discretization techniques are mainly heuristic methods, e.g., k-means, spectral rotation. unfortunately, the goal of the existing methods is not to find a discrete solution that minimizes the original objective. in other words, the primary drawback is the neglect of the original objective when computing the discrete solution. inspired by the first-order optimization algorithms, we propose to develop a first-order term to bridge the original problem and discretization algorithm, which is the first non-heuristic to the best of our knowledge. since the non-heuristic method is aware of the original graph cut problem, the final discrete solution is more reliable and achieves the preferable loss value. we also theoretically show that the continuous optimum is beneficial to discretization algorithms though simply finding its closest discrete solution is an existing heuristic algorithm which is also unreliable. sufficient experiments significantly show the superiority of our method.Here's the word-for-word translation of the text into Simplified Chinese: spectral clustering 和其 extensions 通常包括两个步骤：（1）构建图并计算宽松解决方案；（2）精化宽松解决方案。 although the former has been extensively investigated, the discretization techniques are mainly heuristic methods, e.g., k-means, spectral rotation. unfortunately, the goal of the existing methods is not to find a discrete solution that minimizes the original objective. in other words, the primary drawback is the neglect of the original objective when computing the discrete solution. inspired by the first-order optimization algorithms, we propose to develop a first-order term to bridge the original problem and discretization algorithm, which is the first non-heuristic to the best of our knowledge. since the non-heuristic method is aware of the original graph cut problem, the final discrete solution is more reliable and achieves the preferable loss value. we also theoretically show that the continuous optimum is beneficial to discretization algorithms though simply finding its closest discrete solution is an existing heuristic algorithm which is also unreliable. sufficient experiments significantly show the superiority of our method.
</details></li>
</ul>
<hr>
<h2 id="TabuLa-Harnessing-Language-Models-for-Tabular-Data-Synthesis"><a href="#TabuLa-Harnessing-Language-Models-for-Tabular-Data-Synthesis" class="headerlink" title="TabuLa: Harnessing Language Models for Tabular Data Synthesis"></a>TabuLa: Harnessing Language Models for Tabular Data Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12746">http://arxiv.org/abs/2310.12746</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhao-zilong/tabula">https://github.com/zhao-zilong/tabula</a></li>
<li>paper_authors: Zilong Zhao, Robert Birke, Lydia Chen</li>
<li>for:  This paper focuses on the research area of tabular data synthesis, specifically exploring the use of large language models (LLMs) to generate realistic tabular data.</li>
<li>methods:  The proposed method, called Tabula, is based on the language model structure and utilizes a token sequence compression strategy to reduce training time while maintaining synthetic data quality.</li>
<li>results:  The paper demonstrates the limitations of using pre-trained language models for tabular data synthesis and proposes a dedicated foundational model tailored specifically for this task. Additionally, the proposed method significantly reduces training time while achieving better synthetic data utility compared to current state-of-the-art algorithms.<details>
<summary>Abstract</summary>
Given the ubiquitous use of tabular data in industries and the growing concerns in data privacy and security, tabular data synthesis emerges as a critical research area. The recent state-of-the-art methods show that large language models (LLMs) can be adopted to generate realistic tabular data. As LLMs pre-process tabular data as full text, they have the advantage of avoiding the curse of dimensionality associated with one-hot encoding high-dimensional data. However, their long training time and limited re-usability on new tasks prevent them from replacing exiting tabular generative models. In this paper, we propose Tabula, a tabular data synthesizer based on the language model structure. Through Tabula, we demonstrate the inherent limitation of employing pre-trained language models designed for natural language processing (NLP) in the context of tabular data synthesis. Our investigation delves into the development of a dedicated foundational model tailored specifically for tabular data synthesis. Additionally, we propose a token sequence compression strategy to significantly reduce training time while preserving the quality of synthetic data. Extensive experiments on six datasets demonstrate that using a language model structure without loading the well-trained model weights yields a better starting model for tabular data synthesis. Moreover, the Tabula model, previously trained on other tabular data, serves as an excellent foundation model for new tabular data synthesis tasks. Additionally, the token sequence compression method substantially reduces the model's training time. Results show that Tabula averagely reduces 46.2% training time per epoch comparing to current LLMs-based state-of-the-art algorithm and consistently achieves even higher synthetic data utility.
</details>
<details>
<summary>摘要</summary>
“因为业务中广泛使用表格数据，并且数据隐私和安全问题日益升级，表格数据合成成为一个重要的研究领域。 current state-of-the-art方法显示，大型自然语言模型（LLMs）可以用来生成真实的表格数据。因为LLMs将表格数据视为全文进行处理，因此它们可以避免因一个维度化而带来的味道问题。然而，它们的培训时间比较长，并且在新任务上有限的可重用性，使得它们无法取代现有的表格生成模型。在这篇论文中，我们提出了Tabula，一种基于语言模型结构的表格数据合成器。通过Tabula，我们发现了使用预训练的自然语言处理模型（NLP）在表格数据合成中存在的内在限制。我们的调查探讨了开发专门为表格数据合成的基础模型。此外，我们还提出了一种压缩token序列策略，可以减少模型培训时间，保持合成数据质量。我们在六个数据集进行了广泛的实验，结果表明，不加载已经预训练的模型 weights，使用语言模型结构可以获得更好的表格数据合成起始模型。此外，Tabula模型，之前已经在其他表格数据上培训，可以作为新的表格数据合成任务的优秀基础模型。此外，压缩token序列策略可以减少模型培训时间，并且可以保持合成数据质量。结果显示，Tabula平均每 epoch 减少46.2% 的培训时间，并在多个表格数据合成任务中表现出了更高的合成数据实用性。”
</details></li>
</ul>
<hr>
<h2 id="Canonical-normalizing-flows-for-manifold-learning"><a href="#Canonical-normalizing-flows-for-manifold-learning" class="headerlink" title="Canonical normalizing flows for manifold learning"></a>Canonical normalizing flows for manifold learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12743">http://arxiv.org/abs/2310.12743</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/k-flouris/cmf">https://github.com/k-flouris/cmf</a></li>
<li>paper_authors: Kyriakos Flouris, Ender Konukoglu</li>
<li>for: 这个论文主要目标是提出一种新的推理模型，它可以更好地利用数据的低维度表示，从而提高模型的表达能力和精度。</li>
<li>methods: 这个论文使用了推理模型，并且提出了一种新的优化目标函数，可以让模型学习一个更加有效的嵌入表示。</li>
<li>results: 实验结果显示，这个方法可以在大多数情况下比其他推理模型更好地approximate target distribution，并且可以生成更加简洁和明确的嵌入表示。<details>
<summary>Abstract</summary>
Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such a manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used to optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that the density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an efficient representation of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis, with degenerate information being stored in each dimension. Alternatively, if a locally orthogonal and/or sparse basis is to be learned, here coined canonical intrinsic basis, it can serve in learning a more compact latent space representation. Toward this end, we propose a canonical manifold learning flow method, where a novel optimization objective enforces the transformation matrix to have few prominent and non-degenerate basis functions. We demonstrate that by minimizing the off-diagonal manifold metric elements $\ell_1$-norm, we can achieve such a basis, which is simultaneously sparse and/or orthogonal. Canonical manifold flow yields a more efficient use of the latent space, automatically generating fewer prominent and distinct dimensions to represent data, and a better approximation of target distributions than other manifold flow methods in most experiments we conducted, resulting in lower FID scores.
</details>
<details>
<summary>摘要</summary>
流形学习流程是一种生成模型技术，假设数据有低维度流形描述。通过学习可逆变换，将流形嵌入高维度数据空间中。一旦流形 Correctly aligned via 重建损失， then the probability density is tractable on the manifold, and maximum likelihood can be used to optimize the network parameters. 然而， unless the injective-mapping is analytically predefined, the learned manifold may not be an efficient representation of the data. Specifically, the latent dimensions of such models frequently learn an entangled intrinsic basis, with degenerate information being stored in each dimension.为了解决这个问题，我们提出了一种 canonical manifold learning flow 方法，其中一个新的优化目标函数要求变换矩阵具有少量显著和非分析的基函数。我们示示了，通过最小化离散 manifold 度量元素 $\ell_1$-norm，可以实现这种基函数，这是同时稀疏和/或正交的。 canonical manifold flow 可以更有效地使用封闭空间，自动生成 fewer 和更明显的维度来表示数据，并且更好地近似目标分布 than other manifold flow methods in most experiments we conducted, resulting in lower FID scores.
</details></li>
</ul>
<hr>
<h2 id="Learn-from-the-Past-A-Proxy-based-Adversarial-Defense-Framework-to-Boost-Robustness"><a href="#Learn-from-the-Past-A-Proxy-based-Adversarial-Defense-Framework-to-Boost-Robustness" class="headerlink" title="Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness"></a>Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12713">http://arxiv.org/abs/2310.12713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaohua Liu, Jiaxin Gao, Zhu Liu, Xianghao Jiao, Xin Fan, Risheng Liu</li>
<li>For: This paper focuses on improving the robustness of deep learning models against adversarial attacks, specifically by introducing a new framework called LAST (Learn from the Past) that utilizes historical information to defend against parameter-oriented attacks.* Methods: The paper introduces a two-stage update rule for the target model, which incorporates prior information from the historical state of the model to improve defense against adversarial attacks. Additionally, the paper proposes a Self Distillation (SD) based defense objective to constrain the update process of the proxy model without relying on larger teacher models.* Results: The paper demonstrates significant performance enhancements in improving robust accuracy (RA) across various datasets, backbones, and attack modalities, with improvements of up to 9.2% and 20.5% on the CIFAR10 and CIFAR100 datasets, respectively. The paper also shows that the proposed method can improve training stability and reduce catastrophic overfitting issues.<details>
<summary>Abstract</summary>
In light of the vulnerability of deep learning models to adversarial samples and the ensuing security issues, a range of methods, including Adversarial Training (AT) as a prominent representative, aimed at enhancing model robustness against various adversarial attacks, have seen rapid development. However, existing methods essentially assist the current state of target model to defend against parameter-oriented adversarial attacks with explicit or implicit computation burdens, which also suffers from unstable convergence behavior due to inconsistency of optimization trajectories. Diverging from previous work, this paper reconsiders the update rule of target model and corresponding deficiency to defend based on its current state. By introducing the historical state of the target model as a proxy, which is endowed with much prior information for defense, we formulate a two-stage update rule, resulting in a general adversarial defense framework, which we refer to as `LAST' ({\bf L}earn from the P{\bf ast}). Besides, we devise a Self Distillation (SD) based defense objective to constrain the update process of the proxy model without the introduction of larger teacher models. Experimentally, we demonstrate consistent and significant performance enhancements by refining a series of single-step and multi-step AT methods (e.g., up to $\bf 9.2\%$ and $\bf 20.5\%$ improvement of Robust Accuracy (RA) on CIFAR10 and CIFAR100 datasets, respectively) across various datasets, backbones and attack modalities, and validate its ability to enhance training stability and ameliorate catastrophic overfitting issues meanwhile.
</details>
<details>
<summary>摘要</summary>
在深度学习模型面临抗击样本攻击和相关安全问题的情况下，一系列方法，包括抗击训练（AT）作为代表，努力强化模型对各种抗击攻击的抗御能力。然而，现有方法主要帮助目标模型在抗击攻击中增强对参数的抗御能力，具有显著的计算负担和不稳定的收敛行为。与之前的工作不同，本文重新考虑目标模型的更新规则和相关缺陷，基于目标模型当前状态，提出了一种通用的抗击防御框架，称之为“LAST”（学习从过去）。此外，我们还提出了一种基于自适应融合（SD）的防御目标函数，以防止代理模型更新过程中的潜在混乱。实验表明，我们可以通过改进单步和多步AT方法（例如，在CIFAR10和CIFAR100 datasets上提高了Robust Accuracy（RA）的性能，最高提高达9.2%和20.5%），并在不同的 datasets、后处和攻击模式下达到了显著的性能提升。此外，我们还证明了它能够提高训练稳定性和避免潜在的混乱学习问题。
</details></li>
</ul>
<hr>
<h2 id="On-the-Optimization-and-Generalization-of-Multi-head-Attention"><a href="#On-the-Optimization-and-Generalization-of-Multi-head-Attention" class="headerlink" title="On the Optimization and Generalization of Multi-head Attention"></a>On the Optimization and Generalization of Multi-head Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12680">http://arxiv.org/abs/2310.12680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, Christos Thrampoulidis</li>
<li>for:  investigate the potential optimization and generalization advantages of using multiple attention heads in Transformer’s core mechanism</li>
<li>methods: derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model</li>
<li>results: demonstrate that the conditions for realizability hold for a simple tokenized-mixture model, and expect the analysis can be extended to various data-model and architecture variations.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
The training and generalization dynamics of the Transformer's core mechanism, namely the Attention mechanism, remain under-explored. Besides, existing analyses primarily focus on single-head attention. Inspired by the demonstrated benefits of overparameterization when training fully-connected networks, we investigate the potential optimization and generalization advantages of using multiple attention heads. Towards this goal, we derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, under a suitable realizability condition on the data. We then establish primitive conditions on the initialization that ensure realizability holds. Finally, we demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We expect the analysis can be extended to various data-model and architecture variations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>transformer 核心机制，即注意机制，的训练和泛化动态仍未得到充分探索。另外，现有的分析主要集中在单头注意力。针对此，我们发现了过参数化训练完全连接网络时的优化和泛化优势。为达到这个目标，我们 derive了梯度下降训练单层多头自注意模型的收敛和泛化保证，只要数据满足适当的可能性条件。然后，我们确定了初始化的 primitive conditions，以保证可能性条件成立。最后，我们证明这些条件在一个简单的Tokenized-mixture模型中成立。我们预计这些结果可以推广到不同的数据-模型和架构变化。<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="Neural-networks-for-insurance-pricing-with-frequency-and-severity-data-a-benchmark-study-from-data-preprocessing-to-technical-tariff"><a href="#Neural-networks-for-insurance-pricing-with-frequency-and-severity-data-a-benchmark-study-from-data-preprocessing-to-technical-tariff" class="headerlink" title="Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff"></a>Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12671">http://arxiv.org/abs/2310.12671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/freekholvoet/nnforfreqsevpricing">https://github.com/freekholvoet/nnforfreqsevpricing</a></li>
<li>paper_authors: Freek Holvoet, Katrien Antonio, Roel Henckaerts</li>
<li>for: 这篇论文旨在使用机器学习技术来模型保险公司的索赔频率和严重程度数据。</li>
<li>methods: 该论文使用了深度学习结构，包括梯度抽象树模型、feed-forward neural network (FFNN) 和组合 actuarial neural network (CANN)，对四个保险数据集进行了比较性研究。</li>
<li>results: 研究发现，CANNs 可以在具有多种输入特征的保险数据集上达到最高的准确率，而且可以使用自动编码器将分类变量embed到神经网络中，从而提高模型的性能。此外，研究还构建了全局替身模型，使得可以将神经网络中的频率和严重度模型翻译成 GLM 模型，从而实现技术评估表的生成。<details>
<summary>Abstract</summary>
Insurers usually turn to generalized linear models for modelling claim frequency and severity data. Due to their success in other fields, machine learning techniques are gaining popularity within the actuarial toolbox. Our paper contributes to the literature on frequency-severity insurance pricing with machine learning via deep learning structures. We present a benchmark study on four insurance data sets with frequency and severity targets in the presence of multiple types of input features. We compare in detail the performance of: a generalized linear model on binned input data, a gradient-boosted tree model, a feed-forward neural network (FFNN), and the combined actuarial neural network (CANN). Our CANNs combine a baseline prediction established with a GLM and GBM, respectively, with a neural network correction. We explain the data preprocessing steps with specific focus on the multiple types of input features typically present in tabular insurance data sets, such as postal codes, numeric and categorical covariates. Autoencoders are used to embed the categorical variables into the neural network and we explore their potential advantages in a frequency-severity setting. Finally, we construct global surrogate models for the neural nets' frequency and severity models. These surrogates enable the translation of the essential insights captured by the FFNNs or CANNs to GLMs. As such, a technical tariff table results that can easily be deployed in practice.
</details>
<details>
<summary>摘要</summary>
保险公司通常会使用通用线性模型来模拟养成和严重性数据。由于机器学习技术在其他领域的成功，因此在保险工具箱中受到欢迎。我们的论文对频率-严重保险价格使用机器学习技术进行了贡献，特别是使用深度学习结构。我们对四个保险数据集进行了比较性研究，每个数据集具有频率和严重性目标，同时具有多种输入特征。我们对比了以下四种模型的性能：通用线性模型在分割输入数据上，梯度拟合树模型，feed-forward neural network (FFNN)，以及结合投保险 neural network (CANN)。我们的CANNs将基eline预测使用通用线性模型和梯度拟合树模型，然后使用神经网络修正。我们还解释了对多种输入特征进行数据处理步骤，包括邮政编码、数字和分类变量。我们使用自动编码器将分类变量嵌入神经网络，并考虑其在频率-严重设置中的潜在优势。最后，我们构建了全球抽象模型 для神经网络的频率和严重模型。这些抽象模型使得可以将FFNNs或CANNs的主要启示翻译到GLMs中。因此，我们可以轻松地生成一份技术 tariff table，可以在实践中使用。
</details></li>
</ul>
<hr>
<h2 id="STANLEY-Stochastic-Gradient-Anisotropic-Langevin-Dynamics-for-Learning-Energy-Based-Models"><a href="#STANLEY-Stochastic-Gradient-Anisotropic-Langevin-Dynamics-for-Learning-Energy-Based-Models" class="headerlink" title="STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning Energy-Based Models"></a>STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning Energy-Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12667">http://arxiv.org/abs/2310.12667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Belhal Karimi, Jianwen Xie, Ping Li</li>
<li>for: 本研究提出了一种名为STANLEY的Stochastic gradient ANisotropic LangEvin dYnamics方法，用于采样高维数据。</li>
<li>methods: 该方法基于一种批量 Monte Carlo（MCMC）算法，通过一种随机步长和梯度信息来更新负样本。</li>
<li>results: 实验表明，STANLEY方法可以更好地采样高维数据，并且可以提供更高质量的样本。<details>
<summary>Abstract</summary>
We propose in this paper, STANLEY, a STochastic gradient ANisotropic LangEvin dYnamics, for sampling high dimensional data. With the growing efficacy and potential of Energy-Based modeling, also known as non-normalized probabilistic modeling, for modeling a generative process of different natures of high dimensional data observations, we present an end-to-end learning algorithm for Energy-Based models (EBM) with the purpose of improving the quality of the resulting sampled data points. While the unknown normalizing constant of EBMs makes the training procedure intractable, resorting to Markov Chain Monte Carlo (MCMC) is in general a viable option. Realizing what MCMC entails for the EBM training, we propose in this paper, a novel high dimensional sampling method, based on an anisotropic stepsize and a gradient-informed covariance matrix, embedded into a discretized Langevin diffusion. We motivate the necessity for an anisotropic update of the negative samples in the Markov Chain by the nonlinearity of the backbone of the EBM, here a Convolutional Neural Network. Our resulting method, namely STANLEY, is an optimization algorithm for training Energy-Based models via our newly introduced MCMC method. We provide a theoretical understanding of our sampling scheme by proving that the sampler leads to a geometrically uniformly ergodic Markov Chain. Several image generation experiments are provided in our paper to show the effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
我们在这篇论文中提出了一种名为STANLEY的Stochastic gradient ANisotropic LangEvin dynamics，用于采样高维数据。随着能量基模型（EBM）的生长效力和潜在性在高维数据观测的模型方面的不同种类数据观测上的应用，我们提出了一种终端学习算法，用于EBM的训练，以提高模型采样数据点的质量。由于EBM的未知正常化常量，使训练过程变得不可行，因此通常需要采用Markov Chain Monte Carlo（MCMC）方法。我们认为MCMC方法在EBM训练中的实现，需要一种适应非线性EBM背部的更新方法，这里是一个卷积神经网络。我们的方法，即STANLEY，是一种用我们新引入的MCMC方法进行EBM训练的优化算法。我们提供了对我们采样方案的理论理解，证明该采样方案导致一个几何上均匀 Erdős 链。我们的论文中还提供了一些图像生成实验，以证明我们的方法的效果。
</details></li>
</ul>
<hr>
<h2 id="SecurityNet-Assessing-Machine-Learning-Vulnerabilities-on-Public-Models"><a href="#SecurityNet-Assessing-Machine-Learning-Vulnerabilities-on-Public-Models" class="headerlink" title="SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models"></a>SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12665">http://arxiv.org/abs/2310.12665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/securitynet-research/securitynet">https://github.com/securitynet-research/securitynet</a></li>
<li>paper_authors: Boyang Zhang, Zheng Li, Ziqing Yang, Xinlei He, Michael Backes, Mario Fritz, Yang Zhang</li>
<li>for: 本研究旨在全面描述机器学习模型的安全和隐私漏洞，并在实际应用中进行评估。</li>
<li>methods: 本研究使用公开可用的模型 weights 从互联网（公共模型）来评估机器学习模型的攻击和防御方法。我们建立了一个名为 SecurityNet 的数据库，包含 910 个图像分类模型的注解。我们Then analyze the effectiveness of several representative attacks&#x2F;defenses, including model stealing attacks, membership inference attacks, and backdoor detection on these public models.</li>
<li>results: 我们的评估表明，使用公共模型进行评估时，攻击和防御方法的效果会异常 significatively 不同于使用自己训练的模型。我们将 SecurityNet 分享给研究人员，并建议他们在未来的研究中使用公共模型进行实验，以更好地证明他们的提议的方法的效果。<details>
<summary>Abstract</summary>
While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNet, containing 910 annotated image classification models. We then analyze the effectiveness of several representative attacks/defenses, including model stealing attacks, membership inference attacks, and backdoor detection on these public models. Our evaluation empirically shows the performance of these attacks/defenses can vary significantly on public models compared to self-trained models. We share SecurityNet with the research community. and advocate researchers to perform experiments on public models to better demonstrate their proposed methods' effectiveness in the future.
</details>
<details>
<summary>摘要</summary>
While advanced machine learning (ML) models are deployed in numerous real-world applications, previous works demonstrate these models have security and privacy vulnerabilities. Various empirical research has been done in this field. However, most of the experiments are performed on target ML models trained by the security researchers themselves. Due to the high computational resource requirement for training advanced models with complex architectures, researchers generally choose to train a few target models using relatively simple architectures on typical experiment datasets. We argue that to understand ML models' vulnerabilities comprehensively, experiments should be performed on a large set of models trained with various purposes (not just the purpose of evaluating ML attacks and defenses). To this end, we propose using publicly available models with weights from the Internet (public models) for evaluating attacks and defenses on ML models. We establish a database, namely SecurityNet, containing 910 annotated image classification models. We then analyze the effectiveness of several representative attacks/defenses, including model stealing attacks, membership inference attacks, and backdoor detection on these public models. Our evaluation empirically shows the performance of these attacks/defenses can vary significantly on public models compared to self-trained models. We share SecurityNet with the research community and advocate researchers to perform experiments on public models to better demonstrate their proposed methods' effectiveness in the future.
</details></li>
</ul>
<hr>
<h2 id="Knowledge-from-Uncertainty-in-Evidential-Deep-Learning"><a href="#Knowledge-from-Uncertainty-in-Evidential-Deep-Learning" class="headerlink" title="Knowledge from Uncertainty in Evidential Deep Learning"></a>Knowledge from Uncertainty in Evidential Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12663">http://arxiv.org/abs/2310.12663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cai Davies, Marc Roig Vilamala, Alun D. Preece, Federico Cerutti, Lance M. Kaplan, Supriyo Chakraborty</li>
<li>for: 这 paper 探讨了 Deep Learning 中的不确定性信号，具体来说是 Evidential Deep Learning (EDL) 中的不确定性信号。EDL 是一种提供测试样本上的信息量 (epistemic uncertainty) 的深度学习方法。</li>
<li>methods: 这 paper 使用了 Dirichlet 强度来捕捉 EDL 中的不确定性信号，并对 computer vision 和 bidirectional encoder large language models 进行了实验研究。</li>
<li>results: 研究发现，在某些情况下，EDL 的 &#96;evidential signal’ 可以区分类别，特别是使用大语言模型时。此外，研究还发现了 EDL 中的 KL 规则化项对 uncertainty 的影响。与其他 Dirichlet-based 方法比较，EDL 的不确定性 coupling 是由于训练时没有使用 OUT-OF-distribution 样本而导致的。<details>
<summary>Abstract</summary>
This work reveals an evidential signal that emerges from the uncertainty value in Evidential Deep Learning (EDL). EDL is one example of a class of uncertainty-aware deep learning approaches designed to provide confidence (or epistemic uncertainty) about the current test sample. In particular for computer vision and bidirectional encoder large language models, the `evidential signal' arising from the Dirichlet strength in EDL can, in some cases, discriminate between classes, which is particularly strong when using large language models. We hypothesise that the KL regularisation term causes EDL to couple aleatoric and epistemic uncertainty. In this paper, we empirically investigate the correlations between misclassification and evaluated uncertainty, and show that EDL's `evidential signal' is due to misclassification bias. We critically evaluate EDL with other Dirichlet-based approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior Networks, and show theoretically and empirically the differences between these loss functions. We conclude that EDL's coupling of uncertainty arises from these differences due to the use (or lack) of out-of-distribution samples during training.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个研究揭示了 Evidential Deep Learning (EDL) 中存在的证据信号。EDL 是一种uncertainty-aware深度学习方法，用于提供测试样本的信任度（或epistemic uncertainty）。特别是在计算机视觉和双向编码大语言模型中，EDL 中的 Dirichlet 强度可以在某些情况下区分类别，这是使用大语言模型时 particuarly strong。我们假设了 KL 正则项使得 EDL 将 aleatoric 和 epistemic uncertainty 相互关联。在这篇论文中，我们employmy empirical investigation 证明了 misclassification 和评估不确定性之间的相关性，并显示了 EDL 的 `evidential signal' 是因为分类偏见。我们还与其他 Dirichlet-based 方法，namely Generative Evidential Neural Networks (EDL-GEN) 和 Prior Networks，进行了比较，并通过理论和实验表明了这些损失函数之间的差异。我们结论认为，EDL 的不确定性归功于使用（或缺失）out-of-distribution 样本 durante training。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Descent-Fails-to-Learn-High-frequency-Functions-and-Modular-Arithmetic"><a href="#Gradient-Descent-Fails-to-Learn-High-frequency-Functions-and-Modular-Arithmetic" class="headerlink" title="Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic"></a>Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12660">http://arxiv.org/abs/2310.12660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rustem Takhanov, Maxat Tezekbayev, Artur Pak, Arman Bolatov, Zhenisbek Assylbekov</li>
<li>for: 本研究探讨了使用梯度基本法来训练高频 периоди函数或模块乘法的限制和挑战。</li>
<li>methods: 本研究使用了梯度分析来研究高频 periodic函数或模块乘法的训练难度。</li>
<li>results: 研究发现，当频率或基数$p$较大时，梯度的方差在高频 periodic函数或模块乘法中是非常小的，这使得使用梯度基本法进行训练变得困难。<details>
<summary>Abstract</summary>
Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function.   A set of functions of the form $x\to ax \bmod p$, where $a$ is taken from ${\mathbb Z}_p$, has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of $p$-periodic functions on ${\mathbb Z}$ and is tightly connected with a class of high-frequency periodic functions on the real line.   We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that the variance of the gradient is negligibly small in both cases when either a frequency or the prime base $p$ is large. This in turn prevents such a learning algorithm from being successful.
</details>
<details>
<summary>摘要</summary>
Classes of target functions containing a large number of approximately orthogonal elements are known to be difficult to learn using Statistical Query algorithms. Recently, this classical fact has resurfaced in the context of gradient-based optimization of neural networks. In the new framework, the difficulty of a class is typically measured by the variance of the gradient with respect to a random choice of a target function.A set of functions of the form $x\to ax \mod p$, where $a$ is chosen from $\mathbb{Z}_p$, has garnered attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of $p$-periodic functions on $\mathbb{Z}$ and is closely related to a class of high-frequency periodic functions on the real line.We present a mathematical analysis of the limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We show that the variance of the gradient is negligibly small in both cases when either the frequency or the prime base $p$ is large, which in turn prevents such a learning algorithm from being successful.
</details></li>
</ul>
<hr>
<h2 id="Inverse-Renormalization-Group-of-Disordered-Systems"><a href="#Inverse-Renormalization-Group-of-Disordered-Systems" class="headerlink" title="Inverse Renormalization Group of Disordered Systems"></a>Inverse Renormalization Group of Disordered Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12631">http://arxiv.org/abs/2310.12631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Bachtis</li>
<li>for: 研究三维杂 aligned spin glass 系统的粒子数量增长</li>
<li>methods: 使用 inverse renormalization group 变换和机器学习算法构建粒子数量增长的approximate配置</li>
<li>results: 在三维 Edwards-Anderson 模型中提取了两个极限常数Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to study the growth of the number of particles in the three-dimensional Edwards-Anderson spin glass model.</li>
<li>methods: The paper uses inverse renormalization group transformations and machine learning algorithms to construct approximate configurations for lattices with increasing particle numbers.</li>
<li>results: The paper extracts two critical exponents from the rescaled lattices.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
We propose inverse renormalization group transformations to construct approximate configurations for lattice volumes that have not yet been accessed by supercomputers or large-scale simulations in the study of spin glasses. Specifically, starting from lattices of volume $V=8^{3}$ in the case of the three-dimensional Edwards-Anderson model we employ machine learning algorithms to construct rescaled lattices up to $V'=128^{3}$, which we utilize to extract two critical exponents. We conclude by discussing how to incorporate numerical exactness within inverse renormalization group approaches of disordered systems, thus opening up the opportunity to explore a sustainable and energy-efficient generation of exact configurations for increasing lattice volumes without the use of dedicated supercomputers.
</details>
<details>
<summary>摘要</summary>
我们提出倒数重整化群变换来建构粗糙配置，以探索未曾被超级电脑或大规模模拟的磁铁玻璃系统。具体来说，从三维爱德华兹-安德逊模型的网格量$V=8^{3}$开始，我们使用机器学习算法建构缩小网格，直到$V'=128^{3}$，并将其用于提取两个极限常数。我们最后讨论如何在倒数重整化群变换方法中包含数据精度，以便在无需特别超级电脑的情况下，可以持续和可持续地生成精确配置，探索增加网格量的可能性。
</details></li>
</ul>
<hr>
<h2 id="An-Improved-Metarounding-Algorithm-via-Frank-Wolfe"><a href="#An-Improved-Metarounding-Algorithm-via-Frank-Wolfe" class="headerlink" title="An Improved Metarounding Algorithm via Frank-Wolfe"></a>An Improved Metarounding Algorithm via Frank-Wolfe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12629">http://arxiv.org/abs/2310.12629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rmitsuboshi/metarounding_mitsuboshi">https://github.com/rmitsuboshi/metarounding_mitsuboshi</a></li>
<li>paper_authors: Ryotaro Mitsuboshi, Kohei Hatano, Eiji Takimoto</li>
<li>for:  linear optimization over combinatorial classes</li>
<li>methods:  metarounding algorithm and relax-based approximation algorithm</li>
<li>results:  much more efficient in both theoretical and practical aspects<details>
<summary>Abstract</summary>
Metarounding is an approach to convert an approximation algorithm for linear optimization over some combinatorial classes to an online linear optimization algorithm for the same class. We propose a new metarounding algorithm under a natural assumption that a relax-based approximation algorithm exists for the combinatorial class. Our algorithm is much more efficient in both theoretical and practical aspects.
</details>
<details>
<summary>摘要</summary>
这是一种方法，可以将线性估计算法转换为在同一类别上的在线估计算法。我们提出了一个新的这种方法，基于自然的假设，存在一个松动基于的估计算法。我们的算法在理论和实践方面都比较高效。
</details></li>
</ul>
<hr>
<h2 id="How-a-student-becomes-a-teacher-learning-and-forgetting-through-Spectral-methods"><a href="#How-a-student-becomes-a-teacher-learning-and-forgetting-through-Spectral-methods" class="headerlink" title="How a student becomes a teacher: learning and forgetting through Spectral methods"></a>How a student becomes a teacher: learning and forgetting through Spectral methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12612">http://arxiv.org/abs/2310.12612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jamba15/spectral-regularization-teacher-student">https://github.com/jamba15/spectral-regularization-teacher-student</a></li>
<li>paper_authors: Lorenzo Giambagli, Lorenzo Buffoni, Lorenzo Chicchi, Duccio Fanelli</li>
<li>For: 本研究使用教师-学生模式来解释在实际辅导中的效果。在学生网络过参数化的情况下，这种模式特别有用，因为学生网络可能只需要一部分权重来处理任务。* Methods: 本研究提出了一种新的优化方案，基于层传递信息的spectral representation来计算梯度。与标准训练算法相比，该方法增加的计算复杂度和计算量几乎是零。* Results: 研究发现，在培养学生网络后，可以隔离一个稳定的学生子结构，该结构与教师网络的计算neuron数、路径分布和 topological attribute具有相似性。当去掉学生网络中无关的节点时，对记录的性能没有下降。这种行为可以被描述为一种真正的第二阶段相变，具有普遍性特征。<details>
<summary>Abstract</summary>
In theoretical ML, the teacher-student paradigm is often employed as an effective metaphor for real-life tuition. The above scheme proves particularly relevant when the student network is overparameterized as compared to the teacher network. Under these operating conditions, it is tempting to speculate that the student ability to handle the given task could be eventually stored in a sub-portion of the whole network. This latter should be to some extent reminiscent of the frozen teacher structure, according to suitable metrics, while being approximately invariant across different architectures of the student candidate network. Unfortunately, state-of-the-art conventional learning techniques could not help in identifying the existence of such an invariant subnetwork, due to the inherent degree of non-convexity that characterizes the examined problem. In this work, we take a leap forward by proposing a radically different optimization scheme which builds on a spectral representation of the linear transfer of information between layers. The gradient is hence calculated with respect to both eigenvalues and eigenvectors with negligible increase in terms of computational and complexity load, as compared to standard training algorithms. Working in this framework, we could isolate a stable student substructure, that mirrors the true complexity of the teacher in terms of computing neurons, path distribution and topological attributes. When pruning unimportant nodes of the trained student, as follows a ranking that reflects the optimized eigenvalues, no degradation in the recorded performance is seen above a threshold that corresponds to the effective teacher size. The observed behavior can be pictured as a genuine second-order phase transition that bears universality traits.
</details>
<details>
<summary>摘要</summary>
在理论机器学习中，教师-学生模式经常被用作有效的教学模式。上述方案特别适用于学生网络过参数化于教师网络。在这种情况下，可能存在一个学生可以处理给定任务的能力被储存在学生网络中的一个子网络中。这个子网络应该与教师结构相似，根据适当的指标，而且在不同学生候选网络架构下保持相对不变。然而，现有的普通学习技术无法确定这个不变子网络的存在，因为研究的问题具有内在的非对称性。在这项工作中，我们采用了一种极其不同的优化方案，基于层传递信息的线性转换的спектраль表示。因此，对于学生网络来说，计算梯度的时候不仅考虑权重，还考虑特征向量，这对标准训练算法而言增加了计算量和复杂性负担相对较少。在这个框架下，我们可以隔离出一个稳定的学生子结构，这个子结构与教师结构相似，包括计算neuron数、路径分布和 topological特征。当将训练后的学生网络中的不重要节点剪除，按照记录的性能来进行排名，则不会出现性能下降问题，直到达到教师大小的有效阈值。这种行为可以被描述为一种真正的第二阶段相对稳定过程，具有普遍性特征。
</details></li>
</ul>
<hr>
<h2 id="Causal-Similarity-Based-Hierarchical-Bayesian-Models"><a href="#Causal-Similarity-Based-Hierarchical-Bayesian-Models" class="headerlink" title="Causal Similarity-Based Hierarchical Bayesian Models"></a>Causal Similarity-Based Hierarchical Bayesian Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12595">http://arxiv.org/abs/2310.12595</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sophiewharrie/causal-similarity-based-hierarchical-bayesian-models">https://github.com/sophiewharrie/causal-similarity-based-hierarchical-bayesian-models</a></li>
<li>paper_authors: Sophie Wharrie, Samuel Kaski</li>
<li>for: 这个论文是为了解决机器学习中的泛化问题，即如何将已知数据推广到新数据上。</li>
<li>methods: 该论文提出了基于 causal similarity 的层次 Bayesian 模型，以提高泛化到新任务的能力。具体来说，该方法使用不同任务的 causal 机制之间的相似性来决定数据是否可以被Pool。</li>
<li>results: 通过对 simulate 数据和实际数据进行试验，该论文表明了该方法的优势和实际应用性。<details>
<summary>Abstract</summary>
The key challenge underlying machine learning is generalisation to new data. This work studies generalisation for datasets consisting of related tasks that may differ in causal mechanisms. For example, observational medical data for complex diseases suffers from heterogeneity in causal mechanisms of disease across patients, creating challenges for machine learning algorithms that need to generalise to new patients outside of the training dataset. Common approaches for learning supervised models with heterogeneous datasets include learning a global model for the entire dataset, learning local models for each tasks' data, or utilising hierarchical, meta-learning and multi-task learning approaches to learn how to generalise from data pooled across multiple tasks. In this paper we propose causal similarity-based hierarchical Bayesian models to improve generalisation to new tasks by learning how to pool data from training tasks with similar causal mechanisms. We apply this general modelling principle to Bayesian neural networks and compare a variety of methods for estimating causal task similarity (for both known and unknown causal models). We demonstrate the benefits of our approach and applicability to real world problems through a range of experiments on simulated and real data.
</details>
<details>
<summary>摘要</summary>
“ Machine learning 的主要挑战是为新数据进行扩展。这个工作研究了具有相关任务的数据集中的扩展问题。例如，观察医学数据可能受到病人间病理机制的不同，导致机器学习算法对新病人数据进行扩展具有挑战。常见的方法包括学习全域模型、学习每个任务的数据上的本地模型，或者使用层次、多任务学习方法来学习如何从多个任务中获得新数据的扩展。在这篇论文中，我们提出了因果相似性基于的层次确 Dirichlet 模型，以提高对新任务的扩展。我们将这个通用模型应用到 Bayesian 神经网络中，并比较了不同的方法来估计因果任务相似性（包括知道和未知因果模型）。我们透过一系列实验，证明了我们的方法的好处和实际应用性。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Julearn-an-easy-to-use-library-for-leakage-free-evaluation-and-inspection-of-ML-models"><a href="#Julearn-an-easy-to-use-library-for-leakage-free-evaluation-and-inspection-of-ML-models" class="headerlink" title="Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models"></a>Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12568">http://arxiv.org/abs/2310.12568</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juaml/julearn">https://github.com/juaml/julearn</a></li>
<li>paper_authors: Sami Hamdan, Shammi More, Leonard Sasse, Vera Komeyer, Kaustubh R. Patil, Federico Raimondo</li>
<li>for: 本研究旨在帮助不具有Machine Learning（ML）专业背景的研究人员在ML方面进行研究，避免一些常见的陷阱和错误。</li>
<li>methods: 本研究使用的方法包括创建了一个开源的Python库（julearn），该库提供了一个简单易用的环境，以避免一些最常见的ML陷阱，并且提供了一些特有的功能来帮助研究人员设计和评估复杂的ML管道。</li>
<li>results: 本研究通过三个实际案例示例，展示了julearn可以帮助研究人员轻松实现一些已经发表的研究项目，并且提供了一些特有的功能来帮助研究人员设计和评估复杂的ML管道。<details>
<summary>Abstract</summary>
The fast-paced development of machine learning (ML) methods coupled with its increasing adoption in research poses challenges for researchers without extensive training in ML. In neuroscience, for example, ML can help understand brain-behavior relationships, diagnose diseases, and develop biomarkers using various data sources like magnetic resonance imaging and electroencephalography. The primary objective of ML is to build models that can make accurate predictions on unseen data. Researchers aim to prove the existence of such generalizable models by evaluating performance using techniques such as cross-validation (CV), which uses systematic subsampling to estimate the generalization performance. Choosing a CV scheme and evaluating an ML pipeline can be challenging and, if used improperly, can lead to overestimated results and incorrect interpretations.   We created julearn, an open-source Python library, that allow researchers to design and evaluate complex ML pipelines without encountering in common pitfalls. In this manuscript, we present the rationale behind julearn's design, its core features, and showcase three examples of previously-published research projects that can be easily implemented using this novel library. Julearn aims to simplify the entry into the ML world by providing an easy-to-use environment with built in guards against some of the most common ML pitfalls. With its design, unique features and simple interface, it poses as a useful Python-based library for research projects.
</details>
<details>
<summary>摘要</summary>
Machine learning (ML) 技术的快速发展和在研究中的普及，使得不具有深入学习 ML 背景的研究者面临着挑战。例如，在神经科学中，ML 可以帮助理解大脑行为关系、诊断疾病和开发生物标志物理化学。ML 的主要目标是建立可以在未看到数据上做准确预测的模型。研究人员希望通过CV技术（系统性采样）来评估ML管道的性能，以证明模型的普遍性。然而，选择CV方案和评估ML管道可以是困难的，如果不当使用，可能会导致结果过分估计和错误解释。为了解决这些问题，我们开发了 jullearn，一个开源的 Python 库。julearn 使得研究人员可以设计和评估复杂的 ML 管道，而无需遇到常见的陷阱。在这篇论文中，我们介绍了 jullearn 的设计理念、核心特点和三个已经发表的研究项目的实现。julearn 希望通过提供简单易用的环境，帮助研究者更容易进入 ML 世界，并提供了一些常见 ML 陷阱的防范机制。与其他 Python 基础库相比，julearn 具有独特的设计和简单的界面，成为一个有用的 Python 库 для研究项目。
</details></li>
</ul>
<hr>
<h2 id="Open-World-Lifelong-Graph-Learning"><a href="#Open-World-Lifelong-Graph-Learning" class="headerlink" title="Open-World Lifelong Graph Learning"></a>Open-World Lifelong Graph Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12565">http://arxiv.org/abs/2310.12565</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bobowner/open-world-lgl">https://github.com/bobowner/open-world-lgl</a></li>
<li>paper_authors: Marcel Hoffmann, Lukas Galke, Ansgar Scherp</li>
<li>for: 本研究探讨开放世界下的生命长图学习问题，模型需要处理新任务和可能未知的类。</li>
<li>methods: 我们利用Open-of-Distribution（OOD）探测方法识别新类，并将非图数据上的OOD探测方法适应到图数据上。我们建议将新类探测与图域信息结合进行进行。大多数OOD探测方法忽略确定Vertex是OOD的明确阈值。为解决这个问题，我们提出了弱监睹反馈（Open-WRF）方法，它降低了OOD探测中对阈值的敏感性。</li>
<li>results: 我们对六个 benchmark 数据集进行评估，结果表明我们提出的邻居聚合方法在OOD探测中超过了现有方法独立于图神经网络。此外，我们还证明了我们的Open-WRF方法在阈值选择上更加稳定，并分析了图域对OOD探测的影响。聚合和阈值方法可以与任何图神经网络和OOD探测方法结合使用，使我们的方法强大和适用于许多实际应用。<details>
<summary>Abstract</summary>
We study the problem of lifelong graph learning in an open-world scenario, where a model needs to deal with new tasks and potentially unknown classes. We utilize Out-of-Distribution (OOD) detection methods to recognize new classes and adapt existing non-graph OOD detection methods to graph data. Crucially, we suggest performing new class detection by combining OOD detection methods with information aggregated from the graph neighborhood. Most OOD detection methods avoid determining a crisp threshold for deciding whether a vertex is OOD. To tackle this problem, we propose a Weakly-supervised Relevance Feedback (Open-WRF) method, which decreases the sensitivity to thresholds in OOD detection. We evaluate our approach on six benchmark datasets. Our results show that the proposed neighborhood aggregation method for OOD scores outperforms existing methods independent of the underlying graph neural network. Furthermore, we demonstrate that our Open-WRF method is more robust to threshold selection and analyze the influence of graph neighborhood on OOD detection. The aggregation and threshold methods are compatible with arbitrary graph neural networks and OOD detection methods, making our approach versatile and applicable to many real-world applications.
</details>
<details>
<summary>摘要</summary>
我们研究开放世界enario下的生命周期图学习问题，模型需要处理新任务和可能未知的类别。我们利用外部 Distribution（OOD）检测方法来识别新类别，并将非图形OOD检测方法应用到图 Daten。在新类别检测中，我们建议结合OOD检测方法和图ogram neighborhood中的信息。大多数OOD检测方法避免明确的阈值来决定顶点是否为外部，来解决这个问题，我们提出了弱监督的相关反馈方法（Open-WRF）。我们将这个方法应用到六个标准资料集上，结果显示，我们的邻居统计方法在不同的图形神经网络下表现出色，并且比独立的OOD检测方法更有效。此外，我们还证明了我们的Open-WRF方法具有更好的韧性，并分析了图ogram neighborhood对OOD检测的影响。这些统计和阈值方法是对任何图形神经网络和OOD检测方法都适用的，因此我们的方法是多元的和适用于实际应用。
</details></li>
</ul>
<hr>
<h2 id="Approximate-information-maximization-for-bandit-games"><a href="#Approximate-information-maximization-for-bandit-games" class="headerlink" title="Approximate information maximization for bandit games"></a>Approximate information maximization for bandit games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12563">http://arxiv.org/abs/2310.12563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Barbier-Chebbah, Christian L. Vestergaard, Jean-Baptiste Masson, Etienne Boursier</li>
<li>for: 模型物理系统的动态，如大脑做出决策、访问隐藏变量等</li>
<li>methods: 使用自由能原理和信息瓶颈原理进行优化</li>
<li>results: 提出了一种基于信息最大化的新型抽筋算法，可以在классиical bandit设置中实现强表现，并且在二手抽筋问题中证明其 asymptotic optimality。<details>
<summary>Abstract</summary>
Entropy maximization and free energy minimization are general physical principles for modeling the dynamics of various physical systems. Notable examples include modeling decision-making within the brain using the free-energy principle, optimizing the accuracy-complexity trade-off when accessing hidden variables with the information bottleneck principle (Tishby et al., 2000), and navigation in random environments using information maximization (Vergassola et al., 2007). Built on this principle, we propose a new class of bandit algorithms that maximize an approximation to the information of a key variable within the system. To this end, we develop an approximated analytical physics-based representation of an entropy to forecast the information gain of each action and greedily choose the one with the largest information gain. This method yields strong performances in classical bandit settings. Motivated by its empirical success, we prove its asymptotic optimality for the two-armed bandit problem with Gaussian rewards. Owing to its ability to encompass the system's properties in a global physical functional, this approach can be efficiently adapted to more complex bandit settings, calling for further investigation of information maximization approaches for multi-armed bandit problems.
</details>
<details>
<summary>摘要</summary>
entropy maximization和自由能 minimization是物理系统的通用原理，用于模型各种物理系统的动态。其中一些例子包括使用自由能原理模型大脑做出决策（Tishby et al., 2000）、使用信息瓶颈原理（Vergassola et al., 2007）在随机环境中导航，以及最大化系统中变量的信息。基于这个原理，我们提出了一种新的bandit算法，该算法可以最大化系统中变量的信息。为此，我们开发了一个近似analytical physics-based表示，用于预测每个动作的信息增加。这种方法在 klasische bandit设置中实现了强的表现。受其实际成功的激励，我们证明了其在两手bandit问题上的极限优化性。由于它可以尝试系统的全局物理函数，这种方法可以有效地适应更复杂的bandit设置，这叫做更多的研究信息最大化方法在多手bandit问题上。
</details></li>
</ul>
<hr>
<h2 id="Fast-Model-Debias-with-Machine-Unlearning"><a href="#Fast-Model-Debias-with-Machine-Unlearning" class="headerlink" title="Fast Model Debias with Machine Unlearning"></a>Fast Model Debias with Machine Unlearning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12560">http://arxiv.org/abs/2310.12560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu</li>
<li>for: 这篇论文旨在解决深度神经网络中的偏见问题，尤其是在实际应用中可能会导致不公正的结果。</li>
<li>methods: 本论文提出了一个快速的模型偏见评估和移除框架（FMD），可以快速地识别、评估和移除训练过的模型中的偏见。FMD使用明确的counterfactual概念和影响函数来识别偏见的来源，并且设计了机器学习推卸的策略来实现快速和有效地移除偏见。</li>
<li>results: 在颜色MNIST、CelebA和成人收入数据集上进行了实验，结果显示了我们的方法可以与现有的方法相比，实现更高的准确性，同时具有更低的偏见水平和训练成本。此外，我们的方法只需要一小量的外部数据和更新少量的模型参数，不需要训练数据的存取权。<details>
<summary>Abstract</summary>
Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets along with experiments with large language models demonstrate that our method achieves superior or competing accuracies compared with state-of-the-art methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.
</details>
<details>
<summary>摘要</summary>
最近的发现表明深度神经网络在实际场景中可能会具有偏见性。例如，在大规模人脸识别数据集CelebA上训练的深度网络往往预测女性拥有金色头发，男性拥有黑色头发。这些偏见不仅会影响模型的稳定性，还可能扩大和加剧社会偏见，尤其是在自动决策过程中，这可能会加剧不公平的经济和社会不平等。现有的偏见纠正方法存在高成本的偏见标签或模型重新训练的问题，同时也存在评估偏见来源的缺陷。为此，我们提出了一种快速模型偏见纠正框架（FMD），它提供了一种有效的方法来识别、评估和除去训练过程中的偏见。FMD使用明确的对立思想来识别偏见的特征，并使用数据样本的影响函数来评估偏见的影响。此外，我们还设计了一种基于机器学习的“机器忘记”策略，可以高效地和有效地除去偏见，只需要一小量的对立数据集和更新少量的模型参数。实验表明，我们的方法在颜色MNIST、CelebA和成人收入数据集上以及与大型自然语言模型进行实验，均可以达到或超越当前状态艺的准确率，而且需要远 fewer 偏见和更少的偏见纠正成本。尤其是，我们的方法只需要一小部分的外部数据集和更新少量的模型参数，不需要训练数据的大小或可用性。
</details></li>
</ul>
<hr>
<h2 id="Neural-Likelihood-Approximation-for-Integer-Valued-Time-Series-Data"><a href="#Neural-Likelihood-Approximation-for-Integer-Valued-Time-Series-Data" class="headerlink" title="Neural Likelihood Approximation for Integer Valued Time Series Data"></a>Neural Likelihood Approximation for Integer Valued Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12544">http://arxiv.org/abs/2310.12544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke O’Loughlin, John Maclean, Andrew Black</li>
<li>for: 这种模型用于 capture 物理和生物科学中的小系统动态，它们的个体特性不能忽略， Stochastic effects 是重要的。</li>
<li>methods: 我们使用 causal convolutions 构建了一种神经网络 posterior approximation，可以并发评估整个时间序列的可能性。</li>
<li>results: 我们在一些生态和疫情学模型中进行了推断，并证明我们可以准确地 aproximate 真实 posterior，同时 achieve  significiant 计算速度提升。<details>
<summary>Abstract</summary>
Stochastic processes defined on integer valued state spaces are popular within the physical and biological sciences. These models are necessary for capturing the dynamics of small systems where the individual nature of the populations cannot be ignored and stochastic effects are important. The inference of the parameters of such models, from time series data, is difficult due to intractability of the likelihood; current methods, based on simulations of the underlying model, can be so computationally expensive as to be prohibitive. In this paper we construct a neural likelihood approximation for integer valued time series data using causal convolutions, which allows us to evaluate the likelihood of the whole time series in parallel. We demonstrate our method by performing inference on a number of ecological and epidemiological models, showing that we can accurately approximate the true posterior while achieving significant computational speed ups in situations where current methods struggle.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose a neural likelihood approximation for integer-valued time series data based on causal convolutions. This approach enables us to evaluate the likelihood of the entire time series in parallel, achieving significant computational speedups. We demonstrate the effectiveness of our method by performing inference on several ecological and epidemiological models, showing that we can accurately approximate the true posterior while achieving substantial computational savings in situations where current methods struggle.
</details></li>
</ul>
<hr>
<h2 id="Constructing-Impactful-Machine-Learning-Research-for-Astronomy-Best-Practices-for-Researchers-and-Reviewers"><a href="#Constructing-Impactful-Machine-Learning-Research-for-Astronomy-Best-Practices-for-Researchers-and-Reviewers" class="headerlink" title="Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers"></a>Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12528">http://arxiv.org/abs/2310.12528</a></li>
<li>repo_url: None</li>
<li>paper_authors: D. Huppenkothen, M. Ntampaka, M. Ho, M. Fouesneau, B. Nord, J. E. G. Peek, M. Walmsley, J. F. Wu, C. Avestruz, T. Buck, M. Brescia, D. P. Finkbeiner, A. D. Goulding, T. Kacprzak, P. Melchior, M. Pasquato, N. Ramachandra, Y. -S. Ting, G. van de Ven, S. Villar, V. A. Villar, E. Zinger</li>
<li>for: 本研究旨在为天文学界提供机器学习模型的使用指南，以确保结果的准确性、复现性和方法的有用性。</li>
<li>methods: 本研究使用机器学习模型来解决天文学问题，并提供了一些最佳实践和挑战。</li>
<li>results: 本研究提出了一种方法来报告机器学习模型的结果，以便帮助作者、评审人和编辑者更好地理解和复制研究结果。<details>
<summary>Abstract</summary>
Machine learning has rapidly become a tool of choice for the astronomical community. It is being applied across a wide range of wavelengths and problems, from the classification of transients to neural network emulators of cosmological simulations, and is shifting paradigms about how we generate and report scientific results. At the same time, this class of method comes with its own set of best practices, challenges, and drawbacks, which, at present, are often reported on incompletely in the astrophysical literature. With this paper, we aim to provide a primer to the astronomical community, including authors, reviewers, and editors, on how to implement machine learning models and report their results in a way that ensures the accuracy of the results, reproducibility of the findings, and usefulness of the method.
</details>
<details>
<summary>摘要</summary>
机器学习已经迅速成为天文学界的工具之一。它在各种波长和问题上应用，从脉冲分类到神经网络模拟 cosmological  simulations，并在科学结果的生成和报告方面引发了 paradigm shift。然而，这种类型的方法也有自己的最佳实践、挑战和缺点，现在 frequently 在astrophysical  literature中报道不够 completely。本文的目标是为天文学界提供一份指南，包括作者、评审人和编辑，如何实施机器学习模型和报告结果，以确保结果的准确性、结果的重复性和方法的有用性。
</details></li>
</ul>
<hr>
<h2 id="Parallel-Bayesian-Optimization-Using-Satisficing-Thompson-Sampling-for-Time-Sensitive-Black-Box-Optimization"><a href="#Parallel-Bayesian-Optimization-Using-Satisficing-Thompson-Sampling-for-Time-Sensitive-Black-Box-Optimization" class="headerlink" title="Parallel Bayesian Optimization Using Satisficing Thompson Sampling for Time-Sensitive Black-Box Optimization"></a>Parallel Bayesian Optimization Using Satisficing Thompson Sampling for Time-Sensitive Black-Box Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12526">http://arxiv.org/abs/2310.12526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobin Song, Benben Jiang</li>
<li>for: 这个研究ocuses on time-sensitive black-box optimization problems, and proposes a satisficing Thompson sampling-based parallel Bayesian optimization (STS-PBO) approach to solve these problems.</li>
<li>methods: The proposed STS-PBO approach uses the rate-distortion theory to construct a loss function that balances the amount of information that needs to be learned with sub-optimality, and the Blahut-Arimoto algorithm to compute the target solution that reaches the minimum information rate under the distortion limit at each step.</li>
<li>results: The proposed STS-PBO methods outperform both sequential counterparts and parallel BO with traditional Thompson sampling in both synchronous and asynchronous settings, as demonstrated on a fast-charging design problem of Lithium-ion batteries.Here is the Chinese version of the three key points:</li>
<li>for: 这个研究ocuses on时间敏感黑盒优化问题，并提出了一个 satisficing Thompson sampling-based parallel Bayesian optimization (STS-PBO) 方法来解决这些问题。</li>
<li>methods: 提案的 STS-PBO 方法使用了率调法则来建构一个具有优化与不完整性之间的平衡的损失函数，并使用 Blahut-Arimoto 算法来compute每步的目标解答。</li>
<li>results: 提案的 STS-PBO 方法在同步和异步设定下，均能超越统计类似的序列对照和传统 Thompson sampling 的平行BO，并在快充电设计问题上显示了有效性。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) is widely used for black-box optimization problems, and have been shown to perform well in various real-world tasks. However, most of the existing BO methods aim to learn the optimal solution, which may become infeasible when the parameter space is extremely large or the problem is time-sensitive. In these contexts, switching to a satisficing solution that requires less information can result in better performance. In this work, we focus on time-sensitive black-box optimization problems and propose satisficing Thompson sampling-based parallel Bayesian optimization (STS-PBO) approaches, including synchronous and asynchronous versions. We shift the target from an optimal solution to a satisficing solution that is easier to learn. The rate-distortion theory is introduced to construct a loss function that balances the amount of information that needs to be learned with sub-optimality, and the Blahut-Arimoto algorithm is adopted to compute the target solution that reaches the minimum information rate under the distortion limit at each step. Both discounted and undiscounted Bayesian cumulative regret bounds are theoretically derived for the proposed STS-PBO approaches. The effectiveness of the proposed methods is demonstrated on a fast-charging design problem of Lithium-ion batteries. The results are accordant with theoretical analyses, and show that our STS-PBO methods outperform both sequential counterparts and parallel BO with traditional Thompson sampling in both synchronous and asynchronous settings.
</details>
<details>
<summary>摘要</summary>
泛bayesian优化（BO）广泛应用于黑盒优化问题中，并在实际任务中表现良好。然而，大多数现有BO方法寻求学习最优解决方案，可能在参数空间很大或问题时间敏感时变得不可能。在这些情况下，切换到一个满足解决方案可能更有利。在这项工作中，我们关注时间敏感黑盒优化问题，并提出了一种基于满足 Thompson sampling 的并行 Bayesian 优化方法（STS-PBO），包括同步和异步版本。我们将目标从最优解决方案转换到一个更容易学习的满足解决方案。基于信息率-质量衡量理论，我们构建了一个损失函数，该函数平衡学习所需的信息量与不足的质量之间的平衡。我们采用了Blahut-Arimoto算法来计算每步目标解决方案，以达到最小信息率下的损失最小化。我们在 theoretically  deriv了对 STS-PBO 方法的 Bayesian 束违率下界，以及不COUNT 束违率下界。实验结果表明，我们的 STS-PBO 方法在同步和异步设置下都超过了序列对应方法和传统 Thompson sampling 并行 BO 方法。
</details></li>
</ul>
<hr>
<h2 id="WeaveNet-for-Approximating-Two-sided-Matching-Problems"><a href="#WeaveNet-for-Approximating-Two-sided-Matching-Problems" class="headerlink" title="WeaveNet for Approximating Two-sided Matching Problems"></a>WeaveNet for Approximating Two-sided Matching Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12515">http://arxiv.org/abs/2310.12515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shusaku Sone, Jiaxin Ma, Atsushi Hashimoto, Naoya Chiba, Yoshitaka Ushiku</li>
<li>for: This paper is written for optimizing the assignment of limited resources under various constraints, with a focus on the task of matching in bipartite graphs.</li>
<li>methods: The paper proposes a novel graph neural network (GNN) called WeaveNet, which is designed to preserve edge-wise information while passing messages densely to reach a better solution for matching problems.</li>
<li>results: Despite being a general-purpose model, WeaveNet achieved a comparable performance with state-of-the-art algorithms for fair stable matching, even for small numbers of agents.<details>
<summary>Abstract</summary>
Matching, a task to optimally assign limited resources under constraints, is a fundamental technology for society. The task potentially has various objectives, conditions, and constraints; however, the efficient neural network architecture for matching is underexplored. This paper proposes a novel graph neural network (GNN), \textit{WeaveNet}, designed for bipartite graphs. Since a bipartite graph is generally dense, general GNN architectures lose node-wise information by over-smoothing when deeply stacked. Such a phenomenon is undesirable for solving matching problems. WeaveNet avoids it by preserving edge-wise information while passing messages densely to reach a better solution. To evaluate the model, we approximated one of the \textit{strongly NP-hard} problems, \textit{fair stable matching}. Despite its inherent difficulties and the network's general purpose design, our model reached a comparative performance with state-of-the-art algorithms specially designed for stable matching for small numbers of agents.
</details>
<details>
<summary>摘要</summary>
匹配任务是社会基础技术之一，目标是最优分配有限资源于约束下。这个任务可能有多种目标、条件和约束，但是现有的神经网络架构仍然未得到充分探索。这篇论文提出了一种新的图 neural network（GNN），称为 WeaveNet，用于二分图。由于二分图通常是密集的，通常的GNN架构在深层核stacking时会导致节点信息产生泛化，这是解决匹配问题的不希望的现象。WeaveNet则避免了这种现象，通过保持边信息而传递消息，以达到更好的解决方案。为评估模型，我们约化了一个“strongly NP-hard”的问题——公平稳定匹配。尽管这个问题具有内在的困难和网络通用设计，我们的模型仍然可以与特定为稳定匹配的状态静态算法相比，在小量代理人情况下达到了相似的表现。
</details></li>
</ul>
<hr>
<h2 id="American-Option-Pricing-using-Self-Attention-GRU-and-Shapley-Value-Interpretation"><a href="#American-Option-Pricing-using-Self-Attention-GRU-and-Shapley-Value-Interpretation" class="headerlink" title="American Option Pricing using Self-Attention GRU and Shapley Value Interpretation"></a>American Option Pricing using Self-Attention GRU and Shapley Value Interpretation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12500">http://arxiv.org/abs/2310.12500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanhui Shen</li>
<li>For: The paper is written for investors and financial analysts who want to use machine learning methods to predict the prices of SPY (ETF) options.* Methods: The paper proposes using a gated recurrent unit (GRU) and self-attention mechanism to forecast the prices of SPY options. The authors also compare the performance of their model with traditional binomial models and other machine learning models.* Results: The paper shows that the self-attention GRU model with historical data outperforms other models in predicting the prices of SPY options, and provides insights into the significance and contributions of different input features on option pricing using the SHAP method.<details>
<summary>Abstract</summary>
Options, serving as a crucial financial instrument, are used by investors to manage and mitigate their investment risks within the securities market. Precisely predicting the present price of an option enables investors to make informed and efficient decisions. In this paper, we propose a machine learning method for forecasting the prices of SPY (ETF) option based on gated recurrent unit (GRU) and self-attention mechanism. We first partitioned the raw dataset into 15 subsets according to moneyness and days to maturity criteria. For each subset, we matched the corresponding U.S. government bond rates and Implied Volatility Indices. This segmentation allows for a more insightful exploration of the impacts of risk-free rates and underlying volatility on option pricing. Next, we built four different machine learning models, including multilayer perceptron (MLP), long short-term memory (LSTM), self-attention LSTM, and self-attention GRU in comparison to the traditional binomial model. The empirical result shows that self-attention GRU with historical data outperforms other models due to its ability to capture complex temporal dependencies and leverage the contextual information embedded in the historical data. Finally, in order to unveil the "black box" of artificial intelligence, we employed the SHapley Additive exPlanations (SHAP) method to interpret and analyze the prediction results of the self-attention GRU model with historical data. This provides insights into the significance and contributions of different input features on the pricing of American-style options.
</details>
<details>
<summary>摘要</summary>
Options, 作为投资工具，可以帮助投资者在证券市场中管理和减轻投资风险。正确预测现有选择价格可以帮助投资者做出 Informed 和高效的决策。在这篇论文中，我们提出了一种基于 GRU 和自注意机制的机器学习方法，用于预测 SPY (ETF) 选择价格。我们首先将原始数据 partitioned 成 15 个subset，根据资产价值和到期日的 criterion。对于每个subset，我们匹配了相应的美国政府债券利率和假设权益指数。这种分 segmentation 允许我们更深入地探索风险自由率和下跌权益对选择价格的影响。接着，我们建立了四种不同的机器学习模型，包括多层感知器 (MLP)、长短期记忆 (LSTM)、自注意 LSTM 和自注意 GRU。与传统 binomial 模型相比，自注意 GRU  WITH 历史数据表现最佳，这是因为它可以捕捉复杂的时间相关性和利用历史数据中嵌入的上下文信息。最后，我们使用 SHAP 方法来解释和分析 self-attention GRU 模型 WITH 历史数据的预测结果，这提供了对 American-style 选择价格的预测结果的解释和分析。
</details></li>
</ul>
<hr>
<h2 id="Quasi-Manhattan-Wasserstein-Distance"><a href="#Quasi-Manhattan-Wasserstein-Distance" class="headerlink" title="Quasi Manhattan Wasserstein Distance"></a>Quasi Manhattan Wasserstein Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12498">http://arxiv.org/abs/2310.12498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/evlim/qmwd">https://github.com/evlim/qmwd</a></li>
<li>paper_authors: Evan Unit Lim</li>
<li>for: 这个论文主要是为了解释Quasi Manhattan Wasserstein Distance（QMWD），它是一种用于衡量两个矩阵之间的不同程度的度量，它将沃斯特朗纳度与特定的变换相结合。</li>
<li>methods: 这篇论文使用了特定的变换和矩阵的组合来计算QMWD，具有更高的时间和空间复杂度的 Manhattan Wasserstein Distance（MWD）的缺点，QMWD可以提供更好的时间和空间复杂度。</li>
<li>results: 论文通过对QMWD的计算和复杂度分析，以及与MWD和WD的比较，证明了QMWD在大型数据集或有限的计算资源下的优势。<details>
<summary>Abstract</summary>
The Quasi Manhattan Wasserstein Distance (QMWD) is a metric designed to quantify the dissimilarity between two matrices by combining elements of the Wasserstein Distance with specific transformations. It offers improved time and space complexity compared to the Manhattan Wasserstein Distance (MWD) while maintaining accuracy. QMWD is particularly advantageous for large datasets or situations with limited computational resources. This article provides a detailed explanation of QMWD, its computation, complexity analysis, and comparisons with WD and MWD.
</details>
<details>
<summary>摘要</summary>
“伪 Manhattan Wasserstein 距离”（QMWD）是一个计量，用于量化两个矩阵之间的不同程度，通过组合 Wasserstein 距离和特定的变换。它提供了与 Manhattan Wasserstein 距离（MWD）相同的精度，但时间和空间复杂度更低。QMWD 特别适合大规模的数据或有限的计算资源的情况。本文将提供 QMWD 的详细解释、计算、时间复杂度分析以及与 WD 和 MWD 的比较。
</details></li>
</ul>
<hr>
<h2 id="SDGym-Low-Code-Reinforcement-Learning-Environments-using-System-Dynamics-Models"><a href="#SDGym-Low-Code-Reinforcement-Learning-Environments-using-System-Dynamics-Models" class="headerlink" title="SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models"></a>SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12494">http://arxiv.org/abs/2310.12494</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Emmanuel Klu, Sameer Sethi, DJ Passey, Donald Martin Jr</li>
<li>for: 本研究旨在探讨algorithmic intervention在社会长期影响下的负责任AI的发展。</li>
<li>methods: 本研究使用了reinforcement learning（RL）和system dynamics（SD）两种方法，RL可以在动态环境中优化决策，但是在实际 Setting中建立 robust agent具有困难。为解决这个问题，本研究借鉴了SD的方法，将SD模型作为RL环境的一部分。</li>
<li>results: 研究发现，可以使用SD模型生成高质量的RL环境，并且可以使用RL来改进SD模型中的动态策略发现。这些发现开示了SD和RL之间的双重潜在性，并且预示了这两种方法在负责任AI中的潜在合作性。<details>
<summary>Abstract</summary>
Understanding the long-term impact of algorithmic interventions on society is vital to achieving responsible AI. Traditional evaluation strategies often fall short due to the complex, adaptive and dynamic nature of society. While reinforcement learning (RL) can be a powerful approach for optimizing decisions in dynamic settings, the difficulty of realistic environment design remains a barrier to building robust agents that perform well in practical settings. To address this issue we tap into the field of system dynamics (SD) as a complementary method that incorporates collaborative simulation model specification practices. We introduce SDGym, a low-code library built on the OpenAI Gym framework which enables the generation of custom RL environments based on SD simulation models. Through a feasibility study we validate that well specified, rich RL environments can be generated from preexisting SD models and a few lines of configuration code. We demonstrate the capabilities of the SDGym environment using an SD model of the electric vehicle adoption problem. We compare two SD simulators, PySD and BPTK-Py for parity, and train a D4PG agent using the Acme framework to showcase learning and environment interaction. Our preliminary findings underscore the dual potential of SD to improve RL environment design and for RL to improve dynamic policy discovery within SD models. By open-sourcing SDGym, the intent is to galvanize further research and promote adoption across the SD and RL communities, thereby catalyzing collaboration in this emerging interdisciplinary space.
</details>
<details>
<summary>摘要</summary>
理解算法干预对社会的长期影响是负责任AI的关键。传统评估策略常常因社会的复杂、适应和动态性而受限。而强化学习（RL）可以在动态设置中优化决策，但是在实际设置中建立坚实的代理人表现仍然是一个障碍。为解决这个问题，我们借鉴系统动态学（SD）作为补充方法，该方法包括合作模拟模型规范实践。我们介绍了SDGym，一个基于OpenAI Gym框架的低代码库，可以生成基于SD模型的自定义RL环境。经过一项可行性研究，我们证明了可以从现有的SD模型和一些配置代码生成高质量的RL环境。我们使用Acme框架和D4PG算法对SDGym环境进行了示例训练，并对PySD和BPTK-Py两个SD模拟器进行了比较。我们的初步发现表明SD可以提高RL环境设计，同时RL也可以提高动态政策发现在SD模型中。我们将SDGym公开开源，以促进研究和采用，并且激发SD和RL社区之间的合作，以便在这个新兴交叉领域中推动进步。
</details></li>
</ul>
<hr>
<h2 id="Improved-Operator-Learning-by-Orthogonal-Attention"><a href="#Improved-Operator-Learning-by-Orthogonal-Attention" class="headerlink" title="Improved Operator Learning by Orthogonal Attention"></a>Improved Operator Learning by Orthogonal Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12487">http://arxiv.org/abs/2310.12487</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhijie-group/orthogonal-neural-operator">https://github.com/zhijie-group/orthogonal-neural-operator</a></li>
<li>paper_authors: Zipeng Xiao, Zhongkai Hao, Bokai Lin, Zhijie Deng, Hang Su</li>
<li>for: 学习 partial differential equations (PDEs) 的解决方案</li>
<li>methods: 使用 attention-based neural operators 和 eigendecomposition 进行正则化</li>
<li>results: 在六个标准 benchmark 数据集上，我们的方法可以与基准线性比例出色得分强强Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to explore the use of neural operators for solving partial differential equations (PDEs).</li>
<li>methods: The paper uses attention-based neural operators, which have become a popular approach in the field of scientific machine learning. However, the authors note that existing approaches can suffer from overfitting due to the large number of parameters in the attention mechanism. To address this, they propose an orthogonal attention method based on eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions.</li>
<li>results: The authors report results on six standard benchmark datasets, including both regular and irregular geometries. Their method outperforms competing baselines with a decent margin, indicating the effectiveness of the proposed approach.<details>
<summary>Abstract</summary>
Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.
</details>
<details>
<summary>摘要</summary>
射频运算符，作为科学机器学习中解决 partialling differential equations（PDEs）的有效代理模型，在科学机器学习领域得到了广泛的关注。其中，关注基于的射频运算符在相关研究中成为了主流。然而，现有的方法因射频机制中参数的较大数量而导致过度适应训练数据。为解决这问题，我们开发了基于 eigendecomposition 的射频运算符和神经函数近似的正交化方法。这种正交化自然地对 resulting neural operator 进行了正确的规范化效果，帮助抵御过度适应和提高泛化。在六个标准射频运算符 benchmark 数据集上（包括正则和不规则的geometry）进行了实验，我们发现我们的方法可以与其他基准值相比，表现出较好的性能。
</details></li>
</ul>
<hr>
<h2 id="Balanced-Group-Convolution-An-Improved-Group-Convolution-Based-on-Approximability-Estimates"><a href="#Balanced-Group-Convolution-An-Improved-Group-Convolution-Based-on-Approximability-Estimates" class="headerlink" title="Balanced Group Convolution: An Improved Group Convolution Based on Approximability Estimates"></a>Balanced Group Convolution: An Improved Group Convolution Based on Approximability Estimates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12461">http://arxiv.org/abs/2310.12461</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youngkyu Lee, Jongho Park, Chang-Ock Lee</li>
<li>for: 提高神经网络性能的方法</li>
<li>methods: 使用 grouped convolution 减少计算成本</li>
<li>results: 对 grouped convolution 的数学分析和一种新的变体 balanced group convolution，以及对其他变体的比较<details>
<summary>Abstract</summary>
The performance of neural networks has been significantly improved by increasing the number of channels in convolutional layers. However, this increase in performance comes with a higher computational cost, resulting in numerous studies focused on reducing it. One promising approach to address this issue is group convolution, which effectively reduces the computational cost by grouping channels. However, to the best of our knowledge, there has been no theoretical analysis on how well the group convolution approximates the standard convolution. In this paper, we mathematically analyze the approximation of the group convolution to the standard convolution with respect to the number of groups. Furthermore, we propose a novel variant of the group convolution called balanced group convolution, which shows a higher approximation with a small additional computational cost. We provide experimental results that validate our theoretical findings and demonstrate the superior performance of the balanced group convolution over other variants of group convolution.
</details>
<details>
<summary>摘要</summary>
“神经网络的性能已经由通道数的增加在卷积层中得到了显著改善。然而，这种改善的成本增加了计算成本，导致了许多关于减少计算成本的研究。一种有前途的方法是组卷积，它可以有效地减少计算成本。然而，到目前为止，我们没有对组卷积和标准卷积之间的相似性进行了理论分析。在这篇论文中，我们对组卷积和标准卷积之间的相似性进行了数学分析，并考虑了分组数量对相似性的影响。此外，我们还提出了一种新的组卷积变体 called 平衡组卷积，它在小加计算成本的情况下显示了更高的相似性。我们提供了实验结果，证明了我们的理论发现和平衡组卷积的超越性。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="MuseGNN-Interpretable-and-Convergent-Graph-Neural-Network-Layers-at-Scale"><a href="#MuseGNN-Interpretable-and-Convergent-Graph-Neural-Network-Layers-at-Scale" class="headerlink" title="MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale"></a>MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12457">http://arxiv.org/abs/2310.12457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haitian Jiang, Renjie Liu, Xiao Yan, Zhenkun Cai, Minjie Wang, David Wipf</li>
<li>for: 这篇论文是用于提出一种可扩展的图神经网络（GNN）架构，以便处理大规模的图数据。</li>
<li>methods: 这篇论文使用了一种叫做“随机抽取”的方法，来降低GNN架构的深度，从而提高其可扩展性。具体来说，他们首先使用了一种随机抽取的方法，来选择图中的一部分节点，然后使用这些节点来降低GNN的深度。</li>
<li>results: 根据文章的描述，这种采用随机抽取的GNN架构能够在大规模的图数据上实现竞争力强的准确率和可扩展性。具体来说，文章提出了一种基于这种GNN架构的全局GNN模型，并在大规模的节点分类任务上实现了竞争力强的准确率和可扩展性。<details>
<summary>Abstract</summary>
Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size.
</details>
<details>
<summary>摘要</summary>
amongst many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. in this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. however, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. to tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. we also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size.Note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Constrained-Reweighting-of-Distributions-an-Optimal-Transport-Approach"><a href="#Constrained-Reweighting-of-Distributions-an-Optimal-Transport-Approach" class="headerlink" title="Constrained Reweighting of Distributions: an Optimal Transport Approach"></a>Constrained Reweighting of Distributions: an Optimal Transport Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12447">http://arxiv.org/abs/2310.12447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhisek Chakraborty, Anirban Bhattacharya, Debdeep Pati</li>
<li>for: 本文提出了一种新的方法，用于适应先验数据的重量调整，以满足先验数据的权重约束。</li>
<li>methods: 本文使用了非 Parametric 方法和最大 entropy 原理，以及优化运输方法，来实现重量调整。</li>
<li>results: 本文在三种不同的应用中展示了其方法的灵活性，包括股票配置、复杂调查的 semi-parametric 推断和机器学习算法中的公平性权重调整。<details>
<summary>Abstract</summary>
We commonly encounter the problem of identifying an optimally weight adjusted version of the empirical distribution of observed data, adhering to predefined constraints on the weights. Such constraints often manifest as restrictions on the moments, tail behaviour, shapes, number of modes, etc., of the resulting weight adjusted empirical distribution. In this article, we substantially enhance the flexibility of such methodology by introducing a nonparametrically imbued distributional constraints on the weights, and developing a general framework leveraging the maximum entropy principle and tools from optimal transport. The key idea is to ensure that the maximum entropy weight adjusted empirical distribution of the observed data is close to a pre-specified probability distribution in terms of the optimal transport metric while allowing for subtle departures. The versatility of the framework is demonstrated in the context of three disparate applications where data re-weighting is warranted to satisfy side constraints on the optimization problem at the heart of the statistical task: namely, portfolio allocation, semi-parametric inference for complex surveys, and ensuring algorithmic fairness in machine learning algorithms.
</details>
<details>
<summary>摘要</summary>
通常我们会遇到一个问题，即确定一个最优权重调整后的观察数据的Empirical distribution，并且遵循先defined的约束条件。这些约束通常表现为观察数据的分布的 moments、tail behaviors、形态、数量等的限制。在这篇文章中，我们将提出一种非parametric方法，通过在权重调整后的分布中嵌入分布约束，并利用最大 entropy原理和优化运输工具来开发一个通用的框架。我们的关键想法是使得最大 entropy权重调整后的观察数据的分布与先defined的概率分布在优化运输度量上尽可能接近，同时允许某些微的偏差。我们在三种不同的应用中展示了这种框架的灵活性： namely，股票组合 allocate, 复杂调查的 semi-parametric inference, 和机器学习算法中的公平性。
</details></li>
</ul>
<hr>
<h2 id="CAT-Closed-loop-Adversarial-Training-for-Safe-End-to-End-Driving"><a href="#CAT-Closed-loop-Adversarial-Training-for-Safe-End-to-End-Driving" class="headerlink" title="CAT: Closed-loop Adversarial Training for Safe End-to-End Driving"></a>CAT: Closed-loop Adversarial Training for Safe End-to-End Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12432">http://arxiv.org/abs/2310.12432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linrui Zhang, Zhenghao Peng, Quanyi Li, Bolei Zhou</li>
<li>for: 这个研究是为了提高自动驾驶车辆的安全性。</li>
<li>methods: 这个研究使用了关注闭环境敌对训练（CAT）框架，通过环境增强来不断改善驾驶代码的安全性。</li>
<li>results: 实验结果显示，CAT可以快速生成更有效的物理攻击，并且可以将这些攻击与驾驶代码的训练相互作用，从而提高驾驶代码的安全性。<details>
<summary>Abstract</summary>
Driving safety is a top priority for autonomous vehicles. Orthogonal to prior work handling accident-prone traffic events by algorithm designs at the policy level, we investigate a Closed-loop Adversarial Training (CAT) framework for safe end-to-end driving in this paper through the lens of environment augmentation. CAT aims to continuously improve the safety of driving agents by training the agent on safety-critical scenarios that are dynamically generated over time. A novel resampling technique is developed to turn log-replay real-world driving scenarios into safety-critical ones via probabilistic factorization, where the adversarial traffic generation is modeled as the multiplication of standard motion prediction sub-problems. Consequently, CAT can launch more efficient physical attacks compared to existing safety-critical scenario generation methods and yields a significantly less computational cost in the iterative learning pipeline. We incorporate CAT into the MetaDrive simulator and validate our approach on hundreds of driving scenarios imported from real-world driving datasets. Experimental results demonstrate that CAT can effectively generate adversarial scenarios countering the agent being trained. After training, the agent can achieve superior driving safety in both log-replay and safety-critical traffic scenarios on the held-out test set. Code and data are available at https://metadriverse.github.io/cat.
</details>
<details>
<summary>摘要</summary>
驾驶安全是自动驾驶车辆的最高优先级。在政策层面上处理减少交通事故的算法设计方法已经存在，而我们在这篇论文中则是通过封闭型对抗训练（CAT）框架来提高驾驶代理人的安全性。CAT采用时间 dynamically generates safety-critical scenarios to continuously improve the safety of driving agents through closed-loop training. We develop a novel resampling technique to turn log-replay real-world driving scenarios into safety-critical ones via probabilistic factorization, allowing CAT to launch more efficient physical attacks and reduce the computational cost of the iterative learning pipeline. We incorporate CAT into the MetaDrive simulator and validate our approach on hundreds of driving scenarios imported from real-world driving datasets. Experimental results demonstrate that CAT can effectively generate adversarial scenarios countering the agent being trained, and the trained agent can achieve superior driving safety in both log-replay and safety-critical traffic scenarios on the held-out test set. Code and data are available at <https://metadriverse.github.io/cat>.
</details></li>
</ul>
<hr>
<h2 id="Detecting-and-Mitigating-Algorithmic-Bias-in-Binary-Classification-using-Causal-Modeling"><a href="#Detecting-and-Mitigating-Algorithmic-Bias-in-Binary-Classification-using-Causal-Modeling" class="headerlink" title="Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling"></a>Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12421">http://arxiv.org/abs/2310.12421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wendy Hui, Wai Kwong Lau</li>
<li>for: 该论文提出了使用 causal 模型检测和纠正算法偏见的想法。</li>
<li>methods: 该论文使用了 Adult 数据集，可以从 UC Irvine 机器学习库下载，构建了预测模型和偏见纠正模型。</li>
<li>results: 论文发现预测模型中的性别偏见是 statistically 有效的，并通过 cross-validation  validate 了偏见纠正模型的效果。此外，论文还表明了偏见纠正模型可以提高类别预测精度。<details>
<summary>Abstract</summary>
This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as "lavaan" in R. Hence, it enhances explainability and promotes trust.
</details>
<details>
<summary>摘要</summary>
本文提出了使用 causal 模型探测和 Mitigate 算法偏见的方法。我们提供了简要的 causal 模型介绍和我们的方法概述。我们使用UC Irvine 机器学习库提供的 Adult 数据集来开发 (1) 预测模型（当作黑盒模型）和 (2) 偏见 Mitigation 模型。在本文中，我们关注了性别偏见问题，并使用二分类问题进行探究。我们发现预测模型中的性别偏见是 statistically 显著的（p < 0.05）。我们还证明了 causal 模型可以有效地 Mitigate 性别偏见，并且通过十分法证明了这种方法的可行性。此外，我们还发现了一些轻微的总分率提高。我们的新方法是直观、易用，可以使用现有的统计软件工具such as "lavaan" in R进行实现，因此增加了解释性和信任度。
</details></li>
</ul>
<hr>
<h2 id="Cooperative-Minibatching-in-Graph-Neural-Networks"><a href="#Cooperative-Minibatching-in-Graph-Neural-Networks" class="headerlink" title="Cooperative Minibatching in Graph Neural Networks"></a>Cooperative Minibatching in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12403">http://arxiv.org/abs/2310.12403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gt-tdalab/dgl-coop">https://github.com/gt-tdalab/dgl-coop</a></li>
<li>paper_authors: Muhammed Fatih Balin, Dominique LaSalle, Ümit V. Çatalyürek</li>
<li>for: 降低大规模Graph Neural Networks（GNNs）训练的计算资源需求</li>
<li>methods: 使用Cooperative Minibatching方法，利用批处理器（PE）之间的快速交换机制，实现更好的数据 reuse和减少Neighborhood Explosion Phenomenon（NEP）的影响</li>
<li>results: 在单节点多GPU系统上实现了up to 64%的速度提升，相比独立批处理方法<details>
<summary>Abstract</summary>
Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable for processors equipped with a fast interconnect to work on a large minibatch together as a single larger processor, instead of working on separate smaller minibatches, even though global batch size is identical. We also show how to take advantage of the same phenomenon in serial execution by generating dependent consecutive minibatches. Our experimental evaluations show up to 4x bandwidth savings for fetching vertex embeddings, by simply increasing this dependency without harming model convergence. Combining our proposed approaches, we achieve up to 64% speedup over Independent Minibatching on single-node multi-GPU systems.
</details>
<details>
<summary>摘要</summary>
具有重要计算资源的 Graph Neural Networks (GNNs) 在大规模培育中需要很多计算资源，并且是数据敏感的。一种有效的方法来降低资源需求是使用小批处理并 Graph sampling。GNNs 具有独特的特点，即批处理中的每个进程元素 (PE) 之间的数据协同。然而，通常实现的独立小批处理方法会在每个 PE 上分配自己的小批处理，导致重复的计算和数据访问，从而增加 Neighborhood Explosion Phenomenon (NEP)，这是批处理的主要瓶颈。为了减少 NEP 在多个 PE 设置下的影响，我们提出了一种新的方法called Cooperative Minibatching。我们的方法利用了小批处理中采样的子图大小是批处理大小的凹形函数，从而导致每个种子顶点的工作量减少，因此更有利于配备快速互connect的处理器工作于大批处理中。我们还示出了在串行执行中使用相互依赖的连续小批处理可以获得更大的带宽减少，而不会影响模型的 converges。结合我们的提出的方法，我们在单个节点多卡系统上实现了与独立小批处理相比的最高速度提升达 64%。
</details></li>
</ul>
<hr>
<h2 id="Closed-Form-Diffusion-Models"><a href="#Closed-Form-Diffusion-Models" class="headerlink" title="Closed-Form Diffusion Models"></a>Closed-Form Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12395">http://arxiv.org/abs/2310.12395</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdas0501/Mixing_solution_CFA">https://github.com/kdas0501/Mixing_solution_CFA</a></li>
<li>paper_authors: Christopher Scarvelis, Haitz Sáez de Ocáriz Borde, Justin Solomon</li>
<li>for: 用于生成新样本，而不需要训练。</li>
<li>methods: 使用closed-form score function，并通过近似 Neil 网络来预测score function。</li>
<li>results: 可以在consumer-grade CPU上运行，并且 sampling 速度与 neural SGMs 相当。<details>
<summary>Abstract</summary>
Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves sampling times competitive with neural SGMs while running on consumer-grade CPUs.
</details>
<details>
<summary>摘要</summary>
Score-based生成模型（SGM）通过iterativelytransforming noise使用目标分布中的分数函数来采样。任何固定的训练集，这个分数函数都可以在关闭形式中评估，但是这些SGM会memorize其训练数据并不会生成新样本。在实践中，我们通常通过score-matching来approximate分数函数，并通过这个错误来促进泛化。但是神经网络SGM的训练和采样成本高，而且这种错误的效果不够理解。在这个工作中，我们选择显式简化关闭形式的分数函数，以获得一个不需要训练的SGM，可以生成新的样本。我们分析我们的模型，并提出一种高效的最近邻域基于的分数函数估计器。使用这种估计器，我们的方法可以与神经网络SGM的采样时间竞争，而且可以在consumer-grade CPU上运行。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/cs.LG_2023_10_19/" data-id="clpxp6c4r00tsee88c6e35abj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/eess.IV_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T09:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/eess.IV_2023_10_19/">eess.IV - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Video-Quality-Assessment-and-Coding-Complexity-of-the-Versatile-Video-Coding-Standard"><a href="#Video-Quality-Assessment-and-Coding-Complexity-of-the-Versatile-Video-Coding-Standard" class="headerlink" title="Video Quality Assessment and Coding Complexity of the Versatile Video Coding Standard"></a>Video Quality Assessment and Coding Complexity of the Versatile Video Coding Standard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13093">http://arxiv.org/abs/2310.13093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Amestoy, Naty Sidaty, Wassim Hamidouche, Pierrick Philippe, Daniel Menard</li>
<li>for: 这个论文的目的是对最新的视频编码标准VVC（Versatile Video Coding）与其前一代标准HEVC（High Efficiency Video Coding）的编码性能和复杂性进行比较分析。</li>
<li>methods: 本研究使用了多种测试序列，覆盖了高清分辨率（HD）和超高清分辨率（UHD）的分辨率范围，并在各种比特率范围内进行了编码。测试序列使用了HEVC（HM）和VVC（VTM）的参考软件编码器。</li>
<li>results: 结果表明，VVC在对比HEVC的情况下，可以实现比特率下降范围为31%至40%，具体取决于视频内容、空间分辨率和选择的质量指标。然而，这些编码效率提升的成本是计算复杂性的增加。在平均情况下，VVC解码过程比HEVC解码过程快1.5倍，而编码过程则变得至少8倍于HEVC参考编码器。<details>
<summary>Abstract</summary>
In recent years, the proliferation of multimedia applications and formats, such as IPTV, Virtual Reality (VR, 360-degree), and point cloud videos, has presented new challenges to the video compression research community. Simultaneously, there has been a growing demand from users for higher resolutions and improved visual quality. To further enhance coding efficiency, a new video coding standard, Versatile Video Coding (VVC), was introduced in July 2020. This paper conducts a comprehensive analysis of coding performance and complexity for the latest VVC standard in comparison to its predecessor, High Efficiency Video Coding (HEVC). The study employs a diverse set of test sequences, covering both High Definition (HD) and Ultra High Definition (UHD) resolutions, and spans a wide range of bit-rates. These sequences are encoded using the reference software encoders of HEVC (HM) and VVC (VTM). The results consistently demonstrate that VVC outperforms HEVC, achieving bit-rate savings of up to 40% on the subjective quality scale, particularly at realistic bit-rates and quality levels. Objective quality metrics, including PSNR, SSIM, and VMAF, support these findings, revealing bit-rate savings ranging from 31% to 40%, depending on the video content, spatial resolution, and the selected quality metric. However, these improvements in coding efficiency come at the cost of significantly increased computational complexity. On average, our results indicate that the VVC decoding process is 1.5 times more complex, while the encoding process becomes at least eight times more complex than that of the HEVC reference encoder. Our simultaneous profiling of the two standards sheds light on the primary evolutionary differences between them and highlights the specific stages responsible for the observed increase in complexity.
</details>
<details>
<summary>摘要</summary>
近年来， multimedia 应用和格式的普及，如 IPTV、虚拟现实（VR， 360度）以及点云视频，对视频压缩研究 сообщество带来了新的挑战。同时，用户对高分辨率和改进的视觉质量产生了增加的需求。为了进一步提高编码效率，2020年7月引入了一新的视频编码标准——多样化视频编码（VVC）。本文对最新的VVC标准和其前任者高效视频编码（HEVC）进行了全面的编码性能和复杂度分析。研究使用了一组多样化的测试序列，覆盖了高清（HD）和超高清（UHD）的分辨率，并覆盖了各种比特率范围。这些序列使用HEVC（HM）和VVC（VTM）的参考软件编码器进行编码。结果表明，VVC在主观质量标准下与HEVC进行比较，可以实现比特率下降达40%，特别是在现实比特率和质量水平下。对象质量指标，包括PSNR、SSIM和VMAF，支持这些发现，显示VVC在视频内容、空间分辨率和选择的质量指标下实现比特率下降在31%到40%之间。然而，这些编码效率改进的成本是计算复杂性增加的代价。在我们的结果中，VVC解码过程的计算复杂性提高了1.5倍，而编码过程则变得至少8倍于HEVC参考编码器。我们同时进行了HEVC和VVC的并行分析，并指出了这两个标准之间的主要进化差异，以及具体的阶段贡献到所见的复杂性增加。
</details></li>
</ul>
<hr>
<h2 id="Product-of-Gaussian-Mixture-Diffusion-Models"><a href="#Product-of-Gaussian-Mixture-Diffusion-Models" class="headerlink" title="Product of Gaussian Mixture Diffusion Models"></a>Product of Gaussian Mixture Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12653">http://arxiv.org/abs/2310.12653</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vlogroup/pogmdm">https://github.com/vlogroup/pogmdm</a></li>
<li>paper_authors: Martin Zach, Erich Kobler, Antonin Chambolle, Thomas Pock</li>
<li>for: 这个论文目的是为了估计Random变量X的总体分布f_X。</li>
<li>methods: 这个论文使用Successive Smoothing方法，使得变量Y满足噪声partial differential equation（PDE），即（∂_t - Δ_1）f_Y(⋅, t) &#x3D; 0，并且Initial condition为f_Y(⋅, 0) &#x3D; f_X。具体来说，这个论文提出了一种product-of-experts-type模型，使用Gaussian mixture experts，并研究了可以 analytic expression 的配置。</li>
<li>results: 这个论文通过numerical results表明，这种模型在image denoising中是竞争力强，可读性好、 Parameters少。此外，模型还可以用于静态噪声估计，允许无监测的图像减噪。<details>
<summary>Abstract</summary>
In this work we tackle the problem of estimating the density $ f_X $ of a random variable $ X $ by successive smoothing, such that the smoothed random variable $ Y $ fulfills the diffusion partial differential equation $ (\partial_t - \Delta_1)f_Y(\,\cdot\,, t) = 0 $ with initial condition $ f_Y(\,\cdot\,, 0) = f_X $. We propose a product-of-experts-type model utilizing Gaussian mixture experts and study configurations that admit an analytic expression for $ f_Y (\,\cdot\,, t) $. In particular, with a focus on image processing, we derive conditions for models acting on filter-, wavelet-, and shearlet responses. Our construction naturally allows the model to be trained simultaneously over the entire diffusion horizon using empirical Bayes. We show numerical results for image denoising where our models are competitive while being tractable, interpretable, and having only a small number of learnable parameters. As a byproduct, our models can be used for reliable noise estimation, allowing blind denoising of images corrupted by heteroscedastic noise.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们解决了一个估计 Random Variable X 的概率分布 f_X 的问题，通过连续缓和，使得缓和后的 Random Variable Y 满足噪声partial differential equation（PDE）$ (\partial_t - \Delta_1)f_Y(\,\cdot\,, t) = 0 $ 的初始条件 $ f_Y(\,\cdot\,, 0) = f_X $。我们提出了一种product-of-experts-type模型，使用 Gaussian mixture experts，并研究了可以表示 $ f_Y (\,\cdot\,, t) $ 的配置。特别是在图像处理方面，我们 derivate了对 filter-, wavelet- 和 shearlet responses 的模型。我们的建构 naturally allows the model to be trained simultaneously over the entire diffusion horizon using empirical Bayes。我们展示了对图像减震的numerical result，其中我们的模型能够与其他模型竞争，同时具有可读性、可解释性和只有小量可学习参数。此外，我们的模型还可以用于可靠地 estimating noise，allowing blind denoising of images corrupted by heteroscedastic noise。Note that Simplified Chinese is a written language, and the translation is based on the standardized grammar and vocabulary of Simplified Chinese. The actual translation may vary depending on the specific context and dialect.
</details></li>
</ul>
<hr>
<h2 id="Iterative-PnP-and-its-application-in-3D-2D-vascular-image-registration-for-robot-navigation"><a href="#Iterative-PnP-and-its-application-in-3D-2D-vascular-image-registration-for-robot-navigation" class="headerlink" title="Iterative PnP and its application in 3D-2D vascular image registration for robot navigation"></a>Iterative PnP and its application in 3D-2D vascular image registration for robot navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12551">http://arxiv.org/abs/2310.12551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingwei Song, Keke Yang, Zheng Zhang, Meng Li, Tuoyu Cao, Maani Ghaffari</li>
<li>for: 这 paper 描述了一种新的实时Robot-Centered 3D-2D血管图像对齐算法，可以抗扭变和实现高精度。</li>
<li>methods: 该 paper 使用了高精度 3D-2D  регистра图像技术和计算效率要求，并提出了一种基于 Perspective-n-Point（PnP）问题的解决方案。</li>
<li>results: 实验表明，提出的算法可以在 50 Hz （静止）和 20 Hz （非静止）的速度下进行对齐，并且与其他工作的对齐精度相似。结果表明，Iterative PnP 适用于未来血管 interven  robot 应用。<details>
<summary>Abstract</summary>
This paper reports on a new real-time robot-centered 3D-2D vascular image alignment algorithm, which is robust to outliers and can align nonrigid shapes. Few works have managed to achieve both real-time and accurate performance for vascular intervention robots. This work bridges high-accuracy 3D-2D registration techniques and computational efficiency requirements in intervention robot applications. We categorize centerline-based vascular 3D-2D image registration problems as an iterative Perspective-n-Point (PnP) problem and propose to use the Levenberg-Marquardt solver on the Lie manifold. Then, the recently developed Reproducing Kernel Hilbert Space (RKHS) algorithm is introduced to overcome the ``big-to-small'' problem in typical robotic scenarios. Finally, an iterative reweighted least squares is applied to solve RKHS-based formulation efficiently. Experiments indicate that the proposed algorithm processes registration over 50 Hz (rigid) and 20 Hz (nonrigid) and obtains competing registration accuracy similar to other works. Results indicate that our Iterative PnP is suitable for future vascular intervention robot applications.
</details>
<details>
<summary>摘要</summary>
The centerline-based vascular 3D-2D image registration problems are categorized as an iterative Perspective-n-Point (PnP) problem, and the Levenberg-Marquardt solver on the Lie manifold is proposed. Additionally, the recently developed Reproducing Kernel Hilbert Space (RKHS) algorithm is introduced to overcome the "big-to-small" problem in typical robotic scenarios. Finally, an iterative reweighted least squares method is applied to solve the RKHS-based formulation efficiently.Experiments show that the proposed algorithm can process registration over 50 Hz (rigid) and 20 Hz (non-rigid) and obtains competing registration accuracy similar to other works. Results indicate that the Iterative PnP is suitable for future vascular intervention robot applications.Here is the Simplified Chinese translation:这篇论文报道了一种新的实时机器人 centered 3D-2D血管图像匹配算法，该算法可以抗耗弃和对非固定形态进行匹配。很少的工作能够同时实现实时和高精度的性能，这种工作将高精度 3D-2D 注册技术和机器人应用中的计算效率要求相结合。我们将中心线基于血管 3D-2D 图像注册问题分类为一个迭代 Perspective-n-Point (PnP) 问题，并提议使用 Levenberg-Marquardt 算法在 Lie 替换 manifold 上。然后，我们引入了最近发展的 Reproducing Kernel Hilbert Space (RKHS) 算法，以解决 typical 机器人应用中的 "大到小" 问题。最后，我们使用迭代重点最小二乘法解决 RKHS-based 表示的问题。实验显示，提议的算法可以在 50 Hz (固定) 和 20 Hz (非固定) 的注册速率下进行注册，并与其他工作的注册精度相似。结果表明，Iterative PnP 适用于未来血管介入机器人应用。
</details></li>
</ul>
<hr>
<h2 id="Multi-granularity-Backprojection-Transformer-for-Remote-Sensing-Image-Super-Resolution"><a href="#Multi-granularity-Backprojection-Transformer-for-Remote-Sensing-Image-Super-Resolution" class="headerlink" title="Multi-granularity Backprojection Transformer for Remote Sensing Image Super-Resolution"></a>Multi-granularity Backprojection Transformer for Remote Sensing Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12507">http://arxiv.org/abs/2310.12507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinglei Hao, Wukai Li, Binglu Wang, Shunzhou Wang, Yuting Lu, Ning Li, Yongqiang Zhao</li>
<li>for: 本研究旨在提高遥感图像超分辨率（RSISR）的性能，尤其是在计算成本方面。</li>
<li>methods: 该研究提出了一种多级划分变换器（MBT），它将划分学习策略与转换器框架结合。MBT包括缩放意识划分基于转换器层（SPTL）和层次意识划分基于转换器块（CPTB）。此外，一种划分基于重建模块（PRM）也被引入以增强层次特征。</li>
<li>results: 实验结果表明，MBT可以高效地学习低分辨率特征，不需要过度的模块来处理高分辨率处理。MBT在UCMerced和AID数据集上达到了其他领先方法的州态较高的性能。<details>
<summary>Abstract</summary>
Backprojection networks have achieved promising super-resolution performance for nature images but not well be explored in the remote sensing image super-resolution (RSISR) field due to the high computation costs. In this paper, we propose a Multi-granularity Backprojection Transformer termed MBT for RSISR. MBT incorporates the backprojection learning strategy into a Transformer framework. It consists of Scale-aware Backprojection-based Transformer Layers (SPTLs) for scale-aware low-resolution feature learning and Context-aware Backprojection-based Transformer Blocks (CPTBs) for hierarchical feature learning. A backprojection-based reconstruction module (PRM) is also introduced to enhance the hierarchical features for image reconstruction. MBT stands out by efficiently learning low-resolution features without excessive modules for high-resolution processing, resulting in lower computational resources. Experiment results on UCMerced and AID datasets demonstrate that MBT obtains state-of-the-art results compared to other leading methods.
</details>
<details>
<summary>摘要</summary>
备受期待的Backprojection网络在自然图像超分辨（SR）领域取得了出色的成绩，但在 remote sensing 图像超分辨（RSISR）领域还没有得到充分探索，主要因为计算成本过高。在这篇论文中，我们提出了一种名为 Multi-granularity Backprojection Transformer（MBT）的RSISR方法。MBT将Backprojection学习策略 integrate 到Transformer框架中。它包括Scale-aware Backprojection-based Transformer Layers（SPTLs），用于学习尺度意识的低分辨度特征，以及Context-aware Backprojection-based Transformer Blocks（CPTBs），用于层次特征学习。此外，我们还提出了一种基于Backprojection的重建模块（PRM），用于增强层次特征对图像重建的贡献。MBT的优势在于不需要过多的模块来处理高分辨度数据，从而降低计算资源的消耗。实验结果表明，MBT在UCMerced和AID数据集上达到了与其他领先方法相当的成绩。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/eess.IV_2023_10_19/" data-id="clpxp6cby01cbee885drya57y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_19" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/19/eess.SP_2023_10_19/" class="article-date">
  <time datetime="2023-10-19T08:00:00.000Z" itemprop="datePublished">2023-10-19</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/19/eess.SP_2023_10_19/">eess.SP - 2023-10-19</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Sensing-for-Localization-with-Reconfigurable-Intelligent-Surface"><a href="#Active-Sensing-for-Localization-with-Reconfigurable-Intelligent-Surface" class="headerlink" title="Active Sensing for Localization with Reconfigurable Intelligent Surface"></a>Active Sensing for Localization with Reconfigurable Intelligent Surface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13160">http://arxiv.org/abs/2310.13160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongze Zhang, Tao Jiang, Wei Yu<br>for: This paper proposes a solution to the uplink localization problem for remote users with the aid of reconfigurable intelligent surfaces (RIS).methods: The proposed method uses long short-term memory (LSTM) networks to exploit temporal correlation between measurements and construct scalable information vectors. A deep neural network (DNN) is used to map the LSTM cell state to the RIS configuration and the final DNN is used to map the LSTM cell state to the estimated user equipment (UE) position.results: The proposed active RIS design results in lower localization error as compared to existing active and nonactive methods. The proposed solution produces interpretable results and is generalizable to early stopping in the sequence of sensing stages.<details>
<summary>Abstract</summary>
This paper addresses an uplink localization problem in which the base station (BS) aims to locate a remote user with the aid of reconfigurable intelligent surface (RIS). This paper proposes a strategy in which the user transmits pilots over multiple time frames, and the BS adaptively adjusts the RIS reflection coefficients based on the observations already received so far in order to produce an accurate estimate of the user location at the end. This is a challenging active sensing problem for which finding an optimal solution involves a search through a complicated functional space whose dimension increases with the number of measurements. In this paper, we show that the long short-term memory (LSTM) network can be used to exploit the latent temporal correlation between measurements to automatically construct scalable information vectors (called hidden state) based on the measurements. Subsequently, the state vector can be mapped to the RIS configuration for the next time frame in a codebook-free fashion via a deep neural network (DNN). After all the measurements have been received, a final DNN can be used to map the LSTM cell state to the estimated user equipment (UE) position. Numerical result shows that the proposed active RIS design results in lower localization error as compared to existing active and nonactive methods. The proposed solution produces interpretable results and is generalizable to early stopping in the sequence of sensing stages.
</details>
<details>
<summary>摘要</summary>
To address this problem, this paper proposes using a long short-term memory (LSTM) network to exploit the latent temporal correlation between measurements and construct scalable information vectors (called hidden state) based on the measurements. The state vector can then be mapped to the RIS configuration for the next time frame in a codebook-free fashion via a deep neural network (DNN). After all the measurements have been received, a final DNN can be used to map the LSTM cell state to the estimated user equipment (UE) position.Numerical results show that the proposed active RIS design results in lower localization error compared to existing active and nonactive methods. The proposed solution produces interpretable results and is generalizable to early stopping in the sequence of sensing stages.
</details></li>
</ul>
<hr>
<h2 id="High-Dynamic-Range-mmWave-Massive-MU-MIMO-with-Householder-Reflections"><a href="#High-Dynamic-Range-mmWave-Massive-MU-MIMO-with-Householder-Reflections" class="headerlink" title="High Dynamic Range mmWave Massive MU-MIMO with Householder Reflections"></a>High Dynamic Range mmWave Massive MU-MIMO with Householder Reflections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12835">http://arxiv.org/abs/2310.12835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victoria Palhares, Gian Marti, Oscar Castañeda, Christoph Studer</li>
<li>For: 该论文旨在解决当 simultanoously transmitting user equipments (UEs) with vastly different BS-side receive powers时，低分辨率数字化转换器 (ADCs) 的问题。* Methods: 该论文提出了一种新的高 Dynamics 范围 (HDR) MIMO 技术，该技术结合了适应性的analog spatial transform和数字平衡，以实现同时接收强大和弱 UE 的功能。* Results: 该论文通过使用 Householder reflections 作为空间变换， demonstarted the efficacy of HDR MIMO in a massive MU-MIMO mmWave scenario.<details>
<summary>Abstract</summary>
All-digital massive multiuser (MU) multiple-input multiple-output (MIMO) at millimeter-wave (mmWave) frequencies is a promising technology for next-generation wireless systems. Low-resolution analog-to-digital converters (ADCs) can be utilized to reduce the power consumption of all-digital basestation (BS) designs. However, simultaneously transmitting user equipments (UEs) with vastly different BS-side receive powers either drown weak UEs in quantization noise or saturate the ADCs. To address this issue, we propose high dynamic range (HDR) MIMO, a new paradigm that enables simultaneous reception of strong and weak UEs with low-resolution ADCs. HDR MIMO combines an adaptive analog spatial transform with digital equalization: The spatial transform focuses strong UEs on a subset of ADCs in order to mitigate quantization and saturation artifacts; digital equalization is then used for data detection. We demonstrate the efficacy of HDR MIMO in a massive MU-MIMO mmWave scenario that uses Householder reflections as spatial transform.
</details>
<details>
<summary>摘要</summary>
全数位大规模多用户（MU）多输入多输出（MIMO）在 millimeter 波（mmWave）频率上是未来无线系统的承让技术。低分辨率数字转换器（ADC）可以降低全数位基站（BS）设计的功耗。然而，同时发送用户设备（UE）的大大不同BS-side接收功率会使用量化杂音淹没弱UE，或者使ADC发生饱和。为解决这个问题，我们提出高动态范围（HDR）MIMO，一种新的思想，允许同时接收强UE和弱UE，使用低分辨率ADC。HDR MIMO将适应性的分析Transform与数字平衡相结合：分析Transform将强UE集中在一些ADC上，以降低量化和饱和artefacts；数字平衡后再进行数据检测。我们在大规模MU-MIMO mmWave场景中使用Householder reflections作为分析Transform，并证明HDR MIMO的有效性。
</details></li>
</ul>
<hr>
<h2 id="Capacity-Limitation-and-Optimization-Strategy-for-Flexible-Point-to-Multi-Point-Optical-Networks"><a href="#Capacity-Limitation-and-Optimization-Strategy-for-Flexible-Point-to-Multi-Point-Optical-Networks" class="headerlink" title="Capacity Limitation and Optimization Strategy for Flexible Point-to-Multi-Point Optical Networks"></a>Capacity Limitation and Optimization Strategy for Flexible Point-to-Multi-Point Optical Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12709">http://arxiv.org/abs/2310.12709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ji Zhou, Haide Wang, Liangchuan Li, Weiping Liu, Changyuan Yu, Zhaohui Li</li>
<li>for: 这篇论文旨在探讨可变点对接多点（PtMP）光纤网络的能量负荷和容量。</li>
<li>methods: 该论文使用了Entropy-loading数字次频多plexing（DSCM）技术来实现低延迟和高容量的可变PtMP光纤网络。</li>
<li>results: 该论文提出了可变PtMP光纤网络的能量负荷和容量的理论限制，并提出了最佳的剪辑率来实现最高的容量限制。基于准确的剪辑噪声模型，该论文建立了三维 lookup表来计算比特错误率、spectral efficiency和链损失。该论文还提出了一种优化策略来实现可变PtMP光纤网络的最高容量。<details>
<summary>Abstract</summary>
Point-to-multi-point (PtMP) optical networks become the main solutions for network-edge applications such as passive optical networks and radio access networks. Entropy-loading digital subcarrier multiplexing (DSCM) is the core technology to achieve low latency and approach high capacity for flexible PtMP optical networks. However, the high peak-to-average power ratio of the entropy-loading DSCM signal limits the power budget and restricts the capacity, which can be reduced effectively by clipping operation. In this paper, we derive the theoretical capacity limitation of the flexible PtMP optical networks based on the entropy-loading DSCM signal. Meanwhile, an optimal clipping ratio for the clipping operation is acquired to approach the highest capacity limitation. Based on an accurate clipping-noise model under the optimal clipping ratio, we establish a three-dimensional look-up table for bit-error ratio, spectral efficiency, and link loss. Based on the three-dimensional look-up table, an optimization strategy is proposed to acquire optimal spectral efficiencies for achieving a higher capacity of the flexible PtMP optical networks.
</details>
<details>
<summary>摘要</summary>
点对多点（PtMP）光网成为网络边缘应用的主要解决方案，如无活动光网和无线接入网。Entropy-loading数字子副载多plexing（DSCM）是实现低延迟和高容量灵活PtMP光网的核心技术。然而，高峰值平均功率比ENTROPY-loading DSCM信号限制了功率预算，这可以通过剪辑操作来降低。在这篇论文中，我们 derive了灵活PtMP光网的理论容量限制基于ENTROPY-loading DSCM信号。同时，我们获得了最佳剪辑率来接近最高容量限制。基于最佳剪辑噪声模型，我们建立了三维look-up表，其中包括比特错误率、spectral efficiency和链接产生率。基于三维look-up表，我们提出了一种优化策略，以实现灵活PtMP光网的更高容量。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Information-Based-Integrated-Sensing-and-Communications-A-WMMSE-Framework"><a href="#Mutual-Information-Based-Integrated-Sensing-and-Communications-A-WMMSE-Framework" class="headerlink" title="Mutual Information-Based Integrated Sensing and Communications: A WMMSE Framework"></a>Mutual Information-Based Integrated Sensing and Communications: A WMMSE Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12686">http://arxiv.org/abs/2310.12686</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ROCASSO/MI-based-WMMSE-ISAC-algorithm">https://github.com/ROCASSO/MI-based-WMMSE-ISAC-algorithm</a></li>
<li>paper_authors: Yizhou Peng, Songjie Yang, Wanting Lyu, Ya Li, Hongjun He, Zhongpei Zhang, Chadi Assi</li>
<li>for: 这个论文主要研究了一种权重最小均方差 empowered  интеegrated 感知通信（ISAC）系统，用于服务多个用户的感知目标。</li>
<li>methods: 该论文提出了一种基于积分信息（MI）的新的权重最小均方差-ISAC 方法，通过开发了一种特有的天线设计机制，以最大化该系统的权重感知和通信总bitrate。</li>
<li>results: numerical 结果表明该方法的效果，并 validate了感知和通信之间的性能负荷。<details>
<summary>Abstract</summary>
In this letter, a weighted minimum mean square error (WMMSE) empowered integrated sensing and communication (ISAC) system is investigated. One transmitting base station and one receiving wireless access point are considered to serve multiple users a sensing target. Based on the theory of mutual-information (MI), communication MI and sensing MI rate are utilized as the performance metrics under the presence of clutters. In particular, we propose an novel MI-based WMMSE-ISAC method by developing a unique transceiver design mechanism to maximize the weighted sensing and communication sum-rate of this system. Such a maximization process is achieved by utilizing the classical method -- WMMSE, aiming to better manage the effect of sensing clutters and the interference among users. Numerical results show the effectiveness of our proposed method, and the performance trade-off between sensing and communication is also validated.
</details>
<details>
<summary>摘要</summary>
在这封信中，一种权重最小平均方差 empowered integreated sensing and communication（ISAC）系统被研究。一个发射基站和一个接收无线访问点被考虑，以服务多个用户感知目标。基于互信息（MI）理论，在存在噪声的情况下，通信MI和感知MI率被用作这系统的性能指标。特别是，我们提出了一种新的 MI-based WMMSE-ISAC 方法，通过开发一种特有的天线设计机制，以最大化这个系统的权重感知和通信总速率。这个最大化过程通过使用经典方法——WMMSE，以更好地管理感知噪声和用户之间的干扰。 numerically 的结果表明我们的提出方法的有效性，并且 validate 了感知和通信之间的性能质量负担。
</details></li>
</ul>
<hr>
<h2 id="Can-Electromagnetic-Information-Theory-Improve-Wireless-Systems-A-Channel-Estimation-Example"><a href="#Can-Electromagnetic-Information-Theory-Improve-Wireless-Systems-A-Channel-Estimation-Example" class="headerlink" title="Can Electromagnetic Information Theory Improve Wireless Systems? A Channel Estimation Example"></a>Can Electromagnetic Information Theory Improve Wireless Systems? A Channel Estimation Example</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12446">http://arxiv.org/abs/2310.12446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieao Zhu, Xiaofeng Su, Zhongzhichao Wan, Linglong Dai, Tie Jun Cui</li>
<li>for: 本研究旨在探讨electromagnetic information theory（EIT）如何提高无线通信系统的性能。</li>
<li>methods: 本文提出了一种基于EIT的渠道估计方法，将电磁知识integrated into classical MMSE渠道估计器中，并通过用 Gaussian process regression（GPR） derive the channel estimations。此外， authors还提出了EMkernel learning来调整EM kernel的参数。</li>
<li>results:  simulation results show that EIT-based channel estimator可以超过传统的均方差MMSE算法，证明EIT在实际应用中的实用性。<details>
<summary>Abstract</summary>
Electromagnetic information theory (EIT) is an emerging interdisciplinary subject that integrates classical Maxwell electromagnetics and Shannon information theory. The goal of EIT is to uncover the information transmission mechanisms from an electromagnetic (EM) perspective in wireless systems. Existing works on EIT are mainly focused on the analysis of degrees-of-freedom (DoF), system capacity, and characteristics of the electromagnetic channel. However, these works do not clarify how EIT can improve wireless communication systems. To answer this question, in this paper, we provide a novel demonstration of the application of EIT. By integrating EM knowledge into the classical MMSE channel estimator, we observe for the first time that EIT is capable of improving the channel estimation performace. Specifically, the EM knowledge is first encoded into a spatio-temporal correlation function (STCF), which we term as the EM kernel. This EM kernel plays the role of side information to the channel estimator. Since the EM kernel takes the form of Gaussian processes (GP), we propose the EIT-based Gaussian process regression (EIT-GPR) to derive the channel estimations. In addition, since the EM kernel allows parameter tuning, we propose EM kernel learning to fit the EM kernel to channel observations. Simulation results show that the application of EIT to the channel estimator enables it to outperform traditional isotropic MMSE algorithm, thus proving the practical values of EIT.
</details>
<details>
<summary>摘要</summary>
电磁信息理论（EIT）是一个emerging的interdisciplinary subject，它将经典的Maxwell电磁学和Shannon信息理论相结合。EIT的目标是从电磁（EM）角度来描述无线系统中信息传输机制。现有的EIT研究主要关注度OF（DoF）、系统容量和电磁通道的特性。然而，这些研究并没有解释如何使用EIT提高无线通信系统的性能。为回答这个问题，在这篇论文中，我们提供了一种新的EIT应用示例。我们首先将EM知识编码成一个空间-时间协同函数（STCF），我们称之为EM核。这个EM核在渠道估计器中扮演着侧信息的角色。由于EM核是GP的形式，我们提议使用EIT基于GP回归（EIT-GPR）来 derivate渠道估计结果。此外，由于EM核允许参数调整，我们提议使用EM核学习来适应渠道观测。实验结果表明，通过应用EIT到渠道估计器，可以超越传统的均方差MMSE算法，从而证明EIT在实践中的价值。
</details></li>
</ul>
<hr>
<h2 id="Reconfigurable-Intelligent-Surface-Assisted-High-Speed-Train-Communications-Coverage-Performance-Analysis-and-Placement-Optimization"><a href="#Reconfigurable-Intelligent-Surface-Assisted-High-Speed-Train-Communications-Coverage-Performance-Analysis-and-Placement-Optimization" class="headerlink" title="Reconfigurable Intelligent Surface Assisted High-Speed Train Communications: Coverage Performance Analysis and Placement Optimization"></a>Reconfigurable Intelligent Surface Assisted High-Speed Train Communications: Coverage Performance Analysis and Placement Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12429">http://arxiv.org/abs/2310.12429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changzhu Liu, Ruisi He, Yong Niu, Zhu Han, Bo Ai, Meilin Gao, Zhangfeng Ma, Gongpu Wang, Zhangdui Zhong</li>
<li>for: 增强无线覆盖和提高通信系统的可靠性，提高高速列车通信系统的无线覆盖和可用性。</li>
<li>methods: 利用智能表面技术（RIS）帮助高速列车（HST）通信系统，以提高无线覆盖和可用性。</li>
<li>results: 通过RIS的部署，可以提高高速列车通信系统的无线覆盖和可用性，并且可以适应不同的系统参数。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) emerges as an efficient and promising technology for the next wireless generation networks and has attracted a lot of attention owing to the capability of extending wireless coverage by reflecting signals toward targeted receivers. In this paper, we consider a RIS-assisted high-speed train (HST) communication system to enhance wireless coverage and improve coverage probability. First, coverage performance of the downlink single-input-single-output system is investigated, and the closed-form expression of coverage probability is derived. Moreover, travel distance maximization problem is formulated to facilitate RIS discrete phase design and RIS placement optimization, which is subject to coverage probability constraint. Simulation results validate that better coverage performance and higher travel distance can be achieved with deployment of RIS. The impacts of some key system parameters including transmission power, signal-to-noise ratio threshold, number of RIS elements, number of RIS quantization bits, horizontal distance between base station and RIS, and speed of HST on system performance are investigated. In addition, it is found that RIS can well improve coverage probability with limited power consumption for HST communications.
</details>
<details>
<summary>摘要</summary>
《卷积智能表面（RIS）助成高速列车（HST）通信系统的可靠性和可能性》Abstract:随着下一代无线网络技术的发展，可 Configurable intelligent surface (RIS) 已经成为一种有效和有前途的技术，可以扩展无线覆盖范围并将信号反射向目标接收器。在这篇论文中，我们考虑了一个RIS协助HST通信系统，以提高无线覆盖和提高通信可靠性。首先，我们研究了下降链单输入单出口系统的覆盖性能，并 derivatedclosed-form表达式来计算覆盖可能性。此外，我们还解决了RIS精度设计和RIS布置优化问题，该问题是基于覆盖可能性约束。实验结果表明，通过RIS部署，可以实现更好的覆盖性能和更长的旅行距离。此外，我们还 investigate了一些关键系统参数的影响，包括传输功率、信号噪声比阈值、RIS元素数量、RIS逻辑位数、基站与RIS之间的水平距离和高速列车速度。研究结果表明，RIS可以很好地提高高速列车通信可靠性，同时减少功率消耗。Introduction:With the development of the next generation wireless networks, reconfigurable intelligent surfaces (RIS) have emerged as a promising technology that can extend wireless coverage and improve communication reliability. In this paper, we consider a RIS-assisted high-speed train (HST) communication system to enhance wireless coverage and improve coverage probability. First, we investigate the coverage performance of the downlink single-input-single-output system and derive a closed-form expression for the coverage probability. Moreover, we formulate a travel distance maximization problem to facilitate RIS discrete phase design and RIS placement optimization, which is subject to coverage probability constraint. Simulation results validate that better coverage performance and longer travel distance can be achieved with the deployment of RIS. Furthermore, we investigate the impacts of some key system parameters on system performance, including transmission power, signal-to-noise ratio threshold, number of RIS elements, number of RIS quantization bits, horizontal distance between base station and RIS, and speed of HST. Results show that RIS can well improve coverage probability with limited power consumption for HST communications.Main Body:1. Coverage Performance of Downlink Single-Input-Single-Output SystemWe first investigate the coverage performance of the downlink single-input-single-output system. By deriving the closed-form expression of coverage probability, we can analyze the impact of RIS on the coverage performance. The results show that RIS can significantly improve the coverage probability, especially when the distance between the base station and the user is large.2. Travel Distance Maximization ProblemTo facilitate RIS discrete phase design and RIS placement optimization, we formulate a travel distance maximization problem subject to coverage probability constraint. The problem is to find the optimal phase shifts of the RIS elements that maximize the travel distance of the HST while ensuring a certain coverage probability. We solve the problem using a numerical optimization algorithm and show that the optimized RIS phase shifts can significantly improve the travel distance of the HST.3. Impacts of System Parameters on System PerformanceWe investigate the impacts of some key system parameters on system performance, including transmission power, signal-to-noise ratio threshold, number of RIS elements, number of RIS quantization bits, horizontal distance between base station and RIS, and speed of HST. The results show that these parameters have a significant impact on the coverage performance and travel distance of the HST. In particular, we find that RIS can well improve coverage probability with limited power consumption for HST communications.Conclusion:In conclusion, we have proposed a RIS-assisted HST communication system to enhance wireless coverage and improve coverage probability. By deriving the closed-form expression of coverage probability and formulating a travel distance maximization problem, we have shown that RIS can significantly improve the coverage performance and travel distance of the HST. Furthermore, we have investigated the impacts of some key system parameters on system performance and found that RIS can well improve coverage probability with limited power consumption for HST communications. Our results demonstrate the potential of RIS technology for improving wireless communication systems in high-speed train applications.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/19/eess.SP_2023_10_19/" data-id="clpxp6cds01ghee887smrf4xa" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.SD_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T15:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.SD_2023_10_18/">cs.SD - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-CHiME-7-Challenge-System-Description-and-Performance-of-NeMo-Team’s-DASR-System"><a href="#The-CHiME-7-Challenge-System-Description-and-Performance-of-NeMo-Team’s-DASR-System" class="headerlink" title="The CHiME-7 Challenge: System Description and Performance of NeMo Team’s DASR System"></a>The CHiME-7 Challenge: System Description and Performance of NeMo Team’s DASR System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12378">http://arxiv.org/abs/2310.12378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Jin Park, He Huang, Ante Jukic, Kunal Dhawan, Krishna C. Puvvada, Nithin Koluguri, Nikolay Karpov, Aleksandr Laptev, Jagadeesh Balam, Boris Ginsburg</li>
<li>for: 这份论文是为了开发一个多通道、多个话者的语音识别系统，特别是用于处理分散式麦克风和麦克风数组录音。</li>
<li>methods: 这个系统主要由三个基本模块组成：语音分类模块、多通道声音前端处理模块和语音识别模块。这些模块共同组成一个协同处理的系统，仔细处理多通道和多个话者的音频输入。</li>
<li>results: 这篇论文提出了一种基于NeMo工具包的多通道、多个话者语音识别系统，并通过了7个CHiME挑战任务的评测。系统的性能得到了 significiant进步，表明该系统在实际应用中具有极高的可靠性和精度。<details>
<summary>Abstract</summary>
We present the NVIDIA NeMo team's multi-channel speech recognition system for the 7th CHiME Challenge Distant Automatic Speech Recognition (DASR) Task, focusing on the development of a multi-channel, multi-speaker speech recognition system tailored to transcribe speech from distributed microphones and microphone arrays. The system predominantly comprises of the following integral modules: the Speaker Diarization Module, Multi-channel Audio Front-End Processing Module, and the ASR Module. These components collectively establish a cascading system, meticulously processing multi-channel and multi-speaker audio input. Moreover, this paper highlights the comprehensive optimization process that significantly enhanced our system's performance. Our team's submission is largely based on NeMo toolkits and will be publicly available.
</details>
<details>
<summary>摘要</summary>
我们现在介绍NVIDIA NeMo团队的多通道语音识别系统，用于7个CHiME挑战远程自动语音识别（DASR）任务。我们专注于通过分布式麦克风和麦克风数组记录语音的多通道多发言人语音识别系统的开发。这个系统主要由以下几个基本模块组成：说话人分类模块、多通道音频前端处理模块和ASR模块。这些组件结合起来形成了一个减法系统，精心处理多通道和多发言人的音频输入。此外，这篇论文还描述了我们对系统性能的全面优化过程，这有效地提高了我们的系统性能。我们的提交基于NeMo工具包，将在公共可用。
</details></li>
</ul>
<hr>
<h2 id="Property-Aware-Multi-Speaker-Data-Simulation-A-Probabilistic-Modelling-Technique-for-Synthetic-Data-Generation"><a href="#Property-Aware-Multi-Speaker-Data-Simulation-A-Probabilistic-Modelling-Technique-for-Synthetic-Data-Generation" class="headerlink" title="Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling Technique for Synthetic Data Generation"></a>Property-Aware Multi-Speaker Data Simulation: A Probabilistic Modelling Technique for Synthetic Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12371">http://arxiv.org/abs/2310.12371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tae Jin Park, He Huang, Coleman Hooper, Nithin Koluguri, Kunal Dhawan, Ante Jukic, Jagadeesh Balam, Boris Ginsburg</li>
<li>for: 本文提供了一个复杂的多个说话人语音数据模拟器，用于生成多个说话人语音录音。这个模拟器可以根据参数来调整语音中的沟壑和重叠分布，以提供一个适应性强的训练环境，用于开发适用于 speaker diarization 和 voice activity detection 的神经网络模型。</li>
<li>methods: 本文使用了一种复杂的多个说话人语音数据模拟器，可以根据参数来调整语音中的沟壑和重叠分布。</li>
<li>results: 本文 demonstates 了该模拟器可以生成具有实际统计特性的大规模语音混合数据集，并且可以用于训练 speaker diarization 和 voice activity detection 模型，以实现高效的识别和分离。<details>
<summary>Abstract</summary>
We introduce a sophisticated multi-speaker speech data simulator, specifically engineered to generate multi-speaker speech recordings. A notable feature of this simulator is its capacity to modulate the distribution of silence and overlap via the adjustment of statistical parameters. This capability offers a tailored training environment for developing neural models suited for speaker diarization and voice activity detection. The acquisition of substantial datasets for speaker diarization often presents a significant challenge, particularly in multi-speaker scenarios. Furthermore, the precise time stamp annotation of speech data is a critical factor for training both speaker diarization and voice activity detection. Our proposed multi-speaker simulator tackles these problems by generating large-scale audio mixtures that maintain statistical properties closely aligned with the input parameters. We demonstrate that the proposed multi-speaker simulator generates audio mixtures with statistical properties that closely align with the input parameters derived from real-world statistics. Additionally, we present the effectiveness of speaker diarization and voice activity detection models, which have been trained exclusively on the generated simulated datasets.
</details>
<details>
<summary>摘要</summary>
我团队介绍了一种高级多话者语音数据 simulate器，特性是生成多话者语音录音。这个 simulate器的一个特点是通过调整统计参数来模拟 silence和 overlap 的分布。这种能力提供了一个适应性高的训练环境，用于开发适合 speaker diarization 和 voice activity detection 的神经网络模型。在多话者场景下获得大量的 speaker diarization 数据经常是一项大的挑战，而且 precisetimestamp 注释的语音数据是神经网络模型的训练必要因素。我们的提议的多话者 simulate器解决了这些问题，生成了具有统计性质相近于输入参数的大规模音频混合。我们还展示了通过 exclusively 在生成的模拟数据上训练的 speaker diarization 和 voice activity detection 模型的效果。
</details></li>
</ul>
<hr>
<h2 id="BUT-CHiME-7-system-description"><a href="#BUT-CHiME-7-system-description" class="headerlink" title="BUT CHiME-7 system description"></a>BUT CHiME-7 system description</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11921">http://arxiv.org/abs/2310.11921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Karafiát, Karel Veselý, Igor Szöke, Ladislav Mošner, Karel Beneš, Marcin Witkowski, Germán Barchi, Leonardo Pepino</li>
<li>for: 这项研究是为了开发自动语音识别系统，特别是在CHiME-7挑战中的远场自动语音识别（DASR）领域。</li>
<li>methods: 这项研究使用了许多端到端模型，以及多个工具包。它们 heavily 依赖了指导源分离（GSS）将多通道音频转化为单通道。另外，ASR 使用了自我超vised学习模型生成的语音表示，并进行了多个 ASR 系统的融合。</li>
<li>results: 研究中的系统使用了 oracle 分 segmentation，并在远场自动语音识别（DASR）领域实现了良好的成绩。<details>
<summary>Abstract</summary>
This paper describes the joint effort of Brno University of Technology (BUT), AGH University of Krakow and University of Buenos Aires on the development of Automatic Speech Recognition systems for the CHiME-7 Challenge. We train and evaluate various end-to-end models with several toolkits. We heavily relied on Guided Source Separation (GSS) to convert multi-channel audio to single channel. The ASR is leveraging speech representations from models pre-trained by self-supervised learning, and we do a fusion of several ASR systems. In addition, we modified external data from the LibriSpeech corpus to become a close domain and added it to the training. Our efforts were focused on the far-field acoustic robustness sub-track of Task 1 - Distant Automatic Speech Recognition (DASR), our systems use oracle segmentation.
</details>
<details>
<summary>摘要</summary>
这份报告描述布雷诺技术大学（BUT）、阿格大学（AGH）和布宜诺斯艾利斯大学（UBA）在CHiME-7挑战中开发自动语音识别系统的共同努力。我们使用了准则分离（GSS）将多通道音频转化为单通道，并使用自我supervised学习预训练的语音表示模型。我们还对外部数据集进行了修改，使其成为近频域数据集，并将其添加到训练中。我们的努力主要集中在Task 1 - 远程自动语音识别（DASR）的远场静音环境下，我们使用了oracle分割。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Neural-Network-for-Acoustic-Resonance-Analysis"><a href="#Physics-informed-Neural-Network-for-Acoustic-Resonance-Analysis" class="headerlink" title="Physics-informed Neural Network for Acoustic Resonance Analysis"></a>Physics-informed Neural Network for Acoustic Resonance Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11804">http://arxiv.org/abs/2310.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuya Yokota, Takahiko Kurahashi, Masajiro Abe</li>
<li>for: 这个研究使用物理学生网络（PINN）框架来解决声波方程的共振分析问题。</li>
<li>methods: 这种方法称为ResoNet，它除了使用传统PINN损失函数外，还尝试使用神经网络函数近似能力来有效地解决共振分析问题。此外，它也可以轻松应用于反向问题。</li>
<li>results: 在一个一维声波管中进行了共振分析，并通过对前向和反向分析进行比较，证明了提案的方法的有效性。<details>
<summary>Abstract</summary>
This study proposes the physics-informed neural network (PINN) framework to solve the wave equation for acoustic resonance analysis. ResoNet, the analytical model proposed in this study, minimizes the loss function for periodic solutions, in addition to conventional PINN loss functions, thereby effectively using the function approximation capability of neural networks, while performing resonance analysis. Additionally, it can be easily applied to inverse problems. Herein, the resonance in a one-dimensional acoustic tube was analyzed. The effectiveness of the proposed method was validated through the forward and inverse analyses of the wave equation with energy-loss terms. In the forward analysis, the applicability of PINN to the resonance problem was evaluated by comparison with the finite-difference method. The inverse analysis, which included the identification of the energy loss term in the wave equation and design optimization of the acoustic tube, was performed with good accuracy.
</details>
<details>
<summary>摘要</summary>
这项研究提出了物理学 Informed Neural Network（PINN）框架，以解决音频振荡分析中的波方程。ResoNet，这个研究所提出的分析模型，不仅会遵循普通的PINN损失函数，还会将 périodic solutions 作为损失函数的最小化，从而有效地利用神经网络的函数近似能力，进行振荡分析。此外，它可以轻松应用于反向问题。在这种情况下，我们对一维音频管进行了振荡分析。我们采用了PINN方法和finite-difference方法进行前向和反向分析，并证明了PINN方法的可靠性和精度。Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese. The other version is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Blind-estimation-of-audio-effects-using-an-auto-encoder-approach-and-differentiable-signal-processing"><a href="#Blind-estimation-of-audio-effects-using-an-auto-encoder-approach-and-differentiable-signal-processing" class="headerlink" title="Blind estimation of audio effects using an auto-encoder approach and differentiable signal processing"></a>Blind estimation of audio effects using an auto-encoder approach and differentiable signal processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11781">http://arxiv.org/abs/2310.11781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/peladeaucome/ICASSP-2024-BEAFX-using-DDSP">https://github.com/peladeaucome/ICASSP-2024-BEAFX-using-DDSP</a></li>
<li>paper_authors: Côme Peladeau, Geoffroy Peeters</li>
<li>for: 估算音频效果（BE-AFX）旨在估算一个原始、未处理的音频样本中应用的音频效果（AFX），不需要知道AFX的具体实现。</li>
<li>methods: 我们提出了一种替代方案，使用自适应oder来估算音频质量指标。我们对常用的Mastering AFX进行了不同的实现，包括极差信号处理或神经网络近似。</li>
<li>results: 我们的自适应oder方法可以在不知道AFX的具体实现情况下，对音频质量产生更好的估算，比传统的参数基于方法更好。<details>
<summary>Abstract</summary>
Blind Estimation of Audio Effects (BE-AFX) aims at estimating the Audio Effects (AFXs) applied to an original, unprocessed audio sample solely based on the processed audio sample. To train such a system traditional approaches optimize a loss between ground truth and estimated AFX parameters. This involves knowing the exact implementation of the AFXs used for the process. In this work, we propose an alternative solution that eliminates the requirement for knowing this implementation. Instead, we introduce an auto-encoder approach, which optimizes an audio quality metric. We explore, suggest, and compare various implementations of commonly used mastering AFXs, using differential signal processing or neural approximations. Our findings demonstrate that our auto-encoder approach yields superior estimates of the audio quality produced by a chain of AFXs, compared to the traditional parameter-based approach, even if the latter provides a more accurate parameter estimation.
</details>
<details>
<summary>摘要</summary>
《盲目估计音效（BE-AFX）》的目标是基于原始、未处理的音频样本估计音效（AFX）。传统方法通过优化损失函数来学习AFX参数。这需要了解AFX的具体实现。在这个工作中，我们提出了一种不同的解决方案，即使用自适应网络方法，优化音质指标。我们研究、建议和比较了各种通用音压缩AFX的实现方式，使用演变信号处理或神经网络近似。我们的发现表明，我们的自适应网络方法可以在不知道AFX实现细节的情况下提供更高质量的音频估计结果，与传统参数基本方法相比，即使后者可以更准确地估计参数。
</details></li>
</ul>
<hr>
<h2 id="EchoScan-Scanning-Complex-Indoor-Geometries-via-Acoustic-Echoes"><a href="#EchoScan-Scanning-Complex-Indoor-Geometries-via-Acoustic-Echoes" class="headerlink" title="EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes"></a>EchoScan: Scanning Complex Indoor Geometries via Acoustic Echoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11728">http://arxiv.org/abs/2310.11728</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inmo Yeon, Iljoo Jeong, Seungchul Lee, Jung-Woo Choi</li>
<li>for: 该研究旨在实现准确的室内空间几何计算，以便建立准确的数字双子，其应用包括不熟悉环境导航和高效逃生规划，尤其是在低光照条件下。</li>
<li>methods: 该研究提出了一种深度神经网络模型，利用声学听觉采集到的听觉响应来进行室内空间几何计算。传统的声学技术仅仅估算室内几何参数，而不能直接推断室内几何图像，因此有限了可推断的室内几何类型。相比之下，EchoScan可以直接推断室内几何图像，并且可以处理具有拐角的室内空间。EchoScan的关键创新在于通过多个聚合模块分析室内响应的低频和高频听觉之间的复杂关系。</li>
<li>results: 与视觉方法相比，EchoScan在具有不同形状的室内空间中表现出色，其可以准确地计算室内几何图像。<details>
<summary>Abstract</summary>
Accurate estimation of indoor space geometries is vital for constructing precise digital twins, whose broad industrial applications include navigation in unfamiliar environments and efficient evacuation planning, particularly in low-light conditions. This study introduces EchoScan, a deep neural network model that utilizes acoustic echoes to perform room geometry inference. Conventional sound-based techniques rely on estimating geometry-related room parameters such as wall position and room size, thereby limiting the diversity of inferable room geometries. Contrarily, EchoScan overcomes this limitation by directly inferring room floorplans and heights, thereby enabling it to handle rooms with arbitrary shapes, including curved walls. The key innovation of EchoScan is its ability to analyze the complex relationship between low- and high-order reflections in room impulse responses (RIRs) using a multi-aggregation module. The analysis of high-order reflections also enables it to infer complex room shapes when echoes are unobservable from the position of an audio device. Herein, EchoScan was trained and evaluated using RIRs synthesized from complex environments, including the Manhattan and Atlanta layouts, employing a practical audio device configuration compatible with commercial, off-the-shelf devices. Compared with vision-based methods, EchoScan demonstrated outstanding geometry estimation performance in rooms with various shapes.
</details>
<details>
<summary>摘要</summary>
准确地估算室内空间几何结构是建立精准数字 duplicates 的关键，其广泛的工业应用包括在不熟悉环境中导航和有效逃生规划，特别是在低照明条件下。本研究介绍EchoScan，一种深度神经网络模型，利用声学折射来进行室内空间几何推测。传统的声音基本技术是估算室内几何参数，例如墙position和室内大小，因此限制了可以推测的室内几何类型。相比之下，EchoScan可以直接推测室内平面图和高度，因此可以处理具有任意形状的室内空间，包括拱形墙。EchoScan的关键创新在于使用多维度聚合模块来分析室内响应函数（RIRs）中的低频和高频响应之复杂关系。通过分析高频响应，EchoScan可以推测复杂的室内形状，即使声音在室内设备的位置不可见。在本研究中，EchoScan通过使用来自复杂环境的RIRs进行训练和评估，并使用实际的音频设备配置，与商业市场上可以购买的设备相符。与视觉基本方法相比，EchoScan在具有不同形状的室内空间中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Experimental-Results-of-Underwater-Sound-Speed-Profile-Inversion-by-Few-shot-Multi-task-Learning"><a href="#Experimental-Results-of-Underwater-Sound-Speed-Profile-Inversion-by-Few-shot-Multi-task-Learning" class="headerlink" title="Experimental Results of Underwater Sound Speed Profile Inversion by Few-shot Multi-task Learning"></a>Experimental Results of Underwater Sound Speed Profile Inversion by Few-shot Multi-task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11708">http://arxiv.org/abs/2310.11708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Fan Gao, Junting Wang, Hao Zhang</li>
<li>for: 快速和准确地估算海域声速 Profile (SSP) 对于海域声信号的传播模式具有很大的影响，因此快速和准确地估算 SSP 非常重要。</li>
<li>methods: state-of-the-art SSP inversion methods include frameworks of matched field processing (MFP), compressive sensing (CS), and feedforeward neural networks (FNN), among which the FNN shows better real-time performance while maintaining the same level of accuracy.</li>
<li>results: MTL outperforms the state-of-the-art methods in terms of accuracy for SSP inversion, while inheriting the real-time advantage of FNN during the inversion stage.<details>
<summary>Abstract</summary>
Underwater Sound Speed Profile (SSP) distribution has great influence on the propagation mode of acoustic signal, thus the fast and accurate estimation of SSP is of great importance in building underwater observation systems. The state-of-the-art SSP inversion methods include frameworks of matched field processing (MFP), compressive sensing (CS), and feedforeward neural networks (FNN), among which the FNN shows better real-time performance while maintain the same level of accuracy. However, the training of FNN needs quite a lot historical SSP samples, which is diffcult to be satisfied in many ocean areas. This situation is called few-shot learning. To tackle this issue, we propose a multi-task learning (MTL) model with partial parameter sharing among different traning tasks. By MTL, common features could be extracted, thus accelerating the learning process on given tasks, and reducing the demand for reference samples, so as to enhance the generalization ability in few-shot learning. To verify the feasibility and effectiveness of MTL, a deep-ocean experiment was held in April 2023 at the South China Sea. Results shows that MTL outperforms the state-of-the-art methods in terms of accuracy for SSP inversion, while inherits the real-time advantage of FNN during the inversion stage.
</details>
<details>
<summary>摘要</summary>
水下声速谱（SSP）分布对声音信号的传播模式有着很大的影响，因此快速和准确地估算SSP是建设水下观测系统的关键。现状的SSP拟合方法包括匹配场处理（MFP）、压缩感知（CS）和前向神经网络（FNN）等，其中FNN在实时性方面表现更好，同时保持同等的准确性。然而，FNN的训练需要很多历史SSP样本，这在许多海洋区域是困难的满足。这种情况被称为“少shot learning”。为解决这个问题，我们提出了多任务学习（MTL）模型，其中参数之间有部分共享。通过MTL，可以提取共同特征，因此加速学习过程，降低参考样本的需求，从而提高总体化能力在少shot learning中。为验证MTL的可行性和效果，在2023年4月在南海进行了深海实验。结果显示，MTL比现状的方法在SSP拟合精度方面表现更好，同时继承FNN在拟合阶段的实时优势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.SD_2023_10_18/" data-id="clpxp6c7h011cee88fto01n82" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/eess.AS_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T14:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/eess.AS_2023_10_18/">eess.AS - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Spoofing-Speech-Detection-Using-Rhythm-Information"><a href="#Enhancing-Spoofing-Speech-Detection-Using-Rhythm-Information" class="headerlink" title="Enhancing Spoofing Speech Detection Using Rhythm Information"></a>Enhancing Spoofing Speech Detection Using Rhythm Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12014">http://arxiv.org/abs/2310.12014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingze Lu, Yuxiang Zhang, Wenchao Wang, Zengqiang Shang, Pengyuan Zhang</li>
<li>for: 增强anti- spoofing检测系统的可靠性</li>
<li>methods: 利用rhythm信息分析TTS生成的语音中的缺陷，并通过过滤vocal tract response来保留语音中的rhythm信息，以增强copy-synthesis数据增强方法</li>
<li>results: 提高了anti-spoofing检测系统的检测能力，增强了对TTS生成语音的检测能力<details>
<summary>Abstract</summary>
Spoofing speech detection is a hot and in-demand research field. However, current spoofing speech detection systems is lack of convincing evidence. In this paper, to increase the reliability of detection systems, the flaws of rhythm information inherent in the TTS-generated speech are analyzed. TTS models take text as input and utilize acoustic models to predict rhythm information, which introduces artifacts in the rhythm information. By filtering out vocal tract response, the remaining glottal flow with rhythm information retains detection ability for TTS-generated speech. Based on these analyses, a rhythm perturbation module is proposed to enhance the copy-synthesis data augmentation method. Fake utterances generated by the proposed method force the detecting model to pay attention to the artifacts in rhythm information and effectively improve the ability to detect TTS-generated speech of the anti-spoofing countermeasures.
</details>
<details>
<summary>摘要</summary>
假语言识别是一个热门的研究领域，但目前的假语言识别系统尚缺乏充分的证据。在这篇论文中，为了提高检测系统的可靠性，我们分析了 TTS 生成的语音中的饱和信息的缺陷。 TTS 模型将文本作为输入，利用声学模型预测语音中的饱和信息，这会导致语音中的饱和信息受到质量问题的影响。通过滤除声道响应，保留的喉咙流量仍然具有检测能力。基于这些分析，我们提出了一种饱和抖振模块，以增强复制数据增强法。这种方法生成的假语音让检测模型更加注意饱和信息中的瑕疵，从而提高了对 TTS 生成语音的检测能力。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/eess.AS_2023_10_18/" data-id="clpxp6c96015hee88hybl83hs" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.CV_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T13:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.CV_2023_10_18/">cs.CV - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-Representation-Learning-for-Histopathologic-Images-with-Cluster-Constraints"><a href="#Improving-Representation-Learning-for-Histopathologic-Images-with-Cluster-Constraints" class="headerlink" title="Improving Representation Learning for Histopathologic Images with Cluster Constraints"></a>Improving Representation Learning for Histopathologic Images with Cluster Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12334">http://arxiv.org/abs/2310.12334</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wwyi1828/clusiam">https://github.com/wwyi1828/clusiam</a></li>
<li>paper_authors: Weiyi Wu, Chongyang Gao, Joseph DiPalma, Soroush Vosoughi, Saeed Hassanpour</li>
<li>for: 这个论文旨在提出一种自助学习框架，用于在整个报告图像（WSI）分析中学习表示转移和Semantic clustering。</li>
<li>methods: 该框架结合了不变性损失和归类损失，以实现在WSI分析中的表示学习和归类。</li>
<li>results: 对于Camelyon16和肝癌数据集的下游分类和归类任务，该方法表现出了较好的性能，比较常见的SSL方法更高。<details>
<summary>Abstract</summary>
Recent advances in whole-slide image (WSI) scanners and computational capabilities have significantly propelled the application of artificial intelligence in histopathology slide analysis. While these strides are promising, current supervised learning approaches for WSI analysis come with the challenge of exhaustively labeling high-resolution slides - a process that is both labor-intensive and time-consuming. In contrast, self-supervised learning (SSL) pretraining strategies are emerging as a viable alternative, given that they don't rely on explicit data annotations. These SSL strategies are quickly bridging the performance disparity with their supervised counterparts. In this context, we introduce an SSL framework. This framework aims for transferable representation learning and semantically meaningful clustering by synergizing invariance loss and clustering loss in WSI analysis. Notably, our approach outperforms common SSL methods in downstream classification and clustering tasks, as evidenced by tests on the Camelyon16 and a pancreatic cancer dataset. The code and additional details are accessible at: https://github.com/wwyi1828/CluSiam.
</details>
<details>
<summary>摘要</summary>
（简体中文）最近，整个扫描图像（WSI）扫描器和计算能力的进步有助于人工智能在医学报告板图分析中应用。虽然这些进步非常有前途，但现有的监督学习方法 для WSI 分析带来了大量高分辨率板图标注的劳动和时间的挑战。相比之下，自动学习（SSL）预训练策略是一个可行的选择，因为它们不需要显式数据注释。这些 SSL 策略快速bridging 监督学习方法的性能差距。在这个上下文中，我们介绍了一个 SSL 框架。这个框架的目标是在 WSI 分析中实现可重用的表示学习和具有意义的归一化。尤其是，我们的方法在下游分类和归一化任务中表现出色，如Camelyon16 和胰腺癌数据集的测试所示。代码和更多细节可以在：https://github.com/wwyi1828/CluSiam 上获取。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Video-Transformers-for-Segmentation-A-Survey-of-Application-and-Interpretability"><a href="#Understanding-Video-Transformers-for-Segmentation-A-Survey-of-Application-and-Interpretability" class="headerlink" title="Understanding Video Transformers for Segmentation: A Survey of Application and Interpretability"></a>Understanding Video Transformers for Segmentation: A Survey of Application and Interpretability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12296">http://arxiv.org/abs/2310.12296</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rezaul Karim, Richard P. Wildes</li>
<li>for: 本研究围绕视频分割问题进行了全面的概述，包括对象、场景、演员行为和多modal视频分割等多种类型的问题，以获取任务特定的场景组成部分的像素级mask。</li>
<li>methods: 本文主要介绍了在视频分割领域的转换器模型，以及各种可读性方法，包括后处和先处可读性方法，以及对视频时间准确性模型的分析。</li>
<li>results: 本文提供了一个广泛的视频分割模型的回顾，包括转换器模型的state-of-the-art模型，以及对不同视频分割任务的性能分析。同时，本文还提供了对视频模型的可读性分析，以及对视频时间缓动模型的分析。<details>
<summary>Abstract</summary>
Video segmentation encompasses a wide range of categories of problem formulation, e.g., object, scene, actor-action and multimodal video segmentation, for delineating task-specific scene components with pixel-level masks. Recently, approaches in this research area shifted from concentrating on ConvNet-based to transformer-based models. In addition, various interpretability approaches have appeared for transformer models and video temporal dynamics, motivated by the growing interest in basic scientific understanding, model diagnostics and societal implications of real-world deployment. Previous surveys mainly focused on ConvNet models on a subset of video segmentation tasks or transformers for classification tasks. Moreover, component-wise discussion of transformer-based video segmentation models has not yet received due focus. In addition, previous reviews of interpretability methods focused on transformers for classification, while analysis of video temporal dynamics modelling capabilities of video models received less attention. In this survey, we address the above with a thorough discussion of various categories of video segmentation, a component-wise discussion of the state-of-the-art transformer-based models, and a review of related interpretability methods. We first present an introduction to the different video segmentation task categories, their objectives, specific challenges and benchmark datasets. Next, we provide a component-wise review of recent transformer-based models and document the state of the art on different video segmentation tasks. Subsequently, we discuss post-hoc and ante-hoc interpretability methods for transformer models and interpretability methods for understanding the role of the temporal dimension in video models. Finally, we conclude our discussion with future research directions.
</details>
<details>
<summary>摘要</summary>
视频分割涉及许多问题的类型化问题表示，如对象、场景、actor-action和多模态视频分割，用于在像素级划分场景组成部分。最近，这个研究领域的方法从 concentrate 在 ConvNet 基于的模型转移到 transformer 基于的模型。此外，对 transformer 模型的解释方法出现了多种， motivated 由实际应用中的基本科学理解、模型诊断和社会意义的增长。之前的survey 主要集中在 ConvNet 模型上一部分的视频分割任务或 transformer  для类型任务。此外，transformer 基于视频分割模型的组件化讨论未得到过due focus。此外，以前的解释方法集中在 transformer 模型的类型任务上，而视频时间 Dinamics 模型分析 capabilities 收到了更少的注意。在这种survey中，我们解决了以下问题：提供不同类型的视频分割任务、其目标、特定挑战和标准数据集的介绍；提供 recent transformer 基于模型的组件化评论，并记录不同视频分割任务的state of the art ; 讨论 post-hoc 和 ante-hoc 解释方法 для transformer 模型，以及对视频模型中的时间维度的解释方法。最后，我们 conclude 我们的讨论，并提出未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="Improving-SCGAN’s-Similarity-Constraint-and-Learning-a-Better-Disentangled-Representation"><a href="#Improving-SCGAN’s-Similarity-Constraint-and-Learning-a-Better-Disentangled-Representation" class="headerlink" title="Improving SCGAN’s Similarity Constraint and Learning a Better Disentangled Representation"></a>Improving SCGAN’s Similarity Constraint and Learning a Better Disentangled Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12262">http://arxiv.org/abs/2310.12262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Iman-yazdanpanah/contrastive-SSIM-SCGAN">https://github.com/Iman-yazdanpanah/contrastive-SSIM-SCGAN</a></li>
<li>paper_authors: Iman Yazdanpanah</li>
<li>For: 本研究使用SCGAN模型，通过添加同condition之间的相似性约束，来提高生成器网络的可读性和智能度。* Methods: 本研究使用了SSIM指标来衡量生成图像之间的相似性，并应用了对相似性约束的对照损失原理。* Results: 对比FID和因子VAE指标，改进后的模型表现更佳，并且有较好的泛化性。Here’s a more detailed explanation of each point:* For: The paper uses SCGAN (Similarity-constrained GAN) model to improve the readability and intelligence of the generator network. SCGAN adds a similarity constraint between generated images and conditions, which works as a tutor to instruct the generator network to comprehend the difference of representations based on conditions.* Methods: The paper uses SSIM (Structural Similarity Index Measure) to measure the similarity between generated images and conditions. The authors also apply the contrastive loss principle to the similarity constraint.* Results: The modified model performs better using FID (Fréchet Inception Distance) and FactorVAE (Factorized Variational Autoencoder) metrics. The modified model also has better generalizability compared to other models.<details>
<summary>Abstract</summary>
SCGAN adds a similarity constraint between generated images and conditions as a regularization term on generative adversarial networks. Similarity constraint works as a tutor to instruct the generator network to comprehend the difference of representations based on conditions. We understand how SCGAN works on a deeper level. This understanding makes us realize that the similarity constraint functions like the contrastive loss function. We believe that a model with high understanding and intelligence measures the similarity between images based on their structure and high level features, just like humans do. Two major changes we applied to SCGAN in order to make a modified model are using SSIM to measure similarity between images and applying contrastive loss principles to the similarity constraint. The modified model performs better using FID and FactorVAE metrics. The modified model also has better generalisability compared to other models. Keywords Generative Adversarial Nets, Unsupervised Learning, Disentangled Representation Learning, Contrastive Disentanglement, SSIM
</details>
<details>
<summary>摘要</summary>
SCGAN 添加了生成图像和条件之间的相似性约束作为生成对抗网络的规范项。相似性约束工作如一位教师，让生成网络理解基于条件的表示之间的差异。我们更深入了解 SCGAN 的工作原理，并发现它与对照损失函数相似。我们认为一个高度理解和智能的模型会根据图像的结构和高级特征来衡量图像之间的相似性，就像人类一样。为了使得 modified SCGAN 表现更好，我们对其进行了两个主要变更。首先，我们使用 SSIM 来衡量图像之间的相似性。其次，我们将对照损失原理应用到相似性约束中。这个修改后的模型在 FID 和 FactorVAE  metric 上表现更好，并且有更高的泛化能力。关键词：生成对抗网络、无监督学习、分解表示学习、对照分解、SSIM
</details></li>
</ul>
<hr>
<h2 id="REVAMP-Automated-Simulations-of-Adversarial-Attacks-on-Arbitrary-Objects-in-Realistic-Scenes"><a href="#REVAMP-Automated-Simulations-of-Adversarial-Attacks-on-Arbitrary-Objects-in-Realistic-Scenes" class="headerlink" title="REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes"></a>REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12243">http://arxiv.org/abs/2310.12243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/poloclub/revamp">https://github.com/poloclub/revamp</a></li>
<li>paper_authors: Matthew Hull, Zijie J. Wang, Duen Horng Chau</li>
<li>for: This paper is written for researchers and practitioners who want to study and defend against adversarial attacks on deep learning models in computer vision, specifically in the context of autonomous vehicles.</li>
<li>methods: The paper introduces REVAMP, an easy-to-use Python library that allows users to create attack scenarios with arbitrary objects and simulate realistic environmental factors, lighting, reflection, and refraction. REVAMP uses differentiable rendering to reproduce physically plausible adversarial objects.</li>
<li>results: The paper demonstrates the effectiveness of REVAMP in producing adversarial textures that can cause misclassification of objects in real-world scenarios. The audience can choose a scene, object to attack, desired attack class, and number of camera positions to use, and REVAMP will show how the altered texture causes the chosen object to be misclassified in real time.<details>
<summary>Abstract</summary>
Deep Learning models, such as those used in an autonomous vehicle are vulnerable to adversarial attacks where an attacker could place an adversarial object in the environment, leading to mis-classification. Generating these adversarial objects in the digital space has been extensively studied, however successfully transferring these attacks from the digital realm to the physical realm has proven challenging when controlling for real-world environmental factors. In response to these limitations, we introduce REVAMP, an easy-to-use Python library that is the first-of-its-kind tool for creating attack scenarios with arbitrary objects and simulating realistic environmental factors, lighting, reflection, and refraction. REVAMP enables researchers and practitioners to swiftly explore various scenarios within the digital realm by offering a wide range of configurable options for designing experiments and using differentiable rendering to reproduce physically plausible adversarial objects. We will demonstrate and invite the audience to try REVAMP to produce an adversarial texture on a chosen object while having control over various scene parameters. The audience will choose a scene, an object to attack, the desired attack class, and the number of camera positions to use. Then, in real time, we show how this altered texture causes the chosen object to be mis-classified, showcasing the potential of REVAMP in real-world scenarios. REVAMP is open-source and available at https://github.com/poloclub/revamp.
</details>
<details>
<summary>摘要</summary>
深度学习模型，如自动驾驶车辆中使用的模型，容易受到敌意攻击，攻击者可能会放置一个恶意物体在环境中，导致误分类。在数字世界中生成这些攻击物体已经广泛研究，但在控制真实环境因素时将这些攻击 transferred to the physical realm 是一个挑战。为了解决这些限制，我们介绍了 REVAMP，一个简单易用的 Python 库，它是首个类似的工具，可以创建攻击场景，并模拟真实环境因素，光照、折射和折射。 REVAMP 允许研究人员和实践者快速探索不同的场景，通过配置多种实验设置和使用可微的渲染来生成真实物理上的攻击对象。我们将在演示中让观众选择场景、 Object 和攻击类别，然后在实时中生成攻击的 Texture，并在不同的摄像头位置下显示攻击对象的误分类效果，展示 REVAMP 在真实场景中的潜力。 REVAMP 是开源的，可以在 https://github.com/poloclub/revamp 上下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Rich-Semantics-and-Coarse-Locations-for-Long-tailed-Object-Detection"><a href="#Learning-from-Rich-Semantics-and-Coarse-Locations-for-Long-tailed-Object-Detection" class="headerlink" title="Learning from Rich Semantics and Coarse Locations for Long-tailed Object Detection"></a>Learning from Rich Semantics and Coarse Locations for Long-tailed Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12152">http://arxiv.org/abs/2310.12152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingchen Meng, Xiyang Dai, Jianwei Yang, Dongdong Chen, Yinpeng Chen, Mengchen Liu, Yi-Ling Chen, Zuxuan Wu, Lu Yuan, Yu-Gang Jiang</li>
<li>for:  handles extreme data imbalance in real-world datasets, specifically long-tailed object detection (LTOD)</li>
<li>methods: 	+ explores extra data with image-level labels, but limited results due to semantic ambiguity and location sensitivity	+ proposes RichSem, a simple and effective method that leverages rich semantics from images as additional soft supervision for training detectors	+ adds a semantic branch to the detector to learn soft semantics and enhance feature representations for long-tailed object detection</li>
<li>results: 	+ achieves consistent improvements on both overall and rare-category of LVIS under different backbones and detectors	+ achieves state-of-the-art performance without requiring complex training and testing procedures	+ demonstrates effectiveness on other long-tailed datasets with additional experiments.<details>
<summary>Abstract</summary>
Long-tailed object detection (LTOD) aims to handle the extreme data imbalance in real-world datasets, where many tail classes have scarce instances. One popular strategy is to explore extra data with image-level labels, yet it produces limited results due to (1) semantic ambiguity -- an image-level label only captures a salient part of the image, ignoring the remaining rich semantics within the image; and (2) location sensitivity -- the label highly depends on the locations and crops of the original image, which may change after data transformations like random cropping. To remedy this, we propose RichSem, a simple but effective method, which is robust to learn rich semantics from coarse locations without the need of accurate bounding boxes. RichSem leverages rich semantics from images, which are then served as additional soft supervision for training detectors. Specifically, we add a semantic branch to our detector to learn these soft semantics and enhance feature representations for long-tailed object detection. The semantic branch is only used for training and is removed during inference. RichSem achieves consistent improvements on both overall and rare-category of LVIS under different backbones and detectors. Our method achieves state-of-the-art performance without requiring complex training and testing procedures. Moreover, we show the effectiveness of our method on other long-tailed datasets with additional experiments. Code is available at \url{https://github.com/MengLcool/RichSem}.
</details>
<details>
<summary>摘要</summary>
长尾物体检测（LTOD）目标是处理实际数据中的极端数据不均衡，其中许多尾类具有稀缺的实例数。一种受欢迎的策略是探索带有图像级标签的额外数据，但这会产生有限的结果，因为（1）语义抽象---图像级标签只捕捉图像中的一个突出部分，忽略图像中的剩余 semantics;（2）位置敏感---标签强度取决于原始图像的位置和裁剪，这些位置和裁剪可能会在数据变换后发生变化。为了缓解这些问题，我们提议了RichSem，一种简单 yet有效的方法，可以在不具备精确 bounding box 的情况下，强制学习图像中的丰富 semantics。RichSem 利用图像中的丰富 semantics，并将其作为增强特征表示的额外超级vision。我们在检测器中添加了一个语义分支，以学习这些软 semantics，并增强特征表示以适应长尾物体检测。语义分支只用于训练，并在检测中被移除。RichSem 在不同的背景和检测器下实现了稳定的改进，并 achieved state-of-the-art 性能。此外，我们还在其他长尾数据集上进行了进一步的实验，以证明我们的方法的一致性。代码可以在 \url{https://github.com/MengLcool/RichSem} 中找到。
</details></li>
</ul>
<hr>
<h2 id="Object-aware-Inversion-and-Reassembly-for-Image-Editing"><a href="#Object-aware-Inversion-and-Reassembly-for-Image-Editing" class="headerlink" title="Object-aware Inversion and Reassembly for Image Editing"></a>Object-aware Inversion and Reassembly for Image Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12149">http://arxiv.org/abs/2310.12149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Yang, Dinggang Gui, Wen Wang, Hao Chen, Bohan Zhuang, Chunhua Shen</li>
<li>for: 这个论文的目的是提出一种新的图像编辑方法，以便实现对象级别的细化编辑。</li>
<li>methods: 该方法使用一种新的搜索度量，jointly considering the editability of the target and the fidelity of the non-editing region，来确定最佳的反向步骤数量。然后，对每个编辑对 separately 进行编辑，以避免概念匹配错误。最后，提出一个重新组装步骤，以将各个编辑结果和非编辑区域一起 integrate 得到最终的编辑图像。</li>
<li>results: 该方法在单个对象编辑和多个对象编辑场景中都有优秀的表现，特别是在多对象编辑场景中。<details>
<summary>Abstract</summary>
By comparing the original and target prompts in editing task, we can obtain numerous editing pairs, each comprising an object and its corresponding editing target. To allow editability while maintaining fidelity to the input image, existing editing methods typically involve a fixed number of inversion steps that project the whole input image to its noisier latent representation, followed by a denoising process guided by the target prompt. However, we find that the optimal number of inversion steps for achieving ideal editing results varies significantly among different editing pairs, owing to varying editing difficulties. Therefore, the current literature, which relies on a fixed number of inversion steps, produces sub-optimal generation quality, especially when handling multiple editing pairs in a natural image. To this end, we propose a new image editing paradigm, dubbed Object-aware Inversion and Reassembly (OIR), to enable object-level fine-grained editing. Specifically, we design a new search metric, which determines the optimal inversion steps for each editing pair, by jointly considering the editability of the target and the fidelity of the non-editing region. We use our search metric to find the optimal inversion step for each editing pair when editing an image. We then edit these editing pairs separately to avoid concept mismatch. Subsequently, we propose an additional reassembly step to seamlessly integrate the respective editing results and the non-editing region to obtain the final edited image. To systematically evaluate the effectiveness of our method, we collect two datasets for benchmarking single- and multi-object editing, respectively. Experiments demonstrate that our method achieves superior performance in editing object shapes, colors, materials, categories, etc., especially in multi-object editing scenarios.
</details>
<details>
<summary>摘要</summary>
通过比较原始和目标提示的编辑任务，我们可以获得大量的编辑对，每个编辑对包含一个对象和其对应的编辑目标。为了在保持原始图像的精度的情况下进行编辑，现有的编辑方法通常包括一定数量的反向步骤，这些步骤将原始图像投影到它的噪音卷积表示中，然后通过受引导的目标提示进行净化处理。然而，我们发现在不同的编辑对中，理想的反向步骤数量差异很大，这是因为不同的编辑对具有不同的编辑难度。因此，现有的文献，它依靠固定的反向步骤数量，生成质量不够优化，特别是在处理多个编辑对的自然图像时。为此，我们提出了一种新的图像编辑模式，称为对象感知反向拼接（OIR），以实现对象精细编辑。我们设计了一个新的搜索指标，该指标确定每个编辑对的优化反向步骤数量，同时考虑目标的编辑性和非编辑区域的精度。我们使用这个搜索指标来查找每个编辑对的优化反向步骤数量，然后分别编辑这些编辑对，以避免概念匹配错误。接着，我们提出了一个额外的重新拼接步骤，以精准地结合各个编辑结果和非编辑区域，从而获得最终的编辑图像。为了系统地评估我们的方法的效果，我们收集了两个数据集，用于单个对象编辑和多个对象编辑的benchmark。实验结果表明，我们的方法在对象形状、颜色、材质、类别等方面具有更高的编辑性，特别是在多个对象编辑场景下。
</details></li>
</ul>
<hr>
<h2 id="InViG-Benchmarking-Interactive-Visual-Grounding-with-500K-Human-Robot-Interactions"><a href="#InViG-Benchmarking-Interactive-Visual-Grounding-with-500K-Human-Robot-Interactions" class="headerlink" title="InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot Interactions"></a>InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12147">http://arxiv.org/abs/2310.12147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanbo Zhang, Jie Xu, Yuchen Mo, Tao Kong</li>
<li>for:  addresses the issues of reduced performance in realistic and open-ended scenarios in Human-Robot Interaction (HRI) by presenting a large-scale dataset, \invig, for interactive visual grounding under language ambiguity.</li>
<li>methods:  leverages the \invig dataset and proposes a set of baseline solutions for end-to-end interactive visual disambiguation and grounding.</li>
<li>results:  achieves a 45.6% success rate during validation, presenting a practical yet highly challenging benchmark for ambiguity-aware HRI.<details>
<summary>Abstract</summary>
Ambiguity is ubiquitous in human communication. Previous approaches in Human-Robot Interaction (HRI) have often relied on predefined interaction templates, leading to reduced performance in realistic and open-ended scenarios. To address these issues, we present a large-scale dataset, \invig, for interactive visual grounding under language ambiguity. Our dataset comprises over 520K images accompanied by open-ended goal-oriented disambiguation dialogues, encompassing millions of object instances and corresponding question-answer pairs. Leveraging the \invig dataset, we conduct extensive studies and propose a set of baseline solutions for end-to-end interactive visual disambiguation and grounding, achieving a 45.6\% success rate during validation. To the best of our knowledge, the \invig dataset is the first large-scale dataset for resolving open-ended interactive visual grounding, presenting a practical yet highly challenging benchmark for ambiguity-aware HRI. Codes and datasets are available at: \href{https://openivg.github.io}{https://openivg.github.io}.
</details>
<details>
<summary>摘要</summary>
人类交流中的不确定性是普遍存在的。过去的人机交互（HRI）方法经常采用预定的交互模板，导致在实际和开放式enario中表现不佳。为解决这些问题，我们提供了一个大规模的数据集，\invig，用于在语言不确定性下进行交互视觉固定。我们的数据集包括超过520万张图像和开放目标 oriented的不确定性对话，涵盖了数百万个物体实例和相应的问题答对。利用\invig数据集，我们进行了广泛的研究，并提出了一组基线解决方案，在验证中达到了45.6%的成功率。根据我们所知，\invig数据集是首个大规模的开放式交互视觉固定数据集，提供了实用又高度挑战的底层 benchmark для不确定性意识HRI。代码和数据集可以在：\href{https://openivg.github.io}{https://openivg.github.io}获取。
</details></li>
</ul>
<hr>
<h2 id="HSTR-Net-Reference-Based-Video-Super-resolution-for-Aerial-Surveillance-with-Dual-Cameras"><a href="#HSTR-Net-Reference-Based-Video-Super-resolution-for-Aerial-Surveillance-with-Dual-Cameras" class="headerlink" title="HSTR-Net: Reference Based Video Super-resolution for Aerial Surveillance with Dual Cameras"></a>HSTR-Net: Reference Based Video Super-resolution for Aerial Surveillance with Dual Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12092">http://arxiv.org/abs/2310.12092</a></li>
<li>repo_url: None</li>
<li>paper_authors: H. Umut Suluhan, Hasan F. Ates, Bahadir K. Gunturk</li>
<li>for: 这篇论文旨在提高空中监测的高空间时间分辨率（HSTR）视频，以便更准确地探测和跟踪对象。特别是在广泛监测（WAS）中，需要高精度的视频捕捉，但对象通常很小。</li>
<li>methods: 这篇论文提出了一种基于参考描述的超分辨率（RefSR）技术，使用两个摄像头同时拍摄不同的视频。一个摄像头拍摄高空间分辨率低帧率（HSLF）视频，另一个摄像头拍摄低空间分辨率高帧率（LSHF）视频。一种新的深度学习架构被提出，将HSLF和LSHF视频输入 fusion，并生成HSTR视频帧。该模型combines optical flow estimation和（通道和空间）注意机制，以捕捉视频帧之间的细微运动和细节关系。</li>
<li>results:  simulations show that the proposed model provides significant improvement over existing reference-based SR techniques in terms of PSNR and SSIM metrics. The method also exhibits sufficient frames per second (FPS) for WAS when deployed on a power-constrained drone equipped with dual cameras.<details>
<summary>Abstract</summary>
Aerial surveillance requires high spatio-temporal resolution (HSTR) video for more accurate detection and tracking of objects. This is especially true for wide-area surveillance (WAS), where the surveyed region is large and the objects of interest are small. This paper proposes a dual camera system for the generation of HSTR video using reference-based super-resolution (RefSR). One camera captures high spatial resolution low frame rate (HSLF) video while the other captures low spatial resolution high frame rate (LSHF) video simultaneously for the same scene. A novel deep learning architecture is proposed to fuse HSLF and LSHF video feeds and synthesize HSTR video frames at the output. The proposed model combines optical flow estimation and (channel-wise and spatial) attention mechanisms to capture the fine motion and intricate dependencies between frames of the two video feeds. Simulations show that the proposed model provides significant improvement over existing reference-based SR techniques in terms of PSNR and SSIM metrics. The method also exhibits sufficient frames per second (FPS) for WAS when deployed on a power-constrained drone equipped with dual cameras.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:空中监测需要高空时分解能力（HSTR）视频，以更加准确地探测和跟踪目标对象。这特别是对于广泛监测（WAS）来说，surveyed 区域很大，目标对象很小。这篇论文提议一种双摄像头系统，用于生成HSTR视频使用参考基于超解像（RefSR）。一个摄像头拍摄高空间分辨率低帧率（HSLF）视频，另一个摄像头拍摄低空间分辨率高帧率（LSHF）视频，同时拍摄同一场景。一种新的深度学习架构被提议，用于将HSLF和LSHF视频流合并并生成HSTR视频帧。提议的模型 combining 光流估算和通道 wise 和空间的注意机制，以捕捉视频流中细微的运动和细节依赖关系。实验显示，提议的模型在PSNR和SSIM指标上具有显著改进，并且在功率限制的飞机上部署时，具有足够的帧率（FPS） для WAS。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Imitation-Learning-A-Pose-Estimation-Perspective"><a href="#One-Shot-Imitation-Learning-A-Pose-Estimation-Perspective" class="headerlink" title="One-Shot Imitation Learning: A Pose Estimation Perspective"></a>One-Shot Imitation Learning: A Pose Estimation Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12077">http://arxiv.org/abs/2310.12077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pietro Vitiello, Kamil Dreczkowski, Edward Johns</li>
<li>for: 研究单个示例下的模仿学习，无需进一步的数据收集和任务或物体知识。</li>
<li>methods: 结合弹性轨迹传输和未看过的物体姿态估计，实现单个示例下的模仿学习。</li>
<li>results: 对十种真实世界任务进行深入研究，探讨摄像头准确性、姿态估计误差和空间总结对任务成功率的影响。Here’s the English version of the paper’s abstract for reference:”In this paper, we study imitation learning under the challenging setting of single demonstration, no further data collection, and no prior task or object knowledge. We show how imitation learning can be formulated as a combination of trajectory transfer and unseen object pose estimation. To explore this idea, we provide an in-depth study on how state-of-the-art unseen object pose estimators perform for one-shot imitation learning on ten real-world tasks, and we take a deep dive into the effects that camera calibration, pose estimation error, and spatial generalisation have on task success rates. For videos, please visit <a target="_blank" rel="noopener" href="https://www.robot-learning.uk/pose-estimation-perspective">https://www.robot-learning.uk/pose-estimation-perspective</a>.”<details>
<summary>Abstract</summary>
In this paper, we study imitation learning under the challenging setting of: (1) only a single demonstration, (2) no further data collection, and (3) no prior task or object knowledge. We show how, with these constraints, imitation learning can be formulated as a combination of trajectory transfer and unseen object pose estimation. To explore this idea, we provide an in-depth study on how state-of-the-art unseen object pose estimators perform for one-shot imitation learning on ten real-world tasks, and we take a deep dive into the effects that camera calibration, pose estimation error, and spatial generalisation have on task success rates. For videos, please visit https://www.robot-learning.uk/pose-estimation-perspective.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了一种具有以下三个挑战性 Setting：（1）只有单一示范，（2）没有进一步数据采集，（3）没有先前任务或物体知识。我们示出了如何，在这些限制下，使用imitating learning可以表述为路径传输和未经见过的物体姿态估计的组合。为了探讨这个想法，我们对state-of-the-art unseen object pose estimators进行了一项深入的研究，并对十种真实世界任务进行了一 shot imitation learning的研究。我们还对camera calibration、pose estimation error和空间泛化对任务成功率产生的影响进行了深入的分析。关于视频，请访问https://www.robot-learning.uk/pose-estimation-perspective。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Fairness-in-Pre-trained-Visual-Transformer-based-Natural-and-GAN-Generated-Image-Detection-Systems-and-Understanding-the-Impact-of-Image-Compression-in-Fairness"><a href="#Exploring-Fairness-in-Pre-trained-Visual-Transformer-based-Natural-and-GAN-Generated-Image-Detection-Systems-and-Understanding-the-Impact-of-Image-Compression-in-Fairness" class="headerlink" title="Exploring Fairness in Pre-trained Visual Transformer based Natural and GAN Generated Image Detection Systems and Understanding the Impact of Image Compression in Fairness"></a>Exploring Fairness in Pre-trained Visual Transformer based Natural and GAN Generated Image Detection Systems and Understanding the Impact of Image Compression in Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12076">http://arxiv.org/abs/2310.12076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manjary P. Gangan, Anoop Kadan, Lajish V L</li>
<li>For: This paper aims to explore fairness in transformer-based image forensic algorithms and evaluate their bias in various domains, including gender, racial, affective, and intersectional.* Methods: The study uses a bias evaluation corpora to analyze bias in the algorithms and employs a wide set of individual and pairwise bias evaluation measures. Additionally, the study examines the impact of image compression on model bias using a two-phase evaluation setting.* Results: The paper explores the bias in transformer-based image forensic algorithms and evaluates their fairness in different domains, providing insights into the potential biases in these algorithms and the impact of image compression on their fairness.<details>
<summary>Abstract</summary>
It is not only sufficient to construct computational models that can accurately classify or detect fake images from real images taken from a camera, but it is also important to ensure whether these computational models are fair enough or produce biased outcomes that can eventually harm certain social groups or cause serious security threats. Exploring fairness in forensic algorithms is an initial step towards correcting these biases. Since visual transformers are recently being widely used in most image classification based tasks due to their capability to produce high accuracies, this study tries to explore bias in the transformer based image forensic algorithms that classify natural and GAN generated images. By procuring a bias evaluation corpora, this study analyzes bias in gender, racial, affective, and intersectional domains using a wide set of individual and pairwise bias evaluation measures. As the generalizability of the algorithms against image compression is an important factor to be considered in forensic tasks, this study also analyzes the role of image compression on model bias. Hence to study the impact of image compression on model bias, a two phase evaluation setting is followed, where a set of experiments is carried out in the uncompressed evaluation setting and the other in the compressed evaluation setting.
</details>
<details>
<summary>摘要</summary>
不仅需要建立可准确地分类或检测假图像的计算模型，还需要确保这些计算模型是否具备公平性，以避免对某些社会群体造成伤害或导致严重的安全问题。探索律法算法的公平性是初步减轻这些偏见的初步步骤。由于视觉转换器在最新的图像分类任务中广泛使用，这种研究尝试探索基于转换器的图像律法算法中的偏见。通过建立偏见评估库，这种研究使用了一系列个体和对比偏见评估方法来分析偏见的gender、种族、情感和交叉领域。为了考虑图像压缩对模型偏见的影响，这种研究还分析了图像压缩对模型偏见的影响。因此，这种研究采用了两个阶段的评估设定，其中一个是未压缩评估设定，另一个是压缩评估设定。
</details></li>
</ul>
<hr>
<h2 id="On-the-use-of-Vision-Language-models-for-Visual-Sentiment-Analysis-a-study-on-CLIP"><a href="#On-the-use-of-Vision-Language-models-for-Visual-Sentiment-Analysis-a-study-on-CLIP" class="headerlink" title="On the use of Vision-Language models for Visual Sentiment Analysis: a study on CLIP"></a>On the use of Vision-Language models for Visual Sentiment Analysis: a study on CLIP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12062">http://arxiv.org/abs/2310.12062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cristinabustos16/clip-e">https://github.com/cristinabustos16/clip-e</a></li>
<li>paper_authors: Cristina Bustos, Carles Civit, Brian Du, Albert Sole-Ribalta, Agata Lapedriza</li>
<li>for: 这个研究旨在利用 CLIP 嵌入空间进行视觉情感分析。</li>
<li>methods: 作者使用了两种基于 CLIP 嵌入空间的建模方法，称为 CLIP-E，并在 WEBEmo 上进行了训练。</li>
<li>results: 研究结果表明，CLIP-E 方法在 WEBEmo 上的细致分类 Task 上表现出色，并且在其他视觉情感分析 benchmark 上也进行了较好的泛化。<details>
<summary>Abstract</summary>
This work presents a study on how to exploit the CLIP embedding space to perform Visual Sentiment Analysis. We experiment with two architectures built on top of the CLIP embedding space, which we denote by CLIP-E. We train the CLIP-E models with WEBEmo, the largest publicly available and manually labeled benchmark for Visual Sentiment Analysis, and perform two sets of experiments. First, we test on WEBEmo and compare the CLIP-E architectures with state-of-the-art (SOTA) models and with CLIP Zero-Shot. Second, we perform cross dataset evaluation, and test the CLIP-E architectures trained with WEBEmo on other Visual Sentiment Analysis benchmarks. Our results show that the CLIP-E approaches outperform SOTA models in WEBEmo fine grained categorization, and they also generalize better when tested on datasets that have not been seen during training. Interestingly, we observed that for the FI dataset, CLIP Zero-Shot produces better accuracies than SOTA models and CLIP-E trained on WEBEmo. These results motivate several questions that we discuss in this paper, such as how we should design new benchmarks and evaluate Visual Sentiment Analysis, and whether we should keep designing tailored Deep Learning models for Visual Sentiment Analysis or focus our efforts on better using the knowledge encoded in large vision-language models such as CLIP for this task.
</details>
<details>
<summary>摘要</summary>
Here is the Simplified Chinese translation of the text:这个研究报告介绍了如何利用CLIP嵌入空间进行视觉情感分析。我们实验了两个基于CLIP嵌入空间的CLIP-E模型，并与状态对比模型和CLIP零shot模型进行比较。我们的结果显示，CLIP-E模型在WEBEmo上的细化分类 task 中表现出色，并且在其他视觉情感分析 benchmark 上进行交叉验证时也表现更好。另外，我们发现在FI dataset上，CLIP零shot模型的表现更好于状态对比模型和CLIP-E模型。这些结果提出了一些问题，例如如何设计 benchmark 和评估视觉情感分析，以及是否应该继续设计特定的深度学习模型 для视觉情感分析，还是更好地利用大视语模型如CLIP中的知识来进行这个任务。
</details></li>
</ul>
<hr>
<h2 id="Robust-Class-Conditional-Distribution-Alignment-for-Partial-Domain-Adaptation"><a href="#Robust-Class-Conditional-Distribution-Alignment-for-Partial-Domain-Adaptation" class="headerlink" title="Robust Class-Conditional Distribution Alignment for Partial Domain Adaptation"></a>Robust Class-Conditional Distribution Alignment for Partial Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12060">http://arxiv.org/abs/2310.12060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandipan Choudhuri, Arunabha Sen</li>
<li>for: 这个研究目的是为了解决半领域适应设置中的私有来源类别样本的问题，这些类别样本可能会导致负转移和降低分类性能。</li>
<li>methods: 我们的提案方法是通过进一步探索其他类别的高维度特征，以derive更加精确和单一的分类分布。我们使用域 invariantly 的目标函数来优化类别分布，并设计一个有效的伪标签生成方法来提供有效的目标反馈。</li>
<li>results: 我们的实验结果和抽象分析显示，我们的提案模型在与 benchmark 相比之下，具有更高的表现。<details>
<summary>Abstract</summary>
Unwanted samples from private source categories in the learning objective of a partial domain adaptation setup can lead to negative transfer and reduce classification performance. Existing methods, such as re-weighting or aggregating target predictions, are vulnerable to this issue, especially during initial training stages, and do not adequately address class-level feature alignment. Our proposed approach seeks to overcome these limitations by delving deeper than just the first-order moments to derive distinct and compact categorical distributions. We employ objectives that optimize the intra and inter-class distributions in a domain-invariant fashion and design a robust pseudo-labeling for efficient target supervision. Our approach incorporates a complement entropy objective module to reduce classification uncertainty and flatten incorrect category predictions. The experimental findings and ablation analysis of the proposed modules demonstrate the superior performance of our proposed model compared to benchmarks.
</details>
<details>
<summary>摘要</summary>
不想要的样本从私有领域类别中的学习目标在半领域适应设置中可能导致负向传输和降低分类性能。现有方法，如重新权重或聚合目标预测，在初始训练阶段 especialmente vulnerable to this issue，并不能够有效地处理类别特征对齐。我们提议的方法旨在超越首频级别的一致性， derive distinct and compact categorical distributions。我们使用域无关的目标函数优化内类和间类分布，并设计了一种robust pseudo-labeling来提高目标监督效率。我们的方法还包括一个 complement entropy 目标模块，以减少分类uncertainty和平滑错误类别预测。实验结果和ablation分析表明我们的提议模型在比较中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Decision-based-Black-box-Attacks-on-Face-Forgery-Detection"><a href="#Exploring-Decision-based-Black-box-Attacks-on-Face-Forgery-Detection" class="headerlink" title="Exploring Decision-based Black-box Attacks on Face Forgery Detection"></a>Exploring Decision-based Black-box Attacks on Face Forgery Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12017">http://arxiv.org/abs/2310.12017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaoyu Chen, Bo Li, Kaixun Jiang, Shuang Wu, Shouhong Ding, Wenqiang Zhang</li>
<li>for: 防止face forgery detection的攻击，提高安全性和隐私性</li>
<li>methods: 利用多任务权重矩阵、频谱决策法和空间域约束来实现高效且高质量的攻击</li>
<li>results: 在FaceForensics++, CelebDF和工业API上实现了状态调用攻击性能，并且可以通过面Recognition进行攻击， exposed the security vulnerabilities of face forgery detectors.<details>
<summary>Abstract</summary>
Face forgery generation technologies generate vivid faces, which have raised public concerns about security and privacy. Many intelligent systems, such as electronic payment and identity verification, rely on face forgery detection. Although face forgery detection has successfully distinguished fake faces, recent studies have demonstrated that face forgery detectors are very vulnerable to adversarial examples. Meanwhile, existing attacks rely on network architectures or training datasets instead of the predicted labels, which leads to a gap in attacking deployed applications. To narrow this gap, we first explore the decision-based attacks on face forgery detection. However, applying existing decision-based attacks directly suffers from perturbation initialization failure and low image quality. First, we propose cross-task perturbation to handle initialization failures by utilizing the high correlation of face features on different tasks. Then, inspired by using frequency cues by face forgery detection, we propose the frequency decision-based attack. We add perturbations in the frequency domain and then constrain the visual quality in the spatial domain. Finally, extensive experiments demonstrate that our method achieves state-of-the-art attack performance on FaceForensics++, CelebDF, and industrial APIs, with high query efficiency and guaranteed image quality. Further, the fake faces by our method can pass face forgery detection and face recognition, which exposes the security problems of face forgery detectors.
</details>
<details>
<summary>摘要</summary>
“人脸伪造生成技术可以生成非常真实的人脸，但这也引起了公众对安全和隐私的担忧。许多智能系统，如电子支付和身份验证，均依赖于人脸伪造检测。虽然人脸伪造检测已经成功地分辨出假人脸，但最近的研究表明，人脸伪造检测器很容易受到敌意例采样的影响。此外，现有的攻击方法基于网络架构或训练数据集，而不是预测标签，导致攻击部署应用程序的差距。为了bridging这个差距，我们首先探索了人脸伪造检测的决策型攻击。然而，直接应用现有的决策型攻击方法会导致初始化失败和图像质量低下。为此，我们提出了跨任务杂化 perturbation，利用人脸特征之间的高相关性来处理初始化失败。然后，我们受到人脸伪造检测中使用频率规则的启发，我们提出了频率决策型攻击。我们在频率域添加杂化，然后在空间域强制实现视觉质量。最后，我们进行了广泛的实验，并证明我们的方法可以在FaceForensics++, CelebDF和工业API上 achieve state-of-the-art攻击性能，同时保证图像质量。此外，我们的假人脸可以通过人脸伪造检测和人脸识别，暴露了人脸伪造检测器的安全问题。”
</details></li>
</ul>
<hr>
<h2 id="DynamiCrafter-Animating-Open-domain-Images-with-Video-Diffusion-Priors"><a href="#DynamiCrafter-Animating-Open-domain-Images-with-Video-Diffusion-Priors" class="headerlink" title="DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors"></a>DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12190">http://arxiv.org/abs/2310.12190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ailab-cvc/videocrafter">https://github.com/ailab-cvc/videocrafter</a></li>
<li>paper_authors: Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, Ying Shan</li>
<li>for: 将静止图像转换成动画视频，提供更加有趣的视觉体验。</li>
<li>methods: 利用文本到视频扩散模型的动态PRIOR，将图像与生成过程相互协调，并通过将全图像传输给扩散模型来补充更精确的图像信息。</li>
<li>results: 生成的动画视频具有自然的运动和高准确性，与输入图像之间存在很好的对应关系。与现有竞争者进行比较，该方法显示出了remarkable的优势。<details>
<summary>Abstract</summary>
Enhancing a still image with motion offers more engaged visual experience. Traditional image animation techniques mainly focus on animating natural scenes with random dynamics, such as clouds and fluid, and thus limits their applicability to generic visual contents. To overcome this limitation, we explore the synthesis of dynamic content for open-domain images, converting them into animated videos. The key idea is to utilize the motion prior of text-to-video diffusion models by incorporating the image into the generative process as guidance. Given an image, we first project it into a text-aligned rich image embedding space using a learnable image encoding network, which facilitates the video model to digest the image content compatibly. However, some visual details still struggle to be preserved in the resulting videos. To supplement more precise image information, we further feed the full image to the diffusion model by concatenating it with the initial noises. Experimental results reveal that our proposed method produces visually convincing animated videos, exhibiting both natural motions and high fidelity to the input image. Comparative evaluation demonstrates the notable superiority of our approach over existing competitors. The source code will be released upon publication.
</details>
<details>
<summary>摘要</summary>
加强静止图像可提供更加参与性的视觉体验。传统的图像动画技术主要集中于动感自然场景，如云彩和液体，因此限制其应用范围。为超越这些限制，我们探索将动态内容合成到开放频谱图像上，将图像转换成动画视频。关键思想是利用文本到视频扩散模型的运动优先，将图像 integrate 到生成过程中作为指导。给定一个图像，我们首先将其投影到文本相对丰富的图像嵌入空间中，使用学习图像编码网络，以便视频模型可以快速吸收图像内容。然而，一些视觉细节仍然困难保留在生成的视频中。为了补充更加精细的图像信息，我们进一步将全图像feed 到扩散模型中，将其与初始噪音 concatenate。实验结果表明，我们提出的方法可以生成视觉吸引人的动画视频，具有自然的运动和高准确性。比较评估表明，我们的方法在与现有竞争者进行比较时具有显著的优势。源代码将在出版时释出。
</details></li>
</ul>
<hr>
<h2 id="Image-Super-resolution-Via-Latent-Diffusion-A-Sampling-space-Mixture-Of-Experts-And-Frequency-augmented-Decoder-Approach"><a href="#Image-Super-resolution-Via-Latent-Diffusion-A-Sampling-space-Mixture-Of-Experts-And-Frequency-augmented-Decoder-Approach" class="headerlink" title="Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of Experts And Frequency-augmented Decoder Approach"></a>Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of Experts And Frequency-augmented Decoder Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12004">http://arxiv.org/abs/2310.12004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/frequency_aug_vae_moesr">https://github.com/tencent-ailab/frequency_aug_vae_moesr</a></li>
<li>paper_authors: Feng Luo, Jinxi Xiang, Jun Zhang, Xiao Han, Wei Yang</li>
<li>for: 高效的图像超分辨（SR）技术，尤其是在大量计算成本下。</li>
<li>methods: 利用预训练的文本-图像模型增强扩散先前，并使用特征编码器将图像转换为减少空间的约束。</li>
<li>results: 在 largely explored 4x blind super-resolution benchmarks 上提高性能，并在大放大因子（8x）图像SR benchmarks 上进行扩展。<details>
<summary>Abstract</summary>
The recent use of diffusion prior, enhanced by pre-trained text-image models, has markedly elevated the performance of image super-resolution (SR). To alleviate the huge computational cost required by pixel-based diffusion SR, latent-based methods utilize a feature encoder to transform the image and then implement the SR image generation in a compact latent space. Nevertheless, there are two major issues that limit the performance of latent-based diffusion. First, the compression of latent space usually causes reconstruction distortion. Second, huge computational cost constrains the parameter scale of the diffusion model. To counteract these issues, we first propose a frequency compensation module that enhances the frequency components from latent space to pixel space. The reconstruction distortion (especially for high-frequency information) can be significantly decreased. Then, we propose to use Sample-Space Mixture of Experts (SS-MoE) to achieve more powerful latent-based SR, which steadily improves the capacity of the model without a significant increase in inference costs. These carefully crafted designs contribute to performance improvements in largely explored 4x blind super-resolution benchmarks and extend to large magnification factors, i.e., 8x image SR benchmarks. The code is available at https://github.com/amandaluof/moe_sr.
</details>
<details>
<summary>摘要</summary>
近期使用扩散优先，增强了基于文本图像模型的图像超解析（SR）的性能。为了降低像素级扩散SR的巨大计算成本， latent-based 方法使用一个特征编码器将图像转换为一个紧凑的 latent space，然后实现SR图像生成。然而，latent-based diffusion 存在两个主要问题，首先，压缩 latent space 通常会导致重建误差。其次，巨大计算成本限制了扩散模型的参数缩放。为了解决这些问题，我们首先提出了频率补偿模块，它可以增强 latent space 中的频率成分到像素空间中。这可以减少重建误差，特别是高频信息。然后，我们提出使用 Sample-Space Mixture of Experts（SS-MoE）来实现更强大的 latent-based SR，它可以不断提高模型的容量，而无需显著增加推理成本。这些精心设计的改进措施在 largely explored 4x blind super-resolution benchmarks 中得到了性能提升，并可以扩展到大折射因子，例如 8x 图像 SR benchmarks。代码可以在 GitHub 上找到：https://github.com/amandaluof/moe_sr。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Flow-Networks-in-Continual-Learning"><a href="#Bayesian-Flow-Networks-in-Continual-Learning" class="headerlink" title="Bayesian Flow Networks in Continual Learning"></a>Bayesian Flow Networks in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12001">http://arxiv.org/abs/2310.12001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateusz Pyla, Kamil Deja, Bartłomiej Twardowski, Tomasz Trzciński</li>
<li>for: 本研究旨在探讨 Bayesian Flow Networks (BFNs) 的潜在应用，以及其在非站ARY数据上的生成能力。</li>
<li>methods: 本研究使用 BFNs 来学习非站ARY数据，并通过实验证明其生成能力。</li>
<li>results: 实验结果表明，BFNs 可以成功地生成非站ARY数据，并且在不同的数据上保持高度的生成能力。<details>
<summary>Abstract</summary>
Bayesian Flow Networks (BFNs) has been recently proposed as one of the most promising direction to universal generative modelling, having ability to learn any of the data type. Their power comes from the expressiveness of neural networks and Bayesian inference which make them suitable in the context of continual learning. We delve into the mechanics behind BFNs and conduct the experiments to empirically verify the generative capabilities on non-stationary data.
</details>
<details>
<summary>摘要</summary>
bayesian flow networks (BFNs) 最近被提出为一种最有前途的通用生成模型，能够学习任何数据类型。它们的力量来自于神经网络的表达能力和权重推断，使其适用于连续学习场景。我们深入探究 BFNs 的机制，并通过实验验证它们在非站立数据上的生成能力。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multi-modal-Medical-Neurological-Image-Fusion-using-Wavelet-Pooled-Edge-Preserving-Autoencoder"><a href="#Multi-modal-Medical-Neurological-Image-Fusion-using-Wavelet-Pooled-Edge-Preserving-Autoencoder" class="headerlink" title="Multi-modal Medical Neurological Image Fusion using Wavelet Pooled Edge Preserving Autoencoder"></a>Multi-modal Medical Neurological Image Fusion using Wavelet Pooled Edge Preserving Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11910">http://arxiv.org/abs/2310.11910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manisha Das, Deep Gupta, Petia Radeva, Ashwini M Bakde</li>
<li>for: 该论文旨在提高多模态医疗影像的融合，以提高诊断信息的可见化和分析。</li>
<li>methods: 该论文提出了一种基于 dense autoencoder 网络的无监督融合模型，通过使用波峰分解来提高特征抽取，保留细节信息，并提高融合图像的视觉效果。</li>
<li>results: 实验表明，提出的方法可以提供较好的视觉和量化结果，比其他现有的融合方法更好。<details>
<summary>Abstract</summary>
Medical image fusion integrates the complementary diagnostic information of the source image modalities for improved visualization and analysis of underlying anomalies. Recently, deep learning-based models have excelled the conventional fusion methods by executing feature extraction, feature selection, and feature fusion tasks, simultaneously. However, most of the existing convolutional neural network (CNN) architectures use conventional pooling or strided convolutional strategies to downsample the feature maps. It causes the blurring or loss of important diagnostic information and edge details available in the source images and dilutes the efficacy of the feature extraction process. Therefore, this paper presents an end-to-end unsupervised fusion model for multimodal medical images based on an edge-preserving dense autoencoder network. In the proposed model, feature extraction is improved by using wavelet decomposition-based attention pooling of feature maps. This helps in preserving the fine edge detail information present in both the source images and enhances the visual perception of fused images. Further, the proposed model is trained on a variety of medical image pairs which helps in capturing the intensity distributions of the source images and preserves the diagnostic information effectively. Substantial experiments are conducted which demonstrate that the proposed method provides improved visual and quantitative results as compared to the other state-of-the-art fusion methods.
</details>
<details>
<summary>摘要</summary>
医学图像融合将多种图像模式的诊断信息融合在一起，以提高图像的可视化和分析下面的缺陷。现在，深度学习基本模型已经超越了传统的融合方法，通过同时执行特征提取、特征选择和特征融合任务。然而，大多数现有的卷积神经网络架构使用传统的下采样或步长卷积策略来减少特征图。这会导致图像中的细节信息和Edge detail丢失，从而降低特征提取的效果。因此，本文提出了一种无监督的末端融合模型，基于密集自适应网络。在提议的模型中，特征提取得到了提高，通过使用波峰分解基于注意力卷积特征图。这有助于保留图像中细节信息和Edge detail，并提高融合图像的可见性。此外，提议的模型在多种医学图像对的训练下得到了良好的效果，从而有效地捕捉图像的Intensity分布和保留诊断信息。实验结果表明，提议的方法与其他状态最佳融合方法相比，提供了改进的可见和量化结果。
</details></li>
</ul>
<hr>
<h2 id="A-New-Multimodal-Medical-Image-Fusion-based-on-Laplacian-Autoencoder-with-Channel-Attention"><a href="#A-New-Multimodal-Medical-Image-Fusion-based-on-Laplacian-Autoencoder-with-Channel-Attention" class="headerlink" title="A New Multimodal Medical Image Fusion based on Laplacian Autoencoder with Channel Attention"></a>A New Multimodal Medical Image Fusion based on Laplacian Autoencoder with Channel Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11896">http://arxiv.org/abs/2310.11896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Payal Wankhede, Manisha Das, Deep Gupta, Petia Radeva, Ashwini M Bakde</li>
<li>for: 帮助医疗专业人员在诊断病人疾病的 клиниче阶段和运行期进行更加准确和有效的诊断和手术准备。</li>
<li>methods: 基于深度学习（DL）模型，实现端到端的图像融合，以提高图像融合的稳定性和准确性。</li>
<li>results: 我们提出了一种新的多模态医疗图像融合模型，基于 интеграble Laplacian-Gaussian concatenation with attention pooling（LGCA），能够有效保留多个图像的补充信息和重要的组织结构。<details>
<summary>Abstract</summary>
Medical image fusion combines the complementary information of multimodal medical images to assist medical professionals in the clinical diagnosis of patients' disorders and provide guidance during preoperative and intra-operative procedures. Deep learning (DL) models have achieved end-to-end image fusion with highly robust and accurate fusion performance. However, most DL-based fusion models perform down-sampling on the input images to minimize the number of learnable parameters and computations. During this process, salient features of the source images become irretrievable leading to the loss of crucial diagnostic edge details and contrast of various brain tissues. In this paper, we propose a new multimodal medical image fusion model is proposed that is based on integrated Laplacian-Gaussian concatenation with attention pooling (LGCA). We prove that our model preserves effectively complementary information and important tissue structures.
</details>
<details>
<summary>摘要</summary>
医疗影像融合技术将多模态医疗影像的补充信息融合到一起，以帮助医生更好地诊断病人的疾病和提供操作过程中的导航。深度学习（DL）模型已经实现了端到端的图像融合，并且具有高度的稳定性和准确性。然而，大多数DL模型在输入图像下采样时会产生重要的特征的损失，导致诊断边缘细节和不同脑组织的对比度的损失。在这篇论文中，我们提出了一种基于集成勺板-高斯拼接（LGCA）的新型多模态医疗影像融合模型。我们证明了我们的模型能够有效地保留补充信息和重要的组织结构。
</details></li>
</ul>
<hr>
<h2 id="IRAD-Implicit-Representation-driven-Image-Resampling-against-Adversarial-Attacks"><a href="#IRAD-Implicit-Representation-driven-Image-Resampling-against-Adversarial-Attacks" class="headerlink" title="IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks"></a>IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11890">http://arxiv.org/abs/2310.11890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Cao, Tianlin Li, Xiaofeng Cao, Ivor Tsang, Yang Liu, Qing Guo</li>
<li>For: This paper proposes a novel approach to defending against adversarial attacks, specifically image resampling, which transforms a discrete image into a new one to alleviate the influence of adversarial perturbations while preserving essential semantic information.* Methods: The paper presents basic resampling methods that employ interpolation strategies and coordinate shifting magnitudes, as well as an improved approach called implicit representation-driven image resampling (IRAD) that constructs an implicit continuous representation of input images and automatically generates pixel-wise shifts for resampling.* Results: The paper demonstrates that the proposed method significantly enhances the adversarial robustness of diverse deep models against various attacks while maintaining high accuracy on clean images, and outperforms existing defense methods in terms of accuracy and computational efficiency.<details>
<summary>Abstract</summary>
We introduce a novel approach to counter adversarial attacks, namely, image resampling. Image resampling transforms a discrete image into a new one, simulating the process of scene recapturing or rerendering as specified by a geometrical transformation. The underlying rationale behind our idea is that image resampling can alleviate the influence of adversarial perturbations while preserving essential semantic information, thereby conferring an inherent advantage in defending against adversarial attacks. To validate this concept, we present a comprehensive study on leveraging image resampling to defend against adversarial attacks. We have developed basic resampling methods that employ interpolation strategies and coordinate shifting magnitudes. Our analysis reveals that these basic methods can partially mitigate adversarial attacks. However, they come with apparent limitations: the accuracy of clean images noticeably decreases, while the improvement in accuracy on adversarial examples is not substantial. We propose implicit representation-driven image resampling (IRAD) to overcome these limitations. First, we construct an implicit continuous representation that enables us to represent any input image within a continuous coordinate space. Second, we introduce SampleNet, which automatically generates pixel-wise shifts for resampling in response to different inputs. Furthermore, we can extend our approach to the state-of-the-art diffusion-based method, accelerating it with fewer time steps while preserving its defense capability. Extensive experiments demonstrate that our method significantly enhances the adversarial robustness of diverse deep models against various attacks while maintaining high accuracy on clean images.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的对抗针对攻击方法，即图像重采样。图像重采样将离散图像转换成一个新的图像，模拟了场景重新捕捉或重新渲染的过程，根据几何变换。我们的理念是，通过图像重采样，可以减轻针对攻击的影响，保留基本的semantic信息，从而提供防御针对攻击的自然优势。为验证这个概念，我们进行了对图像重采样 defend against adversarial attacks的全面研究。我们开发了基本的重采样方法，使用 interpolating strategies和坐标Shift的大小。我们的分析表明，这些基本方法可以部分减轻针对攻击，但是它们有显著的限制：clean图像的准确率明显下降，而针对攻击的改进率不substantial。我们提出了基于implicit continuous representation的图像重采样方法（IRAD），以超越这些限制。首先，我们构建了一个implicit continuous representation，允许我们将任何输入图像转换为一个连续坐标空间中的表示。其次，我们引入SampleNet，它自动生成了不同输入的像素偏移，以便重采样。此外，我们可以将我们的方法扩展到当前领域的先进液态方法，通过减少时间步骤而保持防御能力。广泛的实验表明，我们的方法可以对多种攻击进行针对攻击，保持高精度clean images。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-Image-Restoration-Networks-for-General-Backbone-Network-Design"><a href="#A-Comparative-Study-of-Image-Restoration-Networks-for-General-Backbone-Network-Design" class="headerlink" title="A Comparative Study of Image Restoration Networks for General Backbone Network Design"></a>A Comparative Study of Image Restoration Networks for General Backbone Network Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11881">http://arxiv.org/abs/2310.11881</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Andrew0613/X-Restormer">https://github.com/Andrew0613/X-Restormer</a></li>
<li>paper_authors: Xiangyu Chen, Zheyuan Li, Yuandong Pu, Yihao Liu, Jiantao Zhou, Yu Qiao, Chao Dong</li>
<li>for: 这个论文的目的是提出一种通用的图像修复网络，可以在多种图像修复任务中达到优秀的性能。</li>
<li>methods: 这个论文使用了五种代表性的图像修复网络，对五种经典的图像修复任务进行了比较研究。然后，提出了一种基于多任务函数需求的通用图像修复网络设计方法，并进行了广泛的实验验证。</li>
<li>results: 实验结果表明，新提出的通用图像修复网络X-Restormer在多种图像修复任务中具有优秀的任务通用性和高性能。<details>
<summary>Abstract</summary>
Despite the significant progress made by deep models in various image restoration tasks, existing image restoration networks still face challenges in terms of task generality. An intuitive manifestation is that networks which excel in certain tasks often fail to deliver satisfactory results in others. To illustrate this point, we select five representative image restoration networks and conduct a comparative study on five classic image restoration tasks. First, we provide a detailed explanation of the characteristics of different image restoration tasks and backbone networks. Following this, we present the benchmark results and analyze the reasons behind the performance disparity of different models across various tasks. Drawing from this comparative study, we propose that a general image restoration backbone network needs to meet the functional requirements of diverse tasks. Based on this principle, we design a new general image restoration backbone network, X-Restormer. Extensive experiments demonstrate that X-Restormer possesses good task generality and achieves state-of-the-art performance across a variety of tasks.
</details>
<details>
<summary>摘要</summary>
尽管深度模型在不同的图像恢复任务中做出了显著的进步，现有的图像恢复网络仍然面临任务总体性的挑战。我们选择了五种代表性的图像恢复网络，对五个经典的图像恢复任务进行比较研究。首先，我们介绍了不同图像恢复任务和后ION网络的特点。接着，我们公布了标准测试结果，并分析了不同模型在不同任务中的性能差异的原因。基于这次比较研究，我们提出了一个通用的图像恢复后ION网络需要满足多种任务的功能要求。根据这个原则，我们设计了一个新的通用图像恢复后ION网络——X-Restormer。广泛的实验表明，X-Restormer具有优秀的任务总体性和在多种任务中达到了 estado del arte 性能。
</details></li>
</ul>
<hr>
<h2 id="Fractional-Concepts-in-Neural-Networks-Enhancing-Activation-and-Loss-Functions"><a href="#Fractional-Concepts-in-Neural-Networks-Enhancing-Activation-and-Loss-Functions" class="headerlink" title="Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions"></a>Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11875">http://arxiv.org/abs/2310.11875</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/irafm-ai/frac_calculus_01">https://gitlab.com/irafm-ai/frac_calculus_01</a></li>
<li>paper_authors: Zahra Alijani, Vojtech Molek</li>
<li>for: 本研究旨在使用分数概念在神经网络中修改活动函数和损失函数，以提高神经网络的性能。</li>
<li>methods: 本研究使用了分数导数顺序作为额外参数，让神经网络中的neurons可以根据输入数据调整其活动函数，以更好地匹配输入数据并降低输出错误。</li>
<li>results: 研究表明，通过使用分数导数顺序，神经网络可以更好地适应输入数据，并减少输出错误。这可能可以提高神经网络的总性能。<details>
<summary>Abstract</summary>
The paper presents a method for using fractional concepts in a neural network to modify the activation and loss functions. The methodology allows the neural network to define and optimize its activation functions by determining the fractional derivative order of the training process as an additional hyperparameter. This will enable neurons in the network to adjust their activation functions to match input data better and reduce output errors, potentially improving the network's overall performance.
</details>
<details>
<summary>摘要</summary>
文章提出了一种使用分数概念在神经网络中修改活动函数和损失函数的方法。该方法ология让神经网络可以通过确定训练过程中分数导数顺序作为额外参数来定义和优化其活动函数。这将允许神经元在网络中调整其活动函数以更好地匹配输入数据，降低输出错误，并可能提高网络的总性能。
</details></li>
</ul>
<hr>
<h2 id="To-Generate-or-Not-Safety-Driven-Unlearned-Diffusion-Models-Are-Still-Easy-To-Generate-Unsafe-Images-…-For-Now"><a href="#To-Generate-or-Not-Safety-Driven-Unlearned-Diffusion-Models-Are-Still-Easy-To-Generate-Unsafe-Images-…-For-Now" class="headerlink" title="To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images … For Now"></a>To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images … For Now</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11868">http://arxiv.org/abs/2310.11868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optml-group/diffusion-mu-attack">https://github.com/optml-group/diffusion-mu-attack</a></li>
<li>paper_authors: Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, Sijia Liu<br>for:这篇论文主要关注于如何评估安全驱动的扩散模型（DMs）是否能够删除不想要的概念、风格和物品。methods:本研究使用了反击攻击（也称为反击提示）来评估不知道的安全驱动扩散模型（DMs）是否能够删除不想要的概念、风格和物品。我们开发了一个名为UnlearnDiff的新型反击学习方法，利用扩散模型的自然分类能力来快速生成反击提示，使其成为对于生成模型的攻击如同于对于图像分类攻击的程度。results:我们通过对五种常见的安全驱动扩散模型（DMs）进行了多项测试，以评估它们在删除不想要的概念、风格和物品方面的不知道性和效率。结果显示，UnlearnDiff 比预设的反击提示方法更有效率和高效。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/OPTML-Group/Diffusion-MU-Attack">https://github.com/OPTML-Group/Diffusion-MU-Attack</a> 获取。请注意，这篇论文可能会包含一些可能会导致不良影响的模型输出。<details>
<summary>Abstract</summary>
The recent advances in diffusion models (DMs) have revolutionized the generation of complex and diverse images. However, these models also introduce potential safety hazards, such as the production of harmful content and infringement of data copyrights. Although there have been efforts to create safety-driven unlearning methods to counteract these challenges, doubts remain about their capabilities. To bridge this uncertainty, we propose an evaluation framework built upon adversarial attacks (also referred to as adversarial prompts), in order to discern the trustworthiness of these safety-driven unlearned DMs. Specifically, our research explores the (worst-case) robustness of unlearned DMs in eradicating unwanted concepts, styles, and objects, assessed by the generation of adversarial prompts. We develop a novel adversarial learning approach called UnlearnDiff that leverages the inherent classification capabilities of DMs to streamline the generation of adversarial prompts, making it as simple for DMs as it is for image classification attacks. This technique streamlines the creation of adversarial prompts, making the process as intuitive for generative modeling as it is for image classification assaults. Through comprehensive benchmarking, we assess the unlearning robustness of five prevalent unlearned DMs across multiple tasks. Our results underscore the effectiveness and efficiency of UnlearnDiff when compared to state-of-the-art adversarial prompting methods. Codes are available at https://github.com/OPTML-Group/Diffusion-MU-Attack. WARNING: This paper contains model outputs that may be offensive in nature.
</details>
<details>
<summary>摘要</summary>
近期的扩散模型（DM）的进步已经革命化了复杂和多样的图像生成。然而，这些模型也可能导致安全隐患，如生成危险内容和数据权利侵犯。虽然有努力创建安全驱动的卸载方法来解决这些挑战，但是存在uncertainty。为了bridging这个uncertainty，我们提出了基于对抗攻击的评估框架，以评估安全驱动的卸载DM的可靠性。 Specifically, our research explores the（worst-case）Robustness of unlearned DMs in eliminating unwanted concepts, styles, and objects, assessed by the generation of adversarial prompts. We develop a novel adversarial learning approach called UnlearnDiff that leverages the inherent classification capabilities of DMs to streamline the generation of adversarial prompts, making it as simple for DMs as it is for image classification attacks. This technique streamlines the creation of adversarial prompts, making the process as intuitive for generative modeling as it is for image classification assaults. Through comprehensive benchmarking, we assess the unlearning robustness of five prevalent unlearned DMs across multiple tasks. Our results underscore the effectiveness and efficiency of UnlearnDiff when compared to state-of-the-art adversarial prompting methods. 代码可以在https://github.com/OPTML-Group/Diffusion-MU-Attack中找到。注意：本文可能包含有害内容的模型输出。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Fairness-of-Discriminative-Foundation-Models-in-Computer-Vision"><a href="#Evaluating-the-Fairness-of-Discriminative-Foundation-Models-in-Computer-Vision" class="headerlink" title="Evaluating the Fairness of Discriminative Foundation Models in Computer Vision"></a>Evaluating the Fairness of Discriminative Foundation Models in Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11867">http://arxiv.org/abs/2310.11867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junaid Ali, Matthaeus Kleindessner, Florian Wenzel, Kailash Budhathoki, Volkan Cevher, Chris Russell</li>
<li>for: 本研究旨在提出一种新的偏见评估体系，用于评估用于标注任务的推理基础模型，如对比语言预训练（CLIP）模型。</li>
<li>methods: 本研究使用了一系列的方法来mitigate偏见，包括OpenAI的CLIP和OpenCLIP模型，以及fair PCA等post-processing方法。</li>
<li>results: 研究发现，fair PCA方法在大多数任务中能够很好地减轻偏见，但不同的减轻方法在不同任务中的效果不同。因此，选择合适的减轻方法取决于具体的用 caso。<details>
<summary>Abstract</summary>
We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if the task concerns humans; (ii) how subjective the task is (i.e., how likely it is that people from a diverse range of backgrounds would agree on a labeling); and (iii) the intended purpose of the task and if fairness is better served by impartiality (i.e., making decisions independent of the protected attributes) or representation (i.e., making decisions to maximize diversity). Finally, we provide quantitative fairness evaluations for both binary-valued and multi-valued protected attributes over ten diverse datasets. We find that fair PCA, a post-processing method for fair representations, works very well for debiasing in most of the aforementioned tasks while incurring only minor loss of performance. However, different debiasing approaches vary in their effectiveness depending on the task. Hence, one should choose the debiasing approach depending on the specific use case.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的分类法用于评估推理基模型（如语言对比预训练CLIP）的偏见。然后，我们系统地评估了现有的偏见缓解方法，以我们的分类法为基础。具体来说，我们评估了OpenAI的CLIP和OpenCLIP模型，用于关键应用程序，如零shot分类、图像检索和图像描述。我们根据三个轴分类愿望行为：（i）如果任务关注人类；（ii）任务是主观的（即人们来自多样化背景的人们是否能够达成一致的标签）；以及（iii）任务的目的是否更好地服务于公平（即通过免除保护属性来做出决策，或者通过提高多样性来做出决策）。最后，我们为十个多样的数据集提供了量化的公平评估，发现了 fair PCA  POST 处理方法可以很好地减少偏见，但不同的减少方法在不同任务中的效果不同。因此，选择减少方法应该根据具体的用例。
</details></li>
</ul>
<hr>
<h2 id="VQ-NeRF-Neural-Reflectance-Decomposition-and-Editing-with-Vector-Quantization"><a href="#VQ-NeRF-Neural-Reflectance-Decomposition-and-Editing-with-Vector-Quantization" class="headerlink" title="VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization"></a>VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11864">http://arxiv.org/abs/2310.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongliang Zhong, Jingbo Zhang, Jing Liao</li>
<li>For: The paper proposes a novel neural network model called VQ-NeRF that enables discrete material editing in 3D scenes.* Methods: The model consists of two branches: a continuous branch that predicts decomposed materials, and a discrete branch that uses vector quantization to quantize continuous materials into individual ones. The model also employs a dropout-based VQ codeword ranking strategy to predict the number of materials in a scene.* Results: The proposed model demonstrates superior performance in material segmentation and editing, and is evaluated on both computer-generated and real-world scenes. Additionally, the model provides an interactive interface for material editing, making it more user-friendly.<details>
<summary>Abstract</summary>
We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy to predict the number of materials in a scene, which reduces redundancy in the material segmentation process. To improve usability, we also develop an interactive interface to further assist material editing. We evaluate our model on both computer-generated and real-world scenes, demonstrating its superior performance. To the best of our knowledge, our model is the first to enable discrete material editing in 3D scenes.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)我们提出VQ-NeRF模型，一个两个分支神经网络模型，它 integrate了Vector Quantization（VQ）来分解和编辑3D场景中的反射场景。传统的神经反射场景只使用连续表示来模型3D场景，尽管实际上物体通常由不同的材料组成。这种缺失可能导致材料分解过程中的噪声和复杂的材料编辑。为解决这些限制，我们的模型包括一个连续分支和一个精度分支。连续分支遵循传统的管道来预测分解的材料，而精度分支使用VQ机制来量化连续的材料为个体材料。通过归一化材料，我们的模型可以减少分解过程中的噪声并生成分解后的材料分 segmentation 图像。用户可以通过点击相应的分 segmentation 结果中的区域来选择特定的材料进行进一步的编辑。此外，我们还提出了基于Dropout的VQ代码字rankStrategy来预测场景中的材料数量，从而减少材料分 segmentation 过程中的重复性。为了提高可用性，我们还开发了一个交互式界面，以助于物料编辑。我们在计算机生成和实际场景中评估了我们的模型，并证明其在场景中的superior performance。到目前为止，我们的模型是第一个在3D场景中启用精度编辑的模型。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Generate-Parameters-of-ConvNets-for-Unseen-Image-Data"><a href="#Learning-to-Generate-Parameters-of-ConvNets-for-Unseen-Image-Data" class="headerlink" title="Learning to Generate Parameters of ConvNets for Unseen Image Data"></a>Learning to Generate Parameters of ConvNets for Unseen Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11862">http://arxiv.org/abs/2310.11862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiye Wang, Kaituo Feng, Changsheng Li, Ye Yuan, Guoren Wang</li>
<li>for: 提高 ConvNet 的训练效率和可扩展性。</li>
<li>methods: 提出了一种新的训练方法，将 ConvNet 的参数学习转化为预测任务，通过学习一个映射函数来直接预测网络参数。</li>
<li>results: 经过广泛的实验证明，提出的 PudNet 模型可以快速预测 ConvNet 的参数，并且可以在不同的数据集上保持比较高的性能。<details>
<summary>Abstract</summary>
Typical Convolutional Neural Networks (ConvNets) depend heavily on large amounts of image data and resort to an iterative optimization algorithm (e.g., SGD or Adam) to learn network parameters, which makes training very time- and resource-intensive. In this paper, we propose a new training paradigm and formulate the parameter learning of ConvNets into a prediction task: given a ConvNet architecture, we observe there exists correlations between image datasets and their corresponding optimal network parameters, and explore if we can learn a hyper-mapping between them to capture the relations, such that we can directly predict the parameters of the network for an image dataset never seen during the training phase. To do this, we put forward a new hypernetwork based model, called PudNet, which intends to learn a mapping between datasets and their corresponding network parameters, and then predicts parameters for unseen data with only a single forward propagation. Moreover, our model benefits from a series of adaptive hyper recurrent units sharing weights to capture the dependencies of parameters among different network layers. Extensive experiments demonstrate that our proposed method achieves good efficacy for unseen image datasets on two kinds of settings: Intra-dataset prediction and Inter-dataset prediction. Our PudNet can also well scale up to large-scale datasets, e.g., ImageNet-1K. It takes 8967 GPU seconds to train ResNet-18 on the ImageNet-1K using GC from scratch and obtain a top-5 accuracy of 44.65 %. However, our PudNet costs only 3.89 GPU seconds to predict the network parameters of ResNet-18 achieving comparable performance (44.92 %), more than 2,300 times faster than the traditional training paradigm.
</details>
<details>
<summary>摘要</summary>
传统的卷积神经网络（ConvNet）需要大量的图像数据和迭代优化算法（如SGD或Adam）来学习网络参数，这使得训练非常时间和资源浪费。在这篇论文中，我们提出了一种新的训练方法，将参数学习转换成预测任务：给定一个ConvNet架构，我们观察到图像集和其对应的优化网络参数之间存在相关性，并explore我们可以学习一个映射来捕捉这些关系，以便直接预测未经训练的数据集中的参数。为此，我们提出了一种新的权重共享循环神经网络模型，称为PudNet，它的目标是学习图像集和其对应的网络参数之间的映射，并在只需要一次前向传播的情况下预测参数。此外，我们的模型还具有一系列适应性的循环单元，这些单元共享权重来捕捉不同层的参数之间的依赖关系。我们的实验表明，我们的提出的方法可以在两种不同的设置下达到好的效果：内部预测和间部预测。此外，我们的PudNet还可以适应大规模数据集，例如ImageNet-1K。使用GC从零开始训练ResNet-18，需要8967个GPU秒，而我们的PudNet只需要3.89个GPU秒来预测ResNet-18的参数，并达到相同的性能（44.92%），比传统训练方法超过2,300倍快。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Transferable-Adversarial-Image-Examples-Attack-Categorization-Evaluation-Guidelines-and-New-Insights"><a href="#Revisiting-Transferable-Adversarial-Image-Examples-Attack-Categorization-Evaluation-Guidelines-and-New-Insights" class="headerlink" title="Revisiting Transferable Adversarial Image Examples: Attack Categorization, Evaluation Guidelines, and New Insights"></a>Revisiting Transferable Adversarial Image Examples: Attack Categorization, Evaluation Guidelines, and New Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11850">http://arxiv.org/abs/2310.11850</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhengyuzhao/transferattackeval">https://github.com/zhengyuzhao/transferattackeval</a></li>
<li>paper_authors: Zhengyu Zhao, Hanwei Zhang, Renjue Li, Ronan Sicre, Laurent Amsaleg, Michael Backes, Qi Li, Chao Shen</li>
<li>For: This paper aims to address the critical security concerns of transferable adversarial examples in real-world black-box attack scenarios by identifying two main problems in common evaluation practices and proposing new evaluation guidelines.* Methods: The paper proposes a novel attack categorization strategy and conducts systematic and fair intra-category analyses on transferability, as well as considering diverse imperceptibility metrics and finer-grained stealthiness characteristics from the perspective of attack traceback.* Results: The paper provides the first large-scale evaluation of transferable adversarial examples on ImageNet, involving 23 representative attacks against 9 representative defenses, and leads to new insights such as the superiority of an early attack method, the vulnerability of a state-of-the-art defense, and the negative correlation between stealthiness and transferability.<details>
<summary>Abstract</summary>
Transferable adversarial examples raise critical security concerns in real-world, black-box attack scenarios. However, in this work, we identify two main problems in common evaluation practices: (1) For attack transferability, lack of systematic, one-to-one attack comparison and fair hyperparameter settings. (2) For attack stealthiness, simply no comparisons. To address these problems, we establish new evaluation guidelines by (1) proposing a novel attack categorization strategy and conducting systematic and fair intra-category analyses on transferability, and (2) considering diverse imperceptibility metrics and finer-grained stealthiness characteristics from the perspective of attack traceback. To this end, we provide the first large-scale evaluation of transferable adversarial examples on ImageNet, involving 23 representative attacks against 9 representative defenses. Our evaluation leads to a number of new insights, including consensus-challenging ones: (1) Under a fair attack hyperparameter setting, one early attack method, DI, actually outperforms all the follow-up methods. (2) A state-of-the-art defense, DiffPure, actually gives a false sense of (white-box) security since it is indeed largely bypassed by our (black-box) transferable attacks. (3) Even when all attacks are bounded by the same $L_p$ norm, they lead to dramatically different stealthiness performance, which negatively correlates with their transferability performance. Overall, our work demonstrates that existing problematic evaluations have indeed caused misleading conclusions and missing points, and as a result, hindered the assessment of the actual progress in this field.
</details>
<details>
<summary>摘要</summary>
通过我们的研究，我们发现了两个主要问题在常见评估方法中：（1）在攻击传输性能方面缺乏系统性、一对一的攻击比较和公平的超参数设置。（2）在攻击隐蔽性方面缺乏对比。为了解决这些问题，我们建立了新的评估指南，包括提出了一种新的攻击分类策略和对 transferability 进行系统性和公平的内部分析。此外，我们还考虑了多种隐蔽性指标和更细化的攻击特征，从攻击跟踪的角度来评估隐蔽性。为了实现这一目标，我们对 ImageNet 进行了大规模的攻击传输性评估，包括 23 种代表性攻击和 9 种代表性防御。我们的评估导致了一些新的发现，包括：（1）在公平攻击超参数设置下，早期的攻击方法 DI 实际上超越了所有后续方法。（2）一种现状顶尖的防御 DiffPure 实际上给了一种 FALSE 的安全感，因为它实际上被我们的黑盒传输攻击大量绕过。（3）即使所有攻击都受限于同一个 $L_p$  нор，它们在隐蔽性方面表现出了截然不同的表现，这与传输性性能呈负相关。总之，我们的研究表明，现有的问题atic 评估已经导致了误导性的结论和缺失点，因此阻碍了这一领域的实际进步的评估。
</details></li>
</ul>
<hr>
<h2 id="Mesh-Represented-Recycle-Learning-for-3D-Hand-Pose-and-Mesh-Estimation"><a href="#Mesh-Represented-Recycle-Learning-for-3D-Hand-Pose-and-Mesh-Estimation" class="headerlink" title="Mesh Represented Recycle Learning for 3D Hand Pose and Mesh Estimation"></a>Mesh Represented Recycle Learning for 3D Hand Pose and Mesh Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12189">http://arxiv.org/abs/2310.12189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bosang Kim, Jonghyun Kim, Hyotae Lee, Lanying Jin, Jeongwon Ha, Dowoo Kwon, Jungpyo Kim, Wonhyeok Im, KyungMin Jin, Jungho Lee</li>
<li>for: 提高手势估计模型在真实世界场景中的Robustness。</li>
<li>methods: 提议一种mesh表示重复学习策略，通过在训练阶段使用自动生成的Synthetic手图来强化手势表示。</li>
<li>results: 提高手势估计和手图估计的性能，不需要在推理阶段添加计算负担。<details>
<summary>Abstract</summary>
In general, hand pose estimation aims to improve the robustness of model performance in the real-world scenes. However, it is difficult to enhance the robustness since existing datasets are obtained in restricted environments to annotate 3D information. Although neural networks quantitatively achieve a high estimation accuracy, unsatisfied results can be observed in visual quality. This discrepancy between quantitative results and their visual qualities remains an open issue in the hand pose representation. To this end, we propose a mesh represented recycle learning strategy for 3D hand pose and mesh estimation which reinforces synthesized hand mesh representation in a training phase. To be specific, a hand pose and mesh estimation model first predicts parametric 3D hand annotations (i.e., 3D keypoint positions and vertices for hand mesh) with real-world hand images in the training phase. Second, synthetic hand images are generated with self-estimated hand mesh representations. After that, the synthetic hand images are fed into the same model again. Thus, the proposed learning strategy simultaneously improves quantitative results and visual qualities by reinforcing synthetic mesh representation. To encourage consistency between original model output and its recycled one, we propose self-correlation loss which maximizes the accuracy and reliability of our learning strategy. Consequently, the model effectively conducts self-refinement on hand pose estimation by learning mesh representation from its own output. To demonstrate the effectiveness of our learning strategy, we provide extensive experiments on FreiHAND dataset. Notably, our learning strategy improves the performance on hand pose and mesh estimation without any extra computational burden during the inference.
</details>
<details>
<summary>摘要</summary>
通常，手姿估计的目的是提高模型在实际场景中的可靠性。然而，因为现有的数据集是在限制的环境中标注3D信息，因此增强可靠性是困难的。虽然神经网络在量化结果方面取得了高精度，但视觉质量不满足。这种在量化结果和视觉质量之间的差异是手姿表示的开放问题。为解决这个问题，我们提议一种基于循环学习的手姿和三角形估计策略，即在训练阶段使用自动生成的手姿三角形表示来强化模型的输出。具体来说，一个手姿和三角形估计模型首先预测实际世界中手图像的3D键点位置和三角形Vertex，然后使用自动生成的手姿三角形来生成 sintetic手图像。最后，这些 sintetic手图像被 feed 到同一个模型中，以便模型可以进行自我反复学习。因此，我们的学习策略同时提高了量化结果和视觉质量，通过强化自动生成的手姿三角形表示。为保证模型的输出与重新输入的一致性，我们提议一种自相关损失函数，该函数最大化了模型的准确性和可靠性。因此，模型可以通过自己的输出来进行自我反复学习，从而提高手姿估计的性能。为证明我们的学习策略的有效性，我们提供了大量的实验结果，并证明了我们的策略不会在推理阶段增加计算负担。
</details></li>
</ul>
<hr>
<h2 id="HB-net-Holistic-bursting-cell-cluster-integrated-network-for-occluded-multi-objects-recognition"><a href="#HB-net-Holistic-bursting-cell-cluster-integrated-network-for-occluded-multi-objects-recognition" class="headerlink" title="HB-net: Holistic bursting cell cluster integrated network for occluded multi-objects recognition"></a>HB-net: Holistic bursting cell cluster integrated network for occluded multi-objects recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11834">http://arxiv.org/abs/2310.11834</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/d-lab438/hb-net">https://github.com/d-lab438/hb-net</a></li>
<li>paper_authors: Xudong Gao, Xiao Guang Gao, Jia Rong, Xiaowei Chen, Xiang Liao, Jun Chen<br>for:  This paper aims to address the challenges of multi-label classification (MLC) in image recognition, specifically when objects within the visual field occlude one another.methods:  The paper introduces a pioneering integrated network framework called HB-net, built upon the foundation of Holistic Bursting (HB) cell clusters, to recognize multiple occluded objects within images. The framework incorporates various Bursting cell cluster structures and an evidence accumulation mechanism.results:  The models incorporating the HB framework exhibit a significant $2.98%$ enhancement in recognition accuracy compared to models without the HB framework ($1.0298$ times, $p&#x3D;0.0499$). Despite having only three convolutional layers and approximately $1&#x2F;30$ of the parameters, the models that combine the HB framework and EA mechanism achieve a comparable level of accuracy and resilience to ResNet50.Here’s the Chinese version:for: 这篇论文目标是解决图像认知中的多标签分类（MLC）挑战，特别是在视场中物体相互 occlude 时，同时识别 occluded 和 occluding 物体。methods: 这篇论文提出了一种先锋的集成网络框架，名为 HB-net，基于启发式强迫细胞（HB）群集，用于同时识别图像中的多个 occluded 对象。框架包括多种 Bursting 细胞群集结构和证据积累机制。results: 包含 HB 框架的模型比无 HB 框架的模型显著提高 $2.98%$ 的识别精度 ($1.0298$ 倍， $p&#x3D;0.0499$）。尽管 HB-net 模型只有三层 convolutional 层和大约 $1&#x2F;30$ 的参数，但是与 ResNet50 具有相同的精度和鲁棒性。<details>
<summary>Abstract</summary>
Within the realm of image recognition, a specific category of multi-label classification (MLC) challenges arises when objects within the visual field may occlude one another, demanding simultaneous identification of both occluded and occluding objects. Traditional convolutional neural networks (CNNs) can tackle these challenges; however, those models tend to be bulky and can only attain modest levels of accuracy. Leveraging insights from cutting-edge neural science research, specifically the Holistic Bursting (HB) cell, this paper introduces a pioneering integrated network framework named HB-net. Built upon the foundation of HB cell clusters, HB-net is designed to address the intricate task of simultaneously recognizing multiple occluded objects within images. Various Bursting cell cluster structures are introduced, complemented by an evidence accumulation mechanism. Testing is conducted on multiple datasets comprising digits and letters. The results demonstrate that models incorporating the HB framework exhibit a significant $2.98\%$ enhancement in recognition accuracy compared to models without the HB framework ($1.0298$ times, $p=0.0499$). Although in high-noise settings, standard CNNs exhibit slightly greater robustness when compared to HB-net models, the models that combine the HB framework and EA mechanism achieve a comparable level of accuracy and resilience to ResNet50, despite having only three convolutional layers and approximately $1/30$ of the parameters. The findings of this study offer valuable insights for improving computer vision algorithms. The essential code is provided at https://github.com/d-lab438/hb-net.git.
</details>
<details>
<summary>摘要</summary>
在图像识别领域中，特定类型的多标签分类（MLC）挑战在图像中可能存在对象干扰 Each other，需要同时识别干扰和干扰物体。传统的卷积神经网络（CNN）可以解决这些挑战，但这些模型往往很大，只能达到 moderate 级别的准确率。基于最新的神经科学研究，尤其是全局爆发（HB）细胞，这篇论文介绍了一种先进的集成网络框架，名为HB-网。HB-网基于HB细胞群集的基础上设计，用于同时识别图像中多个干扰物体。文中还提出了多种爆发细胞群结构，以及证据积累机制。测试结果表明，包含HB框架的模型与无HB框架模型相比，显著提高了识别精度($2.98\%$，$p=0.0499$）。虽然在高噪设置下，标准CNN模型在鲁棒性方面轻微优于HB-网模型，但HB-网模型与EA机制结合的模型可以与ResNet50模型具有相同的准确率和鲁棒性，即使只有三个卷积层和约$1/30$的参数。本研究发现的结论对计算机视觉算法提供了有价值的指导。代码可以在https://github.com/d-lab438/hb-net.git中找到。
</details></li>
</ul>
<hr>
<h2 id="ShapeGraFormer-GraFormer-Based-Network-for-Hand-Object-Reconstruction-from-a-Single-Depth-Map"><a href="#ShapeGraFormer-GraFormer-Based-Network-for-Hand-Object-Reconstruction-from-a-Single-Depth-Map" class="headerlink" title="ShapeGraFormer: GraFormer-Based Network for Hand-Object Reconstruction from a Single Depth Map"></a>ShapeGraFormer: GraFormer-Based Network for Hand-Object Reconstruction from a Single Depth Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11811">http://arxiv.org/abs/2310.11811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Tawfik Aboukhadra, Jameel Malik, Nadia Robertini, Ahmed Elhayek, Didier Stricker</li>
<li>for: 三维重建手征和物体交互是人工智能模拟人类行为的关键。大多数方法只重点处理孤立的手重建，忽略物体接触的物理和运动约束。一些方法可以生成更真实的结果，但它们偏好粗略的姿势估计或假设已知手和物体的形状。</li>
<li>methods: 我们提出的方法可以从单个深度图重构真实的3D手征和物体形状。与之前的方法不同，我们的体量基于的重构网络可以重构手征和物体的Vertex坐标。我们的管道还预测了手征和物体的VOXEL化形状，这与输入VOXEL化深度之间存在一对一的映射。此外，我们利用Recent GraFormer网络和位置嵌入来重构模板几何体。</li>
<li>results: 我们在HO-3D和DexYCB数据集上进行了广泛的评估，并证明了我们的方法在手重建方面的表现更高，并且能够生成更真实的物体形状。<details>
<summary>Abstract</summary>
3D reconstruction of hand-object manipulations is important for emulating human actions. Most methods dealing with challenging object manipulation scenarios, focus on hands reconstruction in isolation, ignoring physical and kinematic constraints due to object contact. Some approaches produce more realistic results by jointly reconstructing 3D hand-object interactions. However, they focus on coarse pose estimation or rely upon known hand and object shapes. We propose the first approach for realistic 3D hand-object shape and pose reconstruction from a single depth map. Unlike previous work, our voxel-based reconstruction network regresses the vertex coordinates of a hand and an object and reconstructs more realistic interaction. Our pipeline additionally predicts voxelized hand-object shapes, having a one-to-one mapping to the input voxelized depth. Thereafter, we exploit the graph nature of the hand and object shapes, by utilizing the recent GraFormer network with positional embedding to reconstruct shapes from template meshes. In addition, we show the impact of adding another GraFormer component that refines the reconstructed shapes based on the hand-object interactions and its ability to reconstruct more accurate object shapes. We perform an extensive evaluation on the HO-3D and DexYCB datasets and show that our method outperforms existing approaches in hand reconstruction and produces plausible reconstructions for the objects
</details>
<details>
<summary>摘要</summary>
三维重建手Object操作是重要的人工动作模拟领域。大多数方法在困难的物体操作场景中，都会忽略物体与手的物理和遥感约束。一些方法可以生成更加真实的结果，但是它们都是通过粗略的手形估计或者假设已知手形和物体形状来实现。我们提出了首个从单个深度图中真实重建3D手Object形状和姿势的方法。与前一些方法不同的是，我们的小节基于重建网络将手Object的Vertex坐标重建为手Object的3D形状。我们的管道还预测了手Object的小节形状，它们与输入深度图的小节形状一一对应。之后，我们利用手Object形状的图形结构，通过最近的GraFormer网络和位置嵌入来重建模板几何体。此外，我们还展示了在添加另一个GraFormer组件后，可以基于手Object交互来更加精准地重建物体形状，并且可以更加准确地重建物体。我们对HO-3D和DexYCB数据集进行了广泛的评估，并证明了我们的方法在手 reconstruction和物体重建方面的表现比现有方法更好。
</details></li>
</ul>
<hr>
<h2 id="Panoptic-Out-of-Distribution-Segmentation"><a href="#Panoptic-Out-of-Distribution-Segmentation" class="headerlink" title="Panoptic Out-of-Distribution Segmentation"></a>Panoptic Out-of-Distribution Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11797">http://arxiv.org/abs/2310.11797</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BastianSchnitzer/OODPanoptVidSeg">https://github.com/BastianSchnitzer/OODPanoptVidSeg</a></li>
<li>paper_authors: Rohit Mohan, Kiran Kumaraswamy, Juana Valeria Hurtado, Kürsat Petek, Abhinav Valada</li>
<li>For: 本研究旨在解决Scene Understanding中的Out-of-Distribution(OOD)对象问题，提高Scene Understanding的性能。* Methods: 本文提出了Panoptic Out-of Distribution Segmentation（PoDS）网络，包括一个共享背景、OODContextualModule、双对称解码器和任务特定头部。PoDS网络通过我们的准确性不符分配策略和数据增强策略来逐渐学习OOD对象，保持IN-distribution表现。* Results: 我们在Cityscapes和BDD100K两个benchmark上进行了广泛的评估，并证明了PoDS网络能够有效地解决OOD对象问题，并且大幅超越了基eline。我们还提供了数据集、代码和训练模型，并在<a target="_blank" rel="noopener" href="http://pods.cs.uni-freiburg.de上公开发布./">http://pods.cs.uni-freiburg.de上公开发布。</a><details>
<summary>Abstract</summary>
Deep learning has led to remarkable strides in scene understanding with panoptic segmentation emerging as a key holistic scene interpretation task. However, the performance of panoptic segmentation is severely impacted in the presence of out-of-distribution (OOD) objects i.e. categories of objects that deviate from the training distribution. To overcome this limitation, we propose Panoptic Out-of Distribution Segmentation for joint pixel-level semantic in-distribution and out-of-distribution classification with instance prediction. We extend two established panoptic segmentation benchmarks, Cityscapes and BDD100K, with out-of-distribution instance segmentation annotations, propose suitable evaluation metrics, and present multiple strong baselines. Importantly, we propose the novel PoDS architecture with a shared backbone, an OOD contextual module for learning global and local OOD object cues, and dual symmetrical decoders with task-specific heads that employ our alignment-mismatch strategy for better OOD generalization. Combined with our data augmentation strategy, this approach facilitates progressive learning of out-of-distribution objects while maintaining in-distribution performance. We perform extensive evaluations that demonstrate that our proposed PoDS network effectively addresses the main challenges and substantially outperforms the baselines. We make the dataset, code, and trained models publicly available at http://pods.cs.uni-freiburg.de.
</details>
<details>
<summary>摘要</summary>
深度学习已经导致场景理解方面做出了非常出色的进步，而涵盖全场景的场景解释任务——权重分割——也在不断提高。然而，权重分割性能在不同于训练数据分布的对象（Out-of-Distribution，OOD）上受到严重的限制。为了解决这个问题，我们提出了权重分割Out-of-Distribution Segmentation（PoDS），它可以同时进行 pixel-levelsemantic 内存分类和 OOD 分类，并且可以预测实例。我们在Cityscapes和BDD100K两个已有的权重分割 benchmark上添加了 OOD 实例分类注释，并提出了适当的评价指标。我们还提出了一种新的 PoDS 架构，它包括共享背景、OOD 上下文模块和两个对称的解码器，其中每个解码器都有任务特定的头，使用我们的偏移缺失策略来提高 OOD 通用性。通过我们的数据增强策略，这种方法可以逐步学习 OOD 对象，同时保持内存分类性能。我们进行了广泛的评估，结果表明，我们的提出的 PoDS 网络可以有效地解决主要挑战，并且明显超过基线。我们将数据集、代码和训练模型公开发布在http://pods.cs.uni-freiburg.de。
</details></li>
</ul>
<hr>
<h2 id="Progressive3D-Progressively-Local-Editing-for-Text-to-3D-Content-Creation-with-Complex-Semantic-Prompts"><a href="#Progressive3D-Progressively-Local-Editing-for-Text-to-3D-Content-Creation-with-Complex-Semantic-Prompts" class="headerlink" title="Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts"></a>Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11784">http://arxiv.org/abs/2310.11784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, Li Yuan</li>
<li>for: 这个研究旨在创建复杂提示中的正确3D内容，例如多个互动物品与不同属性绑定在一起。</li>
<li>methods: 我们提出了一个通用框架名为Progressive3D，它将生成过程分成多个本地进行进步的编辑步骤，以实现精确的3D内容生成。在每个编辑步骤中，我们使用用户定义的区域提示来限定内容变化的区域。</li>
<li>results: 我们的Progressive3D框架可以实现高精度的3D内容生成，并且可以应对不同的文本至3D方法驱动不同的3D表现。实验结果表明，我们的方法可以实现精确的3D内容生成，并且可以应对复杂的提示。<details>
<summary>Abstract</summary>
Recent text-to-3D generation methods achieve impressive 3D content creation capacity thanks to the advances in image diffusion models and optimizing strategies. However, current methods struggle to generate correct 3D content for a complex prompt in semantics, i.e., a prompt describing multiple interacted objects binding with different attributes. In this work, we propose a general framework named Progressive3D, which decomposes the entire generation into a series of locally progressive editing steps to create precise 3D content for complex prompts, and we constrain the content change to only occur in regions determined by user-defined region prompts in each editing step. Furthermore, we propose an overlapped semantic component suppression technique to encourage the optimization process to focus more on the semantic differences between prompts. Extensive experiments demonstrate that the proposed Progressive3D framework generates precise 3D content for prompts with complex semantics and is general for various text-to-3D methods driven by different 3D representations.
</details>
<details>
<summary>摘要</summary>
现代文本到3D生成方法已经实现了印象深刻的3D内容创造能力，归功于图像扩散模型和优化策略的进步。然而，当前方法在复杂的semantics prompt上难以生成正确的3D内容，即多个交互对象与不同属性绑定的提示。在这项工作中，我们提出了一个通用框架名为Progressive3D，它将整个生成分解为一系列的本地进度编辑步骤，以创造复杂提示的精确3D内容。此外，我们还提出了重叠 semantic component suppression技术，以便促进优化过程更加注重semantic differences between prompts。广泛的实验表明，我们提出的Progressive3D框架可以为复杂semantics prompt生成精确3D内容，并且可以适用于不同的文本到3D方法驱动的不同3D表示。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Consistency-Guided-Source-Free-Test-Time-Domain-Adaptation-Medical-Image-Segmentation"><a href="#Multi-Task-Consistency-Guided-Source-Free-Test-Time-Domain-Adaptation-Medical-Image-Segmentation" class="headerlink" title="Multi Task Consistency Guided Source-Free Test-Time Domain Adaptation Medical Image Segmentation"></a>Multi Task Consistency Guided Source-Free Test-Time Domain Adaptation Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11766">http://arxiv.org/abs/2310.11766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanyu Ye, Zhenxi Zhang, Wei Wei, Chunna Tian</li>
<li>for: 提高医疗图像分割模型对不同和未经见测试集的适应性，提高分割模型的普适性和稳定性。</li>
<li>methods: 利用多任务一致性引导的源自由测试时领域适应医疗图像分割方法，保证测试集边缘预测和对应输入的一致性。具体来说，我们引入了地方边缘一致性约束方法，探讨了组织区域分割和组织边缘Localization任务之间的关系。此外，我们提出了全局特征一致性约束，以增强同类特征的紧凑性。</li>
<li>results: 在 benchmark 肠图像分割任务上进行了广泛的实验，与源Domain模型直接预测相比，分割 dice 得分提高了6.27%和0.96%在RIM-ONE-r3和Drishti GS数据集上。此外，实验结果表明，我们提出的方法在与现有竞争性领域适应分割算法相比，表现出了良好的性能。<details>
<summary>Abstract</summary>
Source-free test-time adaptation for medical image segmentation aims to enhance the adaptability of segmentation models to diverse and previously unseen test sets of the target domain, which contributes to the generalizability and robustness of medical image segmentation models without access to the source domain. Ensuring consistency between target edges and paired inputs is crucial for test-time adaptation. To improve the performance of test-time domain adaptation, we propose a multi task consistency guided source-free test-time domain adaptation medical image segmentation method which ensures the consistency of the local boundary predictions and the global prototype representation. Specifically, we introduce a local boundary consistency constraint method that explores the relationship between tissue region segmentation and tissue boundary localization tasks. Additionally, we propose a global feature consistency constraint toto enhance the intra-class compactness. We conduct extensive experiments on the segmentation of benchmark fundus images. Compared to prediction directly by the source domain model, the segmentation Dice score is improved by 6.27\% and 0.96\% in RIM-ONE-r3 and Drishti GS datasets, respectively. Additionally, the results of experiments demonstrate that our proposed method outperforms existing competitive domain adaptation segmentation algorithms.
</details>
<details>
<summary>摘要</summary>
源无法测试适应技术是为医学影像分割模型提高适应性，使其在不同和未经见过的测试集上具有更高的普适性和可靠性，而无需访问源领域。保证测试领域边缘与对应的输入保持一致性是适时适应技术的关键。为了提高测试适应性的表现，我们提出了基于多任务一致性指导的源无法测试适应医学影像分割方法。这种方法通过 explore 肿瘤区域分割和肿瘤边缘定位任务之间的关系来保证本地边缘预测的一致性。此外，我们还提出了全局特征一致性约束，以提高内类紧凑度。我们对医学影像分割benchmark数据集进行了广泛的实验。与直接使用源领域模型预测相比，我们的提出方法可以提高分割 dice 分数6.27%和0.96%在RIM-ONE-r3和Drishti GS数据集中，分别。此外，实验结果还表明，我们的提出方法可以超越现有的竞争性领域适应分割算法。
</details></li>
</ul>
<hr>
<h2 id="Perceptual-Measurements-Distances-and-Metrics"><a href="#Perceptual-Measurements-Distances-and-Metrics" class="headerlink" title="Perceptual Measurements, Distances and Metrics"></a>Perceptual Measurements, Distances and Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11759">http://arxiv.org/abs/2310.11759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jonathanvacher/perceptual_metric">https://github.com/jonathanvacher/perceptual_metric</a></li>
<li>paper_authors: Jonathan Vacher, Pascal Mamassian</li>
<li>for: 本研究旨在探讨人类视觉中的感知过程，以及这种过程如何将外部物理变量转化成内部心理变量。</li>
<li>methods: 本研究使用了对比测试方法（difference scaling experiments）来探讨感知积分的准确性。</li>
<li>results: 研究发现，感知积分主要受到刺激力谱的影响，而不是其他Physical variables。此外，研究还发现了感知积分与生成模型下的感知准确性之间的关系。<details>
<summary>Abstract</summary>
Perception is often viewed as a process that transforms physical variables, external to an observer, into internal psychological variables. Such a process can be modeled by a function coined perceptual scale. The perceptual scale can be deduced from psychophysical measurements that consist in comparing the relative differences between stimuli (i.e. difference scaling experiments). However, this approach is often overlooked by the modeling and experimentation communities. Here, we demonstrate the value of measuring the perceptual scale of classical (spatial frequency, orientation) and less classical physical variables (interpolation between textures) by embedding it in recent probabilistic modeling of perception. First, we show that the assumption that an observer has an internal representation of univariate parameters such as spatial frequency or orientation while stimuli are high-dimensional does not lead to contradictory predictions when following the theoretical framework. Second, we show that the measured perceptual scale corresponds to the transduction function hypothesized in this framework. In particular, we demonstrate that it is related to the Fisher information of the generative model that underlies perception and we test the predictions given by the generative model of different stimuli in a set a of difference scaling experiments. Our main conclusion is that the perceptual scale is mostly driven by the stimulus power spectrum. Finally, we propose that this measure of perceptual scale is a way to push further the notion of perceptual distances by estimating the perceptual geometry of images i.e. the path between images instead of simply the distance between those.
</details>
<details>
<summary>摘要</summary>
感知通常被看作将外部物理变量转换成内部心理变量的过程。这种过程可以通过一个名为感知尺度的函数来模型。感知尺度可以通过心理物理测量（比如差异检测实验）来推算。然而，这一方法经常被模型和实验社区忽视。我们在这里示出了测量感知尺度的价值，并将其嵌入到了现代感知probabilistic模型中。首先，我们表明了假设观察者有内部表征一个参数，如空间频率或方向，而 stimulus 是高维的时，不会导致矛盾的预测。其次，我们示出了测量的感知尺度与假设的转化函数相关。具体来说，我们表明了它与生成模型下的感知中的 Fisher信息相关，并在一系列差异检测实验中测试了这些预测。我们的主要结论是，感知尺度主要受 stimulus 的能量спектrum影响。最后，我们提议这种感知尺度的测量是一种可以推进感知距离的方式，而不仅仅是简单地测量图像之间的距离。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalized-Face-Anti-Spoofing-with-Unknown-Attacks"><a href="#Domain-Generalized-Face-Anti-Spoofing-with-Unknown-Attacks" class="headerlink" title="Domain-Generalized Face Anti-Spoofing with Unknown Attacks"></a>Domain-Generalized Face Anti-Spoofing with Unknown Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11758">http://arxiv.org/abs/2310.11758</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ai-application-and-integration-lab/dgua_fas">https://github.com/ai-application-and-integration-lab/dgua_fas</a></li>
<li>paper_authors: Zong-Wei Hong, Yu-Chen Lin, Hsuan-Tung Liu, Yi-Ren Yeh, Chu-Song Chen</li>
<li>for: 面对骗降风险，提高防骗检测精度。</li>
<li>methods: 提出了一种基于Transformer的特征提取器和SyntheticUnknownAttackSampleGenerator（SUASG）网络，以便在不同频谱下进行防骗检测。</li>
<li>results: 实验结果表明，我们的方法在防骗检测领域中具有优秀的性能，可以同时处理知道和未知攻击。<details>
<summary>Abstract</summary>
Although face anti-spoofing (FAS) methods have achieved remarkable performance on specific domains or attack types, few studies have focused on the simultaneous presence of domain changes and unknown attacks, which is closer to real application scenarios. To handle domain-generalized unknown attacks, we introduce a new method, DGUA-FAS, which consists of a Transformer-based feature extractor and a synthetic unknown attack sample generator (SUASG). The SUASG network simulates unknown attack samples to assist the training of the feature extractor. Experimental results show that our method achieves superior performance on domain generalization FAS with known or unknown attacks.
</details>
<details>
<summary>摘要</summary>
translate into Simplified Chinese:尽管面部防伪（FAS）方法在特定领域或攻击类型上达到了很高的表现，但很少的研究集中着重于同时面临域名变化和未知攻击，这更接近实际应用场景。为了处理域名总则未知攻击，我们介绍了一种新的方法，DGUA-FAS，它包括一个基于Transformer的特征提取器和一个生成synthetic未知攻击样本网络（SUASG）。SUASG网络模拟未知攻击样本，以帮助特征提取器的训练。实验结果表明，我们的方法在域名总则FAS中具有优秀的表现，包括知道或未知攻击。
</details></li>
</ul>
<hr>
<h2 id="RGM-A-Robust-Generalist-Matching-Model"><a href="#RGM-A-Robust-Generalist-Matching-Model" class="headerlink" title="RGM: A Robust Generalist Matching Model"></a>RGM: A Robust Generalist Matching Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11755">http://arxiv.org/abs/2310.11755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aim-uofa/rgm">https://github.com/aim-uofa/rgm</a></li>
<li>paper_authors: Songyan Zhang, Xinyu Sun, Hao Chen, Bo Li, Chunhua Shen</li>
<li>for: 这个论文的目的是提出一种深度学习模型，用于 sparse 和 dense 匹配。</li>
<li>methods: 该模型使用了一种卷积 GRU 模块进行匹配精度的提高，以及一种额外的不确定性估计模块进行减少。</li>
<li>results: 该模型在零shot匹配和下游几何估计方面实现了Superior性能，与之前的方法相比具有大幅提升。<details>
<summary>Abstract</summary>
Finding corresponding pixels within a pair of images is a fundamental computer vision task with various applications. Due to the specific requirements of different tasks like optical flow estimation and local feature matching, previous works are primarily categorized into dense matching and sparse feature matching focusing on specialized architectures along with task-specific datasets, which may somewhat hinder the generalization performance of specialized models. In this paper, we propose a deep model for sparse and dense matching, termed RGM (Robust Generalist Matching). In particular, we elaborately design a cascaded GRU module for refinement by exploring the geometric similarity iteratively at multiple scales following an additional uncertainty estimation module for sparsification. To narrow the gap between synthetic training samples and real-world scenarios, we build a new, large-scale dataset with sparse correspondence ground truth by generating optical flow supervision with greater intervals. As such, we are able to mix up various dense and sparse matching datasets, significantly improving the training diversity. The generalization capacity of our proposed RGM is greatly improved by learning the matching and uncertainty estimation in a two-stage manner on the large, mixed data. Superior performance is achieved for zero-shot matching and downstream geometry estimation across multiple datasets, outperforming the previous methods by a large margin.
</details>
<details>
<summary>摘要</summary>
寻找图像对应像素是计算机视觉的基本任务，具有各种应用。由于不同任务的特殊需求，以前的工作主要分为紧密匹配和稀疏特征匹配，专注于特有的建筑和任务特定的数据集，这可能会有所限制特殊模型的总体性能。在这篇论文中，我们提出了一种深度模型，称为Robust Generalist Matching（稳健通用匹配），用于紧密和稀疏匹配。特别是，我们精心设计了一个嵌入式GRU模块，通过多个缩放级别的几何相似性进行迭代修养，并附加了一个额外的不确定估计模块以实现稀疏化。为了减少人工训练样本和实际场景之间的差距，我们构建了一个新的大规模数据集，该数据集包含稀疏匹配的真实参照数据，并通过生成更大的间隔来提供更多的流动推导。因此，我们能够将不同的紧密和稀疏匹配数据集混合在一起，大幅提高训练多样性。我们的提出的RGM模型在适应零次匹配和下游几何估计方面表现出色，与之前的方法相比，具有很大的提升。
</details></li>
</ul>
<hr>
<h2 id="BanglaAbuseMeme-A-Dataset-for-Bengali-Abusive-Meme-Classification"><a href="#BanglaAbuseMeme-A-Dataset-for-Bengali-Abusive-Meme-Classification" class="headerlink" title="BanglaAbuseMeme: A Dataset for Bengali Abusive Meme Classification"></a>BanglaAbuseMeme: A Dataset for Bengali Abusive Meme Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11748">http://arxiv.org/abs/2310.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mithun Das, Animesh Mukherjee</li>
<li>for: 本研究旨在提供一个 Bengali 攻击图文的基本 dataset，并透过该 dataset 进行 benchmarking，以建立一个有效的攻击图文识别模型。</li>
<li>methods: 本研究使用了多种基线模型来类别攻击图文，包括文本基线模型、图像基线模型和多Modal 基线模型。</li>
<li>results: 研究发现，使用多modal 信息（文本和图像）的模型可以超过单modal 模型的性能，并取得了70.51的macro F1 分数。 In addition, the study performed a qualitative error analysis of the misclassified memes for each of the best-performing models.<details>
<summary>Abstract</summary>
The dramatic increase in the use of social media platforms for information sharing has also fueled a steep growth in online abuse. A simple yet effective way of abusing individuals or communities is by creating memes, which often integrate an image with a short piece of text layered on top of it. Such harmful elements are in rampant use and are a threat to online safety. Hence it is necessary to develop efficient models to detect and flag abusive memes. The problem becomes more challenging in a low-resource setting (e.g., Bengali memes, i.e., images with Bengali text embedded on it) because of the absence of benchmark datasets on which AI models could be trained. In this paper we bridge this gap by building a Bengali meme dataset. To setup an effective benchmark we implement several baseline models for classifying abusive memes using this dataset. We observe that multimodal models that use both textual and visual information outperform unimodal models. Our best-performing model achieves a macro F1 score of 70.51. Finally, we perform a qualitative error analysis of the misclassified memes of the best-performing text-based, image-based and multimodal models.
</details>
<details>
<summary>摘要</summary>
“社交媒体平台上的信息共享量呈现出剧烈增长趋势，同时也导致了在线骚扰的减震。创建 memes 是一种简单 yet 高效的骚扰方式，通常将图片与短文字层次在上面。这些危险元素在普遍存在， threatening 在线安全。因此，需要开发高效的模型来检测和标识骚扰 memes。在低资源环境（如 Bengali memes，即图片上嵌入 Bengali 文本）下，问题更加挑战性，因为缺乏可用的标准 datasets 用于训练 AI 模型。本文填补这个空白，建立了 Bengali meme 数据集。我们实现了一些基线模型，用于使用这些数据集进行骚扰 memes 的分类。我们发现，使用文本和视觉信息的多模式模型，比单模式模型更高效。我们最佳表现的模型在 macro F1 分数上达到 70.51。最后，我们对最佳文本基于、图像基于和多模式模型的误分类照片进行质量错误分析。”
</details></li>
</ul>
<hr>
<h2 id="DBDNet-Partial-to-Partial-Point-Cloud-Registration-with-Dual-Branches-Decoupling"><a href="#DBDNet-Partial-to-Partial-Point-Cloud-Registration-with-Dual-Branches-Decoupling" class="headerlink" title="DBDNet:Partial-to-Partial Point Cloud Registration with Dual Branches Decoupling"></a>DBDNet:Partial-to-Partial Point Cloud Registration with Dual Branches Decoupling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11733">http://arxiv.org/abs/2310.11733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiqi Li, Jihua Zhu, Yifan Xie</li>
<li>for: 本研究的目的是提出一种高效的点云注册方法，用于解决实际中的半 overlap 注册问题。</li>
<li>methods: 我们提出了一种基于 dual branches 结构的注册方法，其中分别创建了两个个别的匹配矩阵，以消除对翻译和平移的相互干扰。在注册过程中，我们将 overlap 预测作为前置任务，并使用强大的注意力机制来准确预测点 wise 面积。此外，我们设计了一种多resolution 特征提取网络，以捕捉both local和global patters，从而提高 overlap 预测和注册模块的性能。</li>
<li>results: 我们在 both synthetic 和实际数据集上进行了实验，并证明了我们提出的方法的效果。<details>
<summary>Abstract</summary>
Point cloud registration plays a crucial role in various computer vision tasks, and usually demands the resolution of partial overlap registration in practice. Most existing methods perform a serial calculation of rotation and translation, while jointly predicting overlap during registration, this coupling tends to degenerate the registration performance. In this paper, we propose an effective registration method with dual branches decoupling for partial-to-partial registration, dubbed as DBDNet. Specifically, we introduce a dual branches structure to eliminate mutual interference error between rotation and translation by separately creating two individual correspondence matrices. For partial-to-partial registration, we consider overlap prediction as a preordering task before the registration procedure. Accordingly, we present an overlap predictor that benefits from explicit feature interaction, which is achieved by the powerful attention mechanism to accurately predict pointwise masks. Furthermore, we design a multi-resolution feature extraction network to capture both local and global patterns thus enhancing both overlap prediction and registration module. Experimental results on both synthetic and real datasets validate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
Point cloud registration 在多种计算机视觉任务中扮演着关键角色，通常需要解决部分重叠注册问题。现有大多数方法采用串行计算旋转和平移，同时预测重叠，这种对接往往导致注册性能下降。在本文中，我们提出了一种高效的注册方法，即DBDNet，用于解决部分到部分注册问题。具体来说，我们引入了双支结构，以消除旋转和平移之间的相互干扰错误。为部分到部分注册，我们认为重叠预测是注册前置任务，因此我们提出了一种具有显著注意力机制的重叠预测器，以准确预测点 wise 面积。此外，我们设计了一种多resolution 特征提取网络，以捕捉局部和全局特征，从而提高重叠预测和注册模块的性能。实验结果表明，我们提出的方法在真实数据上得到了较好的效果。
</details></li>
</ul>
<hr>
<h2 id="VST-Efficient-and-Stronger-Visual-Saliency-Transformer"><a href="#VST-Efficient-and-Stronger-Visual-Saliency-Transformer" class="headerlink" title="VST++: Efficient and Stronger Visual Saliency Transformer"></a>VST++: Efficient and Stronger Visual Saliency Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11725">http://arxiv.org/abs/2310.11725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nian Liu, Ziyang Luo, Ni Zhang, Junwei Han</li>
<li>for: 这篇论文是为了提高抽象对象检测（SOD）的能力而写的。</li>
<li>methods: 该论文使用了一种基于 transformer 的 sequence-to-sequence 方法，并尝试了一种新的 token 扩展方法来逐渐提高高精度的抽象图像。</li>
<li>results: 实验结果表明，该模型在RGB、RGB-D 和 RGB-T SOD 数据集上的表现比现有方法更好，同时减少了25%的计算成本。<details>
<summary>Abstract</summary>
While previous CNN-based models have exhibited promising results for salient object detection (SOD), their ability to explore global long-range dependencies is restricted. Our previous work, the Visual Saliency Transformer (VST), addressed this constraint from a transformer-based sequence-to-sequence perspective, to unify RGB and RGB-D SOD. In VST, we developed a multi-task transformer decoder that concurrently predicts saliency and boundary outcomes in a pure transformer architecture. Moreover, we introduced a novel token upsampling method called reverse T2T for predicting a high-resolution saliency map effortlessly within transformer-based structures. Building upon the VST model, we further propose an efficient and stronger VST version in this work, i.e. VST++. To mitigate the computational costs of the VST model, we propose a Select-Integrate Attention (SIA) module, partitioning foreground into fine-grained segments and aggregating background information into a single coarse-grained token. To incorporate 3D depth information with low cost, we design a novel depth position encoding method tailored for depth maps. Furthermore, we introduce a token-supervised prediction loss to provide straightforward guidance for the task-related tokens. We evaluate our VST++ model across various transformer-based backbones on RGB, RGB-D, and RGB-T SOD benchmark datasets. Experimental results show that our model outperforms existing methods while achieving a 25% reduction in computational costs without significant performance compromise. The demonstrated strong ability for generalization, enhanced performance, and heightened efficiency of our VST++ model highlight its potential.
</details>
<details>
<summary>摘要</summary>
前些 CNN 基本模型在焦点对象检测（SOD）方面已经展示了漫天的成果，但它们在探索全球长距离相互关联的能力受限。我们之前的工作，Visual Saliency Transformer（VST），在基于 transformer 序列到序列的视角下，解决了这一约束，并同时涵盖了 RGB 和 RGB-D SOD。在 VST 模型中，我们开发了一个多任务 transformer 解码器，同时预测焦点和边框结果。此外，我们还提出了一种新的 токенupsampling 方法，称为反向 T2T，可以在 transformer 结构中简单地预测高分辨率焦点地图。基于 VST 模型，我们在这个工作中进一步提出了更加高效和强大的 VST++ 模型。为了减少 VST 模型的计算成本，我们提出了一种 Select-Integrate Attention（SIA）模块，将前景分成细化的小区域，并将背景信息集中到一个高级别的 токен中。此外，我们还设计了一种适合 depth 图的深度位编码方法，以便在低成本下使用 depth 信息。此外，我们还引入了一种带有指导性的 токен预测损失，以便为任务相关的 токен提供直接的指导。我们在不同的 transformer 基础体系上测试了我们的 VST++ 模型，并对 RGB、RGB-D 和 RGB-T SOD 测试集进行了评估。实验结果表明，我们的模型在性能和效率两个方面都有所提高，而且可以在不同的任务上进行普适的应用。
</details></li>
</ul>
<hr>
<h2 id="Separating-Invisible-Sounds-Toward-Universal-Audiovisual-Scene-Aware-Sound-Separation"><a href="#Separating-Invisible-Sounds-Toward-Universal-Audiovisual-Scene-Aware-Sound-Separation" class="headerlink" title="Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware Sound Separation"></a>Separating Invisible Sounds Toward Universal Audiovisual Scene-Aware Sound Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11713">http://arxiv.org/abs/2310.11713</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyang Su, Ali Vosoughi, Shijian Deng, Yapeng Tian, Chenliang Xu</li>
<li>for: 这篇论文是为了解决音频视频场景中不可见的声音问题，现有方法困难处理这类声音。</li>
<li>methods: 该论文提出了一种新的”音频视频场景意识分离”（AVSA-Sep）框架，包括可见和无法见声音的semantic parser和场景 Informed分离器。</li>
<li>results: AVSA-Sep可以成功分离两类声音，并且通过共同训练和相互对齐提高效果。<details>
<summary>Abstract</summary>
The audio-visual sound separation field assumes visible sources in videos, but this excludes invisible sounds beyond the camera's view. Current methods struggle with such sounds lacking visible cues. This paper introduces a novel "Audio-Visual Scene-Aware Separation" (AVSA-Sep) framework. It includes a semantic parser for visible and invisible sounds and a separator for scene-informed separation. AVSA-Sep successfully separates both sound types, with joint training and cross-modal alignment enhancing effectiveness.
</details>
<details>
<summary>摘要</summary>
《听视频音频分离场景》假设视频中的音频来源可见，但这会排除无法被摄像头捕捉的声音。现有方法在处理这类声音时遇到困难。本文介绍一种新的“听视频场景化分离”（AVSA-Sep）框架。它包括可见和无法被见的声音 semantic parser 和场景 Informed 分离器。AVSA-Sep 成功地分离了这两类声音，并且在共同训练和跨Modal 对齐下提高了效果。
</details></li>
</ul>
<hr>
<h2 id="DPF-Nutrition-Food-Nutrition-Estimation-via-Depth-Prediction-and-Fusion"><a href="#DPF-Nutrition-Food-Nutrition-Estimation-via-Depth-Prediction-and-Fusion" class="headerlink" title="DPF-Nutrition: Food Nutrition Estimation via Depth Prediction and Fusion"></a>DPF-Nutrition: Food Nutrition Estimation via Depth Prediction and Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11702">http://arxiv.org/abs/2310.11702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuzhe Han, Qimin Cheng, Wenjin Wu, Ziyang Huang</li>
<li>for: 这个研究旨在提供一个基于深度学习的自动膳食估算方法，以便日常监控膳食摄取和促进饮食健康。</li>
<li>methods: 这个方法使用单目像估算，并引入深度预测模组以改善食物量估算的精度。此外，我们还设计了RGB-D融合模组，将单目像融合到预测的深度信息上，以提高膳食估算的表现。</li>
<li>results: 我们在Nutrition5k上进行了充分的实验，并证明了DPF-Nutrition的有效性和效率。<details>
<summary>Abstract</summary>
A reasonable and balanced diet is essential for maintaining good health. With the advancements in deep learning, automated nutrition estimation method based on food images offers a promising solution for monitoring daily nutritional intake and promoting dietary health. While monocular image-based nutrition estimation is convenient, efficient, and economical, the challenge of limited accuracy remains a significant concern. To tackle this issue, we proposed DPF-Nutrition, an end-to-end nutrition estimation method using monocular images. In DPF-Nutrition, we introduced a depth prediction module to generate depth maps, thereby improving the accuracy of food portion estimation. Additionally, we designed an RGB-D fusion module that combined monocular images with the predicted depth information, resulting in better performance for nutrition estimation. To the best of our knowledge, this was the pioneering effort that integrated depth prediction and RGB-D fusion techniques in food nutrition estimation. Comprehensive experiments performed on Nutrition5k evaluated the effectiveness and efficiency of DPF-Nutrition.
</details>
<details>
<summary>摘要</summary>
一个合理和均衡的饮食是保持良好健康的必需。随着深度学习技术的发展，基于食物图像自动评估nutrition的方法提供了一个有前途的解决方案，帮助监测每日营养摄入和促进饮食健康。虽然单目图像基于nutrition评估方法方便、高效、经济，但是准确性问题仍然是一个主要挑战。为解决这个问题，我们提出了DPF-Nutrition，一种基于单目图像的综合nutrition评估方法。在DPF-Nutrition中，我们引入了深度预测模块，以生成深度地图，从而提高食物分量估算的准确性。此外，我们设计了RGB-D融合模块，将单目图像与预测的深度信息结合在一起，使nutrition评估表现更佳。据我们所知，这是食物nutrition评估中首次将深度预测和RGB-D融合技术相结合的尝试。我们在Nutrition5k上进行了广泛的实验，证明DPF-Nutrition的效果和效率。
</details></li>
</ul>
<hr>
<h2 id="MOHO-Learning-Single-view-Hand-held-Object-Reconstruction-with-Multi-view-Occlusion-Aware-Supervision"><a href="#MOHO-Learning-Single-view-Hand-held-Object-Reconstruction-with-Multi-view-Occlusion-Aware-Supervision" class="headerlink" title="MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision"></a>MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11696">http://arxiv.org/abs/2310.11696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyangguang Zhang, Guanlong Jiao, Yan Di, Ziqin Huang, Gu Wang, Ruida Zhang, Bowen Fu, Federico Tombari, Xiangyang Ji</li>
<li>for: 实现单一影像中的手持物体重建，不需要3D真实模型的监控。</li>
<li>methods: 使用多视角导入的semantic特征和几何导入，以及一新的2D-3D手 occlusion 意识训练方案，解决手持物体自我遮蔽和手导入遮蔽问题。</li>
<li>results: 在HO3D和DexYCB dataset上进行了广泛的实验，证明了2D监控的MOHO在比较3D监控方法的情况下，获得了更高的成绩。<details>
<summary>Abstract</summary>
Previous works concerning single-view hand-held object reconstruction typically utilize supervision from 3D ground truth models, which are hard to collect in real world. In contrast, abundant videos depicting hand-object interactions can be accessed easily with low cost, although they only give partial object observations with complex occlusion. In this paper, we present MOHO to reconstruct hand-held object from a single image with multi-view supervision from hand-object videos, tackling two predominant challenges including object's self-occlusion and hand-induced occlusion. MOHO inputs semantic features indicating visible object parts and geometric embeddings provided by hand articulations as partial-to-full cues to resist object's self-occlusion, so as to recover full shape of the object. Meanwhile, a novel 2D-3D hand-occlusion-aware training scheme following the synthetic-to-real paradigm is proposed to release hand-induced occlusion. In the synthetic pre-training stage, 2D-3D hand-object correlations are constructed by supervising MOHO with rendered images to complete the hand-concealed regions of the object in both 2D and 3D space. Subsequently, MOHO is finetuned in real world by the mask-weighted volume rendering supervision adopting hand-object correlations obtained during pre-training. Extensive experiments on HO3D and DexYCB datasets demonstrate that 2D-supervised MOHO gains superior results against 3D-supervised methods by a large margin. Codes and key assets will be released soon.
</details>
<details>
<summary>摘要</summary>
previous works concerning single-view hand-held object reconstruction typically utilize supervision from 3D ground truth models, which are hard to collect in real world. In contrast, abundant videos depicting hand-object interactions can be accessed easily with low cost, although they only give partial object observations with complex occlusion. In this paper, we present MOHO to reconstruct hand-held object from a single image with multi-view supervision from hand-object videos, tackling two predominant challenges including object's self-occlusion and hand-induced occlusion. MOHO inputs semantic features indicating visible object parts and geometric embeddings provided by hand articulations as partial-to-full cues to resist object's self-occlusion, so as to recover full shape of the object. Meanwhile, a novel 2D-3D hand-occlusion-aware training scheme following the synthetic-to-real paradigm is proposed to release hand-induced occlusion. In the synthetic pre-training stage, 2D-3D hand-object correlations are constructed by supervising MOHO with rendered images to complete the hand-concealed regions of the object in both 2D and 3D space. Subsequently, MOHO is finetuned in real world by the mask-weighted volume rendering supervision adopting hand-object correlations obtained during pre-training. Extensive experiments on HO3D and DexYCB datasets demonstrate that 2D-supervised MOHO gains superior results against 3D-supervised methods by a large margin. Codes and key assets will be released soon.Here's the translation in Traditional Chinese:previous works concerning single-view hand-held object reconstruction typically utilize supervision from 3D ground truth models, which are hard to collect in real world. In contrast, abundant videos depicting hand-object interactions can be accessed easily with low cost, although they only give partial object observations with complex occlusion. In this paper, we present MOHO to reconstruct hand-held object from a single image with multi-view supervision from hand-object videos, tackling two predominant challenges including object's self-occlusion and hand-induced occlusion. MOHO inputs semantic features indicating visible object parts and geometric embeddings provided by hand articulations as partial-to-full cues to resist object's self-occlusion, so as to recover full shape of the object. Meanwhile, a novel 2D-3D hand-occlusion-aware training scheme following the synthetic-to-real paradigm is proposed to release hand-induced occlusion. In the synthetic pre-training stage, 2D-3D hand-object correlations are constructed by supervising MOHO with rendered images to complete the hand-concealed regions of the object in both 2D and 3D space. Subsequently, MOHO is finetuned in real world by the mask-weighted volume rendering supervision adopting hand-object correlations obtained during pre-training. Extensive experiments on HO3D and DexYCB datasets demonstrate that 2D-supervised MOHO gains superior results against 3D-supervised methods by a large margin. Codes and key assets will be released soon.
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-guided-Semantics-for-Zero-shot-Learning"><a href="#ChatGPT-guided-Semantics-for-Zero-shot-Learning" class="headerlink" title="ChatGPT-guided Semantics for Zero-shot Learning"></a>ChatGPT-guided Semantics for Zero-shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11657">http://arxiv.org/abs/2310.11657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fhshubho/cgs-zsl">https://github.com/fhshubho/cgs-zsl</a></li>
<li>paper_authors: Fahimul Hoque Shubho, Townim Faisal Chowdhury, Ali Cheraghian, Morteza Saberi, Nabeel Mohammed, Shafin Rahman</li>
<li>for: 提高零shot学习（ZSL）任务中类别semantic的质量，以便将知识从训练时seen类传递到未经训练的类。</li>
<li>methods: 使用ChatGPT大语言模型提供类名和描述，并使用word2vec模型将文本生成的word vector与类名 embeddings fusion。</li>
<li>results: 在多个2D图像（CUB和AwA）和3D点云（ModelNet10、ModelNet40和ScanObjectNN） dataset上证明了提高ZSL性能。<details>
<summary>Abstract</summary>
Zero-shot learning (ZSL) aims to classify objects that are not observed or seen during training. It relies on class semantic description to transfer knowledge from the seen classes to the unseen classes. Existing methods of obtaining class semantics include manual attributes or automatic word vectors from language models (like word2vec). We know attribute annotation is costly, whereas automatic word-vectors are relatively noisy. To address this problem, we explore how ChatGPT, a large language model, can enhance class semantics for ZSL tasks. ChatGPT can be a helpful source to obtain text descriptions for each class containing related attributes and semantics. We use the word2vec model to get a word vector using the texts from ChatGPT. Then, we enrich word vectors by combining the word embeddings from class names and descriptions generated by ChatGPT. More specifically, we leverage ChatGPT to provide extra supervision for the class description, eventually benefiting ZSL models. We evaluate our approach on various 2D image (CUB and AwA) and 3D point cloud (ModelNet10, ModelNet40, and ScanObjectNN) datasets and show that it improves ZSL performance. Our work contributes to the ZSL literature by applying ChatGPT for class semantics enhancement and proposing a novel word vector fusion method.
</details>
<details>
<summary>摘要</summary>
zero-shot learning (ZSL) targets classifying objects that are not observed or seen during training. It relies on class semantic descriptions to transfer knowledge from seen classes to unseen classes. Existing methods of obtaining class semantics include manual attributes or automatic word vectors from language models (like word2vec). We know attribute annotation is costly, whereas automatic word-vectors are relatively noisy. To address this problem, we explore how ChatGPT, a large language model, can enhance class semantics for ZSL tasks. ChatGPT can be a helpful source to obtain text descriptions for each class containing related attributes and semantics. We use the word2vec model to get a word vector using the texts from ChatGPT. Then, we enrich word vectors by combining the word embeddings from class names and descriptions generated by ChatGPT. More specifically, we leverage ChatGPT to provide extra supervision for the class description, eventually benefiting ZSL models. We evaluate our approach on various 2D image (CUB and AwA) and 3D point cloud (ModelNet10, ModelNet40, and ScanObjectNN) datasets and show that it improves ZSL performance. Our work contributes to the ZSL literature by applying ChatGPT for class semantics enhancement and proposing a novel word vector fusion method.Here's the text with Traditional Chinese characters:zero-shot learning (ZSL) 目标是类别未在训练过的物件。它依赖类别Semantic descriptions 来传递见到类别的知识到未见到的类别。现有的类别Semantic descriptions 取得方法包括手动字段或自动的字幕Vector  FROM language models (like word2vec)。我们知道字段标签是贵重的，而自动的字幕Vector 则相对较杂。为了解决这个问题，我们探索了 ChatGPT，一个大型语言模型，可以帮助提高 ZSL 模型的性能。ChatGPT 可以提供类别描述文本，每个类别都包含相关的属性和 semantics。我们使用 word2vec 模型从 ChatGPT 的文本中得到字幕Vector。然后，我们将字幕Vector 丰富化，通过结合类别名称和由 ChatGPT 生成的描述文本中的字幕。更 specifically，我们利用 ChatGPT 提供类别描述的额外监督，最终帮助 ZSL 模型提高性能。我们将我们的方法应用到多个 2D 图像 (CUB 和 AwA) 和 3D 点 cloud (ModelNet10, ModelNet40, 和 ScanObjectNN)  datasets 上，并证明它可以提高 ZSL 性能。我们的工作对 ZSL 文献中的应用 ChatGPT 进行类别增强和提出了一个新的字幕融合方法，对 ZSL 领域的发展具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="VKIE-The-Application-of-Key-Information-Extraction-on-Video-Text"><a href="#VKIE-The-Application-of-Key-Information-Extraction-on-Video-Text" class="headerlink" title="VKIE: The Application of Key Information Extraction on Video Text"></a>VKIE: The Application of Key Information Extraction on Video Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11650">http://arxiv.org/abs/2310.11650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyu An, Ye Liu, Haoyuan Peng, Di Yin</li>
<li>for: EXTRACTING HIERARCHICAL KEY INFORMATION FROM VISUAL TEXTS ON VIDEOS</li>
<li>methods: 使用 PipVKIE 和 UniVKIE 两种解决方案，其中 PipVKIE 采用连续阶段完成四个子任务，而 UniVKIE 则将所有子任务统一到一个 backingbone 上。两种方法均利用视觉、文本和坐标信息进行特征表示。</li>
<li>results: 在一个具体定义的数据集上进行了广泛的实验，结果表明我们的解决方案可以实现很高的性能和高效的推理速度。<details>
<summary>Abstract</summary>
Extracting structured information from videos is critical for numerous downstream applications in the industry. In this paper, we define a significant task of extracting hierarchical key information from visual texts on videos. To fulfill this task, we decouples it into four subtasks and introduce two implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially completes the four subtasks in continuous stages, while UniVKIE is improved by unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage multimodal information from vision, text, and coordinates for feature representation. Extensive experiments on one well-defined dataset demonstrate that our solutions can achieve remarkable performance and efficient inference speed. The code and dataset will be publicly available.
</details>
<details>
<summary>摘要</summary>
视频中EXTRACTING结构化信息是产业中的关键应用之一。本文定义了EXTRACTING视频上的层次关键信息这一重要任务。为了完成这个任务，我们将其分解成四个子任务，并提出了两种实现方案：PipVKIE和UniVKIE。PipVKIESequentially完成这四个子任务，而UniVKIE则是通过将所有子任务 integrate into one backbone来进行改进。两种方案均利用视觉、文本和坐标信息来 Representation的特征。我们在一个固定的数据集上进行了广泛的实验，结果表明我们的解决方案可以达到出色的性能和高效的推理速度。代码和数据集将公开。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Towards-Abdominal-3-D-Scene-Rendering-from-Laparoscopy-Surgical-Videos-using-NeRFs"><a href="#Towards-Abdominal-3-D-Scene-Rendering-from-Laparoscopy-Surgical-Videos-using-NeRFs" class="headerlink" title="Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos using NeRFs"></a>Towards Abdominal 3-D Scene Rendering from Laparoscopy Surgical Videos using NeRFs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11645">http://arxiv.org/abs/2310.11645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khoa Tuan Nguyen, Francesca Tozzi, Nikdokht Rashidian, Wouter Willaert, Joris Vankerschaver, Wesley De Neve</li>
<li>for: 本研究旨在使用NeRF技术将 Laparoscopy 视频转化为三维场景，以便更好地探索腹部结构。</li>
<li>methods: 本研究使用NeRF技术对 Laparoscopy 视频进行处理，并通过Synthesize新视图来探索腹部结构。</li>
<li>results: 实验结果表明，NeRF技术可以有效地将 Laparoscopy 视频转化为三维场景，但是该方法还需要进一步的研究以解决一些挑战。<details>
<summary>Abstract</summary>
Given that a conventional laparoscope only provides a two-dimensional (2-D) view, the detection and diagnosis of medical ailments can be challenging. To overcome the visual constraints associated with laparoscopy, the use of laparoscopic images and videos to reconstruct the three-dimensional (3-D) anatomical structure of the abdomen has proven to be a promising approach. Neural Radiance Fields (NeRFs) have recently gained attention thanks to their ability to generate photorealistic images from a 3-D static scene, thus facilitating a more comprehensive exploration of the abdomen through the synthesis of new views. This distinguishes NeRFs from alternative methods such as Simultaneous Localization and Mapping (SLAM) and depth estimation. In this paper, we present a comprehensive examination of NeRFs in the context of laparoscopy surgical videos, with the goal of rendering abdominal scenes in 3-D. Although our experimental results are promising, the proposed approach encounters substantial challenges, which require further exploration in future research.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: conventinal laparoscope 只提供二维ensional (2-D) 视图， Medical 疾病检测和诊断可以很困难。为了突破 Laparoscopy 的视觉限制，使用 Laparoscopic 图像和视频来重建 Abdomen 的三维ensional (3-D) 解剖结构已经证明是一种有前途的方法。Neural Radiance Fields (NeRFs) 最近受到了关注，因为它们可以从三维ensional (3-D) 静止场景中生成高品质图像，从而为 Abdomen 的探索提供更多的可能性。这与 Simultaneous Localization and Mapping (SLAM) 和深度估计方法不同。在这篇论文中，我们对 NeRFs 在 Laparoscopic 手术视频中的应用进行了全面的检视，目标是将 Abdomen 场景rendered 为三维ensional (3-D)。虽然我们的实验结果很有前途，但提出的方法遇到了一些挑战，需要未来的研究来解决。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.CV_2023_10_18/" data-id="clpxp6c2300loee8813aqgeor" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.AI_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T12:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.AI_2023_10_18/">cs.AI - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Learning-to-Solve-Climate-Sensor-Placement-Problems-with-a-Transformer"><a href="#Learning-to-Solve-Climate-Sensor-Placement-Problems-with-a-Transformer" class="headerlink" title="Learning to Solve Climate Sensor Placement Problems with a Transformer"></a>Learning to Solve Climate Sensor Placement Problems with a Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12387">http://arxiv.org/abs/2310.12387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Wang, Victoria Huang, Gang Chen, Hui Ma, Bryce Chen, Jochen Schmidt</li>
<li>for: 该论文目的是提出一种基于深度学习的感知器布局方法，用于解决环境监测和灾害管理中的感知器布局问题。</li>
<li>methods: 该方法使用深度学习方法来自动生成优化策略，以解决环境监测和灾害管理中的感知器布局问题。</li>
<li>results: 对比多种现有方法，该方法能够生成高质量的解决方案，并且比现有方法更有效率。<details>
<summary>Abstract</summary>
The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promising direction for applying advanced DL and RL techniques to challenging climate sensor placement problems.
</details>
<details>
<summary>摘要</summary>
“环境监控和灾害管理中的仪器位置最佳化是一个NP困难的问题。传统方法包括精确、近似或规律方法，但这些方法受到专家智慧和经验的限制。深度学习（DL）已经成为一种可能的方法，用于生成自动生成规律算法。在这篇论文中，我们提出了一种新的仪器位置方法，利用深度强化学习（RL）方法学习改善规律。我们的方法利用了RL的形式来学习改善规律，驱动actor-critic算法来训练政策网。我们对多个现有方法进行了严详的实验，展示了我们的提案方法的有效性和优势。我们的工作呈现了应用进步的DL和RL技术解决气候仪器位置问题的可能性。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Online-Learning-and-Planning-in-Cognitive-Hierarchies"><a href="#Online-Learning-and-Planning-in-Cognitive-Hierarchies" class="headerlink" title="Online Learning and Planning in Cognitive Hierarchies"></a>Online Learning and Planning in Cognitive Hierarchies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12386">http://arxiv.org/abs/2310.12386</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Bernhard Hengst, Maurice Pagnucco, David Rajaratnam, Claude Sammut, Michael Thielscher</li>
<li>for: 本研究旨在探讨复杂 роботи变得需要多种机器人和人工智能技术的集成，以实现全球性和行为的协调。</li>
<li>methods: 本研究使用形式化框架来模型机器人系统的复杂 интеGRATION和有效的决策过程，从符号规划到在线学习策略和过渡系统。</li>
<li>results: 研究人员通过扩展Clark et al.（2016）的形式化框架，实现了复杂机器人系统的可靠和有效的 интеGRATION和决策过程。此外，新的框架还允许更加灵活地模型不同的决策组件之间的交互。<details>
<summary>Abstract</summary>
Complex robot behaviour typically requires the integration of multiple robotic and Artificial Intelligence (AI) techniques and components. Integrating such disparate components into a coherent system, while also ensuring global properties and behaviours, is a significant challenge for cognitive robotics. Using a formal framework to model the interactions between components can be an important step in dealing with this challenge. In this paper we extend an existing formal framework [Clark et al., 2016] to model complex integrated reasoning behaviours of robotic systems; from symbolic planning through to online learning of policies and transition systems. Furthermore the new framework allows for a more flexible modelling of the interactions between different reasoning components.
</details>
<details>
<summary>摘要</summary>
通常需要结合多种机器人和人工智能（AI）技术和组件来实现复杂的机器人行为。将这些不同的组件集成成一个一致的系统，并确保全球性和行为，是认知机器人的主要挑战。使用形式化框架来模型组件之间的交互可以是解决这个挑战的重要步骤。在这篇论文中，我们将对 Clark et al.（2016）的现有正式框架进行扩展，以模型机器人系统的复杂集成推理行为，从 симвоlic 规划到在线学习策略和转移系统。此外，新的框架还允许更加灵活地模型不同推理组件之间的交互。
</details></li>
</ul>
<hr>
<h2 id="Solving-Hard-Analogy-Questions-with-Relation-Embedding-Chains"><a href="#Solving-Hard-Analogy-Questions-with-Relation-Embedding-Chains" class="headerlink" title="Solving Hard Analogy Questions with Relation Embedding Chains"></a>Solving Hard Analogy Questions with Relation Embedding Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12379">http://arxiv.org/abs/2310.12379</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/niteshroyal/solvinghardanalogyquestions">https://github.com/niteshroyal/solvinghardanalogyquestions</a></li>
<li>paper_authors: Nitesh Kumar, Steven Schockaert</li>
<li>for: 本研究的目的是将概念之间的关系模型为路径，同时具有relation embedding的特性。</li>
<li>methods: 本研究使用了知识 graphs（KGs）如ConceptNet，并模型了两个概念之间的关系为一组路径。然而，KGs具有固定的关系类型，并且容易受到噪音和损害。本研究还使用了 fine-tuned语言模型来提取关系嵌入，但这并不适用于间接相关的词语和结构化领域知识。</li>
<li>results: 本研究提出了一种将路径和关系嵌入结合的方法，并通过实验表明其可以解决困难的相似问题。<details>
<summary>Abstract</summary>
Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions.
</details>
<details>
<summary>摘要</summary>
模型两个概念之间的关系是lexical semantics中的一个中心话题。一种常见的策略是通过知识图(KG)such as ConceptNet，并将两个概念之间的关系表示为一组路径。然而，KGs是有限的，并且经常受到干扰和噪声的影响。另一种策略是通过精心调整的自然语言模型提取关系嵌入。然而，这并不适用于间接相关的词语，而且不能轻松地包含结构化领域知识。在这篇论文中，我们决心将这两种方法结合在一起。我们模型关系为路径，并将路径上的边与关系嵌入相关联。我们首先 identific suitable intermediate words，然后选择这些words，以便可以获取有用的关系嵌入。我们的提议的表示方法在解决困难的类比问题上表现出色。
</details></li>
</ul>
<hr>
<h2 id="ClusT3-Information-Invariant-Test-Time-Training"><a href="#ClusT3-Information-Invariant-Test-Time-Training" class="headerlink" title="ClusT3: Information Invariant Test-Time Training"></a>ClusT3: Information Invariant Test-Time Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12345">http://arxiv.org/abs/2310.12345</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dosowiechi/clust3">https://github.com/dosowiechi/clust3</a></li>
<li>paper_authors: Gustavo A. Vargas Hakim, David Osowiechi, Mehrdad Noori, Milad Cheraghalikhani, Ismail Ben Ayed, Christian Desrosiers</li>
<li>for: 提高深度学习模型对不同预测环境的鲁棒性</li>
<li>methods: 提出了一种基于维度信息最大化的无监督测试时培养技术，通过同时在训练时进行多尺度特征图和整数 latent representation 的匹配，实现在测试时使用自动生成的proxy任务来适应不同预测环境。</li>
<li>results: 实验结果表明，该技术可以在不同的测试时适应 benchmark 上达到竞争力的分类性能。<details>
<summary>Abstract</summary>
Deep Learning models have shown remarkable performance in a broad range of vision tasks. However, they are often vulnerable against domain shifts at test-time. Test-time training (TTT) methods have been developed in an attempt to mitigate these vulnerabilities, where a secondary task is solved at training time simultaneously with the main task, to be later used as an self-supervised proxy task at test-time. In this work, we propose a novel unsupervised TTT technique based on the maximization of Mutual Information between multi-scale feature maps and a discrete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Experimental results demonstrate competitive classification performance on different popular test-time adaptation benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Eliminating-Reasoning-via-Inferring-with-Planning-A-New-Framework-to-Guide-LLMs’-Non-linear-Thinking"><a href="#Eliminating-Reasoning-via-Inferring-with-Planning-A-New-Framework-to-Guide-LLMs’-Non-linear-Thinking" class="headerlink" title="Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs’ Non-linear Thinking"></a>Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs’ Non-linear Thinking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12342">http://arxiv.org/abs/2310.12342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang, Zi Lin, Simeng Han, Jingbo Shang<br>for: 这研究旨在强化大语言模型（LLM）的高级逻辑能力，通过模拟人类线性思维和逻辑的混合。methods: 这研究提出了新的提示方法，即排除逻辑提示（IEP），它将排除逻辑和推理结合起来，以便LLM可以更好地模拟人类的非线性思维。results: 研究发现，IEP可以在多种任务上consistently outperform CoT，并且可以和CoT结合使用，以提高LLM的表现。此外，研究还引入了新的benchmark，即MENTAL-ABILITY REASONING BENCHMARK（MARB），以评估LLM的逻辑和语言理解能力。<details>
<summary>Abstract</summary>
Chain-of-Thought(CoT) prompting and its variants explore equipping large language models (LLMs) with high-level reasoning abilities by emulating human-like linear cognition and logic. However, the human mind is complicated and mixed with both linear and nonlinear thinking. In this work, we propose \textbf{I}nferential \textbf{E}xclusion \textbf{P}rompting (IEP), a novel prompting that combines the principles of elimination and inference in order to guide LLMs to think non-linearly. IEP guides LLMs to plan and then utilize Natural Language Inference (NLI) to deduce each possible solution's entailment relation with context, commonsense, or facts, therefore yielding a broader perspective by thinking back for inferring. This forward planning and backward eliminating process allows IEP to better simulate the complex human thinking processes compared to other CoT-based methods, which only reflect linear cognitive processes. We conducted a series of empirical studies and have corroborated that IEP consistently outperforms CoT across various tasks. Additionally, we observe that integrating IEP and CoT further improves the LLMs' performance on certain tasks, highlighting the necessity of equipping LLMs with mixed logic processes. Moreover, to better evaluate comprehensive features inherent in human logic, we introduce \textbf{M}ental-\textbf{A}bility \textbf{R}easoning \textbf{B}enchmark (MARB). The benchmark comprises six novel subtasks with a total of 9,115 questions, among which 1,685 are developed with hand-crafted rationale references. We believe both \textsc{IEP} and \textsc{MARB} can serve as a promising direction for unveiling LLMs' logic and verbal reasoning abilities and drive further advancements. \textsc{MARB} will be available at ~\texttt{anonymity link} soon.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought（CoT）提示和其变种探索将大型语言模型（LLM）具备高级思维能力，通过模拟人类线性认知和逻辑。然而，人类思维是复杂的，混合了线性和非线性思维。在这项工作中，我们提出了《排除并推理》（IEP）提示，它结合排除和推理的原理，以引导 LLM 进行非线性思维。IEP 使 LLM 可以规划，然后通过自然语言推理（NLI）来推理每个可能解的上下文、通用智慧和事实的关系，从而获得更广泛的视野。这种前置规划和后置排除过程使 IEP 更能模拟人类思维过程，相比其他 CoT 基于方法。我们进行了一系列实验研究，并证明 IEP 在多种任务上表现出色。此外，我们发现将 IEP 和 CoT 集成可以进一步提高 LLMS 的表现，强调了训练 LLMs 的混合逻辑过程的必要性。此外，为了更好地评估人类逻辑的全面特征，我们引入了《MENTAL-ABILITY REASONING BENCHMARK》（MARB）。 MARB 包括六个新的任务，共计 9,115 个问题，其中 1,685 个问题采用了手动制作的 rational references。我们认为 IEP 和 MARB 都可以成为探索 LLMs 逻辑和语言逻辑能力的有希望的方向，并驱动进一步的进步。MARB 将在 ~\texttt{anonymity link} 上公开。
</details></li>
</ul>
<hr>
<h2 id="Opportunities-for-Adaptive-Experiments-to-Enable-Continuous-Improvement-that-Trades-off-Instructor-and-Researcher-Incentives"><a href="#Opportunities-for-Adaptive-Experiments-to-Enable-Continuous-Improvement-that-Trades-off-Instructor-and-Researcher-Incentives" class="headerlink" title="Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives"></a>Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12324">http://arxiv.org/abs/2310.12324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilya Musabirov, Angela Zavaleta-Bernuy, Pan Chen, Michael Liut, Joseph Jay Williams</li>
<li>for: 这个论文的目的是提供一种基于机器学习的adaptive experimentation方法，用于持续改进高等教育课程。</li>
<li>methods: 这篇论文使用了机器学习算法来分析数据，并在不同的学生群中采用不同的condition进行比较，以确定最有效的condition。</li>
<li>results: 这篇论文的实验结果表明，使用adaptive experimentation方法可以更好地支持学生的需求，并提高学生的学习效果。<details>
<summary>Abstract</summary>
Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the approach with a case study providing a side-by-side comparison of traditional and adaptive experimentation of self-explanation prompts in online homework problems in a CS1 course. This provides a first step in exploring the future of how this methodology can be useful in bridging research and practice in doing continuous improvement.
</details>
<details>
<summary>摘要</summary>
随机实验比较不同的教学策略可以提供有用的实际证据，帮助教师做出决策。然而，传统的实验没有一个明确的和简单的数据使用路径，这限制了学生在实验中获得最佳条件的机会。我们从技术公司的产品开发中使用机器学习和实验的经验而来，探讨如何使用适应试验来促进课程不断改进。在适应试验中，不同的臂/条件在学生面前采用，并分析数据，以改善未来学生的经验。这可以使用机器学习算法来确定哪些行动更有前途的提高学生体验或成绩。这个算法然后会在未来学生面前动态部署最有效的条件，从而提供更好的学生需求支持。我们通过一个案例研究，对传统和适应试验自适应提示在线作业问题的比较，以示方法的可行性。这是继续改进的未来的一个初步探索。
</details></li>
</ul>
<hr>
<h2 id="The-Sentiment-Problem-A-Critical-Survey-towards-Deconstructing-Sentiment-Analysis"><a href="#The-Sentiment-Problem-A-Critical-Survey-towards-Deconstructing-Sentiment-Analysis" class="headerlink" title="The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis"></a>The Sentiment Problem: A Critical Survey towards Deconstructing Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12318">http://arxiv.org/abs/2310.12318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Narayanan Venkit, Mukund Srinath, Sanjana Gautam, Saranya Venkatraman, Vipul Gupta, Rebecca J. Passonneau, Shomir Wilson</li>
<li>for: 本研究探讨了 sentiment analysis (SA) 在不同社技系统中的应用、模型和数据集方面的问题。</li>
<li>methods: 研究者通过审查 189 篇同行评审文章，探讨 SA 在不同领域中的应用和模型，以及数据集的问题。</li>
<li>results: 研究发现 SA 在不同领域中的定义和应用存在差异，导致可能的挑战和偏见。为解决这问题，研究者提出了一个伦理卡，以帮助实践者在使用 SA 时确保公正使用。<details>
<summary>Abstract</summary>
We conduct an inquiry into the sociotechnical aspects of sentiment analysis (SA) by critically examining 189 peer-reviewed papers on their applications, models, and datasets. Our investigation stems from the recognition that SA has become an integral component of diverse sociotechnical systems, exerting influence on both social and technical users. By delving into sociological and technological literature on sentiment, we unveil distinct conceptualizations of this term in domains such as finance, government, and medicine. Our study exposes a lack of explicit definitions and frameworks for characterizing sentiment, resulting in potential challenges and biases. To tackle this issue, we propose an ethics sheet encompassing critical inquiries to guide practitioners in ensuring equitable utilization of SA. Our findings underscore the significance of adopting an interdisciplinary approach to defining sentiment in SA and offer a pragmatic solution for its implementation.
</details>
<details>
<summary>摘要</summary>
我们进行了一个关于社会技术方面的情感分析（SA）的调查， kritically examining 189 peer-reviewed papers on their applications, models, and datasets。我们的调查源于认识到SA已成为多种社会技术系统的重要组成部分，影响社会和技术用户。通过探究社会学和技术文献中的情感概念，我们揭示了不同领域中情感的不同定义和概念化。我们的研究发现了情感定义和框架的明确性不足，可能导致挑战和偏见。为解决这个问题，我们提议一份伦理宣言，涵盖了重要的伦理问题，以帮助实践者在使用SA时确保公正使用。我们的发现表明了采用多科学方法来定义情感在SA中的重要性，并提供了一个实用的解决方案。
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Framework-for-Learning-Argumentation-Semantics"><a href="#A-Unifying-Framework-for-Learning-Argumentation-Semantics" class="headerlink" title="A Unifying Framework for Learning Argumentation Semantics"></a>A Unifying Framework for Learning Argumentation Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12309">http://arxiv.org/abs/2310.12309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zlatina Mileva, Antonis Bikakis, Fabio Aurelio D’Asaro, Mark Law, Alessandra Russo</li>
<li>for: 这篇论文是关于人工智能领域的论证推理研究，旨在提出一种可解释的论证Acceptability semantics的框架，以便在人机对话中使用。</li>
<li>methods: 该论文使用了逻辑编程方法，通过学习来计算论证的接受性。</li>
<li>results: 经验证试验表明，该框架可以在论证计算中具有较高的性能，并且可以在人机对话中提供更加可靠的结果。<details>
<summary>Abstract</summary>
Argumentation is a very active research field of Artificial Intelligence concerned with the representation and evaluation of arguments used in dialogues between humans and/or artificial agents. Acceptability semantics of formal argumentation systems define the criteria for the acceptance or rejection of arguments. Several software systems, known as argumentation solvers, have been developed to compute the accepted/rejected arguments using such criteria. These include systems that learn to identify the accepted arguments using non-interpretable methods. In this paper we present a novel framework, which uses an Inductive Logic Programming approach to learn the acceptability semantics for several abstract and structured argumentation frameworks in an interpretable way. Through an empirical evaluation we show that our framework outperforms existing argumentation solvers, thus opening up new future research directions in the area of formal argumentation and human-machine dialogues.
</details>
<details>
<summary>摘要</summary>
争议是人工智能的一个非常活跃的研究领域，涉及对人类和/或人工代理人之间的对话中使用的论据的表示和评估。正式争议系统的 Acceptability  semantics 定义了论据的接受或拒绝的标准。一些称为争议解决器的软件系统已经被开发出来计算使用这些标准来接受或拒绝论据。这些系统包括使用非可解释的方法来识别接受的论据的学习系统。在这篇论文中，我们提出了一种新的框架，使用逻辑编程方法来学习多种抽象和结构化争议框架的接受可能性，并在实验评估中证明了我们的框架可以在接受可能性评估方面超越现有的争议解决器，从而开启了新的未来研究方向在正式争议和人机对话领域。
</details></li>
</ul>
<hr>
<h2 id="Preference-Optimization-for-Molecular-Language-Models"><a href="#Preference-Optimization-for-Molecular-Language-Models" class="headerlink" title="Preference Optimization for Molecular Language Models"></a>Preference Optimization for Molecular Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12304">http://arxiv.org/abs/2310.12304</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harmonic-discovery/pref-opt-for-mols">https://github.com/harmonic-discovery/pref-opt-for-mols</a></li>
<li>paper_authors: Ryan Park, Ryan Theisen, Navriti Sahni, Marcel Patek, Anna Cichońska, Rayees Rahman</li>
<li>for: 用于生成新的化学结构</li>
<li>methods: 使用直接偏好优化精度调整</li>
<li>results: 高效、简单、有效地与化学家喜好Alignment of generated molecules<details>
<summary>Abstract</summary>
Molecular language modeling is an effective approach to generating novel chemical structures. However, these models do not \emph{a priori} encode certain preferences a chemist may desire. We investigate the use of fine-tuning using Direct Preference Optimization to better align generated molecules with chemist preferences. Our findings suggest that this approach is simple, efficient, and highly effective.
</details>
<details>
<summary>摘要</summary>
分子语言模型可以有效地生成新的化学结构。然而，这些模型没有先验的编码化学家可能愿望的偏好。我们研究了使用直接偏好优化来更好地将生成的分子与化学家的偏好相Alignment。我们发现这种方法简单、高效并有高效果。Here's a word-for-word translation:分子语言模型可以有效地生成新的化学结构。然而，这些模型没有先验的编码化学家可能愿望的偏好。我们研究了使用直接偏好优化来更好地将生成的分子与化学家的偏好相Alignment。我们发现这种方法简单、高效并有高效果。
</details></li>
</ul>
<hr>
<h2 id="Document-Level-Language-Models-for-Machine-Translation"><a href="#Document-Level-Language-Models-for-Machine-Translation" class="headerlink" title="Document-Level Language Models for Machine Translation"></a>Document-Level Language Models for Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12303">http://arxiv.org/abs/2310.12303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Frithjof Petrick, Christian Herold, Pavel Petrushkov, Shahram Khadivi, Hermann Ney</li>
<li>for: 提高文档翻译系统的 Context-awareness，使其能够更好地理解文档的含义和结构。</li>
<li>methods: 组合现有的 sentence-level 翻译模型和文档级别的语言模型，并使用 novel weighting techniques 来提高系统的灵活性和计算效率。</li>
<li>results: 在四种多样化的翻译任务上进行了全面的评估，并显示了substantially 提高的文档指向得分，同时也更加计算效率。但是，我们还发现，通过回译来获得更好的结果，但是需要重新训练翻译系统。此外，我们还探讨了大语言模型的混合，并发现可能在使用大语言模型时存在强大的潜在性。<details>
<summary>Abstract</summary>
Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores substantially and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the cost of having to re-train the translation system. Finally, we explore language model fusion in the light of recent advancements in large language models. Our findings suggest that there might be strong potential in utilizing large language models via model combination.
</details>
<details>
<summary>摘要</summary>
尽管现有的机器翻译系统 todavía 以句子为单位运行，一个原因是因为大多数平行训练数据只有句子水平的对齐，没有文档水平的元信息可用。在这项工作中，我们设想建立了文本上下文感知的翻译系统，使用文档水平的独立语言模型。我们提高了现有的方法，利用最新的模型组合技术。此外，我们提出了新的权重技巧，使系统组合更加灵活，并显著减少计算负担。在四种多样化的翻译任务上进行了全面的评估，我们发现我们的扩展可以大幅提高文档目标得分，并且计算更加高效。然而，我们也发现，在大多数情况下，回传翻译能够提供更好的结果，但是需要重新训练翻译系统。最后，我们探讨了大语言模型的集成，我们发现大语言模型可以通过模型组合来提供强大的潜在力。
</details></li>
</ul>
<hr>
<h2 id="Jorge-Approximate-Preconditioning-for-GPU-efficient-Second-order-Optimization"><a href="#Jorge-Approximate-Preconditioning-for-GPU-efficient-Second-order-Optimization" class="headerlink" title="Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization"></a>Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12298">http://arxiv.org/abs/2310.12298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddharth Singh, Zachary Sating, Abhinav Bhatele</li>
<li>for: 这篇论文的目的是提出一种高效的二阶优化器，以提高深度学习模型的训练效率和性能。</li>
<li>methods: 这篇论文使用了一种新的二阶优化器 named Jorge，它通过简化预conditioning步骤，从而大大减少了计算成本，使其在GPU上实现高效。</li>
<li>results: 实验结果表明，Jorge可以与现有的优化器，如SGD、AdamW和Shampoo等比肩，并在多个深度学习模型上显示出更高的效率和性能。<details>
<summary>Abstract</summary>
Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate the distinct advantages of using Jorge, outperforming state-of-the-art optimizers such as SGD, AdamW, and Shampoo across multiple deep learning models, both in terms of sample efficiency and wall-clock time.
</details>
<details>
<summary>摘要</summary>
尽管第二顺序优化器在深度学习中的更好的整合性，但由于计算成本高涨，使得它们在实际应用中较少使用。在这篇论文中，我们介绍了 Jorge，一种第二顺序优化器，它可以同时具有第一顺序优化器的快速整合和高效计算性。我们通过完全抛弃矩阵逆计算，使得 Jorge 在 GPU 上具有高效的墙 clock 时间。此外，我们还提出了一种确定 Jorge 的超参数的方法，通过对已经优化的 SGD 基线进行调整，以此减少调整努力。我们的实验表明，使用 Jorge 可以获得明显的优势，在多种深度学习模型上，在样本效率和墙 clock 时间两个方面都超过了状态元优化器，如 SGD、AdamW 和 Shampoo。
</details></li>
</ul>
<hr>
<h2 id="Fact-based-Agent-modeling-for-Multi-Agent-Reinforcement-Learning"><a href="#Fact-based-Agent-modeling-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Fact-based Agent modeling for Multi-Agent Reinforcement Learning"></a>Fact-based Agent modeling for Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12290">http://arxiv.org/abs/2310.12290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Baofu Fang, Caiming Zheng, Hao Wang</li>
<li>for: 提高多智能体系中agent之间协作和互动的效率，在 unknown 环境下实现agent模型化。</li>
<li>methods: 使用fact-based belief inference（FBI）网络模型其他智能体的行为和意图，通过variational autoencoder（VAE）学习智能体政策表示。</li>
<li>results: 在多智能体粒子环境（MPE）中比基eline方法高效地提高agent政策学习效率，在复杂的竞争合作混合enario中实现更高的返点。<details>
<summary>Abstract</summary>
In multi-agent systems, agents need to interact and collaborate with other agents in environments. Agent modeling is crucial to facilitate agent interactions and make adaptive cooperation strategies. However, it is challenging for agents to model the beliefs, behaviors, and intentions of other agents in non-stationary environment where all agent policies are learned simultaneously. In addition, the existing methods realize agent modeling through behavior cloning which assume that the local information of other agents can be accessed during execution or training. However, this assumption is infeasible in unknown scenarios characterized by unknown agents, such as competition teams, unreliable communication and federated learning due to privacy concerns. To eliminate this assumption and achieve agent modeling in unknown scenarios, Fact-based Agent modeling (FAM) method is proposed in which fact-based belief inference (FBI) network models other agents in partially observable environment only based on its local information. The reward and observation obtained by agents after taking actions are called facts, and FAM uses facts as reconstruction target to learn the policy representation of other agents through a variational autoencoder. We evaluate FAM on various Multiagent Particle Environment (MPE) and compare the results with several state-of-the-art MARL algorithms. Experimental results show that compared with baseline methods, FAM can effectively improve the efficiency of agent policy learning by making adaptive cooperation strategies in multi-agent reinforcement learning tasks, while achieving higher returns in complex competitive-cooperative mixed scenarios.
</details>
<details>
<summary>摘要</summary>
在多代理系统中，代理需要互动和合作，在环境中进行交互。代理模型是重要的，以便促进代理之间的交互和适应合作策略。然而，在非站ARY环境中，所有代理策略都是同时学习的，对于代理来模型别人的信念、行为和意图是挑战。此外，现有的方法通过行为做副本来实现代理模型，假设在执行或训练中可以访问其他代理的本地信息。然而，这个假设在未知场景中是不可能的，例如竞争队伍、不可靠的通信和联合学习中的隐私问题。为了绕过这个假设并实现代理模型在未知场景中，我们提出了基于事实的代理模型（FAM）方法。FAM使用事实（即代理所获得的奖励和观察）为重建目标，通过变分自动编码器来学习别人的策略表示。我们在多代理粒子环境（MPE）上进行了评估，并与一些现有的 MARL 算法进行了比较。实验结果表明，相比基eline方法，FAM可以更好地提高代理策略学习效率，在多代理束缚学习任务中实现更高的返回，而且在复杂的竞争-合作混合场景中 achieve higher returns。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-Performance-of-Automated-Grade-Prediction-in-MOOC-using-Graph-Representation-Learning"><a href="#Enhancing-the-Performance-of-Automated-Grade-Prediction-in-MOOC-using-Graph-Representation-Learning" class="headerlink" title="Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning"></a>Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12281">http://arxiv.org/abs/2310.12281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsaatusu/mooper_grade_prediction">https://github.com/dsaatusu/mooper_grade_prediction</a></li>
<li>paper_authors: Soheila Farokhi, Aswani Yaramala, Jiangtao Huang, Muhammad F. A. Khan, Xiaojun Qi, Hamid Karimi<br>for:The paper is written for the purpose of enhancing the performance of predictive machine learning models in student assignment grade prediction for MOOCs.methods:The paper uses graph embedding techniques to extract latent structural information encoded in the interactions between entities in the MOOC dataset, without requiring ground truth labels.results:The paper demonstrates that structural features can significantly improve the predictive performance of downstream assessment tasks, and the code and data are available in \url{<a target="_blank" rel="noopener" href="https://github.com/DSAatUSU/MOOPer_grade_prediction%7D">https://github.com/DSAatUSU/MOOPer_grade_prediction}</a>.<details>
<summary>Abstract</summary>
In recent years, Massive Open Online Courses (MOOCs) have gained significant traction as a rapidly growing phenomenon in online learning. Unlike traditional classrooms, MOOCs offer a unique opportunity to cater to a diverse audience from different backgrounds and geographical locations. Renowned universities and MOOC-specific providers, such as Coursera, offer MOOC courses on various subjects. Automated assessment tasks like grade and early dropout predictions are necessary due to the high enrollment and limited direct interaction between teachers and learners. However, current automated assessment approaches overlook the structural links between different entities involved in the downstream tasks, such as the students and courses. Our hypothesis suggests that these structural relationships, manifested through an interaction graph, contain valuable information that can enhance the performance of the task at hand. To validate this, we construct a unique knowledge graph for a large MOOC dataset, which will be publicly available to the research community. Furthermore, we utilize graph embedding techniques to extract latent structural information encoded in the interactions between entities in the dataset. These techniques do not require ground truth labels and can be utilized for various tasks. Finally, by combining entity-specific features, behavioral features, and extracted structural features, we enhance the performance of predictive machine learning models in student assignment grade prediction. Our experiments demonstrate that structural features can significantly improve the predictive performance of downstream assessment tasks. The code and data are available in \url{https://github.com/DSAatUSU/MOOPer_grade_prediction}
</details>
<details>
<summary>摘要</summary>
近年来，大规模在线开放课程（MOOC）在在线学习中得到了广泛的应用和发展。不同于传统的教室，MOOCs为不同背景和地理位置的学生提供了独特的学习机会。知名大学和MOOC专门提供者，如 Coursera，为多种主题的MOOC课程。由于大量报名和教师与学生之间的直接交互有限，因此自动评估任务如学生的评价和早期退出预测变得必要。然而，当前的自动评估方法忽略了学生和课程之间的结构关系。我们的假设是，这些结构关系，通过互动图表示出来，含有价值信息，可以提高任务的表现。为此，我们构建了一个大 MOOC 数据集的专用知识图，该图将在研究社区中公开。此外，我们利用图像技术来提取数据集中互动图中所隐藏的结构信息。这些技术不需要标注数据，可以用于多种任务。最后，我们将实体特征、行为特征和提取的结构特征相结合，提高预测机器学习模型的学生评价分数预测性能。我们的实验表明，结构特征可以显著提高下游评估任务的预测性能。代码和数据可以在 <https://github.com/DSAatUSU/MOOPer_grade_prediction> 中找到。
</details></li>
</ul>
<hr>
<h2 id="An-Image-is-Worth-Multiple-Words-Learning-Object-Level-Concepts-using-Multi-Concept-Prompt-Learning"><a href="#An-Image-is-Worth-Multiple-Words-Learning-Object-Level-Concepts-using-Multi-Concept-Prompt-Learning" class="headerlink" title="An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning"></a>An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12274">http://arxiv.org/abs/2310.12274</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lxasqjc/mcpl">https://github.com/lxasqjc/mcpl</a></li>
<li>paper_authors: Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe, Philip Teare</li>
<li>for: 本研究旨在学习一种新的”词”来表示图像风格和外观，并将其集成到自然语言句子中生成新的合成图像。</li>
<li>methods: 我们提出了一种多个概念提示学习（MCPL）框架，在单个句子-图像对中同时学习多个新”词”。为了提高词概念相关性的准确性，我们提出了三种REG regularization技术：注意力掩码（AttnMask），提示对比损失（PromptCL）和绑定形容词（Bind adj。）。</li>
<li>results: 我们通过图像生成、编辑和注意力可视化等方式进行了广泛的量化比较，demonstrating that our method can learn more semantically disentangled concepts with enhanced word-concept correlation。此外，我们还介绍了一个新的数据集和评价协议，专门为这种学习对象级概念的新任务。<details>
<summary>Abstract</summary>
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation, editing, and attention visualisation with diverse images. Extensive quantitative comparisons demonstrate that our method can learn more semantically disentangled concepts with enhanced word-concept correlation. Additionally, we introduce a novel dataset and evaluation protocol tailored for this new task of learning object-level concepts.
</details>
<details>
<summary>摘要</summary>
文本倒转，一种快速学习方法，学习一个新的"词"来表示图像风格和外观，以便将其 integrate into natural language sentences 生成新的合成图像。然而，在一个场景中identifying和integrating多个对象水平的概念 pose significant challenges， Even when embeddings for individual concepts are available. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularization techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation, editing, and attention visualization with diverse images. Extensive quantitative comparisons demonstrate that our method can learn more semantically disentangled concepts with enhanced word-concept correlation. Additionally, we introduce a novel dataset and evaluation protocol tailored for this new task of learning object-level concepts.
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Adversarial-Attacks-on-Deep-Neural-Networks-for-Targeted-Class-Manipulation-Using-DeepFool-Algorithm"><a href="#Tailoring-Adversarial-Attacks-on-Deep-Neural-Networks-for-Targeted-Class-Manipulation-Using-DeepFool-Algorithm" class="headerlink" title="Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm"></a>Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13019">http://arxiv.org/abs/2310.13019</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. M. Fazle Rabby Labib, Joyanta Jyoti Mondal, Meem Arafat Manab</li>
<li>for: 本研究旨在提出一种可argeting Specific classes的深度骗客（Targeted DeepFool），以提高深度神经网络（DNNs）的鲁棒性。</li>
<li>methods: 本文提出了一种基于DeepFool算法的Targeted DeepFool算法，并引入了最低信任分数的超参数，以提高灵活性。</li>
<li>results: 我们的实验表明，Targeted DeepFool算法可以在不同的深度神经网络架构上实现高效率和图像质量保持，而且可以增强模型的鲁棒性。 results show that one of the deep convolutional neural network architectures, AlexNet, and one of the state-of-the-art model Vision Transformer exhibit high robustness to getting fooled.<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across different deep neural network architectures while preserving image integrity as much as possible. Results show that one of the deep convolutional neural network architectures, AlexNet, and one of the state-of-the-art model Vision Transformer exhibit high robustness to getting fooled. Our code will be made public when publishing the paper.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在不同领域中得到了 significiant advancement，但它们受到了敌意攻击的威胁，这种威胁的存在对于理解和开发有效防御机制是非常重要。DeepFool算法，由Moosavi-Dezfooli et al.（2016）提出，可以在输入图像上发现微小的扰动，以让图像被误分类。然而，DeepFool算法缺乏目标化方法，这使得其在特定攻击enario下效果较差。此外，在先前的相关研究中，研究人员主要关注成功，而不是图像的纯度和误分类的信息量。因此，在这篇论文中，我们提出了Targeted DeepFool算法，这是对DeepFool算法的扩展，可以对特定的类进行误分类。我们还引入了最小信任分数的启用参数，以提高灵活性。我们的实验表明，提议的方法可以在不同的深度神经网络架构上进行效果和效率的混合，同时保持图像的纯度。结果显示，AlexNet和一种state-of-the-art模型Vision Transformer在深度神经网络架构上具有高度的抗攻击能力。我们将代码公开时出版论文。
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Approach-to-Domain-Incremental-Learning-with-Memory-Theory-and-Algorithm"><a href="#A-Unified-Approach-to-Domain-Incremental-Learning-with-Memory-Theory-and-Algorithm" class="headerlink" title="A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm"></a>A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12244">http://arxiv.org/abs/2310.12244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haizhou Shi, Hao Wang</li>
<li>For: 本研究旨在提出一个统一架构，以应对不同领域的渐进式学习问题，并且仅从先前领域中获取一小部分的数据（即记忆）进行学习。* Methods: 本研究提出了一个统一架构， named Unified Domain Incremental Learning (UDIL)，它整合了多种现有的方法，并且通过在训练过程中适应不同的参数，以获得最紧密的一致 bound。* Results: 实验结果显示，UDIL 比先前的领域渐进式学习方法在both synthetic和实际数据集上表现更好，并且可以适应不同的领域。<details>
<summary>Abstract</summary>
Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. Code will be available at https://github.com/Wang-ML-Lab/unified-continual-learning.
</details>
<details>
<summary>摘要</summary>
域incremental learning aimsto adapt to a sequence of domains with only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, called Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL unifies various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different fixed coefficients; based on insights from this unification, our UDIL allows adaptive coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthetic and real-world datasets. Code will be available at https://github.com/Wang-ML-Lab/unified-continual-learning.Here's the translation of the highlighted phrases:* **unifies**: 统一* **fixed**: 固定的* **adaptive**: 可变的* **tighter**: 更紧的* **generalization error bound**: 泛化误差 bound* **state-of-the-art**: 现有的最佳方法* **synthetic**:  sintetic* **real-world**: 实际的
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-In-Context-Imitation-Learning-via-Implicit-Graph-Alignment"><a href="#Few-Shot-In-Context-Imitation-Learning-via-Implicit-Graph-Alignment" class="headerlink" title="Few-Shot In-Context Imitation Learning via Implicit Graph Alignment"></a>Few-Shot In-Context Imitation Learning via Implicit Graph Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12238">http://arxiv.org/abs/2310.12238</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vitalis Vosylius, Edward Johns</li>
<li>for: 本研究旨在解决机器人学习新任务时，用于几个示例对象中的任务关系推广到新未经见过的对象上。</li>
<li>methods: 本研究使用 conditional alignment 问题来形式化模仿学习，通过对对象图表示的匹配来捕捉任务相关关系。</li>
<li>results: 实验结果显示，我们的方法可以高效地完成几种实际生活中的每日任务，并在比较基eline上表现出色。视频可以在我们项目网站（<a target="_blank" rel="noopener" href="https://www.robot-learning.uk/implicit-graph-alignment%EF%BC%89%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://www.robot-learning.uk/implicit-graph-alignment）中找到。</a><details>
<summary>Abstract</summary>
Consider the following problem: given a few demonstrations of a task across a few different objects, how can a robot learn to perform that same task on new, previously unseen objects? This is challenging because the large variety of objects within a class makes it difficult to infer the task-relevant relationship between the new objects and the objects in the demonstrations. We address this by formulating imitation learning as a conditional alignment problem between graph representations of objects. Consequently, we show that this conditioning allows for in-context learning, where a robot can perform a task on a set of new objects immediately after the demonstrations, without any prior knowledge about the object class or any further training. In our experiments, we explore and validate our design choices, and we show that our method is highly effective for few-shot learning of several real-world, everyday tasks, whilst outperforming baselines. Videos are available on our project webpage at https://www.robot-learning.uk/implicit-graph-alignment.
</details>
<details>
<summary>摘要</summary>
问题如下：给定一些对象的几个示例任务，如何使 robot 能够在新、未经见过的对象上完成同样的任务？这是因为对象类中的巨量对象关系使得推断任务相关关系 между新对象和示例对象困难。我们解决这个问题，通过将仿真学定义为对象图表示的条件对Alignment问题。因此，我们表明，这种conditioning允许机器人在示例后立即在新对象上进行任务，不需要对对象类或进一步训练。在我们的实验中，我们探索和验证我们的设计选择，并证明我们的方法高效地实现了几个真实世界、日常任务的少量学习，并超越基elines。视频可以在我们项目网站上找到：https://www.robot-learning.uk/implicit-graph-alignment。
</details></li>
</ul>
<hr>
<h2 id="An-Eager-Satisfiability-Modulo-Theories-Solver-for-Algebraic-Datatypes"><a href="#An-Eager-Satisfiability-Modulo-Theories-Solver-for-Algebraic-Datatypes" class="headerlink" title="An Eager Satisfiability Modulo Theories Solver for Algebraic Datatypes"></a>An Eager Satisfiability Modulo Theories Solver for Algebraic Datatypes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12234">http://arxiv.org/abs/2310.12234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amar Shah, Federico Mora, Sanjit A. Seshia</li>
<li>for: 这篇论文的目的是提出一种新的满足推理（SMT）解决方案，用于自动推理关于抽象数据类型（ADT）的问题。</li>
<li>methods: 这篇论文使用了一种新的积极的方法，即将 ADT 查询转化为一种 simpler 的逻辑理论，未解释函数（UF），然后使用现有的解决器解决减少后的查询。</li>
<li>results: 作者证明了这种方法的有效性和完整性，并在现有的benchmark集和一个新的劳动ious benchmark集上进行了比较，得到的结果表明该方法在现有的 benchmark 上比 state-of-the-art 的方法更高效。<details>
<summary>Abstract</summary>
Algebraic data types (ADTs) are a construct classically found in functional programming languages that capture data structures like enumerated types, lists, and trees. In recent years, interest in ADTs has increased. For example, popular programming languages, like Python, have added support for ADTs. Automated reasoning about ADTs can be done using satisfiability modulo theories (SMT) solving, an extension of the Boolean satisfiability problem with constraints over first-order structures. Unfortunately, SMT solvers that support ADTs do not scale as state-of-the-art approaches all use variations of the same \emph{lazy} approach. In this paper, we present an SMT solver that takes a fundamentally different approach, an \emph{eager} approach. Specifically, our solver reduces ADT queries to a simpler logical theory, uninterpreted functions (UF), and then uses an existing solver on the reduced query. We prove the soundness and completeness of our approach and demonstrate that it outperforms the state-of-theart on existing benchmarks, as well as a new, more challenging benchmark set from the planning domain.
</details>
<details>
<summary>摘要</summary>
алгебраические данные типы (ADTs) 是一种在函数编程语言中出现的构造，用于表示枚举类型、列表和树等数据结构。在最近几年中，关于 ADTs 的兴趣增加了。例如，流行编程语言如 Python 也添加了对 ADTs 的支持。通过使用满足性模ulo理论 (SMT) 解决方案，可以自动进行 ADTs 的逻辑推理。然而，现有的 SMT 解决方案都是基于同样的怠慢（lazy）方法，我们则提出了一种不同的积极（eager）方法。具体来说，我们的解决方案将 ADT 查询降到了一个更简单的逻辑理论，未解释函数 (UF)，然后使用现有的解决方案对减少后的查询进行解决。我们证明了我们的方法的有效性和完整性，并证明其在现有的benchmark中以及一个新的、更加挑战性的benchmark集中的性能比例较好。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Sampling-of-Balanced-K-Means-using-Adiabatic-Quantum-Computing"><a href="#Probabilistic-Sampling-of-Balanced-K-Means-using-Adiabatic-Quantum-Computing" class="headerlink" title="Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing"></a>Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12153">http://arxiv.org/abs/2310.12153</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan-Nico Zaech, Martin Danelljan, Luc Van Gool</li>
<li>for: 这篇论文探讨了可以用adiabatic quantum computing（AQC）来解决数值和常见NP困难的优化问题。</li>
<li>methods: 现有的AQC技术仅允许使用最佳量子态，将其他量子态视为噪音并抛弃。这篇论文提出了使用这些噪音信息进行概率平衡k-means排序的想法。</li>
<li>results: 这篇论文使用了D-Wave AQC处理 sintetic和实际数据，并证明了这种方法可以更好地识别歧义的解和数据点。<details>
<summary>Abstract</summary>
Adiabatic quantum computing (AQC) is a promising quantum computing approach for discrete and often NP-hard optimization problems. Current AQCs allow to implement problems of research interest, which has sparked the development of quantum representations for many machine learning and computer vision tasks. Despite requiring multiple measurements from the noisy AQC, current approaches only utilize the best measurement, discarding information contained in the remaining ones. In this work, we explore the potential of using this information for probabilistic balanced k-means clustering. Instead of discarding non-optimal solutions, we propose to use them to compute calibrated posterior probabilities with little additional compute cost. This allows us to identify ambiguous solutions and data points, which we demonstrate on a D-Wave AQC on synthetic and real data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fairer-and-More-Accurate-Tabular-Models-Through-NAS"><a href="#Fairer-and-More-Accurate-Tabular-Models-Through-NAS" class="headerlink" title="Fairer and More Accurate Tabular Models Through NAS"></a>Fairer and More Accurate Tabular Models Through NAS</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12145">http://arxiv.org/abs/2310.12145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richeek Das, Samuel Dooley</li>
<li>for: 这种研究旨在使深度学习模型更加公正，具体来说是通过更新模型的结构和训练参数来实现这一目标。</li>
<li>methods: 该研究使用多目标神经网络搜索（NAS）和超参数优化（HPO）来找到一个新的模型，以提高模型的输出。</li>
<li>results: 研究发现，尝试单独优化模型的准确率可能会导致公正性问题，而同时优化模型的准确率和公正性可以共同优化模型的性能。<details>
<summary>Abstract</summary>
Making models algorithmically fairer in tabular data has been long studied, with techniques typically oriented towards fixes which usually take a neural model with an undesirable outcome and make changes to how the data are ingested, what the model weights are, or how outputs are processed. We employ an emergent and different strategy where we consider updating the model's architecture and training hyperparameters to find an entirely new model with better outcomes from the beginning of the debiasing procedure. In this work, we propose using multi-objective Neural Architecture Search (NAS) and Hyperparameter Optimization (HPO) in the first application to the very challenging domain of tabular data. We conduct extensive exploration of architectural and hyperparameter spaces (MLP, ResNet, and FT-Transformer) across diverse datasets, demonstrating the dependence of accuracy and fairness metrics of model predictions on hyperparameter combinations. We show that models optimized solely for accuracy with NAS often fail to inherently address fairness concerns. We propose a novel approach that jointly optimizes architectural and training hyperparameters in a multi-objective constraint of both accuracy and fairness. We produce architectures that consistently Pareto dominate state-of-the-art bias mitigation methods either in fairness, accuracy or both, all of this while being Pareto-optimal over hyperparameters achieved through single-objective (accuracy) optimization runs. This research underscores the promise of automating fairness and accuracy optimization in deep learning models.
</details>
<details>
<summary>摘要</summary>
使深度学习模型更加公平在表格数据上进行研究已经很长时间了，通常采用的技术是对现有的神经网络模型进行修改，以改善数据入口方式、模型权重或输出处理方式。我们采用一种不同的策略，即对模型的建构和训练超参数进行更新，以找到一个从头开始的全新模型，以提高结果的公平性。在这项工作中，我们提出使用多目标神经网络搜索（NAS）和超参数优化（HPO）来优化模型的建构和超参数。我们在多个数据集上进行了广泛的建构和超参数空间的探索（包括MLP、ResNet和FT-Transformer），并证明了模型预测结果中的公平性和准确性指标之间的依赖关系。我们发现，通过solely使用NAS优化模型的准确性，通常无法自动解决公平性问题。我们提出了一种新的方法，即同时优化建构和超参数，以实现多目标约束中的准确性和公平性两个目标的 JOINT 优化。我们生成了一系列可以同时dominates state-of-the-art偏见缓解方法的建构，并且这些建构都是通过多目标约束来实现的。这些研究表明了自动化深度学习模型的公平性和准确性优化的推荐。
</details></li>
</ul>
<hr>
<h2 id="Getting-aligned-on-representational-alignment"><a href="#Getting-aligned-on-representational-alignment" class="headerlink" title="Getting aligned on representational alignment"></a>Getting aligned on representational alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13018">http://arxiv.org/abs/2310.13018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Erin Grant, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi, Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O’Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller, Mariya Toneva, Thomas L. Griffiths</li>
<li>for: The paper aims to improve communication between research communities studying representational alignment in cognitive science, neuroscience, and machine learning, by proposing a unifying framework that can serve as a common language for these fields.</li>
<li>methods: The paper surveys the literature from these fields and demonstrates how prior work fits into the proposed framework.</li>
<li>results: The paper identifies open problems in representational alignment where progress can benefit all three fields, and hopes to catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是为了提高认知科学、神经科学和机器学习等领域研究表征对Alignment的交流，提出一个统一的框架，以便这些领域的研究者之间更好地交流。</li>
<li>methods: 论文将Literature Survey的方法采用到这些领域的文献中，并将先前的工作放入该框架中。</li>
<li>results: 论文标识了表征对Alignment中的开放问题，希望通过跨领域合作，加速所有研究信息处理系统的进步。<details>
<summary>Abstract</summary>
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from the fields of cognitive science, neuroscience, and machine learning, and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.
</details>
<details>
<summary>摘要</summary>
Translation notes:* 表征对齐 (representational alignment) is a term used to describe the study of how different information processing systems form representations of the world and how those representations can be aligned to improve communication and collaboration between systems.* 生物学的信息处理系统 (biological information processing systems) refers to the systems found in living organisms, such as the human brain, that process and analyze information from the environment.* 人工的信息处理系统 (artificial information processing systems) refers to the systems created by humans, such as computers and machine learning algorithms, that process and analyze information.* 形象 (representations) refers to the internal mental or computational models that systems use to represent the world and make decisions.* 类别 (categories) refers to the ways in which systems group and classify objects or concepts in the world.* 理解 (reasoning) refers to the processes by which systems draw conclusions or make decisions based on the information they have.* 规划 (planning) refers to the processes by which systems create and execute a plan to achieve a goal.* 导航 (navigation) refers to the processes by which systems move through the world and avoid obstacles.* 决策 (decision-making) refers to the processes by which systems choose between different options or courses of action.
</details></li>
</ul>
<hr>
<h2 id="A-comprehensible-analysis-of-the-efficacy-of-Ensemble-Models-for-Bug-Prediction"><a href="#A-comprehensible-analysis-of-the-efficacy-of-Ensemble-Models-for-Bug-Prediction" class="headerlink" title="A comprehensible analysis of the efficacy of Ensemble Models for Bug Prediction"></a>A comprehensible analysis of the efficacy of Ensemble Models for Bug Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12133">http://arxiv.org/abs/2310.12133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ingrid Marçal, Rogério Eduardo Garcia</li>
<li>for: 本研究旨在比较和分析使用人工智能技术在软件工程中预测Java类库中存在bug的可能性。</li>
<li>methods: 我们使用了两种Apache Commons Project的Java组件进行训练和测试模型，分别是单个AI模型和集成AI模型。</li>
<li>results: 我们的实验结果表明，集成AI模型可以在预测Java类库中存在bug的可能性方面超过单个AI模型的结果。我们还提供了因素的分析，以便更好地理解 ensemble AI 模型的性能提升的原因。<details>
<summary>Abstract</summary>
The correctness of software systems is vital for their effective operation. It makes discovering and fixing software bugs an important development task. The increasing use of Artificial Intelligence (AI) techniques in Software Engineering led to the development of a number of techniques that can assist software developers in identifying potential bugs in code. In this paper, we present a comprehensible comparison and analysis of the efficacy of two AI-based approaches, namely single AI models and ensemble AI models, for predicting the probability of a Java class being buggy. We used two open-source Apache Commons Project's Java components for training and evaluating the models. Our experimental findings indicate that the ensemble of AI models can outperform the results of applying individual AI models. We also offer insight into the factors that contribute to the enhanced performance of the ensemble AI model. The presented results demonstrate the potential of using ensemble AI models to enhance bug prediction results, which could ultimately result in more reliable software systems.
</details>
<details>
<summary>摘要</summary>
软件系统的正确性是其效果运行的关键。找到和修复软件漏洞是软件开发中重要的任务。随着人工智能（AI）技术在软件工程中的广泛应用，出现了一些可以帮助软件开发人员找到代码中潜在的漏洞的技术。在这篇论文中，我们提供了可读性比较和分析，探讨使用单个AI模型和 ensemble AI模型来预测Java类的可能性。我们使用了两个开源Apache Commons Project的Java组件来训练和测试模型。我们的实验结果表明， ensemble AI模型可以在应用单个AI模型的情况下出perform better。我们还提供了影响ensemble AI模型的表现的因素。该结果表明，使用ensemble AI模型可以提高漏洞预测结果，从而导致更可靠的软件系统。
</details></li>
</ul>
<hr>
<h2 id="DiagrammerGPT-Generating-Open-Domain-Open-Platform-Diagrams-via-LLM-Planning"><a href="#DiagrammerGPT-Generating-Open-Domain-Open-Platform-Diagrams-via-LLM-Planning" class="headerlink" title="DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning"></a>DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12128">http://arxiv.org/abs/2310.12128</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aszala/DiagrammerGPT">https://github.com/aszala/DiagrammerGPT</a></li>
<li>paper_authors: Abhay Zala, Han Lin, Jaemin Cho, Mohit Bansal</li>
<li>for: 这个论文旨在解决现有的文本到图像（T2I）生成模型无法生成符号图文（diagram）的问题。</li>
<li>methods: 该论文提出了一种两stage的文本到图文生成框架，利用大语言模型（LLMs）的布局指导能力来生成更加准确的开放领域、开放 платфор具有的图文。</li>
<li>results: 论文通过使用 LLMs 生成和修改 ‘图文计划’（在一个 плаanner-auditor 反馈循环中），以及使用 DiagramGLIGEN 和文本标签渲染模块来生成图文，实现了更高的准确率和质量。<details>
<summary>Abstract</summary>
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planner-auditor feedback loop) which describe all the entities (objects and text labels), their relationships (arrows or lines), and their bounding box layouts. In the second stage, we use a diagram generator, DiagramGLIGEN, and a text label rendering module to generate diagrams following the diagram plans. To benchmark the text-to-diagram generation task, we introduce AI2D-Caption, a densely annotated diagram dataset built on top of the AI2D dataset. We show quantitatively and qualitatively that our DiagrammerGPT framework produces more accurate diagrams, outperforming existing T2I models. We also provide comprehensive analysis including open-domain diagram generation, vector graphic diagram generation in different platforms, human-in-the-loop diagram plan editing, and multimodal planner/auditor LLMs (e.g., GPT-4Vision). We hope our work can inspire further research on diagram generation via T2I models and LLMs.
</details>
<details>
<summary>摘要</summary>
TEXT-TO-IMAGE（T2I）生成技术在过去几年内有了很大的发展。然而，有很少的研究集中于使用 T2I 模型生成图文。图文是一种使用结构rich和空间复杂的视觉表示方式，用于展示信息（例如，密集的对象、文本标签、指向箭头、连接线等）。现有的 T2I 模型通常在图文生成中存在缺陷，因为它们缺乏细化的对象布局控制，特别是当多个对象密集连接并且有复杂的关系（如箭头/线）时。为解决这个漏洞，我们提出了 DiagrammerGPT，一种新的两stage T2I 生成框架。在第一stage中，我们使用 LLMs（例如 GPT-4）来生成和反复修改 '图文计划'（在计划-审查器反馈循环中），该计划描述了所有对象（包括物体和文本标签）、它们之间的关系（如箭头或线）以及它们的包围盒布局。在第二stage中，我们使用 DiagramGLIGEN 和文本标签渲染模块来生成图文，按照图文计划进行。为了评估 T2I 生成任务，我们提出了 AI2D-Caption，一个密集注释的图文数据集，建立在 AI2D 数据集之上。我们表明了量化和质量上，我们的 DiagrammerGPT 框架可以生成更加准确的图文，超过现有的 T2I 模型。此外，我们还提供了广泛的分析，包括开放平台图文生成、vector graphic diagram生成、人工循环图文计划编辑和多Modal LLMs（例如 GPT-4Vision）。我们希望我们的工作可以鼓励更多的研究人员通过 T2I 模型和 LLMs 来生成图文。
</details></li>
</ul>
<hr>
<h2 id="SHARCS-Efficient-Transformers-through-Routing-with-Dynamic-Width-Sub-networks"><a href="#SHARCS-Efficient-Transformers-through-Routing-with-Dynamic-Width-Sub-networks" class="headerlink" title="SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks"></a>SHARCS: Efficient Transformers through Routing with Dynamic Width Sub-networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12126">http://arxiv.org/abs/2310.12126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Salehi, Sachin Mehta, Aditya Kusupati, Ali Farhadi, Hannaneh Hajishirzi</li>
<li>for: 提高Transformer网络的批处理能力和精度</li>
<li>methods: 使用SHARCS进行适应推理，可以在不同的模型和压缩方法下进行自适应调整</li>
<li>results: SHARCS可以提高推理速度，并且可以保持精度水平，实际测试中SHARCS可以提高推理速度2倍，但是精度下降不значитель<details>
<summary>Abstract</summary>
We introduce SHARCS for adaptive inference that takes into account the hardness of input samples. SHARCS can train a router on any transformer network, enabling the model to direct different samples to sub-networks with varying widths. Our experiments demonstrate that: (1) SHARCS outperforms or complements existing per-sample adaptive inference methods across various classification tasks in terms of accuracy vs. FLOPs; (2) SHARCS generalizes across different architectures and can be even applied to compressed and efficient transformer encoders to further improve their efficiency; (3) SHARCS can provide a 2 times inference speed up at an insignificant drop in accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍SHARCS，一种适应推理的方法，考虑到输入样本的困难程度。SHARCS可以在任何transformer网络上训练路由器，让模型将不同的样本分配到不同宽度的子网络上。我们的实验表明：（1）SHARCS与现有的每个样本适应推理方法相比，在不同的分类任务中获得更高的精度和FLOPs的调整；（2）SHARCS可以适用于不同的架构，并且可以进一步改善压缩和高效的transformerEncoder的效率；（3）SHARCS可以提供2倍的推理速度，而无需对精度造成显著的损失。
</details></li>
</ul>
<hr>
<h2 id="A-Cautionary-Tale-On-the-Role-of-Reference-Data-in-Empirical-Privacy-Defenses"><a href="#A-Cautionary-Tale-On-the-Role-of-Reference-Data-in-Empirical-Privacy-Defenses" class="headerlink" title="A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses"></a>A Cautionary Tale: On the Role of Reference Data in Empirical Privacy Defenses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12112">http://arxiv.org/abs/2310.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caelin G. Kaplan, Chuan Xu, Othmane Marfoq, Giovanni Neglia, Anderson Santana de Oliveira</li>
<li>for: This paper focuses on developing effective privacy-preserving machine learning methods that can provide satisfactory levels of training data privacy without significantly compromising model utility.</li>
<li>methods: The proposed method is based on an empirical risk minimization approach with a constraint on the generalization error, which is evaluated as a weighted empirical risk minimization (WERM) over the training and reference datasets.</li>
<li>results: The proposed method outperforms existing state-of-the-art empirical privacy defenses using reference data for nearly all relative privacy levels of reference and training data, and demonstrates the importance of considering the triad of model utility, training data privacy, and reference data privacy when comparing privacy defenses.<details>
<summary>Abstract</summary>
Within the realm of privacy-preserving machine learning, empirical privacy defenses have been proposed as a solution to achieve satisfactory levels of training data privacy without a significant drop in model utility. Most existing defenses against membership inference attacks assume access to reference data, defined as an additional dataset coming from the same (or a similar) underlying distribution as training data. Despite the common use of reference data, previous works are notably reticent about defining and evaluating reference data privacy. As gains in model utility and/or training data privacy may come at the expense of reference data privacy, it is essential that all three aspects are duly considered. In this paper, we first examine the availability of reference data and its privacy treatment in previous works and demonstrate its necessity for fairly comparing defenses. Second, we propose a baseline defense that enables the utility-privacy tradeoff with respect to both training and reference data to be easily understood. Our method is formulated as an empirical risk minimization with a constraint on the generalization error, which, in practice, can be evaluated as a weighted empirical risk minimization (WERM) over the training and reference datasets. Although we conceived of WERM as a simple baseline, our experiments show that, surprisingly, it outperforms the most well-studied and current state-of-the-art empirical privacy defenses using reference data for nearly all relative privacy levels of reference and training data. Our investigation also reveals that these existing methods are unable to effectively trade off reference data privacy for model utility and/or training data privacy. Overall, our work highlights the need for a proper evaluation of the triad model utility / training data privacy / reference data privacy when comparing privacy defenses.
</details>
<details>
<summary>摘要</summary>
在隐私保护机器学习领域，验证性隐私防御被提出为实现训练数据隐私的解决方案，而不导致模型性能下降。大多数现有的防御机制假设有访问参考数据，定义为训练数据所处的同一个（或类似）分布下的另一个数据集。尽管参考数据广泛使用，但前一些作品却不够明确地定义和评估参考数据隐私。因为获得模型性能和/或训练数据隐私的增进可能会导致参考数据隐私的损害，因此必须同时考虑这三个方面。在这篇论文中，我们首先检查参考数据的可用性和隐私处理方法，并证明其必要性以便比较防御机制。其次，我们提出一种基准防御方法，允许模型性能和训练数据隐私之间的利用率评估，并且可以通过将总体化风险最小化问题转化为权重加总风险最小化问题（WERM）来实现。虽然我们视WERM为简单的基准方法，但我们的实验表明，它在大多数参考数据隐私水平下能够超越目前最具有研究价值和状态艺术的Empirical Privacy防御方法。我们的调查也表明，这些现有方法无法有效地考虑参考数据隐私和训练数据隐私之间的贝叶率。总的来说，我们的工作强调了评估模型性能、训练数据隐私和参考数据隐私的三元模型在比较防御机制时的重要性。
</details></li>
</ul>
<hr>
<h2 id="DASA-Difficulty-Aware-Semantic-Augmentation-for-Speaker-Verification"><a href="#DASA-Difficulty-Aware-Semantic-Augmentation-for-Speaker-Verification" class="headerlink" title="DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification"></a>DASA: Difficulty-Aware Semantic Augmentation for Speaker Verification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12111">http://arxiv.org/abs/2310.12111</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wang, Yang Zhang, Zhiyong Wu, Zhihan Yang, Tao Wei, Kun Zou, Helen Meng</li>
<li>for: 提高深度神经网络模型的总化能力和鲁棒性，通过数据扩充来提高speaker认证模型的性能。</li>
<li>methods: 提出了一种新的困难意识 semantic 数据扩充（DASA）方法，通过对话者嵌入空间中的 semantic 方向进行偏移来生成多样化的训练样本，同时保持采样的计算成本很低。</li>
<li>results: 经验表明，提出的方法可以带来remarkable的性能提升，最好的结果在CN-Celeb评测集上实现了14.6%的相对性能下降。<details>
<summary>Abstract</summary>
Data augmentation is vital to the generalization ability and robustness of deep neural networks (DNNs) models. Existing augmentation methods for speaker verification manipulate the raw signal, which are time-consuming and the augmented samples lack diversity. In this paper, we present a novel difficulty-aware semantic augmentation (DASA) approach for speaker verification, which can generate diversified training samples in speaker embedding space with negligible extra computing cost. Firstly, we augment training samples by perturbing speaker embeddings along semantic directions, which are obtained from speaker-wise covariance matrices. Secondly, accurate covariance matrices are estimated from robust speaker embeddings during training, so we introduce difficultyaware additive margin softmax (DAAM-Softmax) to obtain optimal speaker embeddings. Finally, we assume the number of augmented samples goes to infinity and derive a closed-form upper bound of the expected loss with DASA, which achieves compatibility and efficiency. Extensive experiments demonstrate the proposed approach can achieve a remarkable performance improvement. The best result achieves a 14.6% relative reduction in EER metric on CN-Celeb evaluation set.
</details>
<details>
<summary>摘要</summary>
<<SYS>> traducedata augmentation是深度神经网络（DNN）模型的重要组成部分，它们可以提高模型的通用能力和鲁棒性。现有的增强方法 дляspeaker verification通常是对原始信号进行 manipulate，这些方法需要较多的计算时间，并且增强的样本缺乏多样性。在这篇论文中，我们提出了一种新的困难相关Semantic Augmentation（DASA）方法，可以在speaker embedding空间中生成多样化的训练样本，而且计算成本几乎为零。首先，我们在训练样本中进行增强，通过在语意方向上偏移speaker embeddings来生成多样化的训练样本。其次，我们在训练过程中对robust speaker embeddings进行估计，以获得高精度的covariance矩阵。最后，我们引入difficulty-aware additive margin softmax（DAAM-Softmax）来获取最佳的speaker embeddings。最后，我们假设增强的样本数量为无穷大，并 deriveclosed-form upper bound of the expected loss with DASA，这个目标函数可以实现compatibility和效率。我们的实验表明，提出的方法可以获得显著的性能改进。最好的结果实现了CN-Celeb评估集上的14.6%相对减少EER指标。Note: Some words and phrases in the text have been modified to better fit the Simplified Chinese language, but the overall meaning and content of the text remain the same.
</details></li>
</ul>
<hr>
<h2 id="Quality-Diversity-through-Human-Feedback"><a href="#Quality-Diversity-through-Human-Feedback" class="headerlink" title="Quality Diversity through Human Feedback"></a>Quality Diversity through Human Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12103">http://arxiv.org/abs/2310.12103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Ding, Jenny Zhang, Jeff Clune, Lee Spector, Joel Lehman</li>
<li>for: 提高基本模型的性能 для质量任务</li>
<li>methods: 结合人工反馈来推导多样性度量</li>
<li>results: 比现有多样性算法提高自动多样性发现能力，并与人工定义多样性度量匹配搜索能力<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) has exhibited the potential to enhance the performance of foundation models for qualitative tasks. Despite its promise, its efficacy is often restricted when conceptualized merely as a mechanism to maximize learned reward models of averaged human preferences, especially in areas such as image generation which demand diverse model responses. Meanwhile, quality diversity (QD) algorithms, dedicated to seeking diverse, high-quality solutions, are often constrained by the dependency on manually defined diversity metrics. Interestingly, such limitations of RLHF and QD can be overcome by blending insights from both. This paper introduces Quality Diversity through Human Feedback (QDHF), which employs human feedback for inferring diversity metrics, expanding the applicability of QD algorithms. Empirical results reveal that QDHF outperforms existing QD methods regarding automatic diversity discovery, and matches the search capabilities of QD with human-constructed metrics. Notably, when deployed for a latent space illumination task, QDHF markedly enhances the diversity of images generated by a Diffusion model. The study concludes with an in-depth analysis of QDHF's sample efficiency and the quality of its derived diversity metrics, emphasizing its promise for enhancing exploration and diversity in optimization for complex, open-ended tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传递人类反馈学习（RLHF）已经展示了改进基础模型的表现能力。尽管它的潜力很大，但它的效果往往受限于仅视为提高学习得到的奖励模型的均值人类喜好，特别是在图像生成等需要多样化模型响应的领域。同时，质量多样性（QD）算法，专门寻找多样、高质量的解决方案，经常受到手动定义多样性度量的限制。 Curiously, RLHF和QD的局限性可以通过两者的洞察得到解决。这篇论文介绍了基于人类反馈的质量多样性（QDHF），它利用人类反馈来推断多样性度量，扩展了QD算法的适用范围。实验结果表明，QDHF在自动多样性发现方面表现出色，与人工定义多样性度量相当。尤其是在使用了一种扩散模型进行隐藏空间照明任务时，QDHF明显提高了生成的图像的多样性。研究结束于对QDHF的样本效率和获得的多样性度量的深入分析，强调它在复杂、开端任务中的探索和多样性提高的抢夺。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Non-Intrusive-Adaptation-Input-Centric-Parameter-efficient-Fine-Tuning-for-Versatile-Multimodal-Modeling"><a href="#Non-Intrusive-Adaptation-Input-Centric-Parameter-efficient-Fine-Tuning-for-Versatile-Multimodal-Modeling" class="headerlink" title="Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling"></a>Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12100">http://arxiv.org/abs/2310.12100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaqing Wang, Jialin Wu, Tanmaya Dabral, Jiageng Zhang, Geoff Brown, Chun-Ta Lu, Frederick Liu, Yi Liang, Bo Pang, Michael Bendersky, Radu Soricut</li>
<li>for: 这篇论文的目的是探讨如何实现具有优秀表现的大型语言模型（LLMs）和视觉语言模型（VLMs）的实际应用，以及如何在不需要专门设计的任务下进行适应和服务。</li>
<li>methods: 这篇论文使用了两种Parameter-efficient fine-tuning（PEFT）技术：不断式PEFT和非断式PEFT。不断式PEFT直接改变模型内部架构，可以更加灵活，但是它导入了训练和服务中的复杂性。非断式PEFT则保持模型内部架构不变，仅对输入的嵌入进行适应，这种方法比较简单。这篇论文描述了一种非断式PEFT技术名为AdaLink，它在不同的任务上实现了与SoTA创新PEFT（LoRA）和全模型精确调整（FT）的竞争性表现。</li>
<li>results: 这篇论文的结果显示，AdaLink在文本仅和多媒体任务上均实现了与SoTA intrusive PEFT（LoRA）和FT的竞争性表现。此外，这篇论文还进行了对不同训练 режи和执行环境的测试，以确保AdaLink在实际应用中具有可靠性和稳定性。<details>
<summary>Abstract</summary>
Large language models (LLMs) and vision language models (VLMs) demonstrate excellent performance on a wide range of tasks by scaling up parameter counts from O(10^9) to O(10^{12}) levels and further beyond. These large scales make it impossible to adapt and deploy fully specialized models given a task of interest. Parameter-efficient fine-tuning (PEFT) emerges as a promising direction to tackle the adaptation and serving challenges for such large models. We categorize PEFT techniques into two types: intrusive and non-intrusive. Intrusive PEFT techniques directly change a model's internal architecture. Though more flexible, they introduce significant complexities for training and serving. Non-intrusive PEFT techniques leave the internal architecture unchanged and only adapt model-external parameters, such as embeddings for input. In this work, we describe AdaLink as a non-intrusive PEFT technique that achieves competitive performance compared to SoTA intrusive PEFT (LoRA) and full model fine-tuning (FT) on various tasks. We evaluate using both text-only and multimodal tasks, with experiments that account for both parameter-count scaling and training regime (with and without instruction tuning).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Position-Interpolation-Improves-ALiBi-Extrapolation"><a href="#Position-Interpolation-Improves-ALiBi-Extrapolation" class="headerlink" title="Position Interpolation Improves ALiBi Extrapolation"></a>Position Interpolation Improves ALiBi Extrapolation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13017">http://arxiv.org/abs/2310.13017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faisal Al-Khateeb, Nolan Dey, Daria Soboleva, Joel Hestness</li>
<li>for: 帮助预训练模型使用旋转位嵌入（RoPE）来推断更长的序列长度。</li>
<li>methods: 使用线性位 interpolator来扩展模型使用注意力与直线偏好（ALiBi）的推断范围。</li>
<li>results: 位 interpolator显著提高了预训练模型在语言模型和摘要回传任务中的推断能力。<details>
<summary>Abstract</summary>
Linear position interpolation helps pre-trained models using rotary position embeddings (RoPE) to extrapolate to longer sequence lengths. We propose using linear position interpolation to extend the extrapolation range of models using Attention with Linear Biases (ALiBi). We find position interpolation significantly improves extrapolation capability on upstream language modelling and downstream summarization and retrieval tasks.
</details>
<details>
<summary>摘要</summary>
线性位置 interpolate 帮助预训练模型使用旋转位置嵌入 (RoPE) 来推断更长的序列长度。我们提议使用线性位置 interpolate 来扩展使用 Attention with Linear Biases (ALiBi) 模型的推断范围。我们发现位置 interpolate 对于上游语言模型和下游摘要和检索任务的推断能力有显著改善。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Siren’s-Song-Towards-Reliable-Fact-Conflicting-Hallucination-Detection"><a href="#Unveiling-the-Siren’s-Song-Towards-Reliable-Fact-Conflicting-Hallucination-Detection" class="headerlink" title="Unveiling the Siren’s Song: Towards Reliable Fact-Conflicting Hallucination Detection"></a>Unveiling the Siren’s Song: Towards Reliable Fact-Conflicting Hallucination Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12086">http://arxiv.org/abs/2310.12086</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zjunlp/factchd">https://github.com/zjunlp/factchd</a></li>
<li>paper_authors: Xiang Chen, Duanzheng Song, Honghao Gui, Chengxi Wang, Ningyu Zhang, Fei Huang, Chengfei Lv, Dan Zhang, Huajun Chen</li>
<li>for: The paper is written for evaluating the factuality of text generated by large language models (LLMs) and developing a benchmark for detecting fact-conflicting hallucinations in these models.</li>
<li>methods: The paper introduces a new benchmark called FactCHD, which assimilates a large-scale dataset of factuality patterns and incorporates fact-based chains of evidence to facilitate comprehensive factual reasoning. The authors also present a new method called TRUTH-TRIANGULATOR, which synthesizes reflective considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2 to yield more credible detection.</li>
<li>results: The paper demonstrates the effectiveness of the FactCHD benchmark and shows that current methods fall short of faithfully detecting factual errors. The authors also present results from using TRUTH-TRIANGULATOR, which shows improved detection performance compared to existing methods.<details>
<summary>Abstract</summary>
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating comprehensive and conducive factual reasoning throughout the assessment process. We evaluate multiple LLMs, demonstrating the effectiveness of the benchmark and current methods fall short of faithfully detecting factual errors. Furthermore, we present TRUTH-TRIANGULATOR that synthesizes reflective considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming to yield more credible detection through the amalgamation of predictive results and evidence. The benchmark dataset and source code will be made available in https://github.com/zjunlp/FactCHD.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs），如ChatGPT/GPT-4，在实际应用方面引起了广泛关注，但其普及受到了网络平台上的事实冲突报告的限制。评估 LLMS 生成的文本中的事实真实性仍然不充分探讨，包括单纯的事实以及在复杂的推理任务中出现的事实错误。为此，我们提出了 FactCHD，一个特别设计 для LLMS 的事实冲突报告 benchmark。作为评估“查询-回答”上的事实真实性的重要工具，我们的 benchmark 集成了一个大规模的数据集，包括多种事实真实性模式，如简单、多步、比较和集成模式。我们的 benchmark 的一个特点是通过 incorporating fact-based chains of evidence，以便在评估过程中进行全面和有利的事实理解。我们测试了多个 LLMS，并证明了我们的 benchmark 和现有方法无法准确检测事实错误。此外，我们还提出了 TRUTH-TRIANGULATOR，一种基于 tool-enhanced ChatGPT 和 LoRA-tuning 的 Llama2 的方法，以便通过合并预测结果和证据来提供更可靠的检测。我们的 benchmark 数据集和源代码将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="DHOT-GM-Robust-Graph-Matching-Using-A-Differentiable-Hierarchical-Optimal-Transport-Framework"><a href="#DHOT-GM-Robust-Graph-Matching-Using-A-Differentiable-Hierarchical-Optimal-Transport-Framework" class="headerlink" title="DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework"></a>DHOT-GM: Robust Graph Matching Using A Differentiable Hierarchical Optimal Transport Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12081">http://arxiv.org/abs/2310.12081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Cheng, Dixin Luo, Hongteng Xu</li>
<li>for: 本研究旨在提出一种新的图像匹配方法，用于更好地利用图像中的多Modal信息，提高图像匹配的精度和效率。</li>
<li>methods: 本方法基于一种可导的层次优化交通（HOT）框架，使用各种modal信息来匹配图像。具体来说，我们将每个图像表示为一组相关矩阵，其中每个矩阵代表图像中不同modal信息的信息。然后，我们对两个图像进行匹配，并使用优化交通距离来衡量匹配结果。</li>
<li>results: 我们通过对多个图像匹配任务进行实验，发现我们的方法比前 существу的方法更高效和更稳定。在匹配过程中，我们可以通过调整可导的优化交通距离来控制匹配的精度和稳定性。<details>
<summary>Abstract</summary>
Graph matching is one of the most significant graph analytic tasks in practice, which aims to find the node correspondence across different graphs. Most existing approaches rely on adjacency matrices or node embeddings when matching graphs, whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. In this study, we propose a novel and effective graph matching method based on a differentiable hierarchical optimal transport (HOT) framework, called DHOT-GM. Essentially, our method represents each graph as a set of relational matrices corresponding to the information of different modalities. Given two graphs, we enumerate all relational matrix pairs and obtain their matching results, and accordingly, infer the node correspondence by the weighted averaging of the matching results. This method can be implemented as computing the HOT distance between the two graphs -- each matching result is an optimal transport plan associated with the Gromov-Wasserstein (GW) distance between two relational matrices, and the weights of all matching results are the elements of an upper-level optimal transport plan defined on the matrix sets. We propose a bi-level optimization algorithm to compute the HOT distance in a differentiable way, making the significance of the relational matrices adjustable. Experiments on various graph matching tasks demonstrate the superiority and robustness of our method compared to state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
GRAPH MATCHING 是一个非常重要的图分析任务，旨在找到不同图中节点的对应关系。现有的大多数方法都基于图邻接矩阵或节点嵌入， whose performances are often sub-optimal because of not fully leveraging the multi-modal information hidden in graphs, such as node attributes, subgraph structures, etc. 在这种研究中，我们提出了一种新的和有效的图 matching方法，基于可微的层次优先 transport（HOT）框架，称为DHOT-GM。本方法将每个图表示为不同特征Modalities的信息的集合。给定两个图，我们会枚举所有的关系矩阵对，并计算它们的匹配结果，然后根据匹配结果的权重，进行节点对应。这种方法可以视为计算HOT距离 между两个图，每个匹配结果是一个优先transport plan相关的Gromov-Wasserstein（GW）距离 между两个关系矩阵，并且权重的所有匹配结果的元素是一个上级优先transport plan定义在矩阵集上。我们提出了一种二级优化算法来计算HOT距离，使得关系矩阵的重要性可调。实验结果表明，我们的方法比现有的方法更高效和Robust。
</details></li>
</ul>
<hr>
<h2 id="Black-Box-Training-Data-Identification-in-GANs-via-Detector-Networks"><a href="#Black-Box-Training-Data-Identification-in-GANs-via-Detector-Networks" class="headerlink" title="Black-Box Training Data Identification in GANs via Detector Networks"></a>Black-Box Training Data Identification in GANs via Detector Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12063">http://arxiv.org/abs/2310.12063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukman Olagoke, Salil Vadhan, Seth Neel</li>
<li>for: 本研究探讨了使用生成对抗网络（GAN）时的隐私问题，特别是在黑盒Setting下（即只有 generator 的样本）。</li>
<li>methods: 我们提出了一系列的会员推测攻击，包括一种名为“检测器”的攻击，它通过训练一个第二个网络来评估样本的生成者生成的可能性。</li>
<li>results: 我们在多种图像和表格数据集上，以及不同的攻击和 GAN 架构上，发现了非常有趣的隐私攻击。然而，与其他生成和分类模型相比，GAN 的攻击成功率仍然相对较低。这留下了一个有趣的问题：是 GAN 更加隐私，或者需要更强的攻击？<details>
<summary>Abstract</summary>
Since their inception Generative Adversarial Networks (GANs) have been popular generative models across images, audio, video, and tabular data. In this paper we study whether given access to a trained GAN, as well as fresh samples from the underlying distribution, if it is possible for an attacker to efficiently identify if a given point is a member of the GAN's training data. This is of interest for both reasons related to copyright, where a user may want to determine if their copyrighted data has been used to train a GAN, and in the study of data privacy, where the ability to detect training set membership is known as a membership inference attack. Unlike the majority of prior work this paper investigates the privacy implications of using GANs in black-box settings, where the attack only has access to samples from the generator, rather than access to the discriminator as well. We introduce a suite of membership inference attacks against GANs in the black-box setting and evaluate our attacks on image GANs trained on the CIFAR10 dataset and tabular GANs trained on genomic data. Our most successful attack, called The Detector, involve training a second network to score samples based on their likelihood of being generated by the GAN, as opposed to a fresh sample from the distribution. We prove under a simple model of the generator that the detector is an approximately optimal membership inference attack. Across a wide range of tabular and image datasets, attacks, and GAN architectures, we find that adversaries can orchestrate non-trivial privacy attacks when provided with access to samples from the generator. At the same time, the attack success achievable against GANs still appears to be lower compared to other generative and discriminative models; this leaves the intriguing open question of whether GANs are in fact more private, or if it is a matter of developing stronger attacks.
</details>
<details>
<summary>摘要</summary>
自它们的出现以来，生成对抗网络（GANs）已成为图像、音频、视频和表格数据上广泛使用的生成模型。在这篇论文中，我们研究了给定一个已经训练过GAN的攻击者，以及新的样本从下面分布中获得的情况下，是否可以高效地判断一个点是否属于GAN的训练数据。这对于版权和数据隐私具有重要的意义，因为用户可能想要确定他们的版权数据是否被用来训练GAN，而且在数据隐私方面，能够检测训练集成员是一种称为会员推理攻击的能力。与大多数前期工作不同，本文 investigate GANs在黑盒设置下的隐私问题，攻击者只有Generator的样本而不具有Discriminator的访问权。我们介绍了一组黑盒成员推理攻击，并对图像GAN在CIFAR10数据集和表格GAN在生物数据集进行了评估。我们最成功的攻击方法叫做检测器，它通过训练一个第二个网络来评估样本是否由GAN生成，而不是一个新的样本从分布中。我们证明在一个简单的生成器模型下，检测器是一种相对优化的会员推理攻击。在各种图像和表格数据集、攻击和GAN架构下，我们发现攻击者可以通过Generator的样本进行非常复杂的隐私攻击。尽管GANs在隐私方面的攻击仍然比其他生成和判断模型低，但这仍然留下了一个惊喜的问题：GANs是否更安全，或者是需要更强的攻击。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Nutrient-Application’s-Timeline-Recommendation-for-Smart-Agriculture-A-Large-Scale-Data-Mining-Approach"><a href="#Machine-Learning-based-Nutrient-Application’s-Timeline-Recommendation-for-Smart-Agriculture-A-Large-Scale-Data-Mining-Approach" class="headerlink" title="Machine Learning-based Nutrient Application’s Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach"></a>Machine Learning-based Nutrient Application’s Timeline Recommendation for Smart Agriculture: A Large-Scale Data Mining Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12052">http://arxiv.org/abs/2310.12052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Usama Ikhlaq, Tahar Kechadi</li>
<li>for: 这项研究旨在提供一种可预测肥料应用量的解决方案，以便更好地管理投用肥料，降低成本、保护环境。</li>
<li>methods: 该研究使用大规模不同数据类型的数据集，通过分析这些数据来预测肥料应用量。研究还涉及到肥料应用量和天气数据的相互作用对作物产量的影响。</li>
<li>results: 研究发现，基于天气和土壤特点进行调整的肥料应用量可以提高作物产量，同时降低肥料投用量。该方法也被证明可靠和可扩展。<details>
<summary>Abstract</summary>
This study addresses the vital role of data analytics in monitoring fertiliser applications in crop cultivation. Inaccurate fertiliser application decisions can lead to costly consequences, hinder food production, and cause environmental harm. We propose a solution to predict nutrient application by determining required fertiliser quantities for an entire season. The proposed solution recommends adjusting fertiliser amounts based on weather conditions and soil characteristics to promote cost-effective and environmentally friendly agriculture. The collected dataset is high-dimensional and heterogeneous. Our research examines large-scale heterogeneous datasets in the context of the decision-making process, encompassing data collection and analysis. We also study the impact of fertiliser applications combined with weather data on crop yield, using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aspire to stabilise or even reduce the demand for agricultural nutrients while enhancing crop development. The proposed approach is proven to be efficient and scalable, as it is validated using a real-world and large dataset.
</details>
<details>
<summary>摘要</summary>
The dataset used in this study is high-dimensional and heterogeneous, and we examine the impact of fertilizer applications combined with weather data on crop yield using the winter wheat crop as a case study. By understanding local contextual and geographic factors, we aim to stabilize or even reduce the demand for agricultural nutrients while enhancing crop development.Our proposed approach is efficient and scalable, as it is validated using a real-world and large dataset. This study demonstrates the potential of data analytics in optimizing fertilizer applications and promoting sustainable agriculture practices.
</details></li>
</ul>
<hr>
<h2 id="Is-Channel-Independent-strategy-optimal-for-Time-Series-Forecasting"><a href="#Is-Channel-Independent-strategy-optimal-for-Time-Series-Forecasting" class="headerlink" title="Is Channel Independent strategy optimal for Time Series Forecasting?"></a>Is Channel Independent strategy optimal for Time Series Forecasting?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17658">http://arxiv.org/abs/2310.17658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Peiwen, Zhu Changsheng</li>
<li>for: 这篇论文是为了探讨适用于长期时间序列预测的不同模型。</li>
<li>methods: 这篇论文提出了一种简单 yet effective的策略called Channel Self-Clustering (CSC)，用于线性模型。此外，它还提出了Channel Rearrangement (CR)方法，用于深度模型。</li>
<li>results: 这篇论文的实验结果显示，CSC策略可以提高CI策略的性能，同时减少参数的数量，例如在电力集成数据集上减少了10倍以上。CR方法也可以与基准模型竞争。此外，论文还讨论了是否使用历史时间序列中的同一个通道的历史值来预测未来值。<details>
<summary>Abstract</summary>
There has been an emergence of various models for long-term time series forecasting. Recent studies have demonstrated that a single linear layer, using Channel Dependent (CD) or Channel Independent (CI) modeling, can even outperform a large number of sophisticated models. However, current research primarily considers CD and CI as two complementary yet mutually exclusive approaches, unable to harness these two extremes simultaneously. And it is also a challenging issue that both CD and CI are static strategies that cannot be determined to be optimal for a specific dataset without extensive experiments. In this paper, we reconsider whether the current CI strategy is the best solution for time series forecasting. First, we propose a simple yet effective strategy called CSC, which stands for $\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel Self-Clustering (CSC) enhances CI strategy's performance improvements while reducing parameter size, for exmpale by over 10 times on electricity dataset, and significantly cutting training time. Second, we further propose Channel Rearrangement (CR), a method for deep models inspired by the self-clustering. CR attains competitive performance against baselines. Finally, we also discuss whether it is best to forecast the future values using the historical values of the same channel as inputs. We hope our findings and methods could inspire new solutions beyond CD/CI.
</details>
<details>
<summary>摘要</summary>
有些新的模型已经出现了用于长期时间序预测。最近的研究表明，一个单一的线性层，使用通道依赖（CD）或通道独立（CI）的方法，可以超越许多复杂的模型。然而，当前的研究主要考虑CD和CI为两种 complementary yet mutually exclusive的方法，无法同时挖掘这两种极点。此外，CD和CI都是静态策略，无法确定是特定数据集的优化方法。在这篇论文中，我们重新考虑现在CI策略是时间序预测的最佳解决方案。首先，我们提出了一种简单 yet effective的策略，称为$\mathbf{C}$hannel $\mathbf{S}$elf-$\mathbf{C}$lustering（CSC）策略，用于线性模型。我们的通道自我凝聚（CSC）策略可以提高CI策略的性能改进，同时减少参数大小，例如在电力数据集上减少了10倍以上。其次，我们还提出了一种受到自我凝聚启发的深度模型策略，称为通道重新排序（CR）策略。CR策略可以与基elines相比。最后，我们还讨论了是否应该使用历史值的同一个通道作为预测未来值的输入。我们希望我们的发现和方法可以激发新的解决方案 beyond CD/CI。
</details></li>
</ul>
<hr>
<h2 id="A-General-Theoretical-Paradigm-to-Understand-Learning-from-Human-Preferences"><a href="#A-General-Theoretical-Paradigm-to-Understand-Learning-from-Human-Preferences" class="headerlink" title="A General Theoretical Paradigm to Understand Learning from Human Preferences"></a>A General Theoretical Paradigm to Understand Learning from Human Preferences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12036">http://arxiv.org/abs/2310.12036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, Rémi Munos</li>
<li>for: 本文旨在理解现代学习从人类偏好中学习（RLHF）的实际算法。</li>
<li>methods: 本文使用直接偏好优化（DPO）方法，并进行了深入的理论分析。</li>
<li>results: 研究发现，使用新的通用目标函数$\Psi$PO可以减少RLHF和DPO中的两个重要假设，并且可以提供更多的性能保证。此外，在一些示例中，使用 $\Psi$PO 可以实现更高的效率和更好的表现。<details>
<summary>Abstract</summary>
The prevalent deployment of learning from human preferences through reinforcement learning (RLHF) relies on two important approximations: the first assumes that pairwise preferences can be substituted with pointwise rewards. The second assumes that a reward model trained on these pointwise rewards can generalize from collected data to out-of-distribution data sampled by the policy. Recently, Direct Preference Optimisation (DPO) has been proposed as an approach that bypasses the second approximation and learn directly a policy from collected data without the reward modelling stage. However, this method still heavily relies on the first approximation.   In this paper we try to gain a deeper theoretical understanding of these practical algorithms. In particular we derive a new general objective called $\Psi$PO for learning from human preferences that is expressed in terms of pairwise preferences and therefore bypasses both approximations. This new general objective allows us to perform an in-depth analysis of the behavior of RLHF and DPO (as special cases of $\Psi$PO) and to identify their potential pitfalls. We then consider another special case for $\Psi$PO by setting $\Psi$ simply to Identity, for which we can derive an efficient optimisation procedure, prove performance guarantees and demonstrate its empirical superiority to DPO on some illustrative examples.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字的中文。<</SYS>>现有的学习从人类偏好（RLHF）的广泛部署都 rely 于两个重要的近似：第一个假设每个对比的偏好可以被替换为点 wise 奖励。第二个假设一个基于这些点 wise 奖励的奖励模型可以从收集的数据中泛化到非收集数据。 reciently，Direct Preference Optimization（DPO）被提出，它不需要奖励模型的训练阶段，直接从收集数据中学习策略。然而，这个方法仍然很重视第一个近似。 在这篇论文中，我们尝试了更深入的理论理解这些实际算法。我们 derive 了一个新的通用目标函数called $\Psi$PO，它表示在对比上学习人类偏好的情况下，不需要两个近似。这个新的通用目标函数允许我们对 RLHF 和 DPO（作为 $\Psi$PO 的特殊情况）进行深入的分析，并识别它们的潜在弱点。然后，我们考虑了 $\Psi$PO 中 $\Psi$ 设置为标识函数的特殊情况，可以 derive 一个高效的优化过程，证明性能保证和在一些示例中证明其超越 DPO 的实际性。
</details></li>
</ul>
<hr>
<h2 id="SegmATRon-Embodied-Adaptive-Semantic-Segmentation-for-Indoor-Environment"><a href="#SegmATRon-Embodied-Adaptive-Semantic-Segmentation-for-Indoor-Environment" class="headerlink" title="SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment"></a>SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12031">http://arxiv.org/abs/2310.12031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wingrune/segmatron">https://github.com/wingrune/segmatron</a></li>
<li>paper_authors: Tatiana Zemskova, Margarita Kichik, Dmitry Yudin, Aleksei Staroverov, Aleksandr Panov</li>
<li>for: 这篇论文是为了提出一种适应器模型，用于在具有物体意义的图像 semantic segmentation 中进行图像分割。</li>
<li>methods: 这篇论文使用了一种混合多组件损失函数来适应模型参数在多个图像中进行INF 时的调整。</li>
<li>results: 研究表明，通过使用代理人的行动在室内环境中获取更多图像，可以提高 semantic segmentation 的质量。Here’s the breakdown of each point in English:</li>
<li>for: The paper proposes an adaptive transformer model for embodied image semantic segmentation.</li>
<li>methods: The paper uses a hybrid multicomponent loss function to adapt the model weights during inference on multiple images.</li>
<li>results: The study shows that obtaining additional images using the agent’s actions in an indoor environment can improve the quality of semantic segmentation.<details>
<summary>Abstract</summary>
This paper presents an adaptive transformer model named SegmATRon for embodied image semantic segmentation. Its distinctive feature is the adaptation of model weights during inference on several images using a hybrid multicomponent loss function. We studied this model on datasets collected in the photorealistic Habitat and the synthetic AI2-THOR Simulators. We showed that obtaining additional images using the agent's actions in an indoor environment can improve the quality of semantic segmentation. The code of the proposed approach and datasets are publicly available at https://github.com/wingrune/SegmATRon.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种适应器模型，名为SegmATRon，用于具体图像 semantic segmentation。它的特点是在推理过程中对模型参数进行适应，使用混合多组件损失函数。我们在 Habitat 和 AI2-THOR  sintética simulators 上进行了研究，并证明了通过使用机器人的行为在室内环境中获取更多图像可以提高 semantic segmentation 的质量。代码和数据集可以在 https://github.com/wingrune/SegmATRon 上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Multi-view-Contrastive-Learning-for-Entity-Typing-over-Knowledge-Graphs"><a href="#Multi-view-Contrastive-Learning-for-Entity-Typing-over-Knowledge-Graphs" class="headerlink" title="Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs"></a>Multi-view Contrastive Learning for Entity Typing over Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12008">http://arxiv.org/abs/2310.12008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhiweihu1103/et-mclet">https://github.com/zhiweihu1103/et-mclet</a></li>
<li>paper_authors: Zhiwei Hu, Víctor Gutiérrez-Basulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan</li>
<li>for: 本文旨在提出一种新的知识 graphs 实体类型推断方法（MCLET），以更好地编码知识图中实体和类型的Semantic知识。</li>
<li>methods:  MCLET 方法包括三个模块：一、多视图生成和编码模块，使实体和类型的抽象信息从不同视图得到更好的表示；二、相互视图对比学习模块，使不同视图之间的表示进行协同改进；三、实体类型预测模块，通过多头注意力和混合专家策略来预测缺失的实体类型。</li>
<li>results:  compared to the state-of-the-art, MCLET 方法在实验中显示出了强大的表现。<details>
<summary>Abstract</summary>
Knowledge graph entity typing (KGET) aims at inferring plausible types of entities in knowledge graphs. Existing approaches to KGET focus on how to better encode the knowledge provided by the neighbors and types of an entity into its representation. However, they ignore the semantic knowledge provided by the way in which types can be clustered together. In this paper, we propose a novel method called Multi-view Contrastive Learning for knowledge graph Entity Typing (MCLET), which effectively encodes the coarse-grained knowledge provided by clusters into entity and type embeddings. MCLET is composed of three modules: i) Multi-view Generation and Encoder module, which encodes structured information from entity-type, entity-cluster and cluster-type views; ii) Cross-view Contrastive Learning module, which encourages different views to collaboratively improve view-specific representations of entities and types; iii) Entity Typing Prediction module, which integrates multi-head attention and a Mixture-of-Experts strategy to infer missing entity types. Extensive experiments show the strong performance of MCLET compared to the state-of-the-art
</details>
<details>
<summary>摘要</summary>
知识图Entity类型推断（KGET）目标在于推断知识图中实体的可能性类型。现有的KGET方法主要关注如何更好地编码实体周围的知识和类型到其表示中。然而，它们忽略了实体和类型之间的 semantics 知识，即类型之间的聚合知识。本文提出了一种新的方法called Multi-view Contrastive Learning for knowledge graph Entity Typing（MCLET），它可以有效地编码实体和类型的聚合知识到实体和类型表示中。MCLET包括以下三个模块：1. 多视图生成和编码模块（Multi-view Generation and Encoder module）：编码实体-类型、实体-团队和团队-类型的结构信息。2. 交叉视图对比学习模块（Cross-view Contrastive Learning module）：鼓励不同视图之间的信息进行协同改进视图特定的实体和类型表示。3. 实体类型预测模块（Entity Typing Prediction module）：通过多头注意力和 Mixture-of-Experts 策略来预测缺失的实体类型。广泛的实验表明MCLET在比较顶尖方法的情况下显示出了强大的表现。
</details></li>
</ul>
<hr>
<h2 id="KI-PMF-Knowledge-Integrated-Plausible-Motion-Forecasting"><a href="#KI-PMF-Knowledge-Integrated-Plausible-Motion-Forecasting" class="headerlink" title="KI-PMF: Knowledge Integrated Plausible Motion Forecasting"></a>KI-PMF: Knowledge Integrated Plausible Motion Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12007">http://arxiv.org/abs/2310.12007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Vivekanandan, Ahmed Abouelazm, Philip Schörner, J. Marius Zöllner</li>
<li>for: 预测交通actor的运动轨迹，以实现自动驾驶车辆大规模部署。</li>
<li>methods: 引入非 Parametric 剪除层和注意力层，以整合定义的知识优化。</li>
<li>results: 实现了遵循物理法律和驾驶环境几何学的轨迹预测，提供了安全可靠的运动预测结果，是实现自动驾驶车辆安全有效的关键。<details>
<summary>Abstract</summary>
Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constraints of a vehicle and the geometry of the driving environment. To achieve this, we introduce a non-parametric pruning layer and attention layers to integrate the defined knowledge priors. Our proposed method is designed to ensure reachability guarantees for traffic actors in both complex and dynamic situations. By conditioning the network to follow physical laws, we can obtain accurate and safe predictions, essential for maintaining autonomous vehicles' safety and efficiency in real-world settings.In summary, this paper presents concepts that prevent off-road predictions for safe and reliable motion forecasting by incorporating knowledge priors into the training process.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对于大规模自动驾驶 vehicles 的部署， точно预测交通actor 的运动是非常重要的。当前的轨迹预测方法主要集中在优化特定的损失函数中，可能会导致预测不符合物理法律或违反外部约束。我们的目标是将显式知识假设纳入网络中，以预测未来的轨迹，并且遵循车辆的动力学约束和驾驶环境的几何结构。为 achieve 这一目标，我们引入非参数化剪除层和注意层，以整合定义的知识假设。我们的提出方法旨在保证交通actor 的可达性，并在复杂和动态情况下提供可靠的预测。通过使网络遵循物理法律，我们可以获得高度准确和安全的预测结果，这是保持自动驾驶 vehicle 的安全和效率在实际场景中的关键。总之，本文提出了避免脱离路径的安全和可靠轨迹预测方法，通过在训练过程中纳入知识假设。
</details></li>
</ul>
<hr>
<h2 id="Sociotechnical-Safety-Evaluation-of-Generative-AI-Systems"><a href="#Sociotechnical-Safety-Evaluation-of-Generative-AI-Systems" class="headerlink" title="Sociotechnical Safety Evaluation of Generative AI Systems"></a>Sociotechnical Safety Evaluation of Generative AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11986">http://arxiv.org/abs/2310.11986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, William Isaac</li>
<li>for: 评估生成AI系统的安全性</li>
<li>methods: 提出三层框架，包括能力评估、系统安全原则和人类互动的评估</li>
<li>results: 发现现有评估缺陷，并提出了解决方案，包括实践步骤和不同角色的责任<details>
<summary>Abstract</summary>
Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.
</details>
<details>
<summary>摘要</summary>
生成AI系统的应用涉及到一系列的风险。为确保生成AI系统的安全，这些风险必须进行评估。在这篇论文中，我们提出了两个主要贡献，以便建立这些评估。首先，我们提议一种三层结构的框架，它采用一种结构化的社会技术方法来评估这些风险。这个框架包括功能评估，这是当前主要的安全评估方法。然后，我们又基于系统安全原则，尤其是认为上下文决定了一个给定的功能是否会带来害。为了考虑相关的上下文，我们的框架添加了人机交互和系统影响作为其他两个层次评估。其次，我们对生成AI系统的安全评估状况进行了调查，并创建了一个库存的评估。三个突出的评估漏洞出现在这种分析中。我们提出了方法来填充这些漏洞，并详细介绍了不同角色的角色和责任。社会技术安全评估是一种可行的方法，以确保生成AI系统的安全评估是全面和可靠的。
</details></li>
</ul>
<hr>
<h2 id="InfoDiffusion-Information-Entropy-Aware-Diffusion-Process-for-Non-Autoregressive-Text-Generation"><a href="#InfoDiffusion-Information-Entropy-Aware-Diffusion-Process-for-Non-Autoregressive-Text-Generation" class="headerlink" title="InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation"></a>InfoDiffusion: Information Entropy Aware Diffusion Process for Non-Autoregressive Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11976">http://arxiv.org/abs/2310.11976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rzhwang/infodiffusion">https://github.com/rzhwang/infodiffusion</a></li>
<li>paper_authors: Renzhi Wang, Jing Li, Piji Li</li>
<li>for: 提高文本生成质量和多样性，尝试 bridge 人类自然文本生成过程和当前扩散模型的生成过程之间的差距。</li>
<li>methods:  InfoDiffusion 使用 “keyinfo-first” 生成策略，并在不同文本信息量下应用噪音调度。另外，InfoDiffusion 还结合了自我条件和新提出的部分噪音模型结构。</li>
<li>results: InfoDiffusion 在生成质量和多样性方面表现出色，同时 sampling efficiency 也高于基eline模型。<details>
<summary>Abstract</summary>
Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the "easy-first" text generation process of current diffusion models and the "keyword-first" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a "keyinfo-first" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency.
</details>
<details>
<summary>摘要</summary>
diffusion模型在文本生成领域已经引起了广泛的关注。多个研究已经探索了不同结构的 diffusion模型，并应用于名称识别和概要summarization等任务。然而，现有的"易先"文本生成过程和人类的"关键词-先"自然文本生成过程之间存在显著的差距，这一点很少得到了关注。为了bridging这个差距，我们提出了InfoDiffusion，一种非自适应文本diffusion模型。我们的方法采用了"关键信息-先"生成策略，并在文本信息的量 bases on a noise schedule。此外，InfoDiffusion还结合了自conditioning和一种新提出的部分噪音模型结构。实验结果表明，InfoDiffusion在生成质量和多样性方面都超过了基eline模型，同时也显示了更高的采样效率。
</details></li>
</ul>
<hr>
<h2 id="Improving-Generalization-of-Alignment-with-Human-Preferences-through-Group-Invariant-Learning"><a href="#Improving-Generalization-of-Alignment-with-Human-Preferences-through-Group-Invariant-Learning" class="headerlink" title="Improving Generalization of Alignment with Human Preferences through Group Invariant Learning"></a>Improving Generalization of Alignment with Human Preferences through Group Invariant Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11971">http://arxiv.org/abs/2310.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Zheng, Wei Shen, Yuan Hua, Wenbin Lai, Shihan Dou, Yuhao Zhou, Zhiheng Xi, Xiao Wang, Haoran Huang, Tao Gui, Qi Zhang, Xuanjing Huang</li>
<li>for: This paper aims to improve the stability and generalization of AI assistants based on language models (LLMs) by proposing a novel approach to Reinforcement Learning from Human Feedback (RLHF).</li>
<li>methods: The proposed approach uses a combination of data classification and adaptive exploration to learn a consistent policy across various domains. It deliberately maximizes performance variance and allocates more learning capacity to challenging data.</li>
<li>results: The experimental results show that the proposed approach significantly enhances training stability and model generalization, outperforming traditional RL methods that exploit shortcuts and overlook challenging samples.<details>
<summary>Abstract</summary>
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.
</details>
<details>
<summary>摘要</summary>
成功的语言模型基于AI助手（LLMs）取决于人类反馈学习（RLHF），它使得生成响应更加与人类偏好相align。作为普遍的AI助手，人们对它们的表现具有增加的期望，但previous work表明，使用学习策略（RL）时，经常利用短cut掉到达高奖励，而忽略了复杂的样本。这种围绕快速奖励的专注会使训练不稳定并导致模型无法在新、未看到的数据上Generalize。在这项工作中，我们提出了一种新的方法，可以通过RL在不同的数据组或领域中学习一个一致的策略。由于获得组注释的困难，我们的方法会自动将数据分类为不同的组，故意增加性能的变化。然后，我们会优化策略，以便在复杂的组中表现好。最后，我们通过已有的组来适应性地调整探索空间，将更多的学习资源分配给更加复杂的数据，避免模型在简单的数据上过度优化。实验结果表明，我们的方法可以显著提高训练稳定性和模型泛化。
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Scale-Decomposition-MLP-Mixer-for-Time-Series-Analysis"><a href="#A-Multi-Scale-Decomposition-MLP-Mixer-for-Time-Series-Analysis" class="headerlink" title="A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis"></a>A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11959">http://arxiv.org/abs/2310.11959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zshhans/msd-mixer">https://github.com/zshhans/msd-mixer</a></li>
<li>paper_authors: Shuhan Zhong, Sizhe Song, Guanyao Li, Weipeng Zhuo, Yang Liu, S. -H. Gary Chan</li>
<li>for: 这篇论文是为了提出一种基于多尺度分解和多层混合的深度学习方法，以更好地处理时间序列数据的特殊特点和复杂多尺度时间变化。</li>
<li>methods: 这篇论文使用了一种叫做Multi-Scale Decomposition MLP-Mixer（MSD-Mixer）的方法，它可以输出不同层次的时间序列分解成分，并将这些分解成分与不同层次的时间序列进行混合，以捕捉时间序列的多尺度特征和互相关联。</li>
<li>results: 这篇论文的实验结果显示，使用MSD-Mixer方法可以在不同的时间序列分析任务（长期和短期预测、填充、侦测异常和分类）中，与其他现有的任务特定和任务通用方法相比，实现了更好的性能。<details>
<summary>Abstract</summary>
Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition completeness. Through extensive experiments on various real-world datasets for five common time series analysis tasks (long- and short-term forecasting, imputation, anomaly detection, and classification), we demonstrate that MSD-Mixer consistently achieves significantly better performance in comparison with other state-of-the-art task-general and task-specific approaches.
</details>
<details>
<summary>摘要</summary>
时间序列数据，经常具有独特的组成和复杂的多尺度时间变化，需要特殊地对它进行分解和多尺度模型化的分析。现有的深度学习方法只适用于单变量时间序列，并未充分考虑分解完整性和子序列水平的模型化。为解决这个问题，我们提议了MSD-Mixer，一种多尺度分解MLP-Mixer，可以显式地将输入时间序列分解成不同的组成部分，并将这些部分在不同层中表示。同时，我们提出了一种新的时间补丁方法，可以模型时间序列为多尺度子序列，即补丁，并使用MLP来混合内部和外部补丁的变化和通道之间的相关性。此外，我们还提出了一种约束分解 оста异量和自相关的损失函数，以确保分解完整性。经过对各种实际世界数据集的多种时间序列分析任务（长期和短期预测、填充、异常检测和分类）的广泛实验，我们示示了MSD-Mixer在与其他当前领域的任务特定和任务通用方法相比，具有显著更好的表现。
</details></li>
</ul>
<hr>
<h2 id="Too-Good-To-Be-True-performance-overestimation-in-re-current-practices-for-Human-Activity-Recognition"><a href="#Too-Good-To-Be-True-performance-overestimation-in-re-current-practices-for-Human-Activity-Recognition" class="headerlink" title="Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition"></a>Too Good To Be True: performance overestimation in (re)current practices for Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11950">http://arxiv.org/abs/2310.11950</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrés Tello, Victoria Degeler, Alexander Lazovik</li>
<li>for: The paper aims to raise awareness about the issue of accuracy overestimation in Human Activity Recognition (HAR) studies due to biased data segmentation and evaluation methods.</li>
<li>methods: The paper uses sliding windows for data segmentation and standard random k-fold cross validation, which are common approaches in state-of-the-art HAR studies, but can lead to biased results.</li>
<li>results: The paper shows that these biased methods can produce lower accuracies than correct unbiased methods, and that the problem persists independently of the method or dataset used.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文旨在提醒科学界关于人活动识别（HAR）研究中的准确度过估问题，具体来说是因为数据 segmentation和评估方法的偏见导致的。</li>
<li>methods: 这篇论文使用滑动窗口方法进行数据 segmentation，并使用标准随机k-fold交叉验证法，这些方法是当今HAR研究中最常用的，但可能导致偏见的结果。</li>
<li>results: 这篇论文显示，这些偏见方法可能会生成较低的准确度，而正确的不偏见方法更难在科学期刊上发表。<details>
<summary>Abstract</summary>
Today, there are standard and well established procedures within the Human Activity Recognition (HAR) pipeline. However, some of these conventional approaches lead to accuracy overestimation. In particular, sliding windows for data segmentation followed by standard random k-fold cross validation, produce biased results. An analysis of previous literature and present-day studies, surprisingly, shows that these are common approaches in state-of-the-art studies on HAR. It is important to raise awareness in the scientific community about this problem, whose negative effects are being overlooked. Otherwise, publications of biased results lead to papers that report lower accuracies, with correct unbiased methods, harder to publish. Several experiments with different types of datasets and different types of classification models allow us to exhibit the problem and show it persists independently of the method or dataset.
</details>
<details>
<summary>摘要</summary>
今天，人活动识别（HAR）管道中有标准化和确定的程序。然而，这些传统方法会导致准确性过估。具体来说，使用滑块窗口进行数据分割，然后使用标准随机k-叶值验证，会产生偏见结果。历史和当代研究的分析表明，这些是现代HAR研究中最常见的方法。重要的是，我们需要在科学社区中启示这个问题，以避免这种偏见的负面影响。否则，使用偏见方法的出版物将导致准确方法更难于发表。我们通过不同的数据集和不同的分类模型进行了多个实验，证明了这个问题的存在，并证明它独立于方法或数据集。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-for-Semi-Inductive-Link-Prediction-in-Knowledge-Graphs"><a href="#A-Benchmark-for-Semi-Inductive-Link-Prediction-in-Knowledge-Graphs" class="headerlink" title="A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs"></a>A Benchmark for Semi-Inductive Link Prediction in Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11917">http://arxiv.org/abs/2310.11917</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrian Kochsiek, Rainer Gemulla</li>
<li>for: 本文旨在提出和评估大规模知识图（KG）中 semi-inductive link prediction（LP）模型的一个大规模 benchmark。</li>
<li>methods: 本文使用了 Wikidata5M 作为基础，并提供了三种 LP 任务：推导式（k-shot）、辅助式（transductive）和零批式（0-shot），每种任务都有不同的可用信息，包括 KG 结构、文本提及和实体的详细描述。</li>
<li>results: 据小规模实验结果表明， semi-inductive LP 性能与辅助式 LP 性能在长尾实体上存在差异，并且 semi-inductive LP 性能远远低于辅助式 LP 性能。 这个 benchmark 为未来在 semi-inductive LP 模型中集成文本和上下文信息进行进一步研究提供了一个测试床。<details>
<summary>Abstract</summary>
Semi-inductive link prediction (LP) in knowledge graphs (KG) is the task of predicting facts for new, previously unseen entities based on context information. Although new entities can be integrated by retraining the model from scratch in principle, such an approach is infeasible for large-scale KGs, where retraining is expensive and new entities may arise frequently. In this paper, we propose and describe a large-scale benchmark to evaluate semi-inductive LP models. The benchmark is based on and extends Wikidata5M: It provides transductive, k-shot, and 0-shot LP tasks, each varying the available information from (i) only KG structure, to (ii) including textual mentions, and (iii) detailed descriptions of the entities. We report on a small study of recent approaches and found that semi-inductive LP performance is far from transductive performance on long-tail entities throughout all experiments. The benchmark provides a test bed for further research into integrating context and textual information in semi-inductive LP models.
</details>
<details>
<summary>摘要</summary>
《知识 graphs（KG）中的半推导链预测（LP）任务是预测新、未看过的实体上的事实，基于上下文信息。虽然新实体可以通过重新训练模型来扩展，但这种方法在大规模KG中是不可行的，因为重新训练是昂贵的并且新实体可能会频繁出现。在这篇论文中，我们提出了一个大规模的LP模型评估标准 benchmark。该 benchmark 基于并扩展了 Wikidata5M：它提供了半推导、k-shot、0-shot LP任务，每个任务都不同的提供KG结构、文本提及和实体的详细描述。我们对一些最新的方法进行了一小项研究，发现在长尾实体上，半推导LP性能与推导LP性能在所有实验中都远远不同。该 benchmark 提供了一个研究 semi-inductive LP 模型integrating上下文和文本信息的测试床。》Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Analyze-Mass-Spectrometry-data-with-Artificial-Intelligence-to-assist-the-understanding-of-past-habitability-of-Mars-and-provide-insights-for-future-missions"><a href="#Analyze-Mass-Spectrometry-data-with-Artificial-Intelligence-to-assist-the-understanding-of-past-habitability-of-Mars-and-provide-insights-for-future-missions" class="headerlink" title="Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions"></a>Analyze Mass Spectrometry data with Artificial Intelligence to assist the understanding of past habitability of Mars and provide insights for future missions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11888">http://arxiv.org/abs/2310.11888</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ioannisnasios/marsspectrometry2_gaschromatography">https://github.com/ioannisnasios/marsspectrometry2_gaschromatography</a></li>
<li>paper_authors: Ioannis Nasios</li>
<li>for: 这个研究用于检测古代火星是否可居住，但同时这种方法也可以应用于我们太阳系中的任何天体。</li>
<li>methods: 这个研究使用人工智能分析火星气相学数据，包括演化气相分析（EGA-MS）和气chromatography（GC-MS）两种技术，以确定古代火星样本中的特定化学物质。</li>
<li>results: 研究表明EGA-MS和GC-MS数据可以用于描述外星物质的化学成分，并且提供了一种可靠的方法来分析这些数据。<details>
<summary>Abstract</summary>
This paper presents an application of artificial intelligence on mass spectrometry data for detecting habitability potential of ancient Mars. Although data was collected for planet Mars the same approach can be replicated for any terrestrial object of our solar system. Furthermore, proposed methodology can be adapted to any domain that uses mass spectrometry. This research is focused in data analysis of two mass spectrometry techniques, evolved gas analysis (EGA-MS) and gas chromatography (GC-MS), which are used to identify specific chemical compounds in geological material samples. The study demonstrates the applicability of EGA-MS and GC-MS data to extra-terrestrial material analysis. Most important features of proposed methodology includes square root transformation of mass spectrometry values, conversion of raw data to 2D sprectrograms and utilization of specific machine learning models and techniques to avoid overfitting on relative small datasets. Both EGA-MS and GC-MS datasets come from NASA and two machine learning competitions that the author participated and exploited. Complete running code for the GC-MS dataset/competition is available at GitHub.1 Raw training mass spectrometry data include [0, 1] labels of specific chemical compounds, selected to provide valuable insights and contribute to our understanding of the potential past habitability of Mars.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="From-Neural-Activations-to-Concepts-A-Survey-on-Explaining-Concepts-in-Neural-Networks"><a href="#From-Neural-Activations-to-Concepts-A-Survey-on-Explaining-Concepts-in-Neural-Networks" class="headerlink" title="From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks"></a>From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11884">http://arxiv.org/abs/2310.11884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jae Hee Lee, Sergio Lanza, Stefan Wermter</li>
<li>for: 本文主要用于探讨现代神经网络中概念解释的方法。</li>
<li>methods: 本文使用了多种方法来解释神经网络中的概念，包括特征重要性分析、模型解释和概念映射等。</li>
<li>results: 本文通过对多种神经网络模型进行分析，发现了一些有用的概念解释方法，并且提出了一些可能的应用场景。这些结果可能为实现基于可解释概念的神经网络和符号学AI做出了重要贡献。<details>
<summary>Abstract</summary>
In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们对现代神经网络中的概念解释方法进行了评论。神经网络中的概念可以作为自然的链接连接学习和理解：一旦已经确定了神经学习系统使用的概念，那么可以将这些概念与符号系统集成，以进行推理或使用符号系统来改进或增强神经学习系统。同时，可以从神经网络中提取知识，同时也可以将符号知识插入到神经网络架构中。由于将学习和理解集成是神经 симвоlic AI 的核心，这些评论所获得的启示可以作为实现神经 симвоlic AI 基于可解释的概念的重要一步。
</details></li>
</ul>
<hr>
<h2 id="AI-Nushu-An-Exploration-of-Language-Emergence-in-Sisterhood-Through-the-Lens-of-Computational-Linguistics"><a href="#AI-Nushu-An-Exploration-of-Language-Emergence-in-Sisterhood-Through-the-Lens-of-Computational-Linguistics" class="headerlink" title="AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics"></a>AI Nushu: An Exploration of Language Emergence in Sisterhood -Through the Lens of Computational Linguistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11870">http://arxiv.org/abs/2310.11870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqian Sun, Yuying Tang, Ze Gao, Zhijun Pan, Chuyan Xu, Yurou Chen, Kejiang Qian, Zhigang Wang, Tristan Braud, Chang Hee Lee, Ali Asadipour</li>
<li>for: 这篇论文旨在探讨一种基于女性文化遗产的人工智能语言系统，即“AI Nushu”。</li>
<li>methods: 该论文使用了人工智能技术，将中文词典和女性文字资料库训练两个人工智能代理人，以便共同创建一种标准写作系统，用于编码中文。</li>
<li>results: 该研究提供了一种搭建在人工智能技术和中国文化遗产之上的艺术解读，以及一种将女性视角与计算语言学融合的新的视角。<details>
<summary>Abstract</summary>
This paper presents "AI Nushu," an emerging language system inspired by Nushu (women's scripts), the unique language created and used exclusively by ancient Chinese women who were thought to be illiterate under a patriarchal society. In this interactive installation, two artificial intelligence (AI) agents are trained in the Chinese dictionary and the Nushu corpus. By continually observing their environment and communicating, these agents collaborate towards creating a standard writing system to encode Chinese. It offers an artistic interpretation of the creation of a non-western script from a computational linguistics perspective, integrating AI technology with Chinese cultural heritage and a feminist viewpoint.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Genetic-Improvement-Mutations-Using-Large-Language-Models"><a href="#Enhancing-Genetic-Improvement-Mutations-Using-Large-Language-Models" class="headerlink" title="Enhancing Genetic Improvement Mutations Using Large Language Models"></a>Enhancing Genetic Improvement Mutations Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19813">http://arxiv.org/abs/2310.19813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander E. I. Brownlee, James Callan, Karine Even-Mendoza, Alina Geiger, Carol Hanna, Justyna Petke, Federica Sarro, Dominik Sobania</li>
<li>for: 本研究探讨了使用大语言模型（LLM）进行生成改进（Genetic Improvement，GI）的搜索技术。</li>
<li>methods: 本研究使用了OpenAI的API来生成JCodec工具的编辑。研究采用了5种不同的编辑类型进行随机抽样。</li>
<li>results: 研究发现，使用LLM生成的编辑可以提高单元测试通过率达到75%，但找到最佳改进的patch通常是通过标准插入编辑。此外，LLM增强的GI可以找到许多改进patch，但是最佳改进patch是通过标准GI找到的。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已成功应用于软件工程任务，包括程序修复。然而，它们在基于搜索的技术，如遗传改进（GI）中的应用仍然是未知之地。在这篇论文中，我们评估了使用 LLM 作为 GI 中的变异运算来改善搜索过程。我们扩展了 Gin Java GI 工具包，以调用 OpenAI 的 API 生成 JCodec 工具中的修改。我们随机采样了修改空间，使用 5 种不同的修改类型。我们发现，使用 LLM 生成的修改可以提高单元测试通过率达到 75%，而且发现 LLM 生成的修改通常比标准插入修改更加稳定。此外，我们发现使用 LLM 增强 GI 可以找到更好的改进补丁，但最佳改进补丁仍然由标准 GI 找到。
</details></li>
</ul>
<hr>
<h2 id="The-Value-Sensitive-Conversational-Agent-Co-Design-Framework"><a href="#The-Value-Sensitive-Conversational-Agent-Co-Design-Framework" class="headerlink" title="The Value-Sensitive Conversational Agent Co-Design Framework"></a>The Value-Sensitive Conversational Agent Co-Design Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11848">http://arxiv.org/abs/2310.11848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malak Sadek, Rafael A. Calvo, Celine Mougenot</li>
<li>for: 本研究旨在提出一个价值敏感对话代理（VSCA）框架，实现对价值敏感对话代理的共同设计（co-design）。</li>
<li>methods: 本研究使用了以前的研究中所识别的需求，以及一个实用的框架，包括一个设计工具组。</li>
<li>results: 本研究提出了一个评估协议，以评估框架和设计工具组在设计工作室中的效果。<details>
<summary>Abstract</summary>
Conversational agents (CAs) are gaining traction in both industry and academia, especially with the advent of generative AI and large language models. As these agents are used more broadly by members of the general public and take on a number of critical use cases and social roles, it becomes important to consider the values embedded in these systems. This consideration includes answering questions such as 'whose values get embedded in these agents?' and 'how do those values manifest in the agents being designed?' Accordingly, the aim of this paper is to present the Value-Sensitive Conversational Agent (VSCA) Framework for enabling the collaborative design (co-design) of value-sensitive CAs with relevant stakeholders. Firstly, requirements for co-designing value-sensitive CAs which were identified in previous works are summarised here. Secondly, the practical framework is presented and discussed, including its operationalisation into a design toolkit. The framework facilitates the co-design of three artefacts that elicit stakeholder values and have a technical utility to CA teams to guide CA implementation, enabling the creation of value-embodied CA prototypes. Finally, an evaluation protocol for the framework is proposed where the effects of the framework and toolkit are explored in a design workshop setting to evaluate both the process followed and the outcomes produced.
</details>
<details>
<summary>摘要</summary>
对话代理（CA）在工业和学术界受到推广，特别是在生成AI和大型自然语言模型的出现后。这些代理在公众中更加广泛使用，扮演许多重要的使用案和社会角色，因此需要考虑这些系统中嵌入的价值。因此，本文的目的是提出价值敏感对话代理（VSCA）框架，帮助专业人员和重要参与者在一起设计价值敏感CA。首先，以前的研究中所识别出的实现值敏感CA的需求简述了一下。其次，实际的框架被提出来，并讨论了它的实现方式。这个框架包括三个展示实物，吸引参与者的价值，并对CA团队提供技术实用性，帮助创建具有价值的CA原型。最后，为这个框架和工具组提出评估协议，以评估这个框架和工具组在设计工作室中的影响，以及它们创造的结果。
</details></li>
</ul>
<hr>
<h2 id="Masked-Pretraining-for-Multi-Agent-Decision-Making"><a href="#Masked-Pretraining-for-Multi-Agent-Decision-Making" class="headerlink" title="Masked Pretraining for Multi-Agent Decision Making"></a>Masked Pretraining for Multi-Agent Decision Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11846">http://arxiv.org/abs/2310.11846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Liu, Yinmin Zhang, Chuming Li, Chao Yang, Yaodong Yang, Yu Liu, Wanli Ouyang</li>
<li>for: 这篇论文主要针对多智能体决策问题，旨在建立一个通用智能体可以在零情况下完成决策。</li>
<li>methods: 该论文提出了一种基于trasformer架构的MaskMA模型，通过面具学习策略来解决多智能体设置下的困难。此外，该模型还实现了一种通用行为表示，可以在不同的智能体数量和动作空间下进行扩展。</li>
<li>results: 实验结果表明，使用MaskMA模型可以在11个训练地图上进行零情况下的赢利率达77.8%，并且在其他类型的下游任务中表现良好（如多策略协作和随机团队游戏）。<details>
<summary>Abstract</summary>
Building a single generalist agent with zero-shot capability has recently sparked significant advancements in decision-making. However, extending this capability to multi-agent scenarios presents challenges. Most current works struggle with zero-shot capabilities, due to two challenges particular to the multi-agent settings: a mismatch between centralized pretraining and decentralized execution, and varying agent numbers and action spaces, making it difficult to create generalizable representations across diverse downstream tasks. To overcome these challenges, we propose a \textbf{Mask}ed pretraining framework for \textbf{M}ulti-\textbf{a}gent decision making (MaskMA). This model, based on transformer architecture, employs a mask-based collaborative learning strategy suited for decentralized execution with partial observation. Moreover, MaskMA integrates a generalizable action representation by dividing the action space into actions toward self-information and actions related to other entities. This flexibility allows MaskMA to tackle tasks with varying agent numbers and thus different action spaces. Extensive experiments in SMAC reveal MaskMA, with a single model pretrained on 11 training maps, can achieve an impressive 77.8% zero-shot win rate on 60 unseen test maps by decentralized execution, while also performing effectively on other types of downstream tasks (\textit{e.g.,} varied policies collaboration and ad hoc team play).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Brain-decoding-toward-real-time-reconstruction-of-visual-perception"><a href="#Brain-decoding-toward-real-time-reconstruction-of-visual-perception" class="headerlink" title="Brain decoding: toward real-time reconstruction of visual perception"></a>Brain decoding: toward real-time reconstruction of visual perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19812">http://arxiv.org/abs/2310.19812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yohann Benchetrit, Hubert Banville, Jean-Rémi King</li>
<li>for: 这研究旨在实时解oding brain activity中的视觉过程</li>
<li>methods: 使用 magnetoencephalography (MEG) 和一种基于嵌入学习的解码模型</li>
<li>results: 1. MEG decoder 可以7倍提高图像检索率; 2. 脑响应图像具有高级视觉特征; 3. 图像检索和生成都表明MEG信号主要含有高级视觉特征, 而7T fMRI 则捕捉低级视觉特征。<details>
<summary>Abstract</summary>
In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain responses to images are best decoded with DINOv2, a recent foundational image model. Third, image retrievals and generations both suggest that MEG signals primarily contain high-level visual features, whereas the same approach applied to 7T fMRI also recovers low-level features. Overall, these results provide an important step towards the decoding - in real time - of the visual processes continuously unfolding within the human brain.
</details>
<details>
<summary>摘要</summary>
在过去五年，基于生成和基础AI系统的使用已经大幅提高了脑动力的解码。视觉认知特别是可以通过功能磁共振成像（fMRI）进行高度准确的解码。然而，这种神经成像技术受到时间分辨率的限制（约为0.5Hz），因此在实时应用中受到极大的限制。我们提出了一种备选方案，基于磁共振成像（MEG），这种神经成像设备可以在高时间分辨率（约为5000Hz）下测量脑动力。为此，我们开发了一个基于MEG的解码模型，其包括三个模块：i）预训练的嵌入，ii）基于MEG的练习结构，iii）预训练的图像生成器。我们的结果如下：首先，我们的MEG解码器与 классические线性解码器相比，图像检索的性能提高了7倍。其次，对于图像的晚期响应，DINOv2，一种最新的基础图像模型，表现最佳。最后，图像检索和生成都表明MEG信号主要含有高级视觉特征，而使用7T fMRI也能够恢复低级特征。总之，这些结果为实时解码人类大脑中不断发展的视觉过程提供了重要的一步。
</details></li>
</ul>
<hr>
<h2 id="Classification-Aggregation-without-Unanimity"><a href="#Classification-Aggregation-without-Unanimity" class="headerlink" title="Classification Aggregation without Unanimity"></a>Classification Aggregation without Unanimity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11841">http://arxiv.org/abs/2310.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivier Cailloux, Matthieu Hervouin, Ali I. Ozkes, M. Remzi Sanver</li>
<li>for: 这篇论文主要针对 классификация聚合函数的研究。</li>
<li>methods: 论文使用了 dictatorship 来描述每个公民独立的 классификация聚合函数。</li>
<li>results: 论文显示了每个独立和不同的类别聚合函数都是 dictatorship，这与 Maniquet 和 Mongin （2016）的结果相同。此外，论文还提出了一种新的证明方法，可以涵盖两个类别的情况，除非对象的数量也是两个。最后，论文还列出了两个类别和两个对象的所有独立和一致的 классификация聚合函数。<details>
<summary>Abstract</summary>
A classification is a surjective mapping from a set of objects to a set of categories. A classification aggregation function aggregates every vector of classifications into a single one. We show that every citizen sovereign and independent classification aggregation function is essentially a dictatorship. This impossibility implies an earlier result of Maniquet and Mongin (2016), who show that every unanimous and independent classification aggregation function is a dictatorship. The relationship between the two impossibilities is reminiscent to the relationship between Wilson's and Arrow's impossibilities in preference aggregation. Moreover, while the Maniquet-Mongin impossibility rests on the existence of at least three categories, we propose an alternative proof technique that covers the case of two categories, except when the number of objects is also two. We also identify all independent and unanimous classification aggregation functions for the case of two categories and two objects.
</details>
<details>
<summary>摘要</summary>
一种分类是一个射函数，将一个集合对象映射到另一个集合类别。一个分类汇聚函数将每个vector分类汇聚成一个单一的汇聚结果。我们表明，每个公民独立和自主的分类汇聚函数都是一种独裁统治。这一不可能性等价于 Earlier Maniquet 和 Mongin（2016）的结果，他们表明，每个一致和独立的分类汇聚函数都是一种独裁统治。这两个不可能性之间的关系与 Wilson 和 Arrow 的不可能性在偏好汇聚中有相似之处。此外，我们提出了一种不同的证明技巧，覆盖了三个类别的情况，而不是两个类别和两个对象的情况。我们还确定了所有独立和一致的分类汇聚函数的情况，只有两个类别和两个对象的情况例外。
</details></li>
</ul>
<hr>
<h2 id="IntentDial-An-Intent-Graph-based-Multi-Turn-Dialogue-System-with-Reasoning-Path-Visualization"><a href="#IntentDial-An-Intent-Graph-based-Multi-Turn-Dialogue-System-with-Reasoning-Path-Visualization" class="headerlink" title="IntentDial: An Intent Graph based Multi-Turn Dialogue System with Reasoning Path Visualization"></a>IntentDial: An Intent Graph based Multi-Turn Dialogue System with Reasoning Path Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11818">http://arxiv.org/abs/2310.11818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengguang Hao, Jie Zhang, Binxia Xu, Yafang Wang, Gerard de Melo, Xiaolong Li</li>
<li>for: 本研究旨在提高对话系统的听众感知和响应能力，使其能够更好地理解用户的意图和需求。</li>
<li>methods: 该研究提出了一种基于图的多Turn对话系统，使用了反馈学习来自动地从对话中提取用户的意图元素和标准查询。此外，还提供了可视化组件，以便监视对话中每个转折的直接逻辑路径。</li>
<li>results: 该研究通过实验证明了该系统的可行性和效果，并且可以帮助提高对话系统的实际应用。<details>
<summary>Abstract</summary>
Intent detection and identification from multi-turn dialogue has become a widely explored technique in conversational agents, for example, voice assistants and intelligent customer services. The conventional approaches typically cast the intent mining process as a classification task. Although neural classifiers have proven adept at such classification tasks, the issue of neural network models often impedes their practical deployment in real-world settings. We present a novel graph-based multi-turn dialogue system called , which identifies a user's intent by identifying intent elements and a standard query from a dynamically constructed and extensible intent graph using reinforcement learning. In addition, we provide visualization components to monitor the immediate reasoning path for each turn of a dialogue, which greatly facilitates further improvement of the system.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换给定文本到简化中文。<</SYS>>对话机器人中的意图检测和识别已经广泛研究，例如语音助手和智能客服。传统方法通常将意图挖掘过程视为一个分类任务。虽然神经网络模型在这类分类任务中表现出色，但神经网络模型在实际应用中的实现往往受阻。我们提出了一种新的图表基多轮对话系统，可以通过动态构建和扩展意图图来识别用户的意图，并使用回归学习来确定意图元素和标准查询。此外，我们还提供了可视化组件，可以帮助监测对话中每个转折的直接逻辑路径，这对系统进一步改进很有帮助。
</details></li>
</ul>
<hr>
<h2 id="Conservative-Predictions-on-Noisy-Financial-Data"><a href="#Conservative-Predictions-on-Noisy-Financial-Data" class="headerlink" title="Conservative Predictions on Noisy Financial Data"></a>Conservative Predictions on Noisy Financial Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11815">http://arxiv.org/abs/2310.11815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omkar Nabar, Gautam Shroff</li>
<li>for: 用于降低金融市场中运动风险的Fixed-term returns预测</li>
<li>methods: 使用 tradicional MLPs 和可微分决策树，在 sintetic data 和实际金融市场数据上预测Fixed-term returns，并通过遍历多个模型来减少数据噪音</li>
<li>results: our approach 可以获得更高的总收益，同时降低风险水平，并且提出了一个新的实用指标来衡量每笔交易的平均收益和风险衡量指标。<details>
<summary>Abstract</summary>
Price movements in financial markets are well known to be very noisy. As a result, even if there are, on occasion, exploitable patterns that could be picked up by machine-learning algorithms, these are obscured by feature and label noise rendering the predictions less useful, and risky in practice. Traditional rule-learning techniques developed for noisy data, such as CN2, would seek only high precision rules and refrain from making predictions where their antecedents did not apply. We apply a similar approach, where a model abstains from making a prediction on data points that it is uncertain on. During training, a cascade of such models are learned in sequence, similar to rule lists, with each model being trained only on data on which the previous model(s) were uncertain. Similar pruning of data takes place at test-time, with (higher accuracy) predictions being made albeit only on a fraction (support) of test-time data. In a financial prediction setting, such an approach allows decisions to be taken only when the ensemble model is confident, thereby reducing risk. We present results using traditional MLPs as well as differentiable decision trees, on synthetic data as well as real financial market data, to predict fixed-term returns using commonly used features. We submit that our approach is likely to result in better overall returns at a lower level of risk. In this context we introduce an utility metric to measure the average gain per trade, as well as the return adjusted for downside risk, both of which are improved significantly by our approach.
</details>
<details>
<summary>摘要</summary>
金融市场的价格变化非常具有噪音特性，因此，即使有时存在可以被机器学习算法捕捉的可见模式，这些模式受到特征和标签噪音的干扰，导致预测的准确性受到限制，实际应用中风险较高。传统的规则学习技术，如CN2，会寻找高精度规则，并在其前提不适用时停止预测。我们采用类似的方法，其中一个模型在训练过程中会决定不预测数据点，当前模型不确定时。在测试时，数据会被减少，仅保留一部分（支持）测试数据，并且使用高精度预测。在金融预测设置下，这种方法可以降低风险，只有当 ensemble 模型确定时，才会进行决策。我们使用传统的 MLP 以及可微分决策树，在 sintetic 数据和实际金融市场数据上预测 fixes-term 回报，使用常用的特征。我们认为，我们的方法可能会带来更好的总回报，同时降低风险水平。为此，我们引入了一个实用指标，用于衡量每笔交易的均衡收益，以及对于降低风险的回报调整指标，两者均得到了显著提高。
</details></li>
</ul>
<hr>
<h2 id="Learning-and-Discovering-Quantum-Properties-with-Multi-Task-Neural-Networks"><a href="#Learning-and-Discovering-Quantum-Properties-with-Multi-Task-Neural-Networks" class="headerlink" title="Learning and Discovering Quantum Properties with Multi-Task Neural Networks"></a>Learning and Discovering Quantum Properties with Multi-Task Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11807">http://arxiv.org/abs/2310.11807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ya-Dong Wu, Yan Zhu, Yuexuan Wang, Giulio Chiribella</li>
<li>for: 用深度神经网络预测量子态的性质从限制的测量数据中。</li>
<li>methods: 开发了一种网络模型，可同时预测多种量子性质，包括对量子态的期望值、非线性函数、如异Alignment和多体几何 invariants。</li>
<li>results: 发现一种模型可以通过多用途训练，不仅预测给定集合中的性质，还可以描述全局多体量子系统的性质从本地测量中。同时，模型还可以分类保护型态相对变化、发现不确定的界限。<details>
<summary>Abstract</summary>
Deep neural networks are a powerful tool for predicting properties of quantum states from limited measurement data. Here we develop a network model that can simultaneously predict multiple quantum properties, including not only expectation values of quantum observables, but also general nonlinear functions of the quantum state, like entanglement entropies and many-body topological invariants. Remarkably, we find that a model trained on a given set of properties can also discover new properties outside that set. Multi-purpose training also enables the model to infer global properties of many-body quantum systems from local measurements, to classify symmetry protected topological phases of matter, and to discover unknown boundaries between different phases.
</details>
<details>
<summary>摘要</summary>
深度神经网络是一种 poderous 工具，可以预测量子状态的性质从有限的测量数据中。在这里，我们开发了一种网络模型，可以同时预测多种量子性质，包括不只是量子观测器的期望值，还有一些泛函数，如量子状态的异步率和多体几何抽象。很意外地，我们发现，一个基于给定的性质集合来训练的模型，可以同时揭示未知的性质集合。多用途培训也使得模型可以从地方测量数据中推断全局的多体量子系统的性质，分类保护 topological phases of matter，并发现未知的阶段边界。
</details></li>
</ul>
<hr>
<h2 id="Auction-Based-Scheduling"><a href="#Auction-Based-Scheduling" class="headerlink" title="Auction-Based Scheduling"></a>Auction-Based Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11798">http://arxiv.org/abs/2310.11798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Abarjag/Abarja">https://github.com/Abarjag/Abarja</a></li>
<li>paper_authors: Guy Avni, Kaushik Mallik, Suman Sadhukhan</li>
<li>for: 这种paper是为了解决多个、部分矛盾的决策任务而写的。</li>
<li>methods: 这种方法使用了拍卖机制来解决多个目标的决策问题。每个目标都有一个分立的策略，可以独立创建、修改和替换。</li>
<li>results: 这种方法可以在不同的环境下实现长期公平的决策，并且可以解决多个目标之间的冲突。在路径规划问题上，这种方法可以synthesize一对策略和它们的初始分配的预算，以及拍卖策略。<details>
<summary>Abstract</summary>
Many sequential decision-making tasks require satisfaction of multiple, partially contradictory objectives. Existing approaches are monolithic, namely all objectives are fulfilled using a single policy, which is a function that selects a sequence of actions. We present auction-based scheduling, a modular framework for multi-objective decision-making problems. Each objective is fulfilled using a separate policy, and the policies can be independently created, modified, and replaced. Understandably, different policies with conflicting goals may choose conflicting actions at a given time. In order to resolve conflicts, and compose policies, we employ a novel auction-based mechanism. We allocate a bounded budget to each policy, and at each step, the policies simultaneously bid from their available budgets for the privilege of being scheduled and choosing an action. Policies express their scheduling urgency using their bids and the bounded budgets ensure long-run scheduling fairness. We lay the foundations of auction-based scheduling using path planning problems on finite graphs with two temporal objectives. We present decentralized algorithms to synthesize a pair of policies, their initially allocated budgets, and bidding strategies. We consider three categories of decentralized synthesis problems, parameterized by the assumptions that the policies make on each other: (a) strong synthesis, with no assumptions and strongest guarantees, (b) assume-admissible synthesis, with weakest rationality assumptions, and (c) assume-guarantee synthesis, with explicit contract-based assumptions. For reachability objectives, we show that, surprisingly, decentralized assume-admissible synthesis is always possible when the out-degrees of all vertices are at most two.
</details>
<details>
<summary>摘要</summary>
许多顺序决策任务需满足多个、部分矛盾的目标。现有的方法都是单一的，即所有目标都是通过单一策略（一个函数选择一系列动作）来满足。我们介绍了拍卖机制来解决这类决策问题。在这种机制下，每个目标都是通过一个分离的策略来满足，这些策略可以独立创建、修改和替换。当不同的策略有冲突目标时，我们使用一种新的拍卖机制来解决冲突。我们为每个策略分配一个固定预算，并在每步中让各策略同时从其可用预算中竞拍为执行动作的权利。策略通过竞拍价格表达其排期优先级，并且固定预算确保长期排期公平。我们在路径规划问题上建立了拍卖机制的基础，并提出了三种分类的分解问题：强化合理化（strong synthesis）、弱合理化（assume-admissible synthesis）和合理合同（assume-guarantee synthesis）。对于可达性目标，我们发现了一个意外的结论：在所有顶点出度都不大于2时，分解问题总是可能的。
</details></li>
</ul>
<hr>
<h2 id="Solving-the-multiplication-problem-of-a-large-language-model-system-using-a-graph-based-method"><a href="#Solving-the-multiplication-problem-of-a-large-language-model-system-using-a-graph-based-method" class="headerlink" title="Solving the multiplication problem of a large language model system using a graph-based method"></a>Solving the multiplication problem of a large language model system using a graph-based method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.13016">http://arxiv.org/abs/2310.13016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Turker Tuncer, Sengul Dogan, Mehmet Baygin, Prabal Datta Barua, Abdul Hafeez-Baig, Ru-San Tan, Subrata Chakraborty, U. Rajendra Acharya</li>
<li>for: 解决 chatGPT 模型中的乘法问题，提高其数学运算精度。</li>
<li>methods: 基于图表结构的乘法算法，通过增加 10k 操作符来模拟人类数学运算。</li>
<li>results: 对 1,000,000 个大数乘法任务，提出了 100% 的准确率，成功解决了 GPT 模型中的乘法挑战。<details>
<summary>Abstract</summary>
The generative pre-trained transformer (GPT)-based chatbot software ChatGPT possesses excellent natural language processing capabilities but is inadequate for solving arithmetic problems, especially multiplication. Its GPT structure uses a computational graph for multiplication, which has limited accuracy beyond simple multiplication operations. We developed a graph-based multiplication algorithm that emulated human-like numerical operations by incorporating a 10k operator, where k represents the maximum power to base 10 of the larger of two input numbers. Our proposed algorithm attained 100% accuracy for 1,000,000 large number multiplication tasks, effectively solving the multiplication challenge of GPT-based and other large language models. Our work highlights the importance of blending simple human insights into the design of artificial intelligence algorithms. Keywords: Graph-based multiplication; ChatGPT; Multiplication problem
</details>
<details>
<summary>摘要</summary>
《基于Transformer（GPT）的对话机器人软件ChatGPT具有出色的自然语言处理能力，但对数学问题（尤其是乘法）的解决能力不足。GPT结构使用的计算图在多项式乘法操作上有限的准确性。我们开发了基于图的乘法算法，通过 incorporating a 10k操作符（其中k表示base 10最大幂），实现了人类化数学操作。我们的提议算法在100万大数乘法任务中达到100%的准确率，有效解决了GPT基于和其他大语言模型的乘法挑战。我们的工作强调了人工智能算法设计中的人类智慧的重要性。关键词：图基于乘法; ChatGPT; 乘法问题》Note: Please keep in mind that the translation is done by a machine and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Telecom-AI-Native-Systems-in-the-Age-of-Generative-AI-–-An-Engineering-Perspective"><a href="#Telecom-AI-Native-Systems-in-the-Age-of-Generative-AI-–-An-Engineering-Perspective" class="headerlink" title="Telecom AI Native Systems in the Age of Generative AI – An Engineering Perspective"></a>Telecom AI Native Systems in the Age of Generative AI – An Engineering Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11770">http://arxiv.org/abs/2310.11770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ricardo Britto, Timothy Murphy, Massimo Iovene, Leif Jonsson, Melike Erol-Kantarci, Benedek Kovács</li>
<li>for: The paper explores the integration of foundational models (FMs) in the telecommunications industry, with a focus on the concept of “AI native telco” and the engineering considerations and challenges associated with implementing FMs in the software life cycle.</li>
<li>methods: The paper discusses the use of FMs in natural language processing tasks and content generation, and highlights the need for AI native-first approaches to fully leverage the potential of FMs in the telecom industry.</li>
<li>results: The paper emphasizes the enormous potential of FMs in revolutionizing how we interact with software products and services in the telecom industry, but also acknowledges the need for careful consideration of ethical, regulatory, and operational challenges to ensure the successful integration of FMs in mission-critical telecom contexts.<details>
<summary>Abstract</summary>
The rapid advancements in Artificial Intelligence (AI), particularly in generative AI and foundational models (FMs), have ushered in transformative changes across various industries. Large language models (LLMs), a type of FM, have demonstrated their prowess in natural language processing tasks and content generation, revolutionizing how we interact with software products and services. This article explores the integration of FMs in the telecommunications industry, shedding light on the concept of AI native telco, where AI is seamlessly woven into the fabric of telecom products. It delves into the engineering considerations and unique challenges associated with implementing FMs into the software life cycle, emphasizing the need for AI native-first approaches. Despite the enormous potential of FMs, ethical, regulatory, and operational challenges require careful consideration, especially in mission-critical telecom contexts. As the telecom industry seeks to harness the power of AI, a comprehensive understanding of these challenges is vital to thrive in a fiercely competitive market.
</details>
<details>
<summary>摘要</summary>
“人工智能（AI）的快速进步，特别是生成AI和基础模型（FM），已经在不同行业引入了transformative变革。大语言模型（LLM），一种基础模型，在自然语言处理任务和内容生成方面表现出色，改变了我们与软件产品和服务的交互方式。本文探讨了在电信行业中基础模型的整合，探讨了AI native telco这一概念，其中AI被融入了电信产品的тка料中。文章还讨论了在软件生命周期中实施基础模型的工程准则和特有挑战，强调了AI native-first的方法。虽然FM具有巨大的潜力，但是伦理、法规和运营上的挑战需要仔细考虑，特别在关键的电信上下文中。电信行业想要利用AI的力量，需要深入理解这些挑战，以在竞争激烈的市场中vivify。”
</details></li>
</ul>
<hr>
<h2 id="Stranger-Danger-Cross-Community-Interactions-with-Fringe-Users-Increase-the-Growth-of-Fringe-Communities-on-Reddit"><a href="#Stranger-Danger-Cross-Community-Interactions-with-Fringe-Users-Increase-the-Growth-of-Fringe-Communities-on-Reddit" class="headerlink" title="Stranger Danger! Cross-Community Interactions with Fringe Users Increase the Growth of Fringe Communities on Reddit"></a>Stranger Danger! Cross-Community Interactions with Fringe Users Increase the Growth of Fringe Communities on Reddit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12186">http://arxiv.org/abs/2310.12186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Russo, Manoel Horta Ribeiro, Robert West<br>for: 这些研究旨在解释具有偏见和极端思想的社区在主流平台上快速增长的机制。methods: 这些研究使用文本基因推断技术来研究具有偏见和极端思想的社区在Reddit上的增长。results: 研究发现，与偏见和极端思想相关的社区之间的交互可以吸引新成员加入这些社区。接受这些交互的用户比相似的匹配用户更有4.2%的可能性加入偏见和极端思想社区。这种效应受到社区特点（如左右两派社区）和交互语言的影响。使用恶意语言进行交互可以增加加入偏见和极端思想社区的可能性，比非恶意交互高5pp。对于非偏见和极端思想社区（如r&#x2F;climatechange、r&#x2F;NBA、r&#x2F;leagueoflegends）进行重复分析，未发现这种增长机制。总的来说，我们的发现表明，减少偏见和极端思想社区之间的交互可以减少主流平台上的偏见和极端思想社区的增长。<details>
<summary>Abstract</summary>
Fringe communities promoting conspiracy theories and extremist ideologies have thrived on mainstream platforms, raising questions about the mechanisms driving their growth. Here, we hypothesize and study a possible mechanism: new members may be recruited through fringe-interactions: the exchange of comments between members and non-members of fringe communities. We apply text-based causal inference techniques to study the impact of fringe-interactions on the growth of three prominent fringe communities on Reddit: r/Incel, r/GenderCritical, and r/The_Donald. Our results indicate that fringe-interactions attract new members to fringe communities. Users who receive these interactions are up to 4.2 percentage points (pp) more likely to join fringe communities than similar, matched users who do not.   This effect is influenced by 1) the characteristics of communities where the interaction happens (e.g., left vs. right-leaning communities) and 2) the language used in the interactions. Interactions using toxic language have a 5pp higher chance of attracting newcomers to fringe communities than non-toxic interactions. We find no effect when repeating this analysis by replacing fringe (r/Incel, r/GenderCritical, and r/The_Donald) with non-fringe communities (r/climatechange, r/NBA, r/leagueoflegends), suggesting this growth mechanism is specific to fringe communities. Overall, our findings suggest that curtailing fringe-interactions may reduce the growth of fringe communities on mainstream platforms.
</details>
<details>
<summary>摘要</summary>
极端社区促进阴谋理论和极端思想的发展，在主流平台上蓬勃发展，这引发了关于这些机制的问题。我们提出和研究一种可能的机制：新成员可能通过极端互动被招募到极端社区中。我们使用文本基因ferrer inference技术来研究极端互动对Reddit上三个 prominent fringe community的发展产生的影响：r/Incel、r/GenderCritical和r/The_Donald。我们的结果表明，极端互动会吸引新成员加入极端社区。接受这些互动的用户比相似的匹配用户更有4.2%的可能性加入极端社区。这种效应受到社区的特点（如左右翼社区）以及互动语言的影响。使用恶势力语言进行互动可能使新成员加入极端社区的可能性提高5pp。当我们将极端社区换为非极端社区（如r/climatechange、r/NBA、r/leagueoflegends）进行重复分析时，我们没有发现这种效应，这表明这种生长机制特有于极端社区。总的来说，我们的发现表明，遏制极端互动可能会降低主流平台上极端社区的发展。
</details></li>
</ul>
<hr>
<h2 id="Estimating-Material-Properties-of-Interacting-Objects-Using-Sum-GP-UCB"><a href="#Estimating-Material-Properties-of-Interacting-Objects-Using-Sum-GP-UCB" class="headerlink" title="Estimating Material Properties of Interacting Objects Using Sum-GP-UCB"></a>Estimating Material Properties of Interacting Objects Using Sum-GP-UCB</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11749">http://arxiv.org/abs/2310.11749</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Yunus Seker, Oliver Kroemer</li>
<li>for: 估算物体的物理和动态属性从观察数据中</li>
<li>methods: bayesian优化方法确定物体参数</li>
<li>results: 能够有效地进行逐步学习，不需要重新评估已有观察数据的奖励值<details>
<summary>Abstract</summary>
Robots need to estimate the material and dynamic properties of objects from observations in order to simulate them accurately. We present a Bayesian optimization approach to identifying the material property parameters of objects based on a set of observations. Our focus is on estimating these properties based on observations of scenes with different sets of interacting objects. We propose an approach that exploits the structure of the reward function by modeling the reward for each observation separately and using only the parameters of the objects in that scene as inputs. The resulting lower-dimensional models generalize better over the parameter space, which in turn results in a faster optimization. To speed up the optimization process further, and reduce the number of simulation runs needed to find good parameter values, we also propose partial evaluations of the reward function, wherein the selected parameters are only evaluated on a subset of real world evaluations. The approach was successfully evaluated on a set of scenes with a wide range of object interactions, and we showed that our method can effectively perform incremental learning without resetting the rewards of the gathered observations.
</details>
<details>
<summary>摘要</summary>
Robots需要估算物体的物理和动态性质从观察数据中，以便准确模拟。我们提出了一种 bayesian优化方法，用于根据观察数据中的物体参数进行物体物理性质的估算。我们的注重点是基于不同交互对象的场景中的观察数据进行估算。我们提议利用奖励函数的结构，将奖励函数分割成每个观察中的奖励模型，并只使用场景中的对象参数作为输入。这将导致更好的维度减少，从而更快地优化。为了进一步加速优化过程，并减少需要进行实际评估的运行次数，我们还提议使用部分评估奖励函数，选择的参数只在一部分实际评估中进行评估。我们成功地应用了这种方法在一组具有多种对象交互的场景中，并证明了我们的方法可以进行逐步学习而不需要重置观察得到的奖励。
</details></li>
</ul>
<hr>
<h2 id="Action-Quantized-Offline-Reinforcement-Learning-for-Robotic-Skill-Learning"><a href="#Action-Quantized-Offline-Reinforcement-Learning-for-Robotic-Skill-Learning" class="headerlink" title="Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning"></a>Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11731">http://arxiv.org/abs/2310.11731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianlan Luo, Perry Dong, Jeffrey Wu, Aviral Kumar, Xinyang Geng, Sergey Levine</li>
<li>for: 这篇论文旨在提出一种适应量化动作的策略，以提高在离线学习中的RL性能。</li>
<li>methods: 该论文提出了一种基于VQ-VAE的状态决定动作量化方法，以避免粗略量化所导致的极大幂灵活增长。</li>
<li>results: 该论文在多个离线RL方法，如IQL、CQL和BRAC等方法的基础上，提出了一种适应量化的策略，并在Robomimic环境中进行了详细的实验 validate。结果显示，与离线RL方法相比，该策略可以提高策略性能 by 2-3倍。<details>
<summary>Abstract</summary>
The offline reinforcement learning (RL) paradigm provides a general recipe to convert static behavior datasets into policies that can perform better than the policy that collected the data. While policy constraints, conservatism, and other methods for mitigating distributional shifts have made offline reinforcement learning more effective, the continuous action setting often necessitates various approximations for applying these techniques. Many of these challenges are greatly alleviated in discrete action settings, where offline RL constraints and regularizers can often be computed more precisely or even exactly. In this paper, we propose an adaptive scheme for action quantization. We use a VQ-VAE to learn state-conditioned action quantization, avoiding the exponential blowup that comes with na\"ive discretization of the action space. We show that several state-of-the-art offline RL methods such as IQL, CQL, and BRAC improve in performance on benchmarks when combined with our proposed discretization scheme. We further validate our approach on a set of challenging long-horizon complex robotic manipulation tasks in the Robomimic environment, where our discretized offline RL algorithms are able to improve upon their continuous counterparts by 2-3x. Our project page is at https://saqrl.github.io/
</details>
<details>
<summary>摘要</summary>
“偏离线强化学习（RL）模式提供了一个通用的方法，将静止行为数据转换成可以更好地性能的策略。虽然政策限制、保守主义和其他避免分布Shift的方法有助于减轻偏离线RL的挑战，但是连续动作设置frequently需要一些近似方法。在离散动作设置中，偏离线RL约束和规范可以经常更加精确地计算或者甚至是 exactly。在这篇论文中，我们提议了一种可变的行动量化方案。我们使用了VQ-VAE来学习状态决定的行动量化，以避免由粗粒化所带来的极大增长。我们证明了一些state-of-the-art的偏离线RL方法，如IQL、CQL和BRAC，在与我们提议的量化方案结合后，在标准准确度上提高了性能。我们进一步验证了我们的方法在Robomimic环境中的一组复杂的长期机械抓取任务上，我们的量化的偏离线RL算法能够超过其连续 counterpart的性能，提高了2-3倍。我们的项目页面是https://saqrl.github.io/”
</details></li>
</ul>
<hr>
<h2 id="Federated-Heterogeneous-Graph-Neural-Network-for-Privacy-preserving-Recommendation"><a href="#Federated-Heterogeneous-Graph-Neural-Network-for-Privacy-preserving-Recommendation" class="headerlink" title="Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation"></a>Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11730">http://arxiv.org/abs/2310.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Yan, Yang Cao, Haoyu Wang, Wenchuan Yang, Junping Du, Chuan Shi</li>
<li>For: This paper proposes a federated heterogeneous graph neural network (FedHGNN) based framework for recommendation, which can collaboratively train a recommendation model on distributed Heterogeneous Information Networks (HINs) without leaking user privacy.* Methods: The paper formalizes a privacy definition based on differential privacy for HIN-based federated recommendation, and elaborately designs a semantic-preserving user interactions publishing method to recover the broken meta-path based semantics caused by distributed data storage.* Results: The proposed FedHGNN model outperforms existing methods by a large margin (up to 34% in HR@10 and 42% in NDCG@10) under an acceptable privacy budget, as demonstrated through extensive experiments on three datasets.<details>
<summary>Abstract</summary>
Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's high-order patterns from shared HINs. To recover the broken meta-path based semantics caused by distributed data storage and satisfy the proposed privacy, we elaborately design a semantic-preserving user interactions publishing method, which locally perturbs user's high-order patterns as well as related user-item interactions for publishing. After that, we propose a HGNN model for recommendation, which conducts node- and semantic-level aggregations to capture recovered semantics. Extensive experiments on three datasets demonstrate our model outperforms existing methods by a large margin (up to 34% in HR@10 and 42% in NDCG@10) under an acceptable privacy budget.
</details>
<details>
<summary>摘要</summary>
众所周知的异种信息网络（HIN）已成为推荐系统中强大的工具，它可以通过meta-paths中嵌入的 semantics来解决数据稀缺问题。然而，现实中的数据通常会被分布式存储，这导致了传统的中央化HIN-based推荐模型的失败。在这篇论文中，我们提议将HIN partitioned into private HINs stored on the client side and shared HINs on the server。基于这种设定，我们提出了一种联邦异种图神经网络（FedHGNN）基础架构，可以在分布式HINs上并发训练一个推荐模型，无需透露用户隐私。具体来说，我们首先定义了隐私定义，以保护用户-项交互的隐私以及用户高阶征分的信息。然后，我们采用了一种semantic-preserving用户互动发布方法，通过地方扰动用户的高阶征分和相关用户-项交互来发布。接着，我们提出了一种HGNN模型，通过节点和semantic-level汇聚来捕捉恢复的 semantics。我们对三个数据集进行了广泛的实验，结果显示，我们的模型在遵守隐私预算下，可以大幅提高推荐效果（最多34%的HR@10和42%的NDCG@10）。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-in-Automated-Ontology-Matching-Lessons-Learned-from-an-Empirical-Experimentation"><a href="#Uncertainty-in-Automated-Ontology-Matching-Lessons-Learned-from-an-Empirical-Experimentation" class="headerlink" title="Uncertainty in Automated Ontology Matching: Lessons Learned from an Empirical Experimentation"></a>Uncertainty in Automated Ontology Matching: Lessons Learned from an Empirical Experimentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11723">http://arxiv.org/abs/2310.11723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inès Osman, Salvatore F. Pileggi, Sadok Ben Yahia</li>
<li>for: 本研究旨在探讨自动对照 ontology 的应用，以提高数据集合的相互连接和semantic integrability。</li>
<li>methods: 本研究采用了基于 ontology 的底层知识建构方法，并在实际数据上进行了实验。</li>
<li>results: 实验结果表明，自动对照过程中存在较大的不确定性，而 semi-supervised 方法则显示出了更好的可靠性。<details>
<summary>Abstract</summary>
Data integration is considered a classic research field and a pressing need within the information science community. Ontologies play a critical role in such a process by providing well-consolidated support to link and semantically integrate datasets via interoperability. This paper approaches data integration from an application perspective, looking at techniques based on ontology matching. An ontology-based process may only be considered adequate by assuming manual matching of different sources of information. However, since the approach becomes unrealistic once the system scales up, automation of the matching process becomes a compelling need. Therefore, we have conducted experiments on actual data with the support of existing tools for automatic ontology matching from the scientific community. Even considering a relatively simple case study (i.e., the spatio-temporal alignment of global indicators), outcomes clearly show significant uncertainty resulting from errors and inaccuracies along the automated matching process. More concretely, this paper aims to test on real-world data a bottom-up knowledge-building approach, discuss the lessons learned from the experimental results of the case study, and draw conclusions about uncertainty and uncertainty management in an automated ontology matching process. While the most common evaluation metrics clearly demonstrate the unreliability of fully automated matching solutions, properly designed semi-supervised approaches seem to be mature for a more generalized application.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>数据集成被视为信息科学领域的经典研究领域和紧迫需求。 ontology 在这种过程中扮演了关键的支持角色，以链接和semantic集成数据。本文从应用角度出发，研究基于 ontology 匹配技术的数据集成方法。然而，由于系统规模增加， manual 匹配过程变得不现实。因此，我们在实际数据支持下进行了现有的自动 ontology 匹配工具的实验。尽管使用简单的case study（即全球指标的空间-时间Alignment），实验结果显示了自动匹配过程中的显著不确定性。本文的目标是在真实数据上测试底层知识建构方法，讨论实验结果中的教训，并对自动匹配过程中的不确定性和不确定性管理进行结论。尽管常见的评价指标显示全自动匹配解决方案的不可靠性，但是正确设计的半监督方法似乎已经成熟备用更广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="Quantify-Health-Related-Atomic-Knowledge-in-Chinese-Medical-Large-Language-Models-A-Computational-Analysis"><a href="#Quantify-Health-Related-Atomic-Knowledge-in-Chinese-Medical-Large-Language-Models-A-Computational-Analysis" class="headerlink" title="Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis"></a>Quantify Health-Related Atomic Knowledge in Chinese Medical Large Language Models: A Computational Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11722">http://arxiv.org/abs/2310.11722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaxin Fan, Feng Jiang, Peifeng Li, Haizhou Li</li>
<li>for: This paper aims to evaluate the ability of large language models (LLMs) to provide accurate and factual suggestions for user self-diagnosis queries.</li>
<li>methods: The authors constructed a benchmark of common atomic knowledge in user self-diagnosis queries and evaluated both generic and specialized LLMs on this benchmark. They also performed error analysis and explored different types of data for fine-tuning specialized LLMs.</li>
<li>results: The results showed that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability, and that distilled data can benefit LLMs most. Additionally, the authors found that both generic and specialized LLMs are sycophantic, meaning they tend to cater to users’ claims when it comes to unknown knowledge.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have the potential to revolutionize the way users self-diagnose through search engines by offering direct and efficient suggestions. Recent studies primarily focused on the quality of LLMs evaluated by GPT-4 or their ability to pass medical exams, no studies have quantified the extent of health-related atomic knowledge stored in LLMs' memory, which is the basis of LLMs to provide more factual suggestions. In this paper, we first constructed a benchmark, including the most common types of atomic knowledge in user self-diagnosis queries, with 17 atomic types and a total of 14, 048 pieces of atomic knowledge. Then, we evaluated both generic and specialized LLMs on the benchmark. The experimental results showcased that generic LLMs perform better than specialized LLMs in terms of atomic knowledge and instruction-following ability. Error analysis revealed that both generic and specialized LLMs are sycophantic, e.g., always catering to users' claims when it comes to unknown knowledge. Besides, generic LLMs showed stronger safety, which can be learned by specialized LLMs through distilled data. We further explored different types of data commonly adopted for fine-tuning specialized LLMs, i.e., real-world, semi-distilled, and distilled data, and found that distilled data can benefit LLMs most.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）有可能革命化用户自诊查找结果，提供直接和有效的建议。现在的研究主要集中在GPT-4评估了LMMs的质量或者LMMs能否通过医学考试，但是没有评估LMMs储存的健康相关知识量，这是LMMs提供更加正确的建议的基础。在这篇论文中，我们首先建立了一个benchmark，包括用户自诊查找常见的17种原子知识，总共14,048个原子知识。然后，我们评估了一般和特殊的LMMs在benchmark上。实验结果显示，一般LMMs在原子知识和指令遵循能力方面表现比特殊LMMs更好。错误分析表明，一般和特殊LMMs都具有追求用户的倾听，即当用户提出未知知识时，LMMs都会尽力适应。此外，一般LMMs表现出更强的安全性，可以通过特殊LMMs的滤过资料学习。我们进一步探索了不同类型的特殊LMMs fine-tuning的常用数据，包括实际世界、半滤过和滤过数据，发现滤过数据可以帮助LMMs最多。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Low-resource-Fine-grained-Named-Entity-Recognition-by-Leveraging-Coarse-grained-Datasets"><a href="#Enhancing-Low-resource-Fine-grained-Named-Entity-Recognition-by-Leveraging-Coarse-grained-Datasets" class="headerlink" title="Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets"></a>Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11715">http://arxiv.org/abs/2310.11715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sue991/cofiner">https://github.com/sue991/cofiner</a></li>
<li>paper_authors: Su Ah Lee, Seokjin Oh, Woohwan Jung</li>
<li>for: 本文提出了一种解决Named Entity Recognition（NER）缺乏精细标注数据的问题，特别是在细化NER场景下。</li>
<li>methods: 本文使用了现有的粗化标注数据，并提出了一种叫做Fine-to-Coarse（F2C）映射矩阵来利用粗化和细化实体之间的层次结构。此外，本文还提出了一种矛盾检测方法，以避免粗化实体与细化实体之间的矛盾。</li>
<li>results: 实验结果表明，我们的方法在只有少量细化标注时比$K$-shot学习和监督学习方法表现更好。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although $K$-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is pre-finetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-grained entity types to avoid performance degradation. Our experimental results show that our method outperforms both $K$-shot learning and supervised learning methods when dealing with a small number of fine-grained annotations.
</details>
<details>
<summary>摘要</summary>
翻译结果：Named Entity Recognition (NER)  часто遇到缺乏标签数据的问题，特别在细化NER场景下。虽可以使用$K$-shot学习技术，但其表现往往停滞在数十个标签以上。为解决这问题，我们利用现有的粗化数据，它们提供了大量的标签。一种直接 Addressing this problem is pre-finetuning, which uses coarse-grained data for representation learning. However, it cannot directly utilize the relationships between fine-grained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fine-to-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-grained entity types to avoid performance degradation. Our experimental results show that our method outperforms both $K$-shot learning and supervised learning methods when dealing with a small number of fine-grained annotations.
</details></li>
</ul>
<hr>
<h2 id="Learning-Co-Speech-Gesture-for-Multimodal-Aphasia-Type-Detection"><a href="#Learning-Co-Speech-Gesture-for-Multimodal-Aphasia-Type-Detection" class="headerlink" title="Learning Co-Speech Gesture for Multimodal Aphasia Type Detection"></a>Learning Co-Speech Gesture for Multimodal Aphasia Type Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11710">http://arxiv.org/abs/2310.11710</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsail-skku/multimodal-aphasia-type-detection_emnlp_2023">https://github.com/dsail-skku/multimodal-aphasia-type-detection_emnlp_2023</a></li>
<li>paper_authors: Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han</li>
<li>for: 这种研究旨在为抑制语言障碍的病人提供有效的诊断方法，具体来说是用多模态图像神经网络来识别不同的语言障碍类型。</li>
<li>methods: 该研究使用了多模态图像神经网络，通过学习语音和手势模式之间的相关性，生成敏感于手势信息的文本表示，从而准确地识别不同的语言障碍类型。</li>
<li>results: 对比 exist 方法，该研究实现了状态机器人的Result（F1 84.2%），并显示了手势特征的优越性， highlighting the significance of gesture expression in detecting aphasia types。<details>
<summary>Abstract</summary>
Aphasia, a language disorder resulting from brain damage, requires accurate identification of specific aphasia types, such as Broca's and Wernicke's aphasia, for effective treatment. However, little attention has been paid to developing methods to detect different types of aphasia. Recognizing the importance of analyzing co-speech gestures for distinguish aphasia types, we propose a multimodal graph neural network for aphasia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each aphasia type, our model can generate textual representations sensitive to gesture information, leading to accurate aphasia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting aphasia types. We provide the codes for reproducibility purposes.
</details>
<details>
<summary>摘要</summary>
apraxia, a language disorder caused by brain damage, requires accurate identification of specific apraxia types, such as Broca's and Wernicke's apraxia, for effective treatment. However, little attention has been paid to developing methods to detect different types of apraxia. Recognizing the importance of analyzing co-speech gestures for distinguishing apraxia types, we propose a multimodal graph neural network for apraxia type detection using speech and corresponding gesture patterns. By learning the correlation between the speech and gesture modalities for each apraxia type, our model can generate textual representations sensitive to gesture information, leading to accurate apraxia type detection. Extensive experiments demonstrate the superiority of our approach over existing methods, achieving state-of-the-art results (F1 84.2\%). We also show that gesture features outperform acoustic features, highlighting the significance of gesture expression in detecting apraxia types. We provide the codes for reproducibility purposes.Note: "apraxia" is the traditional Chinese term for aphasia, and "apraxia type" is the Chinese term for aphasia type.
</details></li>
</ul>
<hr>
<h2 id="Live-Graph-Lab-Towards-Open-Dynamic-and-Real-Transaction-Graphs-with-NFT"><a href="#Live-Graph-Lab-Towards-Open-Dynamic-and-Real-Transaction-Graphs-with-NFT" class="headerlink" title="Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT"></a>Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11709">http://arxiv.org/abs/2310.11709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Zhang, Bingqiao Luo, Shengliang Lu, Bingsheng He</li>
<li>for:  This paper is written for investigating the properties of the Non-fungible tokens (NFTs) ecosystem from a temporal graph analysis perspective.</li>
<li>methods:  The paper uses a live graph with NFT transaction network, which is obtained by downloading and parsing the NFT transaction activities. The authors also use a series of measurements to understand the properties of the NFT ecosystem and compare it with social, citation, and web networks.</li>
<li>results:  The paper provides new observations and insights into the characteristics of the emerging NFT ecosystem, including its dynamics and properties. The authors also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community.<details>
<summary>Abstract</summary>
Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it's usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of {\it Live Graph Lab} for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than \$40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a complicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate its dynamics to provide new observations and insights. Specifically, through downloading and parsing the NFT transaction activities, we obtain a temporal graph with more than 4.5 million nodes and 124 million edges. Then, a series of measurements are presented to understand the properties of the NFT ecosystem. Through comparisons with social, citation, and web networks, our analyses give intriguing findings and point out potential directions for future exploration. Finally, we also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community. The source codes and dataset are available at https://livegraphlab.github.io.
</details>
<details>
<summary>摘要</summary>
多个研究已经进行了大规模时间图的性质调查。尽管这些图在实际场景中很常见，但由于隐私问题和技术限制，我们通常无法获取实时图。在这篇论文中，我们介绍了一种名为{\it Live Graph Lab}的概念，该概念可以在区块链上提供开放、动态和实时交易图。其中，非 fungible tokens（NFTs）在过去几年中成为了区块链的一个最 prominent的部分。NFTs的市场规模超过400亿美元，这个分布式生态系统会生成大量、匿名和实时交易活动，自然形成了复杂的交易网络。然而，关于这个emerging NFT生态系统从时间图分析的特点还有很少的理解。为了减少这一差距，我们在实时图中实例化了NFT交易网络，并investigated its dynamics，以提供新的观察和发现。具体来说，通过下载和解析NFT交易活动，我们获得了一个时间图，包含超过450万个节点和124亿个边。然后，我们进行了一系列测量，以了解NFT生态系统的特点。通过与社交、引用和网络相比较，我们的分析发现了有趣的发现，并指出了未来的探索方向。此外，我们还研究了在这个实时图上的机器学习模型，以激励当前的数据集和提供新的探索机会。实时图和数据集的源代码可以在https://livegraphlab.github.io/ obtained。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Survey-on-Vector-Database-Storage-and-Retrieval-Technique-Challenge"><a href="#A-Comprehensive-Survey-on-Vector-Database-Storage-and-Retrieval-Technique-Challenge" class="headerlink" title="A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge"></a>A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11703">http://arxiv.org/abs/2310.11703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Han, Chunjiang Liu, Pengfei Wang</li>
<li>for: 本文旨在为Vector数据库的高维数据存储提供一个概述，以及相关的近似搜索算法的概述。</li>
<li>methods: 本文使用分类法概述了现有的Vector数据库架构，并对近似搜索问题的解决方法进行了分类，包括 hash-based、tree-based、graph-based 和 quantization-based 等方法。</li>
<li>results: 本文提供了现有Vector数据库的挑战和大语言模型的组合，以及它们在新的可能性领域中的应用。<details>
<summary>Abstract</summary>
A vector database is used to store high-dimensional data that cannot be characterized by traditional DBMS. Although there are not many articles describing existing or introducing new vector database architectures, the approximate nearest neighbor search problem behind vector databases has been studied for a long time, and considerable related algorithmic articles can be found in the literature. This article attempts to comprehensively review relevant algorithms to provide a general understanding of this booming research area. The basis of our framework categorises these studies by the approach of solving ANNS problem, respectively hash-based, tree-based, graph-based and quantization-based approaches. Then we present an overview of existing challenges for vector databases. Lastly, we sketch how vector databases can be combined with large language models and provide new possibilities.
</details>
<details>
<summary>摘要</summary>
vector database 是用于存储高维数据的数据库，而这些数据不能由传统的DBMS进行描述。虽然有很少的文章描述了现有或引入新的vector database架构，但近似最近邻居问题（ANNS）在vector databases中的研究已经很长时间了，相关的算法文章在 literatura 中可以找到。本文尝试从ategorize 这些研究，按照解决 ANNS 问题的方法分为hash-based、tree-based、graph-based和quantization-based四种方法。然后，我们介绍vector databases 存在的挑战，最后，我们探讨 vector databases 与大型自然语言模型如何结合，提供新的可能性。Note: "vector database" is a literal translation of the English phrase, and it is not a commonly used term in Chinese. In Chinese, the term "高维数据库" (gāo wěi dà kē) is more commonly used to refer to a database that stores high-dimensional data.
</details></li>
</ul>
<hr>
<h2 id="Runner-re-identification-from-single-view-video-in-the-open-world-setting"><a href="#Runner-re-identification-from-single-view-video-in-the-open-world-setting" class="headerlink" title="Runner re-identification from single-view video in the open-world setting"></a>Runner re-identification from single-view video in the open-world setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11700">http://arxiv.org/abs/2310.11700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomohiro Suzuki, Kazushi Tsutsui, Kazuya Takeda, Keisuke Fujii</li>
<li>for: 这paper是为了解决多视图或单视图体育视频中运动员重新认识的问题，以便实现自动化视频分析。</li>
<li>methods: 该paper使用了预训练的YOLOv8和EfficientNet，以及自适应的gated recurrent unit autoencoder模型来自动处理单视图视频，并使用运动动态特征来提高重新认识精度。</li>
<li>results: 该paper在使用一个运动实践视频数据集上进行测试，并显示了与一种状态的艺术模型相比，该方法可以更高的准确率来重新认识运动员。此外，该paper还证明了自适应运动动态特征提取器的有效性。该runner重新认识系统可以用于自动分析运动视频。<details>
<summary>Abstract</summary>
In many sports, player re-identification is crucial for automatic video processing and analysis. However, most of the current studies on player re-identification in multi- or single-view sports videos focus on re-identification in the closed-world setting using labeled image dataset, and player re-identification in the open-world setting for automatic video analysis is not well developed. In this paper, we propose a runner re-identification system that directly processes single-view video to address the open-world setting. In the open-world setting, we cannot use labeled dataset and have to process video directly. The proposed system automatically processes raw video as input to identify runners, and it can identify runners even when they are framed out multiple times. For the automatic processing, we first detect the runners in the video using the pre-trained YOLOv8 and the fine-tuned EfficientNet. We then track the runners using ByteTrack and detect their shoes with the fine-tuned YOLOv8. Finally, we extract the image features of the runners using an unsupervised method using the gated recurrent unit autoencoder model. To improve the accuracy of runner re-identification, we use dynamic features of running sequence images. We evaluated the system on a running practice video dataset and showed that the proposed method identified runners with higher accuracy than one of the state-of-the-art models in unsupervised re-identification. We also showed that our unsupervised running dynamic feature extractor was effective for runner re-identification. Our runner re-identification system can be useful for the automatic analysis of running videos.
</details>
<details>
<summary>摘要</summary>
在多种运动中，玩家重新认定是自动视频处理和分析的关键。然而，当前大多数player重新认定在多视图或单视图运动视频中的研究都集中在关闭世界设定下使用标注图像集，而在开放世界设定下的自动视频分析中player重新认定还未得到充分开发。在这篇论文中，我们提出了一个runner重新认定系统，Directly处理单视图视频来解决开放世界设定。在开放世界设定下，我们不能使用标注集和处理视频 directly。提出的系统可以自动处理原始视频，并在多次框架外重新认定运动员。为了实现自动处理，我们首先在视频中检测运动员使用预训练的YOLOv8和精度调整的EfficientNet。然后，我们使用ByteTrack跟踪运动员，并使用精度调整的YOLOv8检测他们的鞋。最后，我们使用无监督方法使用闭环回归自适应模型提取运动员的图像特征。为了提高运动员重新认定的准确性，我们使用运动序列图像的动态特征。我们对一个跑步练习视频数据集进行评估，并显示了我们提出的方法可以在无监督下高度准确地重新认定运动员，并且我们的无监督跑动特征提取器是 runner重新认定中有效的。我们的runner重新认定系统可以对运动视频进行自动分析。
</details></li>
</ul>
<hr>
<h2 id="Architectural-Implications-of-GNN-Aggregation-Programming-Abstractions"><a href="#Architectural-Implications-of-GNN-Aggregation-Programming-Abstractions" class="headerlink" title="Architectural Implications of GNN Aggregation Programming Abstractions"></a>Architectural Implications of GNN Aggregation Programming Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12184">http://arxiv.org/abs/2310.12184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingjie Qi, Jianlei Yang, Ao Zhou, Tong Qiao, Chunming Hu</li>
<li>for: 本研究旨在对现有的图数据处理抽象进行全面的评估和分析，以便为未来的图神经网络加速提供依据。</li>
<li>methods: 本研究使用现有的图数据处理抽象，并对其进行了详细的 caracterization 研究，以确定哪些抽象更有效率。</li>
<li>results: 研究发现，使用不同的抽象方法可以得到不同的性能和效率。同时，对于某些特定的图数据处理任务，某些抽象方法可以显著提高性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经吸引了广泛的关注，因为它们可以从图数据中提取有用的表示。随着GNN计算的需求越来越高，各种优化GNN聚合的编程封装出现了，以便加速。然而，现有的编程封装没有得到全面的评估和分析，因此没有明确的共识，哪个方法更好。在这封信中，我们将现有的GNN聚合编程封装分类为数据组织维度和传播方法。通过在当今的GNN库上构建这些封装，我们进行了详细的性能和效率Characterization研究，并提供了一些关于未来GNN加速的反思。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Acceleration-of-Infinite-Horizon-Average-Reward-Reinforcement-Learning"><a href="#Quantum-Acceleration-of-Infinite-Horizon-Average-Reward-Reinforcement-Learning" class="headerlink" title="Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning"></a>Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11684">http://arxiv.org/abs/2310.11684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bhargav Ganguly, Vaneet Aggarwal</li>
<li>for: 这个论文探讨了量子加速器在解决无穷 horizon Markov Decision Processes（MDPs）中提高均衡奖励的可能性。</li>
<li>methods: 我们提出了一种创新的量子框架，让智能机器人在未知MDP中与其进行交互，从而超越传统交互模式。我们的方法基于一种以Optimism为驱动的表格学习算法，利用量子信号来估计量子平均值。</li>
<li>results: 我们通过了对论文的严格理论分析，证明量子估计优势导致量子算法在无穷Horizon Reinforcement学习中获得了极大的进步，具体来说，我们的量子算法可以实现$\tilde{\mathcal{O}(1)$的 regret bound，比 классические对手的$\tilde{\mathcal{O}(\sqrt{T})$ bound明显更高。<details>
<summary>Abstract</summary>
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}(1)$, a significant improvement over the $\tilde{\mathcal{O}(\sqrt{T})$ bound exhibited by classical counterparts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Descriptive-Knowledge-Graph-in-Biomedical-Domain"><a href="#Descriptive-Knowledge-Graph-in-Biomedical-Domain" class="headerlink" title="Descriptive Knowledge Graph in Biomedical Domain"></a>Descriptive Knowledge Graph in Biomedical Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11681">http://arxiv.org/abs/2310.11681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerui Zhu, Jie Huang, Kevin Chen-Chuan Chang</li>
<li>for: 该论文旨在提供一种自动抽取和生成有用和描述性句子的系统，以便有效地搜索生物医学知识。</li>
<li>methods: 该系统使用ChatGPT和一个精度调整的关系合成模型，自动生成有用和可靠的描述句子，从而减少了人类阅读努力。</li>
<li>results: 该系统可以帮助研究人员轻松地获得高级知识和详细参考，并且可以交互地循序搜索到有关的信息。在COVID-19研究中，该系统得到了广泛的应用，如药物重用和文献筛选。<details>
<summary>Abstract</summary>
We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas such as drug repurposing and literature curation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的系统，可以自动提取和生成有用和描述性的句子从生物医学词库，以便高效地搜索关系知识。与过去的搜索引擎或探索系统不同，我们的系统将描述句子组织成关系图，allowing researchers to explore closely related biomedical entities (例如，由化学物质治疗的疾病) or indirectly connected entities (例如，用于治疗疾病的潜在药物).我们的系统还使用ChatGPT和一种精心调整的关系合成模型，从检索到的信息中生成高度可靠和 concise的描述句子，从而减少了人类阅读努力。通过我们的系统，研究人员可以轻松地获得高级知识和详细参考，并且可以互动地导航到 interessant information。我们在COVID-19研究中强调了我们的系统的应用，例如药物重用和文献筛选。
</details></li>
</ul>
<hr>
<h2 id="Using-Experience-Classification-for-Training-Non-Markovian-Tasks"><a href="#Using-Experience-Classification-for-Training-Non-Markovian-Tasks" class="headerlink" title="Using Experience Classification for Training Non-Markovian Tasks"></a>Using Experience Classification for Training Non-Markovian Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11678">http://arxiv.org/abs/2310.11678</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixuan Miao, Xu Lu, Cong Tian, Bin Yu, Zhenhua Duan</li>
<li>for: 解决实际任务中的非Markovian任务，即奖励不仅基于当前状态，而且基于状态历史。</li>
<li>methods: 提出一种新的强化学习方法，利用线性时间逻辑LTL$_f$编码到Markov决策过程中，以便利用先进的RL算法。</li>
<li>results: 通过在多个 benchmark 问题中实践，证明了我们的方法的可行性和效果。<details>
<summary>Abstract</summary>
Unlike the standard Reinforcement Learning (RL) model, many real-world tasks are non-Markovian, whose rewards are predicated on state history rather than solely on the current state. Solving a non-Markovian task, frequently applied in practical applications such as autonomous driving, financial trading, and medical diagnosis, can be quite challenging. We propose a novel RL approach to achieve non-Markovian rewards expressed in temporal logic LTL$_f$ (Linear Temporal Logic over Finite Traces). To this end, an encoding of linear complexity from LTL$_f$ into MDPs (Markov Decision Processes) is introduced to take advantage of advanced RL algorithms. Then, a prioritized experience replay technique based on the automata structure (semantics equivalent to LTL$_f$ specification) is utilized to improve the training process. We empirically evaluate several benchmark problems augmented with non-Markovian tasks to demonstrate the feasibility and effectiveness of our approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improved-Sample-Complexity-Analysis-of-Natural-Policy-Gradient-Algorithm-with-General-Parameterization-for-Infinite-Horizon-Discounted-Reward-Markov-Decision-Processes"><a href="#Improved-Sample-Complexity-Analysis-of-Natural-Policy-Gradient-Algorithm-with-General-Parameterization-for-Infinite-Horizon-Discounted-Reward-Markov-Decision-Processes" class="headerlink" title="Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes"></a>Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm with General Parameterization for Infinite Horizon Discounted Reward Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11677">http://arxiv.org/abs/2310.11677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Washim Uddin Mondal, Vaneet Aggarwal</li>
<li>for: 这个论文是关于设计高效采样学习算法的研究，特别是针对无穷 horizon 折扣奖励Markov决策过程。</li>
<li>methods: 这个算法使用加速的随机梯度下降过程来获得自然策略偏导。</li>
<li>results: 这个算法可以达到 $\mathcal{O}({\epsilon^{-2})$ 样本复杂度和 $\mathcal{O}(\epsilon^{-1})$ 迭代复杂度，比现状态艺术ifactoria 样本复杂度增加 $\log(\frac{1}{\epsilon})$ 因子。此外，这个算法不需要不可证明的假设，即IS重要性的方差是上界bounded。在Hessian-free和IS-free算法中，ANPG beat最佳样本复杂度的记录，同时与其他最佳迭代复杂度匹配。<details>
<summary>Abstract</summary>
We consider the problem of designing sample efficient learning algorithms for infinite horizon discounted reward Markov Decision Process. Specifically, we propose the Accelerated Natural Policy Gradient (ANPG) algorithm that utilizes an accelerated stochastic gradient descent process to obtain the natural policy gradient. ANPG achieves $\mathcal{O}({\epsilon^{-2})$ sample complexity and $\mathcal{O}(\epsilon^{-1})$ iteration complexity with general parameterization where $\epsilon$ defines the optimality error. This improves the state-of-the-art sample complexity by a $\log(\frac{1}{\epsilon})$ factor. ANPG is a first-order algorithm and unlike some existing literature, does not require the unverifiable assumption that the variance of importance sampling (IS) weights is upper bounded. In the class of Hessian-free and IS-free algorithms, ANPG beats the best-known sample complexity by a factor of $\mathcal{O}(\epsilon^{-\frac{1}{2})$ and simultaneously matches their state-of-the-art iteration complexity.
</details>
<details>
<summary>摘要</summary>
我们考虑无限 horizon 折抵质量评估 Markov Decision Process 的问题。我们提出了加速自然策略导数（ANPG）算法，它利用加速随机Gradient Descent 过程来获得自然策略导数。ANPG 实现了 $\mathcal{O}({\epsilon^{-2})$ 样本复杂性和 $\mathcal{O}(\epsilon^{-1})$ 迭代复杂性，其中 $\epsilon$ 定义了优化误差。这超过了现有的 state-of-the-art 样本复杂性中的 $\log(\frac{1}{\epsilon})$ 因子。ANPG 是一个首顺算法，不需要先前的文献中未能证明的不可靠的假设，即 importance sampling 的 variance 的Upper Bounded。在 Hessian-free 和 IS-free 数据中，ANPG 比最好的 known sample complexity 的factor $\mathcal{O}(\epsilon^{-\frac{1}{2})$ ，同时将其state-of-the-art迭代复杂性与最佳的 state-of-the-art 匹配。
</details></li>
</ul>
<hr>
<h2 id="PREM-A-Simple-Yet-Effective-Approach-for-Node-Level-Graph-Anomaly-Detection"><a href="#PREM-A-Simple-Yet-Effective-Approach-for-Node-Level-Graph-Anomaly-Detection" class="headerlink" title="PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection"></a>PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11676">http://arxiv.org/abs/2310.11676</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/campanulabells/prem-gad">https://github.com/campanulabells/prem-gad</a></li>
<li>paper_authors: Junjun Pan, Yixin Liu, Yizhen Zheng, Shirui Pan</li>
<li>for: 本研究旨在提高图structured数据中节点级别异常检测效率，并提供一种简单可行的方法来实现这一目标。</li>
<li>methods: 该方法称为PREM，包括两个模块：预处理模块和ego- neighborg matching模块。PREM方法不需要传输消息传递，而是使用简单的对比损失函数，从而大幅提高训练速度和内存使用效率。</li>
<li>results: 经过对五种真实世界数据集的严格评估，PREM方法显示了robustness和效果。特别是在ACM数据集上，PREM方法与最高效的基线方法相比，提高了5%的AUC，提高了9倍的训练速度，并大幅降低内存使用量。<details>
<summary>Abstract</summary>
Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies - reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreover, through rigorous evaluations of five real-world datasets, our method demonstrated robustness and effectiveness. Notably, when validated on the ACM dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training speed, and sharply reduce memory usage compared to the most efficient baseline.
</details>
<details>
<summary>摘要</summary>
nodal-level 图像异常检测 (GAD) 在不同领域中，如医学、社交网络和电商，扮演了重要的角色，以识别图像中异常的节点。然而，由于异常的多样性以及标注数据的缺乏，存在许多挑战。现有的方法ologies，如重建基于的方法和对比学习，虽然有效，但往往受到效率问题的困扰，这些问题来自于复杂的目标函数和复杂的模块。为了改善 GAD 的效率，我们提出了一种简单的方法，称为 PREprocessing 和 Matching (PREM)。我们的方法通过流elines 节点级别的图像数据，从而减少训练时间和内存使用，同时保持强大的异常检测能力。PREM 包括两个模块：预处理模块和一个 Egon 的匹配模块。我们的方法不需要在训练期间进行消息传递，而是使用一个简单的对比损失函数，从而导致训练时间和内存使用的减少。此外，我们对五个真实世界数据集进行了严格的评估，我们的方法在robustness和效果两个方面具有出色的表现。特别是在 ACM 数据集上，PREM 可以在训练速度、内存使用和 AUC 等方面与最高效的基eline 相比，达到 5% 的提升，9 倍增加训练速度，并显著减少内存使用。
</details></li>
</ul>
<hr>
<h2 id="Prototype-based-HyperAdapter-for-Sample-Efficient-Multi-task-Tuning"><a href="#Prototype-based-HyperAdapter-for-Sample-Efficient-Multi-task-Tuning" class="headerlink" title="Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning"></a>Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11670">http://arxiv.org/abs/2310.11670</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bumble666/pha">https://github.com/bumble666/pha</a></li>
<li>paper_authors: Hao Zhao, Jie Fu, Zhaofeng He</li>
<li>for: 这个研究是为了提高预训练语言模型的扩展性和数据效率。</li>
<li>methods: 这篇论文使用了参数效率的精致调整（PEFT）方法，并提出了一个名为实例紧密抽象（PHA）的新框架，它使用了适应器调整和超级网络来生成条件模组。</li>
<li>results: 这篇论文的实验结果显示，PHA方法在多任务学习和几少例转移学习中比较其他强基eline方法表现更好，尤其是当资料量变少时。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pre-trained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-off between trainable parameters, accuracy on stream tasks, and sample efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SOTOPIA-Interactive-Evaluation-for-Social-Intelligence-in-Language-Agents"><a href="#SOTOPIA-Interactive-Evaluation-for-Social-Intelligence-in-Language-Agents" class="headerlink" title="SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents"></a>SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11667">http://arxiv.org/abs/2310.11667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap</li>
<li>for: 评估人工智能系统的社会智能能力</li>
<li>methods: 使用LLM-based agents和人类角色扮演者进行社会交互 scenario，并使用SOTOPIA-Eval评估框架评估模型的表现</li>
<li>results: 发现GPT-4在SOTOPIA-hard subsets中表现较差，其社交常识理解和战略通信技能受限，而人类则在这些 subsets中表现出优异的社会智能能力。<details>
<summary>Abstract</summary>
Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.
</details>
<details>
<summary>摘要</summary>
人类是社交生物，我们在日常互动中追求社交目标，这是人工智能系统的能力领域中的一个关键方面。然而，人工智能系统在这个领域的能力仍然尚未得到解释。我们提出了SOTOPIA，一个开放式环境，用于模拟人工智能代理人在复杂社交交互中的表现。在我们的环境中，代理人扮演和互动，在多种情况下协同合作、交换和竞争以完成复杂社交目标。我们在这个任务空间中模拟了LLM基于代理人和人类之间的角色扮演互动，并使用SOTOPIA-Eval全面评价框架进行评估。与SOTOPIA的使用，我们发现了不同的人工智能模型在社交智能方面存在显著差异，并确定了一个通用难度集合（SOTOPIA-hard），该集合对所有模型都是挑战性的。我们发现在这个集合中，GPT-4的目标完成率远低于人类，并且它很难展现社交感知和战略通信技能。这些发现表明SOTOPIA的潜在价值，作为一个通用的人工智能社交评价和改进平台。
</details></li>
</ul>
<hr>
<h2 id="Hetero-2-Net-Heterophily-aware-Representation-Learning-on-Heterogenerous-Graphs"><a href="#Hetero-2-Net-Heterophily-aware-Representation-Learning-on-Heterogenerous-Graphs" class="headerlink" title="Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs"></a>Hetero$^2$Net: Heterophily-aware Representation Learning on Heterogenerous Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11664">http://arxiv.org/abs/2310.11664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jintang Li, Zheng Wei, Jiawang Dan, Jing Zhou, Yuchang Zhu, Ruofan Wu, Baokun Wang, Zhang Zhen, Changhua Meng, Hong Jin, Zibin Zheng, Liang Chen</li>
<li>for: 本研究旨在 investigating the heterophily properties in heterogeneous graphs, and developing a heterophily-aware graph neural network (HGNN) to effectively handle more complex heterogeneous graphs.</li>
<li>methods: 我们使用 metapaths to identify the heterophily in heterogeneous graphs, and propose two practical metrics to quantitatively describe the levels of heterophily. We also introduce Hetero$^2$Net, a heterophily-aware HGNN that incorporates both masked metapath prediction and masked label prediction tasks to effectively handle both homophilic and heterophilic heterogeneous graphs.</li>
<li>results: 我们在 five real-world heterogeneous graph benchmarks with varying levels of heterophily 上 evaluate the performance of Hetero$^2$Net, and demonstrate that it outperforms strong baselines in the semi-supervised node classification task, providing valuable insights into effectively handling more complex heterogeneous graphs.<details>
<summary>Abstract</summary>
Real-world graphs are typically complex, exhibiting heterogeneity in the global structure, as well as strong heterophily within local neighborhoods. While a growing body of literature has revealed the limitations of common graph neural networks (GNNs) in handling homogeneous graphs with heterophily, little work has been conducted on investigating the heterophily properties in the context of heterogeneous graphs. To bridge this research gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs exhibiting varying levels of heterophily, we have observed that heterogeneous graph neural networks (HGNNs), which inherit many mechanisms from GNNs designed for homogeneous graphs, fail to generalize to heterogeneous graphs with heterophily or low level of homophily. To address the challenge, we present Hetero$^2$Net, a heterophily-aware HGNN that incorporates both masked metapath prediction and masked label prediction tasks to effectively and flexibly handle both homophilic and heterophilic heterogeneous graphs. We evaluate the performance of Hetero$^2$Net on five real-world heterogeneous graph benchmarks with varying levels of heterophily. The results demonstrate that Hetero$^2$Net outperforms strong baselines in the semi-supervised node classification task, providing valuable insights into effectively handling more complex heterogeneous graphs.
</details>
<details>
<summary>摘要</summary>
To address this gap, we identify the heterophily in heterogeneous graphs using metapaths and propose two practical metrics to quantitatively describe the levels of heterophily. Through in-depth investigations on several real-world heterogeneous graphs with varying levels of heterophily, we find that existing heterogeneous graph neural networks (HGNNs) fail to generalize to heterogeneous graphs with heterophily or low levels of homophily.To address this challenge, we present Hetero$^2$Net, a heterophily-aware HGNN that incorporates both masked metapath prediction and masked label prediction tasks to effectively and flexibly handle both homophilic and heterophilic heterogeneous graphs. We evaluate the performance of Hetero$^2$Net on five real-world heterogeneous graph benchmarks with varying levels of heterophily, and the results show that Hetero$^2$Net outperforms strong baselines in the semi-supervised node classification task, providing valuable insights into effectively handling more complex heterogeneous graphs.
</details></li>
</ul>
<hr>
<h2 id="Cloud-Magnetic-Resonance-Imaging-System-In-the-Era-of-6G-and-Artificial-Intelligence"><a href="#Cloud-Magnetic-Resonance-Imaging-System-In-the-Era-of-6G-and-Artificial-Intelligence" class="headerlink" title="Cloud-Magnetic Resonance Imaging System: In the Era of 6G and Artificial Intelligence"></a>Cloud-Magnetic Resonance Imaging System: In the Era of 6G and Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11641">http://arxiv.org/abs/2310.11641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yirong Zhou, Yanhuang Wu, Yuhan Su, Jing Li, Jianyun Cai, Yongfu You, Di Guo, Xiaobo Qu</li>
<li>for: 解决医疗机构年度生成巨量数据问题，提高医疗诊断精度和工作效率。</li>
<li>methods: integrating 分布式云计算、6G频率、边缘计算、联合学习和区块链技术。</li>
<li>results: 提高数据存储安全性、传输速度、人工智能算法维护、硬件升级和交叉机构医疗协作。<details>
<summary>Abstract</summary>
Magnetic Resonance Imaging (MRI) plays an important role in medical diagnosis, generating petabytes of image data annually in large hospitals. This voluminous data stream requires a significant amount of network bandwidth and extensive storage infrastructure. Additionally, local data processing demands substantial manpower and hardware investments. Data isolation across different healthcare institutions hinders cross-institutional collaboration in clinics and research. In this work, we anticipate an innovative MRI system and its four generations that integrate emerging distributed cloud computing, 6G bandwidth, edge computing, federated learning, and blockchain technology. This system is called Cloud-MRI, aiming at solving the problems of MRI data storage security, transmission speed, AI algorithm maintenance, hardware upgrading, and collaborative work. The workflow commences with the transformation of k-space raw data into the standardized Imaging Society for Magnetic Resonance in Medicine Raw Data (ISMRMRD) format. Then, the data are uploaded to the cloud or edge nodes for fast image reconstruction, neural network training, and automatic analysis. Then, the outcomes are seamlessly transmitted to clinics or research institutes for diagnosis and other services. The Cloud-MRI system will save the raw imaging data, reduce the risk of data loss, facilitate inter-institutional medical collaboration, and finally improve diagnostic accuracy and work efficiency.
</details>
<details>
<summary>摘要</summary>
The workflow of Cloud-MRI commences with the transformation of k-space raw data into the standardized Imaging Society for Magnetic Resonance in Medicine Raw Data (ISMRMRD) format. Then, the data are uploaded to the cloud or edge nodes for fast image reconstruction, neural network training, and automatic analysis. Finally, the outcomes are seamlessly transmitted to clinics or research institutes for diagnosis and other services.The Cloud-MRI system will save the raw imaging data, reduce the risk of data loss, facilitate inter-institutional medical collaboration, and finally improve diagnostic accuracy and work efficiency.Translated into Simplified Chinese:магнитно резонантно изображение (MRI) играет важную роль в медицинском диагнозирању, генеришући петабајтове количине слике података годишње у великим болницама. Овај обимни поток података захтева значајан удео мрежне брзине и екстензивну инфраструктуру за чување. Осим тога, локално обрадање података захтеваsubstantial ljudske ресурсе и инвестиције у хардвер. Ограничење података међу различитим здравственим установама отежава међуустанове медицинску сарадњу у клиникама и истраживањима. У овом раду, очекујемо иновативни систем MRI и његове четири генерације које интегришу емерингве дистрибуировану рачунарску облак технологију, 6G фреквенцију, ивицу рачунара, federated learning и блокчејн технологију. Овај систем се зове Cloud-MRI и има за циљ решења проблема чувања података MRI, брзине преноса, одржавања алгоритама, побољшања хардвера и сарадње.Радни процес Cloud-MRI почиње трансформацијомraw k-простора у стандардизовану форму Imaging Society for Magnetic Resonance in Medicine Raw Data (ISMRMRD). Затим, подаци се upload у облак или ивицу за брзо реконструкцију слике, тренинг неуралних мрежа и автоматско анализирање. На крају, извори се преносе безбедно на клинике или истраживачке институте за дијагнозу и друге услуге.Cloud-MRI систем ће чувати raw слике, смањити ризик губитка података, побољшати међуустанове медицинску сарадњу и на крају побољшати точност дијагнозе и ефикасност рада.
</details></li>
</ul>
<hr>
<h2 id="A-Symbolic-Language-for-Interpreting-Decision-Trees"><a href="#A-Symbolic-Language-for-Interpreting-Decision-Trees" class="headerlink" title="A Symbolic Language for Interpreting Decision Trees"></a>A Symbolic Language for Interpreting Decision Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11636">http://arxiv.org/abs/2310.11636</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diegoemilio01/a-symbolic-language-for-interpreting-decision-trees">https://github.com/diegoemilio01/a-symbolic-language-for-interpreting-decision-trees</a></li>
<li>paper_authors: Marcelo Arenas, Pablo Barcelo, Diego Bustamente, Jose Caraball, Bernardo Subercaseaux</li>
<li>for: 这个论文旨在探讨形式可解释AI的发展，探讨decision trees的可解释性问题，并提出了不同的可解释性查询和处理方法。</li>
<li>methods: 该论文使用了一种名为StratiFOILed的精心构造的 fragments of first-ordered logic，可以计算多种后期解释，包括本地解释（如推理和对比解释）和全局解释（如特征相关性）。</li>
<li>results: 该论文提出了ExplainDT，一种符号语言用于解释decision trees，可以根据用户需求来定制查询。StratiFOILed queries可以写作Boolean combination of NP-problems，可以在实践中使用常数数量的SAT解决器调用来评估。<details>
<summary>Abstract</summary>
The recent development of formal explainable AI has disputed the folklore claim that "decision trees are readily interpretable models", showing different interpretability queries that are computationally hard on decision trees, as well as proposing different methods to deal with them in practice. Nonetheless, no single explainability query or score works as a "silver bullet" that is appropriate for every context and end-user. This naturally suggests the possibility of "interpretability languages" in which a wide variety of queries can be expressed, giving control to the end-user to tailor queries to their particular needs. In this context, our work presents ExplainDT, a symbolic language for interpreting decision trees. ExplainDT is rooted in a carefully constructed fragment of first-ordered logic that we call StratiFOILed. StratiFOILed balances expressiveness and complexity of evaluation, allowing for the computation of many post-hoc explanations--both local (e.g., abductive and contrastive explanations) and global ones (e.g., feature relevancy)--while remaining in the Boolean Hierarchy over NP. Furthermore, StratiFOILed queries can be written as a Boolean combination of NP-problems, thus allowing us to evaluate them in practice with a constant number of calls to a SAT solver. On the theoretical side, our main contribution is an in-depth analysis of the expressiveness and complexity of StratiFOILed, while on the practical side, we provide an optimized implementation for encoding StratiFOILed queries as propositional formulas, together with an experimental study on its efficiency.
</details>
<details>
<summary>摘要</summary>
最近的形式可解AI发展有抵触了传统的说法，证明了决策树不是一种直观可解的模型，并提出了不同的可解性查询和处理方法。然而，没有一个单一的可解性查询或分数可以满足每个情况和用户需求。这自然地提出了“可解性语言”的概念，允许用户根据自己的需求定制查询。在这个上下文中，我们提出了ExplainDT，一种符号语言用于解释决策树。ExplainDT基于我们优化的一种首领逻辑，即StratiFOILed，该逻辑具有较高的表达力和评估复杂性，可以计算多种后期解释（包括地方的推理和对比解释以及全局的特征相关性），同时仍然保持在Boolean Hierarchy中。此外，StratiFOILed查询可以写作一个Boolean组合，因此可以通过一个常数数量的SAT解决器的调用来评估。从理论角度来看，我们的主要贡献是对StratiFOILed的表达力和评估复杂性进行深入分析，而从实践角度来看，我们提供了优化的编码方法和实验研究其效率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.AI_2023_10_18/" data-id="clpxp6bx3005nee8871d8b0lp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.CL_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T11:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.CL_2023_10_18/">cs.CL - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="REMARK-LLM-A-Robust-and-Efficient-Watermarking-Framework-for-Generative-Large-Language-Models"><a href="#REMARK-LLM-A-Robust-and-Efficient-Watermarking-Framework-for-Generative-Large-Language-Models" class="headerlink" title="REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models"></a>REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12362">http://arxiv.org/abs/2310.12362</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, Farinaz Koushanfar</li>
<li>for: 这个论文是为了设计一个高效、可靠的文本生成模型（LLM）水印框架。</li>
<li>methods: 论文提出了三个新组件：（i）一个学习基于的消息编码模块，用于把二进制签名注入到LLM生成的文本中；（ii）一个重parameterization模块，用于将密集分布转换为稀疏分布的水印文本符号；（iii）一个专门 для签名EXTRACTION的解码模块。</li>
<li>results: 论文通过对多个未看过的数据集进行严格的训练，证明REMARK-LLM可以插入2倍多的签名比特数据入文本中，同时保持 semantic integrity，并且在各种水印检测和移除攻击下展现出更好的抗性。<details>
<summary>Abstract</summary>
We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks.
</details>
<details>
<summary>摘要</summary>
我们介绍REMARK-LLM，一种新的高效、可靠的文本杂化框架，适用于大语言模型（LLM）生成的文本。使用LLM生成人类化内容需要庞大的计算资源和广泛的数据集，包括重要知识产权（IP）。然而，生成的内容容易被恶意利用，如垃圾邮件和抄袭。为解决这些挑战，REMARK-LLM提出了三个新组件：（i）一个学习基于的消息编码模块，用于在LLM生成的文本中混入binary标识符；（ii）一个重parameterization模块，将消息编码的稠密分布转换为稀疏分布的杂化文本token；（iii）一个专门 для抽取签名的解码模块。此外，我们引入了优化的搜索算法，以确保生成的内容具有准确性和一致性。REMARK-LLM在 Semantic integrity的保持和有效签名检索方面进行了严格的训练，同时能够插入2倍多的签名比特到同一个文本中，并且维护Semantic integrity。此外，REMARK-LLM表现出更好的抗干扰和抗除法风险。
</details></li>
</ul>
<hr>
<h2 id="GRI-Graph-based-Relative-Isomorphism-of-Word-Embedding-Spaces"><a href="#GRI-Graph-based-Relative-Isomorphism-of-Word-Embedding-Spaces" class="headerlink" title="GRI: Graph-based Relative Isomorphism of Word Embedding Spaces"></a>GRI: Graph-based Relative Isomorphism of Word Embedding Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12360">http://arxiv.org/abs/2310.12360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/asif6827/gri">https://github.com/asif6827/gri</a></li>
<li>paper_authors: Muhammad Asif Ali, Yan Hu, Jianbin Qin, Di Wang</li>
<li>for:  automatic construction of bilingual dictionaries using monolingual embedding spaces</li>
<li>methods:  combines distributional training objectives with attentive graph convolutions to consider the impact of semantically similar words</li>
<li>results:  outperforms existing research by improving the average P@1 by up to 63.6%<details>
<summary>Abstract</summary>
Automated construction of bilingual dictionaries using monolingual embedding spaces is a core challenge in machine translation. The end performance of these dictionaries relies upon the geometric similarity of individual spaces, i.e., their degree of isomorphism. Existing attempts aimed at controlling the relative isomorphism of different spaces fail to incorporate the impact of semantically related words in the training objective. To address this, we propose GRI that combines the distributional training objectives with attentive graph convolutions to unanimously consider the impact of semantically similar words required to define/compute the relative isomorphism of multiple spaces. Experimental evaluation shows that GRI outperforms the existing research by improving the average P@1 by a relative score of up to 63.6%. We release the codes for GRI at https://github.com/asif6827/GRI.
</details>
<details>
<summary>摘要</summary>
自动化建立双语词典使用单语空间的嵌入是机器翻译的核心挑战。这些词典的性能取决于各个空间的几何相似性，即他们的相对几何同构性。现有的尝试都没有考虑semantic关联的影响，即在训练目标中考虑相似的单词。为解决这个问题，我们提出了GRI，它将分布式训练目标与注意力 Graph Convolutions 结合，同时考虑多个空间中相似的单词，以统一评估多个空间的相对几何同构性。实验表明，GRI可以提高平均P@1的表现，相比现有研究提高63.6%。我们在github上分享了GRI代码，可以在https://github.com/asif6827/GRI中下载。
</details></li>
</ul>
<hr>
<h2 id="knn-seq-Efficient-Extensible-kNN-MT-Framework"><a href="#knn-seq-Efficient-Extensible-kNN-MT-Framework" class="headerlink" title="knn-seq: Efficient, Extensible kNN-MT Framework"></a>knn-seq: Efficient, Extensible kNN-MT Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12352">http://arxiv.org/abs/2310.12352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naist-nlp/knn-seq">https://github.com/naist-nlp/knn-seq</a></li>
<li>paper_authors: Hiroyuki Deguchi, Hayate Hirano, Tomoki Hoshino, Yuto Nishida, Justin Vasselli, Taro Watanabe</li>
<li>for: 提高 translate quality，使用 translation examples during decoding</li>
<li>methods: 使用 k-nearest-neighbor machine translation (kNN-MT) 和 vector database (datastore)</li>
<li>results: 实现了一个高效的 kNN-MT 框架，可以快速构建大规模的 datastore，并在 WMT’19 German-to-English 翻译任务中实现了相当的提升。<details>
<summary>Abstract</summary>
k-nearest-neighbor machine translation (kNN-MT) boosts the translation quality of a pre-trained neural machine translation (NMT) model by utilizing translation examples during decoding. Translation examples are stored in a vector database, called a datastore, which contains one entry for each target token from the parallel data it is made from. Due to its size, it is computationally expensive both to construct and to retrieve examples from the datastore. In this paper, we present an efficient and extensible kNN-MT framework, knn-seq, for researchers and developers that is carefully designed to run efficiently, even with a billion-scale large datastore. knn-seq is developed as a plug-in on fairseq and easy to switch models and kNN indexes. Experimental results show that our implemented kNN-MT achieves a comparable gain to the original kNN-MT, and the billion-scale datastore construction took 2.21 hours in the WMT'19 German-to-English translation task. We publish our knn-seq as an MIT-licensed open-source project and the code is available on https://github.com/naist-nlp/knn-seq . The demo video is available on https://youtu.be/zTDzEOq80m0 .
</details>
<details>
<summary>摘要</summary>
k- nearest-neighbor机器翻译（kNN-MT）可以提高一个预训练的神经机器翻译（NMT）模型的翻译质量，通过在解码过程中使用翻译示例。翻译示例被存储在一个vector数据库中，称为datastore，每个目标单词都有一个入口。由于其大小，construct和retrieve示例从datastore是计算昂贵的。在这篇论文中，我们提出了一个高效和可扩展的kNN-MT框架，knn-seq，这是为研究人员和开发人员设计的，可以高效运行，即使数据存储量达到了十亿级。knn-seq是一个plug-in在fairseq上，可以方便地更换模型和kNN索引。实验结果表明，我们实现的kNN-MT可以与原始kNN-MT做比较，并且构建了一个百亿级datastore只需2.21小时在WMT'19德语到英语翻译任务中。我们在MIT许可下发布了knn-seq作为开源项目，代码可以在https://github.com/naist-nlp/knn-seq上获取。demo视频可以在https://youtu.be/zTDzEOq80m0上找到。
</details></li>
</ul>
<hr>
<h2 id="LACMA-Language-Aligning-Contrastive-Learning-with-Meta-Actions-for-Embodied-Instruction-Following"><a href="#LACMA-Language-Aligning-Contrastive-Learning-with-Meta-Actions-for-Embodied-Instruction-Following" class="headerlink" title="LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following"></a>LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12344">http://arxiv.org/abs/2310.12344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joeyy5588/lacma">https://github.com/joeyy5588/lacma</a></li>
<li>paper_authors: Cheng-Fu Yang, Yen-Chun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, Yu-Chiang Frank Wang, Kai-Wei Chang</li>
<li>for: 这种 paper 的目的是提高 Embodied Instruction Following 中的泛化能力，使 agents 能够在未看过的环境中更好地执行任务。</li>
<li>methods: 这种 paper 使用了 contrastive learning 和 meta-actions 来解决 Embodied Instruction Following 中的泛化问题。</li>
<li>results:  compared to a strong multi-modal Transformer baseline, 这种方法 achieved a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following.I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent's insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent's hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent's low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environments. Compared to a strong multi-modal Transformer baseline, we achieve a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following. Additional analysis shows that the contrastive objective and meta-actions are complementary in achieving the best results, and the resulting agent better aligns its states with corresponding instructions, making it more suitable for real-world embodied agents. The code is available at: https://github.com/joeyy5588/LACMA.
</details>
<details>
<summary>摘要</summary>
END-TO-END 转换器在训练中见过环境下的Embodied Instruction Following任务中表现出色，但在未经训练的环境下却表现不佳，这导致了模型的普适性受到限制。这种问题的原因在于模型对自然语言指令的敏感性不够，这使得模型在不同环境下无法适应。为了解决这个问题，我们提议通过对模型隐藏状态与指令进行对齐来提高模型的敏感性。然而，高级语言指令和模型的低级动作空间之间的差距仍然存在，这使得模型困难地将高级语言指令翻译成低级动作。为了解决这个问题，我们提出了一种新的概念——元动作。元动作是在原始动作序列中提取出的普适的动作模式，它们可以帮助模型更好地理解高级语言指令的含义。当元动作作为训练信号时，模型在未经训练的环境下的总成功率得到了显著的提高。相比于一个强大的多Modal Transformer参考点，我们在未经训练的ALFRED Embodied Instruction Following任务中实现了4.5%的绝对提升。更进一步的分析表明，对比于对照学习和元动作的融合，我们的方法更好地实现了模型与指令之间的对齐，使得模型更适合实际的具体体现agent。代码可以在以下链接获取：https://github.com/joeyy5588/LACMA。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Pointwise-mathcal-V-Usable-Information-In-Context-ly"><a href="#Measuring-Pointwise-mathcal-V-Usable-Information-In-Context-ly" class="headerlink" title="Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly"></a>Measuring Pointwise $\mathcal{V}$-Usable Information In-Context-ly</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12300">http://arxiv.org/abs/2310.12300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/boblus/in-context-pvi">https://github.com/boblus/in-context-pvi</a></li>
<li>paper_authors: Sheng Lu, Shan Chen, Yingya Li, Danielle Bitterman, Guergana Savova, Iryna Gurevych</li>
<li>for: 这个论文是关于听 Context 学习（ICL）中的一种新的学习方法。</li>
<li>methods: 这篇论文使用了一种已经提出的困难度度量指标——点wise $\mathcal{V}$-usable information（PVI），并将其适应到了听 Context 版本（in-context PVI）。相比原始的 PVI，in-context PVI 更加高效，只需要一些示例和不需要调整。</li>
<li>results: 我们进行了一项广泛的实验分析，以评估in-context PVI 的可靠性。我们的发现表明，in-context PVI 估计值具有类似的特性于原始 PVI。具体地说，在听 Context 设置下，in-context PVI 估计值具有稳定的特性，不受不同的示例选择和射击数的影响。此外，我们还示了如何使用 in-context PVI 来标识困难的实例。这篇论文强调了 in-context PVI 的潜在价值和ICL的可能性。<details>
<summary>Abstract</summary>
In-context learning (ICL) is a new learning paradigm that has gained popularity along with the development of large language models. In this work, we adapt a recently proposed hardness metric, pointwise $\mathcal{V}$-usable information (PVI), to an in-context version (in-context PVI). Compared to the original PVI, in-context PVI is more efficient in that it requires only a few exemplars and does not require fine-tuning. We conducted a comprehensive empirical analysis to evaluate the reliability of in-context PVI. Our findings indicate that in-context PVI estimates exhibit similar characteristics to the original PVI. Specific to the in-context setting, we show that in-context PVI estimates remain consistent across different exemplar selections and numbers of shots. The variance of in-context PVI estimates across different exemplar selections is insignificant, which suggests that in-context PVI are stable. Furthermore, we demonstrate how in-context PVI can be employed to identify challenging instances. Our work highlights the potential of in-context PVI and provides new insights into the capabilities of ICL.
</details>
<details>
<summary>摘要</summary>
新学习理念“内容学习”（ICL）随着大语言模型的发展而受到关注。在这项工作中，我们对一种最近提出的困难度度量，点对可用信息（PVI）进行了适应。与原始PVI相比，内容PVI更加高效，只需几个示例并无需微调。我们进行了广泛的实验分析，以评估内容PVI的可靠性。我们的发现表明，内容PVI估计具有与原始PVI相似的特征。具体来说，在内容设置下，内容PVI估计具有不同示例选择和射击数量的稳定性。 var（内容PVI估计）在不同示例选择下的差异不显著，这表明内容PVI是稳定的。此外，我们还证明了内容PVI可以用于标识困难实例。我们的工作探讨了内容PVI的潜力和ICL的可能性，并提供了新的视角。
</details></li>
</ul>
<hr>
<h2 id="Direct-Neural-Machine-Translation-with-Task-level-Mixture-of-Experts-models"><a href="#Direct-Neural-Machine-Translation-with-Task-level-Mixture-of-Experts-models" class="headerlink" title="Direct Neural Machine Translation with Task-level Mixture of Experts models"></a>Direct Neural Machine Translation with Task-level Mixture of Experts models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12236">http://arxiv.org/abs/2310.12236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isidora Chara Tourni, Subhajit Naskar</li>
<li>for: 这个论文主要研究了Direct Neural Machine Translation（直接神经机器翻译）系统，它可以将文本翻译成两种不同语言之间。</li>
<li>methods: 论文提出了多种方法来解决直接NMT系统的限制，包括多语言NMT和中间语言NMT（通过英语翻译）。它们还提出了Task-level Mixture of expert models（任务级混合专家模型），一种基于Transformer模型的推理效率优化方法。</li>
<li>results: 论文表明，Task-level MoE-based direct NMT系统在大量低资源和高资源irect对的翻译任务上表现出色，并且在7种语言对上超过了双语和中间语言NMT模型。<details>
<summary>Abstract</summary>
Direct neural machine translation (direct NMT) is a type of NMT system that translates text between two non-English languages. Direct NMT systems often face limitations due to the scarcity of parallel data between non-English language pairs. Several approaches have been proposed to address this limitation, such as multilingual NMT and pivot NMT (translation between two languages via English). Task-level Mixture of expert models (Task-level MoE), an inference-efficient variation of Transformer-based models, has shown promising NMT performance for a large number of language pairs. In Task-level MoE, different language groups can use different routing strategies to optimize cross-lingual learning and inference speed. In this work, we examine Task-level MoE's applicability in direct NMT and propose a series of high-performing training and evaluation configurations, through which Task-level MoE-based direct NMT systems outperform bilingual and pivot-based models for a large number of low and high-resource direct pairs, and translation directions. Our Task-level MoE with 16 experts outperforms bilingual NMT, Pivot NMT models for 7 language pairs, while pivot-based models still performed better in 9 pairs and directions.
</details>
<details>
<summary>摘要</summary>
直接神经机器翻译（直接NMT）是一种NMT系统，用于翻译非英语语言对。直接NMT系统经常面临限制，即非英语语言对的并不充足。多种方法已经提出来解决这个问题，如多语言NMT和中转NMT（通过英语翻译）。任务级别混合模型（Task-level MoE），一种基于转换器模型的推理效率版本，在许多语言对上表现出了优秀的NMT性能。在Task-level MoE中，不同语言组可以使用不同的路由策略来优化对语言之间的学习和推理速度。在本研究中，我们研究Task-level MoE在直接NMT中的适用性，并提出了一系列高性能的训练和评估配置。通过这些配置，Task-level MoE基于直接NMT系统在大量低资源和高资源直接对的翻译方向上表现出了比比较好的成绩。我们的Task-level MoE系统与16个专家相比，超过了双语NMT和中转NMT模型在7个语言对上的性能。然而，中转NMT模型仍然在9个语言对和方向上表现出了较好的成绩。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Retrieval-Augmentation-for-Long-Form-Question-Answering"><a href="#Understanding-Retrieval-Augmentation-for-Long-Form-Question-Answering" class="headerlink" title="Understanding Retrieval Augmentation for Long-Form Question Answering"></a>Understanding Retrieval Augmentation for Long-Form Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12150">http://arxiv.org/abs/2310.12150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hung-Ting Chen, Fangyuan Xu, Shane A. Arora, Eunsol Choi</li>
<li>for: 本研究探讨了含有检索增强的语言模型（LM）在长问题回答中的表现。研究如何在不同的检索文档集中使用LM进行回答生成，以及不同检索文档集对LM的回答生成造成的影响。</li>
<li>methods: 本研究使用了不同的检索文档集，并对LM的回答生成进行了分析。研究包括对生成的答案进行评估，以及使用人工标注来评估答案的归因。</li>
<li>results: 研究发现，使用不同检索文档集可以影响LM的回答生成质量。此外，研究还发现了长文本生成中的归因模式，以及LM的归因错误的主要原因。<details>
<summary>Abstract</summary>
We present a study of retrieval-augmented language models (LMs) on long-form question answering. We analyze how retrieval augmentation impacts different LMs, by comparing answers generated from models while using the same evidence documents, and how differing quality of retrieval document set impacts the answers generated from the same LM. We study various attributes of generated answers (e.g., fluency, length, variance) with an emphasis on the attribution of generated long-form answers to in-context evidence documents. We collect human annotations of answer attribution and evaluate methods for automatically judging attribution. Our study provides new insights on how retrieval augmentation impacts long, knowledge-rich text generation of LMs. We further identify attribution patterns for long text generation and analyze the main culprits of attribution errors. Together, our analysis reveals how retrieval augmentation impacts long knowledge-rich text generation and provide directions for future work.
</details>
<details>
<summary>摘要</summary>
我们提出了一项研究，探讨 Retrieval-augmented 语言模型（LM）在长问答中的表现。我们分析了不同LM在使用同一份证据文档时的响应，以及不同证据文档集的质量如何影响LM生成的答案。我们研究了各种答案特征（如流畅度、长度、变化程度），强调在上下文文档中归因生成的长文答案。我们收集了人类标注答案归因的数据，并评估了自动判断归因的方法。我们的研究提供了新的认知，揭示了 Retrieval-augmented 语言模型在长知识含量文本生成中的影响，以及长文生成中的归因模式和错误的主要原因。这些分析结果为未来工作提供了方向。
</details></li>
</ul>
<hr>
<h2 id="Simple-Mechanisms-for-Representing-Indexing-and-Manipulating-Concepts"><a href="#Simple-Mechanisms-for-Representing-Indexing-and-Manipulating-Concepts" class="headerlink" title="Simple Mechanisms for Representing, Indexing and Manipulating Concepts"></a>Simple Mechanisms for Representing, Indexing and Manipulating Concepts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12143">http://arxiv.org/abs/2310.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanzhi Li, Raghu Meka, Rina Panigrahy, Kulin Shah</li>
<li>for: 这篇论文旨在提出一种新的方法来学习概念，而不是通过传统的分类器来做。</li>
<li>methods: 这种方法基于对概念的时间统计矩阵来生成具体的表示或签名，并通过了解这些签名的结构来找到更高级别的概念。</li>
<li>results: 该方法可以在不同的概念之间找到共同主题，并可以用来建立一个概念字典，以便将输入数据正确地归类到相关的概念中。<details>
<summary>Abstract</summary>
Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is used in this translation, which is a more casual and conversational style of Chinese. Traditional Chinese would be more formal and written.
</details></li>
</ul>
<hr>
<h2 id="Pseudointelligence-A-Unifying-Framework-for-Language-Model-Evaluation"><a href="#Pseudointelligence-A-Unifying-Framework-for-Language-Model-Evaluation" class="headerlink" title="Pseudointelligence: A Unifying Framework for Language Model Evaluation"></a>Pseudointelligence: A Unifying Framework for Language Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12135">http://arxiv.org/abs/2310.12135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shikhar Murty, Orr Paradise, Pratyusha Sharma</li>
<li>for: 评估模型能力的原则性方法</li>
<li>methods: 使用复杂性理论的动态互动模型和学习评估器</li>
<li>results: 可以用来评估语言模型的两个案例研究以及现有评估方法的分析<details>
<summary>Abstract</summary>
With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that "(perceived) intelligence lies in the eye of the beholder". That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model evaluation cast as a dynamic interaction between a model and a learned evaluator. We demonstrate that this framework can be used to reason about two case studies in language model evaluation, as well as analyze existing evaluation methods.
</details>
<details>
<summary>摘要</summary>
With large language models surpassing human performance on an increasing number of benchmarks, we must take a principled approach for targeted evaluation of model capabilities. Inspired by pseudorandomness, we propose pseudointelligence, which captures the maxim that "(perceived) intelligence lies in the eye of the beholder". That is, that claims of intelligence are meaningful only when their evaluator is taken into account. Concretely, we propose a complexity-theoretic framework of model evaluation cast as a dynamic interaction between a model and a learned evaluator. We demonstrate that this framework can be used to reason about two case studies in language model evaluation, as well as analyze existing evaluation methods.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="A-Tale-of-Pronouns-Interpretability-Informs-Gender-Bias-Mitigation-for-Fairer-Instruction-Tuned-Machine-Translation"><a href="#A-Tale-of-Pronouns-Interpretability-Informs-Gender-Bias-Mitigation-for-Fairer-Instruction-Tuned-Machine-Translation" class="headerlink" title="A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation"></a>A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12127">http://arxiv.org/abs/2310.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/milanlproc/interpretability-mt-gender-bias">https://github.com/milanlproc/interpretability-mt-gender-bias</a></li>
<li>paper_authors: Giuseppe Attanasio, Flor Miriam Plaza-del-Arco, Debora Nozza, Anne Lauscher</li>
<li>for: 本研究旨在探讨现有的语言模型是否带有性别偏见，以及如何 Mitigate 这种偏见。</li>
<li>methods: 本研究使用了一系列的可读性方法，包括计算已知的性别偏见指标，以及使用 few-shot learning 的方法来解决偏见问题。</li>
<li>results: 研究发现，IFT 模型默认将 male-inflected 翻译作为结果，甚至会忽略女性职业 gender 标签。此外，研究还发现模型在错误翻译中忽略 masculine 和 feminine  pronoun 的问题。基于这些发现，研究提出了一种简单、有效的偏见 Mitigation 解决方案，通过 few-shot learning 实现了更加公平的翻译结果。<details>
<summary>Abstract</summary>
Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it. Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few-shot learning that leads to significantly fairer translations.
</details>
<details>
<summary>摘要</summary>
现代指导模型可以解决多个自然语言处理任务，机器翻译（MT）是其中的一个重要用例。然而，当前的研究经常关注标准性能指标，而忽略了吸引人的公平和道德考虑。在MT中，这可能导致误射翻译，其中的一些危害包括延续偏见和预设。在这项工作中，我们填补这个遗漏，我们研究了IFT模型在机器翻译中是否存在性别偏见，以及如何缓解它。具体来说，我们在英语到德语和西班牙语的WinoMT corpus上计算了确定性别偏见的指标。我们发现，IFT模型默认使用♂inflected翻译，即使 female occupational stereotypes。然后，使用可见性方法，我们发现模型在误射翻译中系统地忽略指示翻译对象的性别的代名词。最后，基于这一发现，我们提出了一种易于实施的和有效的偏见缓解解决方案，该解决方案基于几 shot learning，可以导致非常公平的翻译。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Dataset-Cartography-for-Improved-Compositional-Generalization-in-Transformers"><a href="#Harnessing-Dataset-Cartography-for-Improved-Compositional-Generalization-in-Transformers" class="headerlink" title="Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers"></a>Harnessing Dataset Cartography for Improved Compositional Generalization in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12118">http://arxiv.org/abs/2310.12118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyberiada/cartography-for-compositionality">https://github.com/cyberiada/cartography-for-compositionality</a></li>
<li>paper_authors: Osman Batur İnce, Tanin Zeraati, Semih Yagcioglu, Yadollah Yaghoobzadeh, Erkut Erdem, Aykut Erdem</li>
<li>for: 提高Transformer模型的compositional generalization能力</li>
<li>methods: 使用dataset cartography作为curriculum learning criterion</li>
<li>results: 实现10%的提高精度在CFQ和COGS数据集上，无需hyperparameter tuning<details>
<summary>Abstract</summary>
Neural networks have revolutionized language modeling and excelled in various downstream tasks. However, the extent to which these models achieve compositional generalization comparable to human cognitive abilities remains a topic of debate. While existing approaches in the field have mainly focused on novel architectures and alternative learning paradigms, we introduce a pioneering method harnessing the power of dataset cartography (Swayamdipta et al., 2020). By strategically identifying a subset of compositional generalization data using this approach, we achieve a remarkable improvement in model accuracy, yielding enhancements of up to 10% on CFQ and COGS datasets. Notably, our technique incorporates dataset cartography as a curriculum learning criterion, eliminating the need for hyperparameter tuning while consistently achieving superior performance. Our findings highlight the untapped potential of dataset cartography in unleashing the full capabilities of compositional generalization within Transformer models. Our code is available at https://github.com/cyberiada/cartography-for-compositionality.
</details>
<details>
<summary>摘要</summary>
神经网络已经革命化语言模型化，并在各种下游任务中表现出色。然而，这些模型是否达到人类认知能力的 Compositional generalization 水平仍然是一个议题。现有的方法主要集中在新的建筑和学习方法上，而我们则提出了一种拓展 dataset cartography（Swayamdipta et al., 2020）的新方法。通过策略地选择 Compositional generalization 数据 subsets，我们实现了模型精度的显著提高，CFQ 和 COGS 数据集上的提高达到 10%。值得注意的是，我们的技术将 dataset cartography 作为课程学习标准，从而消除了 hyperparameter 调整的需求，并一直保持优秀的性能。我们的发现表明，使用 dataset cartography 可以解 liberate 传播模型中的 Compositional generalization 潜力。我们的代码可以在 GitHub 上找到：https://github.com/cyberiada/cartography-for-compositionality。
</details></li>
</ul>
<hr>
<h2 id="On-the-Benefit-of-Generative-Foundation-Models-for-Human-Activity-Recognition"><a href="#On-the-Benefit-of-Generative-Foundation-Models-for-Human-Activity-Recognition" class="headerlink" title="On the Benefit of Generative Foundation Models for Human Activity Recognition"></a>On the Benefit of Generative Foundation Models for Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12085">http://arxiv.org/abs/2310.12085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikang Leng, Hyeokhyen Kwon, Thomas Plötz</li>
<li>for:  solves the problem of limited annotated data in human activity recognition (HAR) by using generative AI to autonomously generate virtual IMU data from text descriptions.</li>
<li>methods:  uses Large Language Models (LLMs) and motion synthesis models to generate virtual IMU data.</li>
<li>results:  identifies several promising research pathways that could benefit from generative AI in HAR, including generating benchmark datasets, developing foundational models specific to HAR, exploring hierarchical structures within HAR, breaking down complex activities, and applications in health sensing and activity summarization.Here is the text in Simplified Chinese:</li>
<li>for: 解决人体活动识别（HAR）中数据稀缺问题，使用生成AI自动生成文本描述IMU数据。</li>
<li>methods: 使用大型语言模型（LLMs）和运动合成模型生成IMU数据。</li>
<li>results: 找到了生成AI在HAR中的许多有优势的研究方向，包括生成数据集、开发特有于HAR的基础模型、阶段分解复杂活动、应用于健康感知和活动概要。<details>
<summary>Abstract</summary>
In human activity recognition (HAR), the limited availability of annotated data presents a significant challenge. Drawing inspiration from the latest advancements in generative AI, including Large Language Models (LLMs) and motion synthesis models, we believe that generative AI can address this data scarcity by autonomously generating virtual IMU data from text descriptions. Beyond this, we spotlight several promising research pathways that could benefit from generative AI for the community, including the generating benchmark datasets, the development of foundational models specific to HAR, the exploration of hierarchical structures within HAR, breaking down complex activities, and applications in health sensing and activity summarization.
</details>
<details>
<summary>摘要</summary>
人类活动识别（HAR）中，数据缺乏问题是一大挑战。我们 Drawing inspiration from the latest advancements in generative AI，包括大型自然语言模型（LLM）和运动合成模型，我们认为生成AI可以解决这种数据缺乏问题，通过自动生成虚拟IMU数据从文本描述中。此外，我们还指出了许多有前途的研究方向，包括生成标准数据集，开发特有的HAR基础模型，探索HAR层次结构，分解复杂活动，以及医疗感知和活动概要应用。
</details></li>
</ul>
<hr>
<h2 id="Towards-Safer-Operations-An-Expert-involved-Dataset-of-High-Pressure-Gas-Incidents-for-Preventing-Future-Failures"><a href="#Towards-Safer-Operations-An-Expert-involved-Dataset-of-High-Pressure-Gas-Incidents-for-Preventing-Future-Failures" class="headerlink" title="Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures"></a>Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12074">http://arxiv.org/abs/2310.12074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cinnamon/incident-ai-dataset">https://github.com/cinnamon/incident-ai-dataset</a></li>
<li>paper_authors: Shumpei Inoue, Minh-Tien Nguyen, Hiroki Mizokuchi, Tuan-Anh D. Nguyen, Huu-Hiep Nguyen, Dung Tien Le</li>
<li>for: 这个研究是为了开发一个新的安全预防 dataset，用于应用自然语言处理（NLP）技术来分析事故报告，以预防未来的失败。</li>
<li>methods: 这个研究使用了三种任务：命名实体识别、 causa-effect 提取和信息检索。这些任务是由域专家 manually annotate，他们至少有六年的实践经验。</li>
<li>results: 初步的结果表明，NLP技术可以有效地分析事故报告，以预防未来的失败。 dataset 可以促进未来的研究在 NLP 和事故管理领域。 dataset 的访问也提供（IncidentAI dataset 可以在：<a target="_blank" rel="noopener" href="https://github.com/Cinnamon/incident-ai-dataset">https://github.com/Cinnamon/incident-ai-dataset</a> 中找到）。<details>
<summary>Abstract</summary>
This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the three tasks show that NLP techniques are beneficial for analyzing incident reports to prevent future failures. The dataset facilitates future research in NLP and incident management communities. The access to the dataset is also provided (the IncidentAI dataset is available at: https://github.com/Cinnamon/incident-ai-dataset).
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一个新的 IncidentAI 数据集，用于安全预防。与过去的数据集不同，我们的数据集包含三个任务：命名实体识别、 causal EXTRACTION 和信息检索。数据集由具有至少六年实践经验的高压气保存管理员进行标注。我们验证了数据集在安全预防方面的贡献。初步结果显示，NLP 技术可以有效地分析事故报告，以预防未来的失败。该数据集将促进未来 NLP 和事故管理社区的研究。数据集的访问权也提供（IncidentAI 数据集可以在：https://github.com/Cinnamon/incident-ai-dataset 中获取）。
</details></li>
</ul>
<hr>
<h2 id="SPEED-Speculative-Pipelined-Execution-for-Efficient-Decoding"><a href="#SPEED-Speculative-Pipelined-Execution-for-Efficient-Decoding" class="headerlink" title="SPEED: Speculative Pipelined Execution for Efficient Decoding"></a>SPEED: Speculative Pipelined Execution for Efficient Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12072">http://arxiv.org/abs/2310.12072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia Shao</li>
<li>for: 提高生成大型自然语言处理模型（LLMs）的实时推理效率，以便在各种自然语言处理任务中使用。</li>
<li>methods: 使用预测值基于早期层隐藏状态来спекулятив执行未来的多个字符，以实现多字符级别的并行推理。</li>
<li>results: 在Transformerdecoder中实现Parameter sharing，通过减少内存操作的负担，提高生成LLM推理的效率，并在模型准确率 versus 延迟时间之间取得平衡。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us to accelerate generative LLM inference. We demonstrate the efficiency of our method in terms of latency reduction relative to model accuracy and demonstrate how speculation allows for training deeper decoders with parameter sharing with minimal runtime overhead.
</details>
<details>
<summary>摘要</summary>
大量的自然语言处理任务中的生成大语言模型（LLM）基于Transformer架构最近占据了主导地位。然而，它们在实时场景中的应用受到了较大的推理延迟的限制。这主要是因为生成LLM的推理是sequential的，每个token都виси于所有前一个输出token。因此，难以实现任务级别的并行计算，使推理变得具有很高的内存约束。在这种情况下，我们提出了SPEED方法，它通过预测基于早期隐藏状态的值来спекулятив执行多个未来的token在并行的方式。对于使用参数共享的Transformer解码器，我们可以归并内存操作，这allow us以加速生成LLM推理。我们通过对响应率和模型精度之间的负载减少来证明我们的方法的效率。此外，我们还示出了通过 especulation进行训练更深的解码器，只需要最小的运行时开销。
</details></li>
</ul>
<hr>
<h2 id="Code-Book-for-the-Annotation-of-Diverse-Cross-Document-Coreference-of-Entities-in-News-Articles"><a href="#Code-Book-for-the-Annotation-of-Diverse-Cross-Document-Coreference-of-Entities-in-News-Articles" class="headerlink" title="Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles"></a>Code Book for the Annotation of Diverse Cross-Document Coreference of Entities in News Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12064">http://arxiv.org/abs/2310.12064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakob Vogel</li>
<li>for: 这篇论文是为了提出一种把核心引用跨文档进行标注的方案，超越传统的标识关系，还考虑到近似关系和连接关系。</li>
<li>methods: 这篇论文使用了一种名为“Inception”的注释工具，并提供了精确的注释实现方法，包括在新闻文章中标注实体，将其与多种核心引用关系连接起来，并将其与Wikidata全球知识图谱连接起来。</li>
<li>results: 这篇论文的主要贡献是提供了一种多层次注释方法，可以应用于媒体偏见分析中的词汇选择和标签。<details>
<summary>Abstract</summary>
This paper presents a scheme for annotating coreference across news articles, extending beyond traditional identity relations by also considering near-identity and bridging relations. It includes a precise description of how to set up Inception, a respective annotation tool, how to annotate entities in news articles, connect them with diverse coreferential relations, and link them across documents to Wikidata's global knowledge graph. This multi-layered annotation approach is discussed in the context of the problem of media bias. Our main contribution lies in providing a methodology for creating a diverse cross-document coreference corpus which can be applied to the analysis of media bias by word-choice and labelling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluating-the-Symbol-Binding-Ability-of-Large-Language-Models-for-Multiple-Choice-Questions-in-Vietnamese-General-Education"><a href="#Evaluating-the-Symbol-Binding-Ability-of-Large-Language-Models-for-Multiple-Choice-Questions-in-Vietnamese-General-Education" class="headerlink" title="Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education"></a>Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12059">http://arxiv.org/abs/2310.12059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duc-Vu Nguyen, Quoc-Nam Nguyen</li>
<li>for: 这个研究旨在评估大语言模型（LLM）在零shot、一shot、少shot设置下的多选符号绑定（MCSB）能力，以解决越南语MCQA任务。</li>
<li>methods: 研究使用了六种知名的LLM模型，namely BLOOMZ-7.1B-MT、LLaMA-2-7B、LLaMA-2-70B、GPT-3、GPT-3.5和GPT-4.0，对ViMMRC 1.0和ViMMRC 2.0数据集和我们提议的数据集进行评估。</li>
<li>results: 研究发现，这些LLM模型在越南语MCQA任务中具有扎实的MCSB能力，特别是在零shot和一shot设置下。<details>
<summary>Abstract</summary>
In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们评估了大语言模型（LLM）在零批、一批和几批设置下的多选符号绑定（MCSB）能力，用于多选问答（MCQA）任务。我们关注越南语言，因为越南语言MCQA数据集比英语更少。我们研究的两个现有数据集是 ViMMRC 1.0 和 ViMMRC 2.0，它们都是文学类。近期的越南语言自然语言处理（NLP）研究主要集中在评估 ChatGPT，但是这些研究主要集中在 ChatGPT 如何解决越南语言高中毕业考试（VNHSGE）。我们希望创建一个新的高质量数据集，提供了 LaTeX 格式的结构化指南，以便用于评估 LLM 和更小的语言模型（LM）的 MCSB 能力。我们的评估结果显示，六种著名的 LLM 在 ViMMRC 1.0 和 ViMMRC 2.0 标准和我们提议的数据集上表现出了预期的 MCSB 能力。数据集仅用于研究目的。
</details></li>
</ul>
<hr>
<h2 id="Concept-Guided-Chain-of-Thought-Prompting-for-Pairwise-Comparison-Scaling-of-Texts-with-Large-Language-Models"><a href="#Concept-Guided-Chain-of-Thought-Prompting-for-Pairwise-Comparison-Scaling-of-Texts-with-Large-Language-Models" class="headerlink" title="Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models"></a>Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12049">http://arxiv.org/abs/2310.12049</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing</li>
<li>for: 这篇论文旨在开发一种基于生成大语言模型（LLM）的文本扩展方法，以便不需要大量文本资料和标注数据，可以有效地处理短文本和缺乏标注数据的情况。</li>
<li>methods: 这篇论文提出了一种基于提示的概念指导链条（CGCoT）方法，通过设计特定的提示来概括想法和标识文本中的 Target 个体，然后使用这些提示来生成概念特定的分析结果，类似于人工编码分析的指导。</li>
<li>results: 这篇论文使用CGCoT方法和大语言模型（LLM）对 Twitter 上的情感语言进行了扩展，并证明了该方法可以生成与人类评价相符的投票结果，而且不需要大量标注数据。<details>
<summary>Abstract</summary>
Existing text scaling methods often require a large corpus, struggle with short texts, or require labeled data. We develop a text scaling method that leverages the pattern recognition capabilities of generative large language models (LLMs). Specifically, we propose concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis. CGCoT effectively shifts pairwise text comparisons from a reasoning problem to a pattern recognition problem. We then pairwise compare concept-specific breakdowns using an LLM. We use the results of these pairwise comparisons to estimate a scale using the Bradley-Terry model. We use this approach to scale affective speech on Twitter. Our measures correlate more strongly with human judgments than alternative approaches like Wordfish. Besides a small set of pilot data to develop the CGCoT prompts, our measures require no additional labeled data and produce binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands of human-labeled tweets. We demonstrate how combining substantive knowledge with LLMs can create state-of-the-art measures of abstract concepts.
</details>
<details>
<summary>摘要</summary>
现有的文本缩放方法通常需要大量数据集，困难处理短文本，或需要标注数据。我们开发了一种基于生成大语言模型（LLM）的文本缩放方法，具体来说是思想导向链条（CGCoT）。CGCoT使用用于概述想法和标识文本中targetparty的提示来生成思想特定的拆分，与人工编码分析类似。CGCoT将对比文本的对比问题转化为pattern recognition问题。然后，我们对每个拆分进行对比，使用LLM来对比拆分。我们使用这些对比结果来估算一个排名使用布莱德利-特里模型。我们使用这种方法来尺度Twitter上的情感语言。我们的度量与人类判断更高相关性，与替代方法如Wordfish相比。除了开发CGCoT提示的小数据集外，我们的度量不需要额外的标注数据，并且生成了与RoBERTa-Large模型 fine-tuned on thousands of human-labeled tweets相同的二进制预测。我们示例了如何将专业知识与LLM结合以创建状态的抽象概念度量。
</details></li>
</ul>
<hr>
<h2 id="CORE-A-Few-Shot-Company-Relation-Classification-Dataset-for-Robust-Domain-Adaptation"><a href="#CORE-A-Few-Shot-Company-Relation-Classification-Dataset-for-Robust-Domain-Adaptation" class="headerlink" title="CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation"></a>CORE: A Few-Shot Company Relation Classification Dataset for Robust Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12024">http://arxiv.org/abs/2310.12024</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnborchert/core">https://github.com/pnborchert/core</a></li>
<li>paper_authors: Philipp Borchert, Jochen De Weerdt, Kristof Coussement, Arno De Caigny, Marie-Francine Moens</li>
<li>for: 这 paper 是关于 few-shot 关系分类 (RC) 的研究, 特点是使用公司wiki页面中的文本证据。</li>
<li>methods: 这 paper 使用了 state-of-the-art RC 模型在 few-shot domain adaptation  Setting 中进行了实验，以评估模型在 CORE  dataset 上的性能。</li>
<li>results: 实验结果表明，当前的 RC 模型在 CORE dataset 上 exhibits substantial performance gaps, 并且模型在不同的领域上适应性不高。  however, 模型在 CORE 上训练显示出了改善的 out-of-domain 性能, 这表明高质量数据的重要性 для robust domain adaptation。<details>
<summary>Abstract</summary>
We introduce CORE, a dataset for few-shot relation classification (RC) focused on company relations and business entities. CORE includes 4,708 instances of 12 relation types with corresponding textual evidence extracted from company Wikipedia pages. Company names and business entities pose a challenge for few-shot RC models due to the rich and diverse information associated with them. For example, a company name may represent the legal entity, products, people, or business divisions depending on the context. Therefore, deriving the relation type between entities is highly dependent on textual context. To evaluate the performance of state-of-the-art RC models on the CORE dataset, we conduct experiments in the few-shot domain adaptation setting. Our results reveal substantial performance gaps, confirming that models trained on different domains struggle to adapt to CORE. Interestingly, we find that models trained on CORE showcase improved out-of-domain performance, which highlights the importance of high-quality data for robust domain adaptation. Specifically, the information richness embedded in business entities allows models to focus on contextual nuances, reducing their reliance on superficial clues such as relation-specific verbs. In addition to the dataset, we provide relevant code snippets to facilitate reproducibility and encourage further research in the field.
</details>
<details>
<summary>摘要</summary>
我们介绍了CORE数据集，专门用于几个shot关系分类（RC），关注公司关系和企业实体。CORE包含4,708个实例，12种关系类型的文本证据，从公司Wikipedia页面中提取。公司名称和商业实体可能会带来很多挑战，因为它们可能会表示法律实体、产品、人员或业务部门，具体取决于上下文。因此，从文本上提取关系类型 между实体是很有所依赖的。为了评估现有RC模型在CORE数据集上的性能，我们在几个shot领域适应设置下进行了实验。我们的结果表明，模型在不同领域的学习后，很难适应CORE。但是，模型在CORE上进行训练后，在其他领域的表现有所提高，这反映了高质量数据的重要性，以及企业实体中嵌入的信息 ricness，使模型更加注重上下文特征，减少对关系特有词的依赖。此外，我们还提供了相关的代码截图，以便复现和进一步研究。
</details></li>
</ul>
<hr>
<h2 id="LoHoRavens-A-Long-Horizon-Language-Conditioned-Benchmark-for-Robotic-Tabletop-Manipulation"><a href="#LoHoRavens-A-Long-Horizon-Language-Conditioned-Benchmark-for-Robotic-Tabletop-Manipulation" class="headerlink" title="LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation"></a>LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12020">http://arxiv.org/abs/2310.12020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengqiang Zhang, Philipp Wicke, Lütfi Kerem Şenel, Luis Figueredo, Abdeldjallil Naceri, Sami Haddadin, Barbara Plank, Hinrich Schütze</li>
<li>for: 本研究的目的是提供一个长期计划任务的公共库存 benchmark，以测试语言条件的机器人在不同情况下的长期推理能力。</li>
<li>methods: 本研究使用了两种方法来处理观察反馈：caption generation和learnable interface。</li>
<li>results: 实验结果显示，现有的两种方法在一些任务上都显示出问题，这表明长期 TABLETOP 推理任务仍然是现代具有问题。<details>
<summary>Abstract</summary>
The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following. Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations. However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. To fill this gap, this work focuses on the tabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption generation and learnable interface for incorporating explicit and implicit observation feedback to the LLM, respectively. These methods serve as the two baselines for our proposed benchmark. Experiments show that both methods struggle to solve some tasks, indicating long-horizon manipulation tasks are still challenging for current popular models. We expect the proposed public benchmark and baselines can help the community develop better models for long-horizon tabletop manipulation tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption generation and learnable interface for incorporating explicit and implicit observation feedback to the LLM, respectively. These methods serve as the two baselines for our proposed benchmark. Experiments show that both methods struggle to solve some tasks, indicating long-horizon manipulation tasks are still challenging for current popular models. We expect the proposed public benchmark and baselines can help the community develop better models for long-horizon tabletop manipulation tasks.Note that "LoHoRavens" is a simulation benchmark, and "LLMs" stands for "large language models".
</details></li>
</ul>
<hr>
<h2 id="Gold-A-Global-and-Local-aware-Denoising-Framework-for-Commonsense-Knowledge-Graph-Noise-Detection"><a href="#Gold-A-Global-and-Local-aware-Denoising-Framework-for-Commonsense-Knowledge-Graph-Noise-Detection" class="headerlink" title="Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection"></a>Gold: A Global and Local-aware Denoising Framework for Commonsense Knowledge Graph Noise Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12011">http://arxiv.org/abs/2310.12011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/gold">https://github.com/hkust-knowcomp/gold</a></li>
<li>paper_authors: Zheye Deng, Weiqi Wang, Zhaowei Wang, Xin Liu, Yangqiu Song</li>
<li>for: constructing high-quality Commonsense Knowledge Graphs (CSKGs) with larger semantic coverage</li>
<li>methods: incorporates entity semantic information, global rules, and local structural information from the CSKG</li>
<li>results: outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks, and benefits the downstream zero-shot commonsense question-answering task on a real-world CSKG<details>
<summary>Abstract</summary>
Commonsense Knowledge Graphs (CSKGs) are crucial for commonsense reasoning, yet constructing them through human annotations can be costly. As a result, various automatic methods have been proposed to construct CSKG with larger semantic coverage. However, these unsupervised approaches introduce spurious noise that can lower the quality of the resulting CSKG, which cannot be tackled easily by existing denoising algorithms due to the unique characteristics of nodes and structures in CSKGs. To address this issue, we propose Gold (Global and Local-aware Denoising), a denoising framework for CSKGs that incorporates entity semantic information, global rules, and local structural information from the CSKG. Experiment results demonstrate that Gold outperforms all baseline methods in noise detection tasks on synthetic noisy CSKG benchmarks. Furthermore, we show that denoising a real-world CSKG is effective and even benefits the downstream zero-shot commonsense question-answering task.
</details>
<details>
<summary>摘要</summary>
共享常识图（CSKG）是对常识理解的关键，但是通过人工标注可能会成本高。因此，多种自动方法已经被提议用于构建CSKG，以提高 semantic 覆盖率。然而，这些无监督方法会引入干扰噪声，这些噪声难以通过现有的噪声除除算法处理，因为CSKG 中节点和结构的特殊特征。为解决这个问题，我们提出了 Gold（全球和本地化噪声除法），一种特有CSKG噪声除法，该法利用实体 semantic 信息，全球规则和CSKG 本地结构信息。实验结果表明，Gold 在噪声检测任务中击败了所有基线方法。此外，我们还证明了对真实世界CSKG进行噪声除法有效，甚至对下游零shot常识问答任务有益。
</details></li>
</ul>
<hr>
<h2 id="From-Interpolation-to-Extrapolation-Complete-Length-Generalization-for-Arithmetic-Transformers"><a href="#From-Interpolation-to-Extrapolation-Complete-Length-Generalization-for-Arithmetic-Transformers" class="headerlink" title="From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers"></a>From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11984">http://arxiv.org/abs/2310.11984</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shaoxiongduan/attentionbiascalibration">https://github.com/shaoxiongduan/attentionbiascalibration</a></li>
<li>paper_authors: Shaoxiong Duan, Yining Shi</li>
<li>for:  investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication.</li>
<li>methods:  through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. we show that transformer models are able to generalize to long lengths with the help of targeted attention biasing.</li>
<li>results:  we demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.<details>
<summary>Abstract</summary>
Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
</details>
<details>
<summary>摘要</summary>
自其引入以来，变换模型在不同任务中表现出色。然而，LENGTH总是一个尚未解决的问题，特别是在算法任务中。在这篇论文中，我们调查变换模型是否具备学习算术算法的能力，如加法和乘法。通过实验和注意力分析，我们确定了一些重要的因素，以实现最佳的长度总结。我们发现，变换模型可以通过targeted注意力偏好来总结到长 lengths。然后，我们引入了注意力偏好准备（ABC），一种准备阶段，它使得模型自动学习合适的注意力偏好，我们将其联系到相对位编码机制。我们示出，使用ABC，变换模型可以实现历史性的长度总结在某些算数任务中。
</details></li>
</ul>
<hr>
<h2 id="Filling-in-the-Gaps-Efficient-Event-Coreference-Resolution-using-Graph-Autoencoder-Networks"><a href="#Filling-in-the-Gaps-Efficient-Event-Coreference-Resolution-using-Graph-Autoencoder-Networks" class="headerlink" title="Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks"></a>Filling in the Gaps: Efficient Event Coreference Resolution using Graph Autoencoder Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11965">http://arxiv.org/abs/2310.11965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Loic De Langhe, Orphée De Clercq, Veronique Hoste</li>
<li>for: 本研究旨在提出一种新的和高效的事件核心参照解决方法（ECR），应用于低资源语言领域。</li>
<li>methods: 本研究使用图重建任务来结合深度 semantics embedding 和结构核心参照链知识，创造一种参数高效的图自编码器模型（GAE）。</li>
<li>results: 本研究在大规模的荷兰事件核心参照 korpus 上显著超过 классиical mention-pair 方法，以至于总分、效率和训练速度。此外，我们的模型能够更好地识别更难的核心参照链，并在低数据设置下显示出较高的Robustness。<details>
<summary>Abstract</summary>
We introduce a novel and efficient method for Event Coreference Resolution (ECR) applied to a lower-resourced language domain. By framing ECR as a graph reconstruction task, we are able to combine deep semantic embeddings with structural coreference chain knowledge to create a parameter-efficient family of Graph Autoencoder models (GAE). Our method significantly outperforms classical mention-pair methods on a large Dutch event coreference corpus in terms of overall score, efficiency and training speed. Additionally, we show that our models are consistently able to classify more difficult coreference links and are far more robust in low-data settings when compared to transformer-based mention-pair coreference algorithms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的和高效的事件核心关系解决方法（ECR），应用于低资源语言领域。我们将ECR视为图像重建任务，可以结合深度 semantics embedding 和结构核心关系链知识，创建一种参数高效的图像自编码器模型（GAE）。我们的方法在荷兰事件核心 correlate 词汇库中表现出色，在总分、效率和训练速度方面均超过了经典的提及对方法。此外，我们还证明了我们的模型在低数据情况下能够更好地分类更难的核心关系链，并且在基于转换器的提及对方法中更加稳定。
</details></li>
</ul>
<hr>
<h2 id="AMR-Parsing-with-Causal-Hierarchical-Attention-and-Pointers"><a href="#AMR-Parsing-with-Causal-Hierarchical-Attention-and-Pointers" class="headerlink" title="AMR Parsing with Causal Hierarchical Attention and Pointers"></a>AMR Parsing with Causal Hierarchical Attention and Pointers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11964">http://arxiv.org/abs/2310.11964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Lou, Kewei Tu</li>
<li>for: 本研究旨在提高简化型AMR分析器的性能，使其能够更好地考虑AMR图表示的结构本地性。</li>
<li>methods: 本研究提出了新的目标形式和模型，即 CHAP，它具有层次嵌入式注意力和指针机制，以便将结构 integrate 到 transformer 解码器中。</li>
<li>results: 实验表明，我们的模型在无额外数据的情况下，在四个benchmark中比基线模型表现出色，提高了性能。<details>
<summary>Abstract</summary>
Translation-based AMR parsers have recently gained popularity due to their simplicity and effectiveness. They predict linearized graphs as free texts, avoiding explicit structure modeling. However, this simplicity neglects structural locality in AMR graphs and introduces unnecessary tokens to represent coreferences. In this paper, we introduce new target forms of AMR parsing and a novel model, CHAP, which is equipped with causal hierarchical attention and the pointer mechanism, enabling the integration of structures into the Transformer decoder. We empirically explore various alternative modeling options. Experiments show that our model outperforms baseline models on four out of five benchmarks in the setting of no additional data.
</details>
<details>
<summary>摘要</summary>
听说过的AMR解析器在最近几年内受欢迎，因为它的简单和效果性。它预测了线性图，作为自由文本，避免了Explicit结构化。然而，这种简单性忽略了AMR图中的结构本地性，并且添加了不必要的标记来表示核心引用。在这篇论文中，我们介绍了新的AMR解析目标形式和一种新的模型，即 CHAP，它具有征识层次注意力和指针机制，使得Transformer解码器中的结构可以被集成。我们在不同的模型化选项上进行了实验，实验结果显示，我们的模型在无额外数据的情况下超过基eline模型在四个benchmark中。
</details></li>
</ul>
<hr>
<h2 id="Fast-Multipole-Attention-A-Divide-and-Conquer-Attention-Mechanism-for-Long-Sequences"><a href="#Fast-Multipole-Attention-A-Divide-and-Conquer-Attention-Mechanism-for-Long-Sequences" class="headerlink" title="Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences"></a>Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11960">http://arxiv.org/abs/2310.11960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanming Kang, Giang Tran, Hans De Sterck</li>
<li>for: 提高Transformer模型在长序列上的性能，解决自我关注的quadratic复杂性问题。</li>
<li>methods: 使用分割策略和层次结构来减少自我关注的时间和内存复杂性，保持全局响应场。</li>
<li>results: 比其他高效注意力变体在中等规模数据集上表现更好，具有更大的内存大小和准确率。<details>
<summary>Abstract</summary>
Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled or not. This multi-level divide-and-conquer strategy is inspired by fast summation methods from $n$-body physics and the Fast Multipole Method. We perform evaluation on autoregressive and bidirectional language modeling tasks and compare our Fast Multipole Attention model with other efficient attention variants on medium-size datasets. We find empirically that the Fast Multipole Transformer performs much better than other efficient transformers in terms of memory size and accuracy. The Fast Multipole Attention mechanism has the potential to empower large language models with much greater sequence lengths, taking the full context into account in an efficient, naturally hierarchical manner during training and when generating long sequences.
</details>
<details>
<summary>摘要</summary>
tranSformer-based models have achieved state-of-the-art performance in many areas, but the quadratic complexity of self-attention with respect to the input length limits their applicability to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}(\log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \log n)$, depending on whether the queries are down-sampled or not. This multi-level divide-and-conquer strategy is inspired by fast summation methods from $n$-body physics and the Fast Multipole Method. We perform evaluation on autoregressive and bidirectional language modeling tasks and compare our Fast Multipole Attention model with other efficient attention variants on medium-size datasets. We find empirically that the Fast Multipole Transformer performs much better than other efficient transformers in terms of memory size and accuracy. The Fast Multipole Attention mechanism has the potential to empower large language models with much greater sequence lengths, taking the full context into account in an efficient, naturally hierarchical manner during training and when generating long sequences.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Emptying-the-Ocean-with-a-Spoon-Should-We-Edit-Models"><a href="#Emptying-the-Ocean-with-a-Spoon-Should-We-Edit-Models" class="headerlink" title="Emptying the Ocean with a Spoon: Should We Edit Models?"></a>Emptying the Ocean with a Spoon: Should We Edit Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11958">http://arxiv.org/abs/2310.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuval Pinter, Michael Elhadad</li>
<li>for: 本研究质疑直接修改LLM生成中的实际错误是否能够成为一种系统性的解决方案。</li>
<li>methods: 本研究比较了三种类似 yet distinct的方法，即retrieval-based architectures, concept erasure methods和attribution methods，以解决LLM生成中的偏见和风险。</li>
<li>results: 研究发现，直接修改LLM模型不能被视为一种系统性的解决方案，而且可能会增加风险。在某些情况下，直接修改LLM模型可能会增加风险，而不是减少风险。<details>
<summary>Abstract</summary>
We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying on editing as a critical component.
</details>
<details>
<summary>摘要</summary>
我团队提出对直接模型编辑的方法进行批判，作为LLM生成中的错误纠正方法。我们将模型编辑与三种相似 yet distinct的方法进行对比：（1）检索型架构，它将知识存储和LLM中的语言能力分离开来；（2）概念消除方法，它们目的是避免生成文本中的系统偏见；以及（3）归因方法，它们强调将生成文本链接到特定的文本来源。我们认为直接模型编辑无法被视为LLM中的系统疾病纠正方法，尽管它在模型解释方面具有潜力。我们呼吁对模型编辑的推广和应用进行谨慎，并限制LLM的使用场景，以避免依赖于编辑的情况。
</details></li>
</ul>
<hr>
<h2 id="MusicAgent-An-AI-Agent-for-Music-Understanding-and-Generation-with-Large-Language-Models"><a href="#MusicAgent-An-AI-Agent-for-Music-Understanding-and-Generation-with-Large-Language-Models" class="headerlink" title="MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models"></a>MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11954">http://arxiv.org/abs/2310.11954</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/muzic">https://github.com/microsoft/muzic</a></li>
<li>paper_authors: Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian</li>
<li>for: 帮助开发者和爱好者快速找到适合他们需求的音乐处理工具，减少了对不同音乐数据表示和模型之间的学习和应用的困难。</li>
<li>methods: 基于大语言模型（LLMs）自动化任务的技术，集成了多种音乐相关的工具，并自动组织了用户请求，将其分解成多个子任务，采用相应的音乐工具进行处理。</li>
<li>results: 提供了一个自动化的音乐处理系统，让用户可以快速地找到适合他们需求的音乐工具，减少了用户对音乐处理技术的学习压力，让用户更能专注于音乐创作。<details>
<summary>Abstract</summary>
AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience.
</details>
<details>
<summary>摘要</summary>
人工智能 empowered 音乐处理是一个多样化的领域，包括多种任务，例如生成任务（如 timbre 合成）和理解任务（如音乐分类）。为开发者和爱好者而言，抓住这些任务的要求非常困难，尤其是在音乐数据表示和模型在不同平台之间的差异非常大。因此，需要建立一个系统来组织和集成这些任务，以帮助实践者自动分析他们的需求，并选择适合的工具来满足他们的要求。受大语言模型（LLM）的成功启发，我们开发了一个名为 MusicAgent 的系统，它集成了多种音乐相关的工具和一个自动化的工作流程，以解决用户的需求。更具体来说，我们建立了以下两个部分：1. 工具集，收集了来自多种源，包括 Hugging Face、GitHub 和 Web API 等等的工具。2. 由 LLM（如 ChatGPT） empowered 的自动化工作流程，用于组织这些工具，并自动将用户的请求分解成多个子任务，并对应的邀请合适的音乐工具。MusicAgent 系统的Primary Goal 是免除用户对 AI-音乐工具的繁琐，让他们可以专注于创作。通过让用户轻松地组合工具，系统提供了一个无缝和丰富的音乐体验。
</details></li>
</ul>
<hr>
<h2 id="Grounded-and-Well-rounded-A-Methodological-Approach-to-the-Study-of-Cross-modal-and-Cross-lingual-Grounding"><a href="#Grounded-and-Well-rounded-A-Methodological-Approach-to-the-Study-of-Cross-modal-and-Cross-lingual-Grounding" class="headerlink" title="Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding"></a>Grounded and Well-rounded: A Methodological Approach to the Study of Cross-modal and Cross-lingual Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11938">http://arxiv.org/abs/2310.11938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timothee Mickus, Elaine Zosa, Denis Paperno</li>
<li>for: 这个论文的目的是研究语义背景的影响在人工智能系统中，以及不同输入模式的效果。</li>
<li>methods: 这篇论文使用了一种方法学框架，用于研究不同输入模式对模型的影响。这个框架包括建立可比较的样本集，以便分析不同输入模式对模型的表现的质量。</li>
<li>results: 实验结果表明，提供不同的输入模式可以导致模型的不同行为，包括跨模式背景、跨语言背景和未grounded模型的不同行为。这些行为的差异可以在全数据集水平和特定词表示水平上被衡量。<details>
<summary>Abstract</summary>
Grounding has been argued to be a crucial component towards the development of more complete and truly semantically competent artificial intelligence systems. Literature has divided into two camps: While some argue that grounding allows for qualitatively different generalizations, others believe it can be compensated by mono-modal data quantity. Limited empirical evidence has emerged for or against either position, which we argue is due to the methodological challenges that come with studying grounding and its effects on NLP systems.   In this paper, we establish a methodological framework for studying what the effects are - if any - of providing models with richer input sources than text-only. The crux of it lies in the construction of comparable samples of populations of models trained on different input modalities, so that we can tease apart the qualitative effects of different input sources from quantifiable model performances. Experiments using this framework reveal qualitative differences in model behavior between cross-modally grounded, cross-lingually grounded, and ungrounded models, which we measure both at a global dataset level as well as for specific word representations, depending on how concrete their semantics is.
</details>
<details>
<summary>摘要</summary>
文本背景是人工智能系统的重要组成部分，有一些研究者认为它可以帮助系统实现更完整和具有真正含义的语言理解能力。文献被分为两个派别：一些人认为，背景可以带来不同的普遍化，而另一些人则认为，它可以通过大量单Modal数据补做。然而，有限的实验证据已经出现了，支持或反对任一位置。在这篇论文中，我们提出了一种方法ológical框架，用于研究不同输入模式对NLP系统的效果。我们 constructed comparable samples of populations of models trained on different input modalities，以便分离不同输入源的qualitative效果和可衡量的模型性能。实验结果显示，在不同的语言和modalities中训练的模型 exhibit 不同的行为，我们在全数据集级别以及特定词表示性的方面进行了测量。
</details></li>
</ul>
<hr>
<h2 id="Investigating-semantic-subspaces-of-Transformer-sentence-embeddings-through-linear-structural-probing"><a href="#Investigating-semantic-subspaces-of-Transformer-sentence-embeddings-through-linear-structural-probing" class="headerlink" title="Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing"></a>Investigating semantic subspaces of Transformer sentence embeddings through linear structural probing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11923">http://arxiv.org/abs/2310.11923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/macleginn/semantic-subspaces-code">https://github.com/macleginn/semantic-subspaces-code</a></li>
<li>paper_authors: Dmitry Nikolaev, Sebastian Padó</li>
<li>for: 研究Transformer模型中不同层次的语言信息编码方法</li>
<li>methods: 使用semantic structural probing方法研究语言模型的句子级表示</li>
<li>results: 发现不同模型家族和模型大小具有不同的表现和层次动态，但表现相对较具同样性<details>
<summary>Abstract</summary>
The question of what kinds of linguistic information are encoded in different layers of Transformer-based language models is of considerable interest for the NLP community. Existing work, however, has overwhelmingly focused on word-level representations and encoder-only language models with the masked-token training objective. In this paper, we present experiments with semantic structural probing, a method for studying sentence-level representations via finding a subspace of the embedding space that provides suitable task-specific pairwise distances between data-points. We apply our method to language models from different families (encoder-only, decoder-only, encoder-decoder) and of different sizes in the context of two tasks, semantic textual similarity and natural-language inference. We find that model families differ substantially in their performance and layer dynamics, but that the results are largely model-size invariant.
</details>
<details>
<summary>摘要</summary>
研究各种语言模型层次的语言信息编码是NLPT社区中的一个非常有趣的问题。现有的工作 however，主要集中在单词水平表示和基于encoder-only语言模型的伪token训练目标上。在这篇论文中，我们进行了叙述结构探索，一种研究句子级别表示的方法，通过找到 embedding空间中任务特定的数据点对之间的适当对比距离来实现。我们将这种方法应用于不同家族（encoder-only、decoder-only、encoder-decoder）和不同大小的语言模型中，并在两个任务（ semantics 文本相似性和自然语言推理）的上下文中进行了测试。我们发现，模型家族之间存在巨大差异，但结果几乎是模型大小不变的。
</details></li>
</ul>
<hr>
<h2 id="Rather-a-Nurse-than-a-Physician-–-Contrastive-Explanations-under-Investigation"><a href="#Rather-a-Nurse-than-a-Physician-–-Contrastive-Explanations-under-Investigation" class="headerlink" title="Rather a Nurse than a Physician – Contrastive Explanations under Investigation"></a>Rather a Nurse than a Physician – Contrastive Explanations under Investigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11906">http://arxiv.org/abs/2310.11906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Eberle, Ilias Chalkidis, Laura Cabello, Stephanie Brandl</li>
<li>For: The paper aims to investigate the claim that contrastive explanations are closer to human explanations than non-contrastive explanations.* Methods: The paper uses four English text-classification datasets and fine-tunes three different models (RoBERTa, GTP-2, and T5) in three different sizes. It also applies three post-hoc explainability methods (LRP, GradientxInput, and GradNorm) to extract explanations.* Results: The paper finds that there is a high agreement between model-based rationales and human annotations, both in contrastive and non-contrastive settings. Additionally, model-based explanations computed in both settings align equally well with human rationales, indicating that humans do not necessarily explain in a contrastive manner.<details>
<summary>Abstract</summary>
Contrastive explanations, where one decision is explained in contrast to another, are supposed to be closer to how humans explain a decision than non-contrastive explanations, where the decision is not necessarily referenced to an alternative. This claim has never been empirically validated. We analyze four English text-classification datasets (SST2, DynaSent, BIOS and DBpedia-Animals). We fine-tune and extract explanations from three different models (RoBERTa, GTP-2, and T5), each in three different sizes and apply three post-hoc explainability methods (LRP, GradientxInput, GradNorm). We furthermore collect and release human rationale annotations for a subset of 100 samples from the BIOS dataset for contrastive and non-contrastive settings. A cross-comparison between model-based rationales and human annotations, both in contrastive and non-contrastive settings, yields a high agreement between the two settings for models as well as for humans. Moreover, model-based explanations computed in both settings align equally well with human rationales. Thus, we empirically find that humans do not necessarily explain in a contrastive manner.9 pages, long paper at ACL 2022 proceedings.
</details>
<details>
<summary>摘要</summary>
“对比性解释”，即将一个决策解释为另一个决策的对比，被认为更接近人类的解释方式。然而，这一laim未经验证。我们分析了四个英文文本分类 dataset（SST2、DynaSent、BIOS和DBpedia-Animals），使用三种不同的模型（RoBERTa、GTP-2和T5），每种模型都有三个不同的大小，并应用三种后处 explainability 方法（LRP、GradientxInput和GradNorm）。此外，我们还收集并发布了BIOS dataset中的一百个样本的人类理由标注，用于对比和非对比设置。我们在这两种设置下进行了模型基于的理由和人类理由的交叉比较，发现两者之间存在高度一致性，并且模型基于的解释在两种设置下均与人类理由相吻合。因此，我们employm empirical研究发现，人类并不一定会在对比性下进行解释。Please note that the translation is in Simplified Chinese, and some words or phrases may have been translated differently in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="From-Dissonance-to-Insights-Dissecting-Disagreements-in-Rationale-Construction-for-Case-Outcome-Classification"><a href="#From-Dissonance-to-Insights-Dissecting-Disagreements-in-Rationale-Construction-for-Case-Outcome-Classification" class="headerlink" title="From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification"></a>From Dissonance to Insights: Dissecting Disagreements in Rationale Construction for Case Outcome Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11878">http://arxiv.org/abs/2310.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shanshan Xu, T. Y. S. S Santosh, Oana Ichim, Isabella Risini, Barbara Plank, Matthias Grabmair</li>
<li>for: 法律自然语言处理领域的案例结果分类（COC）需要不仅准确，还需要可信和可解释。现有的解释COC工作受限于单个专家的标注。</li>
<li>methods: 我们采集了一个新的数据集RAVE：Rationale Variation in ECHR1，该数据集由两名国际人权法律领域专家标注而成，我们观察到了这两个专家之间的弱一致。我们研究了他们的不一致，并构建了两级独立任务的分类法，补充了COC特有的亚分类。这是法律自然语言处理领域中第一次关于人类标注变化的研究。</li>
<li>results: 我们量测不同分类类别的数据，发现主要的不一致来自于法律上下文的不充分规定，这种情况通常具有有限的精度和噪音。我们进一步评估了当今最佳COC模型在RAVE上的解释性，发现模型和专家之间的一致度有限。总之，我们的案例研究暴露了在法律自然语言处理领域创建标准数据集的复杂性，这些复杂性包括确定案例中 факт的重要性。<details>
<summary>Abstract</summary>
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observe limited agreement between models and experts. Overall, our case study reveals hitherto underappreciated complexities in creating benchmark datasets in legal NLP that revolve around identifying aspects of a case's facts supposedly relevant to its outcome.
</details>
<details>
<summary>摘要</summary>
法律自然语言处理（NLP）中的案例结果分类（COC）不仅需要准确，还需要可信和可解释。现有的可解释COC工作都是由单一专家进行标注。然而，法律专业人员在评估案例事实时可能会有差异。因此，我们收集了一个新的数据集RAVE：可理解变化在人权法院1中，该数据集来自两个国际人权法律领域专家，我们观察到了弱一致。我们研究了他们的不一致，并建立了两级无关任务的税onomy，补充了COC特有的亚类。我们知道，这是法律NLP中第一个关注人类标注变化的工作。我们量测不同税onomy类别，并发现，不一致主要来自法律Context的不足，这种情况在COC元数据中通常具有有限的精度和噪音。我们进一步评估了现有最佳COC模型在RAVE上的解释性，并发现模型和专家之间的一致不高。总的来说，我们的案例研究发现了法律NLP中创建benchmark数据集的复杂性，即确定案例事实中可能对结果的影响因素。
</details></li>
</ul>
<hr>
<h2 id="The-Curious-Case-of-Hallucinatory-Unanswerablity-Finding-Truths-in-the-Hidden-States-of-Over-Confident-Large-Language-Models"><a href="#The-Curious-Case-of-Hallucinatory-Unanswerablity-Finding-Truths-in-the-Hidden-States-of-Over-Confident-Large-Language-Models" class="headerlink" title="The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models"></a>The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11877">http://arxiv.org/abs/2310.11877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, Shauli Ravfogel</li>
<li>for:  investigate the behavior of LLMs when presented with unanswerable queries</li>
<li>methods:  use a combination of human evaluation and automated metrics to study the representation of answerability in LLMs’ latent spaces</li>
<li>results:  find strong indications that LLMs encode the answerability of input queries, with the representation of the first decoded token often being a strong indicator, which can be used to develop improved decoding techniques for factual generation.Here’s the full translation in Simplified Chinese:</li>
<li>for: 这篇论文旨在研究 LLMs 当面对不可答案问题时的行为。</li>
<li>methods: 使用人工评审和自动度量来研究 LLMs 的秘密空间中的问题可answerability 表示。</li>
<li>results: 发现 LLMs 对输入问题的表示中具有强度的问题可answerability 表示，首个解码token 的表示frequently 是强度表示。这些发现可以用来发展更好的实际生成技术，特别在问题可answerability 是一个应对的情况下。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of unanswerable queries by LLMs, which often results in hallucinatory behavior, due to overconfidence. In this paper, we explore the behavior of LLMs when presented with unanswerable queries. We ask: do models \textbf{represent} the fact that the question is unanswerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query unanswerability is a concern.
</details>
<details>
<summary>摘要</summary>
Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the original text provided, and it is not a word-for-word translation. Some phrases and sentences may be rephrased or condensed to improve readability and clarity.
</details></li>
</ul>
<hr>
<h2 id="Text-Annotation-Handbook-A-Practical-Guide-for-Machine-Learning-Projects"><a href="#Text-Annotation-Handbook-A-Practical-Guide-for-Machine-Learning-Projects" class="headerlink" title="Text Annotation Handbook: A Practical Guide for Machine Learning Projects"></a>Text Annotation Handbook: A Practical Guide for Machine Learning Projects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11780">http://arxiv.org/abs/2310.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Stollenwerk, Joey Öhman, Danila Petrelli, Emma Wallerö, Fredrik Olsson, Camilla Bengtsson, Andreas Horndahl, Gabriela Zarzar Gandler</li>
<li>for: 这份手册是一本关于文本标注任务的实用指南，用于介绍基本概念和实践技巧。</li>
<li>methods: 本文涉及了主要的技术方面，同时也触及了商业、伦理和法规问题。</li>
<li>results: 文件的重点是在于可读性和简洁性，而不是完整性和科学准确性。该手册可能会用于各种职业，如团队领导、项目经理、IT архитек、软件开发者和机器学习工程师。<details>
<summary>Abstract</summary>
This handbook is a hands-on guide on how to approach text annotation tasks. It provides a gentle introduction to the topic, an overview of theoretical concepts as well as practical advice. The topics covered are mostly technical, but business, ethical and regulatory issues are also touched upon. The focus lies on readability and conciseness rather than completeness and scientific rigor. Experience with annotation and knowledge of machine learning are useful but not required. The document may serve as a primer or reference book for a wide range of professions such as team leaders, project managers, IT architects, software developers and machine learning engineers.
</details>
<details>
<summary>摘要</summary>
这本手册是一本实用的文本标注任务指南。它提供了一个温顺的引导，覆盖了理论概念以及实践建议。覆盖的主题主要是技术性的，但也涉及到业务、伦理和法规问题。文本的重点是易读性和简洁性，而不是完整性和科学严谨性。经验与标注和机器学习知识可能有助于，但并非必需。这份文档可能作为团队领导、项目经理、IT建筑师、软件开发者和机器学习工程师的引导或参考书。
</details></li>
</ul>
<hr>
<h2 id="Language-Agents-for-Detecting-Implicit-Stereotypes-in-Text-to-image-Models-at-Scale"><a href="#Language-Agents-for-Detecting-Implicit-Stereotypes-in-Text-to-image-Models-at-Scale" class="headerlink" title="Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale"></a>Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11778">http://arxiv.org/abs/2310.11778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qichao Wang, Tian Bian, Yian Yin, Tingyang Xu, Hong Cheng, Helen M. Meng, Zibin Zheng, Liang Chen, Bingzhe Wu</li>
<li>for: 检测文本到图像模型中的刻板印象</li>
<li>methods: 提出了一种新的代理体系，可以自动适应免式检测任务，并可以通过生成相关说明和图像来检测刻板印象。</li>
<li>results: 研究发现，一些商业产品和开源文本到图像模型中的模型经常在某些提示中表现出严重的刻板印象，这些刻板印象与人类的性别、种族和宗教等社会维度有关。<details>
<summary>Abstract</summary>
The recent surge in the research of diffusion models has accelerated the adoption of text-to-image models in various Artificial Intelligence Generated Content (AIGC) commercial products. While these exceptional AIGC products are gaining increasing recognition and sparking enthusiasm among consumers, the questions regarding whether, when, and how these models might unintentionally reinforce existing societal stereotypes remain largely unaddressed. Motivated by recent advancements in language agents, here we introduce a novel agent architecture tailored for stereotype detection in text-to-image models. This versatile agent architecture is capable of accommodating free-form detection tasks and can autonomously invoke various tools to facilitate the entire process, from generating corresponding instructions and images, to detecting stereotypes. We build the stereotype-relevant benchmark based on multiple open-text datasets, and apply this architecture to commercial products and popular open source text-to-image models. We find that these models often display serious stereotypes when it comes to certain prompts about personal characteristics, social cultural context and crime-related aspects. In summary, these empirical findings underscore the pervasive existence of stereotypes across social dimensions, including gender, race, and religion, which not only validate the effectiveness of our proposed approach, but also emphasize the critical necessity of addressing potential ethical risks in the burgeoning realm of AIGC. As AIGC continues its rapid expansion trajectory, with new models and plugins emerging daily in staggering numbers, the challenge lies in the timely detection and mitigation of potential biases within these models.
</details>
<details>
<summary>摘要</summary>
现在的研究强化模型在人工智能生成内容（AIGC）的商业产品中得到了加速。而这些优秀的AIGC产品正在获得消费者的认可，并让人们很感到激动。然而，关于这些模型是否无意中强化社会刻板印象的问题仍然未得到解决。我们在语言代理的最新进展基础上，提出了一种适用于刻板印象检测的新型代理体系。这种多功能的代理体系可以自动采用多种工具来执行整个过程，从生成相应的指令和图像到检测刻板印象。我们基于多个开源文本数据集建立了刻板印象相关的 benchmark，并应用这种体系到商业产品和流行的开源文本到图像模型中。我们发现，这些模型在某些个人特征、社会文化背景和犯罪相关的提示下显示了严重的刻板印象。总之，这些实验结果表明了社会各个维度上的刻板印象的普遍存在，包括 gender、race 和 religion 等，这不仅证明了我们的提出的方法的有效性，而且强调了在AIGC领域的可能性潜在风险的紧迫性。随着AIGC不断扩展，新的模型和插件每天都在各种数字平台上出现，因此，检测和 mitigate 这些模型中的潜在偏见的挑战在继续增长。
</details></li>
</ul>
<hr>
<h2 id="Improving-Long-Document-Topic-Segmentation-Models-With-Enhanced-Coherence-Modeling"><a href="#Improving-Long-Document-Topic-Segmentation-Models-With-Enhanced-Coherence-Modeling" class="headerlink" title="Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling"></a>Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11772">http://arxiv.org/abs/2310.11772</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/spokennlp">https://github.com/alibaba-damo-academy/spokennlp</a></li>
<li>paper_authors: Hai Yu, Chong Deng, Qinglin Zhang, Jiaqing Liu, Qian Chen, Wen Wang</li>
<li>for: 提高长文档主题分割性能，强化supervised模型捕捉凝合度信息。</li>
<li>methods: 提出了Topic-aware Sentence Structure Prediction (TSSP)和Contrastive Semantic Similarity Learning (CSSL)两种方法，用于强化supervised模型对凝合度信息的捕捉。</li>
<li>results: 对比旧状态之前方法，Longformer具有我们提出的方法在WIKI-727K上显著提高了$F_1$值（73.74 -&gt; 77.16），并在WikiSection上实现了平均相对减少$P_k$值（15.0 -&gt; 13.89），两个数据集上的average相对减少$P_k$值为4.3%。<details>
<summary>Abstract</summary>
Topic segmentation is critical for obtaining structured documents and improving downstream tasks such as information retrieval. Due to its ability of automatically exploring clues of topic shift from abundant labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship between coherence and topic segmentation underexplored. Therefore, this paper enhances the ability of supervised models to capture coherence from both logical structure and semantic similarity perspectives to further improve the topic segmentation performance, proposing Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations between adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at topic and sentence levels. Moreover, we utilize inter- and intra-topic information to construct contrastive samples and design the CSSL objective to ensure that the sentences representations in the same topic have higher similarity, while those in different topics are less similar. Extensive experiments show that the Longformer with our approach significantly outperforms old state-of-the-art (SOTA) methods. Our approach improve $F_1$ of old SOTA by 3.42 (73.74 -> 77.16) and reduces $P_k$ by 1.11 points (15.0 -> 13.89) on WIKI-727K and achieves an average relative reduction of 4.3% on $P_k$ on WikiSection. The average relative $P_k$ drop of 8.38% on two out-of-domain datasets also demonstrates the robustness of our approach.
</details>
<details>
<summary>摘要</summary>
Topic segmentation是文档结构化的关键之一，可以提高后续任务的信息检索性能。由于自动找到话题转换的灵活信息，最近的supervised神经网络模型在长文档话题分 segmentation方面取得了 significanth� development，但是它们之间的coherence关系还未得到充分探讨。因此，这篇论文提出了一种能够更好地捕捉coherence的方法，包括话题感知sentence结构预测（TSSP）和semantic similarity学习（CSSL）。具体来说，我们提出了TSSP任务，强制模型理解文档中的结构信息，通过学习原始文档中的关系来构建不规则的文档。此外，我们利用 между话题和同话题信息来构建对比采样，并设计了CSSL目标，确保同话题中的句子表示更加相似，而不同话题中的句子表示更加不相似。我们对Longformer模型进行了广泛的实验，并证明了我们的方法可以明显超越老的SOTA方法。我们的方法可以提高老SOTA的$F_1$指标的值，从73.74提高到77.16，并将15.0提高到13.89。此外，我们在WikiSection上 achieve了平均 относи于$P_k$指标的减少4.3%，并在两个out-of-domain数据集上实现了平均相对减少8.38%。这些结果表明我们的方法具有较好的Robustness性。
</details></li>
</ul>
<hr>
<h2 id="Annotated-Job-Ads-with-Named-Entity-Recognition"><a href="#Annotated-Job-Ads-with-Named-Entity-Recognition" class="headerlink" title="Annotated Job Ads with Named Entity Recognition"></a>Annotated Job Ads with Named Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11769">http://arxiv.org/abs/2310.11769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felix Stollenwerk, Niklas Fastlund, Anna Nyqvist, Joey Öhman</li>
<li>for: 本研究是为了开发一个可靠的瑞典employmnetJob advertisement中的各种有用信息（例如，工作者需要具备的技能）的Named Entity Recognition（NER）模型。</li>
<li>methods: 我们使用了KB-BERT进行微调，并采用了一些方法来减少人工标注的困难，包括自动生成标注数据集和人工审核。</li>
<li>results: 我们对模型的性能进行了报告，并证明了模型的可靠性和高效性。<details>
<summary>Abstract</summary>
We have trained a named entity recognition (NER) model that screens Swedish job ads for different kinds of useful information (e.g. skills required from a job seeker). It was obtained by fine-tuning KB-BERT. The biggest challenge we faced was the creation of a labelled dataset, which required manual annotation. This paper gives an overview of the methods we employed to make the annotation process more efficient and to ensure high quality data. We also report on the performance of the resulting model.
</details>
<details>
<summary>摘要</summary>
我们已经训练了一个Named Entity Recognition（NER）模型，用于检测瑞典寻求人员岗位各种有用信息（例如，岗位需求的技能）。它是通过精度调整KB-BERT获得的。我们最大的挑战是创建标注数据集，需要人工标注。本文介绍了我们采用的方法，以提高标注过程的效率和数据质量。我们还对结果模型的性能进行了报告。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-Evaluation-of-Large-Language-Models-on-Legal-Judgment-Prediction"><a href="#A-Comprehensive-Evaluation-of-Large-Language-Models-on-Legal-Judgment-Prediction" class="headerlink" title="A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction"></a>A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11761">http://arxiv.org/abs/2310.11761</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srhthu/lm-compeval-legal">https://github.com/srhthu/lm-compeval-legal</a></li>
<li>paper_authors: Ruihao Shui, Yixin Cao, Xiang Wang, Tat-Seng Chua</li>
<li>for: 研究大语言模型在法律领域的实际应用性。</li>
<li>methods: 基于大语言模型的实用基线解决方案，包括独立回答开放问题和与信息检索系统协作解决简化多选问题。</li>
<li>results: 研究表明，在提供类案例和多选选项的情况下，大语言模型可以更好地回忆域知识，但是如果 IR 系统的能力较强，则 LLM 的作用变得 redundante。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes redundant. Our evaluation pipeline can be easily extended into other tasks to facilitate evaluations in other domains. Code is available at https://github.com/srhthu/LM-CompEval-Legal
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在专业应用中展示了很大的潜力，例如法律领域。然而，最近有关GPT-4的法律评估争议引起了对它们在实际法律任务中的表现的质疑。为了系统地探索它们在法律中的能力，我们设计了实用的基线解决方案，并使用LLMs进行法律判断预测任务的评估。在我们的解决方案中，LLMs可以单独回答开问题，或与信息检索（IR）系统配合，从相似的案例或解决简单多选题中学习。我们发现，在提示中包含的相似案例和多选题（ Label Candidates）可以帮助LLMs回传专业知识，并且我们还发现了一个有趣的 contradicton，即在IR系统超越了LLM+IR的性能时，LLMs的角色变得redundant。我们的评估管线可以轻松地扩展到其他任务，以便在其他领域进行评估。代码可以在 GitHub 上获取：https://github.com/srhthu/LM-CompEval-Legal。
</details></li>
</ul>
<hr>
<h2 id="Bias-in-Emotion-Recognition-with-ChatGPT"><a href="#Bias-in-Emotion-Recognition-with-ChatGPT" class="headerlink" title="Bias in Emotion Recognition with ChatGPT"></a>Bias in Emotion Recognition with ChatGPT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11753">http://arxiv.org/abs/2310.11753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi</li>
<li>for: 这个技术报告探讨了基于文本的情感识别，这可以提供基于交互聊天机器人、数据注释和心理健康分析等应用的基础。</li>
<li>methods: 这个研究使用了ChatGPT进行情感识别，并进行了不同的数据集和情感标签的实验来评估其性能。</li>
<li>results: 研究发现，通过精度调整可以提高ChatGPT的情感识别性能，但是不同的情感标签和数据集的选择会影响ChatGPT的情感识别性能，表明了存在内在的不稳定性和可能的偏见。<details>
<summary>Abstract</summary>
This technical report explores the ability of ChatGPT in recognizing emotions from text, which can be the basis of various applications like interactive chatbots, data annotation, and mental health analysis. While prior research has shown ChatGPT's basic ability in sentiment analysis, its performance in more nuanced emotion recognition is not yet explored. Here, we conducted experiments to evaluate its performance of emotion recognition across different datasets and emotion labels. Our findings indicate a reasonable level of reproducibility in its performance, with noticeable improvement through fine-tuning. However, the performance varies with different emotion labels and datasets, highlighting an inherent instability and possible bias. The choice of dataset and emotion labels significantly impacts ChatGPT's emotion recognition performance. This paper sheds light on the importance of dataset and label selection, and the potential of fine-tuning in enhancing ChatGPT's emotion recognition capabilities, providing a groundwork for better integration of emotion analysis in applications using ChatGPT.
</details>
<details>
<summary>摘要</summary>
这份技术报告探讨了 chatGPT 在文本中识别情感的能力，这可以为互动 chatbot、数据标注和心理健康分析等应用提供基础。 although prior research has shown chatGPT 的基本情感分析能力，其在更复杂的情感识别方面的性能未经探讨。 在这里，我们进行了实验来评估 chatGPT 在不同的数据集和情感标签下的表现。 我们发现了一定的可重复性，通过微调可以得到明显的改进。 然而，不同的情感标签和数据集的表现差异明显，这 highlights the inherent instability and possible bias. 选择数据集和情感标签对 chatGPT 的情感识别表现有着重要的影响。 这篇文章强调了数据集和标签选择的重要性，以及微调的潜在作用，为将来更好地将情感分析 integrate into applications using chatGPT 提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Uncertainty-Calibration-of-Aligned-Language-Models-under-the-Multiple-Choice-Setting"><a href="#Investigating-Uncertainty-Calibration-of-Aligned-Language-Models-under-the-Multiple-Choice-Setting" class="headerlink" title="Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting"></a>Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11732">http://arxiv.org/abs/2310.11732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guande He, Peng Cui, Jianfei Chen, Wenbo Hu, Jun Zhu</li>
<li>for: 本文探讨了适用于多选问题的语言模型（LMs）的偏置阶段对logit基uncertainty准确性的影响。</li>
<li>methods: 本文采用了严谨的实验研究，探索了适应LMs与其预训练LMs的偏置阶段对uncertainty的影响。</li>
<li>results: 实验结果表明，适应LMs存在两种不同的uncertainty，负责答案决策和格式偏好。此外，研究者还发现了这两种uncertainty对适应LMs的准确性的影响，并提出了一种简单的Synthetic alignment scheme来缓解这种情况。<details>
<summary>Abstract</summary>
Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc calibration methods for aligned LMs and propose an easy-to-implement and sample-efficient method to calibrate aligned LMs. We hope our findings could provide insights into the design of more reliable alignment processes for LMs.
</details>
<details>
<summary>摘要</summary>
尽管已经在实际应用中采用了对齐语言模型（LMs），但它们往往会比预训练LMs更加自信。在这项工作中，我们系统地评估了对齐过程对logit基于不确定性调整的LMs的影响。我们首先通过思ful的实验研究了对齐LMs与其预训练对应者的不确定性差异。实验结果表明，LMs在多选设定下存在两种不同的不确定性，一是答案决定不确定性，二是格式偏好不确定性。然后，我们通过简单的合成对齐方案进行了调整，并发现对齐LMs的一种原因是这两种不确定性的混淆。此外，我们还检查了常见后处calibration方法对对齐LMs的效果，并提出了一种容易实现和效率高的方法来调整对齐LMs。我们希望我们的发现能为LMs的设计提供指导。
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Thought-Tuning-Masked-Language-Models-can-also-Think-Step-By-Step-in-Natural-Language-Understanding"><a href="#Chain-of-Thought-Tuning-Masked-Language-Models-can-also-Think-Step-By-Step-in-Natural-Language-Understanding" class="headerlink" title="Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding"></a>Chain-of-Thought Tuning: Masked Language Models can also Think Step By Step in Natural Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11721">http://arxiv.org/abs/2310.11721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Caoyun Fan, Jidong Tian, Yitian Li, Wenqing Chen, Hao He, Yaohui Jin</li>
<li>for: 这 paper aims to improve the performance of Large Language Models (LLMs) on Natural Language Understanding (NLU) tasks by extending the success of Chain-of-Thought (CoT) technique to MLMs.</li>
<li>methods: The proposed method, Chain-of-Thought Tuning (CoTT), is a two-step reasoning framework based on prompt tuning that enables MLMs to implement step-by-step thinking for NLU tasks.</li>
<li>results: The experiments on two NLU tasks, hierarchical classification and relation extraction, show that CoTT outperforms baselines and achieves state-of-the-art performance.<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) is a technique that guides Large Language Models (LLMs) to decompose complex tasks into multi-step reasoning through intermediate steps in natural language form. Briefly, CoT enables LLMs to think step by step. However, although many Natural Language Understanding (NLU) tasks also require thinking step by step, LLMs perform less well than small-scale Masked Language Models (MLMs). To migrate CoT from LLMs to MLMs, we propose Chain-of-Thought Tuning (CoTT), a two-step reasoning framework based on prompt tuning, to implement step-by-step thinking for MLMs on NLU tasks. From the perspective of CoT, CoTT's two-step framework enables MLMs to implement task decomposition; CoTT's prompt tuning allows intermediate steps to be used in natural language form. Thereby, the success of CoT can be extended to NLU tasks through MLMs. To verify the effectiveness of CoTT, we conduct experiments on two NLU tasks: hierarchical classification and relation extraction, and the results show that CoTT outperforms baselines and achieves state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
Chain-of-Thought (CoT) 是一种技术，用于导引大型自然语言模型（LLM）来 decomposition 复杂任务为多个步骤的自然语言形式中的逻辑步骤。简单来说，CoT 使得 LLM 可以一步步地思考。然而，许多自然语言理解（NLU）任务也需要一步步地思考，但 LLM 表现比小规模的面纹语言模型（MLM）差。为将 CoT 从 LLM 迁移到 MLM 上，我们提出了链条思维调整（CoTT），一种基于提问调整的两步逻辑框架。从 CoT 的视角来看，CoTT 的两步框架使得 MLM 可以实现任务的分解；CoTT 的提问调整使得中间步骤可以在自然语言形式下使用。因此，CoT 的成功可以通过 MLM 扩展到 NLU 任务。为验证 CoTT 的有效性，我们对两个 NLU 任务进行了实验：层次分类和关系抽取，并得到的结果表明 CoTT 超过基eline和实现了状态的表现。
</details></li>
</ul>
<hr>
<h2 id="Reflection-Tuning-Data-Recycling-Improves-LLM-Instruction-Tuning"><a href="#Reflection-Tuning-Data-Recycling-Improves-LLM-Instruction-Tuning" class="headerlink" title="Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning"></a>Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11716">http://arxiv.org/abs/2310.11716</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mingliiii/reflection_tuning">https://github.com/mingliiii/reflection_tuning</a></li>
<li>paper_authors: Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, Tianyi Zhou</li>
<li>for: 提高大型自然语言模型（LLM）的输出控制和对输入的Alignment。</li>
<li>methods: 使用“反思调教”方法，利用智能语言模型自我反省和提高数据中的指令和回答质量。</li>
<li>results: LLM经过“反思调教”后，在多种评价指标上表现出色，超过了传统的数据集训练方法。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have expanded the horizons of natural language understanding and generation. Notably, the output control and alignment with the input of LLMs can be refined through instruction tuning. However, as highlighted in several studies, low-quality data in the training set are usually detrimental to instruction tuning, resulting in inconsistent or even misleading LLM outputs. We propose a novel method, termed "reflection-tuning," which addresses the problem by self-improvement and judging capabilities of LLMs. This approach utilizes an oracle LLM to recycle the original training data by introspecting and enhancing the quality of instructions and responses in the data. Extensive experiments on widely used evaluation benchmarks show that LLMs trained with our recycled data outperform those trained with existing datasets in various benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MISAR-A-Multimodal-Instructional-System-with-Augmented-Reality"><a href="#MISAR-A-Multimodal-Instructional-System-with-Augmented-Reality" class="headerlink" title="MISAR: A Multimodal Instructional System with Augmented Reality"></a>MISAR: A Multimodal Instructional System with Augmented Reality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11699">http://arxiv.org/abs/2310.11699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nguyennm1024/misar">https://github.com/nguyennm1024/misar</a></li>
<li>paper_authors: Jing Bi, Nguyen Manh Nguyen, Ali Vosoughi, Chenliang Xu</li>
<li>for: 本研究旨在探讨大语言模型（LLMs）如何在增强现实（AR）中增强人机交互。</li>
<li>methods: 本研究使用大语言模型（LLMs）将视觉、听音和语言渠道融合，以提供更加优化的人机交互。</li>
<li>results: 研究表明，通过使用大语言模型（LLMs）可以更好地估计任务性能，从而为AR系统提供更加适应性。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Augmented reality (AR) requires the seamless integration of visual, auditory, and linguistic channels for optimized human-computer interaction. While auditory and visual inputs facilitate real-time and contextual user guidance, the potential of large language models (LLMs) in this landscape remains largely untapped. Our study introduces an innovative method harnessing LLMs to assimilate information from visual, auditory, and contextual modalities. Focusing on the unique challenge of task performance quantification in AR, we utilize egocentric video, speech, and context analysis. The integration of LLMs facilitates enhanced state estimation, marking a step towards more adaptive AR systems. Code, dataset, and demo will be available at https://github.com/nguyennm1024/misar.
</details>
<details>
<summary>摘要</summary>
现实扩展（AR）需要Visual、听力和语言通道的无缝结合，以便最佳化人机交互。听力和视觉输入可以提供实时和上下文相关的用户指导，但是大型自然语言模型（LLM）在这个场景中的潜在仍然未得到充分利用。我们的研究提出了一种新的方法，利用LLM来融合Visual、听力和上下文modalities。我们在实际任务完成量化中采用egocentric视频、Speech和上下文分析。通过LLM的集成，我们可以提高状态估计，这标志着更适应性AR系统的出发点。代码、数据集和示例将在https://github.com/nguyennm1024/misar上提供。
</details></li>
</ul>
<hr>
<h2 id="Adaptation-with-Self-Evaluation-to-Improve-Selective-Prediction-in-LLMs"><a href="#Adaptation-with-Self-Evaluation-to-Improve-Selective-Prediction-in-LLMs" class="headerlink" title="Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs"></a>Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11689">http://arxiv.org/abs/2310.11689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, Somesh Jha</li>
<li>For: 提高大语言模型在高风险决策场景的可靠性。* Methods: 基于自我评估的参数有效性调整方法，以适应特定任务而进行适应。* Results: 在多个问答（QA）数据集上进行评估，比靡前状态艺的选择预测方法表现更好，例如在CoQA标准测试集上，AUACC从91.23%提高到92.63%，AUROC从74.61%提高到80.25%.<details>
<summary>Abstract</summary>
Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AUROC from 74.61% to 80.25%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Superiority-of-Softmax-Unveiling-the-Performance-Edge-Over-Linear-Attention"><a href="#Superiority-of-Softmax-Unveiling-the-Performance-Edge-Over-Linear-Attention" class="headerlink" title="Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention"></a>Superiority of Softmax: Unveiling the Performance Edge Over Linear Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11685">http://arxiv.org/abs/2310.11685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichuan Deng, Zhao Song, Tianyi Zhou</li>
<li>for: 本研究旨在解释软max和线性注意力之间的实际性能差异的原因。</li>
<li>methods: 本研究采用了比较分析两种注意力机制的方法，以解释软max注意力在大多数场景下的性能优势。</li>
<li>results: 研究发现，软max注意力在大多数场景下具有更高的性能，而linear注意力具有更高的计算复杂度。<details>
<summary>Abstract</summary>
Large transformer models have achieved state-of-the-art results in numerous natural language processing tasks. Among the pivotal components of the transformer architecture, the attention mechanism plays a crucial role in capturing token interactions within sequences through the utilization of softmax function.   Conversely, linear attention presents a more computationally efficient alternative by approximating the softmax operation with linear complexity. However, it exhibits substantial performance degradation when compared to the traditional softmax attention mechanism.   In this paper, we bridge the gap in our theoretical understanding of the reasons behind the practical performance gap between softmax and linear attention. By conducting a comprehensive comparative analysis of these two attention mechanisms, we shed light on the underlying reasons for why softmax attention outperforms linear attention in most scenarios.
</details>
<details>
<summary>摘要</summary>
大型转换器模型在自然语言处理多种任务中取得了状态机器人的Result。转换器架构中的注意机制对于序列中的Token交互进行捕捉具有关键作用，通过使用softmax函数。然而，线性注意presenteda更加计算效率的代替方案，但它在与传统的softmax注意机制相比 exhibits substantial performance degradation。在这篇论文中，我们尝试填补这两种注意机制之间的实践性能差距的理论理解漏洞。通过对这两种注意机制进行全面的比较分析，我们 shed light on the underlying reasons why softmax注意机制在大多数场景下比linear注意机制更高性能。
</details></li>
</ul>
<hr>
<h2 id="Open-ended-Commonsense-Reasoning-with-Unrestricted-Answer-Scope"><a href="#Open-ended-Commonsense-Reasoning-with-Unrestricted-Answer-Scope" class="headerlink" title="Open-ended Commonsense Reasoning with Unrestricted Answer Scope"></a>Open-ended Commonsense Reasoning with Unrestricted Answer Scope</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11672">http://arxiv.org/abs/2310.11672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Ling, Xuchao Zhang, Xujiang Zhao, Yanchi Liu, Wei Cheng, Mika Oishi, Takao Osaki, Katsushi Matsuda, Haifeng Chen, Liang Zhao</li>
<li>for: 解决开放式常识问题，即没有短列表或预定答案范围的问题。</li>
<li>methods: 利用预训练语言模型IterativelyRetrieve reasoning paths on external knowledge base，不需要任务特定的监督。</li>
<li>results: 对两个常识 benchmarck 数据集进行实验，与其他方法相比，提出的方法表现更好， both quantitatively and qualitatively。<details>
<summary>Abstract</summary>
Open-ended Commonsense Reasoning is defined as solving a commonsense question without providing 1) a short list of answer candidates and 2) a pre-defined answer scope. Conventional ways of formulating the commonsense question into a question-answering form or utilizing external knowledge to learn retrieval-based methods are less applicable in the open-ended setting due to an inherent challenge. Without pre-defining an answer scope or a few candidates, open-ended commonsense reasoning entails predicting answers by searching over an extremely large searching space. Moreover, most questions require implicit multi-hop reasoning, which presents even more challenges to our problem. In this work, we leverage pre-trained language models to iteratively retrieve reasoning paths on the external knowledge base, which does not require task-specific supervision. The reasoning paths can help to identify the most precise answer to the commonsense question. We conduct experiments on two commonsense benchmark datasets. Compared to other approaches, our proposed method achieves better performance both quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MixEdit-Revisiting-Data-Augmentation-and-Beyond-for-Grammatical-Error-Correction"><a href="#MixEdit-Revisiting-Data-Augmentation-and-Beyond-for-Grammatical-Error-Correction" class="headerlink" title="MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction"></a>MixEdit: Revisiting Data Augmentation and Beyond for Grammatical Error Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11671">http://arxiv.org/abs/2310.11671</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thukelab/mixedit">https://github.com/thukelab/mixedit</a></li>
<li>paper_authors: Jingheng Ye, Yinghui Li, Yangning Li, Hai-Tao Zheng</li>
<li>for: 本研究旨在解释如何使用数据扩充提高 grammatical error correction（GEC）模型的性能。</li>
<li>methods: 本研究提出了两个可读性和计算效率高的指标：吸引力和多样性。这两个指标可以帮助理解数据扩充对 GEC 模型的影响。</li>
<li>results: 实验结果表明，一个具有高吸引力和合适多样性的数据扩充策略可以更好地提高 GEC 模型的性能。此外，提议的 MixEdit 数据扩充方法可以在不需要额外的单语言 corpus 的情况下，STRATEGICALLY AND DYNAMICALLY 扩充真实的数据，以提高 GEC 模型的性能。<details>
<summary>Abstract</summary>
Data Augmentation through generating pseudo data has been proven effective in mitigating the challenge of data scarcity in the field of Grammatical Error Correction (GEC). Various augmentation strategies have been widely explored, most of which are motivated by two heuristics, i.e., increasing the distribution similarity and diversity of pseudo data. However, the underlying mechanism responsible for the effectiveness of these strategies remains poorly understood. In this paper, we aim to clarify how data augmentation improves GEC models. To this end, we introduce two interpretable and computationally efficient measures: Affinity and Diversity. Our findings indicate that an excellent GEC data augmentation strategy characterized by high Affinity and appropriate Diversity can better improve the performance of GEC models. Based on this observation, we propose MixEdit, a data augmentation approach that strategically and dynamically augments realistic data, without requiring extra monolingual corpora. To verify the correctness of our findings and the effectiveness of the proposed MixEdit, we conduct experiments on mainstream English and Chinese GEC datasets. The results show that MixEdit substantially improves GEC models and is complementary to traditional data augmentation methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable text into Simplified Chinese.<</SYS>>数据扩充通过生成假数据已经证明可以有效地解决语法错误修复（GEC）领域中的数据缺乏问题。各种扩充策略已经广泛探索，大多数是基于两个启发，即增加假数据的分布相似性和多样性。然而，这些策略下面的机制仍然不够了解。在这篇论文中，我们目的是解释如何使数据扩充改进GEC模型。为此，我们引入了两种可解释的计算效率的度量：团结度和多样性。我们的发现表明，一个具有高团结度和合适的多样性的GEC数据扩充策略可以更好地改进GEC模型的性能。基于这一观察，我们提出了 MixEdit，一种数据扩充方法，不需要额外的同语言资料卷。为了验证我们的发现和 MixEdit 的有效性，我们在主流的英语和中文GEC数据集上进行了实验。结果表明，MixEdit 可以大幅提高 GEC 模型的性能，并且与传统的数据扩充方法相комplementary。
</details></li>
</ul>
<hr>
<h2 id="Field-testing-items-using-artificial-intelligence-Natural-language-processing-with-transformers"><a href="#Field-testing-items-using-artificial-intelligence-Natural-language-processing-with-transformers" class="headerlink" title="Field-testing items using artificial intelligence: Natural language processing with transformers"></a>Field-testing items using artificial intelligence: Natural language processing with transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11655">http://arxiv.org/abs/2310.11655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hotaka Maeda</li>
<li>for: 这个研究用了五千个RoBERTa模型来测试英语文本理解能力。</li>
<li>methods: 研究使用了一种人工智能”transformer”来解决英语文本理解问题。</li>
<li>results: 研究发现，RoBERTa模型可以准确地回答英语文本理解测试中的29个多选题。数据还用于计算测试题的心理特性，与人类考生数据显示一定的一致性。<details>
<summary>Abstract</summary>
Five thousand variations of the RoBERTa model, an artificially intelligent "transformer" that can understand text language, completed an English literacy exam with 29 multiple-choice questions. Data were used to calculate the psychometric properties of the items, which showed some degree of agreement to those obtained from human examinee data.
</details>
<details>
<summary>摘要</summary>
五千种RoBERTa模型，一种人工智能"变换器"，通过完成了29个多选题的英语阅读测验。使用数据计算测验项的心理属性，结果与人类考生数据有一定的相似度。Note:* "RoBERTa" is translated as "RoBERTa模型" in Simplified Chinese.* "transformer" is translated as "变换器" in Simplified Chinese.* "English literacy exam" is translated as "英语阅读测验" in Simplified Chinese.* "multiple-choice questions" is translated as "多选题" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Faithfulness-Evaluation-for-Text-Summarization-with-Foundation-Language-Model"><a href="#Zero-shot-Faithfulness-Evaluation-for-Text-Summarization-with-Foundation-Language-Model" class="headerlink" title="Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model"></a>Zero-shot Faithfulness Evaluation for Text Summarization with Foundation Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11648">http://arxiv.org/abs/2310.11648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaqisjtu/faitheval-fflm">https://github.com/jiaqisjtu/faitheval-fflm</a></li>
<li>paper_authors: Qi Jia, Siyu Ren, Yizhu Liu, Kenny Q. Zhu</li>
<li>for: 本文提出了一种新的无参 faithfulness 评估方法，用于评估自然语言生成模型的准确性和可靠性。</li>
<li>methods: 本文使用了一种基于 prefix 的新 metric FFLM，通过探索输出文本的可能性来评估模型的 faithfulness。</li>
<li>results: 实验表明，FFLM 可以与或even outperform ChatGPT 在不同任务上，并且具有24倍 fewer 参数。 FFLM 还可以超过其他强基线。<details>
<summary>Abstract</summary>
Despite tremendous improvements in natural language generation, summarization models still suffer from the unfaithfulness issue. Previous work evaluates faithfulness either using models trained on the other tasks or in-domain synthetic data, or prompting a large model such as ChatGPT. This paper proposes to do zero-shot faithfulness evaluation simply with a moderately-sized foundation language model. We introduce a new metric FFLM, which is a combination of probability changes based on the intuition that prefixing a piece of text that is consistent with the output will increase the probability of predicting the output. Experiments show that FFLM performs competitively with or even outperforms ChatGPT on both inconsistency detection and faithfulness rating with 24x fewer parameters. FFLM also achieves improvements over other strong baselines.
</details>
<details>
<summary>摘要</summary>
尽管自然语言生成技术已经做出了很大的进步，摘要模型仍然面临着不忠问题。前一任的工作通常使用其他任务训练的模型或者域内生成的数据来评估忠诚性，或者激活大型模型如ChatGPT。这篇论文提议使用一个 moderately-sized 基础语言模型进行零 shot 忠诚性评估。我们引入了一个新的度量FFLM，它是基于输出预测概率变化的 prefixing 语句的 intuition。实验表明，FFLM 与 ChatGPT 在不一致检测和忠诚评分中表现竞争，并且具有24倍少的参数。FFLM 还超过了其他强大基elines。
</details></li>
</ul>
<hr>
<h2 id="Systematic-Assessment-of-Factual-Knowledge-in-Large-Language-Models"><a href="#Systematic-Assessment-of-Factual-Knowledge-in-Large-Language-Models" class="headerlink" title="Systematic Assessment of Factual Knowledge in Large Language Models"></a>Systematic Assessment of Factual Knowledge in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11638">http://arxiv.org/abs/2310.11638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linhao Luo, Thuy-Trang Vu, Dinh Phung, Gholamreza Haffari</li>
<li>for: 这个论文是为了系统地评估大语言模型（LLM）中的实用知识，以及如何利用知识图（KG）来评估。</li>
<li>methods: 这个论文使用了一种框架，可以自动生成知识图中的问题和预期答案，然后评估LLM的答案准确性。</li>
<li>results: 实验表明，ChatGPT在各个领域中表现最佳，而LLM的表现受到 instrucion finetuning、领域、问题复杂度和Contextual Adversarial的影响。<details>
<summary>Abstract</summary>
Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:先前的研究通过现有的问答指标来评估大语言模型（LLM）中的知识，但这种方法有限制，因为它主要集中在通用领域，这可能与预训练数据重叠。本文提出了一个框架，可以系统地评估 LLM 中的事实知识，通过利用知识图（KG）。我们的框架可以自动生成基于 KG 中的事实的问题和预期答案，然后评估 LLM 在回答这些问题时的准确性。我们系统地评估了当今最先进的 LLM 在通用和特定领域中的性能。实验结果显示，ChatGPT 在所有领域中占据了首位。我们还发现，LLM 的性能取决于 instrucion 精度调整、领域和问题复杂度，并且容易受到恶意上下文的影响。
</details></li>
</ul>
<hr>
<h2 id="MAGNIFICo-Evaluating-the-In-Context-Learning-Ability-of-Large-Language-Models-to-Generalize-to-Novel-Interpretations"><a href="#MAGNIFICo-Evaluating-the-In-Context-Learning-Ability-of-Large-Language-Models-to-Generalize-to-Novel-Interpretations" class="headerlink" title="MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations"></a>MAGNIFICo: Evaluating the In-Context Learning Ability of Large Language Models to Generalize to Novel Interpretations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11634">http://arxiv.org/abs/2310.11634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arkil Patel, Satwik Bhattamishra, Siva Reddy, Dzmitry Bahdanau</li>
<li>for: 本研究旨在探讨大语言模型（LLM）是否可以通过Contextual learning来学习新的解释。</li>
<li>methods: 我们引入了MAGNIFICo评价工具，该工具基于文本到SQLsemantic parsing框架，并在不同的token和提示设置下进行了多种测试，以模拟实际场景的复杂性。</li>
<li>results: 实验结果表明，LLMs在自然语言描述和长对话中能够很好地理解新的解释，但是我们的研究也发现，当面临不熟悉的词语或同时构建多个新解释时，LLMs的性能仍然有所不足。此外，我们的分析还揭示了LLMs中的semantic predispositions，以及长context中的recency bias的影响。<details>
<summary>Abstract</summary>
Humans possess a remarkable ability to assign novel interpretations to linguistic expressions, enabling them to learn new words and understand community-specific connotations. However, Large Language Models (LLMs) have a knowledge cutoff and are costly to finetune repeatedly. Therefore, it is crucial for LLMs to learn novel interpretations in-context. In this paper, we systematically analyse the ability of LLMs to acquire novel interpretations using in-context learning. To facilitate our study, we introduce MAGNIFICo, an evaluation suite implemented within a text-to-SQL semantic parsing framework that incorporates diverse tokens and prompt settings to simulate real-world complexity. Experimental results on MAGNIFICo demonstrate that LLMs exhibit a surprisingly robust capacity for comprehending novel interpretations from natural language descriptions as well as from discussions within long conversations. Nevertheless, our findings also highlight the need for further improvements, particularly when interpreting unfamiliar words or when composing multiple novel interpretations simultaneously in the same example. Additionally, our analysis uncovers the semantic predispositions in LLMs and reveals the impact of recency bias for information presented in long contexts.
</details>
<details>
<summary>摘要</summary>
人类具有强大的语言表达重新解释能力，可以学习新词和社区特有的含义。然而，大型自然语言模型（LLM）具有知识割辑和重新训练成本高的问题。因此，LLM需要在Context中学习新的解释。本文系统地分析了LLM在Context中学习新解释的能力。为了促进我们的研究，我们提出了MAGNIFICo评价集，该集包括多种Token和提示设置，以 simulate real-world complexity。实验结果表明，LLM在自然语言描述和长 conversations中的讨论中能够很好地理解新解释。然而，我们的发现也表明，当解释不熟悉的词语或者在同一个例子中同时构成多个新解释时，LLM的表现仍然需要进一步改进。此外，我们的分析还揭示了LLM中的含义偏好和长 context中的新信息偏好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.CL_2023_10_18/" data-id="clpxp6bzi00dree8850312glk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_18" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/18/cs.LG_2023_10_18/" class="article-date">
  <time datetime="2023-10-18T10:00:00.000Z" itemprop="datePublished">2023-10-18</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/18/cs.LG_2023_10_18/">cs.LG - 2023-10-18</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="No-Regret-Learning-in-Bilateral-Trade-via-Global-Budget-Balance"><a href="#No-Regret-Learning-in-Bilateral-Trade-via-Global-Budget-Balance" class="headerlink" title="No-Regret Learning in Bilateral Trade via Global Budget Balance"></a>No-Regret Learning in Bilateral Trade via Global Budget Balance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12370">http://arxiv.org/abs/2310.12370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco</li>
<li>for: 本文研究在双方均有私人估价的 bilateral trade 问题中，learner 设定价格，无知到代理人的估价。</li>
<li>methods: 本文引入全局预算平衡的概念，要求代理人在整个时间轴上保持预算平衡。通过全局预算平衡，提供了首个不负 regret 算法，在不同反馈模型下对 adversarial 输入进行评估。</li>
<li>results: 在全部反馈模型中，learner 可以保证 $\tilde{O}(\sqrt{T})$ regret，与最佳固定价格相比，是OPTimal的。在partial feedback模型中，提供了一个 $\tilde{O}(T^{3&#x2F;4})$ regret Upper bound的算法，并且 complement with a nearly-matching lower bound。<details>
<summary>Abstract</summary>
Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can guarantee $\tilde{O}(\sqrt{T})$ regret against the best fixed prices in hindsight, which is order-wise optimal. Then, in the case of partial feedback models, we provide an algorithm guaranteeing a $\tilde{O}(T^{3/4})$ regret upper bound with one-bit feedback, which we complement with a nearly-matching lower bound. Finally, we investigate how these results vary when measuring regret using an alternative benchmark.
</details>
<details>
<summary>摘要</summary>
bilateral trade 环绕着两个策略代理人（一个买家和一个卖家）之间的挑战，这两个代理人都有私人的评价值。我们研究在网络上进行的这个问题，在每个时间步骤中，新的买家和卖家会出现。学习者的任务是设定价格，但是没有任何关于代理人们的评价知识。选择序列的买家和卖家是由一个无知的敌人选择。在这个设定下，已知的负结果规则排除了设计具有下图 regret 的算法的可能性。在这篇论文中，我们引入全面预算平衡的概念，它需要学习者在整个时间频谱上保持预算平衡。通过需要全面预算平衡，我们提供了首个不负担 regret 的算法，在不同的反馈模型下实现了双方贸易。首先，在完整反馈模型中，我们显示学习者可以在对照后获得 $\tilde{O}(\sqrt{T})$ regret，这是很好的估计。然后，在受限反馈模型中，我们提供了一个 garantia $\tilde{O}(T^{3/4})$ regret 的算法，并补充了一个几乎匹配的下限。最后，我们调查了这些结果如何在使用不同的参考基准时变化。
</details></li>
</ul>
<hr>
<h2 id="MARVEL-Multi-Agent-Reinforcement-Learning-for-Large-Scale-Variable-Speed-Limits"><a href="#MARVEL-Multi-Agent-Reinforcement-Learning-for-Large-Scale-Variable-Speed-Limits" class="headerlink" title="MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits"></a>MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12359">http://arxiv.org/abs/2310.12359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhang, Marcos Quinones-Grueiro, Zhiyao Zhang, Yanbing Wang, William Barbour, Gautam Biswas, Daniel Work</li>
<li>for: 这个论文的目的是提出一种基于多代理学习（MARL）的大规模Variable Speed Limit（VSL）控制策略，以提高交通安全性和流体性。</li>
<li>methods: 该论文使用MARL框架，通过使用常见的数据来实现大规模VSL控制。代理学习算法通过考虑交通条件的变化、安全性和流体性的奖励结构进行学习，从而实现代理之间的协调。</li>
<li>results: 对于一段7英里的高速公路，MARL方法提高了交通安全性63.4%，并提高了交通流体性14.6%，相比于现有的实践算法。此外，文章还进行了解释性分析，以了解代理在不同交通条件下的决策过程。最后，文章测试了在实际数据上的策略，以证明该策略的可部署性。<details>
<summary>Abstract</summary>
Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is undertaken to explore the learned policy under different traffic conditions and the results provide insights into the decision-making process of agents. Finally, we test the policy learned from the simulation-based experiments on real input data from I-24 to illustrate the potential deployment capability of the learned policy.
</details>
<details>
<summary>摘要</summary>
Variable speed limit (VSL) 控制是一种有前途的交通管理策略，可以提高安全性和流动性。这项工作介绍了 MARVEL，一种多代理学习 (MARL) 框架，用于实现大规模 VSL 控制在高速公路段上，只使用常见的数据。代理学习的奖励结构包括适应交通条件、安全性和流动性，使代理之间协调。提出的框架可以涵盖覆盖许多斜塔，因为所有 VSL 代理的参数共享。代理在基于微观 simulate 环境中学习，该环境基于一段长7英里的高速公路，涵盖8个斜塔。代理在基于实际数据进行测试，并在I-24公路上进行了17英里的测试。 MARVEL 可以提高交通安全性63.4%，并提高交通流动性14.6%，相比之前的实践算法。 Explainability 分析用于探索不同交通条件下代理学习的策略，结果提供了决策过程中代理的启示。最后，我们将在实际数据上测试从 simulate 中学习的策略，以 illustrate 学习的可部署性。
</details></li>
</ul>
<hr>
<h2 id="Networkwide-Traffic-State-Forecasting-Using-Exogenous-Information-A-Multi-Dimensional-Graph-Attention-Based-Approach"><a href="#Networkwide-Traffic-State-Forecasting-Using-Exogenous-Information-A-Multi-Dimensional-Graph-Attention-Based-Approach" class="headerlink" title="Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach"></a>Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12353">http://arxiv.org/abs/2310.12353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Islam, Monika Filipovska</li>
<li>for: 这篇论文主要针对交通管理和控制策略中的交通状态预测问题，以及用户和系统层次的决策。</li>
<li>methods: 该论文提出了一种基于多维空间时间图注意力网络（M-STGAT）的交通预测方法，该方法使用过去观测到的速度、路况事件、温度和视程，并利用交通网络的结构来学习。</li>
<li>results: 实验结果表明，M-STGAT在使用加利福尼亚交通部门（Caltrans）性能衡量系统（PeMS）提供的交通速度和路况数据，并与国家海洋和大气管理局（NOAA）自动Surface Observing Systems（ASOS）提供的天气数据进行比较，在30、45和60分钟预测时间 horizons 上表现出了较好的预测性能，其中error measures包括 Mean Absolute Error（MAE）、Root Mean Square Error（RMSE）和Mean Absolute Percentage Error（MAPE）。但是，模型的传送性可能需要进一步的调查。<details>
<summary>Abstract</summary>
Traffic state forecasting is crucial for traffic management and control strategies, as well as user- and system-level decision making in the transportation network. While traffic forecasting has been approached with a variety of techniques over the last couple of decades, most approaches simply rely on endogenous traffic variables for state prediction, despite the evidence that exogenous factors can significantly impact traffic conditions. This paper proposes a multi-dimensional spatio-temporal graph attention-based traffic prediction approach (M-STGAT), which predicts traffic based on past observations of speed, along with lane closure events, temperature, and visibility across the transportation network. The approach is based on a graph attention network architecture, which also learns based on the structure of the transportation network on which these variables are observed. Numerical experiments are performed using traffic speed and lane closure data from the California Department of Transportation (Caltrans) Performance Measurement System (PeMS). The corresponding weather data were downloaded from the National Oceanic and Atmospheric Administration (NOOA) Automated Surface Observing Systems (ASOS). For comparison, the numerical experiments implement three alternative models which do not allow for the multi-dimensional input. The M-STGAT is shown to outperform the three alternative models, when performing tests using our primary data set for prediction with a 30-, 45-, and 60-minute prediction horizon, in terms of three error measures: Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). However, the model's transferability can vary for different transfer data sets and this aspect may require further investigation.
</details>
<details>
<summary>摘要</summary>
交通状况预测是交通管理和控制策略以及用户和系统层次的决策中非常重要的一环。自过去几十年来，交通预测已经使用了多种技术，但大多数方法都仅仅基于内生的交通变量进行预测，尽管外生因素可能对交通条件产生重要影响。这篇论文提出了一种多维度空间时间图注意力基本交通预测方法（M-STGAT），该方法基于过去观测到的速度，以及路段 closure事件、温度和视程等外生因素进行预测。该方法基于图注意力网络架构，同时还学习了交通网络上这些变量的结构。我们使用了加利福尼亚交通部门（Caltrans）性能测量系统（PeMS）中的交通速度和路段 closure数据进行数值实验，并下载了国家海洋和大气管理局（NOAA）自动地面观测系统（ASOS）中的天气数据。为比较，我们实现了三种不允许多维度输入的数学模型。M-STGAT在使用我们的主要数据集进行预测时，在30-, 45-, 和 60-分钟预测距离时表现出了与三个错误度量（ Mean Absolute Error，Root Mean Square Error 和 Mean Absolute Percentage Error）相对较高的性能。然而，模型的传输性可能会随着不同的传输数据集而异。这一点可能需要进一步的调查。
</details></li>
</ul>
<hr>
<h2 id="Equipping-Federated-Graph-Neural-Networks-with-Structure-aware-Group-Fairness"><a href="#Equipping-Federated-Graph-Neural-Networks-with-Structure-aware-Group-Fairness" class="headerlink" title="Equipping Federated Graph Neural Networks with Structure-aware Group Fairness"></a>Equipping Federated Graph Neural Networks with Structure-aware Group Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12350">http://arxiv.org/abs/2310.12350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuening-lab/f2gnn">https://github.com/yuening-lab/f2gnn</a></li>
<li>paper_authors: Nan Cui, Xiuling Wang, Wendy Hui Wang, Violet Chen, Yue Ning</li>
<li>for: 这篇论文旨在提出一种解决联合学习中Graph Neural Networks（GNNs）中的偏见问题的方法，以保证GNNs在分布式学习中保持公平性。</li>
<li>methods: 该方法基于两个关键组成部分：一是客户端上的公平性意识更新方案，二是在集成过程中考虑本地模型的公平性指标和数据偏见指标的全球模型更新方案。</li>
<li>results: 该方法在许多基准方法上表现出色，在公平性和模型准确性两个方面均有显著提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a training graph and statistical fairness metrics of the trained GNN models. Based on the theoretical analysis, we design $\text{F}^2$GNN which contains two key components: a fairness-aware local model update scheme that enhances group fairness of the local models on the client side, and a fairness-weighted global model update scheme that takes both data bias and fairness metrics of local models into consideration in the aggregation process. We evaluate $\text{F}^2$GNN empirically versus a number of baseline methods, and demonstrate that $\text{F}^2$GNN outperforms these baselines in terms of both fairness and model accuracy.
</details>
<details>
<summary>摘要</summary>
GRAPH Neural Networks (GNNs) 在不同领域中对各种图数据进行处理和分析任务广泛使用。在中央化图数据上训练 GNNs 可能因为隐私问题和管制约束而成为不可能的。因此，联邦学习 (FL) 成为一种解决这个挑战的趋势。然而， GNNs 可能从训练数据中继承历史偏见，并导致歧视性预测，因此在分布式设置下，本地模型的偏见可能被轻松传播到全球模型。这种挑战需要解决偏见在联邦 GNN 中的问题。为此，我们提出了 $\text{F}^2$GNN，一种增强分布式 Graph Neural Network 的分组公平性。由于偏见可以来自数据和学习算法，$\text{F}^2$GNN 采用了两个关键组成部分：在客户端上使用公平性意识的本地模型更新方案，以及在聚合过程中考虑本地模型的公平性度量和数据偏见的准确度。我们对 $\text{F}^2$GNN 进行了理论分析，并对其与一些基准方法进行了实验比较，并证明 $\text{F}^2$GNN 在公平性和模型准确性两个方面都高于基准方法。
</details></li>
</ul>
<hr>
<h2 id="Tracking-electricity-losses-and-their-perceived-causes-using-nighttime-light-and-social-media"><a href="#Tracking-electricity-losses-and-their-perceived-causes-using-nighttime-light-and-social-media" class="headerlink" title="Tracking electricity losses and their perceived causes using nighttime light and social media"></a>Tracking electricity losses and their perceived causes using nighttime light and social media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12346">http://arxiv.org/abs/2310.12346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel W Kerber, Nicholas A Duncan, Guillaume F LHer, Morgan Bazilian, Chris Elvidge, Mark R Deinert</li>
<li>for: 本研究旨在使用卫星图像、社交媒体和信息提取技术监测停电和其所报导的原因。</li>
<li>methods: 本研究使用了夜晚照明数据（2019年3月的加拉加斯，委内瑞拉），并通过Twitter数据分析公众对停电的感受和看法，以及使用统计分析和话题模型探讨公众归咎政府的停电原因。</li>
<li>results: 研究发现，夜晚照明强度与停电Region之间存在 inverse 关系。twitter上提到委内瑞拉总统的帖子具有更高的负面性和更多的责任相关词汇，这表明公众归咎政府对停电的责任。<details>
<summary>Abstract</summary>
Urban environments are intricate systems where the breakdown of critical infrastructure can impact both the economic and social well-being of communities. Electricity systems hold particular significance, as they are essential for other infrastructure, and disruptions can trigger widespread consequences. Typically, assessing electricity availability requires ground-level data, a challenge in conflict zones and regions with limited access. This study shows how satellite imagery, social media, and information extraction can monitor blackouts and their perceived causes. Night-time light data (in March 2019 for Caracas, Venezuela) is used to indicate blackout regions. Twitter data is used to determine sentiment and topic trends, while statistical analysis and topic modeling delved into public perceptions regarding blackout causes. The findings show an inverse relationship between nighttime light intensity. Tweets mentioning the Venezuelan President displayed heightened negativity and a greater prevalence of blame-related terms, suggesting a perception of government accountability for the outages.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Multivariate-Time-Series-Anomaly-Detection"><a href="#Open-Set-Multivariate-Time-Series-Anomaly-Detection" class="headerlink" title="Open-Set Multivariate Time-Series Anomaly Detection"></a>Open-Set Multivariate Time-Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12294">http://arxiv.org/abs/2310.12294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Lai, Thi Kieu Khanh Ho, Narges Armanfard</li>
<li>for: 提出了一种新的方法来解决时间序列异常检测（TSAD）问题，即在训练阶段只有有限的异常样本可用，并且需要检测未经见过的异常类型。</li>
<li>methods: 该方法包括三个主要模块：特征提取器、多头网络和异常评分模块。特征提取器抽取有用的时间序列特征，多头网络包括生成-, 偏差-和对比头，用于捕捉已见和未见异常类型。</li>
<li>results: 对三个实际数据集进行了广泛的实验，结果显示，我们的方法在不同的设定下都能够超越现有方法，从而在TSAD领域实现了新的状态级表现。<details>
<summary>Abstract</summary>
Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three primary modules: a Feature Extractor to extract meaningful time-series features; a Multi-head Network consisting of Generative-, Deviation-, and Contrastive heads for capturing both seen and unseen anomaly classes; and an Anomaly Scoring module leveraging the insights of the three heads to detect anomalies. Extensive experiments on three real-world datasets consistently show that our approach surpasses existing methods under various experimental settings, thus establishing a new state-of-the-art performance in the TSAD field.
</details>
<details>
<summary>摘要</summary>
Recently, many time series anomaly detection (TSAD) methods have been proposed. Most of these methods are unsupervised and assume the availability of normal training samples, while only a few supervised methods have shown better performance by incorporating labeled anomalous samples in the training phase. However, some anomaly types are difficult for unsupervised methods to distinguish from normal data, while supervised methods are limited to detecting anomalies similar to those present during training and cannot handle unseen anomaly classes. This paper is the first attempt to solve the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are available during training, with the goal of detecting both seen and unseen anomaly classes in the test phase.The proposed method, called Multivariate Open-Set Time Series Anomaly Detection (MOSAD), consists of three primary modules: a Feature Extractor to extract meaningful time-series features; a Multi-head Network consisting of Generative-, Deviation-, and Contrastive heads to capture both seen and unseen anomaly classes; and an Anomaly Scoring module that leverages the insights of the three heads to detect anomalies. Extensive experiments on three real-world datasets consistently show that our approach outperforms existing methods under various experimental settings, thereby establishing a new state-of-the-art performance in the TSAD field.
</details></li>
</ul>
<hr>
<h2 id="A-PAC-Learning-Algorithm-for-LTL-and-Omega-regular-Objectives-in-MDPs"><a href="#A-PAC-Learning-Algorithm-for-LTL-and-Omega-regular-Objectives-in-MDPs" class="headerlink" title="A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs"></a>A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12248">http://arxiv.org/abs/2310.12248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Perez, Fabio Somenzi, Ashutosh Trivedi</li>
<li>for: 表示非Markov决策学中的非Markov目标，使用线性时间逻辑（LTL）和ω-正则目标。</li>
<li>methods: 使用模型基于可能approx Correct（PAC）学习算法，从系统轨迹样本中学习。不需要先知系统结构。</li>
<li>results: 学习omega-正则目标的Markov决策过程中的可能approx Correct算法。<details>
<summary>Abstract</summary>
Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
</details>
<details>
<summary>摘要</summary>
线性时间逻辑（LTL）和奥米加常量目标——LTL的超集——在人工智能中被用来表达非马普朗的目标。我们介绍了基于模型的可能相对正确（PAC）学习算法 для奥米加常量目标在Markov决策过程中。与先前的方法不同，我们的算法从系统样本轨迹中学习，而不需要先知系统结构。
</details></li>
</ul>
<hr>
<h2 id="Fast-Parameter-Inference-on-Pulsar-Timing-Arrays-with-Normalizing-Flows"><a href="#Fast-Parameter-Inference-on-Pulsar-Timing-Arrays-with-Normalizing-Flows" class="headerlink" title="Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows"></a>Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12209">http://arxiv.org/abs/2310.12209</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Shih, Marat Freytsis, Stephen R. Taylor, Jeff A. Dror, Nolan Smyth</li>
<li>for: 这个论文是为了提高�ulsar时间尺度数组（PTAs）的 Bayesian posterior inference 的效率而写的。</li>
<li>methods: 这篇论文使用了模拟数据生成的 conditional normalizing flows 技术来快速和准确地计算狮子时间尺度数组（SGWB）的 posterior distribution，从原来的数天减少到只需几秒钟。</li>
<li>results: 该论文的实验结果表明，使用 conditional normalizing flows 技术可以在狮子时间尺度数组（SGWB）的 posterior distribution 计算中大幅提高效率，从原来的数天减少到只需几秒钟。<details>
<summary>Abstract</summary>
Pulsar timing arrays (PTAs) perform Bayesian posterior inference with expensive MCMC methods. Given a dataset of ~10-100 pulsars and O(10^3) timing residuals each, producing a posterior distribution for the stochastic gravitational wave background (SGWB) can take days to a week. The computational bottleneck arises because the likelihood evaluation required for MCMC is extremely costly when considering the dimensionality of the search space. Fortunately, generating simulated data is fast, so modern simulation-based inference techniques can be brought to bear on the problem. In this paper, we demonstrate how conditional normalizing flows trained on simulated data can be used for extremely fast and accurate estimation of the SGWB posteriors, reducing the sampling time from weeks to a matter of seconds.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dynamic-financial-processes-identification-using-sparse-regressive-reservoir-computers"><a href="#Dynamic-financial-processes-identification-using-sparse-regressive-reservoir-computers" class="headerlink" title="Dynamic financial processes identification using sparse regressive reservoir computers"></a>Dynamic financial processes identification using sparse regressive reservoir computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12144">http://arxiv.org/abs/2310.12144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fredyvides/dynet-cnbs">https://github.com/fredyvides/dynet-cnbs</a></li>
<li>paper_authors: Fredy Vides, Idelfonso B. R. Nogueira, Lendy Banegas, Evelyn Flores</li>
<li>For: 本文研究结构矩阵近似理论，应用于财经系统动态过程的回归表示。* Methods: 使用非线性时间延迟嵌入、稀疏最小二乘和结构矩阵近似方法来探索财经系统的输出封顶矩阵的近似表示。* Results: 通过应用上述技术，可以实现财经系统动态过程的近似识别和预测，包括可能或可能不具有混沌行为的场景。<details>
<summary>Abstract</summary>
In this document, we present key findings in structured matrix approximation theory, with applications to the regressive representation of dynamic financial processes. Initially, we explore a comprehensive approach involving generic nonlinear time delay embedding for time series data extracted from a financial or economic system under examination. Subsequently, we employ sparse least-squares and structured matrix approximation methods to discern approximate representations of the output coupling matrices. These representations play a pivotal role in establishing the regressive models corresponding to the recursive structures inherent in a given financial system. The document further introduces prototypical algorithms that leverage the aforementioned techniques. These algorithms are demonstrated through applications in approximate identification and predictive simulation of dynamic financial and economic processes, encompassing scenarios that may or may not exhibit chaotic behavior.
</details>
<details>
<summary>摘要</summary>
在本文中，我们介绍了结构化矩阵近似理论的关键发现，并应用于金融或经济系统中的回归表现力学过程的重构表示。我们首先探讨了一种通用非线性时间延迟嵌入方法，用于从金融或经济系统中提取时间序列数据。然后，我们使用稀疏最小二乘和结构矩阵近似方法来推导出输出 coupling 矩阵的近似表示。这些表示在建立金融系统中的回归模型中扮演关键角色。文章还介绍了一些原型算法，这些算法利用上述技术来实现精度的回归预测和模拟。这些算法在不同的金融和经济过程中的应用中得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Automatic-prediction-of-mortality-in-patients-with-mental-illness-using-electronic-health-records"><a href="#Automatic-prediction-of-mortality-in-patients-with-mental-illness-using-electronic-health-records" class="headerlink" title="Automatic prediction of mortality in patients with mental illness using electronic health records"></a>Automatic prediction of mortality in patients with mental illness using electronic health records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12121">http://arxiv.org/abs/2310.12121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean Kim, Samuel Kim</li>
<li>for: 预测精神疾病患者30天 mortality rate</li>
<li>methods: 使用predictive machine-learning models with electronic health records (EHR)，包括Logistic Regression、Random Forest、Support Vector Machine和K-Nearest Neighbors四种机器学习算法</li>
<li>results: Random Forest和Support Vector Machine模型表现最佳，AUC分数为0.911，Feature importance分析显示 morphine sulfate等药物具有预测作用。<details>
<summary>Abstract</summary>
Mental disorders impact the lives of millions of people globally, not only impeding their day-to-day lives but also markedly reducing life expectancy. This paper addresses the persistent challenge of predicting mortality in patients with mental diagnoses using predictive machine-learning models with electronic health records (EHR). Data from patients with mental disease diagnoses were extracted from the well-known clinical MIMIC-III data set utilizing demographic, prescription, and procedural information. Four machine learning algorithms (Logistic Regression, Random Forest, Support Vector Machine, and K-Nearest Neighbors) were used, with results indicating that Random Forest and Support Vector Machine models outperformed others, with AUC scores of 0.911. Feature importance analysis revealed that drug prescriptions, particularly Morphine Sulfate, play a pivotal role in prediction. We applied a variety of machine learning algorithms to predict 30-day mortality followed by feature importance analysis. This study can be used to assist hospital workers in identifying at-risk patients to reduce excess mortality.
</details>
<details>
<summary>摘要</summary>
精神疾病影响全球数百万人的生活，不仅妨碍日常生活，而且明显减少生存期。本文使用可预测机器学习模型和电子健康纪录（EHR）预测患有精神诊断的患者 mortality。从 клиничеwell-known MIMIC-III数据集中提取了患有精神疾病诊断的患者数据，并使用LOGISTIC REGRESSION、Random Forest、Support Vector Machine和K-Nearest Neighbors四种机器学习算法。结果表明，Random Forest和Support Vector Machine模型在其他四个模型中表现最佳，AUC分数为0.911。特征重要性分析显示，药物处方，特别是摩革定（Morphine Sulfate），在预测中扮演着关键性角色。我们通过不同的机器学习算法预测30天内死亡，并进行特征重要性分析，以帮助医院工作人员识别高风险患者，从而减少过度死亡。
</details></li>
</ul>
<hr>
<h2 id="MMD-based-Variable-Importance-for-Distributional-Random-Forest"><a href="#MMD-based-Variable-Importance-for-Distributional-Random-Forest" class="headerlink" title="MMD-based Variable Importance for Distributional Random Forest"></a>MMD-based Variable Importance for Distributional Random Forest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12115">http://arxiv.org/abs/2310.12115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clément Bénard, Jeffrey Näf, Julie Josse</li>
<li>for: 这篇论文目的是提出一种基于森林方法的全Conditional分布估计方法，用于 Multivariate output of interest 的输入变量。</li>
<li>methods: 该论文使用了 Drop and relearn 原理和MMD距离来实现变量重要性度量，而传统的重要性度量仅检测输出均值的影响变量。</li>
<li>results: 引入的重要性度量是一致的，在实际数据和模拟数据上具有高效性，并且超越竞争者。特别是，该算法可以通过回归特征减少来选择变量，从而提供高精度的Conditional输出分布估计。<details>
<summary>Abstract</summary>
Distributional Random Forest (DRF) is a flexible forest-based method to estimate the full conditional distribution of a multivariate output of interest given input variables. In this article, we introduce a variable importance algorithm for DRFs, based on the well-established drop and relearn principle and MMD distance. While traditional importance measures only detect variables with an influence on the output mean, our algorithm detects variables impacting the output distribution more generally. We show that the introduced importance measure is consistent, exhibits high empirical performance on both real and simulated data, and outperforms competitors. In particular, our algorithm is highly efficient to select variables through recursive feature elimination, and can therefore provide small sets of variables to build accurate estimates of conditional output distributions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用 Distributional Random Forest（DRF）来估算输入变量的多变量输出分布。在本文中，我们提出了基于drop和重新学习原则以及MMD距离的变量重要性算法，该算法可捕捉输入变量对输出分布的影响，而不仅仅是输出均值。我们证明了该算法的一致性和高效性，并在实际和预测数据上实现了比较高的表现。特别是，我们的算法可以通过 recursively feature elimination来选择变量，从而快速建立高精度的 conditional output distribution 估计。Translation notes:* "Distributional Random Forest" is translated as "多变量随机森林" (mányuànxīn sēn lín)* "full conditional distribution" is translated as "完整的分布" (quèzhè de fēn xiǎng)* "variable importance" is translated as "变量重要性" (biànxīn zhòng yào xìng)* "drop and relearn principle" is translated as "drop和重新学习原则" (drop hé zhòng xīn xué xí yuè)* "MMD distance" is translated as "MMD距离" (MMD jù lù)* "recursive feature elimination" is translated as " recursively feature elimination" (jiē yǐjī zhì xiǎng fāng yì)
</details></li>
</ul>
<hr>
<h2 id="Monarch-Mixer-A-Simple-Sub-Quadratic-GEMM-Based-Architecture"><a href="#Monarch-Mixer-A-Simple-Sub-Quadratic-GEMM-Based-Architecture" class="headerlink" title="Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture"></a>Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12109">http://arxiv.org/abs/2310.12109</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/m2">https://github.com/HazyResearch/m2</a></li>
<li>paper_authors: Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, Atri Rudra, Christopher Ré</li>
<li>for: 这篇论文目的是探讨是否存在可以在序列长度和模型维度上呈现高性能的、不 quadratic 的机器学习模型。</li>
<li>methods: 该论文提出了一种新的 Monarch Mixer（M2）架构，使用同样的不 quadratic  primitives来处理序列长度和模型维度：Monarch 矩阵，一种简单的可表示性structured 矩阵，可以在 GPU 上实现高硬件效率，并且可以呈现高性能。</li>
<li>results: 作为证明，该论文在三个领域中explored M2 的性能：非 causal BERT 样式语言模型、ViT 样式图像分类和 causal GPT 样式语言模型。在非 causal BERT 样式模型中，M2 与 BERT-base 和 BERT-large 相比，在下游 GLUE 质量上具有相同的性能，并且可以达到更高的通过put 性能（最高达 9.1 倍）。在 ImageNet 上，M2 超过 ViT-b 的准确率，仅使用半个参数。在 causal GPT 样式模型中，M2 可以与 Transformer 相比，在 360M 参数的预训练质量上具有相同的性能。<details>
<summary>Abstract</summary>
Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.
</details>
<details>
<summary>摘要</summary>
机器学习模型在序列长度和模型维度上逐渐升级以达到更长的上下文和更高的性能。然而，现有的架构，如Transformers，在这两个轴上 quadratic scaling。我们问：是否存在高性能的架构，可以在序列长度和模型维度上下降幂？我们介绍了一新的架构：宫廷混合器（M2），它使用同样的幂次性 primitive来序列长度和模型维度：宫廷矩阵，一种简单的表达 Structured matrices 的类型，可以在 GPU 上实现高硬件效率，并在序列长度和模型维度上下降幂。作为一个证明，我们探索了 M2 在三个领域的性能：非 causal BERT 风格语言模型、ViT 风格图像分类和 causal GPT 风格语言模型。在非 causal BERT 风格模型中，M2 与 BERT-base 和 BERT-large 相当在下游 GLUE 质量上，并且在序列长度 4K 时间点可以达到 9.1 倍的throughput，而且只需要 27% 的参数。在 ImageNet 上，M2 超过 ViT-b 的准确率，只需要一半的参数。 causal GPT 风格模型引入了一个技术挑战：在 маSKing 中引入的 quadratic bottleneck。为了缓解这个瓶颈，我们开发了一种新的理论视角，基于多Variable 多项式评估和插值，这使得我们可以在 M2 中使用 causal 参数化，而不是 quadratic 参数化。使用这种参数化，M2 与 GPT 风格 Transformers 在 360M 参数的预训练损失上匹配，这是第一次证明可以不使用注意力或 MLP 匹配 Transformer 质量。
</details></li>
</ul>
<hr>
<h2 id="An-Online-Learning-Theory-of-Brokerage"><a href="#An-Online-Learning-Theory-of-Brokerage" class="headerlink" title="An Online Learning Theory of Brokerage"></a>An Online Learning Theory of Brokerage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12107">http://arxiv.org/abs/2310.12107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nataša Bolić, Tommaso Cesari, Roberto Colomboni</li>
<li>For: The paper is written for investigating brokerage between traders from an online learning perspective, with a focus on the case where there are no designated buyer and seller roles.* Methods: The paper uses online learning techniques to achieve a low regret bound in the brokerage problem, specifically providing algorithms achieving regret $M \log T$ and $\sqrt{M T}$ under different assumptions about the agents’ valuations.* Results: The paper shows that the optimal regret rate is $M \log T$ when the agents’ valuations are revealed after each interaction, and $\sqrt{M T}$ when only their willingness to sell or buy at the proposed price is revealed. Additionally, the paper demonstrates that the optimal rate degrades to $\sqrt{T}$ when the bounded density assumption is dropped.<details>
<summary>Abstract</summary>
We investigate brokerage between traders from an online learning perspective. At any round $t$, two traders arrive with their private valuations, and the broker proposes a trading price. Unlike other bilateral trade problems already studied in the online learning literature, we focus on the case where there are no designated buyer and seller roles: each trader will attempt to either buy or sell depending on the current price of the good.   We assume the agents' valuations are drawn i.i.d. from a fixed but unknown distribution. If the distribution admits a density bounded by some constant $M$, then, for any time horizon $T$:   $\bullet$ If the agents' valuations are revealed after each interaction, we provide an algorithm achieving regret $M \log T$ and show this rate is optimal, up to constant factors.   $\bullet$ If only their willingness to sell or buy at the proposed price is revealed after each interaction, we provide an algorithm achieving regret $\sqrt{M T}$ and show this rate is optimal, up to constant factors.   Finally, if we drop the bounded density assumption, we show that the optimal rate degrades to $\sqrt{T}$ in the first case, and the problem becomes unlearnable in the second.
</details>
<details>
<summary>摘要</summary>
我们研究在线学习中的经纪人交易。在任意的回合 $t$ 中，两个经纪人会 arrive  WITH 他们的私人估价，经纪人会提议交易价格。与其他双方贸易问题已经在在线学习文献中研究过的不同，我们专注于情况下没有指定的买方和卖方角色：每个经纪人都会尝试 Either 购买或卖出，根据当前商品价格。  我们假设经纪人的估价是从固定而 unknown 的分布中随机样本。如果该分布具有最大值 $M$，那么，对于任意的时间 horizon $T$：❝ 如果经纪人的估价在每次交互后公布，我们提供了一个算法，其 regret 为 $M \log T$，并证明这个率是最佳的，占常数因子。❞❝ 如果只有经纪人对于提议价格的愿意性被公布在每次交互后，我们提供了一个算法，其 regret 为 $\sqrt{M T}$，并证明这个率是最佳的，占常数因子。❞最后，如果我们取消了均勋度 bound 的假设，我们显示了最佳率下降到 $\sqrt{T}$ 在第一个情况下，并问题变得不可学习在第二个情况下。
</details></li>
</ul>
<hr>
<h2 id="On-the-latent-dimension-of-deep-autoencoders-for-reduced-order-modeling-of-PDEs-parametrized-by-random-fields"><a href="#On-the-latent-dimension-of-deep-autoencoders-for-reduced-order-modeling-of-PDEs-parametrized-by-random-fields" class="headerlink" title="On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields"></a>On the latent dimension of deep autoencoders for reduced order modeling of PDEs parametrized by random fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12095">http://arxiv.org/abs/2310.12095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicola Rares Franco, Daniel Fraulin, Andrea Manzoni, Paolo Zunino</li>
<li>for: 这篇论文的目的是提供对随机场生成的Stochastic Partial Differential Equations (SPDEs)中Deep Learning-based Reduced Order Models (DL-ROMs)的理论分析。</li>
<li>methods: 本文使用了深度学习自动编码器作为ROMs的基础工具，并通过非线性神经网络的能力来减少问题的维度。</li>
<li>results: 本文提供了关于DL-ROMs在随机场中的理论分析，并提供了可导的错误 bound，以帮助域专家在选择深度学习自动编码器的缓存维度时进行优化。 数据示例表明，本文的分析对DL-ROMs的性能产生了显著的影响。<details>
<summary>Abstract</summary>
Deep Learning is having a remarkable impact on the design of Reduced Order Models (ROMs) for Partial Differential Equations (PDEs), where it is exploited as a powerful tool for tackling complex problems for which classical methods might fail. In this respect, deep autoencoders play a fundamental role, as they provide an extremely flexible tool for reducing the dimensionality of a given problem by leveraging on the nonlinear capabilities of neural networks. Indeed, starting from this paradigm, several successful approaches have already been developed, which are here referred to as Deep Learning-based ROMs (DL-ROMs). Nevertheless, when it comes to stochastic problems parameterized by random fields, the current understanding of DL-ROMs is mostly based on empirical evidence: in fact, their theoretical analysis is currently limited to the case of PDEs depending on a finite number of (deterministic) parameters. The purpose of this work is to extend the existing literature by providing some theoretical insights about the use of DL-ROMs in the presence of stochasticity generated by random fields. In particular, we derive explicit error bounds that can guide domain practitioners when choosing the latent dimension of deep autoencoders. We evaluate the practical usefulness of our theory by means of numerical experiments, showing how our analysis can significantly impact the performance of DL-ROMs.
</details>
<details>
<summary>摘要</summary>
深度学习对减少顺序模型（ROMs）的设计产生了深刻的影响，特别是在解决复杂问题上，其中经典方法可能会失败时。在这个情况下，深度自适应神经网络扮演了非常重要的角色，因为它们可以通过神经网络的非线性能力来减少问题的维度。从这个角度出发，已经有许多成功的方法被开发出来，这些方法被称为深度学习基于ROMs（DL-ROMs）。然而，当面临随机场所 parametrized 的问题时，现有的理论分析仅限于具有固定数量的 deterministic 参数的PDEs。本文的目的是扩展现有的文献，提供关于DL-ROMs在随机场所下的理论分析。特别是，我们 derive 了明确的错误 bound，可以帮助域专家在选择深度自适应神经网络的缓存维度时作出决策。我们通过数值实验证明了我们的理论对DL-ROMs的性能产生了显著的影响。
</details></li>
</ul>
<hr>
<h2 id="Contributing-Components-of-Metabolic-Energy-Models-to-Metabolic-Cost-Estimations-in-Gait"><a href="#Contributing-Components-of-Metabolic-Energy-Models-to-Metabolic-Cost-Estimations-in-Gait" class="headerlink" title="Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait"></a>Contributing Components of Metabolic Energy Models to Metabolic Cost Estimations in Gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12083">http://arxiv.org/abs/2310.12083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Gambietz, Marlies Nitschke, Jörg Miehling, Anne Koelewijn</li>
<li>for: 这个研究旨在深入理解人类行走中的代谢能量消耗模型，以便更好地估计代谢能量消耗。</li>
<li>methods: 我们使用了四种代谢能量消耗模型的参数进行 Monte Carlo 敏感分析，然后分析了这些参数的敏感指数、生理上的Context和生理过程中的代谢率。最终选择了一个 quasi-优化的模型。在第二步，我们 investigate了输入参数和变量的重要性，通过使用不同的输入特征来训练神经网络。</li>
<li>results: 我们发现，力量相关的参数在敏感分析中最为重要，而神经网络基于的输入特征选择也显示了承诺。然而，我们发现，使用神经网络模型的代谢能量消耗估计并没有达到传统模型的准确性。<details>
<summary>Abstract</summary>
Objective: As metabolic cost is a primary factor influencing humans' gait, we want to deepen our understanding of metabolic energy expenditure models. Therefore, this paper identifies the parameters and input variables, such as muscle or joint states, that contribute to accurate metabolic cost estimations. Methods: We explored the parameters of four metabolic energy expenditure models in a Monte Carlo sensitivity analysis. Then, we analysed the model parameters by their calculated sensitivity indices, physiological context, and the resulting metabolic rates during the gait cycle. The parameter combination with the highest accuracy in the Monte Carlo simulations represented a quasi-optimized model. In the second step, we investigated the importance of input parameters and variables by analysing the accuracy of neural networks trained with different input features. Results: Power-related parameters were most influential in the sensitivity analysis and the neural network-based feature selection. We observed that the quasi-optimized models produced negative metabolic rates, contradicting muscle physiology. Neural network-based models showed promising abilities but have been unable to match the accuracy of traditional metabolic energy expenditure models. Conclusion: We showed that power-related metabolic energy expenditure model parameters and inputs are most influential during gait. Furthermore, our results suggest that neural network-based metabolic energy expenditure models are viable. However, bigger datasets are required to achieve better accuracy. Significance: As there is a need for more accurate metabolic energy expenditure models, we explored which musculoskeletal parameters are essential when developing a model to estimate metabolic energy.
</details>
<details>
<summary>摘要</summary>
方法：我们在四种代谢能耗模型中进行了Monte Carlo敏感分析，然后分析了模型参数的计算敏感度指数、生理上的文脉和代谢过程中的代谢率。在Monte Carlo优化中，我们选择了最佳的参数组合，并在第二步中，通过不同输入特征的分析，了解输入参数和变量的重要性。结果：在敏感分析中，力量相关的参数具有最大的影响力，而神经网络基于的特征选择也显示了扩展的能力。然而，我们发现，在许多情况下，神经网络模型的准确性不如传统的代谢能耗模型。结论：我们发现，在步行过程中，力量相关的代谢能耗模型参数和输入变量具有最大的影响力。此外，我们的结果表明，神经网络基于的代谢能耗模型是可行的，但需要更大的数据来达到更高的准确性。重要性：由于代谢成本的估计是一个需要更加准确的问题，我们在这篇论文中探讨了 Musculoskeletal 参数是如何影响代谢能耗模型的。
</details></li>
</ul>
<hr>
<h2 id="Differential-Equation-Scaling-Limits-of-Shaped-and-Unshaped-Neural-Networks"><a href="#Differential-Equation-Scaling-Limits-of-Shaped-and-Unshaped-Neural-Networks" class="headerlink" title="Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks"></a>Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12079">http://arxiv.org/abs/2310.12079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mufan Bill Li, Mihai Nica</li>
<li>for: 这篇论文是关于无形activation函数的神经网络性能的研究，尤其是对于两种不同的无形网络架构（即Fully Connected ResNet和Multilayer Perceptron）的性能分析。</li>
<li>methods: 本文使用了不同的方法来分析无形网络的性能，包括使用差分方程来描述无形网络的架构，以及使用初值问题来分析无形网络的层次相关性。</li>
<li>results: 本文发现了两种无形网络架构在初始化时的相同架构准确性限制，并且对无形MLP网络的层次相关性进行了第一项级准确性修正。这些结果表明了无形网络和形态activation函数之间的连接，并开 up了研究正则化方法和形态activation函数之间的关系的可能性。<details>
<summary>Abstract</summary>
Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.   Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.   Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer $\ell$, then $q_t = \ell^2 (1 - \rho_\ell)$ with $t = \frac{\ell}{n}$ converges to an SDE with a singularity at $t=0$.   These results together provide a connection between shaped and unshaped network architectures, and opens up the possibility of studying the effect of normalization methods and how it connects with shaping activation functions.
</details>
<details>
<summary>摘要</summary>
近期的分析表明，在神经网络中使用扩展 activation function（即网络大小增长时Activation function也随着增长）会导致分析限制，这些结果并不直接告诉我们关于“常规”无形网络（即Activation function不变化与网络大小增长）的 anything。在这篇文章中，我们发现了两种类型的无形网络的极限性特征，即：首先，我们证明了以下两个架构在初始化时 converges to the same infinite-depth-and-width limit：（i）一个具有 $d^{-1/2}$ 因子的完全连接 ResNet，其中 $d$ 是网络深度。（ii）一个具有 $d \ll n$ 的多层感知器（MLP），其中 $d$ 是网络深度， activation 是 $d^{-1/2}$ 的折叠函数。其次，对于无形 MLP 的初始化，我们 derive the first order asymptotic correction to the layerwise correlation。 Specifically, if $\rho_\ell$ is the correlation at layer $\ell$, then $q_t = \ell^2 (1 - \rho_\ell)$ with $t = \frac{\ell}{n}$ converges to an SDE with a singularity at $t=0$.这些结果共同表明了无形和形 activation function 之间的连接，并开放了研究正规化方法和 activation function 的拟合方面的可能性。
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-scientific-data-a-pedagogical-review-for-astronomers"><a href="#Transformers-for-scientific-data-a-pedagogical-review-for-astronomers" class="headerlink" title="Transformers for scientific data: a pedagogical review for astronomers"></a>Transformers for scientific data: a pedagogical review for astronomers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12069">http://arxiv.org/abs/2310.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dimitrios Tanoglidis, Bhuvnesh Jain, Helen Qu</li>
<li>for: 该论文主要用于引入transformers深度学习架构和相关的生成AI产品，并为科学家介绍transformers的应用。</li>
<li>methods: 论文使用自注意机制和原始transformer架构，并介绍了在天文学中使用transformers的应用。</li>
<li>results: 论文介绍了自注意机制的数学基础和transformers的应用在时间序列和图像数据中的成果。Note: The above information is in Simplified Chinese text.<details>
<summary>Abstract</summary>
The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. The review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include a Frequently Asked Questions section for readers who are curious about generative AI or interested in getting started with transformers for their research problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>与ChatGPT和相关的生成AI产品相关的深度学习架构被称为transformers。初始应用于自然语言处理，transformers和它们利用的自注意机制已经在自然科学领域引起了广泛的关注。本文的教学和非正式评论的目的是引入transformers给科学家。文中包括自注意机制的数学基础、原始transformer架构的描述和在天文学中对时间序列和图像数据的应用。我们附加了关于生成AI或想要使用transformers解决研究问题的常见问题 section。
</details></li>
</ul>
<hr>
<h2 id="Learning-Gradient-Fields-for-Scalable-and-Generalizable-Irregular-Packing"><a href="#Learning-Gradient-Fields-for-Scalable-and-Generalizable-Irregular-Packing" class="headerlink" title="Learning Gradient Fields for Scalable and Generalizable Irregular Packing"></a>Learning Gradient Fields for Scalable and Generalizable Irregular Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19814">http://arxiv.org/abs/2310.19814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyang Xue, Mingdong Wu, Lin Lu, Haoxuan Wang, Hao Dong, Baoquan Chen</li>
<li>for:  solves the packing problem with irregularly shaped pieces, minimizing waste and avoiding overlap, using machine learning and conditional generative modeling.</li>
<li>methods:  employs the score-based diffusion model to learn gradient fields that encode constraint satisfaction and spatial relationships, and uses a coarse-to-fine refinement mechanism to generate packing solutions.</li>
<li>results:  demonstrates spatial utilization rates comparable to or surpassing those achieved by the teacher algorithm, and exhibits some level of generalization to shape variations.<details>
<summary>Abstract</summary>
The packing problem, also known as cutting or nesting, has diverse applications in logistics, manufacturing, layout design, and atlas generation. It involves arranging irregularly shaped pieces to minimize waste while avoiding overlap. Recent advances in machine learning, particularly reinforcement learning, have shown promise in addressing the packing problem. In this work, we delve deeper into a novel machine learning-based approach that formulates the packing problem as conditional generative modeling. To tackle the challenges of irregular packing, including object validity constraints and collision avoidance, our method employs the score-based diffusion model to learn a series of gradient fields. These gradient fields encode the correlations between constraint satisfaction and the spatial relationships of polygons, learned from teacher examples. During the testing phase, packing solutions are generated using a coarse-to-fine refinement mechanism guided by the learned gradient fields. To enhance packing feasibility and optimality, we introduce two key architectural designs: multi-scale feature extraction and coarse-to-fine relation extraction. We conduct experiments on two typical industrial packing domains, considering translations only. Empirically, our approach demonstrates spatial utilization rates comparable to, or even surpassing, those achieved by the teacher algorithm responsible for training data generation. Additionally, it exhibits some level of generalization to shape variations. We are hopeful that this method could pave the way for new possibilities in solving the packing problem.
</details>
<details>
<summary>摘要</summary>
<<SYS>> packing 问题，也称为割辑或嵌入，在物流、制造、布局设计和地图生成中有广泛的应用。它涉及到将不规则形状的物品安排，以最小化剩下物和避免重叠。 recent advances in machine learning，特别是强化学习，对 packing 问题提出了新的思路。在这项工作中，我们将更深入地探讨一种基于机器学习的新方法，将 packing 问题转化为 conditional generative modeling。为了解决不规则嵌入中的挑战，包括物体有效性约束和碰撞避免，我们的方法使用分数据模型来学习一系列的梯度场。这些梯度场表达了对约束满足和物体间的空间关系的学习。在测试阶段，我们使用一种粗细层次匀化机制，以指导学习的梯度场来生成嵌入解。为了提高嵌入可行性和优化，我们引入了两种关键的建筑设计：多尺度特征提取和粗细层次关系提取。我们对两种典型的工业嵌入领域进行实验，只考虑翻译。实验结果表明，我们的方法可以与教师算法负责数据生成的空间利用率相当，甚至超过。此外，它还有一定的泛化能力。我们希望这种方法可以为嵌入问题开拓新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Reward-Ambiguity-Through-Optimal-Transport-Theory-in-Inverse-Reinforcement-Learning"><a href="#Understanding-Reward-Ambiguity-Through-Optimal-Transport-Theory-in-Inverse-Reinforcement-Learning" class="headerlink" title="Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning"></a>Understanding Reward Ambiguity Through Optimal Transport Theory in Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12055">http://arxiv.org/abs/2310.12055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Baheri</li>
<li>for: 这 paper 的中心目标是寻找在观察到的专家行为中隐藏的奖励函数，以便不仅解释数据，还能够泛化到未经见过的情况。</li>
<li>methods: 这 paper 使用 optimal transport (OT) 理论，提供了一种新的视角来解决高维问题和奖励不确定性的问题。</li>
<li>results: 这 paper 的研究发现，通过 Wasserstein 距离来衡量奖励不确定性，并提供了一种中心表示或中心函数的确定方法，这些发现可以为高维 setting 中的 robust IRL 方法提供一种结构化的途径。<details>
<summary>Abstract</summary>
In inverse reinforcement learning (IRL), the central objective is to infer underlying reward functions from observed expert behaviors in a way that not only explains the given data but also generalizes to unseen scenarios. This ensures robustness against reward ambiguity where multiple reward functions can equally explain the same expert behaviors. While significant efforts have been made in addressing this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation. This paper harnesses the optimal transport (OT) theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying reward ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle reward ambiguity in high-dimensional settings.
</details>
<details>
<summary>摘要</summary>
倒 inverse reinforcement learning（IRL）的中心目标是从专家行为中推理出底层奖励函数，以解释数据并在未看到的情况下推广。这 Ensures  robustness  against 奖励ambiguity， where multiple 奖励函数可以一样 explain 专家行为。 although  significant efforts have been made to address this issue, current methods often face challenges with high-dimensional problems and lack a geometric foundation.this paper  harnesses the optimal transport（OT）theory to provide a fresh perspective on these challenges. By utilizing the Wasserstein distance from OT, we establish a geometric framework that allows for quantifying 奖励ambiguity and identifying a central representation or centroid of reward functions. These insights pave the way for robust IRL methodologies anchored in geometric interpretations, offering a structured approach to tackle 奖励ambiguity in high-dimensional settings.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-ML-Based-Surrogates-in-Bayesian-Approaches-to-Inverse-Problems"><a href="#Applications-of-ML-Based-Surrogates-in-Bayesian-Approaches-to-Inverse-Problems" class="headerlink" title="Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems"></a>Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12046">http://arxiv.org/abs/2310.12046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pelin Ersin, Emma Hayes, Peter Matthews, Paramjyoti Mohapatra, Elisa Negrini, Karl Schulz</li>
<li>for: 寻找波源位置在方正区域的逆问题，给出噪音解的解决方案。</li>
<li>methods: 使用神经网络作为代理模型，提高计算效率，使得Markov Chain Monte Carlo方法可以用于评估 posterior 分布中的源位置。</li>
<li>results: 通过寻找波源位置的方法，可以准确地从噪音数据中提取源位置信息。<details>
<summary>Abstract</summary>
Neural networks have become a powerful tool as surrogate models to provide numerical solutions for scientific problems with increased computational efficiency. This efficiency can be advantageous for numerically challenging problems where time to solution is important or when evaluation of many similar analysis scenarios is required. One particular area of scientific interest is the setting of inverse problems, where one knows the forward dynamics of a system are described by a partial differential equation and the task is to infer properties of the system given (potentially noisy) observations of these dynamics. We consider the inverse problem of inferring the location of a wave source on a square domain, given a noisy solution to the 2-D acoustic wave equation. Under the assumption of Gaussian noise, a likelihood function for source location can be formulated, which requires one forward simulation of the system per evaluation. Using a standard neural network as a surrogate model makes it computationally feasible to evaluate this likelihood several times, and so Markov Chain Monte Carlo methods can be used to evaluate the posterior distribution of the source location. We demonstrate that this method can accurately infer source-locations from noisy data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:神经网络已成为数学问题的强大工具，提供了计算效率的增强。这种效率可以在计算复杂的问题中帮助提高解决时间，或者在评估多个相似的分析场景时提高计算效率。一个科学领域的特别兴趣是反问题，即知道系统的前向动力学方程，并且要从（潜在噪声）观测中推断系统的性质。我们考虑了二维声波方程的反问题，即在平方Domain中推断声源的位置，给出噪声解的情况。在假设 Gaussian 噪声时，可以形式化一个likelihood函数，该函数需要一次前向模拟 per 评估。使用标准神经网络作为模拟模型，可以使计算这个likelihood多次成为可能，然后使用Markov Chain Monte Carlo 方法评估 posterior 分布。我们示示了这种方法可以准确地从噪声数据中推断声源位置。
</details></li>
</ul>
<hr>
<h2 id="Conformal-Drug-Property-Prediction-with-Density-Estimation-under-Covariate-Shift"><a href="#Conformal-Drug-Property-Prediction-with-Density-Estimation-under-Covariate-Shift" class="headerlink" title="Conformal Drug Property Prediction with Density Estimation under Covariate Shift"></a>Conformal Drug Property Prediction with Density Estimation under Covariate Shift</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12033">http://arxiv.org/abs/2310.12033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/siddharthal/CoDrug">https://github.com/siddharthal/CoDrug</a></li>
<li>paper_authors: Siddhartha Laghuvarapu, Zhen Lin, Jimeng Sun</li>
<li>for:  This paper aims to address the challenge of obtaining reliable uncertainty estimates in drug discovery tasks using Conformal Prediction (CP) and to provide valid prediction sets for molecular properties with a coverage guarantee.</li>
<li>methods:  The proposed method, CoDrug, employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift.</li>
<li>results:  In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, CoDrug was shown to provide valid prediction sets and to reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.<details>
<summary>Abstract</summary>
In drug discovery, it is vital to confirm the predictions of pharmaceutical properties from computational models using costly wet-lab experiments. Hence, obtaining reliable uncertainty estimates is crucial for prioritizing drug molecules for subsequent experimental validation. Conformal Prediction (CP) is a promising tool for creating such prediction sets for molecular properties with a coverage guarantee. However, the exchangeability assumption of CP is often challenged with covariate shift in drug discovery tasks: Most datasets contain limited labeled data, which may not be representative of the vast chemical space from which molecules are drawn. To address this limitation, we propose a method called CoDrug that employs an energy-based model leveraging both training data and unlabelled data, and Kernel Density Estimation (KDE) to assess the densities of a molecule set. The estimated densities are then used to weigh the molecule samples while building prediction sets and rectifying for distribution shift. In extensive experiments involving realistic distribution drifts in various small-molecule drug discovery tasks, we demonstrate the ability of CoDrug to provide valid prediction sets and its utility in addressing the distribution shift arising from de novo drug design models. On average, using CoDrug can reduce the coverage gap by over 35% when compared to conformal prediction sets not adjusted for covariate shift.
</details>
<details>
<summary>摘要</summary>
在药物发现中，确认计算模型预测的药品性能需要通过costly的湿lab实验进行验证。因此，获得可靠的不确定性估计是关键的，以便在后续实验验证中PRIORITIZE drug molecules。 Conformal Prediction（CP）是一种可靠的工具，可以创建包含预测性能的prediction sets。然而，CP中的交换性假设在药物发现任务中经常遇到冲击：大多数数据集只包含有限的标签数据，这些数据可能不能代表整个化学空间中的分子。为解决这个限制，我们提出了一种方法called CoDrug，它使用能量基本模型利用训练数据和无标签数据，以及Kernel Density Estimation（KDE）来评估分子集的浓度。然后，使用这些估计的浓度来权重分子样本，以建立预测集和纠正分布shift。在具有实际分布滑动的小分子药物发现任务中，我们通过实验证明CoDrug可以提供有效的预测集，并且在de novo drug design模型中 Addressing the distribution shift。在average上，使用CoDrug可以将覆盖缺口减少超过35%，比不 Rectifying for distribution shift。
</details></li>
</ul>
<hr>
<h2 id="Exact-and-efficient-solutions-of-the-LMC-Multitask-Gaussian-Process-model"><a href="#Exact-and-efficient-solutions-of-the-LMC-Multitask-Gaussian-Process-model" class="headerlink" title="Exact and efficient solutions of the LMC Multitask Gaussian Process model"></a>Exact and efficient solutions of the LMC Multitask Gaussian Process model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12032">http://arxiv.org/abs/2310.12032</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qwerty6191/projected-lmc">https://github.com/qwerty6191/projected-lmc</a></li>
<li>paper_authors: Olivier Truffinet, Karim Ammar, Jean-Philippe Argaud, Bertrand Bouriquet</li>
<li>for: 这个论文是关于多任务 Gaussian process  regression 或分类的一种非常通用的模型，它的表达能力和概念简单性很吸引人。但是，直接实现方式的复杂性是 cubic 在数据点和任务数量的平方，这意味着大多数应用中需要使用简化方法。然而，最近的研究表明，在某些条件下，模型的隐藏过程可以分离，从而实现 linear 复杂性。</li>
<li>methods: 我们在这篇论文中扩展了这些结果，并证明了在最通用的假设下，只需要一个轻度的噪声模型假设，就可以实现高效的精确计算。我们还提出了一种完整的参数化方法，并给出了质量函数，以便高效地优化。</li>
<li>results: 我们在synthetic数据上进行了参数研究，并证明了我们的方法的出色表现，相比之下unrestricted exact LMC和其他简化方法。总之， проекed LMC 模型是一种可靠和简单的代用方法，它可以大大简化一些计算，如离散一个数据点的cross-validation和幻想。<details>
<summary>Abstract</summary>
The Linear Model of Co-regionalization (LMC) is a very general model of multitask gaussian process for regression or classification. While its expressivity and conceptual simplicity are appealing, naive implementations have cubic complexity in the number of datapoints and number of tasks, making approximations mandatory for most applications. However, recent work has shown that under some conditions the latent processes of the model can be decoupled, leading to a complexity that is only linear in the number of said processes. We here extend these results, showing from the most general assumptions that the only condition necessary to an efficient exact computation of the LMC is a mild hypothesis on the noise model. We introduce a full parametrization of the resulting \emph{projected LMC} model, and an expression of the marginal likelihood enabling efficient optimization. We perform a parametric study on synthetic data to show the excellent performance of our approach, compared to an unrestricted exact LMC and approximations of the latter. Overall, the projected LMC appears as a credible and simpler alternative to state-of-the art models, which greatly facilitates some computations such as leave-one-out cross-validation and fantasization.
</details>
<details>
<summary>摘要</summary>
linear 模型的协同地域化 (LMC) 是一种非常通用的多任务 Gaussian 过程 regression 或 classification 模型。 虽其表达能力和概念简洁吸引人，但直接实现的方法具有 кубиック complexity 在数据点和任务数量上，使得大多数应用中需要使用 Approximations。然而，最近的研究表明，在某些条件下，latent 过程的模型可以减少，导致只有 linear 复杂度在数据点和任务数量上。我们在这里扩展这些结果，表明只需要对模型噪声模型进行一定的假设，就可以实现高效的精确计算。我们介绍了该模型的完整均衡 parametrization，并提供了计算 marginal 概率的表达，使得可以高效地优化。我们在synthetic数据上进行了参数研究，并证明了我们的方法在比较于未限制的精确 LMC 和其approximations 上表现出色。总之，Projected LMC 模型看起来是一种可靠和简单的代码，它可以大大简化一些计算，如离开一个 cross-validation 和幻想。
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-Discrete-Choice-Experiments-with-Machine-Learning-Guided-Adaptive-Design"><a href="#Nonparametric-Discrete-Choice-Experiments-with-Machine-Learning-Guided-Adaptive-Design" class="headerlink" title="Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design"></a>Nonparametric Discrete Choice Experiments with Machine Learning Guided Adaptive Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12026">http://arxiv.org/abs/2310.12026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhang Yin, Ruijiang Gao, Weiran Lin, Steven M. Shugan</li>
<li>For: 这个论文旨在设计用于满足消费者偏好的产品，以提高企业的成功。* Methods: 论文提出了一种名为 Gradient-based Survey (GBS) 的不 Parametric 的选择实验方法，用于多Attribute 产品设计。GBS 通过问题序列和响应者之前的选择来逐步定义产品特性。* Results: 对于在 simulations 中进行比较的 parametric 和非 Parametric 方法，GBS 具有更高的准确率和样本效率。<details>
<summary>Abstract</summary>
Designing products to meet consumers' preferences is essential for a business's success. We propose the Gradient-based Survey (GBS), a discrete choice experiment for multiattribute product design. The experiment elicits consumer preferences through a sequence of paired comparisons for partial profiles. GBS adaptively constructs paired comparison questions based on the respondents' previous choices. Unlike the traditional random utility maximization paradigm, GBS is robust to model misspecification by not requiring a parametric utility model. Cross-pollinating the machine learning and experiment design, GBS is scalable to products with hundreds of attributes and can design personalized products for heterogeneous consumers. We demonstrate the advantage of GBS in accuracy and sample efficiency compared to the existing parametric and nonparametric methods in simulations.
</details>
<details>
<summary>摘要</summary>
为商业成功，设计产品根据消费者的偏好非常重要。我们提议 Gradient-based Survey（GBS），一种多Attribute产品设计的灵活选择实验。这种实验通过一系列对半个配置进行对比，抽取消费者的偏好。与传统的随机Utility最大化理论不同，GBS不需要 Parametric Utility模型，因此更具鲁棒性。通过融合机器学习和实验设计，GBS可扩展到产品上百个特征，设计个性化产品 для多样化的消费者。我们通过模拟表明，GBS在准确性和样本效率方面比现有的参数化和非参数化方法有优势。
</details></li>
</ul>
<hr>
<h2 id="Iterative-Methods-for-Vecchia-Laplace-Approximations-for-Latent-Gaussian-Process-Models"><a href="#Iterative-Methods-for-Vecchia-Laplace-Approximations-for-Latent-Gaussian-Process-Models" class="headerlink" title="Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models"></a>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.12000">http://arxiv.org/abs/2310.12000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fabsig/GPBoost">https://github.com/fabsig/GPBoost</a></li>
<li>paper_authors: Pascal Kündig, Fabio Sigrist</li>
<li>for: 这篇论文旨在探讨高维函数模型（Gaussian Process，GP）的精确估计方法，以及组合Vecchia-Laplace近似法和迭代法的优化。</li>
<li>methods: 本论文使用Vecchia-Laplace近似法和迭代法来实现高维函数模型的精确估计，并提出了一些iterative方法来提高 computations的速度。</li>
<li>results: 论文的实验结果显示，使用Vecchia-Laplace近似法和迭代法可以大幅提高估计的速度，并且在一个大型卫星数据集上比起现有方法实现三倍的预测精度。<details>
<summary>Abstract</summary>
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based inference and a threefold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite data set. All methods are implemented in a free C++ software library with high-level Python and R packages.
</details>
<details>
<summary>摘要</summary>
潜在 Gaussian 过程（GP）模型是一种灵活的可信度非参数函数模型。Vecchia  aproximations 是一种精准的GP模型 Approximations 可以在大量数据时提高计算效率，而 Laplace  Approximations 是一种快速的方法，它具有 asymptotic convergence guarantees 来近似 marginal likelihoods 和 posterior predictive distributions 的非 Gaussian 类型。然而，Vecchia-Laplace  approximations 的计算复杂度随着样本大小增加，使用 direct solver methods such as Cholesky decomposition 时会变得不可持久。因此，在大数据集时，Vecchia-Laplace  approximations 的计算变得繁琐。在这篇文章中，我们提出了一些迭代法来实现Vecchia-Laplace approximations 的推理，使计算速度比 Cholesky-based 计算更快。我们还进行了理论分析和实验室测试，并在 simulated 和实际数据集上 obtaint 一个级别的速度提升和三倍的预测精度。所有方法都是在一个免费 C++ 软件库中实现的，并提供了高级 Python 和 R 包。
</details></li>
</ul>
<hr>
<h2 id="Removing-Spurious-Concepts-from-Neural-Network-Representations-via-Joint-Subspace-Estimation"><a href="#Removing-Spurious-Concepts-from-Neural-Network-Representations-via-Joint-Subspace-Estimation" class="headerlink" title="Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation"></a>Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11991">http://arxiv.org/abs/2310.11991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Floris Holstege, Bram Wouters, Noud van Giersbergen, Cees Diks</li>
<li>for: 本研究旨在提高神经网络模型对异常数据的泛化性能，通过禁用干扰因素。</li>
<li>methods: 本研究提出了一种迭代算法，通过同时确定两个低维度正交子空间来分离干扰因素和主任务因素。</li>
<li>results: 对 Wasserstein 和 CelebA 图像Dataset以及 MultiNLI 自然语言处理Dataset进行评估，发现该算法可以超过现有的概念除除法。<details>
<summary>Abstract</summary>
Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
</details>
<details>
<summary>摘要</summary>
neural networks 中的 out-of-distribution 泛化受到假 correlate 的干扰。一般的方法是通过 removing spurious concepts 来 mitigate 这种情况。现有的 concept-removal 方法往往过于积极，不小心 eliminating 主要任务相关的特征，从而害到模型性能。我们提出了一种迭代算法，jointly identifying two low-dimensional orthogonal subspaces 在 neural network representation 中，以分离假 correlations 和主要任务相关的特征。我们在 Waterbirds、CelebA 和 MultiNLI 等 benchmark datasets 上评估了该算法，并显示它在 existing concept removal 方法 上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Image-Clustering-with-External-Guidance"><a href="#Image-Clustering-with-External-Guidance" class="headerlink" title="Image Clustering with External Guidance"></a>Image Clustering with External Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11989">http://arxiv.org/abs/2310.11989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunfan Li, Peng Hu, Dezhong Peng, Jiancheng Lv, Jianping Fan, Xi Peng</li>
<li>for: 提高图像归一化的性能，利用外部知识作为指导信号</li>
<li>methods: 利用WordNet词语来增强特征分化，并在图像和文本模式之间进行相互馈散学习</li>
<li>results: 在五个常用的图像归一化 benchmark 上达到了状态机的性能，包括全部 ImageNet-1K 数据集<details>
<summary>Abstract</summary>
The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted to mining internal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance the feature discriminability. Then, to improve image clustering performance, TAC collaborates text and image modalities by mutually distilling cross-modal neighborhood information. Experiments demonstrate that TAC achieves state-of-the-art performance on five widely used and three more challenging image clustering benchmarks, including the full ImageNet-1K dataset.
</details>
<details>
<summary>摘要</summary>
核心是在含有先验知识的情况下构建监督信号。从经典k-means基于数据压缩到最近的对照集成监督，集群方法的演化都与监督信号的进步相对应。到目前为止，大量的内部监督信号从数据中被挖掘出来。然而，外部知识，如semantic description，尚未得到了适当的利用。在这种情况下，我们提议利用外部知识作为新的监督信号，即使它与给定数据看起来不相关。为了实现和验证我们的想法，我们设计了一种受外部知识引导的集群方法（Text-Aided Clustering，TAC）。TAC首先选择和检索WordNet词汇，以增强特征描述性。然后，为了提高图像集群性能，TAC与文本和图像模式之间进行协同整合，通过相互洗礼距离信息。实验表明，TAC在5个广泛使用的和3个更加挑战的图像集群 benchmark上达到了状态机器人的性能。
</details></li>
</ul>
<hr>
<h2 id="A-Finite-Horizon-Approach-to-Active-Level-Set-Estimation"><a href="#A-Finite-Horizon-Approach-to-Active-Level-Set-Estimation" class="headerlink" title="A Finite-Horizon Approach to Active Level Set Estimation"></a>A Finite-Horizon Approach to Active Level Set Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11985">http://arxiv.org/abs/2310.11985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phillip Kearns, Bruno Jedynak, John Lipor</li>
<li>for: 本文目的是提出一种活动学习方法来实现等值集 estimation（LSE），以最小化最终估计误差和旅行距离。</li>
<li>methods: 本文使用一种finite-horizon搜索过程来实现LSE，并通过调整一个参数来让方法兼顾估计准确性和旅行距离。</li>
<li>results: 实验表明，当cost of travel增加时，我们的方法可以更好地使用距离非偏视来提高估计精度，并在真实的空气质量数据上实现约一半的估计误差。<details>
<summary>Abstract</summary>
We consider the problem of active learning in the context of spatial sampling for level set estimation (LSE), where the goal is to localize all regions where a function of interest lies above/below a given threshold as quickly as possible. We present a finite-horizon search procedure to perform LSE in one dimension while optimally balancing both the final estimation error and the distance traveled for a fixed number of samples. A tuning parameter is used to trade off between the estimation accuracy and distance traveled. We show that the resulting optimization problem can be solved in closed form and that the resulting policy generalizes existing approaches to this problem. We then show how this approach can be used to perform level set estimation in higher dimensions under the popular Gaussian process model. Empirical results on synthetic data indicate that as the cost of travel increases, our method's ability to treat distance nonmyopically allows it to significantly improve on the state of the art. On real air quality data, our approach achieves roughly one fifth the estimation error at less than half the cost of competing algorithms.
</details>
<details>
<summary>摘要</summary>
我们在各种空间采样中考虑了活动学习，其目标是尽可能快地找到一个函数关注的区域是否超过了一定的阈值。我们提出了一种有限距离搜索过程，用于在一维中进行最优化的水平集估计，同时尽量减少最终估计误差和旅行距离。我们使用一个调整参数，以让最终估计误差和旅行距离之间进行负面交易。我们表明，这个优化问题可以在关闭式形式下解决，并且得到的策略可以折衔现有方法。然后，我们展示了如何使用这种方法来进行高维空间下的水平集估计，使用泊松过程模型。我们的实验结果表明，当成本增加时，我们的方法可以不偏袋见茫地减少估计误差。在实际空气质量数据上，我们的方法可以实现约一剑五分之一的估计误差，而且花费比竞争算法少得多。
</details></li>
</ul>
<hr>
<h2 id="Can-bin-wise-scaling-improve-consistency-and-adaptivity-of-prediction-uncertainty-for-machine-learning-regression"><a href="#Can-bin-wise-scaling-improve-consistency-and-adaptivity-of-prediction-uncertainty-for-machine-learning-regression" class="headerlink" title="Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?"></a>Can bin-wise scaling improve consistency and adaptivity of prediction uncertainty for machine learning regression ?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11978">http://arxiv.org/abs/2310.11978</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ppernot/2023_bvs">https://github.com/ppernot/2023_bvs</a></li>
<li>paper_authors: Pascal Pernot</li>
<li>for: 这篇论文是为了提出一种基于不同变量的准备误差抑制方法，以提高机器学习回归问题的预测不确定性calibration的效果。</li>
<li>methods: 这篇论文使用了 uncertainty-based binning 方法，通过基于不同变量的分配来改进calibration的条件，即consistency。</li>
<li>results: 作者在一个 benchmark 数据集上测试了 BVS 和其变体，与 isotonic regression 进行比较，发现 BVS 和其变体可以更好地适应不同的输入特征，提高calibration的效果。<details>
<summary>Abstract</summary>
Binwise Variance Scaling (BVS) has recently been proposed as a post hoc recalibration method for prediction uncertainties of machine learning regression problems that is able of more efficient corrections than uniform variance (or temperature) scaling. The original version of BVS uses uncertainty-based binning, which is aimed to improve calibration conditionally on uncertainty, i.e. consistency. I explore here several adaptations of BVS, in particular with alternative loss functions and a binning scheme based on an input-feature (X) in order to improve adaptivity, i.e. calibration conditional on X. The performances of BVS and its proposed variants are tested on a benchmark dataset for the prediction of atomization energies and compared to the results of isotonic regression.
</details>
<details>
<summary>摘要</summary>
Binwise Variance Scaling (BVS) 是一种最近提出的机器学习回归问题预测不确定性的后处修正方法，能够更有效地 corrections than uniform variance (或温度) scaling。原版本的 BVS 使用不确定性基于的分类，以提高预测条件上的准确性，即一致性。我在这里 explore 了 BVS 的一些变体，包括使用不同的损失函数和基于输入特征（X）的分类方案，以提高适应性，即预测条件下的准确性。我们对一个 benchmark 数据集进行了预测 atomization energies 的测试，并与ISOREG 的结果进行了比较。Here's the translation in Traditional Chinese: Binwise Variance Scaling (BVS) 是一种最近提出的机器学习回归问题的预测不确定性的后置修正方法，能够更有效地 corrections than uniform variance (或温度) scaling。原版本的 BVS 使用不确定性基于的分类，以提高预测条件上的准确性，即一致性。我在这里 explore 了 BVS 的一些变体，包括使用不同的损失函数和基于输入特征（X）的分类方案，以提高适应性，即预测条件下的准确性。我们对一个 benchmark 数据集进行了预测 atomization energies 的测试，并与ISOREG 的结果进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Take-the-aTrain-Introducing-an-Interface-for-the-Accessible-Transcription-of-Interviews"><a href="#Take-the-aTrain-Introducing-an-Interface-for-the-Accessible-Transcription-of-Interviews" class="headerlink" title="Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews"></a>Take the aTrain. Introducing an Interface for the Accessible Transcription of Interviews</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11967">http://arxiv.org/abs/2310.11967</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bandas-center/atrain">https://github.com/bandas-center/atrain</a></li>
<li>paper_authors: Armin Haberl, Jürgen Fleiß, Dominik Kowald, Stefan Thalmann</li>
<li>for: 这个论文是为了帮助研究人员进行多种语言的语音数据转译，不需要编程技能，可以在大多数计算机上运行，不需要互联网连线，且不会上传数据到服务器。</li>
<li>methods: 论文使用OpenAI的Whisper模型和声音识别技术，与MAXQDA和ATLAS.ti等流行的质量数据分析软件集成，提供了易于使用的图形用户界面，可以通过Microsoft Store上的Windows应用程序安装。</li>
<li>results: 根据论文的描述，在现有的移动CPU上，转译时间约为音频档案的2-3倍，如果有入门级的图形卡，则转译速度增加到音频档案的20%。<details>
<summary>Abstract</summary>
aTrain is an open-source and offline tool for transcribing audio data in multiple languages with CPU and NVIDIA GPU support. It is specifically designed for researchers using qualitative data generated from various forms of speech interactions with research participants. aTrain requires no programming skills, runs on most computers, does not require an internet connection, and was verified not to upload data to any server. aTrain combines OpenAI's Whisper model with speaker recognition to provide output that integrates with the popular qualitative data analysis software tools MAXQDA and ATLAS.ti. It has an easy-to-use graphical interface and is provided as a Windows-App through the Microsoft Store allowing for simple installation by researchers. The source code is freely available on GitHub. Having developed aTrain with a focus on speed on local computers, we show that the transcription time on current mobile CPUs is around 2 to 3 times the duration of the audio file using the highest-accuracy transcription models. If an entry-level graphics card is available, the transcription speed increases to 20% of the audio duration.
</details>
<details>
<summary>摘要</summary>
aTrain 是一个开源、离线工具，用于转换多种语言的语音数据。它是特意针对对谈话参与者的质数数据进行研究而设计，并且不需要程式码技能，可以在大多数电脑上运行，不需要网页连线，并且确保没有上传数据到服务器。aTrain 结合 OpenAI 的 Whisper 模型和话者识别系统，以提供与 MAXQDA 和 ATLAS.ti 等受欢迎的质数数据分析软件集成。它具有易用的 графі式界面，通过 Microsoft Store 提供为 Windows 应用程序，让研究人员可以简单地安装。源代码则是免费公开在 GitHub 上。我们透过专注于本地电脑的速度，显示在现有的移动 CPU 上，转换时间约为音频档案的2-3倍，使用最高精度转换模型。如果有入门级的显卡可用，则转换速度将提高到音频档案的20%。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Payload-Configuration-for-Satellites-using-Machine-Learning"><a href="#Flexible-Payload-Configuration-for-Satellites-using-Machine-Learning" class="headerlink" title="Flexible Payload Configuration for Satellites using Machine Learning"></a>Flexible Payload Configuration for Satellites using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11966">http://arxiv.org/abs/2310.11966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcele O. K. Mendonca, Flor G. Ortiz-Gomez, Jorge Querol, Eva Lagunas, Juan A. Vásquez Peralvo, Victor Monzon Baeza, Symeon Chatzinotas, Bjorn Ottersten</li>
<li>for: 提高卫星通信系统的效率和质量，适应不同的吞吐量和延迟要求。</li>
<li>methods: 使用机器学习（ML）技术进行无线资源管理（RRM），将RRM任务定义为一个回归型ML问题，并将RRM目标和约束集成到损失函数中，以便ML算法尽可能地减小。</li>
<li>results: 通过对ML模型的表现进行评估，并考虑模型的资源分配决策对总体通信系统性能的影响，提出了一种Context-aware ML metric。<details>
<summary>Abstract</summary>
Satellite communications, essential for modern connectivity, extend access to maritime, aeronautical, and remote areas where terrestrial networks are unfeasible. Current GEO systems distribute power and bandwidth uniformly across beams using multi-beam footprints with fractional frequency reuse. However, recent research reveals the limitations of this approach in heterogeneous traffic scenarios, leading to inefficiencies. To address this, this paper presents a machine learning (ML)-based approach to Radio Resource Management (RRM).   We treat the RRM task as a regression ML problem, integrating RRM objectives and constraints into the loss function that the ML algorithm aims at minimizing. Moreover, we introduce a context-aware ML metric that evaluates the ML model's performance but also considers the impact of its resource allocation decisions on the overall performance of the communication system.
</details>
<details>
<summary>摘要</summary>
卫星通信，现代连接的关键，扩展至海上、航空和远郊地区， terrestrial 网络无法实现。现有的 GEO 系统在多个扫描面上均匀分配功率和频率，使用多扫描面 fractional frequency reuse。然而， latest research 显示这种方法在多样化流量场景下存在限制，导致不充分利用。为解决这个问题，这篇论文提出一种基于机器学习（ML）的Radio Resource Management（RRM）方法。我们将 RRM 任务视为一个回归 ML 问题，将 RRM 目标和约束 integrate 到 ML 算法目标函数中。此外，我们还引入了一种 context-aware ML 指标，评估 ML 模型的性能，同时考虑它的资源分配决策对通信系统的总性能的影响。
</details></li>
</ul>
<hr>
<h2 id="Recasting-Continual-Learning-as-Sequence-Modeling"><a href="#Recasting-Continual-Learning-as-Sequence-Modeling" class="headerlink" title="Recasting Continual Learning as Sequence Modeling"></a>Recasting Continual Learning as Sequence Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11952">http://arxiv.org/abs/2310.11952</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soochan-lee/cl-as-seq">https://github.com/soochan-lee/cl-as-seq</a></li>
<li>paper_authors: Soochan Lee, Jaehyeon Son, Gunhee Kim</li>
<li>for: 本研究旨在将重要的机器学习领域——启发学习和序列模型——加强连接起来。即我们提议将启发学习视为序列模型问题，使高级序列模型可以用于启发学习。在这种形式下，启发学习过程变成了序列模型的前进传播。</li>
<li>methods: 我们采用了元 continual learning（MCL）框架，在多个启发学习集合中训练序列模型。作为具体的例子，我们示cases了使用 transformers 和其高效变体作为 MCL 方法。</li>
<li>results: 我们在七个 bencmarks 上进行了七个benchmark，包括分类和回归任务，结果显示了序列模型可以是通用 MCL 的有ffektive解决方案。<details>
<summary>Abstract</summary>
In this work, we aim to establish a strong connection between two significant bodies of machine learning research: continual learning and sequence modeling. That is, we propose to formulate continual learning as a sequence modeling problem, allowing advanced sequence models to be utilized for continual learning. Under this formulation, the continual learning process becomes the forward pass of a sequence model. By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes. As a specific example of our new formulation, we demonstrate the application of Transformers and their efficient variants as MCL methods. Our experiments on seven benchmarks, covering both classification and regression, show that sequence models can be an attractive solution for general MCL.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们想要建立两个机器学习研究领域之间的强有力连接：不间断学习和序列模型。即我们提议将不间断学习问题设置为序列模型问题，以便使用高级序列模型进行不间断学习。根据这种设置，不间断学习过程变成了序列模型的前向传播。通过采用meta-不间断学习（MCL）框架，我们可以在多个不间断学习集合上训练序列模型。为了示例，我们展示了使用Transformers和其高效变体作为MCL方法的应用。我们在七个标准准确的benchmark上进行了七种不同的实验，包括分类和回归任务，结果表明序列模型可以是通用MCL的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Spectral-Variational-AutoEncoder-ISVAE-for-time-series-clustering"><a href="#Interpretable-Spectral-Variational-AutoEncoder-ISVAE-for-time-series-clustering" class="headerlink" title="Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering"></a>Interpretable Spectral Variational AutoEncoder (ISVAE) for time series clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11940">http://arxiv.org/abs/2310.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Óscar Jiménez Rama, Fernando Moreno-Pino, David Ramírez, Pablo M. Olmos</li>
<li>for: 这篇论文是为了提出一种新的变量自动编码器（VAE）模型，该模型具有可解释性的瓶颈（Filter Bank，FB），以便学习更加可解释的潜在空间。</li>
<li>methods: 该模型使用了VAE的基本结构，并在其前置了FB。FB强制VAE关注输入信号中最重要的部分，从而学习一个新的编码${f_0}$，该编码具有更高的可解释性和分化性。</li>
<li>results: 实验结果表明，ISVAE模型比传统的VAE模型在分类率上表现更高，并且可以更好地处理复杂的数据配置。此外，${f_0}$的演化征imatters表明了群集之间的相似性。<details>
<summary>Abstract</summary>
The best encoding is the one that is interpretable in nature. In this work, we introduce a novel model that incorporates an interpretable bottleneck-termed the Filter Bank (FB)-at the outset of a Variational Autoencoder (VAE). This arrangement compels the VAE to attend on the most informative segments of the input signal, fostering the learning of a novel encoding ${f_0}$ which boasts enhanced interpretability and clusterability over traditional latent spaces. By deliberately constraining the VAE with this FB, we intentionally constrict its capacity to access broad input domain information, promoting the development of an encoding that is discernible, separable, and of reduced dimensionality. The evolutionary learning trajectory of ${f_0}$ further manifests as a dynamic hierarchical tree, offering profound insights into cluster similarities. Additionally, for handling intricate data configurations, we propose a tailored decoder structure that is symmetrically aligned with FB's architecture. Empirical evaluations highlight the superior efficacy of ISVAE, which compares favorably to state-of-the-art results in clustering metrics across real-world datasets.
</details>
<details>
<summary>摘要</summary>
最佳编码是可解释的编码。在这项工作中，我们提出了一种新的模型，其中包含了一个可解释的瓶颈（Filter Bank，FB），这个瓶颈位于Variational Autoencoder（VAE）的开头。这种设计使得VAE需要关注输入信号中最重要的信息，从而促进了学习一个新的编码${f_0}$，该编码具有更高的可解释性和分布性。通过强制VAE通过FB进行制约，我们故意削弱VAE对输入信号范围广的信息访问权限，从而促进了编码的可读性、分割性和维度减少。${f_0}$的演化学习轨迹更显示出了动态层次树的形式，提供了深刻的群集相似性的启示。此外，为处理复杂的数据配置，我们提议一种适应FB的编码结构。实验证明，ISVAE的效果明显高于州际级的结果，在真实世界数据集上达到了高度的分 clustering  metrics。
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Policy-Gradient-On-the-Nesterov-Momentum-for-Reinforcement-Learning"><a href="#Accelerated-Policy-Gradient-On-the-Nesterov-Momentum-for-Reinforcement-Learning" class="headerlink" title="Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning"></a>Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11897">http://arxiv.org/abs/2310.11897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nycu-rl-bandits-lab/apg">https://github.com/nycu-rl-bandits-lab/apg</a></li>
<li>paper_authors: Yen-Ju Chen, Nai-Chieh Huang, Ping-Chun Hsieh</li>
<li>for: 本研究证明了使用推移 momentum 加速度 gradient 方法可以在 reinforcement learning 中提高 converges 率。</li>
<li>methods: 本文使用 Nesterov 加速度 gradient 方法（NAG），并对其进行了适应以适应 reinforcement learning 中的 softmax 政策参数化。</li>
<li>results: 我们表明了 NAG 在 true gradient 下可以在 $\tilde{O}(1&#x2F;t^2)$ 时间复杂度下连续 converges 到优化的政策。此外，我们还通过数值验证表明了 NAG 可以在实际应用中提高 converge 性。<details>
<summary>Abstract</summary>
Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-concave regime, where APG could benefit significantly from the momentum, within finite iterations. By means of numerical validation, we confirm that APG exhibits $\tilde{O}(1/t^2)$ rate as well as show that APG could significantly improve the convergence behavior over the standard policy gradient.
</details>
<details>
<summary>摘要</summary>
We formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. This is the first characterization of the global convergence rate of NAG in the context of RL. Our analysis relies on one interesting finding: regardless of the initialization, APG could end up reaching a locally nearly-concave regime, where APG could benefit significantly from the momentum, within finite iterations.Numerical validation confirms that APG exhibits a $\tilde{O}(1/t^2)$ rate and shows that APG could significantly improve the convergence behavior over the standard policy gradient.
</details></li>
</ul>
<hr>
<h2 id="A-Hyperparameter-Study-for-Quantum-Kernel-Methods"><a href="#A-Hyperparameter-Study-for-Quantum-Kernel-Methods" class="headerlink" title="A Hyperparameter Study for Quantum Kernel Methods"></a>A Hyperparameter Study for Quantum Kernel Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11891">http://arxiv.org/abs/2310.11891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sebastian Egginger, Alona Sakhnenko, Jeanette Miriam Lorenz</li>
<li>for: 本研究旨在investigating the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels, and exploring the use of the geometric difference as a tool for evaluating the potential for quantum advantage.</li>
<li>methods: 本研究使用了 quantum kernel methods and hyperparameter optimization techniques to evaluate the performance of quantum and classical machine learning models on 11 datasets. The geometric difference was used as a closeness measure between the two kernel-based machine learning approaches.</li>
<li>results: 研究发现，hyperparameter optimization是critical for achieving good model performance and reducing the generalization gap between classical and quantum kernels. The geometric difference can be a useful tool for evaluating the potential for quantum advantage, and can help identify commodities that can be exploited when examining a new dataset.<details>
<summary>Abstract</summary>
Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical machine learning. Especially for the quantum Hamiltonian evolution feature map, the scaling of the input data has been shown to be crucial. However, there are additional parameters left to be optimized, like the best number of qubits to trace out before computing a projected quantum kernel. We investigate the influence of these hyperparameters and compare the classically reliable method of cross validation with the method of choosing based on the geometric difference. Based on the thorough investigation of the hyperparameters across 11 datasets we identified commodities that can be exploited when examining a new dataset. In addition, our findings contribute to better understanding of the applicability of the geometric difference.
</details>
<details>
<summary>摘要</summary>
量子kernels方法是量子机器学习中的一种有前途的方法，这主要归功于它们的 garantías。它们的可见性使得可以对数据进行预选择，以确定它们是否具有量子优势。以前的工作在开发了 геомétríain difference，这可以理解为两种基于kernel的机器学习方法之间的距离度量，主要是quantum kernel和классическийkernel之间的距离。这个指标连接了量子和классиical模型复杂性。因此，它提出了问题，是否可以通过其与模型复杂性的关系来使用 geometric difference 作为评估工具？在这种工作中，我们investigate了hyperparameter的选择对模型性能和量子和классиical kernel之间的泛化差异的影响。特别是 для量子 Hamiltonian 演化特征图，输入数据的涨落Scaling 已经被证明是关键。然而，还有其他参数需要优化，例如最佳的量子bits数量来计算projected quantum kernel。我们 investigate了这些超参数的影响，并将cross validation 方法与基于 geometric difference 的选择方法进行比较。通过对 11 个数据集进行了全面的超参数调整，我们发现了一些可以利用的商品，并对量子和классиical kernel之间的泛化差异进行了更好的理解。
</details></li>
</ul>
<hr>
<h2 id="Building-a-Graph-based-Deep-Learning-network-model-from-captured-traffic-traces"><a href="#Building-a-Graph-based-Deep-Learning-network-model-from-captured-traffic-traces" class="headerlink" title="Building a Graph-based Deep Learning network model from captured traffic traces"></a>Building a Graph-based Deep Learning network model from captured traffic traces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11889">http://arxiv.org/abs/2310.11889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Güemes-Palau, Miquel Ferriol Galmés, Albert Cabellos-Aparicio, Pere Barlet-Ros</li>
<li>for: 本研究旨在提出一种基于图神经网络（GNN）的解决方案，用于更好地捕捉实际网络场景中的复杂性。</li>
<li>methods: 本研究使用了一种新的编码方法，用于从捕捉的包序列中提取信息，以及一种改进的消息传递算法，用于更好地表示物理网络中的依赖关系。</li>
<li>results: 我们的实验结果表明，提议的解决方案能够学习和泛化到未看过的捕捉网络场景。<details>
<summary>Abstract</summary>
Currently the state of the art network models are based or depend on Discrete Event Simulation (DES). While DES is highly accurate, it is also computationally costly and cumbersome to parallelize, making it unpractical to simulate high performance networks. Additionally, simulated scenarios fail to capture all of the complexities present in real network scenarios. While there exists network models based on Machine Learning (ML) techniques to minimize these issues, these models are also trained with simulated data and hence vulnerable to the same pitfalls. Consequently, the Graph Neural Networking Challenge 2023 introduces a dataset of captured traffic traces that can be used to build a ML-based network model without these limitations. In this paper we propose a Graph Neural Network (GNN)-based solution specifically designed to better capture the complexities of real network scenarios. This is done through a novel encoding method to capture information from the sequence of captured packets, and an improved message passing algorithm to better represent the dependencies present in physical networks. We show that the proposed solution it is able to learn and generalize to unseen captured network scenarios.
</details>
<details>
<summary>摘要</summary>
现在的状态艺术网络模型都基于不可countdown事件模拟（DES）。虽然DES具有高度准确的优点，但也有计算成本高和并行化困难，使得模拟高性能网络不实际。此外，模拟场景不能捕捉实际网络场景中的所有复杂性。而现有的网络模型基于机器学习（ML）技术来减少这些问题，但这些模型又是通过模拟数据进行训练，因此也受到相同的局限性。因此，2023年的图解网络挑战（GNN）引入了一个包含流量轨迹的数据集，可以用于构建一个基于机器学习（ML）的网络模型，不受上述局限性的影响。在本文中，我们提出了一种基于图解网络（GNN）的解决方案，通过一种新的编码方法来捕捉从流量序列中获得的信息，以及一种改进的消息传递算法来更好地表示物理网络中的依赖关系。我们示出了我们的解决方案能够学习和掌握未看过的捕捉网络场景。
</details></li>
</ul>
<hr>
<h2 id="Online-Convex-Optimization-with-Switching-Cost-and-Delayed-Gradients"><a href="#Online-Convex-Optimization-with-Switching-Cost-and-Delayed-Gradients" class="headerlink" title="Online Convex Optimization with Switching Cost and Delayed Gradients"></a>Online Convex Optimization with Switching Cost and Delayed Gradients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11880">http://arxiv.org/abs/2310.11880</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spandan Senapati, Rahul Vaze</li>
<li>for: 这个论文研究了在有限信息设定下的在线半正定优化问题，特别是使用quadratic和linear switching cost。</li>
<li>methods: 该论文提出了一种名为online multiple gradient descent（OMGD）算法，用于解决这个问题。</li>
<li>results: 论文显示了OMGD算法的竞争比例upper bound为$4(L + 5) + \frac{16(L + 5)}{\mu}$，并且证明了这个Upper bound是order-wise tight。此外，论文还证明了任何在线算法的竞争比例至少为$\max{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})}$。<details>
<summary>Abstract</summary>
We consider the online convex optimization (OCO) problem with quadratic and linear switching cost in the limited information setting, where an online algorithm can choose its action using only gradient information about the previous objective function. For $L$-smooth and $\mu$-strongly convex objective functions, we propose an online multiple gradient descent (OMGD) algorithm and show that its competitive ratio for the OCO problem with quadratic switching cost is at most $4(L + 5) + \frac{16(L + 5)}{\mu}$. The competitive ratio upper bound for OMGD is also shown to be order-wise tight in terms of $L,\mu$. In addition, we show that the competitive ratio of any online algorithm is $\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})\}$ in the limited information setting when the switching cost is quadratic. We also show that the OMGD algorithm achieves the optimal (order-wise) dynamic regret in the limited information setting. For the linear switching cost, the competitive ratio upper bound of the OMGD algorithm is shown to depend on both the path length and the squared path length of the problem instance, in addition to $L, \mu$, and is shown to be order-wise, the best competitive ratio any online algorithm can achieve. Consequently, we conclude that the optimal competitive ratio for the quadratic and linear switching costs are fundamentally different in the limited information setting.
</details>
<details>
<summary>摘要</summary>
我们考虑在有限信息设定下的线上凸优化（OCO）问题，其中一个线上算法可以根据过去的目标函数GradientInformation选择行动。对于$L$-smooth和$\mu$-强制凸目标函数，我们提出了一个线上多重梯度降低（OMGD）算法，并证明其在具有quadratic switching cost的OCO问题中的竞争比率不大于$4(L + 5) + \frac{16(L + 5)}{\mu}$。此外，我们还证明了OMGD算法的竞争比率Upper bound是order-wise tight in terms of $L,\mu$。另外，我们还证明了在有限信息设定下，任何线上算法的竞争比率都是$\max\{\Omega(L), \Omega(\frac{L}{\sqrt{\mu})\}$。此外，我们还证明了OMGD算法在有限信息设定下具有最佳（order-wise）动态遗憾。在linear switching cost的情况下，我们证明了OMGD算法的竞争比率Upper bound取决于问题实体的路径长度和平方路径长度，而且随着$L, \mu$的变化而变化。此外，我们还证明了OMGD算法在linear switching cost的情况下具有order-wise最佳的竞争比率。因此，我们结论到了quadratic和linear switching cost在有限信息设定下的竞争比率是基本不同的。
</details></li>
</ul>
<hr>
<h2 id="SQ-Lower-Bounds-for-Learning-Mixtures-of-Linear-Classifiers"><a href="#SQ-Lower-Bounds-for-Learning-Mixtures-of-Linear-Classifiers" class="headerlink" title="SQ Lower Bounds for Learning Mixtures of Linear Classifiers"></a>SQ Lower Bounds for Learning Mixtures of Linear Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11876">http://arxiv.org/abs/2310.11876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilias Diakonikolas, Daniel M. Kane, Yuxin Sun</li>
<li>For: 学习混合线性分类器下 Gaussian covariates 问题。* Methods: 使用 Statistical Query (SQ) 算法来解决问题，并提供了一个新的圆形设计技术。* Results: 得到了一个 Statistical Query (SQ) 下界，表明现有算法的复杂性为 $n^{\mathrm{poly}(1&#x2F;\Delta) \log(r)} $，其中 $\Delta$ 是 $\mathbf{v}_\ell$ 对应的下界Pairwise $\ell_2$-separation。<details>
<summary>Abstract</summary>
We study the problem of learning mixtures of linear classifiers under Gaussian covariates. Given sample access to a mixture of $r$ distributions on $\mathbb{R}^n$ of the form $(\mathbf{x},y_{\ell})$, $\ell\in [r]$, where $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n)$ and $y_\ell=\mathrm{sign}(\langle\mathbf{v}_\ell,\mathbf{x}\rangle)$ for an unknown unit vector $\mathbf{v}_\ell$, the goal is to learn the underlying distribution in total variation distance. Our main result is a Statistical Query (SQ) lower bound suggesting that known algorithms for this problem are essentially best possible, even for the special case of uniform mixtures. In particular, we show that the complexity of any SQ algorithm for the problem is $n^{\mathrm{poly}(1/\Delta) \log(r)}$, where $\Delta$ is a lower bound on the pairwise $\ell_2$-separation between the $\mathbf{v}_\ell$'s. The key technical ingredient underlying our result is a new construction of spherical designs that may be of independent interest.
</details>
<details>
<summary>摘要</summary>
我们研究混合线性分类器学习问题，假设我们有一个混合的$r$个分布在 $\mathbb{R}^n$ 上，每个分布的形式是 $({\mathbf{x},y_{\ell})$, $\ell\in [r] $，其中 $\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_n) $ 是一个标准均值为零的均值为 $\mathbf{I}_n $ 的高维Normal分布，$y_{\ell} = \text{sign}(\langle \mathbf{v}_{\ell}, \mathbf{x} \rangle)$ 是一个未知的单位向量 $\mathbf{v}_{\ell} $ 的某种标记。我们的目标是通过总变化距离来学习这个下面的分布。我们的主要结果是一个统计查询（SQ）下界，表明现有的算法是可能最佳的，即使特殊情况下是均匀混合。具体来说，我们证明任何 SQ 算法的复杂度为 $n^{\mathrm{poly}(1/\Delta) \log(r)}$, 其中 $\Delta$ 是 $\mathbf{v}_{\ell}$ 之间的对角线 $\ell_2$  separation 的下界。我们的技术核心是一种新的圆柱体设计，可能具有独立的利用价值。
</details></li>
</ul>
<hr>
<h2 id="Stochastic-Optimization-for-Non-convex-Problem-with-Inexact-Hessian-Matrix-Gradient-and-Function"><a href="#Stochastic-Optimization-for-Non-convex-Problem-with-Inexact-Hessian-Matrix-Gradient-and-Function" class="headerlink" title="Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function"></a>Stochastic Optimization for Non-convex Problem with Inexact Hessian Matrix, Gradient, and Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11866">http://arxiv.org/abs/2310.11866</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Liu, Xuanqing Liu, Cho-Jui Hsieh, Dacheng Tao</li>
<li>for: 这个论文的目的是提出一种基于不确定计算的信任区间（TR）和适应正则化（ARC）方法，以便在非对称优化问题中提高优化效率。</li>
<li>methods: 这种方法使用了不确定计算来计算函数值、梯度和希格曼矩阵，从而选择下一个搜索方向和调整参数。</li>
<li>results: 该论文证明了这种方法可以同时提供不确定计算的函数值、梯度和希格曼矩阵，并且可以在非对称优化问题中实现$\epsilon$-近似二阶优 оптимальность。此外，这种方法的迭代复杂度与前一个研究中的精确计算相同。<details>
<summary>Abstract</summary>
Trust-region (TR) and adaptive regularization using cubics (ARC) have proven to have some very appealing theoretical properties for non-convex optimization by concurrently computing function value, gradient, and Hessian matrix to obtain the next search direction and the adjusted parameters. Although stochastic approximations help largely reduce the computational cost, it is challenging to theoretically guarantee the convergence rate. In this paper, we explore a family of stochastic TR and ARC methods that can simultaneously provide inexact computations of the Hessian matrix, gradient, and function values. Our algorithms require much fewer propagations overhead per iteration than TR and ARC. We prove that the iteration complexity to achieve $\epsilon$-approximate second-order optimality is of the same order as the exact computations demonstrated in previous studies. Additionally, the mild conditions on inexactness can be met by leveraging a random sampling technology in the finite-sum minimization problem. Numerical experiments with a non-convex problem support these findings and demonstrate that, with the same or a similar number of iterations, our algorithms require less computational overhead per iteration than current second-order methods.
</details>
<details>
<summary>摘要</summary>
信任区域（TR）和适应正则化使用立方体（ARC）在非对称优化中有非常吸引人的理论性质。它们同时计算函数值、梯度和偏导数矩阵，以获取下一步搜索方向和调整参数。 although stochastic approximations can significantly reduce computational cost, it is challenging to theoretically guarantee the convergence rate.在这篇论文中，我们探讨了一家Stochastic TR和ARC方法，可同时提供不准确的函数值、梯度和偏导数矩阵计算。我们的算法需要每次迭代 fewer propagations overhead than TR和ARC。我们证明，以 Achieve $\epsilon$-近似第二阶优化的迭代复杂度与前一个研究中的精确计算相同顺序。此外，我们的方法可以通过利用随机抽样技术在finite-sum minimization问题中实现轻度的不准确性条件。numerical experiments with a non-convex problem support these findings and demonstrate that, with the same or a similar number of iterations, our algorithms require less computational overhead per iteration than current second-order methods.
</details></li>
</ul>
<hr>
<h2 id="Effective-and-Efficient-Federated-Tree-Learning-on-Hybrid-Data"><a href="#Effective-and-Efficient-Federated-Tree-Learning-on-Hybrid-Data" class="headerlink" title="Effective and Efficient Federated Tree Learning on Hybrid Data"></a>Effective and Efficient Federated Tree Learning on Hybrid Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11865">http://arxiv.org/abs/2310.11865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinbin Li, Chulin Xie, Xiaojun Xu, Xiaoyuan Liu, Ce Zhang, Bo Li, Bingsheng He, Dawn Song</li>
<li>for: 该论文旨在 Addressing the challenges of federated learning in hybrid data settings, where data from different parties may differ in both features and samples.</li>
<li>methods: 该论文提出了 HybridTree，一种基于分布式学习的新方法，可以在混合数据设置下进行树学习。通过分析了树中具有一致的拆分规则，该方法可以在不需要频繁的通信协议的情况下训练树。</li>
<li>results: 实验表明，HybridTree 可以与中央集成集成环境相比，达到相同的准确率，而且可以减少计算和通信协议的开销，最高可以达到 8 倍的速度提升。<details>
<summary>Abstract</summary>
Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose HybridTree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate that HybridTree can achieve comparable accuracy to the centralized setting with low computational and communication overhead. HybridTree can achieve up to 8 times speedup compared with the other baselines.
</details>
<details>
<summary>摘要</summary>
《联合学习》已经成为一种有前途的分布式学习 paradigma，它使得多个党 collaboration 学习，无需传输原始数据。然而，现有大多数联合学习研究都集中在水平或垂直数据设置中，即不同党的数据假设来自同一个特征或样本空间。在实际应用中，常见的情景是混合数据设置，其中党的数据可能具有不同的特征和样本。为 Addressing 此问题，我们提出 HybridTree，一种新的联合学习方法，可以在混合数据上进行联合树学习。我们发现了共同拆分规则在树中的存在，这些规则帮助我们 theoretically 表明党的知识可以在树的下层级中被包含。基于我们的理论分析，我们提出一种层级解决方案，不需要频繁的通信协议来训练树。我们的实验表明，HybridTree 可以与中央集成设置具有相同的准确率，同时具有较低的计算和通信协议负担。HybridTree 可以与其他基准值进行比较，达到 8 倍的速度提升。
</details></li>
</ul>
<hr>
<h2 id="Accelerate-Presolve-in-Large-Scale-Linear-Programming-via-Reinforcement-Learning"><a href="#Accelerate-Presolve-in-Large-Scale-Linear-Programming-via-Reinforcement-Learning" class="headerlink" title="Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning"></a>Accelerate Presolve in Large-Scale Linear Programming via Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11845">http://arxiv.org/abs/2310.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Kuang, Xijun Li, Jie Wang, Fangzhou Zhu, Meng Lu, Zhihai Wang, Jia Zeng, Houqiang Li, Yongdong Zhang, Feng Wu</li>
<li>for: 这篇论文的目的是提出一种基于机器学习的LP解决方法，以提高现代LP解决器的效率和可靠性。</li>
<li>methods: 该方法使用了动态行为序列学习（RL）框架，将LP解决器的routine设计任务形式化为一个Markov决策过程，并通过适应行动序列来生成高质量的解决方案。</li>
<li>results: 实验结果表明，RL4Presolve可以有效地提高大规模LP的解决效率，特别是在来自业界的benchmark中。此外，通过提取学习政策中的规则，可以将RL4Presolve简单地部署到华为的供应链中。这些结果表明，将机器学习技术应用于现代LP解决器可以实现可负担的经济和学术潜力。<details>
<summary>Abstract</summary>
Large-scale LP problems from industry usually contain much redundancy that severely hurts the efficiency and reliability of solving LPs, making presolve (i.e., the problem simplification module) one of the most critical components in modern LP solvers. However, how to design high-quality presolve routines -- that is, the program determining (P1) which presolvers to select, (P2) in what order to execute, and (P3) when to stop -- remains a highly challenging task due to the extensive requirements on expert knowledge and the large search space. Due to the sequential decision property of the task and the lack of expert demonstrations, we propose a simple and efficient reinforcement learning (RL) framework -- namely, reinforcement learning for presolve (RL4Presolve) -- to tackle (P1)-(P3) simultaneously. Specifically, we formulate the routine design task as a Markov decision process and propose an RL framework with adaptive action sequences to generate high-quality presolve routines efficiently. Note that adaptive action sequences help learn complex behaviors efficiently and adapt to various benchmarks. Experiments on two solvers (open-source and commercial) and eight benchmarks (real-world and synthetic) demonstrate that RL4Presolve significantly and consistently improves the efficiency of solving large-scale LPs, especially on benchmarks from industry. Furthermore, we optimize the hard-coded presolve routines in LP solvers by extracting rules from learned policies for simple and efficient deployment to Huawei's supply chain. The results show encouraging economic and academic potential for incorporating machine learning to modern solvers.
</details>
<details>
<summary>摘要</summary>
大规模LP问题从行业 обычно含有很多重复性，这会严重地降低解决LP的效率和可靠性，因此宏观问题简化模块（i.e., 问题简化模块）成为现代LP解决器中最 kritical 的一部分。然而，如何设计高质量的宏观问题简化程序 --- 即确定（P1）哪些简化器选择，（P2）在哪个顺序执行，以及（P3）何时停止 --- 仍然是一项非常困难的任务，这主要归结于宏观问题简化程序的广泛需求和搜索空间的庞大。由于任务具有顺序决策性和缺乏专家示范，我们提出了一种简单和高效的机器学习（RL）框架 --- 即RL4Presolve --- 以同时解决（P1）-（P3）。具体来说，我们将问题简化任务视为一个Markov决策过程，并提出了一种RL框架，其中包含可适应行为序列来生成高质量的宏观问题简化程序。注意，可适应行为序列可以高效地学习复杂的行为并适应不同的标准。在两种解决器（开源和商业）和八个标准（实际世界和 sintetic）上进行了实验，RL4Presolve显示可以有效地提高大规模LP的解决效率，特别是在行业标准上。此外，我们还使用RL学习到来自学习的策略中的规则，以便简单和高效地在Huawei的供应链中部署。结果表明，通过把机器学习技术应用到现代解决器中，可以获得有优 экономиче和学术潜力。
</details></li>
</ul>
<hr>
<h2 id="On-The-Expressivity-of-Objective-Specification-Formalisms-in-Reinforcement-Learning"><a href="#On-The-Expressivity-of-Objective-Specification-Formalisms-in-Reinforcement-Learning" class="headerlink" title="On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning"></a>On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11840">http://arxiv.org/abs/2310.11840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohan Subramani, Marcus Williams, Max Heitmann, Halfdan Holm, Charlie Griffin, Joar Skalse</li>
<li>for: 这个论文主要针对的是 reinforcement learning（RL）任务中的目标形式化问题。</li>
<li>methods: 这篇论文使用了多种目标规定 formalism，包括Linear Temporal Logic和Multi-Objective Reinforcement Learning，并进行了这些 formalism 之间的比较。</li>
<li>results: 论文发现了不同的目标规定 formalism 之间存在一定的限制，并且没有任何一种 formalism 同时具有优化和表达能力。例如，论文证明了 Regularised RL、Outer Nonlinear Markov Rewards、Reward Machines、Linear Temporal Logic 和 Limit Average Rewards 等 formalism 可以表达其他 formalism 无法表达的目标。这些结论有关于RL中目标规定 formalism的选择和实践中的表达限制。<details>
<summary>Abstract</summary>
To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimise with current techniques. For example, we prove that each of Regularised RL, Outer Nonlinear Markov Rewards, Reward Machines, Linear Temporal Logic, and Limit Average Rewards can express an objective that the others cannot. Our findings have implications for both policy optimisation and reward learning. Firstly, we identify expressivity limitations which are important to consider when specifying objectives in practice. Secondly, our results highlight the need for future research which adapts reward learning to work with a variety of formalisms, since many existing reward learning methods implicitly assume that desired objectives can be expressed with Markovian rewards. Our work contributes towards a more cohesive understanding of the costs and benefits of different RL objective-specification formalisms.
</details>
<details>
<summary>摘要</summary>
要解决一个任务使用强化学习（RL），需要正式 specify 该任务的目标。大多数 RL 算法需要将目标 formalized 为 Markov 奖励函数，但是有其他形式（如线性时间逻辑和多目标强化学习）也有被开发出来。然而，到目前为止，没有任何 thorougly 分析这些形式之间的关系。在这种情况下，我们填充了现有文献中的这种 gap  by 提供了17种目标规定 formalism 在RL中的比较。我们将这些 formalism 按照其表达力排序，并将其显示为一个 Hasse  диаграм。我们发现了不同 formalism 的一些限制，并证明了每种 formalism 都有一些可以表达的任务，而其他 formalism 不能表达。例如，我们证明了 Regularized RL、Outer Nonlinear Markov Rewards、Reward Machines、线性时间逻辑和 Limit Average Rewards 可以表达出其他 formalism 不能表达的任务。我们的发现对于policy优化和奖励学习都有重要的意义。首先，我们identified 表达力的限制，这些限制在实践中需要考虑。其次，我们的结果表明需要将奖励学习适应到不同 formalism 中，因为现有的奖励学习方法通常假设desired objective 可以用 Markov 奖励函数表达。我们的工作对RL objective-specification formalism 的costs and benefits 提供了更加一致的理解。
</details></li>
</ul>
<hr>
<h2 id="Equivariant-Bootstrapping-for-Uncertainty-Quantification-in-Imaging-Inverse-Problems"><a href="#Equivariant-Bootstrapping-for-Uncertainty-Quantification-in-Imaging-Inverse-Problems" class="headerlink" title="Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems"></a>Equivariant Bootstrapping for Uncertainty Quantification in Imaging Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11838">http://arxiv.org/abs/2310.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tachella/equivariant_bootstrap">https://github.com/tachella/equivariant_bootstrap</a></li>
<li>paper_authors: Julian Tachella, Marcelo Pereyra</li>
<li>for:  This paper aims to accurately quantify the uncertainty in solutions to severely ill-posed scientific imaging problems, which is critical for interpreting experimental results and using reconstructed images as scientific evidence.</li>
<li>methods: The proposed uncertainty quantification methodology is based on an equivariant formulation of the parametric bootstrap algorithm, which leverages symmetries and invariance properties commonly encountered in imaging problems. The method is general and can be applied with any image reconstruction technique, including unsupervised training strategies.</li>
<li>results: The proposed method delivers remarkably accurate high-dimensional confidence regions and outperforms alternative uncertainty quantification strategies in terms of estimation accuracy, uncertainty quantification accuracy, and computing time. The method is demonstrated through a series of numerical experiments.<details>
<summary>Abstract</summary>
Scientific imaging problems are often severely ill-posed, and hence have significant intrinsic uncertainty. Accurately quantifying the uncertainty in the solutions to such problems is therefore critical for the rigorous interpretation of experimental results as well as for reliably using the reconstructed images as scientific evidence. Unfortunately, existing imaging methods are unable to quantify the uncertainty in the reconstructed images in a manner that is robust to experiment replications. This paper presents a new uncertainty quantification methodology based on an equivariant formulation of the parametric bootstrap algorithm that leverages symmetries and invariance properties commonly encountered in imaging problems. Additionally, the proposed methodology is general and can be easily applied with any image reconstruction technique, including unsupervised training strategies that can be trained from observed data alone, thus enabling uncertainty quantification in situations where there is no ground truth data available. We demonstrate the proposed approach with a series of numerical experiments and through comparisons with alternative uncertainty quantification strategies from the state-of-the-art, such as Bayesian strategies involving score-based diffusion models and Langevin samplers. In all our experiments, the proposed method delivers remarkably accurate high-dimensional confidence regions and outperforms the competing approaches in terms of estimation accuracy, uncertainty quantification accuracy, and computing time.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimising-Distributions-with-Natural-Gradient-Surrogates"><a href="#Optimising-Distributions-with-Natural-Gradient-Surrogates" class="headerlink" title="Optimising Distributions with Natural Gradient Surrogates"></a>Optimising Distributions with Natural Gradient Surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11837">http://arxiv.org/abs/2310.11837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan So, Richard E. Turner</li>
<li>for: 优化probability distribution的参数</li>
<li>methods: 使用自然偏导法优化参数</li>
<li>results: 扩展了可以使用自然偏导法优化的 distribuition 类型，以及fast、易于理解、简单实现和不需要详细模型Derivation。<details>
<summary>Abstract</summary>
Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and variational inference tasks.
</details>
<details>
<summary>摘要</summary>
自然均方法已经广泛应用于估计概率分布参数，经常导致快速收敛的过程。然而，许多感兴趣的分布中，计算自然均方的问题充满挑战。在这种情况下，我们提出了一种新的技巧，即将估计变换为一种对准ocker分布参数的估计问题，其中计算自然均方是容易的。我们给出了一些现有的方法，可以看作是应用这种技巧，并提出了一种新的方法，可以应用于各种问题。我们的方法可以扩展到更多的分布，并且快速、易于理解、使用标准自动极化软件实现，不需要详细的模型特定的 derivations。我们在最大 LIKELIHOOD估计和variational推断任务中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="CLARA-Multilingual-Contrastive-Learning-for-Audio-Representation-Acquisition"><a href="#CLARA-Multilingual-Contrastive-Learning-for-Audio-Representation-Acquisition" class="headerlink" title="CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition"></a>CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11830">http://arxiv.org/abs/2310.11830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knoriy/CLARA">https://github.com/knoriy/CLARA</a></li>
<li>paper_authors: Kari A Noriy, Xiaosong Yang, Marcin Budka, Jian Jun Zhang</li>
<li>for: 本研究提出了一种多语言语音和声音表示学习框架，用于解决语音处理研究中数据的问题。</li>
<li>methods: 该框架使用了对比学习技术，通过自动生成的卷积数据来增加数据量，并让模型从无标签数据上学习共享表示。</li>
<li>results: 实验结果表明，该模型在识别情绪、音频分类和检索 bencmarks 中表现出色，具有适用于多语言和各种各样的声音条件的共享表示能力，同时还能够编码潜在的情感维度。<details>
<summary>Abstract</summary>
This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.   Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.   Our method trains encoders on a large corpus of multi-lingual audio data. Data augmentation techniques are employed to expand the dataset. The contrastive learning approach trains the model to maximise agreement between positive pairs and minimise agreement between negative pairs. Extensive experiments demonstrate state-of-the-art performance of the proposed model on emotion recognition, audio classification, and retrieval benchmarks under zero-shot and few-shot conditions. This provides an effective approach for acquiring shared and generalised speech representations across languages and acoustic conditions while encoding latent emotional dimensions.
</details>
<details>
<summary>摘要</summary>
We train encoders on a large corpus of multi-lingual audio data, and employ data augmentation techniques to expand the dataset. The contrastive learning approach trains the model to maximize agreement between positive pairs and minimize agreement between negative pairs. Extensive experiments demonstrate state-of-the-art performance of the proposed model on emotion recognition, audio classification, and retrieval benchmarks under zero-shot and few-shot conditions. This provides an effective approach for acquiring shared and generalised speech representations across languages and acoustic conditions while encoding latent emotional dimensions.Here's the Simplified Chinese translation:这篇论文提出了一种新的多语言语音和声音表示学习框架，使用对比学习。由于语言上的大量标注数据缺乏，这阻碍了跨语言语音处理研究的进步。但是，最近的对比学习技术提供了一种无监督的学习方法，可以从无标注数据中学习。我们的方法旨在通过学习多语言数据，以便在不同语言和条件下实现交互转移，并且编码潜在的情感维度。我们将编码器训练在一个大量多语言音频数据集上，并使用数据扩展技术来扩大数据集。对比学习方法将模型训练以最大化正方向对的匹配，并最小化负方向对的匹配。广泛的实验表明，提议的模型在情感识别、音频分类和检索benchmark上实现了顶尖性能，包括零shot和几shot情况下。这提供了一种有效的方法，可以在不同语言和音频条件下获得共享和普适的语音表示，同时编码潜在的情感维度。
</details></li>
</ul>
<hr>
<h2 id="Towards-Graph-Foundation-Models-A-Survey-and-Beyond"><a href="#Towards-Graph-Foundation-Models-A-Survey-and-Beyond" class="headerlink" title="Towards Graph Foundation Models: A Survey and Beyond"></a>Towards Graph Foundation Models: A Survey and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11829">http://arxiv.org/abs/2310.11829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, Chuan Shi</li>
<li>for: 这篇论文旨在探讨基于图像学的基本模型，以及其在不同的人工智能应用中的潜在应用。</li>
<li>methods: 本论文提出了基于图像学的基本模型（GFM）的概念，并对其特点和技术进行了系统的描述。此外，文章还分类了现有的工作，根据它们的依赖于图像神经网络和大语言模型的程度。</li>
<li>results: 文章提供了当前基于图像学的基本模型领域的全面的概述，以及这个领域的未来研究方向。<details>
<summary>Abstract</summary>
Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their reliance on graph neural networks and large language models. Beyond providing a comprehensive overview of the current landscape of graph foundation models, this article also discusses potential research directions for this evolving field.
</details>
<details>
<summary>摘要</summary>
emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. 同时, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. the emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. however, there is currently no clear definition and systematic analysis for this type of work. in this article, we propose the concept of graph foundation models (gfms), and provide the first comprehensive elucidation on their key characteristics and technologies. following that, we categorize existing works towards gfms into three categories based on their reliance on graph neural networks and large language models. beyond providing a comprehensive overview of the current landscape of graph foundation models, this article also discusses potential research directions for this evolving field.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="A-Historical-Context-for-Data-Streams"><a href="#A-Historical-Context-for-Data-Streams" class="headerlink" title="A Historical Context for Data Streams"></a>A Historical Context for Data Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.19811">http://arxiv.org/abs/2310.19811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indre Zliobaite, Jesse Read</li>
<li>for: 这篇论文主要针对的是从数据流中学习的机器学习问题，这是一个活跃和快速发展的研究领域。</li>
<li>methods: 这篇论文使用了一些传统的机器学习算法，但是它们受到了数据流的计算资源限制，例如每个实例只能被检查一次，并且需要在任何时间提供预测结果。</li>
<li>results: 这篇论文提出了一些历史上对数据流机器学习的假设，并将这些假设放在历史上的学术背景中进行了回顾。<details>
<summary>Abstract</summary>
Machine learning from data streams is an active and growing research area. Research on learning from streaming data typically makes strict assumptions linked to computational resource constraints, including requirements for stream mining algorithms to inspect each instance not more than once and be ready to give a prediction at any time. Here we review the historical context of data streams research placing the common assumptions used in machine learning over data streams in their historical context.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将数据流中的机器学习作为活跃和快速发展的研究领域。研究从数据流中学习通常做出严格的计算资源限制，包括流程挖掘算法不能再次检查每个实例，并且要准备任何时间给出预测。我们在这里将数据流研究的历史背景和常见的机器学习假设置在历史上的位置。Translation:机器学习从数据流中是一个活跃和快速发展的研究领域。研究从数据流中学习通常做出严格的计算资源限制，包括流程挖掘算法不能再次检查每个实例，并且要准备任何时间给出预测。我们在这里将数据流研究的历史背景和常见的机器学习假设置在历史上的位置。
</details></li>
</ul>
<hr>
<h2 id="De-novo-protein-design-using-geometric-vector-field-networks"><a href="#De-novo-protein-design-using-geometric-vector-field-networks" class="headerlink" title="De novo protein design using geometric vector field networks"></a>De novo protein design using geometric vector field networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11802">http://arxiv.org/abs/2310.11802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weian Mao, Muzhi Zhu, Zheng Sun, Shuaike Shen, Lin Yuanbo Wu, Hao Chen, Chunhua Shen</li>
<li>for: 这篇论文主要关注的是对蛋白质构造设计的进步，特别是透过蛋白质diffusion的创新，使得蛋白质设计得以进行更加精确和有效的模型化。</li>
<li>methods: 本论文提出了一种新的数据 Computation Network（VFN），可以在蛋白质diffusion中进行更加精确的框架模型化，并且可以同时模型框架和原子。VFN使用了学习可控的 вектор计算，将蛋白质框架中的各个位置转换为可读的 вектор值，然后使用弹性总和将这些 вектор值与蛋白质框架中的各个原子进行相互关联。</li>
<li>results: 本论文的实验结果显示，VFN在蛋白质diffusion中表现出色，比起先前的IPA模型，VFN在设计性（67.04% vs. 53.58%)和多样性（66.54% vs. 51.98%)等方面均有较好的表现。此外，VFN也在倒拾蛋白质（frame和原子模型）中表现出色，比起先前的PiFold模型（54.7% vs. 51.66%），VFN在序列恢复率上有较好的表现。此外，本论文还提出了一种将VFN与ESM模型结合的方法，这种方法在先前的ESM-based SoTA（62.67% vs. 55.65%）上有着较好的表现。<details>
<summary>Abstract</summary>
Innovations like protein diffusion have enabled significant progress in de novo protein design, which is a vital topic in life science. These methods typically depend on protein structure encoders to model residue backbone frames, where atoms do not exist. Most prior encoders rely on atom-wise features, such as angles and distances between atoms, which are not available in this context. Thus far, only several simple encoders, such as IPA, have been proposed for this scenario, exposing the frame modeling as a bottleneck. In this work, we proffer the Vector Field Network (VFN), which enables network layers to perform learnable vector computations between coordinates of frame-anchored virtual atoms, thus achieving a higher capability for modeling frames. The vector computation operates in a manner similar to a linear layer, with each input channel receiving 3D virtual atom coordinates instead of scalar values. The multiple feature vectors output by the vector computation are then used to update the residue representations and virtual atom coordinates via attention aggregation. Remarkably, VFN also excels in modeling both frames and atoms, as the real atoms can be treated as the virtual atoms for modeling, positioning VFN as a potential universal encoder. In protein diffusion (frame modeling), VFN exhibits an impressive performance advantage over IPA, excelling in terms of both designability (67.04% vs. 53.58%) and diversity (66.54% vs. 51.98%). In inverse folding (frame and atom modeling), VFN outperforms the previous SoTA model, PiFold (54.7% vs. 51.66%), on sequence recovery rate. We also propose a method of equipping VFN with the ESM model, which significantly surpasses the previous ESM-based SoTA (62.67% vs. 55.65%), LM-Design, by a substantial margin.
</details>
<details>
<summary>摘要</summary>
新技术如蛋白diffusion已经使得蛋白结构设计得到了重要的进步，这是生命科学中非常重要的话题。这些方法通常依赖于蛋白结构编码器来模拟蛋白质量框架，其中原子不存在。以前的编码器大多数依赖于原子粒子特征，如原子之间的角度和距离，这些特征在这种情况下不可用。只有一些简单的编码器，如IPA，已经被提出，这暴露了框架模型化为瓶颈。在这种工作中，我们提议使用 Vector Field Network（VFN），它使得网络层可以通过学习vector计算来处理坐标相关的操作。VFN在蛋白diffusion（框架模型）中表现出了非常出色的性能优势，比IPA更高，达到67.04% vs. 53.58%的设计性能和66.54% vs. 51.98%的多样性。在 inverse folding（框架和原子模型）中，VFN也超越了之前的SoTA模型，PiFold（54.7% vs. 51.66%），在序列恢复率方面表现出色。我们还提出了使用VFN和ESM模型的方法，该方法在之前的ESM-based SoTA（62.67% vs. 55.65%）之上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-for-Physics-Informed-Neural-Networks"><a href="#Adversarial-Training-for-Physics-Informed-Neural-Networks" class="headerlink" title="Adversarial Training for Physics-Informed Neural Networks"></a>Adversarial Training for Physics-Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11789">http://arxiv.org/abs/2310.11789</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaoli90/at-pinn">https://github.com/yaoli90/at-pinn</a></li>
<li>paper_authors: Yao Li, Shengzhu Shi, Zhichang Guo, Boying Wu</li>
<li>For: 解决复杂的偏微分方程（PDEs）中的缺乏稳定性问题，提高Physics-informed neural networks（PINNs）的测试精度和可靠性。* Methods: 基于投影gradient descent对抗攻击（PGD-based adversarial attack），提出了一种名为AT-PINNs的对抗训练策略，可以增强PINNs的Robustness和稳定性。AT-PINNs可以通过在训练过程中使用对抗样本来准确地识别模型失败位置，并在训练过程中使模型更加注重这些位置。* Results: 应用AT-PINNs于各种复杂的PDEs，包括多尺度约束的圆柱方程、多峰解的波兰射方程、普朗克方程的锐度解和Allen-Cahn方程。结果表明，AT-PINNs可以有效地定位和减少失败区域，并且适用于解决复杂的PDEs，因为对于失败区域的定位无关于失败区域的大小或分布的复杂性。<details>
<summary>Abstract</summary>
Physics-informed neural networks have shown great promise in solving partial differential equations. However, due to insufficient robustness, vanilla PINNs often face challenges when solving complex PDEs, especially those involving multi-scale behaviors or solutions with sharp or oscillatory characteristics. To address these issues, based on the projected gradient descent adversarial attack, we proposed an adversarial training strategy for PINNs termed by AT-PINNs. AT-PINNs enhance the robustness of PINNs by fine-tuning the model with adversarial samples, which can accurately identify model failure locations and drive the model to focus on those regions during training. AT-PINNs can also perform inference with temporal causality by selecting the initial collocation points around temporal initial values. We implement AT-PINNs to the elliptic equation with multi-scale coefficients, Poisson equation with multi-peak solutions, Burgers equation with sharp solutions and the Allen-Cahn equation. The results demonstrate that AT-PINNs can effectively locate and reduce failure regions. Moreover, AT-PINNs are suitable for solving complex PDEs, since locating failure regions through adversarial attacks is independent of the size of failure regions or the complexity of the distribution.
</details>
<details>
<summary>摘要</summary>
物理学 Informed Neural Networks (PINNs) 已经展示了解决partial differential equations (PDEs) 的巨大承诺. 然而，由于不充分的Robustness，vanilla PINNs 经常在解决复杂的PDEs中遇到挑战，特别是包含多尺度行为或解决具有锐利或振荡特征的PDEs. 为了解决这些问题，我们基于Projected gradient descent adversarial attack (PGD-AA)提出了一种名为AT-PINNs的对抗训练策略。AT-PINNs可以增强PINNs的Robustness，通过在训练过程中使用对抗样本，准确地识别模型失败的位置并使模型在训练过程中专注于这些位置。AT-PINNs还可以通过选择时间初值附近的初始坐标来进行时间 causality 的推理。我们将AT-PINNs应用到了各种PDEs，包括具有多尺度系数的圆柱 equation、Poisson equation with multi-peak solutions、Burgers equation with sharp solutions和Allen-Cahn equation。结果表明，AT-PINNs可以有效地定位和减少失败区域。此外，AT-PINNs适用于解决复杂的PDEs，因为通过对抗攻击定位失败区域是独立于失败区域的大小或分布复杂性的。
</details></li>
</ul>
<hr>
<h2 id="NeuroCUT-A-Neural-Approach-for-Robust-Graph-Partitioning"><a href="#NeuroCUT-A-Neural-Approach-for-Robust-Graph-Partitioning" class="headerlink" title="NeuroCUT: A Neural Approach for Robust Graph Partitioning"></a>NeuroCUT: A Neural Approach for Robust Graph Partitioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11787">http://arxiv.org/abs/2310.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya, Sayan Ranu</li>
<li>for: 分区图形式的问题，即将图分成k个独立的部分，以优化特定的分区目标。</li>
<li>methods:  neural approach，包括一个新的框架NeuroCut，它在查询时可以对图的结构和分区数进行泛化，并通过基于节点表示学习的强化学习框架来满足任何优化目标，包括不可导函数。</li>
<li>results: NeuroCut在实验中表现出色，能够找到高质量的分区，具有强大的泛化性和对图结构 modificatiopn的抗颤势性。<details>
<summary>Abstract</summary>
Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning based framework over node representations derived from a graph neural network, NeuroCut can accommodate any optimization objective, even those encompassing non-differentiable functions. Through empirical evaluation, we demonstrate that NeuroCut excels in identifying high-quality partitions, showcases strong generalization across a wide spectrum of partitioning objectives, and exhibits resilience to topological modifications.
</details>
<details>
<summary>摘要</summary>
graf分割的目标是将 Graf 分成 k 个不交叉的子集，同时最大化特定的分割目标。大多数相关的形式化问题都会显示NP困难，因为它们具有各种 combinatorial 特性。因此，现有的approximation算法通常使用了heuristic方法，有时具有approximation保证，有时没有。 unfortunately，传统的方法通常是为特定的分割目标设计的，不能总是泛化到其他从文献中知道的分割目标。为了解决这个限制，并从数据中直接学习heuristics，神经方法出现了。在这项研究中，我们通过一个新的框架，NeuroCut，进一步推动这一线的发展。NeuroCut 具有两个关键创新：首先，它是对 Graf 结构和分割 count  inductive的，可以在查询时提供。其次，通过利用基于节点表示学习的reinforcement learning框架，NeuroCut 可以处理任何优化目标，包括不可导函数。通过实验评估，我们示出NeuroCut 可以提供高质量的分割，具有强大的泛化能力，并且对 Graf 结构的修改 display 强大的抗衡性。
</details></li>
</ul>
<hr>
<h2 id="A-Quasi-Wasserstein-Loss-for-Learning-Graph-Neural-Networks"><a href="#A-Quasi-Wasserstein-Loss-for-Learning-Graph-Neural-Networks" class="headerlink" title="A Quasi-Wasserstein Loss for Learning Graph Neural Networks"></a>A Quasi-Wasserstein Loss for Learning Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11762">http://arxiv.org/abs/2310.11762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minjie Cheng, Hongteng Xu</li>
<li>for: 提高 Graph Neural Network (GNN) 在节点级预测任务中的性能，因为现有的损失函数通常对每个节点独立进行应用，即使节点嵌入和标签不是独立的。</li>
<li>methods: 提出了一种新的 quasi-Wasserstein (QW) 损失函数，基于图上的最优运输定义，用于修改 GNN 的学习和预测方法。该损失函数定义了图边上的 quasi-Wasserstein 距离，用于优化标签的运输定义。</li>
<li>results: 实验表明，提出的 QW 损失函数可以应用于多种 GNN 模型，并且能够提高其性能在节点级预测和回归任务中。此外，该损失函数还可以提供一种新的拟合学习和预测方法。<details>
<summary>Abstract</summary>
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transport. When predicting node labels, our model combines the output of the GNN with the residual component provided by the optimal label transport, leading to a new transductive prediction paradigm. Experiments show that the proposed QW loss applies to various GNNs and helps to improve their performance in node-level classification and regression tasks.
</details>
<details>
<summary>摘要</summary>
当学习图 neural network (GNN) 在节点级预测任务时，大多数现有的损失函数都是对每个节点独立应用的，即使节点表示和其标签不是独立的，因为它们的图结构。为了消除这种不一致，在本研究中我们提出了一种新的 quasi-Wasserstein (QW) 损失函数，基于图上的最优运输定义。在特定情况下，我们定义了 observe 多维节点标签和其估计之间的 "quasi-Wasserstein" 距离，并且优化了图边上的标签运输定义。这些估计是通过一个 GNN 来 parameterize，其中优化的标签运输可能会确定图边权重。通过将 строго的标签运输约束转换为 Bregman 分布定义based REG regularizer，我们获得了我们的提议的 QW 损失函数，并且可以使用两种高效的算法来学习 GNN 和标签运输。在预测节点标签时，我们将 GNN 的输出与标签运输的 residual 组件相加，这导致了一种新的混合预测 paradigm。实验表明，我们的提议 QW 损失函数可以应用于多种 GNN 和提高它们在节点级预测和回归任务中的性能。
</details></li>
</ul>
<hr>
<h2 id="Unintended-Memorization-in-Large-ASR-Models-and-How-to-Mitigate-It"><a href="#Unintended-Memorization-in-Large-ASR-Models-and-How-to-Mitigate-It" class="headerlink" title="Unintended Memorization in Large ASR Models, and How to Mitigate It"></a>Unintended Memorization in Large ASR Models, and How to Mitigate It</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11739">http://arxiv.org/abs/2310.11739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lun Wang, Om Thakkar, Rajiv Mathews</li>
<li>for: 检测大型自动声音识别（ASR）模型中的 memorization 问题，以保护隐私。</li>
<li>methods: 提出了一种简单的检测方法，通过快速生成的句子速度来创建声音和文本信息之间的易于学习Mapping。</li>
<li>results: 在state-of-the-art ASR模型中发现了memorization问题，并通过gradient clipping来 Mitigate memorization。在大规模分布式训练中，clip each example’s gradient可以保持中性模型质量和计算成本，同时提供强的隐私保护。<details>
<summary>Abstract</summary>
It is well-known that neural networks can unintentionally memorize their training examples, causing privacy concerns. However, auditing memorization in large non-auto-regressive automatic speech recognition (ASR) models has been challenging due to the high compute cost of existing methods such as hardness calibration. In this work, we design a simple auditing method to measure memorization in large ASR models without the extra compute overhead. Concretely, we speed up randomly-generated utterances to create a mapping between vocal and text information that is difficult to learn from typical training examples. Hence, accurate predictions only for sped-up training examples can serve as clear evidence for memorization, and the corresponding accuracy can be used to measure memorization. Using the proposed method, we showcase memorization in the state-of-the-art ASR models. To mitigate memorization, we tried gradient clipping during training to bound the influence of any individual example on the final model. We empirically show that clipping each example's gradient can mitigate memorization for sped-up training examples with up to 16 repetitions in the training set. Furthermore, we show that in large-scale distributed training, clipping the average gradient on each compute core maintains neutral model quality and compute cost while providing strong privacy protection.
</details>
<details>
<summary>摘要</summary>
很多人知道神经网络可能会无意地记忆训练示例，这引起了隐私问题。然而，对大型非自动回归自动语音识别（ASR）模型的审核记忆存在高计算成本的问题，使得现有方法如困难度调整不太实用。在这种情况下，我们提出了一种简单的审核方法，可以不增加计算成本来测量大型ASR模型的记忆。具体来说，我们将随机生成的语音快速播放，以创建语音和文本信息之间的易于学习的映射。因此，只有对快速播放的训练示例进行准确预测时，可以作为记忆的证据，并且可以用这个精度来测量记忆。使用我们的方法，我们显示了state-of-the-art ASR模型中的记忆。为了解决记忆问题，我们尝试使用梯度截断法在训练时进行 bounding 梯度的影响。我们经验显示，对每个示例的梯度进行截断可以 Mitigate 记忆，并且可以在快速播放示例中进行16次复制。此外，我们还显示了在大规模分布式训练中，对每个计算核心的梯度平均截断可以保持中立的模型质量和计算成本，同时提供强的隐私保护。
</details></li>
</ul>
<hr>
<h2 id="On-the-Evaluation-of-Generative-Models-in-Distributed-Learning-Tasks"><a href="#On-the-Evaluation-of-Generative-Models-in-Distributed-Learning-Tasks" class="headerlink" title="On the Evaluation of Generative Models in Distributed Learning Tasks"></a>On the Evaluation of Generative Models in Distributed Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11714">http://arxiv.org/abs/2310.11714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiao Wang, Farzan Farnia, Zhenghao Lin, Yunheng Shen, Bei Yu</li>
<li>for: 这篇论文主要关注在分布式学习中评估深度生成模型，包括生成对抗网络（GANs）和扩散模型。</li>
<li>methods: 这篇论文使用了Fréchet对劲距离（FID）和核心对劲距离（KID）等评估生成模型的方法。</li>
<li>results: 论文发现在分布式学习 задача中，使用FID和KID评估生成模型的结果可能会不同，具体来说是FID-avg和FID-all的评估结果可能会不同，而KID-avg和KID-all的评估结果则相同。<details>
<summary>Abstract</summary>
The evaluation of deep generative models including generative adversarial networks (GANs) and diffusion models has been extensively studied in the literature. While the existing evaluation methods mainly target a centralized learning problem with training data stored by a single client, many applications of generative models concern distributed learning settings, e.g. the federated learning scenario, where training data are collected by and distributed among several clients. In this paper, we study the evaluation of generative models in distributed learning tasks with heterogeneous data distributions. First, we focus on the Fr\'echet inception distance (FID) and consider the following FID-based aggregate scores over the clients: 1) FID-avg as the mean of clients' individual FID scores, 2) FID-all as the FID distance of the trained model to the collective dataset containing all clients' data. We prove that the model rankings according to the FID-all and FID-avg scores could be inconsistent, which can lead to different optimal generative models according to the two aggregate scores. Next, we consider the kernel inception distance (KID) and similarly define the KID-avg and KID-all aggregations. Unlike the FID case, we prove that KID-all and KID-avg result in the same rankings of generative models. We perform several numerical experiments on standard image datasets and training schemes to support our theoretical findings on the evaluation of generative models in distributed learning problems.
</details>
<details>
<summary>摘要</summary>
文章研究了深度生成模型（包括生成对抗网络）在分布式学习任务中的评价方法。现有评价方法主要针对中央式学习问题，即训练数据由单个客户端存储。然而，许多生成模型应用场景是分布式学习场景，例如联邦学习场景，其中训练数据由多个客户端分布存储。本文研究了分布式学习任务中各客户端数据分布不同的生成模型评价方法。首先，我们关注Fréchet吸引距离（FID），并考虑以下FID基于客户端的综合分数：1）FID-avg，即客户端个体FID分数的平均值，2）FID-all，即训练模型与所有客户端数据集的FID距离。我们证明了FID-all和FID-avg的模型排名可能不一致，可能导致不同的优化生成模型。接下来，我们考虑核心吸引距离（KID），并定义KID-avg和KID-all综合分数。与FID不同的是，我们证明了KID-all和KID-avg的模型排名是一致的。我们在标准图像集和训练方案上进行了多个数值实验来支持我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Learning-under-Label-Proportions-for-Text-Classification"><a href="#Learning-under-Label-Proportions-for-Text-Classification" class="headerlink" title="Learning under Label Proportions for Text Classification"></a>Learning under Label Proportions for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11707">http://arxiv.org/abs/2310.11707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jatin Chauhan, Xiaoxuan Wang, Wei Wang</li>
<li>for: 本研究旨在探讨在�xygen Learning from Label Proportions（LLP）的挑战性设置下进行NLPR Training，其数据提供在汇总形式下，仅提供每个类别的样本比例作为ground truth。</li>
<li>methods: 本研究提出了一种新的形式ulation，以及一种learnability result，以提供一个generalization bound under LLP。此外，该研究还使用了一种自我supervised objective。</li>
<li>results: 根据实验结果，该方法在大规模模型和多个维度上的文本数据上 achieved better results compared to基elines in almost 87% of the experimental configurations, across multiple metrics。<details>
<summary>Abstract</summary>
We present one of the preliminary NLP works under the challenging setup of Learning from Label Proportions (LLP), where the data is provided in an aggregate form called bags and only the proportion of samples in each class as the ground truth. This setup is inline with the desired characteristics of training models under Privacy settings and Weakly supervision. By characterizing some irregularities of the most widely used baseline technique DLLP, we propose a novel formulation that is also robust. This is accompanied with a learnability result that provides a generalization bound under LLP. Combining this formulation with a self-supervised objective, our method achieves better results as compared to the baselines in almost 87% of the experimental configurations which include large scale models for both long and short range texts across multiple metrics.
</details>
<details>
<summary>摘要</summary>
我们介绍了一项初步的自然语言处理（NLP）工作，在“学习从标签含量（LLP）”的挑战性设置下进行训练，其中数据提供在归一化的形式下，即袋（bag），并且只有每个类别的样本占总数的比例作为真实的地面信息。这种设置符合训练模型下的隐私设置和弱监督。我们对最常用的基线技术DLLP的不规则性进行描述，并提出了一种新的形式ulation，这种形式ulation具有 robustness。此外，我们还提供了一个learnability result，它在LLP下提供了一个通用的泛化 bound。将这种形式ulation与一种自我超vised目标函数相结合，我们的方法在大规模的实验配置中（包括长文本和短文本） across multiple metrics  Achieves better results than baselines in nearly 87% of the cases.
</details></li>
</ul>
<hr>
<h2 id="AUC-mixup-Deep-AUC-Maximization-with-Mixup"><a href="#AUC-mixup-Deep-AUC-Maximization-with-Mixup" class="headerlink" title="AUC-mixup: Deep AUC Maximization with Mixup"></a>AUC-mixup: Deep AUC Maximization with Mixup</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11693">http://arxiv.org/abs/2310.11693</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzhi Xv, Gang Li, Tianbao Yang</li>
<li>for: 提高异常点识别模型的泛化能力，解决深度AUC最大化（DAM）在小据集上存在严重过拟合问题。</li>
<li>methods: 使用混合数据增强（mixup）数据增强技术，并采用AUC环境损失来有效地从混合数据生成的数据中学习，称为AUC-mixup损失。</li>
<li>results: 在异常点识别和医学影像数据集上，与标准DAM训练方法相比，提出的AUC-mixup方法显示出更高的泛化性能。<details>
<summary>Abstract</summary>
While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, e.g., chest X-rays classification and skin lesions classification, it could suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve generalization of DAM by mixup data augmentation -- an approach that is widely used for improving generalization of the cross-entropy loss based deep learning methods. %For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn from data generated by mixup augmentation, which is referred to as the AUC-mixup loss. Our experimental results demonstrate the effectiveness of the proposed AUC-mixup methods on imbalanced benchmark and medical image datasets compared to standard DAM training methods.
</details>
<details>
<summary>摘要</summary>
While deep AUC maximization (DAM) has shown remarkable success on imbalanced medical tasks, such as chest X-rays classification and skin lesions classification, it can suffer from severe overfitting when applied to small datasets due to its aggressive nature of pushing prediction scores of positive data away from that of negative data. This paper studies how to improve the generalization of DAM by using mixup data augmentation -- an approach that is widely used for improving the generalization of cross-entropy loss-based deep learning methods. For overfitting issues arising from limited data, the common approach is to employ mixup data augmentation to boost the models' generalization performance by enriching the training data. However, AUC is defined over positive and negative pairs, which makes it challenging to incorporate mixup data augmentation into DAM algorithms. To tackle this challenge, we employ the AUC margin loss and incorporate soft labels into the formulation to effectively learn from data generated by mixup augmentation, which is referred to as the AUC-mixup loss. Our experimental results demonstrate the effectiveness of the proposed AUC-mixup methods on imbalanced benchmark and medical image datasets compared to standard DAM training methods.
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-on-Transformer-architecture-for-power-system-short-term-voltage-stability-assessment-with-class-imbalance"><a href="#Deep-learning-based-on-Transformer-architecture-for-power-system-short-term-voltage-stability-assessment-with-class-imbalance" class="headerlink" title="Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance"></a>Deep learning based on Transformer architecture for power system short-term voltage stability assessment with class imbalance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11690">http://arxiv.org/abs/2310.11690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Li, Jiting Cao, Yan Xu, Lipeng Zhu, Zhao Yang Dong</li>
<li>for: 本研究提出了一种解决短时电压稳定评估中数据不均衡问题的方法，以提高实时电压稳定评估的精度和可靠性。</li>
<li>methods: 本研究使用了Transformer架构，开发了一种名为StaaT的稳定评估Transformer，并采用了conditional Wasserstein生成敌对网络（CWGAN-GP）来生成Synthetic数据，以帮助创建一个均衡的、代表性的训练集。此外，本研究还采用了半upervised clustering学习来提高划分质量，因为短时电压稳定无一定的量化标准。</li>
<li>results: 数据测试表明，提出的方法在面临100:1的数据不均衡和噪音环境时仍然保持了稳定的性能，并且在增加可再生能源的情况下也保持了一致的效果。比较结果表明，CWGAN-GP生成的数据更具备均衡性，而StaaT也超过了其他深度学习算法。这种方法可以应用于实际短时电压稳定评估中，frequently face着数据不均衡和噪音挑战。<details>
<summary>Abstract</summary>
Most existing data-driven power system short-term voltage stability assessment (STVSA) approaches presume class-balanced input data. However, in practical applications, the occurrence of short-term voltage instability following a disturbance is minimal, leading to a significant class imbalance problem and a consequent decline in classifier performance. This work proposes a Transformer-based STVSA method to address this challenge. By utilizing the basic Transformer architecture, a stability assessment Transformer (StaaT) is developed {as a classification model to reflect the correlation between the operational states of the system and the resulting stability outcomes}. To combat the negative impact of imbalanced datasets, this work employs a conditional Wasserstein generative adversarial network with gradient penalty (CWGAN-GP) for synthetic data generation, aiding in the creation of a balanced, representative training set for the classifier. Semi-supervised clustering learning is implemented to enhance clustering quality, addressing the lack of a unified quantitative criterion for short-term voltage stability. {Numerical tests on the IEEE 39-bus test system extensively demonstrate that the proposed method exhibits robust performance under class imbalances up to 100:1 and noisy environments, and maintains consistent effectiveness even with an increased penetration of renewable energy}. Comparative results reveal that the CWGAN-GP generates more balanced datasets than traditional oversampling methods and that the StaaT outperforms other deep learning algorithms. This study presents a compelling solution for real-world STVSA applications that often face class imbalance and data noise challenges.
</details>
<details>
<summary>摘要</summary>
现有的数据驱动电力系统短期电压稳定评估（STVSA）方法大多假设输入数据具有均衡的分布。然而，在实际应用中，短期电压不稳定的发生率很低，导致数据分布受到很大的偏好问题，从而导致分类器性能下降。这项工作提出了一种基于Transformer的STVSA方法来解决这个挑战。通过利用基本Transformer架构，我们开发了一种稳定评估Transformer（StaaT），用于反映系统运行状态和导致的稳定结果之间的相关性。为了解决偏好数据的负面影响，这项工作采用了 conditional Wasserstein生成敌方网络（CWGAN-GP） для生成人工数据，以帮助创建一个均衡、代表性的训练集 для分类器。 semi-supervised clustering learning 技术被应用以提高归一化质量，因为没有短期电压稳定的准确量标准。 {numeraire tests on the IEEE 39-bus test system extensively demonstrate that the proposed method exhibits robust performance under class imbalances up to 100:1 and noisy environments, and maintains consistent effectiveness even with an increased penetration of renewable energy}. comparative results reveal that the CWGAN-GP generates more balanced datasets than traditional oversampling methods and that the StaaT outperforms other deep learning algorithms. this study presents a compelling solution for real-world STVSA applications that often face class imbalance and data noise challenges.
</details></li>
</ul>
<hr>
<h2 id="Subject-specific-Deep-Neural-Networks-for-Count-Data-with-High-cardinality-Categorical-Features"><a href="#Subject-specific-Deep-Neural-Networks-for-Count-Data-with-High-cardinality-Categorical-Features" class="headerlink" title="Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features"></a>Subject-specific Deep Neural Networks for Count Data with High-cardinality Categorical Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11654">http://arxiv.org/abs/2310.11654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangbin Lee, Il Do Ha, Changha Hwang, Youngjo Lee</li>
<li>for: 提高预测性能和学习效率，适用于高纬度ategorical特征的分布数据处理。</li>
<li>methods: 基于 hierarchical likelihood 学习框架，引入gamma random effects，同时利用最大 likelihood 估计和best unbiased predictors来捕捉输入变量的非线性效应和subject-specific层效应。</li>
<li>results: 通过实验和实际数据分析，证明了提议方法的优势，包括提高预测性能和学习效率，以及适用于高纬度ategorical特征的分布数据处理。<details>
<summary>Abstract</summary>
There is a growing interest in subject-specific predictions using deep neural networks (DNNs) because real-world data often exhibit correlations, which has been typically overlooked in traditional DNN frameworks. In this paper, we propose a novel hierarchical likelihood learning framework for introducing gamma random effects into the Poisson DNN, so as to improve the prediction performance by capturing both nonlinear effects of input variables and subject-specific cluster effects. The proposed method simultaneously yields maximum likelihood estimators for fixed parameters and best unbiased predictors for random effects by optimizing a single objective function. This approach enables a fast end-to-end algorithm for handling clustered count data, which often involve high-cardinality categorical features. Furthermore, state-of-the-art network architectures can be easily implemented into the proposed h-likelihood framework. As an example, we introduce multi-head attention layer and a sparsemax function, which allows feature selection in high-dimensional settings. To enhance practical performance and learning efficiency, we present an adjustment procedure for prediction of random parameters and a method-of-moments estimator for pretraining of variance component. Various experiential studies and real data analyses confirm the advantages of our proposed methods.
</details>
<details>
<summary>摘要</summary>
有越来越多的研究者对特定领域预测使用深度神经网络（DNN），因为实际数据经常具有相关性，传统的DNN框架中通常会忽略这些相关性。在这篇论文中，我们提出了一种新的层次可能性学习框架，以在Poisson DNN中引入γ随机效应，以提高预测性能，同时捕捉输入变量的非线性效应和特定颗集效应。我们的方法同时实现最大可能性估计器和不偏预测器，通过优化单个目标函数。这种方法使得可以快速处理受集分布的端到端算法，这些分布frequently包含高cardinality的分类特征。此外，我们可以轻松地将当前的网络架构 integrate into our proposed h-likelihood framework。例如，我们引入多头注意层和简洁最大化函数，这些功能允许在高维度设置中进行特征选择。为了提高实际性和学习效率，我们提出了预测随机参数的调整方法和预测变量组件的方法-of-moments估计器。多种实验和实际数据分析证明了我们的提出的方法的优势。
</details></li>
</ul>
<hr>
<h2 id="Free-text-Keystroke-Authentication-using-Transformers-A-Comparative-Study-of-Architectures-and-Loss-Functions"><a href="#Free-text-Keystroke-Authentication-using-Transformers-A-Comparative-Study-of-Architectures-and-Loss-Functions" class="headerlink" title="Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions"></a>Free-text Keystroke Authentication using Transformers: A Comparative Study of Architectures and Loss Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.11640">http://arxiv.org/abs/2310.11640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleh Momeni, Bagher BabaAli</li>
<li>for: 这个研究旨在提出一个基于Transformer的网络，以提高键盘识别和验证的精度。</li>
<li>methods: 这个模型使用自我注意力来提取键盘序列中的有用特征，并评估了两种不同的架构， namely bi-encoder 和 cross-encoder，以及不同的损失函数和距离度量。</li>
<li>results: 这个研究发现，使用 bi-encoder 架构、批量全 triplet 损失函数和圆形距离度量可以实现最佳性能，具体Equla Error Rate 为0.0186%。此外，还探讨了不同的相似度评估方法，以提高模型的精度。<details>
<summary>Abstract</summary>
Keystroke biometrics is a promising approach for user identification and verification, leveraging the unique patterns in individuals' typing behavior. In this paper, we propose a Transformer-based network that employs self-attention to extract informative features from keystroke sequences, surpassing the performance of traditional Recurrent Neural Networks. We explore two distinct architectures, namely bi-encoder and cross-encoder, and compare their effectiveness in keystroke authentication. Furthermore, we investigate different loss functions, including triplet, batch-all triplet, and WDCL loss, along with various distance metrics such as Euclidean, Manhattan, and cosine distances. These experiments allow us to optimize the training process and enhance the performance of our model. To evaluate our proposed model, we employ the Aalto desktop keystroke dataset. The results demonstrate that the bi-encoder architecture with batch-all triplet loss and cosine distance achieves the best performance, yielding an exceptional Equal Error Rate of 0.0186%. Furthermore, alternative algorithms for calculating similarity scores are explored to enhance accuracy. Notably, the utilization of a one-class Support Vector Machine reduces the Equal Error Rate to an impressive 0.0163%. The outcomes of this study indicate that our model surpasses the previous state-of-the-art in free-text keystroke authentication. These findings contribute to advancing the field of keystroke authentication and offer practical implications for secure user verification systems.
</details>
<details>
<summary>摘要</summary>
“键盘生物метри学是一种有前途的方法 для用户识别和验证，利用个人键盘实习独特的模式。在本研究中，我们提出了基于Transformer的网络，使用自我对项来提取键盘序列中有用的特征，超越传统的Recurrent Neural Networks的表现。我们探索了两种不同的架构，分别是双向encoder和cross-encoder，并比较它们在键盘验证中的效果。此外，我们寻找了不同的损失函数，包括三重、批量三重和WDCL损失函数，以及不同的距离度量，如Euclidean、曼哈顿和内角距离。这些实验允许我们优化训练过程，提高模型的性能。为了评估我们的提案模型，我们使用了阿尔托桌面键盘数据集。结果显示，双向encoder架构加 batch-all triplet损失函数和内角距离可以取得最佳性能，具体Equla Error Rate为0.0186%。此外，我们还探索了不同的相似度计算算法，以提高准确性。例如，使用一个一阶支持向量机可以降低Equla Error Rate至0.0163%。研究结果显示，我们的模型超越了过去的州际前进于自由文本键盘验证。这些发现对于键盘验证领域的进步做出了贡献，并且提供了实际的应用于安全用户验证系统。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/18/cs.LG_2023_10_18/" data-id="clpxp6c4q00toee88aya42kkg" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/24/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="page-number" href="/page/24/">24</a><span class="page-number current">25</span><a class="page-number" href="/page/26/">26</a><a class="page-number" href="/page/27/">27</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/26/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
